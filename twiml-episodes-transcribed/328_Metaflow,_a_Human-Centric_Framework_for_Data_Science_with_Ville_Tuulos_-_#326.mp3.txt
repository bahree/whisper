Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
I recently attended the AWS re-invent conference in Las Vegas and I'm excited to share
a few of my many interesting conversations from that event here on the podcast this week.
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our re-invent
series.
Capital One has been a huge friend and supporter of this podcast for some time now and I'm
looking forward to sharing my interview with Dave Castillo, Capital One's managing VP
of machine learning with you on Thursday.
Dave and I discussed the unique approach being taken at the company's center for machine
learning as well as some of the interesting AI use cases being developed at the bank and
the platform they're building to support their ML and AI efforts.
Once again, check back in on Thursday, December 19th for that conversation.
To learn more about Capital One's machine learning and AI efforts and research, visit capital
one dot com slash tech slash explore.
And now on to the show.
All right, everyone, I am here at AWS re-invent in Las Vegas and I am with Vila Toulos.
Vila is a machine learning infrastructure manager at Netflix.
Vila, welcome to the Twentha AI podcast.
Oh, thanks for having me, Stan.
Yeah, I'm really looking forward to jumping into this conversation.
It turns out our timing could not have been better. Your team just announced a new open
source project called Metaflow that we'll dig into.
But before we do that, you mentioned before we started that you've been working on machine
learning infrastructure since what did you say 2010?
No, actually, like 2000, I'm sorry, 2000.
Yeah, 2000, I mean, it's kind of, I guess I date myself, but yeah, no, I've been doing
this for a long time.
And I started, even the term data science didn't exist back in 2000.
This was just during the dot com boom.
I was at the start up that tried to commercialize neural networks, neural networks really
weren't that popular back in the day.
We were away ahead of our time.
I mean, what was the startup called?
Will anyone have heard of it?
No, I mean, like, it's called grew soft.
Okay.
No one, like, and actually, it's probably better that no one has heard of it, but I mean,
it was, it was a great lesson learned, like I learned a lot than ever since then.
I've been working with data scientists, people who want to build machine learning, people
who need help with the infrastructure, who want to kind of a scale, who want to make
it better.
So I mean, it's a super exciting topic, and now it is, of course, absolutely mind blowing
that the whole topic has become so popular and it is on top of everybody's minds, it seems.
So, but yeah, no, I'll just keep doing it.
Yeah, well, it's certainly a far cry and popularity from 2000.
That's right.
Yeah.
What was your background?
What kind of, how did you end up doing ML infrastructure?
Right.
Right.
So I have a master's in computer science, and I was, I was part of a research group.
I'm originally from Finland, so I was working at the University of Helsinki, like we did,
many things related to Bayesian statistics, information retrieval, things like that.
And, and like, I was like always like gravitating towards thinking how we can make these things
easier.
I mean, there were people who are absolutely amazing on the theoretical side of things,
and I realized that, well, many of the things that people were already doing back in the
day, I mean, the fact is that machine learning is not the new field per se.
It was only kind of blocked by the fact that it wasn't easy enough.
It wasn't easy enough to apply these things.
It wasn't easy enough to scale them out, and that was really kind of the bottleneck
in the whole process of like kind of making these techniques and methods like more applicable
to more real life use cases.
And, and like, it seems that like now, I mean, 20 years later, I mean, things are much
better.
I mean, especially off the shelf, machine learning like we're absolutely amazing these days,
but I mean, I still think that there's work to be done.
I still, that's why we open source Metaflow, and that's why I keep doing what I'm doing.
Let's dive into Metaflow, and maybe the place to start is paint us a picture of the context,
you know, out of which Metaflow grew, and what the problem is that it tries to solve.
This was just announced yesterday, so now-
Like 20 hours ago, probably not many people will have, you know, heard of it by the time
this all.
Oh, well, people may have heard of it by the time this is actually published, but as of
today, and very few have.
So let me, let me start by giving some context.
So as you and like many, many of the listeners may know Netflix has been doing recommendations
for a long time.
So when you plug into Netflix.com, you see all the TV shows and movies that are recommended
to you.
And then obviously that's done by a machine, it's an algorithm not by people who choose
like what to show there.
And this is something that like the company has been doing for a long time.
And now a couple of years back, I think it was about three years back.
Like we started having the realization that more and more of machine learning use cases
were actually happening outside recommendations.
So so that of course Netflix is now probably like one of the largest movie studios, TV studios
in the world.
And like we have appetite to just like apply data science, machine learning and all kinds
of different areas.
And and like the first question we ask ourselves is that can we apply, can we use the existing
infrastructure that we have for recommendations for these other use cases as well.
And we are talking about things like natural language processing or like operations research,
optimizing production schedules, things like that.
And the realization was that these things are actually quite different like from the production
scale recommendations that we had.
And we started thinking that actually we need new kind of infrastructure that helps data
scientists that are working on these more internal use cases.
And that was really the context and motivation like why we started building Metaflow.
And then when we when we started working on this or when we started thinking even like
what to do, the interesting thing is that there is like two years back when we studied 2017
there wasn't much available in open source or even as commercial products out there.
And we started asking ourselves the question that like what are the things we should be
doing in the first place.
And quite soon the realization was that technically many of these things were kind of possible.
Netflix of course has all kind of infrastructure already available, but nothing was easy enough.
And that's when we kind of a in a way I mean light bulb went off in our heads that like
our job as the machine learning infrastructure team is to make things easier.
It's really the big human centric and like increase the productivity of data science instead
of saying that like look I mean we can build larger scale models than anyone else.
You contextualize this is making the data science process easier when I talk to folks that
are building ML infrastructure.
It's not always clear who their primary audience is whether they think of it as data science
or machine learning engineering.
Is that a hard distinction for you or who do you who you're supporting with?
Yeah, that's that's a great question.
So now the people who we support are data scientists and like really data scientists who
are absolutely capable of what the world's best experts in building statistical models,
building machine learning models.
But oftentimes they don't have a background in computer science.
So they are not software engineers.
So they are like very classical like a scientist with capital is in that sense.
And our job as the infrastructure team is to is to provide them with enough tooling.
So that they can be experts at the kind of really at the level of data science like built
the models.
And then they wouldn't have to worry too much about the software itself.
Now the thing there is that like the Netflix is culture is quite special and like we want
people to operate the end to end machine learning workflows independently.
So it's not so that the data scientist builds the model and hands it over to machine
learning engineer who then pushes the production.
But we want this data scientist to own own the project like from the prototype to production
like from like kind of all the way like kind of a from the raw data to kind of the business
results.
And of course I mean this is very hard like if you don't have the supporting infrastructure
and that's where we come in.
So maybe another bit on context to you know when I think about Netflix having followed
the company for many years the company has been very active in open source particularly
with regard to the way that it does cloud infrastructure generally chaos monkey and lots
of projects have been great contributions to the community as well as interesting projects
on the data science side paper mill is the one that comes to mind most immediately.
We talk a little bit about if we haven't really defined metaflow and like what it's doing
beyond it's a platform to support data science but you know talk about why it was important
for your team to open source this as opposed to build it to support the internal use case.
Yeah that's a great question.
Now as I mentioned we started about 2017 we started with metaflow and as I mentioned
like the motivation was really to help data scientists be really human centric human friendly
we started building this Python library and now over the past couple of years this has
become really popular like inside Netflix and we have given a few conference talks about
this outside Netflix as well and every time we went out and like talked about metaflow
we got like really great response I mean like people in many other companies felt that
like something like this is absolutely needed of course the need is very common like how
do we make data scientists more productive and now another interesting angle like you pointed
out that Netflix has been doing open source for a long time also another thing about Netflix
is that we have been a user of Amazon like Amazon Web Services AWS for a long time metaflow
like from the get go was built to support the cloud and like was built to really work nicely
with the cloud and the combination of having something that like really like makes data science
easy for people who are not software engineers as well as leverage is the scalability of the
cloud was really something quite unique and now when we when we really decided that we
want to open source metaflow we actually started also working with AWS to make sure that
people outside Netflix can have the same seamless integration to the cloud that we have been
enjoying internally at Netflix so so now it's really the kind of the combination of the
productivity and the cloud integration that I think that like makes metaflow a very interesting
package today as open source let's talk about its functionality what are the core features
that metaflow is trying to offer to the user base yeah it's almost a paradox in the sense
that we say that we are a machine learning infrastructure and it's there's one thing that
like we are kind of not too opinionated about and that's machine learning so so the also an
important realization is that they are absolutely amazing off the shelf libraries these days like
TensorFlow, PyTorch, Scikit-learn, HDBoost and we use all these libraries internally and we
absolutely don't think that like we can provide anything better than what you can get from those
libraries also we feel that it's very important that data scientists have the freedom to choose
the libraries kind of the best tool for the job so that's that's one layer but as you know
the models themselves are only a tiny part of into and machine learning pipelines and what metaflow
does is that it kind of helps you with everything else around the models so all the way like from how
you access the data like to how you execute the compute in the cloud in very easily so that like
you don't have to like learn new paradigms you can subscribe idiomatic Python code you can
just use the off-the-shelf libraries we take care of packaging everything with get care of like
scaling it out to the cloud and especially what we do and this is like really one of the key
features of metaflow is that we automatically snapshot absolutely everything you do including the
code the dependencies and the data that like flows through the kind of the data flow and why this is
important for a couple of reasons now it allows you to reproduce results afterwards and also it
allows you to inspect any internal state of the model and this is really important that machine learning
models as any software engineering artifact they never like work perfectly like kind of during
the at the first deployment so it's always a iterative process and and we feel that it's really
really important that like we provide first class support for debugging and troubleshooting and
that's why having the snapshots in the cloud is so useful so this is kind of the package that
metaflow provides like really from from in to end and then like then you can like do the modeling
as you want to do it well let's maybe start at the data and walk through that in more detail
what are all the features that you're providing kind of or maybe another way a better way to
ask this is what is the user experience from the perspective of you know accessing data managing
data creating transformation pipelines things like that yeah that's a that's a good question
so now Netflix has a very very mature data warehouse that's based in amazon s3 so we have
hundreds of data bytes of data in amazon s3 and and also we have a very secure query engines that
we use like presto and spark and and of course like many many data scientists are quite comfortable
using cql to access data so we we support that so you can you can use any libraries you want to kind
of access on many many companies who might want to use metaflow i mean they probably have a spark
or crystal and that's it that's totally valid way to access data now one very specific optimization
that we provide is that we notice that oftentimes in this machine learning applications you kind of
just want to one big data frame you want kind of everything so again and then like you want to
decide how you do feature engineering inside your python code so we provide this extremely fast path
which we call fast data that allows you to pull data directly from s3 to your machine learning
workflows very very fast so we are talking about 10 gigabits per second and and this along this
feature alone has been really really popular among amongst data scientists since previously they
used to wait for 10 minutes 20 minutes to get the data and then they could actually study trading
on the data and so forth and now you can get it in seconds so that really is a big it's a big
like boosting productivity that we provide and then as I said we we are not too opinionated we are
not like proposing a new paradigm for data processing this is not the yet another map reduce
or anything of that storage so you can use all the tools that you like you can use pandas
we also heavily leverage the arrow framework like for like managing in memory data frames
and then like you can just like in your normal python code like do the processing you like so now
of course the question that might be might be going on in your head is that well I mean how does
it scale like how it works if you have massive amounts of data one realization that we we had is
that also that Netflix not everything is absolutely like massive scale we have many many use cases
where the data actually fits in memory when you are a little bit careful how you how you handled
the handle the kind of the data representation and the example I always use that if you imagine
that today Netflix is about hundred and fifty million accounts if you have a data frame with
hundred and fifty million rows and then like a thousand columns maybe maybe we are talking about
the hundred and fifty gigabytes of data so which is something that you can hold in memory in on a
large machine so like then you can just treat it as a big pandas data frame or something of
that sort and it just makes life very easy obviously I mean it doesn't work for the largest
scale use cases but it definitely works for 80 percent of use cases so the fast data abstraction
is primarily focused on getting information in from S3 quickly and making accessible to the rest
of your Python code you kind of describe this as an optimization how how do you optimize that if
it all lives on the cloud yeah much of it beneath your kind of control horizon absolutely and that's
a that's a great question and now what one like unique perspective that we have here is that
Netflix has been working with AWS for more than ten years so we have a lot of operational
expertise and experience how how to deal with AWS and and the fact is that AWS is amazing it's
very easy to use but there are also certain things that just make life easier if you know how to do
right and like these best practices are something that we baked in into the meta flow so for instance
for things like S3 access they're like ways how you handle multiple connections to S3 so that like
you can maximize the throughput things like that absolutely things that we don't expect that like
any data scientists and not even more software engineers know about but I mean things that
that like we want to provide providers a service so it sounds like you're counting on folks doing
ETL and and data pipelines and Spark and other things you're not offering some new infrastructure for
that are you doing anything around like a feature store or some way for data scientists to collaborate
on data pipelines yeah I love that question well now the short answer is surprisingly maybe no
and and like we have been thinking about that a lot and one thing that we do feel very strongly
about is overall the question of collaboration and like how do we enable people to collaborate
and like we provide a lot of tooling to to help everyone to first organize their work so then
and actually it happens pretty much automatically so you automatically everything you do I mean
gets persisted in the cloud anybody else can see what you have done and and they can build on your
results now at the same time collaboration is actually pretty hard and like in general and
in particular in data science and and the reason for that is let me just give you a practical
example that if you have a data scientist let's say who builds some amazing embeddings like
that like these are the ways how we can characterize let's say some content or something
and now like someone else could potentially benefit from these features well that that works
all couldn't find maybe like kind of during the first iteration but now what happens if the
original data scientist who built the embeddings decides to like change something upstream and now
of course like you have this trickle effect to do everything downstream and it takes a lot of
discipline to actually now manage the upstream so that like these defects don't have any any adverse
effects in the in the models downstream and and and there's like a difficult trade off between
the stability like kind of okay being really stable so my consumers can consume my my features
versus then being able to iterate fast right and and like we are definitely we have been always
leaning kind of on the side of allowing people to iterate fast ultimately that is that is really
really like what matters like many of these things are very experimental and then like only in some
like very rare cases we understand that this is something like in some specific domains that like
now we should make it so that like more people can start like sharing a features or like maybe code
that generates the features rather than the features themselves so we have a bit like a shy
the way from like being a super press prescriptive like how one should like store the features
also the realization in our cases that we are supporting hundreds of different use cases
and what features mean for MLP is different what features mean for for for some like a statistical
like experimentation design pipeline which is different than what it means for for operations
research so it's very hard to be press prescriptive about that but I mean we are always keeping our
eyes and ears open like kind of how we should go about doing it so you definitely raise an
interesting point there a lot of the the work that I see happening in the platform space is trying
to drive this idea of reusability with you know either things like a feature store and reusing
features or reusable data pipelines or like an embedding store or things like that but
you know those the kinds of things are best done when the the teams that publish those treat
them as products right yeah contracts right and and you know often there are implicit assumptions
that are made in the thing that are hard to communicate yeah and if someone takes it off the
shelf and tries to use it they need to really know about those those kinds of things yeah
and you mentioned the the issue of when the original user wants to evolve the thing like
you know the complexity is multiplied by the number of us that have ended up using it yeah
so it's interesting to hear that the solution that you've chosen is to just not address it all
yeah or like lead lead users to kind of a maybe kind of a manage it by themselves also the kind
of the question I would like always to ask when people and like you believe me I mean people
ask about this all the time even like data scientists at Netflix that like well and we have something
like that I think like one one reason like why people feel in many cases that it is really important
is that like when when things are slow and when things are hard reusability of course become
super important like if you feel that oh I have to spend like three hours doing this then I'd
be amazing if I could just get get it off the shelf absolutely it makes sense on the other
than like if doing something takes about 15 seconds I mean then the reusability isn't quite that
important because if it's just easy enough I mean then like you can just do it by yourself and you
get exactly what you need so there's this like an interesting kind of a question that like everybody
should be asking that like is it that like we need the reusability only because like everything is
so hard and maybe we should like fix the root cause which is that things are you hard in the first
place or is there like some actual actual need for reusability like when it comes to features and
pipelines and now the thing is that like I absolutely do believe that like we need reusable data
pipelines and features for specific domains so if you have a use case like recommendations or in
my previous live I've used to do real-time bidding stuff like that I mean totally make sense
but then like the use cases that we are dealing with are often there's more experimental in nature
and it's just a question like where you are like kind of a in the life cycle of your project and I
think that the sharing like comes much at much later stages of the project and many many people
imagine and an interesting middle ground that I've seen work pretty effectively is to you know
when folks don't want to build a bunch of infrastructure to support this collaboration to
identify the you know the core features like you know there's going to be user features there's
going to be product features you know in the case of Netflix maybe you know there are you know
specific features around the the films things like that that probably a lot of data scientists are
going to want to use and so the infrastructure team kind of owns you know those not necessarily
through some generic infrastructure for right you know feature reuse but there are kind of vetted
you know data pipelines and features for specific entities that everyone's going to want right
right and I think that there's a crucial difference between kind of facts and features so I mean
like we have a highly created tables that are easily available and widely shared for facts for
things like I mean like what kind of a movie is people watching so forth and then like people can
derive different type of features easily like from those so there's the question that yes I mean
like we want to share data we obviously want to share facts but and like we occasionally like want
to even share code that produces the features but do we want to share the final features like as
is I mean that is a trickier question right right right so the the perspective then to kind of
dig into that is a lot of times we think of the the features as this monolithic thing and you know
them being hard to create and the data scientists spend 80% of their time you know doing feature
engineering and getting the data and we throw that number around it sounds like your perspective
might be well a lot of the challenges in getting access to the facts right so if we make
a getting access to the facts easier then the features from the facts is more incremental and as
were your 15 second versus three hours right right and like when you think about it I mean there's
the question that like ultimately like how do you how do you improve the quality of models I mean
the fact is that oftentimes it is much about data much about feature engineering so in a way I
mean that is like that should be in a way I mean oftentimes the center of your attention I mean
there's nothing wrong about that so I don't know how it could work if all you can do is to check
some checkboxes and say that okay these are the features I want to use we actually want that
people apply their domain knowledge and understanding in data and producing the features we just
want to make it very easy to do it okay so you've got this high performance access to data and then
folks are building out their models in Python does Metaflow include integrate with paper mill
is there a relationship between it too yeah so now we have Netflix employees actually a number of
people who contribute to Jupiter project and we also recently opened source another notebook project
called Polynote like very exciting and and like they are a node Poly Polynote okay Polynote yeah
Polynote dot org and and we work closely with them and it's an interesting question like how
how how we like we see the data like notebook should be used in in conjunction with Metaflow so
actually as of today we we require that people write the Metaflow code outside of notebooks and
this is a very conscious decision like for the reason that we still feel that like much of the
tooling for producing software is is kind of a better like kind of a been in kind of a traditional
ideas and and like you have the the version control for your code and all that stuff now that being
said we support notebooks first class as a way to inspect the results I mean that's where notebooks
absolutely shine that you can you can like what you built a library you can use it in a notebook
exactly totally and this is really surprising because I think a lot of us hold up Netflix as the
poster child of an organization that is trying to extend notebooks beyond experimentation even
into production yeah and and actually like one one thing I mean like now like building on that
thought that like building this visualization in inspecting results is so so easy in notebooks
this kind of relates to the question of of UIs and interestingly Metaflow as of today doesn't come
with any graphical UI per se and again I mean that's a very thoughtful decision based on the
based on the realization that like many of the use cases we support it have very specific needs
and it would be very hard to have like one size fits all UI then like factually yes we could
provide some metrics showing that okay here's our model converges and we actually ask data scientists
if this is something that they would not need or want and like they say that well actually that's
really not the most important thing maybe it would be nice to have but I mean not the most
important thing and often I'm sitting they want to see a something really related to their like
the business question or the model itself and that's how they want to monitor the results
and what we do today is that we allow people to define exactly the visualizations and the metrics
they need in a notebook and they can data scientists stop more than capable and like actually like
really eager to build these visualizations by themselves and now since you brought up the topic
of paper mill what paper mill does is that it allows one to take a notebook and then kind of a
render the notebook in a headless manner like kind of a been in a scheduling system so then
interestingly what we get is that we can render these notebooks automatically whenever the model
updates and like we can push it to some location so that you can just go in your browser to some
location and you always have a place like where you where you have a custom visualization of the
latest state of your model so now I mean in short how we use paper mill would kind of use tools
like paper mill to basically use notebooks almost as a UI in addition to using notebooks as a scratch
patch so so yes I mean like Netflix does a lot of work around notebooks and for notebooks but at
the same time we are like really thoughtful that like what are the kind of good use cases for notebooks.
Going back to the data scientist experience when it comes to experimentation, training,
model development, what is Metaflow providing there? Yeah so one of the first things that we
wanted to do well is the support for the local development experience so in my previous life I
used many other Python frameworks for workflow management like Luigi and Airflow and they
create tools especially in production but it turns out that it's not so easy to actually develop
your workflows. The question is what you handed when you're trying to run another. Exactly so if
you think that what's really the fastest way to iterate on something new is that like you have a
Python script running on your laptop and you can just like keep running it there's no no latency
nothing I mean you just hack the code you iterate and or you do it in a notebook so that's the
perfect experience then we wanted to replicate that experience with Metaflow so that you can run
Metaflow locally and like you can develop everything it's super fast and so forth. Now the next
question is that okay once you have that local version working how do you then like even scale it
out not even take it to production but I mean let's say you want to like try it with larger
amounts of data and in some cases it might be that you just have to re-implement the whole thing
let's say using Spark which is a fine way of doing it but at the same time like requires a bit
of a change in paradigm also it doesn't always play nicely with the off-the-shelf libraries like
TensorFlow, PyTorch and XT boost and so forth. So what Metaflow provides is that you can first
prototype locally as you would do with any Python script and there's a very easy one line of code
or like just one command line switch and you can start running it in the cloud with arbitrary
amount of resources and then like what with the single command you can export the whole workflow
to a production scheduler so that then it becomes totally production ready and like a part of the
overall data ecosystem at your companies. So does the library have its own workflow type of
implementation paradigm separate from you know something like Argo or Airflow or you know
Luigi. Yeah yeah we do have a local for the local experience we do have a local workflow engine
and now at the same time like we absolutely like the the workflow part is only a kind of a small
part the crucial but small part of Metaflow so we don't want to become a generic workflow engine
we think that all these off-the-shelf tools including also AWS that functions could be a great way
to kind of run these workflows in production internally at Netflix we are using a tool called
Mason like there are a few like presentations about it available publicly that provides all kind
of a nice operational features like high availability, alerting, UIs and like you can of course like
get something like that from from Airflow as well. So the idea is that you can define your workflow
in Metaflow and then you can export the workflow without any changes to one of these production
schedulers so you kind of get the both best of the both worlds you get the perfect local experience
as well as the kind of the production grade like workflow scheduling then like when you want to do
things at large scale on a daily basis. We're talking about local and then production grade
is there also a non-local distributed training type of environment that users can
push to to scale up their ability to train a model. So that's where we really heavily leverage
the cloud. So the beautiful thing about about a system like AWS and the same thing of course
applies to any other cloud provider is that you can imagine that you have virtually unlimited
amount of compute capacity and like storage capacity and we really heavily rely on that so you
can define your workflow and like you can define the task in your workflow and like you can add
like in our case at the batch decorator in which case we just like a take your Python functions and
execute them in the cloud using AWS batch in the case of Metaflow today and we take care of like
take packaging your code and like sending it to the cloud and getting the data and persisting the
data all that stuff. So the code look exactly like what you would do locally in Python but at the
same time what's really amazing is that you can specify the resources you want you can say that oh
I want I want 200 gigabytes of RAM or I want eight GPUs and so forth. So the abstraction that we
would like to have is that imagine that you had a laptop that like with the press of a button you
can just like change the configuration on your laptop so you can say that like well I mean what if
my laptop had like 200 gigabytes of RAM or what if my laptop had eight GPUs I mean that would be
just amazing I mean that would be the kind of the ultimate way to do like kind of this development
because you get the low latency as well as like almost unlimited resources. So that's kind of the
vertical scalability part when you just want to get the bigger box there's of course the question
that well I mean what if I want more boxes and and oftentimes what happens in our work flows is
that we are not only training a single model but we are training multiple models and it might be
that let's say we train a different model for every country or it could be that we just want to do
a hyperparameter search. So we we define a agreed off of of 200 different parameterizations of
the model and then we just fan out everything to the cloud and and like the cloud then like the
xcare of like building all the models in parallel and then like metaflow takes care of fending in
the results and you choose the model that you want to use. I know like a model parallelism. Exactly
opposed to data or exactly and then of course like for the cases when you absolutely need like
distributed models and and this happens although again it's more of a like a 20% use case rather than
an 80% use case in our case. Is that roughly correlating to traditional models versus deep learning
use question not not necessarily I mean it's it's amazing like even like how far you can get with
one big box with with the number of GPUs even with with with the deep models but yes I mean like
of course like in extreme cases let's say in computer vision you absolutely do need distributed
learning like for for DNNs and and then like you can you can we also provide integrations of things
like Amazon Sage maker that have distributed TensorFlow built in so that's an easy way. The
fact is that distributed learning and like setting up the infrastructure for that is pretty hard
and managing it pretty hard so what we do is that first I mean like we we try to remind people
that if you don't need it if you can do it without it so I mean that that's always a good first
depending if you need it I mean there are like infrastructure that can take care of it.
You mentioned Sage maker I was surprised to hear that at the same time that AWS is announcing
a ton of new ML ops ML in for our capabilities to Sage maker, experiment management,
piece IDEs, studio debugger they're also you know actively promoting what you're doing with with Metaflow.
You know why do you think that they're excited about it? We think about the data scientists
productivity in very much the same way Amazon has realized that that the kind of the big bottleneck
like for for for companies for being able to benefit more from data science is really the data
scientists productivity like more than anything else and they are heavily investing on improving
that situation like you mentioned the IDEs and the debuggers like all of those things are like a
huge steps towards increasing productivity and now there is in the single formula how to increase
productivity and like what we have done internally at Netflix over the past couple of years it has
proven to work it is widely used inside Netflix and Amazon was really excited to see that there's
a pattern that really works nicely with AWS it leverages the best parts of Sage maker and batch
and S3 and so forth and and like we can just make it available to everybody so I think like you
have very much aligned like when it comes to that. So what are the integration points with Sage maker?
Yeah so like we are working on this integration that would allow you to take any Metaflow artifact
so we like something that like we we haven't touched before is that is the question that like how
we manage data inside this workflows so we automatically persist everything in S3 and and one
thing about the Sage maker is that it's really amazing like with all the built in training algorithms
that come with it but I mean still there's some some kind of a bookkeeping that you have to do to
kind of persist the input data in S3 and like take the outputs and and and so forth so when you use
Sage maker with Metaflow all that stuff is is taking care of for you so like we we handle the the
S3 management for you and then you can just basically with one line say that okay then here's my data
I want to train a model and then the model comes out and then you can use it in your Metaflow workflow
as before and what's really exciting is that also in the workflow you can mix and match different
parts also like an important fact is that this machine learning workflows are never only about
machine learning oftentimes there's the data part and you may have your training part and then
maybe there's some post processing like before you push the results somewhere like maybe that you
write them to a table or maybe you deploy them as a microservice so you take a couple of steps
after that so now Metaflow helps you to manage the whole pipeline and like somewhere in the pipeline
and you can totally leverage Sage maker and like maybe you deploy to Sage maker in the end but then
like Metaflow is kind of the substrate that that combines everything together is but I'm imagining
that there are also a number of areas of overlap for example it sounds like while you're not providing
visualization type of an experience for managing experiments you are managing the experimental data
do you go to any specific length to try to make this you know smooth for for users you know what
you should use where and when given that the company so heavily invested in the AWS tooling and
ecosystem and and users will be you know it sounds like you've got some fair usage of Sage maker
internally right so well I mean now not one thing is that over over the past many years Netflix has
been building a lot of infrastructure like even before Sage maker existed and like for instance
for notebooks we have our own like internally infrastructure how people can provision notebooks so
they can get their managed notebook instances and so forth and now given that that's something that
Netflix built internally it works for us we are using that at the same time we recognize that
that's not available outside Netflix it's really deeply embedded in our environment and that's
on the other hand something that Sage maker provides so we feel that it totally makes sense to use
Sage maker notebooks if you don't have a managed notebook environment of your own and and now I mean
we are like evaluating all the new offerings like what you mentioned about the experiment tracking
UI that we have we have some like approaches how we have solved similar problems in the past now
I mean it just made the release and like now we are evaluating the like like where things will go
in the long terms yeah I'm drawing a lot from the cloud side of things but my impression has
been that you know Netflix has always been hey if we don't have to do it we don't want to do it
we'll let AWS do it yeah and so I imagine as Sage maker evolves you'll be continually evaluating
you know what pieces of it you'll continue to support versus yeah I think you're spot on and I
think it's an important like kind of a important design principle in Metaflow overall that Netflix
is really not in the business for like building this software and we are not in the business for
selling machine learning solutions I mean ultimately we want to entertain the world I mean that's
what we do we really focus on that one one case and now so it happens that like an important
component of our business goal is to be able to use data and data science and like these are
the tools we need so we build the pragmatic tools that like answer like a very pragmatic business
questions and I do feel that many other companies are in the same position that there are not in
the business for building machine learning tools they are not in the business even like building
machine learning per se it's just want to leverage it in their business to improve their whatever
business outcome state they have in mind and I think that's why I mean did the pragmatic approach
that Metaflow has taken like we can leverage the cloud when it makes sense like we can just
focus on making data scientists product even in like a very practical business use cases and
probably it resonates with many other companies as well so one of the things that I'm curious about
is the flow of projects and use cases onto into and out of off of the infrastructure that
you you've built with Metaflow and more broadly ML infrastructure and Netflix you started off by
saying that recommendations you know wasn't even in scope right it's a mature use of machine
learning you know they've been doing it for a very long time so it's not something that you
are building for what you're primarily building for are these smaller teams newer use cases
and correct me at any time but smaller teams newer use cases that you know haven't built up
the infrastructure support internally I imagine that some of those at some point grow in complexity
and sophistication and maybe need more than what the infrastructure can offer and you know may
start to look at building their their own things or I'm generally curious about the way that
dynamic is managed at Netflix and and how both the projects use of the infrastructure and the
infrastructure evolves yeah well let me illustrate this like by by kind of a giving kind of a two
metaphors I mean one is that production is really a spectrum it's not the dot that hears the
production but it's a spectrum and so indeed we do have use cases that are absolutely business
critical absolutely core to the business and like absolutely must not fail but still it might be
that the scale is not huge so it's a like smaller scale maybe in terms of the data size but it's
not smaller scales in terms of importance so it's really like a spectrum or maybe even like two
dimensional plane like where you have different production use cases that's one thing
metaflow there like covers a certain like part of that plane so and then the other important
thing about this data science project is it's a funnel so we have tons of ideas that's like all
companies doing data science employing data scientists know that all kinds of idea ideas what you could
be doing what we would like to have optimally is the situation where people can really cheaply
like a test different ideas experiment and then the fact is that like by design like many of
these things actually like don't don't like so like enough promise that like we want to continue
the investment and that's totally fine I mean that's how it's supposed to work and that's why
it's a funnel and then like then when you go deeper down in the funnel I mean things do
graduate and like then things might get bigger and so forth and again I mean now going back to the
other idea of the spectrum I mean it's totally fine that like in some cases you can like totally
manage the project operate the project in inside metaflow and that's what we aim to do in 80%
of use cases now in some other cases like cases like if you are like super lucky you go through the
funnel like your project becomes super super important also it turns out that it becomes super
super large scale I think in those cases it's totally fine that then we say that now we want to
build like specific infrastructure for this use case I mean there's nothing wrong with that and
I realistically if you think about any frameworks that's how it always works so you start with some
of the shell framework you enthusiastically use it then the company becomes big enough and then
you realize that instead of using this like off the shell frameworks we have to build a custom
solution I mean that's how it works and that's the reality the key thing there is that this happens
in a very small fraction of all use cases and it's actually counterproductive to design a system
that would theoretically like support any use cases one of those like a special snowflakes and
and that's why we are really optimizing for the let's say the 95% of use cases while acknowledging
the fact that maybe then the 5% graduate to a different system in the end. So you mentioned production
as a spectrum and we haven't talked a whole lot about production what is Metaflow supporting
from an inference perspective yeah like another important principle and by the way we list all
the kind of the design principles of Metaflow on the on our in our documentation that really the
kind of the guiding things for our work is the realization that like in real business environments
these data science projects come in all shapes and sizes so there is in like one unified grand theory
that this is how we always do inference or this is how we always deploy they are like so many
different ways so sometimes you want to push results to a table where it's like the results get
shown by a upload dashboard like sometimes the results are just used by another data scientist
in a notebook and sometimes they may be power another system like as a microservice and now
often times when when people talk about the inference and deployment they only think about the
microservice use case that's one use case and we do have internal support for that but that's
like only one of the use cases one of the kind of the output modalities that we support
and internally like what we do with Metaflow is that we have this kind of almost as a function
as a service platform that can consume the results of Metaflow pipelines and then deploy this as
as basically like restful APIs or endpoints that then cost them internal AP like web applications
can use or other microservices can use so this is of course like a very typical thing what you
can also get with with statesmaker hosting service or the many many other platforms that provide
similar functionality that really the important thing in our case is that there is a really seamless
integration to Metaflow itself one one thing there is that we only do immutable deployments so
all the artifacts produced by Metaflow pipelines are strongly versions strongly snapshot as immutable
data blobs in s3 and then when we do a deployment we take one of these immutable data blobs
which might be a model it might be a data set and then we push it as an as an immutable deployment
to as a microservice and and what this current is as this then when you have an internally UI
that like uses the machine learning models we have the perfect lineage that we know exactly that
this model was produced by this workflow that was produced by this data set and like the
important thing here is that the data scientists didn't have to do anything to make this possible
because the fact is that this kind of bookkeeping is a bit boring it says but at the same time it's
super super important so we just wanted to make that the experience happened out of the box
so the data scientist doesn't need to be thinking about you know Git and using those kind of
that's actually an interesting point that you mentioned Git since we like an early on like there
was the question that that like while I'm a worsening like maybe we should use Git and
should we use Git for a model should we use Git for data should we use Git for code obviously I
mean Git is a good match for a code at the same time there's an interesting impedance mismatch between
Git and then it's like a data science workflows that are you supposed to make a commit every time
you run your notebook that's kind of not how Git is supposed to be used I mean the Git history
looks kind of ugly if you do that it's kind of useless anyway so what we what we do is that we
actually have a built-in content at your storage kind of a Git like where we automatically store
like every time you run something we snapshot your code we snapshot your dependencies which is
really important and which not even mean in the context of a notebook though right well I mean
in the case you know like exactly or like in the case of metaflow you just take your workflow
you run it into and unlike we persist all of those things automatically so you don't even have to
use Git and and we kind of there was the idea that well I mean can we just like have some kind
of a requirement or abstraction that like everybody uses and it's just felt that like it's just
didn't feel quite right then we decided that well I mean let's build something that like does the
job while not imposing any additional cognitive overhead that hears it yet another thing that you
have to consider because I know that like if I were a data scientist by myself I wouldn't like
to worry too much I mean my mind is elsewhere when I'm building the model I mean I don't want to
worry about like commits and pool requests and whatnot so what is the you mentioned all the artifacts
are stored in s3 for models and everything else but what is the artifact for a model is it
pickle files jar files that kind of thing is it containers how are you what's kind of the currency
of a model as opposed to the system right right so by default and this is like absolutely like one
of the key features of metaflow is that in addition of like helping you to build your workflow as a
as a DAG we also manage the internal state of the DAG so we automatically when you do something
like you have a self dot model equals something we automatically persist at artifact so there's no
need to write lines like store model load model like store data load data and and that reason for
that again I mean goes back to the cognitive overhead that like every time you use a framework
that forces you to decide like do I want to persist something you kind of have to think that well
do I want to persist this I mean is this important enough and oftentimes people are just like really
conservative thinking that like no I probably not important enough and then what happens is the two
weeks after you deploy the product like it has been running two weeks in production then it fails
then like you need troubleshoot like what was going on and then you don't have the piece of data
that you would need to understand what was the internal state of the workflow so you could actually
understand why it failed and that's why we so enthusiastically persist everything and since you ask
about the format now again I mean we don't believe that there is necessarily like a single universal
model serialization format so since we allow people to use all kind of frameworks what we do
use that for normal basic Python objects we use pickle and like if people works I mean that's
all good and fine we also as I mentioned we use like a content address storage we don't store copies
we also compress everything and then like for models like let's say you have a cross model I mean
if the modeling library has the serialization format of its own I mean we encourage people to use
that so we don't try to abstract the way too much since the realization again is that like our
users use such a heterogeneous set of different libraries that it would be always kind of like a
cat and mouse came to kind of a try to implement the latest like a support for some serialization that
like someone decided to use and and like it turns out that like given the additional support
that people get from metaflow I mean kind of once you have the model adding the one line theory
that says that okay now I need to save my cross model I mean it's not too much works you mentioned
that model deployment for inference is at least the microservice based approaches functions
is that specifically Lambda or do you have your own kind of function runner abstraction thing
that you've built as part of metaflow so yeah good good question so this goes back to the
previous question you had about like how we work with stage maker so Netflix has a very mature
container management system called titles it's actually open source and as a part of that like we
have been developing also kind of function as a service platform and like we are using that
internally at the same time the functionally this is very close to Lambda and like there's actually
an open kit happy issue now for a metaflow to to see like how much appetite there would be outside
Netflix to have something like this available and like if there was appetite we could totally do
something similar on top of Lambda at the same time this goes back to the question that while there
are many other like tools doing doing similar things already available like stage maker deployments
I mean maybe if people would be happy to use that I mean that would be an avenue like for providing
hosting for metaflow so as the idea that the metaflow that's open source is a subset of the
metaflow that is used internally with one of the big differences being hooks into these other
Netflix projects and then when you the thing that an open source user would use is kind of scaled
back so that it doesn't have the dependency on these other internally used projects and then
if they you know if there's enough interest you might build hooks into the publicly available
AWS analogues of those things or does metaflow you know include does it ship with hooks into the
other Netflix open source ecosystem projects like how do you manage yeah all of that well I mean
separately it seems like you know potentially a big distraction if you're your fundamental
goal is to support your data scientists and you have these other people wanting to you know
deploy the lambda like why do you care exactly I mean that's that's like honestly the reason why
two two years for us to actually open source get off low we like wanting to be very conscious
about that question that like internally like one of the one of the reasons why metaflow has been
so so successful is that like really fanatic user support that we provide not only metaflow
comes with really great documentation including the open source version but we also support our
people on slacking on chat like really actively and like we wanted to provide similar kind of
experience for everybody now at the realistically speaking again I mean like we we have very finite
resources so so we wanted to be really thoughtful like how we how we kind of manage the workload
now when you ask about like all these other like systems and like how we think about that well the
fact is that like we release metaflow just 20 hours ago and that and we we like wanted to be very
open about like what exists today in open source and what doesn't so you can go to the documentation
and check the roadmap so we have about five six internal components that are not yet available
open source including interesting stuff like a slackbot and like we have this internal like a
data frame implementation stuff like that and we want to open it like ask people that like is
something that like really would be interesting and useful to you and then we go with that in the
long term the plan is definitely not to have to separate code basis we are actually like in in the
in the first quarter of next year like migrating our internal systems to use the open source
version as well which is like not that far the only difference is that like we we needed to get
rid of these like internal like systems that like practically are not available like outside Netflix
one thing I'll throw out there we recently started doing what we're calling demo casts which are
kind of like an interview like this but with a kind of concrete you know background being a demo
and if folks are interested and you're interested in open to it you know maybe it would be
interesting kind of walk through this and and see see it and absolutely and maybe maybe like this
is a good opportunity for me to pitch like one like really interesting feature that we have
so when we open source metaflow we also realized that our cloud integrations integrations
to AWS is one of kind of the killer features of metaflow now at the same time we knew that like for
many individual data scientists it would be pretty hard to set up all the requirement required
systems in AWS maybe they don't even have access to the companies AWS console so what we provide is
this thing called the metaflow sandbox that allows anybody to just like a sign up for kind of a
test evaluation environment and and we and actually this this like works like kind of it's paid by
by by by Netflix and you get your own private AWS environment where you can test like all these
cloud integrations like without having to set up absolutely anything in AWS and the idea with
this is that like people especially individual data scientists can get the feel that what would
it what would it feel like if I had access to this like infinitely scalable laptop and I can
just like a fan out my compute to batch and like store everything in S3 and then like if they feel
that like well I mean this is really something that would increase their productivity then it's
it's kind of a easier to take the little bit of effort that it takes to kind of set up your own
AWS account to kind of make this possible so like if if people are interested you can go to the
documentation and look at the section about about metaflow sandboxes so it's fully functional you
can sign up today and like kind of get your own own private sandbox for for a while to use and
like that's it that's just a kind of a takeaway to the question about the demo so yes absolutely
I mean like I would be more than happy to do a demo with you if people are interested awesome
so if you're interested in that reach out however you like to reach out to us and let us know and
be happy to do it if there's interest quick question on the the sandbox is there a limitation
there is this like a you know Google collab where everyone gets six hours 12 hours I forget the
limit of free GPU instances or is it only the management plane but not actually instances for
training like yeah no it's it's actual instances like for training you get up to a cluster of eight
boxes with htpu 64 CPU cores like a 32 gigabyte machines each so it's like a decent size I mean
you can do a lot with those machines you can like persist any data you can use it with your own data
this is really important since we have seen so many tutorials and examples in data science where
you have something like the house pricing and everybody knows how it works but like really what
resonates with you is your own data and like you understand of course the important thing is
that like you shouldn't use anything that's actually privacy sensitive but I mean you can get the
feel like with with some data that like you really care about and you can do data science as you do
you can even use any of the shelf libraries you can we come with built-in like integration with
your conduct so you can do by any conduct package you see one inch of flow the xg boost you name
it and and you can execute these things in the cloud and then see how it works and the only
limitation there is that well let me first there's no internet connection we handled the kind of
the dependency installation for you but I mean the idea is not that you use this to kind of call
all kinds of outside services you don't need that for data science and then the second limitation
is that like well instead of like a collab couple of hours you get three days to buy default if you
need more time I mean just reach out to us and we give you more time got it awesome awesome well
Vila sounds like an awesome project that I'm looking forward to digging into and I'm definitely
rooting for folks to reach out and you know express some interest so that we can get back together
and kind of walk through it yeah thanks Tom thanks so much thanks
all right everyone that's our show for today to follow along with our reinvent series visit
twimmelai.com slash reinvent 2019 thanks once again to capital one for their sponsorship of this
series be sure to check out capital one.com slash tech slash explore to learn more about their
ML and AI research thanks so much for listening and catch you next time
