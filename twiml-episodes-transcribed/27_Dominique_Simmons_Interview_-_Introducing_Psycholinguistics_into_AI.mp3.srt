1
00:00:00,000 --> 00:00:16,200
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,200 --> 00:00:21,360
people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,360 --> 00:00:25,240
I'm your host Sam Charrington.

4
00:00:25,240 --> 00:00:28,560
I think you're really going to enjoy today's show.

5
00:00:28,560 --> 00:00:35,160
Our guest this week is Dominique Simmons, applied research scientist at AI tools vendor

6
00:00:35,160 --> 00:00:37,840
dimensional mechanics.

7
00:00:37,840 --> 00:00:43,200
Dominique brings a really interesting background in cognitive psychology and psycho linguistics

8
00:00:43,200 --> 00:00:47,760
to her work in research in AI and well to this podcast.

9
00:00:47,760 --> 00:00:52,120
In our conversation, we cover the implications of cognitive psychology for neural networks

10
00:00:52,120 --> 00:00:57,800
in AI systems and in particular, how an understanding of human cognition impacts the

11
00:00:57,800 --> 00:01:03,360
development of AI models for applications such as media processing.

12
00:01:03,360 --> 00:01:08,400
We also discuss our research into multi modal training of AI models and how our understanding

13
00:01:08,400 --> 00:01:11,400
of the human brain has influenced this work.

14
00:01:11,400 --> 00:01:16,200
In addition, we explore the debate around the biological plausibility of machine learning

15
00:01:16,200 --> 00:01:17,720
and AI models.

16
00:01:17,720 --> 00:01:20,880
Really, this was a great discussion.

17
00:01:20,880 --> 00:01:24,280
Before we jump in though, I've got a question for you.

18
00:01:24,280 --> 00:01:31,000
How would you like one of our beautiful this week in machine learning and AI laptop stickers?

19
00:01:31,000 --> 00:01:32,880
I know you want one.

20
00:01:32,880 --> 00:01:38,120
We've already sent stickers all around the world and we'd love to send you one as well.

21
00:01:38,120 --> 00:01:43,800
All you need to do is pull up the show notes page, which in this case will be at twimmolai.com

22
00:01:43,800 --> 00:01:50,600
slash talk slash 2323 and drop us a note with your favorite quote from the show.

23
00:01:50,600 --> 00:01:56,160
You can also post a quote via Twitter, just mention at twimmolai or on our Facebook page.

24
00:01:56,160 --> 00:02:00,560
Links to all of these will be available in the show notes.

25
00:02:00,560 --> 00:02:04,120
I can't believe it's mid may already.

26
00:02:04,120 --> 00:02:09,320
Next week, I'll be hosting my future of data summit in Las Vegas as part of the interop

27
00:02:09,320 --> 00:02:10,560
ITX conference.

28
00:02:10,560 --> 00:02:16,680
I've got a ton of great speakers lined up for the event, including folks from Intel,

29
00:02:16,680 --> 00:02:23,560
Microsoft, GE, Capital One, level three communications and Walmart, as well as leading industry analysts

30
00:02:23,560 --> 00:02:26,800
and startup executives.

31
00:02:26,800 --> 00:02:32,440
Topics will span IOT and edge computing, data management, and of course machine learning

32
00:02:32,440 --> 00:02:36,200
in AI, blockchain, and much more.

33
00:02:36,200 --> 00:02:40,120
If you're planning to attend interop, I hope you join us at the summit.

34
00:02:40,120 --> 00:02:45,120
And if you've been meaning to attend the summit, but held out until the very last minute,

35
00:02:45,120 --> 00:02:47,040
it is not too late to register.

36
00:02:47,040 --> 00:02:55,280
In fact, you can do so using my 20% off discount code by visiting twimmolai.com slash interop.

37
00:02:55,280 --> 00:03:00,040
Of course, if you have any questions about the summit, please feel free to reach out to

38
00:03:00,040 --> 00:03:02,440
me via the contact page.

39
00:03:02,440 --> 00:03:10,840
And now on to the show.

40
00:03:10,840 --> 00:03:17,640
All right, hey, everyone, I am excited to have Dominique Simmons on the line.

41
00:03:17,640 --> 00:03:23,200
Dominique is an applied research scientist with dimensional mechanics.

42
00:03:23,200 --> 00:03:24,680
How are you, Dominique?

43
00:03:24,680 --> 00:03:26,080
I'm doing great.

44
00:03:26,080 --> 00:03:27,080
Thank you.

45
00:03:27,080 --> 00:03:28,080
How about yourself?

46
00:03:28,080 --> 00:03:29,080
Doing very well.

47
00:03:29,080 --> 00:03:30,080
Very well.

48
00:03:30,080 --> 00:03:33,720
I wanted to start this conversation by talking a little bit about your background.

49
00:03:33,720 --> 00:03:41,680
You have a master's degree in cognitive psychology and psycho linguistics, and you have ended

50
00:03:41,680 --> 00:03:45,680
up doing work in artificial intelligence.

51
00:03:45,680 --> 00:03:51,640
Tell us a little bit about your background and kind of your path to working in AI.

52
00:03:51,640 --> 00:03:58,280
So I'll start from the very beginning.

53
00:03:58,280 --> 00:04:02,480
As a child, I was always fascinated by the brain.

54
00:04:02,480 --> 00:04:06,240
I grew up as an only child, and I found myself journaling.

55
00:04:06,240 --> 00:04:12,320
When I wasn't with my other friends, I'd journal and make observations about what people

56
00:04:12,320 --> 00:04:20,720
are doing, what was going on in my environment, and what I was fascinated by what made them

57
00:04:20,720 --> 00:04:23,040
do the things that they did.

58
00:04:23,040 --> 00:04:29,320
And that carried on throughout school, and eventually in college, I had a great mentor

59
00:04:29,320 --> 00:04:37,480
who brought neuroscience into our curriculum, and I started, you know, tabling for brain

60
00:04:37,480 --> 00:04:38,480
awareness week.

61
00:04:38,480 --> 00:04:44,920
I started, you know, learning about the brain and the circuits and, you know, just how

62
00:04:44,920 --> 00:04:47,080
these processes come about.

63
00:04:47,080 --> 00:04:50,920
And so, again, that carried me through.

64
00:04:50,920 --> 00:04:54,400
I knew that I wanted to do that in graduate school.

65
00:04:54,400 --> 00:05:04,960
And after becoming a lab manager, University of Illinois, Urbana Champaign, being a post-back

66
00:05:04,960 --> 00:05:15,120
intern at UMass Amherst, I got a lot of exposure to different types of psychology, anything

67
00:05:15,120 --> 00:05:24,280
from, you know, infant psychology, cognitive psychology, psycho linguistics, music cognition,

68
00:05:24,280 --> 00:05:25,280
you name it.

69
00:05:25,280 --> 00:05:30,400
I was just very fascinated with all these aspects, and you could see these parallels coming

70
00:05:30,400 --> 00:05:33,640
together as well.

71
00:05:33,640 --> 00:05:41,960
So in graduate school, I studied specifically multi-sensory perception, which is the influence

72
00:05:41,960 --> 00:05:47,520
of one sense over another, and how senses interact.

73
00:05:47,520 --> 00:05:56,120
So I come from a school of thought where the brain is agnostic, if you will, to input.

74
00:05:56,120 --> 00:06:05,120
And once the input, you know, is processed, then, you know, it becomes sound or it becomes

75
00:06:05,120 --> 00:06:08,720
hearing or, you know, any one of these senses.

76
00:06:08,720 --> 00:06:16,000
But the actual input is just information.

77
00:06:16,000 --> 00:06:23,200
The brain likes getting information and, you know, learning and processing.

78
00:06:23,200 --> 00:06:32,280
And then at the, the later stages, that's when it becomes what we know as, as, as senses,

79
00:06:32,280 --> 00:06:33,600
sensory input.

80
00:06:33,600 --> 00:06:34,600
Okay.

81
00:06:34,600 --> 00:06:38,880
So with that, that gave me a unique background.

82
00:06:38,880 --> 00:06:44,400
So I will say that in graduate school, I got a little bit tired of the theory.

83
00:06:44,400 --> 00:06:52,160
You know, of course, it's, you know, critical, it's a critical foundation for a lot of work.

84
00:06:52,160 --> 00:06:58,000
But I started to get, you know, to itch for applications of these ideas.

85
00:06:58,000 --> 00:06:59,000
Right.

86
00:06:59,000 --> 00:07:00,760
You know, this idea of integrating senses.

87
00:07:00,760 --> 00:07:04,240
How can we, how can we make this into a device?

88
00:07:04,240 --> 00:07:09,600
You know, what, how can it help, you know, non-hearing populations or populations that

89
00:07:09,600 --> 00:07:13,640
have sensory deficits?

90
00:07:13,640 --> 00:07:22,360
And so I worked on a project, a brain training project on hearing, veterans with hearing

91
00:07:22,360 --> 00:07:23,360
loss.

92
00:07:23,360 --> 00:07:32,320
And essentially it was a, a program, like an auditory game, auditory video game, where they

93
00:07:32,320 --> 00:07:38,640
had to, to learn complex sounds in order to navigate through the game.

94
00:07:38,640 --> 00:07:42,760
When the sound goes up, they had to jump, but the sound goes down, they had to, you know,

95
00:07:42,760 --> 00:07:43,760
back under something.

96
00:07:43,760 --> 00:07:44,760
Oh, interesting.

97
00:07:44,760 --> 00:07:45,760
And so, yeah.

98
00:07:45,760 --> 00:07:53,480
And so that was my first taste of the applied aspect, you know, of things of the applied

99
00:07:53,480 --> 00:07:54,480
world.

100
00:07:54,480 --> 00:08:01,800
And the goal of that was assessment or trying to rebuild new neural pathways to improve

101
00:08:01,800 --> 00:08:04,600
their hearing or something else.

102
00:08:04,600 --> 00:08:07,560
So it was a little bit of both, but really the latter.

103
00:08:07,560 --> 00:08:16,000
Really trying to re-strengthen those connections, those auditory connections through training

104
00:08:16,000 --> 00:08:17,800
them with, with complex sounds.

105
00:08:17,800 --> 00:08:23,040
And so we build the complex sounds in MATLAB and other tools.

106
00:08:23,040 --> 00:08:27,640
And you know, at that point, we were doing some initial testing, but it's gotten, gotten

107
00:08:27,640 --> 00:08:28,640
pretty far.

108
00:08:28,640 --> 00:08:30,640
It's gotten to the point where it's in, in, in app.

109
00:08:30,640 --> 00:08:31,640
Oh, wow.

110
00:08:31,640 --> 00:08:32,640
Yeah.

111
00:08:32,640 --> 00:08:40,720
So that was part of my graduate work, and then I ended up, so it's funny how I got into

112
00:08:40,720 --> 00:08:41,720
AI.

113
00:08:41,720 --> 00:08:42,720
It's a bit of a jump.

114
00:08:42,720 --> 00:08:50,800
So, still on that applied path, I ended up here at Dimensional Mechanics as a user experience

115
00:08:50,800 --> 00:08:53,520
researcher in VR.

116
00:08:53,520 --> 00:09:00,960
That was the initial space that we were in, creating VR content.

117
00:09:00,960 --> 00:09:02,960
And optimizing it.

118
00:09:02,960 --> 00:09:03,960
Oh, interesting.

119
00:09:03,960 --> 00:09:04,960
Yeah.

120
00:09:04,960 --> 00:09:15,640
And so that's, that's why, so I was trying to work on things like immersion, enriching the

121
00:09:15,640 --> 00:09:20,680
user experience of someone in VR environments.

122
00:09:20,680 --> 00:09:29,560
What are the perceptual aspects that come into it, and how to build a better immersive environment

123
00:09:29,560 --> 00:09:40,760
for users, and also, you know, avoiding things like uncanny valley and nausea and all the,

124
00:09:40,760 --> 00:09:43,320
the not so good stuff that goes with VR.

125
00:09:43,320 --> 00:09:45,920
But eventually, and uncanny valley is what?

126
00:09:45,920 --> 00:09:46,920
Oh, yes.

127
00:09:46,920 --> 00:09:47,920
Uncanny valley.

128
00:09:47,920 --> 00:09:50,800
That's what I forget what, what the phenomena is.

129
00:09:50,800 --> 00:09:51,800
Yeah.

130
00:09:51,800 --> 00:09:59,120
So it's, when you see a character that is human-like, but not quite there.

131
00:09:59,120 --> 00:10:00,120
Okay.

132
00:10:00,120 --> 00:10:06,120
And usually, you can tell in the eyes that's usually a giveaway, and you start to get this

133
00:10:06,120 --> 00:10:14,040
uneasy feeling, because, you know, you see the human-like quality that could be there,

134
00:10:14,040 --> 00:10:16,080
but it's not quite human-like.

135
00:10:16,080 --> 00:10:18,080
Okay.

136
00:10:18,080 --> 00:10:27,080
And that's a big issue with characters in VR and virtual environments, so.

137
00:10:27,080 --> 00:10:31,520
But eventually, we pivoted into the AI space.

138
00:10:31,520 --> 00:10:39,400
And so, I've been using my background to build, help build in the cognitive components

139
00:10:39,400 --> 00:10:50,200
into our system, things like decision-making and perception, and memory as well.

140
00:10:50,200 --> 00:10:57,800
Inside of curiosity, what drove the pivot from VR to AI?

141
00:10:57,800 --> 00:11:08,280
I, it was, you know, business decision, but I believe that we saw a bigger opportunity

142
00:11:08,280 --> 00:11:13,920
if you will in AI, especially a general AI platform that we're working on.

143
00:11:13,920 --> 00:11:20,840
VR, that is, is, you know, is vast as well, and there are a lot of areas that you can

144
00:11:20,840 --> 00:11:24,040
go into that people don't necessarily recognize.

145
00:11:24,040 --> 00:11:33,880
It's way more than the entertainment space, but AI can, it affects almost every single field

146
00:11:33,880 --> 00:11:37,920
there is, almost every single area of business.

147
00:11:37,920 --> 00:11:45,360
I was on a panel a few months ago and someone asked, you know, is there, is there any area

148
00:11:45,360 --> 00:11:48,880
that you can think of that hasn't been touched by AI?

149
00:11:48,880 --> 00:11:55,320
And they're, you know, that we tried to come up with one example, but if it's not already,

150
00:11:55,320 --> 00:11:59,000
it is going to be affected, yeah.

151
00:11:59,000 --> 00:12:06,920
And so just for context, and I really want to dig into your, you know, how your background

152
00:12:06,920 --> 00:12:13,080
ties into your current work, but for context, you said that at dimensional mechanics, you're

153
00:12:13,080 --> 00:12:20,240
working on a platform for, did you say a general platform for AI or a platform for general

154
00:12:20,240 --> 00:12:21,240
AI?

155
00:12:21,240 --> 00:12:26,000
I thought I saw the ladder on the website or something or someplace.

156
00:12:26,000 --> 00:12:27,000
Yeah.

157
00:12:27,000 --> 00:12:35,200
So it's a general AI platform, essentially a set of development tools for companies so

158
00:12:35,200 --> 00:12:42,400
that they can reduce their costs in engineering costs and AI and machine learning.

159
00:12:42,400 --> 00:12:51,280
It's, AI is very expensive to produce and you need quite a bit of manpower and especially,

160
00:12:51,280 --> 00:12:57,040
you know, you have to have essentially, you know, experts currently, but we are trying

161
00:12:57,040 --> 00:13:05,120
to do away with that so that someone who is interested in AI or, you know, a company

162
00:13:05,120 --> 00:13:11,880
that maybe doesn't necessarily have AI and ML experts, they can still build models to

163
00:13:11,880 --> 00:13:13,880
solve their problems.

164
00:13:13,880 --> 00:13:14,880
Okay.

165
00:13:14,880 --> 00:13:15,880
Got it.

166
00:13:15,880 --> 00:13:22,680
I thought I read that you guys were going after the, the AGI problem, artificial general

167
00:13:22,680 --> 00:13:31,480
intelligence, but it sounds like, it sounds more narrowly constrained than that a little

168
00:13:31,480 --> 00:13:32,480
bit.

169
00:13:32,480 --> 00:13:33,480
Yeah.

170
00:13:33,480 --> 00:13:38,360
I mean, we're, we're definitely adding to that conversation and adding to those efforts.

171
00:13:38,360 --> 00:13:46,280
We want to build general AI, but it actually should mention that we are currently in the

172
00:13:46,280 --> 00:13:47,880
media space.

173
00:13:47,880 --> 00:13:52,560
So, you know, you have to narrow it down somewhat.

174
00:13:52,560 --> 00:14:00,800
So we're currently working with companies to build AI models for images, video, and text.

175
00:14:00,800 --> 00:14:02,400
Okay.

176
00:14:02,400 --> 00:14:10,080
And I imagine that the media applications lend themselves particularly well to your

177
00:14:10,080 --> 00:14:16,120
background and a cognitive approach to, you know, there are clear cognitive elements

178
00:14:16,120 --> 00:14:19,720
to AI with regards to media.

179
00:14:19,720 --> 00:14:23,240
Can you talk a little bit about how those intersect?

180
00:14:23,240 --> 00:14:24,240
Right.

181
00:14:24,240 --> 00:14:33,760
So, with, let's say, you know, images and video, for example, I come from it actually as

182
00:14:33,760 --> 00:14:39,800
a matter of fact, I'm working on a computer vision project with a local university.

183
00:14:39,800 --> 00:14:49,760
And we try to find parallels between, you know, how humans ingest or process, I guess,

184
00:14:49,760 --> 00:14:50,760
perceive media.

185
00:14:50,760 --> 00:14:56,840
That's the best way to put it and how a system would do the same thing.

186
00:14:56,840 --> 00:15:04,000
And it's, you know, obviously there's going to be vast differences between how a machine

187
00:15:04,000 --> 00:15:10,600
does and how a human does and we're not trying to make a one-to-one mapping, but there's

188
00:15:10,600 --> 00:15:12,240
a lot to be said.

189
00:15:12,240 --> 00:15:19,720
There's a lot of information and there's a lot of inspiration in how a human perceives

190
00:15:19,720 --> 00:15:28,120
media and that we can apply to how a machine perceives media.

191
00:15:28,120 --> 00:15:38,520
So, for example, and, you know, in computer vision, you can think of it as like a multi-disciplinary

192
00:15:38,520 --> 00:15:45,840
field because you're drawing from vision science, you're drawing from computer science,

193
00:15:45,840 --> 00:15:49,960
psychology, you know, cognitive science.

194
00:15:49,960 --> 00:15:57,080
And so, you know, and part of that is looking at, you know, exactly.

195
00:15:57,080 --> 00:16:07,480
So, part of the way that researchers create ground truth for computer vision experiments

196
00:16:07,480 --> 00:16:10,680
is they look at human ratings.

197
00:16:10,680 --> 00:16:19,000
They'll have, you know, humans view a set of videos, do eye tracking and whatnot and

198
00:16:19,000 --> 00:16:25,800
then try to apply that, you know, that eye gaze data to the system at hand.

199
00:16:25,800 --> 00:16:27,800
So that can be built in.

200
00:16:27,800 --> 00:16:37,000
So, you know, in by using those human ratings, you can build smarter systems.

201
00:16:37,000 --> 00:16:42,400
Just to make sure I'm understanding all of what you're saying.

202
00:16:42,400 --> 00:16:51,280
So we can envision a system where a labeling system where you've got some humans that

203
00:16:51,280 --> 00:16:56,680
are trying to label a set of images.

204
00:16:56,680 --> 00:17:02,920
And you're saying that in addition to just the label that they're, you know, that they

205
00:17:02,920 --> 00:17:07,960
may type in to, you know, or a set of labels that they may type in, you're also doing

206
00:17:07,960 --> 00:17:13,560
something like, you know, eye tracking with the camera or something like that to see where

207
00:17:13,560 --> 00:17:20,160
they're looking on the images and that helps you to refine the training process.

208
00:17:20,160 --> 00:17:21,160
Right.

209
00:17:21,160 --> 00:17:25,960
There's a lot of, there's a lot of rich information.

210
00:17:25,960 --> 00:17:34,960
There can be from eye tracking studies and, you know, how humans, what is the eye gaze

211
00:17:34,960 --> 00:17:36,040
data look like?

212
00:17:36,040 --> 00:17:43,160
What do, you know, one of my focuses is no pun intended but an intention, user attention

213
00:17:43,160 --> 00:17:48,960
in that when we take in an environment, when we're looking around, there's tons and tons

214
00:17:48,960 --> 00:17:49,960
of stimuli.

215
00:17:49,960 --> 00:17:52,880
The way that we can process it all of once.

216
00:17:52,880 --> 00:17:58,760
And so with humans, the whole scene is not relevant at the same time.

217
00:17:58,760 --> 00:17:59,760
Right.

218
00:17:59,760 --> 00:18:04,080
It's going to be relevant in basically spotlights.

219
00:18:04,080 --> 00:18:09,720
So you can apply that with a system as well.

220
00:18:09,720 --> 00:18:17,840
If you may want to enhance certain parts of images or video, but you may want to do it

221
00:18:17,840 --> 00:18:24,640
in a fashion where it's focusing on the most relevant information in that scene.

222
00:18:24,640 --> 00:18:31,800
And so back to your grad school research on multimodal and kind of this model of the

223
00:18:31,800 --> 00:18:37,640
brain where, you know, you have just a bunch of inputs and you're processing them, you

224
00:18:37,640 --> 00:18:38,680
know, kind of as equals.

225
00:18:38,680 --> 00:18:44,720
Are there other examples of inputs or other examples of this, of the multiple modes

226
00:18:44,720 --> 00:18:49,800
that you're incorporating into your work today?

227
00:18:49,800 --> 00:18:56,520
Not particularly currently, but I would like to see, and this is something that I'd like

228
00:18:56,520 --> 00:19:05,840
to implement later and it's the, I guess, intersection or combination of audio and

229
00:19:05,840 --> 00:19:06,840
visual.

230
00:19:06,840 --> 00:19:07,840
Okay.

231
00:19:07,840 --> 00:19:15,040
I have seen some early work in this, but, you know, if, let's say, let's say you're watching

232
00:19:15,040 --> 00:19:21,800
a video and there's likely going to be audio as well.

233
00:19:21,800 --> 00:19:29,960
And where you're focusing your attention is going to be not only influenced by the visuals,

234
00:19:29,960 --> 00:19:39,520
but also, and also by the audio, but the, what's critical is the combination of the, you

235
00:19:39,520 --> 00:19:44,160
know, space spatial temporal information.

236
00:19:44,160 --> 00:19:50,680
That's, that's something that's going to heavily influence where your attention is guided

237
00:19:50,680 --> 00:19:52,000
in the scene.

238
00:19:52,000 --> 00:20:00,560
And so I think there needs to be a lot more research in that space, you know, previously,

239
00:20:00,560 --> 00:20:06,080
there's been a lot of studies on this, but, you know, there's been a lot of focus on

240
00:20:06,080 --> 00:20:12,600
the visual, obviously, and there's been a lot, there's been some focus on the audio,

241
00:20:12,600 --> 00:20:21,480
but really where you're going to find fascinating insights is the, the combination, what's

242
00:20:21,480 --> 00:20:27,520
going on spatially, what's going on temporarily, and that, that intersection.

243
00:20:27,520 --> 00:20:34,000
And practically speaking, how might you expect that to impact an AI project?

244
00:20:34,000 --> 00:20:46,240
So let's say you build a model to detect the tone of a commercial, well, that's going

245
00:20:46,240 --> 00:20:55,560
to use both video, the visuals and the, and throughout the commercial, and then also,

246
00:20:55,560 --> 00:21:07,280
you know, advertisers put a lot of focus on the audit, auditory aspect as well, the, you

247
00:21:07,280 --> 00:21:15,320
know, the, the mood of the, the mood of the music, going with, you know, what's going

248
00:21:15,320 --> 00:21:16,720
on in the scene.

249
00:21:16,720 --> 00:21:30,600
So with an AI system, if you're able to train it on the video, and then train it on the,

250
00:21:30,600 --> 00:21:36,400
the audio, and then, yeah, I can think of like a kind of a thought experiment right now.

251
00:21:36,400 --> 00:21:43,000
So you can see if it can recognize the, the tone of the video just solely through the

252
00:21:43,000 --> 00:21:50,640
visual, and then see if how well it does with just the audio, but I bet you that when

253
00:21:50,640 --> 00:21:57,440
you combine both of them, you have both types of information that will allow it to best

254
00:21:57,440 --> 00:22:03,360
categorize the, the tone of the video.

255
00:22:03,360 --> 00:22:04,360
Got it.

256
00:22:04,360 --> 00:22:08,040
I mean, it sounds like the basic idea is just try to use all the information that you have

257
00:22:08,040 --> 00:22:15,120
to, to make more accurate models, and often it turns out that the information that you

258
00:22:15,120 --> 00:22:18,800
have comes in, you know, multiple modes.

259
00:22:18,800 --> 00:22:25,960
Some of it is, some of it is audio, some of it is video, you know, and there may be others

260
00:22:25,960 --> 00:22:27,280
as well.

261
00:22:27,280 --> 00:22:28,280
Right.

262
00:22:28,280 --> 00:22:31,920
That's, yeah, exactly.

263
00:22:31,920 --> 00:22:36,720
If we, you know, the more information that we have available, the more information we

264
00:22:36,720 --> 00:22:44,440
can train our systems with, and that's how we, as humans learn as well.

265
00:22:44,440 --> 00:22:51,640
For example, in the education room, you know, you have a video of a lecture, you have an

266
00:22:51,640 --> 00:23:00,320
audio of a lecture, but if you are not only, you know, you're using both, and you know,

267
00:23:00,320 --> 00:23:07,040
the best is if you're actually in the classroom and immersed and you have both, both sensory

268
00:23:07,040 --> 00:23:12,600
streams coming in, the visuals and the auditory, you're going to have a better chance of, you

269
00:23:12,600 --> 00:23:17,320
know, remembering the content and, and being able to build off of that as well.

270
00:23:17,320 --> 00:23:18,320
Right.

271
00:23:18,320 --> 00:23:19,320
Right.

272
00:23:19,320 --> 00:23:27,560
Now a lot of your work calls into question the whole notion of, you know, biological

273
00:23:27,560 --> 00:23:34,920
plausibility for neural nets and the extent to which we should be trying to model neural

274
00:23:34,920 --> 00:23:38,080
nets and AI systems after human systems.

275
00:23:38,080 --> 00:23:43,680
And in fact, you wrote about this in a blog post last year.

276
00:23:43,680 --> 00:23:50,200
What are your thoughts on, you know, this whole question?

277
00:23:50,200 --> 00:23:58,160
It's, it's debatable, definitely, and depending on, you know, what your background is, I've

278
00:23:58,160 --> 00:24:04,960
seen some hardcore computer scientists where they're like, you don't need this plausibility.

279
00:24:04,960 --> 00:24:12,840
But since my, my background is very brain heavy, I definitely want to include that in the

280
00:24:12,840 --> 00:24:20,280
conversations, hey, okay, well, to what extent do we need to model these systems after the

281
00:24:20,280 --> 00:24:22,480
human brain?

282
00:24:22,480 --> 00:24:27,440
It does not need to be a one-on-one mapping.

283
00:24:27,440 --> 00:24:36,600
I do believe that because with the brain, the human brain and, you know, a computer, the

284
00:24:36,600 --> 00:24:41,440
lower level bits are very different.

285
00:24:41,440 --> 00:24:52,600
There's some similarity there with neurons firing and the binary bits for computers, but

286
00:24:52,600 --> 00:24:54,360
it wouldn't be in our best interest.

287
00:24:54,360 --> 00:24:56,920
There are so many other things that we need to tackle right now.

288
00:24:56,920 --> 00:25:02,640
It wouldn't be in our best interest to try to make a one-on-one mapping.

289
00:25:02,640 --> 00:25:10,480
But these systems, in my opinion, should be biologically inspired.

290
00:25:10,480 --> 00:25:20,240
So we can take concepts like modularity or integrated systems, localization.

291
00:25:20,240 --> 00:25:28,720
We can take all of these aspects from neuroscience and cognitive science and apply them to building

292
00:25:28,720 --> 00:25:30,440
these models.

293
00:25:30,440 --> 00:25:39,240
And I personally try to keep that in mind when I'm building various AI models.

294
00:25:39,240 --> 00:25:49,040
And I think of modularity and locality as computer science things, as technical things.

295
00:25:49,040 --> 00:25:56,480
Can you talk about how those concepts express themselves biologically and how they've influenced

296
00:25:56,480 --> 00:25:58,800
the types of models you build?

297
00:25:58,800 --> 00:25:59,800
Sure.

298
00:25:59,800 --> 00:26:11,200
So modularity, it's the idea that the brain processes, there are different areas for different

299
00:26:11,200 --> 00:26:12,200
processes.

300
00:26:12,200 --> 00:26:20,720
There's an area for language, there's an area for motor, there's an area for other types

301
00:26:20,720 --> 00:26:23,840
of processes and systems.

302
00:26:23,840 --> 00:26:32,840
So and they're believed to be all encompassing, if you will, or self-contained.

303
00:26:32,840 --> 00:26:35,080
I'm not particularly from that school of thought.

304
00:26:35,080 --> 00:26:39,880
I think there's relevance in that school of thought.

305
00:26:39,880 --> 00:26:48,000
But there's also a lot of ton of interconnectivity.

306
00:26:48,000 --> 00:26:55,560
And that's really, it's literally all connected, and even if there's some localization, a particular

307
00:26:55,560 --> 00:27:03,920
area for language, that's going to influence the visual processes that's going to influence

308
00:27:03,920 --> 00:27:06,040
the auditory processing.

309
00:27:06,040 --> 00:27:15,560
So in my opinion, it's really all about interconnectivity, but as far as building AI models, so this is

310
00:27:15,560 --> 00:27:21,840
kind of going back to the thought experiment that I brought up.

311
00:27:21,840 --> 00:27:27,160
But you can think of it as when you're training these models.

312
00:27:27,160 --> 00:27:33,320
You can think about what kind of, I guess, you can think of it as terms of sensory input

313
00:27:33,320 --> 00:27:37,200
and separate streams or combined streams.

314
00:27:37,200 --> 00:27:42,800
So you can feed an audio, you can feed in video, and then you can feed in the combination

315
00:27:42,800 --> 00:27:43,800
of that.

316
00:27:43,800 --> 00:27:52,680
And you can think of that as the interconnectivity equivalent, if you will.

317
00:27:52,680 --> 00:27:58,880
So this reminds me of some conversations I've had recently about folks working on deep

318
00:27:58,880 --> 00:28:07,840
neural nets, and this question comes up when they're trying to develop these complex,

319
00:28:07,840 --> 00:28:12,520
deep neural networks, whether they, you know, whether they're ultimately developing

320
00:28:12,520 --> 00:28:21,200
this kind of the single model that takes in all the input and produces all of the outputs,

321
00:28:21,200 --> 00:28:26,960
and it's kind of monolithic, or whether they develop, you know, what looks more like

322
00:28:26,960 --> 00:28:35,200
an ensemble, in a sense of, you know, or a hierarchical neural network model where

323
00:28:35,200 --> 00:28:41,360
they've, you know, they're taking in, you know, inputs and training the model to be able

324
00:28:41,360 --> 00:28:46,200
to determine, you know, some kind of higher level feature.

325
00:28:46,200 --> 00:28:50,440
And then they have, you know, many of these in parallel that they feed into kind of a higher

326
00:28:50,440 --> 00:28:55,360
level neural network to, you know, make the ultimate decision.

327
00:28:55,360 --> 00:28:59,000
And this may happen in several layers.

328
00:28:59,000 --> 00:29:06,920
It sounds like what you're saying is that that is ultimately more, you know, that's closer

329
00:29:06,920 --> 00:29:11,600
to what the brain is doing, than the, you know, we think of the brain, I guess, and I

330
00:29:11,600 --> 00:29:17,320
guess as laypeople is kind of this monolithic thing, but you're saying that the, you know,

331
00:29:17,320 --> 00:29:19,120
in many ways is kind of hierarchical.

332
00:29:19,120 --> 00:29:21,120
Is that, is that accurate?

333
00:29:21,120 --> 00:29:27,600
Yeah, I mean, and you can take a particular system, the visual system, for example.

334
00:29:27,600 --> 00:29:34,560
So, you know, you get the early visual sensory input, you know, it's really globs and

335
00:29:34,560 --> 00:29:36,600
shapes and shadows.

336
00:29:36,600 --> 00:29:46,520
And then as the input gets further up the streaming to V3 and V4, then you start to actually

337
00:29:46,520 --> 00:29:53,320
make out a particular image, you know, a scene of a park or a beach or whatever it is.

338
00:29:53,320 --> 00:30:01,880
So yeah, it goes from sort of like a lower level abstract to the more.

339
00:30:01,880 --> 00:30:08,680
And yeah, I mean, I agree with the idea that, you know, we have these lower level processes

340
00:30:08,680 --> 00:30:15,640
that are done first and then as they are being carried out and further processed, then

341
00:30:15,640 --> 00:30:23,360
you get things that are more fine, fine tuned, fine grain, if you will.

342
00:30:23,360 --> 00:30:28,200
You can also apply that to the motor system as well.

343
00:30:28,200 --> 00:30:34,640
But first, when your motor system is still developing, you know, you're going to have

344
00:30:34,640 --> 00:30:43,040
clunky, non-coordinated movements, but then as you go further along and you can fine tune

345
00:30:43,040 --> 00:30:48,920
them and fine train them, then you're able to write, you're able to type, you're able

346
00:30:48,920 --> 00:30:52,280
to do more fine tune movements.

347
00:30:52,280 --> 00:31:00,760
Are there a set of principles? Do you think that, you know, anyone working in this field

348
00:31:00,760 --> 00:31:09,200
should be thinking about as they're approaching, you know, developing AI systems with regard

349
00:31:09,200 --> 00:31:16,600
to, you know, this whole issue of biological reference or plausibility?

350
00:31:16,600 --> 00:31:24,480
I would say, I think in terms of, you know, being interdisciplinary, there are a lot of

351
00:31:24,480 --> 00:31:33,080
areas that come together in AI and I think in order to be, to develop successfully, you

352
00:31:33,080 --> 00:31:36,400
need to borrow from each of those.

353
00:31:36,400 --> 00:31:44,000
Like I said, you know, computer science, cognitive science, psychology, all of these, these

354
00:31:44,000 --> 00:31:51,400
different areas are going to get you, are going to help you build a better foundation,

355
00:31:51,400 --> 00:32:00,800
as opposed to, you know, just focusing on the computer science theories or the, you know,

356
00:32:00,800 --> 00:32:06,040
cognitive science theories, you really need a combination of all of them.

357
00:32:06,040 --> 00:32:12,960
Are there a set of canonical references that folks should take a look at to dig into

358
00:32:12,960 --> 00:32:16,360
this area more?

359
00:32:16,360 --> 00:32:23,240
It wouldn't hurt to read up on the history of AI, I think it's a little known fact for

360
00:32:23,240 --> 00:32:27,920
some that it's actually been around since the fifties.

361
00:32:27,920 --> 00:32:35,360
A man named Marvin Minsky was, you know, he's considered to be the father of AI.

362
00:32:35,360 --> 00:32:41,880
I would, I would read up on, on him, his work, I'd read up on Alan Turing.

363
00:32:41,880 --> 00:32:48,960
He laid some of the foundational, you know, thinking in AI, yeah, it's start there.

364
00:32:48,960 --> 00:32:49,960
Yeah.

365
00:32:49,960 --> 00:32:50,960
Awesome.

366
00:32:50,960 --> 00:32:52,120
Awesome.

367
00:32:52,120 --> 00:33:01,920
So maybe let's, let's try to dig into, you know, some, some more concrete things are there.

368
00:33:01,920 --> 00:33:07,440
Specific applications of what you guys are doing at dimensional mechanics that we can

369
00:33:07,440 --> 00:33:14,600
maybe talk about or what, you know, ways that you're helping customers develop AI applications

370
00:33:14,600 --> 00:33:15,960
with your platform.

371
00:33:15,960 --> 00:33:16,960
Yeah.

372
00:33:16,960 --> 00:33:24,840
So we have a demo on our site, actually, I wish I could go into a lot more detail.

373
00:33:24,840 --> 00:33:32,360
But for now, I'll just mention the demo and the site and that's an image-rinking demo where

374
00:33:32,360 --> 00:33:38,880
you can upload a photo and it will give it a score.

375
00:33:38,880 --> 00:33:39,880
Okay.

376
00:33:39,880 --> 00:33:46,400
And relative to the other, the existing photos on there, and you can see them, you can

377
00:33:46,400 --> 00:33:47,920
scroll through them.

378
00:33:47,920 --> 00:33:49,880
There's a top, top 10 list.

379
00:33:49,880 --> 00:33:50,880
Okay.

380
00:33:50,880 --> 00:33:53,960
And it's scoring it from what perspective?

381
00:33:53,960 --> 00:33:58,040
The, so it's trying to give it the best ranking.

382
00:33:58,040 --> 00:34:07,160
So it's, it's using a lot of different metrics, can't really go into the particulars, but

383
00:34:07,160 --> 00:34:13,600
is it, is it trying to label the image or trying to rate its aesthetic quality or?

384
00:34:13,600 --> 00:34:17,600
It's, yeah, it's trying to rate its aesthetic quality.

385
00:34:17,600 --> 00:34:18,600
Okay.

386
00:34:18,600 --> 00:34:19,920
Got it.

387
00:34:19,920 --> 00:34:24,720
So it's kind of, I don't know if anybody remembers the hot or not app, it's kind of the hot or

388
00:34:24,720 --> 00:34:28,040
not app for, for images.

389
00:34:28,040 --> 00:34:29,040
Right.

390
00:34:29,040 --> 00:34:33,000
We, that's brought up and, that's been brought up at our discussions.

391
00:34:33,000 --> 00:34:34,000
Okay.

392
00:34:34,000 --> 00:34:41,120
But yeah, it's, like I said, it's rating the more aesthetic qualities of, of an image.

393
00:34:41,120 --> 00:34:42,120
Okay.

394
00:34:42,120 --> 00:34:51,640
And in what ways has the, your work around the cognitive psychology influenced how a

395
00:34:51,640 --> 00:34:54,480
system like that works?

396
00:34:54,480 --> 00:35:01,880
So part of this is going, is going to go back to, you know, how, how a person perceives

397
00:35:01,880 --> 00:35:02,880
an image.

398
00:35:02,880 --> 00:35:05,680
And I'll just give, you know, very general example.

399
00:35:05,680 --> 00:35:14,040
So, you know, when we see an image, we're taking in a lot of aspects, the shadows involve

400
00:35:14,040 --> 00:35:20,320
the lighting, the angle of object, the, you know, the, the, the, sort of the busyness

401
00:35:20,320 --> 00:35:24,960
or the, you know, the, the contrast involved.

402
00:35:24,960 --> 00:35:31,600
So we're assessing all of those things when we're an image.

403
00:35:31,600 --> 00:35:36,400
And you could say that the system is doing something similar.

404
00:35:36,400 --> 00:35:37,400
Okay.

405
00:35:37,400 --> 00:35:41,760
Uh, this is, this is kind of getting me into another space.

406
00:35:41,760 --> 00:35:44,400
So one thing, it's interesting.

407
00:35:44,400 --> 00:35:48,400
So have you heard of the black box problem?

408
00:35:48,400 --> 00:35:52,160
Generally, yes, generally, I've heard of a black box problem.

409
00:35:52,160 --> 00:35:56,160
I don't know if it's the same one that you're, when I think of the black box problem,

410
00:35:56,160 --> 00:36:00,040
I think of that, uh, from, uh, an AI explainability perspective.

411
00:36:00,040 --> 00:36:01,040
Right.

412
00:36:01,040 --> 00:36:02,040
Right.

413
00:36:02,040 --> 00:36:03,840
And that's, that's what I was going to get into.

414
00:36:03,840 --> 00:36:04,840
Okay.

415
00:36:04,840 --> 00:36:07,800
So for, and then there's a parallel with humans as well.

416
00:36:07,800 --> 00:36:15,640
So, you know, we, as researchers, we present a set of stimuli and then, you know, a person

417
00:36:15,640 --> 00:36:24,800
responds to those given stimuli, but what's exactly is going on in between is, uh, you

418
00:36:24,800 --> 00:36:30,160
know, it's debatable sometimes more, more times than not.

419
00:36:30,160 --> 00:36:37,280
And with AI, uh, that's one of the things that we're trying to work on as well, not necessarily

420
00:36:37,280 --> 00:36:45,160
dimensional mechanics, but as a, as a field, um, trying to demystify what's going on

421
00:36:45,160 --> 00:36:52,680
in between, um, and they're, they're, I, I think that taking a good, hard look at the,

422
00:36:52,680 --> 00:36:58,400
the training sets that you and, and manipulating those in a way where, you know, you're feeding

423
00:36:58,400 --> 00:37:04,600
in sections at a time and, you know, these have, this section has particular features.

424
00:37:04,600 --> 00:37:09,840
This one doesn't, that can possibly get at, at that question, but it's going to take

425
00:37:09,840 --> 00:37:12,080
a lot more, more work.

426
00:37:12,080 --> 00:37:13,080
Mm-hmm.

427
00:37:13,080 --> 00:37:18,680
And I think once we do that, it'll, we'll be able to, AI models will be that much more

428
00:37:18,680 --> 00:37:24,160
valuable because we'll be able to tweak as, as necessary.

429
00:37:24,160 --> 00:37:25,160
Right.

430
00:37:25,160 --> 00:37:26,160
Right.

431
00:37:26,160 --> 00:37:33,000
Uh, so going back to this demo application, you know, presumably you've trained, you

432
00:37:33,000 --> 00:37:43,080
developed some AI model, uh, to rank these images and you've trained it on lots of input

433
00:37:43,080 --> 00:37:44,080
data.

434
00:37:44,080 --> 00:37:50,600
Uh, did the multi-modal training come into play in, in this case?

435
00:37:50,600 --> 00:37:57,120
Not in the current iteration, but that is something we would definitely want to explore

436
00:37:57,120 --> 00:38:01,320
on how to make it smarter, if you will.

437
00:38:01,320 --> 00:38:03,320
Right.

438
00:38:03,320 --> 00:38:04,320
Yeah.

439
00:38:04,320 --> 00:38:11,840
Yeah, I can imagine, um, you know, if someone is, if someone is rating images on a, uh,

440
00:38:11,840 --> 00:38:20,320
numeric scale, but you also had a camera looking at them, observing them, then you, there's

441
00:38:20,320 --> 00:38:26,240
a ton of additional information like, you know, the creases in their eyes when they smile,

442
00:38:26,240 --> 00:38:33,840
you know, the smile, the eyes, you know, they could, that could perhaps, uh, lend some

443
00:38:33,840 --> 00:38:39,040
additional insight as to whether this is a visually appealing image.

444
00:38:39,040 --> 00:38:40,040
Right.

445
00:38:40,040 --> 00:38:44,640
There's so many factors that, that come into play, you know, like I said, whether you

446
00:38:44,640 --> 00:38:50,440
have the right lighting, whether the, the expression on someone's face, um, what that's

447
00:38:50,440 --> 00:38:52,600
conveying, there, there's so many things.

448
00:38:52,600 --> 00:38:56,120
So, uh, it would be great to further explore it.

449
00:38:56,120 --> 00:39:02,160
We've built it on a, you know, particular set of features, but, um, we would definitely

450
00:39:02,160 --> 00:39:05,400
like to expand that in the, in the future.

451
00:39:05,400 --> 00:39:06,400
Mm-hmm.

452
00:39:06,400 --> 00:39:11,720
Are there any other, uh, kind of applications or use cases that, um, might be interesting

453
00:39:11,720 --> 00:39:13,040
to explore?

454
00:39:13,040 --> 00:39:15,040
Oh, there are tons.

455
00:39:15,040 --> 00:39:18,280
It's really about having time to explore them.

456
00:39:18,280 --> 00:39:19,280
Yeah.

457
00:39:19,280 --> 00:39:20,280
Yeah.

458
00:39:20,280 --> 00:39:25,680
It would be great to get a sense for, you know, I think we, you've given us a sense thus

459
00:39:25,680 --> 00:39:35,880
far that having a cognitive psychology background can really lend insight into, you know, ways

460
00:39:35,880 --> 00:39:41,080
to think about, you know, your modeling and your training, uh, and it would be great

461
00:39:41,080 --> 00:39:48,360
to, you know, then talk through some examples of, you know, how, how that's kind of played

462
00:39:48,360 --> 00:39:57,160
out in kind of a customer scenario or a, you know, just a practical scenario so that folks

463
00:39:57,160 --> 00:40:02,920
can kind of see the line kind of go from beginning to end, if that makes sense.

464
00:40:02,920 --> 00:40:03,920
Okay.

465
00:40:03,920 --> 00:40:08,960
And I've taught, yeah, and I've touched on this a little bit earlier, but, you know, we're

466
00:40:08,960 --> 00:40:20,120
looking at what, what we're looking at, user interest and engagement, um, essentially,

467
00:40:20,120 --> 00:40:25,360
you know, when you see a scene, what is, what's relevant to you, what, what catches your

468
00:40:25,360 --> 00:40:26,360
attention?

469
00:40:26,360 --> 00:40:27,360
Right.

470
00:40:27,360 --> 00:40:28,360
Right.

471
00:40:28,360 --> 00:40:34,840
And once various factors of that can, can be identified and what, by the way, we're,

472
00:40:34,840 --> 00:40:41,800
you know, looking at both lower, lower level features, things like, uh, line orientation

473
00:40:41,800 --> 00:40:48,520
and color, you know, very low level features that would grab your attention automatically,

474
00:40:48,520 --> 00:40:49,760
if you will.

475
00:40:49,760 --> 00:40:56,680
And then also higher level things like, uh, emotion, things that, uh, more, I guess there's

476
00:40:56,680 --> 00:41:05,880
a term top down, um, attention, which is, is intentional based, you're, you're, um,

477
00:41:05,880 --> 00:41:11,680
looking at something intentionally, okay, um, or, you know, goal or oriented.

478
00:41:11,680 --> 00:41:16,440
So the possible applications of that, that research.

479
00:41:16,440 --> 00:41:21,160
So I've, you know, talked about, uh, the advertising space, right.

480
00:41:21,160 --> 00:41:29,000
So, you know, for example, if advertisers can take this research and realize, okay, at

481
00:41:29,000 --> 00:41:35,600
the 32nd mark, this is where this particular, you know, location in the, the scene, this

482
00:41:35,600 --> 00:41:40,640
is where people are going to be most engaged, uh, most, they feel like there's something

483
00:41:40,640 --> 00:41:44,080
most relevant in that, in that, uh, particular area.

484
00:41:44,080 --> 00:41:51,120
Well, that could be a great placement for product ad, you know, or product logo.

485
00:41:51,120 --> 00:41:57,560
You can also get a sense of how long you're going to be able to sustain someone's, how,

486
00:41:57,560 --> 00:42:01,960
how much, how much time someone's attention is going to be sustained, you know, in a world

487
00:42:01,960 --> 00:42:07,840
where we're on our phones all the time, um, that, that gap is shrinking.

488
00:42:07,840 --> 00:42:14,680
So, you know, with advertisers, it's that much more critical to find what's relevant

489
00:42:14,680 --> 00:42:20,280
and, and get in there before, uh, the user's, uh, attention is lost.

490
00:42:20,280 --> 00:42:21,280
Okay.

491
00:42:21,280 --> 00:42:27,440
So, then you're through this research, you're developing models that can, that can look

492
00:42:27,440 --> 00:42:34,040
at, uh, is it only static images or is it, uh, video as well in this project?

493
00:42:34,040 --> 00:42:35,040
It's video as well.

494
00:42:35,040 --> 00:42:36,040
Okay.

495
00:42:36,040 --> 00:42:37,520
Actually, in this particular project, it's video.

496
00:42:37,520 --> 00:42:38,520
Oh, okay.

497
00:42:38,520 --> 00:42:45,640
So, you're, you're looking at, you're basically training models to, to model human attention

498
00:42:45,640 --> 00:42:53,480
and interest, uh, on these videos, and then you can use that to help advertisers assess

499
00:42:53,480 --> 00:42:56,400
their work, for example.

500
00:42:56,400 --> 00:43:03,480
So as opposed to convening a panel or a focus group to try to get a sense for whether an

501
00:43:03,480 --> 00:43:10,200
advertisement is effective, uh, you know, which is probably expensive in time consuming

502
00:43:10,200 --> 00:43:18,480
and maybe not even all that accurate, you can use these models to screen the, the advertisements

503
00:43:18,480 --> 00:43:22,400
to, you know, for screen the advertisements for effectiveness.

504
00:43:22,400 --> 00:43:24,400
Is that the general idea?

505
00:43:24,400 --> 00:43:25,400
Right.

506
00:43:25,400 --> 00:43:27,240
And going off the accuracy point.

507
00:43:27,240 --> 00:43:33,920
So, you know, a lot of times in focus groups and whatnot, there, there's a, uh, uh, verbal

508
00:43:33,920 --> 00:43:42,560
or written response, but a lot of times what we say is not necessarily reflecting what's

509
00:43:42,560 --> 00:43:44,040
going on internally.

510
00:43:44,040 --> 00:43:45,040
Right.

511
00:43:45,040 --> 00:43:46,040
Right.

512
00:43:46,040 --> 00:43:47,040
Right.

513
00:43:47,040 --> 00:43:53,040
And we try to get at that as well, um, we currently use tools like, um, eye tracking and

514
00:43:53,040 --> 00:43:54,440
biometric sensors.

515
00:43:54,440 --> 00:44:01,440
So to get at the physiological responses to, to the input, to the video input.

516
00:44:01,440 --> 00:44:02,440
Okay.

517
00:44:02,440 --> 00:44:10,200
Uh, and, uh, I forget if we cover this, but what do you call that that phenomenon?

518
00:44:10,200 --> 00:44:17,800
Like I know a related idea is, I guess I, you call it attribution error or, uh, you

519
00:44:17,800 --> 00:44:22,000
know, issues around attribution, meaning if you, if I look at an image, I can tell you

520
00:44:22,000 --> 00:44:25,480
I like it, but I can't tell you necessarily why I like it.

521
00:44:25,480 --> 00:44:32,320
Is there a specific, is there specific terminology for, um, you know, the, um, you know, the,

522
00:44:32,320 --> 00:44:37,560
uh, other side of this, which is, you know, I might not even, I might say I like it,

523
00:44:37,560 --> 00:44:41,160
but I might not really like it or vice versa.

524
00:44:41,160 --> 00:44:42,160
Right.

525
00:44:42,160 --> 00:44:46,000
Well, there, there are a lot of things that come to play, especially in an experimental

526
00:44:46,000 --> 00:44:47,080
setting.

527
00:44:47,080 --> 00:44:49,600
So there is experiment or bias.

528
00:44:49,600 --> 00:44:53,960
If, you know, the, there might be some influence of the experiment or you have to be careful

529
00:44:53,960 --> 00:44:57,480
in the way that it's asked if you are going to ask.

530
00:44:57,480 --> 00:45:03,440
Because the question itself can be loaded and, and, and bias the response.

531
00:45:03,440 --> 00:45:10,360
This is getting into another area, but, uh, with eyewitness experiments, the way that

532
00:45:10,360 --> 00:45:17,400
you pose a question can definitely influence how the person will, will respond.

533
00:45:17,400 --> 00:45:24,920
You can, you know, let's say there is, um, an accident and, you know, you, you throw

534
00:45:24,920 --> 00:45:30,600
in an object in the question, like, uh, at the stop sign, blah, blah.

535
00:45:30,600 --> 00:45:37,560
Well, the fact that you mentioned the stop sign, you know, even if there wasn't one there,

536
00:45:37,560 --> 00:45:41,680
the person may still agree, I say, yeah, okay, I remember the stop sign.

537
00:45:41,680 --> 00:45:46,080
It's like, no, there wasn't even a stop sign there at that particular scene.

538
00:45:46,080 --> 00:45:52,880
So, so there's experiment or bias, there's, so you mentioned attribution, but there, you

539
00:45:52,880 --> 00:45:57,920
also want to be seen in a particular way as well when you respond.

540
00:45:57,920 --> 00:46:02,800
Um, so you have to keep that in mind, and we may not even be conscious of that.

541
00:46:02,800 --> 00:46:07,960
We just, we, we want to have, we want to produce positive responses.

542
00:46:07,960 --> 00:46:08,960
Mm-hmm.

543
00:46:08,960 --> 00:46:13,400
So, you know, to get away with all, get away at all that, I think a great measure is,

544
00:46:13,400 --> 00:46:19,080
is more something physiological that, you know, it doesn't get a chance to reach our

545
00:46:19,080 --> 00:46:20,080
thoughts.

546
00:46:20,080 --> 00:46:23,480
Mm-hmm, interesting, interesting.

547
00:46:23,480 --> 00:46:31,640
So, beyond just this attribution issue and my ability to articulate, there is a whole

548
00:46:31,640 --> 00:46:40,520
host of cognitive biases that can, um, that can distance someone's real perception of,

549
00:46:40,520 --> 00:46:46,080
an image or video from what they ultimately say and some kind of panel or focus group

550
00:46:46,080 --> 00:46:56,520
and thus being able to develop machine models that can, you know, not just rate the, the

551
00:46:56,520 --> 00:47:03,120
image or model, the, you know, the human reception of an image, but also maybe tell us a little

552
00:47:03,120 --> 00:47:08,940
bit, you know, as we kind of get further along with the black box issue, tell us what the

553
00:47:08,940 --> 00:47:15,280
issues are, you know, this one may be too dark here or too contrasty there, that kind

554
00:47:15,280 --> 00:47:16,280
of thing.

555
00:47:16,280 --> 00:47:17,280
Right.

556
00:47:17,280 --> 00:47:18,280
Right.

557
00:47:18,280 --> 00:47:22,760
To give, you know, possibly a more objective measure, if you will.

558
00:47:22,760 --> 00:47:23,760
Mm-hmm.

559
00:47:23,760 --> 00:47:29,800
It sounded like you were wrapping up this research project, um, what's on the horizon for

560
00:47:29,800 --> 00:47:30,800
you?

561
00:47:30,800 --> 00:47:34,480
Are there any areas that you're, any particular areas that you're looking forward

562
00:47:34,480 --> 00:47:37,360
to exploring further?

563
00:47:37,360 --> 00:47:44,320
Right now, I am working on a, learning more about the natural language processing space,

564
00:47:44,320 --> 00:47:50,240
which I find really fascinating, especially with my psycho linguistics background.

565
00:47:50,240 --> 00:47:57,480
So for those who don't know, natural language processing, um, is the ability for systems

566
00:47:57,480 --> 00:48:06,960
to take natural text, you know, anything as short as words, phrases, and to documents,

567
00:48:06,960 --> 00:48:15,600
full documents, novels, even, and, uh, be able to get insights out of that input data.

568
00:48:15,600 --> 00:48:21,520
Things like, I mean, you can get more technical things like frequency counts and, and whatnot,

569
00:48:21,520 --> 00:48:28,600
but you can figure out the, the sentiment of a particular, uh, text.

570
00:48:28,600 --> 00:48:37,200
You can figure out all kinds of things that, you know, it would take a person hundreds

571
00:48:37,200 --> 00:48:39,400
and hundreds of hours to do.

572
00:48:39,400 --> 00:48:45,560
You can just load in all of these, uh, documents and, you know, get a score for sentiment.

573
00:48:45,560 --> 00:48:53,160
Is it particularly positive or negative or all kinds of, uh, different insights?

574
00:48:53,160 --> 00:49:02,240
And what, uh, what is peaking your curiosity around, uh, NLP and, uh, the application of

575
00:49:02,240 --> 00:49:05,880
psycho linguistics to, to that field?

576
00:49:05,880 --> 00:49:10,760
Maybe we can start with, uh, what is psycho linguistics relative to, you know, traditional

577
00:49:10,760 --> 00:49:13,520
linguistics or other aspects of linguistics?

578
00:49:13,520 --> 00:49:14,520
Sure.

579
00:49:14,520 --> 00:49:23,840
So linguistics is, you know, the study of language, the parts of language, the structures

580
00:49:23,840 --> 00:49:32,320
and whatnot, and psycho linguistics, it's bringing in psychology into that.

581
00:49:32,320 --> 00:49:37,800
You know, you're, you're looking at things like the way in which people say things, um,

582
00:49:37,800 --> 00:49:44,560
that term is prosody, the inflection and someone's invoice to convey certain messages.

583
00:49:44,560 --> 00:49:45,560
Okay.

584
00:49:45,560 --> 00:49:46,560
It's, you know, it's fascinating.

585
00:49:46,560 --> 00:49:51,320
You take one sentence, the man went to the park.

586
00:49:51,320 --> 00:49:53,360
The man went to the park.

587
00:49:53,360 --> 00:49:59,680
The man went to the park, you know, if the, you can say the exact same thing, but put different

588
00:49:59,680 --> 00:50:02,640
inflection on it and has a completely different meaning.

589
00:50:02,640 --> 00:50:03,640
Okay.

590
00:50:03,640 --> 00:50:10,000
There's, you know, the way, uh, the way in which we produce sentences and the, you know,

591
00:50:10,000 --> 00:50:20,160
syntactic structure and how that affects both the producer of the speech and then also

592
00:50:20,160 --> 00:50:23,760
the, you know, listener as well.

593
00:50:23,760 --> 00:50:33,120
Um, it also gets into, you know, co-articulation and these other mechanics of speech that's

594
00:50:33,120 --> 00:50:35,200
what's co-articulation?

595
00:50:35,200 --> 00:50:43,240
Coarticulation is when it's the, it, it's so it has to do with the flow of words.

596
00:50:43,240 --> 00:50:49,640
When, when we're speaking, all the words seem discreet, but if you look at an audio form,

597
00:50:49,640 --> 00:50:59,200
uh, waveform, you'll notice that the sounds actually overlap and so that's the coarticulation.

598
00:50:59,200 --> 00:51:06,320
And you also get into the social aspects of speech, so when, uh, two people are conversing

599
00:51:06,320 --> 00:51:15,040
and there's a, uh, phenomena that occurs called common ground and that's when, um, you

600
00:51:15,040 --> 00:51:20,840
know, you start out using different terms, but over the course of the conversation, you

601
00:51:20,840 --> 00:51:26,160
start to use similar terms to each other, not the same ones, um, because you've built

602
00:51:26,160 --> 00:51:31,920
up, built up a report, uh, gosh, I mean, there's, there's so much, there's, there's also the

603
00:51:31,920 --> 00:51:37,560
phenomena where, um, you know, you're building up that common ground, but then you also take

604
00:51:37,560 --> 00:51:44,720
on a similar speaking style to that person, um, and that can also convey that you, you

605
00:51:44,720 --> 00:51:46,080
like that person.

606
00:51:46,080 --> 00:51:48,480
You can also, and that's called convergence.

607
00:51:48,480 --> 00:51:49,480
Okay.

608
00:51:49,480 --> 00:51:55,560
There's also divergence where maybe, um, you're, you're not a big fan of that person.

609
00:51:55,560 --> 00:52:04,160
You start, uh, changing your speaking style, maybe, you know, unconsciously, but, um, changing

610
00:52:04,160 --> 00:52:06,880
your speaking style, nevertheless.

611
00:52:06,880 --> 00:52:07,880
Hmm.

612
00:52:07,880 --> 00:52:08,880
Interesting.

613
00:52:08,880 --> 00:52:18,920
So when I think of natural language processing, I tend to think of applications that are,

614
00:52:18,920 --> 00:52:27,680
you know, either primarily textual or, um, you know, translation types of applications,

615
00:52:27,680 --> 00:52:33,520
uh, but you're, you know, just your little explanation of cycle linguistics and some,

616
00:52:33,520 --> 00:52:38,480
given some of the background we've talked about, you know, it, it strikes me that there's

617
00:52:38,480 --> 00:52:45,960
a ton of interesting work and exploration to go into, uh, what's, how to articulate

618
00:52:45,960 --> 00:52:46,960
this.

619
00:52:46,960 --> 00:52:54,360
Well, maybe to articulate it by example, like when you create a neural network to recognize

620
00:52:54,360 --> 00:52:59,320
images, right, when, you know, we know a little bit about how those neural network structure

621
00:52:59,320 --> 00:53:05,680
themselves and you've got kind of your edge detectors and your, you know, shading detectors

622
00:53:05,680 --> 00:53:12,800
and all those things that emerge, I wonder the extent to which, uh, the concepts like

623
00:53:12,800 --> 00:53:17,720
prosody and other things, if you're training a neural network on speech samples, you

624
00:53:17,720 --> 00:53:22,760
know, if there are regions in the neural network that emerge, that somehow reflect, you

625
00:53:22,760 --> 00:53:29,080
know, prosody, for example, or if that's a, if that's, you know, is that kind of the

626
00:53:29,080 --> 00:53:31,360
current frontier of research?

627
00:53:31,360 --> 00:53:38,400
Yeah, that's definitely a frontier of this research in this particular space, um, especially

628
00:53:38,400 --> 00:53:46,600
audio, you know, visual inputs have been fairly well studied in the AI space, but audio

629
00:53:46,600 --> 00:53:53,080
less much part of the issue is the shortage of data in that space there.

630
00:53:53,080 --> 00:53:58,800
There are, there are quite a few open source video and image data sets, but not so much

631
00:53:58,800 --> 00:54:04,680
on the audio side, but going back to your point, yeah, I believe that what you would do

632
00:54:04,680 --> 00:54:10,320
is you would set, you know, prosody, up as a feature, you'd set, I don't know, co-articulation

633
00:54:10,320 --> 00:54:16,560
or these other, you know, syntax, you'd, you'd start to set these up as individual features

634
00:54:16,560 --> 00:54:20,960
to train the models.

635
00:54:20,960 --> 00:54:26,960
Meaning that you would, you would have humans identify them somehow and label them or

636
00:54:26,960 --> 00:54:32,360
you would have, how would you set up prosody, for example, as a feature?

637
00:54:32,360 --> 00:54:41,160
It's a good question, so initially it would likely have to be partially, at least partially

638
00:54:41,160 --> 00:54:49,200
supervised and you'd want to use some sort of existing ratings, so probably human ratings

639
00:54:49,200 --> 00:54:56,600
and I can imagine, you know, you have a waveform and they, you know, listen to the speech

640
00:54:56,600 --> 00:55:04,080
snippet and mark places where, you know, goes high or goes low.

641
00:55:04,080 --> 00:55:11,720
This is also evident in the physical waveform as well, so I could see that even being done

642
00:55:11,720 --> 00:55:19,000
without human ratings, but, you know, the system, you know, you feed it various waveforms

643
00:55:19,000 --> 00:55:27,480
and you feed at the sounds to go with it, it should be able to learn the parts where the

644
00:55:27,480 --> 00:55:33,840
wave, the audio goes up and then, you know, the times where the inflection goes down.

645
00:55:33,840 --> 00:55:39,560
But the tricky part would be associating that with a particular meaning and that's where

646
00:55:39,560 --> 00:55:45,800
you'd probably need humans to come in, so, you know, humans would tag the particular

647
00:55:45,800 --> 00:55:51,160
sentence as, oh, the emphasis was on this, so the emphasis was on that.

648
00:55:51,160 --> 00:55:59,440
And then, once you have those lists of different emphasis, you could train the model on that.

649
00:55:59,440 --> 00:56:07,040
And so, ideally, it would know when the inflection goes up, that's where the, the emphasized

650
00:56:07,040 --> 00:56:09,040
meaning is.

651
00:56:09,040 --> 00:56:20,680
I would imagine that traditional linguistics has a lot to offer in terms of, you know,

652
00:56:20,680 --> 00:56:22,560
just how to represent all of this stuff.

653
00:56:22,560 --> 00:56:26,600
Like, it strikes me that, you know, just there's a representational challenge and, you

654
00:56:26,600 --> 00:56:30,240
know, if someone were to try to take this approach to, you know, building and training

655
00:56:30,240 --> 00:56:36,600
models, but, you know, certainly linguists have been, you know, representing prosody

656
00:56:36,600 --> 00:56:42,200
and some kind of way and developing ways to map that to specific meanings.

657
00:56:42,200 --> 00:56:44,040
Is that correct?

658
00:56:44,040 --> 00:56:48,360
Yes, that is, that is correct.

659
00:56:48,360 --> 00:56:57,560
And so, it's definitely, definitely worth a look if someone is trying to train on audio

660
00:56:57,560 --> 00:57:01,600
models to, you know, take a look at that space.

661
00:57:01,600 --> 00:57:10,360
That's why, going back to what I said about being, you know, thinking multidisciplinary,

662
00:57:10,360 --> 00:57:16,920
wouldn't maybe, you know, wouldn't maybe be obvious to go to the cycle linguistics area.

663
00:57:16,920 --> 00:57:23,720
But it, like we've been, we've been saying, it can give you some great insight into how

664
00:57:23,720 --> 00:57:26,880
to train an audio model.

665
00:57:26,880 --> 00:57:37,120
And it'll give you more than just the surface characteristics of a waveform or the audio.

666
00:57:37,120 --> 00:57:43,680
It'll, you'll be able to, you know, train it on meaningful insights as well.

667
00:57:43,680 --> 00:57:50,400
His prosody is not something that you can necessarily, you can see the inflections, but you

668
00:57:50,400 --> 00:57:54,240
can't necessarily see the meaning on the physical waveform.

669
00:57:54,240 --> 00:57:58,920
You need, you need to add that insight, right, in addition.

670
00:57:58,920 --> 00:57:59,920
Right.

671
00:57:59,920 --> 00:58:00,920
Hmm.

672
00:58:00,920 --> 00:58:05,800
Oh, this is a really, really interesting space.

673
00:58:05,800 --> 00:58:10,200
Any, any other thoughts before we wrap up?

674
00:58:10,200 --> 00:58:16,360
No, I, I think this is a great space as well.

675
00:58:16,360 --> 00:58:22,040
What I, I particularly appreciate about it is, like I said, the multidisciplinary aspect

676
00:58:22,040 --> 00:58:23,040
of it.

677
00:58:23,040 --> 00:58:26,480
We're building these very complex systems.

678
00:58:26,480 --> 00:58:31,240
And I just mean that as a, as a field, right, in addition to my company.

679
00:58:31,240 --> 00:58:39,080
But, you know, we're, yeah, we're building these complex models that, you know, in some

680
00:58:39,080 --> 00:58:43,120
respect, reflect what's going on in a human brain.

681
00:58:43,120 --> 00:58:49,040
But we need to keep in mind that, you know, the more complex they get, the more information

682
00:58:49,040 --> 00:58:54,000
that we're going to need to seek out, and it's going to come from different places.

683
00:58:54,000 --> 00:58:57,960
Yeah, I, I can definitely see that.

684
00:58:57,960 --> 00:59:06,240
Yeah, I don't, I, I think I've commented here on the podcast or, um, certainly on Twitter

685
00:59:06,240 --> 00:59:13,120
that if I wasn't so busy trying to figure out this machine learning and I think linguistics

686
00:59:13,120 --> 00:59:16,240
would be high on my list of things to figure out.

687
00:59:16,240 --> 00:59:22,560
Uh, it's a fascinating field and it's great that you get to, uh, combine the two.

688
00:59:22,560 --> 00:59:24,280
Yeah, it is great.

689
00:59:24,280 --> 00:59:29,320
And one thing I did want to mention is that ideally, you know, the system would be language

690
00:59:29,320 --> 00:59:30,320
agnostic.

691
00:59:30,320 --> 00:59:37,200
Um, so, you know, we're really teaching it about human language, whatever, uh, it could

692
00:59:37,200 --> 00:59:44,240
be French, Spanish, you know, English, whatever it is, um, which is very helpful.

693
00:59:44,240 --> 00:59:49,160
And it can be used in, like you alluded to, um, translation tools.

694
00:59:49,160 --> 00:59:50,160
Mm-hmm.

695
00:59:50,160 --> 00:59:56,680
Yeah, my favorite example of this, in fact, is from a conversation I did with, uh, recently

696
00:59:56,680 --> 01:00:02,640
with Shubos and Gupta from Baidu Labs, and he talked about how, uh, they were able

697
01:00:02,640 --> 01:00:07,680
to build a, uh, English to Mandarin translator.

698
01:00:07,680 --> 01:00:12,600
I believe it was English to Mandarin translator before they even had any, you know, without

699
01:00:12,600 --> 01:00:17,680
having any Mandarin speakers, you know, on their staff, you know, just based on, you

700
01:00:17,680 --> 01:00:23,720
know, this, this property that you're describing, the, the, the fact that a lot of the application

701
01:00:23,720 --> 01:00:26,680
of this is, uh, language agnostic.

702
01:00:26,680 --> 01:00:27,680
Exactly.

703
01:00:27,680 --> 01:00:35,280
Because that's, yeah, it, it's not working on those particular nuances, if you will.

704
01:00:35,280 --> 01:00:39,920
It's, it's looking at it with a more agnostic view.

705
01:00:39,920 --> 01:00:42,760
Mm-hmm.

706
01:00:42,760 --> 01:00:43,760
Awesome.

707
01:00:43,760 --> 01:00:47,880
Uh, well, before we go, what's the best way, uh, folks want to connect with you or get

708
01:00:47,880 --> 01:00:48,880
in touch?

709
01:00:48,880 --> 01:00:52,560
Uh, what's the best way to do that?

710
01:00:52,560 --> 01:01:01,280
You can connect with Dimensional Mechanics on Twitter at DM, INC, underscore AI, and, uh,

711
01:01:01,280 --> 01:01:03,560
we're also on Facebook and LinkedIn.

712
01:01:03,560 --> 01:01:13,360
You can connect with me personally at ArtSci, ART, SCI, uh, two, with two zeroes, um, at Twitter.

713
01:01:13,360 --> 01:01:14,360
Okay.

714
01:01:14,360 --> 01:01:16,480
So at ArtSci, zero, zero.

715
01:01:16,480 --> 01:01:17,480
Yeah.

716
01:01:17,480 --> 01:01:18,480
Awesome.

717
01:01:18,480 --> 01:01:19,480
Awesome.

718
01:01:19,480 --> 01:01:21,480
Well, Dominic, thanks so much for being on the show.

719
01:01:21,480 --> 01:01:26,320
It was great chatting with you and looking forward to reconnecting soon.

720
01:01:26,320 --> 01:01:27,320
Thank you, Sam.

721
01:01:27,320 --> 01:01:28,320
I really appreciate it.

722
01:01:28,320 --> 01:01:29,320
Thanks for having me on.

723
01:01:29,320 --> 01:01:30,320
Absolutely.

724
01:01:30,320 --> 01:01:31,320
Take care.

725
01:01:31,320 --> 01:01:32,320
Bye.

726
01:01:32,320 --> 01:01:38,320
All right, everyone, that's our show for today.

727
01:01:38,320 --> 01:01:43,400
Once again, thanks so much for listening and for your continued support.

728
01:01:43,400 --> 01:01:47,360
Don't forget to share your favorite quote from this show via the show notes page, Twitter

729
01:01:47,360 --> 01:01:53,840
or our Facebook page, and if you do, we'll be happy to send you one of our laptop stickers.

730
01:01:53,840 --> 01:01:57,640
If you're planning to attend the future of data summit next week, please reach out

731
01:01:57,640 --> 01:01:59,880
and let me know to look out for you.

732
01:01:59,880 --> 01:02:06,120
The notes for this show will be up on twimmelai.com slash talks slash 23, where you'll find

733
01:02:06,120 --> 01:02:10,440
links to Dominic and the various resources we mentioned in the show.

734
01:02:10,440 --> 01:02:38,960
Once again, thanks so much for listening and catch you next time.

