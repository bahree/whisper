1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,640
I'm your host Sam Charrington.

4
00:00:23,640 --> 00:00:30,720
Last week I spent some time at CES in Las Vegas exploring the vast sea of drones, cameras,

5
00:00:30,720 --> 00:00:37,160
paper thin TVs, robots, laundry folding closets and other smart devices.

6
00:00:37,160 --> 00:00:39,800
You name it, it was there.

7
00:00:39,800 --> 00:00:44,600
Of course, I was also able to sit down with some really interesting people working on some

8
00:00:44,600 --> 00:00:48,000
pretty cool AI-enabled products.

9
00:00:48,000 --> 00:00:52,000
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

10
00:00:52,000 --> 00:00:55,480
interviews and other quicktakes from the show.

11
00:00:55,480 --> 00:01:01,000
And of course, be on the lookout for our AI and consumer electronic series right here

12
00:01:01,000 --> 00:01:03,800
on the podcast coming soon.

13
00:01:03,800 --> 00:01:10,240
A quick note, this is your final reminder about tomorrow's Twimble Online Meetup.

14
00:01:10,240 --> 00:01:15,320
At 3pm Pacific, we'll be joined by Microsoft Research's Timnett Gebru.

15
00:01:15,320 --> 00:01:19,760
Timnett will be joining us to discuss her paper using deep learning and Google Street

16
00:01:19,760 --> 00:01:24,560
View to estimate the demographic makeup of neighborhoods across the United States.

17
00:01:24,560 --> 00:01:28,200
And I'm especially looking forward to her going into more detail about the pipeline she

18
00:01:28,200 --> 00:01:33,840
used to identify 22 million cars and 50 million street-view images.

19
00:01:33,840 --> 00:01:37,760
As usual, we'll get the meetup kicked off with the discussion segment in which we'll

20
00:01:37,760 --> 00:01:43,080
be exploring your AI resolutions and predictions for 2018.

21
00:01:43,080 --> 00:01:47,960
For links to the paper, to register for the meetup or to check out previous meetups,

22
00:01:47,960 --> 00:01:51,560
visit twimbleai.com slash meetup.

23
00:01:51,560 --> 00:01:56,080
The show you're about to hear is part of a series of shows recorded at the rework,

24
00:01:56,080 --> 00:01:59,840
deep learning summit in Montreal, back in October.

25
00:01:59,840 --> 00:02:06,440
This was a great event, and in fact, their next event, the deep learning summit San Francisco

26
00:02:06,440 --> 00:02:11,400
is right around the corner on January 25th and 26th.

27
00:02:11,400 --> 00:02:15,800
The summit will feature more leading researchers and technologists like the ones you'll hear

28
00:02:15,800 --> 00:02:21,480
here on the show this week, including Ian Goodfellow of Google Brain and Daphne Kohler

29
00:02:21,480 --> 00:02:24,240
of Calico Labs and more.

30
00:02:24,240 --> 00:02:30,560
Definitely check it out and use the code Twimbleai for 20% off of registration.

31
00:02:30,560 --> 00:02:35,560
In this episode, we hear from David Duveno, Assistant Professor in the Computer Science

32
00:02:35,560 --> 00:02:39,480
and Statistics Department at the University of Toronto.

33
00:02:39,480 --> 00:02:43,880
David joined me after his talk at the deep learning summit on composing graphical models

34
00:02:43,880 --> 00:02:49,320
with neural networks for structured representations and fast inference.

35
00:02:49,320 --> 00:02:53,960
In our conversation, we discuss the generalized modeling and inference framework that David

36
00:02:53,960 --> 00:02:58,680
and his team have created, which combines the strengths of both probabilistic graphical

37
00:02:58,680 --> 00:03:02,000
models and deep learning methods.

38
00:03:02,000 --> 00:03:06,880
He gives us a walkthrough of his use case, which is to automatically segment and categorize

39
00:03:06,880 --> 00:03:12,640
mouse behavior from raw video, and we discuss how the framework is applied to this and other

40
00:03:12,640 --> 00:03:14,600
use cases.

41
00:03:14,600 --> 00:03:21,360
We also discuss some of the differences between the frequentist and Bayesian statistical approaches.

42
00:03:21,360 --> 00:03:25,440
I had a great time with this interview, and I think you will too.

43
00:03:25,440 --> 00:03:27,960
And now on to the show.

44
00:03:27,960 --> 00:03:39,480
Alright everyone, so I am here at the rework deep learning conference in Montreal, and

45
00:03:39,480 --> 00:03:45,240
I am with David Duveno, David's a professor at the University of Toronto, and he actually

46
00:03:45,240 --> 00:03:50,200
just got off stage, and I'm excited to be sitting here with him, or here with you.

47
00:03:50,200 --> 00:03:52,800
I should say David, welcome to the podcast.

48
00:03:52,800 --> 00:03:53,800
Thank you, Sam.

49
00:03:53,800 --> 00:03:58,200
So, tell me a little bit about your background and how you got interested in machine learning

50
00:03:58,200 --> 00:03:59,200
in AI.

51
00:03:59,200 --> 00:04:01,840
It sounds like you come a little bit from the statistics side of the world.

52
00:04:01,840 --> 00:04:04,840
No, I've actually been sort of maybe pushed into this.

53
00:04:04,840 --> 00:04:05,840
That's right, man.

54
00:04:05,840 --> 00:04:10,320
I'm half-appointed since that's the University of Toronto.

55
00:04:10,320 --> 00:04:11,320
And it's really nice.

56
00:04:11,320 --> 00:04:15,480
No, actually the way I got into this was actually reading JÃ¼rgen Spadeuber's web page when

57
00:04:15,480 --> 00:04:20,720
I was in undergrad, and he had, I think, you know, to his credit, he really was thinking

58
00:04:20,720 --> 00:04:26,600
about a lot of things that people, you know, later also found interesting, and he has

59
00:04:26,600 --> 00:04:29,160
a way of presenting his ideas as sort of like the last word.

60
00:04:29,160 --> 00:04:32,360
I mean, everyone sort of does, but at the time I didn't really know how to evaluate these

61
00:04:32,360 --> 00:04:35,360
things, and I thought, oh man, I need to go to grad school, because this guy is going

62
00:04:35,360 --> 00:04:37,600
to solve AI in like two years or so.

63
00:04:37,600 --> 00:04:38,600
So, yeah.

64
00:04:38,600 --> 00:04:42,680
I mean, of course, now I have a much broader view, and I feel like if you go to grad school

65
00:04:42,680 --> 00:04:43,680
in Switzerland?

66
00:04:43,680 --> 00:04:44,680
No, no.

67
00:04:44,680 --> 00:04:48,680
Although I certainly, like, actually went to grad school at the University of British Columbia.

68
00:04:48,680 --> 00:04:49,680
Okay.

69
00:04:49,680 --> 00:04:50,680
Yeah.

70
00:04:50,680 --> 00:04:53,320
Initially, all my friends went to grad school, and my first time I applied for scholarships

71
00:04:53,320 --> 00:04:56,240
I got rejected, and so I had to be chip on my shoulder, and I was like grad school

72
00:04:56,240 --> 00:04:57,240
is for jerks.

73
00:04:57,240 --> 00:05:02,160
But yeah, then eventually I applied again and got it, and yeah, I went to work with Kevin

74
00:05:02,160 --> 00:05:07,760
Murphy back when graphical models were still cool, and I did a bit of machine vision and

75
00:05:07,760 --> 00:05:08,760
things like that.

76
00:05:08,760 --> 00:05:09,760
Okay.

77
00:05:09,760 --> 00:05:10,760
Yeah.

78
00:05:10,760 --> 00:05:13,800
And then he actually set me up with an internship at Google.

79
00:05:13,800 --> 00:05:15,040
It wasn't called Google Brain Back then.

80
00:05:15,040 --> 00:05:19,200
It was actually the video content analysis team, but it was all the same people that ended

81
00:05:19,200 --> 00:05:20,200
up forming it.

82
00:05:20,200 --> 00:05:24,560
And we just sort of tried to get confidence to predict whether YouTube videos contained

83
00:05:24,560 --> 00:05:28,440
people dancing for this content, so that YouTube was running at the time.

84
00:05:28,440 --> 00:05:29,440
Interesting.

85
00:05:29,440 --> 00:05:30,440
Interesting.

86
00:05:30,440 --> 00:05:32,880
And so you're now at University of Toronto?

87
00:05:32,880 --> 00:05:33,880
Yes.

88
00:05:33,880 --> 00:05:35,360
How long have you been at the University?

89
00:05:35,360 --> 00:05:36,720
Just a year actually.

90
00:05:36,720 --> 00:05:37,720
Okay.

91
00:05:37,720 --> 00:05:42,040
And you said it, you've kind of been pushed into this joint appointment.

92
00:05:42,040 --> 00:05:43,040
Well, okay.

93
00:05:43,040 --> 00:05:44,040
Wow.

94
00:05:44,040 --> 00:05:45,040
I'm not.

95
00:05:45,040 --> 00:05:50,880
I mean, I started off being interested in machine learning, and then went and did my PhD on

96
00:05:50,880 --> 00:05:55,680
Beijing Non-Priorometrics, and I realized that sort of math was a missing component.

97
00:05:55,680 --> 00:06:00,600
You know, like everyone in machine learning has to be able to code and do math, but you

98
00:06:00,600 --> 00:06:03,040
really can't get away without knowing the math.

99
00:06:03,040 --> 00:06:06,520
And really a little bit of formalism can go a long way.

100
00:06:06,520 --> 00:06:12,000
And especially recently, I've been working on building gradient estimators for discrete

101
00:06:12,000 --> 00:06:13,160
latent variable models.

102
00:06:13,160 --> 00:06:17,840
And this is also shows up in reinforcement learning where having unbiased gradient estimators

103
00:06:17,840 --> 00:06:19,880
is actually really important.

104
00:06:19,880 --> 00:06:23,800
And unbiased and it sounds like this boring horrible thing that frequentes statisticians

105
00:06:23,800 --> 00:06:24,800
care about.

106
00:06:24,800 --> 00:06:28,920
But it actually now, I think, is going to turn out to be sort of a central thing that

107
00:06:28,920 --> 00:06:32,120
people are going to be worrying about over the next few years.

108
00:06:32,120 --> 00:06:39,080
And so you just did your talk, and your talk was on combining graphical models with deep

109
00:06:39,080 --> 00:06:40,080
learning models.

110
00:06:40,080 --> 00:06:41,080
Yeah.

111
00:06:41,080 --> 00:06:45,920
Tell us a little bit about the talk as an overview, and we can kind of dive into the different

112
00:06:45,920 --> 00:06:46,920
elements of it.

113
00:06:46,920 --> 00:06:47,920
Sure.

114
00:06:47,920 --> 00:06:49,880
Well, the way that paper that came about was actually pretty fun.

115
00:06:49,880 --> 00:06:55,480
So I just did a postdoc at Harvard with this guy, Matt Johnson, who is now at Google Brain.

116
00:06:55,480 --> 00:07:00,520
And when I showed up, I had just sort of de-converted from Bayesian non-parametrics where I was

117
00:07:00,520 --> 00:07:04,400
trying to fit giant, infinite dimensional graphical models to everything.

118
00:07:04,400 --> 00:07:08,440
And sort of I had sort of realized the limitations of these approaches.

119
00:07:08,440 --> 00:07:13,800
And I was really excited about variational autoencoders where we learn a generative model

120
00:07:13,800 --> 00:07:14,920
using a neural network.

121
00:07:14,920 --> 00:07:18,880
And we also use a neural network to learn to do inference in that model.

122
00:07:18,880 --> 00:07:22,720
And sort of as I said in the talk, people have been able to write down really rich generative

123
00:07:22,720 --> 00:07:24,360
models for a long time.

124
00:07:24,360 --> 00:07:27,720
But they haven't been able to do inference in them, and that's sort of the limiting factor.

125
00:07:27,720 --> 00:07:28,720
Okay.

126
00:07:28,720 --> 00:07:32,640
With regards to this paper, Matt was coming from a sort of also old school background where

127
00:07:32,640 --> 00:07:37,800
he was saying, oh, I want to use graphical models, linear dynamic systems, like nice models

128
00:07:37,800 --> 00:07:39,880
that we can analyze and understand.

129
00:07:39,880 --> 00:07:42,080
And I was telling him, no, no, no, you've got to forget all of that.

130
00:07:42,080 --> 00:07:44,400
You have to just use neural networks to do inference.

131
00:07:44,400 --> 00:07:46,400
We can fit everything with a variational autoencoder.

132
00:07:46,400 --> 00:07:49,280
And we had this long back and forth over the course of about a year.

133
00:07:49,280 --> 00:07:52,360
In the end, it was kind of like my chocolate and your peanut butter.

134
00:07:52,360 --> 00:07:54,480
And we had this nice synthesis of ideas.

135
00:07:54,480 --> 00:07:57,720
And we sort of said, oh, maybe we can combine these and get sort of the best of both worlds.

136
00:07:57,720 --> 00:07:58,720
Okay.

137
00:07:58,720 --> 00:08:01,720
So when you say graphical model, what do we mean with that?

138
00:08:01,720 --> 00:08:02,720
Yeah.

139
00:08:02,720 --> 00:08:03,720
Let me mean by that.

140
00:08:03,720 --> 00:08:04,720
So that's a pretty broad phrase.

141
00:08:04,720 --> 00:08:09,280
It basically just means it doesn't mean graphics as we traditionally talk about graphics.

142
00:08:09,280 --> 00:08:10,280
That's a starting place.

143
00:08:10,280 --> 00:08:13,000
So the graphical model means like graphs.

144
00:08:13,000 --> 00:08:15,800
And the idea is that initially when people started writing down models, they just wrote

145
00:08:15,800 --> 00:08:16,800
down equations.

146
00:08:16,800 --> 00:08:19,080
I would say what the probability of different things were.

147
00:08:19,080 --> 00:08:21,920
And it was pretty hard to analyze these models.

148
00:08:21,920 --> 00:08:28,680
And so, you know, I think Kevin Murphy, Daphne Collar, near Friedman, these guys said,

149
00:08:28,680 --> 00:08:33,880
what if we actually represent the dependence of different variables as a graph?

150
00:08:33,880 --> 00:08:37,960
And you know, the idea is that if I say that, you know, like smoking causes cancer, then

151
00:08:37,960 --> 00:08:40,960
I would have an arrow going from smoking to cancer.

152
00:08:40,960 --> 00:08:44,840
And this says something about how the probability of these things change together.

153
00:08:44,840 --> 00:08:48,920
So if I get cancer, that doesn't make me smoke, but if I get smoke, I make me get cancer.

154
00:08:48,920 --> 00:08:52,680
And once we start to have, you know, ten or a hundred different variables, keeping

155
00:08:52,680 --> 00:08:57,280
track of all these arrows or these relationships becomes pretty tricky.

156
00:08:57,280 --> 00:09:02,120
But when we express these models as a graph, we can automatically analyze them and ask, you

157
00:09:02,120 --> 00:09:08,760
know, whether we think that like, you know, drinking wine causes, you know, better like test

158
00:09:08,760 --> 00:09:10,960
scores through some complicated mechanism.

159
00:09:10,960 --> 00:09:14,480
Or we can also ask, you know, what information would we need to learn in order to tell us

160
00:09:14,480 --> 00:09:15,480
what the answer is?

161
00:09:15,480 --> 00:09:16,480
Yeah.

162
00:09:16,480 --> 00:09:20,080
And so this is sort of this, it's not quite old fashioned AI, but it's sort of this, like,

163
00:09:20,080 --> 00:09:25,960
we're going to have these rational, understandable models where every sort of human concept has

164
00:09:25,960 --> 00:09:29,200
a little box that we put it in and we try to understand everything that's happening in

165
00:09:29,200 --> 00:09:30,200
our models.

166
00:09:30,200 --> 00:09:31,200
Okay.

167
00:09:31,200 --> 00:09:32,200
Okay.

168
00:09:32,200 --> 00:09:37,160
So when we talk about graphs and applying graphs to these types of models, so one of the

169
00:09:37,160 --> 00:09:41,600
things that I think of at least is the, some of the deep learning frameworks like TensorFlow

170
00:09:41,600 --> 00:09:46,600
allow you to create neural network architectures using graphs, but that's like at a higher level

171
00:09:46,600 --> 00:09:48,000
than what you're talking about, right?

172
00:09:48,000 --> 00:09:49,000
Yeah.

173
00:09:49,000 --> 00:09:53,680
So that's kind of a confusingly similar idea because, you know, we write down these

174
00:09:53,680 --> 00:09:57,760
computation graphs where we have these arrows that mean A is a function of B. And that's

175
00:09:57,760 --> 00:10:01,640
also the same sort of relationship we're talking about when we write down graphical models.

176
00:10:01,640 --> 00:10:05,480
But in graphical models, the relationships are all probabilistic, like we're not saying

177
00:10:05,480 --> 00:10:09,480
that A determines B, we're saying the probability of B depends on A.

178
00:10:09,480 --> 00:10:10,480
Okay.

179
00:10:10,480 --> 00:10:14,560
And then the thing we can do with graphical models that we can't do with neural networks

180
00:10:14,560 --> 00:10:18,960
so easily is to go backwards and say, you know, given that someone has cancer, what is

181
00:10:18,960 --> 00:10:23,480
the chance that they smoked, which is something, you know, you can't, well, it's not as easy

182
00:10:23,480 --> 00:10:25,640
to sort of run a neural network backwards.

183
00:10:25,640 --> 00:10:30,080
But when we run a graphical model backwards, we're sort of asking what are the hidden causes

184
00:10:30,080 --> 00:10:32,440
of the things that we observed?

185
00:10:32,440 --> 00:10:36,120
And this is this, this is what we tend to refer to as like the inference problem.

186
00:10:36,120 --> 00:10:37,120
Okay.

187
00:10:37,120 --> 00:10:41,440
How do we figure out what, like, how does what we saw change what we believe about what

188
00:10:41,440 --> 00:10:43,320
we didn't see?

189
00:10:43,320 --> 00:10:48,240
And you know, this, as an example, you can even view, for instance, like learning grammar,

190
00:10:48,240 --> 00:10:51,760
like when babies learn language, they hear all these sentences and there's this hidden

191
00:10:51,760 --> 00:10:55,800
thing, which is, you know, grammar and vocabulary and all these rules about how language goes

192
00:10:55,800 --> 00:10:56,800
together.

193
00:10:56,800 --> 00:11:01,360
And so the problem of hearing a bunch of sentences and then trying to figure out which

194
00:11:01,360 --> 00:11:05,640
is the likely rules of that language is an inference problem.

195
00:11:05,640 --> 00:11:09,960
And so this is kind of why people have been really excited about these generative models

196
00:11:09,960 --> 00:11:12,960
are like also called latent variable models for a long time.

197
00:11:12,960 --> 00:11:13,960
Yeah.

198
00:11:13,960 --> 00:11:18,440
Like the idea is the grammar is this latent unseen variable that we have to infer.

199
00:11:18,440 --> 00:11:23,320
And for instance, and also this motivates why people have been so excited by inference.

200
00:11:23,320 --> 00:11:27,560
So if you go to nips, there's like the advances in approximate vision inference workshop

201
00:11:27,560 --> 00:11:29,280
and, you know, variance of it.

202
00:11:29,280 --> 00:11:34,320
And it's one of my favorite parts of nips because it sounds really boring and dry.

203
00:11:34,320 --> 00:11:40,280
But, you know, this, the inference problem is kind of the bottleneck for doing all the

204
00:11:40,280 --> 00:11:44,960
cool things that babies can do that we can't do with, at least, that's at least one bottleneck.

205
00:11:44,960 --> 00:11:48,280
You know, even if we solve inference, there's probably more problems that we need to solve.

206
00:11:48,280 --> 00:11:53,160
But for instance, people like Josh Tenenbaum and MIT for, you know, now like 20 years or

207
00:11:53,160 --> 00:11:57,640
so heavy to get him on the show, his name has come up probably like three times just

208
00:11:57,640 --> 00:11:58,640
today.

209
00:11:58,640 --> 00:11:59,640
Yeah.

210
00:11:59,640 --> 00:12:03,120
And he's just a really inspiring person to talk to you because he really saw this vision,

211
00:12:03,120 --> 00:12:07,760
you know, a long time ago that, guys, guys, guys, if we could just, you know, figure out

212
00:12:07,760 --> 00:12:12,600
how to do inference, we would be able to not only explain like how humans do all these

213
00:12:12,600 --> 00:12:15,440
things, but also get machines to do them ourselves.

214
00:12:15,440 --> 00:12:17,840
So he's been, you know, looking into these for a long time.

215
00:12:17,840 --> 00:12:22,400
And I think actually all that I would be sure to keep an eye on the stuff that's coming

216
00:12:22,400 --> 00:12:27,360
out of his, his lab because inference methods have just been making, like these major leaps

217
00:12:27,360 --> 00:12:30,440
and bounds in the last three or four years.

218
00:12:30,440 --> 00:12:34,920
You know, I think inference methods is maybe another, like name collision because we refer

219
00:12:34,920 --> 00:12:41,920
to inference as kind of using models generally, but again, we're talking about something different

220
00:12:41,920 --> 00:12:42,920
here.

221
00:12:42,920 --> 00:12:45,280
We're talking about, yeah, maybe I would say probabilistic inference.

222
00:12:45,280 --> 00:12:46,280
Yeah.

223
00:12:46,280 --> 00:12:47,880
I agree that this word is completely overloaded.

224
00:12:47,880 --> 00:12:52,640
And also in frequent statistics, it has another meaning in frequentistic, what is frequentist

225
00:12:52,640 --> 00:12:53,640
statistics?

226
00:12:53,640 --> 00:12:57,840
Oh, well, I can even say that a lot of long back, so I would call myself a vision.

227
00:12:57,840 --> 00:13:03,040
So it's kind of like asking, you know, like a liberal to describe a Republican or like

228
00:13:03,040 --> 00:13:05,320
a Catholic described a Protestant or something.

229
00:13:05,320 --> 00:13:11,520
But you know, they're interested in like worst case guarantees, computing p-values, doing

230
00:13:11,520 --> 00:13:16,360
hypothesis testing, and basically making procedures that can tell us, you know, what does the

231
00:13:16,360 --> 00:13:21,760
data say about this particular question, regardless of what we happen to think before.

232
00:13:21,760 --> 00:13:29,680
And then the division side says, well, let's actually ask how to combine what we saw today

233
00:13:29,680 --> 00:13:30,880
with what we knew before.

234
00:13:30,880 --> 00:13:34,000
I mean, actually, that's the standard answer.

235
00:13:34,000 --> 00:13:39,120
To me, the real answer is, visions consider all possibilities, and they just keep all them

236
00:13:39,120 --> 00:13:41,960
around and weight them all equally, or not equally, weight them according to how well

237
00:13:41,960 --> 00:13:42,960
they fit the data.

238
00:13:42,960 --> 00:13:47,760
And frequentists try to identify like the best possible hypothesis.

239
00:13:47,760 --> 00:13:53,200
And these are like good reasons to do both, these are very different schools of thought.

240
00:13:53,200 --> 00:13:58,400
And you know, there's been this unfortunate bit of tribalism, I think, where people naturally

241
00:13:58,400 --> 00:14:02,280
tend to like to form groups, and then, yeah, so that has definitely happened, and that's

242
00:14:02,280 --> 00:14:05,120
been a thing for like 70 years, since it's six now.

243
00:14:05,120 --> 00:14:12,280
So yeah, maybe a digression, are there a set of things that you think of about statistics

244
00:14:12,280 --> 00:14:17,960
that you wish more people doing, machine learning, or deep learning, knew or understood better?

245
00:14:17,960 --> 00:14:21,120
Okay, well, I'm going to take that as a yes.

246
00:14:21,120 --> 00:14:25,960
There's like a little rant that I've been wanting to go on for a long time, which even

247
00:14:25,960 --> 00:14:29,080
like Princess at Harvard, when they were teaching machine learning, someone said, what's

248
00:14:29,080 --> 00:14:30,800
the difference between a frequentist and a vision?

249
00:14:30,800 --> 00:14:36,320
And then they said, oh, the frequentist treats the data as a random variable, and the

250
00:14:36,320 --> 00:14:39,320
vision treats the truth as a random variable.

251
00:14:39,320 --> 00:14:44,680
That is sort of technically what is happening, but it's just like a bizarre mis-framing

252
00:14:44,680 --> 00:14:47,160
of the entire discussion.

253
00:14:47,160 --> 00:14:51,600
I mean, like a random variable is sort of not necessarily random, and it's not necessarily

254
00:14:51,600 --> 00:14:56,880
a variable, it's like a very bad name, but the idea is that, you know, the frequentist

255
00:14:56,880 --> 00:15:01,040
is going to say, you know, if this thing was the case, how likely would I have been to

256
00:15:01,040 --> 00:15:02,720
see the data that I saw?

257
00:15:02,720 --> 00:15:06,280
That's a sort of analysis that is often done, I mean, there's all sorts of methods, and

258
00:15:06,280 --> 00:15:10,960
the vision will say, given that I did see this data, what do I believe about the truth?

259
00:15:10,960 --> 00:15:14,760
Even though I think the truth is fixed, and it's not random, because I don't know it,

260
00:15:14,760 --> 00:15:19,800
I can use probability to describe my state of uncertainty, and that doesn't mean that

261
00:15:19,800 --> 00:15:20,800
I think it's random.

262
00:15:20,800 --> 00:15:24,800
That just means I am using probability to describe my uncertainty.

263
00:15:24,800 --> 00:15:25,800
Right.

264
00:15:25,800 --> 00:15:26,800
Okay.

265
00:15:26,800 --> 00:15:27,800
Yeah, so that's my public service announcement.

266
00:15:27,800 --> 00:15:28,800
Okay.

267
00:15:28,800 --> 00:15:34,040
And so is there, you know, so that's kind of describing these two tribes.

268
00:15:34,040 --> 00:15:39,520
Are there ways that you see those two kind of schools of thought influencing the way

269
00:15:39,520 --> 00:15:44,080
folks approach, you know, machine learning and AI that, you know, particularly that you

270
00:15:44,080 --> 00:15:50,680
think, you know, a little bit more kind of commonality or something would kind of advance

271
00:15:50,680 --> 00:15:51,680
us as a community?

272
00:15:51,680 --> 00:15:57,840
Well, it's kind of funny, because deep learning has represented a sort of de facto sort

273
00:15:57,840 --> 00:16:02,720
of third direction, or maybe synthesis, and these debates about vision and frequentist

274
00:16:02,720 --> 00:16:09,520
methods, I think, really took a backseat to saying, let's just define a probabilistic model

275
00:16:09,520 --> 00:16:13,080
that will give us a continuous loss function that we can optimize and use gradient-based

276
00:16:13,080 --> 00:16:17,320
optimization to do maximum likelihood estimation.

277
00:16:17,320 --> 00:16:22,680
And deep learning in a nutshell, and this is sort of like takes elements from both.

278
00:16:22,680 --> 00:16:26,680
Now, of course, that people are saying, how do we, you know, train deep learning models

279
00:16:26,680 --> 00:16:28,000
with less data?

280
00:16:28,000 --> 00:16:30,920
People are looking more into vision, deep learning, which actually can be made to look

281
00:16:30,920 --> 00:16:33,640
a lot like standard deep learning, where you just add a little bit of noise to everything,

282
00:16:33,640 --> 00:16:35,040
which is really nice.

283
00:16:35,040 --> 00:16:39,720
Yeah, I guess the thing is that most of the classic, like, frequentist methods are based

284
00:16:39,720 --> 00:16:44,080
on taking really simple methods that are easy to analyze and prove things about, just

285
00:16:44,080 --> 00:16:50,080
sort of developing those, whereas for neural networks, you can't really say much about them

286
00:16:50,080 --> 00:16:55,960
in these sort of like hard-bound proving asymptotic sense that you could with like maximum likelihood

287
00:16:55,960 --> 00:16:58,160
estimation or, well, okay, sounds.

288
00:16:58,160 --> 00:17:02,080
Let's say with, like, the sort of simple estimators that people like to use in science.

289
00:17:02,080 --> 00:17:08,280
Yeah, so people used to love these frequentist estimators because they were simple and fast.

290
00:17:08,280 --> 00:17:12,720
And now we sort of accepted that if you pay this price of having a little bit more complex

291
00:17:12,720 --> 00:17:17,400
models and like maybe a GPU or something, you're going to get better enough performance

292
00:17:17,400 --> 00:17:21,480
that you'll just forget about any asymptotic guarantees that the other models might have

293
00:17:21,480 --> 00:17:22,480
had.

294
00:17:22,480 --> 00:17:24,760
What are some examples of frequentist models?

295
00:17:24,760 --> 00:17:27,080
Oh, frequentist methods, let's see.

296
00:17:27,080 --> 00:17:34,040
So there was this whole cottage industry of frequentist kernel methods, let's define

297
00:17:34,040 --> 00:17:40,720
a non-parametric estimator by considering all possible functions in some infinite dimensional

298
00:17:40,720 --> 00:17:48,360
Hilbert space that could possibly separate our data or explain it or model its density.

299
00:17:48,360 --> 00:17:52,840
And you know, these methods still have a place and I think one of the, like, oral presentations

300
00:17:52,840 --> 00:17:57,200
that nips the series is on these methods, so they're not like gone, but they're sort of

301
00:17:57,200 --> 00:18:01,760
definitely in a little bit of, there's definitely like a kernel winter happening right now.

302
00:18:01,760 --> 00:18:02,760
Okay.

303
00:18:02,760 --> 00:18:10,640
So then the, in fact, your presentation was talking about in some sense how to combine elements

304
00:18:10,640 --> 00:18:13,400
of both of these schools of thought?

305
00:18:13,400 --> 00:18:14,760
Yeah, exactly.

306
00:18:14,760 --> 00:18:18,680
So I think when I got to Harvard, I was sort of like one of these people who recently

307
00:18:18,680 --> 00:18:23,000
de-converts from a religion and they just had nothing about the fantasy about it and Matt

308
00:18:23,000 --> 00:18:26,520
was sort of, you know, saying, well, are you sure you want to, you know, really throw

309
00:18:26,520 --> 00:18:27,520
this all out?

310
00:18:27,520 --> 00:18:31,320
And yeah, and we really did have this problem of analyzing this, this most data and they

311
00:18:31,320 --> 00:18:36,200
had fit just like the pure graphical model sort of standard approach to the data and it

312
00:18:36,200 --> 00:18:40,120
had a, had a bunch of problems with, you know, well, let's hit pause there and talk about

313
00:18:40,120 --> 00:18:41,120
the data.

314
00:18:41,120 --> 00:18:42,120
Sure.

315
00:18:42,120 --> 00:18:43,520
So that we're all on the same page.

316
00:18:43,520 --> 00:18:44,520
Sure.

317
00:18:44,520 --> 00:18:49,480
So the data is a bunch of connect video of mice running around in the dark and the idea

318
00:18:49,480 --> 00:18:54,120
is that when biologists want to measure what happens when we, you know, change the genes

319
00:18:54,120 --> 00:18:59,680
of a mouse or give it a drug or, you know, expose it to like the odor of a fox or whatever,

320
00:18:59,680 --> 00:19:02,360
they need to quantify how it's behavior has changed.

321
00:19:02,360 --> 00:19:06,080
So they can write about how, you know, our model of autistic mice do this more often, but

322
00:19:06,080 --> 00:19:09,600
then when we give them the drug, they act more normally or something like that.

323
00:19:09,600 --> 00:19:15,080
And I couldn't tell from the video whether that was kind of top down or like through a

324
00:19:15,080 --> 00:19:18,600
glass floor looking up or something like a top down, okay.

325
00:19:18,600 --> 00:19:24,040
So the idea is that right now they have a army of grad students who spend thousands of

326
00:19:24,040 --> 00:19:28,600
hours watching this video and then saying, okay, and now the mouse, you know, ate something

327
00:19:28,600 --> 00:19:32,200
and now he ran over there and now he stood up and now he went over to his buddy.

328
00:19:32,200 --> 00:19:38,000
And you know, this is cruel both to these students and it's also introduces sort of variability

329
00:19:38,000 --> 00:19:43,680
between different people who might, you know, labeling error or just differences, right?

330
00:19:43,680 --> 00:19:47,320
It's hard to like canonically say like, okay, this is a mouse that is, you know, grooming

331
00:19:47,320 --> 00:19:48,320
or not, right?

332
00:19:48,320 --> 00:19:53,920
And also even the labeling noise more so than error, maybe, it's different interpretations

333
00:19:53,920 --> 00:19:56,160
of what the mouse is doing at a given time.

334
00:19:56,160 --> 00:19:57,160
Yeah, exactly.

335
00:19:57,160 --> 00:20:01,200
And really, you know, we can imagine this changing systematically across labs, maybe in

336
00:20:01,200 --> 00:20:05,160
different countries, maybe it's hard, there's like a language barrier, maybe eat something

337
00:20:05,160 --> 00:20:06,160
so they're different, right?

338
00:20:06,160 --> 00:20:11,240
So we'd really like to automate this part of the scientific pipeline, both just because

339
00:20:11,240 --> 00:20:16,080
it will save the grad students time, but also because it'll, you know, help us do better

340
00:20:16,080 --> 00:20:20,640
science by removing one or by standardizing one part of the entire pipeline.

341
00:20:20,640 --> 00:20:21,640
Mm-hmm.

342
00:20:21,640 --> 00:20:27,640
So you have this data, was there, were there a fixed number of classes or activities that

343
00:20:27,640 --> 00:20:33,840
you were trying to capture or was part of the challenge trying to identify, you know,

344
00:20:33,840 --> 00:20:35,960
how many fixed sets of activities there were?

345
00:20:35,960 --> 00:20:36,960
Yeah.

346
00:20:36,960 --> 00:20:37,960
Great question.

347
00:20:37,960 --> 00:20:42,280
So we really wanted to make sure that we didn't have to tell it exactly how many different

348
00:20:42,280 --> 00:20:46,320
classes of activity that were, because that we kind of defeat the purpose and it would

349
00:20:46,320 --> 00:20:50,640
also sort of make you wonder, well, what if I had given it one more class or one less,

350
00:20:50,640 --> 00:20:52,800
what would it, the results have been totally different?

351
00:20:52,800 --> 00:20:58,720
So this is kind of one of the benefits of being patient is that you can compare the model

352
00:20:58,720 --> 00:21:01,680
fit in a systematic way between different models.

353
00:21:01,680 --> 00:21:08,520
So we, what we did was we said, okay, there are up to, I think we chose like 40 different

354
00:21:08,520 --> 00:21:09,720
clusters.

355
00:21:09,720 --> 00:21:17,120
And the idea was that we, 40 clusters are 40 classes within those, well, classes and

356
00:21:17,120 --> 00:21:19,880
clusters are the same thing in the way that I'm talking about this.

357
00:21:19,880 --> 00:21:20,880
Okay.

358
00:21:20,880 --> 00:21:21,880
Sorry.

359
00:21:21,880 --> 00:21:25,800
And then the idea is that we could let the model choose how often different activities

360
00:21:25,800 --> 00:21:29,080
appeared and some of them it would just never use.

361
00:21:29,080 --> 00:21:33,280
And the idea was that we tell there are at least 40 clusters and it will say, I can explain

362
00:21:33,280 --> 00:21:35,880
this, this data with only 20.

363
00:21:35,880 --> 00:21:38,160
And so, you know, the other 20 just stay off forever.

364
00:21:38,160 --> 00:21:40,800
So we're automatically learning the number of clusters.

365
00:21:40,800 --> 00:21:44,920
We just have to make sure we have a good upper bound on how many there could possibly be.

366
00:21:44,920 --> 00:21:45,920
And I'm sorry.

367
00:21:45,920 --> 00:21:50,560
So you make clusters in a sense of cluster data points is that that will define a class.

368
00:21:50,560 --> 00:21:53,400
Is that the right way to think about this or that's a really good point.

369
00:21:53,400 --> 00:21:58,120
So I guess I mean clusters in a more abstract sense, where we're clustering the dynamics

370
00:21:58,120 --> 00:22:00,440
of the mouse movement.

371
00:22:00,440 --> 00:22:03,760
The idea of being that a behavior is not, you know, a particular pose and the most

372
00:22:03,760 --> 00:22:04,760
be it.

373
00:22:04,760 --> 00:22:07,840
It's not a particular pose that the mouse might be in.

374
00:22:07,840 --> 00:22:12,760
But it's the way that he moves from one pose to another or, you know, when he's grooming

375
00:22:12,760 --> 00:22:16,040
he's like, you know, moving in this sort of circular motion or something like that.

376
00:22:16,040 --> 00:22:22,400
So I mean a cluster in the dynamics is one behavior that he could be doing.

377
00:22:22,400 --> 00:22:23,400
Okay.

378
00:22:23,400 --> 00:22:28,400
So how many, how many of these clusters ended up being identified?

379
00:22:28,400 --> 00:22:34,480
To be honest, I forget and I think it was around 20, but so I just have to say Matt was

380
00:22:34,480 --> 00:22:37,760
the first author and he's the one who really spent a lot of time with the data.

381
00:22:37,760 --> 00:22:38,760
Okay.

382
00:22:38,760 --> 00:22:39,760
Okay.

383
00:22:39,760 --> 00:22:45,240
And so how did, how did the graph, the graphical analysis or the graphical element of this

384
00:22:45,240 --> 00:22:46,240
play in?

385
00:22:46,240 --> 00:22:47,240
Yeah.

386
00:22:47,240 --> 00:22:51,400
So the alternative, the baseline that we could have done would just be to say that there's

387
00:22:51,400 --> 00:22:56,280
some recurrent neural network that defines how the mouse sort of changes through time.

388
00:22:56,280 --> 00:23:01,600
And then we'll have some continuous factor that is changing and we don't necessarily really

389
00:23:01,600 --> 00:23:04,640
know what that can you expect our means.

390
00:23:04,640 --> 00:23:08,240
And so that would have probably fit the data pretty well.

391
00:23:08,240 --> 00:23:12,560
We would have been able to like predict the mouse's future movements.

392
00:23:12,560 --> 00:23:14,400
I think about as well.

393
00:23:14,400 --> 00:23:18,840
But we wouldn't have been able to look in and say, oh, there's these distinct clusters.

394
00:23:18,840 --> 00:23:22,680
So that was sort of the home motivation was the interpretability of this model.

395
00:23:22,680 --> 00:23:27,520
So what is the, what's the process for building a graphical model?

396
00:23:27,520 --> 00:23:33,360
Like are you literally identifying states and transition vectors and things, what's things

397
00:23:33,360 --> 00:23:34,360
like that?

398
00:23:34,360 --> 00:23:36,200
Or is it more abstract and mathematical?

399
00:23:36,200 --> 00:23:41,480
Well, we try to make it as abstract as possible because we want to let the data speak for

400
00:23:41,480 --> 00:23:43,640
itself as much as possible.

401
00:23:43,640 --> 00:23:47,760
So all we did was we said, you know, there's 40 different states, the musket being, and

402
00:23:47,760 --> 00:23:51,080
there's some probability of transitioning from each one to each other one.

403
00:23:51,080 --> 00:23:52,160
And we don't know what that is.

404
00:23:52,160 --> 00:23:54,200
So the model has to learn that.

405
00:23:54,200 --> 00:23:59,520
It also has to learn how those states influence the dynamics and it also has to learn how those,

406
00:23:59,520 --> 00:24:04,000
like the mouse's body state corresponds to actual video frames.

407
00:24:04,000 --> 00:24:08,000
So the idea is that all we basically said is there's some discrete stuff that controls

408
00:24:08,000 --> 00:24:12,200
some continuous stuff, that controls some video stuff.

409
00:24:12,200 --> 00:24:15,960
And all the connections between those things and the connections through time had to be

410
00:24:15,960 --> 00:24:17,840
learned automatically.

411
00:24:17,840 --> 00:24:23,640
And did you end up finding that I'm thinking about like the density or sparsity of the

412
00:24:23,640 --> 00:24:28,920
connection graph, like does that play a significant role in this or what did it end up kind

413
00:24:28,920 --> 00:24:29,920
of looking like?

414
00:24:29,920 --> 00:24:30,920
Great question.

415
00:24:30,920 --> 00:24:34,720
So I do think that mouse's behavior transition is probably our sparse.

416
00:24:34,720 --> 00:24:40,320
Like maybe he never goes from eating to like standing up right away or something like that.

417
00:24:40,320 --> 00:24:46,240
We didn't actually put any capacity for the model to, or we didn't sort of put any prime

418
00:24:46,240 --> 00:24:49,640
information about whether the transition matrix was sparse or not.

419
00:24:49,640 --> 00:24:55,080
To be honest, we didn't look at how sparse the learned matrix was, but I bet it was sparse.

420
00:24:55,080 --> 00:24:56,080
Yeah.

421
00:24:56,080 --> 00:24:59,440
So these are the sorts of like refinements that we would generally like to make.

422
00:24:59,440 --> 00:25:03,520
And actually I want to say these are the sorts of refinements that we would like the,

423
00:25:03,520 --> 00:25:06,440
our learning algorithm to be able to propose on its own.

424
00:25:06,440 --> 00:25:10,280
So at the end of the talk, I sort of said, so right now we built.

425
00:25:10,280 --> 00:25:12,440
The model, but you know, what if we got it wrong?

426
00:25:12,440 --> 00:25:16,360
What if mouse behavior isn't discrete or what if, you know, when there's two mice involved,

427
00:25:16,360 --> 00:25:20,280
there's some more complicated structure that, you know, we just don't understand most

428
00:25:20,280 --> 00:25:22,640
sociology so we don't even know how to write it down.

429
00:25:22,640 --> 00:25:28,120
So what we'd really like to do is try learning both all the parameters of these models and

430
00:25:28,120 --> 00:25:30,480
which types of structure they should have as well.

431
00:25:30,480 --> 00:25:33,840
And there's no sort of technical reason why we can't do this.

432
00:25:33,840 --> 00:25:38,040
It's just that it requires searching over this discrete space, which is sort of always

433
00:25:38,040 --> 00:25:39,040
a big pain.

434
00:25:39,040 --> 00:25:43,840
And are there things that, are there things kind of happening in the field that you think

435
00:25:43,840 --> 00:25:46,400
will enable you to do that?

436
00:25:46,400 --> 00:25:50,240
Like is it just going to be brute force, you know, a better compute or are there, you

437
00:25:50,240 --> 00:25:56,400
know, folks doing research that you think will lend themselves to, or what are the research

438
00:25:56,400 --> 00:25:58,840
areas that will lend themselves to figuring this out?

439
00:25:58,840 --> 00:25:59,840
Right.

440
00:25:59,840 --> 00:26:00,840
So I'm glad you asked.

441
00:26:00,840 --> 00:26:05,600
So I am perfectly think that in the next few years we're going to see a lot of progress

442
00:26:05,600 --> 00:26:11,840
in models of how to fit discrete models or methods of discrete, sorry, I think we're going

443
00:26:11,840 --> 00:26:16,000
to see a lot of progress on methods to fit discrete models.

444
00:26:16,000 --> 00:26:20,320
And sort of the deep learning revolution has basically been all about continuous everything.

445
00:26:20,320 --> 00:26:24,560
So we have continuous parameters, you know, continuous predictions, we have a latent

446
00:26:24,560 --> 00:26:25,560
variable model.

447
00:26:25,560 --> 00:26:27,480
The latent variables are mostly continuous.

448
00:26:27,480 --> 00:26:31,320
The stuff that I talked about today was just like a tiny step we added like one sort

449
00:26:31,320 --> 00:26:33,960
of easy to handle discrete latent variable.

450
00:26:33,960 --> 00:26:38,200
But really the stuff that I think is more interesting is going back to the grammar example, like

451
00:26:38,200 --> 00:26:43,120
learning an entire grammar on a language or even learning a parse tree for a given sentence

452
00:26:43,120 --> 00:26:47,520
is this complicated discrete object that, you know, you can't even say it's like one

453
00:26:47,520 --> 00:26:51,680
out of a hundred possibilities is actually like these entire trees of rules.

454
00:26:51,680 --> 00:26:56,720
And so there still isn't a much better way to handle these sorts of models than we had,

455
00:26:56,720 --> 00:26:58,480
you know, 10 years ago.

456
00:26:58,480 --> 00:27:02,600
So the thing, so one thing that I've been really excited about and getting my students to

457
00:27:02,600 --> 00:27:09,920
work on is how do we find sort of continuous relaxations or gradient estimators for models

458
00:27:09,920 --> 00:27:12,120
with discrete latent structure?

459
00:27:12,120 --> 00:27:16,720
And this is going to let us, I mean, if it works, do all sorts of things like learn hard

460
00:27:16,720 --> 00:27:21,560
attention models, train gans to generate text, you know, let's see.

461
00:27:21,560 --> 00:27:26,480
Yeah, as I said, learn grammars, learn models with these interpretable latent structures,

462
00:27:26,480 --> 00:27:28,080
learn to do things like produce programs.

463
00:27:28,080 --> 00:27:32,120
I mean, obviously none of this stuff is going to work out of the box.

464
00:27:32,120 --> 00:27:38,200
But again, gradient based estimation is sort of a really, really great method because it

465
00:27:38,200 --> 00:27:42,360
scales to, you know, millions of parameters in a way that something like evolutionary

466
00:27:42,360 --> 00:27:44,200
algorithms, I think, never will.

467
00:27:44,200 --> 00:27:47,680
So there might be another way forward, but the way forward I'm excited about is trying

468
00:27:47,680 --> 00:27:51,600
to get good gradient estimators for models with the discrete structure.

469
00:27:51,600 --> 00:27:52,600
Okay.

470
00:27:52,600 --> 00:28:00,280
And just so I can make sure I understand the single latent variable in the example that

471
00:28:00,280 --> 00:28:09,360
you presented is the cluster that the mouse is in at a given time, and it's discreet

472
00:28:09,360 --> 00:28:14,360
because, you know, it's a cluster you kind of quantized it inherently, exactly.

473
00:28:14,360 --> 00:28:18,000
Are there other formulations of that same, I mean, I, there are lots of formulations

474
00:28:18,000 --> 00:28:21,360
of that problem that aren't necessarily discreet.

475
00:28:21,360 --> 00:28:27,280
And so you went down this particular path, why again, just for interpretability.

476
00:28:27,280 --> 00:28:30,440
So the idea is, yeah, we could have said that there's, you know, 10 actions that he can

477
00:28:30,440 --> 00:28:34,560
be doing to some degree, you know, maybe he's eating a little bit, maybe he's running

478
00:28:34,560 --> 00:28:35,560
a little bit.

479
00:28:35,560 --> 00:28:40,280
And then the point is that we think it would have been really a lot harder to interpret

480
00:28:40,280 --> 00:28:42,960
what these, these variables meant.

481
00:28:42,960 --> 00:28:46,280
And we would, if we wanted to say what he was doing at a given time, we'd have to say

482
00:28:46,280 --> 00:28:49,080
these 10 numbers instead of just like this nice one.

483
00:28:49,080 --> 00:28:50,080
Right.

484
00:28:50,080 --> 00:28:53,600
And that's the kind of thing we typically see, like in image interpretation, like, you

485
00:28:53,600 --> 00:28:59,320
know, in this image, there's a umbrella with whatever probability and a girl with whatever

486
00:28:59,320 --> 00:29:00,320
probability.

487
00:29:00,320 --> 00:29:05,360
And so you've got this kind of continuous probability distribution of the things that

488
00:29:05,360 --> 00:29:10,560
are in the image and you could do similar with, with a video clip, you know, the mouse

489
00:29:10,560 --> 00:29:15,600
is doing x, y, z with some probability and, you know, eating with some probability and

490
00:29:15,600 --> 00:29:17,240
grooming with another probability.

491
00:29:17,240 --> 00:29:18,240
Right.

492
00:29:18,240 --> 00:29:21,840
But the idea that the, so the probabilities are continuous, but there's still a big difference

493
00:29:21,840 --> 00:29:26,000
between having a model where the variables are continuous and we're certain about them

494
00:29:26,000 --> 00:29:29,440
or where the variables are discrete and we're uncertain about them.

495
00:29:29,440 --> 00:29:33,400
This, this is an interesting point, which is that when you look at reality, sort of reality

496
00:29:33,400 --> 00:29:34,680
is always continuous.

497
00:29:34,680 --> 00:29:37,200
So why do we even have this discrete structure?

498
00:29:37,200 --> 00:29:41,680
And sort of one reason that it arises naturally is because when we have to describe stuff to

499
00:29:41,680 --> 00:29:45,480
each other in language, we have to choose, you know, which word to use, right?

500
00:29:45,480 --> 00:29:47,000
Like, am I hungry or am I sleepy?

501
00:29:47,000 --> 00:29:49,880
We don't have a whole, yeah.

502
00:29:49,880 --> 00:29:53,520
And we can modify this with verbs, but again, we don't have like some continuous signal

503
00:29:53,520 --> 00:29:56,680
that we can send to each other to just, you know, we can't just give each other high

504
00:29:56,680 --> 00:29:57,680
dimensional vectors.

505
00:29:57,680 --> 00:29:58,680
That would be amazing.

506
00:29:58,680 --> 00:30:01,440
Then, you know, language learning would be a lot easier.

507
00:30:01,440 --> 00:30:04,240
And in fact, there's been some work recently by like OpenAI and some other groups on

508
00:30:04,240 --> 00:30:09,000
like how to teach agents to communicate and they come across this exact same problem,

509
00:30:09,000 --> 00:30:14,360
which is, you know, the agents have to choose a discrete word to say to the other agent,

510
00:30:14,360 --> 00:30:16,160
but there's no way to backprop through that.

511
00:30:16,160 --> 00:30:19,760
There's no gradient signal that says, oh, you should have said this word a little bit less,

512
00:30:19,760 --> 00:30:20,760
right?

513
00:30:20,760 --> 00:30:21,760
It's not clear what that would mean.

514
00:30:21,760 --> 00:30:22,760
Have you seen the movie, her?

515
00:30:22,760 --> 00:30:23,760
Right.

516
00:30:23,760 --> 00:30:24,760
Yeah.

517
00:30:24,760 --> 00:30:25,760
Love that movie.

518
00:30:25,760 --> 00:30:26,760
People have been telling me forever to watch it.

519
00:30:26,760 --> 00:30:30,680
And I just started watching it on this trip and I'm not all the way through with it.

520
00:30:30,680 --> 00:30:36,440
But there was this one part where the, for those who haven't seen it, maybe I shouldn't

521
00:30:36,440 --> 00:30:38,800
give any spoilers on the podcast, that wouldn't be right.

522
00:30:38,800 --> 00:30:45,040
But there's an interesting point where, where this one AI talks about is talking to a human

523
00:30:45,040 --> 00:30:50,480
and says, hey, can I go offline with this other AI and communicate post verbally, which

524
00:30:50,480 --> 00:30:52,880
is exactly what you're talking about, exactly.

525
00:30:52,880 --> 00:30:56,200
So, I mean, it does raise the question, why do we want these artificial agents to even

526
00:30:56,200 --> 00:31:00,160
use discrete words to talk to each other when they can just communicate post verbally, as

527
00:31:00,160 --> 00:31:01,160
you say?

528
00:31:01,160 --> 00:31:02,160
Right.

529
00:31:02,160 --> 00:31:05,120
And I think maybe the answer is, well, we still want them to talk to us and they're going

530
00:31:05,120 --> 00:31:07,760
to have to probably use words to do that.

531
00:31:07,760 --> 00:31:09,920
That's one way, place where this comes up.

532
00:31:09,920 --> 00:31:10,920
Okay.

533
00:31:10,920 --> 00:31:12,560
So what other kinds of things are you working on?

534
00:31:12,560 --> 00:31:18,920
So I'm working a little bit on meta learning and just recently, I've sort of decided that

535
00:31:18,920 --> 00:31:22,560
the way that I was approaching this and that, you know, I think to sort of the mainstream

536
00:31:22,560 --> 00:31:25,000
way now, I think there might be another way forward.

537
00:31:25,000 --> 00:31:29,840
So there's been a lot of work recently where people have said, okay, I want to have like

538
00:31:29,840 --> 00:31:32,520
a robot or a little agent that's going to be able to learn really quickly.

539
00:31:32,520 --> 00:31:37,040
So I want to put it in a new environment and it's going to, in a few seconds, you know,

540
00:31:37,040 --> 00:31:40,720
figure out what's going on and then have a good policy of how to act or something like

541
00:31:40,720 --> 00:31:41,720
that.

542
00:31:41,720 --> 00:31:46,680
The sort of brute force way to do this, no one really did very much until a couple years

543
00:31:46,680 --> 00:31:53,240
ago was to back propagate through the entire learning procedure of, you know, the robot.

544
00:31:53,240 --> 00:31:56,960
So take those, you know, three seconds of him learning his way around the world.

545
00:31:56,960 --> 00:32:01,280
And if everything he did is continuous, we can actually just ask, you know, if I had changed

546
00:32:01,280 --> 00:32:06,200
his learning rules a little bit, how much better would he have done on the task?

547
00:32:06,200 --> 00:32:07,200
And this is fine.

548
00:32:07,200 --> 00:32:13,960
Compute this automatically with automatic differentiation and this also shows up when we have to tune

549
00:32:13,960 --> 00:32:15,560
the hyper parameters of our model, right?

550
00:32:15,560 --> 00:32:20,200
We want to fit an entire neural network, but we have like a learning rate or like a regularization

551
00:32:20,200 --> 00:32:23,920
parameter that we said at once at the beginning, and then it totally changes the outcome at the

552
00:32:23,920 --> 00:32:25,160
end.

553
00:32:25,160 --> 00:32:30,600
So a couple of years ago, me and my colleague, Dougal McClaren wrote a paper where we actually

554
00:32:30,600 --> 00:32:34,360
did back propagate through the entire training procedure of training a neural network for

555
00:32:34,360 --> 00:32:35,680
hundreds of iterations.

556
00:32:35,680 --> 00:32:40,040
And we got, you know, exact gradients and we could use gradient based optimization to tune

557
00:32:40,040 --> 00:32:41,040
our hyper parameters.

558
00:32:41,040 --> 00:32:45,240
And this is like, you know, really exciting because before that, everyone had to use these

559
00:32:45,240 --> 00:32:50,520
black box methods like random search or vision optimization that is kind of like Harry.

560
00:32:50,520 --> 00:32:53,520
And then after that, there was like this paper learning to learn by gradient descent,

561
00:32:53,520 --> 00:32:54,520
by gradient descent.

562
00:32:54,520 --> 00:32:55,520
It's like an amazing title.

563
00:32:55,520 --> 00:32:58,760
I wish that I had thought of that title for our paper, which is like much more boring

564
00:32:58,760 --> 00:32:59,760
title.

565
00:32:59,760 --> 00:33:00,760
Yeah.

566
00:33:00,760 --> 00:33:01,760
Doing the same sort of ideas.

567
00:33:01,760 --> 00:33:04,600
And you know, reinforcement learning people are really excited about this right now.

568
00:33:04,600 --> 00:33:10,640
So, but there's another way forward, right, which is to train a neural network to look

569
00:33:10,640 --> 00:33:16,520
at a problem, maybe, you know, take in the hyper parameters and then just directly output

570
00:33:16,520 --> 00:33:19,560
the optimal weights of a neural network.

571
00:33:19,560 --> 00:33:23,640
So skip the entire training procedure and just have the hyper parameters, the weights.

572
00:33:23,640 --> 00:33:24,640
Yeah, exactly.

573
00:33:24,640 --> 00:33:25,640
Yeah.

574
00:33:25,640 --> 00:33:28,480
We're still going to have to tune some hyper parameters, but we can train a neural network

575
00:33:28,480 --> 00:33:32,840
to just directly output the optimal weights.

576
00:33:32,840 --> 00:33:37,800
And that sounds maybe done because if you think, well, if you think what I'm going to do

577
00:33:37,800 --> 00:33:43,040
is now train a whole bunch of neural networks like I did before and then, you know, learn

578
00:33:43,040 --> 00:33:47,520
to predict the final outputs, then that would be slow and that would be a waste of time.

579
00:33:47,520 --> 00:33:51,600
But it turns out we can train a neural network to produce, like, I'll call this like a

580
00:33:51,600 --> 00:33:55,520
hyper network because it produces the weights of another neural network.

581
00:33:55,520 --> 00:33:59,480
I can train a hyper neural network to produce an optimal neural network without ever having

582
00:33:59,480 --> 00:34:01,520
seen an optimal neural network.

583
00:34:01,520 --> 00:34:05,840
I can just have it, you know, start off produce a bad neural network and then ask, you

584
00:34:05,840 --> 00:34:11,360
know, use back prop to determine how should I have adjusted the parameters of my hyper network

585
00:34:11,360 --> 00:34:15,040
so that it would have given me a better actual network.

586
00:34:15,040 --> 00:34:19,440
And then this gets very hard to talk about and everything is very good and confusing.

587
00:34:19,440 --> 00:34:23,720
I thought you were going to go in a direction of something like a GAN or something like

588
00:34:23,720 --> 00:34:24,720
that.

589
00:34:24,720 --> 00:34:25,720
Maybe.

590
00:34:25,720 --> 00:34:30,360
I mean, so the GAN does have this generator and I mean, I guess, yeah, I didn't really

591
00:34:30,360 --> 00:34:31,360
thought about that.

592
00:34:31,360 --> 00:34:36,280
We could train again to produce a network, but I guess the discriminator would have to

593
00:34:36,280 --> 00:34:41,960
see sort of the, our train networks and sort of real optimal networks and distinguish between

594
00:34:41,960 --> 00:34:42,960
them.

595
00:34:42,960 --> 00:34:46,160
But the whole point is I want to avoid ever having to start with a bunch of like optimal

596
00:34:46,160 --> 00:34:47,160
networks.

597
00:34:47,160 --> 00:34:48,160
Right.

598
00:34:48,160 --> 00:34:52,640
So I guess we can call this amortized optimization where amortized optimization is just

599
00:34:52,640 --> 00:34:57,440
like, okay, we learn to do optimization by sort of practicing, producing the optimal

600
00:34:57,440 --> 00:34:58,440
thing.

601
00:34:58,440 --> 00:35:02,400
You know, this idea has been sort of staring us in the face because this is what variational

602
00:35:02,400 --> 00:35:03,760
autoencoders do.

603
00:35:03,760 --> 00:35:09,520
They do amortized inference and that's sort of like a name of this little subfield where

604
00:35:09,520 --> 00:35:13,360
they say we're going to learn to look at the data and train a neural network to produce

605
00:35:13,360 --> 00:35:18,520
the optimal posterior, like the optimal probability of the latent variables.

606
00:35:18,520 --> 00:35:22,720
And again, these are trained in the same way where we never see the optimal posterior directly.

607
00:35:22,720 --> 00:35:26,800
We just can use gradient signals to tell us how to get better and better.

608
00:35:26,800 --> 00:35:30,080
And then after we train for a while, we sort of hope that we're almost optimal.

609
00:35:30,080 --> 00:35:31,080
Cool.

610
00:35:31,080 --> 00:35:36,120
Well, one thing I want to say is that the second idea of the using the hyper network to

611
00:35:36,120 --> 00:35:39,760
avoid training is due to my student John Lorraine.

612
00:35:39,760 --> 00:35:43,760
This is the beauty of this job is now I get to look good because all these brilliant

613
00:35:43,760 --> 00:35:46,160
students are coming up with ideas and then I get asked about them.

614
00:35:46,160 --> 00:35:48,120
So, but I have to give credit, we're credit to Steve.

615
00:35:48,120 --> 00:35:49,120
Awesome.

616
00:35:49,120 --> 00:35:50,120
Awesome.

617
00:35:50,120 --> 00:35:51,120
Well, thank you very much.

618
00:35:51,120 --> 00:35:54,560
This was a really interesting conversation and certainly has a lot of my neurons firing

619
00:35:54,560 --> 00:35:57,000
trying to figure out all the stuff that we talked about.

620
00:35:57,000 --> 00:36:00,920
And there's a lot of interesting stuff to dig into here.

621
00:36:00,920 --> 00:36:02,520
I appreciate you taking the time.

622
00:36:02,520 --> 00:36:03,520
Oh, my foot.

623
00:36:03,520 --> 00:36:04,520
Awesome.

624
00:36:04,520 --> 00:36:05,520
Thank you.

625
00:36:05,520 --> 00:36:11,520
All right, everyone, that's our show for today.

626
00:36:11,520 --> 00:36:17,120
Thanks so much for listening and for your continued feedback and support.

627
00:36:17,120 --> 00:36:23,120
Thanks to your support, this podcast finished the year as a top 20 technology podcast on

628
00:36:23,120 --> 00:36:25,160
Apple Podcasts.

629
00:36:25,160 --> 00:36:30,080
My producer says that one of his goals this year is to crack the top 10, but we definitely

630
00:36:30,080 --> 00:36:33,400
cannot do that without your support.

631
00:36:33,400 --> 00:36:38,760
What we need you to do is to head on over to your podcast app, rate the show, hopefully

632
00:36:38,760 --> 00:36:44,960
we've earned five stars, leave us a glowing review and share it with your friends, family,

633
00:36:44,960 --> 00:36:50,000
co-workers, Starbucks baristas, Uber drivers, everyone.

634
00:36:50,000 --> 00:36:55,760
Every review and rating goes a long way, so thank you so much in advance.

635
00:36:55,760 --> 00:37:00,840
For more information on David or any of the topics covered in this episode, head on over

636
00:37:00,840 --> 00:37:05,600
to twimmolai.com slash talk slash 96.

637
00:37:05,600 --> 00:37:10,440
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

638
00:37:10,440 --> 00:37:13,600
or via Twitter at twimmolai.

639
00:37:13,600 --> 00:37:20,600
Thanks once again for listening and catch you next time.

