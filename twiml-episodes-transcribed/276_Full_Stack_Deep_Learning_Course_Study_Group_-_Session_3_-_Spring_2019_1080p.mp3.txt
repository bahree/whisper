So, other one, it's another full stack, the learning static group, and we have more
presentations for the day, so we have, honestly, we're ready.
We have Avvynier's presentation on the model inter-operability and overview, and also
Sanyam's and Akash's presentation on some OPPOSOR still, JVN, IO, so it's cool.
Let me open my chat so I can also see what's in it.
I'll keep an eye on you in the chat, okay, that's great, thank you.
So let's maybe start with the, so we have about half an hour for each presentation, I think
that should be fine.
So let's maybe start with the full stack deep learning thing, so for today we had quite
challenging agenda, a couple of lessons, 7, 8, 9, 10, 11, but actually 7, let me show
a look, so 7, yeah, so 7 is about the machine learning teams, and that was about half an
hour of video, less than 8, it's not available, so that's easy.
Less than 9 was optional, because that's a guest speaker and that's about 45 minutes
of video.
And then less than 10, it's about troubleshooting, that's about an hour and a half.
And then we said we're going to try to attempt the lesson 11, which is labs, but we said
not all of them, but 6 and, 6 and 7.
So as you remember, labs were in the Geek app somewhere, so the labs were kind of
splitting the four sessions, session 1, 2, so the third session is actually the lesson
7 and 8, that was the plan anyway, so we can start, actually, so I can open slides,
I'm not going to go through all the slides as last time, because there are too many of
them, and I'm just going maybe to scroll, just ask questions or make some comments,
or anything related to the course, what was interesting, what was fun, what was not clear,
would try to talk about this.
Yeah, so he was talking about how many companies are struggling to hire AI experts.
I believe that was the, yeah, that was what he was talking about, so it's a good thing
for people looking for jobs, but at the same time, I think it's not easy to find a job
without experience, that's another kind of comment that is there around machine learning,
finding jobs.
You guys have some experience about finding jobs, so looking for people, for experts in AI.
I have experience of getting rejected from Google.
Oh, yeah, I kind of converse up from one point, people say that's a lot of, is it a big
demand for AI experts, on the other hand, yeah, I also heard that, and I kind of seen
my couple of cities themselves, it's, yeah, it's interesting.
Yeah, so there are different roles, so in machine learning, data science, AI, that's
I think there are a couple of roles we could split that into, so like DevOps, could be
one role, so people who make the deployment of the machine learning models, the data engineers
kind of might be a bit similar, but maybe more on a data, so they keep the look after
the databases and stuff like that, then the machine learning engineer and other similar
kind of like we have machine learning researcher and machine learning engineer, so that's kind
of very similar, but different again, machine learning engineer is more about software, software
engineering site and then machine learning researcher is more about the deep learning or machine
learning algorithms models, making them better, and then the data scientist likes is like
everything else, it's like very broad topic, yeah, I think that's, that's, yeah, so again,
the DevOps, DevOps, data engineering, yeah, building data pipelines, machine learning engineer
or trying to deploy, research to try and better models, yeah, data science is everything
else, so data scientists could be someone who works with Excel, it could be someone who
works with TensorFlow, yeah, it's just to check the description of the job, I think that's
what it's trying to sell as well, I think it's also worth mentioning that many startups
especially use these terms very interchangeably, so someone might be hiring an ML engineer
but they might pitch it as a data scientist position, I've seen that for quite a bit of
course, yeah, yeah, and I've seen a lot of, like, if you just type, when you try to open
the machine learning, and just, yeah, those type of kind of, you see those allow this kind
of like, what's data science, what's computer science, what's math, and every time, every
time you look at it, they are different, so there is no like common understanding, some
of them like very interested in complex, some of them are more like basic, like, like
the data scientist is someone who knows how to work with software, who know math, and
who is good at data communication, and that's the true data scientist, but then, again,
I've sold so many of those, and there is just not common understanding of what that is,
it's, yeah, so this one's different, this one's got data science on the right side,
and IT skills, business skills, there's just no, yeah, this one, yeah, this one's like
again, data science is like, I'm growth term, and then you have the AI, very growth field,
and then machine learning, and then deep learning as a subset of machine learning, so, yeah,
I think this is fun, but we just need to check the job description, and some companies
maybe has got more specific descriptions, some companies who have like more growth,
maybe they don't know themselves, or they don't know what they want to do, so just someone
is mentioning the chat, that they found the working of data kind was a great deal to
recently, but like popular projects, then what to talk about, and also very little, it's
difficult to join the team, even as a volunteer, yeah, yeah, data kind, I'm afraid about
this, so, it's not, I'm not sure, I think it's an NGO, only a few are supposed to, yeah,
it's a non-profit, and they basically data scientists and developers work for free,
and they have a small, small team of project managers, and that basically organize work,
I think on this slack, I saw there was a call for volunteers to help judgegoogle.org challenge,
I think that was a great experience, I think a few people from here were working there,
but after that I started working with a local data kind in Washington, DC, and then once
I started working on the project, because they're kind of open, it's great that you can
talk about the project, and the data kind has a good reputation, so it was, it was a good
way of, you know, presenting skills. So you can apply for like, yeah, they, what I found
is, I registered online with them, but I never got a call for any project through registration
online, but because I was on two slacks, one for Google ML challenge, and then it was
for Washington, DC chapter, periodically they post different projects, and the skill
match, you know, I think it was cool to accept it. Oh yeah, nice one, okay, I'm going
to do it now, but, and the other one you mentioned, there are a couple of different places,
there was a slack, you saw it? Oh yeah, so basically they have, they have a bunch of local
chapters, data kind of local chapters, okay. Yeah. Okay, yeah, the other one that was
hard about this was the, I think the Havan in India, I said, I'm not sure, maybe in Bangalore.
Yeah, I think the one in Bangalore, yeah. Yeah, I think they had something, let's, let's
have a look. Yeah, UK, Bangalore, India, Washington, DC, Singapore, San Francisco, yeah.
So, yeah, so my, my advice would be, I mean, just register, like, didn't have, but like
because I started, I was able to start on one project, then getting on slack, like,
then people start sharing opportunities. Yeah, that was nice, that was useful. Can you
do that remotely? Yeah, so local chapters, they usually, they want local people, but a lot
of times they post remote projects. Okay, nice. If I, if I hear, if anybody hears about
another call for volunteers, like outpost, yeah. That would be great. Another one I've heard
was the fellowship. Yeah. I don't see this website working. I've also heard about inside
fellows program. I think it's it's along similar lines. So I have some experience with inside
fellow fellowship. I actually got accepted there to start last January, but I decided against
joining just at work. They had, like, they had a lot of interesting projects starting and I was
basically could do the same at work and, you know, get paid. But inside, it's an interesting,
it's basically, they don't have any regular classes, but you do research like with them,
and they don't, they sort of like lightly kind of supervise you, but they help you, they help
you sell yourself. And then they, and then they work as, as you, as a job placement, basically,
they get like some, some money from the company if you get once you get hired. So it's completely
free. Interesting. They have two programs like that. So one is fast AI, not fast, one is inside
fellowship, and another is data, the data incubator. But inside, I actually see a lot of people,
a lot of people went through, especially coming from PhD, a lot of people went through that program,
like I see people who are working. Actually, a lot of people who some interviewed went through
inside. Okay. Yeah, interesting. So they also fellowship, guys, I guess something similar.
I think you have to apply for them and complete some sort of a challenge project to get accepted.
And then you kind of work with them for free. Again, it's not paid, but I guess you can learn
for like this September, October, like four months. And after that, and they have a lot of
different locations around the world. So because they don't accept remotes, so we have to be
in one of those positions. And then I think they help you to find a job or something like that. So
that it's again, for four months, full time, do something for them and learn a lot, but yeah. Okay, nice.
The other side of balance, through some scenarios, through Slack, was kind of Delta helpers.
So it's not to find a job or a staff, but there's a lot of people who sit on Twitter. I'm okay to help
with questions and stuff. And at least it's quite long. So I guess that's also a second way to kind
of ask people questions, how to get into data science or obviously a data scientist's work
at a day-to-day on its show. I don't know. Okay. Interesting. We've not seen you screened for sharing it.
So I was showing the data helpers side. Data helpers.org. So there's like someone
asking right now. Could you see? Yeah, good. Someone said on Twitter, hey, can you help
put some questions in their science? And there's a lot of people said, yeah, I'm happy to help,
since the list is quite long. People that said yes. And yeah, some of the research.
We're asking some people about data science.
Yeah. So, yeah. So they're not talking about skills. So like machine learning, skills wise,
and software engineering. What do you need to know to kind of work in such a domain?
So like they develop more mostly software engineering, data engineering is software engineering
with some ML experience. And of course, the machine learning engineering.
So again, machine learning class and software engineering skills.
Machine learning research is something that Jeremy Howard teaches us what to do. So that's mostly
the fast AI courses, I guess. And the data scientists have to be everything.
And then they were talking about teams. And because they did some interviews about
some companies about how to structure a team, but they didn't find any consensus anywhere to
do it. So they're like everything is different. And they do it in different ways.
Yeah, I think that was quite interesting. And on the guest lecture, the
I forgot the person who was talking about the founder of the Sweden biases.
They found actually that the improvements of the accuracy of the model
is mostly coming from the first like two weeks of working in the model. After that,
you're not going to get a lot better. So I thought that was quite interesting. And that's
similar from the other alukas. Alukas was talking about this as well. And there was no slides for that.
But he showed the same if I can find it. Yeah, I think that's exactly the same graphs.
And that's maybe the bar 20. Okay, so I'm going to go through that. So what I found is that
I think that's the one in the middle. The accuracy of the best model
goes up a lot in the first like two weeks or even that's even like two days. And after that,
it doesn't help. Even like I think the one on the right is like number of participating teams
like on the Kaggle. It increases a lot, but the improvement is not getting better.
So this is kind of like the sweet spot between accuracy and how much time you want to spend.
I think they wanted to highlight that. I found it quite interesting.
And of course in Kaggle, you're going to win. Even if it was just like 0.001%,
you're in normal projects. It's not that important to get such an accuracy.
As far as the managing ML teams, like any team, just kind of challenging the ML teams.
Different I guess.
Yeah, they're hiring. We're talking about more hiring like LinkedIn. And we talked about some other
also nice recruiting that conferences this course. Like they talked about interviews a little bit.
Some people still do the whiteboard. Some people ask it to do like part programming.
They would give you some quizzes. I think what's quite interesting, I think, makes most sense these
days. This for me is like to take home ML projects. Although this can be a lot of work from you.
But at least it's kind of more like a more real life project. Because if you take that project home,
you can Google, you can even find help from other people. This is how this works. Even if you work
at the company, you're not going to do stuff on the whiteboard. You're going to use all the resources
you can to solve that project. For me, that kind of like makes sense. But it's going to take
a lot of time from you. It's like a week or maybe even more sometimes. So that depends.
They were talking about how to prepare for interview.
I think they were joking there's eggs on maybe the walls. I don't know. How many of the walls?
Oh, there are some questions. I was watching those ideas some time ago for a
god about that. Do you want to answer those questions? Why does there is no outblock in the
ResNet architecture help with the vanishing gradient problem? Well, I think because the
image of that. Oh, yeah. We can find some some ResNet to the ResNet.
Yeah, I think that's the ResNet stuff. So I think in ResNet, we get this kind of blocks.
Yeah, so compared to the kind of like a normal 34 layer plan layer network, we have those.
They call the ResNet blocks when they have the skip connections. So they go through the
convolution layers and then they get added the same signals signal as it goes into the layers.
But without going through the layers, the kind of called the skip connection.
So I believe those skip connections, they help you to avoid the vanishing gradient problem.
That would be my answer to that question at least.
I also have the following learning curve from the training on single batch,
which in which of the following could be the course.
And this is, oh, this is the error. Okay. So the error is going up and down, but it's still roughly
about 50%. It's not improving. Sometimes it's improving. Sometimes you're getting worse.
So what could be the problem? But,
cheerful flybals, learning rate too low, learning rate too high,
the American stability too big, too big a model. That's interesting. We have any idea?
I'm not so sure.
The learning rate is too low. Yeah.
Maybe just if the learning rate is too low, you would be going like slowly to slow to your
minimum. But in this case, with iterations, we're not really going
for lower error rate. With the two high learning rate, they'll be going
maybe quicker to the minimum, but then you will go, I think, to some higher values.
The shuffle flybals. What do I mean by shuffle flybals? I'm not so sure. I'm with like a mixed
flybals. I don't know. It'll be more than I don't think so. Maybe it's a learning rate, I think.
Question number three. For each of the following prediction tasks,
select the loss function that is best suited for. So the predict sale price of a house listed
for a sale. And then we'll have four, five answers. Okay. That's also a nice one.
So we have the mean squared error, we have categorical cross entropy, binary cross entropy,
CTC loss and gannels. Okay, maybe I need to go through that slacker again.
That was basically the lecture seven,
lecture eight, and then lecture nine. It was a thing was interesting, but I'm going to treat
this as the optional lecture. And then lecture nine, lecture eight was about troubleshooting.
I don't have a lot of time, but I like this picture. So you throw like a lot of data in the model,
I just trust, yeah. And you just trust that you're going to get right output. And if it's not
right, just draw more data or just mix whatever the data will be. So it's interesting.
Yeah.
Yes, I again, a lot of times going to take for you to kind of debug your model.
So there are a couple of,
I'm just going quickly because just ask questions or comments from that lecture because we don't
have that much time to go in detail. Actually, I think that was the, this is the answers for those
questions, because this is the same graph, right? And they're talking here about the labels side
of us. I guess the result is like this, that's going to be because of the shuffle labels.
That must be the case. Okay, interesting.
Yeah, they were talking how to find out if your model is performing poorly. And I like the idea,
yeah, I think this is quite interesting graph. So if you have your learning rates set right,
do your minimum. But if you learn your rate is too high, you're going to get quicker to your
minimum, but it will not get as low as it should. But it's too high learning rate. It will go
up, but if the learning rate is too low, it will get to low point, but it's going to be slow.
But I think it was interesting for me was,
was that like they said, we should start. Yeah, and this slide was quite interesting. So the
common data set construction issues, so we have not enough data. So deep learning models, they need
quite some data. So maybe if you can use transfer learning, maybe not that many data points,
but if you start training from scratch, then you need quite some data. The class imbalances,
so say if one class got like 500 images, another class got just one couple of images,
that's going to be an issue. The noisy label. So I guess this is when your labels not necessarily
are true for the picture. So say if a picture of a car and then on a picture, there's no car.
And then when you're training or your test set is coming from different distributions.
So I think he was given an example when you're training on a car like in an autonomous vehicle
problem, when you're training pictures from like day pictures and then a test are like
two degrees. So that could be a complete different start. Yeah, and it's difficult to
troubleshoot because you might have issues with your data, you might have issues with your code,
you might have issues with the math behind it. So it's not so easy. So what they
what they recommend, and that's actually quite true, just start simple and then gradually
and that's I was thinking about because what we what we've learned on the first day,
we've learned is already quite quite complex models because by default Jeremy uses this like one
one cycle policy, there's a lot of like the dropouts and weight decays and all that stuff.
So I was thinking about that and maybe maybe the next step I'd like to to find out the simplest
model and just to try and a very simple model. But the fast AI works and maybe fast AI is also
a good start. Yeah, so that's that's kind of nice flow chart. Start simple, implement and debug,
evaluate the model, choose hyper parameters, improve your data in the model, and if it is quite
done, start simple, so choose like simple architecture. I'm just going to go very quickly with that
just ask questions or comments from this lecture because I think there's a lot of slides.
It's like 100 40 slides and we don't have that much time.
Yeah, so that's again kind of like because Jeremy is using resident as a first model and
that naturally they recommend something more simple like Linux. So there's different approaches
to this and everything but something to think about. Yeah, but they give you like some default
options for optimizers. For example, possibly other optimizers are the same as in fast AI. It's
also animal optimizer. The learning rate three and like four, I think Jeremy used like three
and like three or something like that. So it's close. Activation is radio, so that's the same with
Jeremy's using. He or had initiation was also the same. Regularization, they suggest to start with
none. I think when the fast AI we get some, I'm not so sure. That's more about
also they recommend to start with a small trading set about 10,000 examples.
And there's a fixed number of objects classes and create as a simpler synthetic trading set.
I think it's a good device. I was I was working recently in a Kaggle dataset and that's like
268 training, 126,000 training images. And and for some reason the model is working very, very slow.
So if you start with the huge number of images, you might have issues somewhere and that's going to
take you time to debug it and so on. So if you start with some make sure you're
model works and then try to increase that maybe more sense.
And also this kind of the same start with like small number of lines. I'm going to mute everyone.
I want to. So just unmute yourself.
Yeah, just first of all, good thing with the ground. That's the first step.
I apologize I'm going so fast with that, but it's just too much of a step to cover in one,
have an hour. Yeah, so and the next step they say it's overfit a single batch.
So if you kind of overfit a single batch, then then there's some issues with your model.
So I think that's interesting, interesting things so they say just overfit your model first.
And once you can do that, then you can try to optimize performance of your model. I think that's
quite interesting. I'm going to skip some slides now to step three. And it's also good if you have,
if we have like known results or like image net, of course, we can compare ourselves to a lot of
other guys who try to do the same thing, but in some cases it's not possible. I guess like Kaggle
gives you that possibility. You can compare your solution to other guys on a leaderboard,
but in some cases it's not possible, but at least if you can.
Yeah, and then once you just keep iterating until your model performs up to expectations,
again, we don't have to make very precise up to 0.001 percent model,
just whatever makes sense for our project.
And then they talk about what pull is a test error. It's a combination of more errors. It's just
to be aware of that. And again, we was talking about the training data could be from
day scenes, and then test data could be from the night. And that's going to cause an issue for
for your model.
What else?
Right, so in the deep plan, you can have underfeiting.
So try to fix that, try different network, different model, more detailed model, like more parameters.
Once you can overfeed, then you need to deal with overfeiting. So that's next step.
So once you start overfeiting, then you need to address that by some sort of dropouts and
other regularization, other techniques that helps you with overfeiting.
I'm going to scroll quickly.
The point see of me. So the address the distribution shift. So again, if you have different test
set, different training set, different data sets, something to think about. And the rebalance
data set if applicable. So yeah, if you have, again, night and day pictures, try to make them balanced.
And that's another problem I had with that data set from Kaggle. I had to like find a thousand
classes. And some classes had like 500 pictures in. And some classes only had like a couple of them.
So I believe that also can cause an issue. So one technique for that would be to over sample.
So it just is basically take the same image a couple of times to make up for the missing numbers in
that class. And then hyper parameter tuning. So like the hyper parameters, like how many layers
in a network. So you can look and choose between the rest of the 34 or 50, 150, I believe.
The white installation, the kernel size could be different stuff like that. You can choose different
optimizers, different batch sizes, different learning rates. Yes, there's a lot of stuff to
play with. And we can do this manually. So just try an error. Another one is like a grid search.
So I believe there will be like certain tooling that would help you to search different
hyperparameters. I don't know if they show any tools for that. Might be in a different lecture or
something. Another one would be just random. I'm not sure about this method by
as an hyperparameter optimization. So then at the end he says that the debugging is hard.
So we start iterative process, start simple and then increase complexity.
They're simple, implement in debug, evaluate, tune hyperparameters and improve model on data.
So that kind of like a workflow they propose can make sense. And they give some of resources
where we can learn more, so the machine learning, yearning. I think this is some sort of a book
or something. I don't know what that is actually. Maybe you can check. So you can sign up for a draft
free copy. Okay. I think it's a book, right? It looks like a book. It's a book I'm doing.
Okay. Yeah. Never heard about that one. And the car part is three. It's a long three.
Maybe I need to read that one. Okay. And there's a blog post.
Okay. It's just something for me to still read about. Some homework.
Yeah. So that was lesson 10, which brings us to a lab session. But because I didn't do lab,
I kind of talked much about the lab. So I still have lab to do as a homework. But if there's
anyone else on the call that did the lab work. And maybe once we talk about this, we can listen to that.
You can chat as well, but I don't see anyone volunteering to. So maybe we can shift labs to another session
on the week. And on the next week, that's what we've done. So do we have any more questions in the
chat? Let's see below. Charters, do you know? No questions. Okay. That's great. So next week,
we're going to have our last session of that. No, no, is it? That's right. That's going to be the last
session. So we're going to finish the labs. So lecture 11, the labs 9, 6 to 9. And that's the
habit. So that's end of the lab sessions. And then the lectures 12, 13, 12 is a testing
on deployment. And 13 is the research directions. Lecture 14 is a guest lecture from Jeremy. And
lecture 15 is a guest lecture from Richard. So those two again, optional. So if we don't count
the optional, we have two video lectures and the labs 8 and 9 for next week. Okay, excellent.
So we also in the agenda, the Avingash's presentation, we have Avingash in the call.
So there's a question from Sagar. He mentions, he says if anyone, he wants to ask if anyone has
had the chance to try the tools mentioned in the data management or infrastructure and doing lecture.
Yeah, so I didn't. And they, they showed a lot of tools in the presentation. They showed a lot of tools.
I'm just trying to show that page again. This kind of is from Google, from some paper from Google
they said, this is your RML called. And it's so small compared to the whole ecosystem of deep learning.
So that just shows the how big that is. And then they show this picture, which shows like all
sorts of different tools for different problems. I've logged into the white and biases, but because
I could not run that lab because yeah, I couldn't install the torch libraries for some reason.
I couldn't run it, but there's a lot of tools to try, but also in a lot of time.
Actually, we'll also be doing the presentation of tools. Yeah,
into that. Excellent, excellent.
Okay, so we can move on to next presentation.
Well, there are more questions. Yeah, the question, how to practice labs. I don't know actually to
answer the best answer to that. So all the code is here in a GitHub. And in a lecture, they use
the suites and biases, and they have the code to run the Jupyter notebooks from there. But because
we don't have that code anymore, it doesn't work anymore, we have to find a way to do it. And so far,
I did not find a way to do it. I know some people did, it's like Michael, he managed to run the code,
and he used call lines instead of the pip end of as they recommend here. Because it's TensorFlow,
I had some issues installing some TensorFlow libraries. I think Pythos requires Google 10 and
TensorFlow requires Google 9. I don't know if that seems or maybe that position. Yeah, because they give
you like installation setup, you have to set up thing, but so I follow that, but it didn't work,
so I don't have time to debug it. But if someone know how to do it and wants to shout to us,
other people have to do it, they'll be great. I haven't tried to do it. Yeah.
Also, hands up to everyone if you try installing TensorFlow, I need a break in everything.
To an extent, but even Zoom is crashing, so a world of caution. Exactly.
So that's kind of, because this was created as a bootcamp thing, it wasn't created as a MOOC,
so they never really, you know, if we go back. Sagar has mentioned it's a problem with TensorFlow's
newest version. Yeah, I think it's common as related to the lab. Yeah, because that was created as a
bootcamp one of them, it's not a MOOC officially, so I guess no one is really maintaining that repo
or making sure it works still. So it's nice we can use it and nice we can see all the videos,
but yeah, maybe the code needs some updating or, you know, my telling. So that's the thing.
But we'll try it for next week, if we cannot make it work. It's a shame, but yeah.
So Sunyan, do you want to take covert presentation and present the
Sure. JVM too.
Oh, definitely. I just requested Avina, she's agreed that I can go first.
My Zoom is crashing constantly because of the Swift thing, so Akash will do the presentation and I'll
monitor the chart. Hi, everyone. This is Akash here. I hope you can see me here. I can see my video.
Great. Yeah, great. It's great to be. I was actually listening in for the past half an hour.
Great discussion. He wonder why I haven't been participating. I've been part of this earlier, but yeah.
So maybe I can I share my screen? Yeah, sure. I'm sharing.
Meanwhile to those unaware, there's been this huge meetup in India that was being run by Akash.
So all of the Bangalore meetups around pasta and even data science would have been mostly
promo cards. Those who might have a take. Excellent. I think this is like the pie torch from zero to
whatever. Yeah, it is. Excellent.
All right. Great. Thanks. Thanks. I am. So, okay, I'm going to share my screen. I hope this was great.
Cool. Can you see my screen? Yep. Yes. Okay. Awesome. So, yeah, quick introduction guys.
My name is Akash. I'm joining from Bangalore. The first time I'm joining the Twimmil AI Study
Group call. So just quick background about myself. I was a software engineer for a few years
and then switched over to machine learning after doing the first AI course online.
And that was just that just completely changed my perspective on how we were looking at data
science and machine learning. Like in college, it was so mathematical and so dry, but faster,
I was so hands on. So that's how I got involved and started actually switched over my career
completely. Started doing ML consulting work and freelancing while also in the mean and as part
of this, I also like along with a friend of mine, we realized that although there are a lot of
people who are interested in data science in India and Bangalore, but there's no group
community, especially having a lot of technical talks. There's too much networking and that kind
of stuff. So we started the data science network about eight months ago and it has been a wonderful
journey that so far we've had about 5,000 members in our meetup groups and we have meetups every
other week. So, yeah, just really excited to be part of the data science community and learning
new things every day. So what I'm here to present today is there's a tool that I've been working
on with a few friends and like Simon has also been involved recently. This is a tool called
Jovian. This is something that was born out of our own problem that we really love Jupiter
notebooks, the interactive nature, the way we can plot graphs and go back and forth and fix things.
I felt like that really opened up things for me in terms of trying a lot of experiments out,
but it also led to a lot of problems, especially coming from a software engineering background.
I was used to having like get kind of a workflow where you sort of branch out, make some change
and come back. But what really happens, what we've seen happens in a lot of machine learning
projects is something like this. If you're using, so the question we asked us is how we'd be tracking
our machine learning experiments, all the different ideas that we're trying. Now notebooks are great
to try things out, but then because they're so rich, they have so many graphs and things like
that, they can be large in size. What we tend to do is we tend to keep creating copies and copies
of data. So this is just like one particular project that I was working on, where we had
over a course of three months, we had like 60 different notebooks and then we would try to put
everything into the just the file name so that and this would be sitting there lying on a cloud
machine and there's no sense getting putting this into gate because this is like 200 MB of Jupiter
notebooks. So that became, you know, that we found that very tricky. We also tried using log files,
but then log files are sort of even trickier where you have like a code in a notebook which you
have updated, but then your log file looks something like this where you have to sort of go into it
and try and parse things out. It's a big mess, right? We tried spreadsheets as well where we would
sort of keep track of things in a spreadsheet like on what date, which experiments, which network
we tried, what hyper parameters we used, and what kind of results we got. But that also sort of
made things really messy because now you have your notebooks somewhere and then you have your data
somewhere and then you have maybe your checkpoints and models somewhere and nobody is really kept
track of the library. So TensorFlow has updated, PyTorch is updated and it gets all very messy.
So we started creating a tool for ourselves and that's what I want to show you. The goal was to
start with Jupiter notebooks and make them more shareable, make them like collaborative so that
people can work or work together on a single notebook, try different ideas and manage the entire
workflow around them. So I will quickly jump into an example. I guess that's the best way to do it.
So this is Jupiter notebook running on my local host. It's an example of classifying handwritten
digits from the MNIST dataset by just training a CNN using Keras. It's pretty straightforward.
You prepare some data, download the data, take a look at the data and then do some pre-processing,
one-hot encoding, have some like a two-categorical which does like one-hot encoding here. Then,
as you start out, as we just saw in the lecture that you start with a very simple model. So I have
a very simple model of single conglare 32 filters and then I just flatten that and pass it through
a fully connected layer. Then I compile it, I train it, I look at the accuracy, okay that looks
pretty good. Maybe it also helps to look at how the model was training so I can plot
how the accuracy is changing. So I guess the dots are the training laws and the validation
laws is the line. So the pretty standard model, right? And then finally, you evaluate it on the
test set and then get the accuracy and laws. Now, suppose you want to share this with somebody
so that they can try it out as well, right? Maybe you're doing this on your local machine or your
Google machine, on your on your DCP cloud machine. You want to send it across to somebody,
just get quick thoughts, get maybe some inputs, maybe they're helping you learn. This is where our
tool comes in, Jovian. So you can just can simply install, install Jovian.
And yeah, I think I already have it. Then you import the library and then you just say
jovian.com it. Okay. And when you say that, it asks you to, it asks you to provide an API key.
An API key is something that you can find by simply logging into the website. It's a, okay,
I'm already logged in, I guess. So I'm just going to copy, it's a one click login with GitHub.
So I just get it, it just gives me an API key, which I can enter here. Now, once I do this,
what we do is we take a snapshot of the Jupyter notebook. We take a snapshot of the entire
backing environment. So here I'm using Anaconda. So we capture the Anaconda environment behind the
scenes. And I'd like to ask you, do you know how to export the Anaconda environment? Because there's
a really complex command that you have to try, especially to make sure that the environment is
reproducible across between like Mac OS and Linux and Windows. But anyway, we do a bunch of these
things behind the scenes. And then we give you, we upload it to the cloud and you get a quick,
like a URL where you can see the notebook. So you don't need to have Jupyter running.
You can, you know, the notebook is there. Apart from that, we also have,
all the file, we also have the environment that has been captured. So these are all the libraries
that I'm using here, right? And then you can simply take this link as I'm taking right now. And
you can simply take this link and send it to someone. And there, that's, that's, you know,
then now you can click the link and you can try it out. Now, if you want to quickly, if you want
to try this out on your own machine, all you need to do is you can click the clone button.
Now go back to your terminal. Let me open the new one here.
What, let me just rename this to something nice. Yeah, there you go. Okay, so I'm just going to,
I'm just going to run the clone command. Again, to run this clone command, I need to have
the Jovian library installed. So I, I'll do like pip install Jovian, which in my case, I already do
better just to show you. And this installs a CLI command called Jovian. So that's where you can
paste the command that gets copied when you click the clone button. And that downloads all the files.
And it gives you a bunch of instructions for what you need to do next, right? So I'm just going to
enter this directory, run this Jovian install command. So what this does, the Jovian install,
depending on like, as you can see here, I have an environment or YAML file, environment or Mac
West file. So it looks into if there's a environment specific, if there was specific file, it uses
that. Otherwise, it uses environment or YAML. The reason we need to do this is because Konda
environments and Konda packages have different versions across different platforms. So sometimes
things don't work. Sometimes things fail and then it just errors out. So what we do is we try to
install. If there's an error, we, like, comment out those lines from your environment, try to
make a best set and try this three four times till the entire environment is installed, right?
And this has been an open issue on Konda for over a year where people are saying, I just can't
reuse my environment fine. But we try to solve that with just doing a few hacks and trying the
next best thing. But yeah. So what this does, you know, you could do some work on your local
machine and then maybe push it to Jovian and then open it up on your cloud machine, pull from
there, clone from there and start running, right? So now at this point, if I do, if I run the
Konda activate command, that activates my new environment, which is just created and I can go to
put a notebook and start playing around with that notebook, right? So simple push and simple
commit and clone flow that that we do. Now, the, now the next thing that we've also done on top of
this is maybe you're just looking for some feedback or some inputs. So what we've done is we've
built out a commenting interface on top of Jupiter. So for instance, if you have a question,
you know, very specifically about, let's say, about the model. Somebody has a question saying,
why are you using kernel size 32? So they can just comment it on that particular cell and then
I will get a email notification where I can click through and reply to them and then they will
get an email notification, right? So this allows for very quick feedback and discussion on Jupiter
notebooks. When you're, you know, one person can actually do the coding and the others can all
come in and give their thoughts on it. This was something that we found missing. So we just
decided to add that as well. So next, the way we use it is we run some workshops here. So often,
we put all the, we put all the Jupiter notebooks, environments, everything on Jogin and then let
people clone it, try out something on their own and then push the new versions and then get feedback
from us, right? So that, it, it helps to do that very easily. Okay. So coming back to this,
now one thing is that, okay, the, it's great that I have the Jupiter notebook. It's great that I
have the environment I can reproduce it. But what ends up happening is, you know, even if you just,
you make some changes, you try, you create a new version, you, you ultimately you end, start
ending up with like a half a dozen dozen or so Jupiter notebooks, right? And what you really want
is, and what you really want is you want to be able to track specific things about notebooks.
Like for instance, in this notebook or in this experiment, what are the hyper parameters that
tried and what were the success metrics I got, right? So that's where we have created a very simple
API as well. Is there a question? Okay. Yeah. That's where we have created a very simple API as well.
You know, all you, all you need to do is just import Jogin and then we have a couple of APIs,
we have log hyperparamts. Okay. So this is two log hyper parameters. So for instance, here I'm
just going to add a small note about the architecture that this is the simpler con 32 plus dense,
right? Maybe I'll also add a note about the batch size. You know, the batch size I'm using is 128.
I'm using three epochs and what else? Yeah. What else is interesting here? I think I'm using the RMS
prop optimizer, right? So this is currently a bit manual, but what we're trying to do is,
now when you do a model.fit, we're trying to create a callback so that all of this information
can be picked up automatically, but I just wanted to show you the basic API as I do that.
So that logs are hyperparameters. Then once the model is trained, I might also want to actually
keep track of the metrics. So that's where I can do jogin.log metrics. And here I'm just going
to put in these things. So I want to mention that even for category competition, like this is how
most people usually track, like they have multiple notebooks and then they create spread sheets.
So this is essentially looking at replacing it with just one single interface.
Yeah. And right now, I'm actually having to type a lot, but the idea is this will be just like a
single callback that you introduced into your history, you know, at least for the popular frameworks.
So now I've captured some metrics. I've captured them hyperparameters and we'll take a look at
what they look like. But apart from that, the other thing is also that, you know, maybe I might
this code, this might be some common code that I use across all my notebooks. So I might
actually put this in like a utils file, right? So maybe I'll do like create a utils for utils file.
And we put this here, call this plot history. Okay, very simple. And I'm just going to import
the from utils import plot history and then this plot history does the same thing, right?
So now I also have a dependency on a certain file. And another thing is like I've trained this
model. Maybe this is a simple example, but then in certain cases, your model can actually have
been trained for a long time and you might want to just keep the save the weights. So that's where
you probably want to do something like, you want to save the model as well. So I'm just going to
save the model, model.savemscnn.h5. Okay. And yeah, so now I have like a dependency on a file,
I've lost some hyperparameter, some metrics. This time when I commit, what I'm going to do is I'm
also going to include the files that are depend on. So this is utils.py and the artifacts that I've
generated. So this is like amnestcnn.h5. Right. So now I commit once again, what it does is it create
updates that notebook. So it creates a new version essentially. Once again, it captures the environment
to make sure if you've created, I did any new libraries that get captured. Any additional files and
any artifacts are also captured. And also the metrics and hyperparameters, everything is then
taken. And then now you have a more start to have a more complete picture of your experiment.
Right. So you have the notebook, you have environment, you have the utils files right here, you have
the artifact. Now we are trying to differentiate between sources and artifacts because, you know,
as you might have faced, if you train a resident 152 and then create, save a checkpoint,
that's going to be like a 120 MB file. If you check that into Git and then you train two or three
more of these, suddenly you have a Git repository, that's one GB. So every time you push and somebody
tries to pull it, they're downloading, you know, one GB worth of stuff because the weight change
every single time, right. So that's why we differentiate between artifacts and sources and,
you know, artifact and we provide an option to actually clone without actually downloading the
artifact because if you just want to reproduce it, you probably don't need it. On the other hand,
if you just need the artifact, you can download it right here as well. So that's the files, the
dependency and the artifact. But also, this is where it starts to get interesting, where,
you know, now you start to get, as you log some hyper parameters and some metrics, you start to get
a timeline of what was actually happening in this project. So, you know, I have here, it's a
corn 32 with a dense, three pox, RMS prop, and I go to an accuracy of 97%. That's pretty good,
but maybe I can do better. And as you start to sort of keep doing this for multiple times,
start to record multiple versions, what you can do is, so here I have another, you know, the same
example that I've tried a few versions. So you can check the versions, okay, not here.
Yeah, so same example I've tried, I've recorded a few versions and all the versions are visible
here and the version drop down. And I can actually go click on compare versions, right? So now,
whichever version has a record, all those records show up here. So I can see when it was created,
who created it? So come back to this, you know, the notes that I've added here and the hyper parameters
and the metrics. So it starts to give you a good overview. One thing that we've added and we're
adding more things is, you know, you can select which columns you're interested in. Maybe
epochs is not that important, maybe you don't need hyper parameters at all. So you can just look at
the metrics. Another thing that we're doing is, you know, if you have some runs which you're not
interested in, you can actually just select them and remove them, you can reorder the columns,
you can actually, you can add in notes here, come and change, change in entry because maybe it was
recorded incorrectly, right? So I'm going to try to make this very powerful, pretty much almost
as powerful as a spreadsheet, but then more targeted for the data science use case, okay?
And then finally, the last thing that I want to show you is collaboration. So now this is just me
working and maybe me sharing with somebody, but you can see here that there are other authors here as
well. That's where we have an option. So a bunch of things, one is a visibility that if you want to,
so currently this is a public notebook, anybody can go on my profile and find it. I can make it secret
so that only people with the link can access it. We're also working on private notebooks so that
you have to be logged in and added as a collaborator. I can archive this notebook once it starts to
get filled up on my profile and then finally, I can add collaborator so I can just enter that username
and yeah, and that will, that will add them as a collaborator, right? So now what happens is
every time they clone and they try to commit, this is going to add to this particular project itself,
right? So slowly starting from a notebook, we're just trying to build a complete story of a machine
learning, of a machine learning project and trying to make it as seamless as possible so that,
you know, as a ML student or engineer or data scientist, you have to do minimum work.
And we have been trying this for about four or five months now using it at workshop.
It has been pretty helpful for us. A few community related, a few inputs for the community were,
hey, this is great. But what if I could, I want to be able to run this immediately as well.
So that's where we have created integrations with binder and Kaggle. So if I click run on binder,
what this does is this immediately goes to mybinder.org. I don't know if you're familiar. Mybinder is
like a free hosted Jupyter notebook which can create a rather Jupyter notebook
instantly from any repository, get repository. So that's what we've added. It's going to take a time
over. Okay, so it looks like, and what it does is it actually caches. So first time you click,
it's going to install all these things, but next time it's just going to keep a Docker image around
so that this can start immediately. Another thing is if you want to use a GPU, you can click run on
Kaggle and then this would run immediately on your, on your Kaggle account, right? And you just
need to connect your Kaggle account. There you go. So yeah, so that's that's pretty much it from my,
I don't know why this is not working. Okay, I'll look into this. We're also adding colab and a bunch
of other platforms soon, but yeah, that's it. So that's pretty much a lot and just wanted to show
you what we are building and if it is useful. So we are still, I would say we are still in beta.
We're still building. We have a lot of things working, but with this, still a long way we want to
go. So would love, you know, if you could try it out, give us feedback, tell us what you would want
in this, because we are very open to just trying to just building what, what will serve the,
serve the user's best, right? Yeah, and we have been personally using it more as like in Kaggle
competitions as like a internal leaderboard to keep track of who's doing what and you know,
we can quickly click through see their approach, see their loss, see their notes and things like that,
right? So building it as we use it. Yeah, so that's that's pretty much it. If you have any questions,
I'm happy to answer. I think you know, it's very good, very nice, very like complete picture.
So I've not, I didn't know about this too, so I know a lot. I've been trying to use also Kaggle,
it's got something kind of not similar, but at least they have this idea of committing your
kernels. Yes. So Kaggle, you can commit and then it saves your data, saves or not book and then
it saves your output files. Yeah. But it doesn't, it doesn't save all the nice information as you
have like architecture. It doesn't have that much. It's not that rich. Yeah. Yeah. One thing also is
that Kaggle, it has to be, you sort of have to commit it on Kaggle, so you can't really run it
on your local machine. It's right. We wanted to have it flexible and have it open.
Yeah. Another quick note was is that this is most of this is open source. So the Jovian Python
library is completely open source. We're trying to open source a backend as well, but it's just,
you know, we have to pull out all the specific backend specific things before we can, you know,
make it generalized enough. So yeah, I'll post this link here as well in case you want to post,
create, you know, try it out. If you're facing issues, you can always switch out to a
share of cellar poll requests. The other thing, whatever isn't open source is free and will stay that. Yeah. Right. Thank you.
How I like envision it is like, especially for Kaggle, like if you run something locally,
and then you also want to run it on a kernel, and you like want to get away from the flakiness
of the kernel, if you run it locally, and say if you're participating in a kernel's only
competition, you get to run it on a kernel. Or if you want to check it out on an AWS instance,
like if you want to run your BFI, it's your most model, you could run the 32 core machine,
just run it there, get the trained weights that have the inference run on Kaggle kernel,
inference will be a competition or even locally. That's once more something you can do with this.
Very easy. Yeah, folks, right. Okay. All right. And you can reach out to me in case you have any
questions, I think. Sayam will share the contact details, possibly. Yeah. Right. Okay. So I'll
start. Yeah. You could join our slack as well. So people can ask them. Yeah. Yeah. Yeah. That's great.
I would love to. Yeah. I'll be there. I'll slack them. Yeah. Then we can ask questions. They're
all right. I'll share the link in the slack as they're like, if anyone has any feedback,
I think it's very open to community feedback also to please ping me or him once he joins for any feedback
or suggestions. Next on. Thank you.
So thank you for the presentation. We have we have one more presentation for today.
It's Avinish. It's going to present. Yeah. Hi, Michael. Hi. I'm a properly audible. Yeah. Hi.
Yeah. So do you want me to take the presentation today or?
Yes, it's 720. Maybe you can at least tell us what you want to talk about. And if that's not
enough time for you, then maybe you can reschedule. But what do you think?
Come on here, Avinish.
Hello, Avinish. Still there? Can you hear me now? Yeah. Yeah. So the presentation that I wanted to do
was on model interpretability and visualization. In that, I will be walking through a Kaggle kernel,
which so it's on food classification like hot dog version 2 where it doesn't it doesn't just say
whether it's hot dog or not, it can also detect the other classes of food. So using that,
I'll be explaining how the Inelaya Activations look and also a bit about activation maps.
So this is what the talk is going to be. But I feel it is going to take at least 22 to 25 minutes.
Okay. And you'll try to talk next week. Yeah, I'm fine with that.
Okay. If you could just share the link with us, we'll check it out for the week. Sure. Yeah.
I will actually really sorry if I took up too much time. Sorry. I'm saying I'm really sorry if I
took up some of your time. Not at all, Akash. It was very good presentation. Okay. Thanks.
Yeah. So I'll share the link, the Kaggle kernel link. And we can discuss that in the next
in the next week's session. Okay. Great. So I'll move that for next week. Do you have anything else
to share for today? And then use? Yeah. I have some open slots for the Chai time data science
conversation that I've shared for this upcoming week. So if anyone wants to talk about anything,
please feel free to use any of the topic. What is the most, what's the most things people
like to talk about in this discussion? Can you? I don't want to hear like names and things.
There might be some statistics. Was the most interesting box most asked things about?
But it's also the thing, but I guess Kaggle and Fastie, then machine learning jobs.
Okay. Nice. Nice. That was very nice of you, Sanyam.
Thank you. Just thank you for helping me. Yeah. That's nice. That's nice.
So when I found this page on data helpers, maybe something you would be
interested to have your name to it, I was thinking. I just learned about it today. I have the tab open
and check it out after this. Yeah. That's excellent. That's excellent. Okay.
So if that's everything for today, I guess we'll just close for today and
we'll meet next week, the same time, the same day, for our last session of
I'm going to show it again. If I can still open for suggestions. Hopefully, a Fastie is released
by then to the public. We could start our Fastie session as soon. I'll be happy to
volunteer to take those notes. Yeah, indeed. Good. You mentioned that. I forgot about this.
So, indeed, it's so next week, we're going to have our, I hope we can see my screen now.
Yeah. Yeah. So next week, we're going to have the last Fast full stack deep learning static
group thing. And then, and then say it's a blank page. So I've collected from all the,
from Slack, from what people were saying that what potentially you could do next,
like secure and private AI, there were some people interested doing
ML course from open data science, which starts September 2nd, I think, for the last session
of that course in the current format. And there was like advanced NLP, which spacey,
all of those are free, by the way, so it's a good look. We can do all of them. Fastie, I released,
kind of silently released, quietly released, NLP because the videos are not released yet,
so it's only notebooks. The statistical learning is more like machine learning from Stanford,
free introduction with PyTorch, free, there are some Stanford courses on MLP,
recommend that the deep unsupervised learning, that we cannot do all of them at once.
Well, we could, if we create like separate groups, led by different people and meeting at
some times, maybe even overlapping with some others. So that's another option. We could do more
courses, but then there would be like smaller groups doing the courses. Or we could try to keep
it as one group and then just select a course. Of course, something that a lot of us,
this is how the group started, was the Fastie I deep learning course, part one, was by June last
year. So the part two, it's not released to public it, and I didn't hear when they're going to do it.
Didn't you do, but I'm not sure if that still works. Yeah, exactly. I think they now busy with,
they now busy with updating the Fastie I library to V version two. So I'm not sure what's the
priority for them now, because they also promised a couple of lectures, livestream lectures to the
part two course, which also it's not sure when they're going to be available.
Yeah, so we can continue that discussion on a slack, but it's just up to us to decide what to do next.
I want to suggest that I gave these courses a look, and in my opinion like most of these scenes are
covered in Fastie, except for a security AI. So and personally, I'd also like to like this
through the, anyways, I will be going through the materials again from part two. So it'll be really
cool to have other people, other people also joining the study group, but again, like I'll wait for
the common opinion. Yeah, it's a fair point, because the Fastie I part to was closed, was only
open to some people. So when we open this to the public, to general public, I think I think it
makes sense to kind of do it again. I tentatively put like one lecture per two weeks, because those
lectures are quite heavy, I guess. Very dense. Yeah. So maybe that's an idea to do like one lecture.
So we can meet every week, but then we can split the lecturing two halves and then talk about one
lecture for two weeks, maybe. Okay. Excellent. So let's continue the discussion on the slack.
And next week we still talk, Fastie, sorry, full stack deep learning, and I'm going to move
the ethnic presentation for next week. So at least we have also agenda. Also the Daniel and
Abin, they also wanted to talk about deployment of deep learning models, but then they put
it on hold. So maybe they also talk about this in future soon. Okay. Excellent. So thank you for joining
today. Talk to you next week. Thank you for hosting, Michael. Thank you.
