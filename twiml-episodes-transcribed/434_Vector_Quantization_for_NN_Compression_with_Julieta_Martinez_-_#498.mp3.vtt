WEBVTT

00:00.000 --> 00:16.400
All right, everyone, I am here with Hulieta Martinez. Hulieta is a senior research scientist

00:16.400 --> 00:22.880
at Wabi. Hulieta, welcome to the Twoma AI podcast. Thank you. Thanks for having me.

00:22.880 --> 00:27.840
I am really looking forward to diving into our conversation. We're going to talk all about

00:27.840 --> 00:34.400
some of your research into computer vision. To get us started, why don't you share a little

00:34.400 --> 00:39.680
bit about your background and how you came to work in the field? Yeah, of course.

00:40.960 --> 00:51.200
So I started, so I was born and raised in Mexico and around 2011, 2012, I finished my undergrad

00:51.200 --> 00:59.040
there, and I really wanted to go somewhere else for my for grad school. So I applied to a bunch

00:59.040 --> 01:05.280
of universities in Canada. Then I got accepted into the UBC, the University of British Columbia,

01:06.560 --> 01:13.120
and then I started when my master's there then eventually went on to do a PhD with the same

01:13.120 --> 01:22.640
institution, some supervisors. And then in 2018, I was grabbing my PhD, but I just had to defend,

01:22.640 --> 01:29.920
so I started looking for jobs, and I ended up moving to Toronto to Uber ATG, which was this

01:31.440 --> 01:39.440
lab that had just been founded and self-driving. Then in earlier this year, that lab shut down,

01:39.440 --> 01:46.320
and a few weeks later, a little bit later after that, I joined Wabi, which is what I'm working

01:46.320 --> 01:52.080
at right now. Awesome. Tell us a little bit about Wabi. What is Wabi doing?

01:53.760 --> 02:00.080
So Wabi is a company dedicated to build an ex-generation of self-driving.

02:01.680 --> 02:07.920
And so what would we believe is that self-driving is the most important and exciting technological

02:07.920 --> 02:17.600
innovation that we will see in the next few years. And Wabi is that Wabi's deal is trying to build

02:17.600 --> 02:23.280
upon the lessons of the last 20 years, and try to build an approach that is AI first. So not just

02:23.280 --> 02:28.720
something that is hand-engineered, and then we try to put AI to make it better, but something

02:28.720 --> 02:35.760
that right from the get-go can really put AI in the center stage. Awesome. And so when you say

02:35.760 --> 02:46.800
when you say kind of AI first versus bolt-on AI after, what are the implications of that approach?

02:46.800 --> 02:53.440
Where do you see it? Express itself. I imagine that everybody says that, but it means different

02:53.440 --> 03:01.680
things. So I guess it means that when you're building a system, you really have this choice of

03:01.680 --> 03:13.760
going for a more traditional approach, and then trying to say alter some parts of the stack

03:14.240 --> 03:22.160
with learned parts, I would say. But if you want to, it actually takes a lot of engineering

03:22.160 --> 03:28.320
effort, and there's a lot of overhead in trying to do this replication on a system that is

03:28.320 --> 03:35.600
in production. But if you build a system that from the start takes this into consideration,

03:36.480 --> 03:43.520
then it should be easier to say, for example, experiment on which parts should be learned,

03:43.520 --> 03:50.000
or which parts should not, or which parts should be trained and to end, and which parts should be

03:50.000 --> 03:55.360
trained separately. There are all these questions that arise when you are trying to decide where

03:55.360 --> 04:03.040
what the role of AI should be. And so I think it's really something that we're trying to do

04:03.040 --> 04:11.760
from the very start. That makes sense. Got it, got it. Yep. And so you, actually this week,

04:12.480 --> 04:22.160
as we're recording this, are presenting at CVPR. You've got a couple of papers at the conference

04:22.160 --> 04:29.440
that we'll talk about, and you're also doing a keynote talk at the Latinx and AI workshop.

04:31.120 --> 04:37.360
You know, let's start there. Tell us about the talk that you're doing at the workshop.

04:38.800 --> 04:43.520
Right. So first of all, I want to thank the organizers of the workshop for reminding me.

04:44.400 --> 04:50.080
It's funny. We have, so there's not a lot of people from Latin America in the field,

04:50.080 --> 04:56.800
let's say, ML and computer vision in particular. So we have been, there's a couple of us,

04:56.800 --> 05:03.760
like maybe five of us, six of us, that we have been trying to meet up every time there's a conference.

05:03.760 --> 05:07.920
We started with, like, you know, hanging out, the conference, like sometimes someone would come

05:07.920 --> 05:13.200
to your poster and be like, you notice that maybe they're like, Latin Americans, you'll be like,

05:13.200 --> 05:19.520
maybe it's big Spanish and so on. And then we would start, like, say, hanging out, like going for lunch,

05:19.520 --> 05:26.320
or trying to build a little bit of a community amongst ourselves. And then there's this Latinx

05:26.320 --> 05:33.360
in AI organization started. I think it was at Newrips where it started. And so then it started

05:33.360 --> 05:38.000
branching out to other conferences. And now there's a more formal presence, I would say,

05:38.000 --> 05:45.040
and more, and there's a workshop where, like, submissions from people from Latin America are

05:45.040 --> 05:54.320
encouraged. And where we want to also highlight tap speakers from Latin America, or Latinx living

05:54.320 --> 06:03.120
in the US or Canada, or, you know, another part of the world. So I think this is the first or

06:03.120 --> 06:08.160
second time that we are doing this official thing with official sponsors and official canals

06:08.160 --> 06:18.320
and posters and so on. Okay. Right. So that's how that happened. Got it. And the presentation that

06:18.320 --> 06:27.200
you're talking, what's the title of your talk? So I decided to name my talk what those large

06:27.200 --> 06:32.080
kill visual search and network compression have in common. Because I think the answer to that

06:32.080 --> 06:36.000
shouldn't be obvious. And I didn't encourage people to come to the talk.

06:38.400 --> 06:44.400
So let's dig into that. What do large-scale visual search and neural network compression

06:44.400 --> 06:55.600
have in common? So the argument that I tried to make there is that, so first of the motivation,

06:55.600 --> 07:02.240
is that we know that there's large neural networks that are that currently are achieving

07:02.240 --> 07:07.680
state-of-the-art performance on all sorts of tasks in computer vision, natural language processing

07:07.680 --> 07:16.080
and so on. And so they're the big elephant in the room. In particular, there is evidence,

07:16.080 --> 07:20.880
and I think this is starting in the NLP side, but now there's some papers that are showing this

07:20.880 --> 07:28.320
for computer vision tasks too, that the performance of these models scales log linearly with how

07:28.320 --> 07:35.600
many parameters they have, which eventually requires more compute and requires more data. So everything's

07:36.800 --> 07:42.240
we are at a moment where we have kind of known for a couple of years that this was going to happen,

07:42.240 --> 07:47.600
but now we have like very, very clear evidence, very clear and critical evidence that larger

07:47.600 --> 07:53.760
models are something that we're going to need. And so this means that we now have models that have

07:53.760 --> 07:59.760
billions of parameters and models that have trillions of parameters are coming. And so this means

07:59.760 --> 08:05.120
that if you want to deploy these models into a system, you have to compress them, like there is

08:05.120 --> 08:12.480
no going around that, unless you are in a very high-end lab where you have access to a super

08:12.480 --> 08:18.720
computer, even if you're trying to even if you have a powerful desktop or a powerful laptop or

08:18.720 --> 08:24.400
a powerful server for these models that have billions of billions of parameters, you're going to

08:24.400 --> 08:32.080
have to compress them anyway. So that is, you know, it sounds like it's a new problem that we are

08:32.080 --> 08:38.080
facing and it is kind of new in the context of neural networks, at no point in history how we

08:38.080 --> 08:48.400
ever build these trainable systems that are so large. But it's also true that we have seen

08:48.400 --> 08:54.480
similar problems in other areas. And we had similar problems before deep learning to

08:54.480 --> 09:04.560
center stage in the computer vision community. So in particular, myself, when I was doing my PhD,

09:04.560 --> 09:10.000
I worked on this problem called, it's called a large scale approximate in just neighbor search.

09:10.640 --> 09:17.200
Okay. And so this problem arises when, say, for example, you have a database of images

09:18.320 --> 09:22.240
and you want to find objects. And so you want to do, you want to match something in them.

09:24.800 --> 09:30.960
So the way that the way you do this and with the pipelines we had would be, say, from a

09:30.960 --> 09:36.000
from a 1080p image, you would run this through a key point detector and then you would find a bunch

09:36.000 --> 09:43.600
of features. And, and fairly high resolution image, but not super high, like, say, 720p 1080p,

09:43.600 --> 09:48.880
it would give you around 10,000 descriptors. And so then when, when an image comes, you have to

09:48.880 --> 09:52.800
extract these descriptors and then you have to find the nearest neighbors in these database.

09:52.800 --> 10:02.240
And so that means that if we have a data set of, say, like, like, say, a thousand images or, sorry,

10:04.240 --> 10:08.080
if you have a data set of a million images, then you're dealing with billions of descriptors.

10:09.840 --> 10:14.480
And so we have these database, these databases that are so large that we

10:15.440 --> 10:19.200
can't really keep them in memory. And then if we want to deploy them, we have to compress them.

10:19.200 --> 10:26.400
We have to somehow get around them. So I think what these two problems have in common is that

10:28.000 --> 10:33.360
they have very large computational requirements, particularly in terms of memory.

10:34.000 --> 10:38.080
And the fact that things are so big that we cannot even keep them in memory,

10:38.080 --> 10:43.760
we have to be clever about how we compress them such that we can still use them while they are

10:43.760 --> 10:49.360
compressed. It's a common team that I try to approach in this talk.

10:51.360 --> 10:58.400
It strikes me that while there's thematic commonality between these two problems,

10:59.840 --> 11:02.160
dealing with large databases and dealing with

11:04.640 --> 11:09.360
high dimensional many parameter neural networks are very different.

11:09.360 --> 11:17.840
And how do you, does the connection extend to technical approaches or is it more of this thematic

11:17.840 --> 11:26.640
connection? It's both. So there's definitely the thematic connection where the database,

11:28.800 --> 11:31.680
let's call them databases, but some of them call them data sets.

11:32.800 --> 11:37.120
They're so large, but they're also high dimensional, right? So usually these descriptors that

11:37.120 --> 11:43.120
we would extract from images, even if you go back 20 years, people were already using

11:43.120 --> 11:49.680
descriptors that had like hundreds of dimensions. So the sort of high dimensional problem was already

11:49.680 --> 11:59.200
there. Now, how do we approach now? Yeah, so I guess the talk kind of builds up upon to motivate

11:59.200 --> 12:07.440
that maybe we are facing the same problems, we can try to apply some of the same solutions. So

12:09.040 --> 12:14.800
in particular, in this talk, we are talking about this paper a little bit more in detail.

12:14.800 --> 12:19.680
The paper is being presented in the main conference where the workshop has been held on that

12:19.680 --> 12:29.520
CVPR. And so here we argue that we can use this particular approach called I guess vector

12:29.520 --> 12:37.440
quantization or some people might know it as product quantization. So product quantization is a

12:37.440 --> 12:44.160
very nice approach. It's not something I developed. It's an idea that originally came

12:44.160 --> 12:52.640
from a researcher that is currently at Facebook. I think his name is Erveje Gu. It's a paper that

12:52.640 --> 13:00.320
was published around 2011. And so it is motivated very much by this need to find nearest neighbors

13:00.320 --> 13:06.800
in high dimensional spaces very fast and with a compressed memory budget. And is it specifically

13:06.800 --> 13:12.880
in the context of neural networks or is it outside of that? Yeah, so this was 2011. This was dealing

13:12.880 --> 13:21.200
with the database problem before neural networks were widely used. So they actually have the

13:21.200 --> 13:27.200
they motivate this with the computer vision setting and experiments. It's a tip on the paper

13:27.200 --> 13:31.760
like transactions in pattern analysis and machine intelligence. It's one of the big computer vision

13:31.760 --> 13:40.160
journals. So the data sets are from computer vision and so on. So in particular, there was this one

13:40.160 --> 13:47.120
descriptor that everyone was using in the day called SIFT that describes a small patch of an

13:47.120 --> 13:51.920
image. And if you have a bunch of them, you can do a lot of interesting things with the database.

13:53.840 --> 13:59.360
So the approach is actually very, very simple. And I think that's part of what makes it so nice.

14:00.640 --> 14:05.360
So we're going to have, so after we extract all these descriptors,

14:05.360 --> 14:12.160
we are going to stack them on a big matrix that is going to have dimensionality D on one side.

14:12.160 --> 14:16.960
That's going to be the tiny side and then the size N where N is the size of the database. And the

14:16.960 --> 14:23.120
thing is that D can be like 100 or something like that, but N can be 100 million or so on.

14:26.320 --> 14:33.920
The simplest way I like to explain this is you can cut these data sets into say four smaller data

14:33.920 --> 14:42.080
sets. If the dimensionality is 100, then each sub data set is going to have dimensionality 25.

14:43.920 --> 14:49.600
And so then we're going to run k-means on each of these little data sets or a small representative set

14:49.600 --> 14:56.080
of that. That is going to give us two things. So the output of k-means is on the one hand the cluster

14:56.080 --> 15:01.920
centers or we're going to call that the codebook. And then it's also going to give us an assignment

15:01.920 --> 15:08.240
for each point in that sub data set to the cluster center that is closest to it.

15:09.920 --> 15:15.040
So now what we're going to do is we're going to run these k-means with a very low k,

15:15.040 --> 15:19.280
so say k equals something like 256 is like the standard value we use.

15:20.720 --> 15:26.240
So that means that the codes are going to be numbers between 0 and 255.

15:26.240 --> 15:30.400
And that means that we can store them with a single byte.

15:32.800 --> 15:38.640
Now the way, now the nice thing is that once we have these groups of codes in codebooks,

15:38.640 --> 15:42.240
we're going to throw away the regional database. The regional database that was so large,

15:42.240 --> 15:47.120
we couldn't fit in memory. And these codes that are small, we can load that in RAM.

15:49.040 --> 15:52.480
This is of course it's going to be an approximation of the regional data set.

15:52.480 --> 15:57.040
But because you can load it in RAM, then you can do clever things with it.

15:57.920 --> 16:01.920
So the first thing is now you have extra space and something that you can manage.

16:01.920 --> 16:07.120
So you can come up with certain data structures that allow you to say do filtering.

16:07.120 --> 16:11.360
Because when you have a query, even if it's compressed, even if it's very nice,

16:11.920 --> 16:16.560
easy to search, you still have hundreds of millions. And you don't want to compute hundreds of

16:16.560 --> 16:22.640
millions of distances. So you want to have some data structure that filters good candidates

16:22.640 --> 16:30.240
that are likely to be the nearest neighbor. But the other thing you can do is that you can use

16:30.240 --> 16:37.760
this code to approximate the distance to the regional vector. So when the query comes,

16:37.760 --> 16:43.840
you can also split it into four chunks. And then you can compute four tables from each little chunk

16:43.840 --> 16:50.960
to the code book, to the code book that we computed before. So that gives us four tables of

16:50.960 --> 16:59.200
255 entries each. And we can just do lookups. So if the element 2,000 in the database is represented

16:59.200 --> 17:09.440
by five numbers, called 5, 200, 120, and it's 255, then you can look up table one in

17:09.440 --> 17:15.680
index five, they will do an index 100, they will tree index, whatever number you had and so on.

17:16.640 --> 17:24.000
Just to make sure I'm following you, is it fair to compare this to or make the analogy of like

17:24.000 --> 17:28.880
a hashing table or hashing algorithm where your hashing algorithm is nearest neighbors

17:30.000 --> 17:35.040
or related to nearest neighbors? Yeah, so hashing is also an approach that will give you

17:35.040 --> 17:43.120
you it's also used to to make nearest neighbors faster. It often counts with nice theoretical

17:43.120 --> 17:49.040
guarantees. And so people like hashing because of that. One of the things that that

17:49.040 --> 17:54.080
product acquisition showed was that even though hashing hashing approaches tend to have

17:54.960 --> 18:04.720
nice theoretical guarantees in practice, they were very much outperformed by doing this

18:04.720 --> 18:13.520
quantization approaches. So yeah, this was 2011, now the literature has moved a lot on both sides.

18:14.880 --> 18:21.520
So, you know, I might be wrong now, I don't follow this closely, but back in the day, that was

18:23.440 --> 18:26.480
pretty much what the evidence was saying.

18:26.480 --> 18:37.520
And so we've got this methodology that we can use on the database side to it sounds like

18:40.560 --> 18:47.600
get more efficient ways to filter the data and you know, based on the reference to

18:47.600 --> 18:55.040
visual search, search as well. What's kind of the next step in your argument, how do we get

18:55.040 --> 19:03.920
closer to the application of neural network compression? Yeah, so I think this is the

19:03.920 --> 19:08.560
last obvious point and it's something that I was very proud of when these ideas started going.

19:09.760 --> 19:17.280
So again, to be fair, like other people had experimented with vector quantization of neural nets.

19:17.280 --> 19:26.160
There are papers on 2014, 2016 and so on, but it is considerably much less explored. So the standard

19:26.160 --> 19:32.240
approach to compressed neural nets is usually scalar quantization or things like network, there's

19:32.240 --> 19:42.000
a lot of focus on network pruning or say low rank approximations. Vector quantization, you can count

19:42.000 --> 19:47.600
the number of papers that do that for neural networks, it's one or two hands.

19:49.520 --> 19:56.640
Can you maybe talk a little bit about scalar quantization versus vector quantization and

19:56.640 --> 20:04.960
how those approaches are worked to set us up for your application?

20:04.960 --> 20:14.160
Yeah, so they're kind of similar. So scalar quantization is what happens when you try to look at each

20:15.520 --> 20:21.520
value in your network or in general in any data that you have and you're going to look at every

20:21.520 --> 20:28.400
number that you have and you're trying to map this real flooring point in practice values

20:28.400 --> 20:37.120
to a discrete subset. And so it's kind of similar to what we're doing in vector quantization.

20:37.120 --> 20:43.360
Just in scalar quantization, you're going to have one code for each scalar.

20:44.080 --> 20:51.040
In vector quantization, you're going to have one code for each vector and of course the question

20:51.040 --> 20:58.880
is how you define a vector, what is going to be grouped together and so on. So theoretically, scalar

20:58.880 --> 21:04.800
quantization is going to be limited in what compression ratios it can achieve because let's say that

21:04.800 --> 21:11.440
you're representing your numbers with 32-bit floating points. Even in the more extreme case,

21:11.440 --> 21:17.760
you're going to reduce that to a binary value, one or zero. So that's going to give you a 32-X

21:17.760 --> 21:24.240
compression rate. It's pretty extreme, though. Which is pretty good, right? It's pretty good.

21:24.240 --> 21:28.880
But when you have a network with trillions of parameters, that might still mean that you need a

21:28.880 --> 21:34.880
machine with 200 gives of RAM or something like that. So now you're saying if you represent

21:34.880 --> 21:44.080
a vector of eight quantities with a single binary, then you've multiplied that out.

21:44.080 --> 21:52.240
Yes, yes. Or if you take like eight values, 16 values, and so on, the bigger the vector,

21:54.080 --> 21:59.920
the more sort of bang for your buck you get. Yeah, exactly. Got it. Okay.

22:01.600 --> 22:10.880
And so then that is, you mentioned that there are only a handful of papers that have explored

22:10.880 --> 22:20.880
vector quantization. Any sense for why that is? Is it just hard? Did it not seem promising?

22:22.800 --> 22:26.960
Did other more promising ideas jump to the fore?

22:28.720 --> 22:34.800
Yeah, it's a good question, actually. I am not entirely sure why that is. I think

22:34.800 --> 22:43.120
a scalar quantization is very intuitive. One thing that might have happened is that

22:44.000 --> 22:52.240
when you have a scalar quantized network in particular, the way how you can use that to

22:52.240 --> 22:58.640
accelerate inference, for example, is more straightforward with current hardware.

22:58.640 --> 23:07.120
Whereas if you start mapping multiple values to a single code, then how you would accelerate that

23:07.120 --> 23:14.720
is not necessarily obvious. And so I think that might be. And so if you don't really have the

23:15.600 --> 23:21.200
the memory limitation being really the main constraint, which if your network has

23:22.400 --> 23:27.440
50 million parameters, 100 million parameters, you can probably get away with scalar quantization

23:27.440 --> 23:33.120
and still run that in, you know, decent desktops or fairly high end phones, for example.

23:34.720 --> 23:39.600
So it might be just now that we're seeing these networks, it's really, really huge

23:39.600 --> 23:42.400
that there's not going to be a way around it.

23:43.120 --> 23:53.520
Right. Right. And so kind of going back to the method that we discussed in the context of

23:53.520 --> 23:59.120
database, I forget you said product. Product quantization product quantization. Yeah. So the

24:00.960 --> 24:08.480
is a fair simplification to say that in scalar and vector quantization, you're only looking at

24:08.480 --> 24:13.680
the specific quantities that you're quantizing. Whereas with product quantization, now you're

24:13.680 --> 24:26.000
looking at the data that you're going to be quantizing it kind of at large. I think that is kind

24:26.000 --> 24:31.760
of fair to say. So the thing with vector quantization is that then you have to design, then you have

24:31.760 --> 24:38.560
to define, you really have to think about what your definition of a vector is going to be.

24:38.560 --> 24:43.840
Mm-hmm. Because for example, before we were saying we have this database that has a hundred

24:43.840 --> 24:50.800
dimensions and we were saying, let's just split it into four groups. But the performance of vector

24:50.800 --> 24:55.760
and sorry, and if you're doing something like scalar quantization, you can take any permutation

24:55.760 --> 25:00.400
of your values. You're going to get the same compression rates. It doesn't really matter for

25:00.400 --> 25:05.040
terms of compression in which order you look at your data. But if you're doing something like

25:05.040 --> 25:13.120
vector quantization, you might say, hey, maybe if I group the first five dimensions and then

25:13.120 --> 25:19.680
dimensions from 55 to 60 and then the last ten dimensions and I make that a single subgroup,

25:20.320 --> 25:25.040
that's going to be much easier to compress than to just take whatever dimensions happen to be

25:25.040 --> 25:37.040
together. Mm-hmm. Yeah, I think that what I was maybe trying to get at is when you're typically

25:37.040 --> 25:44.880
applying vector quantization, you're coming up with this map. Are you also considering the entirety

25:44.880 --> 25:51.920
of your data and using that to create your quantization scheme or is that unique to product quantization?

25:51.920 --> 25:57.280
It sounds like that that may not be. Oh, vector versus product. So I think,

25:58.320 --> 26:04.720
no, so in both cases you have to quantize everything. The gain that you get with product is that

26:04.720 --> 26:10.320
you have this freedom of having subgroups. Okay. Whereas if you're just doing like say more

26:11.120 --> 26:18.240
just taking your whole data set as each entry as a full vector, you wouldn't really have that.

26:18.240 --> 26:26.080
Okay. And so how do you characterize the advantages that you get with product quantization?

26:27.920 --> 26:34.160
So the nice thing is that because you have multiple subgroups in example that I was giving say for,

26:35.440 --> 26:42.480
you have, you can think of it as you're expressing for example, you're expressing your vector as

26:42.480 --> 26:50.960
four numbers. And those four numbers can take each 256 values. So in a sense, you can think of having

26:50.960 --> 27:00.160
a combinatorial sense of clusters to compress your data because you're going to have 256 times 256

27:00.160 --> 27:08.080
times 256 times 256 times four times. Whereas if you, so that allows you to have an implicit code

27:08.080 --> 27:13.840
book that is very, very large, which is good for compression. Whereas if you were looking at the

27:13.840 --> 27:20.960
data as a single vector, then having a code book of that size might not be even feasible for example.

27:22.320 --> 27:27.520
Okay. And then practically, how does this play out when you're,

27:29.680 --> 27:33.040
when you're using this to compress a neural network?

27:33.040 --> 27:38.000
You want to know how to efficiency and accuracy and all those kinds of things, right?

27:38.560 --> 27:44.160
Let's get into it. So the question of how we do it, I think that's a nice point.

27:45.040 --> 27:48.240
So like I was saying, the main question is, what are you going to group together?

27:48.240 --> 27:54.720
So let's say you have a matrix in your neural net that is of size a thousand by a thousand.

27:56.000 --> 28:01.760
You can choose to split this into subsets of four numbers each and then treat those of vectors,

28:01.760 --> 28:07.200
run k-means, and then get a compression that, get a representation that is much more compressed.

28:09.760 --> 28:15.840
So the first thing that we do is, that's why we call the paper permute, permute, then quantize

28:15.840 --> 28:23.280
and find you. So the permute step is, it's a nice, it's a nice observation. So neural nets have

28:23.280 --> 28:31.760
this nice property that if you have two adjacent layers, let's say a linear layer, some non-linear

28:31.760 --> 28:38.320
activation and another linear layer, some non-linear activation, you can express the same

28:39.280 --> 28:47.680
function by having different permutations of the weights. So let's say that you choose one

28:47.680 --> 28:54.160
permutation of this 1024 elements and on the first layer you're going to permute the rows according

28:54.160 --> 28:59.920
to that permutation and then in the second layer you're going to permute the columns. If you pass

28:59.920 --> 29:04.960
data through it, the first thing you're going to notice after the first layer is that your output

29:04.960 --> 29:11.520
is going to be permuted and then when you process it through the second layer because you

29:11.520 --> 29:18.720
permuted the rows, that is going to undo the permutation. The same thing is true for convolutional layers.

29:18.720 --> 29:26.240
So convolutional layers are parameterized by, say, a one-dimensional array of a lot of

29:26.240 --> 29:31.600
little three-dimensional arrays, which are the convolutional filters. The only thing that determines

29:31.600 --> 29:36.240
the order of your output, the order of the channels of the output of the first layer is in which

29:36.240 --> 29:43.920
order you apply these filters. So that is one dimension you can permute and just alter the order

29:43.920 --> 29:50.720
of the output. And then to the layer that's downstream you can just say, oh, you should switch

29:50.720 --> 29:57.600
the filters. You should switch the channels of your filters and then that will undo the permutation,

29:57.600 --> 30:02.960
right? So you're saying that you can permute, but you haven't said yet why you want to permute?

30:02.960 --> 30:09.520
Exactly, exactly, exactly. And that's what we've been hinting at, right? Because the key question

30:09.520 --> 30:15.920
for vector quantization to work and for product quantization to work is how am I going to form

30:15.920 --> 30:22.160
these little subgroups? If I just go and take whatever four numbers happen to be together

30:22.160 --> 30:27.760
because a stochastic gradient descent took these values there, that might not be very easy to

30:27.760 --> 30:35.040
compress. But if I have this other, this whole permutation, it gives me more degrees of freedom.

30:35.040 --> 30:40.160
I can, I can find, I can search over all the permutations of the data. Well, if you search

30:40.160 --> 30:44.000
over all of them, that's going to take you a while because there's a factorial number of permutations.

30:44.880 --> 30:50.880
But you can find efficient algorithms to say stochastically look at look at a subset of them

30:50.880 --> 30:55.760
and then try to find one permutation that is going to make your data easier to compress.

30:55.760 --> 30:59.680
In this case, the data is going to be the weights of the network itself.

31:01.840 --> 31:05.600
And so then the next question is if you have, if I give you to permutations, how do you know

31:05.600 --> 31:15.840
which one is better? That's a natural question. The thing is that vector quantization was a very

31:15.840 --> 31:23.680
hot topic. I'm talking about the 80s, late 80s, early 90s because there was this big push to

31:23.680 --> 31:29.360
digitize a lot of the infrastructure that we had to go from analog to try to make it more digital.

31:30.400 --> 31:38.240
And so the limitation that we had in doing that was that we had very little bandwidth.

31:40.720 --> 31:46.960
So we could only transmit so much binary data over the SL connections or whatever we had.

31:46.960 --> 31:58.960
So there is a lot of papers, usually older papers, that deal very neatly with satioretical guarantees

31:58.960 --> 32:08.080
of how to get good code books, how to get good compression rates. So one thing people studied

32:08.080 --> 32:14.800
in particular, because back then getting real data was really hard to get in. So there's a lot of

32:14.800 --> 32:20.720
papers on how to impress samples from a Gaussian, samples from a Laplacian, samples from other

32:21.680 --> 32:29.280
canonical say distributions. So for example, for a Gaussian, we know that there are well-known

32:29.280 --> 32:34.800
lower bounds, well-known upper bounds. Actually, upper bounds are harder to find. Lower bounds

32:34.800 --> 32:41.840
are lower bounds. And so if you look at expression for this, you will see that there is some constant

32:41.840 --> 32:47.440
numbers based on how big your code book is, how high-dimensional your data is. And then there is

32:47.440 --> 32:53.840
something, there is a term, a linear term based on the determinant of the covariance of the data

32:53.840 --> 33:00.320
that you have. And intuitively, this makes a lot of sense, because geometrically, the determinant

33:00.320 --> 33:04.880
of the covariance gives you a sense of how spread your data is in higher dimensions.

33:06.160 --> 33:10.880
So given two permutations of the data, you can compute your covariance,

33:10.880 --> 33:16.960
compute your determinant, and choose whichever one has lowest, lowest determinant of covariance.

33:17.680 --> 33:23.520
Got it, got it. And that's how you, that's how you create your mappings for your product quantization.

33:24.240 --> 33:28.080
Yes, that's how you create, that's how you decide exactly which things to compress together.

33:28.560 --> 33:37.920
Okay. Yes. And so, yeah, we're pretty proud of that idea, yeah. We're not the first to notice

33:37.920 --> 33:42.400
that, that this permutation invariance exists, like that has been known since the 90s or so on,

33:43.440 --> 33:47.520
probably since people started doing neural nets, I think it's kind of obvious and retrospect.

33:48.480 --> 33:56.160
And there's papers that try to use this to, to make optimization easier, or for example, there's

33:57.200 --> 34:02.000
there's a guy at Google Brain called Simon Cornbullet. He has some papers where

34:02.000 --> 34:08.960
he says, okay, if you have two networks, you train them same data, but different initializations.

34:09.520 --> 34:14.800
How can you tell if they learn the same features? And then the question that is that the features

34:14.800 --> 34:19.200
could be the same, but they're gonna be permuted differently because initialization is different.

34:19.920 --> 34:24.320
So you have to find some sort of canonical permutation to make these things comparable.

34:24.320 --> 34:28.800
And so they do a similar search to what we were doing with a very different end goal,

34:28.800 --> 34:37.280
what exploit in the same properties. Okay, okay, nice. And so then zooming out and looking broadly at

34:38.480 --> 34:45.200
approaches that folks are taking for neural compression or compressing deep neural nets,

34:45.200 --> 34:48.800
how does this approach compare in the grand scheme of things?

34:49.680 --> 34:55.440
Right, so in the paper we compare against a bunch of mostly scalar quantization baselines,

34:55.440 --> 35:02.880
because it's it's the dominant approach in the literature. Okay, and there was a paper that we

35:02.880 --> 35:11.040
are building upon that was doing vector quantization without say, without this permutation inside

35:11.040 --> 35:16.560
in particular. And when you plot this saying in the in the x-axis, you put the compression ratio,

35:16.560 --> 35:23.440
so 10x, 20x, 30x, and in the y-axis, you want to put how much accurate your network is on whatever

35:23.440 --> 35:30.400
task you have. You see that vector approaches sort of create an envelope around all the scalar

35:30.400 --> 35:36.560
ones. And our method just kind of pushes that I would say by a fairly large margin,

35:36.560 --> 35:40.000
especially for high compression ratios.

35:43.280 --> 35:51.200
In terms of implementation, difficulty or efficiency, did you look at any comparisons

35:51.200 --> 35:55.120
along those dimensions? Yeah, that is that is a question we get very often.

35:56.240 --> 36:05.040
So unfortunately not, so it's it takes a lot of engineering effort to to make these things

36:05.040 --> 36:14.560
actually run fast. We did some preliminary experimentation where we noticed that a lot of these

36:14.560 --> 36:22.560
libraries that people often use to do inference, for example, like Kula, Kudian, and so on, they are

36:24.160 --> 36:30.640
kind of hidden, but ones start poking them. They're very optimized for certain settings,

36:30.640 --> 36:35.280
and which means they're really bad for other settings. So they tend to be really good

36:35.280 --> 36:44.160
for particular batch sizes, for particular sizes of data that people tend to use a lot in practice.

36:45.840 --> 36:53.360
But once you take them out of that range, they tend to perform not so great.

36:54.240 --> 37:00.640
And so yeah, the question of how do we actually make this run fast? I think there's two ways.

37:00.640 --> 37:08.720
Either we find very talented engineers, and they do it for us, but another very promising

37:08.720 --> 37:15.760
approach is I think looking at automatic compilers, and compilers that can actually

37:16.480 --> 37:22.160
analyze the whole graph ahead of time and come up and maybe run some experiments, benchmark

37:22.160 --> 37:27.440
these things ahead of time for you, and then try to find a set of instructions that run nicely,

37:27.440 --> 37:36.400
such that the action implementation will be fast on actual hardware. That's not easy.

37:37.360 --> 37:41.920
There's a bunch of efforts, for example, Apache's TVM, I think, is very promising.

37:45.120 --> 37:51.840
And so the promises that there's potentially a greater engineering effort, but

37:51.840 --> 37:57.440
if you put in that effort, this is a potentially higher performance.

37:58.960 --> 38:06.800
Well, is it higher performance at a given compression level or greater compression at a given

38:06.800 --> 38:13.920
performance level or both? I get some thinking of what I'm trying to articulate is one objective

38:13.920 --> 38:24.640
could be to maximize your compression within some given performance bound. Another way you might

38:24.640 --> 38:35.200
approach the problem is to given a required compression level, what's the best performance you're

38:35.200 --> 38:40.240
going to get? If you can fit the model, if you have a hard limit into the model size that you need

38:40.240 --> 38:46.400
to get to, what's the best performance you can get? It's not clear to me that the same

38:46.960 --> 38:50.400
approach necessarily checks both boxes, and that's what I was trying to get at.

38:52.400 --> 38:57.600
Yeah, so I think in practice, you can definitely set the compression rate ahead of time.

38:58.720 --> 39:06.320
And what we see is that when we do that, then in this compression to performance chart,

39:06.320 --> 39:11.840
you tend to be a little bit, so that this would be a cross-section on the x-axis.

39:12.720 --> 39:18.960
So this method tends to be a little bit higher up. It's harder to say I want this performance,

39:18.960 --> 39:25.760
then give me the smallest model I can get, because yeah, a hair of time, networks are hard to

39:25.760 --> 39:28.880
predict. You have to train them, and then you have to hope for the best.

39:31.280 --> 39:35.600
So what we do is we just compress for a bunch of say code book sizes, and that gives us like a

39:35.600 --> 39:42.320
nice, a nice curve that tells you what the trade-offs are. And in practice, that happens to be an

39:42.320 --> 39:47.760
envelope that is pretty optimal with respect to other approaches.

39:50.560 --> 39:56.800
And so the paper that we've been discussing as far per mute quantize and fine-tune is one of a

39:56.800 --> 40:05.840
couple that you presented at or are presenting at CVPR. There's another deep multitask learning

40:05.840 --> 40:12.160
for joint localization, perception, and prediction. Can you give us a high-level overview of that one?

40:13.600 --> 40:21.280
Yeah, of course. So first of all, I want to say I am first author on the per mute quantize paper,

40:21.280 --> 40:26.640
but it was joint work with a bunch of other people. In particular, there's two interns,

40:27.520 --> 40:34.400
Joshan, Shabra Kamani, and Thingway Liu. They did a bunch of the of the work in this

40:35.360 --> 40:39.920
in this part, and you know, shout out to them and my other colleagues at UberHG.

40:41.520 --> 40:51.120
The joint localization paper is, I am not first author there, that is the work

40:51.120 --> 40:57.040
of John Phillips, who was also an intern with us. He's finishing his undergrad right now. He's

40:58.080 --> 41:02.160
he's one of those interns that are doing their undergrad in there, you know,

41:02.160 --> 41:08.400
already publishing at the venues. Oh wow. Yes, it's more common. It's just getting more and more common.

41:09.920 --> 41:16.400
So that's that's a paper actually like a lot too, because it's asking a question that doesn't get

41:16.400 --> 41:25.760
asked very often. So if you go to there's so localization is given a sensor reading for a bunch of

41:25.760 --> 41:32.080
sensor readings and a map, you want to know what in the map you are. And so this is if you want to

41:32.080 --> 41:37.200
build a robot that goes around your house or you want to build an autonomous driving car or

41:37.200 --> 41:42.400
anytime you want to build an autonomous system, there is always going to be some sort of map,

41:42.400 --> 41:46.240
some sort of map of the world that is going to make it easier for the agent to navigate.

41:47.440 --> 41:52.080
And I guess self-driving cars are just the prime example. Their whole thing is that they have

41:52.080 --> 41:58.880
to navigate and that they have to do it safely. So there are a lot of papers in localization,

41:58.880 --> 42:05.760
this has been a problem that has been going on for decades. And there's a lot of approaches,

42:05.760 --> 42:11.520
there's approaches that assume that the map has certain composition, the map is high definition,

42:11.520 --> 42:17.440
low definition, did you build it online, there's memory trade-offs, computational trade-offs,

42:17.440 --> 42:24.560
it's a huge area. And the way people motivate this work in that, they usually will throw on like

42:24.560 --> 42:30.000
say you want to localize and holiday image that you took three years ago and you don't remember

42:30.000 --> 42:36.720
what it was taken. Or you want to or say or you have an autonomous robot or a satellite car.

42:36.720 --> 42:45.040
But the thing that they're actually optimized for is they have this measure called localization error,

42:45.040 --> 42:53.840
which is basically how far you are from given some better sensors telling you, you know you are.

42:54.720 --> 42:57.920
Which makes a lot of sense, right? It's kind of the task that you're trying to solve.

42:57.920 --> 43:05.680
But when you are building say an autonomous system, an autonomous self-driving car,

43:07.680 --> 43:13.760
the thing you really care about is is this right comfortable and is this right safe?

43:15.920 --> 43:22.640
I don't really care if the car thinks that I am five centimeters ahead of what I'm supposed

43:22.640 --> 43:29.760
of where it actually is or 10 or 20 as long as it's safe, it's really fine. And so that was the

43:29.760 --> 43:38.000
question we asked, like how can we link these localization errors? How is that going to affect

43:38.000 --> 43:44.480
autonomous systems? In particular, how is that going to affect things like perception, prediction

43:44.480 --> 43:50.320
and motion planning? By perception, I mean object detection, so detecting the cars, the pedestrians,

43:50.320 --> 43:57.680
the bikes, the motorcycles around you. If my position is if I get that wrong, am I going to be

43:57.680 --> 44:07.520
unable or less good at detecting cars? And am I going to be worse at planning my way around them

44:07.520 --> 44:15.680
or trying to predict what they're going to do? What we notice is that if there is a small enough

44:15.680 --> 44:25.600
error, so say five, then below 20 centimeters, basically your perception system, your motion

44:25.600 --> 44:35.520
planning system doesn't care. It doesn't seem to matter. And then based on that, we're saying, well,

44:35.520 --> 44:44.080
maybe we can build a localization system that runs much faster, but it still gives us the accuracy

44:44.080 --> 44:51.520
that that is good enough for our autonomous system. So we end up designing a neural net that

44:52.160 --> 44:57.520
has a branch that is dedicated to localization, and that branch can be very, very tiny.

44:58.480 --> 45:03.120
And then we can leverage a big backbone that is doing the perception and the motion,

45:03.120 --> 45:09.440
and sorry, the perception and the prediction, and sort of kind of piggyback on top of those

45:09.440 --> 45:14.560
features, try to reuse that computation that's already there, so that we can have a very, very

45:14.560 --> 45:23.120
tiny network that runs in like two milliseconds, three milliseconds, so that and it's still acceptable.

45:23.120 --> 45:26.080
And so we have experiments to show that this is actually possible.

45:27.360 --> 45:35.120
Nice, nice. In the title of the paper is deep multitask learning, after you observe that you

45:35.120 --> 45:43.440
don't need your localization to be quite as accurate, then you've built a multitask network that

45:43.440 --> 45:48.480
has localization as one part of it, as opposed to localization being a standalone system.

45:50.800 --> 45:54.400
Yeah, so that will be the more traditional approach where like you will have some,

45:55.840 --> 46:00.320
either some part of the stock or its own network, or often this would reflect into having

46:00.320 --> 46:08.960
some group of engineers being a set task force or a group whose task is just to improve localization.

46:10.480 --> 46:16.560
So we're kind of doing this thing, joining, and another thing we know is besides the performance

46:16.560 --> 46:25.520
improvements is that if you are deploying this in the real world, this is we argue easier to

46:25.520 --> 46:31.360
deploy as well, because then your training times are much slower, so you can iterate faster

46:32.960 --> 46:37.600
and you don't have to think of two systems at the same time. You can just have

46:38.160 --> 46:42.960
sort of one system that you, after it's trained, you can push it to the car and just get going.

46:43.680 --> 46:46.080
Got it, got it. Awesome, awesome.

46:46.080 --> 46:55.280
I'm curious what you're, you know, when you look at everything that's happening and autonomous

46:55.280 --> 47:01.600
vehicles, just your personal take, what you're excited about, where you think the field is going.

47:04.880 --> 47:09.040
So I am really excited about why I am working here.

47:09.040 --> 47:21.680
I think, so I don't know if, so our co-founder, Professor Hurtason, she has 20 years of experience

47:21.680 --> 47:28.560
within AI, 10 years of experience in self-driving and four years working in industry and just in this

47:28.560 --> 47:36.880
problem. It's a lot of experience, a lot of lessons learned, and I think that having those

47:36.880 --> 47:43.360
insights and that belief that we can build something from scratch that is going to put AI center

47:43.360 --> 47:48.480
stage is not, we're not just saying that, like that's something that's backed up by a lot of

47:48.480 --> 47:55.280
experience and by seeing what limitations current approaches have.

47:56.640 --> 48:03.280
Awesome, awesome. Well, Hulieta, thanks so much for taking the time to chat with us and share

48:03.280 --> 48:07.920
a little bit about what you're up to and your recent presentations at CVPR.

48:07.920 --> 48:37.840
Thank you so much for having me. Thank you, bye bye.

