Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington to close out 2018 and open the new year we're excited
to present to you our first ever AI rewind series.
In this series I interview friends of the show for their perspectives on the key developments
of 2018 as well as a look ahead at the year to come.
We'll cover a few key categories this year, namely computer vision, natural language
processing, deep learning, machine learning and reinforcement learning.
Of course we realize that there are many more possible categories than these, that there's
a ton of overlap between these topics and that no single interview could hope to cover
everything important in any of these areas.
Nonetheless we're pleased to present these talks and invite you to share your own perspectives
by commenting on the series page at twimbleai.com slash rewind 18.
In this episode of our AI rewind series, we're excited to have Sida Gondu back on the show.
Sida, who is now an autonomous vehicle solution architect at Nvidia, shares her thoughts on
trends in computer vision in 2018 and beyond.
We cover her favorite CV papers of the year in areas such as neural architecture search,
training from simulation, application of computer vision to augmented reality and more,
as well as a bevy of tools and open source projects.
Enjoy.
Alright everyone, I've got Sida Gondu on the line.
Sida is a solution architect working on autonomous vehicles at Nvidia.
Sida, welcome back to this week in machine learning and AI.
Thank you.
It's been a great year so far and thank you for having me once again.
Absolutely.
Absolutely.
So for folks that want to go back to your original podcast on the show, it was twimble talk
number 95.
At the time you are, you were at deep vision, but you've since moved over to Nvidia.
Yeah.
What will you be working on at Nvidia?
So at Nvidia, I'm focusing on a particular part of the entire self-driving stack, which
is called simulation and resimulation, which is the part that does the testing and the
verification of the entire perception stack.
So how do we know that the computer vision, the lidar, the radar networks that we have
trained, how do we know that they actually work well in real life because real life testing
for autonomous vehicles is very hard to do because you can't test out the millions and
millions of possibilities on actual roads with real test drivers inside the vehicles.
So simulation is a develop a friendly way to do that.
And it's also, you know, you can squeeze millions and millions of miles.
You can add tens and thousands of scenarios to simulate and test.
So it's really a nice solution to testing.
And so you're joining us to represent computer vision in our Year in Review and Prediction
series.
So let's jump right in.
So you took the time to kind of prepare your thoughts on some recent, some papers this
year that struck you as particularly noteworthy in the computer vision space.
What do you want to start?
So before I start, I think I want to say that, you know, I'm only human and I'm probably
missing tons of game-changing papers that came out this year and, you know, through this
discussion, we're probably just scratching the surface of this year's CV research.
Now that being said, I'm going to start with a couple of papers that are providing different
solutions to problems rather than improving on existing tasks.
So the very first paper that I really, really liked was called learning transferable architectures
for scalable image recognition.
And this is by Google Brain.
Now this is being projected as the future of deep learning and the main reason behind
that is because it introduces something called neural architecture search or NAS.
So you know, Sam, you've been doing these podcasts definitely more than 95 podcasts by
now.
And I'm sure during your discussions, a lot of people have talked about how designing
a network architecture is a big pain point.
And especially for those people who are just entering the field.
So neural architecture search is really the beacon of light for them.
It's a network that searches for the best model structure instead of you manually designing
the network architecture.
Now internally, the search for this ultimate model is based on a reward function that rewards
the model for performing well on the data set.
It has been validated on and at the same time, it's using the same metrics to validate
and test the ultimate network.
So I really like this paper because the details that such architectures achieve better accuracy
than manually designed models.
And searching also allows the network itself, you know, a more brute force type coverage
over the entire service space.
Now we can only imagine the huge benefits that a good NAS algorithm can give, rather than
hand designing a specific network for the millions and millions of specific applications
that we might want to develop.
So this is actually one of its biggest advantage that it is generalizable.
So a well-designed NAS algorithm is supposed to be flexible enough to find a good network
for any specific task.
And within the paper, they've outlined both detection and classification.
So on ImageNet, they reported a 1.2% improvement in top 1% accuracy.
And this is compared to what I guess we can now call the best human invented architecture.
And the coolest thing is that this architecture has 9 billion fewer flops.
Can you imagine the improvement in speed in real-life production systems that we can get
from this?
I mean, NAS is bringing a whole new definition of real time.
And on C for 10, I think NAS net achieves a 2.4% error rate, which is the new state of
the art.
And on the detection side of things, the features learned by NAS net from ImageNet classification
combined with foster R C and N gives a new state of the art, which has a 4% improvement
on the previous state of the art paper.
So NAS is something really exciting, and I expect a lot of people entering the field to really
go and test it out because it gives them a huge advantage of not wasting hours and hours
trying to think what is the ideal architecture for their specific application.
The paper talk at all about the training time to achieve that kind of performance relative
to the training time for a single static architecture.
So when we talk about a single static architecture, it's not like we fix a single architecture and
we stick to it throughout the experimentation pipeline.
As a deep learning engineer, I often start with one architecture and go back and forth
on every single aspect of that architecture to get to the final architecture that is put
into production.
So I think comparing the times between what NAS will do versus what a single architecture
does isn't really an apples to apples comparison.
And that's kind of what I was getting at, the training time to train a single resident
or something like that on ImageNet while it's decreasing can still be significant for
some types of problems and so I'm imagining that kind of searching through solutions like
that is going to be even more complex.
But you're saying that the comparison isn't really, you know, against, you know, just
training, but also the time that the, you know, that a human would typically spend in coming
up with the architecture.
Absolutely.
Yeah.
So that was the learning transferable architectures paper, what else do you have for us?
Mm-hmm.
Yeah.
So the next one is from Nvidia and it's called training deep networks with synthetic
data bridging the reality gap by domain randomization.
So through this paper, Nvidia tries to solve an age old problem, which is can we pre-train
with synthetic data for tasks that demand either expert knowledge or labels that were difficult
to specify manually or images that were difficult to capture in large quantities with, you
know, a huge variety in many such unanswered questions.
So Nvidia built a plugin tool for Unreal Engine 4 UE4 that simulates images for training
data and uses that to train convolutional neural networks.
Now the biggest plus point is that all the factors spanning the main object to structures,
viewpoints, all these factors that the training data depends on are randomized and automated,
making it so much more easier and useful to develop a data set.
So when we talk about the number, variety, texture of objects, the background, then for
destructors, a huge variation in their number, types, colors, scales.
For the camera, you can change the camera location view point, be it the virtual camera with
respect to the scene or the angle of the camera with respect to the scene or the number
and the location of point lights.
An interesting thing that they reported was that with additional fine tuning and real data,
this network yields better performance than using real data alone.
So this result really opens up the possibility of using inexpensive synthetic data for training
neural networks.
I mean, imagine there are millions and millions of applications that don't have concentrated
data collection efforts or maybe collecting data is just too difficult for certain types
of applications.
So such an example of using synthetic data for training would be a game changer in these
cases.
Now, I remember hearing about papers that showed that synthetic data wasn't all that
effective at training agents that can perform in the real world, but results like this are
changing that is that.
It has 2018 been kind of a key year in our ability to incorporate synthetic data into training
or is this paper building on successes that we've kind of seen in recent years?
I think in 2018, we had a lot of papers coming out on simulation or simulating data or synthetic
data and different papers write out different techniques to how do you develop the data?
How do you, for example, change the surface or the material of an object?
How do you make sure it looks different from the background?
So there are a lot of papers in this synthetic data field coming up and a lot of them
achieve good results.
And I was talking about this one because they've made it so much more easier and useful
because they have a plug-in tool.
You know, they've got so many options to change from, for example, the object, the background,
the distractors, the viewpoints.
It's, you know, if you can imagine a Photoshop kind of a generator that, you know, that can
spit out image frames based on how you're coding it up.
It's really similar to that.
So what's next on your list?
So this is a really fun but extremely amazing piece of research and it comes from University
of Washington that makes for more commonplace AI applications and it's called soccer on
your tabletop.
So they've developed a system that takes, as input, a YouTube video of a soccer game and
the system outputs a dynamic 3D reconstruction of the game that can be viewed interactively
on your tabletop with an augmented reality device.
So you know how people right now watch matches on their mobile devices, tablets, laptops,
television sets and so on.
Imagine watching it on your dining table, your work table, your kitchen counter literally
everywhere or anywhere for that motto.
So this system is really multimodal.
They are combining different types of information, for example, bounding boxes, poses, trajectories,
all extracted from the player to segment them and these 3D segments are projected onto
any plane which becomes the virtual soccer field.
And they've released an example video of this and on YouTube and it's really cool and
I encourage all of you to check it out.
It's called soccer on your tabletop.
Oh, that sounds incredible, I haven't made that I haven't come across that video.
Yeah, you said definitely check it out now.
So the next one, let's talk a little bit about vision and language coupled together.
So before we talk about the papers, I think it's worth mentioning the second edition of
the VQA Challenge dataset, VQA 2.0, which actually was released in 2017 but it's worth talking
about it because it was a much more balanced dataset and it reduces the language prices
over VQA 1.0 and it's double the size of VQA 1.0.
So the very first paper in VQA is learning to count objects in natural images for visual
question answering.
This is by Jan Zang and others.
So this paper focuses on developing a counting solution for VQA and they use mostly attention
for that.
They've proposed a differentiable counting component which explicitly counts the number
of objects based on a hand design architecture using a graphical object proposals and non-maximum
suppression.
So it's basically just applying non-maximum suppression on object proposals and counting
the number of objects that are within the image.
What is non-maximum suppression?
So it's an algorithm for mostly use for detection.
So this method improves the baseline by about 6.6% on the counting questions.
And I don't remember the statistics in VQA 2.0, but in the VQA 1.0, the counting was
a very big problem and it was extremely biased because most questions, for most counting
questions, the answers was usually two.
So boosting a baseline by 5% is huge.
So I really like this paper for that.
And so with this paper, is the idea that this model that they've developed for counting
would be used as kind of like a submodel for a broader VQA system?
Yes.
So I was actually coming on to that.
The next paper, which is bilinear attention networks for visual question answering, they
have integrated the counting module that we just talked about from Zang, and along with
that, they've introduced other techniques to improve their VQA accuracy.
And this paper is called bilinear attention networks for visual question answering.
It's by Jean Joachim from SEAL National University.
They are using the counting module.
They are also using bilinear attention, which is the interaction between the word and the
visual concepts.
They also have a low rank bilinear pooling and they have residual learning with attention
mechanism for incremental inference.
That sounds a lot.
And I think I'm not sure, but I think this was among the top VQA 2.0 winners.
And then one paper that is one model that is right for the right reasons is women also
snowboard overcoming bias in captioning models.
So they've introduced a new equalizer model that encourages equal gender probability when
gender evidence is occluded in a scene and confident predictions when gender evidence
is present.
So they're basically forcing the resulting model to look at the person rather than use
contextual cues to make a gender specific prediction.
And they've also introduced two kinds of losses, which is the appearance confusion loss and
the confident loss.
Both of them are generalizable and they can be added to any description model in order
to mitigate the impacts of any unwanted bias in the description dataset.
And when we talk about their results, this research gives two things.
They get a lower error than existing work when describing images with people that mentions
the gender and it much more closely matches the ground truth ratio of sentences that include
women to sentences, including men.
And the second achievement is when we visualize the results, we see that the model is actually
looking at the people when predicting their gender, which is really important because a lot
of visualization results, you know, they focus on, they don't necessarily focus on the
people when they are predicting the gender.
So this was a really interesting find by this paper, women also snowboard overcoming
bias and captioning models.
That one sounds really interesting.
The next one is weekly supervised photo-enhanced for digital cameras.
So I think Sam, you enjoy photography rate.
I do enjoy photography, especially digital photography.
Yeah, so this one, I hope you find interesting because they've trained a generator adversarial
network to improve the asphetic quality of standard or normal looking photos.
So let's say I take really bad photos and I put it in this generator adversarial networks
and it comes out looking completely professional looking, you know, applying through the thirds,
improving lightning, having enhancements that you get from image editing software.
And there are two cool parts about this.
The first one is that they're using GANs, so you don't need a pair of good looking and
bad looking images.
You only need a set of good looking images and a set of bad looking images.
And the second part is that because it's weekly supervised, the pair of input and visual
enhanced images isn't necessary.
So there are these two really cool things about this paper on GANs.
And if you look at the results, they are, you know, GANs have become so photorealistic
that it's just, you know, it's crazy to imagine that GANs are once putting out what that
was, you know, not as aesthetically pleasing, but now when you look at these images, it's
just amazing.
Yeah, the original images that you see with GANs are like these kind of grotesque approximations
and now the celebrity work that Nvidia did.
I think that was earlier this year or maybe late last year.
And there's one that's been going around even more recently, the photorealistic images.
I mean, it is getting quite incredible what they're able to do.
Absolutely.
So another people that I really like is called, this is a really long name.
So bear with me.
So it's called efficient interactive annotation of segmentation data sets with polygon RNN.
So this is an interactive annotation tool.
Now if you think about labeling of data sets and we talk about segmentation data, we know
that class labeling needs labeling of each and every pixel in the image.
Now this is quite literally forever if you're talking about millions and millions of
images.
We know that deep neural networks work well when they can feast on a large and fully annotated
data set.
So this paper is really the economical bridge between these two worlds because with polygon
RNN, you can set rough polygon points around each object in the image that you want to annotate.
And then the network will automatically generate the segmentation annotation.
And a big advantage is that a method generalizes well.
So it can be used to create quick and easy annotations for segmentation tasks.
So it's the idea here that typically for image segmentation, we want this to be on a pixel
by pixel basis so that we can kind of train the network very accurately.
But what this is doing is allowing someone who's doing segmentation to just do it on a polygon
basis.
And then the network will map that to what will find more the more accurate pixel based segmentation.
Yes, that is the idea.
But it makes the process of annotations much faster because you don't have to go pixel by
pixel, right?
You only have to give a rough polygon.
And then the system will adapt the polygon to the actual, the edges of the object.
Mm-hmm.
Yeah.
And do you have a sense for this how close do you need to get?
We still need to get within a few pixels.
Is it meant to be kind of a fine tuning mechanism or can you kind of very roughly put a polygon
around, say, a person, a picture of a street scene?
You can make it pretty rough.
I mean, in their examples, they've shown a real imagery and those examples are pretty
neat.
Oh, interesting.
Cool.
Yeah.
I've got one more that I really, actually two more that I really like.
Okay.
Okay.
So this one is super slow mo, high quality estimation of multiple intermediate frames for
video interpolation.
And this comes from University of Massachusetts at Amherst.
So you're given two consecutive frames.
And they apply a method of video interpolation that aims at generating intermediate frames
that are both spatially and temporally coherent sequences.
So internally, it's utilizing optical flow and convolutional neural networks between
frames to interpolate video frames input at 30 frames per second.
And it produces crisp looking results to 40 frames per second.
And they've shown examples of like a bullet going through an egg.
And you know, to a human eye, you don't really notice much.
But when you look at it in super slow modes, it's really breathtaking.
I mean, you can see how the cracks on the egg shell propagate.
And that's really cool.
Oh, wow.
Yeah.
Wow.
And they're doing this from optical flow and CNN's.
Right.
Right.
Against slow-mo data, or is this interpolation happening without specific training in that
way?
It's got training.
OK.
So the network kind of learns what the effect of slow-mo is from slow-mo training data and
then can apply that effect to non-slomo video.
Yeah.
OK.
Very interesting.
And then the GAN-based photo-enhanced, you can start to see how some of these techniques
can work their way into our everyday devices.
Absolutely.
And that's actually a really good point because handheld devices now have GPUs in them.
You know, they have separate accelerators that accelerate CNNs.
So you know, I guess even the mobile industry is expecting developers to use these techniques
because they're putting these heavy GPUs inside these devices.
And the salt started, I think, in 2015.
So if you go around benchmarking all these devices, you notice that in 2015, there is
a sudden jump in hardware acceleration.
And that is when these GPUs were introduced into mobile phones.
OK.
So I'm really excited for all the new applications that are about to come.
Yeah.
Same here.
Yeah.
So you have one more paper?
Yes.
Now, this is called, who let the dogs out, modeling dog behavior from visual data, and
this comes again from the University of Washington.
OK.
So I remember when this paper came out, I found it through a YouTube recommendation.
And a dog was walking around with all these sensors, GoPro and Arduino attached and a number
of sensors on the dog's limbs, and they'd linked an archive paper with it.
And I was really fascinated by what is this dog doing on archive paper.
And then I read it and it's very interesting because when we talk about visually intelligent
agents.
We tend to break that into smaller, more approachable subproblems, like classification, detection
and planning.
But this paper conquers it as a one big problem.
So it takes input images and it produces planning actions.
And again, the data collection is from a dog using a GoPro, Arduino, and a number of
sensors on the dog's limbs.
So they've got feature extractors from CNNs, which they used to get image features from
the video frames, of course.
And then all of this is passed to a set of LSTMs along with a sensor data.
And the system learns and predicts dog's actions.
And in the paper, they've exemplified three particular tasks.
One is acting like a dog.
So you have a previously seen sequence of images and you want to predict what will be the
future movement of the dog.
The second task is planning like a dog where you have a sequence of a source and a destination
locations.
And the goal is to find a sequence of actions that take the dog from the initial, the source
location and the destination location.
The third and the final task is learning from a dog.
So can we learn, learn representations for a third task?
For example, if I'm a dog, can I try to understand if the surface in front of me is walkable?
So stuff like that.
And I think this paper is really interesting because it's like an end-to-end solution,
not it's not breaking up different tasks like planning detection or classification.
It's really treating it as one big intelligent agent.
And I haven't seen those examples in a really long time.
So I think that's why this paper stands out so much.
Thinking of applications of this kind of paper and a thing that jumps out at me is when
you think about these Boston Dynamics robots, how they might, this kind of training might
help them get to something that's more intelligent.
So the Boston Dynamics robots are really, you know, they've got four limbs and like an upper
body part and then a sense of network of cameras and so on.
But when I think about this paper, I think that it's not only applicable to like the Boston
Dynamics robots, but generally all robots because planning, action, learning, you know, coupling
with understanding of the outside world, all these are, you know, very essential parts
of the visually intelligent agent.
And you know, it's not, it's not just a single robot that can benefit from it.
I mean, you know, this system could be easily applicable in, in phones, for example, you
know, for helping blind people or for the deaf community in certain cases.
So it's, it's really exciting to see how end-to-end applications are developing and coming
into real life.
Definitely some interesting papers from, from 2018 and, you know, particularly with
your caveat, this is just the few that came to mind as, particularly meaningful contributions
and, you know, related to the, the kind of scope of, of the elements of the field that
are of most interest to you.
Yeah.
And I think I've already said this.
I don't know how many times, but the speed of publications is just immense.
And the good thing is that it's all quality work that's coming out.
So keeping up with all of this is so much more challenging than it used to be before.
One of the things that we wanted to talk about was kind of your perspective on the different
kind of research accomplishments of 2018.
But that's been difficult for you.
You said there's so many interesting papers in 2018.
So here's an idea, you know, we know that there are different stages in the AI pipeline.
Like get a collection, labeling, training, and so on.
And then there are some new entries, like modern reproducibility, phoenix, and biasing.
And so why don't we sort of use these stages as a demarcation?
And in each talk about papers and tools and new research that have come up.
Oh, that sounds great.
And again, I think like you've been doing through all the Tremelai AI talks, we will try
to link all the tools and use cases so people can read about them a little more.
You know, if you consider data as the new oil, then labeled data really becomes the new
gold.
And that brings us to a first category, which is data collection and labeling.
So AWS released something called AWS Deep Racer, which is a small scale race car that
gives you an interesting way to get started with reinforcement learning.
So as we all know, reinforcement learning takes a very different approach to training models
than other machine learning techniques, because it learns complex behaviors without requiring
any labeled training data.
And at the same time, you can make short-term decisions while optimizing for a longer-term
goal. And especially with AWS Deep Racer, you can get hands-on reinforcement learning.
You can do a lot of experimentation with cloud-based 3D-racing simulators.
You can raise your friends all while tapping your toes into autonomous driving.
I think AWS Deep Racer really comes out as a top-new tool that Amazon has released this
year.
Yeah, I was at re-invent last week when they announced it.
And they also announced a new extension to their SageMaker tool that's focused on reinforcement
learning.
And I'm really glad you brought up RL in this context.
I think when I think about the work that OpenAI and Deep Mind are doing around reinforcement
learning, for those organizations, I think, a lot of their motivation is that we kind
of, as humans kind of learn in a reinforcement learning way, we don't have labeled training
data.
We kind of explore our worlds and learn.
And so they are aggressively pursuing reinforcement learning as a stepping stone to AGI.
And I think for most of us, the practical implications of RL or that kind of gets us,
it sidesteps this need that we usually have for label training data.
And in the near future, that's going to be its big contribution.
So I'm particularly excited that AWS, which has so much weight in the space, is starting
to kind of shine the light on reinforcement learning and get people, you know, starting
down the path to experimenting with it.
Yeah.
And now that you've mentioned, you know, going into the future, how will we have labels
available?
There's work done by the Microsoft machine learning team and the Apache Spark community.
And they actually worked on creating a deep distributed object detector that works without
any human labels or human generated labels.
Now, you know, if anyone who has ever trained their own detector knows how incredibly
painful it is to get the bounding boxes and the labels right, but now thanks to a technology
called Lime, we can work without any data.
And Lime is local interpretable model agnostic explanations.
And this was built by Marco Ribeiro and a team from University of Washington.
And it helps in understanding the classification of any image classifier.
So it's really telling us where the classifier is looking.
And then here's the coolest thing ever.
It makes no assumptions about the kind of model.
So you can use it for your own secret model, random published models that you downloaded
from the internet or even a very patient human classifier, which means that it has extremely
wide applicability, not just across models, but also across domains.
So Lime was originally published a few years ago, I think, but you're saying that folks
are now using it to beyond its initial intent of model explainability, but to allow you
to create classifiers without label training data.
Yeah.
So I was actually coming to that.
So Lime has a huge drawback that it is extremely computationally intensive.
So what the Microsoft machine learning and the Apache Spark community did was they
made this available in a distributed implementation and SparkML.
And that's how you can make it more real time.
And because of that, you can create image classifiers for classification or detection from
bounding boxes.
And they experimented with this on a real life use case for the conservation of snow leopards
in Krigaston.
So that was pretty interesting.
And so can you give me a sense for how the use case works or what the flow is in working
with this tool?
Yeah.
So it's basically you have all this data, which is not labeled.
So you have plain basic images, which can be, you know, without the actual object, which
is a snow leopard in this case, or it can include the snow leopard.
And you run Lime over all these images.
And then you ask simple questions like, what is the most common object that you're seeing
over all the data?
Where is the object located in most of these images?
And that's how the system really learns what is most common, what it should be looking
for.
And once you have these questions answered, you can use that sort of like a label.
And without any human actually working on labeling.
And then you can generate the entire system of object detection with that.
Oh, interesting.
Yeah.
And then again, all of this is real time.
It's distributed.
So it's pretty quick.
And if I remember correctly, they had a statistic in their published work, which was that
it takes, if it takes one hour for you to evaluate your model on a particular data set,
then it would take 50 days worth of computation to convert these predictions to interpretations.
And now that doesn't sound attractive at all.
And that is where the improvement is coming by making it a more distributed implementation
and packaging it in SparkML.
Okay.
Very cool.
Yeah.
And then a Christmas miracle is, when we talk about labeling, is two companies, digital
divide data by Jeremy Hawkinsstein and Samir Reyna.
And I met it.
Now both these are nonprofit companies that provide labeling solutions.
But you know, when I'm talking about it right now, it probably sounds like the million
other labeling companies out there.
But here is really where the heart is.
They train and employ people who have no other sources of employment.
And so the most part don't have access to higher education.
These people learn by doing and they earn enough money to support themselves and eventually
hone enough skills to go for certifications or undergraduate cases.
And I think digital data divide mentioned that they have uplifted more than 3,000 individuals,
their families and communities all around the world.
That's pretty interesting.
Oh yeah.
And we talk about labeling in terms of how it's really impacting the world.
It's interesting.
We talk a lot about AI for social good.
And it's usually the, you know, we're thinking about the AI itself.
But now this is an example of how even the process of creating AI can be beneficial to
communities.
Yeah.
I mean, imagine what these people can achieve in the future.
It's just limitless.
And then there is another tool called Prodigy, which is helping solve a really big issue.
Now we all know that the AI workflow doesn't really follow what fault development strategy.
It's more like a chicken and egg problem because you can't really start experimentation
until you have at least the first batch of annotations.
And the annotation team can start until they receive the annotation manuals and to produce
the annotation manuals, you need to know what statistical models you need based on the
features you're trying to build.
So all in all machine learning is an inherently uncertain technology, whereas the waterfall
annotation process really relies on accurate and upfront planning.
So what Prodigy does, honestly, living up to its name, it solves this problem by letting
data scientists conduct their own annotations for rapid prototyping.
It puts the model in the loop so it can actively participate in the training process and learns
as you go in an active learning kind of a setup.
It has AB evaluations and a whole suite of tools to help you prototype much faster, bringing
in the much needed increased agility.
And what's really awesome about Prodigy is that it's written by the authors of Spacey,
which is a really cool NLP library.
I've heard quite a bit about Spacey.
And so what's the user experience of working with Prodigy?
Prodigy is extremely efficient.
It's got, you know, you can use it via command line.
It's also got a beautiful UI support.
It helps in both annotation and training, and I've tested it out for detection problems.
And it's really easy to, you know, get like 10, 20 images to find what object you want
to detect, give a few bounding boxes and see how the model predicts the bounding boxes
on the next couple of set of images and then use those as a bigger data set for training.
Huh, interesting.
That sounds like it would take a lot of the early effort out of kind of experimentation.
I remember going through the fast.ai course and, you know, this whole process of like searching
for images on, you know, like Google images and, you know, labeling them and putting them
in the right, you know, the right folders and all this stuff can be pretty time consuming.
Yeah, that's true.
And what's even better is because it's got an active learning setup, it can figure out
which of the following images is something that, you know, if it's included in the training
data set, it will perform better rather than, you know, adding images that don't really
improve the accuracy.
On that note, AWS announced something in the same vein at re-invent SageMaker Ground
Truth, which uses active learning in the same way.
It basically is a labeling pipeline that incorporates human and loop laborers, which can
be mechanical Turk or some of their partners, maybe some of the companies that you previously
mentioned are included in that.
They also use active learning to try to make the labeling process more efficient by only
labeling the images that will, actually, I don't even recall if it's specific to vision
or if it's broader, but only labeling the kind of the data that will add the most value.
That's true, it's definitely available in AWS now.
It's also been, if I'm not wrong, available in Google Cloud from their AutoML team.
And internally, they use Crowdflower or what is now known as Figure 8.
And that also employs a method of active learning, if I remember correctly, and hopefully
I am.
But yeah, so active learning for labeling has been a top priority, I think, for companies
this year.
At least that's what it seems like to an onlooker.
I agree.
Yeah.
Yeah, and then a really interesting trend that's up in coming is gamification of labeling,
which keeps the interest going in human and the loop labeling.
So what you do is, you know, you put these certain objects that you're looking to detect
or classify at really weird places that you wouldn't expect.
And we know that convolutional neural networks, they depend not only on the object, but also
on the usual surroundings of that object.
So it kind of, you know, focuses the network to try to concentrate on the object itself
rather than its surroundings, because you are getting so much variation in the surroundings.
Michael Arthur, I know that actually talks about this in his TEDx talk, and it's really
interesting to see how companies are incorporating gamification in their annotation cycles.
And so is this gamification for human laborers or gamification somehow trying to gain the
neural networks themselves?
So it's kind of both, because you are looking for these certain images that, you know,
you know, will not work as well, because the object isn't in their natural environment.
So that's one.
And then second, it also keeps the interest of the human laborers really ongoing in the
system, because, you know, they are so familiar with the system, they know how to break
it, what are the kind of images that will really break the system, because they are the
ones who've gone through the entire data distribution that the model really depends on.
So it's a bit of both.
It's interesting, it makes me think of the humans and the network in some kind of adversarial
relationship, you know, separated by distance and time.
Yeah, I mean, I actually think it did the same way.
Interesting.
I think we should move on to your next category to make sure we can get everything in.
Okay.
Awesome.
And the category would be training and training, I think there have been a lot of frameworks
which have come out, which help to do hyperparameterization, which help in, you know, maintaining
like a GitHub equivalent of the entire training cycle.
Okay.
And then moving on to an open source experimentation, optimization, library, which is called hyper
OPT.
It's a distributed asynchronous hyperparameterization, optimization, library and Python.
And it does both serial and parallel optimizations over certain spaces, which can be real value,
discrete and conditional dimensions.
It hyperass is actually a very simple, convenient wrapper around hyper OPT, foster prototyping with
Keras models.
And I think hyperass is something that's, that I feel, you know, it can really be a game
changer because a lot of people when they start in deep learning, they don't, you know,
they don't really learn about how do you actually tune the hyperparameters?
What is the initial value at which you want to start?
But with hyperass, you know, you can define an entire hyperparameter range and then internally
it will learn which is the best value for each particular parameter.
And this has really the only difference that, or the only additional thing that you would
have to do apart from just defining your model as you already do in Keras.
And then you can also run multiple models, training, testing in parallel while you're
using MongoDB as a backend.
So hyperass has become really cool, I think, for companies, you know, that are really
looking for a small-scale solution.
Interesting.
Yet one of the other things that I've repeatedly seen as a best practice in this experiment
management category is automating hyperparameter optimization.
I think a lot of people think of that, or historically, a thought of hyperparameter optimization
as this, you know, the cherry on the cake or the icing on the cake or something like the
thing that you do last to kind of eke, you know, a few more percentage points, or, you
know, a few more points of accuracy or performance out of your models.
But all of the folks that I'm seeing doing this bake it very deeply into their experimentation
pipelines so that you're really automating this optimization throughout the, you know,
the whole experimentation cycle as opposed to kind of just at the very end.
And if you parameterize your models correctly, the hyperparameter optimization can help you
kind of define and evolve your architectures as well.
So another product out in this base is Sig Opt, who is a sponsor of this Agile Machine
Learning Platforms eBook that I referred to earlier.
And they've also been, they've presented at the, the Twoma Online Meetup, I think that
might have been last year, about some of the things you're doing, but I definitely expect
to see a lot more folks working with the, with hyperparameter optimization and kind of
building it deeply into their, their pipelines.
Yeah.
And another thing, interesting to note here is that, you know, 2018 has been a year of
neural architecture search and combining that with Hyperass is just, you know, taking
training and optimization to a whole new level, because you're not only optimizing the
parameters, you're also optimizing the entire architecture, which is, I mean, architecture
definition is always something that, you know, it's, it's really difficult to get right.
And you never know, is this exactly right?
Or, you know, if I remove some layers, will it still be the same?
So that's, you know, that's a problem that I don't know if there's any solution for
it, but neural architecture search is definitely amazing.
I mean, it's absolutely stunning how they've been able to achieve it.
And it's results are amazing.
Yeah.
There was a period of time on the podcast where I would, I would ask every guest in this
space, like, how do you come up with architectures?
Where do they come from?
I probably asked you this.
And the conclusion that I came to, you know, a year ago, a couple of years ago, is that,
you know, it's a little bit of black magic, like, you know, people just try a bunch of
things and see what works.
And I think what's interesting about what we're seeing now with the neural architecture
search is that, you know, we're bringing a little bit more of science and methodology
back to it.
Yeah.
Yeah.
It's, it's a little more of, you know, the software engineering techniques that we've
all been learning and incorporating those into the AI development cycle.
Right.
Right.
Yeah.
And I think another tool that's worth mentioning is Rapids from Nvidia.
So, you know, we talk about how training has been accelerated by GPUs, how inference,
for inference, you have a different kind of GPUs or different kind of hardware on which
you want to do inference.
But Nvidia's Rapids is really a suite of software libraries that allow the execution of the
entire end-to-end data science pipeline entirely on GPUs.
So it can be data processing, training, testing, loading, anything.
Everything is inbuilt onto GPUs and of course, internally it's using CUDA primitives for
low-level compute optimization and it exposes the GPU parallelism that we all love.
And it's got really high bandwidth memory speed.
It exposes very easily usable Python interfaces.
And because it's been focusing on a lot on data preparation, which I know is a really,
you know, I mean, I've been struggling with data preparation.
How do you load such heavy data when you know that this data said won't fit into memory?
So you know, these kind of problems that people face every day, Nvidia Rapids has actually
a solution to it because it's introduced the data frame API that integrates with existing
machine learning algorithms for the complete end-to-end pipeline acceleration.
And you know, you forego the serialization costs which you would have to endure otherwise.
I came across the Rapids announcement, which was relatively recently, but never quite
had a chance to dig into it and see what they're doing, I think.
I don't know if it was Rapids or something else that one of the big points about it was
that, you know, we kind of think about GPUs as, you know, having primary, you know, being
primarily useful for neural networks, but they went ahead and created GPU-accelerated versions
of a bunch of traditional machine learning operations like, yeah, okay, yeah, yeah.
And then, you know, it's also available for multi-node, multi-GPU deployment.
So, you know, if your work is scaling up, Rapids is the best, you know, sort of software
libraries that you can use to scale up down as your requirement changes.
You know, when I think about when I started off learning about machine learning and day
planning, we, I think we didn't have anything like this.
And, you know, today I feel like, you know, people have so many sources to learn from.
And, you know, be it fast.ai, Kaggle or deep learning.ai or even Siraj Ravel School
of AI.
There's just so much to learn and so much to do that I feel 2019 is going to be a year
of, you know, every single day, there's going to be something new because it seems like
that sometimes doesn't it?
Yeah, I mean, sometimes it feels like, you know, we're on an episode of, um, um,
Silicon Valley because every single day something is coming up and, you know, one day it's the
state of the art in the next day, there's another state of the art.
And then we have testing and deployment.
And I honestly feel that, you know, as an AI, as a member of the AI community, testing
and deployment have become really, um, like even though there's a lot of focus on training,
and deployment have become even more important these days because, you know, that's where
things are actually in production.
So for example, if you talk about Onyx, Onyx came a couple years ago when Facebook and,
um, Microsoft said, you know, why do you want to limit yourself to only one framework?
And we want to promote model interoperability.
So Onyx came through and now if you look at the benchmarks for Onyx.js, you know, TensorFlow.js
and Keras.js are not even understanding when it comes to how ridiculously fast Onyx.js
is.
And, you know, all this comes through years of research and experimentation when we realize
that, you know, we want to build smaller models when the model size is smaller.
You can have smaller image sizes.
You can use the inherent hardware acceleration in JavaScript and, you know, literally within
the snap of a finger, you can have an entire batch of images, tested and predictions made
on.
And now the way to actually talking about browsers, I have to mention the amazing work
by Osramos, who is using OpenPose to track eyes to help navigate through a browser.
And of course, there's, there is a limit to how much is possible.
But imagine the benefits that people with impairments can read from this.
And, um, you know, the inspiration behind Osramos working on this is also really, um, encouraging.
And, you know, he was a veteran who sadly lost his home and, um, you know, he was seeing
people around him who weren't really capable, um, physically to, you know, use their systems
or laptops or, um, desktops.
So he decided to use an eye tracking software to actually help people navigate browsers
and so on.
I believe he's on patron and, you know, he's doing a lot of cool and interesting work.
Uh, so maybe switching gears a little bit.
Uh, I'm curious what you foresee for 2019 in computer vision.
Um, a lot of people are expecting GANs, you know, to really be in, um, videos, for example.
I mean, this is more evolutionary rather than revolutionary because when you look at the
current state of the art, GANs have achieved almost photorealistic results and it's just
amazing.
And I can't wait to see what's next with GANs and hopefully it's going to be something
in videos.
Mm hmm.
And then, um, in robotic perception, because we know robots are really good at doing only
one thing right now, but when you talk about general understanding, they don't really
work as well.
So, you know, this is their visual understanding, scene understanding, sort of like a multimodal
input comes in and it would be really interesting to see how, um, perception systems or, you
know, how the, uh, human AI interactivity, um, takes off.
And then I think utilizing AI in other fields, for example, now so it's fronted development
lab.
It's amazing here this year, um, I'm looking forward to see what happens next year in
their work in astrobiology, detecting meteors, asteroids and so on.
And then in healthcare, AI is really useful and in particular, computer vision is really
useful for both, you know, learning about healthcare and for patients as well.
So when you're learning about healthcare, you know, for example, you can learn or you can
visualize how the heart is pumping the blood, you know, you can visualize all the different
valves in the heart and see how the pulmonary, um, veins in the pulmonary arteries are sending
blood to the lungs and getting it back to the heart.
You know, that's, that's really something that is absolutely revolutionary and, you know,
imagine how the entire education system would change with just, um, you know, these simple
applications.
Right.
And then, oh, I have to mention this, um, most of my friends have actually stopped buying
cars in our instead of leasing and they're waiting for 2021 to come by to experience self-driving
cars.
I imagine, you know, you're calling a lift or an Uber or taxi and it's self-driving.
I mean, that would be so cool.
Yeah, it's, uh, it's, it's funny.
I was having a conversation with a friend about kind of this idea that, you know, we may
be the last generation for whom, you know, buying a car is a real thing and increasingly,
you know, we're kind of entering this domain of, you know, more transportation as a service,
whether it's, you know, summoning a vehicle from someone else's fleet or, um, you know,
leasing a self-driving vehicle, you know, you know, maybe one per family and you've, you've,
the vehicles kind of always doing something for someone, uh, as opposed to, you know, most
cars sitting around, you know, waiting for you to finish work or finish whatever you're
doing.
Yeah.
I mean, it's really interesting, you know, I mean, no one really knows what the future is
going to be like, but it's really interesting to, you know, um, think about it and I think
it becomes even more exciting when you know you're actually working on it and, you know,
you see the perception that different people have regarding it and you realize, um, oh,
you know, we can do this to improve the system or, you know, this feature would be a really
cool thing that people might even love.
Uh-huh.
Yeah.
So, um, another thing I think that would be really exciting is development around zero
short learning because we all realize that academic data sets are really good, but real
life deployment is very, very difficult and some sort of practical application would be
really interesting around zero short learning.
And, um, a couple more things around reproducibility and finis and biasing.
Um, so the number one thing I think is going to take off and it's really the need of
the R is model cards, which is a revolutionary piece of research that comes from Margaret
Mitchell and a team at Google.
So model cards are like short book reports for training machine learning models that
provide benchmarked evaluations across different kind of conditions like cultural demographic
or different kind of intersectional groups, which are all relevant to the intended application
domain.
So, they also disclose the context in which the models are intended to be used, details
of how the performance of the model is evaluated and other relevant information.
So you know, in two years when, you know, people will be freely communicating through models,
it will seem insane that it's not standard to report how well machine learning models
work when they're actually made available for general public use.
And I can see model cards really taking off in this aspect.
It's interesting, over the past year or so, uh, many of the large AI, many of the research
arms of the large, uh, the companies that are, you know, biggest in AI I'm thinking specifically
about, you know, Microsoft research with the data, data sheets for data sets work that
Timnick, Timnick Gebruh worked on, IBM research did an extension of that concept and it's,
which sounds very similar to the model cards work you're describing, but none of these
companies are publishing any of these, you know, data sheets or model cards for their
commercial projects yet.
That's definitely something I hope to see them do kind of putting their money where their
mouth is so to speak in, you know, 2019 or 2020.
Yeah, and especially given the fact that, you know, we have these, like, a model exchange
service that has really come up this year, for example, TensorFlow has TensorFlow Hub.
So you have, you know, some piece of reproducible code, sorry, reusable code that, you know,
you use in most applications, for example, if I want to write code to load a large piece
of data and I don't know how to do it, I would look it up, find something on GitHub or
Stack Overflow and reuse that code with the appropriate references.
And TensorFlow Hub really makes this much more easier because they've set up a library
that has these small snippets of reusable parts with machine learning code.
It's published, you know, it's easy to discover things that, you know, are efficient and,
you know, just really use the best out that's available out there rather than, you know,
spending hours of your time actually trying to figure it out yourself.
And then similar to this is Model Depot, which is a place where you can find and share
optimized, pre-trained machine learning models that are perfect for your development needs,
similar to what Model Zoo was before.
Okay. Yeah. Interesting. So we're running low on time, any final thoughts or predictions
before we close out? I guess, you know, I'm looking forward to the day
when we will thank AI for taking away our work of holism and then let us get on with the
job of being human. I mean, that would be pretty cool. Yeah. Yes. You know, we talked a lot
about different tools, different research projects that have come up in different parts
of the AI pipeline, how these are applicable in real life examples.
So I mean, it's been a pretty eventful year and I can't wait for 2019 and, you know,
be the downpour of publications. Come on one by one.
All right, Cedar. Well, thanks so much for once again, kind of spending the time to kind
of go through this stuff with us. Thank you.
All right, everyone. That's our show for today for more information on CIDA or any of the topics
covered in this episode, visit twimlai.com slash talk slash 218. You can also follow along with
our AI rewind 2018 series at twimlai.com slash rewind 18. As always, thanks so much for listening
and catch you next time. Happy holidays.
