Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. I'm recording this intro from San Jose, California, where
I'm attending the NVIDIA GTC conference. NVIDIA CEO Jensen Huang made a bunch of interesting
announcements at his keynote this morning, which I live tweeted from my Twitter account
at Sam Charrington. I'll also be writing about my thoughts from GTC for an upcoming addition
of my newsletter, which I'd encourage you to subscribe to by visiting twimbleai.com
slash newsletter. In this episode, I'm joined by Lin Chapme, PhD student in the Stanford
Computational Vision and Geometry Lab, to discuss her paper SIG Cloud, semantic segmentation
of 3D point clouds. SIG Cloud is an end-to-end framework that performs 3D point level segmentation,
combining the advantages of neural networks, trial linear interpolation, and fully connected
conditional random fields. In our conversation, Lin and I cover the ins and outs of semantic
segmentation, starting from the center data that we're trying to segment to 2D and 3D representations
of that data, and how we go about automatically identifying classes. Along the way, we dig
into some of the details, including how she obtained a more fine-grained labeling of points,
and the transition from point clouds to voxels.
Before we jump into the show, a few quick questions for you. Are you an IT technology or business
leader who needs to get smart on the broad spectrum of machine learning and AI opportunities
in the enterprise? Or perhaps someone in your organization could benefit from a level
up in this area? Or maybe you would benefit from them leveling up? If this sounds like
you or someone you know, you'll probably be interested in my upcoming AI Summit event.
Think of the event as a two-day no-fluff technical MBA in machine learning and AI. You'll
leave with a clear understanding of how machine learning and deep learning work, with no
math required, how to identify machine learning and deep learning opportunities within your
organization, how to understand and take advantage of technologies like computer vision and
natural language processing, how to manage and label data to take advantage of ML&AI,
and how to build an AI first culture and operationalize AI in your business. You'll have an informed
perspective on what's going on across the ML&AI landscape, and you'll be able to engage
confidently in discussions about machine learning and AI with your colleagues, customers,
and partners. I am super excited about this event and the speakers will get a chance
to learn from. For more information, visit twimlai.com slash AI Summit, and feel free to contact
me with your questions. And now on to the show.
All right, everyone. I am here at NIPS and I am with Lynn Chapme, who is a PhD student
at Stanford in the Vision and Learning Lab. Lynn, welcome to the podcast.
Thank you. How did you get involved in machine learning and artificial intelligence?
So I have been passionate about science and mathematics from a very young age, and when
I got to college, I started studying electrical engineering and computer science. I got involved
with...
And where was that?
That was at MIT.
Okay.
So I participated in the Super-UR program there, which is an undergraduate research program.
And as part of that program, I got to work on designing, building, and programming wireless
health monitor that essentially keeps track of cardiovascular signals. It connects to the
ear and connects that data and sends it to a smartphone. And what was that program called?
Super-UR.
Super-UR?
Yes. That was super on the Graduate Research Opportunity program.
Oh, okay.
That's what I stands for. Yes. So it was a pretty exciting project, and I found out that
I was really interested in both the hardware and the software side.
Okay.
As a result, I continued my study. Mostly on the hardware side, I took a bunch of circuit
design courses, and continued doing more research. Eventually, I continued with my masters.
So at MIT, doing a power electronics.
Okay.
So my master's thesis was on designing an efficient power converter for low power circuits that
I use in mobile devices.
So I just had this in small phones.
Okay.
So when I finished my masters, I liked research, so I decided I would apply for a PhD.
I got into Stanford. And when I came in, initially, I thought that I was going to continue
and work in power electronics.
Okay.
But Stanford has a PhD rotation program. So during your first year, you can spend all three
quarters working in three different labs.
Oh, really?
Yes. So with three different professors, and it kind of gives students the opportunity
to explore a little bit, all of the different professors, all of the different projects
that are available.
So I took advantage of the program.
I worked with Professor Juan Rivas from the power electronics lab.
Then I worked with Professor Sebastian Trond, who used to do robotics.
And at the time, actually, was more focused on computer vision things.
And that's when I learned a little bit about computer vision, thought I was interesting,
started taking classes.
And during my third rotation, I worked with Professor Sylvia Savarisa, who is currently
my advisor.
Okay.
In his lab, that's when I learned about deep learning.
I started exploring it a bit more, thought he was interesting.
And I think the most fascinating aspect of it was that just the possibility of using the
techniques and the tools that are available to solve various problems ranging from energy,
finance, construction, medical.
So I thought I was pretty exciting.
Awesome.
Awesome.
Tell me a little bit about the current focus of your research.
What are you working on?
In fact, you've got a paper that you're presenting here at NEPS, right?
Yes.
So I focused on 3D scene understanding.
Okay.
So my goal is to take visual information about the 3D world and try to make sense of it.
And the reason why such information is important is that if you have intelligent systems that
are surrounding human beings and that are able, in order for them to be able to assist
human beings, they need to be able to understand what is going on around us.
And some of the applications of that are self-driving cars and just general assistance within
the home.
Let's say an elderly person who needs help with daily tasks or even a surgeon in the hospital
that needs assistance, maybe for getting tools around and things like that.
So I have been working.
My recent paper is about semantic segmentation of 3D point clouds.
So with sensors such as connect sensors, lighter sensors, you can acquire models of 3D environments.
It could be indoor spaces or outdoor spaces.
And that information is not meaningful in and of itself and using deep learning, we can
make sense of it.
And the goal of the project was essentially to identify all of the different elements that
compose the data, the 3D data.
In indoor spaces you might be interested in identifying the walls, the ceilings, the objects,
the tables, chairs, and all that.
And in outdoor spaces you might be interested in identifying roads, the cars, and buildings
for instance.
So the method that we develop is just before you go into the method to just to make sure
I understand.
I'm imagining we've got a connect sensor in here and it's giving us kind of this 3D point
cloud of everything in this room, but it's just points.
And so the points aren't distinguished from the table, the chairs, et cetera, the people.
And so you're applying deep learning to on top of that raw data set, then categorize
or is it a categorization problem for each of the points into some object and you don't
know the objects that are in the field before you do this, right?
Yes, we don't know the objects that are in the field, but we have a certain set of objects
that we're interested in.
So the thing with deep learning or a little supervised learning is that you need to know
the classes that you are interested in ahead of time.
So they're in order to first perform the annotation because the ground to this needed.
So during the annotation process you essentially choose the classes that you are interested
in.
Okay.
And that's what you use for training.
So although you don't know all the classes that are available in your data set, there
will be a specific set of classes that are used during training and during testing.
And usually there's a default class that is called clutter or other.
Okay.
Which is used to identify anything else that was enabled in the data set.
Okay.
And so when you're identifying the classes that you expect, is this a small number of,
you know, I'm expecting to see, you know, this is a home environment, you know, they're
probably, you know, one of these 10 things or is it like, you know, a thousand classes
all of image net or something like that?
So at the level of 3D data, it's still a very small number of classes, in part because
3D data is much more difficult to annotate.
And so the number of classes range somewhere between 8 to 10 or 20 classes.
So it's not yet in the hundreds.
Okay.
It's a lot more difficult to annotate 3D data.
And the fact that sensors, depending on what modality you annotated as well, the sensor
data could also be very noisy.
So it could be difficult to identify certain classes.
So and that's one of the challenges with 3D data compared to images.
Images are much easier to acquire.
And pretty much everyone today has a cell phone, a camera, so there's a lot of images
that are being created at every second.
And the same, it doesn't apply to 3D data.
You know, everyone has a connect sensor or a lighter that they use on a daily basis essentially.
So 3D data is much less available and then you need those specialized tools to really
annotate them.
So it's a bit more difficult.
And when you say annotate, are you talking about using your model to annotate or are you
talking about the pre-training task of having, you know, creating the training set by annotating
sample data?
Yeah.
So I'm talking about the pre, the task before training, the task of generating the data
I say itself in the annotations that are going to be used for training.
So that is the most challenging part around 3D computer vision.
You know, even for 2D data, right?
My sense is that the folks that are doing annotation at scale have all kind of come up with
their own systems for doing this.
There's not like an off-the-shelf open source annotation toolkit.
I think there should be.
But I haven't really seen anything like that.
There are various ones that are out there.
Are they really?
Yeah, they definitely are.
I think one of the, one that I can think of of the top of my head is called LabelMe.
So it's available to the research community and a lot of people use it.
Oh, okay.
Yeah.
So there definitely are four images and also for 3D data, more recently there was an annotation
tool called scannet that was released to annotate 3D data.
So it makes the task a lot more easier.
Okay.
Essentially, the task is to paint over a bunch of segments in 3D and choose a class for
the segments that I painted over.
So it makes the task a lot more, a lot easier.
But even then, it sounds like there's, you've got all kinds of ambiguity because you're
painting in 2D over this 3D point cloud.
Is that right?
Or are you doing like rotations and all that kind of stuff?
Yeah.
Scannet actually allows you to paint in 3D.
Oh, really?
Okay.
So that is, it's more suitable for 3D data.
The challenge there is that the reconstruction that is used as the basis of the annotation needs
to be a very high quality, which is not always the case because the sensors, the 3D sensors
are noisy.
And what happens is that you're able to annotate the bigger objects that are more visible.
Okay.
So you can easily paint the table, but painting a bottle of water on the table is going to be
a lot harder because points will be missing or that kind of thing.
Exactly.
Whereas on an image, it's much easier to do that.
Right.
Yeah.
Well, interesting.
I didn't realize that, you know, I've had this conversation with a bunch of folks that
are working on this and they all say, no, no, there's nothing really out there.
We had to build it ourselves.
There are some tools, depending on the quality, there are mostly research tools.
Okay.
So they may not be very thoroughly developed if you want to use it for production.
Maybe it's not what you want to use.
Right.
But there are tools that are used by the research community.
Okay.
Yeah.
But I imagine then that, you know, even, yeah, if you're trying to do 3D annotation,
is it, I'm imagining that it's harder to, like, get a mechanical turquer to do 3D labeling,
you know, A because, you know, there's probably some kind of special plug-in that has to
be used if you can even do it over the web, but then B, just the complexity of, like, the
instructions you give and that kind of thing.
It sounds very difficult.
It likely is slightly more difficult than images, but it is doable.
Okay.
So the tool that I mentioned scan it, that's essentially what they did.
They crowdsourced both the data collection process and the annotation process.
Okay.
So they had people, they had turkers, I'm not sure who was turkers, but they had workers
actually annotates using their tool, they gave them instructions and the annotations turned
out pretty decent.
Okay.
Yeah.
Interesting.
We've got the, you know, we've got some training data assembled, you know, tell us about
your process and how you went from that to point segmentation.
Okay.
So essentially for each point in the 3D data in the point cloud, we want to assign it label.
And we wanted to leverage convolutional neural networks since they've been shown to work
really well for images.
The data representation for 3D point clouds is not directly suitable for convolutional
neural networks.
So we have it because think about an image, an image is a 2D array and it has a level
of structure that is suitable for convolutional neural networks.
So CNN's kind of take advantage of that structure in the image through the convolutions.
So you've got an array of pixels?
Yes.
And kind of mapping that to a point cloud at any level of resolution, you could have multiple
points in kind of a box.
So how would you translate it?
So the point clouds, when they actually are collected, they come in as a 2D array, but
it's more, it's not structured.
Okay.
So it's a set of XYZ coordinates with RGB color.
Okay.
And so it's a bit less structured.
So there is no, unless you go to the process of oxidization, which is what we did.
Of what?
Of oxidization.
So essentially turning that representation into a grid.
Okay.
So, and the way to do that essentially is to look at the 3D space and divide the 3D space
into a grid essentially and assign, turn your XYZ RGB representation into a different
representation, which has four channels.
The first channel is occupancy, which is zero or one.
And if it's a one, then it means that that space is occupied.
And zero is not occupied.
And the last three channels are not the RGB color.
The RGB.
Yes.
Which usually is the average of all the points that are within the unit grid cell, which
are a color box.
Okay.
Yeah.
So you have to make that transition from point cloud to voxel.
And the voxel usually has a predefined size in our paper.
We start out with five centimeters.
And there is another difficulty with 3D data, which is the added dimensionality.
So in 2D, you just have two dimensions plus the channels.
In 3D, you have the added depth dimension.
So that kind of scales the problem and makes it a bit more difficult to solve in terms
of memory.
So 3D data would take more space when you're using CNNs.
And for this reason, when we're using traditional convolutional neural networks, we actually
have to down sample our input volume further.
So we start out with the five centimeter voxel and we down sample it through the CNN.
So the CNN would down sample the volume to 20 centimeter voxel.
So at the end, what the CNN outputs is a label over 20 centimeter grid unit, which is
relatively coarse.
Yeah.
Exactly.
So what our paper explored is how can we obtain still average CNN, how can we obtain a
more fine green labeling of our point car, how can we label the individual points which
come from the original sensor.
And to do that, we design a trial and error interpolation layer that a trial and error interpolation
layer.
A trial and error interpolation layer.
So we use it to train during training to train the CNN and to end.
And basically what it does is the CNN outputs a set of scores for each voxel in space.
For each point in space, we look at the eight closest voxel centers.
And we compute the score of the point by weighing the combination of the ways that are coming
from the eight nearest voxels.
So you can think of the closest voxel will be contributing more to the final score at
the point than the voxels that are a bit further away from the point.
And so we use that during training and the alternative to using.
And now is this the eight closest occupied voxels or just the eight closest voxels?
The eight closest occupied voxels, yes.
And so during training, the alternative to doing that is simply doing a nearest neighbor.
So taking the closest voxel and assigning that label to the point.
And during training, we find out that actually using trial and error interpolation layer
performs a lot better than using a nearest neighbor.
So that was the first module that we added to our CNN.
And in the end, we also used a conditional random field to refine the predictions that
are given to us by the CNN and the trial and error interpolation layer.
And so the conditional random field essentially defines an energy function that enforces consistency
between labeling intuitively.
You can think, if you take two points in space, if those points are close to each other,
then they should, the probability that they have the same label is high.
And so the energy function of the CRF enforces that close points will have, are more likely
to have similar labels.
And so without energy function, we are able to refine our predictions.
And we use a nice implementation of conditional random fields that was published in a 2015
paper called CRF as RNN.
So the CRF is implemented as a recurrent neural network.
And the advantage of that implementation is that you can actually combine both the CNN,
the trial and error interpolation layer and the CRF and train them into N.
Okay.
It's a nice module that can be used for semantic segmentation.
Okay.
Yes.
Interesting.
And so what kind of, what kind of results have you seen with that?
We've seen great results.
So we evaluated framework on the Stanford Indoor Space Data Set.
And we performed the state of the art on that data set, which was pointed.
And currently we also have, we also tested on the semantic 3D net data set, which is the
largest outdoor point cloud data set that is available.
And at the time of publication, we're also the state of the art, but I think there is
a better method now because that's what happens.
But yeah.
So currently we are, I guess the second best method performing, the second best performing
method on those data sets.
Okay.
And for these data sets, was the improvement like order of magnitude or was it like huge
or was it incremental improvements relative to the prior state of the art?
So depending on what we saw was that the best improvement that we got was on the largest
data set.
Okay.
So we had about, I cannot remember the actual numbers, but it was more significant on
the larger outdoor point cloud data set.
Okay.
There was also a pretty large gap between on the, on the, on the smaller indoor spaces data
set.
But the data set is also relatively small.
And so the performance, the absolute performance is still a little bit low.
And I think as we get more data, the numbers should get better, yeah.
Now this conversation reminds me of another conversation that I've had here at NIPS.
But it doesn't remind me of it.
It is potentially related to another conversation that I've had here at NIPS.
And the, the idea with the research they were doing is that there are, like here you've
had to, you know, a lot of what your research focus was was kind of fitting or transforming
this point cloud to the, the voxels and potentially losing the relationship between some of the,
the individual points in the cloud.
And they, this was an interview as Joanne Bruna at NYU and Michael Browstein and they are
developing, or they're researching algorithms that take more like a graph approach.
Okay.
And the individual point would be seen as a graph as opposed to points in a Euclidean
space.
Okay.
Have you looked at that kind of approach to doing segmentation?
I have not personally, but one, I think it's a promising approach as well.
We do use some amount of graphical representation in our method, not at the level of the CNN
but more at the final module, which is the conditional random field.
Okay.
So that's back to our initial representation, but I do think that approach also is likely
promising.
Yeah.
Interesting.
I guess that's the great thing about Nips is that you get exposure to all of these different
folks working on kind of related things and can, you know, try to pull together different
ideas.
Yeah.
There's definitely a lot of work that is being done in this field.
People are looking into different ways of processing the data.
So one of the papers that we compared to was Poinets, which essentially proposed to process
the points directly rather than doing voxalization.
So they're definitely alternatives to skipping voxalization.
The framework that we have in a way, it could, the conditional random field could still
apply to some of those methods, but definitely the point representation is flexible and
there are several ways that people could do it.
There, for instance, also, are more efficient ways to do convolution.
There's sparse convolutions that could be done so that could help with efficiency.
The opnet is one of the papers that export that direction essentially.
All of it.
Opnet.
Yes.
Okay.
So it's actually trying to perform the convolution operations on the spaces that are occupied
rather than the entire volume.
So they're definitely various approaches to solve this problem, so interesting.
With the conditional random field and the energy function, actually, that's not an area
that I'm well versed in.
Maybe you can talk a little bit more about that, but I'm specifically wondering is the thing
that you did that was interesting was applying conditional random fields and energy functions
or coming up with a specific energy function that worked well in this case.
So the energy function itself was standard except for the fact that it was applied on
3D points rather than traditionally on images.
So we, one of the novel things was that we applied it on the point at the point level.
Okay.
So the end part of our framework essentially is different in that we're not operating
in voxel space.
So traditionally when you're using the CNN and adding a CRF to it, usually the input does
not change from start to end input space.
We sort of have this interpolation there that helps us to go from a discrete space to a
continuous space and we do further processing within our continuous space with the CRF as
well as the tri-linear interpolation layer.
In terms of the energy potential that we use, we use XYZ coordinates and RGB color as
the features.
So the energy function itself has two different terms, the unit potentials as well as the pair
white potentials. So for the unit potentials, those come directly from the CNN true, the
tri-linear interpolation layer and that sort of represents the initial belief of the system
about the classes of each point.
And then you have the pair white potentials that are also added to the energy function and
those are the potentials that ensure the consistency between the neighborhood of each point.
And is that the piece that you mentioned was kind of more graph-like?
Yes, exactly.
Representation.
Yes, exactly.
So we use a fully connected conditional random field.
So it represents relationships between each point and its neighborhood.
In this case, because it's fully connected, it actually has a connection between each point
and every other point in the point cloud.
So that's the graph representation that we're using.
Yeah.
So what's next for this research?
So this research is actually a small subset of a bigger set of problems that I am exploring.
So I am interested in, as I said earlier, I'm interested in trying to see how we can enable
intelligence systems to assist human beings in their environments.
So the first subset of that is seen understanding.
Okay.
What's on the environment?
Exactly.
You can imagine if you give a robot in order to go to the bedroom and get a book or get
the Harry Potter book from my bookshelf, what does the robot need to do in order to accomplish
that task?
There's various fields that come into that.
There's natural language processing, there's vision, there's robotics.
And so the natural language processing to first understand the command, the visual part,
the perception part to navigate through the room, get to the room, identify the object,
and finally, the robotics part to actually do the interaction and pick up the objects.
So I started working on the scene understanding within in terms of a more detailed scene understanding
identifying objects, but there's also a higher level scene understanding that is involved
in those kinds of assistance, which is how do you identify the different places within
a building.
So how do you know where the bedroom is?
And so when we take those 3D point clouds, that information also is not available.
So I am interested in looking at getting the higher level understanding of scenes as well
and specifically using neural networks to obtain a navigation focused floor map of environments.
So there has been work done in getting floor maps of environments mainly segmenting rooms
between each other, so figuring out the separation between rooms.
But in order for those to be a helpful navigation, we need more information.
We need the identity of rooms.
So we need to know that this is a living room, a bedroom, bathroom, and so on and so forth.
And in addition, potentially, we need to know the location of entrances to each room.
So once you have that kind of information, you cannot plan navigation to go to a specific
room.
So those are the two parts of scene understanding that I'm interested in.
So getting a high level understanding of scenes as well as the more detailed object level
understanding of the scenes.
So for the higher level piece, there's been, as you mentioned, there's been a ton of
work that's happened, a lot of it in the robotics community in terms of mapping environments
and things like that is part of what you're trying to do to be able to identify, identify
room, for example, based on what's in the room, as opposed to being given a specific label
or something like that.
Yeah, the idea is to identify the room based on what is in the room, based on the visual
information, but also at the 3D level, because there is work that can identify a room based
on an image of the room.
But if you are in one location of a building and you want to get to the other location,
that kind of information is not going to be very helpful.
You need to have a map of the environment and know where each room is located.
And so we're doing that more at the 3D level and at a higher perspective added in using
images.
Yeah.
And in practice, do you envision that a system like this would be used dynamically or
statically?
And what I mean is, are we going to have like a mini light on the robot and as it navigates
the environment, it's making determinations about what the room is, or more like in the
future, all buildings will have, you know, they'll come with 3D point clouds or something
on whatever the future version of a floppy disk is.
Okay.
I think there would be an initial map of 3D environments.
And as the robot navigates around the environment, then it can update what the map looks like,
at a more local level.
So if you have an initial state of a room and you pass at a certain point and you see
that something has changed, then you can update that in the internal map that you have.
So it doesn't need to be updated in real time, well, the entire map does not need to be
updated always, but there needs to be an initial starting point that can be updated locally
as the, as more information is gathered over time.
So the, the robot would know the kind of overall 3D structure of the environment, but might
be able to infer that, well, you removed the bed and put a desk, so this is an office
now as opposed to a bedroom.
Exactly.
So I guess for an office to change, so a bedroom there would probably be a lot more than
like one single change, but even imagining that a room, the room state is still the same,
but just a chair was moved to a different location.
That can affect navigation planning, for instance, and that's something that is worth
updating as well.
And then, you know, it coming from a group that's that's largely focused on vision or is
your line of research like, in a sense, swimming against the tide, like I get the sense that
I get the sense that, you know, vision folks think that everything will be solved through
cameras, as opposed to, you know, point clouds and things like that.
Well, I would say, in a way, if you think about getting the cameras, they, in a sense,
are also the cameras that give you 2D data and you can transform it into 3D point cloud.
I think the sort of complementary, I think 3D can come from images.
So in a way, you could say that you're using images, if you're, I guess, working mainly
with RGBD data.
In terms of lighter, I think, I guess there is a use for longer range sensors such as
sliders.
So they can allow you to detect objects that are further apart.
And I, you can also, you can try to use reconstruction algorithms, but they're not always as effective
as the more longer, as longer range sensors are in terms of data collection.
So I think both will have a role to play in terms of solving these problems and maybe
in different applications, images would be better than long-range sensors.
Okay.
Yeah.
Okay.
Paulin, thank you so much.
It's been a really interesting conversation.
Any final words or places that you'd like to point folks to or?
I guess if you want to follow my work, go and check out the website, the project that I
worked on is called setcloud, 3D semantic segmentation.
Okay.
Yeah.
Great.
And we'll include a link to that in the show notes.
All right.
Thank you.
All right.
Thank you.
Thank you.
All right.
All right, everyone, that's our show for today.
For more information on Lynn or any of the topics covered in this episode, you'll find
the show notes at twomolai.com slash talk slash one, two, three.
As you know, we love to hear your questions and feedback on the show, so don't hesitate
to comment there.
Thanks so much for listening and catch you next time.
