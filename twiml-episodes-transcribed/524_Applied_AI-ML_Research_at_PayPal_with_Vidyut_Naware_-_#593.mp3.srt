1
00:00:00,000 --> 00:00:09,600
Alright everyone, welcome to another episode of the Twilmal AI podcast.

2
00:00:09,600 --> 00:00:15,480
I'm your host Sam Charington and today I'm joined by Vidyut Napire, director of AI and

3
00:00:15,480 --> 00:00:17,920
machine learning at PayPal.

4
00:00:17,920 --> 00:00:21,280
Before we get going, be sure to take a moment to hit that subscribe button wherever you're

5
00:00:21,280 --> 00:00:23,640
listening to today's show.

6
00:00:23,640 --> 00:00:25,640
Vidyut, welcome to the podcast.

7
00:00:25,640 --> 00:00:26,640
Thanks Sam.

8
00:00:26,640 --> 00:00:27,640
Happy to be here.

9
00:00:27,640 --> 00:00:28,640
Big fan of usual.

10
00:00:28,640 --> 00:00:29,640
Thanks so much.

11
00:00:29,640 --> 00:00:34,120
I'm really looking forward to this conversation and learning a bit more about what you're up

12
00:00:34,120 --> 00:00:43,640
to both from the perspective of the hat you wear leading applied ML and AI at PayPal but

13
00:00:43,640 --> 00:00:48,640
also some of the work you're doing supporting ML ops and the machine learning platform

14
00:00:48,640 --> 00:00:50,440
teams there.

15
00:00:50,440 --> 00:00:53,360
But before we jump into all that, I'd love to have you share a little bit about your

16
00:00:53,360 --> 00:00:56,320
background and how you came to work in the field.

17
00:00:56,320 --> 00:01:03,040
I joined PayPal three years ago and I was, before that, I used to work in the autonomous

18
00:01:03,040 --> 00:01:04,040
driving area.

19
00:01:04,040 --> 00:01:09,880
I used to work at a company called Neo where my team was working on building kind of very

20
00:01:09,880 --> 00:01:14,560
interesting anomaly detection algorithms for catching crazy driving scenarios out there

21
00:01:14,560 --> 00:01:15,560
in the field.

22
00:01:15,560 --> 00:01:21,560
Before that, I used to work at Qualcomm where I led several interesting projects in modern

23
00:01:21,560 --> 00:01:27,320
design, sensor system design and so on, that's where I started getting my hands and feet

24
00:01:27,320 --> 00:01:29,920
wet with AI ML if you will.

25
00:01:29,920 --> 00:01:35,720
For the past three years, I've been at PayPal where I'm leading, as you said, the AI R&D

26
00:01:35,720 --> 00:01:41,960
function applied AI R&D function and in addition, off late, I also started leading some of

27
00:01:41,960 --> 00:01:45,120
our ML ops efforts in this area.

28
00:01:45,120 --> 00:01:51,920
So really, really interested in sharing what we've found in our journey so far.

29
00:01:51,920 --> 00:01:52,920
Awesome.

30
00:01:52,920 --> 00:01:53,920
Awesome.

31
00:01:53,920 --> 00:01:59,600
Just thinking about some of the places you've been on your personal journey, you kind

32
00:01:59,600 --> 00:02:05,360
of started out in these very, you know, edge scenarios, hardware driven scenarios and

33
00:02:05,360 --> 00:02:08,600
I'm imagining that PayPal is not like that at all.

34
00:02:08,600 --> 00:02:13,360
I'd love to hear you kind of comparing, contrast some of your prior experiences to what

35
00:02:13,360 --> 00:02:14,680
you see now in PayPal.

36
00:02:14,680 --> 00:02:15,680
Yeah.

37
00:02:15,680 --> 00:02:16,680
That's a great question.

38
00:02:16,680 --> 00:02:22,760
Actually, you know, as you rightly pointed out, when I was at Qualcomm, you know, it's

39
00:02:22,760 --> 00:02:27,920
a very different vertical, very different space and the kinds of problems that you encounter

40
00:02:27,920 --> 00:02:34,320
there, you know, have more to do with having very, very limited access to compute, limited

41
00:02:34,320 --> 00:02:40,160
access to memory, you know, so the focus there entirely is on how do you kind of build

42
00:02:40,160 --> 00:02:46,560
the most, you know, energy efficient algorithms from an AIML perspective versus when you move

43
00:02:46,560 --> 00:02:53,600
into a company like PayPal, which is primarily kind of like a cloud-based, you know, platform,

44
00:02:53,600 --> 00:02:55,600
it's a very different situation altogether.

45
00:02:55,600 --> 00:03:01,480
I mean, you're no longer strictly limited by the amount of compute, memory, resources

46
00:03:01,480 --> 00:03:03,000
you have.

47
00:03:03,000 --> 00:03:07,280
It becomes a slightly different environment where you are trying to, you know, bring

48
00:03:07,280 --> 00:03:11,600
AI ML to production, so, you know, just as an example, right?

49
00:03:11,600 --> 00:03:18,240
Like, you know, when at Qualcomm, when we were building, you know, various algorithms,

50
00:03:18,240 --> 00:03:19,560
you know, in the sensor area, right?

51
00:03:19,560 --> 00:03:25,480
So this is where we are trying to use Excel's and gyros, you know, to figure out whether

52
00:03:25,480 --> 00:03:30,800
what kind of activity a person is engaged with, with sensors.

53
00:03:30,800 --> 00:03:34,760
You know, we have a very, very small power budget, energy budget to work with.

54
00:03:34,760 --> 00:03:40,000
So we really, really have to focus on making our algorithms very, very efficient.

55
00:03:40,000 --> 00:03:45,840
In PayPal, it's kind of like a different, I think here it's more about how do we, you

56
00:03:45,840 --> 00:03:53,360
know, make sure that we are deploying the algorithm to solve, you know, to improve performance,

57
00:03:53,360 --> 00:03:57,160
but cost is a consideration, but it's not the most important thing, right?

58
00:03:57,160 --> 00:04:03,000
So it's, it's an interesting, it's an interesting set of experiences I've had.

59
00:04:03,000 --> 00:04:06,480
And then autonomous driving kind of that area falls somewhere in the middle of both of

60
00:04:06,480 --> 00:04:11,880
this, because, you know, on the one hand, you're still doing all of your inferencing on

61
00:04:11,880 --> 00:04:18,120
the edge, because ultimately all your computer vision processing has to happen on the car.

62
00:04:18,120 --> 00:04:23,200
But, but so in that sense, it is energy constraint because you have to do all of that, you

63
00:04:23,200 --> 00:04:24,920
know, kind of battery constraint environment.

64
00:04:24,920 --> 00:04:31,360
But on the flip side, the performance that you need, the bar that you need is very high.

65
00:04:31,360 --> 00:04:35,480
And also, the complexity is extremely high, right?

66
00:04:35,480 --> 00:04:40,480
Because you're processing very, very high bit rate video streams from multiple cameras

67
00:04:40,480 --> 00:04:41,480
concurrently, right?

68
00:04:41,480 --> 00:04:47,200
So the amount of compute you need to solve those problems is, you know, is much higher than

69
00:04:47,200 --> 00:04:50,800
what you would, what I was dealing with when I was at Qualcomm, for example.

70
00:04:50,800 --> 00:04:57,160
So it's just like each area has added its own, you know, difficulties and challenges

71
00:04:57,160 --> 00:05:02,920
to deal with, but it's also made me aware of, you know, very, very interesting perspectives

72
00:05:02,920 --> 00:05:04,520
on solving the same problem.

73
00:05:04,520 --> 00:05:10,320
You know, and I think about, again, financial services or PayPal in particular, fraud is

74
00:05:10,320 --> 00:05:15,640
probably the first thing that, you know, jumps to mind for me and for other folks.

75
00:05:15,640 --> 00:05:21,520
What are some of the other use cases that kind of fall in your portfolio and that are important

76
00:05:21,520 --> 00:05:22,520
at PayPal?

77
00:05:22,520 --> 00:05:23,520
Yeah.

78
00:05:23,520 --> 00:05:29,240
It's a good segue into, into kind of what, what overall data science at PayPal looks

79
00:05:29,240 --> 00:05:30,240
like.

80
00:05:30,240 --> 00:05:34,600
Before going there, maybe I'll just spend a few minutes talking a little about PayPal,

81
00:05:34,600 --> 00:05:35,600
right?

82
00:05:35,600 --> 00:05:41,640
Like what we do, you know, and where we are at in terms of scale and so on.

83
00:05:41,640 --> 00:05:47,240
So as you know, PayPal is probably one of the most widely recognized kind of payments,

84
00:05:47,240 --> 00:05:55,880
payment services company, we have last count, maybe around 395 or so million customers,

85
00:05:55,880 --> 00:05:59,560
maybe another 30 to 35 million merchants on the platform.

86
00:05:59,560 --> 00:06:06,560
So basically what we enable is this two-sided network where we have on the one hand, you

87
00:06:06,560 --> 00:06:14,080
know, all these customers who can use PayPal products to move money, to manage money.

88
00:06:14,080 --> 00:06:18,760
We also offer credit services to customers, for example.

89
00:06:18,760 --> 00:06:23,320
And then on the merchant side of the house, we have, you know, our checkout product.

90
00:06:23,320 --> 00:06:29,960
We have, we have several other, we have a whole laundry of products, right, where we enable

91
00:06:29,960 --> 00:06:33,960
our merchants to improve their own metrics, their own business metrics.

92
00:06:33,960 --> 00:06:38,320
We also help them with risk and fraud management, for example.

93
00:06:38,320 --> 00:06:41,560
And last but not least, we also provide merchants with credit, right?

94
00:06:41,560 --> 00:06:49,200
So as and when they need it, also not to forget, but it's, it's probably not widely known

95
00:06:49,200 --> 00:06:54,480
probably to your audience, but Venmo Zoom, they are also actually PayPal products, very,

96
00:06:54,480 --> 00:06:55,480
very popular.

97
00:06:55,480 --> 00:07:00,920
For example, Venmo with Gen Z, and you know, Zoom is a cross-border, cross-border remittance

98
00:07:00,920 --> 00:07:02,400
is right, like super popular.

99
00:07:02,400 --> 00:07:04,400
Zoom with an X, not Zoom with a Z.

100
00:07:04,400 --> 00:07:06,800
Zoom with an X, exactly.

101
00:07:06,800 --> 00:07:12,480
And several more, right, brain tree, for example, which is, you know, a payment gateway solution

102
00:07:12,480 --> 00:07:18,320
he offers, yeah, which not just a caring payment, but basically it's a payment, you know,

103
00:07:18,320 --> 00:07:23,800
gateway for merchants, right, similar to what stripe offers to merchants.

104
00:07:23,800 --> 00:07:27,960
So anyway, so I think the first thing to understand is PayPal is a very diverse ecosystem with

105
00:07:27,960 --> 00:07:30,920
a lot of different services and products.

106
00:07:30,920 --> 00:07:36,200
And you know, each one of them have their own, I would say, flavor of AIML, you know,

107
00:07:36,200 --> 00:07:37,720
embedded within them.

108
00:07:37,720 --> 00:07:42,400
But again, just zooming out, if you, if I were to broadly categorize all of the AIML,

109
00:07:42,400 --> 00:07:47,280
I think driven products, I think the number one would be definitely fraud prevention, that

110
00:07:47,280 --> 00:07:55,320
is the area where we have the most advanced, most complex AI systems in use and deployed.

111
00:07:55,320 --> 00:07:57,440
After that, you know, there is credit.

112
00:07:57,440 --> 00:08:04,040
So several, several of our credit product also is powered by AIML, especially in the

113
00:08:04,040 --> 00:08:09,400
underwriting process in the marketing of those kinds of products to prospective customers

114
00:08:09,400 --> 00:08:11,760
or merchants.

115
00:08:11,760 --> 00:08:19,120
Then there is things like sales and marketing, you know, when we are trying to essentially

116
00:08:19,120 --> 00:08:24,960
cross sell or upsell, you know, existing products to an existing customer or merchant,

117
00:08:24,960 --> 00:08:30,600
we will typically try to use AIML to identify those customers who are most likely to, you

118
00:08:30,600 --> 00:08:35,800
know, basically propensity models, which will help our business teams to identify the

119
00:08:35,800 --> 00:08:42,400
best set of next customers to target on the both sales and marketing side.

120
00:08:42,400 --> 00:08:44,800
There's also customer service, right?

121
00:08:44,800 --> 00:08:50,560
So we, in our, you know, PayPal is a big company, right?

122
00:08:50,560 --> 00:08:55,120
And many people reach out to us whenever they have some issue with the transaction that

123
00:08:55,120 --> 00:08:59,400
didn't go through or a transaction that was put on hold or stuff like that.

124
00:08:59,400 --> 00:09:01,640
So people contact us a lot of the time.

125
00:09:01,640 --> 00:09:09,520
We actually use NLP bird-like models in many of these use cases to identify, for example,

126
00:09:09,520 --> 00:09:14,160
in a chat context, the intent with which potentially a customer is calling us or contacting

127
00:09:14,160 --> 00:09:15,160
us.

128
00:09:15,160 --> 00:09:17,160
And then there is compliance.

129
00:09:17,160 --> 00:09:20,560
So again, this is where the regulatory side comes into the picture.

130
00:09:20,560 --> 00:09:27,440
So, you know, we are expected to catch, you know, any kind of illegal activity on the

131
00:09:27,440 --> 00:09:31,880
platform, things like money laundering or drug trafficking, and so like there are all

132
00:09:31,880 --> 00:09:37,760
these kinds of use cases that we are supposed to catch and flag to regulatory bodies.

133
00:09:37,760 --> 00:09:43,040
So we again use AIML there as well, because ultimately, if you map all of some of these

134
00:09:43,040 --> 00:09:48,320
problems, they all become AIML problems at some level.

135
00:09:48,320 --> 00:09:53,440
And again, this is just, I would say that probably the top five or six use cases, but there

136
00:09:53,440 --> 00:09:59,040
is many, many, I would say there is again kind of like a longish tale of other areas within

137
00:09:59,040 --> 00:10:04,840
PayPal where there is a lot of exploration going on around where AIML can be used to solve

138
00:10:04,840 --> 00:10:06,960
a certain business problem and so on.

139
00:10:06,960 --> 00:10:13,440
So hopefully that gives you a full view of what AIML looks like in PayPal today.

140
00:10:13,440 --> 00:10:14,720
No, that does.

141
00:10:14,720 --> 00:10:22,000
And now if you could then kind of use that as a segue to map to your applied R&D portfolio

142
00:10:22,000 --> 00:10:29,840
and you think about the projects and investments you're making there and what you think is most

143
00:10:29,840 --> 00:10:36,520
promising for the sets of problems that you have in terms of approaches.

144
00:10:36,520 --> 00:10:40,360
Yeah, it's a great question.

145
00:10:40,360 --> 00:10:47,120
So I'll probably provide my perspective on how I look overall at kind of the area of R&D

146
00:10:47,120 --> 00:10:55,040
where the PayPal director and R&D hat, right? So for me, like the overall AIML landscape,

147
00:10:55,040 --> 00:11:00,640
like I break it into four categories, there's kind of like the hardware compute layer,

148
00:11:00,640 --> 00:11:03,400
you know, where I feel there's a lot of innovation happening.

149
00:11:03,400 --> 00:11:08,440
Then there's the core algorithmic layer, which is kind of the focus of most of the academia

150
00:11:08,440 --> 00:11:12,480
and the Facebook's and Google's of the world.

151
00:11:12,480 --> 00:11:16,120
And finally, there's the application layer, right, where a customer is interacting with

152
00:11:16,120 --> 00:11:21,280
the product and he gets some experience driven by AI, right?

153
00:11:21,280 --> 00:11:25,080
So each of these areas, I feel there's a lot of innovation happening.

154
00:11:25,080 --> 00:11:30,120
And so my role as an artist, say four, yeah, sorry, fourth one is tools frameworks and

155
00:11:30,120 --> 00:11:31,120
platforms, right?

156
00:11:31,120 --> 00:11:36,000
I thought you were going there, but just, yeah, I missed it, sorry, thanks for catching it.

157
00:11:36,000 --> 00:11:39,920
So the tools frameworks and platforms are, you know, this is where I look at the overall

158
00:11:39,920 --> 00:11:40,920
ecosystem, right?

159
00:11:40,920 --> 00:11:43,720
And this is where, for example, MLOPS enters the picture, right?

160
00:11:43,720 --> 00:11:49,280
So how do we make sure that not just each cog in this wheel is doing well?

161
00:11:49,280 --> 00:11:53,680
But as a whole, how does everything kind of work well together in the most efficient

162
00:11:53,680 --> 00:11:54,680
manner, right?

163
00:11:54,680 --> 00:11:59,480
So MLOPS, things like interoperability, platforms, because, you know, there is a, you know,

164
00:11:59,480 --> 00:12:03,720
there are all these different machine learning frameworks, you know, there's TensorFlow,

165
00:12:03,720 --> 00:12:04,720
there's PyTorch.

166
00:12:04,720 --> 00:12:10,360
Now, how do we actually, on X, how do we, yeah, so on X is, for example, something that

167
00:12:10,360 --> 00:12:13,920
we are investigating for our use cases.

168
00:12:13,920 --> 00:12:19,680
So now, let me kind of like, you know, kind of like drive deeper into three main, like

169
00:12:19,680 --> 00:12:22,240
I would say, areas of R&D, right?

170
00:12:22,240 --> 00:12:25,920
The first is the kind of hardware compute layer.

171
00:12:25,920 --> 00:12:32,120
Here I think of two primary initiatives where we already made some investments, we are

172
00:12:32,120 --> 00:12:35,080
really, really hopeful about what, where this is going to go.

173
00:12:35,080 --> 00:12:40,080
The first one is this whole idea of federated learning and how do we essentially benefit

174
00:12:40,080 --> 00:12:47,280
from being able to offload some of our inferencing and hopefully in the future, the future training

175
00:12:47,280 --> 00:12:53,240
workloads directly to the customer on merchant devices, right?

176
00:12:53,240 --> 00:12:59,760
So we have some ongoing work in this area where we feel it's the right time.

177
00:12:59,760 --> 00:13:04,520
It's going to be very important from a cost management perspective, also from a privacy

178
00:13:04,520 --> 00:13:12,240
and regulatory perspective, that the closer we move the compute and the lesser we move

179
00:13:12,240 --> 00:13:19,880
data to our data centers, I think the better it is for everybody in the ecosystem.

180
00:13:19,880 --> 00:13:28,120
The second one is around how, again, this is something that we are extremely positive

181
00:13:28,120 --> 00:13:39,160
about, which is how do we essentially make sure that we use the data that we use, right?

182
00:13:39,160 --> 00:13:43,920
Fundamentally, the data layer and the compute layer in the most efficient manner.

183
00:13:43,920 --> 00:13:50,240
And for example, this could be things like, you know, how do we, for example, store data

184
00:13:50,240 --> 00:13:54,960
at 16-bit resolution, do we really need to store data at 32-bit resolution, do we really

185
00:13:54,960 --> 00:13:59,440
need to do compute at 32-bit, can we do it at 16-bit?

186
00:13:59,440 --> 00:14:09,080
Because again, the impact, the impact that this has on the overall cost of building an

187
00:14:09,080 --> 00:14:14,840
AIML solution, especially for a company like PayPal where we have such humongous amounts

188
00:14:14,840 --> 00:14:19,760
of data and to deal with, right?

189
00:14:19,760 --> 00:14:24,840
So we are investing in this area as well, like how do we optimize our compute layer

190
00:14:24,840 --> 00:14:29,360
and how do we optimize our data layer.

191
00:14:29,360 --> 00:14:32,960
And then, so those two, I think, are the kind of, I would say, primary two things we are

192
00:14:32,960 --> 00:14:34,960
focusing on.

193
00:14:34,960 --> 00:14:43,800
Yeah, the hardware silo, we have, for example, done some pilots in the quantum computing

194
00:14:43,800 --> 00:14:44,800
area.

195
00:14:44,800 --> 00:14:51,440
We have done, we are also looking at, for example, in the area of graph compute, right?

196
00:14:51,440 --> 00:14:59,280
You know, what is the best possible hardware layer for graph?

197
00:14:59,280 --> 00:15:05,320
Because as you know, graph computing, I think the underlying hardware ecosystem today

198
00:15:05,320 --> 00:15:10,800
is not optimized for graph because of the fact that, you know, the memory access patterns

199
00:15:10,800 --> 00:15:17,560
when you are basically trying to do graph compute are not well suited to today's, I would

200
00:15:17,560 --> 00:15:19,520
say, computer architecture.

201
00:15:19,520 --> 00:15:27,120
So, so in this particular instance, for example, we are, you know, working with some external

202
00:15:27,120 --> 00:15:31,480
partners trying to understand, you know, where this might go.

203
00:15:31,480 --> 00:15:36,120
But there's several different lines of things at the hardware acceleration layer that we

204
00:15:36,120 --> 00:15:37,360
are also considering.

205
00:15:37,360 --> 00:15:44,120
So kind of in the context of hardware, you're looking at federated learning, I'm curious

206
00:15:44,120 --> 00:15:50,520
when you think about federated learning or most of your efforts focused on what I think

207
00:15:50,520 --> 00:15:56,120
of as kind of the algorithmic elements of federated learning, like differential privacy

208
00:15:56,120 --> 00:16:04,360
and privacy preserving machine learning or more like deployment challenges, logistical

209
00:16:04,360 --> 00:16:10,200
challenges of just getting models, you know, you know, model slash data to and from devices

210
00:16:10,200 --> 00:16:15,200
and coordinating all of that, you know, distributed training, things like that.

211
00:16:15,200 --> 00:16:16,200
Yeah.

212
00:16:16,200 --> 00:16:21,280
So it's a journey for us, Sam, when federated learning, you know, has many components,

213
00:16:21,280 --> 00:16:26,800
pieces, some of them, as you said, more AIML algorithmic in nature, some of them more

214
00:16:26,800 --> 00:16:29,880
engineering and infrastructure oriented.

215
00:16:29,880 --> 00:16:35,000
So in terms of our journey, right now, we are focusing on first, you know, it's kind

216
00:16:35,000 --> 00:16:37,680
of like, you know, crawl first, then walk, then run right.

217
00:16:37,680 --> 00:16:46,480
The first goal is start to actually do inferencing of some models, you know, on the device.

218
00:16:46,480 --> 00:16:53,920
Gradually, as, you know, in parallel, of course, as you said, it just doesn't stop there.

219
00:16:53,920 --> 00:16:57,720
We have to start thinking about, you know, differential privacy, can we, you know, how

220
00:16:57,720 --> 00:17:06,040
do we make, you know, our models differently private, you know, that's one angle to it,

221
00:17:06,040 --> 00:17:11,240
distributed learning where the full, uh, ambit of edited learning actually is also eventually

222
00:17:11,240 --> 00:17:13,160
doing training on device.

223
00:17:13,160 --> 00:17:18,400
Um, but I think that's a bit far out, uh, from our current, uh, from where we are right

224
00:17:18,400 --> 00:17:19,400
now.

225
00:17:19,400 --> 00:17:24,440
But certain things, there are opportunities here, uh, to do that.

226
00:17:24,440 --> 00:17:30,000
I would, uh, I would just say that, uh, that's where we are at basically, uh, in this particular

227
00:17:30,000 --> 00:17:31,000
area.

228
00:17:31,000 --> 00:17:35,480
mentioned a couple of the kind of accelerated hardware

229
00:17:35,480 --> 00:17:41,800
acceleration opportunities that are out there.

230
00:17:41,800 --> 00:17:44,680
Is there anything in particular that you're finding

231
00:17:44,680 --> 00:17:50,000
exciting or interesting among the things

232
00:17:50,000 --> 00:17:51,600
that you've been looking at?

233
00:17:51,600 --> 00:17:55,760
I would say there are definitely interesting things,

234
00:17:55,760 --> 00:17:59,160
but I feel there are a bit like from an overall technology

235
00:17:59,160 --> 00:18:00,400
maturity standpoint.

236
00:18:00,400 --> 00:18:03,360
I think they're not quite there for prime time.

237
00:18:03,360 --> 00:18:07,440
For example, our recent work on quantum computing,

238
00:18:07,440 --> 00:18:08,840
let me just share some context.

239
00:18:08,840 --> 00:18:14,360
We, one of the steps in basically the AML workflow

240
00:18:14,360 --> 00:18:15,360
is feature engineering.

241
00:18:15,360 --> 00:18:18,440
And one of the sub steps there is feature selection.

242
00:18:18,440 --> 00:18:20,440
So let's say you have a catalog of features

243
00:18:20,440 --> 00:18:24,160
and you're trying to find the best possible subset of features

244
00:18:24,160 --> 00:18:27,480
for a given ML problem.

245
00:18:27,480 --> 00:18:31,040
That problem is fundamentally a combinatorial optimization

246
00:18:31,040 --> 00:18:34,960
problem with the complexity of the number of features

247
00:18:34,960 --> 00:18:37,320
that we have.

248
00:18:37,320 --> 00:18:40,640
Today we use greedy approaches, traditional classical greedy

249
00:18:40,640 --> 00:18:41,160
approaches.

250
00:18:41,160 --> 00:18:44,040
So one of the POCs we did on quantum was,

251
00:18:44,040 --> 00:18:49,400
and this was with some external vendors like IBM and D-Wave

252
00:18:49,400 --> 00:18:51,000
in this space.

253
00:18:51,000 --> 00:18:56,080
And what we found is that we didn't get exactly

254
00:18:56,080 --> 00:18:58,400
very, very conclusive results that demonstrated

255
00:18:58,400 --> 00:19:03,240
that the feature selection that you can get with today's

256
00:19:03,240 --> 00:19:09,400
quantum computers can actually outdo classical approaches

257
00:19:09,400 --> 00:19:10,680
yet.

258
00:19:10,680 --> 00:19:19,240
So that was a sobering, I guess, a revelation for us.

259
00:19:19,240 --> 00:19:21,760
And we feel OK, so there's a few more years

260
00:19:21,760 --> 00:19:24,880
for this particular area to evolve.

261
00:19:24,880 --> 00:19:26,680
That's a specific case in point.

262
00:19:26,680 --> 00:19:30,400
But I feel that's a kind of broader industry trend.

263
00:19:30,400 --> 00:19:33,400
I know there's a lot of startups right now

264
00:19:33,400 --> 00:19:38,680
in overall the hardware acceleration space, companies

265
00:19:38,680 --> 00:19:42,400
are building custom silicon to accelerate AI workloads,

266
00:19:42,400 --> 00:19:43,960
either for training or inference.

267
00:19:46,800 --> 00:19:51,360
But we don't yet feel the need to go there yet,

268
00:19:51,360 --> 00:19:52,760
but maybe in the future.

269
00:19:52,760 --> 00:19:55,800
So that's probably the, I would say,

270
00:19:55,800 --> 00:19:58,040
in terms of overall hardware space,

271
00:19:58,040 --> 00:20:01,200
that's probably the closest to, from a technology

272
00:20:01,200 --> 00:20:04,880
maturity standpoint, custom silicon for AI workloads.

273
00:20:04,880 --> 00:20:06,720
But I think that probably is not what

274
00:20:06,720 --> 00:20:09,040
we are the most interested in right now.

275
00:20:09,040 --> 00:20:12,920
And I've got to imagine if you're a significant user

276
00:20:12,920 --> 00:20:17,080
of cloud computing, you also are, to some degree,

277
00:20:17,080 --> 00:20:20,480
banking on the cloud providers, investments

278
00:20:20,480 --> 00:20:23,600
and custom silicon decreasing your costs

279
00:20:23,600 --> 00:20:25,040
and providing better performance.

280
00:20:25,040 --> 00:20:27,040
So maybe it's not the thing that you

281
00:20:27,040 --> 00:20:30,000
need to spend your efforts for focused on.

282
00:20:30,000 --> 00:20:32,960
Spot on, again, absolutely right.

283
00:20:32,960 --> 00:20:35,200
I think for, especially for quantum,

284
00:20:35,200 --> 00:20:37,480
right, like again, as a case in point,

285
00:20:37,480 --> 00:20:41,640
I think with quantum, it's a kind of steep learning curve

286
00:20:41,640 --> 00:20:44,800
to understand how quantum computing works.

287
00:20:44,800 --> 00:20:47,200
And so we actually went ahead a little bit

288
00:20:47,200 --> 00:20:49,720
and because it's a long lead time activity,

289
00:20:49,720 --> 00:20:51,640
we decided to do some work by ourselves.

290
00:20:51,640 --> 00:20:53,680
But as you said, maybe in a few years' time,

291
00:20:53,680 --> 00:20:59,640
quantum computing and all these graph and other hardware

292
00:20:59,640 --> 00:21:04,720
acceleration silicon might all be abstracted away

293
00:21:04,720 --> 00:21:05,600
by cloud providers.

294
00:21:05,600 --> 00:21:07,360
And we kind of don't have to worry

295
00:21:07,360 --> 00:21:11,080
about all of the early technology maturity issues

296
00:21:11,080 --> 00:21:15,000
that otherwise, you know, that you face,

297
00:21:15,000 --> 00:21:17,480
typically when you go down to the hardware layer.

298
00:21:17,480 --> 00:21:19,640
So absolutely, yeah.

299
00:21:19,640 --> 00:21:23,600
What was that second area that you mentioned?

300
00:21:23,600 --> 00:21:24,720
Was that algorithms?

301
00:21:24,720 --> 00:21:26,920
Yeah, algorithms, yeah.

302
00:21:26,920 --> 00:21:30,600
So in the algorithms bucket, Sam,

303
00:21:30,600 --> 00:21:33,400
I think we have, as you can imagine,

304
00:21:33,400 --> 00:21:35,560
the sets of things that we are doing here

305
00:21:35,560 --> 00:21:38,680
are pretty very familiar to your audience,

306
00:21:38,680 --> 00:21:40,560
things around representation learning,

307
00:21:40,560 --> 00:21:44,560
sequence to label learning, transformers.

308
00:21:44,560 --> 00:21:47,040
How do we use these in our most complex use cases?

309
00:21:47,040 --> 00:21:49,960
So this is one area of focus for my team.

310
00:21:49,960 --> 00:21:52,520
Another, like, I would say,

311
00:21:52,520 --> 00:21:55,440
increasing area of focus for the team is,

312
00:21:55,440 --> 00:21:58,440
you know, around causal ML, causal inference.

313
00:22:01,440 --> 00:22:05,880
I think this is, there are two primary use cases here.

314
00:22:05,880 --> 00:22:09,600
One is many problems that we encountered

315
00:22:09,600 --> 00:22:11,920
typically in the marketing area.

316
00:22:11,920 --> 00:22:13,400
You know, they are more causal in nature

317
00:22:13,400 --> 00:22:15,560
where you're trying to influence behavior

318
00:22:15,560 --> 00:22:19,920
of customers and merchants by taking certain actions.

319
00:22:19,920 --> 00:22:21,880
So we believe that instead of looking

320
00:22:21,880 --> 00:22:24,640
at these problems as predictive problems,

321
00:22:24,640 --> 00:22:27,440
they are much better formulated as, you know,

322
00:22:27,440 --> 00:22:30,560
treatment and effect kind of problems, right?

323
00:22:30,560 --> 00:22:33,520
So we're doing some early research here,

324
00:22:33,520 --> 00:22:37,320
partnering with some teams to, you know, to figure out

325
00:22:37,320 --> 00:22:40,960
if there is, you know, an improvement in performance

326
00:22:40,960 --> 00:22:43,800
by changing our approach to those problems.

327
00:22:43,800 --> 00:22:47,480
The other interesting application of causal ML is,

328
00:22:47,480 --> 00:22:52,000
in what we believe is essentially identifying causal features,

329
00:22:52,000 --> 00:22:55,800
right, in going back to that feature selection problem, right?

330
00:22:55,800 --> 00:22:59,320
A lot of today's feature selection problems

331
00:22:59,320 --> 00:23:02,400
fundamentally are rooted in correlation, right?

332
00:23:02,400 --> 00:23:05,040
You're trying to understand how features are correlated

333
00:23:05,040 --> 00:23:06,040
with labels.

334
00:23:07,240 --> 00:23:11,440
But I think with causal ML, there are approaches,

335
00:23:11,440 --> 00:23:15,000
which have actually been shown

336
00:23:15,000 --> 00:23:20,000
where you can try to learn more causative features

337
00:23:20,000 --> 00:23:22,560
for why a certain thing might be happening,

338
00:23:22,560 --> 00:23:27,560
which in turn helps with making our models more robust, right?

339
00:23:27,760 --> 00:23:31,640
Because to our earlier discussion that we were having

340
00:23:31,640 --> 00:23:33,760
around, you know, the fact that, you know,

341
00:23:33,760 --> 00:23:36,400
data distribution is drifting and, you know,

342
00:23:36,400 --> 00:23:38,720
as the data distribution is drifting,

343
00:23:38,720 --> 00:23:40,520
so is the model score, right?

344
00:23:40,520 --> 00:23:44,400
But if we can fundamentally identify features,

345
00:23:44,400 --> 00:23:49,080
which are causal in nature, we expect the model's output

346
00:23:49,080 --> 00:23:52,960
to be more robust to this kind of noise, right?

347
00:23:52,960 --> 00:23:55,720
So this is another area where we feel causal ML could play

348
00:23:55,720 --> 00:23:58,960
a big role, and it should help, for example,

349
00:23:58,960 --> 00:24:02,640
with keeping, you know, our performance of our models

350
00:24:02,640 --> 00:24:07,080
table over longer periods of time, then it is today, for example.

351
00:24:07,080 --> 00:24:10,840
Are there any references you can point us to on the causal

352
00:24:10,840 --> 00:24:12,080
features work?

353
00:24:12,080 --> 00:24:14,840
Yeah, there is some prior work done by Professor Susan

354
00:24:14,840 --> 00:24:20,120
Atty at Stanford, you know, who actually we had done a pilot

355
00:24:20,120 --> 00:24:24,920
with her team back in 2020 on this area.

356
00:24:24,920 --> 00:24:27,920
But again, that was one single pilot

357
00:24:27,920 --> 00:24:30,160
with one single use case, so we are continuing

358
00:24:30,160 --> 00:24:33,960
to explore other areas and other use cases.

359
00:24:33,960 --> 00:24:39,520
But the basic idea is how do you identify causal features

360
00:24:39,520 --> 00:24:42,200
using some causal ML techniques?

361
00:24:42,200 --> 00:24:44,640
So moving on, maybe to the third area.

362
00:24:44,640 --> 00:24:49,240
I imagine you didn't mention it, but I imagine

363
00:24:49,240 --> 00:24:54,320
under algorithms you're looking at various graph machine

364
00:24:54,320 --> 00:24:55,960
learning techniques as well.

365
00:24:55,960 --> 00:24:57,240
Absolutely, absolutely.

366
00:24:57,240 --> 00:24:59,520
We are looking at graph machine learning.

367
00:24:59,520 --> 00:25:03,240
It's a big area of research for us.

368
00:25:03,240 --> 00:25:08,200
This is where we are, you know, using GCNs to learn embeddings

369
00:25:08,200 --> 00:25:12,200
for several kinds of use cases, like collusion detection,

370
00:25:12,200 --> 00:25:15,520
like one form of fraud is collusion, where the buyer

371
00:25:15,520 --> 00:25:20,880
and the seller collude to essentially gain the system.

372
00:25:20,880 --> 00:25:24,920
So in these kinds of use cases, we have found

373
00:25:24,920 --> 00:25:28,480
that, you know, GCN based embeddings can actually really

374
00:25:28,480 --> 00:25:31,400
help improve the performance, you know,

375
00:25:31,400 --> 00:25:33,920
to catch this kind of fraud.

376
00:25:33,920 --> 00:25:41,280
But going beyond that, right, there is graph is such a rich

377
00:25:41,280 --> 00:25:49,120
representation of our user interaction data, and coupled

378
00:25:49,120 --> 00:25:52,200
that with the fact that graph ML itself is evolving so rapidly

379
00:25:52,200 --> 00:25:56,720
right now in the academia, there's so many great papers

380
00:25:56,720 --> 00:25:58,880
and research is coming out in this area,

381
00:25:58,880 --> 00:26:01,720
that it's a continuous area of exploration for us.

382
00:26:01,720 --> 00:26:06,200
How do we is evolving really quickly?

383
00:26:06,200 --> 00:26:10,480
So we are exploring it in several context,

384
00:26:10,480 --> 00:26:12,600
be fraud detection.

385
00:26:12,600 --> 00:26:16,880
We can also, you know, another use case

386
00:26:16,880 --> 00:26:18,800
that comes to mind is compliance, as I said,

387
00:26:18,800 --> 00:26:23,480
like because there are very, very, when you look at how

388
00:26:23,480 --> 00:26:28,440
money laundering occurs on a graph, it is almost,

389
00:26:28,440 --> 00:26:30,960
you'll immediately see how you can actually use graphs

390
00:26:30,960 --> 00:26:32,760
to catch it, right.

391
00:26:32,760 --> 00:26:35,320
But the problem with graph, which I was

392
00:26:35,320 --> 00:26:38,840
alluding to earlier, is the scale of the graph

393
00:26:38,840 --> 00:26:39,800
that we have at PayPal.

394
00:26:39,800 --> 00:26:44,800
We have like millions, you know, millions of customers, right?

395
00:26:44,800 --> 00:26:48,720
And like billions of transactions happening every year.

396
00:26:48,720 --> 00:26:52,360
So to model all of that, to keep that graph up to date,

397
00:26:52,360 --> 00:26:56,960
to be able to train, you know, models on this kind

398
00:26:56,960 --> 00:27:02,360
of evolving graph and so on, it's a very challenging problem.

399
00:27:02,360 --> 00:27:03,800
So that's one part of it.

400
00:27:03,800 --> 00:27:06,440
And then at the algorithmic layer graph also,

401
00:27:06,440 --> 00:27:09,160
as I keep saying, a lot of interesting work happening

402
00:27:09,160 --> 00:27:10,080
in this area.

403
00:27:10,080 --> 00:27:13,280
So we are doing our own pilots in this area.

404
00:27:13,280 --> 00:27:19,000
And just maybe taking a step back, do you

405
00:27:19,000 --> 00:27:25,440
think about kind of applied R&D in the context of algorithms

406
00:27:25,440 --> 00:27:30,800
as taking the stuff that the Stanford's and Google's

407
00:27:30,800 --> 00:27:33,640
and Facebook's are doing from a research

408
00:27:33,640 --> 00:27:35,760
and academic publishing perspective

409
00:27:35,760 --> 00:27:40,800
and trying to understand how they fit into your problem domains?

410
00:27:40,800 --> 00:27:46,160
Or are you also doing academic style research

411
00:27:46,160 --> 00:27:48,560
into the problems that interest you?

412
00:27:48,560 --> 00:27:53,160
Yeah, I think it's more of the former in the sense

413
00:27:53,160 --> 00:27:54,480
we had an applied R&D group.

414
00:27:54,480 --> 00:27:59,200
So we are not focused on kind of fundamental research.

415
00:27:59,200 --> 00:28:02,640
I would call it, we are not, our goal

416
00:28:02,640 --> 00:28:08,720
is not to, let's say, come up with novel problems

417
00:28:08,720 --> 00:28:11,840
and publish them to conferences and journals.

418
00:28:11,840 --> 00:28:14,520
It's definitely an applied R&D function.

419
00:28:14,520 --> 00:28:19,040
In terms of how we, to refresh your question,

420
00:28:19,040 --> 00:28:21,720
how do we kind of through our backlog?

421
00:28:21,720 --> 00:28:24,440
How do we decide what to work on and what not to work on?

422
00:28:24,440 --> 00:28:26,480
It's a combination of things.

423
00:28:26,480 --> 00:28:30,280
I think we're definitely monitoring what the Google's

424
00:28:30,280 --> 00:28:32,000
and the Facebook's are doing.

425
00:28:32,000 --> 00:28:34,600
We are also monitoring conferences,

426
00:28:34,600 --> 00:28:40,520
like top tier conferences, like ICML, ICLR and so on.

427
00:28:40,520 --> 00:28:45,480
And trying to, I think some of the differences

428
00:28:45,480 --> 00:28:48,960
that we see from academia, when you look at everything

429
00:28:48,960 --> 00:28:53,080
from an academia lens versus a applied R&D lens

430
00:28:53,080 --> 00:28:57,160
is, for example, number one, like a lot of AIML research

431
00:28:57,160 --> 00:29:02,960
is focused on NLP, CV, unstructured, kind of,

432
00:29:02,960 --> 00:29:06,920
I would say data, versus R is focused on,

433
00:29:06,920 --> 00:29:09,160
I would say, some portion of it's structured,

434
00:29:09,160 --> 00:29:12,440
some portion of it unstructured.

435
00:29:12,440 --> 00:29:20,200
And I think the other thing is financial services space,

436
00:29:20,200 --> 00:29:24,360
we have our own unique, hard, I would say,

437
00:29:24,360 --> 00:29:27,240
attributes to our data sets, which are not, like,

438
00:29:27,240 --> 00:29:30,320
typically found in kind of academic conferences.

439
00:29:30,320 --> 00:29:33,800
So one of the key things that we and my team do

440
00:29:33,800 --> 00:29:35,800
is really connecting those dots, right?

441
00:29:35,800 --> 00:29:39,280
So you will see a lot of things that show up in CV

442
00:29:39,280 --> 00:29:43,480
or in NLP land, but the key is like to identify,

443
00:29:43,480 --> 00:29:46,960
how do you map, you know, let's say the problem

444
00:29:46,960 --> 00:29:49,680
and the solution to our domain, right?

445
00:29:49,680 --> 00:29:52,960
And to see if there is some potential, right?

446
00:29:52,960 --> 00:29:55,360
It sounds very easy to say, but it's actually

447
00:29:55,360 --> 00:29:56,920
very hard to do in practice, right?

448
00:29:56,920 --> 00:30:02,400
So, but I think this is really the key,

449
00:30:02,400 --> 00:30:04,600
I would say, role that we play in addition

450
00:30:04,600 --> 00:30:08,920
to running the pilots and doing the R&D itself.

451
00:30:08,920 --> 00:30:12,360
I think the key part is ideating and understanding

452
00:30:12,360 --> 00:30:17,840
what trends are happening in these related,

453
00:30:17,840 --> 00:30:20,760
what separate domains and how do we kind of like map it

454
00:30:20,760 --> 00:30:22,600
to our domain, right?

455
00:30:22,600 --> 00:30:26,080
That is the real secret sauce, I think,

456
00:30:26,080 --> 00:30:27,520
to our group and what we do.

457
00:30:27,520 --> 00:30:29,440
So the third area?

458
00:30:29,440 --> 00:30:31,600
Yeah, the third area is kind of more

459
00:30:31,600 --> 00:30:34,520
on the application side, as I mentioned.

460
00:30:34,520 --> 00:30:39,120
So this is primarily, this is where I think, you know,

461
00:30:39,120 --> 00:30:44,120
most of our focus area here is on the responsible AI side.

462
00:30:44,120 --> 00:30:50,040
So things around explainability, fairness, privacy,

463
00:30:50,040 --> 00:30:52,040
adversarial learning, you know,

464
00:30:52,040 --> 00:30:57,240
how do we essentially look at these problems

465
00:30:57,240 --> 00:31:00,840
and solve them for our customers, basically,

466
00:31:00,840 --> 00:31:02,040
ultimately for our customers,

467
00:31:02,040 --> 00:31:04,920
but also, you know, you know,

468
00:31:04,920 --> 00:31:07,240
make sure that we are complying with regulations

469
00:31:07,240 --> 00:31:12,400
and laws and we have a pretty, I think,

470
00:31:12,400 --> 00:31:15,720
in the last year or two, we've actually taken steps

471
00:31:15,720 --> 00:31:21,200
at PayPal to make sure that we really are, you know,

472
00:31:21,200 --> 00:31:25,960
building our models in a very responsible manner.

473
00:31:25,960 --> 00:31:30,280
You know, we have governance and I would say processes

474
00:31:30,280 --> 00:31:33,400
in place to make sure that as and, you know,

475
00:31:33,400 --> 00:31:35,960
as we build models, we are paying close attention

476
00:31:35,960 --> 00:31:37,600
to how do we make them explainable?

477
00:31:37,600 --> 00:31:40,960
How do we ascertain that they are fair?

478
00:31:40,960 --> 00:31:43,200
In some areas, for example,

479
00:31:43,200 --> 00:31:45,720
we are already doing it because it's required by law,

480
00:31:45,720 --> 00:31:48,240
but we are also now starting to think about all the new areas

481
00:31:48,240 --> 00:31:51,640
where we need to start paying attention to all of these things.

482
00:31:51,640 --> 00:31:54,760
I already mentioned a little bit about privacy.

483
00:31:54,760 --> 00:31:57,720
And then finally, on the security side,

484
00:31:57,720 --> 00:32:02,720
we are starting to really look at adversarial learning

485
00:32:02,720 --> 00:32:05,680
as, you know, how do we, for example,

486
00:32:05,680 --> 00:32:07,200
in a front detection context,

487
00:32:07,200 --> 00:32:09,520
how do we use adversarial learning

488
00:32:09,520 --> 00:32:11,480
to make our models even more robust?

489
00:32:13,240 --> 00:32:18,120
So ultimately, I think our customers interact

490
00:32:18,120 --> 00:32:19,560
with us through the application layer.

491
00:32:19,560 --> 00:32:22,480
And so we really need to make sure that everything

492
00:32:22,480 --> 00:32:25,520
that we are doing as a customer sees it

493
00:32:25,520 --> 00:32:27,120
is done in a responsible manner,

494
00:32:27,120 --> 00:32:28,880
which complies with law and so on.

495
00:32:28,880 --> 00:32:33,360
So this is an area that my team is heavily invested in.

496
00:32:35,320 --> 00:32:37,960
I would say that would probably be the number one.

497
00:32:37,960 --> 00:32:41,040
On the second area here is probably more

498
00:32:41,040 --> 00:32:43,720
around the conversational AI side,

499
00:32:43,720 --> 00:32:45,600
you know, where we want to build,

500
00:32:46,800 --> 00:32:51,800
for example, chat parts, which basically delight our customers

501
00:32:53,400 --> 00:32:57,040
in terms of giving them a very good experience.

502
00:32:58,200 --> 00:33:03,200
And so here, there is some R&D happening within my team

503
00:33:03,360 --> 00:33:06,600
where we are trying to look at how do we use techniques

504
00:33:06,600 --> 00:33:08,920
and reinforcement learning, you know,

505
00:33:08,920 --> 00:33:10,720
to essentially train a chat part

506
00:33:10,720 --> 00:33:13,960
with all the previous chat logs that we have

507
00:33:13,960 --> 00:33:17,680
of our customers with our agents, for example, right?

508
00:33:17,680 --> 00:33:22,000
So that's a very interesting area of research for us as well.

509
00:33:22,000 --> 00:33:25,840
And the fourth area was around tools

510
00:33:25,840 --> 00:33:30,160
and ML ops platform technologies, that kind of thing.

511
00:33:30,160 --> 00:33:33,440
Exactly. So I think that actually is the glue

512
00:33:33,440 --> 00:33:36,440
that eventually tries all these three layers

513
00:33:36,440 --> 00:33:37,280
on the top.

514
00:33:37,280 --> 00:33:42,200
So definitely ML ops, interoperability platforms,

515
00:33:42,200 --> 00:33:45,920
how do we make sure that both on the deployment side

516
00:33:45,920 --> 00:33:47,200
as well as on the training side,

517
00:33:47,200 --> 00:33:50,920
we have an ecosystem that you can train

518
00:33:50,920 --> 00:33:55,520
in any framework of your choice, deploy on any kind of device,

519
00:33:55,520 --> 00:33:59,960
be it like a phone or a browser, right?

520
00:33:59,960 --> 00:34:01,920
So these kinds of things are also,

521
00:34:01,920 --> 00:34:04,920
we have various pilots ongoing in this area as well.

522
00:34:04,920 --> 00:34:07,000
And then the big one is ML ops, of course,

523
00:34:07,000 --> 00:34:12,000
which we have been on our own journey, I guess,

524
00:34:13,600 --> 00:34:18,600
but this is basically the place where we try to automate

525
00:34:18,840 --> 00:34:23,680
as many of the steps involved in model development

526
00:34:23,680 --> 00:34:27,160
and delivery to the extent possible, right?

527
00:34:27,160 --> 00:34:28,640
Like that's the ultimate goal.

528
00:34:28,640 --> 00:34:33,640
So this helps, as you said, with, you know,

529
00:34:35,240 --> 00:34:38,160
making our development cycles shorter

530
00:34:38,160 --> 00:34:41,200
and basically increasing deployment velocity, right?

531
00:34:41,200 --> 00:34:46,200
So this is the primary objective of the ML ops area.

532
00:34:47,840 --> 00:34:52,840
And is ML ops from your perspective

533
00:34:52,840 --> 00:34:57,840
from your perspective, is it still applied R&D into ML ops,

534
00:34:59,600 --> 00:35:03,240
meaning looking at kind of the, you know,

535
00:35:03,240 --> 00:35:05,800
have you characterized the time horizon, you know,

536
00:35:05,800 --> 00:35:07,680
hey, what are we gonna be doing in ML ops

537
00:35:07,680 --> 00:35:09,840
three to five years from now?

538
00:35:09,840 --> 00:35:11,920
And then exploring those things

539
00:35:11,920 --> 00:35:15,480
and transitioning them to other internal teams

540
00:35:15,480 --> 00:35:18,000
that are focused on, you know, one of the three years

541
00:35:18,000 --> 00:35:22,880
or this year is that ML ops component,

542
00:35:22,880 --> 00:35:24,800
like you have all this kind of forward looking stuff

543
00:35:24,800 --> 00:35:26,920
and you're also responsible for the platform

544
00:35:26,920 --> 00:35:29,040
that your teams use today in production.

545
00:35:29,040 --> 00:35:30,400
It's a great question.

546
00:35:30,400 --> 00:35:32,800
I think I would say it's a combination of both.

547
00:35:32,800 --> 00:35:36,000
I think there is some part of that team's bandwidth

548
00:35:36,000 --> 00:35:39,920
that goes into more forward looking R&D type of stuff.

549
00:35:39,920 --> 00:35:41,440
So collecting back, for example,

550
00:35:41,440 --> 00:35:45,560
to our first conversation on, you know, unsupervised

551
00:35:45,560 --> 00:35:49,440
versus semi-supervised approaches to, you know,

552
00:35:49,440 --> 00:35:50,360
for fraud detection.

553
00:35:50,360 --> 00:35:54,000
So that team, for example, will look into something like that

554
00:35:54,000 --> 00:35:57,920
because ultimately it's a way for us to improve,

555
00:35:58,880 --> 00:36:03,880
it's a way for us to automate the process of a model refresh

556
00:36:04,720 --> 00:36:08,480
by looking at a technique like semi-supervised learning, right?

557
00:36:08,480 --> 00:36:11,640
That's the R&D side, but as you can imagine,

558
00:36:11,640 --> 00:36:16,640
this is also a very kind of product and execution heavy function.

559
00:36:16,960 --> 00:36:19,880
So I would say 50% of the bandwidth goes in kind of like

560
00:36:19,880 --> 00:36:23,760
the day-to-day, you know, building off of the pipelines

561
00:36:23,760 --> 00:36:27,120
onboarding, you know, new models and customers

562
00:36:27,120 --> 00:36:29,920
onto the ML ops rails and so on.

563
00:36:31,240 --> 00:36:33,960
But and actually, this was also,

564
00:36:33,960 --> 00:36:37,480
by the way, the reason why this function, you know,

565
00:36:37,480 --> 00:36:39,480
was synergized with the R&D group

566
00:36:39,480 --> 00:36:42,640
because we already see that there is,

567
00:36:42,640 --> 00:36:45,400
there's plenty of synergy in terms of the fact

568
00:36:45,400 --> 00:36:48,760
that some of the work happening in this group is R&D nature.

569
00:36:49,760 --> 00:36:54,280
And so, you know, it makes, it just made a lot of sense

570
00:36:54,280 --> 00:36:59,280
for me to actually, you know, basically look at both

571
00:36:59,280 --> 00:37:01,720
the R&D side of the house as well as the ML ops R&D side

572
00:37:01,720 --> 00:37:03,800
of the house so that we can even synergize more.

573
00:37:06,600 --> 00:37:08,960
Well, I feel like we need a whole separate conversation

574
00:37:08,960 --> 00:37:12,640
just to dig into the details around your platform

575
00:37:12,640 --> 00:37:15,760
and tooling choices and the way you think about

576
00:37:15,760 --> 00:37:18,880
automating the machine learning workflow.

577
00:37:18,880 --> 00:37:19,880
I know.

578
00:37:19,880 --> 00:37:22,480
We didn't even get to it.

579
00:37:22,480 --> 00:37:23,520
Even get to it.

580
00:37:23,520 --> 00:37:26,720
That said, it has been wonderful, you know,

581
00:37:26,720 --> 00:37:30,240
exploring the way you're thinking about applied research

582
00:37:30,240 --> 00:37:34,640
and development at PayPal and some of the use cases

583
00:37:34,640 --> 00:37:36,320
that you're looking at and the things

584
00:37:36,320 --> 00:37:38,160
that you're excited about.

585
00:37:38,160 --> 00:37:40,720
Even there would be great to go into a lot more detail.

586
00:37:40,720 --> 00:37:43,120
So we'll have to make sure to, you know,

587
00:37:43,120 --> 00:37:45,880
stay in touch and have some fall on conversations.

588
00:37:45,880 --> 00:37:46,720
Thank you, Sam.

589
00:37:46,720 --> 00:38:16,680
It was great to be on the show.

