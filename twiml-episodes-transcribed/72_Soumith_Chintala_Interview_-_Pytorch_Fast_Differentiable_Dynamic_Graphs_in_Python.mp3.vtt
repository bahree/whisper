WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people, doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.120
I'm your host Sam Charrington.

00:23.120 --> 00:27.960
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

00:27.960 --> 00:30.760
to Kevin T from SIGUP for presenting.

00:30.760 --> 00:34.840
You can find the slides for his presentation in the Meetup Slack Channel, as well as

00:34.840 --> 00:36.880
in this week's show notes.

00:36.880 --> 00:41.280
Our final Meetup of the Year will be held on Wednesday, December 13th.

00:41.280 --> 00:46.480
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

00:46.480 --> 00:48.640
for our discussion segment.

00:48.640 --> 00:54.280
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

00:54.280 --> 01:00.760
the paper understanding deep learning requires rethinking generalization, by Shi Huang Zhang

01:00.760 --> 01:04.120
from MIT and Google Brain and others.

01:04.120 --> 01:09.240
You can find more details and register at twimbleia.com slash Meetup.

01:09.240 --> 01:13.560
If you receive my newsletter, you already know this, but Twimble is growing and we're

01:13.560 --> 01:19.280
looking for an energetic and passionate community manager to help expand our programs.

01:19.280 --> 01:23.680
This position can be remote, but if you happen to be in St. Louis, all the better.

01:23.680 --> 01:27.400
If you're interested, please reach out to me for additional details.

01:27.400 --> 01:31.360
I should mention that if you don't already get my newsletter, you are really missing

01:31.360 --> 01:36.600
out and should visit twimbleia.com slash newsletter to sign up.

01:36.600 --> 01:42.320
Now the show you are about to hear is part of our Strange Loop 2017 series, brought to

01:42.320 --> 01:45.160
you by our friends at Nexusos.

01:45.160 --> 01:50.080
Nexusos is a company focused on making machine learning more easily accessible to enterprise

01:50.080 --> 01:51.080
developers.

01:51.080 --> 01:55.600
Their machine learning API meets developers where they're at, regardless of their mastery

01:55.600 --> 02:00.840
of data science, so they can start cutting up predictive applications immediately and

02:00.840 --> 02:03.680
in their preferred programming language.

02:03.680 --> 02:08.080
It's as simple as loading your data and selecting the type of problem you want to solve.

02:08.080 --> 02:12.920
Their automated platform trains and selects the best model fit for your data and then outputs

02:12.920 --> 02:14.520
predictions.

02:14.520 --> 02:18.840
To learn more about Nexusos, be sure to check out the first episode in this series at

02:18.840 --> 02:27.080
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

02:27.080 --> 02:32.520
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

02:32.520 --> 02:37.680
learning in your next project at nexosos.com slash twimble.

02:37.680 --> 02:43.000
In this show, I speak with Sumit Shintala, a research engineer in the Facebook AI Research

02:43.000 --> 02:44.320
Lab.

02:44.320 --> 02:48.800
Sumith joined me at Strange Loop before his talk on PyTorch, the deep learning framework.

02:48.800 --> 02:53.360
In this conversation, we discussed the market evolution of deep learning frameworks, different

02:53.360 --> 02:58.240
approaches to programming deep learning frameworks, Facebook's motivations for investing in

02:58.240 --> 03:00.200
PyTorch and much more.

03:00.200 --> 03:03.320
This was a fun interview and I hope you enjoy it.

03:03.320 --> 03:06.320
And now on to the show.

03:06.320 --> 03:18.280
Hey everyone, I am here at the Strange Loop Conference in St. Louis and I'm with Sumith

03:18.280 --> 03:24.640
Shintala, who is a research engineer at Fair, the Facebook AI Research Lab.

03:24.640 --> 03:30.280
And Sumith is giving a talk here at the conference tomorrow and graciously agreed to spend some

03:30.280 --> 03:34.760
time with us to talk a little bit about what he's up to and about its talk.

03:34.760 --> 03:35.760
So welcome, Sumith.

03:35.760 --> 03:39.760
Hi Sam, nice to be here at Strange Loop for the first time.

03:39.760 --> 03:42.760
Absolutely, absolutely, and welcome to St. Louis.

03:42.760 --> 03:46.280
So why don't we get started by having you tell us a little bit about your background

03:46.280 --> 03:48.760
and how you got into AI research?

03:48.760 --> 03:49.760
Sure.

03:49.760 --> 03:50.760
Let's see.

03:50.760 --> 03:58.800
About eight years ago, I wanted to be a digital artist trying to make VFX for movies and

03:58.800 --> 03:59.800
stuff.

03:59.800 --> 04:06.960
I didn't internship there and as soon as I went into that internship a weekend, I realized

04:06.960 --> 04:12.760
that I was terrible at this, I was not an artist by any standards.

04:12.760 --> 04:18.040
And then I had to find second choices in life and I was looking at my interests and one

04:18.040 --> 04:23.560
of the things that struck to me was I was a decent programmer since I was a young age.

04:23.560 --> 04:29.880
And I kind of liked the whole computer vision, angle, object detection, you know, these

04:29.880 --> 04:36.200
show picture and some machine shows tells you, oh, there's a person here, there's a cat

04:36.200 --> 04:37.960
here, that just fascinated me.

04:37.960 --> 04:44.840
So I went down that line, I did a small internship at a research lab in India at this place

04:44.840 --> 04:52.400
called Triple IT and like there I did a little bit of random stuff, you know, as an undergrad

04:52.400 --> 04:58.080
you explore all kinds of things and then I tried a little bit of face detection and a

04:58.080 --> 05:03.520
little bit of taking a bunch of pictures of a monument and then stitching them together

05:03.520 --> 05:04.520
in 3D.

05:04.520 --> 05:05.520
Okay.

05:05.520 --> 05:07.240
And I had no idea what I was doing.

05:07.240 --> 05:12.880
I was just like trying to put together things based on various tools and snippets and

05:12.880 --> 05:18.680
you know how you call programmers stack over flow bots, you just take snippets and you're

05:18.680 --> 05:26.040
trying to put together something and I was guilty as charged and then I got an opportunity

05:26.040 --> 05:33.120
to come to CMU and Pittsburgh just doing more exploration, trying to figure out what

05:33.120 --> 05:34.720
I want to do.

05:34.720 --> 05:38.160
Here I got to play robot soccer.

05:38.160 --> 05:45.040
So program robots to play soccer autonomously, autonomously, so you basically like flashed

05:45.040 --> 05:50.040
a robot with your program and then they just play soccer.

05:50.040 --> 05:55.160
And I've seen pictures of a variety of scenarios of these, the humanoid ones.

05:55.160 --> 05:58.840
Yes, the ones I worked on were these humanoid ones.

05:58.840 --> 05:59.840
Okay.

05:59.840 --> 06:07.680
They were called Naoki robots and they were cute, and they were, I mean, I came in with

06:07.680 --> 06:13.040
the expectation that we figured out so much in robotics, we must be pretty good.

06:13.040 --> 06:18.680
And we couldn't make them stand properly, like they would just stand, they would walk

06:18.680 --> 06:21.960
and they would fall down and that's the state of the art.

06:21.960 --> 06:23.960
If you could make them walk without falling down.

06:23.960 --> 06:28.480
We've probably all seen the video, the Boston robotics robots that like tries to turn

06:28.480 --> 06:32.000
the door, the door not pan, and then just fall over totally.

06:32.000 --> 06:33.760
I saw that one.

06:33.760 --> 06:40.800
So that like shocked me because I was like, oh, robotics was so much more advanced than

06:40.800 --> 06:42.200
it is.

06:42.200 --> 06:48.840
And it also, I saw opportunity there, I'm like, oh, this film is still kind of open.

06:48.840 --> 06:53.960
And we were trying to do the whole algorithm of the robot playing soccer with vision.

06:53.960 --> 06:58.440
Like, oh, can you identify where the ball is and walk towards it and stuff.

06:58.440 --> 07:03.760
And that part as well was very, very rudimentary, not working very well.

07:03.760 --> 07:12.280
It would sort of look for an orange pixel in your image and try to make that pixel bigger.

07:12.280 --> 07:16.600
And even something as stupid as that, it wasn't working very well.

07:16.600 --> 07:17.760
That's pretty funny.

07:17.760 --> 07:26.360
So I did a bunch of random stuff there and I had a good time at CME and I decided I want

07:26.360 --> 07:32.480
to go into artificial intelligence, computer vision, robotics.

07:32.480 --> 07:37.600
And then I applied to a bunch of places, CME being one of them and I didn't get accepted

07:37.600 --> 07:38.600
anywhere.

07:38.600 --> 07:45.000
And I was looking for late applications and stuff and that also combined with someone

07:45.000 --> 07:48.440
who there, like where there was computer vision.

07:48.440 --> 07:55.360
And I saw this web page by this guy called Jan LeCouven and he had a like, janky web

07:55.360 --> 08:03.320
page with some object detection stuff going on, but I was like, hey, it's NYU, I'll give

08:03.320 --> 08:04.320
it a shot.

08:04.320 --> 08:09.600
So I applied to NYU and a couple of other places last minute because I got rejected from

08:09.600 --> 08:12.800
all of my top schools I wanted to go to.

08:12.800 --> 08:15.000
No offense, Jan.

08:15.000 --> 08:18.160
I told them the story right away.

08:18.160 --> 08:24.000
And I got accepted at NYU, I wasn't super happy with myself because I was like, oh,

08:24.000 --> 08:28.080
I can do so much better if I worked harder.

08:28.080 --> 08:30.080
And then it gets even more hilarious.

08:30.080 --> 08:35.080
I come in to NYU, I emailed Jan before and I'm like, hey, I did a little bit of work

08:35.080 --> 08:37.760
here and they're at CMU and Triple ID.

08:37.760 --> 08:43.440
I want to work with you, try to do more object detection research and he replied immediately.

08:43.440 --> 08:47.640
He was like, hey, let's meet, once you get here, let's meet this day at this time.

08:47.640 --> 08:53.960
I go, Jan's in his office and it's like, hey, listen, do you know anything about

08:53.960 --> 08:54.960
neural networks?

08:54.960 --> 08:55.960
You know what?

08:55.960 --> 08:56.960
I do my kind of research.

08:56.960 --> 09:04.000
I'm like, nope, I only heard the term neural networks at once and I have no idea what

09:04.000 --> 09:05.000
you're doing.

09:05.000 --> 09:09.720
And so he went on to explain to me how neural networks work.

09:09.720 --> 09:14.960
This was in 2010, neural networks weren't hot and Jan Likun had a lot of time on his

09:14.960 --> 09:15.960
head.

09:15.960 --> 09:21.080
So that relationship went well, he introduced me to one of his PhD students, Pierre

09:21.080 --> 09:27.640
Cervine, who was now at Google, and Pierre and I, I was Pierre's like understudy.

09:27.640 --> 09:33.480
I worked on many things there, like I implemented neural networks, we built like Pierre built

09:33.480 --> 09:39.280
this deep learning framework called eBLearn and I was kind of helping out on that and that

09:39.280 --> 09:44.720
made me understand more about how neural networks work also got me stronger on the engineering

09:44.720 --> 09:46.200
side of things.

09:46.200 --> 09:53.000
That's roughly how I entered the field in my two years at NYU, we published one paper

09:53.000 --> 09:58.920
and another paper on the work we did got published in another conference.

09:58.920 --> 10:03.800
And then in 2012 May, I graduated, I couldn't find a job in deep learning.

10:03.800 --> 10:04.800
Wow.

10:04.800 --> 10:11.080
2012 December was when the whole deep learning boom started.

10:11.080 --> 10:16.920
So 2012 May, I was going to go accept a job at Amazon as a test engineer.

10:16.920 --> 10:17.920
Wow.

10:17.920 --> 10:19.920
You're like, why did I waste the last two years of my life?

10:19.920 --> 10:21.760
It was just so frustrating.

10:21.760 --> 10:25.360
I was trying not to give up, I was still super interested in the field, but you know,

10:25.360 --> 10:27.360
you have practical constraints, right?

10:27.360 --> 10:32.320
Like you're working at that, you need to think of all these things.

10:32.320 --> 10:39.240
So in the last minute, I think I had to accept the Amazon offer by Saturday and on Wednesday

10:39.240 --> 10:46.800
Jan, like, and Jan, at that day, I don't even remember why I met Jan the day, he was like,

10:46.800 --> 10:47.800
oh, where are you going?

10:47.800 --> 10:53.160
And I told him I couldn't find a job anywhere else and I go to Amazon and he's like, oh,

10:53.160 --> 10:57.960
just yesterday, one of the companies I co-founded got fresh funding and they're looking to

10:57.960 --> 10:58.960
hire engineers.

10:58.960 --> 10:59.960
Oh, wow.

10:59.960 --> 11:02.880
So that was a conversation that happened on Wednesday.

11:02.880 --> 11:07.120
I went and gave my interviews on Thursday in Princeton, New Jersey.

11:07.120 --> 11:14.400
And on Friday, I signed for Musami and they were doing music and deep learning on phones.

11:14.400 --> 11:15.400
Okay.

11:15.400 --> 11:18.520
So that was a company that was like, like a shazam kind of thing?

11:18.520 --> 11:24.640
No, it was basically you, you want to, if someone's playing music, you should be able

11:24.640 --> 11:31.120
to transcribe it live onto sheet and if someone takes a picture of sheet, then you have

11:31.120 --> 11:33.400
to be able to play it back for them.

11:33.400 --> 11:39.520
So it was like a full cycle, I want to hear, I want to play, so it was like a tool for

11:39.520 --> 11:40.520
musicians.

11:40.520 --> 11:41.520
Okay.

11:41.520 --> 11:42.920
Does that exist for guitar tab?

11:42.920 --> 11:43.920
Do you have any idea?

11:43.920 --> 11:44.920
There it should.

11:44.920 --> 11:45.920
And it would be awesome.

11:45.920 --> 11:54.520
They do, but it's not a solved problem to decouple, when you play multiple notes together.

11:54.520 --> 12:00.040
If you play a single note at a time, it's very easy, but if you play like five or six

12:00.040 --> 12:06.920
chords at a time, like decoupling and understanding which of those exactly maps to what you played,

12:06.920 --> 12:07.920
okay.

12:07.920 --> 12:09.640
It's still like an ongoing problem.

12:09.640 --> 12:13.560
So I spent time at Musami for a couple of years.

12:13.560 --> 12:16.680
We were building all kinds of mobile stuff.

12:16.680 --> 12:21.200
I mean, we wanted the whole thing to run on phones, so I was training new on networks

12:21.200 --> 12:27.600
on sheet music and it would be called music optical recognition or more.

12:27.600 --> 12:32.240
And then the company kind of had to fold at some point.

12:32.240 --> 12:37.400
In the meanwhile, while it was at Musami, I always started actively maintaining torch,

12:37.400 --> 12:43.320
which was the deep learning framework that was one of the bigger ones at that time.

12:43.320 --> 12:49.920
And I eventually wanted to get out of Musami because we're not sure how the business

12:49.920 --> 12:52.200
set of things was going.

12:52.200 --> 12:58.800
And then Jan started at Facebook six months before that and they were using torch as their

12:58.800 --> 13:00.040
main deep learning framework.

13:00.040 --> 13:05.840
And so they needed good engineers to maintain and develop torch.

13:05.840 --> 13:11.720
And so by the time I was joining Facebook, I was the only maintainer of torch.

13:11.720 --> 13:12.720
So it was perfect.

13:12.720 --> 13:18.040
They just got the engineer who will help like had the side of things.

13:18.040 --> 13:24.640
So I came to Facebook and there were so many smart people that I just learned so much from.

13:24.640 --> 13:27.440
I was also interested in research.

13:27.440 --> 13:33.600
And so I ended up going down this path of generative adversarial networks where we were trying

13:33.600 --> 13:36.680
to synthesize images.

13:36.680 --> 13:42.280
So the neural network kind of just synthesizes images from nothing or like a training or

13:42.280 --> 13:43.600
for what use cases.

13:43.600 --> 13:46.600
This was more of an unsupervised learning use case.

13:46.600 --> 13:51.680
So an unsupervised learning, one of the things you do is generation.

13:51.680 --> 14:00.440
And the motivation there is that if you can generate something, you have generally good concepts

14:00.440 --> 14:03.280
about how that process works.

14:03.280 --> 14:09.480
So the motivation is that if we can do a really good image generation neural network, we

14:09.480 --> 14:14.200
can take parts of that neural network and bootstrap other neural networks which are

14:14.200 --> 14:17.520
doing computer vision tasks to get better performance.

14:17.520 --> 14:22.600
So we could take parts of this neural network and then make it work on a different task

14:22.600 --> 14:26.120
like dog versus clats classification.

14:26.120 --> 14:30.440
And without having as much data, you would still get where you could accuracy.

14:30.440 --> 14:33.840
So that's the whole unsupervised semi-supervised learning motivation.

14:33.840 --> 14:39.320
I worked on a few things on the adversarial network side and then coming to...

14:39.320 --> 14:41.880
And this was back in 2012, 2013?

14:41.880 --> 14:43.880
No, 2014 I joined Facebook.

14:43.880 --> 14:44.880
Oh, got it.

14:44.880 --> 14:46.880
2014, 2015, 2016.

14:46.880 --> 14:53.040
Because the GANs in general have been more, you know, past two, three years, I think,

14:53.040 --> 14:54.880
in the year 2015, 2016.

14:54.880 --> 14:55.880
Yeah.

14:55.880 --> 15:03.240
Coming to what I'm going to talk about tomorrow, what happened was torch has been an aging

15:03.240 --> 15:04.800
design in general.

15:04.800 --> 15:09.240
It's been seven years since the previous release of torch came out.

15:09.240 --> 15:14.360
So it was becoming more flexible, you know, as the field changes, there's this concept

15:14.360 --> 15:19.200
for researchers who use the tools that they have available to them best.

15:19.200 --> 15:21.280
And they push those tools to the limit.

15:21.280 --> 15:27.360
And the new tools come that will then, again, make the researchers more flexible and exploring

15:27.360 --> 15:28.680
new things.

15:28.680 --> 15:32.320
So torch was reaching its limits of flexibility.

15:32.320 --> 15:35.920
So we wanted to develop a new tool.

15:35.920 --> 15:41.920
And so we worked on it for four years, started off as an intern project and then we kept

15:41.920 --> 15:42.920
developing it.

15:42.920 --> 15:45.240
And we released it earlier this year.

15:45.240 --> 15:47.320
It's called PyTorch.

15:47.320 --> 15:52.520
And it's what I'm going to be talking about tomorrow, it's strange, look, I'll be talking

15:52.520 --> 15:58.600
about PyTorch, how it came about, what engineering challenges we faced.

15:58.600 --> 16:05.240
PyTorch generally is a fairly slow language, but it's the most popular language for machine

16:05.240 --> 16:10.120
learning, for, you know, statistics, like all kinds of things.

16:10.120 --> 16:15.880
So the most obvious choice to build something in was PyTorch, because all of the users

16:15.880 --> 16:16.880
were familiar with it.

16:16.880 --> 16:17.880
We have a huge ecosystem.

16:17.880 --> 16:21.520
And the barrier to entry is smaller than the C++.

16:21.520 --> 16:26.480
Yeah, you have so many tutorials and it's very easy to learn and stuff.

16:26.480 --> 16:27.480
Yeah.

16:27.480 --> 16:33.680
So you had that upside, but the downside was deep learning is one of those high performance

16:33.680 --> 16:34.680
computing spaces.

16:34.680 --> 16:35.680
Right.

16:35.680 --> 16:39.080
Every, every second, every millisecond matters.

16:39.080 --> 16:46.240
But PyTorch is slow, like how do you make some packages really fast, but taking a constraint

16:46.240 --> 16:49.080
that the users want to use it from PyTorch.

16:49.080 --> 16:55.600
So generally, like how we worked around these challenges in various ways, I'm just going

16:55.600 --> 16:57.080
to talk about that.

16:57.080 --> 17:01.640
Some of the things we did was we moved the most critical parts into C. We made a large

17:01.640 --> 17:05.320
part of the implementation lock free.

17:05.320 --> 17:09.320
Well, let's make sure not to kind of breeze by these topics, because we really want to

17:09.320 --> 17:11.240
dive into some of these.

17:11.240 --> 17:12.240
Sure.

17:12.240 --> 17:16.800
But, you know, one of the things that is maybe an interesting place to start is, and I've

17:16.800 --> 17:22.000
talked about this, I think possibly on the podcast, definitely in my newsletter, just

17:22.000 --> 17:27.920
the idea that it's actually kind of interesting here in your story and how, you know, in a

17:27.920 --> 17:31.880
lot of ways, it's like all about timing and mistiming and timing windows and things like

17:31.880 --> 17:32.880
that.

17:32.880 --> 17:39.320
I think PyTorch has kind of popped up on the scene, if you will, at a time when I think

17:39.320 --> 17:44.720
a lot of people in Crown TensorFlow was like the heir apparent to the deep learning framework

17:44.720 --> 17:45.720
world.

17:45.720 --> 17:46.720
Right.

17:46.720 --> 17:52.680
And, you know, I wonder if you're just hearing your story, if like your experiences with,

17:52.680 --> 17:58.880
you know, the timing cycle of machine learning and deep learning, like if that influences

17:58.880 --> 18:04.920
your perspective on this kind of the market evolution of, you know, tools and, you know,

18:04.920 --> 18:10.400
where do you see what you see the opportunity is for PyTorch and just kind of where you

18:10.400 --> 18:11.760
think things are going.

18:11.760 --> 18:12.760
Right.

18:12.760 --> 18:17.560
So TensorFlow popped up was a December 2015, I think.

18:17.560 --> 18:24.720
It took the whole deep learning world by a bang and Google put so much effort evangelizing

18:24.720 --> 18:25.720
TensorFlow.

18:25.720 --> 18:33.080
I mean, from Sundar Pichai to like pretty much everyone was like, hey, this is TensorFlow,

18:33.080 --> 18:36.880
this is what Google is going to be about for the next few years, right.

18:36.880 --> 18:43.800
So that, and, you know, they're a huge team and they've been putting great effort into

18:43.800 --> 18:48.800
generally making sure everyone are covered by TensorFlow, whether it's a data scientist

18:48.800 --> 18:52.800
or deep learning researcher or like a production engineer.

18:52.800 --> 18:55.880
And like what Google did was amazing.

18:55.880 --> 19:03.800
They raised the bar for engineering of a deep learning framework so far high.

19:03.800 --> 19:08.800
Until then, like if you think about it, Torch, Tiano Cafe, these were the dominant deep

19:08.800 --> 19:10.720
learning frameworks before TensorFlow.

19:10.720 --> 19:18.200
They were all like, they all started as one man, grad student projects.

19:18.200 --> 19:22.040
And you could totally see that in them.

19:22.040 --> 19:24.640
The quality control was really bad.

19:24.640 --> 19:27.640
They were not planned properly.

19:27.640 --> 19:35.280
If you want to install any of those back in 2014, you would spend a day, maybe more.

19:35.280 --> 19:38.160
You'd install that dependency, this dependency.

19:38.160 --> 19:45.000
The TensorFlow, it made the engineering bar very high, they were like, we're not screwing

19:45.000 --> 19:46.000
around here.

19:46.000 --> 19:47.000
Right.

19:47.000 --> 19:50.320
We want to make the best product out there for people.

19:50.320 --> 19:54.520
And they went with a Tiano style programming model.

19:54.520 --> 19:55.520
Yeah.

19:55.520 --> 20:01.320
So a Tiano style programming model is very, very low level, which means if you want to

20:01.320 --> 20:09.240
write something like a convolution neural network, then you'd spend writing so much

20:09.240 --> 20:11.200
boilerplate code.

20:11.200 --> 20:15.680
And also like the TensorFlow style model, it's called symbolic, which means that like you

20:15.680 --> 20:18.960
create a graph and then you run it later.

20:18.960 --> 20:26.040
The problem with that is if you want to debug anything, you'd have to use the tooling

20:26.040 --> 20:27.440
given by TensorFlow.

20:27.440 --> 20:31.280
Like you can, for example, debug your code by itself.

20:31.280 --> 20:36.120
You have to run your model in this other virtual machine.

20:36.120 --> 20:40.920
And then you can set breakpoints in that virtual machine using TensorFlow's own tools.

20:40.920 --> 20:47.320
Meaning because it's not standard language x Python, in the case of PyTorch, it's being

20:47.320 --> 20:53.560
interpreted by some virtual machine that knows how to read this graph and schedule execution

20:53.560 --> 20:54.560
against this graph.

20:54.560 --> 21:01.640
So the only way to develop debugging and other tooling for it is via that virtual machine.

21:01.640 --> 21:02.640
That is correct.

21:02.640 --> 21:03.640
Okay.

21:03.640 --> 21:08.360
So the upsides to it are that this virtual machine can be as big or small as possible.

21:08.360 --> 21:11.320
You can ship it into phones, you can ship it into anything.

21:11.320 --> 21:16.600
The downside is that now where you define your network was in your Python VM.

21:16.600 --> 21:19.440
And you're running your network in a different VM, there's disconnect.

21:19.440 --> 21:25.640
So as a developer, you always have to keep thinking about how the behavior is something

21:25.640 --> 21:31.240
in the TensorFlow virtual machine maps back to what you wrote in the Python code.

21:31.240 --> 21:35.880
So anyone kind of on the enterprise side that has some Java experience, you know, knows

21:35.880 --> 21:40.840
how exactly how different the right ones run anywhere is from the practice of needing to

21:40.840 --> 21:45.720
know the internals of your heap size and garbage collection strategies and stuff like that.

21:45.720 --> 21:46.720
Exactly.

21:46.720 --> 21:53.360
And also give you, or at least that the ecosystem, if not an individual developer, the flexibility

21:53.360 --> 21:58.000
to decouple the VM from the your code base.

21:58.000 --> 22:03.320
So meaning, you know, you could write your code using TensorFlow and Python and but have

22:03.320 --> 22:08.520
the VM written in, you know, go or whatever the fast concurrent, highly concurrent language

22:08.520 --> 22:13.960
flavor of the day is and even port that over to, you know, distributed models or HPC

22:13.960 --> 22:14.960
or.

22:14.960 --> 22:20.600
So the upside is that the VM can now be written and rewritten in many other things.

22:20.600 --> 22:27.880
The downside is that you have to have a build a whole ecosystem around your VM so that

22:27.880 --> 22:29.960
users don't feel depraved by tooling.

22:29.960 --> 22:30.960
Yeah.

22:30.960 --> 22:32.520
And Tiano was also like that.

22:32.520 --> 22:36.440
That's the whole programming model of symbolic where you define a symbolic model and then

22:36.440 --> 22:38.800
you compile it and then you run it somewhere.

22:38.800 --> 22:44.480
The torch model has always been imperative, which is you don't really have a separate VM.

22:44.480 --> 22:49.000
You just declare things and you don't even like you just like you just write one plus

22:49.000 --> 22:54.480
two and it just executed like there's no like separation between declaration and execution.

22:54.480 --> 22:55.480
Yeah.

22:55.480 --> 22:58.160
We wanted to extend the same thing to PyTorch.

22:58.160 --> 23:03.840
So you just write arbitrary imperative code and that is your neural network itself.

23:03.840 --> 23:08.640
That also lends itself to I think at least in the data science community, there's a lot

23:08.640 --> 23:14.520
of popularity and flexibility around like Jupyter notebooks as a primary you are having

23:14.520 --> 23:21.000
an imperative execution lends itself to being able to yeah, you can basically see your

23:21.000 --> 23:22.480
execution as it goes.

23:22.480 --> 23:24.880
You can print things and all of that.

23:24.880 --> 23:27.560
That's one of the biggest upsides.

23:27.560 --> 23:32.640
So yeah, while we're building PyTorch, we wanted to continue the torch model, the imperative

23:32.640 --> 23:40.040
style, the dynamic nature of things and we wanted to build it in a way that it also reaches

23:40.040 --> 23:43.080
a very high bar of engineering.

23:43.080 --> 23:47.280
So that's been our core philosophy.

23:47.280 --> 23:55.200
And so for Google, I think the way most people have interpreted their strategy behind diving

23:55.200 --> 24:04.680
deep into TensorFlow is they foresee this world where AI workloads, whether they're training

24:04.680 --> 24:07.920
or inference workloads are going to drive a ton of compute.

24:07.920 --> 24:13.720
Their business model, their non advertising business model is heavily geared towards providing

24:13.720 --> 24:16.520
compute via the Google Cloud.

24:16.520 --> 24:22.200
And so if they can own the model in which they can basically own the VM for AI, then they

24:22.200 --> 24:25.640
can be the best place to run AI workloads, right?

24:25.640 --> 24:31.400
What's the Facebook motivation for investing so heavily in PyTorch and tooling?

24:31.400 --> 24:36.520
Is it just not to be controlled by Google or is there more to it?

24:36.520 --> 24:41.600
So Facebook's motivation is to fold.

24:41.600 --> 24:48.520
For Facebook AI research, which is the 100 odd researchers and in general for the community

24:48.520 --> 24:50.280
of AI.

24:50.280 --> 24:56.200
We have a single point agenda at fair, which is to try to solve AI.

24:56.200 --> 24:59.440
And for that, we're building the best tools out there.

24:59.440 --> 25:03.520
And we keep them open just because there's nothing secret to it.

25:03.520 --> 25:08.320
We want to build the best possible AI and we keep publishing about how we're trying to

25:08.320 --> 25:09.840
make progress.

25:09.840 --> 25:13.400
Our motivation is not really on the business model so much.

25:13.400 --> 25:18.040
It's more like, hey, we're trying to solve this very challenging problem.

25:18.040 --> 25:25.680
And you see this manifest in various ways in the Facebook product as a second-hand effect.

25:25.680 --> 25:29.640
Like the product teams are not sitting with the AI researchers and saying how do we improve

25:29.640 --> 25:31.680
Facebook as a product.

25:31.680 --> 25:38.840
Facebook AI research, the people are independently working on their AI research.

25:38.840 --> 25:44.120
But as we build and publish these things, the product teams look at our research and

25:44.120 --> 25:49.520
they're like, oh, we can implement this thing in our product in this way.

25:49.520 --> 25:53.040
And that's just going to be a better product experience for everyone.

25:53.040 --> 25:59.920
Some examples are we've had the accessibility interface improve quite a bit recently about

25:59.920 --> 26:05.800
a year, a year and a half ago, where now if you're a user who is blind or near blind,

26:05.800 --> 26:10.640
you can view Facebook like you can basically touch Facebook as you would and it will tell

26:10.640 --> 26:12.600
you what's going on.

26:12.600 --> 26:17.720
Or if you touch the picture, it would just not tell it's would say a picture posted

26:17.720 --> 26:19.360
by this person.

26:19.360 --> 26:23.800
But if you touch a picture now, it actually tells you, oh, it's a picture where a boy is

26:23.800 --> 26:29.440
playing with a cat and it's very descriptive and similarly we're trying to do the same

26:29.440 --> 26:31.400
for videos as well.

26:31.400 --> 26:36.200
That's one manifestation, others are where you want to make language barriers.

26:36.200 --> 26:43.960
So let's say I have 500 friends, like some of them I met in various places and trips.

26:43.960 --> 26:49.160
Like if one of my good friends writes a huge post in Chinese, I still want to be able

26:49.160 --> 26:51.080
to know what it's about, right?

26:51.080 --> 26:56.560
So we have the translate feature embedded into Facebook, all powered by a Facebook research.

26:56.560 --> 27:00.720
And you see these manifest in various other producty ways.

27:00.720 --> 27:08.360
And I think you see that evolve as well, like if I remember correctly, the Facebook products

27:08.360 --> 27:14.120
only relatively recently switched to switched the way they did translation to neural translation.

27:14.120 --> 27:17.600
I think there was a blog post on a fair blog maybe two months ago.

27:17.600 --> 27:19.080
So very, very interesting.

27:19.080 --> 27:20.880
Okay, so your talk.

27:20.880 --> 27:24.680
So kind of walk us through the main points of your talk.

27:24.680 --> 27:31.880
Basically you're at the high level, you've got PyTorch, it's inherently built around Python,

27:31.880 --> 27:37.240
but you need to find ways to overcome the limitations of Python and kind of how you do

27:37.240 --> 27:38.240
that.

27:38.240 --> 27:40.120
Was that the main thrust of the talk or?

27:40.120 --> 27:45.120
Yeah, so it's mostly just a general engineering talk about PyTorch.

27:45.120 --> 27:46.120
Okay.

27:46.120 --> 27:48.960
So I'm going to give like an overview of PyTorch how it works.

27:48.960 --> 27:49.960
Okay.

27:49.960 --> 27:52.960
A strange little audience doesn't necessarily know about deep learning computations, right?

27:52.960 --> 27:55.320
More of the developer focus audience.

27:55.320 --> 28:01.920
Like a lot of diversity in the programming models we use and like strange loop has everyone

28:01.920 --> 28:02.920
under the sun.

28:02.920 --> 28:03.920
Yeah.

28:03.920 --> 28:05.920
So, which is part of what makes it great, right?

28:05.920 --> 28:06.920
It's awesome.

28:06.920 --> 28:08.200
Like this is my first strange loop.

28:08.200 --> 28:13.440
I've just seen the sessions so far and like looking at the sessions lined up for tomorrow,

28:13.440 --> 28:14.440
it's awesome.

28:14.440 --> 28:20.160
So interesting factoid, the guy who founded Strange Loop, a guy named Alex Miller, Strange

28:20.160 --> 28:25.760
was a group out of a meetup he had here in town called Lambda Lounge that looked at

28:25.760 --> 28:31.960
like functional programming languages and at a startup that I was at years ago, this

28:31.960 --> 28:39.120
was probably eight years ago, maybe we basically hosted, incubated this meetup, they would

28:39.120 --> 28:40.120
meet in office.

28:40.120 --> 28:43.400
So I've been sitting there late at night trying to get finished with my work and hearing

28:43.400 --> 28:47.840
people talk about monads and stuff that I just had no clue about.

28:47.840 --> 28:48.840
Nice.

28:48.840 --> 28:58.560
I didn't realize St. Louis had a big developer community and this is all new to me.

28:58.560 --> 29:04.880
So I'll be talking a little bit about PyTorch, start off with how to relate it to other

29:04.880 --> 29:10.480
things people know and then a little bit of deep learning workloads, the general challenges,

29:10.480 --> 29:12.320
why they're very different from let's say.

29:12.320 --> 29:16.880
Let's say those in turn, how should people think about PyTorch and relating it to things

29:16.880 --> 29:17.880
that they know?

29:17.880 --> 29:23.760
Let's say you're, I mean, the most common thing everyone knows about is JavaScript, right?

29:23.760 --> 29:27.320
So let's look at JavaScript.

29:27.320 --> 29:32.440
If you're trying to build a compiler for JavaScript, the things you most care about is JavaScript

29:32.440 --> 29:40.200
code, which is very branchy, has a lot of control flow and it's very, very like cash

29:40.200 --> 29:41.880
and sensitive.

29:41.880 --> 29:46.360
If you're building a very high performance compiler for JavaScript, you will build it

29:46.360 --> 29:53.320
in a way that you'll try to optimize for like branch prediction and like try to get

29:53.320 --> 29:58.600
traces and do trace compilation, you're tracing, like for example, if you look at

29:58.600 --> 30:05.120
Chrome's JavaScript compiler, V8, yeah, V8, it's tracing jit and or you can look at

30:05.120 --> 30:10.880
Lua jit as another example, it's another tracing jit and they do, we'll quickly trace

30:10.880 --> 30:16.840
through upcoming code and then if something's compatible, we'll run time code generator

30:16.840 --> 30:20.840
really quickly and these are all in the order of nanoseconds even because they're very,

30:20.840 --> 30:28.640
very small computations and they're very branchy and doing something like loop hoisting or

30:28.640 --> 30:34.520
strength reduction, these are the things that would really go well with such workloads.

30:34.520 --> 30:35.520
Okay.

30:35.520 --> 30:41.040
If learning, what you do is you do operations on tensors, let's say like, and eventual

30:41.040 --> 30:48.560
matrices and usually you're doing a computation between A and B and A and B are not two

30:48.560 --> 30:55.720
integers, they're 2,000 integers or 200,000 integers and so tensors is this intervention

30:55.720 --> 31:00.680
on matrix and when you're doing these calculations, you're basically doing a lot of multiplications

31:00.680 --> 31:08.360
or like any kind of point wise or reduction or some kind of convolvingy, like a moving

31:08.360 --> 31:13.560
window kind of operations, these are the most common things in deep learning.

31:13.560 --> 31:19.520
So when you're trying to make these like something like this more efficient, you look

31:19.520 --> 31:25.640
at how fast you can, how fast, like how much you can parallelize each of these operations

31:25.640 --> 31:32.760
individually and it turns out almost all of these operations are bandwidth, so these

31:32.760 --> 31:40.480
operations can one as fast as how fast your memory bandwidth is, like how fast you can

31:40.480 --> 31:46.240
get it in and out of the CPU because inside the CPU, you're just doing a small multiplication

31:46.240 --> 31:52.440
or like an exponential but it's still much more expensive to get 200,000 numbers into

31:52.440 --> 31:55.360
the CPU and out of the CPU.

31:55.360 --> 32:01.920
So the way you would do optimization when building such things, let's say you're building

32:01.920 --> 32:04.120
a compiler for these things.

32:04.120 --> 32:09.600
You would try to fuse a bunch of these point wise operations, a bunch of these reduction

32:09.600 --> 32:15.960
operations into an inner loop and then what you would do is you would get these tensors

32:15.960 --> 32:20.760
in instead of doing one operation, putting it back out in a result and doing another

32:20.760 --> 32:26.640
operation, putting the result back out, you try to get the tensor in, do seven operations

32:26.640 --> 32:32.440
at once and then get all like the result of the seven of them out because that would

32:32.440 --> 32:35.640
make it more compute bond rather than bandwidth bond.

32:35.640 --> 32:36.640
Interesting.

32:36.640 --> 32:42.960
So taking a step back to the analogy that JavaScript is primarily to say you build these high

32:42.960 --> 32:47.760
performance, compilation and execution environments by understanding the property of the language

32:47.760 --> 32:51.600
that you're working with and optimizing around that.

32:51.600 --> 32:59.120
And we can do that here by, in this example, you would take code that is fundamentally written

32:59.120 --> 33:06.040
in a very iterative serial kind of way but maybe parallelize or unfold those loops.

33:06.040 --> 33:07.040
Yeah.

33:07.040 --> 33:08.440
I don't know if that's the right way of thinking about it but.

33:08.440 --> 33:09.960
Say tile the loops.

33:09.960 --> 33:10.960
What's that tile the loops?

33:10.960 --> 33:11.960
Tiling.

33:11.960 --> 33:15.760
It's called tiling because you can, you can break the computation to, oh, I have 200,000

33:15.760 --> 33:16.760
of these.

33:16.760 --> 33:21.760
I'll just make tiles of 20 and send them to like 20 different processors.

33:21.760 --> 33:22.760
Okay.

33:22.760 --> 33:23.760
Interesting.

33:23.760 --> 33:24.760
That was very cool.

33:24.760 --> 33:29.600
Especially with GPUs, with GPUs, you have 3,000 cores on your GPU.

33:29.600 --> 33:30.600
Right.

33:30.600 --> 33:35.680
So you want to like break this computation down and feed those into all the separate processors

33:35.680 --> 33:38.320
and then like get the results back.

33:38.320 --> 33:41.440
But they don't fundamentally change the bandwidth issue.

33:41.440 --> 33:42.440
They don't.

33:42.440 --> 33:43.840
The bandwidth issue is still exist.

33:43.840 --> 33:44.840
Yeah.

33:44.840 --> 33:49.720
So I'm going to be talking a little bit about the GIT that we built into PyTorch.

33:49.720 --> 33:51.960
We built a just in time compiler.

33:51.960 --> 33:55.320
It's also a tracing GIT but it's of a very different kind.

33:55.320 --> 33:58.320
Art tracing is not in the order of nanoseconds.

33:58.320 --> 34:00.280
It's in the order of microseconds.

34:00.280 --> 34:01.280
Okay.

34:01.280 --> 34:04.200
But that's completely fine because general deep learning workloads are in the order of

34:04.200 --> 34:05.200
milliseconds.

34:05.200 --> 34:06.200
Right.

34:06.200 --> 34:12.360
And the kind of optimizations passes we write as well are, as I said, more like fusion

34:12.360 --> 34:19.600
and batching by batching, I mean, let's say you do computation x, y, z.

34:19.600 --> 34:23.440
But between x, computation x and z, they're shared operations.

34:23.440 --> 34:26.920
Let's say x also does multiplies and z also does multiplies.

34:26.920 --> 34:31.720
So the tensors involved in x and tensors involved in z are very small.

34:31.720 --> 34:38.120
What you do is you create a multiply operation, combine the tensors that are involved there

34:38.120 --> 34:41.800
and then after the result comes out separate them on.

34:41.800 --> 34:43.960
So this is called dynamic batching.

34:43.960 --> 34:44.960
Yeah.

34:44.960 --> 34:50.560
So we've been writing this GIT that's very new for us, like as in not many people have

34:50.560 --> 34:52.640
worked generally in this direction.

34:52.640 --> 34:55.360
Tens of flow is building one called XLA.

34:55.360 --> 34:56.960
It's a compiler.

34:56.960 --> 35:01.280
And the kind of optimizations they're doing as well are very similar in nature.

35:01.280 --> 35:06.120
Like everyone's exploring now these like how to make tensor computations faster.

35:06.120 --> 35:10.600
We're just taking the just in time approach and they're taking the head of time analysis

35:10.600 --> 35:11.840
approach.

35:11.840 --> 35:20.200
And I think of GIT and this probably comes from really JavaScript as something that is

35:20.200 --> 35:26.840
more relevant to interactive types of workloads than batch, but deep learning is primarily

35:26.840 --> 35:31.040
a batch workload, at least the training part of it.

35:31.040 --> 35:32.240
Depends on how you see it.

35:32.240 --> 35:35.600
Like we want to keep the interactiveness because remember what we talked about, people

35:35.600 --> 35:41.800
like to keep that interactive Pythani iPad on on book style programming model.

35:41.800 --> 35:47.040
We want people to keep that flexibility, but that's not really where you need like the

35:47.040 --> 35:48.040
super high performance.

35:48.040 --> 35:55.360
No, that's every like that's people do interactively.

35:55.360 --> 36:01.600
They're programming for AGP's like this is this is the norm and deep learning.

36:01.600 --> 36:06.600
So you're backed by your super powerful GPU.

36:06.600 --> 36:12.600
And I'll talk a little bit about this in the talk tomorrow about how you can your PyTorch

36:12.600 --> 36:16.360
sensors can just be transferred to the GPU and you operate on them.

36:16.360 --> 36:21.840
And all the operations are now being done on the GPU with very high performance, but you're

36:21.840 --> 36:23.800
doing this in a very interactive way.

36:23.800 --> 36:24.800
Okay.

36:24.800 --> 36:27.880
It is a little bit of a mind shift for folks that have been kind of immersed in a tensor

36:27.880 --> 36:32.600
flow oriented or a batch oriented world where you kind of create this job.

36:32.600 --> 36:38.120
You send it off to the cloud or wherever to train and then you check on it a few days.

36:38.120 --> 36:39.120
Yeah.

36:39.120 --> 36:45.160
We are the worst nightmare for hardware developers because they're all building solutions

36:45.160 --> 36:46.160
around.

36:46.160 --> 36:51.360
Oh, let's say you build this model beforehand and then you give it to our hardware.

36:51.360 --> 36:52.360
Yeah.

36:52.360 --> 36:58.320
Let's say it takes 30 minutes, but it will map your model in the most effective way to

36:58.320 --> 36:59.640
our hardware.

36:59.640 --> 37:06.040
And after that 30 minutes is done, if you pump any images in or any inputs in, it'll

37:06.040 --> 37:08.120
be like super fast.

37:08.120 --> 37:11.000
And we're like, well, that's kind of dumb in our model.

37:11.000 --> 37:15.520
Like we'll give you a different model in every single iteration.

37:15.520 --> 37:16.520
What are you going to do?

37:16.520 --> 37:17.520
Right, right.

37:17.520 --> 37:19.000
Interesting.

37:19.000 --> 37:24.720
So speaking of hardware and hardware developers, I don't know if this is something that you're

37:24.720 --> 37:29.640
close to it all, but Facebook also is very involved in this OCP Open Compute projects.

37:29.640 --> 37:31.920
I was involved in the Big Sur and the Big Basin.

37:31.920 --> 37:32.920
Don't worry you.

37:32.920 --> 37:33.920
Yeah.

37:33.920 --> 37:34.920
Okay.

37:34.920 --> 37:42.320
So do you see, as I understand it, OCP has primarily been oriented around kind of off the shelf

37:42.320 --> 37:47.440
stuff like system architecture as opposed to, you know, board level architecture or anything

37:47.440 --> 37:53.480
like that, but do you see a future where OCP takes on like this bandwidth problem to try

37:53.480 --> 37:59.080
to make hardware that's more suitable for these kinds of workloads or are those other

37:59.080 --> 38:00.080
people's problems?

38:00.080 --> 38:06.560
I think OCP is one of those huge industry-wide efforts, right?

38:06.560 --> 38:11.480
I think it's truly possible that under OCP you'll get something that's like a custom

38:11.480 --> 38:13.160
ASIC.

38:13.160 --> 38:15.880
And I'm not sure how that will happen in general.

38:15.880 --> 38:20.880
Because there's so many players and so many people who can contribute to OCP just two

38:20.880 --> 38:29.880
days ago, I think, or yesterday, Nvidia released an open spec with very log files and HDL

38:29.880 --> 38:36.560
files off a chip that does convolutional neural networks, like a high performance convolutional

38:36.560 --> 38:37.560
neural network-based thing.

38:37.560 --> 38:41.600
I think I saw the headline for that is like, this is Nvidia's TVU.

38:41.600 --> 38:42.600
Right.

38:42.600 --> 38:45.320
I mean, that was the click-bady headline.

38:45.320 --> 38:48.560
So they basically put it out there.

38:48.560 --> 38:54.840
You can take that very log and HDL is basically the code for fabricating these chips.

38:54.840 --> 38:55.840
Yeah.

38:55.840 --> 39:02.960
You will have to still like remap, very log into actual process, your manufacturing under,

39:02.960 --> 39:07.680
but that's a mechanical process that usually, like you then, you usually give your HDL

39:07.680 --> 39:14.600
files to someone and then they'll spend, they had a company will spend remapping whatever

39:14.600 --> 39:17.680
you have to the process most effectively.

39:17.680 --> 39:21.520
But yeah, it's a fairly mechanical process at the point.

39:21.520 --> 39:26.000
So now, Nvidia just released this open chip.

39:26.000 --> 39:31.880
And that's totally one of the candidates, for example, for open compute.

39:31.880 --> 39:35.800
Like, open compute is all about, okay, everything is open.

39:35.800 --> 39:39.240
You can refer to this as your own, right?

39:39.240 --> 39:44.800
So I think we're still not sure about what's going to come into OCP in that direction.

39:44.800 --> 39:46.680
Like I personally don't know the details.

39:46.680 --> 39:47.680
Right.

39:47.680 --> 39:48.680
So we'll see.

39:48.680 --> 39:50.440
We'll see how it goes.

39:50.440 --> 39:51.440
Okay.

39:51.440 --> 39:59.000
So you also mentioned that parts of the pie torch engine are written in C makes me think

39:59.000 --> 40:04.720
a little bit of, I can, in just kind of the regular Python community, there's, I'm

40:04.720 --> 40:09.320
or incorrectly, there's like pi pi and sithon and all these different implementations of

40:09.320 --> 40:10.320
the Python interpreter.

40:10.320 --> 40:12.160
Is it the same general idea there?

40:12.160 --> 40:14.360
Not at all.

40:14.360 --> 40:21.200
Pi pi is a replacement for the default Python implementation, which is called CPITON.

40:21.200 --> 40:22.200
Okay.

40:22.200 --> 40:28.720
So Python is a programming language and they have a base implementation of that programming

40:28.720 --> 40:33.560
language that the language developers develop on, which is called CPITON.

40:33.560 --> 40:40.520
It's written in C and if you type Python into your desktop, usually that's what it is.

40:40.520 --> 40:41.520
CPITON.

40:41.520 --> 40:48.160
And CITON is just a very cute way of writing extensions to Python, like to see Python

40:48.160 --> 40:49.160
mostly.

40:49.160 --> 40:50.160
Okay.

40:50.160 --> 40:53.760
Pi pi is a replacement interpreter for CPITON.

40:53.760 --> 40:54.760
For CPITON.

40:54.760 --> 40:56.200
It's written in Python.

40:56.200 --> 40:57.520
That can interpret Python.

40:57.520 --> 41:01.040
I'm not sure what it's written in, but I don't think it's written in Python.

41:01.040 --> 41:02.040
Okay.

41:02.040 --> 41:04.720
The implementation is probably not in Python.

41:04.720 --> 41:05.720
Okay.

41:05.720 --> 41:11.760
Pi pi is a just in time interpreter for Python.

41:11.760 --> 41:14.760
It's probably written in part assembly and part C or whatever.

41:14.760 --> 41:15.760
Okay.

41:15.760 --> 41:16.760
Okay.

41:16.760 --> 41:21.880
So the idea of being the significance of it is not its implementation, but just in time

41:21.880 --> 41:24.320
versus not just in time.

41:24.320 --> 41:25.320
Yeah.

41:25.320 --> 41:26.320
Okay.

41:26.320 --> 41:29.720
Pi Torch is just a CPITON extension.

41:29.720 --> 41:35.200
Like all the C bits we have are, the equivalent you can find is NumPy.

41:35.200 --> 41:38.920
NumPy is 90% written in C.

41:38.920 --> 41:39.920
Yeah.

41:39.920 --> 41:40.920
Okay.

41:40.920 --> 41:43.280
But it has, it's a CPITON extension.

41:43.280 --> 41:46.560
That is, it's not an independent C library that you can run.

41:46.560 --> 41:50.560
It's like heavily integrated into the CPITON API and stuff.

41:50.560 --> 41:53.280
So Pi Torch is just like a Python extension.

41:53.280 --> 41:54.280
Okay.

41:54.280 --> 41:59.080
But there's recently been an announcement of, you know, one of the first like, I think

41:59.080 --> 42:06.480
kind of broadly publicize Pi Torch wins, if you will, is like fast.ai, deciding to rewrite

42:06.480 --> 42:10.040
all of their courseware and into Pi Torch.

42:10.040 --> 42:12.240
That was a very pleasant surprise.

42:12.240 --> 42:13.240
I wasn't.

42:13.240 --> 42:18.640
As soon as Pi Torch came out, I think they've tried it and they found it really effective,

42:18.640 --> 42:22.040
especially for teaching and the barrier of entry.

42:22.040 --> 42:26.960
So they've switched over to Pi Torch from TensorFlow.

42:26.960 --> 42:29.960
And we're supporting them in any way we can.

42:29.960 --> 42:30.960
Yeah.

42:30.960 --> 42:31.960
Yeah.

42:31.960 --> 42:32.960
It's interesting.

42:32.960 --> 42:38.000
I think if, you know, if there's anything that this community benefits from is options and

42:38.000 --> 42:43.360
especially options that, you know, have major, you know, both major companies behind them

42:43.360 --> 42:49.760
pushing them forward, but also that are open to community engagement and community contributions.

42:49.760 --> 42:55.920
And so it's definitely great to see from a kind of industry observer point of view that,

42:55.920 --> 43:02.320
you know, we've got, you know, what's starting to look like a second kind of really strong

43:02.320 --> 43:07.320
contender at a time when, you know, again, I think a lot of people said, oh, yeah, it's

43:07.320 --> 43:08.880
just, you know, it's TensorFlow.

43:08.880 --> 43:12.160
So congrats for your part in that.

43:12.160 --> 43:14.760
And thanks so much for taking the time to sit down with me.

43:14.760 --> 43:15.760
Oh, my pleasure.

43:15.760 --> 43:16.760
Appreciate it.

43:19.760 --> 43:21.240
All right, everyone.

43:21.240 --> 43:23.320
That's our show for today.

43:23.320 --> 43:28.280
Thanks so much for listening and for your continued feedback and support.

43:28.280 --> 43:33.440
For more information on to me or any of the topics covered in this episode, head on over

43:33.440 --> 43:37.560
to twomlai.com slash talk slash 70.

43:37.560 --> 43:45.040
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.

43:45.040 --> 43:50.760
Of course, you can send along your feedback or questions via Twitter to act twomlai or

43:50.760 --> 43:55.840
at Sam Charrington, or leave a comment right on the show notes page.

43:55.840 --> 43:59.200
Thanks again to Nexosis for their sponsorship of the show.

43:59.200 --> 44:06.200
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders

44:06.200 --> 44:13.800
and visit nexosis.com slash twimmel for more information and to try their API for free.

44:13.800 --> 44:23.800
Thanks again for listening and catch you next time.

