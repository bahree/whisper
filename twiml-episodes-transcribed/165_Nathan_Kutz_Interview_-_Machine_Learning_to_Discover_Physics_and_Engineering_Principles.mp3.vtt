WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:36.040
I'm your host Sam Charrington, a quick update about the upcoming Twimble Online Meetup.

00:36.040 --> 00:41.620
Next Tuesday July 17th at 5pm Pacific time, Nick Teague will lead a discussion on the paper

00:41.620 --> 00:47.440
Quantum Machine Learning by Jacob Biamante at all, which explores how to devise and implement

00:47.440 --> 00:50.920
concrete software for machine learning tasks.

00:50.920 --> 00:59.120
Visit twimbleai.com slash meetup for more details or to sign up.

00:59.120 --> 01:04.200
In this episode, I'm joined by Nathan Kuts, professor of Applied Mathematics, Electrical

01:04.200 --> 01:08.400
Engineering and Physics at the University of Washington.

01:08.400 --> 01:12.680
Nathan and I met a few months ago at the Prepare AI Conference here in St. Louis, where

01:12.680 --> 01:18.360
he gave a great talk on machine learning to discover physics and engineering principles.

01:18.360 --> 01:23.160
Our conversation is laser focused on his research and to the use of ML to help discover the

01:23.160 --> 01:30.440
fundamental governing equations for physical and engineering systems from time series measurements.

01:30.440 --> 01:35.320
We explore in fact the application of his work to self-tuning fiber optic lasers, as well

01:35.320 --> 01:40.080
as to biology and other complex multi-scale systems.

01:40.080 --> 01:42.840
And now on to the show.

01:42.840 --> 01:49.160
All right, everyone. I am on the line with Nathan Kuts. Nathan is professor of Applied

01:49.160 --> 01:54.200
Mathematics, Electrical Engineering and Physics at the University of Washington.

01:54.200 --> 01:57.040
Nathan, welcome to this weekend machine learning and AI.

01:57.040 --> 01:59.600
Thank you very much. Glad to be here.

01:59.600 --> 02:04.880
I'm really looking forward to our conversation. I had a chance to see a talk you did at

02:04.880 --> 02:11.080
the Prepare AI Conference here in St. Louis a few weeks ago and I thought it was a great

02:11.080 --> 02:18.480
presentation. It was on machine learning to discover physics and engineering principles

02:18.480 --> 02:22.760
and we are going to dive pretty deeply into that.

02:22.760 --> 02:29.520
But before we do, Nathan, take a few minutes to introduce us to your background and how

02:29.520 --> 02:34.000
you got involved in machine learning and artificial intelligence.

02:34.000 --> 02:36.160
Sure. So we'll be happy to do that.

02:36.160 --> 02:42.560
So I come from a pretty standard, let's say, physics, Applied Math training. I did

02:42.560 --> 02:51.480
my PhD back in the early the 90s and then went off to do a postdoc at Princeton University

02:51.480 --> 02:56.760
jointly with Bell Labs. And at the time, I was doing a lot of just standard modeling

02:56.760 --> 03:03.320
of physical systems. I was particularly interested in optical systems. At that time at Bell Labs,

03:03.320 --> 03:08.000
we were doing long haul communications. So the idea there was to, of course, write down

03:08.000 --> 03:13.640
some governing equations. And typically this was some version of Maxwell's equations for

03:13.640 --> 03:19.360
understanding how light propagates down fiber optic cables. And so I would call this coming

03:19.360 --> 03:26.720
from the forward modeling perspective, which is you write down some set of equations.

03:26.720 --> 03:33.120
You drive them and then you do lots of simulations. And then you build your understanding in that

03:33.120 --> 03:37.640
architecture. And of course, there's some data that's collected. But back in the 90s,

03:37.640 --> 03:42.640
the data wasn't the same as what we think about today. And so I spent a long time there

03:42.640 --> 03:48.560
really building out sort of that perspective and applying it to various other types of

03:48.560 --> 03:55.080
problems once they came to the University of Washington. And about 10 years ago, I got

03:55.080 --> 04:00.600
very interested in using some of these dimensionality reduction techniques that were being used

04:00.600 --> 04:06.680
in the fluid, fluid dynamics community. And there it's called proper orthogonality composition,

04:06.680 --> 04:12.560
which really was the same as PCA. And everybody has these different terms for the same thing.

04:12.560 --> 04:16.960
So then I started realizing that you said it was called proper orthogonality. Proper

04:16.960 --> 04:22.000
orthogonality composition. Proper orthogonality composition. Okay. And then interestingly enough,

04:22.000 --> 04:28.880
the atmospheric scientists called it empirical orthogonal functions. So I started realizing

04:28.880 --> 04:33.480
that everybody's doing the same thing, which is taking your data and maybe doing a little

04:33.480 --> 04:38.640
bit of mile pre-processing, but running it through the singularvality composition. And I

04:38.640 --> 04:44.160
started applying it initially to optics problems and started realizing that, oh my God, this

04:44.160 --> 04:50.400
is an amazing idea. And in fact, it was kind of interesting that I had no idea that there's

04:50.400 --> 04:56.640
whole other community existed. And that's how I started getting into the idea of using

04:56.640 --> 05:04.640
data methods over towards more physics engineering-type problems. And since that time, then I brought

05:04.640 --> 05:13.680
in all of this architecture borrowed from the MLAI type communities around model discovery,

05:13.680 --> 05:20.520
regression, dimensionality reduction. And that has completely changed the way I think

05:20.520 --> 05:27.640
about these problems. And it's also allowing me to start thinking about using these ideas

05:27.640 --> 05:32.160
towards control architectures. And that was ultimately the goal we had in the engineering

05:32.160 --> 05:38.160
sciences. It's always going to come down to some kind of control theory. But this allowed

05:38.160 --> 05:43.400
my group and my collaborators to start thinking about framing things in terms of control.

05:43.400 --> 05:50.440
So since that time, we just are really integrating these engineering systems with machine learning

05:50.440 --> 05:57.840
architectures. And that's how I've continued to develop in this direction is to use both

05:57.840 --> 05:59.240
perspectives.

05:59.240 --> 06:08.040
The context that you presented on, if I remember correctly, was in controlling, was it lasers

06:08.040 --> 06:14.440
or controlling the laser coherence or something like that?

06:14.440 --> 06:23.520
Yeah, I can speak to it. So we were interested. A lot of my expertise comes in this laser

06:23.520 --> 06:30.720
physics, which I had started back all the way back in the late 90s working on laser physics.

06:30.720 --> 06:35.360
And one of the interesting things was that over this time period, this is a big commercial

06:35.360 --> 06:43.440
market. Laser technologies are, I mean, the amount of money in that field is enormous.

06:43.440 --> 06:47.240
I mean, hundreds of millions of dollars a year in sales for lasers because they're pretty

06:47.240 --> 06:53.040
much ubiquitous, right? They're from your supermarket scanner all the way to what's emerging

06:53.040 --> 07:00.400
light art technologies. So lasers have become this really important technological piece in

07:00.400 --> 07:06.000
regards to censored technologies. And so the data coming from this is quite large. And

07:06.000 --> 07:10.760
we'd like to tune them. So one of the things that I noticed was that people were like trying

07:10.760 --> 07:15.800
to tune them sort of hand-tune these lasers for performance and then environmentally sealing

07:15.800 --> 07:20.440
it. So whatever you did to tune it, you'd lock it in and you'd sell it. And if anything

07:20.440 --> 07:24.320
happened, then you'd have to have your rep come out and fix the laser. And then I was

07:24.320 --> 07:29.560
thinking, well, this is a lot easier than a self-driving car. In fact, nobody gets hurt.

07:29.560 --> 07:35.200
And it's only a few knobs. And that was the remarkable thing. This community has been

07:35.200 --> 07:41.120
very slow on the uptake, I feel, in terms of adopting this technology. So we've been

07:41.120 --> 07:45.680
really pushing it out there. So one of the things I highlighted at the talk was, here's

07:45.680 --> 07:51.040
an architecture, a software architecture that you could implement today towards essentially

07:51.040 --> 07:55.880
a laser system that would tune itself. And not only would tune itself, it would learn all

07:55.880 --> 08:01.280
the possible behaviors that has available to itself. And even if something happens,

08:01.280 --> 08:06.320
someone hit the laser and so you've changed the parameter space a little bit, the same

08:06.320 --> 08:12.440
could learn its new parameters and learn how to tune itself under this new conditions.

08:12.440 --> 08:17.920
If I can interrupt, let's maybe take a step back and talk through what it means to tune

08:17.920 --> 08:23.240
a laser. What are those knobs that are used in tuning them?

08:23.240 --> 08:30.000
Sure, sure. So the laser that I've mostly worked on is a fiber optic laser. So it's nice.

08:30.000 --> 08:36.160
It has its own cavity. It's just the laser, the fiber itself contains the light. And the

08:36.160 --> 08:40.520
way that the mode, what's called mode locking happens, in other words, you want to create

08:40.520 --> 08:46.520
a nice, very high intensity pulse of light. And in order to shape the light, there are

08:46.520 --> 08:53.120
a set of polarizers and some waveplates that you can turn, you know, you can rotate them

08:53.120 --> 08:58.520
through 360 degrees. And it turns out, as you turn these set of waveplates and polarizers,

08:58.520 --> 09:04.320
you can get very different behavior out of the laser cavity. And so it's a pretty, you

09:04.320 --> 09:08.900
know, there's about five knobs. There's three waveplates, a polarizer, and also the

09:08.900 --> 09:13.960
amount of pumping or energy put into the cavity. And with these five parameters, you can

09:13.960 --> 09:20.240
get either very exceptionally high performing lasers or you can get just garbage. It's completely

09:20.240 --> 09:28.240
in coherent light that's worthless. So the idea is to use these five parameters to

09:28.240 --> 09:36.560
tune this thing into not only a very coherent ultra short pulse with very high peak intensities,

09:36.560 --> 09:42.280
but among all the set of, there's these islands of stable behaviors, which one should you

09:42.280 --> 09:50.960
be on to give you the very highest performance possible for the fiber and for the gain you

09:50.960 --> 09:58.640
have available to you. The approach you took was an interesting one. It was ultimately,

09:58.640 --> 10:04.520
you know, in terms of the spectrum of machine learning applications, it was fairly simple,

10:04.520 --> 10:10.560
but the way you constructed the problem, you know, is what lent it to that simple approach.

10:10.560 --> 10:15.640
Can you kind of talk through the problem construction and ultimately the solution?

10:15.640 --> 10:20.560
Yeah, sure. So, of course, the first thing I, I guess, one way I think about machine learning

10:20.560 --> 10:24.880
in general is the first thing you have to pause it is some kind of loss function, right?

10:24.880 --> 10:29.280
So this is, this is the, I mean, I think of all of machine learning AI is just really

10:29.280 --> 10:34.360
just some giant optimization problem. And so the main thing with an optimization is just

10:34.360 --> 10:38.800
figuring out what your objective function. The nice thing for us in the laser system

10:38.800 --> 10:45.640
is it's very clear what the objective function was. We wanted a very coherent high intensity

10:45.640 --> 10:52.160
pulse. Now, interestingly enough, in optics, you, you don't actually measure the electric

10:52.160 --> 10:57.720
field. You, you measure the intensity of electric field. So you lose phase information. So right

10:57.720 --> 11:02.200
away, you're limited in terms of what your measurement space is. And there's also one

11:02.200 --> 11:05.960
of the interesting thing. You might say I want the maximum energy in the cavity, but that

11:05.960 --> 11:11.600
has actually not the best objective function available to you. It turns out if you just

11:11.600 --> 11:18.000
make the objective function the energy, then what it will do is the best performance of

11:18.000 --> 11:22.760
the laser cavity will put you at the edge of instability. So it's basically a walk you

11:22.760 --> 11:27.440
up this, it'll walk you up to the edge of cliff. And the cliff, of course, has the best

11:27.440 --> 11:34.280
performance, but you move slightly one way and you lose everything. And so we've formulated

11:34.280 --> 11:39.160
a new objective function, which was based upon not only do I want a lot of energy in the

11:39.160 --> 11:46.160
cavity, I also want to constrain the laser bandwidth. So we were able to construct an objective

11:46.160 --> 11:51.480
function that had both the bandwidth and the energy. And that actually allowed us to keep

11:51.480 --> 11:57.480
a boundary away from the instability. So once we have that objective function, by bandwidth

11:57.480 --> 12:02.880
are you ultimately talking about coherence or, but you said you didn't, you weren't able

12:02.880 --> 12:06.640
to directly measure coherence if I remember correctly. Well, so we can't measure the

12:06.640 --> 12:10.840
coherence directly, but we can measure the energy. We can also measure through the

12:10.840 --> 12:17.480
spectrogram, just what the total frequency content is. So when you look on a oscilloscope,

12:17.480 --> 12:21.560
you'll be able to see immediately what the band, the total bandwidth is. And so you're

12:21.560 --> 12:28.080
trying, you know, for a very short pulse, you have a nice broad bandwidth pulse. So typically,

12:28.080 --> 12:33.600
for these laser systems, they can produce somewhere around 30 nanometers of bandwidth. So

12:33.600 --> 12:39.880
it's pretty broad band. It's around telecom wavelengths. And so it's a great light source

12:39.880 --> 12:46.600
this way. So we were able, so we know then that what we were trying to do roughly speaking

12:46.600 --> 12:50.280
is, you know, produce this. And so we were able to construct the objective function.

12:50.280 --> 12:55.120
That was the number one step initially was just to simply figure out, given for what

12:55.120 --> 13:01.040
we think we want, how do we actually mathematically construct this for an algorithm to go after

13:01.040 --> 13:06.440
the search, right, go after the optimization. So we were able to do and also make sure

13:06.440 --> 13:10.960
that we stay away from the edge of instability for physical systems, right? It really matters

13:10.960 --> 13:15.440
that you've got to have a cushion away from the boundary. You can't just walk up a gradient

13:15.440 --> 13:19.040
and off a cliff. Otherwise, it's it's worth this technology.

13:19.040 --> 13:25.280
And did you consider, as opposed to the objective function with two components, energy and

13:25.280 --> 13:33.480
bandwidth, some formulation that tries to maximize energy, but then back it off or maybe even

13:33.480 --> 13:38.360
out, even outside of the machine learning, maximize energy, but then in the implementation,

13:38.360 --> 13:44.720
back it off a little bit. Yeah, we had various perspectives on this. We

13:44.720 --> 13:48.720
automatically could do something like this, which is once we have the learning, we could

13:48.720 --> 13:53.600
back off of these things. But it turned out it was so easy to use this scope tray. So

13:53.600 --> 13:58.520
in others, that was that was probably the easiest measurement of all that we use that directly

13:58.520 --> 14:03.320
as, okay, this is a perfect measurement because that's something that is is actually probably

14:03.320 --> 14:08.720
the easiest thing for to actually make a measurement of you. It's actually very difficult to

14:08.720 --> 14:13.680
even measure the light pulse itself in the cavity because typically these pulses are

14:13.680 --> 14:20.080
in the 100 attempt to second regime. So this is ultra fast optics. And so measurement

14:20.080 --> 14:26.000
of the light pulse itself requires you either to put on some kind of very expensive measurement

14:26.000 --> 14:29.680
device. And of course, you don't want that in a practical setting because you'd like

14:29.680 --> 14:37.160
a cheap device. And so we just use the bandwidth itself as a proxy. Okay. And then once

14:37.160 --> 14:42.320
you have that, you can now you can set the thing free. It's got five input knobs, these

14:42.320 --> 14:48.800
four or three wave plates of polarizer, the gain, and you just say, okay, go look around.

14:48.800 --> 14:53.360
And the nice thing is these parameters, so this is another nice feature. These wave plates

14:53.360 --> 14:59.160
and polarizers, right, they, they, they, there's two pi periodic, right? You rotate a polarizer

14:59.160 --> 15:04.600
around, it goes around two pi, you're back where you started. So it allowed us to do what

15:04.600 --> 15:09.720
we call the toroidal search, right? So you could start spinning these wave plates and polarizers

15:09.720 --> 15:15.480
at irrationally related frequencies. And what this will do, it was will cover some kind

15:15.480 --> 15:21.560
of hyper torus. And so it's a very effective search strategy because if you have parameters

15:21.560 --> 15:28.080
that don't wrap around themselves, right? So the slowest, the slowest search, when it

15:28.080 --> 15:34.200
goes around two pi, all the other ones have gone around even multiples times, it allows

15:34.200 --> 15:39.000
you to map out this, this torus structure and look in the torus where the good regions

15:39.000 --> 15:42.320
are for, for these, for these poll solutions.

15:42.320 --> 15:46.800
There was an element that I recall from the presentation that was not just that you

15:46.800 --> 15:54.000
were able to optimize the subjective function, but that you were also able to do it in such

15:54.000 --> 16:02.480
a way that gave you insight into the underlying physical principles. Can you, are we at that

16:02.480 --> 16:08.360
point in kind of the story yet? Or yeah, we can certainly talk about that because I think

16:08.360 --> 16:13.840
that's actually one of the more exciting places. So we started with this idea of machine

16:13.840 --> 16:18.400
learning on the laser for control, which was, you know what, if this thing can just tune

16:18.400 --> 16:26.240
it, we're fine. It can be a sort of an AI agent sitting on top of sort of these machines

16:26.240 --> 16:32.120
and you just give it the knobs input output and get it going. But the, where this led to

16:32.120 --> 16:37.600
was this idea of data driven discovery of governing equations. So we started thinking

16:37.600 --> 16:44.080
about if I measure system and I don't know it's underlying physics. So a lot of the models

16:44.080 --> 16:50.480
I had spent time with in the past, I actually knew the governing physics. Optical fields

16:50.480 --> 16:54.080
are governed by Maxwell's equations and of course they have these quantum interactions.

16:54.080 --> 16:58.160
So we can write down these governing equations and, you know, we've had a lot of experience

16:58.160 --> 17:04.560
with them. We know that they are true in, in these regimes we're operating in and so

17:04.560 --> 17:09.920
we can do in some sense first principles modeling. But another part of my life that developed

17:09.920 --> 17:14.000
over the last five to ten years was looking at neuroscience. So now you'd measure from

17:14.000 --> 17:20.280
the brain and we don't know the governing equations of the brain. We don't have an F equals

17:20.280 --> 17:26.240
M A formulation. In other words, here are the governing equations. We have posited various

17:26.240 --> 17:34.000
models for what neurons do. But on some level you're interrogating a system where you fundamentally

17:34.000 --> 17:41.040
don't know the governing equations. And so what we started to, what we set out to do then

17:41.040 --> 17:46.720
is to start figuring out, is there a way from measurements alone, time series measurements,

17:47.360 --> 17:53.520
can we infer the governing equations? And the beauty of this is it all comes down to just

17:53.520 --> 17:58.560
Ax equal to B. So we're excited because that's, you know, I like to tell everybody that's

17:58.560 --> 18:03.680
the height of applied math. If you actually understand Ax equal to B, that's something because

18:03.680 --> 18:08.640
Ax equal to B is a lot harder than it looks. You know, when I think of a Maxwell's equation,

18:08.640 --> 18:14.560
it's a lot more complicated than a linear, a linear equation. How, talk a little bit about that

18:14.560 --> 18:22.000
background of how you kind of equate these ultimately complex and and higher order equations

18:22.000 --> 18:28.640
to a linear relationship? Sure, you bet. So the way we've formulated the problem is we take time

18:28.640 --> 18:34.560
series measurements of some dynamical systems. So we think of these things as systems were

18:34.560 --> 18:39.760
measuring, they're changing in time, they're evolving. And we would like to discover the equations

18:39.760 --> 18:48.400
of motion. So one of the my backgrounds mathematically that really framed everything I did application

18:48.400 --> 18:55.840
wise was in dynamical systems. And the basic structure for that is to look at DXDT. The change of

18:55.840 --> 19:02.800
some vector X is equal to F of X. So it could be a nonlinear function of X. It could be

19:02.800 --> 19:08.400
very complicated. It could even be have parametric dependencies. So that's the simplest thing.

19:08.400 --> 19:16.480
DXDT equals F of X. And this is already framing what we want to do. And in a sense that the

19:16.480 --> 19:22.400
standard way we think about when we pick up a textbook on E and M or quantum, we're given F.

19:22.400 --> 19:28.160
We know what it is. Now what we're trying to do is say you give me time series measurement X.

19:28.160 --> 19:34.320
Can I tell you what F was? What was the F that's consistent with the data I read? So

19:34.320 --> 19:40.240
DXDT is the first derivative of with respect to time. And what you're doing is you're taking

19:41.360 --> 19:46.880
you've got some time series measurements that you're taking and you're using them to ultimately

19:46.880 --> 19:53.200
work your way back to the underlying function from these time series measurements. Exactly.

19:53.200 --> 19:58.960
And so the way we do this is we simply say the following. We say, okay, DXDT. So let's call

19:58.960 --> 20:05.440
that X dot. So it's the rate of change of time. So if you give me time series data, I can differentiate

20:05.440 --> 20:13.360
it. But you gave me X. And that means I can compute X dot. Okay, that's for an X dot is going to be

20:13.360 --> 20:19.920
the B vector in AX equal to B. Now let's talk about what the A is going to be. So now what

20:19.920 --> 20:24.000
we're going to do is say, well, what could the right hand side be? Well, the right hand side could

20:24.000 --> 20:28.400
be a lot. There's lots of possibilities. And I don't know what they are. So what I'm going to do

20:28.400 --> 20:33.440
is just make up a library of all kinds of potential right hand sides. Well, maybe the right hand side

20:33.440 --> 20:42.480
depends on X or X squared or or X cubed or sine X or E to the X. It's only limited by my imagination.

20:42.480 --> 20:49.440
So the idea is typically to put in a bunch of potential functions, which could be right hand side

20:49.440 --> 20:57.920
terms. So the matrix A we're going to build is the following. Each column will be a candidate

20:57.920 --> 21:05.360
right hand side functions, a candidate that for describing what F is. Okay, so I could put in

21:05.360 --> 21:12.640
thousands of candidates. And that's what creates the columns of the matrix A. And each row is the

21:12.640 --> 21:19.280
time measurement. So if you give me X, I can compute X squared. I can compute sine X over time.

21:19.280 --> 21:25.120
So the rows will be time columns will be potential functions that might be in the right hand side.

21:25.120 --> 21:30.160
That's the matrix A. So what we're going to do when we do this regression, this AX equal to B is

21:30.160 --> 21:34.640
just say, well, here's a matrix A. The right hand side B I have, which is the time derivative,

21:34.640 --> 21:39.680
solve it. And generally, you're going to have a lot of time measurements. So this is going to be

21:39.680 --> 21:45.760
a system, which is highly over determined. And I want a solution. And here's the key. I want a

21:45.760 --> 21:52.240
sparse solution. So what's going to happen is when you do this regression, it's going to select

21:52.240 --> 22:00.240
out the smallest number of columns that they have to use so that X dot is equal to F of X.

22:00.240 --> 22:08.000
So, and when it does this, it selects out the physical terms. So for instance, what this will do

22:08.000 --> 22:14.000
for you is if you measure a fluid flow, just take measurements of the time series of a

22:14.000 --> 22:20.000
vorticity of a fluid flow, what comes out in this regression process is the navier stokes equations.

22:20.000 --> 22:25.600
If you were to look at trajectories of a flying object, thrown object, what would come out would

22:25.600 --> 22:31.600
be F equals M A? It recovers all of these systems, all the canonical models out of mathematical

22:31.600 --> 22:36.800
physics. You just give me the time series measurements. It recovers back the governing equations,

22:36.800 --> 22:43.760
all by this simple regression of a linear system where the matrix contains all these potential

22:43.760 --> 22:48.400
library terms. And by promoting a sparse solution, it just gives you back your solution.

22:48.400 --> 22:55.680
Yeah, so I thought this part was really clever. Yeah, again, as you just said it, you are solving

22:55.680 --> 23:03.520
just a simple linear regression, but your terms are, you know, they can be anything. They can be

23:03.520 --> 23:10.480
quadratics and qubits and exponentials and all these different things. What is this? You know,

23:10.480 --> 23:16.240
in a lot of ways, it's like a decomposition. Do you consider this part of what you did to be

23:16.240 --> 23:24.640
kind of a novel contribution of your researcher is that, you know, is it really more, you know,

23:24.640 --> 23:31.680
standard or well explored and you're applying it to these types of physical problems?

23:31.680 --> 23:38.400
As far as we've seen, this is the first application of this towards really discovering dynamics

23:38.400 --> 23:46.240
itself and getting out dynamical systems. What's kind of remarkable about it is, in some sense,

23:46.240 --> 23:52.320
how to say it in a good way, we had no business discovering this. While this wasn't discovered

23:52.320 --> 23:57.920
30 years ago, it's so simple, right, that like, you know, it's like, why didn't someone do this 30

23:57.920 --> 24:01.840
years ago? They should have. They should have. They really should have. And it's really interesting.

24:01.840 --> 24:06.000
I think from the physics community and the engineering community, and I'm going to talk

24:06.000 --> 24:11.760
personally for a minute here, if I look back at my educational experience, whenever we solved

24:11.760 --> 24:17.280
under or over determined systems that were AX equal to B, you just did it with Lee Square fitting.

24:18.000 --> 24:23.120
There was no concept of sparse solutions there. You know, it's interesting because

24:23.120 --> 24:28.160
Tip Shirani has a paper on the last so method from 1996, which have gotten a ton of traction,

24:28.160 --> 24:34.160
this basically variable selection technique, but that never made it over to like this physics

24:34.160 --> 24:39.120
engineering community. And in some sense, the technology has been just sit there, and we just

24:39.120 --> 24:44.320
finally made the observation. Well, wait a minute. This actually can work here and look at this.

24:44.320 --> 24:49.920
It actually works. And you can just run it on your laptop. It's not like this. It's not one of these

24:49.920 --> 24:57.040
neural, you know, neural networks or even another competing technology developed by Hod Lipson at

24:57.040 --> 25:02.320
he's now Columbia. They had done something similar, but they had done a genetic search. So,

25:02.320 --> 25:08.080
you know, they basically would put a genetic algorithm that's positing some right hand side terms,

25:08.080 --> 25:12.560
which ones are best. Let's keep them, let's modify them and have them evolve until we find this.

25:12.560 --> 25:17.840
So it was fairly expensive computationally, because it was really going through tons of

25:17.840 --> 25:25.760
possibility versus this regression is extremely simple. It's fast, it's effective, it's very robust.

25:25.760 --> 25:31.920
And since that time, we've been starting to use this on more complicated problems where we

25:31.920 --> 25:36.320
don't know the answer. So for all the problems that we know the answer, which are these canonical

25:36.320 --> 25:40.880
models out of the mathematical physics engineering community, we've been able to discover them all.

25:40.880 --> 25:46.080
So we've been able to sort in some sense validate or cross validate the method. And now we're

25:46.080 --> 25:51.120
moving it on to biological applications. And one of the exciting areas we've been trying

25:51.120 --> 25:57.120
to find this on is the sea elegance worm, which is a one millimeter worm. It's got 302 neurons.

25:58.480 --> 26:03.600
And when you watch these neurons, they can now do whole brain imaging of this little worm.

26:03.600 --> 26:09.920
It's amazing what people can do with this worm. We are now starting to understand that when the

26:09.920 --> 26:14.800
worm is forward crawling or backward crawling, it's very low dimensional structure that's

26:14.800 --> 26:18.960
being executed there. And what we really want to understand is the control architecture

26:18.960 --> 26:25.040
and the governing equations of how 302 neurons coordinate their dynamics to produce

26:25.920 --> 26:32.160
very simple dynamics, which are functional across the entire connectome of that worm.

26:32.160 --> 26:37.840
So this is an exciting way forward because there is no truth in this case. Nobody has the right

26:37.840 --> 26:43.200
answer, right? So this is going to give us the ability to get towards understanding biological

26:43.200 --> 26:49.360
neural systems with a very new architecture, which seems to be incredibly robust and easy to use.

26:51.440 --> 26:58.240
So I want to dig a little bit further into the biological applications. But before we do that,

26:58.240 --> 27:07.120
I wanted to connect a couple of points that you mentioned. You mentioned the idea of

27:07.120 --> 27:16.400
sparsity as being key to what you've done here. And the idea there is that if you collect all

27:16.400 --> 27:24.320
these data points, even if the equations are perfectly expressed in simple underlying

27:24.320 --> 27:31.680
relationships between these terms, you've got noise, you've got other things, and the result of

27:31.680 --> 27:40.560
your analysis, your optimization is going to likely include some very small fractional

27:40.560 --> 27:46.800
terms of things that don't necessarily need to be there. And so what you mentioned,

27:47.600 --> 27:52.240
the lasso work in passing, if I remember correctly, what you did was you implied lasso to

27:54.000 --> 27:57.600
bring out that sparsity to reduce those terms. Is that correct?

27:57.600 --> 28:03.360
Yeah, so we use something lasso-like. So what we actually did, and this is

28:04.880 --> 28:09.920
maybe taken as a criticism alasso, it's interesting if you really look at what people do,

28:09.920 --> 28:17.200
people tend not to use lasso directly, but something like elastic net. So the lasso itself is

28:17.200 --> 28:22.640
a little bit unstable in terms of actually finding for you the right sparsity pattern that you're

28:22.640 --> 28:30.560
looking for. And so elastic net helps regularize this process. But we actually avoid those

28:30.560 --> 28:38.240
altogether. What we do is we do a sequential least square thresholding. So essentially least

28:38.240 --> 28:44.480
squares is extremely fast, but what least squares will do is says everybody participates. So there

28:44.480 --> 28:50.480
is nobody that's zero. However, there's a bunch of terms that, well, you know, it depends on how

28:50.480 --> 28:58.000
you define zero. It's 10-6-0. So we start doing this in the sense that we say, it works incredibly

28:58.000 --> 29:03.120
well where you can just do these squares, which is very fast. All the terms below some threshold,

29:03.120 --> 29:08.000
you throw away. And then you do it again with the terms that are left over, and this thing converges

29:08.000 --> 29:15.680
right back right to the solution you want. So it's in some sense a more stable way. We found

29:15.680 --> 29:21.680
it to be stable. We're working actually right now on convergence proofs around this, but we found

29:21.680 --> 29:30.480
it to be a very stable robust way to do sparse regression. And so it's lasso-like, but it doesn't

29:30.480 --> 29:37.840
inherit some of the known problems of lasso itself. How do you end up setting the threshold? If you're

29:37.840 --> 29:45.120
working on modeling something that's got a constant like a Planck's constant that's ultimately

29:45.120 --> 29:50.960
very, very small, how do you make sure you're not getting rid of important things? Yeah, so that's

29:50.960 --> 29:56.240
always a great question. I mean, so what we've been doing so far for most of the models that we have

29:56.240 --> 30:04.640
is we will do some kind of cross validation for this threshold level. And actually it's interesting

30:04.640 --> 30:10.320
the threshold level, we haven't done this yet, but probably the most effective way to do it is to

30:10.320 --> 30:16.880
have an adaptive threshold level. So if you're on threshold, if you're going to apply the threshold

30:16.880 --> 30:20.880
and your iteration number three, it should probably be different than on iteration number two.

30:20.880 --> 30:26.640
But even if you don't do something like that, you can typically learn it through cross validation

30:26.640 --> 30:32.480
what the sparsity, what the threshold level should be. But that is your one tuning knob. And by the

30:32.480 --> 30:41.920
way, it's very, very similar to last so where in the last so you have to tune what this L1 penalty

30:41.920 --> 30:48.800
strength is. So typically there's this lambda parameter where you can make it bigger or smaller.

30:48.800 --> 30:55.200
And the bigger you make it, the more it enforces sparsity. And that's very similar to the threshold

30:55.200 --> 31:02.640
here, which is it's it's a tuning knob that all of these sparsity promoting techniques seem to

31:02.640 --> 31:09.280
carry around. And again, there are ways that people suggest or argue you can kind of cross validate

31:09.280 --> 31:14.800
to get the right values for these things. But it's still a little bit of a still a little bit of

31:14.800 --> 31:20.560
alchemy on that one. Okay. And so you were starting to introduce us into to some of your future

31:20.560 --> 31:27.200
application of this moving away from these underlying physical control systems where we we know a

31:27.200 --> 31:35.600
lot of the equations to something that is more complex and unknown. And in this case, the

31:36.640 --> 31:42.960
the relationship between neurons and the C elegans. Is that the right way to say it? Yeah, yeah,

31:42.960 --> 31:48.640
exactly. And I would say more broadly, the thing that we're really trying to go after

31:48.640 --> 31:56.400
sort of in sort of the grand challenge problem of this, I still remain very rooted in thinking

31:56.400 --> 32:02.720
about engineering and physical applications. But one of the things that's become increasingly

32:02.720 --> 32:11.440
clear to me is what we've developed over the last centuries is the ability to exceptionally well

32:11.440 --> 32:17.120
with what I call uniscale physics problems. In other words, I can write down one set of equations

32:17.120 --> 32:22.800
and models in the whole system. But it's increasingly clear that a lot of these complex systems we

32:22.800 --> 32:29.040
look at that we want to understand today, they're multi-scale systems. There's a fast scale that

32:29.040 --> 32:35.360
is doing some kind of physical process that's connected to some mezzo scale, which is connected

32:35.360 --> 32:40.480
to some macro scale. And by the way, they're all talking to each other. And they're all very

32:40.480 --> 32:47.920
different physics at these levels. And yet that is the dynamics that really produces the manifestation

32:47.920 --> 32:52.240
of the dynamics on the measurement space you may be looking at. So for instance, you may not care

32:52.240 --> 32:56.400
about the micro scale because you're watching it from the macro scale. But whatever happened

32:56.400 --> 33:01.840
in the micro scale is in fact feeding that information up. And you need to, if you're going to model

33:01.840 --> 33:06.720
the system well, you have to understand how these different scales are talking to each other.

33:06.720 --> 33:12.400
So it really requires us to start moving away from uniscale physics models, where I just write

33:12.400 --> 33:17.600
everything down and do simulations, too. Actually, there's different physics at each level.

33:17.600 --> 33:23.280
I've got to figure out how they're connected together and discover different physics models at

33:23.280 --> 33:29.440
each level. And that's going to be the way we make progress forward. At least that's those,

33:29.440 --> 33:34.240
and by the way, biology is a perfect representation of this. Even in the sea elegans, there's

33:34.240 --> 33:40.240
quite a different number of different time scales that ultimately produce time and spatial

33:40.240 --> 33:44.720
scales that ultimately produce the thing that I'm most interested in, which is this macroscopic

33:44.720 --> 33:50.960
behavior of the worm crawling forward or crawling backward or doing what are called omega turns,

33:50.960 --> 33:56.320
where it turns its body around. And the only way to understand that completely is to understand

33:56.320 --> 34:02.160
this architecture of multi-scale physics. And so to make that a little bit more concrete,

34:02.160 --> 34:09.200
we're talking about one scale might represent the chemical, the underlying chemical reactions,

34:09.200 --> 34:15.760
and then there's a scale above that that's representing neurological activity and connections

34:15.760 --> 34:22.240
and things like that. And then finally, at the top scale, how the entire set of neurons are

34:22.240 --> 34:28.640
sort of evolving together in a functional form, all being driven by the chemistry, which drives

34:28.640 --> 34:33.760
the voltage activity of neurons, which then drives this bigger functionality of muscle activation.

34:34.960 --> 34:40.080
Yeah, that's exactly the cascade. And you know that the muscle activation is a very different physics

34:40.080 --> 34:46.000
than the chemical activations, right? These are different physical processes. And you can't just

34:46.000 --> 34:52.160
write one model down that says, you know, and again, I think the discovery we've made in physics,

34:52.160 --> 34:56.240
right? We have Maxwell's equation, we have quantum mechanics, we have the fluid dynamics,

34:56.240 --> 35:01.680
navier strokes of fluids, we've done impressive things, right? We can build airplanes,

35:01.680 --> 35:07.440
we can build iPhones, you know, quantum based devices. The kind of stuff we can do is quite

35:07.440 --> 35:12.400
remarkable, but we've kind of hit a wall there in terms of our modeling capabilities.

35:13.440 --> 35:21.920
It strikes me that in the physical case, you know, we've got these multi-scale problems do exist.

35:21.920 --> 35:26.720
Maybe we just don't understand them because of their complexity or the number of skills or

35:26.720 --> 35:32.480
things like that, meaning, you know, at some level, there's quantum mechanics and there are

35:34.320 --> 35:41.280
I'm thinking even for simple kind of mechanics problems, you know, there's, you know, maybe even an

35:41.280 --> 35:47.920
infinite number of levels, you know, ranging to, you know, atomic cohesion and all kinds of stuff,

35:47.920 --> 35:55.120
I don't know. Yeah. Are you getting a question in there somewhere? Yeah. No, but it's actually

35:55.120 --> 35:58.880
interesting you asked this, right? Because I think the way I think of multiple scale physics,

35:58.880 --> 36:04.960
like, you know, if I was in a push a table and see how it moved, I push it with a certain force,

36:06.240 --> 36:11.040
you know, typically we do that, we draw like a vector diagram, okay? Your force is this,

36:11.040 --> 36:15.200
you know, this much at this angle and you could compute through ethical, so may what's going to

36:15.200 --> 36:22.000
happen to the table movement. An alternative to this would be I'd model the atomic interaction

36:22.000 --> 36:26.800
between my hand pushing the table and the atoms of the table. Of course, nobody does this, but

36:26.800 --> 36:31.520
there are systems where you're required to do that. You really need to do that kind of, so

36:31.520 --> 36:36.880
especially molecular dynamics simulations, this is what they're trying to do, right? Really

36:36.880 --> 36:45.040
model the molecular interactions that produce some macro scale structure that's important to see.

36:45.040 --> 36:51.120
Okay. Okay. So that's where we're headed, I think. Interesting. And so how far have you gotten

36:51.120 --> 37:00.880
in applying this technique to these kinds of multi-scale problems? So far, we have a first paper that

37:00.880 --> 37:07.520
just came out, it's on the archive, where we did essentially some toy problems where we've had mixed

37:07.520 --> 37:13.840
dynamics, a fast scale and a slow scale dynamical system and we asked the following question. If I

37:13.840 --> 37:19.760
just gave you the data and I'm just measuring this joint dynamics together, what would I get?

37:21.040 --> 37:25.680
Do I actually, can I extract them from each other? Can I, is there a way to disambiguate what's

37:25.680 --> 37:30.800
going on in the fast scale from the slow scale and could I really discover these two dynamics? And

37:30.800 --> 37:36.640
the initial evidence is suggest yes, you can. In fact, you can discover a slow and a fast dynamics,

37:36.640 --> 37:41.280
you can even discover how they're connected together. So we've done this on some, let's call it the

37:41.280 --> 37:45.840
baby toy problem. I mean, that's where you always have to start. I always like to start with something

37:45.840 --> 37:51.360
where we can test the truth against it, right? One of the things that makes me uncomfortable in some

37:51.360 --> 37:57.200
of the application areas of some of the, these methods that I've seen is people will do hard problems

37:57.200 --> 38:01.200
and sometimes they'll neglect to do the very simple problem to see if it even just works on the

38:01.200 --> 38:05.760
simple problem. And I never go forward unless I can get it to work on a simple problem because

38:05.760 --> 38:09.600
if you can't get that to work, it's certainly not going to say anything about a hard problem.

38:09.600 --> 38:17.520
Right. And so I think the ability to test bet it and see if it works is important. So we've got

38:17.520 --> 38:22.080
that first result along that direction and then our hope is to continue building this way.

38:22.080 --> 38:28.480
And then also acquiring good data sets is a really important thing. You know, getting a good

38:28.480 --> 38:33.360
data set from a, you know, physical biological system where there's good time resolution, good

38:33.360 --> 38:38.160
spatial resolution where you can really bring these methods on. We're getting better and better

38:38.160 --> 38:42.000
about how to handle bad data. But you know, at the end of the day, there's a great deal of pressure.

38:42.000 --> 38:47.680
If you want to build a good model, you need good data, right? I think that's one of the really

38:47.680 --> 38:53.760
big advantages of computer vision, right? You can, you can bring in a bunch of really good

38:53.760 --> 39:01.200
data sets, right? Here's pictures. They're high quality HD cameras. And that helps a lot. They're

39:01.200 --> 39:07.280
not, you imagine if you were trying to train image net where all the images were corrupt and blurred.

39:07.280 --> 39:11.760
You know, how well would it do now on classification tasks? I think it would be much

39:11.760 --> 39:16.800
challenging. And the same thing for us is the case is we want to be able to have really high quality

39:16.800 --> 39:21.600
of data initially. And you know, the biologists are, are getting their job done. It's amazing

39:21.600 --> 39:26.240
the revolutions you're seeing in biology in terms of the kind of recordings they can do.

39:28.000 --> 39:32.880
The, just the vast sheer quantity of what they're able to extract now from even in the

39:32.880 --> 39:37.360
C. elegans worm. They'll be able to image this thing in real time where you're seeing the whole

39:37.920 --> 39:45.840
brain region 150 neurons just lighting up as it's doing its, its motion is, is just unbelievable.

39:45.840 --> 39:50.160
This is something that 10 years ago, I don't think anybody thought was possible. And here we are.

39:50.160 --> 39:56.560
We've got it. So it's really exciting. So you started to answer one of the questions I had, which

39:56.560 --> 40:04.160
was for these different scales for C. elegans, where the data was coming from. It sounds like

40:04.640 --> 40:12.240
at some level, at one of the levels you described is coming from imagery. But is there also a level

40:12.240 --> 40:20.320
beneath that that is looking at actual chemical activity? And I imagine the level above where we're

40:20.320 --> 40:26.720
talking about physical motion is coming from imagery as well. Yeah. So the motion comes from imagery.

40:26.720 --> 40:31.520
A lot of times you can easily just see it moving around. There's the actual imaging of the neurons.

40:32.480 --> 40:38.080
One level down from that would be electrophysiology recordings where you actually stick in a

40:38.080 --> 40:43.600
voltage probe into neurons and you monitor those directly. And they do have this available in

40:44.320 --> 40:49.920
monkeys and rats and other creatures like this where you can actually get some direct recordings

40:49.920 --> 40:55.280
from neurons to see how they're spiking. And so there, of course, is a little harder because,

40:56.080 --> 41:02.240
even if even in your best days you might be able to get a tetrode fork in the brain with

41:02.240 --> 41:09.920
the four prongs or the fork have four recordings each. So maybe get up to 16. And so we can do this

41:09.920 --> 41:15.520
for instance with insects and tenelopes of moths. We work with a gentleman over here in biology,

41:15.520 --> 41:20.880
Jeff Riffle, who has this. So there is more and more data becoming available, but it's still

41:20.880 --> 41:25.280
hard to acquire. And in humans, of course, it's very difficult, right? Because you can't just

41:25.840 --> 41:30.880
go in and put probes in people. They have done this, of course, with certain people like, for

41:30.880 --> 41:36.320
instance, we are working with people with where there are e-cog recordings from the brain,

41:36.320 --> 41:42.000
often a big array. But these were from patients who needed this for because they had such severe

41:42.000 --> 41:48.560
epilepsy that they put this into monitor that. And as a byproduct, we actually have great data

41:48.560 --> 41:54.720
to collect on the brain itself. So we'll see what the future holds in terms of what it's, I think

41:54.720 --> 41:59.360
it's just remarkable how they keep coming up with new and novel ways. I think what we want to do

41:59.360 --> 42:04.560
is be prepared on our end to have the mathematical architecture in place so that when they do produce

42:04.560 --> 42:10.320
these data sets, we can essentially squeeze out as much as possible, as much learning and science as

42:10.320 --> 42:15.840
possible. I think that's what we're really after is that is the mathematical engine so that as

42:15.840 --> 42:21.680
these data sets come in, we can we have a new way to start interrogating that data and discovering

42:21.680 --> 42:27.360
underlying biophysical principles or physical principles. Well, that's fantastic. Well, Nathan,

42:27.360 --> 42:33.520
I really appreciate you taking the time to walk us through this. It's really, really interesting work

42:33.520 --> 42:38.000
and I'm looking forward to kind of keeping tabs on it and seeing what else you guys you all come

42:38.000 --> 42:42.800
up with. Great. Thank you so much for having me. It's been a pleasure and hopefully, hopefully

42:42.800 --> 42:47.040
everybody who listened got something out of this. Absolutely. I'm sure they will. Thanks Nathan.

42:47.040 --> 42:47.920
Okay, take care.

42:47.920 --> 42:57.920
All right, everyone. That's our show for today. For more information on Nathan or any of the topics

42:57.920 --> 43:07.760
covered in this episode, head on over to twomolei.com slash talk slash 162. Don't forget to visit twomolei.com

43:07.760 --> 43:15.680
slash nominate to cast your vote for us and the people's choice podcast awards. And as always,

43:15.680 --> 43:24.400
thanks so much for listening and catch you next time.

