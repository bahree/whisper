1
00:00:00,000 --> 00:00:15,320
Welcome to the Tumel AI Podcast, I'm your host Sam Charrington.

2
00:00:15,320 --> 00:00:23,120
Hey, what's up everyone?

3
00:00:23,120 --> 00:00:26,720
Before we get to the show, I'd like to send a huge thanks to everyone who participated

4
00:00:26,720 --> 00:00:32,720
in last week's first ever interactive podcast listening session with Kwak Le.

5
00:00:32,720 --> 00:00:36,360
Kwak and I had a great time answering your questions and discussing his perspective

6
00:00:36,360 --> 00:00:41,000
on the state of deep learning research and all the cool things he's up to at Google.

7
00:00:41,000 --> 00:00:44,680
If you missed the session, we'll have a transcript of the Q&A linked from the show notes

8
00:00:44,680 --> 00:00:50,520
page at twumelai.com slash talk slash 366.

9
00:00:50,520 --> 00:00:55,840
That said, I am pleased to announce our next live program, responsible data science in

10
00:00:55,840 --> 00:01:01,160
the fight against COVID-19, which will be live streamed this Wednesday, April 22nd at

11
00:01:01,160 --> 00:01:03,080
noon pacific time.

12
00:01:03,080 --> 00:01:07,240
Since the beginning of the coronavirus pandemic, we've seen an outpouring of interest on the

13
00:01:07,240 --> 00:01:12,200
part of data scientists and AI practitioners wanting to make a contribution.

14
00:01:12,200 --> 00:01:16,240
At the same time, some of the resulting efforts have been criticized for promoting the spread

15
00:01:16,240 --> 00:01:20,800
of misinformation or being disconnected from the applicable domain knowledge.

16
00:01:20,800 --> 00:01:25,560
In this discussion, we explore the question of how data scientists and MLNI practitioners

17
00:01:25,560 --> 00:01:30,520
can contribute responsibly to the fight against coronavirus and COVID-19.

18
00:01:30,520 --> 00:01:34,760
This discussion will be streamed live on YouTube and other social media channels.

19
00:01:34,760 --> 00:01:40,160
For more information, visit twumelai.com slash RDSCOVID.

20
00:01:40,160 --> 00:01:42,320
And now on to the show.

21
00:01:42,320 --> 00:01:43,800
Alright, everyone.

22
00:01:43,800 --> 00:01:49,080
I am here in Vancouver at NURBS 2019, and I've got the pleasure of being seated with Ristu

23
00:01:49,080 --> 00:01:50,080
Mikkuleinen.

24
00:01:50,080 --> 00:01:56,000
Ristu is a former guest of the podcast, as well as an associate VP of Evolutionary AI at

25
00:01:56,000 --> 00:02:01,480
Cognizant and Professor of Computer Science at the University of Texas at Austin.

26
00:02:01,480 --> 00:02:04,640
Ristu, welcome back to the Twimal AI podcast.

27
00:02:04,640 --> 00:02:05,640
Thank you.

28
00:02:05,640 --> 00:02:06,640
Pleasure to be here.

29
00:02:06,640 --> 00:02:11,840
So it's been a bit since we last spoke at least a year, because that's how long you've

30
00:02:11,840 --> 00:02:13,400
been at Cognizant.

31
00:02:13,400 --> 00:02:18,880
In addition to being at UT Austin, you were working on a startup called Sentient.

32
00:02:18,880 --> 00:02:21,160
Tell us a little bit about the story and transition there.

33
00:02:21,160 --> 00:02:22,160
Yeah, that's right.

34
00:02:22,160 --> 00:02:27,440
So Sentient was a startup in AI, Evolutionary AI in particular.

35
00:02:27,440 --> 00:02:34,280
And we had at Sentient, made a couple of products, one in stock trading, another one in Evolutionary

36
00:02:34,280 --> 00:02:37,440
Optimization of Webpages.

37
00:02:37,440 --> 00:02:41,600
And at one point, those were spun off at their own units.

38
00:02:41,600 --> 00:02:47,600
And the research team now moved to Cognizant, where we continue as an Evolutionary AI research

39
00:02:47,600 --> 00:02:52,040
team and powering applications, trying to take Evolutionary AI to the real world in multiple

40
00:02:52,040 --> 00:02:53,040
applications.

41
00:02:53,040 --> 00:02:56,760
Cognizant is primarily a consulting company?

42
00:02:56,760 --> 00:03:05,040
Yes, it has done a lot of outsourcing and now a lot of consulting in AI and data modernization,

43
00:03:05,040 --> 00:03:10,640
digitization, and AI is becoming a much bigger part of the company because that's a natural

44
00:03:10,640 --> 00:03:14,600
extension of digitizing and starting to work with data.

45
00:03:14,600 --> 00:03:22,440
And so before we started rolling, you mentioned that a big part of what this transition does

46
00:03:22,440 --> 00:03:27,440
for you and the team gives you more exposure to real world scenarios and maybe exposes

47
00:03:27,440 --> 00:03:32,000
some of the things that you need to be thinking about when you're trying to apply Evolutionary

48
00:03:32,000 --> 00:03:33,600
AI in the real world.

49
00:03:33,600 --> 00:03:35,080
Talk a little bit more about that.

50
00:03:35,080 --> 00:03:42,840
Yes, that's been a lot of fun doing consulting and doing modernization in many different industries.

51
00:03:42,840 --> 00:03:48,520
It turns out that AI is not just building something like self-driving cars that never existed

52
00:03:48,520 --> 00:03:53,160
before, but there's a lot of opportunity in just about everywhere to take advantage

53
00:03:53,160 --> 00:03:54,160
of AI.

54
00:03:54,160 --> 00:03:55,160
AI.

55
00:03:55,160 --> 00:04:00,320
And that's because almost all industries now have data.

56
00:04:00,320 --> 00:04:05,080
They collect data about the customers, about their products, how they work, how they sell,

57
00:04:05,080 --> 00:04:07,680
and also feedback on what works what doesn't.

58
00:04:07,680 --> 00:04:11,840
And that data provides a great opportunity to bring in AI, to learn from that data and

59
00:04:11,840 --> 00:04:13,680
then improve it in the future.

60
00:04:13,680 --> 00:04:16,440
And it turns out that it doesn't really matter what industry you are in.

61
00:04:16,440 --> 00:04:21,840
You might be in retail, healthcare, or oil exploration, or manufacturing, or many, many

62
00:04:21,840 --> 00:04:23,640
different industries.

63
00:04:23,640 --> 00:04:27,520
All of those are now on the verge of being able to take advantage of AI.

64
00:04:27,520 --> 00:04:29,040
And that's what we're trying to do.

65
00:04:29,040 --> 00:04:34,080
Take the technology there and make it actually work and add value.

66
00:04:34,080 --> 00:04:41,560
And so how is that change in perspective is kind of exposure to different use cases changed

67
00:04:41,560 --> 00:04:45,760
how you think about your particular work with evolutionary AI.

68
00:04:45,760 --> 00:04:50,160
And maybe we can start with having you do a little refresher or primer on evolutionary

69
00:04:50,160 --> 00:04:54,040
AI for folks who haven't yet heard your, you know, our first conversation.

70
00:04:54,040 --> 00:04:55,040
Sure.

71
00:04:55,040 --> 00:04:59,840
And it is different from what you might imagine what AI is today, because today, most

72
00:04:59,840 --> 00:05:04,960
of AI is taking that data and then building a model that predicts what will happen in

73
00:05:04,960 --> 00:05:05,960
the future.

74
00:05:05,960 --> 00:05:10,960
And that is, for instance, image recognition, object recognition, language understanding,

75
00:05:10,960 --> 00:05:15,560
speech recognition, all of those are trying to get AI to do what we already know how to

76
00:05:15,560 --> 00:05:16,560
do.

77
00:05:16,560 --> 00:05:22,760
Now, with evolutionary AI, we're trying to take the next step and that's making AI creative.

78
00:05:22,760 --> 00:05:28,000
Having AI actually come up with new solutions or designs.

79
00:05:28,000 --> 00:05:30,200
Discover something that does not already exist.

80
00:05:30,200 --> 00:05:33,200
And that might be, for instance, a design for a web page.

81
00:05:33,200 --> 00:05:38,240
Humans can design web pages, turns out that AI, evolutionary AI can do it better.

82
00:05:38,240 --> 00:05:43,600
Come up with solutions or designs that take advantage of interactions that humans have

83
00:05:43,600 --> 00:05:45,080
hard time understanding.

84
00:05:45,080 --> 00:05:47,800
Humans can only keep a couple of variables in mind.

85
00:05:47,800 --> 00:05:51,800
With evolution, we can keep hundreds, so hundreds of thousands of variables in mind and

86
00:05:51,800 --> 00:05:54,280
optimize in a much larger scale.

87
00:05:54,280 --> 00:05:55,680
And that's what evolutionary AI is.

88
00:05:55,680 --> 00:05:57,520
It's making AI creative.

89
00:05:57,520 --> 00:06:02,320
And there's a whole new opportunity here in applying it to different places.

90
00:06:02,320 --> 00:06:06,240
Not just predict, but also prescribe and create.

91
00:06:06,240 --> 00:06:10,920
And this web page design use case is one that you spent quite a bit of time working

92
00:06:10,920 --> 00:06:11,920
on at Centian.

93
00:06:11,920 --> 00:06:17,680
Is that still something that you're in the context of cognizant going out and working

94
00:06:17,680 --> 00:06:22,280
with people on or if you brought ends to other use cases?

95
00:06:22,280 --> 00:06:26,840
Yes, that was one of the first we started with and it's now its own company called

96
00:06:26,840 --> 00:06:28,080
Evolve.

97
00:06:28,080 --> 00:06:34,200
And they are marketing and they are building and developing that application, automated

98
00:06:34,200 --> 00:06:37,560
web site optimization to maximize conversions.

99
00:06:37,560 --> 00:06:38,560
We are doing other things.

100
00:06:38,560 --> 00:06:45,640
We're really expanding out the opportunity to use AI to optimize decision making.

101
00:06:45,640 --> 00:06:50,840
And that means that in companies, there's a lot of data about situations where decisions

102
00:06:50,840 --> 00:06:55,560
had to be made in order to achieve certain objectives like maximize performance while

103
00:06:55,560 --> 00:06:57,320
minimizing cost.

104
00:06:57,320 --> 00:07:03,000
It turns out that data is absolute gold mine because we can learn from it what happens.

105
00:07:03,000 --> 00:07:07,720
If you decide these actions in these situations, are you going to achieve your objectives?

106
00:07:07,720 --> 00:07:10,240
So we can learn from it and we can optimize it.

107
00:07:10,240 --> 00:07:15,040
And that's not something that is done today, but it can be done as soon as we get those

108
00:07:15,040 --> 00:07:22,680
data sources together so we can learn from history, we can start optimizing for the future.

109
00:07:22,680 --> 00:07:31,000
When you summarize the technical foundations of evolutionary AI or in essence how it works

110
00:07:31,000 --> 00:07:34,520
what it does, how do you capture that quickly for folks?

111
00:07:34,520 --> 00:07:35,520
Yes.

112
00:07:35,520 --> 00:07:39,520
So Evolve or AI is population-based search.

113
00:07:39,520 --> 00:07:41,240
So you are trying to discover something new.

114
00:07:41,240 --> 00:07:45,440
You're trying to develop a new design or maybe a decision strategy.

115
00:07:45,440 --> 00:07:48,000
You create a population of potential solutions.

116
00:07:48,000 --> 00:07:53,000
That might be 100, 200 different possible solutions.

117
00:07:53,000 --> 00:07:56,120
And then you evaluate how well each one of those work.

118
00:07:56,120 --> 00:07:58,880
And then you discover that these are the good ones, these are the bad ones, we throw away

119
00:07:58,880 --> 00:07:59,880
the bad ones.

120
00:07:59,880 --> 00:08:05,240
You keep the good ones and then form recommendations and mutations of those good ones.

121
00:08:05,240 --> 00:08:07,480
It's like it's after biological evolution.

122
00:08:07,480 --> 00:08:10,640
This is how search takes place.

123
00:08:10,640 --> 00:08:15,040
In computational evolution you can throw away a lot of solutions that don't work.

124
00:08:15,040 --> 00:08:20,280
You're looking for some where the building blocks coincide so that you get a better solution

125
00:08:20,280 --> 00:08:22,280
than your previous ones.

126
00:08:22,280 --> 00:08:25,960
And this way you can find solutions in huge search spaces.

127
00:08:25,960 --> 00:08:31,120
Two to the two to the 70th is one example we came up with designing a multiplexer for

128
00:08:31,120 --> 00:08:33,360
70 bits, multiplexer problem.

129
00:08:33,360 --> 00:08:37,880
Two to the two to the 70th is so large that if you print it out on a piece of paper it

130
00:08:37,880 --> 00:08:42,960
takes light 95 years to go from the beginning of the number to the end of that number.

131
00:08:42,960 --> 00:08:46,040
That's how many states there are where you're trying to find a solution.

132
00:08:46,040 --> 00:08:49,440
So that's the power of evolution because it's population-based search.

133
00:08:49,440 --> 00:08:53,920
You find building blocks, recombine them and gradually find the solutions.

134
00:08:53,920 --> 00:08:56,160
And it turns out there are many problems like that in the world.

135
00:08:56,160 --> 00:09:01,160
We haven't really thought of them as machine learning problems because they are so huge.

136
00:09:01,160 --> 00:09:02,160
But now we can.

137
00:09:02,160 --> 00:09:05,560
And part of that is that we have the compute and we have the technology.

138
00:09:05,560 --> 00:09:12,600
And so is the idea to identify the problems that naturally lend themselves or are naturally

139
00:09:12,600 --> 00:09:19,640
kind of inherently population-based or evolution-based or is this a technique that can be applied

140
00:09:19,640 --> 00:09:22,400
broadly wherever we're trying to do machine learning?

141
00:09:22,400 --> 00:09:25,600
Yeah, it is very broad.

142
00:09:25,600 --> 00:09:30,160
It applies to problems where we need to find solutions we don't already know.

143
00:09:30,160 --> 00:09:35,240
So it's not object detection, object recognition because we already know what those objects

144
00:09:35,240 --> 00:09:36,240
are.

145
00:09:36,240 --> 00:09:37,240
We have the power status.

146
00:09:37,240 --> 00:09:40,120
It's problems where we don't know what the right solutions are.

147
00:09:40,120 --> 00:09:43,320
What is the optimal design of a web page, for instance?

148
00:09:43,320 --> 00:09:44,320
We don't know that.

149
00:09:44,320 --> 00:09:48,840
We have to explore, try out alternatives and learn what works and what doesn't.

150
00:09:48,840 --> 00:09:54,640
Now the population-based search is unique in that it allows you to explore a lot more.

151
00:09:54,640 --> 00:09:59,360
You could do some of this with reinforcement learning and it'd be successful.

152
00:09:59,360 --> 00:10:04,000
But it's based on improving an individual solution.

153
00:10:04,000 --> 00:10:07,960
And if you have to refine it, you can with that technique.

154
00:10:07,960 --> 00:10:10,400
With population-based search, you can hedge your bets.

155
00:10:10,400 --> 00:10:13,520
You can have a population that's very broadly distributed.

156
00:10:13,520 --> 00:10:18,320
You try things that you don't know anything about and you find stepping stones.

157
00:10:18,320 --> 00:10:22,000
But then you can inspire in order to find something that's surprising.

158
00:10:22,000 --> 00:10:24,840
And that's the crux of it.

159
00:10:24,840 --> 00:10:28,280
Evolution allows you to explore and find surprises.

160
00:10:28,280 --> 00:10:31,480
Those that human designers might not think of.

161
00:10:31,480 --> 00:10:37,520
You mentioned reinforcement learning and I was going to ask about this, the relationship

162
00:10:37,520 --> 00:10:41,840
between the two as you describe the two.

163
00:10:41,840 --> 00:10:48,480
It almost sounds like in a way that evolution or these population-based approaches is like

164
00:10:48,480 --> 00:10:52,200
a breadth-first search, whereas reinforcement learning is a depth-first search.

165
00:10:52,200 --> 00:10:56,760
Yeah, that's kind of an interesting way of looking at it, I think it's accurate in a

166
00:10:56,760 --> 00:11:01,200
sense that it's breadth-first search, but you don't try to find every possible solution.

167
00:11:01,200 --> 00:11:06,160
You are casting your white net in that sense, have a lot of breadth.

168
00:11:06,160 --> 00:11:11,440
But when you find something useful, partial solutions, then you do a recombination of

169
00:11:11,440 --> 00:11:12,440
those.

170
00:11:12,440 --> 00:11:15,800
That's the recombination, that's the interesting secret.

171
00:11:15,800 --> 00:11:19,720
So you don't have to systematically do breadth-first search, you find something that works and

172
00:11:19,720 --> 00:11:23,520
you recombine and focus where you go next.

173
00:11:23,520 --> 00:11:28,280
That's the key of evolution and the theory of evolution is evolution, the computation is

174
00:11:28,280 --> 00:11:29,280
based on that.

175
00:11:29,280 --> 00:11:35,080
Building blocks, hypothesis and schema theorem, there's some theory that suggests that when

176
00:11:35,080 --> 00:11:39,520
you find good partial solutions, they will become more prevalent in the population and that's

177
00:11:39,520 --> 00:11:42,280
how you find those solutions that solve the problem.

178
00:11:42,280 --> 00:11:47,480
So kind of going back to that analogy in deep reinforcement learning, there's work that's

179
00:11:47,480 --> 00:11:52,720
happening to try to maximize what we learn as we're making, taking actions as opposed

180
00:11:52,720 --> 00:11:59,120
to just throwing them out, you know, once we've gotten to an end state, that's kind of

181
00:11:59,120 --> 00:12:02,480
analogous to what we're doing with evolution.

182
00:12:02,480 --> 00:12:06,880
We're trying to take, you know, what we're learning from kind of this breadth-first search

183
00:12:06,880 --> 00:12:10,400
if you will and combine the best of these elements together.

184
00:12:10,400 --> 00:12:12,760
I think I may be pushing this analogy too far.

185
00:12:12,760 --> 00:12:18,040
They really are different in that in reinforcement learning, you're trying to improve in a single

186
00:12:18,040 --> 00:12:19,040
solution.

187
00:12:19,040 --> 00:12:24,760
You get a little closer when you're thinking of off-policy learning where you're trying

188
00:12:24,760 --> 00:12:32,120
to combine knowledge from multiple hypothetical universes, but fundamentally, population search,

189
00:12:32,120 --> 00:12:39,160
space search, distribute your potential solutions, and it's based on statistics, obviously, it's

190
00:12:39,160 --> 00:12:44,600
a stochastic search method, but stochastic is represented by the population.

191
00:12:44,600 --> 00:12:48,360
So what's there in the population represents what you know about the domain.

192
00:12:48,360 --> 00:12:53,560
So you gradually focus your individuals where the most likely solutions are.

193
00:12:53,560 --> 00:12:56,480
But you always want to, there's another part of it.

194
00:12:56,480 --> 00:12:58,480
You always want to maintain diversity.

195
00:12:58,480 --> 00:13:04,480
You always want to explore areas that you don't know about, and that's unique to evolution.

196
00:13:04,480 --> 00:13:07,840
That allows you to do that without much cost.

197
00:13:07,840 --> 00:13:11,320
I mean, you can afford it because you have multiple solutions if some don't turn out.

198
00:13:11,320 --> 00:13:12,320
That's no big deal.

199
00:13:12,320 --> 00:13:13,600
You try something else.

200
00:13:13,600 --> 00:13:18,000
And you eventually find a component that's been missing and then you recombine it with

201
00:13:18,000 --> 00:13:21,680
other solutions to make big leaps in performance.

202
00:13:21,680 --> 00:13:23,440
It's not a gradual search method that way.

203
00:13:23,440 --> 00:13:28,280
You make these recombinations that actually allow you to make large changes and large improvements.

204
00:13:28,280 --> 00:13:33,520
One of the areas that this has been applied is in like neural architecture search.

205
00:13:33,520 --> 00:13:34,520
Yes.

206
00:13:34,520 --> 00:13:35,520
Is that an area that you work in?

207
00:13:35,520 --> 00:13:36,520
Yes, absolutely.

208
00:13:36,520 --> 00:13:39,520
That's something I continue to work on.

209
00:13:39,520 --> 00:13:43,600
And this has also been happening for a long time.

210
00:13:43,600 --> 00:13:45,480
Architectural matter in neural networks.

211
00:13:45,480 --> 00:13:51,560
And currently it is only a few people who have expertise to construct these architects.

212
00:13:51,560 --> 00:13:53,120
So they're very different.

213
00:13:53,120 --> 00:13:55,800
Now we can automate that by doing architects of search.

214
00:13:55,800 --> 00:13:59,920
And again, population based search is great in that it allows you to hedge your bets.

215
00:13:59,920 --> 00:14:01,240
Try out different architectures.

216
00:14:01,240 --> 00:14:04,840
If you find an innovation, you can combine it with other kinds of architectures.

217
00:14:04,840 --> 00:14:06,640
And therefore, come up with better ones.

218
00:14:06,640 --> 00:14:09,320
And this is something that's been going on for decades.

219
00:14:09,320 --> 00:14:15,000
We used to do it so that evolution optimizes the entire network, including its weights.

220
00:14:15,000 --> 00:14:20,040
And now more recently, we optimized the architecture, the topology of the network.

221
00:14:20,040 --> 00:14:23,680
And then use stochastic gradient to train them.

222
00:14:23,680 --> 00:14:26,800
So they're interesting synergies between different approaches.

223
00:14:26,800 --> 00:14:35,160
But again, the evolution based architecture search is the most, perhaps, bold or exploratory.

224
00:14:35,160 --> 00:14:39,360
Because in architecture search, you can get quite a good performance by just tuning hyper

225
00:14:39,360 --> 00:14:42,720
parameters, tuning some small modifications.

226
00:14:42,720 --> 00:14:45,800
But with evolution, you can search a lot larger space.

227
00:14:45,800 --> 00:14:51,920
We can look at the topologies modules, as well as hyper parameters, and find more surprising

228
00:14:51,920 --> 00:14:53,640
architectures that way.

229
00:14:53,640 --> 00:14:56,920
So this is, of course, very compute-intensive.

230
00:14:56,920 --> 00:14:57,920
But that's what you want.

231
00:14:57,920 --> 00:14:59,720
Because compute is still coming out.

232
00:14:59,720 --> 00:15:02,000
We have access to more compute than ever.

233
00:15:02,000 --> 00:15:05,800
And we can actually utilize it to do this more broad search and find more surprises.

234
00:15:05,800 --> 00:15:11,280
And so from a research perspective, where is the research frontier around evolutionary

235
00:15:11,280 --> 00:15:15,840
search relative to a year plus ago when we spoke like, how is it?

236
00:15:15,840 --> 00:15:19,840
I'm assuming it like everything else in this space is evolving quickly.

237
00:15:19,840 --> 00:15:22,640
What are the contemporary topics that folks are talking about?

238
00:15:22,640 --> 00:15:30,600
Yeah, there's one area is novelty, trying to encourage indeed search to discover things

239
00:15:30,600 --> 00:15:34,400
that are surprising, how to do that right.

240
00:15:34,400 --> 00:15:37,280
We are starting to get a handle on that.

241
00:15:37,280 --> 00:15:42,000
It's not just that you reward novelty, but you do it in a systematic way so that you guarantee

242
00:15:42,000 --> 00:15:44,080
a certain level of quality, as well.

243
00:15:44,080 --> 00:15:49,600
So there's a synergy between optimizing the performance and encouraging surprises and

244
00:15:49,600 --> 00:15:51,440
novelty to be discovered.

245
00:15:51,440 --> 00:15:57,960
What I think is still a biggest leap that we need to make in evolution is even though we

246
00:15:57,960 --> 00:16:03,280
have computational methods that have demonstrated the discovery of new things, we're still not

247
00:16:03,280 --> 00:16:06,520
quite at the level of natural biological evolution.

248
00:16:06,520 --> 00:16:10,360
For instance, we are missing major transitions.

249
00:16:10,360 --> 00:16:16,040
How do you search and change your own representations so that now you can search in a different level?

250
00:16:16,040 --> 00:16:19,680
Like transition from single cell to multicell organisms.

251
00:16:19,680 --> 00:16:24,360
Creating evolution, discover representations that allow you to build a larger structure

252
00:16:24,360 --> 00:16:25,360
systematically.

253
00:16:25,360 --> 00:16:30,440
What's an example of that particular challenge applied to a real use case?

254
00:16:30,440 --> 00:16:32,880
Well, it could be, for instance, in behavior.

255
00:16:32,880 --> 00:16:37,440
In AI, we see a lot of agents in games, for instance, so in that domain, it might be

256
00:16:37,440 --> 00:16:43,920
that you have individuals now that perform very well in Atari games, for instance, maybe

257
00:16:43,920 --> 00:16:44,920
Starcraft.

258
00:16:44,920 --> 00:16:51,240
But the next level there might be that they start building roads, they start building buildings

259
00:16:51,240 --> 00:16:59,120
or vehicles and then discover strategies that utilize those structures that they constructed.

260
00:16:59,120 --> 00:17:04,520
That would be a concrete major transition from one level where we operate at the level

261
00:17:04,520 --> 00:17:08,560
of whatever objects they are, to constructing new objects that then you use.

262
00:17:08,560 --> 00:17:12,240
Yes, that strikes me as a big leap, to get its creativity.

263
00:17:12,240 --> 00:17:22,320
In some ways, I don't want to necessarily invoke AGI, but it's a creative essence that

264
00:17:22,320 --> 00:17:26,080
has been very difficult for us to achieve with AI.

265
00:17:26,080 --> 00:17:30,680
Presumably, you're scaling that back to small leaps.

266
00:17:30,680 --> 00:17:31,680
What exactly does that mean?

267
00:17:31,680 --> 00:17:34,400
How do you formulate that problem from a research perspective?

268
00:17:34,400 --> 00:17:36,400
That's exactly right.

269
00:17:36,400 --> 00:17:38,280
That's why it's called major transitions.

270
00:17:38,280 --> 00:17:40,360
We've seen it in biology a few times.

271
00:17:40,360 --> 00:17:44,680
It's not something that happens every day in biology, ain't it?

272
00:17:44,680 --> 00:17:48,360
But we haven't really seen it in evolution, computational evolution yet.

273
00:17:48,360 --> 00:17:49,360
But we are getting there.

274
00:17:49,360 --> 00:17:51,280
It's done to understand how that might work.

275
00:17:51,280 --> 00:17:54,000
Indeed, they happen in small steps.

276
00:17:54,000 --> 00:17:58,200
Open and open and discover is another term that's used there.

277
00:17:58,200 --> 00:18:01,960
Something that doesn't really run out of steam after you solve one problem, but it creates

278
00:18:01,960 --> 00:18:04,080
another level of problem that you then start.

279
00:18:04,080 --> 00:18:06,240
Typically, those are done in games.

280
00:18:06,240 --> 00:18:10,680
So you perhaps evolve the game environment together with the solution.

281
00:18:10,680 --> 00:18:14,680
So you start with navigation, a simple environment, and then you have more objects there that

282
00:18:14,680 --> 00:18:19,760
you have to get around on maybe other opponents that become more sophisticated.

283
00:18:19,760 --> 00:18:21,680
So that's how you approach it.

284
00:18:21,680 --> 00:18:27,160
Sounds a little bit like curriculum learning in a sense that you're kind of gradient-graduating.

285
00:18:27,160 --> 00:18:32,320
You're making your environment or your problem more complex than trying to...

286
00:18:32,320 --> 00:18:33,320
Yes.

287
00:18:33,320 --> 00:18:38,480
It's much like curriculum learning with one exception, the curriculum is designed automatically.

288
00:18:38,480 --> 00:18:41,960
It's a co-evolutionary system.

289
00:18:41,960 --> 00:18:45,520
So you have solutions and problems that evolve at the same time.

290
00:18:45,520 --> 00:18:48,800
And that's how you might get open-endedness in the end, and that's the goal.

291
00:18:48,800 --> 00:18:55,120
We're not there completely yet, but there are some indications that we might get there one day.

292
00:18:55,120 --> 00:18:57,600
So that's one big area.

293
00:18:57,600 --> 00:19:03,960
There are also, I think, an interesting opportunity in looking at what's been learned in biology

294
00:19:03,960 --> 00:19:09,280
in the last 50 years or so, because most of the techniques are based on our understanding

295
00:19:09,280 --> 00:19:14,840
of how genetic algorithms work based on population biology.

296
00:19:14,840 --> 00:19:17,560
But we know a lot more now than we did in 50 years ago.

297
00:19:17,560 --> 00:19:22,400
So looking at what current biology thinks is happening in evolution and discovery might

298
00:19:22,400 --> 00:19:23,800
give us better computation.

299
00:19:23,800 --> 00:19:30,480
So one of them is that it's not just selection, mutation and recombination, but there's also

300
00:19:30,480 --> 00:19:31,480
drift.

301
00:19:31,480 --> 00:19:36,840
There's also large populations with weak selection that allows you to come up with solutions

302
00:19:36,840 --> 00:19:38,960
that don't necessarily help.

303
00:19:38,960 --> 00:19:43,480
They are weakly deleterious, but in the long run, serve a stepping stone to something

304
00:19:43,480 --> 00:19:44,480
bigger.

305
00:19:44,480 --> 00:19:48,280
And our computational methods don't actually do that today, and we're starting to look

306
00:19:48,280 --> 00:19:50,080
into how we can do that.

307
00:19:50,080 --> 00:19:54,600
And a third area might be indirect representations.

308
00:19:54,600 --> 00:20:00,400
So we are currently in evolutionary computation focusing on genetic representation that maps

309
00:20:00,400 --> 00:20:04,000
directly to the solution, one to one.

310
00:20:04,000 --> 00:20:07,440
In biology, there's a developmental process.

311
00:20:07,440 --> 00:20:09,280
There's interaction with environment.

312
00:20:09,280 --> 00:20:12,040
There's even interactions between genes.

313
00:20:12,040 --> 00:20:16,000
And then the final product, like a human being, is not determined by genes.

314
00:20:16,000 --> 00:20:19,400
It's determined by genes plus environmental interactions.

315
00:20:19,400 --> 00:20:20,960
How do we take advantage of that?

316
00:20:20,960 --> 00:20:25,720
How do we create a learning process that takes into account that there's a mechanism of

317
00:20:25,720 --> 00:20:27,840
interaction that makes you what you are?

318
00:20:27,840 --> 00:20:32,640
So that's a third area where we're going today and in the future.

319
00:20:32,640 --> 00:20:39,160
I'm going back to the analogy of, or the use case of architecture search and trying to

320
00:20:39,160 --> 00:20:44,960
think through the way that this problem of creativity expresses itself.

321
00:20:44,960 --> 00:20:54,600
And when you're trying to apply evolutionary AI to something like an architecture search,

322
00:20:54,600 --> 00:21:01,920
are you starting with, presumably you're starting with some known architectures and evolving

323
00:21:01,920 --> 00:21:07,800
based on those, and at some point you want to get to something new, right?

324
00:21:07,800 --> 00:21:14,920
Kind of refine that, you know, how do you represent the knowns in a given problem generally

325
00:21:14,920 --> 00:21:19,680
and then, you know, what does it mean to, you know, recombine things?

326
00:21:19,680 --> 00:21:21,520
It depends how ambitious you want to be.

327
00:21:21,520 --> 00:21:25,880
I mean, you could start with an architecture that already exists and mostly refine it.

328
00:21:25,880 --> 00:21:32,400
You take dense net, you take rest net, you take something like that, modify some of the topology

329
00:21:32,400 --> 00:21:33,400
of that.

330
00:21:33,400 --> 00:21:37,200
So you've got layers and connections and kind of these existing, you know, modules and

331
00:21:37,200 --> 00:21:40,560
things like that and you're reorganizing these things that you already know.

332
00:21:40,560 --> 00:21:41,560
And that already helps.

333
00:21:41,560 --> 00:21:46,280
And you're taking, you're taking the components and principles and then defining a search

334
00:21:46,280 --> 00:21:51,320
space around it and finding if there's a better solution that utilizes those components.

335
00:21:51,320 --> 00:21:56,520
And that works quite well now, but you could try to make it more ambitious.

336
00:21:56,520 --> 00:22:02,720
And that means that you are defining a larger space, maybe, so you take one principle,

337
00:22:02,720 --> 00:22:06,400
but might be that architectures are build on modules.

338
00:22:06,400 --> 00:22:09,800
And then you repeat those modules many time in some kind of organization.

339
00:22:09,800 --> 00:22:14,640
So now you start by evolving a module, which is a combination of different layer types.

340
00:22:14,640 --> 00:22:19,360
You program those up first, define a module and then you define how you combine it at

341
00:22:19,360 --> 00:22:20,360
the second level.

342
00:22:20,360 --> 00:22:22,680
So that's another good approach.

343
00:22:22,680 --> 00:22:26,400
It's a little bit broader search space now because you have a search of modules as well

344
00:22:26,400 --> 00:22:28,240
as the whole architecture.

345
00:22:28,240 --> 00:22:35,520
But you have to, if you have to have a hunch or an idea where the solutions are likely

346
00:22:35,520 --> 00:22:40,520
to be, so that the search will be, you know, finished in our lifetime.

347
00:22:40,520 --> 00:22:41,520
Yeah.

348
00:22:41,520 --> 00:22:43,200
That's kind of the thing that I was poking at.

349
00:22:43,200 --> 00:22:46,280
It's like, you know, how do you represent the thing that you have no idea what it is?

350
00:22:46,280 --> 00:22:52,280
You have to, it has to be at least directed and, you know, not totally open ended or

351
00:22:52,280 --> 00:22:53,280
else.

352
00:22:53,280 --> 00:22:54,280
Yeah.

353
00:22:54,280 --> 00:22:59,240
Well, this is a really a crucial research question because you scientifically, well, you

354
00:22:59,240 --> 00:23:00,920
want it to be as open and as possible.

355
00:23:00,920 --> 00:23:04,680
You want the system to be able to discover things you don't already know.

356
00:23:04,680 --> 00:23:05,680
Right.

357
00:23:05,680 --> 00:23:09,000
But the more knowledge you put in, the more likely you are to find some solutions.

358
00:23:09,000 --> 00:23:10,000
Right.

359
00:23:10,000 --> 00:23:15,120
So it comes back to the major transitions that we really need a system that can evolve

360
00:23:15,120 --> 00:23:20,360
its own representations so that when it discovers something useful, it can use that as a new

361
00:23:20,360 --> 00:23:23,000
representation and build upon it.

362
00:23:23,000 --> 00:23:24,640
And we're not quite there yet.

363
00:23:24,640 --> 00:23:25,640
Yeah.

364
00:23:25,640 --> 00:23:29,280
I mean, we have, live an idea how to put together modules and then structures based on

365
00:23:29,280 --> 00:23:34,480
those modules, but eventually it bottoms out on certain layer types that you program

366
00:23:34,480 --> 00:23:37,360
up, certain ways of doing the connectivity.

367
00:23:37,360 --> 00:23:41,920
And that is based on what we know of what kind of architectures people have built and

368
00:23:41,920 --> 00:23:42,920
how they work.

369
00:23:42,920 --> 00:23:47,640
So you try to abstract those principles and expand a little bit, but not too much.

370
00:23:47,640 --> 00:23:52,920
So it's not a needle in a, well, it's needle in a haystack, but it's still guided towards

371
00:23:52,920 --> 00:23:53,920
architectures.

372
00:23:53,920 --> 00:23:55,200
We believe I like it to work.

373
00:23:55,200 --> 00:23:59,800
And so is this space gotten more accessible over the past year in change or they're like

374
00:23:59,800 --> 00:24:03,840
easily accessible toolkits that someone can, you know, download and play around with

375
00:24:03,840 --> 00:24:04,840
those implementations.

376
00:24:04,840 --> 00:24:05,840
Yes.

377
00:24:05,840 --> 00:24:06,840
Yeah.

378
00:24:06,840 --> 00:24:07,840
There's a lot more now.

379
00:24:07,840 --> 00:24:09,800
They look more approaches as well.

380
00:24:09,800 --> 00:24:16,920
And most of those approaches are clever ideas on how to make do with less, make more

381
00:24:16,920 --> 00:24:17,920
with less.

382
00:24:17,920 --> 00:24:22,280
So, for instance, you evolve a smaller architecture and then there's a mechanism of expanding

383
00:24:22,280 --> 00:24:29,080
it, like just copying and making more, adding more depth and so on, which adds more power.

384
00:24:29,080 --> 00:24:35,840
So you're trying to discover a principle and then mechanically expand it to get more performance.

385
00:24:35,840 --> 00:24:41,040
But there's another direction, I think, that's really interesting in the future also.

386
00:24:41,040 --> 00:24:47,400
And it's not just to try, not just try to come up with better performing architectures,

387
00:24:47,400 --> 00:24:51,520
but optimize something else about that architecture as well, like its size.

388
00:24:51,520 --> 00:24:57,200
You're trying to come up with a small architecture that does the job to a certain specification.

389
00:24:57,200 --> 00:25:05,760
So maybe architectures that are more robust against adversarial attacks or other aspects

390
00:25:05,760 --> 00:25:11,760
like that, use less energy, people are becoming quite aware that these are really not sustainable

391
00:25:11,760 --> 00:25:12,760
architectures.

392
00:25:12,760 --> 00:25:14,680
You have hundreds of millions of parameters.

393
00:25:14,680 --> 00:25:20,000
You can't really use it on a car or a phone or a doll or something like that.

394
00:25:20,000 --> 00:25:21,920
They have to be manageable.

395
00:25:21,920 --> 00:25:26,640
So optimizing the architectures, not just for performance, but other metrics as well.

396
00:25:26,640 --> 00:25:30,760
I think it's an interesting future direction and practical one.

397
00:25:30,760 --> 00:25:36,960
Sounds analogous to multi-task learning approach, which is shown to have some great

398
00:25:36,960 --> 00:25:39,000
advantages and other applications.

399
00:25:39,000 --> 00:25:44,440
Yeah, and that carries over the idea that we shouldn't necessarily start from scratch

400
00:25:44,440 --> 00:25:45,440
every time.

401
00:25:45,440 --> 00:25:52,600
Because we already have some good models, use them and build on them and use other tasks

402
00:25:52,600 --> 00:25:56,400
to bring them together, you don't have to start from scratch, you already have representations

403
00:25:56,400 --> 00:25:59,200
that support multiple different tasks.

404
00:25:59,200 --> 00:26:01,520
They might support a new task much more easily.

405
00:26:01,520 --> 00:26:05,760
It might be possible to build something that supports a new task more easily.

406
00:26:05,760 --> 00:26:14,960
And similarly, it has to do with data who has 100 million examples of their own application.

407
00:26:14,960 --> 00:26:19,040
Those 100 million examples exist for ImageNet and maybe a couple of other benchmark tasks.

408
00:26:19,040 --> 00:26:27,200
And if you actually try to solve your problem, maybe lung disease classification or something,

409
00:26:27,200 --> 00:26:29,400
there aren't that many cases.

410
00:26:29,400 --> 00:26:35,160
So you actually have to use other datasets in order to support learning of your own task.

411
00:26:35,160 --> 00:26:40,240
And that's a multi-task domain and there also architecture search makes a big difference.

412
00:26:40,240 --> 00:26:41,240
So that's interesting.

413
00:26:41,240 --> 00:26:43,480
We asked what has happened since we last talked.

414
00:26:43,480 --> 00:26:49,240
That has happened that we used to focus kind of one single track mine on performance, but

415
00:26:49,240 --> 00:26:52,160
now we realize that there are many other things that need to be optimized in order to make

416
00:26:52,160 --> 00:26:53,160
these things practical.

417
00:26:53,160 --> 00:27:00,440
Switching topics briefly, you recently co-authored a position paper on kind of the historical evolution

418
00:27:00,440 --> 00:27:04,320
of AI and what that says about where things are going or need to go.

419
00:27:04,320 --> 00:27:05,920
Can you give us a quick summary of that?

420
00:27:05,920 --> 00:27:07,320
Yeah, I'd love to.

421
00:27:07,320 --> 00:27:09,120
How many hours do you have?

422
00:27:09,120 --> 00:27:12,760
It's why I qualified that.

423
00:27:12,760 --> 00:27:22,120
Well, there's a lot of discussion about AI and its role in society and good and bad.

424
00:27:22,120 --> 00:27:29,440
And there are some reactions that suggest that AI is going to be dangerous and it's going

425
00:27:29,440 --> 00:27:34,680
to increase inequality and various other challenges like that.

426
00:27:34,680 --> 00:27:39,720
But what we took a look at is some of the other technologies that have been in a similar

427
00:27:39,720 --> 00:27:41,280
role in the past.

428
00:27:41,280 --> 00:27:45,120
And what happened there and what is happening now and discovered that there's actually an

429
00:27:45,120 --> 00:27:49,720
analogy of what AI can do and what the pitfalls are as well.

430
00:27:49,720 --> 00:27:54,520
So those other technologies were computing in general and the world by web.

431
00:27:54,520 --> 00:28:01,560
So if you look at computing, it was initially just for a few government or industry research

432
00:28:01,560 --> 00:28:08,680
labs, then became out-can PCs and Macs and now other people can use them, graphical interfaces

433
00:28:08,680 --> 00:28:09,680
to them.

434
00:28:09,680 --> 00:28:15,560
And cell phones that you have now computing in your pocket and it is now right now computing

435
00:28:15,560 --> 00:28:18,880
has become like plumbing or like electricity.

436
00:28:18,880 --> 00:28:23,200
So you don't even know where it happens, you don't have to, it's available.

437
00:28:23,200 --> 00:28:31,040
So we call those stages standardization like PCs, usability, like user interfaces, consumerization

438
00:28:31,040 --> 00:28:32,440
like cell phones.

439
00:28:32,440 --> 00:28:37,680
And then last phase is making it part of the infrastructure.

440
00:28:37,680 --> 00:28:44,360
And we see that in world by web as well, HTML, style seats, web 2.0 and what's happening

441
00:28:44,360 --> 00:28:50,760
now is that everything is based on web interfaces or commerce and social media.

442
00:28:50,760 --> 00:28:56,640
So in the world by web, the same kind of stages have happened and in both computing and

443
00:28:56,640 --> 00:29:01,880
world by web, beyond the last stage, it's becoming the fabric of society.

444
00:29:01,880 --> 00:29:05,720
Computing happens like plumbing, world by web is everywhere.

445
00:29:05,720 --> 00:29:10,160
And you have your personal as well as business interactions in it.

446
00:29:10,160 --> 00:29:15,840
So if you look at that and look at AI, you can see that there should be similar stages

447
00:29:15,840 --> 00:29:16,920
in the future.

448
00:29:16,920 --> 00:29:18,960
We are at the very beginning right now.

449
00:29:18,960 --> 00:29:26,240
AI is done by a few experts, well in New York, there's 13,000 such experts, but at least

450
00:29:26,240 --> 00:29:27,520
people are learning from it.

451
00:29:27,520 --> 00:29:33,640
But it's still quite difficult to understand and apply and see the opportunities.

452
00:29:33,640 --> 00:29:39,680
So that's before any of these four stages, but you can think of standardization.

453
00:29:39,680 --> 00:29:41,440
So that's what does that even mean?

454
00:29:41,440 --> 00:29:45,560
I'm trying to wrap my head around what would standardization mean for such a broad topic

455
00:29:45,560 --> 00:29:46,560
like AI.

456
00:29:46,560 --> 00:29:51,000
And would that is that I can't imagine that same question could have been asked about

457
00:29:51,000 --> 00:29:53,440
computing before it was standardized.

458
00:29:53,440 --> 00:29:59,760
Well, the way I look at it, it's, it's, it's, it means that the different AI's can

459
00:29:59,760 --> 00:30:01,120
talk to each other.

460
00:30:01,120 --> 00:30:05,520
So we have standard interfaces you can have a visual recognition system that can talk

461
00:30:05,520 --> 00:30:07,640
to a natural language processing system.

462
00:30:07,640 --> 00:30:11,440
And these are developed by different people, different companies, different research labs,

463
00:30:11,440 --> 00:30:12,800
and they can talk to each other.

464
00:30:12,800 --> 00:30:17,800
So we have standardized interfaces so that the AI's can build upon each other and talk

465
00:30:17,800 --> 00:30:18,800
to each other.

466
00:30:18,800 --> 00:30:22,880
But it's still a machine starting to machines, it's program starting, that's a standardization.

467
00:30:22,880 --> 00:30:26,240
It becomes usability when people can talk to them.

468
00:30:26,240 --> 00:30:28,320
And we are not there yet.

469
00:30:28,320 --> 00:30:31,280
We don't really have an easy interface to talk to an AI.

470
00:30:31,280 --> 00:30:34,040
It's very difficult to take advantage of those.

471
00:30:34,040 --> 00:30:39,960
We have some initial attempts to that programming language is perhaps an interface that allow

472
00:30:39,960 --> 00:30:44,000
you to direct the other certain solutions.

473
00:30:44,000 --> 00:30:46,120
But it's not, by and large, not yet happening.

474
00:30:46,120 --> 00:30:48,120
That's a usability part.

475
00:30:48,120 --> 00:30:53,600
Consumeration means that anyone can take blocks away AI and put together a solution.

476
00:30:53,600 --> 00:30:57,960
They can manage their finances, their health, they can, I don't know, design their garden,

477
00:30:57,960 --> 00:31:03,360
the home, they use AI to do it, and they parameterize it, they run it, and they interact

478
00:31:03,360 --> 00:31:04,360
with it.

479
00:31:04,360 --> 00:31:09,440
So making it a consumer product, something that everybody can use for their advantage.

480
00:31:09,440 --> 00:31:11,040
That's a consumerization.

481
00:31:11,040 --> 00:31:20,360
Karl Semain, one of the popular visions of AI, that of everyone's going to have this

482
00:31:20,360 --> 00:31:24,280
potentially a cadre of intelligent agents that are out acting in the world on their

483
00:31:24,280 --> 00:31:30,120
behalf, and that requires the ability for these agents to talk to other agents and to

484
00:31:30,120 --> 00:31:32,560
talk to other systems and things like that.

485
00:31:32,560 --> 00:31:37,480
Yes, it requires that, that was a standardization, but also talk to people, the usability part.

486
00:31:37,480 --> 00:31:42,720
But then the fact that you are actually in control, you consumer picks what you want

487
00:31:42,720 --> 00:31:46,560
to do and what components to use and how to put them together.

488
00:31:46,560 --> 00:31:52,320
So it becomes such a second nature for humans that you can actually solve problems with it.

489
00:31:52,320 --> 00:31:58,080
So it becomes a point where you think of say furnishing a room, you call an AI agent to

490
00:31:58,080 --> 00:32:02,240
try out different alternatives, make suggestions, you are totally running it, you're in control,

491
00:32:02,240 --> 00:32:05,840
you calling it, it's not controlling you, that's an important part of it.

492
00:32:05,840 --> 00:32:12,440
It becomes consumer goods just like, you know, cell phones or running errands on the

493
00:32:12,440 --> 00:32:14,560
web, through the web.

494
00:32:14,560 --> 00:32:20,480
And the last step I think is really quite interesting and in the future, that it could become

495
00:32:20,480 --> 00:32:22,880
AI could become a fabric of society.

496
00:32:22,880 --> 00:32:29,680
And what that means is that we as a society decide what we want.

497
00:32:29,680 --> 00:32:35,520
We want to maximize perhaps productivity and growth, but we might also say that we want

498
00:32:35,520 --> 00:32:43,160
to minimize impact on the environment or maximize equality and access.

499
00:32:43,160 --> 00:32:49,880
And then we can use AI to design policies and execute those policies so that those goals

500
00:32:49,880 --> 00:32:50,880
are met.

501
00:32:50,880 --> 00:32:52,360
I mean, that's what AI does.

502
00:32:52,360 --> 00:32:57,840
It's really good in the end on optimization and discovery of how to achieve certain goals.

503
00:32:57,840 --> 00:33:03,600
But when it becomes institutionalized in the sense and in society, it means it gives

504
00:33:03,600 --> 00:33:09,440
humans the power to come up and to achieve the goals that we decide.

505
00:33:09,440 --> 00:33:12,280
And that's remarkable because it's not happening today.

506
00:33:12,280 --> 00:33:13,360
It's never happened.

507
00:33:13,360 --> 00:33:16,320
There's always been individual personal agendas.

508
00:33:16,320 --> 00:33:22,000
There's always been dishonesty graph, various things that get in the way and create conflict.

509
00:33:22,000 --> 00:33:31,280
But if we let AI do the optimization, all we have to do is agree on what we want.

510
00:33:31,280 --> 00:33:38,080
And that's not a small challenge, but it separates all the challenges that get in the way today

511
00:33:38,080 --> 00:33:43,160
from the really what we should agree upon is what we want.

512
00:33:43,160 --> 00:33:45,200
That's the ultimate goal.

513
00:33:45,200 --> 00:33:49,880
When getting there, I think there's a stage where it becomes irresponsible to try to make

514
00:33:49,880 --> 00:33:53,880
decisions and create policies without the use of AI.

515
00:33:53,880 --> 00:33:54,880
AI is objective.

516
00:33:54,880 --> 00:33:55,880
It's based on data.

517
00:33:55,880 --> 00:34:00,920
It allows you to optimize and currently it's done by humans and not so well.

518
00:34:00,920 --> 00:34:05,760
And it becomes irresponsible to try to make these decisions as humans when AI can do

519
00:34:05,760 --> 00:34:06,760
it better.

520
00:34:06,760 --> 00:34:10,840
But we have to still take the responsibility of setting those goals so that we can use

521
00:34:10,840 --> 00:34:13,960
those AI in a responsible and productive manner.

522
00:34:13,960 --> 00:34:15,520
So that's the last stage for AI.

523
00:34:15,520 --> 00:34:18,280
And that's why I think it's exciting.

524
00:34:18,280 --> 00:34:21,200
And it's also important that we are now at the crossroads.

525
00:34:21,200 --> 00:34:29,760
We can actually adopt that vision and make AI such that it helps us achieve what we want.

526
00:34:29,760 --> 00:34:32,000
And we have to make the right decisions in a way.

527
00:34:32,000 --> 00:34:36,440
We cannot let the mistakes of the computing enrolled by web get in a way.

528
00:34:36,440 --> 00:34:38,240
There were mistakes along the way.

529
00:34:38,240 --> 00:34:42,840
And our monopolies as well as over regulation, they can get in a way.

530
00:34:42,840 --> 00:34:47,960
And we have to recognize that we need to build these capabilities over time.

531
00:34:47,960 --> 00:34:49,360
We need to adjust to them.

532
00:34:49,360 --> 00:34:55,960
So is the main thrust of the paper and the model, is it that is it to kind of locate us

533
00:34:55,960 --> 00:34:56,960
in time?

534
00:34:56,960 --> 00:34:59,160
Hey, we are here in the next stage of standardization.

535
00:34:59,160 --> 00:35:04,800
Is it to reassure us that everything's going to be okay because there's this kind of bright

536
00:35:04,800 --> 00:35:10,320
future for AI driven policy making like what is the main thing that you're trying to convey?

537
00:35:10,320 --> 00:35:16,640
Yeah, that's, it's definitely trying to raise awareness that we have a great opportunity

538
00:35:16,640 --> 00:35:21,000
to make AI work for us and improve the world that way.

539
00:35:21,000 --> 00:35:22,240
But we have to recognize it.

540
00:35:22,240 --> 00:35:23,240
We have to recognize it.

541
00:35:23,240 --> 00:35:26,920
We have to build it most likely through these stages because we've seen two examples of

542
00:35:26,920 --> 00:35:27,920
them.

543
00:35:27,920 --> 00:35:32,720
And at the time when we were working on computing a world where we made some mistakes,

544
00:35:32,720 --> 00:35:36,640
we could learn from those and do better this time.

545
00:35:36,640 --> 00:35:41,640
And it's possible, for instance, that AI is over-regulated.

546
00:35:41,640 --> 00:35:43,040
There's no access to data.

547
00:35:43,040 --> 00:35:47,080
There's no decision making done by AI because people are afraid of it and they don't really

548
00:35:47,080 --> 00:35:53,880
see the potential benefit and it will never develop or it will, development will be delayed.

549
00:35:53,880 --> 00:35:58,160
But recognizing the potential, recognizing the pitfalls allows us to see what needs to

550
00:35:58,160 --> 00:36:03,200
be done and hopefully nurture the field so that it will, will get there and it will happen

551
00:36:03,200 --> 00:36:09,280
in smaller steps. We can't replace everything a decision-making with AI today.

552
00:36:09,280 --> 00:36:10,680
We'll have to develop the technology.

553
00:36:10,680 --> 00:36:11,880
We have to develop the datasets.

554
00:36:11,880 --> 00:36:17,680
We have to develop the, it's a humans' understanding how to use it.

555
00:36:17,680 --> 00:36:24,120
And there are some challenges like you pointed out that humans are not willing to give control

556
00:36:24,120 --> 00:36:26,120
very easily to machines.

557
00:36:26,120 --> 00:36:30,640
But there's also, I've seen, as a professor, I've seen a change in the last couple of years

558
00:36:30,640 --> 00:36:38,000
and decades that people are much more accepting of it now because they can see the benefits.

559
00:36:38,000 --> 00:36:41,640
And that's happened in, say, just one example, self-driving cars.

560
00:36:41,640 --> 00:36:47,720
2003, we were working with manufacturers developing self-driving systems and they said that they

561
00:36:47,720 --> 00:36:52,560
will never happen because people will not let machines take over their car.

562
00:36:52,560 --> 00:36:55,520
They can only warn you while you're driving.

563
00:36:55,520 --> 00:36:57,600
But somehow the attitudes changed.

564
00:36:57,600 --> 00:37:01,640
People saw what the opportunities are and now we have self-driving cars almost ready

565
00:37:01,640 --> 00:37:03,240
to hit the road.

566
00:37:03,240 --> 00:37:09,960
That kind of change of attitudes has to happen and people have to be educated and learn

567
00:37:09,960 --> 00:37:15,280
of what's possible and learn to understand what the limitations are, so to avoid them.

568
00:37:15,280 --> 00:37:19,400
But the sooner we understand, both of those dimensions, the opportunities as well as

569
00:37:19,400 --> 00:37:20,400
the challenges.

570
00:37:20,400 --> 00:37:24,120
I think better off we are and we can make this future happen sooner and another day.

571
00:37:24,120 --> 00:37:27,440
Unfortunately, we're going to have to leave that there, put a pin in it, we probably should

572
00:37:27,440 --> 00:37:32,440
have started the conversation there and we could have gotten into it for the whole time.

573
00:37:32,440 --> 00:37:37,520
But interesting perspective for sure and I am very appreciative of the update on Evolutionary

574
00:37:37,520 --> 00:37:42,640
AI, so he still thanks so much for taking the time to chat with us.

575
00:37:42,640 --> 00:37:43,640
My pleasure.

576
00:37:43,640 --> 00:37:44,640
Thank you.

577
00:37:44,640 --> 00:37:45,640
Awesome.

578
00:37:45,640 --> 00:37:46,640
Thank you.

579
00:37:46,640 --> 00:37:47,640
All right, everyone.

580
00:37:47,640 --> 00:37:50,280
That's our show for today.

581
00:37:50,280 --> 00:37:56,200
To learn more about today's guest or the topics mentioned in this interview, visit twimmelai.com.

582
00:37:56,200 --> 00:38:01,200
Of course, if you like what you hear on the podcast, please subscribe, rate and review

583
00:38:01,200 --> 00:38:03,960
the show on your favorite pod catcher.

584
00:38:03,960 --> 00:38:29,840
Thanks so much for listening and catch you next time.

