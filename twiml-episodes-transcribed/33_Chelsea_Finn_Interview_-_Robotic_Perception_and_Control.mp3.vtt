WEBVTT

00:00.000 --> 00:16.840
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.840 --> 00:21.840
people doing interesting things in machine learning and artificial intelligence.

00:21.840 --> 00:24.440
I'm your host Sam Charrington.

00:24.440 --> 00:30.180
This week we continue our series on industrial applications of machine learning and AI with

00:30.180 --> 00:35.960
a conversation with Chelsea Finn, a PhD student at UC Berkeley.

00:35.960 --> 00:41.200
Chelsea's research is focused on machine learning for robotic perception and control.

00:41.200 --> 00:45.680
Despite being early in her career, Chelsea is an accomplished researcher with more than

00:45.680 --> 00:51.400
14 published papers in the past two years on subjects like deep visual foresight, model

00:51.400 --> 00:56.560
diagnostic meta learning, visual motor learning, just to name a few.

00:56.560 --> 01:01.040
And we talk about all of these in the show, along with topics like zero shot, one shot

01:01.040 --> 01:02.360
and few shot learning.

01:02.360 --> 01:06.680
I'd also like to give a shout out to Shreyes, a listener who wrote into a request that

01:06.680 --> 01:11.000
we interview a current PhD student about their journey and experiences.

01:11.000 --> 01:14.080
Chelsea and I spend some time at the end of the interview talking about this and she

01:14.080 --> 01:19.560
has some great advice for current and prospective PhD students, but also for independent learners

01:19.560 --> 01:20.880
in the field.

01:20.880 --> 01:24.960
Also, during this part of the discussion, I wonder out loud if any listeners would be

01:24.960 --> 01:28.720
interested in forming a virtual paper reading club of some sort.

01:28.720 --> 01:32.720
I'm not sure yet exactly what this would look like, but please drop a comment in the

01:32.720 --> 01:34.640
show notes if you're interested.

01:34.640 --> 01:39.000
Okay, once again, I'm going to deploy the nerd alert for this episode.

01:39.000 --> 01:43.880
Chelsea and I really dig deep into her research and this conversation gets pretty technical

01:43.880 --> 01:48.120
at times, to the point that I had a hard time keeping up myself.

01:48.120 --> 01:53.760
Thanks again to our sponsor for this series and my industrial AI research, bonsai, bonsai

01:53.760 --> 01:59.360
offers an AI platform that empowers enterprises to build and deploy intelligent systems.

01:59.360 --> 02:04.440
If you're trying to build AI powered applications focused on optimizing and controlling the systems

02:04.440 --> 02:07.920
in your enterprise, you should take a look at what they're up to.

02:07.920 --> 02:12.720
They've got a unique approach to building AI models that let you use high level code

02:12.720 --> 02:16.520
to model the real world concepts in your application.

02:16.520 --> 02:21.560
Really generate, train, and evaluate low level models for your project, using technologies

02:21.560 --> 02:27.240
like reinforcement learning, and easily integrate those models into your applications and systems

02:27.240 --> 02:29.240
using APIs.

02:29.240 --> 02:35.800
You can check them out at bonds.ai, B-O-N-S.ai, and definitely let them know you appreciate

02:35.800 --> 02:47.640
their support of the podcast and now on to the show.

02:47.640 --> 02:51.280
Alright, hey everyone, I've got Chelsea Finn on the line with me.

02:51.280 --> 02:55.640
I'm super excited to have Chelsea here to speak with us.

02:55.640 --> 03:03.880
Chelsea is a PhD student at UC Berkeley and she is co-advised by both Peter Abiel and

03:03.880 --> 03:05.400
Sergei Levine.

03:05.400 --> 03:10.560
By the time this podcast is posted, you'll have heard my interview with Peter and we'll

03:10.560 --> 03:14.960
be digging in a little bit deeper into some of the things we spoke about with Peter with

03:14.960 --> 03:21.320
regards to reinforcement learning, but in particular, we'll focus on Chelsea's research

03:21.320 --> 03:27.680
into topics like deep sensory motor learning and few shot learning and some other things

03:27.680 --> 03:28.680
she's working on.

03:28.680 --> 03:31.320
Chelsea, thanks so much for joining us.

03:31.320 --> 03:32.320
Thanks for having me.

03:32.320 --> 03:33.800
It's great to have you.

03:33.800 --> 03:39.280
Why don't we get started by having you tell us a little bit about how you got interested

03:39.280 --> 03:42.280
in AI and how you got to where you are?

03:42.280 --> 03:48.600
Great, so I did my undergraduate at MIT as an undergrad and I pretty early on decided

03:48.600 --> 03:56.080
that I wanted to go, I wanted major in computer science and once I made that decision, there's

03:56.080 --> 04:03.080
a lot of different things that you can do with computer science, but machine learning and

04:03.080 --> 04:08.040
AI was the thing that I found myself most interested in given how much math it has, unlike

04:08.040 --> 04:13.640
some other areas of computer science, there's a lot of math involved, probability, statistics

04:13.640 --> 04:20.760
and I like that grounding in math and I also find that AI has a lot of very important

04:20.760 --> 04:26.720
applications and I think it's the potential to have a big impact on society.

04:26.720 --> 04:33.560
Absolutely, and I think the further we get with AI, the more of these potential applications

04:33.560 --> 04:42.200
we're seeing in particular some of these industrial use cases where we're using AI to control

04:42.200 --> 04:48.640
robotics and help automate things and that is a big focus of your research, is it not?

04:48.640 --> 04:54.280
Absolutely, I work a lot with real hardware and trying to get robots to learn how to do

04:54.280 --> 04:57.600
tasks and act intelligently, ultimately.

04:57.600 --> 05:02.560
So why don't we talk a little bit about some of the challenges that are involved in doing

05:02.560 --> 05:03.560
that?

05:03.560 --> 05:11.080
So unlike many problems in machine learning, in robotics you have a physical system

05:11.080 --> 05:17.040
that is in the real world and collecting data and the actions that you take affect

05:17.040 --> 05:22.160
the environment and affect the world and that affects what actions you want to take

05:22.160 --> 05:23.160
next.

05:23.160 --> 05:26.320
So you can't just download some data set and process it in a passive way.

05:26.320 --> 05:33.040
You need to actually be collecting data online and then learning from that data and then

05:33.040 --> 05:35.280
repeating essentially.

05:35.280 --> 05:43.280
And when you're collecting that data online, that poses a big challenge, particularly if

05:43.280 --> 05:49.080
you're working with systems that aren't in a lab environment, but in some production

05:49.080 --> 05:50.080
use.

05:50.080 --> 05:55.920
Can you talk a little bit about some of the specific challenges with regard to data collection

05:55.920 --> 06:02.360
and are there techniques or is there research being done that's focused on that particular

06:02.360 --> 06:07.120
slice, how to make that data collection more effective and efficient?

06:07.120 --> 06:12.520
So some of the big success stories in robotics in industrial applications has largely been

06:12.520 --> 06:16.920
in factories and in very controlled settings where you can essentially just pre-program

06:16.920 --> 06:21.280
exactly what motion the robot is going to be doing ahead of time and then just have the

06:21.280 --> 06:24.240
robot repeat that action again and again.

06:24.240 --> 06:28.760
And in these industrial applications, you really don't see robots even using any type of

06:28.760 --> 06:29.760
perception.

06:29.760 --> 06:33.080
They're simply just blindly executing motions.

06:33.080 --> 06:37.400
In machine learning research for robotics, we're trying to move beyond that and I think

06:37.400 --> 06:41.640
that what learning will bring to robotics is the ability to adapt to new environments

06:41.640 --> 06:47.280
and learn to do tasks in very unstructured environments where you don't know what the

06:47.280 --> 06:52.000
world looks like ahead of time and the environment might be dynamic.

06:52.000 --> 06:57.680
A lot of research right now in robotic learning is still in lab environments because that's

06:57.680 --> 07:00.240
where we can set up controlled experiments.

07:00.240 --> 07:04.320
It's more convenient to collect data in your lab than actually putting the robot out into

07:04.320 --> 07:05.320
the real world.

07:05.320 --> 07:09.160
But I think that in the very near future we're going to start seeing more and more research

07:09.160 --> 07:13.280
where robots are actually out in the real world and collecting data because that's where

07:13.280 --> 07:17.480
we'll be able to get the diversity of data that we need to be able to effectively generalize

07:17.480 --> 07:20.960
to new tasks, to new types of objects, etc.

07:20.960 --> 07:26.480
One of the things that I've seen that's been really interesting is been the use of clusters

07:26.480 --> 07:33.040
if you will of robots that are operating in parallel to try to accelerate both data acquisition

07:33.040 --> 07:34.040
and learning.

07:34.040 --> 07:36.840
Are you doing anything with that kind of environment?

07:36.840 --> 07:42.920
Yeah, so I did an internship at Google Brain about a year ago now where I worked on 10

07:42.920 --> 07:48.400
robot arms that were all the same and could collect data in parallel and share their experiences

07:48.400 --> 07:52.080
so that they could more efficiently collect a very large amount of data.

07:52.080 --> 07:56.560
And with that we were able to basically give a bunch of objects to each of the robots

07:56.560 --> 08:00.480
and let them play around with those objects and share their experience with one set of

08:00.480 --> 08:04.400
on one set of objects with another robot who had an experience with the different set

08:04.400 --> 08:05.840
of objects.

08:05.840 --> 08:12.600
We also now have four robot arms here at Berkeley and we might get more that we're just getting

08:12.600 --> 08:16.040
that set up right now and we're soon going to be able to have the capability to set something

08:16.040 --> 08:19.680
up at Berkeley in a similar fashion to what we did at Google.

08:19.680 --> 08:21.040
Oh, that's great.

08:21.040 --> 08:27.400
Is there a name for at use case of the robotics where you're training them in parallel and

08:27.400 --> 08:35.160
you're in the middle of the training transferring knowledge between the different robots?

08:35.160 --> 08:39.480
It sounds like, you know, some version of like active transfer learning or something like

08:39.480 --> 08:40.480
that.

08:40.480 --> 08:42.480
Is there a standard name for that yet?

08:42.480 --> 08:43.480
No, not yet.

08:43.480 --> 08:46.480
We typically just call it large scale robotic learning.

08:46.480 --> 08:47.480
Okay.

08:47.480 --> 08:48.480
Okay.

08:48.480 --> 08:55.520
Well, that's maybe you take a step back when I saw your talk at the rework deep learning

08:55.520 --> 08:56.920
summit.

08:56.920 --> 09:04.200
You started with a particular example that you used in your research and kind of built

09:04.200 --> 09:09.520
your discussion of the, you know, the various challenges and the research you were doing

09:09.520 --> 09:10.840
around this example.

09:10.840 --> 09:15.120
And if I remember correctly, it was, you know, something along the lines of taking, you

09:15.120 --> 09:19.080
know, triangular blocks and putting them in the right holes or something like that.

09:19.080 --> 09:24.360
Can you maybe, you know, walk us through that scenario and then talk us through, you

09:24.360 --> 09:27.240
know, some of what you discussed in your presentation?

09:27.240 --> 09:28.240
Yeah.

09:28.240 --> 09:34.400
So the first thing that I set out to do in my PhD was to try to see if it was possible

09:34.400 --> 09:41.120
to learn a deep neural network that maps from what the robot sees to the actions that

09:41.120 --> 09:46.520
the robot takes and see if we were able to learn, learn a policy that does this for manipulation

09:46.520 --> 09:52.080
skills and be able to do that successfully and one of the first tasks that we experimented

09:52.080 --> 09:59.440
with was inserting a block into a shapeshifting cube and the, I guess, usually when I begin

09:59.440 --> 10:04.320
with that example in my presentations, I talk about for a human, this is very intuitive

10:04.320 --> 10:09.880
to do because over the course of your life, you've learned how to guide your arm such that

10:09.880 --> 10:12.520
the, the block falls into the hole nicely.

10:12.520 --> 10:18.440
But for a computer for a robot, what the robot sees is just a bunch of numbers, a huge array

10:18.440 --> 10:24.040
of numbers in the picture and likewise, the actions that it's taking, maybe the torque

10:24.040 --> 10:28.520
supplied to the joints of the arm is also just a bunch of numbers.

10:28.520 --> 10:34.440
And to be able to map from one set of numbers to another set of numbers, to do that accurately

10:34.440 --> 10:40.000
and to do that in a way that will robustly handle a variety of environments, there's no way

10:40.000 --> 10:41.720
that a hand engineer approach will work.

10:41.720 --> 10:45.920
We need, we need to be able to learn that function and we need things like deep neural networks

10:45.920 --> 10:51.760
to provide a very flexible function in order to be able to effectively do learning in

10:51.760 --> 10:52.760
that scenario.

10:52.760 --> 10:57.560
Yeah, I think that's a, you know, a great way for folks to think about what deep learning

10:57.560 --> 11:04.680
is, you know, thinking about it as a function that, you know, maps from, you know, one set

11:04.680 --> 11:10.920
of, from a set of inputs to a set of outputs, it's interesting that, you know, that we're

11:10.920 --> 11:16.440
able to throw all this data at the problem and have the computer figure out these functions.

11:16.440 --> 11:17.840
Yeah, absolutely.

11:17.840 --> 11:23.200
And actually, one of the challenges in robotics is that typically in deep learning, you gather

11:23.200 --> 11:28.760
a very large data set and then train your neural network on that data set, whereas in robotics,

11:28.760 --> 11:33.760
it's impractical to collect a huge amount of data for a task that you might want to train

11:33.760 --> 11:38.560
because you're actually collecting that data on a real physical system.

11:38.560 --> 11:43.880
And so are there techniques that can be used to, or let's maybe talk a little bit about

11:43.880 --> 11:48.520
the techniques that can be used to address that problem.

11:48.520 --> 11:53.800
Deep learning historically requires lots and lots of data in these industrial environments.

11:53.800 --> 11:56.640
It's difficult to collect lots and lots of data.

11:56.640 --> 11:57.640
What can we do then?

11:57.640 --> 12:00.880
Yeah, so there's been a number of approaches.

12:00.880 --> 12:04.560
One is to, as we talked about before, just get a bunch of robots and have them collect

12:04.560 --> 12:05.880
data in parallel.

12:05.880 --> 12:09.480
Well, actually, one of the challenges with that approach is that if you're having a

12:09.480 --> 12:13.920
lot of robots collect data, you don't want to have a human for every single robot labeling

12:13.920 --> 12:17.720
the data or resetting the environment or providing other means of supervision because

12:17.720 --> 12:20.720
that defeats the point of having the robot there in the first place.

12:20.720 --> 12:25.200
So you can get a lot of data, but you need to algorithm that can learn from the raw data

12:25.200 --> 12:29.600
rather than from labeled data like we see in some of the most successful applications

12:29.600 --> 12:31.120
of deep learning.

12:31.120 --> 12:37.360
Another approach to this problem is to train in simulation where it is very practical

12:37.360 --> 12:44.600
to acquire a lot of data and then try to use what you learned in simulation to be able

12:44.600 --> 12:48.560
to effectively act in the real world, either with zero shot transfer where you don't get

12:48.560 --> 12:54.840
meet where you get zero data in the real world or with or a few shot transfer or just fine

12:54.840 --> 12:57.680
tuning in the real world where you just need less data in the real world than you would

12:57.680 --> 13:00.120
need otherwise if you didn't have that simulated data.

13:00.120 --> 13:03.920
Well, maybe let's talk a little bit about those three things.

13:03.920 --> 13:11.920
So with simulation, maybe walk through the process of using simulator to train a deep

13:11.920 --> 13:12.920
neural net.

13:12.920 --> 13:13.920
Yeah.

13:13.920 --> 13:20.720
So in simulation, we can use algorithms that require a lot of data, specifically reinforcement

13:20.720 --> 13:25.120
learning algorithms that reinforcement learning algorithms are typically very data inefficient

13:25.120 --> 13:31.480
because they don't have the exact input output labels that you have in supervised learning.

13:31.480 --> 13:34.760
You're not just trying to map from one thing to another where you know exactly what the

13:34.760 --> 13:36.240
output should be.

13:36.240 --> 13:40.160
Instead, you get experience in the environment, then you get feedback from the environment

13:40.160 --> 13:42.000
on how well you did.

13:42.000 --> 13:43.240
And that feedback might be delayed.

13:43.240 --> 13:44.240
It might be sparse.

13:44.240 --> 13:48.000
So you might not get it very often or it might not be very detailed.

13:48.000 --> 13:52.760
And so as a result, reinforcement learning algorithms are significantly slower than super

13:52.760 --> 13:56.440
and require significantly more data than supervised learning problems.

13:56.440 --> 13:59.840
It'd be already know that supervised learning problems require a lot of data.

13:59.840 --> 14:05.720
So typically what this kind of the, what an approach like this might look like is train

14:05.720 --> 14:10.920
a policy in simulation using your favorite reinforcement learning method and then take

14:10.920 --> 14:15.680
that policy and try to transfer it into the real world, either just by running it in

14:15.680 --> 14:20.480
the real world and hoping that it works potentially with some modularity to like a vision system

14:20.480 --> 14:25.000
on the robot, like a specific to the robot versus specific to the simulation or a controller

14:25.000 --> 14:28.840
that specific to the robot and specific to the simulation or trying to initialize with

14:28.840 --> 14:31.440
that policy and then fine tune in the real world.

14:31.440 --> 14:36.160
One of the reasons why that transfer doesn't just happen automatically is that one, similarly

14:36.160 --> 14:41.920
vision is usually lower fidelity and not as realistic as the vision that we get from

14:41.920 --> 14:44.120
cameras in the real world.

14:44.120 --> 14:50.440
And physics simulation, the physics in simulators is actually not at all accurate, especially

14:50.440 --> 14:55.960
when you encounter a contact between two different objects.

14:55.960 --> 15:00.960
Modeling contacts and exactly what goes on within that contact is quite complex, so that's

15:00.960 --> 15:05.920
not something that we can accurately model in simulators.

15:05.920 --> 15:07.480
So that's really interesting.

15:07.480 --> 15:16.960
So the specific challenge there is, for example, in simulation, you're modeling a robot manipulator

15:16.960 --> 15:24.960
like a hand that's picking up a block, I can imagine that the physics governing how that

15:24.960 --> 15:30.600
simulator is grasping that block and the coefficients of static friction and dynamic

15:30.600 --> 15:34.440
friction and all the things that determine the way the block will ultimately sit in the

15:34.440 --> 15:37.680
gripper can be quite difficult to model.

15:37.680 --> 15:41.440
Is that basically what you're describing in terms of when you're modeling two bodies?

15:41.440 --> 15:42.760
Yeah, exactly.

15:42.760 --> 15:48.360
And so with the difficulty in modeling that, how do you account for that?

15:48.360 --> 15:54.800
Do you just do a rough approximation and assume that the difference is noise in the system

15:54.800 --> 16:00.480
that your model just needs to account for or are there specific techniques for dealing

16:00.480 --> 16:01.480
with that?

16:01.480 --> 16:02.480
Yeah.

16:02.480 --> 16:07.360
Simulators have various ways to approximate them, then yeah, generally the learning algorithm

16:07.360 --> 16:10.920
doesn't look any different in simulation versus in the real world.

16:10.920 --> 16:15.520
Okay, one conversation that I had that I think I mentioned in the conversation with Peter

16:15.520 --> 16:20.920
as well was a conversation with Stefano Irman over at Stanford who was talking about like

16:20.920 --> 16:28.400
incorporating physics into models and Peter and I talked about that, I think fairly generally,

16:28.400 --> 16:32.600
is that something that comes into play specifically in your work?

16:32.600 --> 16:40.120
And in this issue of the grippers, for example, or the contact between the robot and other

16:40.120 --> 16:41.120
objects.

16:41.120 --> 16:44.720
So you're asking, like, do we try to learn models of the world?

16:44.720 --> 16:52.680
I guess I'm asking, how do you try to incorporate pre-existing knowledge about the way grippers

16:52.680 --> 16:58.480
grip and objects respond to being gripped into your deep learning models?

16:58.480 --> 17:04.960
So some of our algorithms, we do incorporate knowledge about the physical world.

17:04.960 --> 17:10.280
For example, we have a certain type of model usually in one of the algorithms that we use

17:10.280 --> 17:12.280
and that we can run on real robots.

17:12.280 --> 17:18.560
We use a Gaussian mixture model as a prior on a learned time-varing linear model, it

17:18.560 --> 17:21.720
is getting a bit complicated but a bit technical.

17:21.720 --> 17:25.960
But generally a Gaussian mixture model is or at least a prior from a Gaussian mixture

17:25.960 --> 17:34.160
model is a fairly reasonable way to model physics and that oftentimes there are different

17:34.160 --> 17:40.360
modes of physics, whether you're in different types of contact, whether you're in free space

17:40.360 --> 17:44.800
and that sort of model is well suited for that.

17:44.800 --> 17:48.880
Actually, I'd love to have you walk us through the details on that.

17:48.880 --> 17:51.600
Can we start with what is a Gaussian mixture model?

17:51.600 --> 17:58.240
Yeah, so this is one small part of a much larger algorithm for learning policies on the

17:58.240 --> 18:00.040
real robot.

18:00.040 --> 18:07.880
So a Gaussian mixture model is a distribution where there are multiple mixture components

18:07.880 --> 18:12.400
and each mixture component is a Gaussian distribution.

18:12.400 --> 18:18.280
So a normal distribution and then each component also has a weight.

18:18.280 --> 18:25.520
And then the Gaussian mixture model is simply the weighted sum of all of the Gaussians.

18:25.520 --> 18:34.280
And is the Gaussian mixture model are using a mixture to model one specific thing or using

18:34.280 --> 18:40.920
that mixture to model a several phenomena at once?

18:40.920 --> 18:48.120
We're using the mixture to model a mode of dynamics where by dynamics I mean a conditional

18:48.120 --> 18:53.800
distribution of the next state given the current state in action.

18:53.800 --> 18:58.320
So you're basically trying to predict what the next state is going to be given where

18:58.320 --> 19:04.280
you are right now and the action that you take and that conditional distribution will

19:04.280 --> 19:07.760
depend on whether or not you're in contact, whether or not your finger is sliding across

19:07.760 --> 19:12.880
the table versus in static contact or versus whether or not you're in free space.

19:12.880 --> 19:17.760
And so we're using each mixture component of the Gaussian mixture model to model those

19:17.760 --> 19:21.280
different modes of your dynamics.

19:21.280 --> 19:26.120
And so you mentioned a few things in terms of your finger sliding in those things.

19:26.120 --> 19:32.960
Do each of those map one to one to a component of the mixture or is all of that modeled

19:32.960 --> 19:35.240
by the whole of the mixture?

19:35.240 --> 19:40.520
Each of those will typically map to one component, although usually you don't know the number

19:40.520 --> 19:46.600
of components a priori, so you set it to a number that you think is slightly larger

19:46.600 --> 19:52.880
than the actual number of components, just like in k-means or in clustering algorithms.

19:52.880 --> 19:53.880
Okay.

19:53.880 --> 19:54.880
All right.

19:54.880 --> 20:00.920
So you've got this mixture that models some of the physics that rolls up into the broader

20:00.920 --> 20:03.160
model that you're trying to build.

20:03.160 --> 20:07.360
And you said you used that as a prior for fitting a model.

20:07.360 --> 20:08.360
Okay.

20:08.360 --> 20:14.640
And so when you say using that as a priori, you're basically using the output of that Gaussian

20:14.640 --> 20:19.920
mixture model as an input to your deep neural net, is that the right way to think about

20:19.920 --> 20:20.920
it?

20:20.920 --> 20:24.880
So in this case, actually, the dynamics model that we're learning is not a neural network.

20:24.880 --> 20:31.960
We're learning a local time varying linear model where by time varying linear, I mean

20:31.960 --> 20:37.120
that you basically sample a bunch of trajectories on your robot.

20:37.120 --> 20:43.480
And then at every time step in those set of trajectories, you fit a linear model using

20:43.480 --> 20:45.400
linear regression.

20:45.400 --> 20:50.720
And then the Gaussian mixture model is fit to all of the time steps of all of the samples

20:50.720 --> 20:54.280
and that's used as a prior for linear regression, done at every time step.

20:54.280 --> 20:55.280
Okay.

20:55.280 --> 20:56.280
Got it.

20:56.280 --> 20:57.280
Got it.

20:57.280 --> 21:01.680
And then ultimately we use this, well, there's more steps involved, but you use this

21:01.680 --> 21:09.600
this dynamics model that you fit to acquire an optimal policy for a certain version of

21:09.600 --> 21:10.600
your problem.

21:10.600 --> 21:11.600
Okay.

21:11.600 --> 21:18.400
And then when you have an optimal policy, you can at a high level, the top down explanation

21:18.400 --> 21:25.800
is that given a certain manipulation problem, you can decompose your problem into different

21:25.800 --> 21:31.080
instances of the problem, like for a single start position and a single end position.

21:31.080 --> 21:37.840
And then we are solving for the optimal policy for each individual condition using optimal

21:37.840 --> 21:42.120
control and using this linear model that we fit.

21:42.120 --> 21:49.080
And then once we solve each of the individual problems, then we use that for supervised

21:49.080 --> 21:53.360
learning of a deep neural network that can solve all of the instances of the problem.

21:53.360 --> 21:54.360
Okay.

21:54.360 --> 21:57.520
So let me try to paraphrase that to make sure I'm following.

21:57.520 --> 22:02.200
So you've got, it sounds like we are talking here.

22:02.200 --> 22:08.720
You mentioned point A and point B, are we talking about, you know, strictly the problem

22:08.720 --> 22:15.680
of you've got a robot arm, let's say, with, you know, N degrees of freedom, and you're

22:15.680 --> 22:20.040
trying to figure out a path, you know, to translate, you know, to get the gripper from

22:20.040 --> 22:23.560
point A to point B using those motors.

22:23.560 --> 22:28.040
Is that the scope of the problem that we're talking about or have I read into that to

22:28.040 --> 22:29.040
narrowly?

22:29.040 --> 22:32.720
That is a little bit too narrow, so it's not just moving the gripper of the robot, it

22:32.720 --> 22:34.920
could also involve moving objects.

22:34.920 --> 22:41.880
It could be, so this algorithm has also been applied to manipulating objects within a five

22:41.880 --> 22:42.880
fingered hand.

22:42.880 --> 22:48.280
A slightly different version of the algorithm has also been used for a locomotion, robot locomotion.

22:48.280 --> 22:49.280
Okay.

22:49.280 --> 22:53.880
So then maybe taking a step back, it sounds like you've got, you've got something that's,

22:53.880 --> 22:59.720
you're trying to figure out how to get it from point A to point B, and you've got some,

22:59.720 --> 23:03.560
you know, underlying dynamics that you need to model.

23:03.560 --> 23:10.200
And so you use, you train a linear model to tell you basically how to move your motors

23:10.200 --> 23:14.880
to get the thing from point A to point B or state A to state B.

23:14.880 --> 23:23.480
And then once you have those linear models, you're able to use them to generate more data

23:23.480 --> 23:26.440
that you can train deep networks with.

23:26.440 --> 23:30.800
It's basically you're, you're training a data generator.

23:30.800 --> 23:31.800
Essentially.

23:31.800 --> 23:32.800
Yeah.

23:32.800 --> 23:39.160
So the data generator is, is that in any given kind of robot manipulation problem, you

23:39.160 --> 23:42.880
can see your current observation, but you don't know what action you should take.

23:42.880 --> 23:47.160
And what you want to figure out is what action to take, and that's, and so the data generator

23:47.160 --> 23:51.520
is figuring out what action you should take for any given observation.

23:51.520 --> 23:55.560
And then once you have the actions that you should take, you can then just apply play

23:55.560 --> 23:58.280
and supervise learning with your neural network.

23:58.280 --> 23:59.280
Mm-hmm.

23:59.280 --> 24:00.280
Mm-hmm.

24:00.280 --> 24:06.240
Maybe let's take a step back to kind of this, the, you know, the broader problem, which

24:06.240 --> 24:14.000
is the putting the blocks in the right places, a big part of your research is the relationship

24:14.000 --> 24:19.600
between the robot is seeing that problem from a computer vision perspective.

24:19.600 --> 24:24.080
And you're essentially, the intelligence that the robot is acting on is kind of largely

24:24.080 --> 24:30.000
driven by, you know, the manipulation of, or an observation of, you know, the pixels

24:30.000 --> 24:33.160
coming from a camera or a set of cameras.

24:33.160 --> 24:38.680
Can you talk a little bit about the, you know, broadly speaking, the relationship between,

24:38.680 --> 24:43.400
you know, computer vision and the work that you're doing in robotics.

24:43.400 --> 24:47.080
And then, you know, maybe we can drill into some of the specifics.

24:47.080 --> 24:54.520
Yeah, so a lot of the algorithms that we develop for robotics, we aren't necessarily specific

24:54.520 --> 24:57.880
to certain sensory modalities, like tactile or vision.

24:57.880 --> 25:02.680
We want them to work with a fairly wide variety of, of modalities.

25:02.680 --> 25:08.440
Vision is perhaps one of the most interesting because it's, gives you a lot of information

25:08.440 --> 25:13.400
about the environment and it also is one of the most challenging because it is very

25:13.400 --> 25:20.400
high dimensional and not high dimensional in a way that's readily interpretable by a lot

25:20.400 --> 25:21.720
of these algorithms.

25:21.720 --> 25:25.360
One of the other reasons why we use vision a lot is because tactile sensing, while it's

25:25.360 --> 25:30.920
very, can give you a lot of information, good tactile sensors are hard to acquire.

25:30.920 --> 25:35.200
And they're typically either very expensive or very fragile and break a lot.

25:35.200 --> 25:42.200
So a lot of the algorithms that we develop, we try to incorporate, I mean, we use convolutional

25:42.200 --> 25:46.040
neural networks if we're going to use vision, convolutional neural networks are very efficient

25:46.040 --> 25:53.640
and very effective at doing their job, at these localizing objects or inferring things

25:53.640 --> 25:55.920
about the environment, et cetera.

25:55.920 --> 26:02.160
So a lot of my work will be developing algorithms based off of reinforcement learning, imitation

26:02.160 --> 26:07.520
learning, or inverse reinforcement learning, but focusing on algorithms which can scale

26:07.520 --> 26:09.680
to high dimensional inputs like vision.

26:09.680 --> 26:13.040
Okay, you mentioned inverse reinforcement learning.

26:13.040 --> 26:14.040
What's that?

26:14.040 --> 26:15.040
Yeah.

26:15.040 --> 26:21.200
So reinforcement learning first is the problem of given a reward function and the ability

26:21.200 --> 26:26.360
to sample experience from your environment, figure out what the optimal policy is for

26:26.360 --> 26:32.000
that reward function, figure out what actions you should take given a current observation.

26:32.000 --> 26:35.120
Inverse reinforcement learning is essentially the inverse of that.

26:35.120 --> 26:41.040
So in inverse reinforcement learning, you assume that you have rollouts or basically trajectories

26:41.040 --> 26:45.640
from the optimal policy, from an expert, like a human.

26:45.640 --> 26:48.640
And your goal is to figure out what the reward function is.

26:48.640 --> 26:53.240
So your goal is to figure out what the human was trying to accomplish.

26:53.240 --> 26:57.480
And then ultimately, once you figured out what the human was trying to accomplish, then

26:57.480 --> 27:01.840
you also want to learn a policy for yourself that also accomplishes what the human was trying

27:01.840 --> 27:03.760
to accomplish.

27:03.760 --> 27:09.800
So it was an application area that when you're doing imitation learning and you have a human

27:09.800 --> 27:16.880
explicitly move a robot from one position to another, you can then use inverse reinforcement

27:16.880 --> 27:21.800
learning to learn a policy that would produce that same motion.

27:21.800 --> 27:23.280
Yeah, exactly.

27:23.280 --> 27:28.480
So the typical application is given a set of demonstrations from a human, try to figure

27:28.480 --> 27:32.760
out what the human was doing and then figure out how to do it yourself.

27:32.760 --> 27:38.000
And then you said in your previous description, you said not just what the human was doing,

27:38.000 --> 27:41.920
but you kind of characterize it as why the human was doing that, or at least that's what

27:41.920 --> 27:46.640
I read into the way you said it, like the human's intent, is that do you make that distinction

27:46.640 --> 27:55.160
between what the human was doing and the model learning some degree of intent or higher level

27:55.160 --> 27:58.240
purpose, or am I reading too much into it?

27:58.240 --> 27:59.800
No, that's absolutely right.

27:59.800 --> 28:06.160
So the reason why this is interesting is that if a human is doing something, you don't

28:06.160 --> 28:10.400
want to necessarily mimic the exact actions that they do because you might have a different

28:10.400 --> 28:14.880
arm that looks slightly differently, or maybe the way that they're doing it isn't quite

28:14.880 --> 28:20.280
optimal, or you want to be able to generalize what they are doing to new scenarios.

28:20.280 --> 28:22.480
And in those new scenarios, you don't want to do exactly what they did.

28:22.480 --> 28:24.880
You want to achieve what they were trying to achieve.

28:24.880 --> 28:28.720
And so in reverse reinforcement learning, you're adding structure to the problem of imitation

28:28.720 --> 28:32.200
learning, where you're trying to, one, you're assuming that they were acting according

28:32.200 --> 28:36.920
to some reward function, and you're trying to infer what that was and what they were

28:36.920 --> 28:38.800
trying to achieve.

28:38.800 --> 28:39.800
Mm-hmm.

28:39.800 --> 28:47.160
And so bring this, bring all this back to the problem of placing the blocks for us.

28:47.160 --> 28:48.160
Okay.

28:48.160 --> 28:54.720
So in the block scenario, you'd see an example of a human putting the block in the shape

28:54.720 --> 29:00.000
sorting cube, and then you would try to infer the fact that their goal was to get it inside

29:00.000 --> 29:03.440
the cube, not just to take the actions that they were taking.

29:03.440 --> 29:07.560
And then once you infer that, figure out a policy for doing that.

29:07.560 --> 29:12.120
The block example, perhaps, isn't the best example for inverse reinforcement learning,

29:12.120 --> 29:16.400
because providing a reward function for that task is fairly straightforward.

29:16.400 --> 29:21.240
Your goal is to physically get this red block into the shape sorting cube.

29:21.240 --> 29:25.320
But in many other scenarios, it's actually hard to write down what their reward function

29:25.320 --> 29:26.480
should be.

29:26.480 --> 29:29.760
And that's actually one of the big challenges in applying reinforcement learning to real

29:29.760 --> 29:31.240
world scenarios.

29:31.240 --> 29:37.440
So for example, say you want your robot to pour a cup of water from one cup to another

29:37.440 --> 29:38.440
cup.

29:38.440 --> 29:42.800
In that task, you want all of the water to end up in the target cup.

29:42.800 --> 29:44.280
You don't want any water to spill.

29:44.280 --> 29:49.360
You probably want the robot to be somewhat gentle, and encoding those things in a reward

29:49.360 --> 29:54.760
function requires a lot of engineering, like you might need actually like a liquid detector

29:54.760 --> 29:59.200
to be able to detect where the liquid is and to detect if something got wet.

29:59.200 --> 30:03.000
And then also characterizing whether or not the robot was gentle.

30:03.000 --> 30:07.520
And so engineering that reward function may even be more work than engineering the behavior

30:07.520 --> 30:08.520
itself.

30:08.520 --> 30:13.160
And as much easier just to show the robot, this is the task that I want you to do.

30:13.160 --> 30:21.280
But you still need to do you still need to give the robot examples of a failure or of

30:21.280 --> 30:22.280
failures?

30:22.280 --> 30:28.160
Like, to what degree does that come into play as well with supervised learning?

30:28.160 --> 30:34.320
So you've got examples of, hey, I'm successfully getting the cup of water from point A to point

30:34.320 --> 30:39.000
B. And you can label those as successes, but what about labeling?

30:39.000 --> 30:44.480
You know, this is a failure explicitly, you know, even though 80% of the water may have

30:44.480 --> 30:47.840
gotten from point A to point B, does that make any sense?

30:47.840 --> 30:48.840
Yeah.

30:48.840 --> 30:54.320
So typically in inverse reinforcement learning, you only give successful demonstrations,

30:54.320 --> 31:00.600
although thinking about how you could incorporate failed examples would also be, is an interesting

31:00.600 --> 31:03.600
research direction, not a lot of research has been done there.

31:03.600 --> 31:08.760
And actually, perhaps an interesting analogy for people is that for people that are familiar

31:08.760 --> 31:14.480
with generative adversarial networks, you can actually show that the inverse reinforcement

31:14.480 --> 31:19.160
learning objective, that is one of the ones that's most widely used, is mathematically

31:19.160 --> 31:23.560
equivalent to the objective of a discriminator in a generative adversarial network.

31:23.560 --> 31:27.560
So in generative adversarial networks, you're given a data set of images and the goal is

31:27.560 --> 31:31.880
to be able to generate images from that data set or that look like images from that data

31:31.880 --> 31:32.880
set.

31:32.880 --> 31:37.240
And the goal of the discriminator in a generative adversarial network is to figure out

31:37.240 --> 31:42.240
if an image was generated in its fake or if the image came from that data set.

31:42.240 --> 31:46.840
And the reward function that you're trying to learn in inverse reinforcement learning plays

31:46.840 --> 31:48.320
the same role as the discriminator.

31:48.320 --> 31:52.560
So it's trying to, it's generally trying to say that the demonstrations, the example

31:52.560 --> 31:56.840
demonstrations that you got, which is the same as your data set, have high reward.

31:56.840 --> 32:01.240
And things that your policy is trying to generate is try, it has low reward.

32:01.240 --> 32:06.640
So just like in generative adversarial networks, you only get examples of positive data points.

32:06.640 --> 32:07.640
Okay.

32:07.640 --> 32:09.320
Of successful demonstrations.

32:09.320 --> 32:10.320
Okay.

32:10.320 --> 32:17.640
It strikes me that there's, you know, some signal and partial successes to some degree

32:17.640 --> 32:22.600
and it makes me wonder what research is being done out there, you know, in that direction

32:22.600 --> 32:23.600
if anything.

32:23.600 --> 32:24.600
Yeah.

32:24.600 --> 32:29.440
So actually one interesting thing is that humans, even if someone isn't successfully

32:29.440 --> 32:34.160
doing a task, you can typically infer humans can typically infer what they were trying

32:34.160 --> 32:35.160
to accomplish.

32:35.160 --> 32:36.160
Right.

32:36.160 --> 32:41.000
And that's actually one of the things that I'm working on right now is that is an algorithm

32:41.000 --> 32:46.040
which tries to learn from unsuccessful demonstrations that we're still going in the right direction

32:46.040 --> 32:49.920
and still have enough signal to indicate what goal they were trying to achieve.

32:49.920 --> 32:50.920
Okay.

32:50.920 --> 32:51.920
And so what's the approach there?

32:51.920 --> 32:57.000
So the approach that we're trying right now is an approach based on few shot learning

32:57.000 --> 32:58.400
or metal learning.

32:58.400 --> 33:02.120
And I guess another term for metal learning is learning how to learn.

33:02.120 --> 33:03.120
Okay.

33:03.120 --> 33:10.360
And just if I can interject, I think you mentioned also one shot learning previously.

33:10.360 --> 33:14.840
So we've got this, you know, spectrum of learning, if you will.

33:14.840 --> 33:20.600
One shot is, you know, learning on, you know, one example, few shots is learning on a few

33:20.600 --> 33:21.600
examples.

33:21.600 --> 33:25.280
There's also no shot learning, which is learning on no examples.

33:25.280 --> 33:32.840
And then metal learning, it's orthogonal to the end shot issue or is it not?

33:32.840 --> 33:38.000
So a few shot learning and one shot learning are typically achieved using metal learning

33:38.000 --> 33:39.000
algorithms.

33:39.000 --> 33:40.000
Okay.

33:40.000 --> 33:44.640
So metal learning is somewhat of a broader class of algorithms.

33:44.640 --> 33:45.640
Right.

33:45.640 --> 33:46.640
Right.

33:46.640 --> 33:47.640
Okay.

33:47.640 --> 33:48.640
So apologies.

33:48.640 --> 33:49.640
You were saying.

33:49.640 --> 33:54.720
I think you were saying that you were talking about how you're using metal learning or

33:54.720 --> 34:01.520
learning to learn to, you know, solve this problem of learning from failed examples.

34:01.520 --> 34:03.000
Yeah.

34:03.000 --> 34:09.080
So at a high level, what we're working on is being able to show collecting a data set

34:09.080 --> 34:16.760
of examples of demonstrations for different tasks and corrupting some of those demonstrations

34:16.760 --> 34:25.760
with noise and then trying to have a system that learns that the corrupted demonstrations

34:25.760 --> 34:30.400
that learns basically how to do the correct thing from the corrupted demonstrations.

34:30.400 --> 34:36.240
And so in this example, what is the, what is the demonstration?

34:36.240 --> 34:41.720
Are we still talking about the shape sorting cube or is this something else?

34:41.720 --> 34:43.720
And then what, what is the noise?

34:43.720 --> 34:47.720
Are we talking about noise added to sensory input?

34:47.720 --> 34:52.960
Are we talking about noise in some represent or perturbations in some representation of,

34:52.960 --> 34:54.360
you know, the underlying process?

34:54.360 --> 35:00.080
Are we talking about, you know, noise injected into some layer of the deep neural network

35:00.080 --> 35:03.840
or by noise, they just mean noise injected onto the actions.

35:03.840 --> 35:08.400
So the output of the neural network of the demonstrations, so essentially the labels

35:08.400 --> 35:12.640
of the demonstrations, okay, and that will make the demonstrations sub optimal.

35:12.640 --> 35:13.640
Okay.

35:13.640 --> 35:20.440
So basically, you've got some label data that you're training a model on and you just

35:20.440 --> 35:22.680
mess up some of the labels sometimes.

35:22.680 --> 35:26.520
Essentially, although the, we're not training a model in the typical way.

35:26.520 --> 35:32.080
So in this work, we're building on some of my recent work on few shot learning or one

35:32.080 --> 35:33.080
shot learning.

35:33.080 --> 35:38.400
Few shot learning is just kind of the general case where if you could be one to ten or maybe

35:38.400 --> 35:44.360
a little bit more, where we try to learn a representation that's very quickly adaptable

35:44.360 --> 35:46.800
to many different tasks.

35:46.800 --> 35:56.000
And maybe let's dig into the few shot learning issue and talk a little bit about just,

35:56.000 --> 36:00.400
you know, your approach, what you have done in your recent research, but also what others

36:00.400 --> 36:05.000
have done and, you know, a little bit of a background into the problem domain if we

36:05.000 --> 36:06.000
could.

36:06.000 --> 36:07.000
Yeah, absolutely.

36:07.000 --> 36:15.040
So in few shot learning, the goal is to be able to do a new task from only a very small

36:15.040 --> 36:17.480
number of data points from that task.

36:17.480 --> 36:23.800
So an example of this is in the, the visual recognition realm is say that you're given

36:23.800 --> 36:30.240
a picture of a segue and then your goal is given that single picture of a segue, be

36:30.240 --> 36:35.000
able to classify other examples of segues successfully.

36:35.000 --> 36:40.800
And the way that you learn how to do this is get a data set of lots of different types

36:40.800 --> 36:46.400
of objects with a few types of each different object and you learn about the, the variation

36:46.400 --> 36:47.400
across objects.

36:47.400 --> 36:52.120
So you actually learn how to identify objects just from one example.

36:52.120 --> 36:56.920
And so is the data set that you're referring to?

36:56.920 --> 37:01.880
Is that a label data set or that is that an unlabeled data set that you're just learning

37:01.880 --> 37:08.880
a bunch of things from right now methods, well, it's a label data set, okay.

37:08.880 --> 37:13.120
And is it a label data set?

37:13.120 --> 37:20.880
What are the, the nature of the labels, meaning is it labeled with, you know, objects or

37:20.880 --> 37:26.040
meaning this is an orange, this is a cat, or is it, you know, some kind of labels of the

37:26.040 --> 37:28.960
physical attributes of the things that are depicted?

37:28.960 --> 37:30.440
It depends on what you want to do.

37:30.440 --> 37:37.320
If your goal is to do what's called one shot image classification, then you take a standard

37:37.320 --> 37:42.000
image data set that has an image and it's corresponding label and a full data set of

37:42.000 --> 37:43.000
that.

37:43.000 --> 37:48.440
And just like image net, Mness, there's a data set called Omniglott that's very popular

37:48.440 --> 37:54.120
for one shot learning and yeah, that's the nature of the data set that you use for

37:54.120 --> 37:55.560
metal learning.

37:55.560 --> 38:01.560
And so you, you have, let's say image net, you have image net, huge database of labeled

38:01.560 --> 38:09.360
images and, you know, we'll assume that there's no segways in image net and then you're basically

38:09.360 --> 38:14.240
trying to show it a picture of a segway and then you're showing it a labeled picture

38:14.240 --> 38:18.920
of a segway, a single labeled picture of a segway and you want it to be able to identify

38:18.920 --> 38:21.080
subsequent segways.

38:21.080 --> 38:22.080
Yes.

38:22.080 --> 38:23.080
Correct.

38:23.080 --> 38:32.120
And I guess the question that what I'm wondering is why do the labels in image net

38:32.120 --> 38:33.120
matter?

38:33.120 --> 38:37.320
Like what doesn't matter if, you know, we have labels for oranges and these other things.

38:37.320 --> 38:43.240
I'm imagining that what's happening is you're throwing all that data, oh, you're training

38:43.240 --> 38:45.800
a deep neural network against all of that data.

38:45.800 --> 38:52.200
And then, you know, within, you know, the various layers of the neural net, it's kind of

38:52.200 --> 38:58.960
figuring out, you know, textures and colors and geometries and curves and edges and things

38:58.960 --> 38:59.960
like that.

38:59.960 --> 39:03.400
And it's using that to identify, you know, a segway.

39:03.400 --> 39:07.760
What is, what is the fact that the data set is labeled matter in that case?

39:07.760 --> 39:08.760
Yes.

39:08.760 --> 39:09.760
That's a very good question.

39:09.760 --> 39:16.000
It actually doesn't matter that it labeled an orange as an orange or a banana as a banana.

39:16.000 --> 39:21.160
What matters is that it is labeled in the sense that it knows like this set of images is

39:21.160 --> 39:22.160
one type of object.

39:22.160 --> 39:24.080
This set of images is another type of object.

39:24.080 --> 39:25.080
Yeah.

39:25.080 --> 39:26.080
That makes sense.

39:26.080 --> 39:27.080
Okay.

39:27.080 --> 39:31.680
So it's, you're basically, the label is is a way to communicate, you know, clustering or

39:31.680 --> 39:38.840
similarity of like images and it can use that to properly form the internal structure

39:38.840 --> 39:41.320
of your neural net to reflect all these things.

39:41.320 --> 39:42.320
Yes.

39:42.320 --> 39:43.320
Okay.

39:43.320 --> 39:49.920
Have you gotten yet the specifics of how folks are attacking fuchsot learning based

39:49.920 --> 39:53.000
on, you know, having, you know, with image and things like that?

39:53.000 --> 39:59.280
Like are there specific network architectures or specific training techniques or things like

39:59.280 --> 40:05.360
that that lend themselves to building these fuchsot models or meta learning models?

40:05.360 --> 40:12.360
Yeah. So there's actually a few fairly broad classes of techniques for solving this problem.

40:12.360 --> 40:19.480
One of the perhaps easier class of methods to explain is a class of methods that tries

40:19.480 --> 40:27.480
to learn an embedding of images such that when you run nearest neighbor or do like comparisons

40:27.480 --> 40:35.120
in that embedding space, you can very accurately generalize from just one or a few examples.

40:35.120 --> 40:44.600
And just for, for background, an embedding is basically taking a set of images and you

40:44.600 --> 40:49.880
mathematically kind of turning them into vectors that somehow relate one to the other.

40:49.880 --> 40:54.560
So say that again with that as background, you, you, you, you learn an embedding space

40:54.560 --> 41:00.440
such that when you make comparisons in that embedding space, you can generalize well from

41:00.440 --> 41:02.840
just a single example.

41:02.840 --> 41:09.280
So you're kind of learning, you're learning and embedding that kind of maximizes the,

41:09.280 --> 41:12.760
the distance between your examples basically.

41:12.760 --> 41:20.840
Yeah. And then a meta test time or a test time, you're given five examples of, or one

41:20.840 --> 41:26.840
example of five different objects. So five images total. And then your goal is, well,

41:26.840 --> 41:31.200
put those images into your embedding function to get the embedding of each of them.

41:31.200 --> 41:35.800
And then you can just do, when you then get a new image image, then you compare it to

41:35.800 --> 41:41.160
all of those, each of those five embeddings. And the closest one is the, then you assign

41:41.160 --> 41:44.920
the class of that example to the new image.

41:44.920 --> 41:51.160
Mm-hmm. Okay. Yeah, I need to do a show just on embeddings and word to vac and all these

41:51.160 --> 41:56.600
things that I've been meaning to learn more about and haven't really had a chance to dig

41:56.600 --> 41:57.880
into yet.

41:57.880 --> 42:02.800
So you said there are a number of techniques and that's one of them. Are there others that

42:02.800 --> 42:07.960
come to mind? Yeah, I'll talk a bit about the approach that we developed for my most

42:07.960 --> 42:14.840
recent paper. So the method that we developed was largely inspired by fine-tuning. So in

42:14.840 --> 42:19.800
computer vision, if you want to get good results on whatever tasks that you're doing, typically

42:19.800 --> 42:25.680
you'll take a network that was trained on ImageNet, start from that network, start from

42:25.680 --> 42:30.000
the weights of that network, and then fine-tune it on the tasks that you care about.

42:30.000 --> 42:35.400
Mm-hmm. Okay, transfer learning, right? Yeah, exactly. But if you try to do this directly

42:35.400 --> 42:41.800
for one shot learning, where you only fine-tune it on one example is going to overfit a lot.

42:41.800 --> 42:47.040
So it works well for transfer learning with a sufficiently large training set for your,

42:47.040 --> 42:51.040
for your, the tasks that you care about, where sufficiently large isn't as big as ImageNet,

42:51.040 --> 42:59.360
but it's a reasonable size. And so the approach that we take is to actually optimize for a set

42:59.360 --> 43:05.520
of features like ImageNet, such that when you fine-tune on a small number of examples,

43:05.520 --> 43:10.880
you get good performance, good generalization on that task. Okay, and how does that work?

43:10.880 --> 43:15.000
So you can write down this objective. It has a gradient in it that comes from the fine-tuning

43:15.000 --> 43:19.960
procedure. What the objective looks like is essentially you have your, you have your

43:19.960 --> 43:25.360
original weight vector, the features that you're trying to learn. And the updated feature

43:25.360 --> 43:30.640
vector, which is just the fine-tune version of that, and you're trying to minimize the

43:30.640 --> 43:37.120
loss of that updated feature vector with respect to your original parameter vector.

43:37.120 --> 43:42.840
Mm-hmm. I say on home, but I think the, the limits of not being able to have a white

43:42.840 --> 43:47.440
point in front of me is just, you know, we've reached that point. But what I kind of heard

43:47.440 --> 43:55.160
in there was, yeah, if you think to linear regression, right, you are using these gradients

43:55.160 --> 44:02.560
to try to get you to some kind of optimum and you are iterating over or descending, you

44:02.560 --> 44:08.760
know, these gradients, those gradient descent. And what I heard was, you are kind of tweaking

44:08.760 --> 44:16.440
the way you are descending the gradient so as to do something. Is there a way to finish

44:16.440 --> 44:21.560
that sentence? Yeah. So you're, you're tweaking your loss function such that you're optimizing

44:21.560 --> 44:28.000
for their performance after a gradient descent update on that task. And you do this for

44:28.000 --> 44:32.960
a wide variety of tasks. So I guess the, the just is to train for a parameter vector that

44:32.960 --> 44:39.320
can be very quickly adopted for a wide variety of tasks. And we can do this with, just

44:39.320 --> 44:46.240
with gradient based methods, just essentially just with SGD. Okay. So you're, you're basically

44:46.240 --> 44:51.880
changing your loss function so that I mean, it's kind of like a technique, like dropout

44:51.880 --> 44:56.200
and some of these other things where you're doing, you know, funky things to your loss

44:56.200 --> 45:05.200
function to be more impervious to overfitting. Kind of, we're actually optimizing. That, that

45:05.200 --> 45:12.200
kind of was like at the 300 million thousand foot, you know, level. Yeah. You can kind

45:12.200 --> 45:16.960
of see as optimizing for good generalization. Right. Right. And one of the nice things

45:16.960 --> 45:20.680
about this approach is that, well, it sounds kind of complicated, but we actually write

45:20.680 --> 45:27.280
it down. It's incredibly simple. I implemented it in less than a day. And you could apply it

45:27.280 --> 45:32.960
to a few shot classification, like I talked about before and get really good results. But

45:32.960 --> 45:38.040
you could also apply it to a wide range of other few shot learning problems, including

45:38.040 --> 45:44.280
few shot learning of behavior. Okay. So I think I'm just going to take this as a challenge

45:44.280 --> 45:50.640
to myself and anyone else who's listening that wants to dig into this to actually get

45:50.640 --> 45:57.120
your paper and go through it. And, you know, perhaps we'll reconvene after I've done

45:57.120 --> 46:02.000
that and see if I am able to have a coherent conversation about what we're discussing

46:02.000 --> 46:07.160
here. And so in order to facilitate that, what is the name of the paper that you're describing?

46:07.160 --> 46:13.400
Yeah. It's called model agnostic meta learning. Okay. Yeah. Those are the first four words

46:13.400 --> 46:19.720
of the title. Okay. Well, we'll find that paper and we'll make sure that the link is

46:19.720 --> 46:25.640
in the show notes. And anyone else who wants to, you know, dig into this with me, you

46:25.640 --> 46:29.760
know, can just drop a comment in the show notes and we'll kind of exchange notes as we

46:29.760 --> 46:34.720
learn us together. Yeah. I'm also thinking about writing a blog post on it at some point

46:34.720 --> 46:39.640
in the near future. Oh, really? Awesome. Well, if you do that, I would, you know, be happy

46:39.640 --> 46:45.560
to, you know, help in any way review it or ask you more dumb questions or whatever. Okay.

46:45.560 --> 46:51.120
Great. And so since we're talking about some of your, your research and your papers,

46:51.120 --> 46:58.120
any other pointers to papers that, you know, folks can dig into based on the things that

46:58.120 --> 47:02.760
we've talked about, you know, what are the top three, you know, papers that you'd want

47:02.760 --> 47:07.880
folks to take a look at to get a sense for your work? So the first one would be the one

47:07.880 --> 47:13.760
that I just mentioned. The second would be a paper on inverse reinforcement learning called

47:13.760 --> 47:21.400
guided cost learning. Okay. The third, let's see, it depends on how much, how much reading

47:21.400 --> 47:28.360
they want to do. I think that all refer them to the paper that starts with a deep visual

47:28.360 --> 47:34.120
foresight. Okay. That sounds compelling. So that one is, we didn't talk about that much,

47:34.120 --> 47:39.600
but essentially that one's trying to learn a predictive model of video, being able to

47:39.600 --> 47:45.600
predict the future video given the actions that the robot's going to take. Okay. Okay.

47:45.600 --> 47:53.480
Awesome. Awesome. Well, we'll have links to all of those in the show notes. Before we go,

47:53.480 --> 47:59.920
I got a request from one of our listeners a while ago, Shreys, who was about to embark

47:59.920 --> 48:08.680
on his own PhD pursuits. And he asked if we could get a PhD student on and talk about

48:08.680 --> 48:16.960
a little bit about their experiences as a PhD student and what are some things to keep

48:16.960 --> 48:25.040
in mind to be successful in pursuing research in this field. And I was wondering if you would

48:25.040 --> 48:31.080
maybe share some of your thoughts. You are obviously doing amazing research. You've

48:31.080 --> 48:38.400
got two great advisors. What are your secrets to success? Yeah. I think that it's important

48:38.400 --> 48:46.600
to continuously develop and learn throughout your PhD. So reading a lot of papers, especially

48:46.600 --> 48:52.320
these days with archive, having a tremendous number of relevant machine learning papers

48:52.320 --> 48:58.520
every day and working on research skills, learning from others around you. So at the

48:58.520 --> 49:04.000
beginning of my PhD, I started working with a postdoc very closely and I learned a lot

49:04.000 --> 49:10.840
from him on my first project. And the ability, like at the beginning, learn from people

49:10.840 --> 49:16.960
who are more seniored from you is very helpful because doing, like, there's no one perfect

49:16.960 --> 49:22.200
way to do research. I mean, if people, you're trying to solve problems that people haven't

49:22.200 --> 49:29.640
solved before. And that's hard. And the way to approach that is, is different for everyone,

49:29.640 --> 49:32.720
but I think that there's a lot to be learned from people who have been at it for a few

49:32.720 --> 49:39.160
years or more. So I think that learning from others and being open to that is very important.

49:39.160 --> 49:43.040
I've also developed my writing skills a lot in graduate school, depending on where you

49:43.040 --> 49:47.680
want to go. That can be very important, especially if you want to go into academia or if you

49:47.680 --> 49:54.000
want to continue publishing. But let's see. And then I think that work ethic is important,

49:54.000 --> 49:59.800
trying to keep up with the field these days and trying to actually get things to work takes

49:59.800 --> 50:05.040
a lot of work by nature. Research is, is trying to tackle on solve problems. And if those

50:05.040 --> 50:11.000
problems were easy to solve, then they wouldn't be on solve problems. Right. Right. So in other

50:11.000 --> 50:18.640
words, no shortcuts. Yeah. How do you keep up with archive and your paper reading list?

50:18.640 --> 50:25.040
I don't think I necessarily have a good solution, but typically what I do is check archive

50:25.040 --> 50:31.320
every day or every other day and see if there's any relevant papers. I don't, I will read

50:31.320 --> 50:37.440
papers to a varying degree based on how relevant it seems and how good the paper seems. And

50:37.440 --> 50:43.600
I also don't worry too much about missing papers because if it is a really good paper,

50:43.600 --> 50:49.680
then it will rise up through conferences, through publications. I'll see them at different

50:49.680 --> 50:58.800
publication venues or they'll become popular or common knowledge. Right. And so is it a

50:58.800 --> 51:03.920
worthwhile question to ask like how many papers do you read a, you know, day, week month

51:03.920 --> 51:11.640
or what have you or yeah, I read a paper end to end very infrequently. I guess the papers

51:11.640 --> 51:16.840
that I have to review for conferences, I will read end to end. And I will only read

51:16.840 --> 51:26.840
papers, other papers end to end if they are very relevant or if I volunteer to present

51:26.840 --> 51:32.680
it at a group meeting, we have, we have reading group meetings fairly regularly. So that's

51:32.680 --> 51:37.600
another good way to keep up with, keep up with a lot of new papers is find a group and

51:37.600 --> 51:41.640
have someone volunteer to present a paper or kind of present the key findings from a

51:41.640 --> 51:47.040
paper in that group. So then not everyone has to read everyone so we can just read it

51:47.040 --> 51:52.360
and then summarize the points that are relevant to that group. And is there for papers that

51:52.360 --> 51:59.240
are directly relevant to your research, is there a level beyond reading end to end where,

51:59.240 --> 52:05.040
you know, you're actually kind of digging into the math and trying to figure out, you know,

52:05.040 --> 52:09.160
what was some maybe missing steps that were glossed over in the paper or you're implementing

52:09.160 --> 52:14.480
them or things like that? Or do you do that, you know, fairly infrequently as well?

52:14.480 --> 52:20.520
Yeah, so if a paper is along the lines of what I'm working on, then typically I'll want

52:20.520 --> 52:25.880
to compare to that paper or compare to some version of that paper. And so that will

52:25.880 --> 52:31.160
involve grabbing an open source implementation or emailing the author or re-implementing

52:31.160 --> 52:36.240
it myself. And yeah, also thinking about the shortcomings of the paper is important.

52:36.240 --> 52:44.480
So one example is that I'm currently working on a certain application of metal learning

52:44.480 --> 52:49.400
and there was a new paper that came out on a very similar topic and one of the shortcomings

52:49.400 --> 52:55.000
was that the data set required for metal learning was huge. And so one of the benefits

52:55.000 --> 52:58.720
of metal learning is that at test time you can learn from a very small amount of data.

52:58.720 --> 53:05.520
But if you need a ton of data to learn that few shot learner, then it's not going to

53:05.520 --> 53:12.200
be feasible for applying to real robotic systems. And so that's, so that shortcoming is something

53:12.200 --> 53:17.520
that I'm one aware of when I try to, when I'm working on this problem and to something

53:17.520 --> 53:22.240
that I want to address concretely in the work that I do.

53:22.240 --> 53:31.080
Got it. Got it. I'm wondering out loud now, but I am intrigued by the thought of doing

53:31.080 --> 53:37.400
like a virtual paper reading group, like something along the lines of a podcast or an extension

53:37.400 --> 53:41.600
of the podcast. And I wonder if there are any readers or listeners rather that would,

53:41.600 --> 53:46.040
I guess, also be readers would be interested in something like that. So if you are, you

53:46.040 --> 53:50.600
know, shout out in the comments or Twitter or something like that and people are interested

53:50.600 --> 53:55.560
maybe we can find a way to do something. But I guess with that, Chelsea, you have been

53:55.560 --> 54:02.040
very gracious with your time. And I really appreciate you jumping on the, the skypline

54:02.040 --> 54:08.400
here. It's been a really interesting conversation. And I have definitely learned a ton. And, you

54:08.400 --> 54:14.080
know, my brain exploded a little bit, which is also a good sign like, and yeah, just,

54:14.080 --> 54:18.200
you know, thank you. Thanks so much. Yeah. Happy. Awesome. All right. Bye-bye.

54:18.200 --> 54:26.200
All right, everyone. That's our show for today. Thanks so much for listening and for your

54:26.200 --> 54:32.040
continued support, comments, and feedback. We really, really appreciate hearing from you

54:32.040 --> 54:38.040
and we love to incorporate your ideas into the show. I'd also like to thank our sponsor,

54:38.040 --> 54:45.520
Banzai once again. Be sure to check out what they're up to at Banz.ai. And one last reminder,

54:45.520 --> 54:51.160
next week, I'm at the O'Reilly AI Conference in New York City. You can still register using

54:51.160 --> 55:00.040
our discount code, PC Twimble, PC-TW-I-M-L, for 20% off. And if you live in New York or

55:00.040 --> 55:06.760
will be at the event, let's plan to meet up. I'm partnering with the NYAI Meetup to host

55:06.760 --> 55:12.960
a happy hour on Thursday evening after the event. If you'd like more details, please sign

55:12.960 --> 55:20.040
up using the form at twimbleai.com slash NY Meetup and we'll keep you posted. The notes

55:20.040 --> 55:27.080
for this episode can be found at twimbleai.com slash talk slash 29. For more information on

55:27.080 --> 55:34.560
industrial AI, my report on the topic or the industrial AI podcast series, visit twimbleai.com

55:34.560 --> 55:40.400
slash industrial AI. As always, remember to post your favorite quote or takeaway from

55:40.400 --> 55:45.160
this episode and we'll send you a laptop sticker. You can post them as comments to the

55:45.160 --> 55:51.840
show notes page, via Twitter, at twimbleai or via our Facebook page. Thanks again for

55:51.840 --> 56:19.080
listening and catch you next time.

