Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A quick thanks to everyone who participated in last week's Twimble Online Meetup.
It was another great one.
If you missed it, the recording will be posted to the Meetup page at twimbleai.com slash
meetup.
Definitely check it out.
I never cease to be amazed by the generosity and creativity of the Twimble community.
And I'd like to send a special shout out to listener Sharon Glander for her exceptional
sketch notes.
Sharon has been creating beautiful hand sketch notes of her favorite Twimble episodes
and sharing them with the community.
Sharon, we truly love and appreciate what you're doing with those.
So please keep up the great work.
We'll link to her sketch notes in the show notes for this episode.
And you should definitely follow her on Twitter at sharingglander for more.
This is your last chance to register for the rework, deep learning and AI assistant
summits in San Francisco, which are this Thursday and Friday, January 25th and 26th.
These events feature leading researchers and technologists like the ones you heard in
our Deep Learning Summit series last week.
The San Francisco event is headlined by Ian Goodfellow of Google Brain, Daphne Kohler
of Calico Labs and more.
Definitely check it out and use the code Twimbleai for 20% off of registration.
Now, a bit about today's show.
In this episode, I speak with Thomas Sanholm, Carnegie Mellon University professor and
founder and CEO of startups optimized markets and strategic machine.
Thomas, along with his PhD student, Noam Brown, won a 2017 NIPPS Best Paper Award for their
paper, Safe and Nested Subgames Solving for Imperfect Information Games.
Thomas and I dig into the significance of the paper, including a breakdown of perfect
versus imperfect information games, the role of abstractions in game solving, and how
the concept of safety applies to gameplay.
We discuss how all these elements and techniques are applied to poker, and how the algorithm
described in this paper was used by Noam and Thomas to create LeBrotus, the first AI
to beat top human pros and no limit Texas Holdum, a particularly difficult game to beat
due to its large state space.
This was a fascinating interview that I'm really excited to share with you.
Alright everyone, after a busy week at NIPPS, I am back home, and on the line with
Tuomas Sanholm, Tuomas is a professor in the computer science department at Carnegie
Mellon University, as well as founder and CEO of two start-ups, Strategic Machine, and
Optimize Markets, as well as an author of the Best Paper Award winning Safe and Nested
Subgames Solving for Imperfect Information Games at this year's NIPPS.
Tuomas, welcome to the podcast.
Thank you for having me.
Absolutely.
It's a little bit of a tradition here to have our guests get started by telling us a
little bit about their backgrounds and how they got involved in machine learning, and
you sound like quite a busy guy, who with your two start-ups and posts at CMU, so feel
free to take some time and let us know how all that fits together.
Yeah, happy to.
So I've been working on AI as in around 1989, and in my lab at CMU there are maybe 25
different research trends, maybe six of them active at any one time.
We're probably best known for combinatorial auctions and kidney exchange, so we run the
nation what kidney exchange for units with our algorithms and game solving.
So solving these large imperfect information games where we actually reach superhuman level
at strategic reasoning, this January by beating the top players in Heads Up No Limit
Texas Holder.
And my two start-ups, well, the first one of my current ones, I'm also a serial entrepreneur
with successful start-ups in the past, but the first one, Optimized Markets, is really
like a combinatorial sales support system for advertising campaigns, cross media.
So in TV, both linear TV and non-linear, streaming, both TV and radio and display advertising
and so forth, and where we're optimizing the allocation of inventory to campaigns and
pricing and scheduling and so forth.
And the newer start-up, which is really brand new, is called strategic machine, where we
have taken this strategic reasoning technology that we have developed in my lab here at Carnegie
Milan over the last 14 years, and we are commercializing it across a wide range of applications, ranging
from, of course, recreational games like poker to business applications and automated negotiation,
strategic pricing, product portfolio construction, auctions and so on, all the way to military
applications like cyber security, physical security, physical military, and even in steering
biological evolution and adaptation for treatment planning.
Well, that's a pretty broad set of applications.
Yeah, when you're doing the early stages of our start-up with a platform technology like
this, really the first challenge and the first order to do after the technology is ready,
is to really prioritize our markets and pick and choose where it makes the most benefit
and where you can penetrate the fastest and doing that prioritization.
And we're actually in that process right now.
So the paper that we want to talk about is safe and nested sub-game, solving for information
in perfect games and it sounds like the big distinction there between what you're working
on and some of the other things we've seen in the kind of machine learning apply to game
arena is in that imperfect information part.
Can you tell us a little bit about that and why that's important?
Yeah, that's exactly right.
So when you talk about games like checkers, chess or go where AI has seen a lot of success,
they are what's called perfect information games.
So when it is a player's turn to move, the player knows the exact state of the world.
In contrast, in most real world applications where you have more than one party acting,
they're what's called imperfect information games where when it's your turn to move,
you don't really know the state of the world exactly.
So there's hidden information and that can come from us not observing what chance has
done so far, as in let's say pomity peas, but there's an additional element that there
are other players, at least one other player and you may not even observe what the other
player has done in the game so far.
And also the other player may have observed different things about what chance has done
so far than you have.
So now you have private information, you know things that your opponent or opponents don't
know and vice versa, they know things that you don't know.
So now it becomes much harder to solve, because you have to think about issues not present
in perfect information games.
Like how do my actions signal to my opponent about my private information?
And conversely how do my opponent's actions, how should they signal to me about the opponent's
private information?
And so the approaches that you need to take to solve these games are these two different
classes of games are very different than?
They are exactly very, very different and one difference at an intuitive level is that
in a complete perfect information game, which is also called a complete information game,
you can actually solve a sub game over the game tree with information from that sub tree
only.
That is not true in imperfect information games.
In perfect information games you have to balance your strategies across the different
sub games and that ties the whole game together so it doesn't decompose.
The idea being that you've got a given player only has access to private information, but
there's some other broader set of information that the other player has that you can't
use in solving the sub tree.
Yes, that's right.
And conversely there's information you have that the other player or players don't have.
And so you've applied this with pretty great success to poker.
Can you talk a little bit about the poker as a field of application for these types of
approaches and what some of the past approaches have been to solving it?
Yeah, happy to.
So to me, poker is not really an application.
To me, poker is the benchmark and in particular heads up no limit Texas Holden has become the
leading benchmark in the AI community for testing these application independent algorithms
for solving imperfect information games.
Okay.
And poker actually goes way back in the history of your game theory.
So if you think about early pioneers like for Neumann, Morgan Stern, Nash and so on,
they actually were working on poker as perhaps the lead application of game theory.
And they were working on small, very small variants of poker that could be solved by hand
like what's called corn poker.
But now we're able to solve these much larger games where you have a full deck and you have
the full complexity of let's say no limit Texas Holden.
If I can interrupt briefly, one of the distinctions you make in the paper is that with is between
limit Texas Holden and no limit Texas Holden and I am not a poker aficionado, but what
you specifically call out that the former that limit Texas Holden has 10 to the 13 decision
points, whereas no limit creates a much bigger game of 10 to 161 decision points.
What does all that mean?
And what's different about the games that makes them so different in terms of the number
of possibilities?
Yeah.
That's right.
Those numbers are exactly right.
So we can talk about the size of the game based on the number of different situations
that the player can face.
And in limit, it's 10 to the 13 in no limit, it's 10 to the 161.
So fundamentally different and what makes it different is that in limit Texas Holden,
whenever it is your turn to bet or raise, there's only one size that you can make.
So the branching factor in your actions is like two or three.
In contrast, in no limit, you can bet any number of your chips up to all of your chips.
So the branching factor is much larger.
And so based on the different kind of sizes and scopes of these games with the smaller
games, one way to solve it is to just solve the whole game.
Is that right?
Yes, that's exactly right.
So like in 2005, my student Andrew Gilpin and I, we solved exactly in previous AI challenge
problem called Rhode Island Holden.
And that has 10 to the power of nine different situations.
And we solved it as a whole using first technique called lossless abstraction.
And then the remaining game, which was about 10 to the 7 or 10 to the 8, that we could
solve holistically.
And similarly, Michael Bowling's research group from Alberta, near optimally, solved
the two player limit Texas Holden by first doing the lossless abstraction and then having
a custom algorithm for holistically solving the whole game near optimally.
And what does it mean to do abstraction as a solution for a game like this?
Yeah, abstraction means that you are generating a smaller but strategically similar game.
And then you are solving that smaller but similar game using a Nash equilibrium finding
algorithm.
And the first work on abstraction really in games was manual.
So people did the abstraction phase manually and equilibrium finding phase computationally.
And they started around really over 15 years ago.
And nowadays for the last I would say 12 years or 13 years, both phases had been done computationally.
So there are abstraction algorithms that will find just from the rules of the game, find
a strategically similar but small enough game to solve or almost exactly solve using
equilibrium finding algorithms.
Is there a simple example of abstraction applied to poker that you can give?
Yes, let me give you two kinds of examples.
So one kind of example is information abstraction or abstracting the actions of chance.
So I might say that in poker, you are dealt ace ace or you dealt king king.
Those are different situations but they're really very similar.
So I could say that, okay, I'm going to treat them as if they were the same.
And the other form also known as action abstraction is abstracting out the players actions.
So for example, I might say that if you bet 200 chips, it's almost the same as if you
bet 201 chips.
So I'm going to treat them as if they were the same.
Okay.
And so does that tend to manifest itself like some kind of binning or quantization type
of approach?
You could call it that but the abstraction techniques are really much more sophisticated
than just clustering.
You have to think about their downstream effects and all of that.
So for example, if I have the ace of spades and ace of hearts and king of spades and king
of hearts, those might seem very similar in the beginning.
But depending on whether they end up in a flush later, they might look very different.
I mean, depending on whether they end up in a flush or a straight and so on, well,
the other hand that was original is similar might actually be very different later.
And so the abstraction is, it sounds like a key piece in solving these types of games,
but not something that scales for the very large games.
No, I would say almost opposite is that for the very large games, you need to do some
form of abstraction because you cannot holistically solve these very large games.
So this information abstraction, we talked about an action abstraction and then there's
also a third approach, which is what has traditionally been called phase-based abstraction,
where you solve some head of the game and have some sort of approximation for the rest
of it instead of solving the rest of it exactly.
And then you keep doing it over and over.
I got it.
And so the approach that you describe in the paper that uses abstraction as a way, as
kind of a framework for getting to the solution.
It can, it can, although just to be clear, abstraction algorithms were not the contribution
of this particular paper.
Abstraction in perfect information game solving and poker goes way back at least to 2001,
if not earlier.
So the new contribution in this paper was a little different.
So why don't you tell us a little bit about what this paper contributes to this broader
problem and with the example application and poker?
Say yeah.
So all of these algorithms are game independent, so they're not specific to poker, but we have
a lot of experiments in poker in the paper.
So what the new techniques are in this paper, they are what we call safe and nested sub-game
solving techniques.
So let me try to unpack that a little bit here.
So safe means that we can guarantee that as we refine our solution, so again, they start
by computing a core solution or what we call a blueprint solution for the whole game and
then you start refining your strategy to make it better in sub-games that you reach during
play.
So offline you compute the blueprint and then online when you're playing, you refine
the strategy in those parts of the search space that you actually reach.
And you can afford to do that because there's less game tree lift than in the whole game.
And you can also afford to do it in a finer and finer model or finer and finer abstraction,
the closer to the end of the game you are.
But the problem is, as we talked about earlier in the podcast, in that these imperfect information
games, the sub-games don't separate out, they don't decompose.
So how I should play in this current sub-game depends on how I should play in other sub-games,
so I can't really reason about them independently.
And here what we're doing, we're taking the blueprint strategy that gives values for different
alternatives that the opponent could have taken to get here.
And we use that type of reasoning to enable sub-game solving that has guarantees.
So it guarantees that the solution quality is no worse than that of the blueprint strategy.
So even our worst case Nemesis couldn't take us for more than it could take the blueprint.
And similarly, if the values from the blueprint are off by a little bit compared to what ideal
play would give for different states, we can guarantee that the answers that come out
of the end-game solver or sub-game solver also are off by only a little bit.
So that's a safe part, then there are two other innovations in the paper.
One is that we can do much better than prior safe end-game solving techniques by taking
into account the fact that actually there's several techniques there to do so, but one
of them I'd like to highlight here, which is what we call taking into account the gifts
that the opponent has given us so far.
So if the opponent has made a mistake in the game so far, we can afford to give back
to the opponent as much payoff as the expected mistake cost our opponent so far.
And you might say, why do we want to give anything back?
Now that allows us to have a bigger space of safe strategies to optimize over and we can
do better against other hands or other private information in general that the opponent might
have.
So that gives us more opportunity to worry about hands that the opponent is really actually
likely to have and do even better against those hands when we can, for some unlikely hands
that the opponent is going to have, which would have been mistakes to play into this situation,
we can afford to give some money back.
If I'm understanding this piece, it's a bit counterintuitive.
Can you provide some intuition or example of how this plays out in a game?
Yeah, absolutely.
So if you're a poker player, you know that that 2-7 offsuit is the worst starting hand
in Texas holder.
And that should be folded right away.
But let's say we get later into the game and there are 2-7s and a 2 on the board.
Now if you're there with a 2-7, that's a great hand because you have a full house.
But if I think about it, I don't know your cards, but I can pretty much figure out that
you don't have the full house because if you had 2-7 in your hand, you would have already
folded.
So let's say at the mistake of getting to this current situation with 2-7 would have
been $100 for you, then I can afford to give you $100 back just in case in that scenario
where you have that full house.
In other words, I have to not be as defensive against that 2-7 hand and because I don't
have to be as defensive there, I can use the flexibility of balancing my strategies to
play better against you, against other hands you are more likely to have, like for example
a pair of aces or a pair of kings.
Right.
Got it.
And then the final piece in this paper, final conceptual piece is this nested end game
solving or nested sub-game solving where we are not just taking the traditional approach
of solving the end game, a new and then playing it out.
But we are actually resolving it over and over every time the opponent has made a move.
So as we get closer and closer to the end of the game, we can actually add the opponent's
moves into our model if the opponent has played actions that were not in our model in the
first place.
So that way we don't get confused as to what the opponent has exactly done and how much
money is in the pot and so forth.
And we can still do this in a way that is probably safe.
And a lot of the paper is concerned with this formal guarantees.
How does this method perform?
Oh, it performs very well.
So we actually reach superhuman level with this technique.
And just to be clear, we reach superhuman level by playing our AI called Liberatus against
four of the top 10 human players at Hedge-Up No Limit Texas Holden in January in this huge
120,000-hand 20-day event.
And we beat them with high statistical significance and by a big margin.
And this end game solving technique was one of the three new algorithms that we had in
the three different modules of Liberatus.
Okay.
And this algorithm, does the paper give someone what they would need to implement the algorithm?
Yes.
Well, actually, let me go back on your previous question also in addition to humans, we have
shown that Liberatus beats the best prior AI, which was called Baby Targaryen 8, which
won the annual computer poker competition in 2016.
So Liberatus actually beats Baby Targaryen 8 by a huge margin of 63 millibig blinds per
hand.
So just to clarify, we're not just able to beat the best humans, we're also able to beat
the best prior AI.
Got it.
And what was that unit 63?
Yeah, good.
Good question.
63 millibig blinds per game.
And that's a big blind is a measure of the size of the ante in poker.
And millibig blind is 1,000th of that.
And if you're a poker player and you don't like to use millibig blinds per game as we
do in AI, it's called 6.3 big blinds per 100.
That's just another way of saying the same thing.
That's a big margin of victory.
So typically when the AIs play each other in the annual computer poker competition, the
top two AIs are separated maybe by 10 or 20 millibig blinds per hand.
But Liberatus beats Baby Targaryen 8, which was the best prior AI by 63 millibig blinds
per hand.
And so you were about to describe how someone might go about implementing this for poker
or another game.
Yeah, so we do lay out the algorithms in the paper and we prove their safety.
Of course these are fairly complex algorithms to be honest, especially if you run them on
a super computer.
So it's not that easy to implement, but we try to make it do our best to explain how
they're done.
And are they, is that the typical way that you'd run them on a super computer?
Depends on the game.
So most games you probably could run on a laptop, but when we get into these very large
games, you can still run them on a laptop, but then you're not in doing as well as you
would on a super computer.
And for the competition, were you running them on a super computer?
Yes, that's right.
We were running on bridges, which is the newly super computer at the Pittsburgh Super
Computing Center.
Okay.
And, you know, this is an evolving game with humans.
So each of the moves was then input into the, you know, the super computer and it would
run what's the typical response time between, you know, after entering the humans' latest
move to getting back what the machines next move should be.
Yeah, just to be clear, so the humans played through a browser-based UI.
So there was no human actually entering the moves into the super computer, like there
was with deep lose, a playing chess.
So it goes quite quickly.
The entry of data back and forth goes very quickly because it's all automated.
And the thinking time, well, in the first two betting rounds, the AI doesn't think at
all.
It has pre-computed its strategy.
And then when it, or as you say, typically, it doesn't think at all.
And then it typically starts thinking on the first move of the third betting round.
And that's where it thinks the longest.
Overall, if you think about a game of heads up, no limit, Texas hold them.
These top humans were playing on average at 20 seconds per game.
So not per move, 20 seconds per game.
And there's a big difference between the humans.
Some of them may have been going at like 17 seconds, others may have taken 25 seconds
by on average, but pretty much 20 or 21 seconds on average.
Liberators played at 13 seconds per average, a per per game.
So we were playing a little bit faster than the top humans, but by and large at the same
speed on average.
But it's interesting, the thinking pattern is very different.
So humans think also on the first two betting rounds while we don't typically, or sometimes
in certain situations, the AI will think there too, but typically not.
And the funny part, in my opinion, is that humans typically think the longest when there's
a lot of money in the pot, because that is a big decision.
In contrast, the AI does the opposite.
The AI thinks longer if there's less money in the pot, because that's when there is
more game tree left.
So there's more to be thinking about.
So it can be quite frustrating for humans when they're used to playing other humans who
think a lot on the big pots and go fast on the small pots when the AI does the opposite.
It makes big decisions instantaneously almost.
And then on the tiny decisions, it can take a long time to think.
And so this is one of three modules that the Bratis used in this, this most recent match
are the other two published, or are there plans to publish them?
We have talked about them.
So for example, in my keynote talk at the International Joint Conference on Artificial Intelligence,
in August, I talked about all three modules, so you can find that on YouTube.
And we are about to publish the whole thing.
It's currently still under review.
Okay.
And going back to you mentioning this running on a super computer, what language or other
tools does it use?
Is it written in something like a Python or something totally different?
Python would be a way too slow for this.
So we wrote it in C++.
Okay.
And does it use like an MPI for interprocess communications or something like that?
Okay.
Interesting.
Interesting.
And do you are there any plans to publish source code for this and the other algorithms?
No, not really.
And it wouldn't be usually that helpful.
The super computing codes, they're very, very complex.
We don't really have any plans to do that.
But with some other students in my lab, we're writing a game solving framework that's kind
of easier to understand, wouldn't be as scalable as this.
But we might open source something like that so people can use it for teaching.
And like I use in my AI courses, when I teach AI, we do some amount of imperfect information
game solving there as well for small games.
And there we do provide source code so that the students can start from that.
Yeah, I would think folks would be very interested in that.
Where can folks learn more about your lab and your work?
I would say at the best spot to start, would be my homepage.
So www.cs.cmu.edu slash till the suned column.
So S-A-N-D-H-O-L-M.
And if you go into the section on equilibrium finding and abstraction in games on my homepage,
that's where you can find our papers on this topic.
But there's a section for all the different strands of research going on in my lab so you
can find papers on all of the different topics.
Awesome.
Well, to all of us, thank you so much for taking some time to chat with us about your
paper and congratulations on the award.
My pleasure.
Thank you very much.
Thanks.
Alright, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on to all of us or any of the topics covered in this episode, head
on over to twomolei.com slash talk slash 99.
Of course, we'd be delighted to hear from you, either via comment on the show notes page
or via Twitter directly to me at At Sam Charrington or to the show at At Twomolei.
Thanks once again for listening and catch you next time.
