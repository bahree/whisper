Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Thanks so much to those of you who participated in the Twimble Online Meetup last week, and
it's Kevin T from Sigopt for presenting.
You can find the slides for Kevin's presentation in the Meetup Slack channel as well as in
this week's show notes.
Our final Meetup of the Year will be held on Wednesday, December 13th.
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion segment.
For our main presentation, prior Twimble Talk guest Bruno Gonz√°lez will be discussing
the paper, Understanding Deep Learning requires rethinking generalization.
By Xiu Juan Zhang from MIT and Google Brain and others.
You can find more details and register at twimlai.com slash Meetup.
If you receive my newsletter, you already know this, but Twimble is growing and we're
looking for an energetic and passionate community manager to help expand our programs.
This full-time position can be remote, but if you happen to live in St. Louis, all the
better.
If you're interested, please reach out to me for additional details.
I should mention that if you don't already get my newsletter, you are really missing
out and should visit twimlai.com slash newsletter to sign up.
This week we'll be featuring a series of shows recorded from Strange Loop, a great developer
focus conference that takes place every year right in my backyard.
Not literally in my backyard, but here in St. Louis.
The conference is a multidisciplinary melting pot of developers and thinkers across a variety
of fields and we're happy to be able to bring a bit of it to those of you who couldn't
make it in person.
Later this week, you'll hear from Sumit Shintala, a research engineer at Facebook AI Research
Lab, Matt Taylor, open source community manager at Nementa, Alison Parrish, professor in the
interactive telecommunications program at NYU and Sam Richie, software engineer at Stripe.
We'd like to send a huge shout out to Nexosis, who helped make this series possible.
Nexosis is a company focused on making machine learning more easily accessible to enterprise
developers.
In fact, you'll learn a bunch more about the company and what they're up to in this
show, which features my interview with Nexosis Founders, Ryan C.V. and Jason Montgomery.
Ryan and Jason and I discuss how they got their start by applying ML to identifying cheaters
in video games, the application of machine learning for time series data analysis, and
of course, the Nexosis machine learning API.
If you like what you hear, then invite you to get your free Nexosis API key and discover
what they can bring to your next project.
You can do that at nexosis.com slash twimmel, that's n-e-x-o-s-i-s dot com slash twimmel.
And now on to the show.
Hey everyone, I am on the line with Ryan C.V. and Jason Montgomery.
Ryan is the CEO and co-founder of Nexosis and Jason is the C.T.O. and also a co-founder.
Welcome to this week in machine learning and AI, guys.
Thanks for having us.
Yeah, awesome to be here.
Thanks so much for hosting us.
Fantastic.
Fantastic.
So, as is the tradition here on the show, why don't we get started by having each of
you introduce yourselves and tell us a little bit about your journey to machine learning
and AI?
Yeah, sure, so happy to, as you mentioned, I'm the CEO and co-founder of Nexosis, Jason
here is our C.T.O. and kind of just going back to maybe the beginning of how all this
came to be.
Jason and I actually met each other at a company called American Electric Power, which is headquartered
here in Kamba, Ohio.
Him and I were both on the cybersecurity engineering team.
And AP, I believe is the largest generator of electricity in the United States.
So when you think about that, you can think about all the different assets that they have
to put in the field, which kind of directly correlates to how much data is being collected.
And basically our task bear was to ensure that all their assets were secure from both
internal and external attackers.
We quickly realized that the amount of data that was being generated would be pretty much
impossible for the analyst team to go through and identify any kind of anomalies things of
that nature.
So Jason and I then started thinking and hearing more about machine learning and there were
classes offered online, I believe through Stanford University.
And him and I both signed up to do these online courses.
And we went and did all the homework, did all the, you know, kind of bonus material, if
you will, and we got to a point where the instructor said, okay, you now know more than
everyone in the valley.
And we're like, all right, that's a good stopping point.
So it was becoming very, very, just becoming very mathematical and that's not necessarily
that thing.
But, you know, we are both very technical individuals, lots of development experience, things of that
in our background and just quite frankly, like learning six players of mathematics isn't
that appealing.
But it's not, you know, like cool, I guess it's neat to understand the math behind this
one algorithm, but quite frankly, that wasn't where we're taking it, right?
We were, we were taking a class early, I understand how we could use machine learning
to solve a very particular problem.
So out of that, we just kind of long story short, Jason ended up leaving American Electric
Power.
He went to varicode.
He was a dot net, principal dot net researcher for them, varicode is an information security
company that does software.
Yeah, we do a binary static analysis, you submit your binaries and we find flaws in the
code and give you sort of a report that says, you need to fix code in these places or
what not.
So I did, I was a researcher there for about three years.
Reason we bring that up is when he left AP to go to varicode, I shortly thereafter left
AP to go to healer Packard, but Jason, I kept doing joint research projects together
just mostly out of fun, right, in our spare free time.
So that means that we lost access to the AP data, which was, that's fine.
And we were kind of still enthralled with this notion of what machine learning could
do.
And we started thinking about other use cases.
And I forget exactly how it happened, but I asked Jason one day why he didn't enjoy
online gaming and his response was basically, there's a lot of cheaters in it and I want
to know if I'm getting, you know, under nominated by someone that video gave that it's because
they're actually better than me and not because they're using cheats.
So I thought, well, that's kind of an interesting feedback.
I said, I wonder if we could use machine learning to identify patterns and, you know, public
data sets available via the steam API that would identify a cheater.
And I think really the whole notion back then was if you're cheating, you're trying
to do something that a normal human wouldn't do, so it should show up in the stats, right?
Like if you see this ridiculously high, like headshot percentage, you're probably cheating
unless you're a professional, right?
Or into the stats show, like for the last two years, you have like an accuracy rating
of like 15%, and then suddenly overnight your accuracy jumps to 50%.
Well, that's suspicious, right?
So steam makes it pretty easy to get all that data that we needed via their API.
So we spent about six months and we created a proof of concept that basically had a pretty
good accuracy rating.
It was like 88% accurate on identifying whether or not a player should be banned by
valving a Thai cheat.
Well, let me, if I can interrupt and ask a question or a couple of questions, I'm assuming
then that you were using supervised learning for this task, and if that's the case, what
did you use for labels?
Did you manually label some number of users that you thought were cheaters?
And then how did you validate this to determine your 88% accuracy rate did?
Did steam also publish, you know, whether they thought folks were cheating or did you just
look and see if they were eventually banned or something like that?
Yeah, that's a good question.
So what we did was we took the professional gaming league websites.
We took data from there and our theory behind using them to train not cheaters was that
people tend to be watching the professional gamers more, there's more eyes on them.
So cheating would be more obvious.
And then we went to the, there was a website called Backband.
Its whole purpose was to take steam IDs and categorize those that got banned based on
certain dates.
We were able to correlate those with the steam API and pull down their game stats.
We did some cleaning of the data.
We looked at, you know, when they purchased CSGO if they had the game because we, the data
set wasn't perfect like all data has lots of crap in it.
But we spent a lot of time sort of analyzing it and making sure we had a pretty, pretty
confident set of cheaters and a pretty confident set of those we felt weren't cheating.
And that was sort of the process we went.
Awesome. And so what do you, what do you do with this proof of concept?
Yeah.
So at this point in time, we released the research to the public at a security conference
called Derby Con and I think 2014.
And there were some Reddit posts and it got a lot of traction actually valve reach out
to me about the research and kind of was asking how we did it and whatnot.
And I told them because at the time, that was like, well, fix the issue that be more
than enough for us.
So that kind of naturally led to the formation of Nexosis because as we were going through
the model building, we tried using different services like Microsoft's ML Studio, we tried
using Amazon's API for machine learning.
We tried using Google's product API.
And at the time, that toe was still a company which became Tory, which then sold the Apple.
So we tried all these different things on the market and the conclusion that we reached
from all this was nothing's really out there for the developer.
Everything is really catered and aimed at the data scientists.
You so often understand how to train and build a model and then you just get into this
question of how do I make the model better.
And like fundamentally, you have to know the algorithm you want to use and you have to
know why you would want to use algorithm A over algorithm B. So there's just lots of issues
that from a developer point of view, i.e., we want to just get something out there that's
doing a good job, a very, very painful experience.
So we then set out with that knowledge and tact and we said, look, why don't we look at
machine learning under the lens of a developer and how would a developer want to consume machine
learning.
And this has been done in other industries for a while, right?
So the one that I can point to just off off my head is Twilio.
And you think about what Twilio did, kind of very similar when you look at the data science
machine learning landscape.
Prior to Twilio, if you wanted to create a communications application, you would have
to really have someone on staff that understood the telcal infrastructure, understood voice
over IP protocols and how to configure it.
We actually have a slide here internally that kind of compares, here's what they'll look
like before Twilio and here's what they'll look like after.
I think the world today looks very similar on the data science point of view, right?
You have to know all the different just how do you do VTL, how do you deal with missing
variables?
There's just so much to it.
And then again, how do you know what algorithm you want to use and when blah, blah, blah,
right?
It becomes very complicated.
So the whole notion of exosys is saying, look, developers, you use the same language that
you're used to using.
If you're a.NET developer, come to us, you know,.NET, C-Sharve, if you're a Java developer,
then go use Java.
You don't have to learn a new language to incorporate machine learning to your project.
And that's really the whole basis of exosys.
Okay.
When I think about what, let's take Azure ML for instance, when I think about what they're
trying to do.
It sounds exactly like what you're describing, right?
They're trying to create an API that lets a developer deliver, you know, machine learning
types of applications.
You know, they've got the studio, you can even do it drag and drop if you're so interested.
If you're interested in doing so, maybe we can dig a little deeper and you can, when
you're faced with skeptics that say, you know, how can you enable a developer to do machine
learning without knowing anything about data science, like how do you address that?
It's a good question.
When we looked at ML Studio, for instance, a lot of the work, as you know, is data preparation,
data cleaning, ETL, it's like 80% of the work.
And so, and then they're scaling, there's how do you do imputation strategies, how do
you aggregate data, all these sort of questions.
And developers are very used to using working with data and data types.
And you can typically take a data type, identify, and you kind of know what step you need to
do to impute or aggregate at that point.
And so, giving the developer the tools in their hands to sort of define maybe the data types
with metadata, but not really having to worry about what needs to happen before it
can go into the algorithm, before we convert it all into numbers, whether we need to one
hot and code, categorical data, things like that, you know, how do we pick features.
All of that stuff is, there's a lot of automation that can be done to simplify that.
Now, you still have to know your data, you have to know a good question you want to ask,
you have to validate that, you know, you are submitting features that make sense.
We're not going to know your data for you, but in a lot of ways, we can automate some
of that heavy lifting at scale, and then we can build lots and lots of models, looking
at different combinations of those things, and then sort of finding what falls out of
that.
And just to add to it, the other thing with all the different platforms is that you
also have to know how to go in and manually tune the models to make them better, right?
So if you're in Azure Studio, and I don't know, you pick an algorithm, which I think
is the first hurdle, right?
So your developer, you're in Azure, and now you have, you know, 15 different algorithms
underneath regression.
So which one are you going to pick, right?
Like your developer, you're sitting there, you're seeing 15 algorithms, how do you know
which one to use?
And how do you then go about making that model better, right, be it tuning, or maybe
you have new features, with Nexus, we do all that for you.
So we have probably close to like 150 different algorithms at this point that are in the platform.
And basically just kind of high level how it works is we hold these tournaments and the
algorithm that's performing the best based on scoring metrics is the one that's used.
So you as a developer don't even have to come to us and say, hey, I'm not, like I'm going
to use, who's the decision trees, you don't even have to know that you just have to say,
hey, here's the problem that I have, here's what I want to solve for you guys build all
the models and then you guys tell me they want us doing the best, right?
They're sort of like a universal pipeline we use, depending on the type of that problem
you want to solve.
And it sort of makes a lot of those decisions based on metadata that they provide as
well.
Are there specific types of problems that that this works for?
It sounds a little bit too good to be true to be kind of applicable to everything or anything.
Like I can give it any kind of data and it could do any kind of algorithm.
Yeah, so right now we're not going super deep into what is traditionally viewed as the
deep learning space, which that's pretty well solved, right?
Like if you want to go predict, is this a hot dog or not, like, there's something up there
or do that for you already.
So looking at Valley reference, right?
And like so we're not really super focused on deep learning, we are starting to incorporate
more things around voice and speech, so you know, P and not exactly sure how deep
I'm going in that vein, but we're really more focused on the true machine learning and
kind of the layer that's above deep learning.
So you know, we're not going to probably release any time soon, kind of like an image recognition
element into our API.
Again, I think that's been solved and it's been solved pretty well, but where you don't
see a lot of stuff is underneath like regression classification, clustering, and again, kind
of all the real machine learning types of elements.
So today we launched the API doing time series, which I think is also a little bit unique.
We don't see many platforms out there that have true time series capabilities in it.
And time series is just naturally kind of very hard thing to create models in and really
apply machine learning to them.
So we launched with any kind of time series problems that today, if you have a time series
like question like demand forecasting, API, we do that.
We just release regression, so any type of regression problem can be solved with the API.
And later this quarter will be releasing the classification endpoint, and then we'll
just continue going down that vein of machine learning if that makes sense.
Okay.
And now it sounds like, you know, when I think about the the tournaments that you described
as kind of conceptually happening on the back end, you know, it strikes me that for a
given problem, you know, let's say I've got a bunch of time series data and I am trying
to do a predictive maintenance type of application.
Is that the kind of thing that you might do?
Yeah.
That's one of the most just because of where we were and we did the XR's retail accelerator,
a lot of the early use case with the API are more about, hey, I have this store is in
Columbus, Ohio, and I need to know of the 100,000 products that I have, how many I need
to have on the shelf for next week, right, so when I place my reorder, how many new products
I need to have in my warehouse.
So more than the main forecast and type of element, but yeah, predictive maintenance would
work as well.
Okay.
And so the time series in that that lot of cases in the retail case is transactions, transaction
history.
Right.
Yep.
Okay.
And either of these cases, like I'm thinking about the, you know, you've got some set
of models then that you are training the, that you're training against this data.
So there's kind of a fan out there, but and then for each of these models, you've also
got to, you know, do the hyper parameter tuning and all that.
So there, there's a fan out there.
It strikes me that I guess I'm trying to put my hands on like the, the scale aspects of
the problem.
It seems like for any given individual problem, you end up doing a ton of different training
runs.
And I'm wondering, you know, if there's some way you can kind of characterize or help me
understand the, you know, the, the way that that looks from an underlying resource perspective.
Yeah.
We have a very dynamic workflow engine that starts, it's cute driven.
So we can scale in and out of number of CPUs we want to use.
So it, it scales up and down automatically based on demand.
So it's, we built, we spent a lot of time building automation to sort of handle that.
So we're not, you know, we have the cloud to work with so we can do scale sets.
We can do a lot of different things in parallel.
So you know, we could build a hundred or a thousand models all simultaneously and compare
different results, try different hyper parameters, try different feature combinations and things
like that.
And then once we get sort of down to the, to a solution that works well that they're
happy with, they can sort of tune more parameters around that or go with that model and use it
to predict.
Okay.
Is there any particular method that you're using to do the hyper parameter optimization?
I would have to ask our data science team to get into details of that.
We do not have PhDs and, you know, 10 years of experience in the industry.
While we do have some understanding of machine learning, we thought it important to hire
a research team to solve some of the more complicated issues that we're not qualified to
solve.
And so in just as we, he means him and I, yes, we do have PhDs at Nexosis.
Yes, we do.
Right.
Yeah.
Got it.
Got it.
And so when we talk about the kind of the time series, I guess it kind of makes sense
that you, that that was a, uh, uh, initial place to start in that that was a little bit
of the kind of data problem that you ran into it at the power company, is that right?
Yep.
Can you talk it all to kind of a unique, you know, anything unique challenges or things
that you do with regards to time series?
One thing that I don't think people think about is using the product skew example that
we were discussing earlier, if you have a hundred thousand skews, that means ideally you
have a hundred thousand models, right, so every skew has its own model.
And the real power of what we're doing is that each individual skew could have a completely
different type of algorithm winning, right?
So if you're selling snow shovels has one skew item, a different type of fundamental
algorithm might be winning based on location too, right?
So if you have a store in Columbus, Ohio, then you have a store in like Atlanta, Georgia,
very, very different results, even though that's theoretically the exact same item, right?
So that's really the other powers that we're able to take into account, okay, you're selling
a snow shovel in Columbus, Ohio, and it's probably going to be snowier here, so you're going
to sell a lot more.
Right.
And again, just a different algorithm might be winning here on Columbus, even though it's
the same exact item, and then just thinking that even farther out when you think about
things like bottled water as an example, you know, an algorithm that's going to predict
how much water you're going to sell might look a lot different than an algorithm that's
going to predict, you know, how many white t-shirts you're going to sell, right?
The feature importance in both of those might be dramatically different and they are different,
right?
So that's the other thing when you start thinking about scale, one model or one algorithm
doesn't fit all, right?
And so you're able to like, so if as a developer, in this maybe dig into this retail
case, as a developer, how am I giving you my data?
I'm assuming that's the starting place.
Yeah.
So what we do is we have a couple different ways you can import data through the API right
now, and it's through JSON or CSV, and then we have some S3 endpoints where you can put
larger data sets, and then we'll ingest them from there.
Okay.
And so I'm giving you this data via the API.
And how am I doing anything then to kind of describe this data or are you like figuring
it all out somehow?
So we try to have what we call sane defaults.
So if you didn't give us any metadata and you uploaded a data set, we would do some basic
analysis of that data set and try to create appropriate data types for each one.
Now is that going to be, is that going to work great?
Probably not, but will it work well enough to sort of get some results?
That's our hope.
As people sort of learn more about what they need to do with the data, we have metadata
that you can use to sort of describe this is a string, this is categorical data, or
I need an imputation strategy on this numeric field that does mean mode or, you know, that's
sort of thing.
So you can start to describe and we'll try to make some of those decisions for you,
but it's better certainly once the developer gets in and sort of gets their hands around
their own data and understand what they need to do with it.
Okay.
And you've mentioned imputation strategy a couple of times.
Tell us what that means.
The idea there is if you want to look at aggregating data over time, the question is around
do you add it up?
Do you summon it or do you take an average, right?
If you're looking at the weather, adding them, temperatures doesn't make any sense, right?
You want to average it over time.
So it's that general idea there that you can sort of indicate what type of data it is
and then we'll take that step of how you want to handle that aggregation or imputation
around that.
So fields are missing, right?
We might put in something else, or if you want to roll up your daily or hourly forecasts
up to a monthly forecast, we're going to use different strategies to sort of roll that
data up as well as fill in empty values.
So I give you this time series of transactional data for this retail use case that has date
time, skew purchase price, and maybe some other stuff.
Like how do I then, how do I tell you what problem I'm trying to solve?
You're basically going to tell us the column that you're wanting to predict off of, right?
Yep.
So in that same metadata, you just say, these are features and this is a target.
And you can turn those on and off for each run as well.
So if you want to turn off a feature, you could then predict on McCollum.
You could turn it back on.
You could do what if scenarios that way, too?
Just sort of say what if we did this or that with those features, too.
And then are you able to do anything with around like artificial features, like so features
that aren't in the data and the, you know, predicting home price example, you might want
to look at the number of rooms times the number of bathrooms and that, you know, particular
and artificial feature might have some predictive value, you know, that either of those features
by themselves doesn't have.
I mean, it depends, I would say in those cases, no, we're not going to just sort of brute
force through all your columns and try multiplying them and see if something happens or dividing
by something, you know what I mean?
Like, that's a hard problem.
So I think in that sense, you need to know a little bit of your data and what the indicators
are.
But in other cases, we can sort of do some interesting things with maybe holiday calendars
and we can automatically overlay with your time series data.
We've done some work sort of with, you know, Latin launch locations, we can incorporate
whether automatically certain weather features that might help.
So it depends on what it is and we have some interesting ideas and plans in the future
for that as well.
But no, I mean, at some point you have to sort of understand what the indicators are that
may help you predict, right?
Like, we're not, we're not a crystal ball.
So I tell you what, what columns I want to predict on and how does the, does, you mentioned
earlier that the, the platform will, you know, build individual models for each of the,
the excuses.
Is that something that I need to tell it to do or is that, is it always doing that?
It always does that with a caveat, you know, once you have a model, though, you can reuse
it.
The problem with time series data is, of course, as often yesterday as a better predictor
of today or tomorrow than a week ago.
So there's sort of the notion of how, how long my algorithm is good for, right?
And so on the time series, you end up rebuilding models a lot more frequently than you might
with like a regression model that's just, you know, not as concerned about those sort
of yesterday as the future or two days ago, three days ago, last week.
Okay.
Maybe tell me a little bit about some of the kind of technical challenges that you had
to overcome to put all this together.
Boy, yeah, that's, that's a big question.
That's the word start.
That could be a second podcast.
Yeah, I think really, you know, when you try to build a generalized platform, it's, there's
so many challenges, you know, around handling all sorts of data, not, you know, and we can't
boil the ocean.
So we have to make a lot of trade-offs and choices around what is the MVP going to look
like?
What is the next version going to look like?
What can wait?
What do we have to have now?
How do we get something to market?
And so from the product side, it's been a big challenge to sort of make those trade-offs
and then get the work done in a way that we feel good about the results.
So yeah, I think building sort of that ETL pipeline in a general sense, sort of that pipeline
around, you know, do you scale the data before you run the algorithms?
When do you have to do that?
And sort of building in sort of all that core capability around, how do we get any sort
of data into a common sort of matrix to do ML on?
And there's lots of different paths to get there.
And I think one of the big challenges was trying to define the best use cases for us
to sort of get the broadest hit.
Now, you know, we have to trade-off some accuracy for that and that's okay.
As we go on, I think we feel like we can get better and go deeper in a lot of areas.
Mm-hmm.
Yeah, that was a question that I had earlier and I didn't ask it.
You know, often when, you know, I've talked to folks working on generalized machine learning
platforms, the idea tends to be, you know, you're trying to get the developer, the organization
from, you know, zero to 80 percent.
And then maybe once they're, you know, at 80 percent have some maturity, they may find
it they, you know, that they need to invest further to get to, you know, 90 percent.
Is that the general way you think about the problem as well?
Yeah, to a certain degree, it's always, I just think where we are today, getting people
familiar with what machine learning can do is kind of the first hurdle.
And then as you were saying, how do we get better performance after we actually have
something in the environment or some kind of application deploy that's using machine
learning?
So I think there's this natural evolution and that's one of the things that we've kind
of built into our own platform is let's talk about like a regression problem.
You were talking about house prices, I think earlier, well, let's say your initial data
set has like 15 features and it's like room size and how many bedrooms and whatever, right?
And you build your model and it's pretty good.
But then as time goes on, you might think, well, hey, you know what, maybe this new thing
that I just thought about is going to have a big impact and maybe that new thing in this
scenario is the school rating in your area.
So now you're going to bring in the whole new feature of school ratings for the house
that you're looking at.
And then with us, it's just naturally going to build a new model.
So you don't even have to think about it.
All right, well, now I have the same feature to idea into algorithm or not.
If you do need a new algorithm, our platform is just going to figure that out and it's going
to say, all right, well, you know, maybe you're using like a classio regression or something
before and now you're going to use whatever else because you added all these new features.
I think that's really the other big power of this, right?
And as people start looking at the accuracy and the results, I think the natural question
is always going to be, well, how can we do better, right?
And we are trying to spend a lot of time and energy on that educational component, which
is kind of answering that question, okay, we have this today.
How do we get a better, right?
Is it going to get better if we add in school ratings?
Maybe probably, but let's figure out what happens when we actually do it and then they'll
figure out, okay, well, this had no impact or had a big impact and then that should lead
to the next question of, all right, well, what else could we add, maybe to make it even
better, right?
And you could just keep going down that old trend and naturally just our platform is going
to figure out, okay, I should mature and add new features, we're going to pick maybe
a new algorithm.
It's going to perform even better.
Yeah, it's funny.
A lot of those things, those activities that you're describing on the spectrum of improving
your algorithms are things that I think of as data science.
Like if you separate out the knowledge of the underlying math from the process and the
way of thinking about data and features that have some predictive value and using those
to create predictions, like that to me, a lot of that is what data science is really
about.
And I'm wondering if you, you know, is it that you end up teaching developers, those parts
of data science in order to get them productive on this platform or are you finding that there
are, you know, maybe folks with different roles that understand that stuff but don't understand
the math.
How do you kind of see the audience and for what you're doing and is it evolving at all?
I think the audience is absolutely evolving.
I think as we look at the future, we're releasing some additional features that should really
marry this notion of developers teaming up with data scientists.
We want to enable more collaboration in that space.
We think one of the main things as we look at more mature organizations is assorting
the time from R&D to production.
So if you're a data scientist and now you're collaborating real time over, you know, this
maybe beta application that the developer has made that kind of really speeds things up
and then as the data scientist is looking at what the developer may be made as an initial
group of concept, you know, yes, the data scientist might really be honed in on how to make
this better.
So that's one approach and then I think again, you just have so many, from an innovation
perspective, enabling developers to quickly prototype things as a tremendous value, right?
And then just being able to show if you're a developer, you know, maybe you have a data
science friend or someone at your company is a data scientist, you know, being able to
say, hey, look, I made this prototype application and it's doing X, you know, what do you think
what else maybe could I do to it?
So just fostering that collaboration is obviously really important to us.
As we look at especially 2018, you should see more and more things start to be released.
Just to add on to that, there's really not enough of data scientists to go around unfortunately.
So we also enabling, I think, people to get some capability up is better than, you know,
them just having to go, I guess, go hungry.
So to speak.
Yeah.
Yeah.
And where it's industry or vertical focused, where you're able to, you know, you've got
data scientists that are thinking about the problems in those verticals and what the
data needs to look like and what the features are so that you can, you know, guide or offer
a special, you know, features for specific verticals or is that not a focus for you right
now?
Yeah.
So shortly, we should be releasing this notion of kind of data templates.
The idea behind that is that the bare minimum here are kind of the features that you would
need in order to be successful with this type of problem, right?
So skew level forecasting as an example, you know, the bare minimum features would be
like the time, right?
So ideally, you want a year or data, it can work with less than that, but best performances
a year.
And then you're going to want, you know, probably daily sales activity.
And then beyond that, like those are just the bare minimum, you can create a forecast
off just those two things because the platform automatically extracts the database on the
timestamp, you know, if it's Monday to say Wednesday and I'll find the weekly and daily
trends things of that nature.
But then you could start adding into that template additional things that maybe aren't necessarily
required with a nice to have, right?
So a nice to have would be, do you have a promotion going on for that particular item,
right?
And that could be a binary one or zero.
You could extractulate out like what kind of motion it was or it isn't like a buy one
get one.
Is it just a percent off?
Things of that nature.
But yeah, we have plans to kind of put out there.
Here's a bare minimum that you need and here's kind of like the nice to have and kind
of go from there.
Oh, interesting.
So in that example, where you're, you've got a developer, they've gone out and collected
some sales and marketing, you know, historical data and they're doing a forecast.
How do you articulate to this developer that may not be, you know, statistically sophisticated,
the extent to which they should rely on this result that your platforms put out for
them?
Yeah, currently we put out some metrics on that.
And what we've determined is we need to get a little more friendlier on those metrics.
So we have some sort of education around what the metrics mean and sort of our, some
of our learning on our documentation site.
But we want to really go to the next level I feel like and really sort of hold their
hand a little more on what the metric is saying about the model based on what the data
they have.
It's currently, I think we return mate scores if it's a time series site forecast.
And yeah, what we plan to make it even more friendly than that, right?
Because developer might not understand what mate scores may mean absolute percentage
error.
Uh-huh.
Okay.
So this is like these interfaces between kind of what you might expect the data scientists
to know and what the developer might not know or some of the key challenges that you
face as you evolve this and bring more people onto the platform.
Yeah, honestly, I think that's a, you're just quite frankly, I think that's a minor challenge.
I think the biggest challenge that we have and I think just the industry has in general
is access to data.
If I want to build, and obviously I was thinking about this over the weekend, I wanted to build
a couple of different applications and the hurdle is getting the data sets, right?
So if I wanted to build an application off of predicting cancer, there's certain cancer
data sets already out there.
It's a very famous, the breast cancer data set that kind of has the size of the tumor
and things of that nature.
But maybe a big important feature there is where the people live, right?
If you live next to, I don't know, a waste site, you know, that's a little bit extreme,
but let's say that you live there.
Maybe that's why you have cancer, right?
So maybe where people live might actually be a huge indicator whether or not you're going
to develop cancer or that lump that you discovered is the minor eminigment, right?
But it's very hard to get that data set, right?
For one, you have the HIPAA issue, so there's that hurdle to get over.
But then more than that, it's just, are people going to want to share the data?
The other idea I had was predicting whether or not a startup would be successful.
And one of the things that I think you would need to do that would be kind of the financial
information on the startup, right, and how much money they're spending, where and the
return.
But again, trying to get reliable data that's going to help me build that model is kind
of what I think right now.
It's the biggest obstacle in the field.
Any other thoughts on that particular point, any other obstacles that you see?
Yeah, I mean, there's plenty of them.
I think really walking them through what their data might be telling them is going to be
helpful.
So like one of the big challenges is you submit a data set and I think there needs to be
more of an indicator of, you really can't do anything with this potentially in some
situations.
It's like, it's okay, and then like this is really good data.
So I think sort of kind of defining and helping them along without just just raw metrics,
you know, like give them a little more, I guess, safety feeling about their models is still
a challenge.
And then we have some other ideas around that.
Yeah.
I think just also the add to that one of the challenges is how machine learning AI, whatever
you want to call it, today has been talked about in the media, it's put into the minds
of a lot of people that machine learning is this magic bullet.
And to give kind of a real example, we have some people today that have signed up for
the API that want to use it to protect things like Bitcoin price.
And the data set is just the price point on a day.
And they think that machine learning is just going to like magically figure out what
the price is going to be.
And you know, sometimes we have to talk to the people who sign up on that use case or
like, well, if that's all the features you have, do you really think that to say is the
reason why Bitcoin jumped up or do you think there's this other feature out there that
is really impacting and influencing it, right?
So again, it's just people really have to, I think, get beyond that machine learning isn't
a magical bullet.
It's only going to find the patterns in the correlation in your data set.
If it's not there in the data, you know, it's never going to find a pattern.
Yeah.
That's awesome.
Anything else that you'd like to share with the audience as we come no close?
You know, there's just the other only thing that I was thinking about is as we're talking
about kind of our overall goal.
I think really most of 2018 is going to be this theme of how do we enhance collaboration
between developers and data scientists.
One of the newer features that we'll be releasing in 2018 is something called bring your
own algorithm.
So what this is going to do is it will allow a data scientist to create maybe a very specific
algorithm that is really good at solving for a particular thing.
So let me give an example.
Let's say that you are a data scientist at Best Buy or some big box retailer and let's
say the retailer really cares about the price or the sales for like, I don't know, LED
TVs.
Well, the notion is that they might create a very specific algorithm that's really good
at figuring out, hey, how many flat screen, LED 55 inch TVs that we're going to sell.
And all that they would have to do is take that algorithm that they build in house.
They could just plop it into our API and their algorithm will live just in their instance.
It'll live by itself.
Well, it'll live just in their instance by itself, but it will live side by side our 150
plus algorithms.
So they'll be able to see your real time.
Hey, here's where our algorithm is winning and here's where it's not winning.
I guess a related question that I had from earlier, as you're starting to get into collaboration
and data scientists working with developers and developers exploring their data and creating
new features and things like that, it opens up this whole set of issues around model management
and model governance and model providence and stuff like that, are you the company thinking
about any of those things or do you offer support for those kinds of challenges?
Yeah, that's certainly come up in the past.
We've had it come up primarily in the insurance industry, you know, the regulations around
the model building.
You really have to understand how the decision is coming about right because you can't
discriminate in things of that nature, so yeah, for kind of the more enterprise customers
will allow them to actually download that model and algorithm and kind of have that supporting
documentation they need to have to prove that, you know, we're not using for lack of
better word illegal types of data sets to create, you know, the prediction result.
So I think as time goes on, we're going to see more and more of that.
And again, right now, one way that we can solve it is just by giving a dock rise container
that contains the algorithm and explain how it works and things of that nature.
So yeah, I mean, as time goes on, I think we're going to see more and more of that.
Okay.
All right.
Great.
And then did you have an offer for like free access to the platform or free API key or
something like that available to listeners?
Yeah.
One can sign up for the API for free, that's part of our philosophy as a company and being
developer first is that we want people to use it.
So we do have a community edition, it's 100% for free.
It will always be free that community tier.
So yeah, anyone can go to nexos.com, sign up for an API key and get started in under five
minutes.
Okay.
Awesome.
Well, guys, thank you so much for taking the time to chat with us.
I think what you're doing is really interesting and I will certainly be looking forward to
keeping up with you and I'd love for you to keep in touch and, you know, let me know about
you know, as the platform evolves, your continued success, et cetera.
Yeah, definitely was pleasure being on here and thanks so much for having us.
Okay.
So before we go, why don't you take a second and tell me a little bit about what's attraction
been to date?
How many users do you have or how many companies are using it?
Yeah.
So currently we released the act of the public on July, no, on 2017, today we have over
4,000 developers that have signed up and we have close to 500 applications that have
been deployed.
Oh, wow.
All right.
Well, once again, thank you so much for taking the time to chat with me.
I appreciate it.
Yeah.
Likewise, pleasure.
It was fun.
Thanks, guys.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Ryan, Jason, or any of the topics covered in this episode, head
on over to twimlai.com slash talk slash 69.
This interview kicks off our Strange Loop 2017 series.
To follow along with the series, visit twimlai.com slash ST Loop.
Of course, you can send along your feedback and questions via Twitter to at twimlai or
at Sam Charrington or leave a comment right on the show notes page.
Thanks once again to Nexosis for their sponsorship of the show and this series.
For more info on them and to get your free API key, visit nexosis.com slash twimlai.
And of course, thanks once again to you for listening and catch you next time.
