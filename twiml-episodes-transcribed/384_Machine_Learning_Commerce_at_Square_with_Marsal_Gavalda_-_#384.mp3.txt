Welcome to the Twemal AI Podcast.
Hey, what's up everyone?
Before we get to today's show, just a few quick updates.
First off, I'd like to ask for your help with our latest survey, this time around on
the Python data science ecosystem.
This is our second year exploring this space and we'd love to have your input.
If you use Python for data science, please visit twemalai.com slash Python survey to take
the survey.
The survey will only take a few minutes to complete and three lucky submissions will receive
a $25 Amazon gift card.
Speaking of languages, we are super excited to announce the great machine learning language
on debate.
Join us for a thoughtful discussion exploring the strengths, weaknesses, and approaches
of popular and emerging programming languages for machine learning like Python, R, C++,
Swift, Julia, Closure, and more, and an opportunity for you to engage with experts and peers on
the topic.
It's not too late to let us know the languages you'd like to see included and who you'd
like to see representing them.
Send me your suggestions via Twitter at at Sam Charrington or hit me up at Sam at twemalai.com.
Visit twemalai.com slash languages for more information or to register.
And now on to the show.
All right, everyone.
I am on the line with Marsal Gavolda.
Marsal is head of machine learning for the commerce platform at square.
Marsal, welcome to the twemalai podcast.
Thank you so much.
I'm very excited to be in your podcast.
It's great to have an opportunity to speak with you.
And I'm looking forward to digging into how ML is working at square, what you're up to.
But before we do that, share a little bit about your background.
What got you interested in machine learning and what are some of your interest areas?
Yeah, sure.
I'm happy to share my sort of life story here.
I grew up in Barcelona, Catalonia, Spain, which actually is in the news still quite a bit
because there's an unresolved conflict between Catalonia and the rest of Spain.
And a lot of it is based on language.
So growing up there, I got very interested in languages because I wondered how come half
of my classmates speak Catalan and they all have speak Spanish.
When if you look at the history as the Roman Empire extended throughout Europe, they were
all supposed to be speaking Latin, right?
And yet Latin gave rise to Italian in one place, Romanian, French, Portuguese, Spanish,
Catalan, all the languages that we now call Latin or Roman's languages.
So that got me curious about where do languages come from and how do they evolve?
What is common across all languages?
What is unique?
Take the word for window, right?
And Catalan window is finistra, which actually is the exact same word as in Italian finistra.
Very close to the French finetra, right?
Even close to the German finster, even though Germany is not even a Latin language, whereas
in Spanish, I don't know if you know any Spanish.
Spentana, right?
Exactly.
Yes.
And where is the etymology of Entana?
Where does it come from?
Entana comes from Viento, which means wind.
And it's super interesting because the work, the origin of the, in English, of window also
comes from like all Germanic wind out, so like a wind's eyelid, a little hole for the
wind to go through, which if it's, you know, what would you call now something for air
or wind to go through that doesn't have a glass?
You would call that event, right?
So you have window and ventana, all these sort of related to wind, and on the other hand,
there's finistra, right?
Which apparently...
This might be the first episode of my language podcast.
Oh, yeah.
Let's expand our horizons.
Yeah.
You know what?
It's interesting that you're starting off with this conversation about language is something
that I'm super excited about.
I don't spend nearly enough time studying it, but this conversation around the etymology
of words reminds me of a book that I read, the history of the English language that kind
of goes through.
The thing that I remember most vividly from that book is this concept of the Great Language
Shift, I don't know if something similar happened in the room, the, or sorry, the Great
vowel shift.
If something similar happened in the romance languages, but in English, there was this
period of time in which the vowels all shifted in the sounds that they made that made, and
it's apparent when you look at like Beowulf and other writing, and you see these words
that don't really make any sense until you realize that there was this vowel shift.
Anyway, I just...
Yeah.
Yeah, and one very interesting thing about English is that it basically has this duality
of vocabulary, because English is a Germanic language, but then it had a huge influence
of Latin through French, right?
And so you get these pairs, you know, what is the difference between freedom and liberty?
Well, it's really kind of the same, it's just, you know, freedom is a Germanic one and
liberty is the romance.
On the other hand, there's also these super fun pairs like pig and pork, or cow and beef,
where the animal, the word for the animal is the Germanic one, whereas the one for the,
for the, for the, for the dish or the cook thing is the, the Latin one.
Even the difference between kitchen and cuisine, right?
Look at that.
It's cuisine is just kitchen in French, right?
Yeah.
Yeah.
But so word is endlessly fast, languages are endlessly fascinating.
It's also not just the words, but also the constituents like in Japanese, you, you say
the girl, a book reads, right?
You put the verb at the end, which incidentally is also how Yoda talks.
So it just appears to kind of, it lends more gravitas if you at the end of the sentence,
the verb place.
But of course, so, but so the interesting is that all languages have nouns and verbs,
which is not surprising, because this is how we humans perceive the world.
There's sort of static objects, like a cup, and then there's actions.
You pour into the cup, or you drink from the cup, right?
But then the way in which you put these building blocks together is more, it's more arbitrary.
But of course, fundamentally, all languages have the same expressive power, it's just
that certain languages tend to put more emphasis in certain aspects, like another cool example
is in Chinese, you could not just say brother or sister, you have to specify whether it's
older brother or younger sister, right?
There's like four different words for sibling.
So an interest in kind of a political interest in language, drove an interest in languages broadly,
it sounds like, and is that right?
Right.
So you and to machine learning.
Yes.
So that's one aspect that've always been just, and I've tried to like learn a few languages,
et cetera, sort of can mumble my way through a few.
But then the other piece of technology, don't put me on the spot.
But basically, so Catalan and Spanish, German, English.
And then Chinese and Japanese, a little bit, I can know some basic stuff.
And then actually, other languages like French and Italian Portuguese, just through sort of
Romans, Osmosis, right, it's they're so close that you can basically understand.
Cool.
Yeah.
But then the other piece is sort of technology.
One day my dad brought home an HB85, one of the early home computers, a beautifully
design machine, right?
So the CRT monochrome screen, magnetic cartridge for storage, a thermal printer, this built
in keyboard.
And then some manuals that were very well done and taught you how to program at that time
that was in basic.
So that's how I sort of got hooked into coding and I ended up studying computer science as
an undergrad in Barcelona at the UPC, the University of Polytechnic at Catalonia, which
is I think now goes by Barcelona tech.
But when I finished, I realized that there was this sort of nascent field at the time called
computational linguistics, now better known as sort of maybe language technologies, which
is this attempt of getting computer systems to understand aspects of human language.
And then I thought, wow, that's absolutely amazing.
I can sort of merge, combine my two interests in languages and technology.
So after my undergrad in Barcelona, I did one year in Germany at the University of
Cascua and then I came to the U.S. for grad school.
I was at Carnegie Mellon for quite a few years because I first did a master's in computational
linguistics and then I stayed for a PhD in computer science and in language technologies.
Okay.
So I start working on things like speech recognition, machine translation.
Some of the early were calling sort of end to end speech translation, which was doing
the recognition and then a fair amount of minimum amount of understanding so that you could
sort of generate into a target language.
Something that now is more common, but 25 years ago was sort of a pioneering work with
Alex Vival, the Janus project and some other researchers.
Okay.
Nice.
Nice.
And I've got a note here that you went to CMU with Manuel Ovalosa.
We've talked obviously lots of folks on the past, of course.
We went on through CMU, but were you there at the same time?
Do you know one?
Yeah.
I had her as a teacher, as a professor, of course, yes.
Oh.
Yeah, yeah.
I think like some like just like ML or some, I remember an assignment that I did for
Manuel Ovalosa, which was to automatically create crossword puzzles, not to solve them
but to create them.
Okay.
That was one of the assignments I remember.
Yeah.
Awesome.
Cool.
So how long have you been at Square?
So at Square's, but it's three years now, I did some, I was, so I spent some time doing
like speech analytics for like call centers and then I moved more into kind of the startup
world.
It was with mine melt with Yic Yac.
And then three years ago, I took it with some other team from Yic Yac we joined Square.
Okay.
And you're particularly focused on commerce at Square.
I imagine there are lots of different ways that machine learning is used at the company,
but what is the, you know, earlier, a handful of core use cases for ML at commerce.
Yeah.
Yeah.
Yeah.
The cool thing about Square is that sort of machine learning is foundational to the company,
right?
Because we can be so open as a platform and allow for anybody to sign up to be a square
seller or even a square developer through our API is also because we have the mechanisms
in place to watch out for bad actors, right?
Anytime that you deal with money, you have to watch out for all kinds of fraud and risk.
And so to some extent, risk management is sort of the foundational piece of the application
of ML at Square, but then over the course of the years, because we're not just the people
know about the little white reader that converts a smartphone into a credit card or a point
of sale system, but actually we've expanded quite a lot the offerings in terms of products
and services.
You can run a small business just using Square with things like inventory management and
even team management, we can do payroll and taxes for your employees.
So that on the one hand, obviously, it's an excellent way of helping our sellers.
On the other, from a risk perspective, you also have to watch out for sort of new avenues,
new avenues for sort of malicious users.
I'll give you a cool example, which is we have a product called Square Marketing, right?
Which allows our sellers to have a conversation with their buyers through email campaigns.
And we provide some nicely designed templates so that they can show their latest products
or loyalty program, et cetera, except that in a couple of occasions, we saw that that
was being misused as phishing attempt, right?
So someone would craft what looked like was supposed to be an email marketing campaign
that actually contain some link to some nefarious phishing site.
So what happens there, sort of ML to the rescue, right?
So we quickly develop a classifier so that before an email gets sent, it gets inspected.
And then we look at what are some signals here that tells us that something is off.
And of course, you can think of, well, usually there's a button associated with that sort
of with a call to action, with a phishing attempt, so we can inspect the URL and compare
whether the domain of the URL matches the URL of the seller if they have a website.
If it doesn't, that's a bit of a red flag.
But interestingly, and going back to the language, just by looking at a couple of examples,
you can see that the language being used in the phishing attempts is very different from
the normal sort of marketing campaign.
In fact, it tends to be much more negative, right?
It says, problem with your account, go fix it.
And so interestingly, as I'm sure you know, there's this technique in natural language processing
called sentiment analysis, where you can take a bunch of text and basically map it onto
a single number, you know, minus one sad to plus one happy.
So the sentiment polarity is a signal that we now use for this classifier and not by
itself, but in conjunction with other features.
And of course, now, so I tell, I always say, you know, don't tell these to the bad guys,
right?
Otherwise, they're going to adapt and start crafting marketing campaigns, saying wonderful
opportunity.
We can press this button, so we can have that.
That's right.
Wouldn't you want to freshen up your password?
Yeah.
So, but so the overall point here is that anytime that you have a new product, anytime that
you have a new API, you obviously have to also think about, you know, how could these
be misused and how can we sort of protect against that?
So, manage degrees remains one of the sort of largest applications of ML at square.
Another one though, which is also very interesting is just being able to be better at automating
operations and being able to, for example, do more accurate marketing campaigns.
As our number of products grows, it's also important to be able to let a certain subset
of our sellers know about that new product, the ones that are going to be most interested
in in them, right?
So, a good example is a square appointments, square appointments, if you're a hairdresser,
and you don't want to be on the phone all the time, you know, are you open at three, can
I come in tomorrow?
You'd rather have a self-serve external portal where people can just a calendar where people
can book themselves, right?
So that's exactly what square appointments is, and actually you're offering these for
free for just for an initial location.
So we've had a lot of success, but then the next question is, among our millions of sellers,
who would benefit the most from square appointments, right?
Because we don't want to send like a mass email that people are going to consider spam
if they're not, if they're just a little kiosk, and they're not like booking doesn't
make sense for that particular seller.
So obviously we can do that by sort of what we call MCC Merchant Category Code, but also
there's some cool ways of kind of inspecting the items and services that are in our seller's
catalogs to understand whether there's certain things that are being booked by time, right?
Is there any place in the description of your item where you mentioned one hour or a half
an hour?
Well, that's a perfect candidate for square appointments.
So we're able to sort of use also a mail to have a much more targeted marketing campaigns.
And so that's going to put a million operations.
And then there's a third category, which you can think of driving sort of product features
something that is visible to the seller, right?
And so a couple of examples are we now are able to make suggestions in your catalog like
based on what type of business you are, or what type of items you already have in your
catalog.
We're able to sort of suggest something that other products that you may want to carry.
Or a simple thing is when you create a new item, let's say you're a bakery, which by
the way, it's interesting.
We can also know what type of business you are, obviously based on your name, right?
If you're Josephine's bakery, you're a bakery, also this is a true story.
If you're, what was the, oh yeah, rolling scones, you're also a bakery.
And then so we're able to based on your name and certainly based on the type of items
that you sell.
Have a good name.
Where is that business?
It's a real bakery somewhere.
I'm not sure.
I think maybe Seattle will have what can look it up.
So we know what type of business you are based also a little bit of sort of this sort of
behavior, not just the name, but also the items that you sell and the ticket sizes, etc.
But then when you create an item, let's say you're a bakery and you create a new item
called croissant, well, we know that there's going to be certain very common variations,
like plain, almond, chocolate, right?
And we actually, what, you don't like croissants?
No, I was thumbs down, plain, almond, meat, chocolate, yeah, it's a progression, yeah.
Yes.
And so we're able to auto suggest that because those are sort of, you know, we, because
we understand a bit of the domain, but in a way that is sort of fully automated.
So that's one example, suggestions in the catalog.
And then another very sort of new area where we're just starting with is with what we call
the square assistant.
This is the, the result of an acquisition we did last year of eloquent labs.
We now call it the conversations team.
And they recently released square assistant right now for appointments, which means that
when you, when a buyer gets a reminder of one of these square appointments, they can
now reply and say, oh, yes, I'll be there or oh, shoot, can I come in tomorrow instead?
And rain out so the assistant is able to respond on the seller's behalf about the appointments.
And so these are kind of a new area where I think there's a lot of potential.
So when you have a business, you know, for which ML is so fundamental, yeah, I'm thinking
about how you kind of manage the portfolio of projects and how you kind of organize
around that.
Very different from, you know, an enterprise that is, you know, has some other business
and is thinking about how can we, you know, take advantage of machine learning.
Let's spin up a project here, spin it, spin up a project there.
Right.
You know, here you've got, you know, I can imagine, you know, every, you know, every product,
every feature effort you're thinking about, you know, an engineer is thinking in the
back of their head is, you know, is my goal best served by machine learning or something
simpler, you know, and that might have you with a lot of, you know, places where you're
trying to have a lot of impact across the, the product portfolio.
How do you, well, yes, yes, I would say to that is it's certainly the case that it's
where I guess we're sort of enough, you know, sort of a lucky situation where, sort
of technology and ML was understood as a kind of a core component from the get go.
But for any other organization, I think there's some steps that anybody can take, which is
the first one is just kind of more awareness, right?
So provide some broad machine learning training so that people at least sort of understand
a little bit of the concepts of how do you have this.
So the key thing about machine learning is that it's something that learns from the data,
learns from experience, right?
And so what we do even at square is we have sort of a training that we call ML for everyone
where sort of we sort of teach this concept.
And then we follow that up with a brainstorming session, like now that we know a little bit
about ML, let's think about how could ML help your team or your customers and we come
up with specific examples, right?
And actually it's cool to also use the, you know, Andruang's sort of heuristic about
what ML can do, but he says any task that a human can do in about a second of thought,
about about one second of cognitive effort is something that we can probably automate,
right?
And of course, it's not a perfect heuristic, like, you know, chess playing, we would
already take way longer than a second to decide the next move.
But it gives us a little bit of some parameters.
And so that gives us a lot of ideas to our teams.
And then of course, you need to kind of aggregate and prioritize.
But that typically leads.
So a lot of these ideas are very good.
But then you realize that they depend on the availability of data.
So the third point is from an organizational perspective to treat data as a first class
citizen, meaning that it's not that you have some sort of analytics as an afterthought
of some, you know, kind of looking over and trying to, you know, look at some logs.
But rather half the data already have a, when you design a functionality, when you design
a product or an API already having mine, which data is this service going to consume?
And most importantly, which data the service is able to generate so that other services
can easily use that data, right?
So there's quite a bit of work in terms of standardizing how we produce that data.
And then also kind of the data infrastructure to have kind of a common repository of that
data and not just the raw data, but then also have you sort of aggregated into sort of
useful features, meaning, for example, in terms of transactions, you don't have to some,
for in some cases, you don't need to know every single transaction about a seller.
You just want to know what their average weekly, you know, processing volume is, right?
And so you aggregate that into a signal and you can put that into what we call a feature
store, then can be used by other teams and other, even other ML models.
So once you have these sort of data as a first-class citizen, then it's much easier to start
sort of weaving machine learning into your kind of products and processes by making use
of these data.
And then all of these, obviously, also, there's all these talk about, and necessarily talk
about sort of ethical AIML principles and also being aware of regulations like GDPR in
the European Union or CCPA in California, that you want to make sure that these systems
sort of are, the technical term is, you need to show that they don't have a so-called
disparate impact on protected classes, right?
But it basically means that they're sort of neutral and there's a side where, so when
you create a certain model, there's certain features that you don't want to look at,
at the same time, for a compliance perspective, you still need to know what those features
are to show that there's no disparate impact.
But that's done by different teams, okay?
So if you do that, so provide broad ML training, brainstorming about what ML can do for you,
treat data for first-class citizen, then you can start doing some cool ML driven.
Initially, it could just be some smart defaults, personalization, and then over time, you
can have almost entire divisions, like Square Capital, which is sort of the armament within
Square where we facilitate loans to our sellers.
You can basically argue that that's pretty much all driven by ML, because, in fact, Square
Capital is a great example where we can really show that the over-arching purpose at Square
we call it economic empowerment, and that means making it easier for everyone to be able
to participate in the economy and to have very clear understanding of what the processing
fees are, exactly what is expected, and there's no hidden fees or issues like that.
And so one of the good examples of this is how Square Capital just looks at how a seller
is doing within our platform.
We don't necessarily need to go out and check your FICO score or your history outside.
We just look at how you've been doing within Square, and that has allowed us to basically
facilitate these loans to many sellers to expand their business.
On this idea of data as a first-class citizen, does that automatically mean just kind of
save everything, or are you still needing to be very selective as to what you save, how
to obtain it, compliant applications, how do you balance those operations?
There's a good point, I mean, your initial instinct might be, well, let's just save everything,
but that quickly becomes unmanageable.
If you look at the log files generated by every single service, and we have hundreds
if not thousands of services, so even there's some operational logs and stuff that you
just retain for a little bit.
But then there's the important piece, more of the semantics of this user logged in, and
they have these transactions.
That is certainly recorded.
So there is a little bit of certainly sort of filtering of the important actions, and
then there's also a lot of what I was mentioning before of converting these raw data into
more useful aggregates.
I'm so curious about given all of the places that you are trying to make an impact at Square,
how does your particular team, you run an ML organization as opposed to engineering,
a particular feature, or I'm trying to get at the relationship between your organization
and the product teams, and are you kind of embedding people, are you kind of a centralized
organization that takes on ownership of ML capabilities that get plugged into other
features.
How does all that work?
Yeah, it's a good question because it's really a bit of a fluid situation.
So, a couple of things there, first of all, Square is very much a product oriented, and
so ultimately what really drives the adoption of machine learning is how well it can serve
the ultimate function of overall its economic empowerment, make the life of our sellers easier,
and create sort of in the parlance of these remarkable or delightful experiences that
really make it sort of almost fun to use, you know, a Square software.
So that is the ultimate sort of driver, and so we see ML as an enabler to that, and the
question about how exactly do you do that, so they're supposed sort of kind of vertical
and horizontal in the sense that first of all, we do want every engineer at Square to
be at least aware of machine learning, right?
In the same way that you want every engineer to be infosec conscious, right, aware of information
security issues, and when you develop an API, how do you check that you're not being hacked?
We also encourage all the engineers to get more familiar with what ML is capable of.
Now we do also have teams that are sort of highly skilled data scientists and ML engineers,
and they can sort of drive more of that, but typically, so we have some teams that are
almost like all data science ML, but then the other idea that they were increasingly
doing is kind of in the same way that you in order to have a full stack team, you need
backend and you need front end, well, you will also add some like ML sort of expert
or some engineer that has more interest in ML to kind of balance the team.
Is there a typical project or a way that you engage with these product groups to help
deliver new ML capability?
So yes, typically there's an assessment of kind of the backlog, like what all the features
are already sort of planning to do, but then we also do these brainstorming sessions
to kind of bubble up some new ideas, and then of course there's this interesting sort
of assessment of the idea in terms of, well, what is the expected impact, but also how
feasible is it from a ML perspective, right?
And also do we have enough data for the model to be able to make a good decision.
And so if we go through a bit of that, and then we kind of select the sort of the most
promising projects, and then we typically, what we, a good model that we have found is
that the ML team is responsible for kind of the backend, kind of computing these suggestions
or being able to respond to all these different messages from buyers.
And then the product team that owns that sort of the product surface that we're talking
about kind of ends up owning the feature, but basically they make calls to the backend
that is managed by an ML team.
Yeah, one of the issues that I think that that kind of relationship raises is that there's
so much crossover in terms of the way machine learning originated information is surface
to users, and do you express a degree of certainty on a probabilistic determination?
And if so, how, you know, kinds of design issues that come into there, you know, something
that you think a lot about there, or as you approach that.
Right.
Yeah, yeah.
As I mentioned, so fundamental to how we approach the development of an ML driven feature
is how is it going to look like?
What's the experience going to be both when the model is accurate and most importantly,
what happens when the model fails, right?
And failing, there's ways of failing failing because there's no suggestion that is above
a certain confidence threshold, in which case there are like, there's no suggestion.
That's one situation and then the other one, which is obviously worse, is what happens
when the suggestion is incorrect, right?
And there's different degrees of that.
So for example, in the case of item suggestion, you know, what is the worst that could happen?
Well, you know, you're a bakery, and instead of suggesting a croissant, we suggest sock,
right?
Well, a little bit odd, but not the end of the world, right?
So in that case, we can be a bit more lenient.
On the other hand, for something like, you know, the credit, you know, issuing a loan or
not, we have to be quite accurate in terms of modeling precision versus recall and the
cost of a false positive, for example, a seller that would not pay us back versus a false
negative, a seller that would have benefited, but we didn't issue a loan.
And so ultimately, it's kind of a business decision, but it's highly informed by the model
and being able to kind of set the correct operating point.
To your point, it's certainly something that we have sort of front and center is the design
of the feature and being able to make sure that that overall the product and the feature
is still useful even when the model makes a mistake.
And so that could go a little bit about sort of the sort of these four aspects that we
think about when designing an ML driven feature, certainly the design and the actual UI and
the different sort of experience.
So the other one is the modeling, right?
So how can we make the model more accurate?
How do we make sure that it actually improves over time?
And that typically means that we also have kind of a product analytics layer that kind of
looks at how the adoption of that feature and how often those suggestions get accepted
or not, and that feeds back into the model.
And then the other piece is sort of engineering, right?
Sometimes you may have a very complex model, but then when you need to run it, not for
one user, but for millions of users, then you suddenly realize that maybe you need something
a little bit simpler, so that it's actually computationally tractable.
I'm curious when you kind of look across your portfolio of models, what is the technology
mix? The square is obviously working with a lot of kind of traditional tabular data.
Does that mean you're attending a more traditional techniques, or do you have a significant deep
learning footprint?
How do you think about technology landscape?
Yeah.
Yeah.
Well, so it's interesting because on a per-task basis, you always want to start with a simplest
model possible, right?
It's sort of very alluring to say, oh, we're going to move everything to deep learning.
But in fact, if taking a weighted average of what happened in the last two weeks gives
you a good sense, you don't need a super complex model.
And certainly when we started, now the company is 11 years old, there wasn't even deep
learning at the time.
So certainly some of the models are simpler, like the XG boost, et cetera, et cetera.
But over time, now we also have some very sophisticated models that do use things like RNNs and
CNNs.
But we always sort of compare and contrast kind of making sure that the additional complexity
is worth it and that the accuracy actually goes up.
Are there any particular examples of places where you found that the additional complexity
was justified because the problem was that interesting or complex or nuanced?
Yeah.
So I'll give you a relatively simple example.
But going back to these item suggestions, we basically, rather than trying to do some
simple, I don't know, like TFI, DF, like work frequencies and stuff, we just trained
our own embeddings, right?
So we applied something similar to Word2Vec in the description of the item.
So we have, usually an item has a category like pastry, the item is the croissant and
then the variations are in your favorite order, chocolate, almond, plain, although some
people call it butter instead of plain, to make it less plain, they call it butter.
And so you put all of these millions of descriptions together and you train a Word2Vec, which is
basically you learn these vector representations in this hyper-dimensional space that is very
sort of semantically true to the semantics.
So interestingly, when we do that, we realize that even though, say, small and large are
kind of opposites of each other in general English, within the context of these item catalogs,
small and large are actually semantically very close and literally they are geometrically
close in this hyper-dimensional vector space.
And that is because in this case, both small and large encode a value for the attribute
size, right, which can be applied to a coffee or to a t-shirt.
And so this is how we kind of approach some of more sophisticated models, but we kind
of make sure that it works.
And also, one thing to add is we recently, so Square recently acquired Desa from Toronto,
where they have a lot of deep learning expertise.
In fact, they were famous for some cool sort of deep fakes.
And so we have a lot of ideas about how to apply some of more of these sophisticated
techniques to both the Square Cellar, POS, and also to Square Cash, the Cash app.
What kind of tooling and technology platforms have you established to allow your data scientists
to move more quickly, be more agile, get models and a production more repeatedly?
Yeah.
This is also evolving.
As you can imagine in the beginning, it's, and also because Square is fairly decentralized,
we have sort of different teams exploring different solutions.
But it's also a combination of, we have our own data centers, but we also use quite a
bit of cloud, both AWS, GCP.
And so, you could, roughly speaking, we do a lot of the training and model development
in the cloud.
But when it comes time to serve, we still serve from our data centers, not, you know,
not an absolute, but as, you know, kind of overall.
And then in terms of making it easy for internally to write.
That proximity to, to feature data or some other factor that has you as that split.
Yeah, also kind of historical reason.
And like we, we do have for some of the core services, we do want to maintain sort of
full control.
We have a good record of sort of availability and we want to maintain that.
And then we have kind of a platform team that is developing some things like, you know,
hosted Python notebooks and some feature store and some data infrastructure that is sort
of used across all the teams.
And then each team also has a, but so each team also has some sort of the freedom to use
slightly different, you know, text acts, depending on their preferences.
But we do have a very sort of active internal sort of community where we get together every
week and we sort of share the different projects across the different teams and units so that
people are aware of sort of best practices and we can collaborate.
Are the folks in your, or primarily engineers or data scientists or a mix?
Yeah, it's a mix because at the end of the day, we're still shipping features, right?
And so we can partner with product teams or to some extent, we now also have ML heavy
teams that end up being full stack because that's also made the faster path to productizing
a feature.
A good example is this conversations team, right, that is responsible for score assistant.
They are extremely sort of ML AI heavy, but now they also have, you know, front and
engineers to help them get the product out faster.
And so when you think about that spectrum of, you know, ML heaviness or readiness and kind
of a role or an organization that's focused on ML, you know, that's ML in the, in the
name, like head of ML, you know, being your title, like is ultimately like, do you think
that your role is to put yourself out of a job by making the product teams, you know,
self-sufficient that, you know, it's, ML is kind of a corporate capability.
And if so, like, you know, what's the timeline on that or do you, do you think that standalone
ML organizations are kind of long-term, you know, have a long-term sustainable role?
And if so, kind of what do you think that is?
Right.
Yeah, I do like a lot the idea of basically increasing the overall skill set of, of engineers and
product managers and designers about ML. And so ultimately, that is my goal.
That doesn't mean though that obviously there's going to be always a core that is sort of
more following the latest developments and doing some research of their own.
But I think that's certainly to make a product feature successful, you could not have too
much of a separation between ML and product, right?
It has to be more embedded.
I mean, it's certainly been amazing to see, you know, how quickly kind of innovation jumps
from pure research academia into commercial environments.
And, you know, your typical kind of, yeah, I guess just comparing it to, you know, other
kind of technology waves that, you know, we've seen like mobile and cloud and these other
things like they didn't necessarily require people to read papers in order to, you know,
have an impact, whereas, you know, with machine learning, you know, I'm, you know, when
I'm talking to people doing kind of practical things to push a business forward, that often
involves, you know, being under being aware of kind of the latest developments and research
and using those to push the kick the ball forward, push the needle forward, whatever
the right analogy is.
Do you see that well?
Yes.
So, to some extent, I mean, obviously there, it's amazing to have people that are super excited
and following the latest research and we do have, you know, paper reading groups that some
of us participating, having said that, I would actually argue that there is also beginning
to be quite a strong sort of a democratization process of ML.
And if you look at what the, you know, the usual suspects offer in terms of ML capabilities,
they started with some basic, oh, we can do model hosting for you, right?
But now it's not just model hosting, but basically there's some like ready-made models that
you don't even need to like train yourself or, or, or, or, or, you don't, you don't even
need to have almost like a training set, you can use sort of off the shelf things for
say recognizing the, the license plates, right, or, or analyzing driver's license.
So to some extent, it's also going to be much easier in the future to use some of these
models and it's almost just going to put these building blocks together, right?
Have teams that square built products on third party, you know, AI as a service types
of models?
So yeah, typically not, typically, because we are actually very protective of our data,
right?
So we typically like do our own models, although, I mean, for certain, you know, explorations
outside of the core business, certainly, we, you know, we, we, we kind of play with have
some prototypes with some external models.
And I'm interested, I have more questions about kind of the, you know, all the stuff
that we talked about the relationship between your group and others and kind of the tooling
and platform.
I mean, maybe a good place to, to go to start to wind down is, you know, just in terms
of the things you've learned, kind of building and, you know, what was, was the team small
when you arrived or did you inherit a team even or did you build it up from scratch?
Yeah.
Well, this specific team, the Commerce and Mail, I, I, yeah, I, that, that one, I sort
of, I started as a, you know, as a, as a single person and then, and then sort of growing,
growing over time.
I think the most interesting lesson is, um, there's certainly a lot of interest in how
ML can improve the, you know, the, the, the day to day of both internal teams and, and,
um, and our sellers and, and customers.
Um, but of course, the, the fun part is kind of identifying which projects are going to
be the most relevant and be able to do some quick prototyping to kind of either validate
or realize that that may not be the, a good path forward.
So being able to kind of do some quick iterations is, is important.
And the other thing was, as in any organization, there's always ways to improve the data quality,
right?
So, so not just the kind of the, the infrastructure and the pipelines, but also, uh, the type
of data that is, uh, that is locked and, and how it is locked so that, um, other teams basically
increase the, the, the clarity of the semantics about, uh, what that data is, um, meaning these
team call this field, uh, you know, item description, um, by what do you mean by item description
because there's, there's the category, there's the variation, does it include the price?
Um, so different people may have different assumptions about what that field means.
Um, and so also being able to have a, kind of a consistent semantics across services and
teams, um, this is, uh, you know, I'm not saying that we've solved it, but this is a, sort
of an ongoing and something that we've made a huge, uh, advances on.
Mm-hmm.
Yeah, you mentioned, uh, kind of being thoughtful about the types of projects that you go after.
One of the, the often recurring themes that, uh, has come up when I've talked to folks
that are kind of in this, uh, head of ML type of role is kind of balancing the practicality
of your portfolio, but also kind of having some moonshot aspects of it that if slash one
achieve significantly can, you know, make a big dent and, you know, can be kind of game
changes. Is that, do you, do you think about it similarly? And you kind of get your portfolio
in that way?
Yeah.
Yeah.
Um, we do want to have a mix of more sort of short-term, uh, quote unquote realistic,
uh, projects, you know, something that we feel is very doable.
It's just going to take, you know, one quarter or two quarters or three quarters.
But then there's also the kind of a longer horizon of, um, more sort of, yeah, one shot
as, as you called it, um, or just explorations of features that would be extremely cool
if we could do these or that.
So what would typically what we do is, um, almost like the, the, the old famous, you know,
the Google 8020.
So we, we do have some, uh, hack weeks and we have some people devoting a sort of amount
of their time to more of these, uh, long-term, uh, projects.
Um, and it's, it's, they also need a bit of a, of a balance, um, um, and so, but so,
yeah, typically the way I organize it is making sure that in any given quarter, we have
some work done for some of these more long-term, um, projects and some of it is just sort
of researching new technologies, um, doing some, uh, sort of just, just kind of mock ups
of how things could be, if only we had this, so if we, we only, only had that technology.
Are the long-term plays as equally product-driven as the, the short-term or are they, you
know, did they come from the product teams like each, each of the product teams has their
long-term vision?
Or are they, you know, more driven by the opportunity created by the technology and, you
know, yeah.
But almost say it's, it's more like the, the latter.
Like so, because the product teams already have a, a well-defined backlog and, and some
of it is already very sort of forward-looking.
And so the thought experiment is more about even in, you know, like a, the, you know, three
to five years, not even, you know, one to two years, but like three to five, what, where
could we be, um, what are the, the general trends in society and in technology?
I mean, frankly, like, you know, the whole COVID situation and working from home, that seems
to, you know, it will, it will stay here for quite a while.
So how does society change because of that and, and how can we sort of position square
to help our sellers in that environment, um, and also in general, what, you know, what
kind of, um, new forms of almost like human behavior are going to arise from these
new circumstances? Hmm. Very cool, very cool. Well, Marcel, thanks so much for taking
the time to chat. If I do decide to start the Sam, you know, oh, yeah, the linguistics.
Yeah. Yeah. Yeah. Yeah. I can tell you some more anecdotes about, about, uh, funny languages
like, uh, Google Imuthir, where you could not say I'm standing in front of Sam. I would
have to say I'm standing northwest of Sam or something. Anyway, of course, this is all
changed by, by remote technologies too. But, uh, yeah, but it's been a pleasure. Um, and
I thank you for having me on your, on your podcast, Sam. All right. Thanks so much, Marcel.
Take care. All right, everyone. That's our show for today. For more information on today's
show, visit twomolai.com slash shows. As always, thanks so much for listening and catch
you next time.
