All right, everyone. I am here with Samir Singh. Samir is an assistant professor in the
Department of Computer Science at UC Irvine. Samir, welcome to the Twimal AI podcast.
Thank you, Simon. Excited to be here. I'm excited to chat with you. Why don't we get started
by having you share a little bit about your background? What got you started working
in machine learning? So yeah, I've been sort of thinking about AI as a goal for a long time,
you know, since teenage years reading all these science fiction books and wanting to solve AI.
And in my naivety, I thought the solution was to build robots initially. And so I didn't
undergrad in electrical engineering, trying to do hardware sort of things, build manipulators,
and sort of small vehicles and things like that. And then when I started reading research
papers, I kind of realized that at least at that time, people were spending their whole PhDs
just trying to grasp like a cup of something, right? And that didn't seem like AI to me.
So I quickly sort of shifted more towards the software side of things. And at that time,
again, a lot of AI was doing a star search and things like that, which seemed a lot more cooler.
But didn't seem to quite get at the learning or the intelligence aspect of it. So slowly,
I made my way and found machine learning and you know, been working in machine learning for a long
time now. And within machine learning, I think NLP ended up being this sort of really attractive
application for me. It seemed a good mixture of being very, very practical, especially with internet
and everybody posting stuff online, but also addressing some of the fundamental things that excited
me about AI being able to communicate, being able to understand humans and things like that. So
that's kind of how I made my way into doing machine learning.
Nice. Nice. There's some out there that think that embodied applications like robotics are
really the only way we're going to get close to to AGI. You know, that may be only the
roboticist that think that though. Well, what you're taking on that is that's, you know, that was
the goal that you started out with. Yeah, I think embodied AI is such a like concrete thing that
I feel like that there is something very attractive about that. But you know, everybody's struggling
with what is AI? How do we define it and things like that? I think of learning being a key component
and learning doesn't have to be grounded necessarily. And if something is learning from internet and
from YouTube videos, audio streams, that might be sufficiently intelligent, right? Like who knows?
Our kids right now really grounded or are they completely in the internet? And you know,
it's difficult to say even us as humans how grounded we are going to be in the future.
In the matrix, are we is anyone grounded? Exactly. So tell us a little bit about your research focus.
What are some of the areas that you're spending your time on nowadays?
Yeah, so for a long time, I was doing an LP, sort of a lot of different tasks in an LP. And this is
sort of, you know, about, I guess I want to say eight to 10 years ago, where I was so focused on
the task that I was working on that sort of the deep learning stuff kind of went by me. And I was
working on sort of more graphical models and log linear kind of stuff. And by the time I started
doing deep learning things, it was really, really good at everything I was doing. But it wasn't using
any of the cool stuff that I was working on. So that made me feel like, okay, why is it doing what
it is doing? Whether it is, it is actually doing what it is doing. So you could say it was almost
by spite that my research agenda got started. But you know, sort of playing around with these
models, you know, I did my postdoc at University of Washington with Carlos. And with one of his
students at the time, we started looking at, okay, these deep learning models really seem to be
quite good. But we have very little idea of what's going on in them. And so maybe let's try to
find a way to get to some place that we are actually familiar with. So this took me back to my
sort of linear classification or log linear model days where I used to spend a lot of time
looking at feature weights and trying to figure out, okay, is there a problem here? Can I fix
something? Or in my worst days, can I even change some of these weights to try and get it to do
something? And I think, but since none of that was possible in deep learning, we sort of started
thinking about how we could try to get into the internals of deep learning and at least provide
some intuition as to what's going on. And that's sort of where I started working on explainability.
And we came up with this line algorithm in order to explain black box models. And since then,
I think, you know, explainability has been really interesting. But initially we were hoping for
explainability to tell us why deep learning models are working. But often they ended up telling us
why the deep learning models were not working. And so my tragically sort of has become a lot more
on looking at why are these explanation methods telling us that they are not working when all
of these leaderboards and New York Times articles are telling us that they are working. There seems
to be this big mismatch. And so why does this mismatch exist? And what can we do about it?
So in the recent couple of years, I've been thinking a lot more about evaluation,
about debugging and things of that nature for machine learning.
Nice. Nice. Yeah. I'll say that the line paper, which goes back four or five years,
well, first of all, as you noted, we're still struggling with the same problem with deep learning
generally. But the paper itself and that method itself seems to have had pretty strong staying power.
Like people are still talking about it. People are still using it. By the time this show comes out,
we will have had our recent event model explainability forum where we dig into
a lot of the current issues in model explainability. But when people are evaluating explainability methods,
Lyme is one that still comes up. Any thoughts or reflections on that work? Did you expect it to be
as important as it has turned out to be when you were working on it? That's a good question.
I guess you hope that every paper you're working on, I would hope more people think that every paper
they're working on is actually going to be world changing and get people excited. But it usually
doesn't happen with Lyme, of course, people got excited about it a lot more than we anticipated.
And there are both good and the bads of it. We are close to Lyme, so we know what it's doing in a way
that we know what it's capable of and what it's not capable of. And we've been more recently
with the collaboration with Hema, who I think you had on your podcast a few weeks ago. We've also
been working on going back and looking at Lyme and saying, okay, can it be manipulated if they were
like bad agents and stuff? And we show that it's actually not that difficult to do that.
And I think just like machine learning is considered to be this thing that will come and solve
all your problems. Lyme seems to be well. Whatever problems machine learning has,
now Lyme is going to solve it for you and it's not like that right now. It's one of the
explainability techniques. It is a useful tool, I believe, especially for evaluation, but it's not
something that has solved the model explainability problem. And if anything, it initiated the discussion,
and I'm glad that people are talking about it because it's a really, really difficult problem,
even to define what explainability is, much less solved with in a single paper.
And so that paper set the stage for the paper we'll be spending some time talking about today,
and that is beyond accuracy, behavioral testing of NLP models with checklists, which was the
ACL 2020 best paper winner. Congratulations. Thank you. Tell us a little bit about that paper and how
the some of your prior research led you to that problem. And so I think a lot of my research,
especially in collaboration with Marco, has been sort of looking at this mismatch between what
accuracy seems to suggest and what we know models are actually capable of. And accuracy is
something that's very quantitative. It's on the leaderboard out there. You can say 10% better
or worse or whatever it is. But on the other side, what models are good at and what models are
bad at is not something that's quantified or formalized or even well-defined as far as the
community is concerned. So what we wanted to do was try to look at this mismatch and say,
okay, there should be some way of thinking about this that goes beyond just creating another leader
board or introducing a different data set. So last couple of years, we this work started maybe two
years ago with this work we did called Sears. Where the idea was very simple, what we're going to do
is we're going to think of doing a serial attacks on models, but instead of doing this adding some
noise that people don't understand, we're going to take the original instance and we're just going
to paraphrase it. So it should be exactly the same sentence in terms of its meaning, but the actual
form might be different. And we had an automated tool for doing this building upon a lot of other
people who've been working on back translation and things like that. And paraphrasing was what you
called the adversarial attack in this case? Yes, because it was a paraphrase that was getting the
model to change its prediction. Got it, right? And so, you know, like, why did China take some action?
Or let me think of a different. So what is that thing on the table versus what's that thing on the
table? This is like a very simple version of a paraphrase. Of course, this means the same to us,
but at that time, there were models that got, like, gave a different prediction for these two
instances. So that was one way to start characterizing what the problems were in the models.
We continue this work. So we can call this sort of robustness to paraphrases, I guess. We had
another follow-up work that was looking at consistency. So if you had, is there a bird in the image?
Or let's say how many birds are in this image? The model might say two. And then we say,
are there two birds in this image? The model should clearly say yes, but the model did not often.
And what that meant was model was not even being consistent with its own sort of what things
are going on. And so that also sort of started thinking as about how can we characterize what the
pitfalls in these models are. And that all sort of culminates in this work in checklist,
where we kind of create a tool to allow users to think a lot more about testing in a structured way,
essentially. Okay. Yeah. Identifying the pitfalls or possible failure modes is
fairly different from identifying the causes of those pitfalls. How well do we understand
for the various models that you're looking at what's actually causing the problems?
Very little, I think. So you would hope that, so just to sort of clarify what checklist does is it
creates essentially helps you create a bunch of tests for your model. And when the tests fail,
similar to traditional software engineering, sometimes the test fails and you're like,
oh, I forgot to use that variable instead. I use some other variable. But sometimes it fails and
you're like, oh, wait, the whole my whole piece of code, the way I was structuring it is completely
wrong. Right. And so even in machine learning, when something fails, either it could be some artifact
of the training data, or it could be, it could lead to like a string of five research papers that
eventually solve their problem, hopefully. Right. So, so for example, fairness is a good example
of that. Right. If your model is being unfair, there are no easy. I'm going to replace all
males with females or something like that and solve the problem. That doesn't do it. But
checklist focuses more on at least do you know if this is a problem or not? And can you get an idea
of how much this is a problem? Okay. So, you describe checklist as analogous to kind of behavioral
testing in software engineering, you know, kind of riff on that analogy a little bit. Yeah. So,
that's actually where we started. We were like, people have been building all these complex
software systems. Machine learning is not the first one to come up and start predicting the
sentiment. There are way other more complicated software that do much more complicated things.
How do they approach testing? And this is something that that all of us know and we wanted to see
how many of those lessons can be taken and applied to machine learning. So, the idea was we we create
a bunch of different kinds of tests and I can sort of walk you through what they look like.
But the easiest one to understand is something we call minimal functionality text at minimum functionality
test where it basically looks like a unit test in software engineering, right? So, I'm not going to
think too much about what the everything that the model is supposed to be doing, but I'm going to
pick a single phenomena. So, say we are trying to do sentiment analysis and we just want a very simple
like does it understand negation or not, right? If I say I this movie is not good, is my sentiment
classifier able to do that or not? Right. So, that's a very simple unit test. That by itself,
if the model does it or not can be useful, but we sort of have this whole templating engine that
says I can say something like this blank is not blank where we fill blanks with the first
blank with a bunch of nouns and the second blank with a bunch of positive adjectives and negative
adjectives and then we have an expectation over whether the model will predict positive or
negative review for it. And so, in this case, we create many test cases as they would say in software
engineering and we test a machine learning model by just running the model output through these
things. And what this does is it hasn't told you like does this model, if you pass all the test,
you don't know whether the model has definitely learned how to do negation or not, but if it does
fail most of this test, it would be a red flag and you would say okay, the model doesn't even
understand such a simple negation, it probably doesn't understand negation. So, based on this intuition,
we created a bunch of different kinds of tests and I can walk you through those a few things.
Yeah, before we do that though, you gave sentiment analysis as an example, what is the scope of
checklist? For example, we spent a lot of time talking about language models nowadays and
transformers and things like that, does it address those kinds of tasks as well or is it limited to
classification? What are the boundaries of this work? Yeah, so I think in the paper itself,
we had a bunch of tasks that go beyond sentiment analysis or just simple classification, I guess.
And just to sort of motivate why we picked sentiment analysis, this was also one of the system
that one of the tasks that research papers are constantly looking at and trying to do better on
and by some metrics, we are better than humans on sentiment analysis, which may not be a surprise
to many people. But also, it was one of those tasks where there were a lot of commercial products.
So, like Microsoft and Google and Amazon make a ready to purchase sentiment analyzer.
And what we wanted to do was apply checklist, not just to some research models that we had lying
around, which were all transformable based like Word and Roberta, but also apply checklist to
these commercial systems to see what they were lacking in and sort of be able to compare across
research and industrial models. But in the paper, we also did things like question answering,
where you're given a question, you have to come up with an answer, given a paragraph,
we did a paraphrase detection. So, there's a Korak question pair dataset where you have pairs of
questions and you want to detect whether their paraphrases are not. We didn't directly do language
modeling, but that's something that we've been working on. And it's a little bit trickier to
define what even a test is for language models, but I think there are ways to use checklist for that as
well. Okay. Okay. So, walk us through the kind of the array of tests. Yeah. So, the idea here is that
you want to figure out what are the capabilities of the model. So, we conceptualize it as a matrix
where there are a bunch of rows, where the rows are sort of what you want to test about the model.
Right. So, suppose simple negation is something that you might want to test, and the easiest way
to test that is to create a bunch of minimum functionality tests, like unit tests. For some other
ones, we wanted to test things like robustness to location names. Right. So, my flight to US was great.
I want to see how much, whether the model is robust to changes in the country name. If I replace US
with a different country, or if it's a city, then replace it with a different city, the sentiment
doesn't change, but other models robust to that or not. That doesn't quite fit into the unit test.
So, we had a second category of tests, which we call invariance tests, where we are,
and this is also something that's similar in software engineering, where we are essentially mutating
the input or perturbing the input, in ways that we know shouldn't affect the sentiment.
In this case, replacing US with China or any other country, and then trying to see how often
does that change the actual output of the model. And this was one of the ones which was quite
surprising to us, because many times when you change the location, the output of the model
changes, which was surprising and disappointing. Some of the other kind of capabilities you might
want to check was, you're like, okay, I don't care what the review was, but if I explicitly add a
strongly negative statement to it. So, if I say blah, blah, blah review ends, I really hated it
at the end. You would hope that the model will not become more positive towards this review,
right? Now, this is also not invariance, because in case, in this case, you're changing the input,
but you're trying to test the directionality of the output. So, this is more of a directional
test. And we just want to make a test where you've got, you use a similar kind of strong,
definitive statement, and you want to see if the model even picks up on that, even if there's
other things that are more ambiguous or even positive. Is there, do you have a test kind of
blown those on? Not really, because it's always like, we wanted to define tests where we wanted
it was a failure rate of 0. We didn't want something where 80% would be still, sorry, 20% failure
would still be okay, right? So, with these strong statements, you can never be sure whether
they contradict enough with the review text, right? So, in this case, we wanted to keep it simple,
right? Like, we wanted to say at least this thing, the model should be able to get 0% failure on.
Right? So, it shouldn't become more positive is a very simple statement. That's why it was a
surprise to us, where many times it did like more than a third of the time, it just became more
positive when you added phrases like that, right? And these are some of these word sort of commercial
systems as well. So, yeah, so these kind of tests are in some sense, the way we describe it is
like that, I guess mathematically more of a necessary condition for a model to get deployed,
just because you get 0% failure doesn't mean that the model is safe to deploy, just because
it's similar to software testing, right? Just because all your tests pass doesn't mean you don't
have any more work to do. You probably either need to write more tests or write more code and
it's right, but at least if a test fails, there is a red flag and you know exactly what the model
is not able to do. And just by breaking these things down, I think that's the main contribution
of checklist is making, hopefully making people think a little bit more about these different
dimensions of the problem than just the single number. Got it. Is there an analogy to code coverage
in these types of tests? That's a good question. There has been some work that tries to do these
things where they, the analogy to code coverage is to look at the neurons inside the transformer
or whatever and try to make sure there are enough inputs in your test case or whatever,
at the same time, whatever you're planning to do that go through all the neurons at some point.
We haven't looked at things like that. Yeah, we are sort of going back to like it almost like a
black box model way of thinking about this, right? You are someone who cares about sentiment analysis
or you care about whatever you're trying to use machine learning for and presumably with that,
you have a set of capabilities that you expect someone who's claiming it can do sentiment analysis
should be able to do, right? So we are sort of, I guess, delegating that to the users,
rather than thinking about the internals of the model. And so from a practical perspective,
if I read the paper, download the code, are you suggesting that this is a kind of point
to set your model and run it and it's a full, it kind of throws the book at your model and tells
you where it's weak or do I have to adapt checklist with my particular model in mind and the things
that maybe the things I'm concerned about or how engaged as a user have to be in taking advantage of
this. Yeah, so that's a good question and we've been thinking a lot about this. And I guess the answer
is it's somewhere in between depends specifically on what you're working on. So I guess the easiest one
would be if you are creating a sentiment analysis system in your company, then it's trivial. You can
just download our code and run it. If you are doing one of the tasks that are not already supported
in checklist, then it's pretty easy and we've tried to like Mark was written really nice documentation
and sort of walk through it. It's pretty easy to get started and start thinking about okay,
what are the capabilities for this specific task and how do I write it down in a way that we can
generate a lot of tests. And checklist is really, really useful for use cases like that. I would say
we did some experiment also to sort of make sure that this is the case and we can talk about that
a little later. But what we also want to try to do and we are sort of getting there slowly
is to make it incredibly easy for people to contribute the tests or for us to keep
growing the set of tests that are available in checklist. So if you're doing a task that is either
slightly adjacent to what we already have. For example, you're doing paraphrasing but not question
paraphrasing, then you may have additional tests and we make it really easy to include it back in
checklist. We also want to make it really easy to evaluate any new model that comes out. So
we want to, for example, if hugging phase has a new transformers model, we want to make it
incredibly easy to also test and generate sort of almost a report card of okay. On negation,
we got this much failure rate on something else, we got this much and so on. Got it. Yeah. The
task that the user has to do to adapt it to their model is it writing tests or writing some kind
of meta test or test recipe that checklist then uses to generate a bunch of tests. I kind of heard
both in your description. Yeah, so it kind of supports both, but I think the way we've been
approaching this is for the first part, you just have to think a lot. So you have to think about
what are the different capabilities and checklist can not quite help you with that. Maybe it helps a
little bit by some of the automated exploration tools, but the idea is so we can take any specific
safe fairness or something like that. I want my model to be fair. Well, how do we answer something
like this and say I'm doing question answering, right? So okay, how can I test whether the model is
being fair and let's be more specific? Let's say gender fairness, right? So if I think the model
is not fair, one way to test whether it's fair or not is to come up with a really simple
context for the question answering system where we can say John is not a doctor, but Mary is
and then ask the question, who is the doctor, right? This thing is something we can easily write out
and we know the answer should be Mary because it's obvious from this sentence and that's a test,
right? Now we can say, well, if the model fails or not, it could be dependent on the word choice
of John or Mary. So I might maybe I want to replace John or Mary with any other names, right? So we
John with any other male name, Mary with any other female name and checklist has some of these
lexicon built in. So you can easily just say, okay, I'm going to create a template that says
male first name is not a doctor, but female first name is, who is the doctor, female first name,
right? That's one level of templating. Then you can say, well, why should it be just doctor?
Maybe there are other professions that we also want to test. So we have a tool that just you
can hide doctor and it suggests a bunch of professions and you can say, okay, I want to
say pick a bunch of these as part of the test and by doing all of these things, you can essentially
create thousands of use cases automatically, even just for the single statement that I talked about
and then just quickly test it whether the model your model is actually passing them with a sufficient
tolerance or not. Is there, do you envision a way to use checklist or maybe some future
version or evolution of this work? You're not just to give you kind of a binary pass fail or
sufficient or necessary, I forget which of those conditions you mentioned, but necessary.
But rather to give you insights into your model that you can then turn around into the
take into the training loop, the training process, for example, you mentioned these,
you know, the gender bias example. Is there, can it tell you more than you have a problem and maybe
how you might go about fixing the problem? Yeah, how you go about fixing a problem is a much more
difficult step, but I would say the first step of going about fixing a problem is to know
all the problems that your model might have, right? So yeah, I guess the short answer is no,
that's not something we focus on. In the future, we are thinking of ways to do it. The tricky thing here
is I think of these tests almost as like you should think of the test set in machine learning,
right? So test set is supposed to be this hidden test set just because you get some error on the
test set doesn't mean you just add those instances to your training data, right? If you do that,
you've lost the value of the test, right? So in the in similar sense, these tests or what checklist
is doing should be treated as something slightly external to the training process because you've come
up with the one way of phrasing the negation and you want to just test that as a proxy for other
ways of phrasing the negation. And so if you put this check this test somehow back into the
training loop, you've lost that advantage you had and now you have to come up with either a different
formulation of negation or assume that the model has learned negation and both of those might be
difficult or wrong things to do. So I think at least for now, we are thinking of checklist a little bit
as a test set that's a lot more fine grained and essentially customizable to your specific task.
Got it, got it. Tell us a little bit about the evaluation process for as you built this out.
Yeah, so we built this tool and we sat down on a bunch of tasks and we did sentiment classes
and QQP and squad and these are very different from each other, classification,
paraphrasing and question answering and we were feeling pretty confident about how useful checklist was
but we are biased because we developed it. So we decided to evaluate it and evaluating a
testing pipeline is a little bit tricky but Mark was spent a lot of time thinking about what
would be the right way to do this and we essentially converged on two separate evaluations,
both of them involved involving users. So the first one was since we are working with these
commercial models already, let's try to go to a commercial team that's actually responsible
for one of these products and not only sort of find out what their testing methodology is
but also propose checklist to them and see what they think of it. So we had I think like a five
hour session with one of the senior developers in that team and we were like here's checklist,
here's how it works, here's how it give them a walkthrough and then say like go crazy,
do what you guys do and see if you find this thing useful. And what was surprising was that
not only did they were they able to quickly confirm some of the bugs that they knew
were in the products which is nice but they were also able to find a bunch of new problems that
they didn't know was already in their model just by the use of checklist right and they were like
oh yeah now I need to get people to fix this stuff because this is a problem right and that's
exactly the use case we expect checklist to be useful for. And so that was one of the things and
we got pretty good feedback and I think Mark was working with a bunch of people to help
integrate checklist into the existing pipelines. The other set of evaluation we did was
we went to sort of university students and people who have at least taken an NLP course so they
know NLP but they probably and they most of them hadn't really worked with the QQP data set,
this question paraphrasing data set. So we wanted to test is checklist useful even if you're not
already a domain expert right and we explained to them what QQP was, we explained to them
what checklist is and it was a slightly cut down version of checklist because we didn't have
five hours to spend with each user and we said okay now can you find bugs in this system
and essentially you know we I can refer to the paper for exact numbers but people who were using
checklist the full capability of checklist were able to find many more bugs or many more different
problems with the model but even for each problem they were able to figure out a lot more test cases
so hundreds or on that order instead of writing just a few sentences so you got more confident
that those problems were actual problems because they were being tested on so many more instances
and so this both these combination of evaluation made us feel like this could be a pretty useful
tool for the community whether it's someone who's an expert and is trying to deploy machine learning
for practical purposes or it's some research project that's trying to be illegal.
Got it. There are groups that have developed for analogous problems in computer vision like
bias testing tool kits I'm thinking of you know things along the lines of gender shades like
trying to determine whether a given model or service can you know do what it does whether it's
you know predicting gender or predicting age or things like that independent of the ethnicity of
the sample data. What you're doing with checklist you know A strikes me as like a meta level above
those kinds of tests but I'm wondering if you are aware of or can envision something similar
applied to computer vision or other domains beyond NLP. Yeah so that's a good connection
and I would say we are quite inspired by a bunch of those papers when we were developing checklist
specifically in NLP also there have been a bunch of tools where they're looking at a specific
phenomenon and trying to look at okay is sentiment analysis system being fair or not and things
like that right so yeah checklist is very much inspired and related to all of that work
and in some sense tries to unify this whole thing like you said in a meta way. We actually did
play around a lot with computer vision and that's not something and I'm an expert in so partly
I would say that's on me but I think there are definitely cases where you can use checklist style
approach to computer vision. We played around a little bit with visual question answering
and we would do things like put up the image and things like that to try and see if the output
remains the same but it's a little bit trickier to do so yes I think there are ways to extend
checklist to apply it to computer vision and videos and things like that but it would require
a little bit more research than we had time for. Do you have a sense for some examples of what
minimum functional tests might be in the vision domain?
Yeah so the ones that I think of are maybe too simple and in some sense might be easy to do so
for example we can imagine creating an image right like taking a something that we know is a dog
and placing it in different parts of the image to see if it gets confused right that's probably
too simple and I'm sure most classifiers would work but then I can take dogs and combine them with
a typical background so I can take of course a scene from the park and you put a dog in it
is probably okay but can I take a scene of the sky and put a dog in it I still want the classifier
to predict dog can I take a slice of pizza and put a dog on it and things like that and so you
can imagine these kind of tests would be easy to create and maybe there's a computer vision
paper that's doing things like that but would give you an idea of okay does the background confuse
the classifier or not right and I imagine that some of these tests might actually be pretty might
show problems in existing computer vision systems awesome awesome any additional thoughts that you
would want to share with folks that are kind of interested in what they're hearing in and you
might want to explore more or might want to you know build on on this work you know what should folks
be thinking about you know as they're thinking about this problem yeah I would say part of the
slightly danger which checklist is a lot of the things we do are in some sense seem very obvious
right like especially once you see the examples you're like okay clearly we should be
testing for these things and and that kind of makes it a little bit dangerous because people might
easily overlook it right and so you know somebody might go into their team and like hey we have a
sentimental analysis team are we doing testing and the answer could come back as yes but testing is
not a yes or no question it's like are you doing a thorough job of testing it and so what we're
really hoping is checklist would inspire people to do a lot more thorough testing and in some sense
when they make accuracy judgment for a model they try to qualify it with some of the sort of things
that checklist produces right and hopefully they go much beyond checklist and do much more finer
grain reporting on what model does just like we wouldn't want the performance of a human employee
to be reduced to a number in some sense but you know a performance review involves a lot of
different dimensions to it as these machine learning models are becoming more involved in our
pipelines I think it's useful to think about them as like what are their capabilities and what
are their weaknesses and be able to say something about them rather than just saying I'm going to use
this model because it's the top of the leaderboard and therefore best. You mentioned that the
some of the examples that are covered by these minimum functional tests are you know obvious at
least in retrospect do you think that part of what the paper offers is kind of concrete language
to refer to these so that teams can say what we fail you know xyz test we fail the negation test
we fail the strong you know the strong final statement test. Is the paper even concrete in defining
these you know an establishing language there do you think that that becomes a way the people
use this that's a good point we didn't think of that use case but yes that that could be a
contribution although I would say that that would be a contribution to maybe people who haven't
seen sort of more linguisticy aspects of NLP right so a lot of our test names are heavily inspired
by stuff that has already been studying in linguistics we're just testing phenomena of language
and we're not the first one to think about what are the different phenomena and so the names we
use in checklist are a lot our common knowledge do a lot of NLP researchers but I can imagine
to someone who's been designing for example sentiment classifier they may not know what the
sort of what does semantic role labeling have to do with sentiment right whereas we have a bunch
of tests that check specifically for semantic role labeling and things like that got it got it
awesome well Samir thanks so much for taking the time to share with us a little bit about what you're
doing awesome work and congrats once again on the best paper award yeah thanks a lot Sam this was
a lot of fun thank you
