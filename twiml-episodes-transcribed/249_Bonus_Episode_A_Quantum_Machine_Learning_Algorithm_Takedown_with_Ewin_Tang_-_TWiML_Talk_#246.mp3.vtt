WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.200
I'm your host Sam Charrington.

00:31.200 --> 00:36.120
Alright gang, in this special bonus episode of the podcast, I'm joined by Ewin Tang, a

00:36.120 --> 00:41.320
PhD student in the theoretical computer science group at the University of Washington.

00:41.320 --> 00:46.000
In our conversation, Ewin and I dig into her paper, A Quantum Inspired Classical Algorithm

00:46.000 --> 00:52.160
for Recommendation Systems, which took the Quantum Computing community by storm last summer.

00:52.160 --> 00:57.360
Now we haven't called out a nerd alert interview in quite a long time, but this interview inspired

00:57.360 --> 01:01.440
us to dust off that designation, so get your notepad ready.

01:01.440 --> 01:03.440
And now on to the show.

01:03.440 --> 01:09.360
Alright everyone, I am on the line with Ewin Tang, Ewin is a PhD student at the University

01:09.360 --> 01:12.960
of Washington in the theoretical computer science group.

01:12.960 --> 01:15.840
Ewin, welcome to this week in machine learning and AI.

01:15.840 --> 01:17.760
Hi thanks, thanks for having me.

01:17.760 --> 01:21.360
I'm really glad we finally got a chance to connect.

01:21.360 --> 01:30.360
I think I first reached out to you back in the summer when you came on my radar for your,

01:30.360 --> 01:37.240
I think it was your undergrad thesis around, was this like July or so?

01:37.240 --> 01:40.320
Yeah, that sounds about right, yeah.

01:40.320 --> 01:45.400
So I'm not going to explain it, I'm going to let you explain it, but before we do that,

01:45.400 --> 01:52.200
tell us a little bit about your background, what you're up to now, and how your interests

01:52.200 --> 01:56.480
in machine learning evolved.

01:56.480 --> 01:58.800
Right now I'm like a first year grad student.

01:58.800 --> 02:06.000
So last year, and the four years before that I was at the UT Austin University of Texas

02:06.000 --> 02:07.800
at Austin.

02:07.800 --> 02:15.440
And initially actually starting my undergrad, I was a math major, so I was like interested

02:15.440 --> 02:24.000
in doing some more much more theoretical stuff, but then I sort of realized how interesting

02:24.000 --> 02:28.960
computer science was and how a lot of it has a lot more tangible applications that seem

02:28.960 --> 02:32.080
really interesting.

02:32.080 --> 02:39.680
And that's what led me to take Scott Erinzen's course in quantum information theory, so

02:39.680 --> 02:43.360
quantum computing type stuff.

02:43.360 --> 02:48.400
And from there, I kind of just stumbled into the field of quantum machine learning.

02:48.400 --> 02:54.920
So I've done a little bit of research in like a machine learning type things or sort

02:54.920 --> 02:59.880
of like the theoretical side, but this sort of work within quantum machine learning was

02:59.880 --> 03:05.480
just brought up because I wanted to do a thesis and undergrad thesis advised by him, and

03:05.480 --> 03:08.240
then that was a problem that he suggested.

03:08.240 --> 03:14.040
And so that basically was, he sort of phrased it to me as like a problem of actually solving

03:14.040 --> 03:16.000
a, like a lower bound.

03:16.000 --> 03:20.920
So technically it, it wouldn't have been like machine learning per se if I actually got

03:20.920 --> 03:22.040
that result.

03:22.040 --> 03:27.040
But since the result I got was now algorithm, it's now I guess kind of machine learning.

03:27.040 --> 03:29.960
But yeah, there is at least started my interest in the field.

03:29.960 --> 03:33.920
I'm working on that a little bit here at UW, and I'm also working on some other things

03:33.920 --> 03:36.320
with my advisor, James Lee.

03:36.320 --> 03:44.000
And so the headlines I remember from back in the summer were like this major kind of

03:44.000 --> 03:51.160
quantum computing advance, which were, which I took to be that there were these this

03:51.160 --> 03:57.080
class of machine learning quantum machine learning algorithms that were to a large degree

03:57.080 --> 04:04.040
considered to be a big part of the justification for quantum computing, at least by some people.

04:04.040 --> 04:10.480
And your thesis basically showed how well you didn't really need quantum computers to

04:10.480 --> 04:11.480
do this.

04:11.480 --> 04:17.760
Is that, you know, how, how close is that to describing the thesis and can you kind of put it

04:17.760 --> 04:21.120
in your own words and maybe give us an overview of it?

04:21.120 --> 04:23.880
Yeah, that sounds, that sounds actually pretty accurate.

04:23.880 --> 04:32.000
So the general idea of my thesis is that there's this field of algorithms called like quantum

04:32.000 --> 04:35.520
algorithms, you know, that do that solve machine learning tasks.

04:35.520 --> 04:40.600
And usually this is like very basic tasks like linear algebra type things.

04:40.600 --> 04:49.320
And the reason people were so excited about them when they were like when they were released

04:49.320 --> 04:56.640
is that they produce exponential speed ups for these linear algebra problems.

04:56.640 --> 05:03.800
So basically, if you're familiar with quantum computing, the sort of the highest, the best

05:03.800 --> 05:06.720
type of speed up that you can get is an exponential speed up.

05:06.720 --> 05:13.360
And that's, for example, what we believe occurs for like factoring and like shows algorithm

05:13.360 --> 05:18.880
and the problems that quantum computing is known for solving.

05:18.880 --> 05:22.800
And so an exponential speed up would be great.

05:22.800 --> 05:30.280
But it kind of has some caveats in the sense that it, these algorithms assume some very

05:30.280 --> 05:33.600
strong assumptions about their input.

05:33.600 --> 05:42.840
Actually they say, okay, if you give me all of the data in a quantum state, so that means

05:42.840 --> 05:51.600
like if you give me everything in like superposition basically, then I can do it exponentially fast.

05:51.600 --> 05:56.080
Then I can, you know, do your machine learning incredibly fast.

05:56.080 --> 06:01.120
However, doing this is actually creating these quantum states is actually a hard task

06:01.120 --> 06:07.080
computationally. And so people didn't really were kind of skeptical about whether you could

06:07.080 --> 06:13.480
actually implement these things, you know, people weren't sure whether you could, whether

06:13.480 --> 06:18.880
even when the subroutines, when these like linear algebra routines are fast, that just

06:18.880 --> 06:22.480
getting to that point would take so much time that it wouldn't have been worth the effort

06:22.480 --> 06:23.480
to begin with.

06:23.480 --> 06:28.800
So that's the sort of conflict that's sort of motivating the work.

06:28.800 --> 06:34.920
So this, my thesis was about a particular algorithm.

06:34.920 --> 06:40.880
It's an algorithm by Jordan Karen Edison, a new form per cache called quantum recommendation

06:40.880 --> 06:42.600
systems.

06:42.600 --> 06:47.480
And it purports to like give an exponential speed up for the problem of like recommending

06:47.480 --> 06:52.920
products to users like, like what Netflix or Amazon do.

06:52.920 --> 06:58.560
And the interesting thing about that algorithm is that it actually has, it actually has

06:58.560 --> 07:00.400
relatively few assumptions.

07:00.400 --> 07:08.840
So it doesn't assume that you, that you, that you can just implement quantum states out

07:08.840 --> 07:09.840
of thin air.

07:09.840 --> 07:13.760
It gives like a protocol, it gives like a data structure for you to construct these

07:13.760 --> 07:14.760
things.

07:14.760 --> 07:20.000
And further, a lot of the previous quantum algorithms were sort of trying to solve problems

07:20.000 --> 07:24.120
that weren't being studied classically.

07:24.120 --> 07:30.000
So these previous quantum algorithms were just constructing problems that they believed

07:30.000 --> 07:35.680
could be solved faster with quantum computing, and then they justify their usefulness.

07:35.680 --> 07:39.720
But the nice thing about this algorithm is that it had been studied previously, and then

07:39.720 --> 07:47.120
people had not been able to get even close to as fast as what the quantum algorithm achieved.

07:47.120 --> 07:53.880
So basically this recommendation system algorithm is, was one of the best examples of quantum

07:53.880 --> 07:55.440
machine learning that we had.

07:55.440 --> 08:00.640
And it had an exponential, it was exponentially faster than all previous classical algorithms

08:00.640 --> 08:02.040
that we knew of.

08:02.040 --> 08:10.080
Is this recommendation system or algorithm that they proposed is it based on matrix factorization

08:10.080 --> 08:17.360
and is that why did they kind of inherit the exponential speed up of some matrix factorization

08:17.360 --> 08:22.040
algorithms, or is there something kind of fundamental to their approach that gave them

08:22.040 --> 08:24.280
that exponential speed up?

08:24.280 --> 08:25.280
Yeah, yeah.

08:25.280 --> 08:27.480
So it's basically like a matrix factorization.

08:27.480 --> 08:28.480
Yeah.

08:28.480 --> 08:37.080
It's essentially what happens is that you want, so your recommendation matrix is some

08:37.080 --> 08:43.320
like low rank matrix, basically that just the implication is that people base their preferences

08:43.320 --> 08:46.360
on some small number of attributes that something has.

08:46.360 --> 08:52.000
So maybe you judge an Amazon product by its popularity, its price, and then the

08:52.000 --> 08:54.280
number of stars or something.

08:54.280 --> 09:01.080
And so you have relatively few attributes which tends to lead to low rank matrices.

09:01.080 --> 09:08.080
And the general idea is that you want to factor this matrix, like compute the SVD, and

09:08.080 --> 09:12.960
this will separate your users into user classes, basically.

09:12.960 --> 09:18.680
So you can say, okay, these are like prototypical users, and then these are sort of recommendations

09:18.680 --> 09:20.560
for prototypical users.

09:20.560 --> 09:25.640
And thereby, if you give me a user, I can look at it and say, how similar is it to these

09:25.640 --> 09:29.360
users that I already know the recommendations for, and then compute the recommendations

09:29.360 --> 09:30.360
accordingly.

09:30.360 --> 09:34.600
And so this is basically what the quantum recommendation system does.

09:34.600 --> 09:41.800
It does all of this in like the quantum superposition world, like it doesn't actually explicitly write

09:41.800 --> 09:47.200
down any of these things, but it's able to do that without writing anything down, and

09:47.200 --> 09:51.480
so it can do it a lot faster than it would be to write anything down.

09:51.480 --> 09:55.920
So I'm curious, maybe we can hit pause on the thesis conversation.

09:55.920 --> 10:04.640
I had several interviews about quantum stuff, and to be honest, I don't really understand

10:04.640 --> 10:07.280
it still.

10:07.280 --> 10:10.680
And so that's probably the case for at least a few people in the audience as well.

10:10.680 --> 10:17.040
I'm curious, in your words, how do you think of this whole quantum computing stuff?

10:17.040 --> 10:19.920
How do you explain it to people?

10:19.920 --> 10:26.680
It's kind of hard because I'm not really that good at quantum computing intuition myself.

10:26.680 --> 10:35.040
So I think the way that I like to think about it at least when I'm like, at least the way

10:35.040 --> 10:43.120
that it's relevant to my research is that it's useful to think of quantum superposition.

10:43.120 --> 10:49.520
If you think of like saying a quantum superposition of a state, usually you can like pretty well

10:49.520 --> 10:55.560
approximate the meaning by replacing like quantum superposition with probability distribution.

10:55.560 --> 11:04.640
So like generally what I like to do is whenever I hear about like quantum computing is just

11:04.640 --> 11:10.920
about setting up these probability distributions in the right way, and these probability distributions

11:10.920 --> 11:16.120
are souped up so that you can have these like interference.

11:16.120 --> 11:19.480
So you can have probabilities cancel each other out.

11:19.480 --> 11:20.480
Yeah.

11:20.480 --> 11:28.400
And basically it seems like a lot of the like exponential type speed ups especially come

11:28.400 --> 11:31.760
from this type of like way of thinking about it.

11:31.760 --> 11:37.600
So roughly we can think of a computer that's made up of not buses and cells and gates

11:37.600 --> 11:43.040
that deal with ones and zeros but rather those similar kinds of things that deal with probability

11:43.040 --> 11:48.080
distributions and weird ones that interact in weird ways.

11:48.080 --> 11:49.080
Right.

11:49.080 --> 11:51.080
So yeah.

11:51.080 --> 12:01.600
If you think about like like, yeah, so it's useful a lot to think of like, okay, like a probability

12:01.600 --> 12:08.840
like a like a quantum superposition of zero and one is essentially just like a probability

12:08.840 --> 12:15.120
distribution like a coin flip picking zero or one each half the time.

12:15.120 --> 12:22.200
And obviously this fails sometimes because whenever you want to like do some cancellation,

12:22.200 --> 12:27.640
you might have a situation where you know technically it's okay, so you have a one

12:27.640 --> 12:33.200
fourth probability of one thing and then you add that to one quarter probability of the

12:33.200 --> 12:37.160
same thing and then you get zero somehow.

12:37.160 --> 12:47.400
But a lot of the techniques that are used in quantum in quantum computing are basically

12:47.400 --> 12:53.480
similar to what similar to their like classical analogs similar to what you would do to manipulate

12:53.480 --> 12:55.480
probability distributions.

12:55.480 --> 13:04.400
And so then back to your thesis, you, there's this recommendation algorithm based on matrix

13:04.400 --> 13:14.440
factorization and you started out by trying to identify what initially we believe that

13:14.440 --> 13:24.200
the the quantum algorithm that they provided that the quantum recommendation system algorithm.

13:24.200 --> 13:28.800
You could prove that it was exponentially faster than any classical algorithm.

13:28.800 --> 13:33.560
And so what this would do is basically say show, hey, we found an example where quantum

13:33.560 --> 13:37.360
can help exponentially in machine learning.

13:37.360 --> 13:41.520
And so this is like a, this is something that people had already believed.

13:41.520 --> 13:49.440
So there was an existing proof that this algorithm couldn't be done in classical computers.

13:49.440 --> 13:50.680
We wanted to show such a proof.

13:50.680 --> 13:51.680
Okay.

13:51.680 --> 13:52.680
Got it.

13:52.680 --> 13:59.640
We wanted to actually show that you could, you could not do the same thing classically.

13:59.640 --> 14:04.240
And people sort of already believed this because of how strong the results looked.

14:04.240 --> 14:08.760
And so this is, this would have been like the, I guess the final nail in the coffin.

14:08.760 --> 14:14.680
But the thing is that, well, I worked on it for a while and I kind of got nowhere.

14:14.680 --> 14:17.720
So like, this is for my undergrad thesis.

14:17.720 --> 14:22.800
I guess I had like, you know, nine ten months to dissolve it and like halfway through I had,

14:22.800 --> 14:24.800
you know, nothing.

14:24.800 --> 14:32.120
I didn't have any approach to allow around the things that I tried didn't seem to work.

14:32.120 --> 14:39.920
And gradually I came to realize that the reason the strategies didn't work was because

14:39.920 --> 14:44.760
the recommendation algorithm can actually be mimicked with a classical algorithm.

14:44.760 --> 14:46.920
What was it that told you that?

14:46.920 --> 14:49.520
There are a couple of factors that led me to realize this.

14:49.520 --> 14:56.760
So, so the first factor was basically that whenever I tried to prove some sort of lower

14:56.760 --> 15:02.240
bound, tried to prove that classical algorithms couldn't do the problem, I would run into

15:02.240 --> 15:04.240
some sort of roadblock.

15:04.240 --> 15:10.520
And then in analyzing what these roadblocks were further, you could basically, I could

15:10.520 --> 15:17.200
basically sort of figure out what were the parts that seemed that the classical algorithm

15:17.200 --> 15:18.200
could do.

15:18.200 --> 15:21.160
So, so it's kind of like a push and pull.

15:21.160 --> 15:25.520
If you want to design a classical algorithm, you have to try to do it without hitting any

15:25.520 --> 15:32.160
roadblocks that because the problems that you come up with are hard, whereas if you're

15:32.160 --> 15:36.600
trying to prove a lower bound, you're trying to show hardness, the roadblocks are actually

15:36.600 --> 15:42.320
things that the, that the, that a classical algorithm would find easy.

15:42.320 --> 15:49.840
Essentially, all of the roadblocks that I found were basically, were indications for

15:49.840 --> 15:54.920
what could be done for a, and a classical algorithm.

15:54.920 --> 16:00.440
And so, so that was one factor, basically just that research into lower bounds gives

16:00.440 --> 16:05.240
you some understanding of like how, where the hardness of the problem is and how you

16:05.240 --> 16:07.720
could avoid it.

16:07.720 --> 16:14.120
And the other factor was I, I early into the research I found this paper by says Alan

16:14.120 --> 16:20.440
Fries, Robbie canon and Santosh Vampala, it's a machine learning type algorithm, basically

16:20.440 --> 16:27.800
it does low rank matrix factorization and it does it in time independent of the input

16:27.800 --> 16:28.800
size.

16:28.800 --> 16:33.800
So, basically what that means is like no matter, no matter the dimension, it can do it

16:33.800 --> 16:36.520
just as fast.

16:36.520 --> 16:42.560
And it does this with some, some assumptions, so it is, namely it assumes that you can

16:42.560 --> 16:48.480
sample somehow from the entries of this matrix, this input matrix.

16:48.480 --> 16:54.000
And the interesting thing is that these input assumptions, these sampling assumptions that

16:54.000 --> 16:59.520
are crucial for the algorithm were also present for the recommendation system, the recommendation

16:59.520 --> 17:00.520
system model.

17:00.520 --> 17:07.840
So you recall that we don't get the matrix just to like read from like we don't get like,

17:07.840 --> 17:16.120
we don't just get like RAM access memory to a random access memory to the matrix, the

17:16.120 --> 17:19.720
matrix that contains like user product preferences.

17:19.720 --> 17:22.960
We also get it in a data structure.

17:22.960 --> 17:29.760
So in order for the quantum algorithm to be able to prepare the superposition that

17:29.760 --> 17:33.520
it wants, it designs a data structure.

17:33.520 --> 17:38.400
And this data structure classically also allows for you to perform the exact type of samples

17:38.400 --> 17:42.840
that you would need to do this fast classical algorithm.

17:42.840 --> 17:50.720
Now just from this algorithm, it's not obvious at all what really to do with it.

17:50.720 --> 17:56.520
It kind of shows that you could do it and then you could factor the matrix and stops there

17:56.520 --> 18:02.520
when in fact for the recommendation system problem, you need to do something with those,

18:02.520 --> 18:05.360
do something with those vectors that you find.

18:05.360 --> 18:10.360
And it's not obvious how you would do that, but it was an indication that said, okay,

18:10.360 --> 18:13.360
you can do some things in this model.

18:13.360 --> 18:17.280
Some of the tasks that you might have thought were hard classically.

18:17.280 --> 18:20.360
You might need to rethink those, those problems.

18:20.360 --> 18:29.080
So a big part of your thesis was coming up with this classical algorithm that was analogous

18:29.080 --> 18:32.360
to this quantum algorithm.

18:32.360 --> 18:40.000
You mentioned that you'd found some prior work that had similar constraints as the algorithm

18:40.000 --> 18:44.200
did in the quantum side.

18:44.200 --> 18:50.600
What did you need to do to get from the prior work to the algorithm that corresponded to

18:50.600 --> 18:52.120
the quantum side?

18:52.120 --> 18:53.920
So it's kind of interesting.

18:53.920 --> 18:59.760
I wasn't familiar with any of this, so this is like in the area of the radical recommendation

18:59.760 --> 19:00.760
system literature.

19:00.760 --> 19:05.520
There's this set of papers that basically talk about how to like theoretically model

19:05.520 --> 19:10.040
recommendation systems and do like that sort of analysis.

19:10.040 --> 19:16.720
So they use the name recommendation system instead of like, I think people use collaborative

19:16.720 --> 19:20.800
filtering more recently, is that that's on right?

19:20.800 --> 19:31.080
But yeah, so there were a set of papers that basically in like the early 2000s to late

19:31.080 --> 19:36.360
90s that basically laid out the entire model for recommendation systems.

19:36.360 --> 19:42.200
And this is the model that is used for my algorithm and for the quantum algorithm as

19:42.200 --> 19:43.200
well.

19:43.200 --> 19:47.360
And they actually came up with something.

19:47.360 --> 19:51.560
It's basically the same thing as what Frie's canon and been polited.

19:51.560 --> 19:57.200
They basically found this prior work and then sort of got stuck because what you have

19:57.200 --> 20:03.760
to do is just to lay out the groundwork.

20:03.760 --> 20:10.760
What we want to do is we want to give a sublinear algorithm for this recommendation system

20:10.760 --> 20:11.760
problem.

20:11.760 --> 20:19.160
And because it is a sublinear algorithm, this means that we can't read all of the matrix

20:19.160 --> 20:21.520
in all of the input in.

20:21.520 --> 20:28.600
And so if you imagine your input as being a set of preferences that users have for particular

20:28.600 --> 20:36.160
products, it even means that we can't list out all of the use that we have or list out

20:36.160 --> 20:37.320
all of the products we have.

20:37.320 --> 20:46.280
We don't have enough time to do that because we want to give like a logarithmic time algorithms.

20:46.280 --> 20:51.000
It's the time that we are allotted is so small that we can't do much.

20:51.000 --> 20:56.200
And the problem with that is that we're dealing with like a linear algebra problem.

20:56.200 --> 21:04.760
And so what this prior work outputs is it outputs the singular vectors of the input matrix.

21:04.760 --> 21:09.920
But it can't output the singular vectors because the vectors are the length of the matrix.

21:09.920 --> 21:11.680
And so you can't do that in enough time.

21:11.680 --> 21:12.680
Does that make sense?

21:12.680 --> 21:13.680
I think so.

21:13.680 --> 21:14.680
Yeah.

21:14.680 --> 21:15.680
Yeah.

21:15.680 --> 21:18.440
So you don't have enough time to output the whole vector.

21:18.440 --> 21:23.520
So you have to describe what the vector looks like in, you have to give what they call

21:23.520 --> 21:26.720
a succinct description.

21:26.720 --> 21:32.960
And so what they do is they can't say, here's the vector that we give as output because

21:32.960 --> 21:35.320
to write out all of that down it takes too long.

21:35.320 --> 21:43.880
And so what you say is you say, look at this set of users and then perform some linear

21:43.880 --> 21:53.400
combination or some like you do some vector addition or something on these users.

21:53.400 --> 21:55.720
And that's going to be your singular vector.

21:55.720 --> 22:00.720
So you've described the vector that you want to output based on the vectors that you were

22:00.720 --> 22:03.240
given.

22:03.240 --> 22:09.600
And that way you can avoid the whole process of giving out each entry one by one.

22:09.600 --> 22:15.520
But that's a problem if you actually want to use this result because now you can't

22:15.520 --> 22:16.680
actually read the vector.

22:16.680 --> 22:21.040
You have to deal with all of this stuff beforehand and that stuff takes a lot of time presumably,

22:21.040 --> 22:23.040
right?

22:23.040 --> 22:30.520
And essentially you run into this issue where you really want to write your vectors down

22:30.520 --> 22:34.000
but you can't because you don't have enough time.

22:34.000 --> 22:39.160
And the quantum algorithm sort of foregoes this issue by just having everything in a quantum

22:39.160 --> 22:40.160
state.

22:40.160 --> 22:44.880
And so in like if you have something in like a quantum superposition or something like

22:44.880 --> 22:50.880
that, you don't have to write anything down.

22:50.880 --> 22:56.640
So that was the main issue from going from the prior work to the recommendation system

22:56.640 --> 22:58.120
algorithm.

22:58.120 --> 23:04.280
And so what ended up happening, what I ended up doing is figuring out, okay, you can avoid

23:04.280 --> 23:11.240
this problem by looking at the quantum, like by taking quantum super, super positions.

23:11.240 --> 23:15.440
And so maybe you can actually do the same thing with the distribution.

23:15.440 --> 23:20.840
So you can avoid the problem of writing everything down by using a probability distribution.

23:20.840 --> 23:27.120
And what I mean by that is basically to say the following, using the fact that we can sample

23:27.120 --> 23:31.880
high-weight entries of basically anything that we want about the input matrix.

23:31.880 --> 23:41.040
So you know, basically I can get the user product preferences that matter most.

23:41.040 --> 23:52.880
I can sample randomly, so I can just pick a random, a bunch of random samples from the matrix,

23:52.880 --> 23:54.840
from the input matrix.

23:54.840 --> 24:00.360
And use that to give a decent approximation of the vectors that I want.

24:00.360 --> 24:05.600
What's interesting is that basically you can do this through the entire process.

24:05.600 --> 24:14.360
So basically once you have this succinct description, you can actually use that for sampling.

24:14.360 --> 24:18.720
So even though I haven't written anything down about the vector, I can actually still

24:18.720 --> 24:20.520
sample from it.

24:20.520 --> 24:23.360
It's basically not obvious, but you can actually do it.

24:23.360 --> 24:26.960
And so once you have these samples, you can sample from the high-weight entries of these

24:26.960 --> 24:27.960
vectors.

24:27.960 --> 24:29.360
You can go the rest of the way.

24:29.360 --> 24:32.000
You can do whatever you want to do.

24:32.000 --> 24:37.000
And for the recommendation system case, basically what you want to do is you want to perform

24:37.000 --> 24:38.320
some projection operation.

24:38.320 --> 24:46.440
You want to project your user vector onto these matrix singular vectors.

24:46.440 --> 24:51.440
And it turns out that you can do that using these sampling strategies.

24:51.440 --> 24:54.360
And that basically gives you the algorithm.

24:54.360 --> 24:56.280
You can do everything without writing anything down.

24:56.280 --> 24:59.360
And so you take poly logarithm at time.

24:59.360 --> 25:02.640
So you take a sublinear amount of time.

25:02.640 --> 25:06.640
The quantum algorithm is, you said invariant with the length.

25:06.640 --> 25:12.480
So it's like big of one kind of time for this recommendation system.

25:12.480 --> 25:16.600
And so you needed to do it in something sublinear.

25:16.600 --> 25:17.600
Right.

25:17.600 --> 25:24.080
So the quantum algorithm is like big of log n, something like that.

25:24.080 --> 25:29.200
So that's like logarithmic time, anything that's like that to some powers, like called

25:29.200 --> 25:30.600
poly logarithmic time.

25:30.600 --> 25:34.280
So what we achieve is we achieve poly logarithmic time.

25:34.280 --> 25:40.120
So we achieve something like log n squared or cubed or something like that.

25:40.120 --> 25:41.120
Okay.

25:41.120 --> 25:45.680
Initially we thought that the quantum algorithm could do log n and the classical algorithm

25:45.680 --> 25:48.520
could do only like O of n.

25:48.520 --> 25:52.320
And so I brought the time down from O of n to O of like log squared n.

25:52.320 --> 25:56.080
So it's like a like exponential improvement.

25:56.080 --> 26:04.560
And is this something that you envision would actually be used or deployed in some recommendation

26:04.560 --> 26:05.560
system?

26:05.560 --> 26:14.840
Or is it more the kind of theoretical implementations on, you know, the what it means for this quantum

26:14.840 --> 26:21.200
algorithm and the theoretical analysis of this classical algorithm?

26:21.200 --> 26:27.000
I mean, I certainly hope it could be useful in the future.

26:27.000 --> 26:34.920
So the thing is like because this algorithm wasn't really intended to be like, it was just

26:34.920 --> 26:40.720
something that I like hacked together and it like because because for my theoretical

26:40.720 --> 26:47.240
purposes, anything that's like even close to the quantum algorithm would have worked.

26:47.240 --> 26:53.600
But I didn't think that hard about trying to optimize everything about the algorithm.

26:53.600 --> 26:56.480
So I'm not sure if it would actually work in practice is something that I still want

26:56.480 --> 26:58.080
to try to figure out.

26:58.080 --> 27:03.280
But I do think that it would be possible for somebody to like clean up the algorithm

27:03.280 --> 27:07.480
and actually get a version that works a lot better and then maybe is competitive with

27:07.480 --> 27:09.000
other techniques.

27:09.000 --> 27:13.280
I think what's done in practice right now is sort of limited by the like the amount of

27:13.280 --> 27:16.480
time that these factorization techniques take.

27:16.480 --> 27:24.920
So I think if you look at like what recommendation systems do in practice, they have to like update

27:24.920 --> 27:25.920
regularly.

27:25.920 --> 27:32.640
So they have to perform the algorithm every week or every, I don't know, I don't know

27:32.640 --> 27:33.640
how often they do it.

27:33.640 --> 27:39.840
But because you have to take so much time out to actually just sit down and compute the

27:39.840 --> 27:44.960
things that you want to compute, it just that if you could speed up that problem, you

27:44.960 --> 27:51.320
could probably help a lot with the flow of the I guess recommendation system.

27:51.320 --> 27:56.760
I guess I guess I'm not the right person to ask about like the classical what it means

27:56.760 --> 28:00.360
for like, you know, practice.

28:00.360 --> 28:08.280
But I can at least say that for the quantum machine learning side of things, it's pretty

28:08.280 --> 28:12.920
interesting that you can actually just simulate all of these things with simulate all of these

28:12.920 --> 28:16.200
quantum algorithms with classical sampling techniques.

28:16.200 --> 28:23.840
And did you a big part of your algorithm is replacing knowing the input vector with the

28:23.840 --> 28:27.320
distribution of the input vector?

28:27.320 --> 28:33.480
Did you characterize at all how that replacement impacts the ultimate result or performance

28:33.480 --> 28:35.240
of the recommendation system?

28:35.240 --> 28:41.840
I guess my thinking is that when you replace the full vector with some distribution, that's

28:41.840 --> 28:49.120
essentially like injecting noise or adding noise to this recommendation system.

28:49.120 --> 28:50.120
Yeah.

28:50.120 --> 29:00.120
And I'm just wondering if you looked at all at what the impact of that noise is on the

29:00.120 --> 29:03.600
performance of the recommendation system theoretically?

29:03.600 --> 29:09.480
Actually, so the noise is quite bad from a theoretical perspective.

29:09.480 --> 29:16.680
So in fact, so the sampling algorithm that I mentioned, the classical sampling algorithm,

29:16.680 --> 29:20.920
the prior work was released in, I believe, 2007.

29:20.920 --> 29:27.160
And since then, there have been a lot of like improvements on this algorithm that basically

29:27.160 --> 29:33.480
say, okay, these sampling assumptions were too weak because they added too much, they

29:33.480 --> 29:38.960
sort of, we can basically do better just by assuming a little bit more.

29:38.960 --> 29:43.440
And so a lot of these more recent algorithms follow the line of, okay, let's assume that

29:43.440 --> 29:50.200
our matrix is a sparser, okay, let's assume that we're able to read our matrix with

29:50.200 --> 29:52.040
some number, some number of times.

29:52.040 --> 29:56.960
So like, we're basically streamed all the matrix entries and then we're supposed to

29:56.960 --> 29:59.360
develop an algorithm out of that.

29:59.360 --> 30:08.920
And so in these models, you can actually do, you can improve your error quite a bit.

30:08.920 --> 30:16.120
Um, it turns out that for the, it turns out that for the recommendation system problem,

30:16.120 --> 30:22.120
a lot of noise is okay, that's also kind of just because we have strong assumptions

30:22.120 --> 30:24.440
for how well, how good our data is.

30:24.440 --> 30:31.960
So even, even way back in like 2002, we had to assume theoretically that our data was

30:31.960 --> 30:35.840
really, really nice in order for us to give any recommendation algorithm.

30:35.840 --> 30:40.640
So we're still kind of operating off of some of these assumptions.

30:40.640 --> 30:46.600
So like if you, if you look at my paper, um, like there are a lot of really weird assumptions

30:46.600 --> 30:50.920
that I have to cover in like the first, like the first 10 pages is to like sort of set

30:50.920 --> 30:54.880
the groundwork and these are things that you could weaken presumably or that you would

30:54.880 --> 30:57.760
have to weaken in practice.

30:57.760 --> 31:00.760
And so I'm not sure how the noise would impact that.

31:00.760 --> 31:04.240
It's not something that it looked at, but it does impact it quite a bit.

31:04.240 --> 31:12.120
Do you have a sense for the ultimate impact on the quantum machine learning kind of community

31:12.120 --> 31:16.280
or our research based on the, the results that you came up with?

31:16.280 --> 31:19.960
So, so there have been a couple of developments since this, uh, this paper actually there.

31:19.960 --> 31:26.280
So since this algorithm was, okay, I call it like de-quantize, basically, I mean, like

31:26.280 --> 31:27.560
you took the quantum out of it, right?

31:27.560 --> 31:30.840
So you, you gave a classical algorithm corresponding to it.

31:30.840 --> 31:36.640
Um, there have been, uh, three more quantum machine learning algorithms that have been

31:36.640 --> 31:41.120
essentially like this, this kind of de-quantized, uh, that have been de-quantized.

31:41.120 --> 31:46.320
So I guess, so I guess to like, um, a bystander looks like quantum machine learning is like

31:46.320 --> 31:47.880
birding to the ground or something.

31:47.880 --> 31:53.760
Like, um, like all of these algorithms are going down one by one, but I think I've talked

31:53.760 --> 31:58.280
to like some quantum machine learning researchers who were like, uh, who basically said, yeah,

31:58.280 --> 32:01.920
um, we didn't really believe that these algorithms were that strong anyway.

32:01.920 --> 32:07.160
So it's, it's good that you're, uh, that you're, um, showing that, you know, like that

32:07.160 --> 32:14.120
these are, um, being de-quantized because, uh, essentially, there is a lot of hype in

32:14.120 --> 32:20.520
quantum machine learning and these types, like, these hyper, hype is spurred by, um, papers

32:20.520 --> 32:22.560
that claim a lot of exponential speedups.

32:22.560 --> 32:30.640
Um, and so, so basically, uh, these classical algorithms, there have been some classical

32:30.640 --> 32:36.880
algorithms that try to say, okay, these exponential speedups seem really hard to achieve in practice

32:36.880 --> 32:40.400
just because these are easy tasks classically anyways.

32:40.400 --> 32:46.360
So you have to, um, basically, you, you should, you, this should be like something stronger

32:46.360 --> 32:52.800
that you can do quantumly or, um, in order to actually get a exponential speedup that's,

32:52.800 --> 33:00.200
uh, I guess, stronger, uh, depending on what your model is, some exponential speedups

33:00.200 --> 33:03.200
are weaker and some are stronger, I guess.

33:03.200 --> 33:08.120
And so we're just, right now, I guess, uh, this, this work is sort of, serves to separate

33:08.120 --> 33:10.600
some of the wheat from the, the shaft, I guess.

33:10.600 --> 33:18.000
Are there currently existing strong use cases for the, these quantum algorithms or, or

33:18.000 --> 33:24.920
strong algorithms, or are they, these, you know, have we, uh, dequantized a bunch of

33:24.920 --> 33:30.720
them and, you know, do we need to now find the, the new strong cases?

33:30.720 --> 33:35.120
I think that question, like, the answer to that question definitely depends on like, it's

33:35.120 --> 33:37.960
like an ongoing topic of discussion.

33:37.960 --> 33:45.000
So I think, um, like, personally, I don't think that there's any quantum of human learning

33:45.000 --> 33:50.680
algorithm that has like a, like a strong theoretical backing for being good, um, for being like,

33:50.680 --> 33:55.240
forgiving exponential speedups, um, but I don't know, maybe that could change depending

33:55.240 --> 33:56.240
on the day.

33:56.240 --> 34:04.160
Uh, there is one algorithm, there, there, there's still hope and that hope is, uh, because

34:04.160 --> 34:11.920
a lot of the algorithms that remain are based on this procedure for matrix inversion.

34:11.920 --> 34:18.040
So you may have heard of this quantum algorithm by Harrow, Hasidim, and Lloyd, that inverts

34:18.040 --> 34:23.880
a, um, a matrix in time poly logarithmic in the dimension.

34:23.880 --> 34:29.760
And this algorithm is actually, it, it's what we call BQP complete, um, so this is like

34:29.760 --> 34:35.680
a complexity class that basically says, if you can de-quantize this algorithm, then you

34:35.680 --> 34:41.640
can de-quantize all of quantum computing, um, and so basically we believe it's hard, uh,

34:41.640 --> 34:46.320
we believe it actually does give exponential speedups in like certain circumstances.

34:46.320 --> 34:52.960
However, the thing is that, uh, the, the situations for which we can devise exponential

34:52.960 --> 34:54.800
speedups are very contrived.

34:54.800 --> 35:01.600
Um, so, you know, if your input data happens to be exactly this matrix, then you can get

35:01.600 --> 35:07.160
an exponential speedup, for example, um, but of course, in practice, you want to be able

35:07.160 --> 35:12.400
to apply this, uh, apply this to just any matrix that you have.

35:12.400 --> 35:18.040
And we know for a fact that, or essentially we, we, we can't do this for any arbitrary

35:18.040 --> 35:19.040
matrix.

35:19.040 --> 35:24.040
We can't get a speedup for any matrix, at least it doesn't seem that way right now.

35:24.040 --> 35:27.280
And so it's kind of like a, like a balancing act.

35:27.280 --> 35:34.640
So you have to try to find like the machine learning problem that's just hard enough to

35:34.640 --> 35:39.360
be out of reach for classical computers, but not too hard so that quantum computers can't

35:39.360 --> 35:40.360
solve them.

35:40.360 --> 35:42.480
So you need to be in like that sweet spot.

35:42.480 --> 35:47.440
And I guess the final thing is that even if we could find such a thing, the work would

35:47.440 --> 35:53.840
still be somewhat speculative because we don't actually know whether we can build quantum

35:53.840 --> 35:54.840
memory.

35:54.840 --> 35:59.360
Uh, so right now we have quantum computers and they can like do circuits and things like

35:59.360 --> 36:00.360
that.

36:00.360 --> 36:05.880
Um, but for a lot of these machine learning algorithms to work, we assume that we have

36:05.880 --> 36:11.720
some really strong, like, I guess, the RAM, they call it quantum RAM, Q RAM.

36:11.720 --> 36:16.440
Uh, we have some really strong RAM that holds all our data and that we can do some like,

36:16.440 --> 36:21.840
we can, we can query to and, um, construct states from quickly.

36:21.840 --> 36:27.120
And so this is, uh, additionally is something that we don't know whether it works in practice.

36:27.120 --> 36:31.560
So, so it works for classical computers, but we're not sure whether it works for quantum

36:31.560 --> 36:32.560
computers.

36:32.560 --> 36:36.280
And this is something that an experimentalist would have to confirm, I guess.

36:36.280 --> 36:40.800
It's all a little bit speculative, but it's, it's interesting to think about all the

36:40.800 --> 36:43.160
different scenarios that could happen, right?

36:43.160 --> 36:47.440
It sounds like it's, uh, you know, considering that you, that so many of these algorithms

36:47.440 --> 36:55.800
have been de-quantized in, uh, just a few months since your paper, uh, was published.

36:55.800 --> 36:59.880
It, it sounds like it's a very fast moving area.

36:59.880 --> 37:05.360
And there's, I didn't realize that the, that when we talk about these quantum algorithms

37:05.360 --> 37:09.240
and quantum computers, that, you know, there's this huge missing piece, which is this quantum

37:09.240 --> 37:10.240
RAM.

37:10.240 --> 37:13.800
I didn't realize that that wasn't part of what we, you know, when we talk about quantum

37:13.800 --> 37:17.600
computers, that that wasn't more of a complete package, but, uh, it sounds like there's

37:17.600 --> 37:20.240
still a ton of work to do in this space.

37:20.240 --> 37:21.240
Right.

37:21.240 --> 37:27.480
It's, it's interesting, actually, like, um, some quantum algorithms don't need memory, if

37:27.480 --> 37:28.480
that makes sense.

37:28.480 --> 37:33.400
So, so, for example, you can, you can, do, you can run short as algorithm without requiring

37:33.400 --> 37:40.080
memory because basically you can, for, for whatever number I give you, you can give

37:40.080 --> 37:44.760
me a circuit or a quantum circuit that basically simulates the RAM.

37:44.760 --> 37:49.320
So, I can construct, you can give me the input and I can construct the RAM quickly.

37:49.320 --> 37:54.400
So, let's say, like, you gave me a number four and you wanted me to construct it.

37:54.400 --> 38:00.080
Well, basically what I can do in the terms of like the, what, what, what the, what we

38:00.080 --> 38:06.240
care about for, like quantum algorithms, what you can do is just devise a circuit that

38:06.240 --> 38:07.560
just outputs four.

38:07.560 --> 38:11.840
It sounds like you're saying that we kind of cheat, right?

38:11.840 --> 38:19.760
Like we, if you, if you have some given number or a vector, you could show that there exists

38:19.760 --> 38:25.000
some quantum circuit, you know, that could serve as a RAM that will return this, that could

38:25.000 --> 38:30.840
store this number, but we don't really have a fully functioning system for getting that

38:30.840 --> 38:33.600
number into memory and then retrieving it back.

38:33.600 --> 38:34.600
Right.

38:34.600 --> 38:37.840
So, what I'm saying is, like, you can hard-coded it.

38:37.840 --> 38:38.840
Yeah.

38:38.840 --> 38:39.840
Yeah.

38:39.840 --> 38:40.840
Yeah.

38:40.840 --> 38:45.800
And the thing is that for algorithms that are faster than like O of N, so they're sublitting

38:45.800 --> 38:50.040
your algorithms, you can't hard-code something like that in because it takes too much time

38:50.040 --> 38:51.040
to read it.

38:51.040 --> 38:57.280
And so, you really need the RAM, whereas for other algorithms, you maybe can get away without

38:57.280 --> 38:58.280
using the RAM.

38:58.280 --> 38:59.280
Well, awesome.

38:59.280 --> 39:04.720
And even thanks so much for taking the time to walk me through what you've done and kind

39:04.720 --> 39:06.400
of the broader implications.

39:06.400 --> 39:10.680
It's, you know, it's, as we just said, it's an interesting space.

39:10.680 --> 39:15.240
And because of that, I keep, I'll keep kind of coming back to it and, you know, banging

39:15.240 --> 39:19.560
my head against it a little bit and see if anything manages to permeate.

39:19.560 --> 39:25.320
But certainly what you've done, you know, you got a lot of publicity for your results

39:25.320 --> 39:30.800
back in the summer and it's really interesting work and I appreciate you taking the time

39:30.800 --> 39:32.560
to walk us through it.

39:32.560 --> 39:33.560
Yeah.

39:33.560 --> 39:34.560
No problem.

39:34.560 --> 39:35.960
Thanks for having me.

39:35.960 --> 39:40.000
All right, everyone.

39:40.000 --> 39:41.640
That's our show for today.

39:41.640 --> 39:47.360
For more information on Ewin or any of the topics covered in this episode, visit twimmelai.com

39:47.360 --> 39:50.800
slash talk slash 246.

39:50.800 --> 39:57.800
As always, thanks so much for listening and catch you next time.

