Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week we have a very special interview for you.
Those of you who've been receiving my newsletter for a while might remember that while in
Switzerland last month, I had the pleasure of interviewing Yuggen Schmiedhuber in his
lab Idzia, which is the Dalmahl Institute for Artificial Intelligence in Lugano, Switzerland,
where he serves as scientific director.
In addition to his role at Idzia, Yuggen is co-founder and chief scientist at Nacense,
a company that's using AI to build large-scale neural network solutions for superhuman perception
and intelligent automation.
This is absolutely the furthest I've ever traveled for an interview, but I was in the
neighborhood, so to speak, and boy was it worth it.
Yuggen is an interesting, accomplished, and in some circles controversial figure in the
AI community, and we covered a ton of really interesting ground in our discussion.
So much so that I couldn't truly unpack it all until I had a chance to sit with it after
the fact.
We talked a bunch about his work on neural networks, especially LSTMs, or long short-term
memory networks, which are a key innovation behind many of the advances we've seen in
deep learning and its application over the past few years.
Along the way, Yuggen walks us through a deep learning history lesson that spans 50
plus years, it was like walking back in time with the three-eyed raven.
I know you're really going to enjoy this one, and by the way, this is definitely a nerd
alert show.
A few key announcements before we jump into the show though.
First off, I want to give a big thank you to our friends at Cloudera for sponsoring this
episode.
You probably think of Cloudera primarily as the Hadoop company, and you're not wrong
for that, but did you know they also offer software for data science and deep learning?
They do.
The idea is pretty simple.
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop
cluster is filled with lots of data that you want to use in building your models.
But you still need to easily access that data, process it using the latest open source tools,
and harness bursts of compute power to train your models.
This is where Cloudera's data science workbench comes in.
With data science workbench, Cloudera can help you get up and running with deep learning
without massive new investments by implementing an on-demand self-service deep learning platform
right on your existing CDH clusters.
From a tech perspective, DSW is pretty cool.
It uses Kubernetes to transparently scale workloads across the cluster, supporting our Python
and Scala and deep learning frameworks like TensorFlow, Keras, Cafe, and Theano.
And as of last month's 1.1 release, GPUs on the Hadoop cluster are fully supported.
The folks at Cloudera are so confident that you're going to like what you see, that for a limited
time, they're offering a drone to qualified participants simply for meeting with their
sales representative for a demonstration of the data science workbench.
For your demo and drone, visit twimlai.com slash Cloudera.
And C-L-O-U-D-E-R-A.
Next up, a reminder to register for our online meetup.
If you missed the first meetup, we expect to have the recording online later today.
You'll be able to view it at twimlai.com slash meetup, which is also where you can register
for the next one, which will be held on Wednesday, September 13th.
We're currently finalizing the topic, but if you're registered, we'll notify you as
soon as that's done.
Finally, if you like the podcast, please go ahead and sign up for our newsletter, which
is the best way to keep up to date with me and what's going on on the show.
My picks and thoughts for what's new and important in machine learning and AI for the
week and exclusive promos and giveaways just for newsletter readers.
To subscribe, visit twimlai.com slash newsletter.
And now on to the show.
All right, everyone.
I have the distinct pleasure of being here with Jürgen Schmidt-Huber, who is the scientific
director of the Swiss AI lab Idsia, as well as a professor of Artificial Intelligence
with Usi, the University of Lugano, and Subsi, the University of Applied Science and Art
of Southern Switzerland, as well as being the Chief Scientist for Nacense Company that
he co-founded.
Jürgen has a long history in artificial intelligence and is widely recognized as being a pioneer
in the development of recurrent neural networks and in particular LSTM neural networks, which
we've talked about quite a bit on this podcast.
I'm super excited to be here with you, Jürgen, welcome to this week in machine learning
and AI.
It's my pleasure, Sam.
So I like to get these conversations started by just having you introduce yourself and
tell us a little bit about how you found your way into the field of artificial intelligence.
I can do that.
When I was a boy, I tried to figure out how can I maximize my impact and then quickly
it became clear to me that I have to build something that learns to become smarter than
myself, such that I can retire and such that this smarter thing is going to further self-improve
and solve all the problems that I cannot solve.
And that's what I've been trying to do for the past three decades or so.
So you've got a lot going on, I mentioned a bit of it in the intro.
Tell us a little bit about how you spend your time and some of the research efforts that
keep you occupied.
In many ways, I'm still doing the same thing that I've started to publish on in 1987,
which is both this general purpose learning machine, which learns to better and better understand
the world and learns to exploit that knowledge about the world to become a better and better
and more and more general problem solver.
Over the years and decades, we have made a couple of insights related to fundamental building
blocks that you need to build this full AGI system, but we are not yet done.
And some of the puzzle pieces still have to fall in place in a way that we are trying
to push both at the Institute for Artificial Intelligence, at the Swiss AI Lab.
It's here here and in my company, Nasens, where the goal is really to build a general
purpose AI that learns to do not only one thing in a particular domain, but then learns
on top of that to learn a new profession in another domain.
And then on top of that, yet another thing, such that it becomes more and more general problems
over and even learns the learning algorithm itself, such that it's not always stuck with
the same initial learning algorithm, but learns to improve that method itself, such that
learns to improve the way it improves itself and so on.
And you can imagine that on the way to that goal, there are all kinds of little subproblems
coming up and we always focus on the most promising ones.
Sometimes you don't have to build a full AGI to solve interesting problems.
Sometimes just better patterned recognition will do the job in certain applications and
so patterned recognition, which is a tiny part of the full AI thing, that is something
that has become really commercial in recent years.
Some of the techniques that we have developed in the past decades are now really useful
in commercial applications, such as speech recognition and machine translation and all
kinds of patterned recognition.
So what is the daily procedure?
There is no recipe for that.
You try to make progress here and there and there and sometimes you seem to be in a
dead end.
In fact, most of the time you are in a dead end, but then you backtrack and after 100 dead
ends as and again some sort of progress and that's worth it because from there, then
new search paths are opening up and new dead ends, but also new progress is coming in.
I get the impression that a lot of contemporary machine learning and AI researchers are not
driving towards a vision of AGI, they're solving specific point problems.
Do you feel that pursuing your research in the context of trying to create a generalized
AGI gives you a different perspective or heads you down a different path or gives you
a different approach?
The goal of AGI certainly made a difference to us.
First of all, if you want to build a general purpose learning machine, then you will need
a general purpose computational architecture and in our field of neural networks, that means
a recurrent neural network with feedback connections, which is much more powerful than what most
of my colleagues have been studying most of the time, which are so-called feed forward
neural networks, which don't have feedback connections.
The difference between a recurrent network and a feed forward network is a bit like the
difference between a general purpose computer and a mere calculator because on a general
purpose, a recurrent network, you can run arbitrary sequential parallel programs as opposed to
a feed forward network where you just can shift information from an input to the next
layer and then the next and finally you have a result that is determined through a very
limited feed forward computation where many things that you know from traditional computer
signs are not possible.
So to build a general AGI within your networks, you have to start with a recurrent neural
networks.
And the work that we have been doing in the past decades really has focused on that.
In the late 80s, I started to work on these recurrent networks, which are more challenging
than the feed forward networks because you have to deal with the space of our possible
programs that you can implement on a general purpose computer.
So basically, you have to search in the space of all programs to find solutions to problems.
This makes it hard and soon we ran into problems with that approach and the traditional recurrent
networks didn't really work well and so we had to come up with a couple of improvements
such that you can really use and exploit this power, the theoretical power of recurrent
neural networks.
Before we dive into that, I'd like to further explore this notion of a RNN as a general
computer, as general compute framework.
I think that's not obvious to a lot of people and as an example, I don't know if you've
ever heard of this, FizzBuzz with TensorFlow, does that mean anything to you?
TensorFlow means something, but FizzBuzz does not.
FizzBuzz is a common programmer interview question and you tell the candidate to write
a program that basically, I forget the specifics, but it's something like Prince Fizz when it
runs through a loop of numbers and Prince Fizz when the number is divisible by three and
buzz when the number is divisible by five and FizzBuzz when the number is divisible
by 15.
And so if you know how to do a loop and use mod, it's very easy to do, and someone with
some experience in deep learning in TensorFlow got asked this question on an interview and
they decided to, it inspired them to try to figure out how they would accomplish this
with deep learning.
And in fact, I think the result of their experimentation was that this thing that's
extremely, extremely simple to do with common general purpose programming is very, very difficult
to do with neural networks.
You have to come up with a lot of data, you have to train these networks and they still,
because it's probabilistic, they still get it wrong.
And so with that as context, I'd love to hear you further explore this idea of RNNs
as a general purpose computer.
Yes.
Now, first of all, how do you see that an RNN, a recurrent neural network, is a general
purpose computer, as general as your laptop?
Well, you can take some parts of the recurrent network and wire them up as NAND gates, not
AND gates, NAND gates, very simple, very tiny little subnetworks.
And then you just have to recall that the...
That's what your computer is.
And the CPU of your phone or of your laptop can be emulated by a network of NAND gates.
It essentially is a network of NAND gates, which means you can essentially emulate your
laptop and whatever program is running on it on a vicon network.
In a certain sense, your microchip in your phone is just a very sparsely connected recurrent
network.
So that's step number one.
Let me give you an even simpler example where you see how elegant and powerful recurrent
networks can be in comparison to feet forward networks.
Let's look at the problem of parity.
Suppose somebody gives you N bits of information.
0, 111, 0, and you should classify that as to whether the number of ones in that sequence
is even or odd.
So that's the parity problem.
Now you can indeed wire up a feet forward network, which has, say, 10 different input units
to implement this mapping, which maps the input string to a decision either this is an
odd number of ones or an even number of ones.
And it will take you a rather big network with lots of connections.
And it will never generalize to 11 bits or 12 or something because the size, the input
size is so limited.
Now, with a recurrent network, you can solve the parity problem much more elegantly.
You just feed in the bits one by one into a recurrent network that just has one input
unit and one internal hidden unit and one output unit and maybe an additional bias input
unit, if you know what that is.
And then you have just five connections in this little network and you feed in the input
string, 1 0, 111, 1 by 1 and all the network has to learn to become a flip flop because
whatever, whenever a new one is appearing, then the internal unit that represents what
the network has seen so far just has to flip it state.
And that determines then is the current number of ones, is it odd or even.
So this program for the recurrent network is so simple.
It fits into five connections and you can easily guess those connections.
You just do 1,000 random guesses for the five weights, something between minus 10 and
plus 10 and you test the result on a tiny little validation set, maybe three patterns,
three different parity problems.
And if it works on that, you can be almost sure that it's going to generalize to all possible
parity problems, not only those where there are 10 bits coming in, but also those where
there are five bits coming in, but also those where there are 5,000 bits coming in.
So the natural and elegant sequential solution to the parity problem easily fits into a simple
recurrent network and it needs a really awkward, complicated feed forward network to solve
a tiny part of that for just exactly 10 bits, say, or 15 bits.
So this is to illustrate that the difference between a recurrent neural network and the
feed forward network is like the difference between a general purpose, computational architecture
and a mere calculator.
Is that to say then that the referring RNNs as a general purpose computing system is way
more general purpose than feed forward, but we still have a ways to go in our ability
to train these things in order for them to be truly as general as our current computing
architectures.
Maybe another way to ask that question is, what's the fly in the ointment here?
What are the limitations in the way we deal with these RNNs that prevent us from using them
as general purpose computing system as you kind of assert?
Yeah.
So it's one thing to have a general purpose computing architecture and it's another thing
to run the programs running on that architecture, which are solving your favorite programs.
So to put that into an example, so we know that these RNNs are basically functions to
transform inputs and outputs, and so there's some set of inputs that produces the output
of your arbitrary software program, whether that's Mac OS or what have you, but learning
that function based on the inputs is a whole different story.
It's a whole different story.
The learning of the programs running on your general purpose devices on your recurrent
neural networks, that is the really interesting part.
And people have tried it for a long time, but only with certain tweaks to the original
concept of recurrent networks, it became feasible in practice, and it became so powerful
that it's now all over the place in every smartphone.
So we're going to talk about that in one second, but I'd like you to provide a little bit
of historical context for RNNs.
You started working on these at a time when feed-forward networks were considered the state
of the art, and you've seen the way, you've seen the evolution of RNNs, tells a little
bit about that history.
So maybe let's start with the history of deep feed-forward networks, which go back to
1965.
When I was a baby, I was two years old back then.
And there was this famous mathematician, Eva Knenko, in the Ukraine, and with his student
Lapa, they published this method for training deep feed-forward networks.
They didn't even call them neural networks.
They called it the group method of data handling, but that's what this stuff was.
It was deep multilayer perceptrons, and they found a way of learning internal representations
in these deep multilayer perceptrons, and by 1970, they already had networks with seven
eight layers, and many people built on that later.
And deep in there, at that time, was on the order of ten?
On the order of ten, yes.
And even by the standards of the new millennium, this was deep.
Because most people in the early 2000s didn't use networks that were as deep as those
of Eva Knenko.
That's true.
And he did that 1965.
That was four years before Minskia and Papad wrote a book about the limitations of shallow
networks with one single layer, which is called perceptron, and which is sometimes credited
for killing neural network research in America, because people thought, oh, if these eyes
are so limited, then we shouldn't further progress there, or we shouldn't further work
on that.
And maybe it was a cold war thing over there, on the other side of the Iron Curtain, I
think this was the Soviet Union, wasn't the Ukraine, and was the Soviet Union.
And there were these people who had already deep learning networks back then.
However, then in the 80s, the more general attempts came where you really tried to work
with recur networks, with general-purpose computers, not just feed-forward networks, and you want
to have these recur networks for all kinds of interesting applications, such as, for
example, speech recognition or video recognition, everything where there are sequences.
For example, in the video, you don't just have one single image, which you want to classify.
No, you have a whole stream of images, and every few milliseconds, a new image is coming
in, which means your network somehow has to memorize what it saw before in order to make
sense of the current input within the temporal context of before.
So it has to learn to memorize the important stuff, and to ignore the unimportant noise.
And this proved to be very challenging, and people then already in the 80s realized that
the first attempts at recur networks didn't really work well.
And what specifically didn't work well about them?
They especially failed when there were long time lags between relevant input events.
For example, in speech recognition, suppose somebody says 11, and another guy says 7.
And then the end of that is always 7.
So to see the difference, you have to memorize that he says 50 times steps ago, because
Evan by itself already consumes 50 times steps more or less, because roughly every 10
milliseconds, a new input vector is coming from the microphone.
So you have to look back 50 steps.
And in many other applications, for example, as you are listening to me now, to make sense
of what I'm saying, you have to look much further back in time.
You have to think back of the beginning of our discussion to make sense of what I'm
saying now.
So hundreds of thousands and millions and maybe billions of steps, you have to look back
to take into account the temporal context.
And it turned out that these original networks are the 80s.
They could look back only for five steps, six, seven, something like that.
So it was completely insufficient for doing interesting things.
And this is a problem that can be called the vanishing gradient problem.
And then we try to figure out what is the reason for that.
And my first student in 1991 was Seb Hochreiter.
My first student, Evan, and his task was to figure out where's the problem?
Were you here at the time?
Or...
And we were back then, not in Switzerland, we were in Munich.
Okay.
At the Tech University Munich, T. U. München.
And Seb in his thesis, in his diploma thesis, showed that the problem is that these errors
signals that you are getting through backpropagation in these, we can't do our networks, as you
are propagating them backwards from the output layer towards the previous layer and then
the layer before the previous layer and so on and so on.
They get smaller and smaller in a way that is catastrophic in the sense that the shrinking
happens in an exponential fashion.
So as these signals which tell the system how to change its connections in order to become
a better system, as these signals are being propagated back into time, so to speak, they
are getting smaller and smaller in an exponential way or they explode in an exponential way.
So either the gradients, for those who know what that is, they explode or they vanish and
in both cases, these traditional recon networks cannot learn anything.
So just...
At least they cannot learn to look further back in time than say just a few time steps.
Right.
So just to try to paraphrase that to make sure that I'm understanding.
So the way we solve these deep neural networks is to use backpropagation basically to start
with the output and work our way back to the input layers and we do that using gradients
which essentially, you know, let's say modulate the error that is propagated out to the output.
But if you go back far enough in your network or if your network is too deep, then you're
not getting enough signal back towards the beginning of your network to make the necessary
corrections to improve.
Yes.
That basically what you found.
Yes.
Let's maybe quickly step back a few decades.
In 1970 there was this finished guy, Seppel-Linainmar in Helsinki and he described for the first
time the modern version of what is now called backpropagation, which is a method for adjusting
a deep network of nodes, of computational nodes, such that you can figure out for each
of these nodes and for each of these connections how much did this connection contribute to the
error that you observed at the output of the network.
So at the output of the network, the network is computing some result which is different
from what it should have computed and the difference is called the error.
And now you want to figure out for each connection in your system how much did this particular
connection contribute to this error out there and you want to change then the connection
such that the total error gets smaller.
And Seppel-Linainmar, this guy in Finland, who was a master student back then, he wrote
down this technique of automatic differentiation or today it's called backpropagation.
And then people started using that in the 80s for training neural networks.
And then also generalized it to the case of recon neural networks where you have feedback
connections and where you still have the same basic approach as you are trying to compute
for each connection in a system how much did it contribute to the error at the output side
at some later point in time.
You are moving backwards from the output layer to watch the time step before the computation
of what you find in the output layer and then there are certain things which are called
error signals are being propagated back and then from there they go one step further back
and then from there one step further back and so on until they basically traverse the
entire network in a way that allows you to compute for each connection in the system
how much was it responsible for the final error.
And then through the work of Seppel-Heiter, my first student, 1991 in his diploma thesis
it became clear that the gradients are vanishing these error signals that are being propagated
back they are quickly vanishing which means that although the whole idea is nice in principle
it doesn't work in practice because for a quickly you don't have any signal any longer
which allows you to improve your network such that it becomes a better network.
Where or when did gradient descent come into play in all this as a method for solving
these types of networks.
Seppel-Heiter in my 1970 formulated it as a very general method for all kinds of networks
consisting of differentiable nodes and that paper where he talks about backpropagation
was he also applying gradient descent and did he introduce that then as the method
or gradient descent is a much older technique which goes back at least to Hadamard around
1900 or something like that.
So these are gradient descent is a very old technique.
And then the main question is how do you efficiently compute gradients in complex systems such
as these complex networks where one node is computing something that depends on the
activations of other nodes and then it's broadcasting it's on result to all kinds
of other computational nodes which again compute something you based on these broadcasts
from all the other nodes and then send their results even further and so on.
So once you have a very complex system like that then how do you do the credit assignment
how do you figure out how much should you change this connection there which was active
maybe 100 time steps ago which nevertheless had an impact on what happened 100 time
steps later at the output side.
So that credit assignment problem is essentially is how we find these weights or that's the
problem of finding these the weights for your network.
Yes gradient descent is a very general technique a super general concept from more than a
century ago which can be used to credit a sign in complex systems like these recolonial
networks the individual components of the networks which are the connections and the
weights on these connections so each of these connections has a little number on it which
says how strongly does this neuron over here influence this neuron over here at the next
time step and the numbers maybe 0.2 or minus 0.5 or minus 6.2 and it's contributing to the
error at the output side which was maybe observed a thousand steps later.
And now the question is how can I trace back this influence and how can I then change
this connection such that becomes a better connection and that my entire network becomes
a better network such that the error gets smaller.
And then the efficient way of computing these gradients through automatic differentiation
or backpublication as it is called today that was introduced in 1970 by Sepulina Inma but
then he praised that as a very general thing and he didn't apply it to neural networks
that was then done by other people in particular Paul Werbers around 1982 when he really applied
it to neural networks.
And then in the 80s computers became faster and faster and by 1985 they were about let
you see they were about 1000 times faster than when Linna Inma wrote this down and also
many academic labs for the first time in the 80s started to have really computers to play
around with desktop computers today everybody has that but has a smartphone instead but
back then this was exciting new stuff and then people started playing around and for the
first time with very small networks they learned interesting behavior on certain little
toy problems.
But we couldn't move past toy problems because of this vanishing grading issue which
sub identified in his research paper which led to the creation of LSTM networks tell us
about LSTM and how the creation of those fell out of the findings.
Yeah.
So the long short term memory as it is called the LSTM falls out of this finding because
it basically overcomes the problem of vanishing gradients through a very very simple trick.
All we have to do is have a very stupid simple unit a particular type of neuron in there
which has the simplest activation function one can think of which is the identity function
which is the identity function and then if that neuron is connected to itself by a connection
of 1.0 then at the next time step it will basically replace what it had before as an activation
number for example 0.7 by the same number 0.7 so now you can imagine that if you run
that for a thousand time steps and nothing happens for a thousand time steps and all the
time 0.7 will be stored in this unit.
At the same time as you are later propagating error signals back because there was a difference
between what the network should have done and what it really did.
Then if you know something about gradients then you know that all the time as you are propagating
errors backwards you have to multiply by the derivative of the activation function which
is 1.0 because it is a very stupid activation function which is just the identity function
and then you multiply by 1.0 again through these recurrent connection which also has a
weight of 1.0 and then now it is obviously as long as there are no external perturbations
in little stupid networks like that you can propagate errors back not only five steps
or ten steps but hundreds and thousands and millions and billions of steps.
This is not finished yet because with that simple type of network you have all the limitations
that you get through linear systems because the identity function is just a linear function
and there are many things you cannot learn by just linear functions.
Now that's the reason why you have to surround the little tiny linear unit, the constant error
carousel as we call it, you have to surround it by a cloud of very non-linear units which
we call gates and these gates then basically learn through exploitation of the error signals
which are being propagated back in these simple guys in the center of these LSTM cells
they learn to adjust their own connections through gradient descent such that they open up
at the right moment and let new stuff into these cells into these memory cells
and they close down at other good moments such that the memory in there is protected for a while
until they open up again and let it out such that can influence the rest of the network
and so all of this is now being learned by these long short term memory networks, the LSTM networks
and they are called long short term memory networks because they are basically inspired by what
the biologists know as the short term memory memories of recent events circling around in
form of circulating activations in your brain but it's a short term memory that lasts for a long time
and that's why it's a long short term memory because it can last not only for five or six or
ten steps like in the first original Recon neural networks but it can last forever basically
so that's why it's a long short term memory and LSTM yeah now you said that this was the solution
to everything however it's not quite true because we also had to wait for faster computers
back then already there was an old trend which held at least since 1941 when Suzy built the first
program controlled computer that it really were back then Suzy could do roughly one operation per
second but since then every five years computing became roughly ten times cheaper and then by the
time when Lina Inma for example the test thing that was thirty years later computing was already
one million times faster but it wasn't good enough and then in the beginning of the 90s that
was another twenty years later was roughly ten thousand times faster than during Lina Inma's time
so it was roughly ten billion times faster than during Suzy's time is that correct 40 plus 90
yeah seems correct but it was still not good enough to to be commercially viable but then by
2010 roughly computers were cheap enough and the factor was high enough such that all the potential
in the LSTM networks really could unfold itself and I think since 2015 it's
widely used in commercial applications for example for the speech recognition on two billion
android phones I'll maybe ask you to talk a little bit about the use cases the applications of
LSTM's do you find most interesting but before we get to that has the fundamental technology changed
much between 1995 and 2015 the fundamental insights were really from the previous millennium
one has to say that however there came a couple of really nice improvements around 2000 my
second LSTM PhD student whose name was Felix Gehrs he was the first author on a paper which
introduced something called the Forgetgate which is a particular recurrent gate unit which turns
out to be really useful for many applications where the network has to also learn to forget
sometimes stuff that it has observed in the past to make room for new things and to do that in a
very controlled fashion and so today the Vanilla LSTM as we call it is a little bit different
from the original LSTM and there are a couple of topology evolutions that came in for example in 2009
just in buyer another student of mine was the first author of a paper which showed that you can
evolve through artificial evolution LSTM like architectures which have the basic concepts of LSTM
in there but with which sometimes change a connection here and have a different topology there
and so on that it was in 2009 and there it was possible to show that in certain applications
this type of LSTM topology works better and faster and learns a little bit faster than this type
of LSTM architecture so there has been a couple of improvements of this kind although the basic
fundamental ingredients they did date back to the previous millennium so this this last thing you
were describing the evolutionary LSTM if that's the right way to describe it the core insight
there is basically you've got several different types of structures for LSTMs and you can
based on the you know the problem you're solving or the objectives switch between them on the
flyer or is it something else this is about the basic architecture of an LSTM like network
and if you look at your own brain for example it's a product of evolution and all these little
special types of architectures that we find in your neurons somehow they evolved over millions
of years because some of them just work better and are better at learning certain things which
are important for your survival than others and so the same thing happened there in our artificial
evolution of topology is where you give the system the freedom to come up with new topologies
which which deviate from the original LSTM architecture a little bit because we don't have a
proof that a particular LSTM topology is optimal for all kinds of things okay there is no proof
like that so we try to figure out maybe we can automatically find the optimum topology in a
problem specific fashion because maybe for this problem over here you need a different topology
is better than for this problem over here and so we were able to show them that there are indeed
applications where you can profit from this additional optimization of the topology through evolution
can you elaborate a little bit on the notion of topology what makes a network fundamentally LSTM
and what are the things that can vary from one LSTM topology to another yes so the vanilla LSTM cell
has four little neurons in there there is the central neuron which is the stupid one that I mentioned
before which has just the identity function as an activation function then it is surrounded by
three gates which are multiplicative so there is this thing called the input gate which is a normal
standard neuron and non-linear standard neuron and if that one is active it can completely
open the access to the central unit the constant error carousel or if it is shut down if it is
zero then nothing can flow into that central cell and similar for the output gate
and then this is recurrent gated unit the forget gate which basically can manipulate the self
connection of the stupid cell of the cell with the linear activation function to itself and can
make the self forget stuff that was stored there for a while and can learn to do that on
now you can come up with all kinds of variants of that topology maybe you have a little connection
going out directly from the central cell to these gates and these are called peephole connections
that's what Felix Ghears also called them my second LSTM student in 2001 roughly 2000 something
like that you can have additional modifications of the architecture and sometimes it's useful
because as I said there is no proof that a particular LSTM topology is optimal so you might want
to use the principles of evolution to search for good topologies automatically and to take the
existing one which is pretty good for many applications vanilla LSTM is really good for many
applications but even further improve it so that is the approach behind that okay you talked a
little bit about this but if we can take a step back what was the kind of intuition for all of
the you know for the LSTM cell architecture right at the point that that was developed that
and correct me if I'm wrong here I think of traditional RNNs as being just a lot flatter whereas
LSTMs have these cells that are doing all this funky stuff is that do you think about it that way I like
yeah so in it is true that the traditional become networks are very straightforward it's just a bunch
of nodes with non-linear activation functions and everything is connected to everything and at every
time step each of these little nodes is computing essentially the weighted sum of all the connected
guys at the previous time step super basic it's very simple and basic and beautiful also because
everything that simple has a tendency to be beautiful except that it was not quite it was a little
bit too simple so you need a little bit of additional structure to make sure that these memory
effects hold now the LSTM is more complicated but it's also very simple because you can write it
write it down in five lines of code five lines of pseudo code are sufficient to explain it so
maybe a traditional recon network you can write it down in one line of pseudo code and this one needs
five lines of pseudo code maybe something like that and still the principles are very simple
because you have a very stupid simple linear cell at the heart of each of these LSTM cells and then
you surround it by a cloud of non-linear gates such that these gates can learn to open access
or close access to these memories and the network itself can learn to use these gates to
put important stuff into short-term memory and ignore noise and so on yeah well as Einstein said
a long time ago you should make things as simple as possible but not simpler yeah very good to what
degree does the LSTM you know there are some related I think concepts that come up in deep learning
like attention and things like that how does how does attention for example relate to LSTMs yeah
so any recon your network of course already has something like internal attention
because what it can do is essentially it can learn to direct internal spotlights of attention
if you will to certain parts of the network we can say let's highlight this part of myself
and let's ignore this part of myself so in a way the internal computation which is based on the
program that is implemented on the recon network in form of its weight matrix this attention can be
learned in a problem dependent fashion and so I think my first paper on that was in 1993 where
basically a recon network learn to direct its internal focus of attention is is that the
correct plural the plural of focus focus is it focus okay so the focus of attention and then could
could use these highlighted internal patterns in hand like fashion such that it could associate
through something called fast weights these internal attention highlighted patterns such that it
could do certain things that you cannot do with traditional networks because it had these
extra these extra fast weights which is actually a topic that has become really popular again
very recently so attention is something that was implicit in many many recon networks already
in the in the past and now it is it is waking up in very interesting commercial applications
again speaking of commercial applications we talked about a few of the commercial applications
of LSTMs are there others that come to mind for you that exemplify its you know power flexibility
interest yeah so I think it's interesting to see that not only speech recognition but also
the next higher level natural language processing can be done by the same architecture so with
speech for example every 10 milliseconds a new vector of numbers maybe 14 numbers or something
streaming into the system from the microphone with language it's quite different there you already
have letters and words and so on and now on this higher level which is basically derived from this
elementary level of speech you again can use LSTMs to for example understand text for example read
some document and then try to make a short summary or classify that document maybe say the
the document is a CV of a person and you want to compare to another document with which is a
job advertisement and then you classify the document with respect to the job ad and you say okay
this guy's a match or not machine translation maybe the most visible application is now what
Google is doing in since 2016 since november 2016 Google translate is not based on the old
system anymore but it has LSTM at its core and it has become much better than used to be
and especially the most important language pair in the world which is English to Mandarin Chinese
and back that is they're the performance is much better there was a time when the Chinese they
laughed at the translations and they're not laughing any longer and then the same thing can be used
for example with slight modifications to segment images which again seems like a totally different
problem right the same thing can be used to well that seems again closer to natural language
processing can be used to train chat parts so you have lots of chats between A and B and you
then just train your LSTM to imitate B whenever it answers something to A but maybe you have lots
of A's and B's and so it can learn from many many different chats and so it's only even relevant
for this old idea of an AI test which is called the touring test which is about chatting with some
other partner and the question is is he a human or is he a machine right and so let's see where
that is going to end at some point then the same LSTM also can be used for all kinds of other
sequences such as music composition one more people again interested in that Doug Eck in my lab
about more than 10 years ago was the first who applied LSTM to music composition where the gold
was to overcome the traditional neural composition neural music composition which existed back then
already which was able to learn the basic harmonies but then if you listen to compositions they
sounded like music like stuff that in principle for example when you train it on certain pieces by
Bach which is a favorite of many of these neural network guys then even in the 80s and 90s it was
possible by Mike Moser and Pete Todd and a couple of guys like that to learn the basic harmonies
that you find in this type of music and then come up with randomized version there of compositions
of the network so to speak which sound loss are like kind of this type of music except no there was
no overarching structure so more like elevator music versions of Bach or something and then the
gold was to also learn high level structure that you find in many songs like first section eight
and there's another section eight and there comes B then there comes A again and then again B
something like that yeah that was Doug Eck in in the early 2000s and it was one of your students
yeah yeah yeah he was now at Google he is now at Google and he's running the what is called
magenta magenta I just envied him last week okay he was a postark here actually oh wow yeah okay
and he was the first author on these LSTM for music papers yes 2003 I think right yes wow
great great so maybe as a way to to pull the conversation back to your broader research around general
artificial intelligence or artificial general intelligence depending on your preference
you did an AMA on Reddit not too long ago actually it was a couple of years ago maybe yeah
yeah and one of the questions that I thought was pretty interesting was what are some
beliefs that you hold that are controversial in essence that was the question and one of those
was that intelligence is actually simple although we think of it as rather complex can you elaborate
on that a little bit yes I think I can do that so LSTM by itself is nice for patent recognition
and for doing really complex applications but we should see that it's just a patent recognizer
so the full AGI thing requires more it requires reinforcement learning in an unknown
partially observable environment where there's no teacher who tells you what to do and where you have
to maximize your future expected reward okay and so whoever can solve that problem in a general
fashion has solved the the grand problem of AI and that's why almost all of our research
of the past 30 years really focused on that and so to us the LSTM which has now become popular
is just a side effect that's just a byproduct of this more general work you can use the LSTM
to build a model of the world to predict what's going to happen if you do that and that but you
still need another module which is learning which is inventing new experiments and trying to
figure out which sequences of actions lead to success and which don't and that leads to this
field of reinforcement learning with general purpose recurrent architectures with recurrent
works basically now if you look at the LSTM it's just five lines of pseudo code so it's very simple
it's not the full true AI thing yet because there you have to have the full loop through the
environment act perceive act perceive act perceive maximize future reward or reward until the end
of your lifetime and for that I sometimes speculate we need another five lines
and why am I so optimistic because we understand in many ways how to train the separate action
module and how to combine that with a model of the world which can be used through the five lines
of something like the LSTM or something that is in the same ballpark at least right and I'm
further motivated to believe it's very simple because here in my lab in the early 2000s at least
we already have found certain theoretically optimal journal problem solvers which are also very
simple so there are many computer scientists actually don't or machine learning scientists don't
even know that but there is an asymptotically optimal way of solving all kinds of computational
problems and this goes back to Marcus Hutter in 2002 here in my lab he was a senior researcher
now he's a professor in Australia and he wrote down a very simple method which solves any
well defined problem as fast as the unknown fastest meta program for solving that kind of problem
save for a constant factor or a for say for an additive constant which does not depend on the
size of the problem so when you have for example a traveling salesman problem which is about
finding the shortest path through n cities where each city can be visited only once then as the
number of cities n grows the problem becomes larger and larger but it's still just a struggling
salesman problem now we do not know the best way of solving traveling salesman problems but
suppose you can solve it and suppose there's an unknown method which solves it in n to the 17 steps
then this method of Marcus the fastest and shortest algorithm for all value fine problems as it is
called will also solve it in n to the 17 steps plus a constant number of steps and overhead
which however is constant and does not depend on n so as n grows into the 17 grows even much more
crazily and the constant overhead vanishes which means that almost all problems are already solved
in optimal fashion because almost all problems are large problems they're just a few problems that
are so small that the overhead still plays a role just so I can make sure I understand what
what he did and what you're saying is the idea that if a problem can be solved that basically
it can be transformed into some other kind of solution modulo a constant so what I'm saying is
you have a specification of a problem and then you want to find a program that solves the problem
to approach that you want to build a general problem solver which finds that program automatically
now there is a general problem solver which is the fastest and shortest way of solving all
well defined problems by Marcus who published in 2002 here in my lab which essentially does that
and it's optimal however only in an asymptotic sense what does that mean if you're problem for
example you're traveling salesman problem with n cities can be solved and into the 17 steps
then this super algorithm of Marcus is also going to solve it and into the 17 steps plus
all of one as the computer scientists say and all of one means there is a constant overhead associated
with it now the constant overhead is large it turns out because there's a proof search involved
and we hide all the complexity of that in a constant which can be done no matter even if the
constant is large then as n is getting larger and larger as the size of the problem is getting
larger and larger the constant pales in comparison sure and that's the reason why you already have
since the other millennium an optimal way a mathematically optimal way asymptotically optimal
way of solving all kinds of problems especially the large problems which are so large that we
can ignore the overhead the constant overhead does this result just say that we can asymptotically
solve them it doesn't necessarily tell us how to do that no it says it is a constructive method
it really tells us how to do that why I'm not doing that all the time because the small problems
that we are looking at here on this planet in our daily lives they are so small that the the
constant overhead does play a role and that's why we are still doing sub optimal things such as
deep learning and all kinds of things you know but at least from a mathematical perspective
we already know there is a mathematically optimal way of solving all kinds of problems
in the fastest possible fashion the fastest possible in a non practical way but at least
an asymptotically optimal way we already have that and it's very simple it's a very short thing
so in that sense you gain additional motivation to believe that in the end the whole
AI thing is going to be really simple and in hindsight we will look back and we'll say we can't
believe that it took so many thousands of years to understand how to understand and how to solve
problems automatically so this tells you that by saying that if we can get far enough along with
compute power sophistication and approaches to overcome this constant then we're there like we
can solve anything is that the idea not quite because we really have to do is try to find a
similarly simple thing for the small problems too and one step in that direction is the so-called
girdle machine which I published in 1993 in 2003 which which you can initialize with Markus's
algorithm but also with other algorithms and which essentially learns to rewrite itself in an
optimal fashion once it has found a proof that the rewrite is going to improve its performance
in a way that is not only asymptotically optimal but generally optimal so however also this is
not yet practical at the moment however getting out of that is that the very general
problem solvers may be very simple and can be written down in a few lines of code
we still don't have the few lines of code that we need for a practical general problem solver in
this universe but I think we are close and the puzzle pieces are starting to fall in place
and I hope I will see it in my lifetime awesome maybe in the next few years awesome I've really enjoyed
the opportunity to talk with you I've certainly learned a lot for folks that want to learn more
about what you're up to what's the best way for them to do so it's probably to look at my sprawling
website which contains more information than you ever wanted to see about us but not only has
the original papers but also overview pages which try to explain in rather simple terms
where the experts might want to study in detail in the original papers great and we'll include
a link to that in our show notes that will be so awesome great well thank you so much Erick
it was my pleasure send great
all right everyone that's our show for today thanks so much for listening and for your continued
feedback and support for the notes for this episode including links to you again and the many
resources mentioned in the show head on over to twimmalei.com slash talk slash 44 please be
sure to comment there with your feedback or questions also please note if you share your
favorite quote with us via a comment or via twitter we'll send you one of our fab laptop stickers
i'll be in san francisco september 18th through 20th for the artificial intelligence conference
and i hope to see you there too if you've already registered send me a shout on twitter and let me
know if you're still interested in coming but haven't registered yet we've got a link
and discount code on the show notes page good for 20% off most conference packages the following
week i'll be at strange loop a great conference held each year right here in st louis strange loop
is a multi disciplinary conference that brings together the developers and thinkers building
tomorrow's technology and fields such as emerging languages alternative databases concurrency
distributed systems security and the web will link to the conference on the show notes page as well
finally another huge thanks to this show sponsor cloudara for more information on their data
science workbench for the schedule your demo and receive your drone visit twimmalei.com slash cloud
dera thanks again for listening and catch you next time
