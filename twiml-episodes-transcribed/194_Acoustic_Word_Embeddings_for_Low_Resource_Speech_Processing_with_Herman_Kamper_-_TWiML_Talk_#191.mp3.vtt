WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.560
I'm your host, Sam Charrington. In this episode of our Deep Learning and Daba series, we're

00:34.560 --> 00:39.760
joined by Airman Kemper, lecturer in the Electrical and Electronics Engineering Department

00:39.760 --> 00:45.920
at Stellan Bosch University in South Africa, and the co-organizer of the endaba.

00:45.920 --> 00:51.600
Airman denied discuss his work on limited and zero resource speech recognition, how those

00:51.600 --> 00:56.480
differ from regular speech recognition and the tension between linguistic and statistical

00:56.480 --> 01:02.320
models in this space. We dive into the specifics of the methods being used and developed in Airman's

01:02.320 --> 01:11.040
lab as well, including how phoneme data is used for segmenting and processing speech.

01:11.040 --> 01:15.400
Before we jump in, I'd like to send a big shout out to our friends at Google AI, which

01:15.400 --> 01:20.520
recently opened up applications for its 2019 residency program.

01:20.520 --> 01:25.320
The Google AI Residency is a one year machine learning research training program with the

01:25.320 --> 01:29.840
goal of helping individuals from all over the world and with a diverse set of educational

01:29.840 --> 01:35.280
and professional backgrounds who come successful machine learning researchers. Find out more

01:35.280 --> 01:46.240
about the program at g.co slash AI Residency. And now on to the show.

01:46.240 --> 01:51.320
Alright everyone, I'm on the line with Airman Kemper. Airman is a lecturer in the Electrical

01:51.320 --> 01:57.320
and Electronics Engineering Department at Stellan Bosch University in South Africa and one

01:57.320 --> 02:02.000
of the organizers for the recent Deep Learning and Daba event. Airman, welcome to this week

02:02.000 --> 02:05.680
in machine learning and AI. Oh, thanks a lot for having me.

02:05.680 --> 02:12.400
I guess we should start with the endaba. The event just finished. I saw an incredible

02:12.400 --> 02:19.480
flow of tweets from the event. It looked amazing from your perspective as an organizer. How

02:19.480 --> 02:20.480
did it go?

02:20.480 --> 02:26.520
I think it went pretty well. I think obviously as the organizers were a lot more aware of

02:26.520 --> 02:34.600
the things that might break behind the scenes but overall it was really successful. I enjoyed

02:34.600 --> 02:39.520
it a lot and from everything I've heard from all the people that was here, I think that's

02:39.520 --> 02:43.200
where we got the biggest encouragement. Just people telling us how much they enjoyed

02:43.200 --> 02:46.920
it and how much they got from the event. So I think it went well.

02:46.920 --> 02:49.840
And the highlights from your perspective?

02:49.840 --> 02:56.480
That's very difficult. I think maybe three things that really stood out for me. It was

02:56.480 --> 03:04.080
two talks. The one was by King Yongchou. He led sessions on basically the fundamentals

03:04.080 --> 03:11.040
of building natural language processing systems and to me was just amazing. The amount of

03:11.040 --> 03:16.400
kind of dense content that he packed into a lecture but then also giving kind of his

03:16.400 --> 03:21.640
high level overview of kind of where the field is going. That was one big thing. The talk

03:21.640 --> 03:29.760
by Jeff Dean at the end, that was just amazing. And then the post decisions which was basically

03:29.760 --> 03:36.280
students from across Africa presenting their work and I've been lucky to be at international

03:36.280 --> 03:41.320
conferences. And this post decision at this African event was just amazing to see the

03:41.320 --> 03:46.320
quality of the work and that the students here are doing.

03:46.320 --> 03:52.040
That's fantastic. Before we dive into some of your research, why don't we have you introduce

03:52.040 --> 03:59.120
yourself to the audience? You are relatively new on the faculty at Stellan Bosch. Tell us

03:59.120 --> 04:00.960
a little bit about your background.

04:00.960 --> 04:07.360
Yeah. So I'm actually a Stellan Bosch boy kind of born and bred. I grew up here. I did

04:07.360 --> 04:14.560
my undergrad and a master's year. And then I also worked for a bit and then I went to

04:14.560 --> 04:19.840
Edinburgh to do my PhD there in the School of Informatics. And that was really amazing in

04:19.840 --> 04:24.480
Scotland. Really struggled with the weather there but other than that, it was an amazing

04:24.480 --> 04:33.360
experience. And then I did a postdoc in Chicago for relatively short time for about a year

04:33.360 --> 04:41.840
at Toyota Technological Institute. And I grew up here. So as soon as I left, I kind of

04:41.840 --> 04:47.200
was campaigning already to come back. I really saw my kind of long-term goals to come back.

04:47.200 --> 04:52.560
And yeah, while I was doing my postdoc, I was very happy to get the appointment here.

04:52.560 --> 04:57.680
Actually, I have a question about TTI Chicago. Is that relatively new? Or is it affiliated

04:57.680 --> 05:01.680
with another school? I lived in Chicago for many years and never heard of it.

05:01.680 --> 05:09.360
Yeah, so it's fairly new. It's really like an institute. It's kind of a lab. It was

05:09.360 --> 05:16.560
funded by Toyota initially, but now it's kind of on its own. And it's a lab that's affiliated

05:16.560 --> 05:23.600
with the University of Chicago. And it's kind of a research-only institution. So you can do

05:24.080 --> 05:30.320
PhD at TTI Chicago. And they do a bit of postgraduate teaching as well. It's actually an amazing

05:30.320 --> 05:35.920
place for specifically machine learning. It's quite a small group, but it's very tight net.

05:35.920 --> 05:42.240
So you have great people in kind of theoretical machine learning, theoretical learning theory,

05:42.240 --> 05:48.080
and then also speech processing, natural language processing, and computer vision.

05:48.880 --> 05:56.640
So Hermann, your research is focused on speech processing. In particular, you're concerned with

05:56.640 --> 06:02.320
how to do speech processing when you don't have a lot of resources. Tell us a little bit more

06:02.320 --> 06:09.920
about what that means. Yeah, so actually this already started while I was doing my masters.

06:11.360 --> 06:16.320
When you want to do, when you want to build speech processing systems, which I think can really

06:16.320 --> 06:22.240
improve people's lives, this works well for English and for German and for Spanish,

06:23.040 --> 06:27.760
because big companies are invested in these languages. And what they do is they collect a lot

06:27.760 --> 06:34.560
and a lot of annotated data. So they get people to say things and then they transcribe it as well.

06:34.560 --> 06:43.520
They give labels to basically the input speech. But there's so many languages on earth that

06:43.520 --> 06:51.520
it's basically impossible to do this for all the languages. So my research focus is really on

06:51.520 --> 06:58.800
very low resource languages, languages for which you just don't have that much data and annotate

06:58.800 --> 07:04.880
the data. So data with labels and actually languages for which you where it might actually be

07:04.880 --> 07:12.560
impossible to get these type of annotated resources. So there's a large proportion of the languages

07:12.560 --> 07:16.960
spoken on the earth that doesn't even have a written form. So you can't write them down.

07:16.960 --> 07:24.800
And if we want to build speech systems for these type of languages, or if you want to maybe kind

07:24.800 --> 07:30.560
of preserve or document these languages, a lot of these languages are dying out. And you want to

07:30.560 --> 07:36.320
build kind of speech processing systems that's able to look at these data sets, then you're in

07:36.320 --> 07:41.360
the setting where you basically don't have any labels at all. You just have a collection of speech

07:41.360 --> 07:47.120
audio. And what you're trying to do is kind of find this structure in the raw audio and to do this

07:47.120 --> 07:53.600
without any form of supervision, any signal. So we call this unsupervised learning. I think a lot

07:53.600 --> 07:59.760
of the listeners might know this. And kind of try to find the raw structure in the audio without

08:00.400 --> 08:07.120
without any guidance. And this really has application. It kind of has this double motivation.

08:07.120 --> 08:13.280
The one motivation is that if we can crack this problem, we can build speech systems in settings

08:13.280 --> 08:18.960
where it was just not possible before for languages that we just can't build systems for at the

08:18.960 --> 08:24.160
moment. And then the second motivation is that a lot of the people that's interested in these

08:24.160 --> 08:29.760
kind of unsupervised model are also interested in how humans learn language. Because human infants

08:29.760 --> 08:36.000
in a sense, they never see any texts labels. They don't get any hard supervision. They're kind of

08:36.000 --> 08:42.160
just bombarded with the stream of audio. And from that, it's kind of a miracle how they then

08:42.160 --> 08:47.920
start to learn language kind of automatically from just this raw sensory input. So people,

08:47.920 --> 08:52.400
they call it zero resource speech processing sometimes. And people that's interested in this

08:52.400 --> 08:58.400
area normally as this double motivation of either building speech systems, practical systems,

08:58.400 --> 09:03.760
or actually using these type of models to investigate language acquisition in humans.

09:03.760 --> 09:09.760
When I think about the trajectory of speech recognition systems over the years, there was this

09:09.760 --> 09:16.640
transition from kind of strongly linguistic based models to more statistical models.

09:18.400 --> 09:25.120
Does the fact that we don't have labeled data kind of push us back towards linguistic

09:25.120 --> 09:31.360
based models or are we still able to operate in the statistical domain? That is an extremely

09:31.360 --> 09:35.440
interesting question. I think a very, very good question because it's something that I think

09:35.440 --> 09:40.720
this unsupervised community, this unsupervised speech community is actually struggling with,

09:40.720 --> 09:47.040
is this question. And I think, so I can kind of say from a practical viewpoint, a lot of the

09:47.040 --> 09:52.480
systems are still very of the, how could it zero resource speech processing systems, although

09:52.480 --> 09:58.720
I might not like the term, but a lot of these zero resource systems still operate in the statistical

09:58.720 --> 10:06.560
domain. So you can think about that is if I give you a ton of data or a collection of data and I

10:06.560 --> 10:13.360
ask you to describe it statistically, then that's basically what it boils down to. At the same time,

10:13.360 --> 10:19.600
I think people in this field are becoming more and more interested in kind of what is the structures

10:19.600 --> 10:24.720
or what is the small things that you need to build into these systems in order for it to actually

10:24.720 --> 10:31.520
learn something. So you might have a statistical system, but you can put in specific cues

10:32.240 --> 10:38.080
that it can, can pick up on. And I think the reason this question is so interesting is,

10:38.080 --> 10:43.280
if we can figure out what type of cues, what type of extract structure, what type of biases

10:43.280 --> 10:49.040
we need to build into our unsupervised models for it to actually learn language, if we can answer

10:49.040 --> 10:53.920
those questions, what are kind of the minimal things that we need to put in, then that might tell

10:53.920 --> 11:01.680
us something about how the type of cues that humans, human infants use to do this. And it's really

11:01.680 --> 11:07.120
interesting because it's also the converse. So people actually are looking, people in this field,

11:07.120 --> 11:15.040
keep a close eye on cognitive studies, cognitive psychologists, who actually try to answer these

11:15.040 --> 11:21.120
questions on infants. So they, we try to read a lot of literature from that side, which tell us

11:21.120 --> 11:27.840
what type of cues do humans use. And in the cognitive literature, there's a lot of studies that

11:27.840 --> 11:32.960
look at what are the cues that infants use. And how can we use some of those ideas and build those

11:32.960 --> 11:39.680
into our systems. I think really what this community is doing is actually seeking the answer to

11:39.680 --> 11:44.720
that question. What are the things that we need to explicitly build in and what should we just

11:44.720 --> 11:53.280
let the model learn? Right. I think it sounds like in, in this field, as in some other areas in

11:53.280 --> 11:58.640
machine learning, there's kind of a pendulum that's swinging that, you know, we started with

11:58.640 --> 12:04.720
these very strongly, you know, physics-based or model-based approaches. And then we kind of swung

12:04.720 --> 12:10.720
hard to statistical-based approaches. And now folks that are kind of on the frontier or, or many,

12:10.720 --> 12:16.560
folks, not all are trying to figure out ways to incorporate the models back into the statistical

12:16.560 --> 12:20.560
approaches to kind of get the best of both worlds. And it sounds like that's what's happening

12:20.560 --> 12:28.160
here as well. Yeah, I think that's right. There's actually a very interesting kind of avenue that

12:28.160 --> 12:32.960
people are starting to explore in this area. And I'm also working on this a little bit.

12:34.080 --> 12:39.760
So infants, they're not just bombarded with raw audio. Of course, infants also have a lot of

12:39.760 --> 12:47.280
other senses, right? Touch and also vision. So there's a group of people, and we're also working

12:47.280 --> 12:53.280
on this, that kind of look at if you have, for example, a speech signal, but it's paired with the

12:53.280 --> 13:00.960
image. So you have a spoken signal. So it's not a written caption, it's a bit of speech, but it

13:00.960 --> 13:08.880
describes an image. Does that image actually allow you to more easily kind of learn the words that

13:08.880 --> 13:13.680
use them in the language? So you might have a picture of people skiing or something, and then

13:13.680 --> 13:19.360
someone describing that image in speech. And then you can kind of use that image to ground the

13:19.360 --> 13:26.560
things that you're discovering. So I think apart from thinking about kind of these linguistic

13:26.560 --> 13:32.880
insights, there's also just this general question of how can we glue different medalities and

13:32.880 --> 13:38.080
different signals together? And that very much operates in a statistical point of view. But again,

13:38.080 --> 13:42.960
we need to figure out what are the specific things that we need to build into these models

13:42.960 --> 13:47.120
for it to actually learn, because it's a very, very hard task if you don't have labels.

13:47.120 --> 13:52.160
It is really interesting how this pendulum actually sues. And I think in the supervised case,

13:52.160 --> 13:59.440
if you have a lot of labeled data, then following a kind of a very pureist, let's just learn from

13:59.440 --> 14:05.520
the raw data approach and learn to predict these labels that we know are important. I think that

14:05.520 --> 14:10.800
makes a lot of sense. But if we're kind of going to this low resource case and generally

14:10.800 --> 14:15.440
machine learning actually, I think there's a lot of evidence that shows that if you don't have

14:15.440 --> 14:20.880
so much labeled data, then building instructors actually helps you in the short term.

14:21.520 --> 14:28.880
You mentioned that the zero resource speech recognition community follows closely what's

14:28.880 --> 14:35.040
happening in the cognitive science community. Are there specific examples of insights from that

14:35.040 --> 14:43.920
community that have advanced the state of the art in your community? That is also a very good

14:43.920 --> 14:51.520
question. And I didn't say, we follow them. I said, we try to follow them. We should follow them

14:51.520 --> 15:00.240
a lot more. Actually, Emmanuel Depou is a researcher in France. And he really is at this intersection

15:00.240 --> 15:06.080
of these two communities. And my supervisor Sharon Goldwater in Edinburgh, she's also very close

15:06.080 --> 15:12.000
to the intersection of these communities. So there's definitely an overlap. I'm trying to think

15:12.000 --> 15:23.600
of concrete examples. I think from, I'll give two quick examples. So actually, and I might not

15:23.600 --> 15:29.760
do it completely justice, but I'll try. There's a lot of evidence that shows that infants can actually

15:29.760 --> 15:37.280
pick up things like syllables. Even before they understand a complete language, they are able to

15:37.280 --> 15:45.280
figure out these puffs, these little bursts of energy, which are syllables. Actually, if you

15:45.280 --> 15:50.800
listen to a language that you don't understand, you would probably, I don't know, maybe you speak

15:50.800 --> 15:57.920
Japanese, but if you listen to Japanese or African, so Tetsonga, you would probably be pretty good

15:57.920 --> 16:04.480
in figuring out where syllables start and end. And human infants are able to do this. So there's

16:04.480 --> 16:10.960
a number of researchers which have tried to explicitly build these things into their models

16:10.960 --> 16:17.200
to explicitly use syllables, or we can kind of use that to guide the systems. And that's one

16:17.200 --> 16:25.040
example. In our own work, what we've been doing is, so there's actually also a lot of evidence

16:25.040 --> 16:31.520
that even before infants can distinguish fine-grained phonemic categories, so this is like the

16:31.520 --> 16:37.760
minimal sound units, you know, vowels and consonants and so on, before they can actually distinguish

16:37.760 --> 16:42.880
these things, or while they're learning to distinguish these sound categories, they can already

16:42.880 --> 16:48.880
identify reoccurring word patterns in speech. So if they keep on hearing specific words,

16:48.880 --> 16:58.480
they can start to identify kind of larger spanning chunks that reoccur. And it's quite interesting

16:58.480 --> 17:03.840
how psychologists test these things in the lab, so they would teach children essentially,

17:03.840 --> 17:08.960
or human infants, like something like Klingon, which obviously the child hasn't heard before,

17:08.960 --> 17:14.400
hopefully. And then they would use eye-tracking experiments to figure out whether the child

17:14.400 --> 17:19.600
has actually learned a specific word. Now, we know from these lab experiments that children are

17:19.600 --> 17:25.760
able to figure out longer spanning word segments. And what we've done in our own work is we've said,

17:25.760 --> 17:32.160
well, okay, if a child can do this, can we actually build that into our model? So maybe you don't know

17:32.160 --> 17:36.720
everything that's going on in the language, but maybe you can run a kind of unsupervised system

17:36.720 --> 17:42.880
that identifies longer spanning words of phrases. You identify these and then you use these as a

17:42.880 --> 17:50.480
signal downstream in building a statistical model or a neural network model to kind of use that

17:50.480 --> 17:58.880
information. And that has proven to be very successful. So when you think about the approaches that

17:58.880 --> 18:08.240
are required to do the zero resource speech recognition, can you walk us through the various

18:08.240 --> 18:15.280
elements of it and how it differs from the way we might approach speech recognition traditionally?

18:15.920 --> 18:22.640
Yeah, that's also a good question. So it's quite of kind of interesting, because this is a

18:22.640 --> 18:28.160
fairly young community. When I started my PhD, there were maybe two groups working on this,

18:28.160 --> 18:33.040
and now all of a sudden, there's like, I don't know, 10, which doesn't sound like a lot, but

18:33.040 --> 18:40.400
in the speech community, that is actually a kind of a big growth. And it's kind of interesting.

18:41.040 --> 18:46.800
I just came back from just before the end of our, we had inter-speech in India, and there

18:46.800 --> 18:53.040
I went to a workshop, spoken language technologies for under-resourced languages. And it was kind of

18:53.040 --> 18:58.800
interesting there, if you look at the type of techniques that the zero resource community is using,

18:58.800 --> 19:05.280
it's a little bit all over the place. And I think it's because we're kind of trying to redefine

19:05.280 --> 19:10.320
the stream that you need to follow, the type of techniques that you need to use, because we're

19:10.320 --> 19:15.520
seeing that a lot of the standard supervised speech processing techniques are just not working.

19:16.480 --> 19:22.400
If you don't have label data, then the trends we're seeing is very different from when you have

19:22.400 --> 19:28.480
label data. To actually answer your question, you've asked just about kind of the little steps

19:28.480 --> 19:36.080
that we take. I think at the moment, it kind of seems like we're converging to maybe two big

19:36.080 --> 19:42.480
important problems that we need to crack first. If I just give you a corpus of audio and Yini's

19:42.480 --> 19:49.120
kind of process there, then the first step you need to do is figure out good what we call features

19:49.120 --> 19:55.600
at the fine grain level. So kind of standard supervised systems will always start in the same way.

19:55.600 --> 20:00.720
They kind of break a speech stream up into these small windows and you hope that the signal

20:00.720 --> 20:08.080
is stationary in that window. And in a supervised system, what we're seeing now is if you break the

20:08.080 --> 20:15.200
speech stream up in this way, and then you feed this into a deep neural network and you tell it that

20:15.200 --> 20:21.200
in this little window, this speech sound occurs, then what the system can do is it can kind of

20:21.200 --> 20:26.960
learn what should it extract from this little chunk of speech to identify a particular sound.

20:27.760 --> 20:32.400
And in the zero resource community, it's very similar. The first step is we need to break it up

20:32.400 --> 20:38.640
into these little overlapping windows. But then our challenge is we need an unsupervised method

20:38.640 --> 20:44.240
to kind of figure out what is it that makes these little speech sounds. And there people are

20:44.240 --> 20:49.200
kind of either using kind of classical echolag Gaussian mixture model based approaches, which

20:49.200 --> 20:55.440
kind of tries to identify these small acoustic units, these kind of sub word units. And then

20:55.440 --> 21:02.240
another group of people are using unsupervised neural networks. And then once you've got features

21:02.240 --> 21:07.760
at this kind of fine grain level, then you need a system on top of that to kind of try and

21:07.760 --> 21:16.320
glue these features together to find structures corresponding to bigger units, things like syllables,

21:16.320 --> 21:23.760
and then words, and then ultimately sentences. And for that, it's also mixed. Some people use

21:25.360 --> 21:30.960
kind of more classical Gaussian mixture based models and unsupervised heat and mark of models.

21:30.960 --> 21:35.680
And I think there's a big push to try and get these neural models, which are working so well

21:35.680 --> 21:40.480
in the supervised case, to also kind of work very well in the zero resource case,

21:40.480 --> 21:46.080
using neural models to discover these word units and sentences ultimately.

21:46.800 --> 21:54.480
Okay, so let me try to recap that to see what I was able to capture. So you've got the speech signals,

21:55.360 --> 22:04.080
and traditionally we'll like window those and try and within a given window, feed that into

22:04.080 --> 22:10.720
a neural network, let's say, along with the label, and then essentially train that neural network

22:10.720 --> 22:17.120
to match those labels to those little segments of speech. And that's more of this right. Yeah.

22:17.120 --> 22:25.840
In this world, we don't have the labels. So we're kind of capturing these windows. And we're using

22:25.840 --> 22:33.120
some number of techniques. Gaussian mixture models are one and the neural networks are another.

22:34.320 --> 22:39.120
But we're trying to identify patterns within these windows, but are we?

22:39.840 --> 22:44.320
So we're kind of comparing across windows in order to do this, is that right?

22:44.800 --> 22:50.800
Yeah, that's right. And I think you're stating three things very carefully. I think that's good.

22:50.800 --> 22:55.840
I think the ideal case what we want is in these kind of short windows,

22:56.480 --> 23:04.480
we want to find a representation which captures something like phonemes, kind of or at least

23:04.480 --> 23:10.800
the small set of units that's used in a particular language. So if I give you a big corpus of

23:10.800 --> 23:17.440
audio, what you want to find is, can I find features that tells me this language uses R,

23:17.440 --> 23:24.640
R, R, R, okay, and in some other language, they might not be the if distinction. And you want a

23:24.640 --> 23:29.760
method to kind of automatically figure out what are the kind of these basic building blocks. So

23:29.760 --> 23:36.480
if I take one window and I get I get a representation for that window and I get another window,

23:36.480 --> 23:41.680
maybe later on and I get a representation for that window, then I want to know that kind of

23:41.680 --> 23:46.560
the fine grain, the sub word unit used in these two windows, although the same or the not.

23:46.560 --> 23:50.960
That's kind of what it boils down to and in the supervised case,

23:50.960 --> 23:56.320
a neural network can kind of figure out exactly what should it look at in the speech stream

23:56.320 --> 24:00.880
to identify the speech sound because I'm telling it what speech sound to look for.

24:00.880 --> 24:04.480
But in the unsupervised case, how do you do this because you don't know?

24:04.480 --> 24:09.840
So you need to start to think about unsupervised models that can do something like

24:09.840 --> 24:17.120
clustering, so a Gaussian mixture model essentially. If I just give you a whole bunch of these

24:17.120 --> 24:24.560
like little windows and I tell you group them according to what you think are phonemes or speech

24:24.560 --> 24:29.040
sounds, then you can start to do that. You can train a Gaussian mixture model in an unsupervised way

24:29.040 --> 24:34.640
and then find these clusters of these groupings of speech sounds that you think are the units

24:34.640 --> 24:42.080
that used in the language. How do you even get the window sizing correct?

24:42.080 --> 24:51.120
I'm imagining that phonemes have different lengths and phonemes over some sample of speech.

24:51.120 --> 24:57.040
You know, maybe one of them is 2x the length of the other and so if you kind of choose a larger

24:57.040 --> 25:03.040
window, you end up with multiple phonemes in the window or at least maybe the end of one in the

25:03.040 --> 25:09.680
start of another. Is it kind of a sliding window approach and you're trying to maximize

25:09.680 --> 25:15.680
something that tells you that you've got a single phonem or are there variable windows?

25:15.680 --> 25:20.160
Like how does that that seems like pretty fundamental to this but also hard?

25:21.760 --> 25:28.880
Yes, it is hard. So actually, supervised systems also have this question like how do you

25:28.880 --> 25:35.840
choose a window? Because you need to, so just to answer your question, we always use a sliding

25:35.840 --> 25:41.200
window. So you kind of have this window that, okay, you need to click a length and then you slide

25:41.200 --> 25:50.080
it across the speech stream. And in the first, the first systems, you take a window and you take

25:50.080 --> 25:55.680
a Fourier transform, which tells you about the spectral content of that window and you just

25:55.680 --> 26:00.080
keep on doing this. So you just slide this across and you look at the spectral content and now

26:00.080 --> 26:05.360
you've got this question of the straight off. If I make the window long, then I've got a lot of

26:05.360 --> 26:11.840
data to kind of tell me what's going on in that window. If I make the window too short, okay,

26:11.840 --> 26:16.800
then I, if I've got a very short window, then I know that it's only going to be one phonem or

26:16.800 --> 26:21.120
part of a phonem, which is kind of fine. If I only get parts of phonemes that's also okay,

26:21.120 --> 26:28.000
I just need to model that. So I might take a very small window, which means that I know that this

26:28.000 --> 26:32.880
is only one speech sound in this window, but then you've got a lot less data to kind of figure

26:33.840 --> 26:38.240
to kind of estimate the content of that window. And this straight off is something that you don't

26:38.240 --> 26:44.560
just see in speech processing. You see it in any type of kind of signal processing task. And

26:44.560 --> 26:51.680
luckily, people in the 80s really fiddled with this and tried to figure out how long should the

26:51.680 --> 26:57.440
window be and how much should I move it. And I mean, I can just tell you the answer now because

26:57.440 --> 27:04.640
people have been trying to do this for 30 years. And so I think the community has a whole,

27:04.640 --> 27:11.440
including the supervised speech community has kind of settled on a relatively specific window

27:11.440 --> 27:17.920
length and then a frame skip. And then so in the supervised community, you kind of you don't

27:17.920 --> 27:22.880
care if it's if if you suite this window across and it's just predicting the same phonem for

27:22.880 --> 27:26.960
multiple windows, then you kind of know that you're in a phoning and then you transition to the

27:26.960 --> 27:32.320
next one. I kind of oversimplify that approach. So what they normally do is they take a window,

27:32.320 --> 27:36.880
you predict the label, then you shift the window, you predict the label, shift the window,

27:36.880 --> 27:42.720
predict the label. And now what you get is kind of the sequence of posteriors over labels.

27:42.720 --> 27:47.520
And that actually goes into another system, a decoder or something that you can train jointly,

27:47.520 --> 27:53.680
which kind of clues these things together and tells you I was in this phoning for this long,

27:54.880 --> 27:59.840
phonemes teach together to form words in this way, words teach together to form sentences in

27:59.840 --> 28:04.400
this way. And in the end, you get a speech recognizer. One question that that raises for me though,

28:04.400 --> 28:10.400
that we've kind of arrived at this answer for the window length and the the stride,

28:10.400 --> 28:18.640
essentially the the step length. But does that suffer from the same asymmetry in terms of

28:18.640 --> 28:24.240
the language that the language is that it's optimized for as speech recognition in general,

28:24.240 --> 28:29.920
like if you're trying to apply these techniques to a language that is under-resourced,

28:29.920 --> 28:34.880
it could be that these values that are traditionally used don't work as well,

28:34.880 --> 28:38.560
are they more properties of human speech than of a given language?

28:40.080 --> 28:46.080
That's a super cool question. Because I think if you read the original papers in the like late

28:46.080 --> 28:53.520
80s and early 90s describing these techniques, a lot of these things were inspired by the human

28:53.520 --> 29:00.000
perceptual system. So they were kind of hand designed initially, they were hand designed to match what

29:00.000 --> 29:08.720
they we know about the human perceptual system. So those papers say that these things are language

29:08.720 --> 29:14.560
universal and I think that's not something that people question a lot. But it is interesting if you

29:14.560 --> 29:22.480
look at supervised system performance on different languages, then if you use the same amount of

29:22.480 --> 29:26.960
data and you compare a English system and you use the same amount of data and you compare that

29:26.960 --> 29:32.160
to a Zulu system, the Zulu system will always perform worse even though it might be trained on

29:32.160 --> 29:38.960
the same amount of data. Now there's a lot of other reasons for that. There can be a lot of other

29:38.960 --> 29:44.320
reasons for that but it's a it's a very interesting question whether we should actually go back to

29:44.320 --> 29:51.760
those first hand engineered question research because they were all evaluated on English,

29:51.760 --> 29:56.000
although the claim was made at their language universe, so they were all evaluated on English

29:56.000 --> 30:05.120
or at least well resource languages. And now we apply these things to Zulu and SOMGA and other languages

30:05.120 --> 30:09.600
and we see that it's just very hard to get these systems to work as well as they do on English,

30:09.600 --> 30:16.640
even if the experiment is controlled. So this was actually I spoke to someone from MIT,

30:16.640 --> 30:22.960
I think it was Jennifer Drexler and she said, ask this question like, are these basic features,

30:22.960 --> 30:27.920
this kind of fundamental questions? Are they overtailer to English because the people that

30:27.920 --> 30:33.680
wrote the papers were English? And I think this euro resource community is starting to ask these

30:33.680 --> 30:39.040
questions or bring that up again because our systems are even more sensitive to these

30:39.040 --> 30:48.080
to these type of decisions. You mentioned that in the case of Zulu, which was the example,

30:48.080 --> 30:54.800
I think you just picked randomly of systems underperforming when trying to recognize that language

30:54.800 --> 31:01.840
that there are some lists of reasons why they do. What are those reasons? That's also difficult

31:01.840 --> 31:11.920
to answer. So I picked Zulu, but I can give you a list of reasons. Very often, so for example,

31:11.920 --> 31:20.480
Afrikans, I think it's SOMGA as well, and I'm not sure about Zulu, are basically languages

31:20.480 --> 31:26.000
that were if we write them down, we often glue words together, they're kind of written as one,

31:26.000 --> 31:30.400
right? So in English, if you have two concepts, then they're written as two separate words,

31:30.400 --> 31:36.080
but in Afrikans, when it's one thing, even though it might be composed of multiple things,

31:36.080 --> 31:41.360
then we write them as one word. Yeah, so it's something like narrow band speech processing,

31:41.360 --> 31:47.680
right? I just said the sentence narrow band speech processing will have a word narrow band speech

31:47.680 --> 31:53.280
processing, okay? But in Afrikans, or in German, you will just write it as one word. It will just

31:53.280 --> 31:58.320
be like one word. And for speech systems, that's something that's very, very tricky to get

31:58.320 --> 32:03.280
right, because you basically don't know when should I split a word and when should it be one.

32:03.280 --> 32:09.280
So that's just one example of a case where it's kind of obvious why the system doesn't perform

32:09.280 --> 32:16.800
as well. Another thing is, so in Sutu, for example, we have tones, which you also have in Mandarin,

32:16.800 --> 32:23.920
and sometimes that's not marked. So you can't see that there's a specific tone being used when

32:23.920 --> 32:30.240
you write something down. Speakers of the language know which tone to use based on the context,

32:31.440 --> 32:36.080
but you can't see it. If you're a non-native speaker, you won't know which tone is being used,

32:36.800 --> 32:44.160
and that might be one other reason why these systems don't perform as well as English. And this

32:44.160 --> 32:50.320
is all apart from the kind of what features do we actually put in that to begin with. Those

32:50.320 --> 32:56.080
were just two examples. Code switching is another big example, specifically in South Africa. So

32:56.720 --> 33:02.640
a lot of the time what we do is we switch between languages in a single sentence. So very often,

33:02.640 --> 33:07.200
you would switch between your native language and English and then switch back. So if you just

33:07.200 --> 33:13.040
look at the output of your speech recognizer, it just got all those words wrong. And a lot of

33:13.040 --> 33:19.200
corpora actually have this code switching in them. It just jumped out at me that even that code

33:19.200 --> 33:25.200
switching alone sounds like an interesting research area for these types of systems. Are there

33:25.200 --> 33:30.720
folks out there specializing in that? Yeah, there are. So actually in the lab downstairs,

33:30.720 --> 33:36.640
there's a whole bunch of people working on this. And there was a special session I think at

33:36.640 --> 33:42.960
Interspeech as well. And in here, just a while ago, focusing specifically on code switching,

33:42.960 --> 33:48.720
because it's something that happens a lot in South Africa because we have a live and official

33:48.720 --> 33:55.200
languages and they're kind of all spoken geographically in overlapping regions. But in other parts

33:55.200 --> 34:01.840
of Africa, it also happens. And then also in a lot of Asian countries and India, this happens a

34:01.840 --> 34:06.960
lot. So people are I think like over the last five years, maybe people have really started to work

34:06.960 --> 34:13.200
on this. And it's not just even in spoken language, also in written language. People, for example,

34:13.200 --> 34:21.520
in tweets would switch between different languages. And that really messes up these NLP systems,

34:21.520 --> 34:25.680
which are kind of tailored for specific language or speech recognition. You kind of build a system

34:25.680 --> 34:31.520
for language. And that really messes up the system. So there's a lot of interesting questions

34:32.480 --> 34:38.960
about how you build these models to kind of handle arbitrary switches between language.

34:38.960 --> 34:45.280
So I think I was trying to recap your representative flow for low resource speech processing.

34:45.280 --> 34:50.880
And I think I got to like the beginning of the first step. Right.

34:52.480 --> 34:57.920
So we talked about this windowing. We talked about this windowing thing and using Gaussian

34:57.920 --> 35:05.200
mixture models to try to determine what phonemes are spoken or in the case of unsupervised,

35:05.200 --> 35:10.400
you're trying to, well, I guess the same thing. You're trying to determine the, like the

35:10.400 --> 35:16.720
universe of phonemes and which ones are represented in an individual sample. And if that's close to

35:16.720 --> 35:23.920
correct, what's next? Yeah. Okay. So now let's say we figure that out. Now what I give you is,

35:23.920 --> 35:32.000
now I can take a bit of speech and then I can kind of pause it through this feature representation

35:32.000 --> 35:37.200
model. And that could just tell me this phonem is present, this phonem is present, this phonem,

35:37.200 --> 35:42.960
and you don't know whether it's a phonem. So it's more like pseudo phonem, or maybe just cluster.

35:42.960 --> 35:48.960
Okay. Now you've got the sequence of clusters or sequence of pseudo phonemes. And okay,

35:48.960 --> 35:53.840
that's helpful. Okay. Maybe you can use that if you're, if you're a language documenting a

35:53.840 --> 36:00.080
language, then that can maybe prove insightful. But if you actually want to build a speech processing

36:00.080 --> 36:07.680
system, you need to go from that to words or some higher level unit. And that in itself is quite

36:07.680 --> 36:16.480
tricky. And it's because of this reason that you already alluded to that one word can be three

36:16.480 --> 36:22.960
phonemes long. Another word can be five phonemes long. Another word can be two phonemes long. So how

36:22.960 --> 36:30.480
do you group these things according to words? And that's very, very difficult. So kind of the

36:30.480 --> 36:37.600
classic approach to doing this is kind of a type of compression model. So what you try and do

36:37.600 --> 36:45.120
is you try to say, okay, if I treat these three units, reoccurring units as a word, how much does

36:45.120 --> 36:52.720
that allow me to kind of compress these sequences? And so you, that's just, that's just one approach.

36:52.720 --> 36:57.920
So but essentially what you need to do is you now need to add a model on top of these unsupervised

36:57.920 --> 37:06.240
discovered phonemes to tell you how you group these clusters together to form words. And there's

37:06.240 --> 37:13.120
maybe I don't know a handful of techniques that you can use to do this. Should I talk through them?

37:13.120 --> 37:18.960
Please. Okay. Yeah. So people at MIT actually, when they started doing this, Jackie Lee and

37:18.960 --> 37:26.640
Jim Gloss, they had a hidden mark of model, which you can kind of train on top of these pseudo phonemes

37:26.640 --> 37:32.960
sequences. And they actually broke down their model. The whole thing is trained in kind of one

37:32.960 --> 37:38.000
go. So at the bottom, you've got a kind of a Gaussian mixture model, which you can interpret as

37:38.000 --> 37:44.080
finding these phone-like units. And then on top of that, now you've got the sequence of phone-like

37:44.080 --> 37:49.440
units. So you feed them into a hidden mark of model, which is kind of like a sequence Gaussian

37:49.440 --> 37:55.200
mixture model. And you treat that hidden mark of model that they are on top of that as syllables.

37:55.760 --> 37:59.600
And then on top of that, now you're getting out the sequence of syllables. And

38:01.200 --> 38:05.280
using the sequence of syllables, now you can train another hidden mark of model. I haven't

38:05.280 --> 38:10.160
had another hidden mark of model layer on top of these syllable layers to model words.

38:10.720 --> 38:16.320
And the whole thing is kind of trained in as one big. They used, I think,

38:16.320 --> 38:20.640
Gibbs sampling to train this whole thing from the bottom up to the top and so on.

38:21.360 --> 38:27.600
And that's one approach. And that seemed to work pretty well if you have single speakers,

38:27.600 --> 38:33.040
because that really messes things up. But if you go to multiple speakers, then that becomes

38:33.040 --> 38:37.680
much harder. I can tell you about our approach if you're interested.

38:38.800 --> 38:41.920
I am, but I guess it just occurred to me that there's...

38:42.960 --> 38:46.720
So, right, we'd ask this question. There's like a fundamental thing missing for me,

38:46.720 --> 38:51.920
and maybe I need to step back and ground out on the goal of this. We're talking about going

38:51.920 --> 38:59.120
from speech to text. Is that correct or no? No. You're basic? Okay. Okay. You're awesome.

38:59.120 --> 39:02.880
I was wondering where there's like a, you know, then the miracle occurs step in here,

39:02.880 --> 39:12.480
and I'm not seeing it. So what you want is impossible, right? So what you want is you want to

39:12.480 --> 39:18.080
ground this in some way. And if you don't have text, you just can't do this. You can't do it.

39:18.080 --> 39:23.920
Okay. So why do you want to do this? I'll give you two reasons, and then I'll ask your question

39:23.920 --> 39:30.240
of what even are we, do we want to do? Yeah. So what you want to do is if I give you a big

39:30.240 --> 39:37.520
corpus of audio, then what I want is I want you to figure out where words start and end in the

39:37.520 --> 39:42.240
speech stream. I want you to snap it up the speech into these things that look like words.

39:42.240 --> 39:48.400
Okay. And then if I, if I tell you, okay, these are the word boundaries, okay, which you've predicted.

39:48.400 --> 39:55.600
Then I want you to tell me this snippet. I don't know what word it is, but this snippet also reoccur

39:55.600 --> 40:01.280
is here and here and here and here and here and here in my speech corpus. So it's this combination

40:01.280 --> 40:06.560
of segmentation breaking it up into things that look like words. And then the second part,

40:06.560 --> 40:12.160
which is clustering, grouping these words together. Now, okay. So that's what we want to do. You

40:12.160 --> 40:19.760
want to know why this is useful? Sure. Okay. So I mean, I'm imagining that it's useful in that I

40:19.760 --> 40:26.000
could take that data and have a prioritized list of words to get someone to transcribe for me and

40:26.000 --> 40:32.400
then start to figure out texts. You know, if that's what I ultimately want, but why else would it be

40:32.400 --> 40:39.200
useful? Exactly. Okay. So that's a great use case that you've just described there. But the years

40:39.200 --> 40:43.680
to two more reasons. Okay. One, if you want, if you're interested in how infants do this,

40:43.680 --> 40:48.160
then having a model that can do this is useful. So then you can fiddle around with the model,

40:48.160 --> 40:53.200
check the type of mistakes that the model makes and see if then you can go and test in a lab if

40:53.200 --> 40:59.120
infants use the same type of cues and whether they make the same type of mistakes. And in that way,

40:59.120 --> 41:06.080
we can actually learn something about how humans learn language. Okay. I'm not a cognitive,

41:06.080 --> 41:13.360
a cognitive modeling person, cognitive scientist. So I probably didn't describe that well, but that

41:13.360 --> 41:19.760
is one motivation. A third motivation is, and this is actually a project that I'm working on

41:19.760 --> 41:28.400
with some of the colleagues here, is the following setting where this project is in Uganda.

41:28.960 --> 41:34.480
It's actually a project within United Nations. And they're really interested in

41:34.480 --> 41:40.720
very, very specific keywords. They have these systems that collect broadcast news.

41:41.520 --> 41:48.480
So it's basically servers that capture radio broadcasts. And then you've got the server full

41:48.480 --> 41:55.120
of speech data, but you don't have a speech recognizer in Uganda, the language that's spoken

41:55.120 --> 42:00.000
there. Okay. But now as the United Nations, you're really interested in figuring out what people

42:00.000 --> 42:06.240
are talking about in these local radio broadcasts. So what you can do is you can get a small number

42:06.240 --> 42:10.880
of people. And you can tell them, listen, I'm really interested in education. I'm really interested

42:10.880 --> 42:17.760
in maybe specific diseases, maybe in specific disasters, things like that. And I can get a small

42:17.760 --> 42:23.520
number of people to give me a bunch of keywords that I'm interested in. Okay. Now I've got my unsupervised

42:23.520 --> 42:28.720
model. I can label these keywords that a small number of people have given me. And then what I can

42:28.720 --> 42:33.680
do is I can go and search this big corpus of audio and find all the radio broadcasts that

42:33.680 --> 42:38.960
contains those keywords. And then maybe I can pass only those broadcasts to an analyst and ask

42:38.960 --> 42:44.560
them, please just translate these ones. I know they're important to me. And then we can, I don't know,

42:44.560 --> 42:48.880
figure out what people are talking about in Uganda. I hope that makes sense. Yeah, no, that does,

42:48.880 --> 42:56.080
that does make, that does make a lot of sense. And so you were about to describe the approach that

42:56.080 --> 43:07.680
you use in your lab to go from the phoning data to the segmenting. Yeah. So I actually want to

43:07.680 --> 43:14.080
answer another question first. Okay. Let me answer this question. And then I'll answer your other

43:14.080 --> 43:18.880
question, which was about grounding. How do you actually know what people are talking about when

43:18.880 --> 43:23.760
you don't have any labels? Okay. So I'll first answer the question about what we use. And this is

43:23.760 --> 43:31.440
really a technique that I developed and my PhD of my supervisors. So we basically argued that

43:31.440 --> 43:38.960
this idea of getting these fine grained units. And then, and then having a syllable layer on top

43:38.960 --> 43:44.080
of that and then a word layer on top of that, that becomes quite here. The whole thing becomes

43:44.080 --> 43:50.880
quite difficult. So what we've been doing is we've been thinking about this idea of something

43:50.880 --> 43:55.440
called acoustic word embeddings. So I think a lot of people know what word embeddings are. It's

43:55.440 --> 44:02.160
these kind of continuous vector representations of written words. And we wanted to take that same

44:02.160 --> 44:10.320
idea to speech. So when you're building these language discovery systems, inevitably what happens

44:10.320 --> 44:16.480
is you end up having to compare two snippets of speech with each other, but they're not of the

44:16.480 --> 44:22.640
same length. Okay. Two words are never the same length. Even if I say Apple and use Sam says Apple

44:22.640 --> 44:30.000
then these two apples will not have the same duration. So inevitably you end up comparing things

44:30.000 --> 44:36.800
that's of different duration, a half a second, one to one second. So what we started to develop

44:36.800 --> 44:42.400
was these acoustic word embeddings. And the idea behind these are basically you'd take a variable

44:42.400 --> 44:51.040
duration segment of any duration and you train them, you have a model that just maps that sequence

44:51.040 --> 44:56.480
that chunk of speech you map that to a single vector. Okay. Now if you could do this,

44:56.480 --> 45:02.560
what you could do is you can just embed basically all the sequences in your language. You can

45:02.560 --> 45:08.160
embed all of them, get vectors for each of them and now you can easily compare the different vectors.

45:08.160 --> 45:15.680
And in very short what we, what I developed in my PhD is something that kind of does this jointly.

45:15.680 --> 45:22.480
It starts by, it basically breaks the speech stream up into things that it thinks are words.

45:22.480 --> 45:28.640
It's random, it doesn't know. It embeds all of these. It classes those things into things that

45:28.640 --> 45:34.640
it now thinks are words and then it goes back and resegments and it has this kind of back and forth

45:34.640 --> 45:39.360
thing. I don't know if that makes sense and there was a 30 second discussion about something

45:39.360 --> 45:50.160
it took me four years to do. Are you kind of iteratively creating this embedding space and then

45:50.160 --> 45:56.640
performing some operations on it to try to determine the segments and then updating the embedding

45:56.640 --> 46:03.120
space and like kind of optimizing the embedding space or were you saying something else about this,

46:03.120 --> 46:08.400
I kind of picked up on an iterative cycle in there but I'm not sure what the, the iterations are.

46:08.400 --> 46:12.640
Yeah, you should have, you should have written the abstract for my thesis. So what you just said

46:12.640 --> 46:20.560
is exactly right. So you start with like a random segmentation of your input corpus and then

46:20.560 --> 46:27.200
on that random segmentation you build an embedding space. Now if I actually have an embedding space,

46:27.200 --> 46:32.160
okay, initially it's, it's going to be pretty bad, okay, but I have an embedding space.

46:32.160 --> 46:38.720
Then what you can do is you can say, given this embedding space, how should I split up my input

46:38.720 --> 46:45.280
stream to kind of have a higher score if you want under this embedding space? Okay, so you go back,

46:45.840 --> 46:56.080
you resegment. Let me pause you there. So yeah, you've got this embedding space and are you doing

46:56.080 --> 47:03.120
some kind of clustering within the embedding space? Exactly right. So I actually use a Gaussian

47:03.120 --> 47:09.120
mixture model. I also played a, a Bayesian Gaussian mixture model and I've also played around with

47:09.120 --> 47:14.720
some non parametric like infinite Gaussian mixture models to do that. So the idea behind these

47:14.720 --> 47:21.760
embeddings are that if you say Apple and I say Apple, then we're going to have two embeddings

47:21.760 --> 47:28.640
and we want all the instances of specific words that are acoustically the same

47:28.640 --> 47:33.680
to end up in similar regions in this embedding space. So that's really the goal.

47:33.680 --> 47:39.200
But when you start out, you don't know where words, whether words start an end, so you kind of

47:39.200 --> 47:44.400
start randomly. And then what you do is so you start randomly, you get all these embeddings

47:44.400 --> 47:49.680
and now you group them, okay, and you cluster them, I usually Gaussian mixture model to do the clustering.

47:49.680 --> 47:57.040
And the idea is that every cluster in the Gaussian mixture model should be hypothesized word.

47:57.040 --> 48:01.600
It should be something that you think or the model at the moment think that this thing,

48:01.600 --> 48:07.200
this group of embeddings, they all correspond to the same type of word, the same word, okay,

48:07.200 --> 48:10.480
initially that's wrong, but that's what you kind of hope where the model ends up.

48:11.120 --> 48:17.040
So you cluster, you start, you snap off your speech, you embed, you get this embedding space,

48:17.040 --> 48:21.200
you cluster in that embedding space using a Gaussian mixture model.

48:21.200 --> 48:25.600
Now, under this Gaussian mixture model, I can now say, go back to my inputs,

48:25.600 --> 48:30.800
pretend I don't know where the words came from. And under this Gaussian mixture model, how should

48:30.800 --> 48:37.360
I split up the speech stream so that I get a higher score under my current Gaussian mixture model

48:37.360 --> 48:42.160
or the embeddings that I would get if I split it up, okay. So then you break it up and then you

48:42.160 --> 48:48.480
re-embed and then you build your Gaussian mixture model again. Under that model, you go back and say,

48:48.480 --> 48:53.760
even these groupings of words, how should I chunk up my speech to get a higher score? And you just

48:53.760 --> 48:58.640
kind of iterate through this thing. Now, I describe it as this iterative process, but actually,

48:58.640 --> 49:04.480
it's implemented this one, Gibbs sampler. So it's this one model that kind of does things in one go.

49:04.480 --> 49:14.560
The projecting backwards step there where you are asking the question, given a set of groupings,

49:14.560 --> 49:22.320
how could you change the segmentation to improve the groupings? Is that a difficult piece in this

49:22.320 --> 49:31.120
or is that a pretty straightforward element? No, that is a difficult piece. So that's where I spend

49:31.120 --> 49:38.800
a lot of my time. Okay, also, there's two answers. It is a difficult piece, but it is also something

49:38.800 --> 49:45.760
that people have been looking at for a relatively long time. Meaning in the context of an embedding

49:45.760 --> 49:54.240
space or in other contexts and it carries over? No, in other contexts, kind of in computer science

49:54.240 --> 50:04.720
in general, but specifically in, actually, it came from the NLP literature. So it actually

50:04.720 --> 50:09.600
comes from a different part of literature. So in Chinese, you have this problem that we don't know

50:09.600 --> 50:14.240
where words start like Chinese than the way it's written. It's written with outward boundaries.

50:15.200 --> 50:21.520
So in that literature, people have started to look at, if I kind of know the words in my language,

50:21.520 --> 50:26.240
or I think I know the words in my language, how can I figure out where all the word boundaries

50:26.240 --> 50:32.960
in Chinese? And using that silent type of ideas, that's exactly what we used here, except that now

50:32.960 --> 50:39.440
we're doing it on this kind of continuous embedding space, but the mathematics for that is very,

50:39.440 --> 50:45.600
very similar to this question of, if I give you an unsegmented Chinese corpus, how do you figure

50:45.600 --> 50:52.320
out where there's words? So it ends up being like a dynamic programming procedure where you

50:52.320 --> 50:57.680
basically ask, okay, I'm going to start at the end of my sentence, and then what I'm going to do

50:57.680 --> 51:02.560
is I'm going to say, okay, words can be between 200 milliseconds and one second. Okay, so you're

51:02.560 --> 51:08.080
built in that constraint. And then what you do is you basically say, okay, I'll start at the end

51:08.080 --> 51:13.440
of my sentence. If the last word in my sentence is 200 milliseconds long, what's the score?

51:13.440 --> 51:19.200
If it's 250 milliseconds long, what's my score? If it's 300 milliseconds long, what's my score? Okay,

51:19.200 --> 51:25.440
and then you basically looking for all the possible word segmentations in this range,

51:25.440 --> 51:31.440
and then you kind of get an overall score using a dynamic programming method for figuring out where

51:31.440 --> 51:38.960
words started in the speech stream. And the score in this case is what? Yeah, so the score, if we're

51:38.960 --> 51:44.480
just looking at a single question of should I put a word boundary year or not, then the score is

51:44.480 --> 51:52.720
basically, if I chunked up this chunk of speech and I treated that as a word, how likely will that

51:52.720 --> 51:59.840
be? How close would that be to a cluster mean under my current plus string? Does that make sense?

51:59.840 --> 52:05.280
You're trying to set your segments up so that as many of the segments as possible are words

52:05.280 --> 52:10.640
basically? That's right. You can think about that. It's not really as many as possible. It's kind of like

52:10.640 --> 52:17.840
if I gave you, if I gave you an utterance, a whole sentence, and I told you big places that you

52:17.840 --> 52:23.600
want to put boundaries, big places that you want to put boundaries, that if I look at the overall

52:24.000 --> 52:28.880
okay, so now you put boundaries, now you have a whole bunch of different embeddings, right?

52:28.880 --> 52:33.600
And you want to look at the overall score for that utterance. Each of the embeddings gets a score

52:33.600 --> 52:37.920
and what you want to maximize is kind of the overall score for that specific utterance.

52:38.720 --> 52:46.720
One last piece you mentioned that as opposed to the multiple phases, you do everything in kind of

52:46.720 --> 52:52.560
one pass with Gibbs sampling. Can you give us kind of the high level overview of Gibbs sampling and

52:52.560 --> 52:57.680
how you apply it here? Yeah. Okay, so Gibbs sampling is this very cool technique where

52:57.680 --> 53:05.920
you basically try to get samples from a distribution and in general that's tricky, especially if your

53:05.920 --> 53:11.360
model is quite complicated. So what Gibbs sampling does is it basically keeps everything fixed

53:12.000 --> 53:17.680
and you want to know, you want to get a sample for a latent variable. So in our case, the latent

53:17.680 --> 53:23.760
variable might be something like which clause does this embedding get assigned to? Okay, so

53:23.760 --> 53:31.360
so in Gibbs sampling, how it works is you pick a specific latent variable and you keep all your

53:31.360 --> 53:36.640
other latent variables, you keep that fixed at previous samples. Okay, and then what you say is

53:36.640 --> 53:41.360
given that all the others are fixed, sample this thing from my distribution, this one that I'm

53:41.360 --> 53:46.320
interested in. Okay, after you sample that one, now you keep this guy fixed and you go to the next

53:46.320 --> 53:50.720
latent variable. Okay, you keep all the others fixed and now you sample from this guy,

53:50.720 --> 53:57.280
then you go on to the next latent variable and so forth. Does that make sense? Okay, so in our case,

53:57.280 --> 54:03.920
what we do is we basically say given we've got this entire corpus, the whole corpus have been

54:03.920 --> 54:10.320
segmented and they've been clustered already. Okay, everything is fixed and we pretend we know where

54:10.320 --> 54:15.520
words start and end and we know we pretend we know which clusters they should belong to.

54:15.520 --> 54:22.800
Okay, then what you do is you take one utterance in your dataset and you say now I'm going to

54:22.800 --> 54:27.680
take this utterance and I'm going to remove that from my model and I'm going to pretend that

54:27.680 --> 54:31.680
this is the only utterance that I don't know the segmentation and the clustering of.

54:31.680 --> 54:36.720
So you remove that utterance and then you say given the model which is now defined by all the

54:36.720 --> 54:41.840
rest of the utterances for which I know the word boundaries and the clusters, what is the best

54:41.840 --> 54:48.480
segmentation for this utterance? Okay, and then what you do is you segment the utterance according to

54:49.360 --> 54:54.880
all of the rest of the data that's been fixed and you clustered that utterance and then what you do

54:54.880 --> 54:58.880
is you fix the segmentation and the clustering for that utterance and you go to the next utterance

54:59.440 --> 55:07.680
and so basically your this utterance procedure that I kind of said was like segment cluster segment

55:07.680 --> 55:12.160
cluster and I kind of described it at the corpus level, it's really happening at kind of a per utterance

55:12.160 --> 55:17.600
level. Interesting. And I want to answer one question that you asked really early on or just a while

55:17.600 --> 55:22.880
ago, you asked if you're just going to do this right, you're never going to get to text, you're

55:22.880 --> 55:29.200
never going to figure out exactly what is the meaning in this utterance right, you're going to

55:29.200 --> 55:33.520
segment that and that that's exactly what's going to happen. I don't know if you saw this movie

55:33.520 --> 55:41.440
arrival. Yes. Yeah, some people loved it, some people hated it, I really liked it, but there's

55:41.440 --> 55:49.200
the scene where they go to her and they ask her, I mean they play her the snippet of audio,

55:49.200 --> 55:56.160
right? And she's a linguist, a linguist that documents languages, so they play her the snippet

55:56.160 --> 56:02.400
of audio and then asks her, listen, what is being said in this language? And she looks at them and

56:02.400 --> 56:08.960
tells them in a very, it's probably very unholy word, but tells them, I can't help you, right?

56:08.960 --> 56:14.080
I have no idea what this snippet of audio means because I don't have any context, I haven't seen

56:14.080 --> 56:18.960
the people that speak these things, I haven't seen when they use it, when they do not use it.

56:18.960 --> 56:25.840
And a lot of the research into the zero processing models is exactly like that. We just want to see

56:25.840 --> 56:32.160
how can you learn something from the raw audio and infants can do this to some extent and our

56:32.160 --> 56:37.760
models can do this to some extent and then there are these good use cases for these models,

56:37.760 --> 56:43.280
but ultimately you can never figure out what does this word actually mean. And this is why a lot

56:43.280 --> 56:47.760
of people in this space are moving to the setting where you have other signals that goes with the

56:47.760 --> 56:54.720
speech. So if you have a chunk of unlabeled speech but you have an image describing the context in

56:54.720 --> 57:00.080
which the speech is used, because if you can do that then you can segment your words hopefully

57:00.080 --> 57:04.560
and then figure out, okay, this word, I don't know what it is, but I can ground it because I have

57:04.560 --> 57:10.240
an image and I can try and figure out what is it in the image that calls this word to being said.

57:10.240 --> 57:14.720
So that's a very, very interesting avenue for feature work that a lot of people are looking at.

57:15.520 --> 57:20.960
Well, I was going to ask you, if you had some words on where you see this going, but that sounds

57:20.960 --> 57:29.680
like you anticipated that question there. Maybe final thoughts, again, circling back to

57:29.680 --> 57:37.360
where we started all this on the endaba and you know what you see for that community and more

57:37.360 --> 57:46.080
broadly machine learning and AI in Africa. Yeah, so this I think it's a super exciting time for

57:46.080 --> 57:53.280
machine learning and AI in Africa and the endaba is one part of that and that has really started

57:53.280 --> 58:01.040
I think to both communities and kind of bring people together and in Africa and kind of

58:01.840 --> 58:08.320
helped people to see that the stuff we're doing here is very, very relevant also at the broader

58:08.320 --> 58:13.920
scale. I was quite a broad question so I'll give a whole bunch of small answers.

58:15.920 --> 58:22.240
One thing that I'm very excited about is that in Africa we have very unique challenges

58:22.240 --> 58:30.720
and unique opportunities and so taking language as an example there's so many languages spoken

58:30.720 --> 58:37.120
here in the same geographical area and that's very, very unique and I think what's going to happen

58:37.120 --> 58:42.240
if we start to push this community forward and if people actually believe that they can do this

58:42.240 --> 58:48.400
because we can then what I think is going to start to happen and I hope for this is that we're going

58:48.400 --> 58:58.560
to start to develop unique solutions for our unique problems and I think if that happens it's

58:58.560 --> 59:06.080
not just going to be like we are users of machine learning tools that's being developed in Europe

59:06.080 --> 59:10.560
of course we need to be that as well we need to solve our problems using the tools that's being

59:10.560 --> 59:16.080
developed in Europe and in the US but I think if we start to solve the problems here then we

59:16.080 --> 59:22.400
are actually going to start to contribute to the global scene and we're going to start to say

59:22.400 --> 59:27.520
this and this is a unique solution and actually you guys can also use this in some other problem

59:27.520 --> 59:32.720
areas in machine learning so I think it's going to be a combination of developing exciting

59:32.720 --> 59:38.560
applications but then also contributing to foundational research in machine learning and AI

59:39.600 --> 59:45.280
and that's I think that's really what I hope will happen. Being Dava is growing a lot it's going

59:45.280 --> 59:52.320
to Kenya next year which is super exciting and then in Dava has this kind of dual motivation the

59:52.320 --> 59:58.080
one is to strengthen machine learning and AI in Africa but then also to fix the problems of

59:58.080 --> 01:00:04.160
diversity in ML and I hope that that is something that Africa can also contribute to because

01:00:05.120 --> 01:00:09.920
I mean Google can just start to hire a lot more African researchers right and from across the world

01:00:09.920 --> 01:00:14.720
I think that would be very exciting. For me personally the thing that excites me most is actually

01:00:14.720 --> 01:00:21.680
the in Dava spin-offs I'm very passionate about developing local communities because I think

01:00:22.560 --> 01:00:26.800
so in the Cape area for example there's a lot of people working on ML and we can learn a lot

01:00:26.800 --> 01:00:32.560
from each other but up until fairly recently a lot of people have worked in isolation so I'm very

01:00:32.560 --> 01:00:39.200
very passionate about building local things at universities and in regions and the in Dava X is

01:00:39.200 --> 01:00:48.640
a spin-off of the in Dava kind of funding these local regional little in Davas and I think that's

01:00:48.640 --> 01:00:53.680
that's really where we're going to see some some interesting things happening people starting to

01:00:53.680 --> 01:00:59.280
collaborate and and and working together and I'm very excited to see what will happen there.

01:00:59.920 --> 01:01:06.480
Fantastic fantastic well Erman thank you so much for taking the time to chat with us this

01:01:06.480 --> 01:01:11.600
morning it's really really interesting research you're doing and I enjoyed learning about it.

01:01:12.160 --> 01:01:13.840
Cool thanks so much for having me.

01:01:17.120 --> 01:01:23.360
All right everyone that's our show for today. For more information on Erman or any of the topics

01:01:23.360 --> 01:01:30.720
covered in this show visit twimmelai.com slash talk slash 191. For more information on the deep

01:01:30.720 --> 01:01:38.560
learning in Dava podcast series visit twimmelai.com slash in Dava 2018. Thanks again to Google

01:01:38.560 --> 01:01:45.200
for their sponsorship of this series be sure to check out the 2019 AI residency program at g.co

01:01:45.200 --> 01:02:01.120
slash AI residency as always thanks so much for listening and catch you next time.

