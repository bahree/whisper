1
00:00:00,000 --> 00:00:17,440
Hello and welcome to another episode of Twymilthalk, the podcast where I interview interesting

2
00:00:17,440 --> 00:00:22,640
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:22,640 --> 00:00:26,280
I'm your host Sam Charrington.

4
00:00:26,280 --> 00:00:31,400
Once again, thanks to everyone who sent in their favorite quote from a recent podcast.

5
00:00:31,400 --> 00:00:36,760
Another batch of stickers got mailed out this week, and please keep those quotes coming.

6
00:00:36,760 --> 00:00:40,280
We've had a blast getting your perspective from each talk.

7
00:00:40,280 --> 00:00:44,000
If you're new to the Twymil family, you too can get a sticker.

8
00:00:44,000 --> 00:00:50,360
Just send us a comment via the show notes page, tweet us at Twymilai or at Sam Charrington

9
00:00:50,360 --> 00:00:54,680
or share your quote via a post on our Facebook page.

10
00:00:54,680 --> 00:00:58,880
Before we jump into today's show, which I'm sure you're going to enjoy, I'd like to take

11
00:00:58,880 --> 00:01:03,880
a moment to remind you all about my upcoming event, the Future of Data Summit, which will

12
00:01:03,880 --> 00:01:08,080
be held May 15th and 16th in Las Vegas, Nevada.

13
00:01:08,080 --> 00:01:12,160
I'm really looking forward to the summit, and I hope you can join me there.

14
00:01:12,160 --> 00:01:16,060
You'll hear from industry leaders and technology users on how they're taking advantage of

15
00:01:16,060 --> 00:01:22,560
emerging data-centric technologies like IoT, blockchain, deep learning, and more.

16
00:01:22,560 --> 00:01:24,480
You'll learn a ton.

17
00:01:24,480 --> 00:01:29,360
I'm continuing to add new speakers to the lineup, including Jennifer Prinkey from Walmart

18
00:01:29,360 --> 00:01:34,560
Labs, who will be joining us to talk about what she calls data mixology.

19
00:01:34,560 --> 00:01:39,120
Jennifer currently leads a team of data scientists and engineers working on improving the online

20
00:01:39,120 --> 00:01:46,240
experience of the Walmart customer by integrating Walmart's stores data with e-commerce data.

21
00:01:46,240 --> 00:01:50,960
She also manages the metrics and measurements team there, a group in charge of creating

22
00:01:50,960 --> 00:01:56,320
metrics to measure the impact of all new features and algorithms, as well as solutions to

23
00:01:56,320 --> 00:02:01,840
automatically control and manage all machine learning algorithms in production.

24
00:02:01,840 --> 00:02:06,160
So we'll be talking about the importance of finding and fusing data to making machine

25
00:02:06,160 --> 00:02:10,760
learning predictions in production and describing the evolving platform that Walmart's put

26
00:02:10,760 --> 00:02:13,400
in place to facilitate all this.

27
00:02:13,400 --> 00:02:18,480
Jennifer is just one of a whole lineup of great speakers I've got for the summit, and

28
00:02:18,480 --> 00:02:23,560
the summit is just the first two days of the Interrupt ITX conference, which offers a

29
00:02:23,560 --> 00:02:29,880
week full of great educational content for folks interested in data and analytics.

30
00:02:29,880 --> 00:02:34,200
In addition to a dedicated data and analytics track at the conference, there will also be

31
00:02:34,200 --> 00:02:40,120
an AI theater and demo showcase dedicated to the practical application of AI, deep learning

32
00:02:40,120 --> 00:02:46,520
and machine learning for traditional enterprise tasks, such as dynamic pricing and insurance,

33
00:02:46,520 --> 00:02:52,720
perfecting the retail customer experience, optimizing job costing, transportation expenses

34
00:02:52,720 --> 00:02:54,360
and more.

35
00:02:54,360 --> 00:02:58,920
Case studies will be presented by businesses employing successful AI strategies and will

36
00:02:58,920 --> 00:03:03,600
provide strategic guidance and suggestions for attendees to utilize in their respective

37
00:03:03,600 --> 00:03:05,000
businesses.

38
00:03:05,000 --> 00:03:11,000
Accompanying the AI theater is the demo showcase, which will provide a hands-on lab demonstrating

39
00:03:11,000 --> 00:03:14,200
real world AI technologies.

40
00:03:14,200 --> 00:03:19,240
If you have any questions at all about the summit, don't hesitate to reach out to me.

41
00:03:19,240 --> 00:03:24,680
And to learn more, visit twimmolai.com slash future of data.

42
00:03:24,680 --> 00:03:29,400
The code I provide on that page, which is simply my last name, Charrington, provides

43
00:03:29,400 --> 00:03:35,160
Twimmol listeners with a 20% discount when they register for Interrupt ITX.

44
00:03:35,160 --> 00:03:37,320
And now about today's show.

45
00:03:37,320 --> 00:03:44,400
I guess this week is Scott Stevenson, Scott is co-founder and CEO of DeepGram, which has

46
00:03:44,400 --> 00:03:50,200
developed an AI-based platform for indexing and searching audio and video.

47
00:03:50,200 --> 00:03:55,760
Scott and I cover a ton of interesting topics in this talk, including applying machine learning

48
00:03:55,760 --> 00:04:01,000
techniques to particle physics, his time in a lab two miles below the surface of the

49
00:04:01,000 --> 00:04:07,320
earth, applying neural networks and deep learning to audio, and the deep learning framework core

50
00:04:07,320 --> 00:04:09,520
that his company is open sourced.

51
00:04:09,520 --> 00:04:12,080
I know you're going to love this one.

52
00:04:12,080 --> 00:04:14,320
And now on to the show.

53
00:04:14,320 --> 00:04:26,960
Hey everyone, this is Sam Charrington and welcome to another episode of this week in Machine

54
00:04:26,960 --> 00:04:32,840
Learning and AI, this week I've got Scott Stevenson on the line with me.

55
00:04:32,840 --> 00:04:39,160
Scott is co-founder and CEO of Machine Learning and AI Startup DeepGram.

56
00:04:39,160 --> 00:04:40,160
Scott, say hi.

57
00:04:40,160 --> 00:04:43,640
Hi, thanks for having me, Sam.

58
00:04:43,640 --> 00:04:45,760
It's great to have you on the show.

59
00:04:45,760 --> 00:04:51,640
I think the best place to get started is to maybe have you spend a little bit of time

60
00:04:51,640 --> 00:04:57,800
talking about your background because you come out of physics, right, not audio, speech,

61
00:04:57,800 --> 00:04:58,800
and all that.

62
00:04:58,800 --> 00:05:00,160
Yeah, absolutely.

63
00:05:00,160 --> 00:05:06,040
We are, you know, DeepGram is an audio AI company, but our background or most of the

64
00:05:06,040 --> 00:05:11,880
technical people in DeepGram, our background is in particle physics or at least some form

65
00:05:11,880 --> 00:05:13,960
of, you know, deep physics.

66
00:05:13,960 --> 00:05:20,000
And for me and my co-founder Noah Shetty, we both were doing particle physics before we

67
00:05:20,000 --> 00:05:25,560
started DeepGram, which was searching for dark matter.

68
00:05:25,560 --> 00:05:32,360
And this is about, you build an experiment that sits around, miles underground, essentially,

69
00:05:32,360 --> 00:05:35,760
you know, like one mile, two miles underground, the deepest labs in the world are two miles

70
00:05:35,760 --> 00:05:36,760
underground.

71
00:05:36,760 --> 00:05:41,000
And we were, our experiment was in the deepest lab in the world in Western China.

72
00:05:41,000 --> 00:05:42,960
It was called Panda X.

73
00:05:42,960 --> 00:05:48,680
And we built this experiment over the course of four years with a lot of other people,

74
00:05:48,680 --> 00:05:51,440
you know, with around two dozen other people.

75
00:05:51,440 --> 00:05:56,520
And with the help of the Chinese government, and that's an interesting story.

76
00:05:56,520 --> 00:06:02,920
But yeah, and the techniques that we learned in building this experiment, we've figured

77
00:06:02,920 --> 00:06:06,000
out were like really applicable to audio.

78
00:06:06,000 --> 00:06:11,320
And the reason is that physics is very, at least particle physics at the very hairy

79
00:06:11,320 --> 00:06:14,480
edge of research is still analog.

80
00:06:14,480 --> 00:06:21,600
And you still read, you still look for particles using analog detectors, they're called photomultiplier

81
00:06:21,600 --> 00:06:22,600
tubes.

82
00:06:22,600 --> 00:06:27,440
And the signals that you get out of these are just a waveform.

83
00:06:27,440 --> 00:06:31,680
It looks a lot like an audio waveform, but it's at a much higher sampling frequency.

84
00:06:31,680 --> 00:06:40,400
And those waves that are contained in the output of these photomultiplier tubes, the signals

85
00:06:40,400 --> 00:06:43,880
that are in there tell you the signature of the particle that you're seeing.

86
00:06:43,880 --> 00:06:49,920
So you might see a single photon, you might see a big splash of a muon in your detector.

87
00:06:49,920 --> 00:06:51,440
But it's all contained in those waves.

88
00:06:51,440 --> 00:06:56,560
And so what you have to do is extract that information from the photomultiplier tubes,

89
00:06:56,560 --> 00:07:00,960
and then make a guess, you know, did you see dark matter or not.

90
00:07:00,960 --> 00:07:03,400
And so we got very good at doing that.

91
00:07:03,400 --> 00:07:08,120
And it's actually really interesting because the state of the art in particle physics is

92
00:07:08,120 --> 00:07:12,760
essentially a lot of humans sitting down, figuring out how do you extract information from

93
00:07:12,760 --> 00:07:14,280
these signals.

94
00:07:14,280 --> 00:07:19,120
And you know, maybe you've heard of like the Higgs boson being discovered a few years

95
00:07:19,120 --> 00:07:20,120
ago.

96
00:07:20,120 --> 00:07:21,480
A lot of people, yeah.

97
00:07:21,480 --> 00:07:28,040
And that was done by thousands of scientists sitting down and saying, hey, I really want

98
00:07:28,040 --> 00:07:29,520
to find the Higgs boson.

99
00:07:29,520 --> 00:07:33,800
How do I make cuts in my data?

100
00:07:33,800 --> 00:07:39,760
How do I process my data to figure out, to make my signal to noise, you know, good enough

101
00:07:39,760 --> 00:07:41,840
to find these particles.

102
00:07:41,840 --> 00:07:50,800
And this is actually a lot of this hard manual labor of figuring out these cuts is done

103
00:07:50,800 --> 00:07:53,040
by machine learning now.

104
00:07:53,040 --> 00:07:59,080
So people will build boosted decision trees or kind of, that's not necessarily the realm

105
00:07:59,080 --> 00:08:04,440
for neural networks, but boosted decision trees and other statistical machine learning

106
00:08:04,440 --> 00:08:05,440
techniques.

107
00:08:05,440 --> 00:08:08,160
People are figuring out how to sort of automate this.

108
00:08:08,160 --> 00:08:11,480
But that's like the world that we came from essentially.

109
00:08:11,480 --> 00:08:16,920
And we were on that, you know, we were in that area thinking, like, man, this sucks.

110
00:08:16,920 --> 00:08:21,480
Let's automate this, you know, how do we automate it?

111
00:08:21,480 --> 00:08:25,080
Because I feel like a machine already just going through, like, here's another plot.

112
00:08:25,080 --> 00:08:26,080
Did this work well?

113
00:08:26,080 --> 00:08:27,080
Did that not work well?

114
00:08:27,080 --> 00:08:28,080
Like, what went on?

115
00:08:28,080 --> 00:08:29,840
And it's like, this could all be done with a machine.

116
00:08:29,840 --> 00:08:33,920
And so a lot of the people that are in deep-gram now, we were all sitting at University of

117
00:08:33,920 --> 00:08:39,080
Michigan thinking, you know, how can we make machine learning, or how can we jam machine

118
00:08:39,080 --> 00:08:44,120
learning into physics, particle physics, and then, like, extract something out of it.

119
00:08:44,120 --> 00:08:46,640
And did you, how far did you get down that path?

120
00:08:46,640 --> 00:08:52,400
Did you actually complete the jamming, or did you end up leaving and going off to start

121
00:08:52,400 --> 00:08:56,840
deep-gram before you fully applied ML to the particle physics?

122
00:08:56,840 --> 00:09:00,920
Yeah, so it's varied for the different people in deep-gram.

123
00:09:00,920 --> 00:09:05,040
So I did finish my PhD, I finished about two years ago.

124
00:09:05,040 --> 00:09:11,440
And Noah, who was my co-founder, went to a PhD program at Stanford, like, actually didn't

125
00:09:11,440 --> 00:09:13,440
even start it.

126
00:09:13,440 --> 00:09:18,000
He came out to the Bay Area, and we were working on deep-gram at the time, and we got a little

127
00:09:18,000 --> 00:09:23,120
bit of funding, and he was like, you know what, let's not do this physics thing.

128
00:09:23,120 --> 00:09:29,840
And anyway, but the couple other people in deep-gram either finished their PhD or, you know,

129
00:09:29,840 --> 00:09:32,560
got a master's and left after that.

130
00:09:32,560 --> 00:09:38,680
But we were definitely successful in sticking machine learning into a particle physics analysis

131
00:09:38,680 --> 00:09:40,000
pipeline.

132
00:09:40,000 --> 00:09:47,040
And in particular, for dark matter, what we did is reconstruct events in a 3D position,

133
00:09:47,040 --> 00:09:50,360
reconstruct the 3D position of the events that are happening inside.

134
00:09:50,360 --> 00:09:56,480
So the way that this works, I mean, it's like not to go too deep, but a particle detector

135
00:09:56,480 --> 00:09:58,280
is just at least for dark matter.

136
00:09:58,280 --> 00:10:02,520
It's just a tub of liquid, and it's a tub of cryogenic liquids, so it's very cold.

137
00:10:02,520 --> 00:10:06,680
But, and it's put two miles underground, and it's under a shield, and it's all very,

138
00:10:06,680 --> 00:10:09,880
you know, like, particular, but it's all, it's just a tub of liquid.

139
00:10:09,880 --> 00:10:14,880
And that tub of liquid has a lot of atoms in it, and those atoms happen to be, in this

140
00:10:14,880 --> 00:10:16,520
case, xenon.

141
00:10:16,520 --> 00:10:22,080
And you choose xenon because it doesn't interact with other things, it's a noble gas.

142
00:10:22,080 --> 00:10:28,120
So just like helium or neon or argon, it doesn't really interact, it doesn't form molecules,

143
00:10:28,120 --> 00:10:32,080
it doesn't really have, like, a chemical decay to it, nothing like that.

144
00:10:32,080 --> 00:10:38,160
But it's really big, and scientists surmise that dark matter particles, like really big

145
00:10:38,160 --> 00:10:39,360
particles, essentially.

146
00:10:39,360 --> 00:10:43,760
So you find the biggest non-reactive particle that you can, you put it into a tub, and

147
00:10:43,760 --> 00:10:49,840
then that tub is just sitting there with tons of atoms, and it's just waiting for a dark

148
00:10:49,840 --> 00:10:50,840
matter particle to hit it.

149
00:10:50,840 --> 00:10:54,640
It's just sitting there, just, you know, that its sole purpose in life is to get hit by

150
00:10:54,640 --> 00:10:58,880
a dark matter particle, and almost none of them have a dark matter particle hit it.

151
00:10:58,880 --> 00:11:06,760
But that's what's happening, and those signals, that atom that got hit, as soon as it gets

152
00:11:06,760 --> 00:11:12,960
hit, it flies through the other xenon atoms that are sitting around in this liquid, and it

153
00:11:12,960 --> 00:11:19,120
creates a puff of light, basically, and stirs up some photons, essentially, and it's

154
00:11:19,120 --> 00:11:23,720
like ten photons, you know, it's a very small number, but you have these detectors that

155
00:11:23,720 --> 00:11:30,200
can detect the single photons, and those single photons are bouncing around inside your detector,

156
00:11:30,200 --> 00:11:34,600
and they're finally getting picked up by these single pixels, essentially, and you need

157
00:11:34,600 --> 00:11:37,360
to figure out, like, where was that event, right?

158
00:11:37,360 --> 00:11:43,280
And so you're talking about, like, really low statistics pattern matching, and how do

159
00:11:43,280 --> 00:11:49,000
you make some kind of algorithm that says, hey, this is what the pattern that I got, where

160
00:11:49,000 --> 00:11:50,760
do I think this event was?

161
00:11:50,760 --> 00:11:56,840
And that was really successful, and compared to what a human can come up with, essentially,

162
00:11:56,840 --> 00:12:02,440
and the other is determining the energy of the particle, how big of a splash was it?

163
00:12:02,440 --> 00:12:07,720
That's a little easier to do, but machine learning made it a little bit better.

164
00:12:07,720 --> 00:12:14,240
And what about boosted decision trees lent themselves to application for this problem,

165
00:12:14,240 --> 00:12:15,240
domain?

166
00:12:15,240 --> 00:12:19,520
Yeah, so boosted decision trees, one thing, is that they're, like, fast to train.

167
00:12:19,520 --> 00:12:22,160
So that was really good.

168
00:12:22,160 --> 00:12:28,800
They also, when you have a low number of statistics, or at least a fairly low number of statistics,

169
00:12:28,800 --> 00:12:32,520
they can still do pretty well, compared to, like, a neural network that would over train

170
00:12:32,520 --> 00:12:33,520
or something.

171
00:12:33,520 --> 00:12:39,800
So we were very hot at the time on doing boosted decision trees for that type of problem,

172
00:12:39,800 --> 00:12:45,560
and I still think it's actually very good, and people are still using this, you know?

173
00:12:45,560 --> 00:12:51,320
So yeah, it lent itself very well to doing that, the reconstruction, and it also lent itself

174
00:12:51,320 --> 00:12:57,520
really well to doing a determination of whether it is a signal or background.

175
00:12:57,520 --> 00:12:59,520
Was it a dark matter particle or not?

176
00:12:59,520 --> 00:13:00,520
Right.

177
00:13:00,520 --> 00:13:04,320
Because the signal that you see in this photo multiplier tube in the waveform, it actually

178
00:13:04,320 --> 00:13:05,720
does look different.

179
00:13:05,720 --> 00:13:08,560
It's like a little bit noisier, and it's a little fatter, and things like that.

180
00:13:08,560 --> 00:13:12,960
But it's only kind of statistically noisier and statistically fatter, you know?

181
00:13:12,960 --> 00:13:19,160
And so these machine learning algorithms can do a better job than are just standard hard

182
00:13:19,160 --> 00:13:20,160
cuts.

183
00:13:20,160 --> 00:13:23,960
A human can still do better than the machine learning algorithm.

184
00:13:23,960 --> 00:13:27,760
You can look at all of these, but you don't want to look at billions of events.

185
00:13:27,760 --> 00:13:28,760
Interesting.

186
00:13:28,760 --> 00:13:29,760
Interesting.

187
00:13:29,760 --> 00:13:34,000
Do you have a sense that there are maybe more that physics, people with a physics background

188
00:13:34,000 --> 00:13:38,960
are somehow disproportionately represented in the machine learning and AI community?

189
00:13:38,960 --> 00:13:45,600
I'm only working on a small data set here, sample size, but I recently interviewed Josh

190
00:13:45,600 --> 00:13:52,560
Blum, who is an astrophysicist and did an AI start up.

191
00:13:52,560 --> 00:13:53,800
Have you seen that at all?

192
00:13:53,800 --> 00:13:56,000
Yeah, I do think so.

193
00:13:56,000 --> 00:14:00,360
And I run into a lot of people that have a physics background.

194
00:14:00,360 --> 00:14:07,920
And I think the reason that these two fields are kind of coming together is that AI is

195
00:14:07,920 --> 00:14:10,200
kind of just information physics.

196
00:14:10,200 --> 00:14:15,480
The way you try to solve a problem is exactly the same as how you would solve a particle

197
00:14:15,480 --> 00:14:16,480
physics problem.

198
00:14:16,480 --> 00:14:20,600
Like, get a ton of data, try to figure out what the trends are, try to see what the

199
00:14:20,600 --> 00:14:21,600
outliers are.

200
00:14:21,600 --> 00:14:24,520
Like, how do you actually tackle a problem?

201
00:14:24,520 --> 00:14:25,520
It's identical.

202
00:14:25,520 --> 00:14:29,080
I feel like I'm doing the exact same thing as physics, but I'm just doing it with voice

203
00:14:29,080 --> 00:14:31,080
now.

204
00:14:31,080 --> 00:14:33,840
Super interesting.

205
00:14:33,840 --> 00:14:39,800
So before we get too far from it, he mentioned a lot of your work happening in China and

206
00:14:39,800 --> 00:14:42,040
that there were some stories there.

207
00:14:42,040 --> 00:14:44,040
How did that happen?

208
00:14:44,040 --> 00:14:45,040
So connection.

209
00:14:45,040 --> 00:14:52,000
Yeah, there was just an upstart experiment as I was starting graduate school.

210
00:14:52,000 --> 00:14:55,840
This experiment was just sort of being circulated as a possibility.

211
00:14:55,840 --> 00:15:00,440
It wasn't even, there wasn't really anything going on yet, but people were like, hey, there's

212
00:15:00,440 --> 00:15:04,480
a spot underground in Western China, it's two miles underground, it would be the deepest

213
00:15:04,480 --> 00:15:09,760
lab in the world and it's not a lab yet, but we're trying to petition the Chinese government

214
00:15:09,760 --> 00:15:13,840
to turn it into a lab so that we can put a dark matter experiment there.

215
00:15:13,840 --> 00:15:20,240
And I was like, I am so in, you know, this, whatever that is that you just said, let's

216
00:15:20,240 --> 00:15:21,960
do it.

217
00:15:21,960 --> 00:15:24,080
You know, and that's actually what it was.

218
00:15:24,080 --> 00:15:28,560
I went to my, you know, my new advisor at the time, I was like, let's do it.

219
00:15:28,560 --> 00:15:30,400
He's like, you know, this isn't really a thing yet.

220
00:15:30,400 --> 00:15:31,400
And I'm like, I don't care.

221
00:15:31,400 --> 00:15:32,400
Let's do this, right?

222
00:15:32,400 --> 00:15:34,280
And it eventually turned into a thing.

223
00:15:34,280 --> 00:15:38,360
It involved, China is an interesting place, so I'll say that.

224
00:15:38,360 --> 00:15:47,320
But at first, there were failed talks to turn this into a lab and they went something like

225
00:15:47,320 --> 00:15:50,400
this, oh, well, okay, to back up just a second.

226
00:15:50,400 --> 00:15:54,920
Like why was this lab there anyway or why was this spot there?

227
00:15:54,920 --> 00:15:58,480
They were building the world's tallest hydroelectric dam right next to it.

228
00:15:58,480 --> 00:16:01,840
And they have all these tunneling machines and these mountains are made out of marble and

229
00:16:01,840 --> 00:16:03,480
marble is really easy to tunnel through.

230
00:16:03,480 --> 00:16:08,240
So they like Swiss cheese the mountain, you know, they just cut a whole bunch of holes

231
00:16:08,240 --> 00:16:15,320
in the mountain and there just happen to be a tunnel that is two miles underground.

232
00:16:15,320 --> 00:16:17,520
And this is where they were diverting water through.

233
00:16:17,520 --> 00:16:24,000
So it's actually a new type of hydroelectric dam where they have a really tall dam part

234
00:16:24,000 --> 00:16:25,640
and then they extract energy there.

235
00:16:25,640 --> 00:16:30,960
And then they go where there is a river that would go around a mountain, like a long distance

236
00:16:30,960 --> 00:16:34,080
around a mountain, instead they just tunnel through the mountain.

237
00:16:34,080 --> 00:16:37,240
And so the mountain itself is now a secondary dam essentially.

238
00:16:37,240 --> 00:16:38,240
Oh, wow.

239
00:16:38,240 --> 00:16:39,240
Yeah, you can look this up.

240
00:16:39,240 --> 00:16:42,920
It's the Jin Ping one and Jin Ping two dam in China and it was just completed like a

241
00:16:42,920 --> 00:16:44,760
year or two ago.

242
00:16:44,760 --> 00:16:47,120
But yeah, they, that's what they were doing.

243
00:16:47,120 --> 00:16:49,440
And so we were like, hey, you know, this is a great spot to do it.

244
00:16:49,440 --> 00:16:52,600
And we went there and said hello hydroelectric dam company.

245
00:16:52,600 --> 00:16:55,880
We are like ten scientists and we'd love to put an experiment down here and they're

246
00:16:55,880 --> 00:16:58,920
like, no.

247
00:16:58,920 --> 00:17:01,680
And so we were like, hmm, how can we work this a little better?

248
00:17:01,680 --> 00:17:06,920
And we were in, in kuhuts with Shanghai Jiao Tong University, which is, you know, one

249
00:17:06,920 --> 00:17:12,080
of, a very well known technical university in China.

250
00:17:12,080 --> 00:17:16,200
And we went to them and, well, we were in collaboration with them.

251
00:17:16,200 --> 00:17:20,200
And so like along with them and the leadership there, they said, okay, we'll talk to some

252
00:17:20,200 --> 00:17:21,360
people essentially.

253
00:17:21,360 --> 00:17:26,760
And so the educational system in China has a ton, a ton of power.

254
00:17:26,760 --> 00:17:32,240
And so they went to the hydro, the, the, like leaders of the education community went

255
00:17:32,240 --> 00:17:35,640
to the hydroelectric dam company and said, hey, do you guys want to do this?

256
00:17:35,640 --> 00:17:36,960
And they said, sure.

257
00:17:36,960 --> 00:17:39,960
So they have some sway there.

258
00:17:39,960 --> 00:17:42,320
It always helps to have the right people involved.

259
00:17:42,320 --> 00:17:43,600
Yeah, absolutely.

260
00:17:43,600 --> 00:17:46,080
And so, so that's how that all got started.

261
00:17:46,080 --> 00:17:51,040
And from that, from that yes moment to having a lab to actually work in, it was less than

262
00:17:51,040 --> 00:17:52,040
nine months.

263
00:17:52,040 --> 00:17:56,880
They had to, you know, get dynamite blast out a lab, like, turn it into an actual thing

264
00:17:56,880 --> 00:18:00,400
with cranes and, like, yellow railing.

265
00:18:00,400 --> 00:18:03,640
So it looks like a James Bond layer, you know, they had to do all of that.

266
00:18:03,640 --> 00:18:05,840
And they did it in, in less than nine months.

267
00:18:05,840 --> 00:18:09,280
And we were in there building our experiment right after that.

268
00:18:09,280 --> 00:18:16,720
And were you physically in China in this lab or virtually connected to it from Michigan?

269
00:18:16,720 --> 00:18:23,520
So I, I would physically, I would physically go there, like, four months out of the year,

270
00:18:23,520 --> 00:18:24,520
essentially.

271
00:18:24,520 --> 00:18:25,520
Okay.

272
00:18:25,520 --> 00:18:26,520
Yep.

273
00:18:26,520 --> 00:18:31,120
And that was, again, like for me, I grew up in a really small town in Michigan.

274
00:18:31,120 --> 00:18:37,720
And I kind of figured out that, like, the world exists, you know, outside of like a 38

275
00:18:37,720 --> 00:18:42,160
hundred person city or city, I shouldn't say city, you know, tiny town.

276
00:18:42,160 --> 00:18:48,480
And I, you know, was it, I went to, you know, undergrad in, in Missouri at a University

277
00:18:48,480 --> 00:18:49,480
of Missouri, St. Louis.

278
00:18:49,480 --> 00:18:52,560
And then I went to graduate school at University of Michigan.

279
00:18:52,560 --> 00:18:57,200
And the world is just getting bigger to me, you know, and, and then now I'm like, in

280
00:18:57,200 --> 00:19:00,880
China, halfway across the world, totally different people, totally different culture.

281
00:19:00,880 --> 00:19:02,760
And I was loving every minute of it.

282
00:19:02,760 --> 00:19:03,760
I thought it was great.

283
00:19:03,760 --> 00:19:04,760
And it's awesome.

284
00:19:04,760 --> 00:19:05,760
Yeah.

285
00:19:05,760 --> 00:19:09,200
And there was, there, there was so much to learn from being over there.

286
00:19:09,200 --> 00:19:14,240
But one of the biggest is that they have very few roadblocks, essentially.

287
00:19:14,240 --> 00:19:17,240
If you want to get something done and you have the resources to do it, you will get it

288
00:19:17,240 --> 00:19:19,680
done very quickly.

289
00:19:19,680 --> 00:19:23,640
And so that's why we could start the experiment, well, you know, that's why they could build

290
00:19:23,640 --> 00:19:24,880
a lab in nine months.

291
00:19:24,880 --> 00:19:27,040
That's why they could build this dam in five years.

292
00:19:27,040 --> 00:19:33,040
That's why we could start our experiment, like design the experiment, build the experiment,

293
00:19:33,040 --> 00:19:35,080
put it in, run it, et cetera.

294
00:19:35,080 --> 00:19:39,600
You, all of our data analysis, all in under four years, you know, because they, they were

295
00:19:39,600 --> 00:19:42,400
just like so helpful in removing roadblocks.

296
00:19:42,400 --> 00:19:46,320
So, so that was, that was really nice to see coming from the US.

297
00:19:46,320 --> 00:19:47,320
That's awesome.

298
00:19:47,320 --> 00:19:48,320
That's awesome.

299
00:19:48,320 --> 00:19:56,920
You, uh, so you worked on this project, finished your PhD, um, connected with your, your

300
00:19:56,920 --> 00:19:59,360
co-founder was also from, um, Michigan.

301
00:19:59,360 --> 00:20:00,360
Yeah.

302
00:20:00,360 --> 00:20:01,360
You was before.

303
00:20:01,360 --> 00:20:02,360
Yeah.

304
00:20:02,360 --> 00:20:06,560
Yeah, we were working on that experiment together, um, he's a decade younger than me.

305
00:20:06,560 --> 00:20:11,320
So, so essentially, he came into University of Michigan at like 15 or 16 years old and

306
00:20:11,320 --> 00:20:15,640
I was, you know, like an old graduate, old jaded graduate student, you know, at that

307
00:20:15,640 --> 00:20:16,640
point.

308
00:20:16,640 --> 00:20:20,760
And, uh, it started working in our lab and, you know, he is just very good and we hit

309
00:20:20,760 --> 00:20:26,720
it off and would spend a lot of time together, um, outside of his ex too, like mining bitcoin

310
00:20:26,720 --> 00:20:31,800
or building drones or whatever, you know, just kind of having fun and doing technical

311
00:20:31,800 --> 00:20:32,800
stuff together.

312
00:20:32,800 --> 00:20:33,800
Uh-huh.

313
00:20:33,800 --> 00:20:34,800
Yeah.

314
00:20:34,800 --> 00:20:35,800
Yeah.

315
00:20:35,800 --> 00:20:40,520
Uh, and so you guys, uh, you guys started this company, the, you, you talked about how the

316
00:20:40,520 --> 00:20:45,880
problems were similar, but how did the, what was the genesis for, you know, the idea behind

317
00:20:45,880 --> 00:20:47,880
what you're doing at deep-gram?

318
00:20:47,880 --> 00:20:48,880
Yeah.

319
00:20:48,880 --> 00:20:52,120
So, so Noah definitely has to take credit for this, my co-founder.

320
00:20:52,120 --> 00:20:57,640
So I was a little naysayer at first, um, where he, he wanted to start recording his life.

321
00:20:57,640 --> 00:21:02,600
So he built a, a little device out of like an Intel Edison, um, or a Raspberry Pi, one

322
00:21:02,600 --> 00:21:08,200
of these that it just had a battery, a Wi-Fi antenna, microphone, and a, you know, process

323
00:21:08,200 --> 00:21:14,240
around it, essentially, and he set it up so that it would record 24-7 and basically every

324
00:21:14,240 --> 00:21:18,480
minute make a new audio file and just sort of dump it into, um, storage.

325
00:21:18,480 --> 00:21:22,080
And then whenever it came in contact with Wi-Fi, it would upload all of it.

326
00:21:22,080 --> 00:21:23,080
Oh, wow.

327
00:21:23,080 --> 00:21:27,200
So essentially, he was backing up his life, backing up his audio life at least, uh-huh.

328
00:21:27,200 --> 00:21:29,840
And I was like, what are you going to do with all of that?

329
00:21:29,840 --> 00:21:32,920
You know, you're, trust me, your life's not that interesting.

330
00:21:32,920 --> 00:21:37,320
You know, in real time, you're not going to go back and listen to it again, right?

331
00:21:37,320 --> 00:21:39,000
And he's like, yeah, whatever, right?

332
00:21:39,000 --> 00:21:43,720
And so he recorded like two weeks of his life, you know, several hundred hours and, you

333
00:21:43,720 --> 00:21:46,920
know, you're sitting there at the end of it thinking like, oh yeah, okay, now this is

334
00:21:46,920 --> 00:21:47,920
a real problem.

335
00:21:47,920 --> 00:21:50,840
How are we going to go back and find anything that we want to hear?

336
00:21:50,840 --> 00:21:55,040
And so we started, we did the first thing that you would think of, um, hey, let's turn

337
00:21:55,040 --> 00:21:57,240
this into text, you know, speech to text.

338
00:21:57,240 --> 00:22:02,560
And, um, when that didn't work very well, because, uh, this, that's just like the state of

339
00:22:02,560 --> 00:22:09,400
speech to text, essentially, where if you have a microphone that is not super high quality

340
00:22:09,400 --> 00:22:14,400
and it's not right up in somebody's face and the person is not annunciating really well,

341
00:22:14,400 --> 00:22:17,800
then you're going to get pretty bad word error rate, meaning, you know, it's not going

342
00:22:17,800 --> 00:22:18,800
to be that accurate.

343
00:22:18,800 --> 00:22:19,800
Right, right.

344
00:22:19,800 --> 00:22:24,360
And, and so, so that's, that's a situation for like almost all of the world's audio.

345
00:22:24,360 --> 00:22:28,480
It's not great audio, um, but in this case, all of the audio was like that.

346
00:22:28,480 --> 00:22:31,840
And we're like, okay, this speech to text thing is not going to work.

347
00:22:31,840 --> 00:22:33,640
Um, can we do something better?

348
00:22:33,640 --> 00:22:36,960
Maybe, maybe somebody in the world out there like Google, hey, they know how to do search,

349
00:22:36,960 --> 00:22:38,280
they know how to do audio.

350
00:22:38,280 --> 00:22:41,840
Maybe they've made some kind of API or something that can do this audio search, right?

351
00:22:41,840 --> 00:22:45,760
And so we started playing around and looking there and we just couldn't find anything.

352
00:22:45,760 --> 00:22:51,680
And, uh, we found papers actually papers from like 2008, 2009, where Google wrote about

353
00:22:51,680 --> 00:22:55,480
this type of thing like doing search and audio, um, their technique was to turn it into

354
00:22:55,480 --> 00:22:59,160
text and then tried to search in it and it worked pretty well when it was on a reduced

355
00:22:59,160 --> 00:23:00,160
domain.

356
00:23:00,160 --> 00:23:06,880
So they did like, uh, political speeches and, uh, yeah, and they had fairly good results,

357
00:23:06,880 --> 00:23:09,320
but, you know, when you try to generalize it, it doesn't work very well because everybody

358
00:23:09,320 --> 00:23:13,360
speaks a little bit differently and political speeches are well recorded and, you know, everybody's

359
00:23:13,360 --> 00:23:16,760
annunciating, you know, so it goes pretty well then.

360
00:23:16,760 --> 00:23:20,200
But if you try to do this in a general sense, it doesn't work that well and we, we emailed

361
00:23:20,200 --> 00:23:24,920
us or, um, you know, set up a Skype call, um, with one of the engineers on the paper and

362
00:23:24,920 --> 00:23:26,960
said like, what are you guys doing now?

363
00:23:26,960 --> 00:23:31,080
Because, um, you know, some years had passed, you know, surely guys are working on this

364
00:23:31,080 --> 00:23:32,080
problem, right?

365
00:23:32,080 --> 00:23:33,880
And they're like, no, no, no, we gave up on that.

366
00:23:33,880 --> 00:23:34,880
We're not doing that.

367
00:23:34,880 --> 00:23:38,080
Um, we're just trying to make our speech a text better.

368
00:23:38,080 --> 00:23:39,080
Okay.

369
00:23:39,080 --> 00:23:40,080
Um, we're like, huh, okay.

370
00:23:40,080 --> 00:23:41,400
So it's still not very good.

371
00:23:41,400 --> 00:23:42,680
That doesn't help us.

372
00:23:42,680 --> 00:23:45,680
Um, so, so we started just working on ourselves.

373
00:23:45,680 --> 00:23:49,640
We were like, okay, you know, we know signals, we know machine learning, we know how to deal

374
00:23:49,640 --> 00:23:54,200
with lots and lots of data and, uh, let's see if we can make some kind of search engine

375
00:23:54,200 --> 00:23:55,200
for audio.

376
00:23:55,200 --> 00:24:01,040
And over the course of about four or five months, we went from, you know, very poor accuracy,

377
00:24:01,040 --> 00:24:07,720
meaning like maybe 20% of the time you'll find what you're looking for to, uh, 80 or 90%

378
00:24:07,720 --> 00:24:09,360
of the time finding what you're looking for.

379
00:24:09,360 --> 00:24:13,800
So that's like going, you know, state of the arts, 20%, you're at 90%, you're feeling

380
00:24:13,800 --> 00:24:14,800
pretty good.

381
00:24:14,800 --> 00:24:19,560
Um, so, so that's, uh, what the genesis of Deepgram essentially, we're like, whoa,

382
00:24:19,560 --> 00:24:21,640
this has a lot of applications.

383
00:24:21,640 --> 00:24:24,680
And at the time, you know, we were coming out of academia and we're like, hey, students

384
00:24:24,680 --> 00:24:26,800
can use this for lectures and whatever.

385
00:24:26,800 --> 00:24:31,040
But when we start to think about it a lot more, we're like, you know what, the world, like,

386
00:24:31,040 --> 00:24:36,480
the big audio source in the world, the huge data lake of audio in the world is like recorded

387
00:24:36,480 --> 00:24:37,800
business calls.

388
00:24:37,800 --> 00:24:42,000
So customer service calls and things like that, just call center stuff.

389
00:24:42,000 --> 00:24:43,360
And they're all really low quality.

390
00:24:43,360 --> 00:24:47,040
Nobody knows what to do with them, essentially, and we're like, man, like a Deepgram could

391
00:24:47,040 --> 00:24:50,560
be a massive company, um, if we do this right, yeah.

392
00:24:50,560 --> 00:24:56,760
Well, I think the other killer use case is, uh, I don't know if you're married or not,

393
00:24:56,760 --> 00:25:00,840
but my wife and I always have this conversation where, oh, I just told you this or no, you

394
00:25:00,840 --> 00:25:02,160
didn't say that or whatever.

395
00:25:02,160 --> 00:25:07,240
If I had his device and recorded all of our audio and then could easily go back and prove

396
00:25:07,240 --> 00:25:11,560
whether I said that or not, uh, there's got to be a huge market for that.

397
00:25:11,560 --> 00:25:16,600
I love, I love this use case for Deepgram because yes, I feel the same way.

398
00:25:16,600 --> 00:25:19,680
Actually, we've run across a lot of people though that are like, oh, no, this will make

399
00:25:19,680 --> 00:25:20,680
things worse.

400
00:25:20,680 --> 00:25:27,600
They probably would because my, uh, I think I probably think I'm right more than I actually

401
00:25:27,600 --> 00:25:29,400
am much more than I actually am.

402
00:25:29,400 --> 00:25:34,720
Yeah, exactly, but, um, but no, I think there's a regularizing effect there that if you

403
00:25:34,720 --> 00:25:38,920
know that there's something, uh, recording maybe before you say, I know I said this.

404
00:25:38,920 --> 00:25:43,880
You'll actually go back and check or something, you know, so I think it could work out.

405
00:25:43,880 --> 00:25:47,720
Actually, we do, we do kind of think about this problem.

406
00:25:47,720 --> 00:25:50,320
Um, I think that Mark is not really ready for it yet.

407
00:25:50,320 --> 00:25:55,400
You know, if you just put a recorder on everybody and said, you know, yeah, now we're backing

408
00:25:55,400 --> 00:25:58,000
up everybody's life and everything you say is recorded.

409
00:25:58,000 --> 00:26:02,520
Uh, I don't think people right now are going to be super into that.

410
00:26:02,520 --> 00:26:08,400
But it might come, um, and, and I kind of hope it does like, like for me, I wish I, I

411
00:26:08,400 --> 00:26:12,200
had it and we do have these devices like laying around our office from those days, you

412
00:26:12,200 --> 00:26:13,840
know, where I think, man, if I was just, you know, you know, I think, man, if I was just

413
00:26:13,840 --> 00:26:16,600
wearing that all day every day, um, then that would be great.

414
00:26:16,600 --> 00:26:18,240
Of course, they're very heavy.

415
00:26:18,240 --> 00:26:20,920
So we don't do that now, but you could make it much smaller.

416
00:26:20,920 --> 00:26:21,920
Yeah.

417
00:26:21,920 --> 00:26:23,480
Probably a lot later than they were then.

418
00:26:23,480 --> 00:26:24,480
Yep.

419
00:26:24,480 --> 00:26:26,800
Um, interesting.

420
00:26:26,800 --> 00:26:33,600
So you described the, the frustration that you were having with, uh, trying to do this

421
00:26:33,600 --> 00:26:40,680
and the state of the art being turning the audio into text and then running a search

422
00:26:40,680 --> 00:26:45,800
engine against that text is the implication then that you guys are not, uh, using text

423
00:26:45,800 --> 00:26:47,160
as an intermediary.

424
00:26:47,160 --> 00:26:48,160
Yes.

425
00:26:48,160 --> 00:26:49,160
That is true.

426
00:26:49,160 --> 00:26:52,240
So, um, we, we do build an index just like you would.

427
00:26:52,240 --> 00:26:55,840
So, you know, this, this is sort of the terminology that you would say before you built

428
00:26:55,840 --> 00:26:57,120
an index out of text, right?

429
00:26:57,120 --> 00:26:59,520
Well, we're not building an index out of text anymore.

430
00:26:59,520 --> 00:27:03,840
We're building an index, you know, like actually out of activations in a deep neural network,

431
00:27:03,840 --> 00:27:08,720
you know, uh, so, so activations in a representation that's deep in a deep neural network.

432
00:27:08,720 --> 00:27:13,280
Um, the best way to think of that though is that you're doing something kind of like searching

433
00:27:13,280 --> 00:27:14,440
through phonemes.

434
00:27:14,440 --> 00:27:18,520
So phonemes are like the, uh, the alphabet of speech, right?

435
00:27:18,520 --> 00:27:24,680
Um, but they're kind of a human, um, they're, like humans have assigned to those.

436
00:27:24,680 --> 00:27:27,880
Like those, these are the 40 phonemes or these are the 53 phonemes.

437
00:27:27,880 --> 00:27:29,480
Like you don't have to do that.

438
00:27:29,480 --> 00:27:33,720
You can just have it be defined by the data and, um, so that, that's what we do.

439
00:27:33,720 --> 00:27:39,920
But if you look at the, if you look at what these match up to, it's very similar to phonemes.

440
00:27:39,920 --> 00:27:45,480
So that, uh, statement that you quickly, uh, went by kind of blew my mind a little bit.

441
00:27:45,480 --> 00:27:51,280
You're indexing, you're building an index out of activations deep in a neural network that

442
00:27:51,280 --> 00:27:53,160
elaborate on that a little bit for us.

443
00:27:53,160 --> 00:27:54,160
Yeah, absolutely.

444
00:27:54,160 --> 00:27:59,720
So the, the output of a neural network is, is again, just an activation essentially, um,

445
00:27:59,720 --> 00:28:05,360
and what you're doing is you're forcing it to like, in the case of a speech recognition,

446
00:28:05,360 --> 00:28:09,800
you're forcing it to guess like, what is the word being said at this time, um, or, or

447
00:28:09,800 --> 00:28:15,840
in the case of like doing some multi-class, um, uh, uh, network that's trying to guess,

448
00:28:15,840 --> 00:28:20,480
you know, is this a handwritten zero or handwritten two or something like that that's trying

449
00:28:20,480 --> 00:28:21,480
to guess that.

450
00:28:21,480 --> 00:28:22,480
Yeah.

451
00:28:22,480 --> 00:28:23,480
Right.

452
00:28:23,480 --> 00:28:27,960
And so it, if you just sort of back up one layer from that, it's still pretty close

453
00:28:27,960 --> 00:28:32,760
to, to that output, you know, it's just some like linear combination and that has been

454
00:28:32,760 --> 00:28:36,240
stuck through an activation, um, to get to that output.

455
00:28:36,240 --> 00:28:42,240
And so, so, you know, you can, you can think of, um, images where it, at the very top,

456
00:28:42,240 --> 00:28:46,440
it depends on how you think of neural networks, but at the, at the, near the input of the,

457
00:28:46,440 --> 00:28:52,600
of the neural network, um, there's, it's looking for edges or rower things or something

458
00:28:52,600 --> 00:28:53,600
like that, right?

459
00:28:53,600 --> 00:28:56,760
And then as you get deeper, it's looking more for like faces or trees or something like

460
00:28:56,760 --> 00:28:57,840
that, right?

461
00:28:57,840 --> 00:29:01,240
And the same thing is happening in audio and now it's looking for things that are kind

462
00:29:01,240 --> 00:29:05,000
of word like or kind of like portions of a word like, right?

463
00:29:05,000 --> 00:29:10,000
And, um, so, so those activations are what we are interested in.

464
00:29:10,000 --> 00:29:14,080
And how deep are the networks that you're typically using and how do you know which of

465
00:29:14,080 --> 00:29:20,720
those activations to tap into or capture and, uh, is that static or dynamic?

466
00:29:20,720 --> 00:29:27,720
Does that change, uh, based on the utterances or how do you figure all that out?

467
00:29:27,720 --> 00:29:32,440
Very, very great question, um, you can, so there's several ways to go about this problem.

468
00:29:32,440 --> 00:29:36,560
You can just say literally let the data define this and, um, go to town.

469
00:29:36,560 --> 00:29:39,360
Um, that, that does work pretty well.

470
00:29:39,360 --> 00:29:43,560
Um, you can, you can sort of get a boost in accuracy, uh, a little bit of a boost in

471
00:29:43,560 --> 00:29:44,560
accuracy.

472
00:29:44,560 --> 00:29:49,040
If you, if you pin it at first, like when you're training, you might pin it to phonemes

473
00:29:49,040 --> 00:29:55,920
or you might pin it to some, uh, larger subset and, uh, then, then remove that restriction.

474
00:29:55,920 --> 00:29:59,160
So it's sort of seeded that it should have learned something like this.

475
00:29:59,160 --> 00:30:03,760
And then now it sort of builds off that knowledge, um, so, so that's kind of how you can guide

476
00:30:03,760 --> 00:30:08,120
the network, you know, you're pointing it in a direction that's approximately correct,

477
00:30:08,120 --> 00:30:13,200
but then you relax that and allow it to pick the things that are, that are working best.

478
00:30:13,200 --> 00:30:17,760
Um, the things that we're working on right now to make this even more accurate are instead

479
00:30:17,760 --> 00:30:24,040
of, uh, a word target, uh, you are searching for a light, oh, sorry, not searching.

480
00:30:24,040 --> 00:30:28,200
You're trying to force the network to guess like a topic or a part of speech, you know,

481
00:30:28,200 --> 00:30:30,160
is it a noun or a verb or something like that?

482
00:30:30,160 --> 00:30:32,440
And you do all of these things at the same time.

483
00:30:32,440 --> 00:30:36,800
So your network is sort of branched out and it knows, you know, I'm, I'm trying to guess

484
00:30:36,800 --> 00:30:37,800
words fine.

485
00:30:37,800 --> 00:30:38,800
I'm trying to guess the topic.

486
00:30:38,800 --> 00:30:40,400
I'm trying to guess all of these things.

487
00:30:40,400 --> 00:30:46,080
And so when you, when you force that, um, uh, restriction on the network, then it, it

488
00:30:46,080 --> 00:30:51,600
actually, it, you can control kind of what the activations are, are the information

489
00:30:51,600 --> 00:30:55,360
that the activations are holding, you can kind of control that by the target.

490
00:30:55,360 --> 00:31:02,880
Uh, so how, uh, can you say how deep your network's typically are, they, what you would

491
00:31:02,880 --> 00:31:09,000
think of is, you know, very deep, you know, tens, hundreds, uh, or more layers or, are

492
00:31:09,000 --> 00:31:10,000
they relatively shallow?

493
00:31:10,000 --> 00:31:14,040
Yeah, we're in the tens, um, in the tens category.

494
00:31:14,040 --> 00:31:19,400
And, um, yeah, we, so we, we have had a lot of success in that regime.

495
00:31:19,400 --> 00:31:23,840
If you go deeper, you need, um, a lot more firepower on the computational side.

496
00:31:23,840 --> 00:31:28,040
So, and you actually, and you need, um, more engineering on the, like, where do I put

497
00:31:28,040 --> 00:31:33,040
my skips and where do I put, like, uh, everything like that, you know, um, architecture becomes

498
00:31:33,040 --> 00:31:34,040
more complex.

499
00:31:34,040 --> 00:31:35,040
Yeah, exactly.

500
00:31:35,040 --> 00:31:40,200
And so, um, so we're, uh, a little, we're like an eight person team.

501
00:31:40,200 --> 00:31:46,040
So we don't, uh, yeah, we, we, we tried to, um, minimize some of those, uh, engineering

502
00:31:46,040 --> 00:31:51,320
bottlenecks as much as possible, um, and we've seen very good, oh, well, I guess, I guess

503
00:31:51,320 --> 00:31:55,040
another way to say this is that if the state of the arts 20% and you're like in the 80 and

504
00:31:55,040 --> 00:32:00,040
90% range with the networks that you have, you know, then, like, you could spend a lot

505
00:32:00,040 --> 00:32:05,160
of time turning it into, like, 92% accurate, um, by making your, um, making your network

506
00:32:05,160 --> 00:32:07,120
a lot deeper or something like that.

507
00:32:07,120 --> 00:32:12,200
But we, yeah, we, we don't focus too much on that yet until we have the pressure, um,

508
00:32:12,200 --> 00:32:13,200
to do that.

509
00:32:13,200 --> 00:32:14,200
Okay.

510
00:32:14,200 --> 00:32:15,200
Okay.

511
00:32:15,200 --> 00:32:22,080
So with the network in the tens of layers, then it sounds like it's, you have a pretty

512
00:32:22,080 --> 00:32:27,240
good sense based on your architecture of where, uh, you know, what layers, the phoneme

513
00:32:27,240 --> 00:32:34,680
layer versus, I don't even know names for the, the other, um, we don't know names either.

514
00:32:34,680 --> 00:32:35,680
Honestly.

515
00:32:35,680 --> 00:32:41,760
Like this, this is, it would be really, um, this hasn't been explored as much as images,

516
00:32:41,760 --> 00:32:45,640
you know, there, in, you know, there's sort of reasons for that, like, where are the data

517
00:32:45,640 --> 00:32:46,640
sets to do this?

518
00:32:46,640 --> 00:32:54,160
You know, um, also humans, I think, are not as interested, um, or I guess it's harder to,

519
00:32:54,160 --> 00:32:58,520
to ingest audio than it is images, images, you know, you're like instantly see it.

520
00:32:58,520 --> 00:33:03,360
You're also entertained by the images that you see that you see, but the, um, if you hear

521
00:33:03,360 --> 00:33:08,440
audio that you're not super interested in, um, you're like, you, you, your eyes glaze over

522
00:33:08,440 --> 00:33:09,440
right away, right?

523
00:33:09,440 --> 00:33:14,400
And so labeling the audio data is expensive and time consuming.

524
00:33:14,400 --> 00:33:18,080
And you can't just like click through a bunch of things and now you've labeled a ton

525
00:33:18,080 --> 00:33:21,840
of images, you know, you can't just click through a bunch of things and label a bunch of

526
00:33:21,840 --> 00:33:25,920
audio files, you know, like, that didn't work.

527
00:33:25,920 --> 00:33:31,240
And so those, those are kind of the problems that audio faces, but also, uh, you can't

528
00:33:31,240 --> 00:33:36,560
point to things and, um, you can't, you can't point to things and say, like that little

529
00:33:36,560 --> 00:33:40,760
edge feature right there is important because you're hearing it, right?

530
00:33:40,760 --> 00:33:44,480
It's, it's just not, we don't have that visualization capability.

531
00:33:44,480 --> 00:33:46,960
And so, uh, it's a little more challenging in that regard.

532
00:33:46,960 --> 00:33:50,560
So, so yeah, we don't have names for this stuff either, you know, um, there are, there

533
00:33:50,560 --> 00:33:55,800
are linguists out there in the world that probably have some, some names, you know, uh, but

534
00:33:55,800 --> 00:34:00,480
we just look at like a 2D FFT, um, you know, a spectrogram of the audio and say, like,

535
00:34:00,480 --> 00:34:02,880
oh, I can sort of see what's going on there.

536
00:34:02,880 --> 00:34:06,560
And you just kind of treat it like it's an image and think of it that way.

537
00:34:06,560 --> 00:34:07,560
Okay.

538
00:34:07,560 --> 00:34:14,520
Uh, so the other interesting, um, the interesting use case that occurred to me when I first

539
00:34:14,520 --> 00:34:20,880
saw your stuff was, you know, I've got a bunch of audio in the form of podcasts, podcast

540
00:34:20,880 --> 00:34:22,200
interviews.

541
00:34:22,200 --> 00:34:27,160
And, uh, I'd love to find a way to make that more accessible.

542
00:34:27,160 --> 00:34:31,520
And one idea that I've had for a while is, hey, it'd be great if I had a, a bot, like

543
00:34:31,520 --> 00:34:37,160
a Facebook messenger bot or a chat bot or whatever, where, you know, you, you pull this thing

544
00:34:37,160 --> 00:34:42,920
up and it's a, and you can ask it, hey, I want to find 20 episodes about convolutional

545
00:34:42,920 --> 00:34:44,200
neural nets.

546
00:34:44,200 --> 00:34:49,560
And it will, you know, look in its index and tell you, oh, check out, you know, episodes,

547
00:34:49,560 --> 00:34:55,160
you know, 13 and five at times x, y and z.

548
00:34:55,160 --> 00:34:59,640
It sounds like you're saying that your stuff could be a part of building that.

549
00:34:59,640 --> 00:35:04,120
So what would the process be to, you know, to get to what I'm talking about?

550
00:35:04,120 --> 00:35:08,480
Forget all the bot stuff, but, you know, just in terms of the voice search engine piece.

551
00:35:08,480 --> 00:35:10,360
Yeah, that's exactly right.

552
00:35:10,360 --> 00:35:17,320
The, um, the, the, the mentions is a part of it and topics is another part of it.

553
00:35:17,320 --> 00:35:20,360
And that's exactly the type of problem that DeepGram solves.

554
00:35:20,360 --> 00:35:24,040
So you have, uh, you have a lot of audio, right?

555
00:35:24,040 --> 00:35:27,760
And you have, you have listeners that don't want to sit through, you know, 20 hours of

556
00:35:27,760 --> 00:35:32,480
audio, trying to look for that one little tidbit, um, of information that they're, that

557
00:35:32,480 --> 00:35:34,400
they're interested in that time.

558
00:35:34,400 --> 00:35:37,360
And, um, that is what DeepGram solves.

559
00:35:37,360 --> 00:35:45,320
And that is actually, um, kind of a, a, a really interesting interactive experience once

560
00:35:45,320 --> 00:35:49,800
you finally do it, you know, so like when we first built DeepGram and we made, you know,

561
00:35:49,800 --> 00:35:54,680
the search engine and actually found moments where we were like, whoa, there, I, this is exactly

562
00:35:54,680 --> 00:35:55,680
what I was looking for.

563
00:35:55,680 --> 00:35:59,880
You know, it's a totally, totally different experience than what you would feel if you

564
00:35:59,880 --> 00:36:01,760
had tried to do this with speech detects.

565
00:36:01,760 --> 00:36:05,840
And so, so yeah, we're, we're very into solving that type of problem.

566
00:36:05,840 --> 00:36:11,320
And, and the way that you do it with DeepGram is we, um, we provide a, an API.

567
00:36:11,320 --> 00:36:17,040
So you go and sign up at dgram.com and, um, create an account and there, you upload your

568
00:36:17,040 --> 00:36:19,920
audio and you give us the query that you're looking for.

569
00:36:19,920 --> 00:36:24,480
So if you're looking for convolutional neural nets or CNNs or, uh, convolution, you

570
00:36:24,480 --> 00:36:28,720
know, put those keywords in, send it to the API and you'll get the results back of all

571
00:36:28,720 --> 00:36:32,080
of the, uh, mentions that were in your audio.

572
00:36:32,080 --> 00:36:37,840
And, uh, if you want to make that go even deeper to, it's, where it's more topic based

573
00:36:37,840 --> 00:36:42,060
rather than keyword based, then you like sort of work with us, uh, for a custom API end

574
00:36:42,060 --> 00:36:48,160
point to build a model that is more, um, tuned to the topics that you care about.

575
00:36:48,160 --> 00:36:54,440
And that is essentially you guys working in a professional services or whatever capacity

576
00:36:54,440 --> 00:36:59,880
needs, uh, build some, um, I don't know, what is that, what does that process look like

577
00:36:59,880 --> 00:37:00,880
from your perspective?

578
00:37:00,880 --> 00:37:03,960
What are you actually doing behind the scenes to make the, the topical stuff work?

579
00:37:03,960 --> 00:37:04,960
Yeah.

580
00:37:04,960 --> 00:37:05,960
Yeah.

581
00:37:05,960 --> 00:37:06,960
Good, good question.

582
00:37:06,960 --> 00:37:08,560
Um, the, so the general stuff kind of works right off the bat.

583
00:37:08,560 --> 00:37:11,640
Hey, you, you want to search for a keyword, you know, we have a lot of data that we've

584
00:37:11,640 --> 00:37:13,000
already trained on.

585
00:37:13,000 --> 00:37:16,360
And even if your keyword isn't in there, we can find it because it's essentially a fuzzy

586
00:37:16,360 --> 00:37:17,360
search.

587
00:37:17,360 --> 00:37:22,200
Um, but if you want to do this topic modeling, uh, then what you do is give us labeled

588
00:37:22,200 --> 00:37:23,200
data.

589
00:37:23,200 --> 00:37:29,000
So in your case, it would be, um, I have, I have all these episodes and they were the

590
00:37:29,000 --> 00:37:34,280
contents of these episodes, uh, you would have to know this, um, you know, yeah, we talked

591
00:37:34,280 --> 00:37:38,200
about convolutional neural networks in this one, um, we talked about, you know, recurrent

592
00:37:38,200 --> 00:37:41,840
neural networks in this one and boosted decision trees in this and, you know, we talked

593
00:37:41,840 --> 00:37:44,080
about productizing in this one.

594
00:37:44,080 --> 00:37:50,600
And, uh, you just put those simple labels in and, um, then you train a, um, model and

595
00:37:50,600 --> 00:37:53,600
it doesn't have to be a deep neural network either at that point because you have such

596
00:37:53,600 --> 00:37:58,560
a small amount of data, you probably do something like a, uh, boosted decision tree, um,

597
00:37:58,560 --> 00:38:03,840
but you train this model to do that topic prediction based on the search in your, in your

598
00:38:03,840 --> 00:38:04,840
audio.

599
00:38:04,840 --> 00:38:07,840
And this is actually getting really into the weeds, but the way that it actually works is

600
00:38:07,840 --> 00:38:11,040
we utilize the search for that topic modeling.

601
00:38:11,040 --> 00:38:17,640
So our, our search is, is the best in the world is the most accurate, um, and, and if you

602
00:38:17,640 --> 00:38:21,760
train a model with a target that says, here are these topics.

603
00:38:21,760 --> 00:38:25,480
And I want you to be able to predict these topics and I want you to be able to generalize

604
00:38:25,480 --> 00:38:31,000
to other audio files, then our, our model that we're building, whatever it is, but that

605
00:38:31,000 --> 00:38:37,640
topic modeling, uh, model, it is going through and it's not doing any heuristics whatsoever.

606
00:38:37,640 --> 00:38:41,880
It's sort of exhaustively searching all the possible phrases and saying, like, which

607
00:38:41,880 --> 00:38:44,120
ones help and which ones don't?

608
00:38:44,120 --> 00:38:48,440
And it's recursively eliminating the ones that don't help and, and it's arriving on things

609
00:38:48,440 --> 00:38:50,960
to search for that are very good.

610
00:38:50,960 --> 00:38:52,960
And it doesn't have to be a single search term.

611
00:38:52,960 --> 00:38:57,280
It could be tensing, you know, tens search terms and it also might be the non-existence of

612
00:38:57,280 --> 00:38:58,280
a term.

613
00:38:58,280 --> 00:39:01,400
It's like, you know, if, if you're talking about these 10 things and this other one, then

614
00:39:01,400 --> 00:39:04,880
it's some other topic, you know, and it, et cetera, and I don't even know how it works,

615
00:39:04,880 --> 00:39:05,880
right?

616
00:39:05,880 --> 00:39:07,800
It, that's, that's how it works.

617
00:39:07,800 --> 00:39:11,720
And then in the end, you look at the output and you say, like, these are the things

618
00:39:11,720 --> 00:39:14,920
it's saying search for and these are the things it's saying don't search for the area,

619
00:39:14,920 --> 00:39:17,040
you know, you don't want these to exist.

620
00:39:17,040 --> 00:39:20,560
And that's how the model works, you know, and it works extremely well.

621
00:39:20,560 --> 00:39:25,840
So, um, so yeah, that's, that's sort of, you know, how the sausage is made.

622
00:39:25,840 --> 00:39:33,400
And how much, how much data do you need in order for you to, to start getting, uh, interesting

623
00:39:33,400 --> 00:39:37,480
results, both from the basic search and, uh, the topic modeling?

624
00:39:37,480 --> 00:39:43,720
Or is the fact that you guys have already trained your model, like, well, let's start with

625
00:39:43,720 --> 00:39:44,720
that question.

626
00:39:44,720 --> 00:39:49,520
Um, is the fact that you guys have already trained the model on the data that you already

627
00:39:49,520 --> 00:39:50,520
have?

628
00:39:50,520 --> 00:39:57,640
Um, does that mean that I don't need to have some, uh, some specific level of data about

629
00:39:57,640 --> 00:40:00,480
my domain in order to get good results?

630
00:40:00,480 --> 00:40:01,480
Yeah.

631
00:40:01,480 --> 00:40:05,840
In some cases, that, that's how it works where, you know, if you have sort of like a broad

632
00:40:05,840 --> 00:40:10,840
topic, like, is this about sports or is this about politics, you know, um, then, then

633
00:40:10,840 --> 00:40:15,760
that data is sort of already lying around and, and, and incorporated into a model.

634
00:40:15,760 --> 00:40:19,800
But if you're talking about niche, niche markets, you know, um, then you probably want to

635
00:40:19,800 --> 00:40:24,240
supply something and, uh, then we'll, you know, build a model on top of that.

636
00:40:24,240 --> 00:40:28,480
But that doesn't, the data that you supply probably isn't the only training data that

637
00:40:28,480 --> 00:40:31,560
we're using, you know, we're just, we're just supplying that.

638
00:40:31,560 --> 00:40:35,040
It's essentially a form of transfer learning and then we're like changing the target of

639
00:40:35,040 --> 00:40:37,600
the model, mm-hmm.

640
00:40:37,600 --> 00:40:39,360
And so what am I supplying then?

641
00:40:39,360 --> 00:40:40,360
Yeah.

642
00:40:40,360 --> 00:40:44,400
You would supply like audio with some tags, um, so like here's an hour long audio and

643
00:40:44,400 --> 00:40:48,680
here are the 20 things we talked about or here are the 10 questions I asked or something

644
00:40:48,680 --> 00:40:49,680
like that.

645
00:40:49,680 --> 00:40:50,680
Okay.

646
00:40:50,680 --> 00:40:57,600
And, uh, the neural net can just figure out what's relevant and what is, uh, and kind of

647
00:40:57,600 --> 00:40:59,560
map those to the tags.

648
00:40:59,560 --> 00:41:00,560
Exactly.

649
00:41:00,560 --> 00:41:06,280
And even given, uh, you know, I would imagine a fairly limited or what, how much, how much

650
00:41:06,280 --> 00:41:12,120
audio and tags do I need to give the model for that to be useful for it, for it to be

651
00:41:12,120 --> 00:41:17,320
useful to a, like a consumer that's searching for something, um, because they're fairly

652
00:41:17,320 --> 00:41:18,880
error tolerant actually, right?

653
00:41:18,880 --> 00:41:23,160
Like if you get, if you get one out of two wrong, that's not too bad as long as one of

654
00:41:23,160 --> 00:41:24,160
those is okay, right?

655
00:41:24,160 --> 00:41:27,600
As long as one of those is what you're actually looking for, so you probably need something

656
00:41:27,600 --> 00:41:33,200
like, um, 50 or 100, um, different labeled files and you'll get a results that are similar

657
00:41:33,200 --> 00:41:34,200
to that.

658
00:41:34,200 --> 00:41:38,000
If you have around a thousand labeled files, then, um, then you get results that are

659
00:41:38,000 --> 00:41:40,680
more like, you know, 90% accuracy.

660
00:41:40,680 --> 00:41:43,760
And these are hour long files or shorter?

661
00:41:43,760 --> 00:41:48,000
It, it kind of depends on what you're searching for, but, um, they're generally in like the

662
00:41:48,000 --> 00:41:49,800
10 to hour long range.

663
00:41:49,800 --> 00:41:52,720
Um, you'll need more files if it's only like a minute long.

664
00:41:52,720 --> 00:41:53,720
Okay.

665
00:41:53,720 --> 00:41:54,720
Yeah.

666
00:41:54,720 --> 00:41:56,320
Oh, interesting, interesting.

667
00:41:56,320 --> 00:42:03,240
And so that, so then what you guys are offering is a service, um, and it, that exposes an API,

668
00:42:03,240 --> 00:42:07,960
but you also have done some work around, uh, an open source framework.

669
00:42:07,960 --> 00:42:10,560
Um, let's talk about that for a little bit.

670
00:42:10,560 --> 00:42:13,960
So, uh, why don't you walk us through what you've done there?

671
00:42:13,960 --> 00:42:14,960
Yeah.

672
00:42:14,960 --> 00:42:16,000
We're, we're extremely pumped about it.

673
00:42:16,000 --> 00:42:17,000
It's called Kerr.

674
00:42:17,000 --> 00:42:18,000
It's open source.

675
00:42:18,000 --> 00:42:19,320
You can go contribute to it.

676
00:42:19,320 --> 00:42:25,840
It's on GitHub, um, but the, the idea behind Kerr is that, uh, years ago, you know, around

677
00:42:25,840 --> 00:42:30,560
a decade ago, GPUs came onto the scene for neural networks, um, data started to become

678
00:42:30,560 --> 00:42:36,600
available and it became a really smart idea to start training neural networks on GPUs.

679
00:42:36,600 --> 00:42:41,320
And the people doing it back in the day knew something about GPUs, it, you know, in order

680
00:42:41,320 --> 00:42:45,200
to accomplish that task, you, you had to have some like domain expertise in order to pull

681
00:42:45,200 --> 00:42:46,960
it off in a good way.

682
00:42:46,960 --> 00:42:51,800
And, uh, Nvidia started to pick up on this and said, hey, uh, we're going to make CUDA,

683
00:42:51,800 --> 00:42:57,080
a framework that allows like C developers to get a little handle into the GPU and be

684
00:42:57,080 --> 00:43:03,480
able to train things that way, or be able to be able to use the GPU for matrix, um, multiplication

685
00:43:03,480 --> 00:43:04,960
and things like that.

686
00:43:04,960 --> 00:43:05,960
And so that's like another layer.

687
00:43:05,960 --> 00:43:10,560
You have the bare hardware and then you have like CUDA and, uh, then you have, um, like,

688
00:43:10,560 --> 00:43:15,440
uh, Brian Catanzaro, uh, making CUDNN so that it even works better for deep neural networks,

689
00:43:15,440 --> 00:43:16,440
right?

690
00:43:16,440 --> 00:43:20,920
But this is still all very, um, low level, uh, type stuff.

691
00:43:20,920 --> 00:43:28,880
And then, uh, you know, outcome other frameworks like theano or, uh, cafe or, um, TensorFlow.

692
00:43:28,880 --> 00:43:31,120
And those are another abstract layer on top.

693
00:43:31,120 --> 00:43:33,000
They make it more human palatable, right?

694
00:43:33,000 --> 00:43:34,000
Right.

695
00:43:34,000 --> 00:43:35,000
Right.

696
00:43:35,000 --> 00:43:36,000
Right.

697
00:43:36,000 --> 00:43:38,600
They're, they're much more palatable for the, um, developer or computer scientist that,

698
00:43:38,600 --> 00:43:41,240
you know, like really knows their stuff, right?

699
00:43:41,240 --> 00:43:44,560
And, um, and that, and that's totally fine and they work very well.

700
00:43:44,560 --> 00:43:49,400
Um, what we, what we have found out though at working internally, um, at deepgram is

701
00:43:49,400 --> 00:43:54,840
that working in those frameworks still is very, it's very slow for us to, uh, to try to

702
00:43:54,840 --> 00:44:00,240
do experiments, essentially, to try new model architectures and then, and then see the

703
00:44:00,240 --> 00:44:06,520
result of because we are not, we're not necessarily training time limited where, uh, we would

704
00:44:06,520 --> 00:44:08,120
really love to be training time limited.

705
00:44:08,120 --> 00:44:12,040
We were engineering limited, you know, and so it's like the time that you're putting in

706
00:44:12,040 --> 00:44:15,920
to sort of cook all of this up and make sure, do all the error checking and whatnot to

707
00:44:15,920 --> 00:44:17,720
make sure it's like training the way it's supposed to.

708
00:44:17,720 --> 00:44:21,600
We're like, man, why don't we just make some other framework on top?

709
00:44:21,600 --> 00:44:24,880
Let's just stack another framework on top that is more abstract.

710
00:44:24,880 --> 00:44:29,440
That sort of doesn't care about the back end and it, it doesn't care, you know, if you're

711
00:44:29,440 --> 00:44:33,760
using theano or TensorFlow or whatever, you can just switch it with a flip with a flip

712
00:44:33,760 --> 00:44:34,760
of the switch.

713
00:44:34,760 --> 00:44:40,360
And, um, if, if something's faster, fine, whatever, but like all we really want to do is

714
00:44:40,360 --> 00:44:41,360
describe our model.

715
00:44:41,360 --> 00:44:44,880
We want to say, hey, the input is going to be this audio.

716
00:44:44,880 --> 00:44:49,520
Then we're going to have a convolution, a 2D convolution and another con, a 2D convolution

717
00:44:49,520 --> 00:44:50,760
with a batch norm.

718
00:44:50,760 --> 00:44:54,240
And then we're going to have some, uh, a few recurrent, uh, layers.

719
00:44:54,240 --> 00:44:55,800
Those are going to have batch norm too.

720
00:44:55,800 --> 00:44:59,720
And then we're going to stick those out into, you know, a dense that then predicts every

721
00:44:59,720 --> 00:45:03,600
time, time slice, which character is going to be there, which word is going to be there.

722
00:45:03,600 --> 00:45:05,240
And so that's how we want to think of it.

723
00:45:05,240 --> 00:45:09,240
We don't want to think about like Python code and like, how do we like put all of that

724
00:45:09,240 --> 00:45:10,240
in there?

725
00:45:10,240 --> 00:45:14,800
And so, so that's what, um, we start working on a deepgram is like, how do we do that?

726
00:45:14,800 --> 00:45:20,000
So that we can multiply our engineering effort, essentially, you know, um, I just go in and

727
00:45:20,000 --> 00:45:23,960
I change a few settings and, and then start running my network again and then go off

728
00:45:23,960 --> 00:45:25,080
and do something else.

729
00:45:25,080 --> 00:45:27,600
And that's, that's exactly what cur is.

730
00:45:27,600 --> 00:45:30,520
It's a descriptive, deep learning framework.

731
00:45:30,520 --> 00:45:35,120
And, um, you know, we have examples on how to do image classification, examples on how

732
00:45:35,120 --> 00:45:40,720
to do speech recognition, um, with a network that's very similar to BIDU's deep speech

733
00:45:40,720 --> 00:45:45,280
model and, uh, like language, uh, language modeling and things like that.

734
00:45:45,280 --> 00:45:50,080
And it's all in this descriptive format, um, it's still not like, don't get me wrong.

735
00:45:50,080 --> 00:45:55,600
Like deep learning is still not easy because, um, right, because there's a computational

736
00:45:55,600 --> 00:45:58,400
problem there and there's a data problem there.

737
00:45:58,400 --> 00:46:00,360
Like how do you get it into the format that you need?

738
00:46:00,360 --> 00:46:01,640
You know, how do you collect the data?

739
00:46:01,640 --> 00:46:02,640
How do you clean it?

740
00:46:02,640 --> 00:46:03,680
Uh, everything like that.

741
00:46:03,680 --> 00:46:07,520
But the model part, you know, once you have your data and once you're all set up, the

742
00:46:07,520 --> 00:46:11,160
model part is now like so much easier for us using curve.

743
00:46:11,160 --> 00:46:16,360
And so, so we thought, you know, um, this is kind of a competitive advantage, yeah, it

744
00:46:16,360 --> 00:46:23,000
is, um, but we have gained so much knowledge by, um, using open source software and talking

745
00:46:23,000 --> 00:46:26,680
with people very freely, sort of in the deep learning community that, you know, this

746
00:46:26,680 --> 00:46:28,280
tool has been so valuable to us.

747
00:46:28,280 --> 00:46:33,080
Let's just release it like we're, we're probably going to get more, you know, than if we

748
00:46:33,080 --> 00:46:37,480
just kept it to ourselves, you know, and we actually have like, we, we released

749
00:46:37,480 --> 00:46:42,960
the current framework and within like weeks, somebody had added multi GPU support, you

750
00:46:42,960 --> 00:46:46,520
know, and I was like, whoa, oh wow, these people are serious, right?

751
00:46:46,520 --> 00:46:53,600
And so, so yeah, and, and this is also another way to like, um, if you're, if you're an

752
00:46:53,600 --> 00:46:57,760
AI company out there in the world and you want to hire engineers, you know, this is another

753
00:46:57,760 --> 00:47:00,920
way to find good ones and they could be across the world.

754
00:47:00,920 --> 00:47:06,480
And so, so it's like we're sort of, we want to, we want to be giving to the community.

755
00:47:06,480 --> 00:47:12,680
We also just want to be, um, we want to be part of that conversation because I think we

756
00:47:12,680 --> 00:47:14,760
have a lot to add, essentially.

757
00:47:14,760 --> 00:47:21,240
And, um, I think that the deep learning community, there's so much demand to, um, there's

758
00:47:21,240 --> 00:47:26,320
so much hype, first of all, um, but there's also, there's a ton of demand for on talent

759
00:47:26,320 --> 00:47:30,720
and there's a ton of demand for the type of critical thinking you need in order to

760
00:47:30,720 --> 00:47:32,480
solve these deep learning problems.

761
00:47:32,480 --> 00:47:34,040
You don't have to be secretive.

762
00:47:34,040 --> 00:47:38,920
You don't have to be like, this is our secret soft that whatever, no, everybody is like

763
00:47:38,920 --> 00:47:43,440
talent limited, they're computationally limited, they're data limited, they're not like

764
00:47:43,440 --> 00:47:44,440
good idea limited.

765
00:47:44,440 --> 00:47:45,440
So, right.

766
00:47:45,440 --> 00:47:46,440
Right.

767
00:47:46,440 --> 00:47:47,440
So, yeah.

768
00:47:47,440 --> 00:47:49,880
The, you mentioned it's, it's declarative.

769
00:47:49,880 --> 00:47:51,640
That's one of the, the main things that's doing it.

770
00:47:51,640 --> 00:47:59,280
Are you, have you created like a DSL to define your, neural net or is it a different, uh,

771
00:47:59,280 --> 00:48:00,880
type of express differently?

772
00:48:00,880 --> 00:48:01,880
Yeah.

773
00:48:01,880 --> 00:48:06,280
So, okay, great, yeah, great question and the way that you interact with Kerr is you

774
00:48:06,280 --> 00:48:12,640
pip install it and, um, so, so it's, you know, written Python and, uh, you can use it

775
00:48:12,640 --> 00:48:17,800
as an API just as you would carous or something like that where, um, you know, you're programming

776
00:48:17,800 --> 00:48:21,720
in Python and that's just how you use Kerr and that's totally fine.

777
00:48:21,720 --> 00:48:27,440
But sort of the DNA of your model is contained in what we call a Kerr file and that is YAML

778
00:48:27,440 --> 00:48:28,440
or JSON.

779
00:48:28,440 --> 00:48:33,480
And so, so that, that contains like your hyper parameters, your model architecture, like

780
00:48:33,480 --> 00:48:36,400
how you want your data to be supplied and things like that.

781
00:48:36,400 --> 00:48:42,520
Um, and a lot of that is boilerplate that is already, um, out there in examples, essentially.

782
00:48:42,520 --> 00:48:46,840
And so you just like use a Kerr file that somebody else has put out there in the world

783
00:48:46,840 --> 00:48:51,680
and then just edit it a little bit in, for your purpose, essentially.

784
00:48:51,680 --> 00:48:55,880
And so, yeah, that's how it operates, um, just basically YAML files and if you want to

785
00:48:55,880 --> 00:49:01,440
do, um, like a deeper, uh, surgery, then you do it in Python using API.

786
00:49:01,440 --> 00:49:02,440
Hmm.

787
00:49:02,440 --> 00:49:07,520
And does it, uh, can you, uh, does it sit on top of any of the other frameworks or can

788
00:49:07,520 --> 00:49:13,280
you, how do you leverage the work that's, um, being done on, you know, TensorFlow and

789
00:49:13,280 --> 00:49:14,960
all the other frameworks out there?

790
00:49:14,960 --> 00:49:15,960
Yeah, absolutely.

791
00:49:15,960 --> 00:49:16,960
I should have said that earlier.

792
00:49:16,960 --> 00:49:22,480
It does, um, it supports theano, it supports TensorFlow and it supports PyTorch.

793
00:49:22,480 --> 00:49:28,440
And, um, maybe we'll support other backends, um, since, since, uh, like deep learning for

794
00:49:28,440 --> 00:49:34,360
a J is, uh, is working its way into Keras now, uh, you know, we'll probably be supporting

795
00:49:34,360 --> 00:49:35,360
them soon.

796
00:49:35,360 --> 00:49:41,120
So, so yeah, that's, uh, that's, that's kind of how it goes, um, if, if you have a high

797
00:49:41,120 --> 00:49:47,120
level API that would fit into something like Keras, um, then it'll fit into Kerr as well.

798
00:49:47,120 --> 00:49:48,120
Okay.

799
00:49:48,120 --> 00:49:49,120
Yeah.

800
00:49:49,120 --> 00:49:53,920
So we've already done the legwork for those three, you know, TensorFlow, theano and PyTorch.

801
00:49:53,920 --> 00:49:54,920
Awesome.

802
00:49:54,920 --> 00:49:58,760
And you mentioned, uh, you mentioned this already, but it's not just for audio.

803
00:49:58,760 --> 00:50:04,360
It's for images and basically anything that you're trying to use a deep neural net, uh,

804
00:50:04,360 --> 00:50:05,360
with.

805
00:50:05,360 --> 00:50:06,360
Absolutely.

806
00:50:06,360 --> 00:50:10,680
We, all that we do at DeepGram is audio, that's true, um, but the net, but the, uh, Kerr,

807
00:50:10,680 --> 00:50:14,000
you know, and the networks that you can train using Kerr are agnostic.

808
00:50:14,000 --> 00:50:15,000
It doesn't matter.

809
00:50:15,000 --> 00:50:16,200
You know, you can do sequences.

810
00:50:16,200 --> 00:50:21,240
You can do audio, you can do, you know, text, audio, images, you know, it, cook it up

811
00:50:21,240 --> 00:50:22,520
and you'll be able to do it.

812
00:50:22,520 --> 00:50:28,400
Um, we just had a hackathon, um, last weekend and, uh, people were doing all sorts of things,

813
00:50:28,400 --> 00:50:33,560
you know, uh, music, uh, editing, editing videos on the fly so that, you know, your cat will

814
00:50:33,560 --> 00:50:39,120
look like a van go as it's, um, you know, crawling, crawling around and things like that,

815
00:50:39,120 --> 00:50:42,880
you know, like the, and they're using, they're using Kerr to do these things.

816
00:50:42,880 --> 00:50:50,560
So, so yeah, it's, um, it's, it's sort of agnostic, hmm, uh, that's very cool, very

817
00:50:50,560 --> 00:50:51,560
cool.

818
00:50:51,560 --> 00:50:58,560
Um, and so you mentioned, uh, the Baidu deep speech stuff, was that the, uh, was that

819
00:50:58,560 --> 00:51:06,400
the inspiration for Kerr or to what extent do you, is your, uh, is the DeepGram work, um,

820
00:51:06,400 --> 00:51:09,720
you know, your product based on that research?

821
00:51:09,720 --> 00:51:16,200
Sure, so our, our, the models that we use, um, to, to build our indexes and to ingest

822
00:51:16,200 --> 00:51:20,560
audio are extremely similar to the deep speech networks, um, you have a convolutional stack

823
00:51:20,560 --> 00:51:25,900
and you have a recurrent stack and the target is, uh, characters or words, um, in the deep

824
00:51:25,900 --> 00:51:31,400
speech cases characters, but, but nevertheless, the, the architecture is extremely similar.

825
00:51:31,400 --> 00:51:36,360
Um, and so the networks that we supply like in Kerr, or sorry, the, uh, examples that

826
00:51:36,360 --> 00:51:42,160
we supply in Kerr, um, are extremely similar to what we use, um, but, but we, we have tried

827
00:51:42,160 --> 00:51:45,360
to make networks that people are already familiar with, like they can go read a paper

828
00:51:45,360 --> 00:51:46,760
and figure out how it works, right?

829
00:51:46,760 --> 00:51:47,760
Okay.

830
00:51:47,760 --> 00:51:51,880
So yeah, and well, you know, we put that into the example file as well, saying like, hey,

831
00:51:51,880 --> 00:51:54,600
if you want to read about the architecture, this is where it came from.

832
00:51:54,600 --> 00:51:59,320
So that's, yeah, we, we're not trying to, um, confuse anybody about like how it works.

833
00:51:59,320 --> 00:52:03,520
So, so we sort of stick to the things that you can go out and look at a paper for, um,

834
00:52:03,520 --> 00:52:04,520
sorry.

835
00:52:04,520 --> 00:52:09,880
It has not written, um, a paper on what we're doing yet, um, it's kind of in the works

836
00:52:09,880 --> 00:52:15,480
always, um, but, you know, we, we really would love to, but, you know, that when you have,

837
00:52:15,480 --> 00:52:21,760
when you have like businesses to, uh, you know, to take care of and, uh, you know, customers,

838
00:52:21,760 --> 00:52:27,120
I guess, to take care of and, um, and only, you know, eight people, then, then you, that

839
00:52:27,120 --> 00:52:28,960
kind of gets thrown by the wayside.

840
00:52:28,960 --> 00:52:29,960
Right.

841
00:52:29,960 --> 00:52:30,960
Right.

842
00:52:30,960 --> 00:52:37,960
Any standout use cases for, uh, for this approach and deep-gram, anything that you're

843
00:52:37,960 --> 00:52:43,080
seeing, uh, as, you know, kind of coming to the fore in terms of what people want to do

844
00:52:43,080 --> 00:52:44,080
with it?

845
00:52:44,080 --> 00:52:45,080
Absolutely.

846
00:52:45,080 --> 00:52:51,200
Um, from, from the business side, um, there is, uh, fraud, fraud detection is a really big

847
00:52:51,200 --> 00:52:58,520
one, uh, where people will call into financial services companies and try to, um, you know,

848
00:52:58,520 --> 00:53:01,720
try to get money from them, essentially, try to take money out of your account or use,

849
00:53:01,720 --> 00:53:05,200
you know, get a credit card sent to the wrong address or something like that so that they

850
00:53:05,200 --> 00:53:06,680
can take advantage of it.

851
00:53:06,680 --> 00:53:11,880
And there are sort of patterns, um, in this and you can, the, these companies have, you

852
00:53:11,880 --> 00:53:15,480
know, uh, millions, hundreds of millions of calls every year.

853
00:53:15,480 --> 00:53:19,160
And they're trying to find, they're trying to correlate these things and say, like, hey,

854
00:53:19,160 --> 00:53:21,440
we know that fraud happened on these calls, et cetera.

855
00:53:21,440 --> 00:53:26,040
Can you, like, help us find where that's happening, you know, like every day people are calling

856
00:53:26,040 --> 00:53:29,520
in, trying to defraud us, can you at least give us an alert so that we can look at those

857
00:53:29,520 --> 00:53:34,400
harder, you know, and, and that's just, that's, that's one of the channels, um, that,

858
00:53:34,400 --> 00:53:38,440
that, that, that, like, provides a lot of value, uh, essentially to the, to the world, you

859
00:53:38,440 --> 00:53:43,120
know, because if, if there's lower fraud, you know, then, uh, everything becomes less expensive

860
00:53:43,120 --> 00:53:48,720
for everyone, essentially, um, but there's also like a quality assurance aspect to this

861
00:53:48,720 --> 00:53:53,600
and, and compliance aspect, um, again, this is still in calls where, you know, are people

862
00:53:53,600 --> 00:53:57,400
just saying the things they're supposed to say, you know, are they having good responses,

863
00:53:57,400 --> 00:54:03,600
you know, do customers have, uh, nice interactions, um, and having, uh, the way the companies

864
00:54:03,600 --> 00:54:08,680
deal with this now is they pay humans, um, to look at maybe anywhere from one to five

865
00:54:08,680 --> 00:54:15,360
percent of the calls, and in, in general, this is outsourced where, uh, you send them,

866
00:54:15,360 --> 00:54:18,960
you know, a random selection of your calls, and then you say, like, tell us what happened

867
00:54:18,960 --> 00:54:24,480
in these calls and they'll report back with a rubric of maybe like 10 or 20 different

868
00:54:24,480 --> 00:54:30,400
things and the quality will be pretty low. And that's like the only source of truth

869
00:54:30,400 --> 00:54:34,680
for these companies about their customer interactions that happen through phone calls.

870
00:54:34,680 --> 00:54:38,920
And so that's the type of thing that, you know, Teapgram is trying to help with. We, we

871
00:54:38,920 --> 00:54:43,040
take like that QA data that you've sent out to have humans label will help you figure

872
00:54:43,040 --> 00:54:47,320
out which of those labels are actually good. And then we'll build a model based on those

873
00:54:47,320 --> 00:54:52,280
labels to predict all of your audio, essentially, uh, you know, to predict the, uh, contents

874
00:54:52,280 --> 00:54:57,240
of all of your audio. And then you can take your QA team or your compliance team or whatever

875
00:54:57,240 --> 00:55:00,640
they're doing. You still have hundreds of these people like listening to all these calls.

876
00:55:00,640 --> 00:55:03,720
You point them in a new direction, you know, so they aren't doing the same like wrote

877
00:55:03,720 --> 00:55:08,120
thing over and over. They're like actually using their brain to do things that humans are

878
00:55:08,120 --> 00:55:12,760
really good at, which are like creative things like figure out, you know, a new way to, to

879
00:55:12,760 --> 00:55:17,960
find fraud rather than just sort of listening and, uh, hoping that they detect it randomly.

880
00:55:17,960 --> 00:55:25,360
You know, um, so we, this is, this is like, um, where Teapgram has a huge impact, I think,

881
00:55:25,360 --> 00:55:30,480
or has the, uh, ability to have a huge impact is just automating that entire process. Um,

882
00:55:30,480 --> 00:55:36,400
so for, for the consumer side, we are, um, we're not putting as much effort into making

883
00:55:36,400 --> 00:55:42,640
our product like, like Google for the internet, but for audio. I wish we were like, actually

884
00:55:42,640 --> 00:55:48,160
this is a product and somebody should build it using DeepGram. But, um, but, and we have

885
00:55:48,160 --> 00:55:53,120
built demos of this, uh, where, where you just scrape a lot of YouTube videos and then

886
00:55:53,120 --> 00:55:59,360
you're able to search it, um, using DeepGram's tech. Um, but, but I think, um, from a like

887
00:55:59,360 --> 00:56:05,320
company health perspective, like, in other words, DeepGram not dying in two years, um, and

888
00:56:05,320 --> 00:56:11,440
not having to raise like 200 million in the process of that death. Um, you know, it

889
00:56:11,440 --> 00:56:17,120
could go a lot of ways, but, but essentially, um, I think that product is, is, is like available

890
00:56:17,120 --> 00:56:21,480
and it's something to test. I think that, um, that just the greater human world right

891
00:56:21,480 --> 00:56:24,800
now is not necessarily ready, ready for it at this moment. But, you know, in the next

892
00:56:24,800 --> 00:56:29,080
couple of years, uh, that's what we're going to expect. We're going to expect that all

893
00:56:29,080 --> 00:56:33,000
of the content in the world is searchable. That, you know, when I think of like a movie

894
00:56:33,000 --> 00:56:37,080
quote or when I think of something that I listened to in a podcast or when I think

895
00:56:37,080 --> 00:56:41,240
of some interview that I saw on YouTube and I think like, oh, yeah, they talked about

896
00:56:41,240 --> 00:56:43,640
this and they talked about that. I should be able to search for it and I should be able

897
00:56:43,640 --> 00:56:48,600
to find it and people will start demanding that soon. Um, but yeah, we haven't spent, we

898
00:56:48,600 --> 00:56:53,600
did some, um, some market research on this, you know, um, to, to figure out is, is this

899
00:56:53,600 --> 00:56:58,480
an area that we should spend our effort right now, but it just isn't yet. Um, maybe, maybe,

900
00:56:58,480 --> 00:57:02,320
maybe we'll start doing that, you know, in the next couple of years, but, uh, it, it,

901
00:57:02,320 --> 00:57:08,840
it's not our focus yet. Awesome. Awesome. Um, well, this has been a great conversation.

902
00:57:08,840 --> 00:57:13,840
We, we talked about Kerr and that's open source and we'll put a link to the GitHub and the

903
00:57:13,840 --> 00:57:20,920
show notes, anything else folks should know to, uh, look for or how to get in touch. Yeah,

904
00:57:20,920 --> 00:57:26,480
you can, you can get in touch with deepgram at, uh, on Twitter at deepgram AI and it

905
00:57:26,480 --> 00:57:31,320
where you can send a message to me, um, at Scott at deepgram.com. I am not shy about

906
00:57:31,320 --> 00:57:35,680
throwing my email out there. So if you want, if you want to contact me, just, just go

907
00:57:35,680 --> 00:57:40,040
ahead. Awesome. Awesome. Well, thanks so much. God, it's been great having you on the

908
00:57:40,040 --> 00:57:49,840
show. Sam, I appreciate it. Thanks. All right, everyone. That's our show for today. During

909
00:57:49,840 --> 00:57:54,960
this interview, you may have heard me mention my previous interview with Josh Blue, whose

910
00:57:54,960 --> 00:58:01,840
company wise.io was acquired by GE to help them permeate machine learning and AI throughout

911
00:58:01,840 --> 00:58:07,480
that company. If you haven't already listened to that show, which was number five, you should

912
00:58:07,480 --> 00:58:12,880
because it was a great one. But even better, you should plan to attend the future of data

913
00:58:12,880 --> 00:58:17,960
summit because Josh will be speaking there on building AI products and running them in

914
00:58:17,960 --> 00:58:25,160
production. Really, you don't want to miss the summit. So check it out at twimlai.com.

915
00:58:25,160 --> 00:58:28,940
And if you work on machine learning and AI for your company and you think you've got

916
00:58:28,940 --> 00:58:33,880
an interesting story to share, don't hesitate to reach out. I'm finalizing the agenda

917
00:58:33,880 --> 00:58:39,960
for the summit soon, but I'm always looking for interesting user stories. This podcast

918
00:58:39,960 --> 00:58:45,280
is full of great quotes. Don't forget to share your favorites for one of our Twomo

919
00:58:45,280 --> 00:58:50,840
stickers. You can share them via the show notes page via Twitter and via our Facebook

920
00:58:50,840 --> 00:58:58,640
page. The notes for this show will be up on twimlai.com slash talk slash 19 where you'll

921
00:58:58,640 --> 00:59:05,120
find links to Scott and the various resources mentioned in the show. Thanks so much for listening

922
00:59:05,120 --> 00:59:35,080
and catch you next time.

