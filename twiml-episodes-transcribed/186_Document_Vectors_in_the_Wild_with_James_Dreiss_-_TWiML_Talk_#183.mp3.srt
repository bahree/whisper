1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,640
I'm your host Sam Charrington.

4
00:00:31,640 --> 00:00:36,280
A couple of weeks ago I spent some time at the Stratidata conference in New York City, presented

5
00:00:36,280 --> 00:00:40,800
by O'Reilly and Claude Error, and over the course of the next few shows we'll be bringing

6
00:00:40,800 --> 00:00:46,880
you my interviews with some of the awesome speakers that I had the pleasure to meet with.

7
00:00:46,880 --> 00:00:52,560
In this episode of our Stratidata series, we're joined by James Drice, senior data scientist

8
00:00:52,560 --> 00:00:55,960
at International News Syndicate Reuters.

9
00:00:55,960 --> 00:01:00,200
James and I sat down to discuss this talk at the conference, document vectors in the

10
00:01:00,200 --> 00:01:06,080
wild, building a content recommendation system, in which he details how Reuters implemented

11
00:01:06,080 --> 00:01:12,240
document vectors to recommend content to users of their new infinite scroll page layout.

12
00:01:12,240 --> 00:01:16,400
In our conversation we take a look at what document vectors are and how they're used,

13
00:01:16,400 --> 00:01:20,920
how they tested the accuracy of their models, and the future of embeddings for natural

14
00:01:20,920 --> 00:01:23,440
language processing.

15
00:01:23,440 --> 00:01:27,760
Before we move on, I'd like to send a quick shout out to our friends at Claude Error and

16
00:01:27,760 --> 00:01:34,240
Capital One for their continued support of the podcast and their sponsorship of this series.

17
00:01:34,240 --> 00:01:39,880
Claude Error's modern platform for machine learning and analytics optimized for the cloud

18
00:01:39,880 --> 00:01:47,160
that you build and deploy AI solutions at scale efficiently and securely anywhere you want.

19
00:01:47,160 --> 00:01:52,840
In addition, Claude Error Fast Forward Labs' expert guidance helps you realize your AI

20
00:01:52,840 --> 00:01:54,800
future faster.

21
00:01:54,800 --> 00:02:02,680
To learn more, visit Claude Error's machine learning resource center at cloudera.com-ml.

22
00:02:02,680 --> 00:02:07,960
At the NIPPS conference in Montreal this December, Capital One will be hosting a workshop focused

23
00:02:07,960 --> 00:02:14,080
on challenges and opportunities for AI in financial services and the impact of fairness,

24
00:02:14,080 --> 00:02:17,520
explainability, accuracy, and privacy.

25
00:02:17,520 --> 00:02:21,720
A call for papers is open now through October 25th.

26
00:02:21,720 --> 00:02:30,440
For more information on submitting, visit twimmelai.com-c-number-one-NIPPS.

27
00:02:30,440 --> 00:02:33,440
And now onto the show.

28
00:02:33,440 --> 00:02:39,700
All right, everyone, I am in New York for the Stratocomference and I am here with James

29
00:02:39,700 --> 00:02:40,700
Drice.

30
00:02:40,700 --> 00:02:43,680
James is a senior data scientist at Reuters.

31
00:02:43,680 --> 00:02:45,960
James, welcome to this week in machine learning and AI.

32
00:02:45,960 --> 00:02:46,960
Hi, Sam.

33
00:02:46,960 --> 00:02:47,960
It's great to be here.

34
00:02:47,960 --> 00:02:48,960
Thank you very much.

35
00:02:48,960 --> 00:02:49,960
Awesome.

36
00:02:49,960 --> 00:02:50,960
Awesome.

37
00:02:50,960 --> 00:02:53,480
So, James, you've got a background in art history.

38
00:02:53,480 --> 00:02:57,440
You've studied economics at graduate school.

39
00:02:57,440 --> 00:02:59,920
You currently work at Reuters doing data science.

40
00:02:59,920 --> 00:03:01,680
Tell us a little bit about your background.

41
00:03:01,680 --> 00:03:02,680
Yeah, sure.

42
00:03:02,680 --> 00:03:07,160
So I think it is sort of a weird background of the art history and English and sort of

43
00:03:07,160 --> 00:03:08,160
the humanities.

44
00:03:08,160 --> 00:03:11,040
But that's what I studied in undergrad and I sort of kicked around in those worlds for

45
00:03:11,040 --> 00:03:12,840
a while in New York City.

46
00:03:12,840 --> 00:03:18,880
And eventually I started a nonprofit with a friend that was focused on international development.

47
00:03:18,880 --> 00:03:20,920
And that is what led me to grad school.

48
00:03:20,920 --> 00:03:24,840
So I studied econometrics and statistics in London.

49
00:03:24,840 --> 00:03:29,920
And that was sort of my first exposure to, you know, this sort of world of data science

50
00:03:29,920 --> 00:03:34,760
although at the time it wasn't really talked about as much.

51
00:03:34,760 --> 00:03:37,520
And it also first exposed me a little bit to programming.

52
00:03:37,520 --> 00:03:43,040
We use something called STATA, which is like a SAS sort of, you know, statistical software.

53
00:03:43,040 --> 00:03:45,000
But you do do some coding.

54
00:03:45,000 --> 00:03:48,520
So that was a really good sort of skill acquisition period.

55
00:03:48,520 --> 00:03:53,320
And after school, yeah, I learned art.

56
00:03:53,320 --> 00:03:57,280
And there was this great job at the Met Museum in New York that opened up and they were

57
00:03:57,280 --> 00:04:00,000
hiring for predictive analysts.

58
00:04:00,000 --> 00:04:04,640
And that was, so given my art history background and also having studied what I studied, this

59
00:04:04,640 --> 00:04:07,720
was sort of a perfect combination of those two things.

60
00:04:07,720 --> 00:04:08,720
And yeah, that was great.

61
00:04:08,720 --> 00:04:12,720
I mean, it's, you know, you work at the museum sort of ruined me for any future office.

62
00:04:12,720 --> 00:04:13,720
That was awesome.

63
00:04:13,720 --> 00:04:15,720
Yeah, it was really short.

64
00:04:15,720 --> 00:04:18,960
Like, you know, like, whatever, like, I don't really care, like, you know, if you have

65
00:04:18,960 --> 00:04:23,200
any like, Fuseball tables you have, or a bigger like one-coil tower is because, you

66
00:04:23,200 --> 00:04:26,560
know, I'm like, I can go like, look at like five premieres and my, you know, my lunch

67
00:04:26,560 --> 00:04:27,560
break.

68
00:04:27,560 --> 00:04:29,240
So that was amazing.

69
00:04:29,240 --> 00:04:37,040
And was your, was your focus there on kind of operational analysis or marketing analysis,

70
00:04:37,040 --> 00:04:38,640
or was it art related?

71
00:04:38,640 --> 00:04:45,400
It was, it was, yeah, more, more focused on administration and specifically fundraising.

72
00:04:45,400 --> 00:04:50,840
So trying to match donors with the museum, people who really love the museum and have

73
00:04:50,840 --> 00:04:56,720
capacity to give, and yeah, it was interesting because the data there is really, really rich.

74
00:04:56,720 --> 00:05:02,120
You know, I work in web analytics now, but, you know, there it's, you have postal addresses

75
00:05:02,120 --> 00:05:04,720
and, you know, these people really love the museum too.

76
00:05:04,720 --> 00:05:07,360
So it's not, they're happy to work with you.

77
00:05:07,360 --> 00:05:08,680
So that was great.

78
00:05:08,680 --> 00:05:12,800
But yeah, a few years ago, I moved to Raiders and that's where I am now.

79
00:05:12,800 --> 00:05:19,240
So we've done a lot of stuff we had a big redesign in the website last year and built

80
00:05:19,240 --> 00:05:23,800
it out, the design platform and there's been a lot of stuff in NLP, obviously.

81
00:05:23,800 --> 00:05:29,000
And yeah, I'm here giving a talk tomorrow and that's, that's what I'm focusing on.

82
00:05:29,000 --> 00:05:30,000
Fantastic.

83
00:05:30,000 --> 00:05:34,480
And for those who don't know Raiders, what does Raiders do or what is the part of Raiders

84
00:05:34,480 --> 00:05:36,000
that you work with?

85
00:05:36,000 --> 00:05:37,000
Yeah.

86
00:05:37,000 --> 00:05:39,400
So Raiders is a news organization.

87
00:05:39,400 --> 00:05:43,840
It's an international, there's a lot of, do a lot of syndicated news.

88
00:05:43,840 --> 00:05:49,120
So we have reporters all over the world stationed in, you know, many, many different countries.

89
00:05:49,120 --> 00:05:52,720
So wherever there's breaking news report, it will be their end filing stories.

90
00:05:52,720 --> 00:05:53,720
And they go to different outlets.

91
00:05:53,720 --> 00:05:56,560
But we also obviously publish ourselves.

92
00:05:56,560 --> 00:06:00,840
And it's, you know, we've been around for well over 150 years, I think.

93
00:06:00,840 --> 00:06:01,840
Awesome, awesome.

94
00:06:01,840 --> 00:06:06,960
And so the talk that you're giving tomorrow is called Document Vectors in the Wild, building

95
00:06:06,960 --> 00:06:09,160
a content recommendation system.

96
00:06:09,160 --> 00:06:12,040
Tell us a little bit about the motivation for this project.

97
00:06:12,040 --> 00:06:13,040
Sure.

98
00:06:13,040 --> 00:06:18,800
So last year, we decided to redesign the website and to move to something called

99
00:06:18,800 --> 00:06:21,720
an infinite scroll model.

100
00:06:21,720 --> 00:06:25,680
A lot of online publishers have moved to this model.

101
00:06:25,680 --> 00:06:29,680
Basically, it's one where you click on an article and then you read it.

102
00:06:29,680 --> 00:06:33,760
And then you have a string of articles that come after it.

103
00:06:33,760 --> 00:06:36,880
So it's a really, it's a big misdemeanor because it's, you know, it's, of course, it's

104
00:06:36,880 --> 00:06:37,880
not an infinite.

105
00:06:37,880 --> 00:06:42,840
In fact, we just, we wanted to just give five to ten articles for the user.

106
00:06:42,840 --> 00:06:48,360
And yeah, so my job is really to figure out what articles to give to them.

107
00:06:48,360 --> 00:06:54,240
And specifically, so we, we don't have registration on the website, meaning we're really, we're

108
00:06:54,240 --> 00:07:00,440
not doing too much personalization, about 75% of our users have two or fewer articles

109
00:07:00,440 --> 00:07:01,920
that they read.

110
00:07:01,920 --> 00:07:04,720
So it's really a content recommendation system.

111
00:07:04,720 --> 00:07:10,400
So we did some tests previously and found out that if a user is on a page, you know,

112
00:07:10,400 --> 00:07:14,440
if it's a business page or tech page, they're going to probably want more of that content,

113
00:07:14,440 --> 00:07:15,600
which it makes sense.

114
00:07:15,600 --> 00:07:23,640
So we started out thinking, okay, we want to find some more content into the slide programmatically.

115
00:07:23,640 --> 00:07:29,680
And the other challenges were, well, you know, news is breaking and happens quickly.

116
00:07:29,680 --> 00:07:33,360
And editors can always provide tags metadata to the articles.

117
00:07:33,360 --> 00:07:38,600
So essentially, we need to do categorization just purely based on the text of the article.

118
00:07:38,600 --> 00:07:40,560
And maybe some tags as well.

119
00:07:40,560 --> 00:07:45,400
So the challenge is really how do you get at specific sub topics within these larger

120
00:07:45,400 --> 00:07:51,360
topics that we have like tech and business, and how do we find articles about discrimination

121
00:07:51,360 --> 00:07:55,240
and tech, which is not something that would necessarily be tagged by an editor.

122
00:07:55,240 --> 00:07:56,240
Okay.

123
00:07:56,240 --> 00:07:57,240
Yeah.

124
00:07:57,240 --> 00:08:01,280
And so presumably based on the talk title, you did this using document vectors.

125
00:08:01,280 --> 00:08:02,280
Yes.

126
00:08:02,280 --> 00:08:04,440
We've talked quite a bit about word vectors.

127
00:08:04,440 --> 00:08:06,680
What's a document vector?

128
00:08:06,680 --> 00:08:08,600
It's very similar to a word vector.

129
00:08:08,600 --> 00:08:10,480
It's sort of like a sidecar or two word vector.

130
00:08:10,480 --> 00:08:14,000
In fact, the document vector is treated as another word.

131
00:08:14,000 --> 00:08:21,080
So yeah, we, you know, this is like the problem in NLP is that if you just tokenize tags

132
00:08:21,080 --> 00:08:25,400
and you're just looking at the trading the words as these atomic objects, you're losing

133
00:08:25,400 --> 00:08:31,120
so much information, you know, if a build a vector that has the word president and another

134
00:08:31,120 --> 00:08:35,960
one that has the word Trump, you know, those two vectors can't, they don't know that Trump

135
00:08:35,960 --> 00:08:40,280
is president, which is, you know, good for them, but that's, we need to know that sort

136
00:08:40,280 --> 00:08:41,280
of thing.

137
00:08:41,280 --> 00:08:47,600
So, yeah, I mean, word embeddings, which encode semantic meeting, get at this, but the

138
00:08:47,600 --> 00:08:52,560
challenges for us were that we have, the length of our articles are highly variable.

139
00:08:52,560 --> 00:08:55,600
We have really short articles, really long articles.

140
00:08:55,600 --> 00:09:00,040
So doing something like just taking the first hundred words of an article and taking

141
00:09:00,040 --> 00:09:03,720
the embeddings for those words, wasn't really going to cut it.

142
00:09:03,720 --> 00:09:10,040
And you know, having done some research into document vectors, I also knew that performance

143
00:09:10,040 --> 00:09:15,160
in these sorts of tasks was higher than just using, you know, word embeddings that were

144
00:09:15,160 --> 00:09:19,160
cut off at a certain length or average word embeddings or that sort of thing.

145
00:09:19,160 --> 00:09:21,040
So this is something we wanted to explore.

146
00:09:21,040 --> 00:09:25,720
Is that pretty typical for using word embeddings to cut them off at a certain lengthwise

147
00:09:25,720 --> 00:09:27,320
their requirement to do that?

148
00:09:27,320 --> 00:09:31,840
I think, well, if you wanted to compare two articles that were different lengths, you

149
00:09:31,840 --> 00:09:35,240
had to, you know, they had to have similar sizes, the same sizes.

150
00:09:35,240 --> 00:09:43,960
So I'm not sure how wide that is in the industry, but document vectors allow you to obtain

151
00:09:43,960 --> 00:09:47,880
vectors that are the same size, where the articles would be different lengths.

152
00:09:47,880 --> 00:09:52,040
And so how do you go about creating these document vectors?

153
00:09:52,040 --> 00:09:58,120
So word vectors, the way, you know, I'm sure your audience knows how they are graded, but

154
00:09:58,120 --> 00:09:59,520
it's, you know, they're very similar.

155
00:09:59,520 --> 00:10:01,520
Document vectors are very, happen in very similar ways.

156
00:10:01,520 --> 00:10:07,720
Essentially, you know, word vectors are getting at a semantic meeting of words.

157
00:10:07,720 --> 00:10:13,400
I usually call them, I treat them as, you know, their address is basically in this multi-dimensional

158
00:10:13,400 --> 00:10:14,400
space.

159
00:10:14,400 --> 00:10:19,240
So you know that the word dog and the word cat are close together because the model on

160
00:10:19,240 --> 00:10:24,400
which they were trained determined that their co-occurrence happens frequently.

161
00:10:24,400 --> 00:10:29,800
So in the model specifically, it's calculating the probability of co-occurrence and that's

162
00:10:29,800 --> 00:10:32,040
the output layer.

163
00:10:32,040 --> 00:10:36,760
And the middle layer is the embedding layer and that layer is being maximized.

164
00:10:36,760 --> 00:10:42,160
So it's sort of set up as this fake prediction task where you're predicting words from

165
00:10:42,160 --> 00:10:44,640
context or the opposite.

166
00:10:44,640 --> 00:10:49,520
And as a result, you have these weights that are richer with all that semantic information.

167
00:10:49,520 --> 00:10:53,280
Now word, excuse me, document vectors are very similar.

168
00:10:53,280 --> 00:10:57,560
You're doing, the training is virtually the same, but you're just adding an additional

169
00:10:57,560 --> 00:11:03,400
vector at the input layer that is representative of the document.

170
00:11:03,400 --> 00:11:08,800
So you train in a similar way, you have a context window, you sort of scan across the

171
00:11:08,800 --> 00:11:14,160
article, the document, you know, doing the predictions.

172
00:11:14,160 --> 00:11:17,480
And at the end, after you've gone through the entire corpus, you have this additional

173
00:11:17,480 --> 00:11:21,160
vector that is sort of picked up all of the semantic information that was not picked

174
00:11:21,160 --> 00:11:23,200
up by the word vectors.

175
00:11:23,200 --> 00:11:28,640
And you can make these comparisons between articles, between documents, because all the

176
00:11:28,640 --> 00:11:31,000
documents have shared context and words.

177
00:11:31,000 --> 00:11:36,160
So if they have shared context, then the article should be similar.

178
00:11:36,160 --> 00:11:41,560
So you're doing, you're able to do kind of similar vector operations on these documents.

179
00:11:41,560 --> 00:11:47,640
You know, there's the classic word embedding, our word vector examples like, you know,

180
00:11:47,640 --> 00:11:51,760
man, king, woman, queen, kind of thing, you're able to do those similar kinds of things

181
00:11:51,760 --> 00:11:52,760
with documents.

182
00:11:52,760 --> 00:11:53,760
Yeah.

183
00:11:53,760 --> 00:12:00,840
You're able to compare to articles where they don't share any, to any pieces of text that

184
00:12:00,840 --> 00:12:04,200
may not share any words at all in maybe different lengths, and you can get it.

185
00:12:04,200 --> 00:12:09,920
You can know that they have similar meaning, and that's how we got at these subtopics.

186
00:12:09,920 --> 00:12:13,400
There are some sort of peculiarities with them, especially with inference.

187
00:12:13,400 --> 00:12:18,880
So when you want to make a prediction for a new document that you haven't seen before,

188
00:12:18,880 --> 00:12:22,120
you're essentially running a new training step, an additional training step.

189
00:12:22,120 --> 00:12:26,960
Or maybe many 10 to 20 training steps depending on how many of us you had when you're training

190
00:12:26,960 --> 00:12:29,240
the original model.

191
00:12:29,240 --> 00:12:33,760
You're also holding the word vectors and the output weights constant.

192
00:12:33,760 --> 00:12:39,040
So you hold this constant, you do an additional training step, and this new document vector

193
00:12:39,040 --> 00:12:46,200
that was initialized randomly comes to, you know, it essentially picks up all the information

194
00:12:46,200 --> 00:12:50,520
that is present in the context and the output layer.

195
00:12:50,520 --> 00:12:52,040
Can you elaborate on that?

196
00:12:52,040 --> 00:12:58,000
This is when you're during inference time, you're also, there's a training step?

197
00:12:58,000 --> 00:13:02,080
Yeah, there is.

198
00:13:02,080 --> 00:13:09,120
Essentially, so that new vector is initialized randomly, the other weights are held constant.

199
00:13:09,120 --> 00:13:11,520
And it's sort of strange.

200
00:13:11,520 --> 00:13:15,280
Essentially, you're going to have a new document vector that you're obtaining is going

201
00:13:15,280 --> 00:13:21,200
to be a little different each time if you do new references, which is a little strange,

202
00:13:21,200 --> 00:13:27,840
but there are ways of getting to sort of do like sanity checks to know that, okay, this

203
00:13:27,840 --> 00:13:33,640
new document vector that I just inferred should be similar to itself essentially.

204
00:13:33,640 --> 00:13:39,480
So once sort of sanity check is, you have your model, you've trained it, make inferences

205
00:13:39,480 --> 00:13:46,240
on all the training documents that you trained on, and then ensure that those new inferred

206
00:13:46,240 --> 00:13:50,120
document vectors are, their nearest neighbors are themselves.

207
00:13:50,120 --> 00:13:54,360
So if that's the case, then your model should be trained correctly.

208
00:13:54,360 --> 00:14:01,720
And so how did you get from these document vectors to the categories, to the larger topic

209
00:14:01,720 --> 00:14:02,720
categories?

210
00:14:02,720 --> 00:14:05,440
Or to the subcategories?

211
00:14:05,440 --> 00:14:11,040
So they're not really, we don't have labels for the subcategories, it just sort of

212
00:14:11,040 --> 00:14:12,040
happens.

213
00:14:12,040 --> 00:14:14,560
It's a result of cutting the model, yeah.

214
00:14:14,560 --> 00:14:20,520
Clustering in this document space and then manually associating a label with individual

215
00:14:20,520 --> 00:14:21,520
clusters.

216
00:14:21,520 --> 00:14:27,120
Yeah, so we put the unsupervised learning model in production, which is, I think, some

217
00:14:27,120 --> 00:14:33,360
unique and mostly often embeddings are used for downstream tasks as part of the future

218
00:14:33,360 --> 00:14:37,760
of the model, but we're actually using the cluster strictly.

219
00:14:37,760 --> 00:14:43,400
We got at, we tried to ensure that it was accurate and I can get into that, but one

220
00:14:43,400 --> 00:14:51,360
thing that we did, so we're only make, when we rank the article scrolls, we're ensuring

221
00:14:51,360 --> 00:14:56,600
we're just, we're just pulling all tech articles, for instance, and then within tech we rank,

222
00:14:56,600 --> 00:15:02,240
okay, based on the very, the article that these are requested, what are the nearest articles

223
00:15:02,240 --> 00:15:05,600
that are, you know, within the last 24 hours that are also tech, and, you know, there's

224
00:15:05,600 --> 00:15:08,280
also like a popularity filter there as well, so.

225
00:15:08,280 --> 00:15:14,000
So the example you gave earlier where you were interested in a subtopic discrimination

226
00:15:14,000 --> 00:15:19,440
in tech, for example, that subtopic is never explicitly identified.

227
00:15:19,440 --> 00:15:24,080
It's just based on, it's determined based on this, the document space.

228
00:15:24,080 --> 00:15:25,080
Right, exactly.

229
00:15:25,080 --> 00:15:30,560
And we can go back and look at the labels that were applied by editorial after the fact

230
00:15:30,560 --> 00:15:35,520
and get that, that sort of thing, but, you know, similarly to something like LDI, like

231
00:15:35,520 --> 00:15:39,560
it's not, you know, that's not, there's not an automatic assignment of label, like this

232
00:15:39,560 --> 00:15:40,560
is discrimination tech.

233
00:15:40,560 --> 00:15:44,760
You have to sort of, if you're looking at trying to identify subtopics in LDI, you're

234
00:15:44,760 --> 00:15:49,000
really just looking at the words, you know, that were assigned for that topic.

235
00:15:49,000 --> 00:15:51,520
So, it's a similar sort of thing.

236
00:15:51,520 --> 00:15:56,200
You were mentioning some of the testing that you were doing around this.

237
00:15:56,200 --> 00:16:02,720
Yeah, it was interesting, so, you know, measuring performance of unsupervised learning models

238
00:16:02,720 --> 00:16:08,080
is always a little tricky, there's always some eyeballing that goes, goes on, you know,

239
00:16:08,080 --> 00:16:12,560
like something like doing the elbow method for k-means, like that's always been sort of

240
00:16:12,560 --> 00:16:13,560
hard for me.

241
00:16:13,560 --> 00:16:16,160
It's never actually like, you never get like a true elbow, like to determine the right

242
00:16:16,160 --> 00:16:20,360
number of clusters, you know, there should be even like this break and in the graph of

243
00:16:20,360 --> 00:16:24,960
clusters versus how dispersed they are.

244
00:16:24,960 --> 00:16:30,320
But, you know, looking through the literature and also remembering other methods, something

245
00:16:30,320 --> 00:16:37,560
like a silhouette index, which is unsupervised learning metric, where you're essentially comparing

246
00:16:37,560 --> 00:16:42,840
the average distance of a point, a data point, and between it and its own cluster and its

247
00:16:42,840 --> 00:16:46,840
neighboring clusters, and you're trying to get at how close it is between its own cluster

248
00:16:46,840 --> 00:16:51,000
and its neighboring ones, and you're assigning accuracy based on that.

249
00:16:51,000 --> 00:16:57,360
In one of the Doctavec papers, the authors used something called triplet accuracy, where

250
00:16:57,360 --> 00:17:04,240
they took Wikipedia articles and found articles that were in the same topic, and then they

251
00:17:04,240 --> 00:17:08,240
took a randomly drawn non-topic article.

252
00:17:08,240 --> 00:17:13,560
And if the distance between the two similar topic articles was closer than the non-topic

253
00:17:13,560 --> 00:17:17,520
article, then that counted as an accuracy point for the model.

254
00:17:17,520 --> 00:17:18,520
Okay.

255
00:17:18,520 --> 00:17:19,520
So we did something similar.

256
00:17:19,520 --> 00:17:26,080
We knew that we wanted to do subtopics, so, you know, we, we, for the document doctor

257
00:17:26,080 --> 00:17:30,560
model, would take two articles in tech, and one that was not tech.

258
00:17:30,560 --> 00:17:36,200
And if it determined that those two same articles were closer than the one in business or,

259
00:17:36,200 --> 00:17:39,920
you know, sports or something, then that counted as a, as a point to the accuracy.

260
00:17:39,920 --> 00:17:45,960
And we ended up with, I think it was about 77%, accuracy, which is very close to what the

261
00:17:45,960 --> 00:17:49,560
authors who devised the triplet accuracy down as well.

262
00:17:49,560 --> 00:17:56,120
So you're basing this model on accuracy at the category level, but ultimately you're

263
00:17:56,120 --> 00:17:59,960
trying to make inferences at a sub-category level.

264
00:17:59,960 --> 00:18:05,120
Did that play into, did that play into it at all?

265
00:18:05,120 --> 00:18:06,560
In terms of looking at accuracy?

266
00:18:06,560 --> 00:18:11,120
In terms of looking at accuracy or the model, you know, construction or performance?

267
00:18:11,120 --> 00:18:16,680
Yeah, we definitely did some, some sort of, we couldn't have done too many, but yeah,

268
00:18:16,680 --> 00:18:21,720
we looked at, you know, just the performance generally, relying on the editors as well,

269
00:18:21,720 --> 00:18:26,520
to sort of say, like, well, this is a good, you know, this is a good match, this isn't.

270
00:18:26,520 --> 00:18:31,320
But ultimately, the real test of any models when you put in productions who did performance

271
00:18:31,320 --> 00:18:36,440
and if it's performing well, you know, if the topic assigned it is not good, then, you

272
00:18:36,440 --> 00:18:39,960
know, it should not really, it should perform less well than your controller, your other

273
00:18:39,960 --> 00:18:40,960
test branch.

274
00:18:40,960 --> 00:18:43,440
So that was the ultimate test for us.

275
00:18:43,440 --> 00:18:46,160
And so what was the metric for that test?

276
00:18:46,160 --> 00:18:48,520
What are you ultimately trying to drive?

277
00:18:48,520 --> 00:18:52,880
We, so ultimately we want to drive engagement on our site.

278
00:18:52,880 --> 00:18:57,920
There's a lot of ways of, a lot of ways we can optimize, you know, the way users interact

279
00:18:57,920 --> 00:19:04,280
with their website, but engagement is the sort of catch all thing that if you try to get

280
00:19:04,280 --> 00:19:08,160
at that, then you're going to lift all the other ships of, you know, whatever, looking

281
00:19:08,160 --> 00:19:10,800
at ads and engaging with the content itself.

282
00:19:10,800 --> 00:19:16,360
So, so that was ultimately what we were trying to get at in that, their proxies for that.

283
00:19:16,360 --> 00:19:22,200
So specifically how deep an art, a user would get into the article scroll and also how deep

284
00:19:22,200 --> 00:19:24,400
they're getting into the articles themselves.

285
00:19:24,400 --> 00:19:30,000
We did three different test branches, one where we're having similar scrolls and one

286
00:19:30,000 --> 00:19:34,320
where we have, where we have, we have what I was calling dissimilar scrolls.

287
00:19:34,320 --> 00:19:40,440
So I had this sort of theory that users would get sort of tired of reading the same content

288
00:19:40,440 --> 00:19:44,840
so if you were reading an article about like Trump's cabinet was, maybe you want to move

289
00:19:44,840 --> 00:19:47,760
on to something else after you read that article.

290
00:19:47,760 --> 00:19:49,640
So that was the intuition behind that.

291
00:19:49,640 --> 00:19:55,440
So we would, and essentially that's same topic articles just sorted in reverse cosine order,

292
00:19:55,440 --> 00:20:00,600
which essentially what you're obtaining is there's a lot of randomness there.

293
00:20:00,600 --> 00:20:06,160
It sort of doesn't seem, you know, you can't immediately tell that two articles are very

294
00:20:06,160 --> 00:20:07,160
dissimilar.

295
00:20:07,160 --> 00:20:08,160
It's sort of a strange thing.

296
00:20:08,160 --> 00:20:11,880
They're just going to seem like that they were randomly drawn.

297
00:20:11,880 --> 00:20:17,880
So that was one test branch and then as a control, we had top of these articles.

298
00:20:17,880 --> 00:20:21,720
And yeah, the end result was sort of, you know, what we expected, which is good, which

299
00:20:21,720 --> 00:20:27,040
is that similar articles really upformed the scrolls outperform both of these, these

300
00:20:27,040 --> 00:20:29,240
other test branches.

301
00:20:29,240 --> 00:20:36,200
And within topic categories as well, things like sports and culture, those performed

302
00:20:36,200 --> 00:20:37,520
especially well.

303
00:20:37,520 --> 00:20:40,560
Those are sort of on writers, they're a little harder to find on our sites.

304
00:20:40,560 --> 00:20:47,000
So if users are going to those topics, it's likely that, you know, they're going to

305
00:20:47,000 --> 00:20:48,640
be highly engaged.

306
00:20:48,640 --> 00:20:51,760
So that was the intuition behind that as well.

307
00:20:51,760 --> 00:20:59,280
And to what, to what degree did the similar school model outperform the other models?

308
00:20:59,280 --> 00:21:04,640
It was a few fractions of percentage, but it's, you know, when you're looking at statistically

309
00:21:04,640 --> 00:21:08,120
significant results, that's what you would expect.

310
00:21:08,120 --> 00:21:15,160
We were lucky in that, so essentially every article page was a randomized test.

311
00:21:15,160 --> 00:21:18,760
So user comes to the page and they're served one of the three test branches.

312
00:21:18,760 --> 00:21:21,600
And that randomization happens at the article level.

313
00:21:21,600 --> 00:21:24,680
So we ran hundreds of tests.

314
00:21:24,680 --> 00:21:30,080
And that was the real, you know, no matter the, the lift, I mean, that was very important

315
00:21:30,080 --> 00:21:31,360
as well, of course.

316
00:21:31,360 --> 00:21:36,920
But in terms of like winning pages, the similar scrolls blew the other two out of the water.

317
00:21:36,920 --> 00:21:41,600
So that was, that was a very good, similarly about that winning pages.

318
00:21:41,600 --> 00:21:48,080
That the one test branch was significant over the other two in terms of these metrics

319
00:21:48,080 --> 00:21:49,080
that I mentioned.

320
00:21:49,080 --> 00:21:51,120
So that was counted as a winner.

321
00:21:51,120 --> 00:21:54,640
And then if you look at all the pages in aggregate, that was the result.

322
00:21:54,640 --> 00:21:59,760
A couple of random things I remember from flipping through your presentation, there was a reference

323
00:21:59,760 --> 00:22:01,400
to Kung Fifi.

324
00:22:01,400 --> 00:22:02,400
I know.

325
00:22:02,400 --> 00:22:10,080
I was trying to, I was like, I really want to use this, this tweet as an example, you know,

326
00:22:10,080 --> 00:22:12,400
because a lot of people, the wedding events have been around for a while now.

327
00:22:12,400 --> 00:22:15,120
I wanted to sort of, I don't know, live in another bit.

328
00:22:15,120 --> 00:22:19,480
But yeah, I, Kung Fifi, I don't know what would add or subtract, what vectors would add

329
00:22:19,480 --> 00:22:20,840
or subtract to, to equal that.

330
00:22:20,840 --> 00:22:23,280
I think it's this, it's completely nonsense.

331
00:22:23,280 --> 00:22:28,360
So I know, I want to dig into that a little bit more.

332
00:22:28,360 --> 00:22:32,040
I'm glad that you pronounce it the way I'm going to pronounce it, because I'm not entirely

333
00:22:32,040 --> 00:22:33,040
sure.

334
00:22:33,040 --> 00:22:39,080
So I think I pronounce it differently every time I say it, which is not very often.

335
00:22:39,080 --> 00:22:42,680
Well, that's NLP is hard for the story, I mean, this is like, we're getting at this,

336
00:22:42,680 --> 00:22:48,680
like what, you know, pronunciation, meaning, like this is, yeah, I mean, someone described

337
00:22:48,680 --> 00:22:53,200
English to me the other day, and this is any language, but there's a connotative sort

338
00:22:53,200 --> 00:22:57,640
of system, and it really gets at it for me, like in a pithy way, like, it's all built

339
00:22:57,640 --> 00:23:02,920
around these connotations, everything has higher, higher meaning, you know, and they're

340
00:23:02,920 --> 00:23:08,560
also the, the, the meaning may depend on cultural associations, yeah, all sorts of things.

341
00:23:08,560 --> 00:23:14,400
What, what kind of interesting or surprising really things that you have, did you learn

342
00:23:14,400 --> 00:23:16,560
along the way with this project?

343
00:23:16,560 --> 00:23:20,960
Probably the most surprising was that, you know, people were just sort of consumed, they,

344
00:23:20,960 --> 00:23:24,280
they come to a website that, you know, in terms of news, at least, they really go after

345
00:23:24,280 --> 00:23:26,200
one subject.

346
00:23:26,200 --> 00:23:29,160
And I think that that makes sense, and they've seen sort of the development of, like,

347
00:23:29,160 --> 00:23:33,600
the media ecosystem generally, and, you know, it's, they have, like, these one track minds

348
00:23:33,600 --> 00:23:38,440
and they keep delivering on, you know, certain subjects over a certain time period.

349
00:23:38,440 --> 00:23:44,080
So that was, I don't know if it's surprising, but maybe, like, I don't know, but disheartening.

350
00:23:44,080 --> 00:23:50,440
You know, I was hoping that, you know, we want to get a sort of variety, and, you know,

351
00:23:50,440 --> 00:23:54,160
especially when we have, like, these long investigative pieces that, during those

352
00:23:54,160 --> 00:23:58,640
events, have worked on for a while, you know, we want to sort of get at, when is the

353
00:23:58,640 --> 00:24:01,240
best time to serve them?

354
00:24:01,240 --> 00:24:06,040
But I think that is, to do that, you really, that's that personalization.

355
00:24:06,040 --> 00:24:09,560
And we do want to get more into personalization, but it is difficult given that, you know, we're

356
00:24:09,560 --> 00:24:12,520
entirely cookie based, and we don't have registration.

357
00:24:12,520 --> 00:24:18,400
Yeah, really, I think speaks to a lot of what we see with the whole, you know, filter

358
00:24:18,400 --> 00:24:23,840
bubble kind of situation that if you, if you see such strong results by serving up more

359
00:24:23,840 --> 00:24:24,840
of the same.

360
00:24:24,840 --> 00:24:28,800
You know, you want to see the same kind of results and that's just, we're all getting

361
00:24:28,800 --> 00:24:30,800
every time we're the same, right?

362
00:24:30,800 --> 00:24:31,800
Exactly.

363
00:24:31,800 --> 00:24:32,800
Yeah, yeah.

364
00:24:32,800 --> 00:24:33,800
But what can you do?

365
00:24:33,800 --> 00:24:34,800
I don't know.

366
00:24:34,800 --> 00:24:37,800
Talk to the editors about that one.

367
00:24:37,800 --> 00:24:38,800
Yeah, so.

368
00:24:38,800 --> 00:24:42,400
You mentioned inference, stochasticity at some point in your talk.

369
00:24:42,400 --> 00:24:43,400
What was that reference?

370
00:24:43,400 --> 00:24:49,200
Yeah, that was, so I was trying to get this, it's really about, when you make an inference

371
00:24:49,200 --> 00:24:50,720
on a new document vector.

372
00:24:50,720 --> 00:24:56,320
So it's initialized randomly, and if you do the inference multiple times, it's going

373
00:24:56,320 --> 00:24:58,760
to be a bit random each time.

374
00:24:58,760 --> 00:25:01,400
And that was a bit strange to me.

375
00:25:01,400 --> 00:25:05,520
You can look at the variance of the vector, the document vector, but, you know, it's always

376
00:25:05,520 --> 00:25:08,240
going to be a little different.

377
00:25:08,240 --> 00:25:14,560
We tried to get at this by just having a higher number of training iterations.

378
00:25:14,560 --> 00:25:19,440
And that was, I think, especially important because we were working with 100-length vectors

379
00:25:19,440 --> 00:25:20,440
for a number of reasons.

380
00:25:20,440 --> 00:25:21,440
I mean, we looked at it.

381
00:25:21,440 --> 00:25:26,760
I looked at longer vectors, but surprisingly, they didn't really perform too much better.

382
00:25:26,760 --> 00:25:32,000
So given that would have just taken up more memory, we went with the shorter one.

383
00:25:32,000 --> 00:25:33,000
And it was unusual.

384
00:25:33,000 --> 00:25:37,480
I mean, I was actually trying to, I was looking into why shorter vectors, you know, there's

385
00:25:37,480 --> 00:25:45,480
a limit to how long they can be and why shorter vectors tend to perform better.

386
00:25:45,480 --> 00:25:48,560
And it's, no one really has really gotten at that.

387
00:25:48,560 --> 00:25:52,680
And maybe something about overtraining, I don't know.

388
00:25:52,680 --> 00:25:57,640
But just surprising, that's 100 dimensions is commonly used for word vectors.

389
00:25:57,640 --> 00:26:03,960
And I would think that for a document, you'd want kind of a richer space.

390
00:26:03,960 --> 00:26:07,120
And so you'd want a lot more, a lot higher dimension.

391
00:26:07,120 --> 00:26:11,040
Yeah, I agree, but that's not what we saw.

392
00:26:11,040 --> 00:26:13,120
So which was fine.

393
00:26:13,120 --> 00:26:19,920
And in terms of the SOCasticity, there is another source of randomness in the context window.

394
00:26:19,920 --> 00:26:24,360
So we use Gensum for the document vectors and the way the author is simple.

395
00:26:24,360 --> 00:26:25,360
What is that?

396
00:26:25,360 --> 00:26:33,160
It's a topic modeling library in Python, which is really, really terrific, it's very fast.

397
00:26:33,160 --> 00:26:38,080
And yeah, so it handles LDA and word vectors and document vectors.

398
00:26:38,080 --> 00:26:42,400
But the way that they implemented it was, so when you're training, there's a context

399
00:26:42,400 --> 00:26:44,360
window that you can specify.

400
00:26:44,360 --> 00:26:49,040
But it doesn't always draw, say, if you say, okay, well, five words should be the window.

401
00:26:49,040 --> 00:26:50,240
It doesn't always draw five words.

402
00:26:50,240 --> 00:26:57,320
It draws randomly based on how, so the near words, the word, the center of the window

403
00:26:57,320 --> 00:27:00,720
will be, there's a higher likelihood that they'll be drawn.

404
00:27:00,720 --> 00:27:05,880
And this is how the original word detect authors implemented there, the model.

405
00:27:05,880 --> 00:27:07,120
So that's what they did.

406
00:27:07,120 --> 00:27:11,960
So that is also, even if you weren't, you know, even if you could see the document vector

407
00:27:11,960 --> 00:27:14,640
at inference, you would still have the source of randomness.

408
00:27:14,640 --> 00:27:20,760
So, yeah, I mean, you can, another tactic would be to average the vectors, but, you know,

409
00:27:20,760 --> 00:27:28,200
we did the sanity check where you're comparing the inferred vectors to the vectors of training.

410
00:27:28,200 --> 00:27:33,120
And in every case, they were, you know, that you came up as the result where, you know,

411
00:27:33,120 --> 00:27:34,640
the nearest neighbor was the, themselves.

412
00:27:34,640 --> 00:27:39,080
So that was, that was the check and we figured we didn't have to do that.

413
00:27:39,080 --> 00:27:40,080
So.

414
00:27:40,080 --> 00:27:46,120
I still have some fuzziness around the inference thing in this, in this case, what is the,

415
00:27:46,120 --> 00:27:50,440
what's the input and what's the output of this inference step?

416
00:27:50,440 --> 00:27:56,840
So the input is, is all the vectors that were trained previously.

417
00:27:56,840 --> 00:28:01,120
So it's really, it's not too complicated because this is, these are really pretty simple

418
00:28:01,120 --> 00:28:02,120
models.

419
00:28:02,120 --> 00:28:03,120
Mm-hmm.

420
00:28:03,120 --> 00:28:08,040
Yeah, you're just keeping constant the words that were trained.

421
00:28:08,040 --> 00:28:11,680
So you, you're only looking up the words that are in a particular document they were trying

422
00:28:11,680 --> 00:28:13,000
to infer.

423
00:28:13,000 --> 00:28:17,160
So those, you know, those vectors already.

424
00:28:17,160 --> 00:28:23,560
And you know the output, the probability of co-carns for word-to-word pairings with those

425
00:28:23,560 --> 00:28:29,560
two weights established, just their gradient descent, the, the vector that can change,

426
00:28:29,560 --> 00:28:33,920
which is the document vector that was newly initialized, that will change, that will sort

427
00:28:33,920 --> 00:28:37,320
of converge to the result they found in training.

428
00:28:37,320 --> 00:28:38,320
Okay.

429
00:28:38,320 --> 00:28:41,960
Yeah, I don't know why I was, why I was kind of weighing out on that.

430
00:28:41,960 --> 00:28:48,480
So basically you, you trained this set of document vectors based on some number of articles.

431
00:28:48,480 --> 00:28:50,600
I think it was 350,000 or something.

432
00:28:50,600 --> 00:28:51,600
Yeah.

433
00:28:51,600 --> 00:28:52,600
Mm-hmm.

434
00:28:52,600 --> 00:28:56,640
And then you've got some new article that was just someone, some editor just hit Publish

435
00:28:56,640 --> 00:28:58,120
or Submit or whatever.

436
00:28:58,120 --> 00:29:01,280
You've got this new document you're trying to figure out where it fits and you need to

437
00:29:01,280 --> 00:29:04,560
produce a document vector for it and that's the inference step.

438
00:29:04,560 --> 00:29:05,560
Yeah.

439
00:29:05,560 --> 00:29:06,560
Exactly.

440
00:29:06,560 --> 00:29:11,320
But it's quite, another way was, because we're doing this in production, another way was,

441
00:29:11,320 --> 00:29:14,040
you know, just speed at which you can infer.

442
00:29:14,040 --> 00:29:19,280
But it was, it's quite fast, it was less than 10 milliseconds and we do this asynchronously.

443
00:29:19,280 --> 00:29:25,920
So there isn't, you know, if you're going to see a page, the inference has already been

444
00:29:25,920 --> 00:29:26,920
done.

445
00:29:26,920 --> 00:29:28,720
So we're just pulling that from a database, essentially.

446
00:29:28,720 --> 00:29:34,360
And we also are, this, we can also were caching frequently and we can catch at the future

447
00:29:34,360 --> 00:29:35,360
level.

448
00:29:35,360 --> 00:29:40,400
If your request is made for a particular article, I mean, if the request is, okay, we want

449
00:29:40,400 --> 00:29:43,760
to sell in our scroll, if it's not cash, we'll just pull it from the database and it's

450
00:29:43,760 --> 00:29:44,760
very fast.

451
00:29:44,760 --> 00:29:49,680
But after the fact that response is, that's, spring of articles, this is cash so we can

452
00:29:49,680 --> 00:29:54,080
just serve it subsequently to users who come after that first one.

453
00:29:54,080 --> 00:30:01,360
Meaning for a given article, you cash the preferred similar scroll articles, yes, exactly.

454
00:30:01,360 --> 00:30:07,720
And do you, do you cash kind of the exact order or do you cash of, you know, some number

455
00:30:07,720 --> 00:30:11,480
of similar ones in a randomized or something like that?

456
00:30:11,480 --> 00:30:13,400
It's the exact response.

457
00:30:13,400 --> 00:30:17,320
So yeah, there's a, there's a filtering step where, yeah, we're, like, we're only pulling

458
00:30:17,320 --> 00:30:23,800
from our, so the, the tech stack is, and this is sort of one of the more interesting parts

459
00:30:23,800 --> 00:30:27,480
in this whole development is that we had to build out a data science platform.

460
00:30:27,480 --> 00:30:31,240
And it was great because it's, it's just a Python web app and it's serving, knowing

461
00:30:31,240 --> 00:30:33,360
you need users today.

462
00:30:33,360 --> 00:30:37,560
So to get actually, you know, to come upon the realization that you can, you know, build

463
00:30:37,560 --> 00:30:41,080
a Python web app and scale in that way, that was great.

464
00:30:41,080 --> 00:30:47,240
And is it, is it kind of a spoke Python or something like Django or something?

465
00:30:47,240 --> 00:30:53,200
We use a web app framework called Pyramid, which is, it's like Django, except there

466
00:30:53,200 --> 00:30:57,040
are significant differences, one of which is, you can use something called SQL Alchemy,

467
00:30:57,040 --> 00:31:04,200
which is a, allows you to do database queries by writing Python, so ORM.

468
00:31:04,200 --> 00:31:05,880
And that's very powerful.

469
00:31:05,880 --> 00:31:09,160
So that was one of the main reasons for going at that.

470
00:31:09,160 --> 00:31:16,840
Other than that, we use Amazon, AWS, we have a MySQL database that stores all of the

471
00:31:16,840 --> 00:31:18,240
articles.

472
00:31:18,240 --> 00:31:23,520
And then we use RDS, a Redis cache for responses.

473
00:31:23,520 --> 00:31:24,440
And that also stores.

474
00:31:24,440 --> 00:31:29,600
So when we do, we do personalize, to some extent, for like sponsored stories, and, you

475
00:31:29,600 --> 00:31:34,280
know, we serve up sponsored stories based on where users in the world.

476
00:31:34,280 --> 00:31:36,880
So that's a user level feature.

477
00:31:36,880 --> 00:31:37,880
So we do do that.

478
00:31:37,880 --> 00:31:40,760
And there's, Redis is caching those responses as well.

479
00:31:40,760 --> 00:31:43,200
So we're able to scale with the users as well.

480
00:31:43,200 --> 00:31:44,200
Okay.

481
00:31:44,200 --> 00:31:49,000
So you've got this Python based web framework website.

482
00:31:49,000 --> 00:31:53,240
And it sounds like you're saying kind of starting this effort up.

483
00:31:53,240 --> 00:31:59,200
You didn't have a data science stack built up around this, this website around the project.

484
00:31:59,200 --> 00:32:05,680
So what were, what were some of the steps in thinking it went into building up that stack?

485
00:32:05,680 --> 00:32:08,240
It sort of was built out feature by feature.

486
00:32:08,240 --> 00:32:13,200
The original feature was making recommendations for videos.

487
00:32:13,200 --> 00:32:18,440
Because so I started with, at Reuters with one of their apps called Reuters TV, okay.

488
00:32:18,440 --> 00:32:22,560
And we put the Reuters TV player on the website.

489
00:32:22,560 --> 00:32:25,520
And we wanted to, so there's algorithms that go into that.

490
00:32:25,520 --> 00:32:29,880
There's a program, something called a program assembly algorithm that takes all the available

491
00:32:29,880 --> 00:32:34,720
videos that we can show and builds the best program based on the time that the user selects.

492
00:32:34,720 --> 00:32:36,840
And there's other sort of recommendations that go into that.

493
00:32:36,840 --> 00:32:44,120
So we needed to have a system to build that for the website and also for Reuters TV.

494
00:32:44,120 --> 00:32:49,680
Meaning a user goes to the website and to some TV section and they say how long they

495
00:32:49,680 --> 00:32:50,680
intend to watch.

496
00:32:50,680 --> 00:32:54,000
Yeah, that's one of the key features of the app.

497
00:32:54,000 --> 00:32:55,000
Okay.

498
00:32:55,000 --> 00:32:59,080
And that was one of the, that was a really interesting data science problem as well, because it's

499
00:32:59,080 --> 00:33:05,680
actually, so to determine, you know, based on these 25 stories that have three links each

500
00:33:05,680 --> 00:33:07,480
of their variants.

501
00:33:07,480 --> 00:33:11,480
How do you, you know, if user wants to watch a program that's 10 minutes long, how do

502
00:33:11,480 --> 00:33:15,360
you take all of that content and get as close to 10 minutes as possible without going

503
00:33:15,360 --> 00:33:16,360
over?

504
00:33:16,360 --> 00:33:20,360
And how do you also make sure that you're filling the editorial requests of, well, we

505
00:33:20,360 --> 00:33:24,320
want to show X number of world stories and this many culture stories and that sort of

506
00:33:24,320 --> 00:33:25,320
thing.

507
00:33:25,320 --> 00:33:27,600
Kind of like we did been packing kind of thing.

508
00:33:27,600 --> 00:33:28,600
Mm-hmm.

509
00:33:28,600 --> 00:33:29,600
Yeah.

510
00:33:29,600 --> 00:33:30,600
It's called a napsack problem.

511
00:33:30,600 --> 00:33:34,200
And it sort of comes up in, you know, like these coding interviews where you think,

512
00:33:34,200 --> 00:33:35,800
like, I'm never going to have to use this.

513
00:33:35,800 --> 00:33:36,800
Right.

514
00:33:36,800 --> 00:33:41,040
But it was, I used it and it was, and there was some sort of new, what I thought were

515
00:33:41,040 --> 00:33:42,200
new things that went into it.

516
00:33:42,200 --> 00:33:47,400
Like there's a multiple choice component where you're choosing because each main story

517
00:33:47,400 --> 00:33:48,480
has three different variants.

518
00:33:48,480 --> 00:33:58,680
So that was very interesting, but yeah, the framework was really built out, yeah, feature

519
00:33:58,680 --> 00:33:59,680
by feature.

520
00:33:59,680 --> 00:34:03,040
So there's that, there's the scroll model.

521
00:34:03,040 --> 00:34:09,000
Meaning, so you started with this video, we're talking features at that level that, you

522
00:34:09,000 --> 00:34:15,320
know, there's a, the framework has this video optimization feature that scroll optimization

523
00:34:15,320 --> 00:34:20,880
feature, or are you talking about lower level features that support the building of these

524
00:34:20,880 --> 00:34:24,520
different higher level features, I guess.

525
00:34:24,520 --> 00:34:25,520
I guess, I guess both.

526
00:34:25,520 --> 00:34:26,920
I mean, it's all contained there.

527
00:34:26,920 --> 00:34:31,040
So even the models, models are loaded into memory, which we're trying to actually get

528
00:34:31,040 --> 00:34:35,880
away from that and build out another framework where requests were just made to a machine

529
00:34:35,880 --> 00:34:41,880
learning app for, you know, give us a scroll for this article and the rest of the business

530
00:34:41,880 --> 00:34:49,240
logic that was built out during this building, the state of science framework is still there.

531
00:34:49,240 --> 00:34:53,960
So yeah, there's that there's other, I'm trying to think of other data science things

532
00:34:53,960 --> 00:34:56,200
that live on this, this web app.

533
00:34:56,200 --> 00:35:04,600
We do something called a trending people module, so where we're doing name entity recognition

534
00:35:04,600 --> 00:35:08,840
on articles and we're also looking at how articles are trending.

535
00:35:08,840 --> 00:35:15,960
And we have a little module on one of the sites on the website where we show people who

536
00:35:15,960 --> 00:35:20,440
are trending and that's just based on the articles that they've been in and the fact that

537
00:35:20,440 --> 00:35:24,680
we've determined, okay, this person is being referenced and also that they're the subject

538
00:35:24,680 --> 00:35:26,520
of this lead of the article.

539
00:35:26,520 --> 00:35:27,520
Okay.

540
00:35:27,520 --> 00:35:28,520
Yeah.

541
00:35:28,520 --> 00:35:34,480
And are you, are you doing that using that same document toolkit that you mentioned or

542
00:35:34,480 --> 00:35:37,200
how are you doing the any our stuff?

543
00:35:37,200 --> 00:35:44,880
Well, we have some in-house and some using spacey, which is sort of a general really great

544
00:35:44,880 --> 00:35:51,820
NLP library, but Thomson Reuters, which is the larger company of Reuters, they have, they

545
00:35:51,820 --> 00:35:56,960
built on a really terrific name entity system, so for any given article, they will tell

546
00:35:56,960 --> 00:36:05,160
you who's in itch and that's done in, you know, in the typical ways, but from that, there's

547
00:36:05,160 --> 00:36:10,920
also something that I built out was determining, you know, we don't want to just pull out

548
00:36:10,920 --> 00:36:15,000
all the people in an article and show them like, okay, all these people are trending.

549
00:36:15,000 --> 00:36:18,920
The task is really, okay, well, we know these people are in the article, but who are the

550
00:36:18,920 --> 00:36:20,720
main subjects of it?

551
00:36:20,720 --> 00:36:25,560
And that is, that's a task of dependency parsing and saying, okay, well, if you'd like

552
00:36:25,560 --> 00:36:30,240
at the lead of the article, this person is performing the action on this person.

553
00:36:30,240 --> 00:36:35,560
So let's take the subject and say that this person's trending and that's sort of how we

554
00:36:35,560 --> 00:36:36,880
get at that.

555
00:36:36,880 --> 00:36:42,440
And so you've got these different components that you use, you mentioned spacey, and I forget

556
00:36:42,440 --> 00:36:44,680
the name of the other Gremlin?

557
00:36:44,680 --> 00:36:45,680
Jensen.

558
00:36:45,680 --> 00:36:46,680
Jensen, Jensen.

559
00:36:46,680 --> 00:36:48,680
Maybe Jensen, I'm not sure.

560
00:36:48,680 --> 00:36:49,680
Jensen or Jensen?

561
00:36:49,680 --> 00:36:50,680
Jensen again.

562
00:36:50,680 --> 00:36:51,680
Okay.

563
00:36:51,680 --> 00:36:52,680
Perfect.

564
00:36:52,680 --> 00:36:53,680
Yeah.

565
00:36:53,680 --> 00:36:58,960
You describe this kind of data science platform, trying to get at kind of what really

566
00:36:58,960 --> 00:37:04,200
characterizes this platform is at this high, these higher level features that you're offering

567
00:37:04,200 --> 00:37:10,200
through the site or are there lower level components that you've pulled together that

568
00:37:10,200 --> 00:37:13,680
accelerate the ability to make the N plus 1 feature?

569
00:37:13,680 --> 00:37:14,680
Mm-hm.

570
00:37:14,680 --> 00:37:18,240
I think it's probably the features that characterize the web app.

571
00:37:18,240 --> 00:37:19,240
Okay.

572
00:37:19,240 --> 00:37:27,000
Yeah, I mean, just thinking in terms of the end users, that's sort of what they care about.

573
00:37:27,000 --> 00:37:28,000
And who are they?

574
00:37:28,000 --> 00:37:30,000
Are they developers on the web app?

575
00:37:30,000 --> 00:37:31,000
Mm-hm.

576
00:37:31,000 --> 00:37:37,080
Oh, they're, so I built the web app, but they're front-end, mostly, developers.

577
00:37:37,080 --> 00:37:44,440
But yeah, I mean, if there's the sort of the check that goes into the modeling, the lower

578
00:37:44,440 --> 00:37:49,080
level stuff, yeah, I mean, I think down the line that could be something that, you know,

579
00:37:49,080 --> 00:37:55,480
we want to try to build out and Riders is, you know, we have a lot of resources and even

580
00:37:55,480 --> 00:37:57,760
though the team is very small.

581
00:37:57,760 --> 00:38:01,040
That's something that I think that would be good to, you know, if you wanted to think

582
00:38:01,040 --> 00:38:05,320
about contributing to the community, the science community, that would be really terrific.

583
00:38:05,320 --> 00:38:06,320
Mm-hm.

584
00:38:06,320 --> 00:38:09,480
How big is the data science team there?

585
00:38:09,480 --> 00:38:10,800
It's not big.

586
00:38:10,800 --> 00:38:16,080
It's less than 10 people, so we're, you know, trying to grow in different ways.

587
00:38:16,080 --> 00:38:21,320
But yeah, and then the developer team is also not big, but it's good to, you know, be

588
00:38:21,320 --> 00:38:24,040
small and agile in that way.

589
00:38:24,040 --> 00:38:28,440
Cool, anything else that we, that you plan to cover in the presentation that we haven't

590
00:38:28,440 --> 00:38:30,120
already talked about?

591
00:38:30,120 --> 00:38:31,720
I think we got to most of it.

592
00:38:31,720 --> 00:38:36,880
At the end, I do talk about just sort of the future of embeddings.

593
00:38:36,880 --> 00:38:42,200
And I think that there's some question now because they have been outperformed in some

594
00:38:42,200 --> 00:38:44,280
ways by different types of embeddings.

595
00:38:44,280 --> 00:38:48,400
There it's sort of a suite of models.

596
00:38:48,400 --> 00:38:53,600
One of the better known ones is something called Elmo, which is an acronym called Elmo.

597
00:38:53,600 --> 00:38:54,600
Elmo.

598
00:38:54,600 --> 00:38:55,600
Okay.

599
00:38:55,600 --> 00:38:56,600
It's a language model.

600
00:38:56,600 --> 00:38:57,600
It's types of embeddings.

601
00:38:57,600 --> 00:39:02,520
And essentially, it's getting at, essentially, just deeper context.

602
00:39:02,520 --> 00:39:10,000
So, you know, with normal word vectors, you capture the context and of that word in a data

603
00:39:10,000 --> 00:39:11,000
set.

604
00:39:11,000 --> 00:39:12,320
But this actually gets at multiple contexts.

605
00:39:12,320 --> 00:39:18,400
So the, something called a polysemi, which is the multiple meanings of individual words.

606
00:39:18,400 --> 00:39:23,960
So it's, it's, it's more advanced models, more complex.

607
00:39:23,960 --> 00:39:28,280
There are also some sort of structural things about it that allow it to be used in downstream

608
00:39:28,280 --> 00:39:31,160
tasks, a variety of downstream tasks.

609
00:39:31,160 --> 00:39:36,320
And it seems that it's, it's really outperforming in these tasks.

610
00:39:36,320 --> 00:39:39,080
Word embeddings, just sort of an yellow word embeddings.

611
00:39:39,080 --> 00:39:44,240
And I think a lot of the inspiration, this is coming, just lifting this from this guy,

612
00:39:44,240 --> 00:39:49,720
Sebastian Reuters, I could have that, and he developed Elmo and he had a blog post talking

613
00:39:49,720 --> 00:39:56,360
about when his NLP is going to have, it's image net moment, referring to, yeah, it's

614
00:39:56,360 --> 00:40:02,680
really thought provoking and, yeah, getting at, you know, when we, when is NLP really

615
00:40:02,680 --> 00:40:07,080
going to break out and in terms of, you know, a single type of model or a single data

616
00:40:07,080 --> 00:40:09,160
set, that's why he was talking about image net.

617
00:40:09,160 --> 00:40:13,320
And, you know, for computer vision, it was really both like the acceleration that happened

618
00:40:13,320 --> 00:40:21,240
so, swiftly there was because of the data set, and then, and also the models, but, yeah,

619
00:40:21,240 --> 00:40:26,000
and then, but, you know, moving that, doing transfer learning, moving those models and

620
00:40:26,000 --> 00:40:30,160
the weights that were trained on this very rich data set, to other tasks, that was sort

621
00:40:30,160 --> 00:40:32,320
of the image net moment.

622
00:40:32,320 --> 00:40:36,080
And with these embeddings, you know, they're trying to get it, universal embeddings that

623
00:40:36,080 --> 00:40:42,360
can be used in a variety of different tasks downstream, and they're similar to, they're,

624
00:40:42,360 --> 00:40:46,800
you know, they're looking in context, they're, they're sequence models, so you're predicting

625
00:40:46,800 --> 00:40:50,840
words based on context, but it's in a much deeper way and it's, um, my directional

626
00:40:50,840 --> 00:40:56,240
LSTMs. So, you know, given that they, you know, it seems like they've, the metrics are much

627
00:40:56,240 --> 00:41:00,360
higher for these, um, there's a lot of question about, like, you know, what's going to happen

628
00:41:00,360 --> 00:41:04,840
with, with the NLP were embedding some document vectors, so, but, yeah, we'll see.

629
00:41:04,840 --> 00:41:11,040
Yeah, Sebastian and I are, uh, over due to doing interview, we're supposed to be getting

630
00:41:11,040 --> 00:41:12,480
back in touch now, I think.

631
00:41:12,480 --> 00:41:13,480
Fantastic.

632
00:41:13,480 --> 00:41:15,080
Well, I'll tell you much more than I go to.

633
00:41:15,080 --> 00:41:16,080
Yeah.

634
00:41:16,080 --> 00:41:20,320
Anyone listening to this that was intrigued by the comment, um, we should be diving into

635
00:41:20,320 --> 00:41:21,320
that set.

636
00:41:21,320 --> 00:41:22,320
Cool.

637
00:41:22,320 --> 00:41:23,320
I'm looking forward to that.

638
00:41:23,320 --> 00:41:24,320
Awesome.

639
00:41:24,320 --> 00:41:25,320
Well, thanks so much.

640
00:41:25,320 --> 00:41:26,320
It was great to have you on the show.

641
00:41:26,320 --> 00:41:27,920
I appreciate you stopping by and, uh, looking forward to your talk.

642
00:41:27,920 --> 00:41:28,920
Yeah.

643
00:41:28,920 --> 00:41:29,920
Thanks, Simon.

644
00:41:29,920 --> 00:41:30,920
Appreciate it.

645
00:41:30,920 --> 00:41:36,480
All right, everyone, that's our show for today.

646
00:41:36,480 --> 00:41:42,640
For more information on James or any of the topics covered in this show, visit twimlai.com

647
00:41:42,640 --> 00:41:46,120
slash talk slash one 83.

648
00:41:46,120 --> 00:41:52,360
For more information on the Stratidata podcast series, visit twimlai.com slash Stratid and

649
00:41:52,360 --> 00:41:54,880
Y 2018.

650
00:41:54,880 --> 00:42:08,840
As always, thanks so much for listening and catch you next time.

