Hello and welcome to another episode of Twimble Talk,
the podcast where I interview interesting people,
doing interesting things in machine learning
and artificial intelligence.
I'm your host, Sam Charrington.
So as you know, a couple of weeks ago,
I announced the first anniversary of the podcast
and to celebrate it our first anniversary
listener appreciation contest.
I wanna start this show by thanking everyone
who participated in the contest.
You all have come out in droves over the past couple of weeks
to shower the podcast with love and for that,
we are humbled and forever grateful.
Overall, we receive nearly a hundred entries
via our website and iTunes from listeners
from 16 countries.
Your stories have been awesome
and we're most proud that we can help
light that AI fire for you every week.
Of course, everyone who entered will receive
a couple of Twimble and AI stickers,
one for you and one for a friend.
Those will be on their way soon.
But now, the moment you've all been waiting for,
we have our winners for the first anniversary
listener appreciation contest.
Just a reminder, our grand prize winner receives
a bronze pass for the O'Reilly AI conference
at the end of this month.
And our runner-up winner gets, well, hey Google,
what's the second prize?
The second prize in Twimble's first anniversary listener
appreciation contest is a brand spanking
your Google home, like me.
All right, without further ado,
our second prize winner is anchor Patel.
Anchor wrote in on the show notes page with this.
Twimble, congratulations on your first anniversary.
I have loved listening to the show.
I remember the news and machine learning
from the first few months, what a sheer amount of information.
But the more recent interviews format is excellent.
I particularly love the ones where you interview folks
at AI and ML conferences, very insightful indeed.
Thanks for contributing to the broader ML community.
Well, thank you, anchor, for being a loyal listener
and will be in touch with you about that Google home.
And now, our grand prize winner is Mason Grimshaw.
Mason also wrote in via the show notes page.
Here's what he said.
The show is fantastic.
I started listening right when you switched to the interview
format, and I've definitely noticed your improvement
as a journalist and the improvement of the show.
Great job.
I go to school in Boston, so the show keeps me company
on my 20 minute walk to school every morning,
and I couldn't ask for a more interesting companion.
I'm studying analytics.
And while I may not directly use AI in my profession,
I certainly use it as a hobby.
I really enjoyed Carlos Gastron when he came on
to talk about the line paper and the Danny Lang discussion
about video games.
Well, Mason, we hope to see you in New York in a few weeks.
Congratulations, and thank you for being a loyal listener.
Although there can only be one first prize winner,
we'd like to give everyone the opportunity
to attend the O'Reilly AI conference with us.
With the code PC Twimmel, that's PC-TW-I-M-L,
all of our listeners will get 20% off of registration fees
and purchasing passes for the conference.
Please let us know if you're planning to attend the event.
We're looking forward to meeting up
with Twimmel listeners there.
Okay, to continue with the O'Reilly AI theme,
this week I've got a very special guest.
I've invited my friend Ben Lorca onto the show.
Ben is chief data scientist for O'Reilly media
and program director for strata data
and the O'Reilly AI conference.
Ben has worked on analytics and machine learning
in the finance and retail industries
and serves as an advisor for nearly a dozen startups.
In his role at O'Reilly,
he's responsible for the content
for seven major conferences around the world this year.
In the show, we discuss all of that.
Touching on how publishers can take advantage
of machine learning and data mining,
how the role of data scientist is evolving
and the emergence of the machine learning.
Hey, everyone, I am on the line
with Ben Lorca.
Ben is the chief data scientist at O'Reilly media
and he's also responsible for content
for both the strata data conference
as well as the O'Reilly AI conference.
Hey, Ben, how are you doing?
Great, great to be here, Sam.
Awesome, awesome.
So every time I talk to you,
you are just getting off of a plane
and it sounds like that is the case this time as well.
Yeah, so we had an outstanding strata data conference
in London.
Yeah, basically many, many people from many different parts
of Europe came and spoke and attended.
And this year in particular,
we had concerted effort to focus on deep learning
on the machine learning site.
But the staple topics of strata were still remain popular
particularly on architecting big data application.
Okay, awesome, awesome.
Well, I thought we'd start this conversation
by having you spend a little bit of time
talking about your background
and how you got started with data
and how you kind of your path leading up to O'Reilly.
Sure, so I have a PhD in math
and focused on non-linear partial differential equations
and towards the end of graduate school,
I became interested in specific set
of differential equations called stochastic differential equations
which turn out to be at least theoretically important
for quantitative finance.
So I had some interest already
in kind of the industrial applications
of what I was doing,
but I definitely was on the academic track.
So after grad school,
I was an academic for five years.
But then at some point I decided
I was actually much more interested in industry.
And at the risk of dating myself,
at least at the time when I was contemplating
moving to industry,
there was no data science track.
So the exit strategy was becoming a quant
and so that's what I did.
I did that for about two and a half years
in a small hedge fund,
designing, trading models, risk management, portfolio management,
and things like that.
And one of the first things I learned, of course,
is the stuff I thought was going to be super important,
the stochastic PDEs,
while good to know,
not exactly what I needed to do for my job.
So at that time, actually the term machine learning,
I would say was kind of nascent.
The people were still using the term data mining.
And so that's what I did at this.
I applied basically statistical techniques
and machine learning to financial time series.
But then at some point I realized my interest
actually were much more on the tech side,
the programming and building software applications
for analyzing these time series.
And so then I ended up moving to Silicon Valley
around, actually believe it or not,
at the peak of the nascent at the time,
which was around March, 2000.
And so then I was here,
I was here just in time for that first bust.
And then the first bust, yeah.
I was there around the same time.
Yeah, yeah, yeah.
And so,
Do you keep track of what's going on
in the quantitative side of the financial markets
and how folks or how the technology has evolved
since you worked in that space?
I'm not that close to, I still have friends.
And obviously my work leading these two big conferences
that you described, try to data
in the O'Reilly Artificial Intelligence Conference
brings me in contact with the people working in finance.
So I keep track of it that way.
Yeah, so to some extent I do,
but I'm not immersed in the latest techniques
that they're using.
Although I would say that they are actually moving more
towards our world of using machine learning,
alternative data sources, big data,
and some of these more bleeding edge machine learning
techniques, including deep learning.
So to the extent that they're actually showing up
at the events I organize, that's how I keep track.
Okay, okay.
So I interrupted you.
You, you, oh yeah.
So then at some point to the technical side of things.
I got into the technical side of things.
And when I first moved to tech,
I think you know, this will surprise a lot of your listeners,
but you know, one of the big users of data in tech
are the marketing and sales people.
So that's how I kind of moved into tech was basically
talk what I learned in in finance
and started applying it in marketing and sales applications.
So did a couple of startups,
joined a couple of startups that didn't really take off
and then eventually at some point ended up at O'Reilly
as a data scientist working on the types of data that we have,
which is a lot of it is sales data,
but a lot of unstructured and semi structured tech.
So I was, I was definitely one of the first people
doing a lot of these techs mining and machine,
machine learning for techs from the early days.
I mean, I may have, I may have been one of the first people
to actually use this topic models LDA
that David Bly and Drew Aing and Mike Jordan wrote about
for an industrial consulting project.
So, oh wow, can you tell us a little bit more about that?
Oh, I mean, so it's with a well known tech company, right?
So hired us at O'Reilly to analyze our tech sources,
unstructured tech sources, which include job posting,
all the job posting to the US.
Okay, and things like that.
And to just get them, give them strategic advice.
So I thought, I thought using LDA and topic models
would provide some quantitative basis
for the advice we were giving.
And from what I understand,
it did change the direction of this major,
major tech company, tech company that everyone knows about.
Mm, so I guess one thing that I've been meeting
to ask you for a while now actually is,
you've now got five strata conferences, right?
And the two AI conferences,
and I think you're involved in all of those, is that right?
Yeah, so I'm responsible for the program for all of those.
So basically everything that we have at these conferences,
we have two day trainings, we have tutorials,
we have sessions, and then we have keynotes.
So basically, I'm ultimately the person responsible
for the lineup for all of these conferences.
Okay, and so my question then is,
do you actually have any time for doing data science
at O'Reilly nowadays?
Or are you, how could you possibly
with all of these conferences?
You know, I would say that it's become less and less, right?
So I think as we kept adding more conferences,
and as you know, many of these conferences
are spread out across geographic regions, right?
So we have conferences in the US,
but we have conferences in Europe and Asia.
And so yeah, I've found myself with less and less time.
So to be honest.
So now I'm much more less of a practitioner,
more of basically watcher from afar.
But I think that my background and my ability
to read the original papers and talk to the researchers,
many of which whom I've known for many years,
I think I still have kind of some feel,
but maybe not as much of the hands-on feel
that I would like.
But I would say that the trade-off for that though
is I've gained a much more global perspective, right?
So I don't know to what extent you've organized events,
but you know, I mean, for us,
particularly since we organize events across the world,
you can't really just take your set of speakers
from California.
And take them to somewhere else, right?
So you really have to know the local scene,
the local companies, the local communities.
And so to me, I kind of found that very kind of rewarding,
so that I know the data scene in Southeast Asia,
in China, in Europe, and things like that.
So one thing that if we could maybe spend some time
talking through a little bit of the kinds of problems
that you tend to see at O'Reilly with regard
to data science and machine learning in AI
and the types of approaches and techniques
that you are using to solve them,
I think listeners would enjoy hearing a little bit
at that detail.
I think at this, like many companies,
this is not going to be a surprise.
O'Reilly is an older company, it's not a startup.
So we do have many, many different systems,
some are kind of the new bleeding edge open source system,
some are older proprietary systems, right?
And so I actually, to believe it or not,
one of our main problems is still
to this day data integration, you know?
I mean, just because we do have many, many systems,
just getting the data all together in one place
is it remains a challenge.
And then beyond that, I think,
luckily we do have a team dedicated to the data engineering part.
If it remains a, for us, it remains a work in progress
because we also keep adding systems that we're using.
You know, there's many, many software as a service,
DCs, right?
So different parts of the company starts using,
start using different things.
So that's one problem, the other problem is still,
I think a lot of our data is still unstructured, right?
So I mean, I guess there's some structure.
I mean, so if you think about books,
there's some structure there, right?
So, but we also now,
I don't know to what extent you're following
our Safari platform,
which is increasingly relying on, for example, video, right?
So, okay.
So there's a lot of data that we rely on
that remains unstructured.
And one of the, one of the challenges for us,
too, as a kind of a media company
that is building a learning platform for training
is to take all of these many, many data sources,
both structured and unstructured and organized.
So it turns out actually that, you know, search
and a nice human curated taxonomy
is still kind of,
remain the basic problems for companies like us, right?
So let's say, for example, you wanted to learn something
in a new field of machine learning, right?
So we may have thousands and thousands of sources
because our Safari platform doesn't just rely on our content,
it relies on our content partners as well.
So we will have to organize, you can do a search,
so that's one way for you to probably navigate
our Safari platform.
But increasingly, we've, we're finding that
people want kind of curated content, right?
So how do I learn about this new topic?
Well, we'll use kind of the combination
of humans and machines to organize a learning path for you
or a resource center inside Safari.
So I think that whole taxonomy creation
and grappling with data integration
and unstructured data, those are our main challenges.
And have you invested much in personalization using
some of the machine learning and AI techniques
that folks are using for that kind of stuff?
I think I would say we're still in the early phases
work in progress.
Yeah, that's a good point to be honest
because people learn differently, right?
So each individual learn differently from another.
And so to the extent that we are,
at least on our online division,
we are trying to build the best learning platform.
So a lot of that will increasingly
have to rely on personalization.
But I would say we're still in the early, early stages.
And do you have a particular vision in mind
for how the technology is put to use,
to enable a certain kind of experience?
Like do you have a sense for what that experience
looks and feels like and how it's different
from what someone might experience today
and then what the supporting pieces might need to be?
Yeah, in our case, because for example,
many Safari users use them through their companies.
Let's say you work for a large company
that has a subscription to Safari.
Right.
So it will be kind of a combination of you
us serving you content personalized to you,
but us also serving you content
that kind of reflects what your particular organization
is emphasizing or the rest of your team members
are learning about.
So, yeah.
And then the other thing Sam actually,
I should mention is that inside Safari,
we now have live online training on many of the topics
that your listeners might be interested in,
including big data, infrastructure and architecture,
machine learning, data science,
and increasingly we're finding actually
there's a lot of demand for content
that I would describe as much more non-technical.
So a lot of people are grappling with.
They read about a specific topic.
They may not need to implement it right away,
but they need to know at a high level what it is about
and should they be bringing that into their company.
And if so, what are some of the steps they should do
to integrate such and such technology
or technique into their existing products?
Well, so you talked a little bit about the kind of exposure
you get in terms of in your roles
with the different conferences.
I'm interested in kind of taking your temperature on,
you know, the various trends you're seeing out there
and what you're finding most interesting.
Ah, so good.
So I just gave a keynote about this in Israel yesterday.
Oh, really?
Oh, awesome.
Yeah, so I would say on the machine learning side,
last year I kind of told people that this year
I think deep learning will become a machine learning technique
that the people in the data science community
will start using.
So as you know, deep learning is a lot more associated
with the other conference that I run,
which is the AI conference, where they're grappling
with data from images and video and audio,
also computer vision and speech technologies
and things like that.
So what we're seeing is there's a lot of hunger
in the data science community to see
if they can take deep learning and use it
to replace some other existing machine learning technique
that they're using.
So for example, some people are looking at it
in terms of recommender systems.
Some people are looking at it to replace
how they do search rankings and things like that.
So that's one trend.
And so on the data science side,
the other thing we're kind of seeing,
and this might at this point be much more of a day area thing
is that people are starting to talk about a role
that is a hybrid between the classic data scientists
and the data engineer.
So a lot of people use the term machine learning engineer.
So what it is is basically someone who is maybe
a little stronger on the software engineering side.
So they write code with the express intention
that this might or this will be deployed into production.
So it's not one off, sloppy.
And then they also tend to think much more holistically.
So if we're going to deploy this into production,
what is our logging infrastructure,
what is our AB testing infrastructure and so on.
And then, yeah.
So then the emphasis is on production
and less on prototypes.
Okay.
And it's interesting how the role, you know,
these roles just keep being redefined, right?
I think a few years ago,
the big conversation was that we saw,
we actually thought of the data scientists
as this monolithic person, right?
They needed to know how to do everything, right?
They needed to be, you know, statistically savvy,
you know, and know the math behind, you know,
the analytics and the machine learning stuff,
they needed to, you know, understand how to get data
from all these systems because, you know,
their reality was that they spent, you know,
80% of their time or more,
just kind of shuffling data around.
And then we started to get the, you know,
the role started to split off a little bit.
And you started to see, you know, data engineers,
you know, being thought of as separate from, you know,
machine learning people.
And in some places, you'd pair those, you know,
those two with, or I'm sorry, data engineers being,
you know, separate from your data scientists.
And in some places, you'd pair those two with, you know,
your real professional programmers, software engineers.
You know, now it sounds like you're saying,
you know, we kind of came up with a new name.
And it's, you know, again, it's this kind of unicorn
that's supposed to know how to do everything.
Am I reading it?
Am I reading that right?
I would say, no, I mean, I slightly, yeah,
but I think the original term of data scientists
was exactly what you described, which was the unicorn.
This is not someone who has a PhD in machine learning
necessarily, right?
So maybe the background here is much more
of us on the engineering side.
And then they learn it enough machine learning
to know how to basically build machine learning
enabled products.
Got it, got it.
And part of that is that, you know,
the emphasis on production means also knowing
what to do with these models once they hit production, right?
So how to tell when a model gets stale, you know,
and things like that, and when do we need to retrain it?
So, but I would say the background might be much more
on the engineering side and then learn enough machine learning
to start being able to move much more fast, right?
And there might still be data scientists in the organization
to kind of build the prototypes,
but increasingly, I think at least a simpler machine learning
things, maybe this machine learning engineer can take on.
And in fact, actually, if you talk to companies
that use a lot of deep learning,
they even have a much more specific role,
which they call the deep learning engineer, right?
Okay, so then other trends.
So on the data, so the other thing that I pointed,
I've been pointing out to people is this notion
that we've had a lot of progress in machine learning, right?
So you can just read online publications
and there's all sorts of papers being released,
lots of interesting developments, right?
So, and that's great, right?
So, but then I've been, what I've been telling people,
there's an imagine a scenario where nothing happens
on the research front for the next five to 10 years,
or next five years, right?
So, I mean, so my position is that there's still
so much low-hanging fruit in many companies,
including us or Riley, that you can just take what we know now,
you can take what we know now and implement it,
implement it, and we're going to be fine.
We're still going to be fine, right?
So, like I said earlier, we're still grappling
with data integration, right?
Yeah, and I don't think that you guys
are necessarily unique among enterprises.
I think there are some large sophisticated enterprises
that are kind of at the front edge of this thing,
but a lot of folks are really just in the stage
of trying to figure out where it fits
and how to best apply it and where they can extract
the most value and how to put together the teams
to be able to do it because of the talent shortages
as you're well aware.
Yeah, and then to be honest, there's been a lot of progress.
So, now we have to take a lot of those ideas and use them.
And to a large extent, actually, I think that
while we have been fascinated with kind of horizontal platforms,
I think a lot of the interesting applications
of these machine learning models
will be in kind of verticalized applications, right?
So, and I think we'll increasingly see companies
specialized in inserving these industries.
And interestingly, actually, there's a intersection
with the AI community in the sense that
while we read a lot about the general AI,
actually the way the DC community and investors,
at least when you talk to them, have been investing
is they're investing in focused applications, right?
So, whatever that might be, security, drug discovery
and things like that.
So, the other thing that I've been talking to people
about is training data, right?
So, we sometimes forget that a lot of the developments
in deep learning really relied on the existence
of large labeled training data sets.
And to the extent that if you survey companies,
I think that still remains the bottleneck, right?
So, it's not the model.
It's not delivering great models.
It's just coming up with training data.
And there's a lot of interesting things happening, right?
So, in the deep learning community,
you have these generative models
to mostly around generative adversarial networks, right?
But then also in the data science community,
there's interesting work, for example,
by my friend Chris Ray at Stanford, who had a system
called deep dive, and now the next generation is snorkel,
where they basically are able to take
noisier data sources.
So, they start with less labeled data,
supplemented with noisier data sources,
and then they're able to build much more accurate models.
So, I think there's a lot of,
we sometimes forget the importance of data.
Yeah.
And so, I think there'll be still a lot of interesting research
in how we get to training data sets much more efficiently.
And then on the machine learning side,
the other thing that I've been talking a lot about recently
is real time or live data.
So, here I owe my inspiration to the rice lab,
the successor to the amp lab.
So, the amp lab, as people know,
is developed that originated Apache Mesos,
Apache Spark in Alaxia.
So, the new lab, rice,
stands for real time, intelligent, secure execution.
So, live data is basically,
basically think about an agent interacting with an environment,
right, so a user interacting with a website,
a robot navigating its environment, self-driving car,
a player playing a computer assistant player,
playing a computer game,
like an Atari game or go.
So, there you have an environment,
you have an agent interacting with an environment.
So, in the classic reinforcement learning sense,
you're trying to learn a policy, right?
So, given a state of the environment,
what action should, what action should I take?
But, you know, if you actually take a step back
and kind of look at the flow of data
in these types of applications,
the first part looks a little bit like what we've been dealing
with in recent years.
So, it might look like a streaming application.
So, you have data ingestion and things like that,
stream processing and things like that.
But the machine learning part is slightly different, right?
So, in the reinforcement learning sense,
you're trying to learn this policy,
you have to run a lot of simulations,
you have heterogeneous computer graphs,
and if it's truly a live application,
you need to have merely second latency, right?
So, then it turns out the existing frameworks
we have are not able to do machine learning
in these really live dynamic environments.
Yeah, so the classic tools that we've been using
aren't able to do the machine learning
you need in these environments,
which I think increasingly will be much more common, right?
So, as the tools get better,
the use cases will make up much more clear.
So, the people at Rice Lab,
took a look and kind of surveyed,
so what's out there,
and then they realized the computational framework
for these types of applications don't exist, right?
So, then they ended up building one called Ray,
which is out in Alpha,
and Ashley, we're gonna do a tutorial
on reinforcement learning using Ray
at our AI conference in San Francisco.
But the other thing I wanted to emphasize here
is that the interesting thing to me
is kind of machine learning in this live environment, right?
Sorry.
It turns out right now that people
are using reinforcement learning,
but there's other techniques that might emerge.
And the interesting thing about Ray
is that you can use it for reinforcement learning
or other approaches.
So, for example, the OpenAI folks
recently published a paper
where they use evolutionary algorithms, right?
So to solve some of the things
that you would do with reinforcement learning.
But the interesting thing is that Rice Lab people
took that paper and basically they just implemented
this evolutionary algorithm in Ray, no problem.
So, like I said,
I think that the tools are kind of a work in progress
and that includes Ray and as the tools get better,
then people will start using these tools
and then the use cases will be much more clear.
But basically think about any kind of dynamic setting
where you want to be able to take advantage
of machine learning.
And right now I would say Sam,
that my other conference, the AI conference is definitely
much more aware and interested in focus
on these types of applications.
So, for example, we're already seeing
that reinforcement learning content in tutorials
are very popular at that conference.
But I think as the tools get better,
maybe we'll start seeing the data science community
start using them to write.
So, for example, look back three years ago,
deep learning would have been inaccessible
to the data science community.
We didn't write the tools, like TensorFlow, MxNet,
BigDL, and PyTorch, and all these things.
But as the tools got better,
people start kicking the tires, right?
So, go ahead, Sam.
I was just going to drill into your comment
about reinforcement learning.
You know, typically the literature in talking
about reinforcement learning is looking at kind of an agent
exploring an environment and trying to maximize some policy.
So, the off-sighted example is like an agent
being trained to play Atari video games,
like breakout and maximize their score.
Do you have a sense for how this translates
into the real-time streaming machine learning example
that, or even industrial enterprise type scenarios?
You know, I think that the use cases still need to be worked out
by you can imagine a personalization
on the website, maybe, right?
So, where you're interacting with a website,
much like you're interacting with a game.
Right.
I mean, it's on the AI side, you're gonna see applications
in autonomous vehicles and drones.
Sure.
Maybe in inventory management,
if you really need real-time inventory management,
definitely the finance people might be interested in this
from trading strategies or portfolio design.
And then resource allocation, if you imagine the scenario
where resource allocation with live data becomes prevalent.
But definitely robots are in any kind of robotic environment,
so a robotic application is already using it.
But I think I imagine a place, a scenario where
if the tools get better, the people will probably
find the right use cases.
Sure, sure.
Yeah, one of the other things that I find interesting
about the whole real-time machine learning scenario,
and let me know if you have come across this
or have any thoughts on this,
but there seems to be,
there seems to be in those kinds of environments,
a merging of traditionally your training and inference
are two totally different things.
And when we're talking about kind of real-time streaming data
more and more, I see people wanting to do things
like active learning where they're,
the learning is real-time in addition to the inference,
which folks more easily do real-time now.
Is that what you're seeing as well?
Uh, yes, to some extent.
I mean, I think that the use cases that I'm interested in
tend to still separate the training inference.
I mean, the use cases that I'm much more familiar with
tend to still separate the training inference.
I think there are people who are kind of pushing
the envelope towards much more of this online learning scenario.
But then, but then now we start getting into the scenario
I just described, right?
So we're just kind of learning really with live data
and interacting with an environment, right?
So where you have,
where simulations and explorations,
the ability to do those at large scale at very low latency
come into play and those scenarios
are really quite different.
Yeah.
And actually, this is a good tag way
to the last thing I wanted to emphasize,
which is compute, right?
So you were in the age of big models,
which is deep learning, big data,
which is the training data and live data and big compute,
right? So as you mentioned,
in this scenario, we need everything, right?
So we need scale, throughput, latency,
all of that with low power consumption.
So I think there's a lot of interesting things
happening there like what is the future infrastructure
for machine learning, right?
So I think those are still very active areas.
A lot of things happening at a rapid clip, right?
So both from the GPU side, the CPU side,
the CPU side, FPGAs and ASICs and all of that, right?
So, yeah, so I think sometimes people forget
that to make all of this work,
you still need hardware, right?
And hardware, there's a lot of trade offs
when you get to hardware.
Yeah, yeah, and unfortunately, I think for a lot of people
working in the space, you can't forget it enough, right?
I think over time, the level of abstraction
is gonna have to raise where people can just have
the full flexibility to do the things that they wanna do
without having to think about how they can figure their jobs
to run on GPUs or distributed or what have you.
But still, I think a lot of thoughts still has to happen.
More thought than it should be, right?
Yeah, but I think, I think we're getting to the point
where you got these tools for hardware
and software acceleration and then the software libraries.
So I think that for most practitioners,
the only time they'll think about it
is when they look at their bill, right?
So, it's nice.
At least for most practitioners, right?
But then for the bleeding edge researchers
who have to worry really at the low level,
like at the level of interconnects and things like that,
because they're trying to break or set the record
for speech recognition a lot of that still matters.
But for regular people, it's just the cost,
I think, is what they're gonna end up.
That's how they're gonna know what they're using, right?
So the other thing that, or another thing that I often enjoy
chatting with you about is the interesting startups
that are doing interesting things,
both in Silicon Valley and around the world.
What, anything come to mind there,
or I'm particularly interested in ones that we don't hear
about all the time that may be in other parts of the world.
You know, there's a bunch of startups in China
that I'm not sure people here have heard of,
but just generally around applying deep learning
to whatever vertical, right?
So manufacturing, drones.
And to some extent, there's similar applications
that we would see here, but for that market, right?
So for speech recognition and intelligent chatbots
and things like that.
But I would say, actually, so if you look at the AI,
so the one country that I think is really interesting
in terms of its excitement and fascination for AI
is China, right?
So just organizing a conference there
and people just are dying for content in this area.
So in terms of startups, I would say,
I don't know how you feel about it,
but I'm really interested in the startups
that are much more focused.
Yeah, because you know, I think the whole,
if you're gonna do a platform,
it's gonna be tough to compete with these cloud providers,
right, so unless you have a platform
that's focused on a vertical maybe, right?
But, you know, I mean, Amazon, Google, Microsoft,
they have pretty impressive tools for doing,
for doing almost the end to end, right?
So for building these applications.
But if you were very, very focused,
I think that's where you can excel.
So you might be a focused startup on drug discovery
or even actually you can take an area like computer vision, right?
So I happen to advise a startup called Matroid
that's trying to be kind of a computer-fishing enabler
for many companies.
So then they can, they can,
they can, they can take kind of much more of the product approach
in terms of how do companies use this easily
to solve problems, right?
So whatever, whatever, it might be summarizing surveillance cameras
and things like that.
I mean, I guess you can build all of these things yourself
using existing tools or the cloud,
but they already have a product
that your analysts can use, non-programmers, right?
So I'm quite interested in companies like that.
I've been trying to kind of get a,
as you mentioned earlier,
as to whether I pay attention to finance.
I've been recently trying to figure out
what's happening in finance on some of these technologies
and I haven't.
I don't have a quick answer, right?
So it seems like there's a mix of hype and reality.
But finance is kind of also a peculiar industry
in the sense that maybe the most interesting things
are happening in companies who don't want to talk to you.
Yeah, yeah.
Finance can be like.
I know what I mean.
So, so recently, for example, I tried to,
I had one of my editors,
I introduced him to a bunch of companies, right?
So in finance and to try to get a handle on what's happening.
It's just hard.
The people that we think are doing interesting things
don't want to talk.
So.
Yeah.
Yeah.
What else?
So I think,
I think,
like I said, data,
is still going to be an important thing.
So I think people who have interesting data
who are able to take public data and make it usable.
Right?
I guess Chris Ray and Mike Afarella
have this notion of dark data.
Right?
So,
taking data that's very unstructured
in making,
in infusing it with structure
so that you can use it for applications.
Of course,
I think there's still a lot of,
there's still a lot of competitive advantage
to people who have good data.
So what's next?
Well, the AI conference is next literally.
It's coming up at the end of June.
And I mentioned to you
that this podcast is going to be published
at the same time we're announcing a winner
of a giveaway,
a ticket giveaway for the AI conference.
And one question that I had for you was,
you know, I attended the first one.
It was a great event.
I had lots of great conversations there.
I heard great talks.
Yeah.
What's going to be different
about the second event?
So a few things.
One,
the first event was a 2D event.
We did not have training
or tutorials.
So at this event,
we will have both.
So for example,
we have 2D training
on deep learning with TensorFlow.
And then a bunch of other trainings.
And one that stands out is 2D training on,
on NLP with deep learning
with my friend Delip Brown.
Along.
And Delip is also,
Delip is also the organizer of
a fake, fake news challenge.
And so the winners of which
will present at the conference.
Is this generating on the tutorial side?
Detecting.
Yeah.
Yeah, yeah.
And on the tutorial side,
we have a bunch of interesting tutorials
from reinforcement learning.
In this particular edition of the conference,
we're actually going to offer tutorials on a variety of
deep learning frameworks.
Right.
So not just TensorFlow.
We have.
Okay.
Big D L and MX net.
Okay.
We're also trying not to be a deep,
we're trying to be the industry gathering place for AI
where you can learn about many,
many different techniques in AI
and how to use it in your organization or your company.
Okay.
So to that,
and we're also,
we're also going to offer trainings in non-deep learning techniques
like probabilistic programming.
Uh-huh.
And what else?
So the other actually,
so reinforcement learning is a popular tutorial.
It's emerging.
As I mentioned earlier,
and the other popular tutorials have
are once aimed at the non-technical audience, right?
So how do I bring,
how do I manage an AI project?
How do I bring AI back into my company?
Right.
Um,
and then all the keynotes are going to be great.
Right.
So we have,
uh,
David Farucci,
Dave Farucci,
who led the IBM team,
uh, that one jeopardy,
the quiz show.
Okay.
So he hasn't spoken in many,
in many years,
but he has a new research outset
called elemental cognition.
Mm-hmm.
So he's going to give a keynote about what they're up to,
which is basically,
they're trying,
they're taking one of the grand challenges
of AI natural language understanding
and, and basically,
uh,
trying to,
uh,
come up with a system
that can,
uh,
do that well.
Um,
um,
and then, uh,
besides Dave giving a talk,
uh,
one of his colleagues
will do a 40-minute session,
deep dive,
uh,
what, uh,
the technology and techniques
elemental cognition,
uh,
is doing to,
to correct natural language understanding.
Okay.
Uh,
Josh,
uh,
Josh Tanenbaum of MIT,
uh,
you know,
I've long been fascinated by,
uh, what they do.
So basically,
they're trying to develop,
uh,
techniques
that make,
that help machines learn
and think like people.
So one,
I think one of the things that,
uh,
deep learning is great at,
is,
uh,
uh,
perception
and large scale,
uh,
and pattern recognition.
But it's still,
it's still relies on a lot of data.
And so,
Josh and his crew
are trying to come up
with alternative methods
for maybe taking deep learning
and infusing it
with startup knowledge,
uh,
making it,
uh,
much more efficient,
and much more similar
to how people think.
Okay.
And then,
uh,
I don't know if you followed recently,
but a group at Carnegie Mellon,
uh,
led by Thomas Sandholm,
one,
uh,
uh,
I'm not a poker player,
but,
uh,
one of these poker tournaments,
uh,
where they beat out a bunch of,
uh,
human,
human top human players.
Okay.
Uh,
so similar in,
you can think of this achievement
that's basically almost
at the scale of AlphaGo.
Right.
Right.
People don't,
uh,
aren't as aware of it.
So he's giving
a key note about this.
A lot.
About,
uh,
how they,
uh,
won the tournament.
Sounds like a great line up.
And so,
yeah,
and just like the previous conference,
we have,
sessions on many of the techniques
that the people are interested in,
but much more,
our focus is,
uh,
you know,
we're also going to try to provide
a track for people who are interested in,
uh,
how to bring these ideas
and,
uh,
technologies and methods
back into their organizations
and,
uh,
implement them into their products.
Okay.
But, uh,
we also try to,
uh,
I invited a bunch of my academic friends,
uh,
are going to be speaking at the conference about,
uh,
really cool things that,
uh,
industry people
will find interesting,
and maybe,
kind of spark a conversation
and,
and see how,
uh,
we can,
uh,
you know,
be a true gathering place for industry,
uh,
interested in,
uh,
building AI products.
Awesome.
Uh,
it sounds like it's going to be a great time,
and I'm,
certainly looking forward to it.
Um,
and it'll be great to,
you know,
catch up with you in person once again.
Cool.
Yeah, yeah, yeah.
And, uh,
at the risk of,
uh,
being, uh,
putting another plugin,
but, uh,
we also have an AI conference in San Francisco,
in September.
Absolutely.
Uh,
and we're still,
uh,
uh,
where I would say 80%
there,
as far as completing the lineup,
but it's already looking great.
And,
I'm sure you're,
you'll be there too, right?
Of course.
Yep.
Looking forward to it.
Um, is,
is there,
uh,
CFP still open for that,
or has that been closed out?
That's been closed out for,
okay.
San Francisco.
Okay.
All right.
Great.
Well, Ben,
thanks so much for,
uh,
taking the time to be on the podcast.
It was wonderful having you on,
and again,
looking forward to seeing you in a few weeks.
Thank you, sir.
Thanks.
Bye-bye.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
All right, everyone.
That's our show for today.
We love, love, love,
caring from listeners about the show.
You can leave your questions and comments
over on the show notes page
at twimmalai.com slash talks,
slash 26,
where you'll find links to Ben
and the various resources we mentioned in the show.
And as always,
our quote contest continues.
Just drop us your favorite quote
on the show notes page
or your social media network of choice
and we'll send you a laptop sticker.
Once again,
thanks so much for listening
and catch you next time.
