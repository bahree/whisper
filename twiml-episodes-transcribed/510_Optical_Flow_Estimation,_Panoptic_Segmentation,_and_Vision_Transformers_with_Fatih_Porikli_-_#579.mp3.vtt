WEBVTT

00:00.000 --> 00:11.320
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I'm your host,

00:11.320 --> 00:16.960
Sam Charrington. And today I'm joined by Fatih Pratikli, Senior Director of Artificial

00:16.960 --> 00:22.440
Intelligence at Qualcomm. Before we get into today's conversation, be sure to take a moment

00:22.440 --> 00:27.840
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,

00:27.840 --> 00:33.160
we'd greatly appreciate your five-star rating and review. Fatih, welcome to the podcast.

00:33.160 --> 00:38.640
Thank you so much for hearing me, Sam. This is pleasure. I'm very excited. I'm all yours.

00:38.640 --> 00:43.520
All righty, I'm really looking forward to digging into our conversation. We'll be talking a lot

00:43.520 --> 00:49.920
about your research in the areas of computer vision and perception. But before we do that,

00:49.920 --> 00:54.160
I'd love to have you share a little bit about your background and how you came to work in the field.

00:54.160 --> 01:02.160
I am a computer scientist and electrical engineer. I was on the both sides of the fence,

01:02.160 --> 01:08.560
the fence, dividing academia and industry several times. You know, as everyone, I started my PhD.

01:08.560 --> 01:14.320
I did some research. I was a research assistant. I stayed at the university. Then I moved to

01:14.320 --> 01:20.400
industry. Many years after that, working on real problems. We're challenging problems.

01:20.400 --> 01:27.280
Maybe 13 years after that, I switched back to academia again. I was a full professor,

01:27.280 --> 01:36.160
tenor professor for a long time. Then I found myself intentionally, of course, in industry again,

01:36.960 --> 01:44.320
even trying to solve bigger problems, trying to create bigger impact for everyone. So now I'm with

01:44.320 --> 01:51.040
Qualcomm. And the whole time, have you been working on computer vision or have you switched areas of

01:51.040 --> 01:59.760
interest? Computer vision was always there. That is one of the things really excites me,

02:00.480 --> 02:09.280
amazes me, because if we consider human brain electrical activity, maybe 70-75% of what we

02:09.280 --> 02:18.400
actually consume in our brain dedicated to visual perception, significant portion of brain is also

02:18.400 --> 02:26.160
goes to visual understanding. So vision is the way that we understand, makes sense of the

02:27.120 --> 02:33.600
world life, everything around us. Actually, if we close our eyes, you know, I just close,

02:33.600 --> 02:40.800
a loss of vision might be the most devastating disability. You know, it comes so naturally to us,

02:40.800 --> 02:47.360
the way that we understand the 3D scene, recognize people, recognize faces, recognize objects. So

02:48.000 --> 02:55.040
then I was almost always interested in how we can make computers, machines to, you know,

02:55.920 --> 03:01.200
have that capability as well, this visual understanding, visual perception. So computer vision

03:01.200 --> 03:08.240
has been always there. Before maybe, let's say, 25-30 years ago, it was more conventional,

03:08.240 --> 03:15.840
engineered solutions, you think about, okay, what would the human perception do? How would

03:15.840 --> 03:21.840
brain work? And how I'm going to sit down and try some mathematical description to convert it

03:21.840 --> 03:30.640
to something a computer would understand. Now we are kind of as many of us are very familiar,

03:30.640 --> 03:42.480
we are using AI artificial intelligence to make it natural, look into data and learn something

03:42.480 --> 03:48.880
automatically from observing the environment around us. Awesome, awesome. Can you share a little bit

03:48.880 --> 03:54.800
more around your areas of interest from a research perspective? Under perception, there are

03:54.800 --> 04:02.800
several modalities. One is working with image and video data. This would be directly related to

04:02.800 --> 04:10.960
computer vision and then there is 3D data, point cloud and 3D representations. That's also,

04:10.960 --> 04:17.840
I will say, that computer vision as well. But perception is not only in these two modalities,

04:17.840 --> 04:25.040
3D point cloud and image video visual data, there is also RF radio frequency signals all around us.

04:25.040 --> 04:32.560
And they are, in a way, kind of the lights that we see, they are all around us feeling

04:32.560 --> 04:38.800
the space. We also look into those invisible frequencies and try to understand everything about

04:38.800 --> 04:47.520
the scene, about the world, about how everything works, objects, intrex. So these are the modalities that

04:47.520 --> 04:53.680
I am very much interested in it. What do we do with these modalities? For instance, in images

04:53.680 --> 05:02.640
and videos and 3D point cloud data, RF signals, any signal actually, including X-ray and ultrasound,

05:02.640 --> 05:14.160
we detect things. For instance, whether there is a vehicle or a person on the street, we reconstruct

05:14.160 --> 05:20.160
3D model of the world around us. That's also very interesting, very challenging, actually,

05:20.160 --> 05:26.160
if you want to just use one single image, not two, like we do. We have two left and right eye,

05:26.160 --> 05:32.800
so we use stress-coffee vision, but can you do it just using a single camera image? And the answer

05:32.800 --> 05:40.160
is yes, for a while, you know, I was really impressed with that one, and recognizing activities,

05:41.600 --> 05:47.760
labeling everything in the scene. In a way that what goes on the lower level in our brain, we

05:47.760 --> 05:56.800
want to do all of these, accomplish all of those processes, perception, and all of them were

05:56.800 --> 06:04.400
very interesting to me personally, and these tests, these understanding goes into many applications

06:04.400 --> 06:11.120
from, let's say, XR, augmented reality and virtual reality to autonomous vehicles, to robotics,

06:11.120 --> 06:19.600
IoT, you can imagine, wherever there is a human being, and if you replace or put a machine in front

06:19.600 --> 06:26.800
of it, kind of those applications exist, all around us and a computer vision enables all of those,

06:26.800 --> 06:34.480
and that's why I think it's very exciting to me. And some of these problems are big problems,

06:34.480 --> 06:42.160
they are not solved problems, they are presenting a big challenge, so that's another attractiveness

06:42.160 --> 06:50.000
for many people. So I want to dig into a few of the perception-related papers that you've got

06:50.640 --> 07:03.920
at CVPR this year, and the first of the ones is a paper on panoptic segmentation. The full title is

07:03.920 --> 07:09.840
Panoptic Instance and Semantic Relations, a relational context encoder to enhance

07:09.840 --> 07:16.400
panoptic segmentation. Let's start at the beginning, what is panoptic segmentation?

07:16.400 --> 07:25.680
Yeah, it's a long title, San, panoptic segmentation. So there are things and stuff around us,

07:25.680 --> 07:30.960
right? Things are the comfortable things like there is one vehicle, another vehicle, another vehicle,

07:30.960 --> 07:36.080
one glass, another glass, one person, another person, but there are also uncomfortable things like

07:36.720 --> 07:43.680
sky, like building, like road, is not comfortable, right? So segmentation, the goal of segmentation,

07:43.680 --> 07:51.280
take this visual information, images and video, or point cloud, and then label every pixel,

07:51.280 --> 07:57.600
every region with the identity of that region. For instance, if it's a sky, if we see sky,

07:57.600 --> 08:03.600
it will tell, computer will tell, okay, this is a sky pixel, that's specific pixel and the region,

08:03.600 --> 08:09.280
if there is a person, it will tell, this is person, but it's not going to just say person,

08:09.280 --> 08:15.680
it's going to say that this is person A, and another person B, even though they are occluded

08:15.680 --> 08:21.920
each other, maybe half of the person B is visible, it will still distinguish. So this is a very

08:21.920 --> 08:30.800
challenging task. You are trying to label all data pixels in this case with these corresponding

08:30.800 --> 08:38.160
things and stuff identities. That is what penoptic segmentation does. And so from that,

08:38.160 --> 08:45.520
in that sense, it is kind of a superset of instant segmentation, which is identifying the things

08:45.520 --> 08:50.960
and semantic segmentation, which is more focused on the stuff. Very good, absolutely. Yes, actually,

08:50.960 --> 08:58.720
that's the right technical description, instant segmentation and semantic segmentation together

08:58.720 --> 09:05.760
will give, would be under the penoptic segmentation. Describe the setting for this paper,

09:05.760 --> 09:11.360
what is the problem that you set out to the solve? So now we understand what penoptic segmentation is,

09:12.080 --> 09:18.880
and as maybe I should point out that recently there has been significant attention and

09:18.880 --> 09:26.320
excitement around a new technology in AI, which is called as transformers. So transformers,

09:26.320 --> 09:35.360
let me briefly mention that when we give an image data video algorithm learns which parts of

09:35.360 --> 09:43.040
for instance, image are related to each other. It knows to collect such supportive information,

09:43.040 --> 09:49.040
pay right attention to the right parts. For instance, if there is a vehicle part,

09:49.040 --> 09:55.840
a tire would, you know, put more confidence, if less everyone to detect a vehicle,

09:57.280 --> 10:04.000
with the hood of the vehicle, door of the vehicle. But road pixels, sky pixel, they will,

10:04.000 --> 10:11.200
even though they may look similar, attention mechanism will ignore those. So it will focus on the

10:11.200 --> 10:18.480
better parts, supportive evidence to, you know, make such deductions. So transformer is

10:19.680 --> 10:26.800
self-attention, advanced self-attention mechanism. It is important to relate these areas,

10:26.800 --> 10:35.760
image areas, and it has been applied to semantic segmentation before. Now the challenging part for

10:35.760 --> 10:42.160
penoptic segmentation, there are, you know, unknown random number of things like these instances

10:42.160 --> 10:49.600
of the other classes, like people and vehicles in the scene. How are we going to use transformers

10:49.600 --> 10:58.080
to make sure that these instance of, let's say, this person kind of is different. This transform

10:58.080 --> 11:04.240
mechanism would distinguish from the other person. Otherwise, since they are, they look same,

11:04.240 --> 11:10.560
they will make the transformer will think that they are same thing, and it may not distinguish

11:10.560 --> 11:19.600
these two. For penoptic segmentation, we want to label them separately. So this paper explains,

11:19.600 --> 11:27.600
first time in the world, how you can actually kind of combine this type of attention mechanism

11:27.600 --> 11:35.680
into a segmentation framework. Got it. And so what have prior approaches tried to do

11:36.640 --> 11:44.480
for solving penoptic segmentation? On a high level, we can consider there are a single shot or,

11:46.320 --> 11:53.280
you know, multi-shot setting, single-shot setting. It aims to take an input image and directly

11:53.280 --> 12:01.680
label each pixel as a different object, instance of an object, like this person and the other person.

12:02.320 --> 12:10.720
And there are multi-shot algorithms, multi-shot meaning first, we find these regions of interest.

12:11.680 --> 12:17.360
You can think that those are like boxes. You say, okay, this is like a box. There's a person here,

12:17.360 --> 12:24.000
another person here, another person here, then in the second stage, we go and look, okay,

12:24.000 --> 12:31.840
these two, this box contains a single instance of a person, and it doesn't overlap with anything,

12:31.840 --> 12:36.560
okay, good, then I will just go, you know, do segmentation between these boxes and I will find

12:36.560 --> 12:43.360
segmentation to person. If there is overlap, then I will kind of infer which one, which pixel

12:43.360 --> 12:49.920
is blocked, which person there are two or multiple persons. So this is the other way of doing that.

12:50.560 --> 12:58.720
Usually these algorithms lack an attention mechanism across different instances. We can do it

12:58.720 --> 13:04.240
pixelized, but if I have one person here, another person behind it, another person far away,

13:04.240 --> 13:14.240
how I'm going to learn where to focus if I want to detect all of them at the same time. So this

13:14.240 --> 13:19.760
is what we accomplish. For a different number, varying number of instances,

13:21.360 --> 13:32.160
CVPR paper shows that you can learn or train an algorithm that would learn to focus on the

13:32.160 --> 13:38.480
right areas of the image, which then improves the accuracy of the segmentation. That's what we

13:38.480 --> 13:44.800
show in the paper. Got it, got it. I think this is maybe related to the single shot versus multi-shot,

13:44.800 --> 13:49.440
but I got the impression from the paper that one of the big things that you're doing differently

13:49.440 --> 13:57.440
here is that previous attempts have tried to, hey, in order to solve penoptic segmentation,

13:57.440 --> 14:04.800
let's do image segmentation or instance segmentation and then semantic segmentation

14:05.440 --> 14:12.640
separately and kind of put the results together, whereas you're doing them as part of a consistent

14:12.640 --> 14:20.480
coherent system. Absolutely, very good point. Now, the same network can do in an end-to-end

14:20.480 --> 14:27.200
fashion, these two tests together. And when you do it together in an end-to-end fashion in the same

14:27.200 --> 14:34.880
network, they support each other. They don't kind of dismiss, but for instance, semantic segmentation

14:34.880 --> 14:42.160
will generate or instance segmentation will generate the leverage together and which generates

14:42.160 --> 14:50.400
better improved numbers, honestly. What were some of the biggest challenges to this approach?

14:50.400 --> 14:57.040
Transformer architecture or self-attention architecture. One challenge, I can say, that they are

14:57.040 --> 15:07.520
computationally, you know, intensive and how to make them efficient was a challenge. And also,

15:07.520 --> 15:14.480
you know, our people is not specific to any specific backbone. A backbone usually considered

15:14.480 --> 15:22.480
as a pre-processing neural network takes image or video and then creates useful features for the

15:22.480 --> 15:28.000
downstream test, like semantic penoptic or instance segmentation or many other tests.

15:28.000 --> 15:36.880
Our algorithm, our idea actually, can apply to any backbone, any, you know, kind of a segmentation

15:36.880 --> 15:43.680
framework, it could be plugged into improve their performance. In the paper, we tried maybe more than

15:43.680 --> 15:52.960
15 or 20 different segmentation algorithms and every time we plugged in this type of transformer-based

15:54.160 --> 16:04.640
instance self-attention with, you know, kind of semantic segmentation, the results were much better.

16:04.640 --> 16:13.520
How do you have both the ability to plug in whatever segmentation model you want to use?

16:15.200 --> 16:19.760
Was that specifically for the instance segmentation or for either of the components?

16:20.640 --> 16:28.800
What our algorithm does, it leverages these features coming from the previous segmentation algorithm

16:28.800 --> 16:36.000
and then it takes them and it learns, reweighting them. In a way, that's what transformer does.

16:36.000 --> 16:44.160
The input transformer is some kind of, let's simplify it, let's say it is an image, output is

16:44.160 --> 16:54.720
another image, let's say, but what you see, like maybe now much clear and focus on the right parts.

16:54.720 --> 16:59.840
Maybe input image, you can't think that it's a noisy image and there are some, you know, kind of like

16:59.840 --> 17:07.280
things not very visible in the output, now much sharper and kind of these things are clearly

17:07.280 --> 17:14.880
distinguishable. Of course, it is not an image but goes into this network, it is a set of feature

17:14.880 --> 17:23.280
vectors and it's called as the features for image. Each pixel has a feature descriptor,

17:23.280 --> 17:31.440
those descriptors goes into this component and comes in a better, more trustable,

17:32.960 --> 17:42.800
kind of more useful manner. You know, then we do that, then we make the features better,

17:42.800 --> 17:49.360
any downstream test will benefit from that. So all these segmentation algorithms, they have

17:49.360 --> 17:55.040
this type of feature generators either at the beginning or at the end. So this idea can plug

17:55.920 --> 18:02.880
kind of and directly apply to those feature maps. Got it, got it. So the core of what you've done

18:02.880 --> 18:08.320
is this transformer that takes as input these feature vectors and you don't really care how the

18:08.320 --> 18:16.160
feature vectors are created, any of these algorithms that you've tried, it worked just fine as part of

18:16.160 --> 18:23.040
your network. Yes, this is a strength of it, it can take any of the features and make them better,

18:23.040 --> 18:30.240
you know, more representative of the test that we want to accomplish. However, this is also

18:30.240 --> 18:37.440
something known in the paper, then we use this thing and we also go back to the previous

18:37.440 --> 18:44.880
stages like feature generator and other branches to make them even more accurate. So kind of

18:44.880 --> 18:52.880
when we do end-to-end training using PISR, the paper, the idea that we talk about in the paper,

18:53.760 --> 19:01.600
all network becomes kind of updated and even the previous part, feature generation parts

19:01.600 --> 19:09.760
improves. So overall kind of that further improves the accuracy. Semantic segmentation, you know,

19:09.760 --> 19:15.680
penoptic segmentation is one of the most challenging tests in computer vision. It's very difficult

19:15.680 --> 19:21.040
for a human to segment. By the way, if you want to do, if I, if you ask me to go, you know, label each

19:21.040 --> 19:27.920
instance, I will do something, but if you ask another person, it will do differently, you know,

19:27.920 --> 19:36.080
even for humans, there is significant variation in the outputs of how we, you know, do penoptic

19:36.080 --> 19:43.040
segmentation, instance and semantic segmentation for a computer vision algorithm for a machine to

19:43.040 --> 19:51.200
do it even more challenging. So this type of ideas really kind of pushes the state off to our

19:51.200 --> 19:59.200
such that, you know, they are becoming more and more feasible for bigger use cases to improve

19:59.200 --> 20:05.840
our daily lives through these applications in XR and auto and other type of use cases.

20:06.480 --> 20:14.240
How did you measure the performance of your model? There are very recognized metrics and there

20:14.240 --> 20:22.960
are benchmark data sets. So the huge benchmark data sets, those benchmark data sets have the

20:22.960 --> 20:29.440
ground truth information one way or another defined. These are real also data sets, real images,

20:29.440 --> 20:41.040
real videos. We use those metrics, for instance, mean IOU or similar metrics. It defines how well

20:41.520 --> 20:50.320
one mask, object mask overlaps with another one. So this is very common in semantic segmentation.

20:50.320 --> 20:56.000
For instance, segmentation, there are similar, you know, advanced versions of this thing now

20:56.000 --> 21:04.560
considering whether kind of we are confusing identity of the instances or not. So there are

21:04.560 --> 21:11.280
these common metrics and benchmarks that this how we evaluate the algorithm. And what kind of results

21:11.280 --> 21:18.480
did you see? Then we submitted the paper, we look at all the existing state of the art,

21:18.480 --> 21:25.440
existing work including the archive, things appeared on the archive, not maybe published,

21:25.440 --> 21:32.400
but very fresh things. So just before the submission deadline, the previous week we applied

21:32.400 --> 21:41.680
whatever we found and every time we observed improvement on this segmentation pipelines

21:41.680 --> 21:49.280
and our results also when we submitted to different or investigated, you know, generated results

21:49.280 --> 21:55.520
on these benchmarks were the top of the line. And in the paper, we showed that, you know,

21:55.520 --> 22:02.480
those are the best segmentation results possible. Of course, the field is changing, maybe

22:03.600 --> 22:09.600
next CVPR, there might be even better numbers that might be coming from us or other people,

22:09.600 --> 22:16.800
but at the time, it was the number one on multiple datasets also, not one dataset.

22:16.800 --> 22:24.000
Okay. And where do you see this particular line of research heading? Is it a solve,

22:24.000 --> 22:32.960
is panaptic segmentation a solve problem now? I think when the conditions are right, it is,

22:32.960 --> 22:40.880
this performance is, you know, almost product quality level, but there is significant variation

22:40.880 --> 22:47.600
in the input quality. For instance, it could be a dark image, it could be a very noisy image,

22:47.600 --> 22:53.040
it could be a blurred image, you know, you can imagine, you know, there might be many problems

22:53.040 --> 23:00.480
in the image, things may be very small, some of the objects might be very tiny and not maybe only,

23:00.480 --> 23:05.680
let's say, a hand of a person would be visible, you know, significantly up to the country,

23:05.680 --> 23:11.440
our goal is actually to take that hand as well, you know, kind of even the only is the hand.

23:11.440 --> 23:19.520
So those type of very difficult settings still require more work to make the algorithm

23:20.400 --> 23:25.440
to be more robust, generalize those type of challenging situations. And also,

23:25.440 --> 23:33.040
another thing, if, for instance, we use a data set and that data set collected in a specific

23:33.040 --> 23:39.200
manner using maybe the similar type of cameras and labeling might be similar and lighting might

23:39.200 --> 23:46.720
be similar, but now in a specific application, the environment lighting, everything would be

23:46.720 --> 23:52.560
different, how to adapt to that such domain changes, domain variations is one thing.

23:52.560 --> 24:01.360
Another thing, some, I mean, we don't, these things, instances could be any class,

24:01.360 --> 24:06.800
right? I may be repeated many times, like it could be a person, it could be a vehicle,

24:06.800 --> 24:12.240
but it could be anything, right? It could be, you know, a piece of machinery,

24:12.240 --> 24:18.400
it could be a kind of component in an assembly line, you can imagine, you know,

24:18.400 --> 24:24.400
it could be a bird, multiple birds, you know, for kind of, you can imagine, this is like a commodity,

24:24.400 --> 24:30.000
this type of AI solution, people would like to take it and count the number of ends, you know,

24:30.000 --> 24:37.600
in a lab setting, those type of things. So how to adapt automatically with minimal labeling

24:37.600 --> 24:43.760
to such different classes, different type of things, different type of stuff for semantic

24:43.760 --> 24:50.640
and instant segmentation, it is, I think, still a challenge and we are working on all of those

24:50.640 --> 24:56.640
problems, which would, for instance, take our solution and automatically adapt to a very

24:56.640 --> 25:02.960
different, completely different, you know, class, set of classes, different type of objects.

25:02.960 --> 25:12.880
So there is still work to be done, but the quality of the segmentation results for key applications,

25:12.880 --> 25:20.080
for instance, camera essentials for autonomous vehicles, for XR applications, for robotics,

25:20.080 --> 25:28.160
I think it's very promising and soon such solutions, either from us or maybe, you know,

25:28.160 --> 25:36.080
everyone in the world using our solutions will appear in products. I'm very confident about it.

25:36.080 --> 25:43.360
Awesome. Does this approach assume prior knowledge of the classes? I was envisioning this like

25:43.360 --> 25:49.040
an autonomous vehicle type of scenario where you, you know, you have a camera off of,

25:49.760 --> 25:55.040
you know, a picture of a road, a camera off of the front of the vehicle and there's something

25:55.040 --> 26:02.240
in the road and you're trying to differentiate, you know, not something in the road versus something

26:02.240 --> 26:06.720
in the road, but that's a different problem than what we're talking about here. Oh, this may be

26:06.720 --> 26:13.680
obstacle detection or, you know, we have a class of unrecognizable things as well. There are ways to

26:13.680 --> 26:21.360
solve it. Yes, that is also possible, but if it's supervised solution, if the target classes

26:21.360 --> 26:28.640
changes, it's a problem. You have to do this transfer learning. There is a specific term for that,

26:28.640 --> 26:37.440
you know, my goal target classes now change. How I'm going to leverage on the previous network

26:37.440 --> 26:43.200
that I trained and maybe some portion of the previous data, anything semantically related,

26:43.200 --> 26:51.200
semantic information and now I can adapt to this type of things. Two, the day things are not even

26:51.200 --> 26:59.600
be defined. Usually, there is a class of unidentified things. We don't know what they are,

26:59.600 --> 27:07.200
like these are unidentified object classes. You have all, like you all see something, you know,

27:07.200 --> 27:14.320
we can give it a name, but then maybe some other intelligent mechanism has to decide,

27:14.320 --> 27:19.520
oh, is this something that I should worry about it? If I'm approaching that thing, is it going to

27:19.520 --> 27:25.200
be a problem for us or not? We are, of course, that's a more higher level inference. There are

27:25.200 --> 27:31.520
ways of doing that. Supervised learning, of course, limited. Now, at Qualcomm, also, we are looking

27:31.520 --> 27:37.600
at self-supervised solutions or unsupervised solutions. Exactly for, this is one of the reasons,

27:37.600 --> 27:43.520
you know, we cannot expect people to generate these data labels, ground truth, to train these

27:43.520 --> 27:49.760
algorithms over and over again. And human beings, we don't learn that way, right? I mean, it's not

27:49.760 --> 27:57.680
like here's an example of a cat, example of a dog, example of a tiger, and then we remember that

27:57.680 --> 28:06.400
we know when we see an animal, that is an animal, you know, it is not like, I don't need a training

28:06.400 --> 28:12.240
data. Even if I don't see it, I would infer, you know, I would deduce that, okay, it looks like a

28:12.240 --> 28:22.240
tiger, so it might be some type of tiger. So we do it using this continual self and unsupervised

28:22.240 --> 28:29.120
learning. And we are, we have solutions actually, you know, kind of applied to different tests.

28:29.120 --> 28:35.600
In this paper, we don't really talk about that, but that is something we are very active on it as

28:35.600 --> 28:42.080
well. Another paper that we wanted to chat about was the imposing consistency for optical

28:42.080 --> 28:49.040
flow estimation paper. Tell us about the problem that you're trying to solve there.

28:49.760 --> 29:01.040
Yeah, absolutely. So optical flow is finding where each pixel in the current image was in the

29:01.040 --> 29:11.200
previous image. So in a way, it is motion. It describes pixelized motion. Why this is important,

29:11.200 --> 29:17.600
because if I know where the pixel was in the previous frame, then, you know, first of all, I know

29:17.600 --> 29:25.440
how things are moving in the scene. I can deduce about the camera motion, and then I can also

29:25.440 --> 29:34.160
understand object motion. For instance, we have a headset, XR headset, and or AR glass, we are

29:34.160 --> 29:41.120
moving our head. And this is most, but then some other people also most. So we know this type of

29:41.120 --> 29:49.920
motion, which is important. And also, I can relate the previous frame to when I compute or make

29:49.920 --> 29:57.680
deduction for the current frame. So motion is important. And optical flow is what you will

29:57.680 --> 30:08.240
to obtain if you correspond this pixel with its previous location in the previous frame. So it

30:08.240 --> 30:14.000
looks like a field, you know, last so kind of like motion vector from individual pixels to the next

30:14.000 --> 30:19.440
to where that pixel ends up in the next frame. Of course. Yeah, it could be from current frame to

30:19.440 --> 30:25.120
the previous frame or previous frame to the current frame or current to the future frame. We can

30:25.120 --> 30:32.560
predict also. Absolutely, sir. Got it. And so what's the approach that you took with this paper?

30:33.200 --> 30:41.200
So one challenge, again, in AI, in data driven learning, the dataset, what I mentioned before,

30:41.920 --> 30:51.920
we need ground-through data for supervised training. Here is, let's say, two images like the

30:51.920 --> 30:59.120
current frame and the previous frame, video frames. And this is the optical flow motion between them.

30:59.840 --> 31:07.680
So we can, you know, if we synthesize those images, we can we know we control everything,

31:07.680 --> 31:13.920
we might have a game engine, for instance, or any, you know, any software can generate this type

31:13.920 --> 31:19.920
of different, we can, for instance, move the image, things in the image in a game, and we know how

31:19.920 --> 31:25.600
they moved, the type of ground-through is available. But if we have real images, you know, how we are

31:25.600 --> 31:33.520
going to find the ground-through, like how each pixel is moved is not like something measurable.

31:33.520 --> 31:37.040
You can think that there's a much harder labeling problem than what we just talked about.

31:37.040 --> 31:45.280
Absolutely. So such datasets, I mean, still computed datasets, there are some datasets,

31:45.280 --> 31:52.320
smaller scale. And in AI, we want big datasets, you know, huge datasets with tens of thousands of

31:52.320 --> 31:59.680
samples. It doesn't exist. So if we don't have the dataset, we will not have a good model.

32:00.480 --> 32:06.640
So in this paper, but we show that, you know, you do not need such a big dataset. We will do

32:06.640 --> 32:12.640
self-supervised learning, unsupervised learning. We will, for instance, take the previous image,

32:12.640 --> 32:20.160
and we will do some transformations on it. We will rotate it, we will warp it, and we will

32:20.160 --> 32:25.520
apply lots of different degradations, you know, without really destroying the image, still,

32:25.520 --> 32:30.320
you know, it's like similar scene, and then we look at it, you know, it would maybe look a little

32:30.320 --> 32:37.600
bit noisier or less noisier or the color is different, but maybe it's also warped. So we do all

32:37.600 --> 32:45.520
type, this type of transformations, and we know, because we applied those, we defined those

32:45.520 --> 32:51.600
transformations, so we know the ground truth in that way. So what about leveraging on that thing,

32:51.600 --> 32:58.080
and if we do that, you know, kind of optical flow should be consistent with the way that we warp

32:58.080 --> 33:08.080
transform the image, this one thing, we also look, okay, if I do forward, if I go backward, what would happen.

33:08.080 --> 33:17.040
Let's say there's, my hand is moving, right, in front of my face, and then it moves, either it

33:17.040 --> 33:24.240
occludes some parts or reveals some other part, and that's important. If I have two frames, it's

33:24.240 --> 33:31.440
not like every pixel is going to be visible in both images, right. This is called as occlusion,

33:31.440 --> 33:41.840
and occlusion map, we want our network to not, you know, handicapped by the occlusion areas. So

33:41.840 --> 33:48.560
if we detect such occlusion areas, automatically, and if we manage them, automatically again,

33:48.560 --> 33:55.200
in the network, maybe as a separate, you know, channel estimating those occlusion maps, it will,

33:55.920 --> 34:02.080
overall, it will benefit during training and in the inference time also explicitly by estimating

34:02.080 --> 34:10.320
such occlusion maps. So this paper does all of these things that I mentioned. And just to elaborate on

34:10.320 --> 34:15.360
what you just said, it sounds like what you're trying to do is not necessarily

34:15.360 --> 34:24.160
teach the network to predict occlusion or anything like that for its own benefit, but rather,

34:24.160 --> 34:29.920
you're trying to teach the network to identify when there is occlusion. So it doesn't take

34:31.600 --> 34:37.360
the ground truth that it's creating otherwise, and it does, it knows if that date is going to be

34:37.360 --> 34:45.600
bad because it's, you can't relate the one pixel to the next. Absolutely. Occlusion masks are not

34:45.600 --> 34:54.320
available. I mean, we synthesize them. This is a sub supervision part, and also these transformations,

34:54.320 --> 35:02.640
we define them, and it will be, it is a large set of transformations, and we then apply all these

35:02.640 --> 35:09.840
training, you know, improvements, enhancements, novelties to a network, which is

35:11.680 --> 35:20.560
one of the state-of-the-art models for optical flow motion computation. It creates these cost volumes

35:20.560 --> 35:27.520
and in different scales, and then it starts with a previous optical flow or just random, you know,

35:27.520 --> 35:35.200
initialize optical flow, and every time it trace itself at these iterations, the optical flow,

35:35.200 --> 35:41.040
estimates optical flow becomes more and more refined, more accurate and especially higher resolution.

35:41.040 --> 35:49.360
So this is what we do, then we, for instance, take these ideas, training ideas, self-provised,

35:49.360 --> 35:54.880
unsupervised training ideas, and modify the network such that now it can also do occlusion,

35:54.880 --> 36:01.280
reasoning, and kind of train it in this manner, with the existing data sets, you know,

36:01.280 --> 36:07.760
still simple data sets, even on those data sets, it improves the performance.

36:09.040 --> 36:13.760
And there is a benchmark, there are multiple benchmarks, actually one is called as Kitty,

36:13.760 --> 36:20.960
the other one is Sintal. In both those benchmarks, our solution was when we submitted,

36:20.960 --> 36:29.840
and later also, because it's an open benchmark, it was ranking on the top of the data board,

36:29.840 --> 36:38.240
and if I'm not mistaken, there are more than 200 solutions, 100 of them is somehow AI-based,

36:38.240 --> 36:48.160
deep learning-based. So that was quite good news. We weren't expecting, but we were confident,

36:48.160 --> 36:53.520
this is the right solution to do, and yeah, it went to the top of the leader board with the

36:53.520 --> 37:01.840
solution. What's the future direction for this particular approach? Very good question, Sam.

37:02.640 --> 37:11.920
This solution, since it requires big cost volumes, and iterations, they are computationally

37:11.920 --> 37:18.400
expensive, and they require a lot of memory because of the cost volumes, and because of

37:18.400 --> 37:26.480
iterations, they are slow. So what we are working on now, and you know, kind of we will have a demo

37:26.480 --> 37:34.720
very soon, now we show that the same solution could run on a mobile phone, on a call-com platform,

37:34.720 --> 37:45.360
in real time, for a large input size, input image size, large video size. So this has been

37:45.360 --> 37:53.600
quite an exactment for us. We put a lot of effort to make it more efficient. Yeah, this is our

37:54.480 --> 38:00.000
current work, and we also want to extend it to other things. You can do this type of things

38:00.000 --> 38:07.760
for one camera, and this is one video, right, on the same camera, but you can do optical flow,

38:07.760 --> 38:15.840
which is called a scene flow, across multiple cameras, and you will find 3D motion, not 2D motion.

38:15.840 --> 38:22.960
So we are extending to that, and optical flow is core for many other, you know, perception

38:22.960 --> 38:29.760
test, and higher level understanding. We are now plugging this solution into different pipelines to

38:29.760 --> 38:35.200
see how much improvement we would get. Super solution is one of them, for instance. Yeah.

38:36.800 --> 38:43.840
Okay. Well, there's one more paper that you have at CVPR, and we wanted to make sure to touch

38:43.840 --> 38:50.560
on that one as well. The next one is dense vision transformers for single image inverse rendering,

38:50.560 --> 38:55.840
and newer scenes. This particular one is focused on inverse rendering. What's that problem?

38:55.840 --> 39:04.400
So usually, when we synthesize a scene, we know about 3D, we know about the objects, like there's a

39:04.400 --> 39:10.480
catch, there's a chair, there's a ball, and we know the color of those. We also know their properties,

39:11.280 --> 39:18.400
reflectance properties. We want to generate a scene, computer graphics. So we know the location

39:18.400 --> 39:25.680
of the light. We know many things about the scene, like surface, normals, albeda, roughness,

39:25.680 --> 39:33.600
you know, you can imagine. So it will look real time, real life. So inverse rendering, so this is

39:33.600 --> 39:39.600
rendering, this synthesizing that I mentioned, inverse rendering starts on the other end, it takes

39:39.600 --> 39:46.800
an image, natural image, and then it tries to find these components, for instance, the lighting,

39:46.800 --> 39:56.560
location, lighting direction, lighting intensity, room shape, I mean, for indoors, and the properties

39:56.560 --> 40:03.680
of the everything, objects, all the objects in the scene, their shape, their color, their

40:03.680 --> 40:09.360
materials, whether it's leather or, you know, metal or wood or cloth, those type of things. So

40:09.360 --> 40:18.160
inverse rendering takes an image, real image, and kind of finds these components. Each one of them

40:18.160 --> 40:23.520
you can think that is either an image, you know, a reflectance image, a color image, albedo image,

40:24.560 --> 40:31.040
or a 3D model, you know, or lighting location, you know, the heat maps and those type of things.

40:31.040 --> 40:39.680
This is the inverse rendering pipeline, and these people use a transformer based idea to accomplish

40:39.680 --> 40:46.080
that. And so how is inverse rendering previously done when you're not using transformers?

40:46.080 --> 40:52.640
Previously, but recently also, I shall say, because deep learning based solutions for inverse

40:52.640 --> 41:02.240
rendering are not that old either, maybe at most, a couple of years, but people did, okay,

41:02.240 --> 41:09.760
here is the input image, and I know 3D model, because I generated that input image, and now I'm going

41:09.760 --> 41:16.480
to switch the order, I will give input image and try to estimate the 3D shape, and I know also

41:16.480 --> 41:21.440
the surface normals, and I know the color, you know, I know the color of the objects, I know the

41:21.440 --> 41:26.320
locations of the objects, I know the reflectivity of the objects, I know the lighting location.

41:26.320 --> 41:34.080
So, kind of this is done in a supervised manner, and separately. I think kind of the

41:34.080 --> 41:42.800
keyword here is these type of things done separately, and there has not been any attempt to

41:42.800 --> 41:51.920
learn for each test, and across multiple tests, where to focus when we are doing this type of

41:53.200 --> 41:59.600
inverse rendering. If there is an image, if I want to, let's say, generate the lighting location

41:59.600 --> 42:07.120
and lighting direction, which part of the scene image provides the right information that has

42:07.120 --> 42:15.360
not been done before. And so, this particular approach in using the transformer, again,

42:15.360 --> 42:21.760
you referenced this idea of the transformer's ability to attend to the right, and the most

42:21.760 --> 42:27.440
important parts of the image. That's a big part of what's making this work. Yes, transformers

42:27.440 --> 42:35.280
does that, in this case, we incorporated such self-attention or cross-attention mechanisms into

42:36.320 --> 42:44.320
our work, into inverse rendering to improve the accuracy of each of these inverse rendering

42:44.320 --> 42:50.800
tests, and also lighting estimation. So, when we have that, when we decompose an image into

42:50.800 --> 42:57.440
this type of components that we factorize it, then we can, for instance, put anything in the scene,

42:57.440 --> 43:02.400
you know, the lighting location, you know, how it reflects from other objects in the scene,

43:03.920 --> 43:10.240
and it would look more realistic, more natural, much more natural. So,

43:11.840 --> 43:18.560
and for instance, one application will be here is an input image, and then we put a completely

43:18.560 --> 43:24.320
virtual objects in a way that, you know, all the shadows are correct. It's very difficult to,

43:24.320 --> 43:31.920
you know, distinguish what we inserted, edited in the image than any other things already exist in

43:31.920 --> 43:37.920
the image. Yeah, I'm so excited about that work. That's what I forgot, but I should have said it.

43:37.920 --> 43:45.920
For instance, we have an image of a real scene, a house, let's say, and walls have a specific color

43:45.920 --> 43:52.720
set. Now, I can change the wall color, or I can change, for instance, there's the catch here,

43:52.720 --> 44:00.240
I can make the leather catch to a cloth, you know, some different fabric. So, we can really modify

44:00.240 --> 44:06.400
the scenes in a very realistic manner. Right, in a way that preserves that realism without it

44:08.080 --> 44:15.440
falling apart. And so, what were the, I'm imagining challenges here, again, you know, with

44:15.440 --> 44:19.680
using transformers or the computational intensity of the approach as one of them?

44:21.200 --> 44:28.720
In this case, we didn't focus on a computational kind of intensity. Yes, it's computation very heavy,

44:28.720 --> 44:36.240
and we planned to kind of make it also very efficient. There's no question about that, but one of

44:36.240 --> 44:42.800
the challenges was lighting direction estimation is not a straightforward problem, because it's

44:42.800 --> 44:48.560
difficult to take an image, and before knowing about the location of the light, and also 3D

44:48.560 --> 44:54.480
scene structure deduce about the lighting direction. I mean, you can imagine this lighting,

44:54.480 --> 45:00.640
there's a window, there's a sun outside the window. By the way, this window is not visible in

45:00.640 --> 45:06.480
the image. It is like right in front of me, there is a window, and you don't see it, right?

45:06.480 --> 45:12.960
In this test, our goal is to find where that window is and where the sun is, you know, not this direction,

45:12.960 --> 45:20.000
but this direction. So, this is kind of what we want to do, if we want to really put an object

45:20.000 --> 45:25.680
in a way that, you know, shadows are correct, ambient lighting is correct. So, we are imagining

45:26.400 --> 45:33.680
estimating the invisible things in the image. If it's visible, it's much more easier. So,

45:33.680 --> 45:39.760
we are estimating the remaining part of the scene room, for instance, and that cannot be done,

45:39.760 --> 45:45.760
you know, using just an input image directly going to there. You need to go step by step,

45:45.760 --> 45:51.200
first get an idea about the room layout, 3D scene structure, you know, reflectivity,

45:51.200 --> 45:57.920
all type of cues, then leveraging on those, and with attention mechanism transformers,

45:57.920 --> 46:04.800
back to believe in the previous computer information, okay, there is the light now, invisible lights.

46:04.800 --> 46:12.640
What was the direction of the light? What was the properties of lights? So, that was the challenging part.

46:14.160 --> 46:23.120
What kind of constraints are you making on the image? For example, you have a good number of

46:23.120 --> 46:28.880
candles behind you. Are you limiting the number of light sources that you're assuming to be in the

46:28.880 --> 46:34.400
image? For example, it's a good question. We don't actually, that is quite restricted to

46:34.400 --> 46:41.840
number of light sources. Type of light sources are maybe shape of the light sources. There are some

46:41.840 --> 46:48.720
assumptions for windows, you know, there are maybe some additional assumptions. It is retained

46:48.720 --> 46:56.160
for indoor scenes. Maybe I should mention this is specifically due to the data set that we use,

46:56.160 --> 47:03.280
open rooms, data set, and scan it. It is for indoor scenes, but it could be any number of lights,

47:03.280 --> 47:11.040
you know, kind of, yeah. Before we wrap up, Qualcomm has a number of other activities at CVPR. Let's

47:11.040 --> 47:18.480
briefly have you share a little bit about those. One is a workshop on wireless AI perception.

47:18.480 --> 47:25.840
What's that one about? Absolutely. That is the first time a wireless AI perception, wireless

47:25.840 --> 47:36.080
itself is becoming a workshop at CVPR. CVPR is more visual data and, you know, there has been some

47:36.080 --> 47:43.200
other models as well, but not at a degree of a workshop. And if you look at the field, we see that

47:43.200 --> 47:52.160
people are using cameras in addition to together with, let's say, Wi-Fi or, you know, 5G or

47:52.160 --> 47:59.440
terrorist imaging. So, for instance, there is a Wi-Fi around me right now and there is a camera

47:59.440 --> 48:04.400
and together they accomplish more things than just using the camera or the Wi-Fi

48:04.400 --> 48:13.440
separately. In this workshop, we bring the leaders in the field, they will, they are going to give

48:13.440 --> 48:23.840
us several keynotes, maybe 7-6 keynotes and very exacting presentations, talks about tools

48:23.840 --> 48:33.120
available, data sets available for this type of research. So, we bring these leaders and create a

48:33.120 --> 48:41.200
platform so people can discuss further improvement ideas and share information, share their

48:42.480 --> 48:48.240
observations. So, that is going to accelerate more research in this area.

48:48.240 --> 48:53.680
When you mentioned Wi-Fi in that context, I know that Qualcomm has done some research around

48:55.280 --> 49:01.040
using Wi-Fi signals to determine presence in a room, that kind of thing. Is that the sense in

49:01.040 --> 49:06.320
what you're using it or more traditionally as a communication? That we showed we can do it.

49:06.320 --> 49:12.880
It is not only sensing the person in the room, but some we actually know when person moves,

49:12.880 --> 49:18.720
where the person is less than 10 centimeter accuracy, you know, depending on the number of

49:18.720 --> 49:24.640
like the Wi-Fi access points, it could be even better. So, we know kind of like, and we can

49:24.640 --> 49:31.200
track people in these Wi-Fi or 5G environments, it could be your phone, could be track, for instance,

49:33.040 --> 49:41.120
using these access points. But in addition to that, we can also estimate the body pose of the people,

49:42.240 --> 49:48.000
for instance, people know that they actually pull down to ground and they need help or not.

49:48.000 --> 49:53.520
You know, those are the things that we are trying to accomplish. If there's a camera in the system,

49:53.520 --> 50:01.760
it would make even stronger. In this workshop, we are not really presenting our work,

50:01.760 --> 50:10.480
but our goal was to accelerate further research innovation in this area and support

50:11.040 --> 50:20.000
everyone, academia and, you know, anyone interested in virus perception, and we will release

50:20.000 --> 50:27.680
some data says as well. Okay, great. There's also a omnidirectional workshop. What's that one about?

50:31.520 --> 50:38.800
So, the previous workshop initiated at Qualcomm and kind of several Qualcomm members are

50:38.800 --> 50:43.360
in the organizing committee. They are sharing the event. The only directional

50:43.360 --> 50:50.240
computer vision workshop is another one. We have the similar setting. It is the third time of

50:50.240 --> 50:58.880
the third workshop, and in this edition, there are many people from all around the different

50:58.880 --> 51:05.920
companies, autonomous vehicle companies, and academia joining this event, showing, presenting

51:05.920 --> 51:14.560
ideas for a setting where cameras may not be just like our phone cameras, but could be

51:14.560 --> 51:21.520
fisheye cameras or 360 cameras. The only directional kind of indicates in colors,

51:22.240 --> 51:27.200
cameras with wider, much wider field of view. Of course, those cameras have different

51:27.200 --> 51:35.520
geometry and different type of images. If you look at, for instance, maybe people who are

51:35.520 --> 51:44.560
holding a stick 360 images, those images look like not like the pictures we take in our phones,

51:44.560 --> 51:54.880
right? And because of that solutions, computer vision solutions kind of has to work in that setting

51:54.880 --> 52:01.280
rather than, okay, I'm going to take this funny image and create kind of like regular rectangular

52:01.280 --> 52:07.280
version of it. Let me do that, we lose information. So there's a reason why there are dedicated

52:07.280 --> 52:15.280
solutions for omnidirectional cameras, and this workshop combines such recent, latest state of

52:15.280 --> 52:25.280
the art research work and provides a platform for people to discuss and learn from each other,

52:25.280 --> 52:33.440
share their experiences. One big application is autonomous vehicles. As you know, autonomous

52:33.440 --> 52:39.920
vehicles have multiple cameras anywhere from, you know, four, five, six, seven, including the

52:39.920 --> 52:48.160
internal one, external one. So you get a 360 feeling, a perception around the vehicle, and

52:48.160 --> 52:57.520
how you are going to make sure all these information coming from these different cameras,

52:57.520 --> 53:02.560
some of them are like the fisheye cameras, and there are other sensors can be combined,

53:02.560 --> 53:08.160
in a way that, you know, all the process optimizes works better at a higher accuracy,

53:08.160 --> 53:13.520
you know, more robust manner. So these are the things I think people will be discussing,

53:13.520 --> 53:22.480
and these are the kind of some of the talks in the workshop as well. Awesome, awesome. And your

53:22.480 --> 53:30.320
team also usually is showcasing some number of demos at conferences like CVPR, do you have any

53:30.320 --> 53:36.320
demos this year? Yeah, usually we brought, we tried to bring mini demos, this time they are

53:36.320 --> 53:45.280
only bringing two. There are much more demos at CVPR from Qualcomm. I mean, I'm just talking about

53:45.840 --> 53:54.640
kind of my team, Qualcomm AI research, perception part. And one of the demos is called

53:54.640 --> 54:05.440
this auxilire adaptation for semantic segmentation. The other one is 4K image super resolution. Oh, wow.

54:06.560 --> 54:16.000
So folks who, whether you saw that at CVPR or not, there's a blog post that we'll link to in

54:16.000 --> 54:21.280
the show notes and you might be able to catch those demos there. Well, Fatih, it was great chatting

54:21.280 --> 54:28.240
with you and congrats on so many accepted papers at the conference and looking forward to

54:28.240 --> 54:33.920
catching up again soon. Thank you so much, Sam. Thank you so much for having me. It was a pleasure.

54:34.480 --> 54:39.360
I hope, you know, sometimes I state very high level, but there are many things in the papers,

54:39.360 --> 54:45.600
you know, kind of I'm sure people will love the details we provide in the paper. Please let me know

54:45.600 --> 54:51.920
if you have any questions also later. We'll definitely link to those papers in the show notes

54:51.920 --> 55:21.760
and encourage folks to reach out if they have any questions.

