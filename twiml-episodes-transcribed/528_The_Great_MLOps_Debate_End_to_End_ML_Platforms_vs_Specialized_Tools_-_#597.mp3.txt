All right. So without further ado, I would like to get started with our first session for today. This session is the great ML ops debate.
When talking to ML and AI teams about platform strategy, two of the big questions that always come up are one build versus by which we addressed in our panel.
Our panel on Tuesday as well as our keynote yesterday a bit as well as you know what are the best tools to use to create a platform.
And this end to end versus specialized question is a really interesting one I wrote about it quite a bit in the definitive guide to ML platforms ebook, which I talked about the wide versus deep paradigm.
I encourage you to check that out. I'm not going to go into the detail of that here, but rather I am going to cue things up for our panelists to debate it out.
With that, I would like to hand things over to my friend and friend of the show, Demetrios Brinkman, organizer of the ML ops community to get things to get things started.
If you can't tell from his avocado shirt, make sure that this is a fun session.
That is right Sam. Thanks for the intro, man. I tried to create a little background, make it a little more interesting while we were in the green room there.
So we've got the great debate, man.
I'm thankful that you asked me to come and do this again. I had a great time last year when we did it. And so now I'm looking forward to doing it again.
Awesome. Awesome. I'm going to turn it over to you. I guess I've got one more comment. And that is, let's see, I see a poll. Oh, the poll is already up. I'm going to let you take it away.
Excellent. So I'll explain the poll in just a minute, but we've got so much good stuff that's about to happen here. I hope you all are as excited as I am.
I want to go through a few of the logistical pieces before we jump into the full on session and get into the debate.
So let me explain what exactly it is that we are planning on doing.
I'm going to introduce everyone that is joining us in just a minute, but first, what I want to do is talk to you about this cool giveaways that we've got everywhere I go whenever I try and do some kind of
virtual hosting. I try and get people to give us stuff so that we can give it to you that are listening. And today is no different. We've got no gift here who is one of the participants.
And he wrote a book called practical ML ops. We've got a few of those copies to give away to people in the audience. So be active in the chat, just like comment on everything that's happening and all randomly be giving those away throughout the debate.
The other thing that I can give away are points. So anyone that is interested in getting more points, just be active in the chat. I'm pretty liberal with how I give them away.
Now the next piece that I want to talk about is the vote and the poll that we've got.
You can see on the tab, there's a poll that is up right now. And what I want to do is figure out what's the Delta who wins this debate by how much they can swing the vote back and forth.
So that means that you go and you vote in the poll right now. And at the end of this session, you're going to vote again. And we're going to see how much your mind has changed. So go into this. I just ask that you go into this open minded and you allow some of these incredible debaters to change your mind.
If they have some things that make sense. All right. So what are the logistics of how this debate is going to work? Well, first of all, we're going to have each side. We've got two sides. We've got four contestants or four debaters.
And each member, each debater is going to have three to four minutes to argue their point for or against and basically saying that there is one tool to rule them all or it's a best of breed solution.
Next up, after they talk their talk for their three minutes, we're going to go and have them do one minute rebuttals because we don't know what everybody's going to bring up and they get to respond to some of the points that the other team has brought up.
And then from there, I'll give some final thoughts. I'll try and wrap put everything that I've heard into a cohesive statement so that we can understand it. That's where I may need your help in the chat who wants some free books. And then we will ask you to vote again.
And that's where we'll see what the Delta is and who wins this debate. All right. So without further ado, I want to introduce who is with us today. And for the best of breed team, the ones that are saying it is the best of breed.
They're arguing against that one tool to rule them all. We've got Dan Jeffries, my man and Sean Mohanti. Let's let's talk about who these guys are real fast.
Dan Jeffries just told me today that he is in Berlin. So I'm going to go ahead and go out on a limb, say his intro.
He's a futurist. He's also the managing director and kind of the brainchild behind the AI infrastructure alliance. Awesome. If you have not seen that, definitely go check it out.
And now he's the CIO of stability AI, you know, you may have heard of a little thing called stable diffusion. And last but not least, the dude isn't no mad. Every time I talk to him, he's in a different place. It makes me quite jealous. Anyway.
Now we've got Shiamm who is the CEO and co-founder of watchful company that automates the process of creating label training data. Very useful stuff that we all know that we need.
He's got decades of experience leading data engineering teams at companies like Facebook. That is a company you've probably heard of. And what he did while he was there was he served as the lead for the stream processing team responsible for processing 100% of the ad metrics data for all of Facebook products.
If anyone has dove into the streaming ecosystem, you know what a feat that is right there. All right. So that is our best of breed team right there. Good luck fellas.
We'll see you in just a second. Next up, I want to introduce the one tool to rule them all team. Now on this team, we've got no gift.
And we've got Bindu ready. No gift. I mean, I, what's up, Bindu? I cannot say enough things about Mr. Noah. I have had the pleasure of meeting Noah in person in Portugal. He gave a talk at our MLOPS community local meetups.
He was incredible. He's also given various talks on our MLOPS community virtual meetups. And as I mentioned before, he wrote the book practical MLOPS. You can find all kinds of his material on pragmatic AI YouTube channel. He's got so much good material on there. I encourage anyone to go check it out.
He's teaching MLOPS courses. Maybe I think it might be the first MLOPS course I've ever heard of being taught at a university. So just good on you. No, I'm for doing that. And now Bindu.
He's the CEO and co-founder of Abacus AI. Before Abacus, she was the general manager for AI verticals at AWS. She created Amazon personalization and Amazon forecasting. And prior to that, she was the co-founder of post intelligence, a deep learning company for social media influencers, which was ultimately acquired by Uber.
And prior to that, she was at Google. So we've got quite the line up here. This is some top notch stuff. I'm excited to hear what you all got to say. I'm going to keep Noah on the screen because Noah, I'm going to ask you to argue your point first.
Are you ready for this man? Sure, let's go for it.
All right. I'm going to throw three minutes on the clock. And let's before we fully start, let's make sure everybody did the vote for the poll. Everybody do that. And let's see what we got. All right.
We can we might be able to throw up what it's will at the end will throw up the differences. All right. Noah, I'm giving it to you, man.
All right. So a little bit about my background, I worked in the barrier for startups, enterprise companies. And so I have experienced both in managing teams of people, managing companies, and then also doing the work myself.
And one thing I really realized was that the building bespoke solutions is or picking risky technology is actually not a good strategy.
If you're running a company or running a team as an individual, it actually feels great because you're you're actually getting more and more power. So that's the really the scope of what I'm going to discuss in these five key points.
The first thing I'll mention is that one of the more surprising things about the startup industry is that people are still entering them because they're extremely risky. And if you look at startups, it's I think that the latest stats I found were that one out of 50 startup founders employ anyone other than the founder.
After 10 years, and if you think about that from an expected value position, right, you take the probability times the outcome, most startup employees, their expected value is like 2000 versus regular company. It's maybe 250,000. So these are really risky companies.
And similarly, you can think of technology as a proxy for that. And so I think you should be very careful about taking a risk just because it sounds fun.
And the second is this concept of 90 and risk, which is a lack of any quantifiable knowledge about a possible occurrence. We don't know all of the details about what's going to happen with machine learning in the future.
For example, are people going to go to jail if they made bad recommendation engines or, you know, is there going to be systematic bias that destroy society.
So there's some real interesting problems here. And the more aligned you are with mainstream technology vendors, I think you're reducing your 90 and risk.
Third is hiring and maintenance are way, way easier if you take the biggest market share. Right. So if you look about train talent, you're going to get a bigger share of talent from the biggest companies. If you look at training material, they're going to have a wider variety of training materials going to be easier to get your staff on board.
You can get people in your company certified. And then finally, you can call someone up on the phone after your star engineer leaves and actually get enterprise support to actually help fill in that role.
So really continuity is a huge issue with a platform. And then the other one, as I mentioned before, is that if you're an individual, and I guess I am right now, you know, independent for the most part, you have a lot of power.
Or if you pick very exotic solutions, because if you leave your company is in big trouble on the flip side, if you're an employer and you want to have kind of uniformity and have consistent results, you want to pick things where there isn't a key employee that's going to have the hit by the bus problem where if they leave the company or get hit by a bus, all of a sudden your company fails.
The thing I'll mention is that we actually have a lot of data around what's a good platform strategy. In fact, the business author Jim Collins mentioned in good to great that technology accelerates existing advantages in all of his analysis of the best companies in the world, it doesn't create them.
So I think it's a mistake to think that you in a kind of single individual role are going to heroically solve everything by creating this, you know, best of breed scenario when really you should pick boring solutions and accelerate the existing technology advantage.
That's my that's my pitch.
All right, we've got somebody else that is going to argue the other side of this, let's see, Cheyenne, where you at man, where you at paging Cheyenne to the edge, you ready, you ready to rebuttal and give me a coach.
Yeah, three minutes on the clock, here we go.
All right, so I think that was a fair argument for kind of like end to end platforms. Let me, let me give like a counter argument.
First of all, kind of just to set the stage, best and breed products or best of breed products tend to sort of push technological innovation forward.
That's where a lot of technological innovation typically originates.
And then platforms tend to be slow innovating and often not fully encompassing. We have to recognize the fact that real world machine learning problems are super diverse.
And it's very likely that you'll run into a shape of problem that doesn't quite fit with an existing and then provider perfectly.
It might be able to do 75 to 80% of what you want it to do, but the remaining 20% you're kind of out of luck.
And that sucks. Now, on the other hand, with best of breed products, you're able to sort of like incrementally build up your internal systems.
And as a result, you're able to sort of de-risk the overall system as a whole.
So namely, you can start small like by maybe one to small products that can kind of fit together like Lego blocks.
And as your uses your use cases expand, you can also expand your technology stack and sort of like incrementally build it up.
Now, by combining several of these best of breed products, you're able to really like tailor design a system for your organization and for your needs.
Now, I know that a lot of the arguments against best of breed products are actually around like integration and support and sort of like as as as no one mentioned earlier that that bus factor.
But really, I think like in today's age, a lot of that has been de-risked. So especially with like the advent of containerization and like the widespread use of scheduler systems like Kubernetes.
A lot of that integration risk is now de-risked simply because in the past, you needed like in a data ops analog, you needed folks like a cloud era or a Hortonworks implement things like Spark and Hadoop where you because it was very cumbersome to manage.
And that sort of thing. But now with things like Kubernetes, it's like all right, just deploy a bunch of containers into the same environment and administrative with the same grammar that you use to administrative everything else.
So in that way, there's very little integration risk because generally speaking most companies now are moving towards kind of a containerized future.
So you can use the same sort of logical constructs and the same systems to administrative all of your platforms.
And finally, like one last point, I kind of touched on it just now is we have prior art for this. There's nothing to indicate that the data science world, machine learning world is going to trend towards an end to end like winner takes all solution simply because most other industries haven't done that either.
So if you take a look at most companies, you see installations for individual pieces like Salesforce, Dropbox, Slack, Zoom, JIRA, and so on.
There's nothing indicating that data science or machine learning as a whole is going to be any different. So that's that's sort of my overall take.
The history points to the fact that best of breed solutions sent to win because they fit best for the problems that you have.
Okay, I see you. I like it. So now we've got a little intermission because the results from the poll are in.
Let's see what we got. Can we bring those up onto the screen? Can we see what the results for the poll were? Is that is that possible here? Let me see IT IT.
Maybe not. We'll wait until the next one after I'm about to bring up Bindu then let me or maybe they're up and I can see them.
They're up and I just can't see them. So everyone, everyone can see them in the poll tab.
All right, so if you go to the poll tab, you should be able to see it. Oh my God. This, the results for this are very strong, very strong. We've got best of breed that is shown up with a huge lead, gigantic lead.
So Bindu, no pressure, but this is kind of all on you to bring us home.
All right. You got three minutes. Let's put it on the clock.
Hi, everyone. My name is Bindu Reddy. See your co-founder of advocacy, which is an end to end MLObs platform.
So I'd like to like reframe this slightly. Obviously, there's never going to be one tool which roots all parts of AI that's impossible to do.
But enter and MLObs platforms make sense in the enterprise AI cases for common enterprise AI use cases. Right.
If you are building an autonomous air driving company or a robotics company. Hell no, don't use an end to end MLObs platform.
I mean, go build your own platform because it makes sense in that context. But if you're an IP or a GM or a Macy's, then of course you should use an end to end MLObs platform.
Here's why without a data machine learning is useless for common enterprise use cases like say forecasting, personalization, you know, text labeling and so on.
So data and models are very intertwined with each other. And more importantly, if you build something and you don't know what you know what data sets went into that model.
Or you don't know what cost the drift, then you're not going to be able to do kind of model CI CD.
So for common simple enterprise use cases, it does actually make sense to have a tool which actually strings together all parts of the ML lifecycle.
So you can actually see what data you used, what features you used, how the model was built, how, why is something is drifting and how to fix it.
Imagine if you had a feature store, you know, from one vendor drift from another vendor and let's say there was a model drifting.
You wouldn't even know how to exactly fix it without a lot of life kind of communicating between the two vendors.
So far, you know, simple applications and to end helps you actually debug your models easily get to production easily and quickly go to, you know, multiple different models at scale.
Now, you know, shy and mentioned that there is no kind of like that, you know, prior after this, I actually strongly disagree.
Amazon, Google, Microsoft, all believe in entry and I'm a lot of platforms.
Siege maker has the biggest market today, 99% of all enterprise AI today is an entry and platform.
Again, we're not talking about, you know, some esoteric thing that and video might be where building or we're not talking about what crews this are doing, we're talking about standard enterprise AI and data science.
And it makes a lot of sense to kind of build this in a way where you can see the beginning and the end of a particular problem.
Classic problem would be churn.
If you don't really know what feature is going to your churn model or if that's sitting in a different vendor, it's going to be very difficult for you to interface between these different systems.
And have you been in an enterprise? Do you know how long it takes to actually get a best of breed or any tool going?
Now, imagine getting 10 tools for it.
Now, imagine getting those 10 tools to actually integrate with each other. Good luck with that.
The other case that Cheyenne made was in a sales force drop box are all individual platforms.
Exactly. And therefore, this is one, too. There are a few use cases for entry and MLObs to make sense.
And they are today in production working a large number of enterprises.
So the market is already saying that.
The second piece of this is, you know, what do companies take, for example, someone like Macy's or someone like Nike?
What is their end goal going to look like?
I strongly believe that the end goal is going to be a hybrid, where one is they're going to build models on their own based on their first party data.
It's going to be an MLObs like platform.
And by the way, the same MLObs like platform exists today in Google. It exists today in Netflix, it exists today in Amazon, Uber everywhere.
They all believe in enter-end platforms. They haven't thought best of breed.
Facebook, for example, just released something called looper and Google Uber obviously had Michelangelo for a long time.
So the end game is going to be an entry-end MLObs platform for the first party data, but also APIs.
I really like what OpenAI is doing.
I think into some extent stability will do that, too.
What Hugging Facebook is doing is great.
So it's going to be a mix of these.
We're going to use APIs.
And we're going to get results for certain tasks, simple tasks.
We're also going to have an entry-end MLObs platform for first party data.
That's how I see the future of enterprise there.
And finding you to the point of advanced applications, you see Figma.
Figma is a very good in-depth advanced application.
It has a lot of depth as well as breadth.
So I think there's a false dichotomy here of saying it should be either deep or wide.
I completely disagree. Figma actually got sold to Adobe for $20 billion.
It's one of the best apps.
It has both depth and width.
We're in 2023. We can get both done.
In fact, not only at Cloud Renders, I would like to think we, advocacy AI,
are the best of breed entry-end MLObs platform.
Thank you.
We've got one more full debate.
The master debater is coming up right now.
Dan, where are you at?
You laid it on thick, man.
You know, no, I got to deliver here.
I'm just, I'm just sad that you're more not going to see each other tonight.
I didn't realize that you were in Berlin.
And now, so I got a.
I'm leaving it 50-50.
I mean, let's see, you know, where they were in the 50-50.
Anyway, before you start, I want to mention that there are some people in the comments
that are doing an amazing job.
And I got some books to give away.
So Peter, no.
I think that's how you pronounce the name.
Forgive me.
If that is not how you pronounce it, you're getting a practical MLObs book.
And is that you are getting some practical MLObs books?
And I got some points to give away.
So Angel, Mario, you get some points.
We're going to be just given those points away.
I feel like Oprah right now.
Who else wants a car?
All right, Zachary, give Zachary some points.
I'm going to give one more book away by the end of it.
Let me know in the chat if you want a book.
And let us get back to the final chat.
And then we're going to have some rebuttals, a round of rebuttals.
So Dan, it's on you, man.
And we're going to bring it home here.
Look, so the answer to this is pretty simple.
You can't have an end-to-end platform because it doesn't actually exist.
It only exists in the mind of marketers and in hype cycles.
So that's the main problem.
For instance, you may desire it to exist.
You may want it to exist.
You may believe that it exists.
But there's a delta between reality and what we think about it.
And you're free to believe whatever you want.
You could believe that gravity doesn't exist.
But if you go up to 100 story window and jump out of it,
you'll find out that gravity has enough defeated record, right?
So I tend to look at things from a practical reality standpoint.
And the fact is when you think about the classic diffusion of innovation curve,
where you have the invention and the early adopters,
and the early majority and the late majority and the lag groups,
we're still very much in the early adopter phase.
And so many of these tools are immature.
And even the ones that are sort of developed,
which we've seen in the infrastructure alliance report,
where we surveyed over 300 different companies,
there's different levels of maturity.
When you think about all of the things that an ML ops platform has to do well
from adjusting the data, cleaning it,
data versioning, providence, lineage, experimentation,
training, distributed training, you know, deployment,
inference, monitoring, management, it's just impossible for any product
at this stage, despite the marketing hype,
to actually be fantastic at all of those things.
Now it is feasible if you have a very limited use case.
You know, if all you're doing is churn prediction and basic analytics,
then you're probably fine with a single solution.
If you've got a single data scientist with a laptop,
trying to figure out your first ML problem, yeah, sure.
One solution's going to do it.
But if you've got hundreds or thousands,
and they're all working together, it simply doesn't exist.
There's no platform on the planet that can do tremendous distributed training
like we do at the stability or open AI.
There's that can also do deployment and inferencing at scale
that has tremendous monitoring and management capabilities,
plus a beautiful experimentation engine.
It just doesn't exist.
And if you look at something like some of the tools that we talked about,
you know, earlier in my illustrious panel stars,
think about something like SageMaker.
You know, when we looked at SageMaker,
the AI infrastructure alliance, you know,
90% of its use cases are structured data.
So, you know, if you're using gigantic video and imagery and audio,
you're mostly out of luck.
And so that's the problem.
If you actually have a diverse set of use cases,
it's simply just not going to fit in there.
So that's the real problem.
If you look at something like Figma,
it's one of the best tools of all time,
like we talked about earlier.
But the reason they bought it is because it's dramatically different
than Photoshop.
And Photoshop does a bunch of things that Figma simply can't do,
it doesn't try to do.
And so if you're trying to do Photoshop level things in Figma,
you're going to find yourself quickly out of luck.
So what you're going to find out,
as you get into any sort of scale,
is exactly what other companies have found,
that you're going to have a platform that you fate does everything.
And then when you start having a diverse set of use cases,
you're going to quickly find out that that falls apart
when faced with actual reality.
Woo!
All right.
Shots fired.
As Virginia said,
Virginia, you get a practical MLObs book.
And I know just the person to give a rebuttal for this.
No, where you at, man.
I want to hear what you got to say,
because that was hot and heavy.
Sure.
I mean, I think it's really the nature of the human mind.
The human mind gravitates towards low probability
but exciting events.
I'll give you two use cases.
So if you can get it to Harvard and then drop out in your first year,
you'll be a billionaire.
But the probability is, you know,
let's say, one hundred million to do that.
Likewise, the majority of people that play high school football
probably think they're going to go to NFL,
but there's a one and five thousand chance you'll even make the roster.
And so we're really thinking about things in an exciting way.
Likewise, anybody with real world experience in the software industry
knows that the bespoke solutions that get created,
what it means is if you actually can create a bespoke solution
for a corporation, most likely,
you're going to leave in a year to get hired by some big company.
Because that company isn't going to take your bespoke solution
and then turn it into a product and then sell it
because they probably do something else like rent cars out.
So I think that's really the biggest issue is it.
It's exciting to think about building your own solutions,
kind of packing it all together.
And we have the history of this, you know,
you know, conspiracy theories,
low probability events that are exciting,
moving to LA to be an accurate actress.
These are all super exciting things, but the reality is,
in the real world,
the boring solutions win a great example is Python.
Python is actually, in some sense, a horrible language, right?
It's super slow.
There's no real concurrency solution,
but it's the most popular language of the world
because things regress to the name
and the boring solutions win.
Likewise, Jira is a great example.
Most people hate Jira,
but companies gravitate towards the boring solutions that work.
Okay.
Yes.
Yes.
Or I'm seeing the chat is starting to light up.
I love this.
So for anybody that wants to add some points that you feel
are not being said,
we're going to go over those at the end.
All right.
So throw in there some of the different talking points
that you feel are not being talked about enough,
and we're going to put them in the end.
We're going to mention all of them.
But next up,
Cheyenne, where are you at?
Where are you at, man?
We're going to get again.
You got to be in the hair.
Oh, yeah.
One minute on the clock.
Let's do it.
Okay.
It's real quick.
I think Python is actually a really great example
for best and breed.
Let me explain.
Yeah.
Everyone tends to use Python.
It's not a great language.
Absolutely.
We regress to the mean totally.
But I'm sure most of the data scientists here
have like pip installed pandas,
pip installed numpy.
All that numerical processing is actually not written in Python.
It's actually written in C.
And you've got like a,
you know, an interface between the Python and the C
to actually do the number crunching.
So in that case, the best tool did win.
Python wasn't used for the things that people typically use
Python for these days.
It's actually C under the hood.
And I also think that it also makes a very good argument
for an integration layer.
My opinion personally and that of my company and I think
several others is that.
What we really want to see is tighter integrations
between lots of different best of breed tools.
And the question is how do we achieve that?
Well, we come up with a common interface for all of these things
to communicate.
Onyx did the same thing for, you know,
PyTorch, TensorFlow, and so on,
recognizing that some folks have just differing opinions about
which platform they'd like to use.
And that's okay.
As long as these things can work together
and have a common interface amongst them,
then it all works just fine.
So I think in terms of
choose the most boring technology,
I don't even think that exists in machine learning today.
It's just so new and NLOps is still a kind of a new concept
that that boring technology still doesn't exist.
What we have are end-to-end platforms that try to do a lot
but don't cover most of, you know,
kind of like the more interesting and more valuable use cases
that lots of companies have.
And you've got best and breed tools
that actually do need those integrations amongst themselves.
And they, frankly, they're doing an okay job at it right now.
But I think what we're going to see over time
we're going to have common interfaces amongst all these tools
emerge as we sort of build up more of a grammar around NLOps.
I like it.
Okay.
And the chat right now is going off.
I love this.
I love seeing everything that we see at James
just brought up an excellent point.
But I don't want to derail you too much.
Bindu, you're up next.
But James, incredible point in the chat.
What's the difference between an integration layer
and a flexible end to end offering?
Oh, right.
Bindu, you don't have to answer that question.
Happy to give us a minute.
Give us a minute on your model.
Here we go.
All right.
All right.
Here we go.
So actually, actually, we're mostly agreeing,
not disagreeing from what I can tell.
Let's start with saying that Dan basically said,
hey, if there is an entry and platform that exists and actually works,
that's great.
It's all marketing out.
There's none that exists.
I actually disagree.
For most common enterprise these cases,
whether it's churn, forecasting, personalization,
NLP, text summarization, vision segmentation,
object identification, which, by the way,
is almost 90 to 100% of these cases for common companies.
These are not AI first companies.
These are not open AI.
This is not stability.
This is someone like Macy's.
This is someone like FedEx.
This is someone like Procter and Gambo.
The entry and demolops does exist.
These use cases are a lot of them are structured data use cases.
Some of them are deep learning use cases.
For those kinds of people where it's not one data scientist,
they have more like, you know, 20 to 100 data scientists.
It exists today.
It works.
It's in production, right?
And I think more or less everybody agrees that for those use cases,
there are platforms that exist.
You know, those platforms are great.
And yes, I also agree with a child.
There's actually innovation required there.
And they should make it so that there is more and more innovation
in those entry and demolopsies cases as well.
And, you know, people, there should be more companies doing that
and that sort of thing.
And to the question, which is like, what's the difference between
an emolops platform and like, you know, actually that integration there?
I actually think there isn't.
Because a lot of enter and emolops platforms do take in open source APIs.
For example, we would be integrating interstability.
So, you know, I think we agree there.
I think in terms of like, you know, AI first companies,
in terms of like open AI, in terms of like stability.
Of course, you can't use an enter and demolops platform today.
Because, you know, they have diverse needs.
And you have to then go see what makes sense for you.
But, you know, fundamentally, the debate though is,
like, what happens for use cases, which are standard,
which are prevalent, not just standard, but there is a problem.
They're universal.
And for companies, which are kind of what I would say,
kind of like brick and mortar companies, right?
Who's actually like, you know, focus is not AI,
but AI is a big enabler.
Think, actually, Shopify.
Classic example.
Shopify is used by tons and tons of e-commerce companies.
And it's one platform.
It does everything.
It does all kinds of stuff from, you know, getting orders,
you know, fulfilling those orders,
making inventory decisions, doing fraud management.
And it's a $50 billion company.
Another example of an end-to-end
a platform, which works for that particular industry.
So, I think in some sense, in this debate,
we're actually converging and agreeing to like,
what makes sense for you based on who you are.
Strong points.
This is good.
Dan, you're going to bring us home, man.
It's all on you.
Last one.
One minute on the clock.
So, I think, look, the simplest thing to say here is that
it exists for simple use cases,
but even in those kinds of use cases,
you know, you're monitoring your management
or bringing in someone like Arise or Y-Labs or Truera,
or any of the kind of companies, for instance,
and the infrastructure alliance,
that monitoring and management is just simply going to be
infinitely more robust than whatever is baked into, you know,
a normal platform.
It's just as simple as that.
So, even for simple use cases,
I would argue that it doesn't really exist at this point.
I think I agree with Bindu that if it does exist,
we want it.
And eventually, an ecosystem does evolve to that.
In other words, you get to the point where you have a VM
where in the space, and then everybody just writes to the
API of that thing.
But we're just, we're in the diffusion of innovation curve
right now, where we're in the early stage.
And it doesn't exist.
We haven't figured out all the best ways to do distributed trading
or experimentation tracking or, you know,
maybe we've done deployment pretty well,
you know, versus some of the other things.
But, you know, it's scaled, inferencing,
all those kinds of things.
We just haven't really nailed it.
And there's still development that's happening.
So, how can we possibly have a finished system
at this point in time?
And certainly, if you have diverse use cases,
it's not enough to say, well, it kind of exists for the,
for the small folks, you know, who are just doing,
you know, one or two use cases,
what happens when they expand into another use case?
What happens is they buy another platform.
And then when you certainly get into more complex use cases,
it's simply, it just simply doesn't exist at this point in time.
And so, again, there's a delta between a desire
for something to exist.
And it actually exists.
And it will happen.
We'll get to the lamp stack of AI.
And then we'll move up the stack to, you know,
higher orders of abstraction.
And just like, you know, people moved up the stack
and went to WordPress.
And then moved up the stack and had Divi
and a drag and drop editor.
So any idiot like me can, who has very limited design skills
can make something.
And we're just not even at that WordPress level of abstraction yet.
And where we've standardized on kind of the best ways to do things
because the best ways are still developing.
Beautiful. Beautiful.
That is it for the debaters.
We're going to bring them all on to the stage right now.
And we're going to get the questions
and all of these chat things that we've got in happening.
But maybe before we do, well, no, no, no.
We're going to throw the pull-up right now.
Again, while we're talking about the different chat questions
that are happening.
So I saw Noah.
Noah was being kind of funny in the chat.
Noah, what were you mentioning there?
Hell is hiring?
Well, I mean, I don't know how many people have managed companies.
I think a few of you have, you know,
the life cycle of a company that is on its success
and then it goes on its way down.
But hiring for multiple languages,
multiple technologies, it has killed organizations.
I've worked at for sure.
And I would say the boring solutions are the ones
that actually you can hire for.
And you're also betting on the technology provider
increasing the future.
So even if they're not there at first,
look at Facebook.
And we have some people that know Facebook.
Facebook just, you know, got in, got in a good relationship
with the Obama administration and then bought all the competitors.
So they just, right, today just steal all the features of everybody,
everybody else.
So that strategy does work.
So, so I, I, that's, that's basically what I'm saying is that
the boring solution where the platform buys up all the competitors
is, is a pretty well known solution.
And that is echoing the sentiment of what Chuck was saying
about Jira versus Salesforce and how they're all just basically
giant collections of extensions and stealing one from another.
There's a few other great things in the chat that I'm seeing
right now.
We've got Williams saying one thing that should be considered,
often an ML team is building in the context of an existing
engineer stack.
In that case, it may be difficult to work with a third party
N10 platform if it's not flexible enough.
So there's another point like anybody want to talk about that?
Are we encounter this all the time?
I think that's a really good solid point.
You know, to the point of like there being existing infrastructure,
you, you want ideally a platform which can integrate at whatever
like part of the Lego break plan, right?
If you, you know, if you have something which does training
and deployment and you want somebody to do a feature store,
you want, ideally, once you want a platform which is easily
extensible, but is also modular.
And to downs point, I feel like, are we there there?
Probably not. This is an innovative field.
We'll get there sooner or later.
But, you know, you also want flexibility and customization.
So you can just switch off whatever you have.
Hmm.
So there's an uncle is giving something that I want to mention
in the chat and he's talking about how ML is supposed to provide
this differentiating factor with the business, right?
And if we're all using the same tool and it's some end to end
tool, doesn't that just make it so that we're all doing the same
thing at the end, kind of like when you're everyone in the
tour de France is doping.
It's just how good of a dope or are you?
I have my thoughts on this, but I don't think it is quite that
simple of a narrative.
Anybody want to jump in on that?
Do you mind if I jump in again on this one?
Yeah, go for it.
So that's the same as saying if I use Figma,
I don't have an advantage if I'm doing design.
Like the ML ops platform is just a tool.
All you're advantage is in your data.
And so if you, I mean fundamentally the data that you have
and extracting signal pattern from that data is what is the
differentiator, right?
That being said, of course, there are techniques.
And ML ops doesn't mean that you can't use techniques.
It doesn't mean that you can't do distributed training.
It doesn't mean that you can't use neural architecture.
A search of different types.
So I think using a tool is very different from like actually
innovating on that tool.
And so you can't just say, hey, just because I'm using the same
platform, me and you know, Macy's and Neiman Marcus are the same.
I want to quickly jump in.
I agree with the sentiment that just because like two people
use a hammer doesn't mean that their work is going to be
exactly the same.
However, I think I disagree with the sentiment that all
of the values and the data.
And this is coming from someone who's company like focuses on
the data side of this.
The data is like naturally very, very important.
But also is the treatment of that data.
And the way you're.
No, I agree.
Sorry.
I mean, the techniques are important to absolutely.
Yeah.
So I guess the point I'm trying to make is that if you're
bounded by like a solution, if you're bounded by a platform
that has a ceiling to the types of techniques that you can
apply to your data.
And that's like fundamentally problematic.
That's terrible.
Yeah.
So.
Cost is asking what are the boring off the shelf tools that
people like and are using.
Maybe there's a top three or top five list.
And it may be doesn't have to be in just ml who's got an answer
or Noah.
Well, I would just say the, I mean, it's just look at the data,
right?
The top vendors of cloud computing and whatever your company is
currently using, just pick what they have.
If it's AWS and you're already on AWS, pick SageMaker.
If you're on Azure, pick Databricks or pick Azure ML Studio.
If you're on Google, pick Vertex AI.
I mean,
that may not be the exciting answer that people want to know.
But that's going to be a pretty safe strategy that.
Yeah, it's funny that you mentioned that because I just
talked to the SageMaker team about stability where we have
a 4,000 node a 100 cluster.
And unfortunately, SageMaker won't work for what we're doing.
In other words, their thing is primarily structured data.
And it can't handle any of the image audio and video that
we're handling.
And in fact, we're even talking to the team on the back end
who is quite wonderful that has a whole series of unreleased
scripts and monitoring and management systems that they use
for supercomputing clusters.
Where they basically have kind of automations for when
the chips start to die down or need to be rebooted in those
kinds of things.
And then you look across the pond.
And there are other groups that have focused entirely on things
like that, like CoreWeaver.
We have a lot of our inferencing.
And they have all these kind of scripts sort of built in
because that's all they do is sort of GPU sort of super computers.
Right?
So again, it sort of.
It boils down to your right.
Like Vindu has said this earlier, like if you have a sort of.
I don't think there's,
it's not a matter of the,
like I disagree with the boring set of tools.
In other words, I'll take the boring so the tool any day.
Back in the day,
I take Vm wherever anything.
I'm not going to go spin up my own,
you know,
visualization cluster just because Zen is cooler.
Right?
But in this case,
I'm saying it doesn't exist for a lot of things.
It exists.
The boring use case exists.
Absolutely.
And, you know,
if you're got,
if you've got,
you know, again,
churn prediction and these kinds of sort of like,
you know,
analysis that everybody's doing,
I think,
you're going to go to date.
You're going to do these kinds of things.
And yeah,
it's going to work absolutely fine.
But I think as soon as you get into like,
you know,
cutting edge of an hour and those kinds of things are,
you're just kind of,
you're just out of luck.
Well, I agree with it.
So we're actually in agreement,
but you're talking about the 1% use case.
And I'm talking about the 99% use case.
It would be like saying,
hey,
look,
if you want to make $10 million and fly.
Private,
private planes each year,
and actually be an NFL quarterback,
there's only one thing you can do is try out for the NFL.
It's like,
you're right.
But the 5,000 other high school athletes
should probably study math,
or they should probably study data science.
So similarly,
the 99% of companies
should use the boring technology.
If it is the case,
if you're the one in the 100 companies
that actually is trying to compete with Google
or hugging face with
taking,
making pre-trained models.
And yeah,
don't,
don't use the platform.
But that's,
but that's one percent of the people.
But I don't know if it's one percent.
I think that's something.
But maybe.
Yeah.
But I don't know if it's one percent or I think that's okay.
But maybe.
Yeah.
I don't know if it's one percent.
But I also like,
let me actually,
you know,
counternova.
A little bit here.
The point is.
Friendly fire.
It's all the same team.
You can't counternova.
Well,
we still agree.
It's gone after all.
Oh,
that's not.
So basically,
the big tech.
I mean,
I see kind of big tech as a Walmart,
right?
And I agree with this thing
that they're actually boring.
And they'll actually even say to make her to be honest or vertex
aren't there there yet.
An example of this is some data warehouses.
The best data warehouse.
There is a snowflake.
And it's something somebody else.
So I think just because it's part of Google,
because of Amazon,
you shouldn't adopt it.
I am for entering an ML platform,
ML ops platforms.
I will.
It makes sense for it to be the best of breed,
enter and ML ops platform.
I don't know which one it is yet.
I think the jury is out there, right?
But I think just adopting the Google tool or the Amazon tool
doesn't make sense because Amazon and Google are just
praying and praying right now.
I mean,
it's every everywhere in every area.
There is some tool that they are developing.
And they have like 10 people on that tool.
And the tool is not great, right?
So I think that shouldn't be the reason to use it,
just because big tech is using it or because it's
foreign technology.
I think we should figure out what is.
If a doesn't exist,
b is it really good for my task for my problems and my models?
And then see,
okay, then let's go to it.
Yeah.
So really quickly,
I think that there's a lot of talk about the distribution
of types of problems in the AI space.
I think like everyone here,
both panelists and attendees,
probably want more proliferation and more penetration of AI
into interesting business use cases.
Even if you're a Macy,
even if you're a Macy's,
even if you're a GM,
even if you're, you know,
whatever insert big incumbent company here,
there exists some set of problems within your organization
that are extremely high value,
that you simply can't build AI for very easily
because you either lack the tooling
or you lack the data or some combination of the two.
Now,
I think everyone here wants those use cases to be solved for,
using AI.
I think that's sort of a given.
So I think really what we should be arguing for is just
deep innovation where we bring kind of like the floor up.
And I think that's really what we're talking about here,
especially like from the camp of Bess of Breed,
because that's sort of like the most linear path towards that.
You know,
if we can focus on these ultra high value use cases,
then it's possible for these companies to basically be priced in
to solving really their hardest problems using AI.
Okay.
So before Dan jumps in
because I know he wants to say something,
everybody just to let you know
we've got the second round of the polls.
So I want to see if there has been any change
in the way that you would vote.
Go answer the polls again.
It's the same question.
But let's have you changed your mind.
Did you get swayed one way or another?
And remember,
I told you,
come into this with an open mind.
Be willing to change your mind.
All right,
because it'd be no fun if everybody just held to their morals
and they said, ah, I don't believe it just because they are
aggressively holding on to it like a belief.
Dan,
what was this score for the best study?
I had a question.
What was the score for the initial poll?
The initial one.
It was 72% for end to end.
And then I think 28% was the way around.
Yeah.
Yeah.
All right.
I was close enough.
I was close enough.
Just like,
thank you.
Thanks.
Just kind of messing it up for everybody.
Look,
I'd like to thank all my worthy, you know, competitors.
When I really competitors,
as Noah said on any given day,
depending on our paycheck,
we could certainly switch to the other side of the debate,
like,
like,
good softness from ancient Greece.
I will say though that,
you know,
best of breed is something that I actually do sort of buy
into and believe in because, you know,
my study with.
I think that's what it does come back to the diffusion of
innovation curve and where we're at in, in the thing.
And eventually you do get to like two or three tools that
just everybody uses a big sense.
The lamp stack is there.
Why are you going to be in the wheel?
But I think actually the perfect example of that to do is,
is snowflake.
And that snowflake is like a very late stage product,
where,
where there were already two every generations that exist.
And the same thing that Kubernetes is like a third stage product.
Yeah.
Versus like two internal systems at Google belt.
So,
and in fact,
I think somebody tweeted the other day that snowflake had
one of the funniest,
like value props in the early days that they said,
they just came in and they're like,
it's,
it's like redshift,
but it works and you don't have to think about it.
And,
and really that was,
that was actually their pitch.
And people were like,
you don't have fancy slides and like a bunch of demos.
And they're like,
no, no, no, no,
we just make it so you never have to think about running out of space
or replication or whatever.
And it took redshift existing.
For snowflake to exist, right?
And in,
by the way,
redshift is still a pretty cool product and continues to improve.
But, you know, snowflake is on an order of magnitude.
It's just awesomeness,
but it's better and it's because it's later in the,
the development stage.
Yeah,
you get to learn from the mistakes people made, right?
Like,
if you look at the data robots and stage makers of the world,
the next generation,
we believe strongly.
I mean,
you're making my case personally,
selfishly for me,
but yes, absolutely agree.
So,
a Troy just said something incredible in the comments.
And I have to relay this to everyone
in case they're not looking at it.
The real debate here is
not quite there yet,
end-to-end platform,
versus box of parts
that don't work that well together.
That's,
that's 100%.
That's real.
That's reality right there.
That is,
I agree.
I agree.
And that's,
but that's the whole point
from a microeconomics standpoint, right?
Is, is, who is going to actually
be the person that's going to be successful?
It's probably the company with the most money.
Ooh.
That's,
that's so interesting.
It is interesting that it's going to end
us on that.
Man, I can't,
we can't end on that.
Actually, what
about the best meritocracy in the best possible?
No, I think,
what about the most positive innovation?
Let's go with what Chai has said.
Innovation.
That's right.
Yeah.
We can dream.
We can dream.
We can dream.
So let's end on the poll.
Let's see.
The results are in.
I think we can throw them up on the screen.
And what do we got here?
What do we have as far as the poll results?
Is it?
Oh.
Oh.
So end to end was swayed.
Now more people are going for end to end.
And I don't know if that's because more people just showed up.
And they voted for end to end or if they actually were swayed.
They do set out a male to all of her followers.
Yeah.
Back in the deck.
Yeah.
Maybe there was some shenanigans going on on the back end here.
I didn't cheat.
I promise.
No, I told all his students if they want extra credit,
get on here and vote for end to end.
Well, this has been awesome, y'all.
This has been super cool.
Hopefully everyone enjoyed it.
I gave away a few different books.
I think I gave away three of them.
But in case I didn't, I'll give away a fourth to, I mean,
I see Chuck here gave an awesome point up above.
So Chuck, where you at, Chuck, you get a book and also William.
You get some points when that was awesome.
Last person, David Palmer, you get some points.
I'll go on away from this with a good time.
Hopefully that was excellent.
It was as good as, no, it was as good for you as it was for me.
And we're going to finish on that note.
I think we've got, we've got Thomas from weights and biases coming up.
Sam, you're going to, you're going to bring in Thomas, right?
Yeah.
Yep.
All right.
There we go.
Awesome.
For you, Sam.
That was amazing.
Paul here, also in the chat, which was very lively.
So big thanks to our audience for engaging and keeping it lively.
And thank you, Demetrios and all of our debaters this morning for a very lively conversation.
Thanks for having us.
This is a lot of fun.
See you all later.
