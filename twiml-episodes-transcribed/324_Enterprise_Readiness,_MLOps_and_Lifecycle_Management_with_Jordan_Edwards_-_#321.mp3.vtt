WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:15.600
I'm your host Sam Charrington.

00:15.600 --> 00:23.200
Hey, what's up everyone?

00:23.200 --> 00:24.200
This is Sam.

00:24.200 --> 00:29.040
A quick reminder that we've got a bunch of newly formed or forming study groups, including

00:29.040 --> 00:34.800
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders

00:34.800 --> 00:36.760
part one courses.

00:36.760 --> 00:42.960
It's not too late to join us, which you can do by visiting TumelAI.com slash community.

00:42.960 --> 00:47.960
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.

00:47.960 --> 00:50.360
If you're at either event, please reach out.

00:50.360 --> 00:52.360
I'd love to connect.

00:52.360 --> 00:57.860
Alright, this week on the podcast I'm excited to share a series of shows recorded in

00:57.860 --> 01:01.580
Orlando during the Microsoft Ignite Conference.

01:01.580 --> 01:05.820
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship

01:05.820 --> 01:07.140
of this series.

01:07.140 --> 01:11.900
Thanks to decades of breakthrough research and technology, Microsoft is making AI real

01:11.900 --> 01:18.660
for businesses with Azure AI, a set of services that span vision, speech, language processing,

01:18.660 --> 01:21.740
custom machine learning, and more.

01:21.740 --> 01:26.180
Millions of developers and data scientists around the world are using Azure AI to build

01:26.180 --> 01:31.260
innovative applications and machine learning models for their organizations, including

01:31.260 --> 01:35.260
85% of the Fortune 100.

01:35.260 --> 01:41.260
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven

01:41.260 --> 01:47.420
enterprise grade capabilities and innovations, wide range of developer tools and services

01:47.420 --> 01:49.460
and trusted approach.

01:49.460 --> 01:54.500
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS

01:54.500 --> 02:00.260
and DevOps professionals across all skill levels to increase productivity, operationalize

02:00.260 --> 02:06.660
models at scale, and innovate faster and more responsibly with Azure machine learning.

02:06.660 --> 02:11.660
Learn more at aka.ms slash Azure ML.

02:11.660 --> 02:14.460
Alright, onto the show.

02:14.460 --> 02:18.100
Alright, everyone.

02:18.100 --> 02:23.420
I am here in sunny Orlando, Florida at Microsoft Ignite.

02:23.420 --> 02:26.940
I've got the pleasure of being seated across from Jordan Edwards.

02:26.940 --> 02:31.860
Jordan is a principal program manager for the Azure machine learning platform.

02:31.860 --> 02:34.180
Jordan, welcome to the Twinwall AI podcast.

02:34.180 --> 02:35.180
Oh, thanks.

02:35.180 --> 02:41.260
So, I'm really looking forward to talking with you about our subject for the day, MLOPS

02:41.260 --> 02:46.180
and related topics, but before we do that, I'd love to hear a little bit about your background.

02:46.180 --> 02:51.220
It sounds like you got started off at Microsoft where a bunch of folks that are now working

02:51.220 --> 02:54.900
on ML and AI, got started in the Bing group.

02:54.900 --> 02:55.900
Right.

02:55.900 --> 03:01.060
So, I started Microsoft a little over seven years ago, started off working on the big data

03:01.060 --> 03:07.700
platforms and related machine learning platforms, then ended up working on engineering systems

03:07.700 --> 03:13.860
for those platforms, then we decided to take those engineering systems and apply them to

03:13.860 --> 03:18.220
machine learning as well, hence the internal machine learning platform was born.

03:18.220 --> 03:22.380
As you mentioned, like a bunch of other folks who used to work on Bing, we all got moved

03:22.380 --> 03:26.020
into, hey, let's take the cool stuff we built for Bing's internal engineering platform

03:26.020 --> 03:29.300
and bring it to external customers on Azure.

03:29.300 --> 03:33.540
And so, I've been in the Azure machine learning team a little bit over a year now.

03:33.540 --> 03:34.540
Nice.

03:34.540 --> 03:35.540
Nice.

03:35.540 --> 03:36.540
And your role here on the team?

03:36.540 --> 03:37.540
Yes.

03:37.540 --> 03:42.460
I'm the product area lead for what we call MLOPS, which is really all about how do you bring

03:42.460 --> 03:44.620
your machine learning workflows to production?

03:44.620 --> 03:49.460
A topic that we've spent a lot of time talking about here on the podcast, as well as our

03:49.460 --> 03:55.220
recent Twomelcon AI Platforms event, you know, maybe starting, kind of directly connecting

03:55.220 --> 04:03.420
to your background, I'm curious the transition from, you know, a team that largely came

04:03.420 --> 04:10.340
out of this internal product or project Bing and is now trying to generalize those, you

04:10.340 --> 04:18.380
know, systems, but broader knowledge and learnings to the market, kind of, what are the commonalities

04:18.380 --> 04:22.620
and differences, I guess, that you encounter in trying to do that?

04:22.620 --> 04:27.300
So there's actually a lot of commonalities when you double click on it, but the biggest

04:27.300 --> 04:33.180
thing is, you know, Bing and Office 365 internal Microsoft teams have been doing AI and ML

04:33.180 --> 04:34.980
for a long time.

04:34.980 --> 04:39.340
And so they built up a lot of habits and tools and technologies, but also a lot of things

04:39.340 --> 04:44.300
that don't necessarily map to how we see enterprises getting started, right?

04:44.300 --> 04:48.100
So there's the, you know, most of our external customers today are coming in wanting to

04:48.100 --> 04:51.940
do Python-based development, and we have some of that internally, but we also have, you

04:51.940 --> 04:55.780
know, languages that predate the popularity of Python as a data science platform.

04:55.780 --> 05:01.580
So we have, you know, engineers doing machine learning work in.NET and C++, and so those

05:01.580 --> 05:03.100
workflows are a bit different.

05:03.100 --> 05:07.700
Also a lot of the machine learning platforms at Microsoft, as you would imagine, were previously

05:07.700 --> 05:12.940
Windows-based, whereas the new customers coming in wanted to do things using Linux and containers

05:12.940 --> 05:16.380
and their newer techniques that are being applied as well.

05:16.380 --> 05:21.700
So there's similarities in there, the ways they want to solve the problem, but just different

05:21.700 --> 05:27.820
tools that they're using, and also just different amounts of context that it builds up.

05:27.820 --> 05:30.140
There's also the matter of scale.

05:30.140 --> 05:34.820
So when you look at teams like Bing, they've got, you know, a thousand data scientists

05:34.820 --> 05:38.580
that are collaborating together to train these huge models.

05:38.580 --> 05:43.260
Most of the enterprise customers that we're talking to, they have small teams scattered

05:43.260 --> 05:48.060
all over the place, or they're trying to staff a team, or they have a team, and they're

05:48.060 --> 05:51.980
not sure how to make best use of their time.

05:51.980 --> 05:57.140
Also the most common problem that we're seeing that they come to us with is, hey, we have

05:57.140 --> 06:01.540
all these data scientists who are doing work in Jupyter notebooks, whatever, the work

06:01.540 --> 06:04.180
is happening on their local machines.

06:04.180 --> 06:10.660
We have no idea where the code is, if the code's even checked in, and they're doing all

06:10.660 --> 06:15.100
this work, but we can't leverage any of it on the business side.

06:15.100 --> 06:20.100
There's so many, so many problems in that problem statement, right?

06:20.100 --> 06:24.460
There's kind of a reproducibility problem.

06:24.460 --> 06:29.460
There's a business value path to production problem.

06:29.460 --> 06:32.900
There's kind of an accountability problem.

06:32.900 --> 06:33.900
Can you unpack that?

06:33.900 --> 06:37.260
How do you do prioritize those?

06:37.260 --> 06:40.620
We try to put it in terms of a process maturity model.

06:40.620 --> 06:42.340
It's exactly how you framed it.

06:42.340 --> 06:46.780
There's the reproducibility of the work.

06:46.780 --> 06:51.820
Another data scientist and the team could reproduce the same work that one person did, and

06:51.820 --> 06:55.700
then an automated system could also reproduce that work, which means you need clean modeling

06:55.700 --> 07:00.100
around the code and data and configuration that you're using in your model development

07:00.100 --> 07:01.100
process.

07:01.100 --> 07:06.780
Then there's the, how do you transition this model, this thing you've created, to production,

07:06.780 --> 07:11.300
so how do you package it, how do you certify it, and how do you roll it out in a controlled

07:11.300 --> 07:16.540
fashion, and then at the end, how do you determine the business value of your model?

07:16.540 --> 07:19.020
Is it making your business more effective?

07:19.020 --> 07:22.940
From a cost point of view, is it worth the amount of compute hours you're spending and the

07:22.940 --> 07:26.340
amount of man hours you're spending training these models?

07:26.340 --> 07:31.020
Then on the absolute end of the process maturity model is, okay, I've got this

07:31.020 --> 07:32.020
model.

07:32.020 --> 07:33.020
It's reproducible.

07:33.020 --> 07:34.020
I've got it deployed out.

07:34.020 --> 07:36.780
I'm using it for a production scenario.

07:36.780 --> 07:41.420
How do I know when I might need to retrain it, so completing the circle, and that's always

07:41.420 --> 07:46.300
the question that customers will come and start with is, how do we do automated retrain?

07:46.300 --> 07:48.700
It's like, let's walk back and begin it.

07:48.700 --> 07:52.980
How do you reproduce these models in the first place?

07:52.980 --> 07:56.740
That strikes me as a mature customer that's asking about automated retraining, right?

07:56.740 --> 07:57.740
Correct.

07:57.740 --> 08:00.580
Those people are trying to get the model into production in the first place, or many.

08:00.580 --> 08:04.220
They see the marketing hype where they read all the things like, oh, look at this company

08:04.220 --> 08:08.820
doing cool automated retraining stuff, and realistically, it takes a long time to get

08:08.820 --> 08:13.260
to that degree of maturity where you can trust that you have the high-quality data coming

08:13.260 --> 08:17.820
into your production systems to be able to analyze and compare and figure out I do need

08:17.820 --> 08:19.020
to retrain.

08:19.020 --> 08:25.860
Even in the case of being your office ML development teams, there's never a fully automated

08:25.860 --> 08:31.300
retraining loop. It's always there's a scorecard that gets generated in humans go and do

08:31.300 --> 08:36.300
some sort of review process prior to your new larger models going out, especially when

08:36.300 --> 08:39.740
they deal with things like how do you monetize ads, for instance?

08:39.740 --> 08:44.220
There's a lot there to dig into, but before we do that, one of the questions that I had

08:44.220 --> 08:47.740
for you is you've got ML ops in your title.

08:47.740 --> 08:51.340
What does that mean to you?

08:51.340 --> 08:56.020
That means to me that it's all about how do you take the work that data scientists are

08:56.020 --> 09:02.980
doing and make their lives easier, but also make it easier for others, other personas

09:02.980 --> 09:06.500
to come into the fold and take advantage of data science.

09:06.500 --> 09:11.420
The three personas I like to talk about is you have your data engineer who's got this

09:11.420 --> 09:15.700
giant lake of data. They want to figure out what value they can derive from it.

09:15.700 --> 09:20.260
You've got your data scientist who's tasked with finding interesting features in that data

09:20.260 --> 09:22.140
and training models on top.

09:22.140 --> 09:27.820
You've got this new emerging persona called the ML engineer whose responsibility it is

09:27.820 --> 09:32.660
to take the work the data scientist is doing and bring it to production.

09:32.660 --> 09:39.100
My job is to help the ML engineer be able to be successful and help the ML engineer be

09:39.100 --> 09:44.740
able to interact well with the data engineering and data science personas that are required

09:44.740 --> 09:47.260
to complete that circle.

09:47.260 --> 09:52.980
Of course, you also have the hub in the center of it, your IT ops persona, who's giving

09:52.980 --> 09:58.060
them all of the raw compute and storage resources to get started, making sure everybody plays

09:58.060 --> 10:02.540
nicely together and actually connects things and to end.

10:02.540 --> 10:13.140
There's an obvious echo to DevOps, to what extent is it inspirational, is it directly

10:13.140 --> 10:18.180
applicable, or is it counter applicable, meaning just don't try to do exactly what you're

10:18.180 --> 10:20.180
doing in DevOps because that's the...

10:20.180 --> 10:25.380
I think it's sort of all three of the things that you mentioned shocking, right, to me up.

10:25.380 --> 10:29.580
As far as how it's inspirational, definitely the practices that have been developed in

10:29.580 --> 10:35.260
the DevOps field over the past 20 years or so are useful.

10:35.260 --> 10:40.980
However, data scientists are not software engineers and they're not even engineers.

10:40.980 --> 10:45.780
A lot of them are scientists, so talking to them they need to care about things related

10:45.780 --> 10:53.020
to the infrastructure and package version management and dealing with all of the intricacies

10:53.020 --> 10:57.820
of how to run a production infrastructure, that's just not something that they're interested

10:57.820 --> 10:58.820
in at all.

10:58.820 --> 11:04.100
Trying to force these habits onto them, we've seen this even trying to get them to write

11:04.100 --> 11:05.820
tests for their code.

11:05.820 --> 11:10.300
It takes a lot of education on the net value add they're going to get from it before

11:10.300 --> 11:14.460
they're willing to onboard, so definitely inspirational from a process point of view.

11:14.460 --> 11:20.100
A lot of the same tools are applicable, but then you also need new tools that are domain-specific

11:20.100 --> 11:21.100
too.

11:21.100 --> 11:26.540
How do you do data versioning, how do you do model versioning, how do you validate and run

11:26.540 --> 11:34.300
integration testing on models, how do you release and do AB comparison on a model as

11:34.300 --> 11:39.820
opposed to a normal software application, and know if it's better or not, so yeah.

11:39.820 --> 11:45.060
You know, applicable and you'll get hit in the face by a data scientist if you tell

11:45.060 --> 11:48.860
them to go and implement all these things themselves.

11:48.860 --> 11:52.220
One of the things you mentioned earlier was testing.

11:52.220 --> 12:01.260
What's the role of testing in an MLOPS process and what kind of experiences if you had

12:01.260 --> 12:07.260
working with real customers to implement testing procedures that make sense for you?

12:07.260 --> 12:15.340
The place we try to start is by integrating some sort of tests on the data itself, so ensuring

12:15.340 --> 12:21.020
that your data is of the same schema, you have high quality data, like a columnized feature

12:21.020 --> 12:25.700
hasn't just been dropped or the distribution of values in that feature haven't changed

12:25.700 --> 12:26.700
dramatically.

12:26.700 --> 12:30.900
A lot of the stuff that we've built into the machine learning platform, especially

12:30.900 --> 12:36.620
on the data set profiling side, are designed to help you with that, to help you with skewed

12:36.620 --> 12:43.420
testing and analyzing, is your data too different to the point where you shouldn't need to be

12:43.420 --> 12:48.060
training it, or is it your data too similar, or is it in that sweet spot where the same

12:48.060 --> 12:51.820
training pipeline is actually applicable to go and solve the problem.

12:51.820 --> 12:58.380
That's on the profiling side, and then we also have some advanced capabilities on the

12:58.380 --> 13:04.940
drift side, so analyzing the over time, how are the inputs or features into your model

13:04.940 --> 13:10.300
changing, whether that's training versus scoring, or day every day we cover week and month

13:10.300 --> 13:15.300
every month of the data coming into your model, when it's making predictions, does that

13:15.300 --> 13:19.940
data, has the shape of that data changed over time, do you still trust the model based

13:19.940 --> 13:24.940
on the input values, and then of course you have the other end of it too, which is looking

13:24.940 --> 13:30.660
at the predictions, the model is making, whether it's from a business application, so say

13:30.660 --> 13:36.580
I'm using the Outlook app on my phone, and I've got the smart reply model running there.

13:36.580 --> 13:43.260
Now either they didn't click on any of my suggestions, they clicked on a different suggestion

13:43.260 --> 13:48.580
from the one I did, they clicked on the top suggestion that I had, or they said I didn't

13:48.580 --> 13:54.500
like any of these suggestions, all those types of feedback come into telling you, is the

13:54.500 --> 14:00.460
quality of data that you've trained your model on, giving you a useful model on the prediction

14:00.460 --> 14:01.460
side.

14:01.460 --> 14:08.860
So, yeah, skew testing, validating your data's quality, correctness, consistency between

14:08.860 --> 14:11.260
training and inference, it's all those things.

14:11.260 --> 14:12.260
Okay, okay.

14:12.260 --> 14:13.260
Yeah.

14:13.260 --> 14:15.580
So I'm kind of pulling your threads here, I mean, you're taking a step back, you talked

14:15.580 --> 14:21.940
a little bit about a maturity model that you, when you look at customers, they kind of

14:21.940 --> 14:29.220
fall in these end buckets, is there a prerequisite for, you know, starting to think about

14:29.220 --> 14:30.900
ML ops?

14:30.900 --> 14:38.340
So I think the prerequisite is you have to have a desire to apply a model to a business

14:38.340 --> 14:39.340
need.

14:39.340 --> 14:43.380
If your only goal is to write a model to say, you know, publish a paper, like, hey, I have

14:43.380 --> 14:47.460
this model to solve this school problem, then you don't really need any of the ML ops

14:47.460 --> 14:48.460
stuff.

14:48.460 --> 14:51.140
And if you're still, you know, if you're just mucking around in a Jupyter notebook,

14:51.140 --> 14:56.220
trying some different things by yourself, it's also a stretch to say, like, oh, you need

14:56.220 --> 15:00.940
these ML ops practices now, but the second you go beyond, you know, I'm keeping all my

15:00.940 --> 15:04.900
notes in Jupyter or I'm dumping them into one note somewhere and just keeping track of

15:04.900 --> 15:10.380
all my experiments on my own, the second you want collaboration or reproducibility or

15:10.380 --> 15:15.740
the ability to scale up and scale out to run your jobs in the cloud, that's where ML

15:15.740 --> 15:17.660
ops starts coming into play.

15:17.660 --> 15:23.980
I agree that kind of collaboration is a big driver, but even an individual researcher,

15:23.980 --> 15:28.700
you know, that's tracking hyperparameters and file names or on posted notes or some

15:28.700 --> 15:35.500
equipment worse can benefit from some elements of the tooling that we kind of refer to as

15:35.500 --> 15:36.500
ML ops.

15:36.500 --> 15:37.500
Would you agree with that?

15:37.500 --> 15:38.500
I would.

15:38.500 --> 15:39.500
Yeah.

15:39.500 --> 15:42.340
But just, you know, trying to sell them on using everything from the very beginning is

15:42.340 --> 15:43.340
the tougher sell.

15:43.340 --> 15:47.900
So we start by saying, you know, start by tracking your work, so it's the whole process

15:47.900 --> 15:48.900
maturity flows.

15:48.900 --> 15:53.900
You start with work tracking, then making sure everything's in a reproducible pipeline.

15:53.900 --> 15:57.060
And then making sure that others can go and take advantage of that pipeline.

15:57.060 --> 16:01.100
And then you actually have the model that you can go and use in other places.

16:01.100 --> 16:02.100
Yeah.

16:02.100 --> 16:06.940
Yeah, which, which I like the way you pull that together because in a lot of ways, one

16:06.940 --> 16:13.340
of the questions that I've been kind of noodling around for a while now is the, you know,

16:13.340 --> 16:18.900
where does ML ops, you know, start and end relative to kind of platforms and tooling and

16:18.900 --> 16:21.060
the things that enable and support ML ops?

16:21.060 --> 16:26.060
And it's, you know, very much like the conversation we were having around DevOps, like DevOps

16:26.060 --> 16:31.940
isn't, you know, containers and Kubernetes and things like that DevOps is a set of practices.

16:31.940 --> 16:37.420
And it's very much to your point, kind of a, you know, that end and process.

16:37.420 --> 16:43.180
So you might need, you know, any one of a number of the tools that someone might use to

16:43.180 --> 16:47.340
enable ML ops, but that doesn't necessarily mean that you need ML ops.

16:47.340 --> 16:48.340
Right.

16:48.340 --> 16:53.060
So when, sure, I work on Azure machine learning, when I'm talking to customers about how does

16:53.060 --> 16:58.340
ML ops actually work, you're going to have at least like three different tools and technologies

16:58.340 --> 16:59.340
being used, right?

16:59.340 --> 17:00.660
Because you have three different personas.

17:00.660 --> 17:04.580
You have data engineering, data science and DevOps, ML engineering, whatever it means,

17:04.580 --> 17:11.100
you're going to have some sort of data pipelining tool, something like data factory or, you know,

17:11.100 --> 17:15.060
airflow in the open source world, something to help with managing your training pipelines,

17:15.060 --> 17:19.220
whether it's, you know, Azure ML is a managed service or something like cube flow.

17:19.220 --> 17:22.940
If you're in the open source community and then same thing on the release management side,

17:22.940 --> 17:28.340
whether it's using Azure DevOps or get up actions or you're running your own like Jenkins

17:28.340 --> 17:32.020
server, either way, there's going to be at least those three different types of tools

17:32.020 --> 17:37.300
with different personas and they all need to work together and interoperate.

17:37.300 --> 17:43.340
So that's another key part of our pitch is like, make sure that you're being flexible

17:43.340 --> 17:49.980
in how you're producing and consuming events because ML ops is more than just model ops

17:49.980 --> 17:53.820
and you need to make sure it fits into your data and dev side of the house.

17:53.820 --> 17:54.820
Yeah.

17:54.820 --> 17:55.820
Yeah.

17:55.820 --> 17:56.820
Awesome.

17:56.820 --> 18:03.020
You mentioned Azure DevOps playing a role in here and Jenkins on the open source side.

18:03.020 --> 18:04.020
Yeah.

18:04.020 --> 18:08.300
These are tools that, you know, from the DevOps perspective, you associate with CICD,

18:08.300 --> 18:11.020
continuous integration and continuous delivery.

18:11.020 --> 18:14.740
The idea being that there's a parallel on the model deployment side, can you elaborate

18:14.740 --> 18:16.580
a little bit on how those tools are used?

18:16.580 --> 18:17.580
Yeah.

18:17.580 --> 18:22.820
So the way we like to look at it from a DevOps point of view is we want to treat a model

18:22.820 --> 18:28.500
as a packaged artifact that can be deployed and used in a variety of places.

18:28.500 --> 18:32.180
So your model is sure you have like, you know, your pickle file or whatever, but you also

18:32.180 --> 18:37.140
have the execution context for, you know, I can instantiate this model into a class and

18:37.140 --> 18:42.260
Python or I can embed it into like my Spark processing pipeline or I can deploy it as an

18:42.260 --> 18:46.060
API and a container onto, you know, Kubernetes cluster, something like that.

18:46.060 --> 18:50.140
So it's all about how do you bring the model artifact in as another thing that can be

18:50.140 --> 18:52.980
used in your release management process flow?

18:52.980 --> 18:54.980
It does not have to be a pickle file.

18:54.980 --> 18:55.980
It could be anything.

18:55.980 --> 18:56.980
It could be anything.

18:56.980 --> 18:57.980
Exactly.

18:57.980 --> 18:58.980
Yeah.

18:58.980 --> 18:59.980
Yeah.

18:59.980 --> 19:01.940
It's, this is my, you know, serialized graph representation.

19:01.940 --> 19:06.140
Here's my code file, my config that I'm feeding in.

19:06.140 --> 19:09.420
So it's a model is just like any other type of application.

19:09.420 --> 19:12.860
It just happens to come from some, or have some sort of association to like a machine

19:12.860 --> 19:18.540
learning framework or to have come from some data, which is actually, you know, another

19:18.540 --> 19:23.620
important part of the, the ML ops story is what's the, what's the end to end lineage look

19:23.620 --> 19:24.620
like, right?

19:24.620 --> 19:29.380
So ideally, you should be able to go from, I have this application that's using this model.

19:29.380 --> 19:34.140
Here's the code and config that was used to train it and here is the data set that this

19:34.140 --> 19:39.860
model came from, especially when we're talking to customers in more of the highly regulated

19:39.860 --> 19:40.860
industries.

19:40.860 --> 19:45.940
So, you know, healthcare, financial services, say you have a model deployed that's determining

19:45.940 --> 19:49.340
if it's going to approve or reject somebody for a loan.

19:49.340 --> 19:54.980
You need to be very careful that you've maintained your full audit trail of exactly, you know,

19:54.980 --> 19:59.700
where that model came from in case somebody decides to come in and ask further.

19:59.700 --> 20:04.660
This also becomes more complicated than more of a black box that your model is, but in

20:04.660 --> 20:10.100
general, the goal of having all these different technologies work together and interoperate

20:10.100 --> 20:16.620
is so that you can track sort of your correlation, ID or correlation vector across your entire

20:16.620 --> 20:21.540
data and software and modeling landscape.

20:21.540 --> 20:30.780
Can we talk about that kind of end-to-end lineage, is that a feature like you use tool X,

20:30.780 --> 20:36.340
use Azure ML and create a button and you have that or is it more than that and a set

20:36.340 --> 20:40.980
of, you know, disciplines that you have to follow as you're developing the model?

20:40.980 --> 20:45.780
So, yeah, it's kind of the ladder leads to an element of the former.

20:45.780 --> 20:46.780
Okay.

20:46.780 --> 20:51.100
So, assuming that you use the, I think you're the all of the above guy.

20:51.100 --> 20:52.100
Yeah.

20:52.100 --> 20:55.780
You're seeing it up right now.

20:55.780 --> 21:00.460
So yeah, when it comes to using the tools the right way is like, sure, you could just,

21:00.460 --> 21:05.420
you know, have a random CSV file that you're running locally to train a model on, but if

21:05.420 --> 21:10.940
you want to assert you have proper lineage of your end-to-end ML workflow, like that CSV

21:10.940 --> 21:17.260
file should be uploaded into blob storage and locked down and accessed from there to guarantee

21:17.260 --> 21:21.740
that you can come back, you know, a year later and reproduce where this model came from.

21:21.740 --> 21:25.660
Same thing on the code and packaging and the base container images that you're using

21:25.660 --> 21:27.380
when you're training the model.

21:27.380 --> 21:31.780
All that collateral needs to get, needs to be kept around.

21:31.780 --> 21:36.180
And what it's that allow you to do is, you know, we have the inside of the machine learning

21:36.180 --> 21:42.700
service, internal metastore that keeps track of all the different entities and the edges

21:42.700 --> 21:44.780
that connect them together.

21:44.780 --> 21:50.100
And right now we have sort of a one hop exposure of that, but one of the things we're working

21:50.100 --> 21:53.900
on is more of a comprehensive way to to peruse the graph.

21:53.900 --> 21:59.020
So it's like, hey, across my enterprise, show me every single model that's been trained

21:59.020 --> 22:05.260
using this data set, not scope to, you know, a single, a single Azure or a single project

22:05.260 --> 22:06.260
that my team is doing.

22:06.260 --> 22:11.420
But across the entire canvas, show me everybody using this data set, what types of features

22:11.420 --> 22:15.820
are they extracting from it, is somebody doing work that's similar to mine?

22:15.820 --> 22:19.020
Can I just fork their training pipeline and build on top of it?

22:19.020 --> 22:24.940
And you know, going back to how has this work we've done for internal teams inspired the

22:24.940 --> 22:30.380
work we're doing on Azure, that's probably the most powerful part of our platform for internal

22:30.380 --> 22:34.740
Microsoft teams is the discovery, the collaboration, the sharing.

22:34.740 --> 22:40.340
That's what allows you to do ML at high scale, at high velocity.

22:40.340 --> 22:44.020
So we want to make sure as much as we can that the tools and technologies that we have

22:44.020 --> 22:48.860
on Azure provide that same capability with all of the enterprise ready features that you

22:48.860 --> 22:54.780
would come to expect from Microsoft and Azure.

22:54.780 --> 23:01.260
So in that scenario, you outlined the starting place is a data set that's uploaded to blob

23:01.260 --> 23:02.980
storage.

23:02.980 --> 23:09.020
Even with that starting place, you've kind of disconnected your ability to do lineage

23:09.020 --> 23:13.900
from the kind of the source data set, which may be in a data warehouse or something like

23:13.900 --> 23:14.900
that.

23:14.900 --> 23:15.900
Yeah.

23:15.900 --> 23:21.100
Is there also the ability to kind of point back to those original sources?

23:21.100 --> 23:22.100
Oh, yes.

23:22.100 --> 23:25.860
So you know, sometimes you'll have a CSV there, but you can also connect to a SQL database

23:25.860 --> 23:30.740
or to your raw data lake and have a tracking of, okay, this is the raw data, here's say

23:30.740 --> 23:36.140
the data factory job that did all these transformations, here's my curated data set, here's all the

23:36.140 --> 23:40.860
derivations of that data set, here's the one I ended up using for training this model.

23:40.860 --> 23:45.900
Here's, I took this model and you know, transfer learned on top of it to produce this new model

23:45.900 --> 23:50.340
and then I deployed this model as this API and you can trace things all the way back to

23:50.340 --> 23:55.660
there and then going the other way, you know, when this model is now running, I can be collecting

23:55.660 --> 23:59.420
the inputs coming into my model and the predictions my model is making.

23:59.420 --> 24:05.420
I log those into Azure monitor and then my data engineer can set up a simple job to take

24:05.420 --> 24:10.380
that data coming in and put it back into the lake or put it back into a curated data set

24:10.380 --> 24:15.100
that my data scientist can now go and experiment on and say, well, you know, how is my, how's

24:15.100 --> 24:20.940
it coming into my model that's deployed compared to the one I trained it, that's completing

24:20.940 --> 24:22.580
the circle back to the beginning, yeah.

24:22.580 --> 24:24.820
Nice, nice.

24:24.820 --> 24:31.820
Which conceivably, you could, as opposed to talking about a data set, which, you know,

24:31.820 --> 24:37.060
this data set has produced what models you could point to a particular, you know, row

24:37.060 --> 24:41.220
in a data warehouse or something like that or a value and say, you know, what's been

24:41.220 --> 24:43.380
impacted by this particular data point.

24:43.380 --> 24:44.380
Exactly.

24:44.380 --> 24:47.300
And that's, that's, you know, the value that we're trying to get out of the new generation

24:47.300 --> 24:51.100
of Azure data lake store and some of the work we're doing on the Azure data catalog side

24:51.100 --> 24:56.900
is to give you exposure into like, what's all the cool stuff that's being done or not

24:56.900 --> 25:03.020
being done with this data, it goes back to letting your decision makers know, am I occurring

25:03.020 --> 25:07.780
business value from these, you know, these ETL pipelines that I'm spinning all these

25:07.780 --> 25:11.740
compute dollars to go and cook these curated data sets in.

25:11.740 --> 25:17.500
And that's a large part of what are the larger ML platform to team did before as well

25:17.500 --> 25:22.300
was we helped with creating curated data sets for, for being in office to go and build

25:22.300 --> 25:23.540
models on top of.

25:23.540 --> 25:28.420
So we had the data engineering pipelines and the machine learning pipelines and the

25:28.420 --> 25:33.940
release management pipelines all like in under the same umbrella which helped to inform

25:33.940 --> 25:37.980
the way we're designing the system now to be designed to meet enterprises where they

25:37.980 --> 25:41.060
are and help them scale up and out as they go.

25:41.060 --> 25:42.060
Yeah.

25:42.060 --> 25:48.540
I'm curious what are some of the key things that you're learning from customers kind of

25:48.540 --> 25:55.540
on the ground who are working to implement this type of stuff, kind of how would you characterize

25:55.540 --> 26:00.940
you know where folks are if you can generalize and what are the key stumbling blocks that

26:00.940 --> 26:01.940
kind of thing.

26:01.940 --> 26:07.100
So there if we're to think about it in terms of like four phases where phase one is kicking

26:07.100 --> 26:11.980
the tires, phase two is models reproducible, phase three is models deployed and being

26:11.980 --> 26:16.300
used in phase four is I've all the magical automated retraining wizardry.

26:16.300 --> 26:19.220
They're mostly between phase one and phase two right now, like very few of them have

26:19.220 --> 26:22.580
actually gotten a model deployed into the wild.

26:22.580 --> 26:26.460
If they have it deployed, it's only deployed as like a dev test API, they don't trust

26:26.460 --> 26:27.860
it yet.

26:27.860 --> 26:32.220
So that's you know one learning is customers were a lot earlier in the journey than we've

26:32.220 --> 26:38.100
been expecting coming from doing this for internal Microsoft teams.

26:38.100 --> 26:43.860
Another one is that the for the customers we're talking to their internal organizations

26:43.860 --> 26:50.060
are not always structured to let them innovate most effectively, elaborate on that.

26:50.060 --> 26:56.660
Yeah, so they'll have part of their org, you know, their data team and their IT department

26:56.660 --> 27:02.900
and their research teams are totally disconnected, disjointed, don't communicate to each other,

27:02.900 --> 27:04.620
don't understand each other.

27:04.620 --> 27:08.460
And so IT just sees what the researchers are doing and says there's no way you're doing

27:08.460 --> 27:10.340
any of this in production.

27:10.340 --> 27:16.420
The data engineers are unsure what data the data scientists are using, like they might

27:16.420 --> 27:19.680
data scientists might be off running SQL cores in the side, but they have no idea from

27:19.680 --> 27:23.300
which tables, tables will disappear under the data scientist.

27:23.300 --> 27:28.100
So we're just instead of doing a pure like here's how to use the platform.

27:28.100 --> 27:33.540
It's more, hey, let's get all the right people in the room together from IT and research

27:33.540 --> 27:38.700
and your data platform and your software development platforms and start a conversation

27:38.700 --> 27:45.540
and build up the domain expertise and the relationships on the people side before you get started

27:45.540 --> 27:47.780
with the process or the platform.

27:47.780 --> 27:52.020
That's been yeah, one big learning is to step back and focus on getting the right people

27:52.020 --> 27:55.940
involved first and then they can figure out the process that's going to work well for

27:55.940 --> 28:00.060
their business and then they can adopt the platform tools that we've been building to

28:00.060 --> 28:04.180
help them be more efficient at doing end-to-end ML.

28:04.180 --> 28:11.740
Are you finding that there's a pattern in organization that allows organizations to

28:11.740 --> 28:17.580
move more quickly like centralized versus decentralized or a quote unquote center of excellence

28:17.580 --> 28:24.500
or embedded into business units that is it one of the other of those works best?

28:24.500 --> 28:31.740
I think what we've seen work best is to have one business unit sort of act as the incubator

28:31.740 --> 28:36.540
to vet the end-to-end flow and actually get a model working in production, but then

28:36.540 --> 28:43.620
have the overall center of excellence centralized team observe what they're doing and take

28:43.620 --> 28:51.620
notes and let them flesh out what the canonical reference ML ops architecture pipeline should

28:51.620 --> 28:53.020
look like.

28:53.020 --> 28:58.020
So I think that's seem out of all the patterns we've seen a lot of patterns being applied.

28:58.020 --> 29:03.060
That one seems to be the best so far though is let a small team give them some flexibility

29:03.060 --> 29:08.620
to go and build a model, take it to production with some light guard rails and they can build

29:08.620 --> 29:14.980
out the reference architecture, get repository and CICD pipeline templates that the rest of

29:14.980 --> 29:17.340
the teams and the company can use.

29:17.340 --> 29:24.420
Is the salient point there that the end business unit that has the problem owns the deployment

29:24.420 --> 29:30.140
of the model as opposed to the centralized but somewhat disconnected data science or

29:30.140 --> 29:31.140
AI.

29:31.140 --> 29:32.140
Correct.

29:32.140 --> 29:33.140
Yes.

29:33.140 --> 29:37.820
So your DevOps team for your business unit needs to know and understand the fact that

29:37.820 --> 29:42.220
a model is going to be entering their ecosystem and needs to be able to manage it with the

29:42.220 --> 29:48.220
same tools they manage their other application releases with hence the integration with Azure

29:48.220 --> 29:52.900
DevOps to make sure that all your pipelines are tracked and managed in one place and there's

29:52.900 --> 29:57.660
not this one row release pipeline that's coming in and causing issues and havoc with

29:57.660 --> 29:59.860
the rest of your production system.

29:59.860 --> 30:07.020
And generally when you look at these production pipelines did the pipelines and the tooling resonate

30:07.020 --> 30:12.860
with the DevOps teams or are they like this strange beast that they take a long time

30:12.860 --> 30:13.860
for them to run around.

30:13.860 --> 30:17.180
So they freak out until they see the Azure DevOps integration and then they'll go,

30:17.180 --> 30:22.700
OK, I understand this, yeah, hence where I'm like, you need to have the tools that your

30:22.700 --> 30:25.020
audience can understand.

30:25.020 --> 30:28.260
You show them a Jupyter notebook, they'll jump out of their seats and run away scared.

30:28.260 --> 30:32.940
Whereas you show them like, oh, here's a managed multi-phase release pipeline with like

30:32.940 --> 30:36.980
clearly defined declarative ammo for the different steps like that that resonates well

30:36.980 --> 30:37.980
with them.

30:37.980 --> 30:41.180
Whereas data scientists, you show them a big complex approval flow and they're going

30:41.180 --> 30:42.660
to be like, I'm never using any of this.

30:42.660 --> 30:46.820
You show them a Jupyter notebook, they're happy or you know an ID with like low friction

30:46.820 --> 30:54.020
and Python and then your data engineers, again, you show them a, you know, a confusing notebook

30:54.020 --> 30:55.020
process flow.

30:55.020 --> 30:57.980
They're not going to like that as much, but you show them a clean like ETL where they

30:57.980 --> 31:03.060
can drag and drop and run their SQL queries and understand are there pipelines running

31:03.060 --> 31:07.180
in a stable fashion like that resonates with them say it's yeah.

31:07.180 --> 31:11.140
Different personas, different tools, they need to work together and figure out what

31:11.140 --> 31:14.500
process is going to work for their business needs.

31:14.500 --> 31:18.180
Because I've kind of looked at primarily this machine learning engineer role that has

31:18.180 --> 31:22.180
been emerging over the past few years and now we're talking about the DevOps engineers

31:22.180 --> 31:28.020
like a separate thing that the line is kind of a great movie and blurred line, right?

31:28.020 --> 31:32.060
What we've seen in terms of, we have customers to ask us, well, how do we hire these ML

31:32.060 --> 31:33.060
engineers?

31:33.060 --> 31:37.220
It's like, basically, you need a person who understands DevOps, but also can talk to

31:37.220 --> 31:41.540
your data scientists or can, can figure out the work they're doing help them get their

31:41.540 --> 31:47.060
work into a reproducible pipeline on the training side and help with deploying the model and

31:47.060 --> 31:52.100
integrating it into the rest of your application lifecycle management tools.

31:52.100 --> 31:58.620
So yeah, your ML engineer needs to be a DevOps person with some understanding of ML.

31:58.620 --> 32:05.740
And is a DevOps person necessarily a software engineer that is coding a model based on

32:05.740 --> 32:10.460
not necessarily, they just need to be really good at operational excellence.

32:10.460 --> 32:15.980
So do they understand how to write things declaratively, how to set up process control

32:15.980 --> 32:21.420
flows so that things work nicely and like, you don't need to understand the ML, the data

32:21.420 --> 32:24.700
scientists is doing, you need to understand the process they're going through to produce

32:24.700 --> 32:25.700
that model.

32:25.700 --> 32:31.380
So they have a bunch of code and a Jupyter Notebook, like help them factor right into modules

32:31.380 --> 32:35.700
that you can stitch together, but you don't need to understand like, you know, the machine

32:35.700 --> 32:40.940
learning framework that they're using specifically in that context.

32:40.940 --> 32:45.380
You've mentioned Jupyter Notebooks a few times, you know, one of the things that I see

32:45.380 --> 32:49.900
that folks are trying to figure out is like, should we do ML in Notebooks or should we

32:49.900 --> 32:58.740
do ML in IDEs, Microsoft, you know, has a huge investment in IDEs, but you've also been

32:58.740 --> 33:03.740
like in Visual Studio Code, making it more kind of interactive, integrated kind of real

33:03.740 --> 33:08.780
time to incorporate some of the Notebook Eskys, so interaction.

33:08.780 --> 33:11.940
We wanted to be fluid to go from one of the other.

33:11.940 --> 33:18.540
We've seen the value and the interactive canvases for doing, you know, rapid fire experimentation.

33:18.540 --> 33:24.540
We also talked to large companies like Netflix to learn how they use Notebooks in automation

33:24.540 --> 33:28.260
at scale and some of the ML project, for example, exactly.

33:28.260 --> 33:31.180
So we've actually integrated paper mill into our platform as well.

33:31.180 --> 33:36.660
So you can, if you're designing your training pipeline, you can stitch together a mix of scripts

33:36.660 --> 33:42.820
and Notebooks and, you know, data processing steps together, and we try to be as fluid

33:42.820 --> 33:43.820
as we can.

33:43.820 --> 33:48.580
And we're working with the developer division as well to figure out how to more cleanly

33:48.580 --> 33:51.580
integrate Notebooks into our ID experiences.

33:51.580 --> 33:55.980
And you saw some of that in the BS code side and there's more stuff coming to help with

33:55.980 --> 33:56.980
that.

33:56.980 --> 34:02.940
We've talked a little bit about kind of this automated retraining aspect of managing

34:02.940 --> 34:03.940
model life cycles.

34:03.940 --> 34:08.060
Are there other aspects of managing model life cycles that you find important for folks

34:08.060 --> 34:09.220
to think about?

34:09.220 --> 34:14.820
So yeah, knowing when to retrain the model is one thing, knowing when to deprecate the

34:14.820 --> 34:20.460
model is nothing to say that the data the model is trained with is stale or can't be

34:20.460 --> 34:21.460
used anymore.

34:21.460 --> 34:27.460
We've got removed for GDPR reasons, this is why having the whole lineage graph is so important

34:27.460 --> 34:32.060
to be able to figure out exactly what data was used to train the model.

34:32.060 --> 34:37.900
Other things around model life cycle management, yeah, I really know who's using it, know where

34:37.900 --> 34:43.500
the model's running, know if the model's adding business value, know if the data coming into

34:43.500 --> 34:48.540
the model has changed a lot since you trained it, know if the model is dealing with some

34:48.540 --> 34:53.460
type of seasonal data and needs to be retrained on a seasonal basis there.

34:53.460 --> 34:58.100
And then also know the resource requirements for your model.

34:58.100 --> 35:03.580
So another big thing we see a trip a lot of our customers up is they train the model

35:03.580 --> 35:08.980
on these big beefy VMs with massive GPUs and go to deploy and it's like, hey, my model

35:08.980 --> 35:09.980
is crashing.

35:09.980 --> 35:10.980
What do I do?

35:10.980 --> 35:11.980
Right.

35:11.980 --> 35:13.900
And so we've tried to build tooling in to help with that as well.

35:13.900 --> 35:20.500
So profiling your model, running sample queries into it, different sizes of sample queries

35:20.500 --> 35:24.660
to not always the same thing and making sure you know, does your model have enough CPU

35:24.660 --> 35:30.940
and memory and the right size GPU to perform effectively.

35:30.940 --> 35:35.940
And we're also doing some work on the on X framework to help with taking those models

35:35.940 --> 35:41.660
and quantizing them or optimizing them for a specific business use case on the hardware

35:41.660 --> 35:47.580
side, which is really slowly coming in, especially we have customers in the manufacturer, manufacturing

35:47.580 --> 35:52.060
sector who want to run models quickly on the edge on small hardware.

35:52.060 --> 35:55.540
So it's like, how do you, how do you manage that transition from this model I train on

35:55.540 --> 35:59.420
this beefy machine to this model running on this tiny device?

35:59.420 --> 36:08.900
You finding that that most customers are deploying models or even thinking about an individual,

36:08.900 --> 36:12.220
I've got this, you know, this model that I've created and I'm going to think about the

36:12.220 --> 36:17.500
way I deploy this model versus, I've got a model I've built it to a standard, it's

36:17.500 --> 36:21.460
just like any other model that I'm going to just going to throw it into my model deployment

36:21.460 --> 36:22.460
thing.

36:22.460 --> 36:24.420
Like, are they there yet?

36:24.420 --> 36:26.580
Some of them are there.

36:26.580 --> 36:30.980
The ones that have been doing this for a while longer and develop like their template

36:30.980 --> 36:32.500
for their model deployment flow.

36:32.500 --> 36:33.500
Right.

36:33.500 --> 36:37.260
We try to provide as much tooling as we can, you know, in the platform and in the registry

36:37.260 --> 36:40.540
for you to track all the relevant things about it.

36:40.540 --> 36:44.540
But really just getting, getting the model deployed into your existing apico system, making

36:44.540 --> 36:49.980
sure that you have the ability to do controlled rollout and A-B testing because you don't

36:49.980 --> 36:52.780
want to just always pave over the previous model.

36:52.780 --> 36:56.260
So that's the most advanced customers are just getting to that point now where they're

36:56.260 --> 37:01.460
ready to start doing A-B testing and looking for our help to go and do that.

37:01.460 --> 37:02.460
Yeah.

37:02.460 --> 37:07.860
So, A-B, along the lines of testing, we've talked about this a little bit.

37:07.860 --> 37:15.060
There's both kind of the, you know, online testing of your models, freshness, but then

37:15.060 --> 37:23.020
also all kinds of deployment scenarios that have been developed in the context of DevOps,

37:23.020 --> 37:27.300
like Canary, then Red, Green, Blue kind of stuff, like all the colors.

37:27.300 --> 37:28.300
Yeah.

37:28.300 --> 37:29.300
Yeah, all the colors, right?

37:29.300 --> 37:32.900
Do you see all of that stuff out in the wild?

37:32.900 --> 37:34.580
Yes.

37:34.580 --> 37:39.540
The main difference we've seen with models compared to normal software being rolled out is oftentimes

37:39.540 --> 37:45.420
they'll develop a model and test it offline and batch for a while before using it.

37:45.420 --> 37:49.660
So they won't need to necessarily deploy it to receive real traffic right away.

37:49.660 --> 37:54.940
They'll let the model, you know, get the new model, they'll wait a week, run the model

37:54.940 --> 37:59.420
and batch against the past weeks worth of data, and then compare how different it is to it.

37:59.420 --> 38:04.300
So it's just the fact that you can test the model offline as opposed to having to do everything

38:04.300 --> 38:05.300
in an online fashion.

38:05.300 --> 38:06.300
Yeah.

38:06.300 --> 38:07.300
Yeah.

38:07.300 --> 38:10.460
That's probably the biggest Delta, but otherwise we see all the same patterns as with normal

38:10.460 --> 38:11.460
software.

38:11.460 --> 38:12.460
Right.

38:12.460 --> 38:13.460
Right.

38:13.460 --> 38:14.460
Because you're testing two things, right?

38:14.460 --> 38:17.460
You're testing the model's statistical ability to predict something, but then it's also

38:17.460 --> 38:18.460
software.

38:18.460 --> 38:19.460
Right.

38:19.460 --> 38:22.100
And you don't necessarily want to put a software, work in pieces of software out there.

38:22.100 --> 38:23.100
Correct.

38:23.100 --> 38:29.860
So it's a software with uncertain behavior, or more uncertain behavior than any normal

38:29.860 --> 38:31.940
software application you threw out there, yeah.

38:31.940 --> 38:32.940
Yeah.

38:32.940 --> 38:36.100
What can we look forward to in this space from your perspective?

38:36.100 --> 38:40.580
So as far as things to look forward to, there's lots of investments coming in improving

38:40.580 --> 38:46.620
our story around enterprise readiness, making it easier for customers to do, and secure

38:46.620 --> 38:51.660
data science and ML workloads, work to help improve collaboration and sharing across

38:51.660 --> 38:57.860
the enterprise, how do I figure out which other teams have been doing modeling work similar

38:57.860 --> 38:58.860
to mine?

38:58.860 --> 38:59.860
How do I take advantage of that?

38:59.860 --> 39:05.860
So accelerating collaboration, velocity, more work on the enterprise readiness front,

39:05.860 --> 39:09.820
and then a tighter-knit integration with the rest of the big data platform stuff.

39:09.820 --> 39:16.140
So integration with data lake, data catalog, data factory, DevOps get up, and it's all

39:16.140 --> 39:20.220
about helping customers get to production ML faster.

39:20.220 --> 39:23.020
Well Jordan, thanks so much for chatting with me.

39:23.020 --> 39:24.020
Thanks for having me.

39:24.020 --> 39:25.020
Yeah.

39:25.020 --> 39:26.020
Appreciate it.

39:26.020 --> 39:31.780
All right everyone, that's our show for today to learn more about this episode, visit

39:31.780 --> 39:34.100
Twomlai.com.

39:34.100 --> 40:02.220
As always, thanks so much for listening and catch you next time.

