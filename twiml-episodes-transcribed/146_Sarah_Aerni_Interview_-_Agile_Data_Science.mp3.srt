1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:36,040
Today we continue our train AI series with a conversation with Sarah Ayrney, director

5
00:00:36,040 --> 00:00:39,300
of data science at Salesforce Einstein.

6
00:00:39,300 --> 00:00:44,200
Sarah and I sat down at the train AI conference to discuss her talk notes from the field,

7
00:00:44,200 --> 00:00:48,680
the platform people and processes of Agile Data Science.

8
00:00:48,680 --> 00:00:52,920
Sarah and I dig into the concept of Agile Data Science, exploring what it means to her

9
00:00:52,920 --> 00:00:57,360
and how she's seen it done at Salesforce and other places she's worked.

10
00:00:57,360 --> 00:01:01,520
We also dig into the notion of machine learning platforms, which is a keen area of interest

11
00:01:01,520 --> 00:01:02,600
for me.

12
00:01:02,600 --> 00:01:06,640
We discuss some of the common elements we've seen in M.O. platforms and when it makes

13
00:01:06,640 --> 00:01:10,720
sense for an organization to start building one.

14
00:01:10,720 --> 00:01:14,600
As you may know by now, we're celebrating the second anniversary of the podcast this

15
00:01:14,600 --> 00:01:15,600
week.

16
00:01:15,600 --> 00:01:20,320
Have you found this podcast useful? If so, we want to hear how.

17
00:01:20,320 --> 00:01:26,480
Submit your written comments via the page at twimble.ai slash 2av or leave us a voicemail

18
00:01:26,480 --> 00:01:32,160
at 636-735-3658.

19
00:01:32,160 --> 00:01:35,840
We'll be sharing a few of your stories with your permission, of course, and a special

20
00:01:35,840 --> 00:01:39,240
podcast episode celebrating the Twimble community.

21
00:01:39,240 --> 00:01:45,560
Again, the second anniversary page is at twimble.ai slash 2av.

22
00:01:45,560 --> 00:01:49,960
Finally, a shout out to our friends over at Figure 8 for their sponsorship of this week's

23
00:01:49,960 --> 00:01:50,960
series.

24
00:01:50,960 --> 00:01:55,320
Figure 8 provides the essential human and aloop AI platform for data science and machine

25
00:01:55,320 --> 00:01:56,880
learning teams.

26
00:01:56,880 --> 00:02:01,520
The Figure 8 software platform trains, tests, and tunes machine learning models to make

27
00:02:01,520 --> 00:02:04,000
AI work in the real world.

28
00:02:04,000 --> 00:02:08,880
Learn more at www.figure-8.com.

29
00:02:08,880 --> 00:02:14,720
This episode was recorded live on site at the train AI conference, so there is some unavoidable

30
00:02:14,720 --> 00:02:21,720
background noise, and now on to the show.

31
00:02:21,720 --> 00:02:22,560
All right, everyone.

32
00:02:22,560 --> 00:02:27,640
I am at the train AI conference, and I've got the pleasure to be seated across from Sarah

33
00:02:27,640 --> 00:02:28,960
A. Ernie.

34
00:02:28,960 --> 00:02:32,080
Sarah is Director of Data Science with Salesforce Einstein.

35
00:02:32,080 --> 00:02:34,680
Sarah, welcome to this week in machine learning and AI.

36
00:02:34,680 --> 00:02:36,440
Thank you so much for having me.

37
00:02:36,440 --> 00:02:42,160
I'm really excited to jump into the topic of Agile Data Science, which is what you were

38
00:02:42,160 --> 00:02:46,240
speaking about here at the conference today, but before we do that, tell us a little bit

39
00:02:46,240 --> 00:02:51,320
about your background and how you got into ML and AI and data science.

40
00:02:51,320 --> 00:02:52,320
Absolutely.

41
00:02:52,320 --> 00:03:00,040
Actually, I started off way back in undergrad when I had the opportunity to work on my

42
00:03:00,040 --> 00:03:02,720
two passions, biology and computer science together.

43
00:03:02,720 --> 00:03:08,400
They opened up a major bioinformatics, and back then it was an ascent field, but I was

44
00:03:08,400 --> 00:03:13,640
able to do some research in cancer biology, kind of on leukemia and detecting patterns,

45
00:03:13,640 --> 00:03:21,200
and then spent some time doing remote research in Caltech on DNA binding and trying to understand

46
00:03:21,200 --> 00:03:25,560
if there were patterns we could recognize there, and while I didn't have a specific path

47
00:03:25,560 --> 00:03:31,440
and decided to go to grad school for biomedical informatics, kind of building on that.

48
00:03:31,440 --> 00:03:35,280
The lab that I ended up joining was a AI lab, although it was completely focused on

49
00:03:35,280 --> 00:03:40,000
bioinformatics, it still was generally part of that department, and so I got a little bit

50
00:03:40,000 --> 00:03:47,120
more exposure, and started doing things again related to DNA and genomics, but as any

51
00:03:47,120 --> 00:03:52,360
grad student at Stanford started company on the side.

52
00:03:52,360 --> 00:03:58,880
That's not cliche at all, but it was actually with a good purpose, at the time, actually

53
00:03:58,880 --> 00:04:04,320
Obama had announced the stimulus grants, and it was over one of these late-night conversations

54
00:04:04,320 --> 00:04:08,840
with professors that we realized there was this opportunity to support other researchers

55
00:04:08,840 --> 00:04:14,200
that didn't necessarily have all of the things required in order to complete and apply for

56
00:04:14,200 --> 00:04:18,400
these grants, where they might have really strong background in some biological portion,

57
00:04:18,400 --> 00:04:20,920
but didn't have the informatics side of it.

58
00:04:20,920 --> 00:04:24,600
So originally we thought, well, we would offer consulting services to them and kind of

59
00:04:24,600 --> 00:04:31,480
stick to our field, and turns out that space of consulting is a lot bigger, and our group

60
00:04:31,480 --> 00:04:33,880
ended up doing things well outside of that.

61
00:04:33,880 --> 00:04:39,680
So I did some work predicting what websites people would be interested in browsing, and

62
00:04:39,680 --> 00:04:44,240
kind of completely went off the rails from the biomedical background, but all related

63
00:04:44,240 --> 00:04:46,960
to the stimulus grants are totally a field.

64
00:04:46,960 --> 00:04:51,640
No, it was, it's almost as if you put out a press release on the topic, and it just

65
00:04:51,640 --> 00:04:56,480
snowball from there, and people kind of hear that you're available and around.

66
00:04:56,480 --> 00:05:01,600
So no, it ended up coming through other avenues, it was very interesting.

67
00:05:01,600 --> 00:05:04,880
There were a lot of things that originally started off really around grant writing,

68
00:05:04,880 --> 00:05:08,560
and helping startups that wanted to apply for these small business innovation research

69
00:05:08,560 --> 00:05:09,560
grants.

70
00:05:09,560 --> 00:05:10,560
SRS, yeah.

71
00:05:10,560 --> 00:05:11,560
So, yeah.

72
00:05:11,560 --> 00:05:12,560
So people...

73
00:05:12,560 --> 00:05:13,560
Been there.

74
00:05:13,560 --> 00:05:17,440
So it's interesting, because I think people apply to them when they hear about them,

75
00:05:17,440 --> 00:05:21,640
and if they're not from an academic background, don't necessarily know how to write these

76
00:05:21,640 --> 00:05:25,840
grants that will be received well by an academic audience, and so our experience on kind

77
00:05:25,840 --> 00:05:29,320
of academic grant writing was already very helpful.

78
00:05:29,320 --> 00:05:33,680
The informatics part was maybe only a portion of it, and then eventually it moved outside

79
00:05:33,680 --> 00:05:39,440
of the healthcare realm, and I realized I had just passion and data in general.

80
00:05:39,440 --> 00:05:44,280
And although when I was graduating, I still knew that I cared about healthcare and doing

81
00:05:44,280 --> 00:05:48,760
something in that space, so I was completely ready to let go of it.

82
00:05:48,760 --> 00:05:53,360
I wanted to leave it a little bit open, and so I joined a software company where I was

83
00:05:53,360 --> 00:05:56,880
actually more or less a data scientist for hire.

84
00:05:56,880 --> 00:06:04,880
I worked on the particular distributed database that the company Greenblum actually was selling,

85
00:06:04,880 --> 00:06:09,800
and working with my customers with the expertise in biomedical and chromatics, I'd be able

86
00:06:09,800 --> 00:06:13,000
to speak their language, but still really be doing raw data science.

87
00:06:13,000 --> 00:06:17,880
And a lot of the time, it wasn't necessarily in my area of expertise, but it was somebody

88
00:06:17,880 --> 00:06:19,880
who spoke their language.

89
00:06:19,880 --> 00:06:24,240
And there again, over time, I drifted from that background, started doing work on jet

90
00:06:24,240 --> 00:06:28,480
plane engines and banking fraud, and it was just all over the place.

91
00:06:28,480 --> 00:06:30,960
So that was great, and it was exciting.

92
00:06:30,960 --> 00:06:36,960
But what I really wanted to do was deploy models at crazy scale to my customers, and so

93
00:06:36,960 --> 00:06:44,080
Salesforce really offers that opportunity to build models in this really amazing platform

94
00:06:44,080 --> 00:06:49,400
that allows me to just reach a lot of people, so that they are empowered to use AI.

95
00:06:49,400 --> 00:06:53,480
And I can do the work around in innovative data science on the other end.

96
00:06:53,480 --> 00:07:01,000
Before we jump into the agile data science element, Einstein, and a lot of ways Einstein

97
00:07:01,000 --> 00:07:04,840
is starting to feel like Watson, for me, is like this umbrella brand that covers so many

98
00:07:04,840 --> 00:07:07,720
things, like in your words, what is Einstein?

99
00:07:07,720 --> 00:07:11,800
So the purpose of Einstein really is just to democratize AI.

100
00:07:11,800 --> 00:07:17,440
So Salesforce has many customers, and any customer, regardless of size and industry, should

101
00:07:17,440 --> 00:07:18,800
be able to harness that power.

102
00:07:18,800 --> 00:07:24,120
And so the goal is to allow our customers to use AI to have better interactions with

103
00:07:24,120 --> 00:07:25,920
our customers to make them more predictive.

104
00:07:25,920 --> 00:07:29,920
And so we have products, some packaged apps that I've worked on like predictive leads

105
00:07:29,920 --> 00:07:35,280
scoring that allow sales rep to understand which of the sales leads are likely to convert

106
00:07:35,280 --> 00:07:39,640
to an opportunity based on the data that they have that have historically converted.

107
00:07:39,640 --> 00:07:44,160
So while it might sound like a big umbrella, there are some very specific use cases for

108
00:07:44,160 --> 00:07:46,000
which we have packaged apps.

109
00:07:46,000 --> 00:07:49,800
And then there's a product that I'm currently working on the Einstein prediction builder,

110
00:07:49,800 --> 00:07:56,120
which is really intended to, again, democratize AI by allowing our admins to set up things

111
00:07:56,120 --> 00:08:01,320
that they want to predict, explicitly some prediction that we don't know they'll want,

112
00:08:01,320 --> 00:08:04,840
but they would like to have to augment what they're currently able to have their users

113
00:08:04,840 --> 00:08:05,840
do.

114
00:08:05,840 --> 00:08:15,080
So we have, you know, Salesforce admin, I do some kind of forecast, and I want to predict

115
00:08:15,080 --> 00:08:17,600
the close date on a deal or something like that.

116
00:08:17,600 --> 00:08:24,120
And I can train using my own historical data and maybe some of yours and so now it is

117
00:08:24,120 --> 00:08:25,120
absolutely.

118
00:08:25,120 --> 00:08:32,200
We are, we are very strict on kind of focusing on single tenant, being able to leverage

119
00:08:32,200 --> 00:08:33,200
its own data.

120
00:08:33,200 --> 00:08:38,640
So only my data, but the example is the kind of thing that I might be able to predict

121
00:08:38,640 --> 00:08:39,640
using this platform.

122
00:08:39,640 --> 00:08:40,640
Yeah.

123
00:08:40,640 --> 00:08:43,360
Well, we're moving forward on right now in prediction builder are binary classifications

124
00:08:43,360 --> 00:08:49,480
and regressions and the goal is to allow you to build these predictions and we're working

125
00:08:49,480 --> 00:08:55,920
on making the UI really intuitive and allow the admins to do this in a way that's making

126
00:08:55,920 --> 00:09:01,120
it easy to set up and then following on, allowing them to inject those results directly

127
00:09:01,120 --> 00:09:06,800
back because sort of going back to my talk a bit today, there's this part around AI

128
00:09:06,800 --> 00:09:11,400
and certainly when I was working at my previous job, where we spend a lot of time building

129
00:09:11,400 --> 00:09:15,640
models without thinking about how to get them in front of our customers in the end.

130
00:09:15,640 --> 00:09:18,880
And so I spent all this time building a model thinking it's really great and then I'm

131
00:09:18,880 --> 00:09:22,720
ready to push live to production and that doesn't go anywhere.

132
00:09:22,720 --> 00:09:26,960
It just kind of sits with me and that's what's amazing with Einstein is really that it's

133
00:09:26,960 --> 00:09:27,960
right there.

134
00:09:27,960 --> 00:09:32,360
You're going to be able to have it immediately available when it's done building.

135
00:09:32,360 --> 00:09:36,320
So agile data science, what does that mean for you?

136
00:09:36,320 --> 00:09:42,880
Yeah, well, my talk today focused around that concept and insisting that it's of course

137
00:09:42,880 --> 00:09:46,680
around people in process to make it possible, but there really is a platform underlying

138
00:09:46,680 --> 00:09:48,320
that enables it.

139
00:09:48,320 --> 00:09:55,000
So pivotal, my previous company, there is a practice pivotal labs that is all about agile

140
00:09:55,000 --> 00:09:56,400
and extreme programming.

141
00:09:56,400 --> 00:10:00,320
And when we were part of that team, it was really important for us to try and understand

142
00:10:00,320 --> 00:10:03,640
how to integrate into that process.

143
00:10:03,640 --> 00:10:09,120
And agile has been around, it's meant to be adaptive and to work, but there certainly

144
00:10:09,120 --> 00:10:13,480
is a mindset that comes with how to approach it, how to have sprints, how to have stories,

145
00:10:13,480 --> 00:10:15,720
how to be able to go live and test things.

146
00:10:15,720 --> 00:10:19,840
And it doesn't necessarily translate perfectly in data science.

147
00:10:19,840 --> 00:10:25,840
So what's interesting is when we think about for us trying to push our changes live, it's

148
00:10:25,840 --> 00:10:31,480
important to have a way to not feel once you've built a model that is just going nowhere.

149
00:10:31,480 --> 00:10:36,640
It needs to be connected, so really thinking MVP with what it means to actually add any

150
00:10:36,640 --> 00:10:38,040
model to a product.

151
00:10:38,040 --> 00:10:43,680
And that's exactly where I was focusing was a model that is part of a product.

152
00:10:43,680 --> 00:10:48,080
And then secondarily, making it possible to run experiments so that you're not moving

153
00:10:48,080 --> 00:10:53,680
off into a corner and doing it elsewhere and then figuring out how to translate it over.

154
00:10:53,680 --> 00:10:58,360
So really that you have frameworks in place and for Salesforce, we have this platform that

155
00:10:58,360 --> 00:11:04,240
makes it possible for data scientists to use services to get access to what they need

156
00:11:04,240 --> 00:11:12,200
and to share algorithms, to share future engineering steps, and to allow us to run experiments

157
00:11:12,200 --> 00:11:16,880
without having access to data, but with understanding how performance the results might be at

158
00:11:16,880 --> 00:11:18,560
the end.

159
00:11:18,560 --> 00:11:28,080
So setting aside what the customer might use, how do you kind of express agile internally

160
00:11:28,080 --> 00:11:31,800
and the way you do data science within Salesforce?

161
00:11:31,800 --> 00:11:37,560
Yeah, so the team that I run, the prediction builder team, we have multiple models in production,

162
00:11:37,560 --> 00:11:42,640
but we use one underlying code base that automates the entire future engineering process.

163
00:11:42,640 --> 00:11:44,880
So it's AutoML that we use.

164
00:11:44,880 --> 00:11:50,520
And the platform team as a whole is responsible for having services around, you know, safe

165
00:11:50,520 --> 00:11:55,000
access to data around making compute scalable and possible.

166
00:11:55,000 --> 00:11:59,560
And these elements around automated machine learning are really where we get to implement

167
00:11:59,560 --> 00:12:02,680
that agile methodology really cleanly.

168
00:12:02,680 --> 00:12:07,400
So when you have these shared repositories, what that means is if you have models in production

169
00:12:07,400 --> 00:12:12,440
and you can continuously monitor their performance, which is really, really critical, you're able

170
00:12:12,440 --> 00:12:17,560
to understand where there may be a problem, where models are not performant and be able

171
00:12:17,560 --> 00:12:21,320
to go ahead and iterate and come up with solutions to it.

172
00:12:21,320 --> 00:12:25,800
So some of the examples that I gave was, you know, you see a drop in AUROC and you decide

173
00:12:25,800 --> 00:12:29,160
that you actually want to do something about it and, you know, assuming you have access

174
00:12:29,160 --> 00:12:33,840
to the data, you can do a deep dive and understand what has happened.

175
00:12:33,840 --> 00:12:38,080
Understand if there are some underlying elements, maybe there are fields that are not being

176
00:12:38,080 --> 00:12:40,040
processed correctly.

177
00:12:40,040 --> 00:12:44,000
Salesforce has some very unique problems around leakage detection.

178
00:12:44,000 --> 00:12:49,240
Turns out that when you have a pull of data, that is from a moment where they're setting

179
00:12:49,240 --> 00:12:50,880
up the predictions.

180
00:12:50,880 --> 00:12:55,200
We don't know what came before a certain, yes, no event.

181
00:12:55,200 --> 00:13:00,240
So an easy example is if you were trying to predict churn and the customer has a field

182
00:13:00,240 --> 00:13:04,320
called Reason for Churn and they don't explicitly exclude it, we actually need to detect that

183
00:13:04,320 --> 00:13:10,520
problem because if we're now using Reason for Churn in our model, we'll have a problem.

184
00:13:10,520 --> 00:13:14,600
So those types of things would appear up front in a holdout set that we're doing extremely

185
00:13:14,600 --> 00:13:17,760
well, but over time you would see there's a degradation in performance because of course

186
00:13:17,760 --> 00:13:18,760
you're not doing a good job.

187
00:13:18,760 --> 00:13:23,640
So many of the new data sets that were in that scoring open set and so it's that kind

188
00:13:23,640 --> 00:13:28,360
of monitoring and identifying of problems to which we then have to create resolutions.

189
00:13:28,360 --> 00:13:34,360
So if it's introducing a new test to find leakers, if it's coming up with a new way of processing

190
00:13:34,360 --> 00:13:40,360
text, if it's, you know, at some point you can imagine wanting to use sentiment as just

191
00:13:40,360 --> 00:13:44,720
a feature that you engineer out of any text field that comes in.

192
00:13:44,720 --> 00:13:49,280
It's really having that shared repository being able to add that new feature.

193
00:13:49,280 --> 00:13:54,680
So taking the time, building a new way of engineering it, adding it on to that library or

194
00:13:54,680 --> 00:13:59,680
that service, and then iterating, that's sort of one portion.

195
00:13:59,680 --> 00:14:02,800
There's like the user story around implementation.

196
00:14:02,800 --> 00:14:07,120
There's also really that platform though around how do you now run an experiment?

197
00:14:07,120 --> 00:14:12,240
So if you have a new feature that you're adding, and in our case you can have so many

198
00:14:12,240 --> 00:14:17,520
customers that are building models using your underlying AutoML, you would need to understand

199
00:14:17,520 --> 00:14:21,760
what the impact is of adding that feature to any one of your customers.

200
00:14:21,760 --> 00:14:25,960
And so being able to do that in a way that doesn't require us to look at data, but allows

201
00:14:25,960 --> 00:14:29,200
us to run an experiment at scale to understand what the impact would be.

202
00:14:29,200 --> 00:14:33,480
Are there any regressions that happen for lack of a better term in our model performance

203
00:14:33,480 --> 00:14:35,160
across the board?

204
00:14:35,160 --> 00:14:42,440
So to make sure I understand that you've got these customers, customer data, it's all

205
00:14:42,440 --> 00:14:44,160
single tenant.

206
00:14:44,160 --> 00:14:47,960
It sounds like you're not testing against it, but you are.

207
00:14:47,960 --> 00:14:52,040
What exactly are, how exactly are you doing that or what are you doing?

208
00:14:52,040 --> 00:14:57,920
Our team is not able to do is so in general, we can't look at the data, so trust is our

209
00:14:57,920 --> 00:15:01,720
number one value across the board it sells for us, and so we cannot request access that

210
00:15:01,720 --> 00:15:02,720
is not our data.

211
00:15:02,720 --> 00:15:07,920
But can you deploy models to it that don't return the data and test statistically?

212
00:15:07,920 --> 00:15:11,160
Yeah, so you can understand it on aggregate level.

213
00:15:11,160 --> 00:15:12,160
What is your performance?

214
00:15:12,160 --> 00:15:14,360
Is there a change in performance?

215
00:15:14,360 --> 00:15:19,120
And we do have customers that we might pilot a product with where we can work and understand

216
00:15:19,120 --> 00:15:20,680
what's going on.

217
00:15:20,680 --> 00:15:25,000
They can request investigations, but it's not a system where we need to be able to go

218
00:15:25,000 --> 00:15:28,280
in and look at every single model.

219
00:15:28,280 --> 00:15:33,240
Similarly, although Salesforce is at such a massive scale, any company will have multiple

220
00:15:33,240 --> 00:15:35,160
models in production at some point.

221
00:15:35,160 --> 00:15:38,680
I don't believe any company sets out to have exactly one model.

222
00:15:38,680 --> 00:15:43,480
And it's unlikely that at some point, every model is going to have such a heavy data science

223
00:15:43,480 --> 00:15:48,440
component that rather than monitoring and alerting to a degradation performance, you would

224
00:15:48,440 --> 00:15:52,400
want to be able to just have multiple models in production and probably a small number

225
00:15:52,400 --> 00:15:55,720
of data scientists supporting those efforts and iterating.

226
00:15:55,720 --> 00:16:01,000
So having a platform where you have a shared set of services for engineering and for understanding

227
00:16:01,000 --> 00:16:05,840
that if you add a feature or if you change an algorithm or you decide to tune parameters

228
00:16:05,840 --> 00:16:10,240
in a certain way, understanding the impact across the board becomes really critical.

229
00:16:10,240 --> 00:16:14,160
And so having those monitoring services there, although it's really important for Salesforce

230
00:16:14,160 --> 00:16:17,160
is extremely important for any company that's out there.

231
00:16:17,160 --> 00:16:22,920
I'm envisioning, when you say platform, I'm envisioning something analogous to like Uber's

232
00:16:22,920 --> 00:16:28,080
published quite a bit on Michelangelo there, internal platform, and some other companies

233
00:16:28,080 --> 00:16:29,080
have.

234
00:16:29,080 --> 00:16:33,200
Netflix talks about their platform a lot too, it's really great.

235
00:16:33,200 --> 00:16:37,560
LinkedIn, I mean, a lot of companies have spent time, I think what's so unique about

236
00:16:37,560 --> 00:16:44,000
what Salesforce does is our platform is around running multiple models for our customers.

237
00:16:44,000 --> 00:16:49,120
And it's really around taking what all of these companies have done and the number of

238
00:16:49,120 --> 00:16:52,880
data scientists and engineers that go into building those platforms and making it accessible

239
00:16:52,880 --> 00:16:58,040
to our customers so that they can benefit from what it is that we have done and be able

240
00:16:58,040 --> 00:17:04,480
to access a few clicks instead and sort of know that you have that data science prowess

241
00:17:04,480 --> 00:17:08,440
behind it and people that are paying attention to constantly improving and making things

242
00:17:08,440 --> 00:17:10,440
better on your behalf.

243
00:17:10,440 --> 00:17:16,480
So the first of these elements is a repository and you've mentioned a couple of things that

244
00:17:16,480 --> 00:17:19,280
might be in that repository, models and features.

245
00:17:19,280 --> 00:17:20,680
What else is in that repository?

246
00:17:20,680 --> 00:17:26,720
So in general, and again, this will vary by companies that are building for their purposes.

247
00:17:26,720 --> 00:17:30,320
If it's, for example, a single application that you have, there's always going to be an

248
00:17:30,320 --> 00:17:32,080
element around data.

249
00:17:32,080 --> 00:17:33,080
So how is it there?

250
00:17:33,080 --> 00:17:34,080
You're storing data.

251
00:17:34,080 --> 00:17:35,080
How are you indexing it?

252
00:17:35,080 --> 00:17:36,080
How do you access it?

253
00:17:36,080 --> 00:17:39,120
Data scientists don't want to be paying attention to like what folder is this living in

254
00:17:39,120 --> 00:17:41,640
or what table or whatever the history is.

255
00:17:41,640 --> 00:17:46,760
So what we do is we build APIs to let us request data in a certain way so that it can be

256
00:17:46,760 --> 00:17:52,760
again automated, there exists a customer that wants to use the services and therefore being

257
00:17:52,760 --> 00:17:57,840
able to reference by APIs and running code against that is then possible.

258
00:17:57,840 --> 00:18:03,320
And although again, not everyone is operating at Salesforce scale, no data scientist necessarily

259
00:18:03,320 --> 00:18:07,200
wants to spend time thinking about how data moves in and out.

260
00:18:07,200 --> 00:18:12,080
But those services need to be there to make it possible and to make a possible index

261
00:18:12,080 --> 00:18:17,280
what's happening, look for drifts, understand how to monitor that scores are being pushed

262
00:18:17,280 --> 00:18:18,280
back.

263
00:18:18,280 --> 00:18:20,240
The scores have changed over time.

264
00:18:20,240 --> 00:18:25,880
All of these elements around data services, monitoring services and then actual modeling

265
00:18:25,880 --> 00:18:27,640
and feature engineering as well.

266
00:18:27,640 --> 00:18:30,360
So interestingly enough, this topic is timely for me.

267
00:18:30,360 --> 00:18:35,080
I just organized an event last week where we spent quite a bit of time talking about

268
00:18:35,080 --> 00:18:40,400
this notion of, you know, machine learning platforms and agile data science and things

269
00:18:40,400 --> 00:18:47,400
of that like and one of the questions that came up is, you know, for a company that is

270
00:18:47,400 --> 00:18:55,480
not necessarily on a platform like Salesforce, but is doing data science, you know, when

271
00:18:55,480 --> 00:19:00,840
do they start to collect some of this stuff into a platform?

272
00:19:00,840 --> 00:19:03,280
When does it make sense to build a platform?

273
00:19:03,280 --> 00:19:09,040
If you're a, you know, an internet company that's probably earlier than if you're a traditional

274
00:19:09,040 --> 00:19:14,400
enterprise, how do you know when it's time to start consolidating some of this functionality,

275
00:19:14,400 --> 00:19:19,040
building repositories, building, monitoring, framework, stuff like that?

276
00:19:19,040 --> 00:19:21,720
So monitoring is from the start.

277
00:19:21,720 --> 00:19:25,640
Like anytime you have an application, you're, you're going to monitor everything that's

278
00:19:25,640 --> 00:19:27,480
happening on those apps.

279
00:19:27,480 --> 00:19:31,360
So as soon as you're wanting to add a model to it, you have to have monitoring around

280
00:19:31,360 --> 00:19:32,360
your models.

281
00:19:32,360 --> 00:19:38,600
It's to treat a model any different from another feature of your app.

282
00:19:38,600 --> 00:19:43,360
And if does your, your, your data scientists is service because they want to know what's

283
00:19:43,360 --> 00:19:48,400
happening, not by always requesting access and moving over, they need to be alerted to

284
00:19:48,400 --> 00:19:51,160
an issue the same way anyone else would be.

285
00:19:51,160 --> 00:19:57,160
Also for allowing them to triage the problem more quickly, I understand that in general,

286
00:19:57,160 --> 00:20:05,080
there might be a question about waiting until you have multiple models maybe or some opportunities

287
00:20:05,080 --> 00:20:10,960
for sharing, but I argue and that was sort of one of my take homes on the talk today.

288
00:20:10,960 --> 00:20:15,440
Sort of my main point is to plan for multiple models always.

289
00:20:15,440 --> 00:20:21,160
I cannot imagine an organization that doesn't commit to having more than one model.

290
00:20:21,160 --> 00:20:26,080
And by doing so, really doing some upfront planning, certainly at my last job, there

291
00:20:26,080 --> 00:20:31,640
were many companies that were kind of in this position of having a lot of smaller organizations

292
00:20:31,640 --> 00:20:34,320
within that could have benefited from sharing, from understanding.

293
00:20:34,320 --> 00:20:37,400
And of course, there's always a question of investment.

294
00:20:37,400 --> 00:20:42,960
And I think it's a struggle when you don't invest time in the platform, I was just talking

295
00:20:42,960 --> 00:20:47,040
with several people today about the fact that there's an expectation that you should

296
00:20:47,040 --> 00:20:51,000
have good return on investment for your modeling and for the data scientists to hire and everything

297
00:20:51,000 --> 00:20:52,240
that's there.

298
00:20:52,240 --> 00:20:57,040
And of course with that, they want to have the normal agile processes in place of having

299
00:20:57,040 --> 00:21:00,520
waste assess the risk and how much longer and like, when am I going to see something

300
00:21:00,520 --> 00:21:06,600
from it? And as a data scientist, and certainly coming from graduate school, we think of it

301
00:21:06,600 --> 00:21:08,800
as like, oh, we're entering a research project.

302
00:21:08,800 --> 00:21:14,200
We have absolutely no idea if this will work and, you know, let us go work on it for three

303
00:21:14,200 --> 00:21:17,080
months, six months a year and we'll get back to you.

304
00:21:17,080 --> 00:21:23,840
And that may be one way to approach it, but inevitably that leads to all around frustration,

305
00:21:23,840 --> 00:21:26,960
both on kind of the business that is trying to get something.

306
00:21:26,960 --> 00:21:29,640
And also the data scientists, who then once they're ready, don't necessarily know how

307
00:21:29,640 --> 00:21:31,160
to go to production.

308
00:21:31,160 --> 00:21:37,480
And so if we really want a more agile approach as a whole, you must build platforms to support

309
00:21:37,480 --> 00:21:41,880
your data scientists' ability to iterate, ability to understand how well something is

310
00:21:41,880 --> 00:21:45,400
performing and ability to fail early.

311
00:21:45,400 --> 00:21:50,600
Because if we don't commit to having data pipelines, monitoring services, waste to deploy

312
00:21:50,600 --> 00:21:55,080
models from the start, we will inevitably find ourselves in the position of every single

313
00:21:55,080 --> 00:22:01,040
time starting over, rather than building for multiple apps from the beginning.

314
00:22:01,040 --> 00:22:10,560
Do you feel like data science is standardized enough across organizations and use cases

315
00:22:10,560 --> 00:22:12,280
that we can do that, right?

316
00:22:12,280 --> 00:22:18,520
Like a classic problem is, you know, building a platform before you're, you know, the problem

317
00:22:18,520 --> 00:22:20,920
that the platform is trying to solve, right?

318
00:22:20,920 --> 00:22:25,040
You build all kinds of features, oh, I need a, you know, model repository with versioning

319
00:22:25,040 --> 00:22:26,040
that does XYZ.

320
00:22:26,040 --> 00:22:30,560
This is what the API looks like, and then you go to actually build something and it doesn't,

321
00:22:30,560 --> 00:22:34,000
you know, it's like you're building the bridge from both sides and they don't meet in the

322
00:22:34,000 --> 00:22:35,000
middle.

323
00:22:35,000 --> 00:22:36,000
Oh, I love that.

324
00:22:36,000 --> 00:22:41,800
There's like a famous bridge between Germany and Switzerland that every time I drive by

325
00:22:41,800 --> 00:22:45,800
with my dad who lives there, he always talks about how the German side and the Swiss side

326
00:22:45,800 --> 00:22:47,600
agreed to build it.

327
00:22:47,600 --> 00:22:52,680
And when they came to me in the middle, they were off by a huge number because Switzerland

328
00:22:52,680 --> 00:22:58,560
measures from elevation from the Mediterranean and Germany from the North Sea.

329
00:22:58,560 --> 00:23:00,960
And that was the reason why that happened.

330
00:23:00,960 --> 00:23:02,560
It's like very funny.

331
00:23:02,560 --> 00:23:04,600
It's like starting a race at zero and one.

332
00:23:04,600 --> 00:23:08,360
No, there's like a, there are, you can find blogs about this on that.

333
00:23:08,360 --> 00:23:09,360
It's very funny.

334
00:23:09,360 --> 00:23:11,840
But I totally understand what you're saying.

335
00:23:11,840 --> 00:23:15,040
And absolutely at the outset, I, and I should have done the same here.

336
00:23:15,040 --> 00:23:17,560
I level set my talk with saying, look, there are different ways of doing this.

337
00:23:17,560 --> 00:23:18,560
Different types of models.

338
00:23:18,560 --> 00:23:24,800
And my examples are, there are models that are informing strategic decisions and my classic

339
00:23:24,800 --> 00:23:29,800
examples are from, you know, back in my biomedical informatics days, they're, you know, trying

340
00:23:29,800 --> 00:23:32,240
to identify the next drug target to go after.

341
00:23:32,240 --> 00:23:36,160
And so we're doing a bunch of data science to understand where to head next with our research.

342
00:23:36,160 --> 00:23:39,120
And they're kind of these others that are like doing automated decisions and they're

343
00:23:39,120 --> 00:23:42,920
ones that are like augmenting an app and maybe informing something.

344
00:23:42,920 --> 00:23:46,320
So like predictive leads going, we'll dislate, convert.

345
00:23:46,320 --> 00:23:50,560
And you're absolutely right, depending on what it is that you're building, you would want

346
00:23:50,560 --> 00:23:54,280
to focus on the different elements first.

347
00:23:54,280 --> 00:23:58,760
But across the board, there will always be things that make sense like how to bring your

348
00:23:58,760 --> 00:24:05,960
data in a way that's access controlled in a way that is also accessed via APIs.

349
00:24:05,960 --> 00:24:08,720
I cannot think of a reason to not want to do that.

350
00:24:08,720 --> 00:24:13,440
There's no reason to have to like, imagine what folder or table or scheme, it's a, it's

351
00:24:13,440 --> 00:24:15,120
painful to think of it that way.

352
00:24:15,120 --> 00:24:19,040
There's a corollary there that the data lake is not enough.

353
00:24:19,040 --> 00:24:21,360
Look, it's, it's all iterative.

354
00:24:21,360 --> 00:24:25,600
You know, we all started on a path together on a journey.

355
00:24:25,600 --> 00:24:31,560
And what I really hope and the hope of my talk was to share how we're, we're moving forward

356
00:24:31,560 --> 00:24:32,960
on this.

357
00:24:32,960 --> 00:24:36,720
And, and to learn from each other with it.

358
00:24:36,720 --> 00:24:45,000
So I, I know that again, during my consulting days where I was asked to do these projects

359
00:24:45,000 --> 00:24:49,160
and then kind of convince the organizations that what we had done was good enough.

360
00:24:49,160 --> 00:24:51,960
And now let's, let's figure out how to build an app.

361
00:24:51,960 --> 00:24:55,560
That was interesting because it was almost like the companies needed convincing that data

362
00:24:55,560 --> 00:24:58,280
science was going to do something good.

363
00:24:58,280 --> 00:25:02,800
And if we can all agree that there will be value in models, if only we accepted it, can't

364
00:25:02,800 --> 00:25:07,080
we start by saying that we're looking to build a use case into something.

365
00:25:07,080 --> 00:25:11,600
And let's come up with some, let's create a roadmap for it and then decide what to execute

366
00:25:11,600 --> 00:25:14,960
against first in an MVP mindset.

367
00:25:14,960 --> 00:25:20,320
And the other element is that that agile request that we have of data scientists does need

368
00:25:20,320 --> 00:25:24,320
to come with that commitment to support them and to support the differences between data

369
00:25:24,320 --> 00:25:26,800
scientists and software developers.

370
00:25:26,800 --> 00:25:31,160
They have a lot of things in common around wanting to monitor their models or their, their

371
00:25:31,160 --> 00:25:35,960
services around wanting to, you know, understand how to improve around trying out new technologies

372
00:25:35,960 --> 00:25:41,400
or algorithms, but they just have slightly different ways that those need to be solved.

373
00:25:41,400 --> 00:25:44,520
And platform commitment needs to be there to make that possible.

374
00:25:44,520 --> 00:25:46,320
Let's take it from a software development perspective.

375
00:25:46,320 --> 00:25:47,320
I'm a software engineer.

376
00:25:47,320 --> 00:25:51,600
I'm building some kind of component or service, microservice, whatever.

377
00:25:51,600 --> 00:25:54,160
It's part of agile, part of DevOps.

378
00:25:54,160 --> 00:25:57,160
I don't know, part of whatever you want to call it, like the current best practices.

379
00:25:57,160 --> 00:26:03,200
I'm also building the tests and responsible for the tests that, you know, would ultimately

380
00:26:03,200 --> 00:26:06,520
monitor the performance of my service, right?

381
00:26:06,520 --> 00:26:12,520
A big part of DevOps is I also own that service and production and I'm responsible for

382
00:26:12,520 --> 00:26:17,280
its upkeep, so it's in my, it's to my advantage to build the tests that will allow me to

383
00:26:17,280 --> 00:26:19,120
be able to do that.

384
00:26:19,120 --> 00:26:25,560
Data sciences is a bit different at most places, I think, because there's this bit of a,

385
00:26:25,560 --> 00:26:29,920
I don't know if you want to call it a wall, that may be too strong, but there's, you know,

386
00:26:29,920 --> 00:26:33,600
the person who's building the models isn't the person that's putting that in the model

387
00:26:33,600 --> 00:26:39,320
into production and is responsible for, they're responsible for different elements of it,

388
00:26:39,320 --> 00:26:40,320
right?

389
00:26:40,320 --> 00:26:44,280
Data sciences is responsible for the performance of that model over time.

390
00:26:44,280 --> 00:26:51,160
Are they also building those monitoring tests or is the engineering team that's implementing

391
00:26:51,160 --> 00:26:54,240
the model, building those overall monitoring tests?

392
00:26:54,240 --> 00:26:56,040
Like, where does that sit?

393
00:26:56,040 --> 00:27:02,880
Those, I think those are hard questions to answer as a whole, I think, in the abstract

394
00:27:02,880 --> 00:27:03,880
meaning.

395
00:27:03,880 --> 00:27:04,880
Yeah.

396
00:27:04,880 --> 00:27:05,880
Yeah.

397
00:27:05,880 --> 00:27:06,880
So that's elsewhere for what is that?

398
00:27:06,880 --> 00:27:12,360
Well, so I think one thing is to consider that I guess this, this concept of like completely

399
00:27:12,360 --> 00:27:14,680
isolated teams is, is difficult.

400
00:27:14,680 --> 00:27:19,760
I think it's important for data science and engineering to sit extremely closely in,

401
00:27:19,760 --> 00:27:25,040
in prediction builder, for example, we have a very hybrid team, we have, we have a designer,

402
00:27:25,040 --> 00:27:31,680
we have front end, we have data scientists, and our data scientists are very good engineers

403
00:27:31,680 --> 00:27:38,840
as well, in the sense that they've learned how to write code or at least work on these

404
00:27:38,840 --> 00:27:42,560
libraries that are able to run in production.

405
00:27:42,560 --> 00:27:50,320
And part of that is because of the way we have built that platform, we can run that code

406
00:27:50,320 --> 00:27:54,080
and run experiments using that code and make changes.

407
00:27:54,080 --> 00:27:57,680
And if you just think about it as adding a component in and not necessarily what you're

408
00:27:57,680 --> 00:28:02,880
describing, which is kind of completely offline building and then, okay, it looks good on

409
00:28:02,880 --> 00:28:06,880
the sample data and then what you're saying throwing it over the wall, but instead having

410
00:28:06,880 --> 00:28:11,440
the ability to run it in those same environments and make sure that it's performant.

411
00:28:11,440 --> 00:28:15,480
And of course, having a team that's dedicated to understanding, you know, scalability concerns

412
00:28:15,480 --> 00:28:21,360
of those exist, to make sure that those things are all very intertwined and that monitoring

413
00:28:21,360 --> 00:28:25,640
happens in the same way that we have the same alerts for, you know, SLAs are not met,

414
00:28:25,640 --> 00:28:29,920
well, your SLAs are not met for models aren't running fast enough, they're not reaching

415
00:28:29,920 --> 00:28:35,240
completion fast enough, you're not, you know, pushing back scores, you've seen a drop

416
00:28:35,240 --> 00:28:38,680
in the number of scores, you've seen your valuations have changed.

417
00:28:38,680 --> 00:28:44,840
I mean, you can kind of hijack some of the same tools, or you can build your own, but

418
00:28:44,840 --> 00:28:50,960
I would suggest looking at how suitable existing infrastructure is and existing tools that

419
00:28:50,960 --> 00:28:56,160
exist for the rest of your app and seeing how many of those could also now support data

420
00:28:56,160 --> 00:28:57,920
scientists as well.

421
00:28:57,920 --> 00:29:04,320
I think there is a desire across the board as well to integrate these teams into stop

422
00:29:04,320 --> 00:29:08,960
treating them as completely isolated and I really think part of it is by introducing them

423
00:29:08,960 --> 00:29:13,720
to that same culture, the same agile processes, then maybe they wouldn't feel the need to

424
00:29:13,720 --> 00:29:19,800
feel so separate as well, but you really do need to support them in that change.

425
00:29:19,800 --> 00:29:25,080
And I think that's maybe an interesting segue to people in process, like we've talked

426
00:29:25,080 --> 00:29:29,360
a lot about technology, that's only one leg of the stool, so to speak, you've certainly

427
00:29:29,360 --> 00:29:36,920
touched on people in process throughout, but did you have a specific message in your talk

428
00:29:36,920 --> 00:29:38,880
around the people aspect of all this?

429
00:29:38,880 --> 00:29:44,280
Yeah, so some of it was really around my own journey from starting an extremely research

430
00:29:44,280 --> 00:29:48,480
oriented world, and to be honest, back in grad school, I wish that somebody had introduced

431
00:29:48,480 --> 00:29:54,920
agile to me, I would have benefited from really not spending time on obsessively thinking

432
00:29:54,920 --> 00:30:01,280
about one particular problem and instead creating an MVP and iterating, and probably would

433
00:30:01,280 --> 00:30:06,560
have also helped in terms of when do I publish a paper if you think of that as your quote

434
00:30:06,560 --> 00:30:08,280
unquote release.

435
00:30:08,280 --> 00:30:14,800
So it was around that journey on acknowledging the similarities that I have with developers

436
00:30:14,800 --> 00:30:22,560
around the needs that I have, how they are served by either a process or by a monitoring

437
00:30:22,560 --> 00:30:29,520
tool or tests that they write, and how those aren't identical for me, but there are things

438
00:30:29,520 --> 00:30:34,200
that I want that, again, bring the platform in, if I could only modify those elements

439
00:30:34,200 --> 00:30:39,240
I might be able to get, but also a little bit around what it means to have a user story

440
00:30:39,240 --> 00:30:47,760
for a data scientist, so sort of acknowledging my asks and my desire to want to push live,

441
00:30:47,760 --> 00:30:52,360
thinking then next on the process side, given that there's a platform in place, how can

442
00:30:52,360 --> 00:30:57,720
I now go through identifying opportunities where the team focuses on things that are

443
00:30:57,720 --> 00:31:00,240
near-term and things that are long-term.

444
00:31:00,240 --> 00:31:05,920
So we have a backlog of things that we want to achieve, we have investigations that are

445
00:31:05,920 --> 00:31:10,320
ongoing, and those investigations follow a certain pattern, which is identify a source

446
00:31:10,320 --> 00:31:15,440
cause, and from that, create user stories around solutions that you want to implement,

447
00:31:15,440 --> 00:31:21,640
and those are sometimes very, very tangible, very easy, tuning a hyper parameter, trying

448
00:31:21,640 --> 00:31:26,880
to understand if you could add a new feature, maybe there's a bug, something I give an example

449
00:31:26,880 --> 00:31:31,800
of like a text field that's blank, should be treated as a null, something very explicit

450
00:31:31,800 --> 00:31:37,000
like that, very tangible, and you can have this running backlog of those items, but we

451
00:31:37,000 --> 00:31:43,400
also acknowledge that at any stage there could be a lot of open opportunities.

452
00:31:43,400 --> 00:31:48,320
Maybe you want to try new word embeddings, or you want to try a segmented model, and

453
00:31:48,320 --> 00:31:54,520
those are larger problems that require longer-term efforts, and to kind of balance out those

454
00:31:54,520 --> 00:32:00,480
near-term data science focused and require data science chops to identify the issues against

455
00:32:00,480 --> 00:32:04,880
the longer-term or innovative opportunities, we have architects on the team that are

456
00:32:04,880 --> 00:32:10,840
able to take on those bigger meteor problems to understand how do we tackle using something

457
00:32:10,840 --> 00:32:15,880
completely different, and choose when to promote that into something that will push into

458
00:32:15,880 --> 00:32:16,880
production.

459
00:32:16,880 --> 00:32:21,120
So you do need to have a large enough team to support both sides of that.

460
00:32:21,120 --> 00:32:27,840
And so is this, are you identifying a role that's specifically a data science architect,

461
00:32:27,840 --> 00:32:32,880
as opposed to like a traditional engineering architect, or developer architect?

462
00:32:32,880 --> 00:32:37,920
So yeah, I guess the role of architect is very specific, which is somebody who's at the

463
00:32:37,920 --> 00:32:44,520
level of solving these meteor problems, not necessarily the architect that you're mentioning,

464
00:32:44,520 --> 00:32:50,240
but they do have a solid understanding and ability to work and communicate with a engineering

465
00:32:50,240 --> 00:32:51,640
architect.

466
00:32:51,640 --> 00:32:59,920
So yeah, is this architect on the data science side, is that even the right way to say it,

467
00:32:59,920 --> 00:33:05,560
is someone that's come up through data science, what I've not heard much of the notion

468
00:33:05,560 --> 00:33:09,320
of a data science architect, if that's what we're even calling this?

469
00:33:09,320 --> 00:33:13,560
Yes, I mean, that's sort of the term we use, and maybe that's too specific to us, but

470
00:33:13,560 --> 00:33:20,360
it is the folks that serve that role are individuals that have strong machine learning chops and

471
00:33:20,360 --> 00:33:24,960
strong engineering chops to the sense that they can understand, you know, where something

472
00:33:24,960 --> 00:33:29,240
would fail, where there might be a problem with scalability, where there might be a whole

473
00:33:29,240 --> 00:33:32,120
new paradigm that needs to be introduced.

474
00:33:32,120 --> 00:33:37,400
And at times, folks need to come together and learn, but there are people that are essentially

475
00:33:37,400 --> 00:33:42,840
given the space and time to be able to go after these other problems, as opposed to the ones

476
00:33:42,840 --> 00:33:45,520
that are more focused on the MVP near term.

477
00:33:45,520 --> 00:33:50,840
That we have kind of our productivity zone and our incubation zone, and yeah, a team

478
00:33:50,840 --> 00:33:53,280
needs to be large enough to support it, but it's important.

479
00:33:53,280 --> 00:33:54,280
Makes perfect.

480
00:33:54,280 --> 00:34:01,240
Yeah, and how about on the process side, are there observations that you've made from

481
00:34:01,240 --> 00:34:07,760
a process perspective that makes this all more efficient, more smoothly flowing?

482
00:34:07,760 --> 00:34:14,160
Yeah, aside from actually using, you know, your traditional backlog and stories, what

483
00:34:14,160 --> 00:34:20,560
we do that, I think, again, borrowing these techniques from elsewhere, really around

484
00:34:20,560 --> 00:34:25,040
doing prioritizations around, you know, what is the value of going after a certain element

485
00:34:25,040 --> 00:34:29,440
versus an ease of implementation to allow us to triage where to spend our time?

486
00:34:29,440 --> 00:34:31,360
And value is something really hard to assess.

487
00:34:31,360 --> 00:34:34,440
How do I know what the value is going to be of introducing a new algorithm?

488
00:34:34,440 --> 00:34:38,200
How do I know what the value is going to be of, you know, maybe going after a new way

489
00:34:38,200 --> 00:34:44,960
to process texts or engineering features, what I can do, or, you know, something like

490
00:34:44,960 --> 00:34:50,800
a segmented model, what I can do is look at my models and how they're currently failing,

491
00:34:50,800 --> 00:34:55,520
what, you know, are there a lot of, a lot more text fields in one of my models, and therefore

492
00:34:55,520 --> 00:34:59,720
I believe that if I make a change, it would have a lot of impact.

493
00:34:59,720 --> 00:35:04,520
And sort of measuring where I believe there would be a big shift as one way to prioritize

494
00:35:04,520 --> 00:35:09,360
my backlog, and again, around the ease of implementation really focusing around what

495
00:35:09,360 --> 00:35:16,480
will be a longer term and more risky element that I need to work on, but I won't have

496
00:35:16,480 --> 00:35:19,920
the same kind of immediate need to shift.

497
00:35:19,920 --> 00:35:27,880
Have you made any attempts to, you know, taking all these, you know, the technology approach,

498
00:35:27,880 --> 00:35:30,720
platform approach, the people approach, the process approach?

499
00:35:30,720 --> 00:35:38,160
Have you made any attempts to try to characterize the net advantage over not doing these things?

500
00:35:38,160 --> 00:35:43,480
I mean, I'm imagining they've all evolved organically, and so the answer is probably

501
00:35:43,480 --> 00:35:50,160
no, but you know, certainly there's some degree of overhead associated with these different

502
00:35:50,160 --> 00:35:51,160
steps.

503
00:35:51,160 --> 00:35:54,280
How do you continue to justify it?

504
00:35:54,280 --> 00:36:00,360
Yeah, so the most obvious, I guess, metric that you could use is how quickly your models

505
00:36:00,360 --> 00:36:02,960
are improving over time, and then how quickly you're able to ship.

506
00:36:02,960 --> 00:36:08,760
So I know at previous companies, there might be like the actual how often we have new releases

507
00:36:08,760 --> 00:36:09,840
that could be used.

508
00:36:09,840 --> 00:36:12,840
What's interesting is in data science, of course, it's a little bit challenging because if

509
00:36:12,840 --> 00:36:15,760
I release a new model, that could impact my end customers.

510
00:36:15,760 --> 00:36:21,520
So, but what we can do is understand how much improvement we're getting on in performance,

511
00:36:21,520 --> 00:36:25,600
because we do have models and we do have summary stats on how they're performing and we can

512
00:36:25,600 --> 00:36:28,840
see how they've changed over time.

513
00:36:28,840 --> 00:36:33,480
And of course, it's a bit tricky when you're in an early phase or in a company that has

514
00:36:33,480 --> 00:36:38,320
a smaller number of models in production, but if you can see how quickly you're making

515
00:36:38,320 --> 00:36:43,280
progress or improving outcomes or, you know, in the instance of something more explicit

516
00:36:43,280 --> 00:36:48,920
like getting customer feedback, you know, we might have like a time to resolution that

517
00:36:48,920 --> 00:36:52,680
their improvements there, that those are metrics that you can capture at the end of your

518
00:36:52,680 --> 00:36:57,760
sort of unwilling to say, like, you know, how quickly am I releasing as you're a measure?

519
00:36:57,760 --> 00:37:01,440
Any other topics that you touched on in your talk that we haven't covered yet?

520
00:37:01,440 --> 00:37:09,360
Well, I think overall, at Salesforce, we have this really cool AutoML pipeline that we've

521
00:37:09,360 --> 00:37:15,840
been developing that really focuses around future engineering and, you know, automating

522
00:37:15,840 --> 00:37:18,200
the process of selecting models.

523
00:37:18,200 --> 00:37:24,920
And I know that I think that term AutoML is something that isn't always so well received

524
00:37:24,920 --> 00:37:29,880
by data scientists as a whole, I think I've noticed that at times there's this question

525
00:37:29,880 --> 00:37:34,520
around, oh, is like this automation way of data scientists, essentially.

526
00:37:34,520 --> 00:37:36,320
And it actually isn't at all.

527
00:37:36,320 --> 00:37:41,560
And the reason why I think it's such an important concept for everyone to introduce is that what

528
00:37:41,560 --> 00:37:47,560
AutoML really represents is harnessing your data scientists, repeated future engineering

529
00:37:47,560 --> 00:37:52,200
techniques, repeated analyses, finding opportunities to do these things.

530
00:37:52,200 --> 00:37:56,360
And instead of having everybody come up with a new, you know, like, stop word removal,

531
00:37:56,360 --> 00:38:01,120
or language class virus sentiment, now share those.

532
00:38:01,120 --> 00:38:07,360
Work together on saying, I would like to use, you know, this shared library and let's

533
00:38:07,360 --> 00:38:12,040
instead focus together on how to automate that process and identifying ways to, you know,

534
00:38:12,040 --> 00:38:16,360
produce more features and instead focus on the fun, hairy problems, like, what's the

535
00:38:16,360 --> 00:38:19,440
next set of features that I want to engineer?

536
00:38:19,440 --> 00:38:21,320
So it's not really such a scary space.

537
00:38:21,320 --> 00:38:26,840
It's actually very empowering, I feel, for data scientists to consider what they can contribute

538
00:38:26,840 --> 00:38:27,840
there.

539
00:38:27,840 --> 00:38:28,840
It's been a great chat.

540
00:38:28,840 --> 00:38:29,840
Thank you so much.

541
00:38:29,840 --> 00:38:31,800
Anything else you'd like to mention before we close out?

542
00:38:31,800 --> 00:38:34,840
No, I'm so appreciative of this opportunity.

543
00:38:34,840 --> 00:38:35,840
Thank you so much.

544
00:38:35,840 --> 00:38:36,840
All right.

545
00:38:36,840 --> 00:38:37,840
Thank you.

546
00:38:37,840 --> 00:38:43,440
All right, everyone, that's our show for today.

547
00:38:43,440 --> 00:38:48,640
For more information on Sarah or any of the topics covered in this episode, head over

548
00:38:48,640 --> 00:38:53,760
to twimmel.ai slash talk slash 143.

549
00:38:53,760 --> 00:38:57,000
Thanks again to figure eight for their sponsorship of this episode.

550
00:38:57,000 --> 00:39:04,440
To follow along with the train AI series, visit twimmel.ai slash train AI 2018.

551
00:39:04,440 --> 00:39:09,720
And finally, show us some love for the podcast's second anniversary and share how it's been

552
00:39:09,720 --> 00:39:15,600
helpful to you over at twimmel.ai slash 2 AV.

553
00:39:15,600 --> 00:39:21,240
Thank you so much for listening and catch you next time.

