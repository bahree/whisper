1
00:00:00,000 --> 00:00:09,920
All right, everyone. I am here with your Donnus,

2
00:00:09,920 --> 00:00:16,080
Katanidis. Your Donnus is research director at CNRS in Paris.

3
00:00:16,080 --> 00:00:19,440
That's the National Center for Scientific Research,

4
00:00:19,440 --> 00:00:24,240
as well as the head of quantum algorithms with QCWARE.

5
00:00:24,240 --> 00:00:27,680
Your Donnus, welcome to the Twelma AI podcast.

6
00:00:27,680 --> 00:00:32,320
Thanks for having me. I'm looking forward to this talk.

7
00:00:32,320 --> 00:00:35,520
It's been a while about a year, a little over a year,

8
00:00:35,520 --> 00:00:39,920
since we talked about quantum machine learning on the show here.

9
00:00:39,920 --> 00:00:44,720
And I imagine that the field has advanced quite a bit.

10
00:00:44,720 --> 00:00:49,120
So I'm looking forward to an update as well as your take.

11
00:00:49,120 --> 00:00:54,720
You recently delivered a main conference keynote at ICML on the top.

12
00:00:54,720 --> 00:00:59,360
So can we say that quantum machine learning is going big time now or blowing up?

13
00:01:01,680 --> 00:01:06,080
I always say that quantum machine learning is the most overhyped

14
00:01:06,080 --> 00:01:09,280
and underestimated area of quantum computing.

15
00:01:09,280 --> 00:01:11,680
And it can be both in superposition, right?

16
00:01:11,680 --> 00:01:16,160
So I think we have many exciting new results.

17
00:01:16,160 --> 00:01:22,240
This is true. And to me, it was very important to give this talk at ICML,

18
00:01:22,240 --> 00:01:27,040
because I really believe in the fact that we need to work together

19
00:01:27,040 --> 00:01:29,280
the quantum and the classical ML community,

20
00:01:29,280 --> 00:01:31,360
because you have the problems.

21
00:01:31,360 --> 00:01:36,320
You know what are the bottlenecks on the computational side?

22
00:01:36,320 --> 00:01:39,920
You've been looking at these problems and the specifics for many years.

23
00:01:39,920 --> 00:01:41,600
We are coming from quantum algorithms.

24
00:01:41,600 --> 00:01:45,760
We understand what quantum computing can offer.

25
00:01:45,760 --> 00:01:48,720
And if we put these things together, I think we can do great things together.

26
00:01:49,920 --> 00:01:51,680
So tell us a little bit about your background.

27
00:01:51,680 --> 00:01:54,400
How did you come to work on quantum machine learning?

28
00:01:55,680 --> 00:02:01,280
So I started working on quantum algorithms more general 20 years ago.

29
00:02:01,280 --> 00:02:05,440
So I started my PhD in 2000 in Berkeley.

30
00:02:05,440 --> 00:02:07,600
So I was in California for a few years.

31
00:02:08,720 --> 00:02:14,160
And there I worked on different things, mostly quantum algorithms for communication

32
00:02:14,160 --> 00:02:21,520
networks, and for cryptography, and some quantum algorithms, not so much on machine learning.

33
00:02:21,520 --> 00:02:25,840
And the only machine learning I did was a classical result on

34
00:02:26,960 --> 00:02:30,160
classical recommendation systems through an interest you that I did with

35
00:02:30,160 --> 00:02:33,920
Pravagkar Lagavan at some point in the beginning of 2000.

36
00:02:34,800 --> 00:02:37,600
And after that, I went to MIT.

37
00:02:37,600 --> 00:02:40,080
I worked with Peter Schor for a couple of years.

38
00:02:40,080 --> 00:02:45,360
And then I moved to Paris in 2006 as a research director for CNRS.

39
00:02:46,560 --> 00:02:51,040
Okay. And so you've been working on quantum algorithms for 20 years.

40
00:02:51,040 --> 00:02:53,680
How long is quantum algorithms even been a thing?

41
00:02:56,320 --> 00:02:57,440
Not much more than that.

42
00:02:57,440 --> 00:02:58,320
21 years.

43
00:03:01,360 --> 00:03:06,080
So I think the first breakthrough result that people

44
00:03:06,080 --> 00:03:11,360
cite on quantum algorithms is Peter Schor's algorithm for factoring large numbers.

45
00:03:11,360 --> 00:03:15,440
And this was in 93, so it's like 27 years ago.

46
00:03:16,080 --> 00:03:21,520
And this was kind of the result that made people very interested in looking at this new model of

47
00:03:21,520 --> 00:03:30,960
computation and try to figure out what does it have to offer when we look at trying to

48
00:03:30,960 --> 00:03:37,280
compute and code information using quantum mechanics and not classical physics.

49
00:03:38,960 --> 00:03:46,720
So to me, it's a very exciting thing because it kind of tries to understand in some sense also

50
00:03:46,720 --> 00:03:52,720
what are the computational limits of nature, because nature is a quantum mechanical system.

51
00:03:52,720 --> 00:03:55,200
So this is what we try to understand.

52
00:03:56,000 --> 00:04:00,800
And maybe it's important to say in the beginning that quantum computing is not

53
00:04:00,800 --> 00:04:06,800
a faster processor, right? So it is not that everything that you run nowadays on a classical

54
00:04:06,800 --> 00:04:10,960
computer, you can just put it on a quantum computer and it will run faster.

55
00:04:10,960 --> 00:04:13,840
This is really not how it works.

56
00:04:13,840 --> 00:04:17,920
It's a completely different paradigm.

57
00:04:17,920 --> 00:04:20,080
It's a different way of computing.

58
00:04:20,080 --> 00:04:26,720
And we need to design new algorithms and different algorithms in order to harness the power of

59
00:04:26,720 --> 00:04:33,280
quantum mechanics. And we can do it for certain tasks and we can provide algorithms that run

60
00:04:33,280 --> 00:04:37,760
much faster, even exponentially faster sometimes compared to classical computers.

61
00:04:37,760 --> 00:04:39,200
But this is not always the case.

62
00:04:39,200 --> 00:04:44,640
For many tasks, a classical computer will be as good as a quantum computer.

63
00:04:44,640 --> 00:04:49,520
So we need to figure out exactly for what applications a quantum computer can offer

64
00:04:49,520 --> 00:04:54,880
something more, whether it is about speed up, speed or speed or performance.

65
00:04:54,880 --> 00:04:59,200
And to do that, we need to look at the algorithms and design algorithms.

66
00:05:04,800 --> 00:05:11,440
In what ways has the way that we think about the computing itself,

67
00:05:11,440 --> 00:05:16,960
a quantum computing change over these 27, you know, both years?

68
00:05:16,960 --> 00:05:24,960
As it just been advancing, you know, getting more qubits or has the fundamental architecture

69
00:05:24,960 --> 00:05:27,440
or approaches of quantum computing change?

70
00:05:29,120 --> 00:05:34,000
Yeah. So I guess in the beginning, there were two quite disjoint communities that were working

71
00:05:34,000 --> 00:05:38,880
on quantum information. I come from the theoretical computer science part.

72
00:05:38,880 --> 00:05:44,720
And there, quantum computing was something which it was completely theoretical.

73
00:05:44,720 --> 00:05:50,560
It was part of like the theoretical computer science groups as in Berkeley or in other places.

74
00:05:51,200 --> 00:05:57,840
And it was just a model of computation that we were trying to understand what we can do with it.

75
00:05:57,840 --> 00:06:00,960
It was something completely mathematical and abstract.

76
00:06:00,960 --> 00:06:06,320
We had no idea if it would ever, you know, realize exactly.

77
00:06:06,880 --> 00:06:11,040
But in some sense, we were almost mathematicians so we didn't care about these things, right?

78
00:06:11,040 --> 00:06:15,840
It was a great thing to do math and we were proving theorems, right?

79
00:06:15,840 --> 00:06:23,200
At the same time, physicists were trying to actually implement and figure out ways to control

80
00:06:23,200 --> 00:06:29,840
these very, very small quantum systems that could be one electron or one photon or one atom, right?

81
00:06:29,840 --> 00:06:34,800
And in order to be able to control these type of systems, you need the very, very precise ways

82
00:06:34,800 --> 00:06:40,960
of doing it. So, and physicists in the beginning were doing this because they wanted to study physics,

83
00:06:40,960 --> 00:06:45,840
right? And understand the laws of physics. And little by little, you know, the two communities

84
00:06:45,840 --> 00:06:49,840
started coming together because on our side, we were saying, look, if we had the quantum

85
00:06:49,840 --> 00:06:54,640
computer, there are all these amazing things that you can do. And I think the physicists realized

86
00:06:54,640 --> 00:07:01,040
that maybe there is more to do than study physics. And they started getting into doing circuits

87
00:07:01,040 --> 00:07:09,200
and gates and cubits. And and little by little, we started seeing the first very small quantum

88
00:07:09,200 --> 00:07:15,760
machines. We are quite far still from having a universal fault tolerant quantum computer.

89
00:07:16,320 --> 00:07:20,720
But we have seen many strides when it comes to the hardware development.

90
00:07:21,360 --> 00:07:26,000
We've seen Google's announcement a few months ago about the supremacy experiment.

91
00:07:26,000 --> 00:07:32,480
Many other technologies are, you know, they are not only for superconducting cubits,

92
00:07:32,480 --> 00:07:40,000
but with trapped ions and other technologies. And we also, I think, have many advances when

93
00:07:40,000 --> 00:07:46,400
it comes to thinking algorithmically about this model of computing. There will be many new algorithms.

94
00:07:46,880 --> 00:07:52,080
And quantum machine learning is something that started quite later. So, I think that the result

95
00:07:52,080 --> 00:08:00,720
that kind of initiated this, the subfield was the quantum algorithm for solving linear systems.

96
00:08:00,720 --> 00:08:08,080
And that was sometime in 2009. So, I could say that quantum machine learning started maybe very,

97
00:08:08,080 --> 00:08:17,040
you know, sporadically from 2009. And then we had kind of the first end-to-end application or

98
00:08:17,040 --> 00:08:23,920
quantum recommendation system in a theoretical paper sometime in 2016. So, between five and ten

99
00:08:23,920 --> 00:08:31,760
years of quantum machine learning. And it was at the quantum recommendation algorithm that

100
00:08:32,640 --> 00:08:41,680
that Ewan Tang did that research about. We did a show with Ewan just about a year ago April

101
00:08:41,680 --> 00:08:48,960
of last year. Some of that research. Yeah. So, yes, it was exactly this quantum recommendation

102
00:08:48,960 --> 00:08:56,800
system. It's a very interesting story, the sense that, you know, I was getting to know this new

103
00:08:56,800 --> 00:09:01,760
quantum techniques that had to do with linear algebra. And because I had worked on classical

104
00:09:01,760 --> 00:09:08,000
recommendation systems back in 2001, I kind of figured out that maybe this is the right problem

105
00:09:08,000 --> 00:09:14,080
to actually look into and try to find the quantum algorithm. So, we started working with

106
00:09:14,080 --> 00:09:22,560
Michael Thor and Pamprakash. And we came up with this quantum application. And in the beginning,

107
00:09:22,560 --> 00:09:28,000
we, you know, we benchmarked our result compared to the best classical results that existed out

108
00:09:28,000 --> 00:09:34,240
there. And we could say that there was an exponential gap between our quantum algorithm and the

109
00:09:34,240 --> 00:09:42,320
classical algorithm. And then probably Ewan, you know, said more about this, she started working

110
00:09:42,320 --> 00:09:47,040
on this problem in the beginning in order to prove that classical algorithm is cannot do better

111
00:09:47,040 --> 00:09:52,240
than the ones we already had. But little by little, she realized that maybe there are some different

112
00:09:52,240 --> 00:09:57,920
techniques inspired by our quantum algorithm that could give you a new classical algorithm,

113
00:09:57,920 --> 00:10:05,600
right? And this is what happened. She came up with a very nice classical result that showed that

114
00:10:05,600 --> 00:10:12,000
at least in theory that the gap, the speed, the speed up that we can expect between the quantum

115
00:10:12,000 --> 00:10:18,320
and the classical algorithm is not any more exponential, but it's polynomial. And of course,

116
00:10:18,320 --> 00:10:24,480
the polynomial in the absolute is less than an exponential gap. But somehow this is not the end

117
00:10:24,480 --> 00:10:30,240
of the story in the sense that this polynomial is actually a very, very large polynomial,

118
00:10:30,240 --> 00:10:35,120
meaning that if you look at instances where you would want to do recommendation systems,

119
00:10:35,120 --> 00:10:41,040
like looking at Amazon or Netflix or, you know, online purchase systems like that,

120
00:10:41,040 --> 00:10:46,080
then actually this polynomial gap is even bigger than the one we had before, which was an exponential

121
00:10:46,080 --> 00:10:52,960
one. But somehow, from a theoretical point of view, it says that the gap that we have cannot be

122
00:10:52,960 --> 00:10:58,400
exponential is only polynomial. But from a practical point of view, it's to be more interesting

123
00:10:58,400 --> 00:11:03,280
when it comes to machine learning because we really need to solve real problems. The gap is even

124
00:11:03,280 --> 00:11:08,400
bigger than it was before. So, so we still have to see how to take this gap and actually

125
00:11:08,400 --> 00:11:14,800
implemented in a quantum computer that we don't have yet. And there will be, you know, slowdowns

126
00:11:14,800 --> 00:11:19,840
because the clock speed of a quantum computer is not as fast as a classical computer and things

127
00:11:19,840 --> 00:11:26,560
like that. But we still have a very decent theoretical gap of something like a million or a

128
00:11:26,560 --> 00:11:31,840
billion times faster, right? And hopefully out of this billion times faster, we can keep something

129
00:11:31,840 --> 00:11:37,120
that makes sense. Even if you say it's a thousand times faster in practice, this would be a really,

130
00:11:37,120 --> 00:11:45,920
really great thing to do. Also speaks to the idea that even if we don't ever get quantum computers

131
00:11:45,920 --> 00:11:51,120
that can run these algorithms, perhaps we learn, you know, from the approaches we're developing

132
00:11:51,120 --> 00:11:57,280
and exploring quantum algorithms and kind of bring some of what we learn back to classical algorithms.

133
00:11:57,280 --> 00:12:03,360
This is absolutely true. And this was a great example on the work of you and on the

134
00:12:03,360 --> 00:12:08,480
recommendation systems. In machine learning, there are more examples like that. For example,

135
00:12:08,480 --> 00:12:13,840
when it comes to neural networks, which is something that we know that it works very well,

136
00:12:13,840 --> 00:12:19,200
but we don't necessarily understand why and why this one doesn't, the other one doesn't somehow.

137
00:12:20,000 --> 00:12:24,480
Like we've been trying to figure out what is the right architectures for defining some quantum

138
00:12:24,480 --> 00:12:31,120
neural networks. And the difference there, for example, I have to do with things that it's not

139
00:12:31,120 --> 00:12:36,720
easy to apply an own linearity because it quantum is a reversible and linear evolution.

140
00:12:36,720 --> 00:12:43,280
So then you have these extra constraints that make you think of different ways of defining

141
00:12:43,840 --> 00:12:48,800
neural networks. And this can also go back to classical neural networks and define new ways

142
00:12:48,800 --> 00:12:54,160
of doing classical neural networks that could use the intuition of the quantum to provide

143
00:12:54,160 --> 00:12:59,440
classical neural networks that can be more accurate or more efficient. So there is a lot of

144
00:12:59,440 --> 00:13:03,680
back and forth between classical and quantum information. And this is why we need to work more

145
00:13:03,680 --> 00:13:10,080
closely with the classical ML community as well. Maybe we can take a step back and have you

146
00:13:12,160 --> 00:13:20,240
ground us on quantum computing and what are the fundamental ideas there? I'll be honest,

147
00:13:20,240 --> 00:13:28,640
I've heard it numerous times. It's still difficult to wrap my head around, supervisition and

148
00:13:28,640 --> 00:13:36,240
similar concepts. And so maybe you can start by talking about what you think are the key ideas

149
00:13:36,240 --> 00:13:45,680
for folks that are coming from classical approaches. So yeah, it's not an easy task to do to explain

150
00:13:45,680 --> 00:13:54,560
quantum in a live, but I will do my best. I guess the first comment is to say that one does not need

151
00:13:54,560 --> 00:13:59,120
to really understand quantum mechanics and all the postulates of physics and quantum mechanics

152
00:13:59,120 --> 00:14:03,600
in order to start playing around with quantum algorithms and quantum information.

153
00:14:03,600 --> 00:14:09,120
So for us, it's mostly a mathematical model that we need to understand, which is a little

154
00:14:09,120 --> 00:14:14,080
different than the one that we use for classical computing. And once you have this mathematical

155
00:14:14,080 --> 00:14:20,000
model in place, then in some sense the physics part stops and then the algorithmic and the

156
00:14:20,000 --> 00:14:25,360
computer science part starts. So the main difference between classical and quantum computing,

157
00:14:25,360 --> 00:14:30,800
people probably have heard this many times, is that the carrier of information is not a bit,

158
00:14:30,800 --> 00:14:36,320
which is a zero or a one, but it's a quantum bit, which can be a zero and one at the same time.

159
00:14:37,760 --> 00:14:41,840
It's not very different than saying that I have a random bit, that sometimes it's zero,

160
00:14:41,840 --> 00:14:50,640
sometimes it's one. So you can define the space of this random bit with two probabilities,

161
00:14:50,640 --> 00:14:57,360
the probability of getting zero and the probability of getting one. So you can associate in a random

162
00:14:57,360 --> 00:15:03,280
bit a two-dimensional vector. It's the same thing that quantum, you can associate in a quantum bit

163
00:15:03,280 --> 00:15:09,520
a two-dimensional vector, but now this vector doesn't have positive probabilities that sum up to one.

164
00:15:09,520 --> 00:15:16,320
It has complex numbers whose squares sum up to one. In other words, if you have a quantum

165
00:15:16,320 --> 00:15:22,480
system, you can think of a quantum system as a vector in some very high-dimensional

166
00:15:22,480 --> 00:15:29,200
Euclidean space. The same way that if you have a random variable, which is more than one bit,

167
00:15:29,200 --> 00:15:37,600
you can think of a distribution, which is in an exponential space and you have positive

168
00:15:37,600 --> 00:15:43,440
probabilities that sum up to one. Here, the quantum system is defined by, again, an exponential

169
00:15:43,440 --> 00:15:50,080
size vector with positive or negative numbers, actually even complex numbers whose squares sum up

170
00:15:50,080 --> 00:15:57,760
to one. So this is the basic difference, right, of how do you encode information and what is the

171
00:15:57,760 --> 00:16:03,840
carrier of information in quantum? And once you have this, then you want to understand how can

172
00:16:03,840 --> 00:16:08,080
they evolve the system? So I have a quantum system. What kind of operations can they apply to

173
00:16:08,080 --> 00:16:17,120
this quantum system to evolve it? And it's very simple in some sense because you want to evolve

174
00:16:17,120 --> 00:16:22,960
a quantum system in a way that the quantum system remains a quantum system. So since the quantum

175
00:16:22,960 --> 00:16:30,400
system was this vector in Hilbert space with some fixed norm, let's say norm one, then the only

176
00:16:30,400 --> 00:16:35,360
kind of operations that you can apply to it is some unitary operation because this is the

177
00:16:35,360 --> 00:16:41,120
operation that keeps the norm of the vector the same, right? The same way that we're saying,

178
00:16:41,120 --> 00:16:46,640
if I have a probability distribution, how can I evolve it? I can apply a stochastic matrix

179
00:16:46,640 --> 00:16:51,440
because it keeps probability distribution, it will give you probability distribution.

180
00:16:51,440 --> 00:16:56,320
Here you have a quantum state, you apply a unitary matrix, you're going to get a different quantum

181
00:16:56,320 --> 00:17:02,960
state, that's still a quantum state, right? So this is easily the way of applying operations on

182
00:17:02,960 --> 00:17:08,400
quantum systems. The quantum system is a vector, you apply a unitary matrix, you get a different vector,

183
00:17:09,200 --> 00:17:14,000
right? And the second thing, and this is also very important that you can do to this quantum

184
00:17:14,000 --> 00:17:20,560
system is that you can try to observe it, right? But kind of what we know from the postulate

185
00:17:20,560 --> 00:17:25,760
of quantum mechanics is that when you observe something, this is not just the passive observation

186
00:17:25,760 --> 00:17:30,880
that doesn't change anything, but the moment you try to observe the system, the state of the

187
00:17:30,880 --> 00:17:39,280
system changes itself, right? So the quantum measurement just specifies what will be the outcomes

188
00:17:39,280 --> 00:17:44,320
that you're going to get, the possible outcomes that you're going to get out of this observation,

189
00:17:44,320 --> 00:17:50,240
and with what probability each outcome will come up, right? So when you measure something,

190
00:17:50,240 --> 00:17:56,080
then you'll have a distribution over possible classical outcomes that come from this quantum

191
00:17:56,080 --> 00:18:02,720
state that you had before. So I know it's a mouthful, but basically if we understand this,

192
00:18:02,720 --> 00:18:08,320
this the notion of a tube bit, and the fact that I can apply a unitary operation to it,

193
00:18:08,320 --> 00:18:14,160
or I can measure it and observe some classical information out of it, then we basically have

194
00:18:15,280 --> 00:18:18,560
the basics that we need in order to start talking about quantum algorithms.

195
00:18:18,560 --> 00:18:27,280
And maybe the one simple quantum procedure that I can discuss and it's quite interesting for

196
00:18:27,280 --> 00:18:34,240
machine learning is the one that has to do with estimating the distance between quantum states,

197
00:18:34,240 --> 00:18:42,080
right? So you can think of two quantum states as encodings of two data points, right? And one

198
00:18:42,080 --> 00:18:46,560
of the things that we need to do many times in machine learning is figure out the similarity

199
00:18:46,560 --> 00:18:51,280
between these two points, right? So you want to estimate something like the inner product

200
00:18:51,280 --> 00:18:58,640
between these two points. As long as these data points are now encoded into this quantum state,

201
00:18:59,200 --> 00:19:07,200
it's fairly efficient to compute the distance or the inner product between these data points.

202
00:19:07,840 --> 00:19:12,880
And this is one of the things that we use in order to do, for example, classification or

203
00:19:12,880 --> 00:19:21,920
clustering based on similarity learning. I can talk maybe about one more, a little more elaborate

204
00:19:21,920 --> 00:19:27,440
thing that we can do without getting into many of the details, which is linear algebra procedures.

205
00:19:28,080 --> 00:19:33,200
And again, I'm talking about this because machine learning is a lot of, you know, linear algebra,

206
00:19:33,200 --> 00:19:40,160
whether you want to train neural networks, but also when you want to do more traditional machine

207
00:19:40,160 --> 00:19:46,320
learning like principal component analysis or support vector machines, they're always these

208
00:19:46,320 --> 00:19:51,200
matrices that you have to handle. You need to figure out, you know, top eigen spaces,

209
00:19:51,200 --> 00:19:59,600
eigenvectors and things like that. So quantum is particularly powerful in performing this type

210
00:19:59,600 --> 00:20:04,560
of computations that have to do with eigenvalues and eigenvectors in a more efficient way.

211
00:20:04,560 --> 00:20:10,720
These tools are very powerful, but they're also very subtle. And this is where a lot of the hype

212
00:20:11,520 --> 00:20:17,520
comes also when it comes to quantum machine learning. These procedures are not always faster,

213
00:20:18,080 --> 00:20:22,640
but they can be faster if we really pay attention and we apply them to the right

214
00:20:23,200 --> 00:20:29,120
applications and do the right use cases. And when we do that, this is how we can get very fast

215
00:20:29,120 --> 00:20:35,280
to accommodation systems faster both than the classical or the quantumly inspired classical

216
00:20:35,280 --> 00:20:40,640
still but much faster when we applied to things like spectral clustering or expectation

217
00:20:40,640 --> 00:20:49,680
maximization, things where this linear algebra part is really the bottleneck of the computation.

218
00:20:51,040 --> 00:20:56,800
So you alluded to this earlier and I think it's really coming out in this conversation around the

219
00:20:56,800 --> 00:21:04,080
algorithms, the speed up that we're talking about here doesn't come from just running it on

220
00:21:04,080 --> 00:21:09,520
a super fast hardware that actually doesn't really exist. It's because there's something fundamental

221
00:21:09,520 --> 00:21:18,400
about the way we can work with qubits that we can't do bits. And I'm thinking of it as a non-linearity

222
00:21:18,400 --> 00:21:22,160
that's probably not the right way to think about it, but you can do the different things.

223
00:21:22,160 --> 00:21:26,240
Can you help us get to the essence of what these different things are that we can do that make

224
00:21:26,240 --> 00:21:33,680
better? Sure. I will do my best. Again, you know, quantum mechanics is not the easiest thing

225
00:21:33,680 --> 00:21:37,200
to get into issue about and there are many things that we don't really understand.

226
00:21:39,600 --> 00:21:50,160
But I think the main idea of why you would expect a quantum algorithm to be faster, for example,

227
00:21:50,160 --> 00:21:56,560
than a classical algorithm, is because we can utilize this notion of superposition. When I talk

228
00:21:56,560 --> 00:22:03,120
about superposition, it means that for example, imagine you want to estimate the distance between

229
00:22:03,120 --> 00:22:10,400
one point and many different points. It could be the centroids that you have calculated from

230
00:22:10,400 --> 00:22:17,520
different classes or some other data points. What we can do is things like, instead of estimating

231
00:22:17,520 --> 00:22:23,120
the distance of the point with each one of the data points one after the other, so we have to

232
00:22:23,120 --> 00:22:29,520
spend a lot of time if the number of these points is great, right? We can kind of go into a super

233
00:22:29,520 --> 00:22:36,480
position of the data points and kind of start estimating fast things that have to do with the

234
00:22:36,480 --> 00:22:45,760
average distance or things like that. So I would maybe say that it's some sort of parallelism

235
00:22:45,760 --> 00:22:51,200
that is happening at some point, but we should not think of it as a parallel computer either.

236
00:22:52,160 --> 00:22:55,840
So it's not a nonlinear computer and it's not a parallel computer.

237
00:22:57,120 --> 00:23:03,040
And it's certainly not just a faster processor, right? So I usually get this question,

238
00:23:03,760 --> 00:23:08,320
shouldn't be just simpler if we just get a compiler that will take my classical algorithm,

239
00:23:08,320 --> 00:23:12,960
make it into a quantum algorithm and then I don't have to learn anything new and I will be done.

240
00:23:12,960 --> 00:23:21,280
It's not that easy. It's not that easy because you really have to understand mentally. It's just not

241
00:23:21,280 --> 00:23:30,240
that easy. It's fundamentally different. You can use this parallelism, the super position.

242
00:23:30,240 --> 00:23:34,880
At the same time, as we said, every time you try to extract information out of these things,

243
00:23:35,440 --> 00:23:41,840
you end up destroying your states and you get only a small part of the information. So there's

244
00:23:41,840 --> 00:23:49,120
this interplay between using this big Hilbert space to encode information, but figuring out very

245
00:23:49,120 --> 00:23:53,760
clever ways of extracting the information that you need and not care about the rest.

246
00:23:54,560 --> 00:24:00,080
So I guess this is the best way I could explain it in a couple of minutes.

247
00:24:00,720 --> 00:24:08,480
What I'm hearing is something, the picture that's forming, that's probably inaccurate, is

248
00:24:08,480 --> 00:24:17,920
something along the lines of classical computing. We do a lot of iterative types of computation

249
00:24:18,560 --> 00:24:26,000
and there is an element of these properties of quantum super position and observation that

250
00:24:26,720 --> 00:24:35,200
allows us to look at a quantum data structure and get a lot of what we might otherwise have to

251
00:24:35,200 --> 00:24:40,640
iterate in classical computing. Is there any of that that is true?

252
00:24:46,880 --> 00:25:00,880
Yes, I know. No, no, it's true that I think a different way of looking at it could be that

253
00:25:00,880 --> 00:25:08,160
what the quantum computer enables you to do is kind of search many different computational paths

254
00:25:08,160 --> 00:25:15,200
at the same time, but this is not enough because then if you just say, let me search all of them in

255
00:25:15,200 --> 00:25:20,400
super position and then let me measure, then what you get is just a random path, which you could

256
00:25:20,400 --> 00:25:26,560
have done it also by a randomized classical algorithm, right? So what you need to do is first of all

257
00:25:26,560 --> 00:25:31,920
go into a super position of these paths, but then figure out clever ways of just

258
00:25:32,800 --> 00:25:38,480
getting rid of the paths that you don't like so that at the end, only the good paths that you

259
00:25:38,480 --> 00:25:43,600
that will lead to a solution remain as possibilities of the quantum algorithm.

260
00:25:44,720 --> 00:25:52,000
So there is this inherent stochasticity in quantum, which is also in randomized algorithm.

261
00:25:52,000 --> 00:25:57,520
The next thing is that because as I said, we don't deal only with probabilities, but also with

262
00:25:57,520 --> 00:26:04,160
this positive, negative, complex amplitudes, there is a lot of interference between these paths,

263
00:26:04,160 --> 00:26:09,840
and if you are clever enough, you will interfere the back paths and you will make them disappear,

264
00:26:09,840 --> 00:26:14,800
and you will only get good paths that lead to solutions at the end of your algorithm.

265
00:26:14,800 --> 00:26:27,840
Okay. And so you've got these kind of principles like being able to do distances and linear algebra,

266
00:26:28,320 --> 00:26:32,640
how does that get us to quantum machine learning algorithms and kind of what's

267
00:26:32,640 --> 00:26:39,120
different, you know, state and landscape of quantum ML? Yeah, it's a very nice question.

268
00:26:39,120 --> 00:26:45,280
So I can start maybe from the simplest thing, so we can start with supervised learning and,

269
00:26:45,280 --> 00:26:50,640
you know, the first thing that one might want to do would be some sort of classification, right? So

270
00:26:52,160 --> 00:26:55,840
again, there are two different ways of, okay, there are many different ways of trying to do

271
00:26:55,840 --> 00:27:03,040
classification. One of them is based on similarity learning, right? So you somehow map your data

272
00:27:03,040 --> 00:27:08,160
points into some points in space, and then you try to figure out which points are close to each

273
00:27:08,160 --> 00:27:12,800
other so that you can give them one label and which other points are far and close so you can

274
00:27:12,800 --> 00:27:16,800
give them a different label, right? So this is things that kind of everyone knows in classical

275
00:27:16,800 --> 00:27:21,600
machine learning, we are, you know, getting to understand more and more what's happening.

276
00:27:21,600 --> 00:27:26,160
And again, there one thing, the simplest thing you can do is to say that, okay, every time you want

277
00:27:26,160 --> 00:27:32,000
to estimate the distance, why don't you use a quantum computer to estimate the distance, right?

278
00:27:32,000 --> 00:27:37,040
But you can go a little bit more in the more quantum versions of these things, where you say,

279
00:27:37,040 --> 00:27:43,520
okay, but maybe not only do I want to estimate each distance separately, but imagine that I

280
00:27:43,520 --> 00:27:50,240
want to, I have a point, I have a bunch of different centroids, I want to soft classify my point,

281
00:27:50,240 --> 00:27:58,800
depending on which centroid is closer to me, right? So different things you can do is not do it one

282
00:27:58,800 --> 00:28:04,720
by one sequentially, but again, go to the superposition of the centroid, and this will allow you to sample

283
00:28:04,720 --> 00:28:09,360
the right centroid with the correct probabilities, for example. So these are kind of the things that

284
00:28:09,360 --> 00:28:14,480
you can do in the simplest problems, like you already have your point and you want to classify them

285
00:28:14,480 --> 00:28:22,800
in the space that you are, right? If you want to go to a next level, we can say that many times

286
00:28:23,760 --> 00:28:29,360
before being able to classify your points, you need to pre-process your data and map them from

287
00:28:29,360 --> 00:28:35,280
one space to a different space, for example, through some dimensionality reduction techniques. We can

288
00:28:35,280 --> 00:28:40,960
think of principle component analysis or feature analysis, linear discriminant and all this type of

289
00:28:41,840 --> 00:28:47,360
very powerful classical techniques. And the reason they are very powerful is also because they

290
00:28:47,360 --> 00:28:52,720
are computationally hard to do, right? Because you need to figure out the eigenvectors and the

291
00:28:52,720 --> 00:28:59,760
eigenvalues of your of your data matrices. And there we can use more powerful and elaborate

292
00:28:59,760 --> 00:29:06,720
quantum procedures that have to do with inverted matrices, finding eigenvectors. So this mapping

293
00:29:06,720 --> 00:29:14,960
from a higher dimensional space of your data points to a smaller dimensional space where you

294
00:29:14,960 --> 00:29:20,320
believe that the classification will be good. For example, by looking at your top eigenspace,

295
00:29:20,320 --> 00:29:26,960
then this mapping can also be done in a quantum way. And this is usually most of the times the

296
00:29:26,960 --> 00:29:33,040
bottleneck of the classification, how to find the right space to put your points.

297
00:29:35,360 --> 00:29:40,480
We can also try to define quantum deep learning and quantum neural networks.

298
00:29:40,480 --> 00:29:48,240
Before we go. So you started talking about quantum with quantum supervised learning.

299
00:29:48,240 --> 00:29:57,520
You know, we've you've got this the way you described it was used quantum computing for

300
00:29:58,560 --> 00:30:05,840
the distance part of that task as opposed to the entire task. Is that typical of a quantum

301
00:30:05,840 --> 00:30:10,320
approach that you kind of cherry pick a particular part that's hard classically and apply

302
00:30:10,320 --> 00:30:18,880
quantum to it as opposed to end and quantum. Both both cases are possible, right? For example,

303
00:30:19,520 --> 00:30:26,000
if you have an algorithm like I give you a number and you need to factor the number into two prime

304
00:30:26,000 --> 00:30:31,200
factors, then the entire algorithm is equal to algorithm. And because the entire algorithm is

305
00:30:31,200 --> 00:30:35,760
a quantum algorithm, this is why we need like millions and millions of tubes to actually implement

306
00:30:35,760 --> 00:30:46,640
this algorithm, right? There are more hybrid algorithms where there is a classical outer algorithm

307
00:30:46,640 --> 00:30:54,720
and then at the specific points, one can do a specific part of the computation on on a quantum

308
00:30:54,720 --> 00:31:00,720
computer, get back a result and continue with the classical program, the classical algorithm,

309
00:31:00,720 --> 00:31:05,600
right? For example, you already get ready to say that when I was discussing about something like

310
00:31:05,600 --> 00:31:11,760
near a central classification, many parts will still happen classically because quantum cannot

311
00:31:11,760 --> 00:31:18,720
offer something to that, right? Figuring out the centroids of a labeled set of data, it's pretty

312
00:31:18,720 --> 00:31:24,160
simple, it's never the bottleneck classically, so let's do it classically, right? Then at some

313
00:31:24,160 --> 00:31:29,280
points, when you have to find distances or you have to start projecting data points to different

314
00:31:29,280 --> 00:31:35,040
spaces, then for the specific task, we can use a quantum computer and then we can, you know,

315
00:31:35,040 --> 00:31:40,400
continue within our classical algorithm. So for the way we do it, that you see where, for example,

316
00:31:40,400 --> 00:31:46,960
when we say we have a quantum nearest centroid algorithm, what it looks to someone who wants to

317
00:31:46,960 --> 00:31:53,200
use it is basically the same that using psychic learn to do classical nearest centroids,

318
00:31:53,200 --> 00:31:58,080
there is a Python environment, you have a notebook and you run it, so this is exactly what

319
00:31:58,080 --> 00:32:05,840
the quantum thing feels like, you just say instead of run your classical nearest centroid and

320
00:32:05,840 --> 00:32:12,080
fit your data and predict it, you say fit and predict your data with the quantum nearest

321
00:32:12,080 --> 00:32:16,880
centroid. And what happens at the back is that there is a classical program that at some point

322
00:32:16,880 --> 00:32:22,320
says send this data to your quantum computer, measure your quantum computer, get some classical

323
00:32:22,320 --> 00:32:29,440
data out and continue with your classical computation. Another question that this raises for me is,

324
00:32:29,440 --> 00:32:34,160
I think of, you know, when you've got these quantum algorithms operating in the

325
00:32:35,680 --> 00:32:42,880
context of classical approaches and classic problems and kind of regular data,

326
00:32:45,840 --> 00:32:51,200
I guess there was a part of the way I was thinking about this that wanted the classical algorithms

327
00:32:51,200 --> 00:32:57,760
to be operating on classical data, like really complex data with these, you know, weird states

328
00:32:57,760 --> 00:33:03,920
and, you know, crazy tensions and things like that. You know, what's the relationship between

329
00:33:03,920 --> 00:33:10,160
the data and the quantum algorithms and do you have to do something to data to make it

330
00:33:10,160 --> 00:33:13,440
operable in a quantum environment? It doesn't have to be more complex.

331
00:33:13,440 --> 00:33:21,120
It's a very good question. You're really pinpointing on one of the very important

332
00:33:21,120 --> 00:33:26,720
subtle points for quantum machine learning. And this is what do we do with classical data

333
00:33:26,720 --> 00:33:33,200
and how do we load classical data into a quantum format, which is the one that we need in order

334
00:33:33,200 --> 00:33:38,400
to run our quantum algorithms, right? So let me try to explain a little bit what this is.

335
00:33:38,400 --> 00:33:44,560
So you have some classical data, you have some classical description of a point in some

336
00:33:44,560 --> 00:33:52,320
end-dimensional space, right? What we would need in order to apply some fast, for example,

337
00:33:52,320 --> 00:33:56,960
quantum linear algebra techniques or estimating the distances and things like that,

338
00:33:56,960 --> 00:34:03,520
we need some quantum state that in some sense encodes this classical data point into this quantum

339
00:34:03,520 --> 00:34:11,920
state, okay? And this is not a priori trivial thing to do. We are asking to create a quantum state

340
00:34:11,920 --> 00:34:19,840
out of a classical description of classical data. So there are many different ways of approaching

341
00:34:22,000 --> 00:34:30,240
this task. One is to say that we will need to develop some special type hardware,

342
00:34:30,240 --> 00:34:36,720
like your classical RAM that you have on your computer, that you can say, okay, just load the

343
00:34:37,760 --> 00:34:45,040
data that exists in position 35, then your RAM goes to position 35 and brings back this data.

344
00:34:45,040 --> 00:34:49,680
You don't have, you know, as it used to be with tapes and this starts from the beginning and,

345
00:34:50,320 --> 00:34:55,440
go all the way to the point that you want. So you have this fast access to the data, right?

346
00:34:55,440 --> 00:35:01,840
What we would need for a quantum computer is also to have some sort of fast quantum access

347
00:35:01,840 --> 00:35:09,680
to this data, right? So there were a few proposals that had to do with some kind of more exotic

348
00:35:09,680 --> 00:35:17,120
technological things to do it on hardware, right? What we try to do with QCware and I've been working

349
00:35:17,120 --> 00:35:23,600
on this for quite some time now is to figure out can we efficiently load classical data with

350
00:35:23,600 --> 00:35:28,960
current quantum technology? Like I don't want to use anything that doesn't exist. I want to use

351
00:35:28,960 --> 00:35:36,240
machines that Google or IBM is coming up with, right? And can I use these types of machines to load

352
00:35:36,240 --> 00:35:41,840
classical data on quantum states? And what we found is kind of the optimal ways of doing it.

353
00:35:41,840 --> 00:35:49,600
So the optimal way says that the circuit that you need in order to load a data point that has

354
00:35:49,600 --> 00:35:56,160
and features must have size n otherwise you're missing your data, right? But you can have it

355
00:35:56,160 --> 00:36:00,880
in a very, very shallow way. So the depth of the circuit that kind of corresponds to the time

356
00:36:01,520 --> 00:36:08,000
that it will take for the quantum circuit to actually apply this operation is only logarithmic

357
00:36:08,000 --> 00:36:15,680
on the dimension of the data. Which means that I need these two bits which have optimal size

358
00:36:15,680 --> 00:36:22,480
and I can make them very, very shallow, which means very, very fast. Okay? And this is one of the

359
00:36:22,480 --> 00:36:30,160
ways, one of the bottlenecks that we managed to get over with. And this is why if someone told me

360
00:36:30,160 --> 00:36:36,400
maybe three years ago how far is real quantum machine learning applications? I would have predicted

361
00:36:36,400 --> 00:36:40,400
something which would have been further down the road than I could say now.

362
00:36:40,400 --> 00:36:56,560
It sounds like the the process of applying quantum algorithms then if it assumes that you are

363
00:36:56,560 --> 00:37:05,360
accessing quantum data and that quantum data was originally real world data points say some

364
00:37:05,360 --> 00:37:15,680
time series data points or something. Does the process of using quantum algorithms inject some

365
00:37:15,680 --> 00:37:22,320
noise, you know, noise in a sense of, you know, these kind of, you know, static data points are

366
00:37:22,320 --> 00:37:32,320
projected into some probabilistic quantum space. And, you know, is that a disadvantage that

367
00:37:32,320 --> 00:37:37,840
quantum has to overcome in order to be useful on? I don't know, what is not, is there a set of,

368
00:37:37,840 --> 00:37:43,760
I guess you could apply quantum to physics problems and you've got this inherently quantum

369
00:37:44,560 --> 00:37:55,360
source, but for everything else. Yeah, very good question. So we will have to deal with noise

370
00:37:55,360 --> 00:38:01,440
in the quantum setting. And we do have to deal with noise first because the computers that we have

371
00:38:01,440 --> 00:38:09,200
now that we will have in a few years will be noisy, meaning that when I have a qubit and I tell my

372
00:38:09,200 --> 00:38:14,880
qubit just go to the state, the qubit doesn't really go to this state but to a state close to the state.

373
00:38:17,440 --> 00:38:22,640
And this is because it's extremely difficult to really control quantum systems at that level

374
00:38:22,640 --> 00:38:29,680
of precision. So there will be noise and this is why we call the error that we are now and we

375
00:38:29,680 --> 00:38:35,360
will be in the next few years, the NISC error for noise intermediate scale quantum machines.

376
00:38:37,120 --> 00:38:43,520
And now it's so fresky who coined the term, it wasn't me. So

377
00:38:45,840 --> 00:38:52,800
and the question is, is this noise going to kill the quantum machine learning applications or not?

378
00:38:52,800 --> 00:39:00,400
Right. I'm quite optimistic and I will tell you why I'm quite optimistic because

379
00:39:02,880 --> 00:39:08,480
the data that we have even for classical machine learning are already very noisy.

380
00:39:08,960 --> 00:39:14,640
Right. The whole point of machine learning is try to extract the signals out of very noisy data.

381
00:39:15,040 --> 00:39:21,280
Right. So if I tell you that you have a bunch of data and then I perturb a little bit the data

382
00:39:21,280 --> 00:39:27,440
all of them. Right. If I have you know cats here and dogs there and I perturb all the cats and the dogs

383
00:39:28,080 --> 00:39:33,440
you expect your machine learning algorithm to still be able to discern the cat from a dog.

384
00:39:34,000 --> 00:39:39,680
Right. Because already the data that you learned on, for example, were very noisy and very

385
00:39:39,680 --> 00:39:46,240
fuzzy images of cats and dogs. Right. So somehow there is already noise on the data.

386
00:39:46,240 --> 00:39:52,240
Okay. So for the quantum case, there will be noise on the data in the sense that if you give me a quantum

387
00:39:52,240 --> 00:39:58,080
a classical data point, the quantum state that I will construct will be close to the correct point,

388
00:39:58,080 --> 00:40:04,560
but not exactly. And then quantum will add even more noise when I try to estimate, for example,

389
00:40:04,560 --> 00:40:11,200
the inner product or the distance between two points, that computation will also have a little bit

390
00:40:11,200 --> 00:40:16,640
of noise there. But again, this is something that we even do in classical machine learning.

391
00:40:17,200 --> 00:40:22,640
For example, when we are training neural networks, many times we inject artificial noise

392
00:40:22,640 --> 00:40:28,400
on the computation because we want a neural network to be robust. Right. We wanted to be robust.

393
00:40:28,400 --> 00:40:39,040
We want to to be robust against both adversarial adversaries, but also as a way to increase the

394
00:40:39,040 --> 00:40:45,440
privacy of the data. So this is also something very interesting that I'm quite interested recently,

395
00:40:45,440 --> 00:40:55,920
is that one, the main way maybe to deal with privacy in machine learning is again to make your

396
00:40:55,920 --> 00:41:00,480
data a little bit more noisy or the computation a little bit more noisy. In the sense that

397
00:41:01,040 --> 00:41:07,760
it's enough noise to hide specifics of the data. Right. But still you can extract the useful

398
00:41:07,760 --> 00:41:12,880
information that you need in order to solve your problem. So somehow this is how the quantum

399
00:41:12,880 --> 00:41:18,320
thing will do, not because you want it to do it, but because it will do it by itself. Right. So

400
00:41:19,200 --> 00:41:22,960
privacy or something like that. Yes, it's inherently private.

401
00:41:24,480 --> 00:41:33,280
And so is the idea of understanding the way that noise

402
00:41:33,280 --> 00:41:42,880
is injected from kind of classical round to quantum round. Is this

403
00:41:44,880 --> 00:41:50,400
kind of a pedestrian thing that is assumed and no one cares about? Or is this like a research

404
00:41:50,400 --> 00:41:57,440
topic that people are working towards an information theory of quantum or something like that?

405
00:41:57,440 --> 00:42:05,120
Yeah. No, it's very important. It's very important. For example, I will just give you a small

406
00:42:05,120 --> 00:42:13,680
example. Right. And we were, we started working on unsupervised learning and we started from,

407
00:42:13,680 --> 00:42:18,560
you know, clustering 101. So we wanted to find a quantum analog of k-means.

408
00:42:18,560 --> 00:42:28,080
Right. And we figured out what the algorithm should look like. But as I said, every time we will

409
00:42:28,080 --> 00:42:34,960
be doing a quantum procedure, we were adding noise to the computation. Right. So what we had to do

410
00:42:34,960 --> 00:42:40,960
is to go back to the classical algorithm and say, okay, even classically, if I start adding noise

411
00:42:40,960 --> 00:42:46,800
now and we know what type of noise we were adding, would the clustering still be good or not?

412
00:42:46,800 --> 00:42:55,200
Right. What we did is that we did extensive simulations on real data sets like M-list and

413
00:42:55,200 --> 00:43:00,560
Iris and many different, you know, canonical. Let's call them data set where people actually

414
00:43:00,560 --> 00:43:08,080
not for clustering. There were more, you know, synthetic data in other runs. But we looked at what

415
00:43:08,080 --> 00:43:15,280
happens when your classical k-means algorithm has noise in it. Right. We defined a new

416
00:43:15,280 --> 00:43:22,080
classical clustering algorithm. And what we found out is that, obviously, as long as your noise is

417
00:43:22,080 --> 00:43:29,600
not enormous where everything becomes noise. Right. Your clustering doesn't lose anything from

418
00:43:29,600 --> 00:43:36,240
from the accuracy for decent amounts of noise. Right. Even for decent amount of noise,

419
00:43:36,240 --> 00:43:42,400
you still have very good clustering. Okay. Of course, you can find data that will destroy your

420
00:43:42,400 --> 00:43:47,680
algorithm. Right. But when you test it on the data that you expect to run your clustering algorithm

421
00:43:47,680 --> 00:43:55,040
on, you can handle a lot of noise. And this is what also enabled us. And this is some results that

422
00:43:55,040 --> 00:44:02,480
we published last year at Newribs to figure out both what we expect from the quantum algorithm to

423
00:44:02,480 --> 00:44:10,320
give as accuracy. And also how much faster it will actually be because the runtime of the quantum

424
00:44:10,320 --> 00:44:15,920
algorithm depends on how precise or how noisy you can, you want your computation. Right. The more

425
00:44:15,920 --> 00:44:21,280
precise you need the computation the more time you have to spend. But the fact that even having

426
00:44:21,280 --> 00:44:28,480
quite big noise, the accuracy did not suffer. We could say that the running time of the quantum

427
00:44:28,480 --> 00:44:34,640
algorithm when you have a bigger computer would be much faster than the classical k-means algorithm.

428
00:44:34,640 --> 00:44:42,080
And so with those results that were only accessible to you via simulation in comparison or is there

429
00:44:42,080 --> 00:44:50,480
some theoretical framework that says under set of conditions, we know that the noise will be decent

430
00:44:50,480 --> 00:44:59,040
in converting this over to quantum. Yeah. So yeah, I come from a computer science background.

431
00:44:59,040 --> 00:45:06,080
So obviously we have to prove the theorems. Right. And we proved exactly the trade-off between

432
00:45:06,880 --> 00:45:14,320
how much error you can have in the computation. How much running time, how long you have to run.

433
00:45:15,600 --> 00:45:22,960
And then by simulations, we figured out the errors that you can handle and still have good

434
00:45:22,960 --> 00:45:27,920
accuracy. And for that error, we went back and we said, okay, so what is the running time

435
00:45:27,920 --> 00:45:35,440
and how fast will the algorithm be? And this we took from asymptotic theoretical analysis of how

436
00:45:35,440 --> 00:45:41,760
the k-means algorithm, the quantum k-means algorithm works. Okay. Yeah. Cool. So you're about to

437
00:45:43,680 --> 00:45:52,000
speak about quantum neural networks. Yes, quantum neural networks is a very intriguing to me

438
00:45:52,000 --> 00:45:57,520
because we are in a very bizarre situation. And I think maybe in classical machine learning,

439
00:45:57,520 --> 00:46:04,640
people were in a similar situation maybe 20 or 30 years ago where we kind of think we have ideas

440
00:46:04,640 --> 00:46:11,280
on what the architectures of quantum neural networks should be, right, to do things like classification.

441
00:46:12,480 --> 00:46:18,080
But we only have like 10 or 20 qubits to try things out and see how they work.

442
00:46:19,200 --> 00:46:24,160
So it's the same as telling you propose to me a neural network that you think can classify

443
00:46:24,160 --> 00:46:31,600
well, but you cannot simulate it. So then you're kind of stuck because you cannot prove many things

444
00:46:31,600 --> 00:46:36,640
for neural networks. The main thing that you do is that you run it and you see that it works.

445
00:46:36,640 --> 00:46:42,240
And if it doesn't work, then you see how to tweak it to make it work, right? So for us, it's very

446
00:46:42,240 --> 00:46:47,840
difficult because we don't have the ability to actually simulate this quantum neural networks

447
00:46:47,840 --> 00:46:53,280
because every time you try to simulate it on a classical computer, there is this exponential

448
00:46:53,280 --> 00:47:00,560
blow-up on the time. So if I have a neural network of 100 qubits, then I need two to the 100

449
00:47:00,560 --> 00:47:07,200
dimension for my classical computer to simulate it. And I don't have 100 qubits to run it on either.

450
00:47:08,000 --> 00:47:17,280
So it's very difficult to find ways of really giving evidence if not proofs of why we would expect

451
00:47:17,280 --> 00:47:23,840
this quantum neural networks to work. So what we did on our side, it's two things. The first

452
00:47:23,840 --> 00:47:30,560
thing we said, okay, let's not define quantum neural networks. Let's go back to classical neural

453
00:47:30,560 --> 00:47:37,760
networks. We know that they work very well. Can I speed up the training on them if I have a quantum

454
00:47:37,760 --> 00:47:43,280
computer on the side? So I'm not going to use a quantum circuit as a quantum neural network,

455
00:47:43,280 --> 00:47:48,160
but I'm going to use the algorithms on a quantum computer to train my classical neural network

456
00:47:48,160 --> 00:47:54,000
faster, right? And again, because there is a lot of linear algebra there, we can also prove that

457
00:47:54,000 --> 00:48:00,640
in some cases there are speed ups that you can get from from quantum training, okay? And the second

458
00:48:00,640 --> 00:48:09,600
case was that we are trying to use the intuition that we have from quantum algorithms

459
00:48:09,600 --> 00:48:15,040
to at least come up with quantum architecture is where we can prove something very simple

460
00:48:15,600 --> 00:48:21,360
that they're not going to be words that the classical ones. Even that is not very clear, right?

461
00:48:22,160 --> 00:48:28,000
So I'm trying to get kind of guarantees that say that if you run this quantum neural network,

462
00:48:28,720 --> 00:48:36,160
at least you have the guarantee that it will run at least as well as an equivalent classical one

463
00:48:36,160 --> 00:48:42,000
and hopefully it will run better, but you know, what can we do?

464
00:48:45,280 --> 00:48:52,240
And so is that second part? Have you developed the quantum neural networks or

465
00:48:54,240 --> 00:49:00,800
so we have some for now there's some internal work with QCWARE where we develop some new

466
00:49:00,800 --> 00:49:07,360
architectures based on the intuition that we got from from how to load quantum data because

467
00:49:07,360 --> 00:49:12,000
as I said, this is also something fairly new, but what are the optimal ways of

468
00:49:12,000 --> 00:49:16,640
floating data because for a neural network this is kind of, you know, half of the thing that you

469
00:49:16,640 --> 00:49:23,120
have to do is load the data and then pass it through the neural network. So the fact that we

470
00:49:23,120 --> 00:49:29,760
figure out the optimal ways of loading the data and doing inner products with different data,

471
00:49:29,760 --> 00:49:36,320
this is what kind of gave us the intuition of how we should be defining this quantum architectures

472
00:49:36,320 --> 00:49:43,200
and hopefully we will be able to get some more theoretical guarantees. We are working on it,

473
00:49:43,200 --> 00:49:50,800
we are not there yet, but we are quite hopeful that we will be able to at least propose some

474
00:49:50,800 --> 00:49:58,480
architecture with some provable guarantees now. Is there a way to characterize where we are with

475
00:49:58,480 --> 00:50:05,760
quantum neural networks and the kind of the language that we use for classical neural networks

476
00:50:05,760 --> 00:50:12,800
like, you know, we're at the single hidden layer feed for network stage or, you know,

477
00:50:14,560 --> 00:50:21,200
so I think we're orthogonal to the kind of complexity that we see in classical.

478
00:50:21,200 --> 00:50:30,240
So if you're asking about what type of experiments we can do, right, on a real quantum computer,

479
00:50:31,120 --> 00:50:40,640
I think some of the most impressive experiments is to say that let me look at the M-nist

480
00:50:41,600 --> 00:50:48,560
data set of 100 in digits. I will only pick two digits set of 10 because I cannot handle 10,

481
00:50:48,560 --> 00:50:55,760
I can handle two of them and instead of 700 pixels, we used four pixels and I do

482
00:50:57,280 --> 00:51:02,320
figure out if you have three versus one by looking at four pixels, you know,

483
00:51:02,320 --> 00:51:07,520
blurry images of three's and ones. So when it comes to real hardware, you know,

484
00:51:08,320 --> 00:51:13,520
real data, we're very far from figuring out whether quantum neural networks will work or not,

485
00:51:13,520 --> 00:51:18,640
right? And the implementation of the number of qubits that we have to work with kind of the

486
00:51:18,640 --> 00:51:25,440
the web bar compute bar. At the same time, we have many different proposals for architecture,

487
00:51:25,440 --> 00:51:32,800
so how this quantum neural networks should look like. It's very difficult to prove anything

488
00:51:32,800 --> 00:51:40,400
and you cannot test them. So that's why what we're trying to do is come up with an architecture where

489
00:51:40,400 --> 00:51:46,880
you can at least have some guarantees, some provable guarantees. And it's not easy, it's not easy,

490
00:51:46,880 --> 00:51:52,640
but we're making progress, everyone, many people are working on this, we're making progress,

491
00:51:52,640 --> 00:51:58,160
the progress I think will be much faster when we get our hands on better hardware because this

492
00:51:58,160 --> 00:52:02,960
is kind of a deep letting is empirical to some extent, right? You need to try it out.

493
00:52:02,960 --> 00:52:12,560
And so our people doing, you know, anything approximating kind of, you know,

494
00:52:12,560 --> 00:52:20,320
the, you know, very sophisticated networks, you know, quantum CNNs or quantum deep reinforcement

495
00:52:20,320 --> 00:52:29,760
learning or, you know, on paper at least. So we had, we had the paper that last year's ICLR,

496
00:52:29,760 --> 00:52:36,080
where we discussed quantum convolutional neural networks. Okay.

497
00:52:37,360 --> 00:52:42,960
On our side, that paper was very theoretical in the sense that we used classical CNNs

498
00:52:42,960 --> 00:52:51,680
and quantum ways of training the CNNs. Okay. They have been some proposals on quantum CNNs as well.

499
00:52:53,520 --> 00:52:58,320
You have to find quantum ways to do the different layers, like the pooling, the, you know,

500
00:52:58,320 --> 00:53:07,840
applying the nonlinearity and this type of things. Again, ideas are out there. We need to

501
00:53:08,560 --> 00:53:15,600
find better ideas. We'll be getting better ideas when we find ways of, of trying out things and

502
00:53:15,600 --> 00:53:21,600
figuring out why they work or why they not work. For example, a quantum circuit is a reversible

503
00:53:21,600 --> 00:53:27,600
computation. It's a linear of this unit that is a linear operations. So the first thing is how do

504
00:53:27,600 --> 00:53:35,600
you apply a nonlinearity if you, if you have a linear operator, right? So even that is not

505
00:53:35,600 --> 00:53:40,720
obvious what is the correct way of injecting nonlinearity in a quantum neural network.

506
00:53:41,280 --> 00:53:48,080
The many different things. Maybe I will measure and then I get a sample and this induces

507
00:53:48,080 --> 00:53:54,000
a nonlinear element. Maybe I need to get rid of some of the cubits and look at the subset of

508
00:53:54,000 --> 00:54:00,080
cubits. This also includes some nonlinear element. So even that is not trivial in the many different

509
00:54:00,080 --> 00:54:10,560
ways. And anything on the reinforcement learning side? Reinforcement learning. I think it's probably

510
00:54:10,560 --> 00:54:20,000
the least advanced area. It's also not the easiest one. That's fine. So we started with supervised

511
00:54:20,000 --> 00:54:27,360
learning a few years ago. I'm supervised learning last year. And there have been very few

512
00:54:27,360 --> 00:54:37,360
papers on quantum reinforcement learning. And what we did with the student of mine is to look at

513
00:54:38,960 --> 00:54:44,880
policy iteration before we go to deep reinforcement learning even through, you know,

514
00:54:44,880 --> 00:54:52,000
iterative methods as you said and solving linear systems one after the other to update your policy

515
00:54:52,000 --> 00:54:59,200
and improve your policy and things like that. There are things we can do. And I think reinforcement

516
00:54:59,200 --> 00:55:05,440
learning is quite interesting for quantum for different reasons. One of the reasons is that you

517
00:55:05,440 --> 00:55:10,720
don't have this problem that we discussed earlier of loading data because it's not like you have

518
00:55:10,720 --> 00:55:14,960
images which you have no idea about and you need to really look at all the data and load it.

519
00:55:15,680 --> 00:55:21,200
Your data and the reinforcement learning is basically what you have learned of your game by

520
00:55:21,200 --> 00:55:26,800
taking some moves and figuring out where you're going. So you can kind of produce the data

521
00:55:26,800 --> 00:55:36,320
as you are exploring your state space, right? And this is easier data in the sense that we can

522
00:55:36,320 --> 00:55:43,680
construct data and it's not just data that we have to store and load from memory, right?

523
00:55:43,680 --> 00:55:50,720
So this is one of the reasons why I do think that reinforcement learning is a very interesting

524
00:55:50,720 --> 00:55:59,600
thing for quantum algorithms. And then adding deep RL into the mixture. Yes, it's a great thing to do

525
00:55:59,600 --> 00:56:06,880
and not much has been done. But as I told you, we have a very young field. We have pretty much five

526
00:56:06,880 --> 00:56:14,000
good years of doing things and we have a very small community. So hopefully after the ICML talk

527
00:56:14,000 --> 00:56:19,360
and the interview with you, more people from the classical ML community will start getting

528
00:56:19,360 --> 00:56:26,480
interesting about what is this thing out there. And I'm very happy to talk to people and figure

529
00:56:26,480 --> 00:56:36,240
out how to work together. What are the limitations of, we do this kind of distinction

530
00:56:36,240 --> 00:56:42,160
early on between classical algorithms, I'm sorry, between quantum algorithms and quantum computing.

531
00:56:43,040 --> 00:56:51,680
And a lot of the things we're looking for, a lot of the interesting work is in the algorithms

532
00:56:51,680 --> 00:57:01,920
and independent of being able to run them. To what degree is simulation viable for quantum

533
00:57:01,920 --> 00:57:14,000
algorithms? Can we simulate quantum algorithms in a classical machine? So if you want to simulate a

534
00:57:14,000 --> 00:57:22,640
general quantum computation, a quantum algorithm on hundred qubits, then the equivalent classical

535
00:57:22,640 --> 00:57:29,840
problem that you need to solve has dimension two to the hundred. Precisely because there's a state

536
00:57:29,840 --> 00:57:36,880
of a quantum system of a hundred qubits is a two to the hundred dimensional vector.

537
00:57:36,880 --> 00:57:45,840
So this is why the maximum quantum system that we can simulate classically is something between

538
00:57:45,840 --> 00:57:52,960
30 and 40. I think for now there's something like 32, 34, 36 and this is the limit because you get

539
00:57:52,960 --> 00:57:58,560
to things like two to the 32 when you're touching the limits of what you can do.

540
00:57:58,560 --> 00:58:04,480
And we're depending on what kind of qubits or how you count or which vendor we're kind of at

541
00:58:04,480 --> 00:58:15,680
that point with quantum machines now. So we're beyond the point where it makes sense to simulate

542
00:58:15,680 --> 00:58:21,120
because we have access to the actual machines that are beyond the capacity of what we can simulate.

543
00:58:21,120 --> 00:58:29,120
You're right that for example both Google and IBM have a 53 qubit machine.

544
00:58:29,120 --> 00:58:38,480
So we will never be able to simulate a general computation on 53 qubits because you would have

545
00:58:38,480 --> 00:58:47,200
this two to the 53 object that you have to handle. At the same time this would be the case if you

546
00:58:47,200 --> 00:58:54,880
had perfect qubits somehow that you know what they do. Because if you have 50 qubits that are so

547
00:58:54,880 --> 00:59:00,080
noisy then it's very easy to simulate what will happen at the end it would be just garbage.

548
00:59:02,320 --> 00:59:08,400
And it was exactly the point where what Google managed to do with the supremacy experiment is to

549
00:59:08,400 --> 00:59:16,080
have these 53 qubits good enough that at the end you don't get total garbage but with very small

550
00:59:16,080 --> 00:59:20,880
probability you get something that has to do with what you were trying to compute.

551
00:59:20,880 --> 00:59:29,680
And this is exactly the point where where the reason we call this you know and I think it's a

552
00:59:29,680 --> 00:59:36,160
very important experiment right the experiment said that you have nowadays a machine that can do

553
00:59:36,160 --> 00:59:44,720
something completely useless but something that you cannot simulate classically right now the

554
00:59:44,720 --> 00:59:50,720
holy grail is to go from something completely useless to something very useful that also you cannot

555
00:59:50,720 --> 00:59:56,720
do with a classical computer. And this is what we are trying to reach this point with quantum

556
00:59:56,720 --> 01:00:02,800
machine learning with quantum chemistry with quantum optimization we are not there yet but we are

557
01:00:02,800 --> 01:00:10,560
doing good progress and I think you know the only thing we can do and I think this is my responsibility

558
01:00:10,560 --> 01:00:17,200
there's a scientist as well is to try to accelerate this process so that we can get to real world

559
01:00:17,200 --> 01:00:23,120
applications as fast as possible because at the end we do want to have an impact and to make

560
01:00:23,120 --> 01:00:29,600
this well the better place so we're trying our best. Awesome awesome well you're done is thanks so

561
01:00:29,600 --> 01:00:36,880
much for taking the time to share with us a bit about your recent keynote and your research

562
01:00:36,880 --> 01:00:49,280
fascinating topic and conversation and I appreciate it. Thank you very much for the invitation.

