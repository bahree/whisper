Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This past week, I spent some time in San Francisco at the Artificial Intelligence Conference
by O'Reilly and Intel Nirvana.
I had a ton of fun and got a bunch of great interviews from some amazing people doing
awesome work in ML and AI.
I got to talk to folks like Gunner Carlson of IAZD and Stanford who's applying topological
models to machine learning like Ian Stoica of UC Berkeley whose RISE lab is building
Ray, a distributed computing platform for reinforcement learning, and like Mo Patel and
Laura Furlick of Think Big Analytics who shared a bunch of great use case stories with me.
I'm super excited about my interviews from the conference and I'm looking forward to
sharing them with you.
Make sure you check back with us on October 9th to catch the full series.
In the meantime, I've got a great interview for this week.
Like last week's interview with Bruno Gunn's office on Word to Vic and Friends,
this week's interview was also recorded at the last O'Reilly AI conference back in New
York in June.
Also like last week's show, this week's is focused on natural language processing and
I think you'll enjoy it.
I'm joined by Jonathan Mugen, co-founder and CEO of Deep Grammar, a company that's building
a grammar checker using deep learning and what they call deep symbolic processing.
This interview is a great compliment to my conversation with Bruno and we cover a variety
of topics from both subsimbolic and symbolic schools of NLP, such as attention mechanisms
like sequence to sequence and ontological approaches like wordnet, syn sets, frame net and sumo.
I'm looking forward to your feedback on this show.
Jump over to the show notes at twimmalei.com slash talk slash 49 to let me know what you liked
and learned.
Finally, before we dive into the show, the details for the upcoming Twimo Online Meetup
have been set.
On October 18th at 3pm, Pacific Time, we'll discuss the paper visual attribute transfer
using deep image analogy by Jing Liao and others from Microsoft Research.
The discussion will be led by Duncan Stothers, for anyone who's missed the last two meetups
or for those who haven't yet joined the group, please visit twimmalei.com slash meetup for
more information.
There, you'll find video recaps of the last two meetups along with the link to the paper
we'll be reviewing next month.
If you'd like to present your favorite paper, we'd love to have you do it.
Just shoot us an email at teamattwimmalei.com to get the ball rolling.
And now on to the show.
All right, everyone, I am here at the O'Reilly AI conference.
And I am with Jonathan Mugan, who's the founder and CEO of Deep Grammar, Jonathan, welcome
to the podcast.
Oh, thanks for having me.
I'm excited to get into this conversation.
So you are speaking later today at the conference.
And I'm looking forward to having you walk us through your presentation.
But why don't we start by having you tell us a little bit about your background and
how you got into AI?
Yeah, sure.
So I started off in psychology.
Went and got my undergraduate in psychology and I wanted to understand the human mind.
And as my advisor used to say, the interesting parts weren't scientific and the scientific
parts weren't quite interesting.
And I love that.
Yeah, we didn't quite have a firm grasp on concrete principles that we could
that we could use to really understand what was going on.
So I became a little disillusioned and got my MBA and company called Pricewaterhouse Coopers
trained me up in computer programming and set me off in a consulting world.
Join the ranks.
I did.
Yes.
And then as I was, I was programmed and I was like, you know, you have to tell a computer
exactly what to do.
This might be the kind of, the kind of rigor that we need if we're going to, we're going
to do psychology.
Uh-huh.
And then of course, AI is a mix of psychology and computer science.
So it seemed natural.
So I decided to go back and get my PhD, but my undergraduate was a bachelor.
And when did you do that?
Well, I went back in 2003.
I went back to get my masters because my undergraduate was in psychology.
So I couldn't get into a PhD program straight away.
So I had to take calculus and all that kind of stuff and all the kids.
Yeah.
The masters at UT Dallas and then got into the PhD program at UT Austin and started
working with Ben Kipers.
Okay.
And so what was the focus of your research there?
Yeah.
My focus was on developmental robotics.
So how can you get a robot to learn about the world in the same way a child does?
And so the idea is the robot just wakes up or is born and has some knowledge built in,
but not much.
And it wants to build it all up from the beginning and the robot pushes objects around
and learns relationships between its hand and the object and it learns how to form actions
and how to build up perceptions like, oh, my hand is to the left of the block.
That's actually significant because that determines when I can hit it to the right.
And so it built it up that way.
And then I finished that up and graduated in 2010.
And at that time, AI was not hot like it is now and it's amazing to change.
And so I got a postdoc at Carnegie Mellon and I studied under Norman today and at the
intersection between kind of computer science and human computer interaction.
So we studied if you have this location device that gives you your location or excuse me
that broadcast your location to family and friends, when exactly do you want to share your
location?
So at the time, this was somewhat novel being able to share your location.
Right.
And there were a lot of privacy issues around it then, like, of course, they're still
hard now.
And so the device would learn through interaction with you when you want to share based on who's
asking and where you are in time of day.
Interesting.
Interesting.
So my wife actually used a Texas gal and when I went to Pittsburgh, she didn't come with
me.
I flew back home every weekend.
Oh, wow.
Yeah.
It was quite a deal.
And so I flew back home every weekend and eventually she says, you know, we're not going
to Pittsburgh.
So I got a job in Austin at a small company called 21CT where we do defense contracting
work for the Department of Defense.
So data mining and at that job, I pushed into natural language processing because one
problem I found with the development of robotics is it was really hard to get funding
unless you're at a university because it's so far off.
You know, we have robots and factories all over the place, but we don't want them staring
at their navel and wondering about life.
And so most of the funding, most of the push is towards robots that can do actual concrete
things right now, right?
And I'm more interested in the fundamental concepts below that.
The fundamental concepts that enable a child to just grab the world and understand it.
And so I saw that language might be a good kind of in between.
So language is very important right now.
It's very useful chat bots are a thing.
Language interfaces are important, computers have to read tons of documents.
I thought, okay, well, language might be a way that I can both feed my family and study
the stuff I care about.
And then of course, along the way, I found a deep grammar co-founded with Christian Storm.
But that ties into my talk today because my talk today is about how can we go from natural
language processing down to these fundamental concepts into real understanding.
Because right now, natural language processing is kind of sad because we're just at the surface.
We treat these tokens.
I was amazed when I first got in the field.
We treat these words, we tokenize in the words and maybe we parse, but we just have
this string of tokens.
And then we do stuff with the tokens.
The computer has no idea what these tokens mean.
We just look for patterns in the tokens.
And so in my talk, I start off with a TF IDF, which you take a document and convert it
into a vector.
If you're vocabulary is 50,000 words, it's a 50,000 long vector.
And you lose, you lose the word ordering.
So man by its dog is the same as dog by its man.
And TF IDF is term-frequency inverse document.
That's right.
So the term frequency is, you know, if ardvark shows up twice, then you get a two in the
ardvark's one.
And then you scale that with the inverse document frequency by how often ardvark shows
up in your corpus.
So the less frequently it shows up, the more important it is in the context of the document
is that's the idea.
That's right, that's right.
And then that way, that vector helps discriminate that document better because it has that
scaling.
And so it's interesting that you start off by talking about, you know, the fact that
NLP is not, you know, it's not based on a lot of inherent structure because previous
conversations I've had with folks, I might, the general kind of understanding I've come
into is that that's where, you know, that lack of structure, meaning taking a statistical
approach as opposed to a linguistic approach, is been the source of all of the advancements
in, or much of the advancements in NLP over the last few years.
Do you disagree with that generally or?
Well, it's definitely true that we've been able to do a lot of cool stuff.
And so in my talk, I talk about two paths, the symbolic path, and the sub symbolic path,
which is the deep learning stuff that everybody's doing now.
Okay.
And yeah, with deep learning, we're able to generalize across tokens.
So one problem we had before, if you said, I got into my car and went to the store versus
I got into my truck and went to the supermarket, those looked like very different sentences
in TF IDF.
And you had to manually go in and say truck and car are pretty similar.
Yeah.
And store and supermarket are pretty similar.
And you can do that for a few things, but you just can't think of all of these possibilities.
And deep learning is really great from that, the word to VEC.
So everything's a vector, and it turns out that, of course, car and vehicle are going
to be very similar in car and truck and supermarket and store.
And so if you, instead of due to TF IDF, you do like a, you can just even average the
word vectors, or you can do an RNN where the last state is the meaning of sentence, you're
able to really capture similarity across sentences in a way you can't do as well with symbolic
methods.
But you still don't have any understanding there.
So when you do word to VEC, what you're doing is you're learning a vector for a word based
on the words that typically go around it.
And so the algorithm is, is you go through your whole corpus and for every word in the
corpus, you go through one by one, you take the vector for that word, and you push the
vectors for the other words closer to it, and you push all the vectors for the other words
that aren't close to it away, and then you move to the next one, and you keep doing
that over and over again until you've converged.
And that's great, but it only captures what people say.
So most of the knowledge that's needed to understand language is so obvious that we
never mentioned it.
And so that kind of stuff just doesn't show up in word vectors.
And so even when you get this vector at the end, you still not clear what to do with it.
And so you think about some of the biggest advances have been, or most exciting ones
have been in machine translation, the machine still has no idea what it's just spitting
out tokens.
It encodes it with the encoding RNN, and then the decoder, it spits out the next word
based on the previous state, the previous word, and then if it has attention, all of the
previous encodings in the encoder, but it's just a softmax putting out tokens, it doesn't
have any understanding of what it's doing, which is in some degree why it's so applicable
in so many different domains, you can create a parse tree with it, you can even encode
a picture into a vector using using a CNN, and then run the decoder, and that's how you
get this captioning work.
That's really exciting, but there's still no understanding there.
And so you end up with this vector, so now we're on the sub symbolic path, but what can
you do?
And so the next thing that people started doing was, well, so attention, what you're doing
when they added attention to the encoder decoder method, you're, when you're about to generate
a word in your translation, you're looking at all of the previous words in the sentence,
or they're encoded representation, the hidden state representation of the sequence.
And so what it's doing is it's looking at facts about the world to figure out which ones
are relevant for generating the next word.
And so what people started doing was they said, well, what if I just feed it in a story?
And so I can feed it in a story where like Tom went to the store, Tom came home, Tom
picked up a jar, Tom went to the airport, and now the question is, where is the jar?
And if you feed it enough of these stories, it's pretty amazing.
The computer can answer, it's at the airport, presuming that you never put down this jar
that's just carry with you for your life.
And that's really cool, but you have to generate these stories automatically.
And the reason you have to generate them automatically is because you need so many stories
that it needs to be able to find these statistical patterns underneath.
Okay.
And this mechanism that's enabling this as attention, can we maybe double click on that
to talk about how that's implemented to maybe get a...
I've heard attention come up a bunch of times, but I haven't dug into it in any level
of detail, and I'm wondering how that manifests itself in some of these deep networks and stuff
like that.
Yeah.
So I'm thinking that Google just came out with this new tensor to tensor thing, which is,
and I'm thinking of how they do attention.
So you have like a set of keys and a query and keys and values.
And what you're doing is for some query, you're looking at all of the keys to find the
most similar key, and then you take that value.
And the similarity between the query and the key is the weight that you use for the value.
So you end up doing a weighted average of the values.
Is that implemented in a neural network?
Are we talking about...
There's an external structure like a database or a key value store or something there.
No, it's all neural network.
And so when I say key and query and value, these are all vectors.
Okay.
Yeah.
Got it.
And so in the sequence-to-sequence model, what you're looking at is these, and maybe
that the keys and values are one and the same, but you're looking at where your query is
my current state when I'm trying to generate.
So, you know, you could have, I went to the store, and then you're translating to Spanish
and Yofuya Super Mercado, and when you're trying to, my great accent, when you're trying
to generate a Super Mercado, you look back at, I went to the, and you went to, you look
at those encoded representations along the way, and you take your state at Super Mercado
or at the state before you generate Super Mercado, and you compare how similar those are.
And then you take the weighted average of those values, and then that value comes in to
where you would normally generate Super Mercado, and that value is taken into account.
This is just another vector along with the vector for the previous state in your decoder
and the vector for the previous word you generated.
And then for each vector, you have another matrix, which you multiply it by, and then you
add those things up, and you throw that in the softmax, and then that's your output.
Okay.
Yeah, and so neural network at each point is generally a, or very often, a multiplication
of a matrix by a vector, and then you put some nonlinearity on the result.
So.
Okay.
So attention basically is, is your storing kind of, you're kind of storing up these vectors
and referencing them from the past, essentially, to be able, and including those in your,
your end calculation.
That's right.
And so what they're doing in the story generation, or excuse me, in a story question answering,
is they're encoding the parts of the story as vectors.
And then when they want to ask, answer a question, they go back and look at the parts of
the story and figure out which parts of the story are most relevant to answering that
question.
And then they do a little computation on top of that, and that's, that's where your answer
comes from.
Interesting.
And are there limitations to the amount of memory that you're able to refer back to?
Well, so generally, there's not limitations in the amount of memory, but you're generally
taking a weighted average.
Mm-hmm.
And you do that because if you just take kind of a hard attention, then you can't do the
back propagation as well.
And so you take a weighted average, so it, things kind of get watered down a little bit.
But I don't think that's a huge problem.
More of the problem is it's just a very simple mechanism, and it can only do so much.
And I think that's where you were going before I kind of interrupted you to push into this
attention, is starting to approximate things that look and feel like meaning, but not,
it's still not quite there.
Yeah, you're going back over your previous experiences and say, oh, this one's relevant.
Right.
Yeah, which is cool.
But the robot doesn't have any previous experiences.
So these story generating, or these story question answering systems are really cool,
but there's no built-in knowledge.
Right.
So when we answer questions about stories, we bring a whole lifetime of knowledge.
And these all start from scratch.
And what we need to do, I think the next step on the sub symbolic path is we need to have
systems that interact in our world with the objects and relationships in our world.
And so you can imagine like a little robot that can pick up objects and move them around.
And then it knows what a bottle is, because a bottle is partially the hand fixture that
it needs in order to pick it up.
A bottle is partially that, if it knocks it off the table of bricks, a bottle is partially
that that turns it to the side.
Water comes out.
All these things are part of the definition of a bottle.
And so when it's pulling up memory, it's not just pulling up parts of a story.
It's pulling up huge banks of things that it is experienced before.
And then you can make inference from that that you wouldn't be able to otherwise.
Now we don't quite know how to do these advanced inferences based on experience other than
the kind of basic models we have now, which is like sequels of sequence and CNN and some
other ones.
And it's going to be exciting to see one of my, one of the things I really enjoy about
the deep learning is every time a new configuration comes out or you know, a new one that goes
into zoo.
I'm like, oh cool, we're getting a little closer.
Right.
I envision in the brain, you know, there's just thousands upon thousands of different kinds
of configurations and neurons and at least to some approximation, one of them might be
a sequence of sequence.
And another one might be a CNN, but there's, you know, hundreds more that we haven't discovered.
Right.
Right.
It would be cool as we get better and better with each new one.
So I think most of what we covered now, I mean, it sounds like a lead up to, you know,
a specific area of research or interest that you have that kind of promises to help address
this issue.
Like where does it, where do we go from the sub symbolic, maybe another way to ask this
is, is it your observation that a more symbolic approach is kind of the answer to the ills
of the sub symbolic approach, or do you think the path forward is still sub symbolic,
but extending it to incorporate more understanding?
I don't know.
Yeah.
So in my talk, I cover both approaches as if they're separate approaches.
And there's been surprisingly little overlap in the approaches.
And what we've talked about the sub symbolic mostly, have we talked, have you, should we
take a few minutes talking about the symbolic stuff and what's happening there?
Sure.
Sure.
We can do that.
So we can, yes, we were talking about a TIF IDF factor and that is, it throws out word
order.
But you can do a lot of stuff with it.
You can say this document is similar to this other document.
You can even throw it into machine learning classifier and do sentiment analysis or document
classification.
And that's pretty neat.
And I mentioned sentiment analysis.
So the next step in sentiment analysis, getting a little closer to actual meaning is a sentiment
dictionary.
Okay.
So often you'll have a dictionary that says, okay, the word terrible has a negative sentiment.
And the word good has a positive sentiment.
And you have some simple mechanism that says, well, not terrible.
You have the inverse, reverse to word.
And that can get you pretty far.
And but you're, for each symbol now, you're going into your dictionary and you're assigning
some very simple meaning.
So that's kind of the first step to assigning meaning or one, you could consider it one
first step.
But there's also a whole set of representations that people have built.
And so when you build a representation, what you're doing is you're taking symbols and
you're creating relationships just between symbols.
And then presumably if you set up the symbol system, you can map what people say to the
symbol and then you can map what the computer should do based on whatever symbol got lit
up.
So if you had like, let's say you're a tire company and you want to watch Twitter to
see who you should try to sell tires to, you could mention anybody, you could have set
up a symbol system where it says, okay, a car has tires, a truck has tires, you know, Toyota
is a kind of car.
And therefore anybody mentions a Toyota, it links in the tires.
Okay.
And then you can just, that helps you when you take a sentence, you say, given the sentence,
can I find tires linked anywhere in there?
And I don't mention tires explicitly.
But of course, this is kind of brittle and there's been a lot of work in setting up these
symbol systems.
The most famous is probably wordnet, okay, where you have these set of synths, which is
a synth set is a group of words that all mean the same thing.
It's like a meaning.
And so vehicle might be one synth set in that vehicle, you'd have, well, maybe car is
one synth set.
And then car, you'd have motor car, car, and you could have, you know, a car in all different
languages, but it means car.
And then you set up relationships between these things like car is a kind of vehicle.
And then you would have sports car as a subset of that.
And so wordnet is really popular and it's really good.
It kind of gives a, it kind of gives a sense of a definition for most words.
And you can also have a word being two different synths set.
So bank would be in the synth set for river bank, but also a bank where you deposit money.
Okay.
That's one, another one is frame net.
And so frame net builds up a little bigger situation.
So wordnet is about individual words.
Frame net is about situation.
So one example is the frame commerce by, which means somebody buys something from someone
else.
And so that frame is triggered by some set of keywords, like bought, purchased, sold.
And when that frame is triggered, what frame net does, or at least an implementation that
uses frame net, goes into tries to find the roles.
Who was the buyer?
Could be Bob.
Bob bought a car from Tom.
Okay.
Buyers, Bob, the seller is Tom and the thing purchases car.
And so you've converted this sentence into a frame with roles and now that's machine understandable.
Okay.
And it's kind of nice because you move up from individual words into kind of meaning
of situations.
Yes.
Yeah.
But frame net doesn't go very deep.
You know, if frame net doesn't say, you don't have like the fundamental things going on,
like forces and, and well, there's a little bit of that.
But you don't have the things that a child knows, a very young child.
And it turns out in, in AI, that's the hardest part.
You know, we started off thinking that chess was the pinnacle of intelligence.
And now it turns out that picking up a bottle of water is really hard.
And who would have thought?
So all the things, you know, they say, all of the things a kid knows by three or four,
if we could get those in their computer, that would just be amazing.
And that's what I would really love to try to do.
And so if we're going to build that with a symbol system, we have to go deeper.
And so one symbol system that does go deeper is Sumo.
And so what Sumo is is, is a full ontology, meaning it goes all the way down.
And so you look at like cooking, what does the word cooking mean?
Well cooking is, I don't remember the exact, the exact thing, but cooking is a process,
which is a thing, which is an entity, it goes all the way, it takes it all the way down.
And so, so that's really useful, but what we need to do now is figure out how we can get,
how we can get things like Sumo tied in the word net.
And there has been some, some linkages, Sumo already does tie in word net.
But how we can get all these different representations together, because what we want to do next
is build, if we're staying in a symbolic land, build a causal model of how the world works.
And here at this conference, Josh Chenabong yesterday was talking about that.
And so we need, you know, an entity needs to understand that when it pushes a table, all
the things on top of the table are going to move.
And if you try to put that in logic, it's hard.
And you need like a model where you can just read it off the model.
So in some sense, frame net is kind of like that.
So if, if there's a frame where Bob sold a car to Tom, and then you ask, well, who has
the car afterwards?
It's Tom.
You can just read it right off the frame, or you can have that associated with the frame.
You can put that in with the frame.
And so what we need to do is build deep causal models that go all the way down to these things
called image schemas, or image schemas are the language independent concepts that we
use to understand everything in our world. So like Lake often Johnson and, and these kind
of guys, Manler.
And so you put a, she's a psychologist and you put a developmental psychologist.
And so like one is containment.
So when you have a bottle of water, the water is contained in the bottle, which means
that if you move the bottle, the water goes along with it.
Another support.
So the bottle is on the table.
And so you need these concepts before you can understand language, because the language
understanding is built on all this stuff.
When we talk to each other, we never, we never say these things.
Sure.
So yeah, one of my, one joke I like to say is you can imagine a romance novel where there's
a table in between two lovers, and the man pushes the table aside, and then the novel
they would never say, and as he pushes the table aside, all the objects on the table mood
because they were spushing down.
Right.
Like that's just not in there.
And it was a sound of the light scraping across the floor, right?
That produces gashes in the floor, and suddenly the objects were at a new location.
We make a lot of assumptions when we talk.
We do.
I mean, if we didn't, we'd never get anything done.
Right.
Yeah.
And so, so what we need to do is build up cause and models of the world onto which we
can put these symbols that we define.
I can't decide if it would be fun or extremely boring and tedious to run these models in
reverse and generate that romance novel.
That'd be the worst novel ever.
That'd be awesome.
It's like, you know, you have the, you have different editions of books, the big print,
and then this is the computer edition.
Yeah.
Yeah.
And so those are, those are basically the two paths and to get from where we are now
in natural language processing, which is just working at the symbols either with the
sum symbolic, where we're able to learn vectors for these individual things or the symbolic
where we just write down what they are in the computer that can really understand because
it understands the fundamentals.
So it's hard to say where to go from here because one problem is you need to find a commercially
viable need for the simplest possible common sense knowledge.
So we all have chatbots everywhere now, but they require already way too much knowledge
to be good.
If you go out out of, if you stay within a particular domain and you basically just
hard code everything, and you can, and you can have chatbots where it's learned using
sequence sequence models, but that's just gibberish back and forth.
It's no different from Eliza, really.
If we have a chatbot that actually is general, if you ask it things off script and can answer
your questions, we're going to need these fundamental concepts.
In fact, one of my dreams is to build a chatbot for children that you get at age three or
four, and it lives in your mom's phone.
And it teaches you concepts about the world, and it's also your friend, and it learns
about you.
And the cool thing about being a teacher is that if it teaches you, then it knows what
you know.
So then it explains other things to you.
It can explain it to you and things you understand in terms you already understand.
And it can also make things interesting, because let's say it knows that your favorite animals
are draft.
And I can say, when it teaches you math, I can say, if you have six drafts, and you buy
two more, how many do you have?
And that's the kind of thing that engaged parents do.
And it would be cool to do that in an app, and I have this dream that you put that in
for children, and you have your developers feverishly working behind the scenes, making better
and better and better technology, so that when a child gets older, the app just turns
into the operating system for the child.
And so the child now uses this app to interface with its whole world.
And since the app has been with the child since the beginning, the app really knows the
child, and so it can be the ultimate and customized.
And when it's, you know, and then as an adult, when it's guiding me through how to fix
my dishwasher, it knows that I know nothing, and it literally has to tell me, lefty Lucy
Rady Tidy, remember, as opposed to going Wikipedia or on the web, you just have no idea.
Yeah, and then even when you're old, you know, if you become, your faculty start to go,
and if you're standing in the kitchen, you can't remember how to make coffee, you know,
the app can then be in the cameras in the room and say, hey, you make coffee this time
of day.
The filters are in the cupboard over there.
That's the first step.
And it guides you through, and maybe we could stay independent longer.
That's...
I would have great vision for an app.
That would be awesome, yeah.
Now, how much of that is deep grammar trying to take on?
None.
So, yes.
Deep grammar is trying to take on.
When I write, I make a lot of really dumb mistakes.
Okay.
It's just the human in me, right?
I think one thing in my hands just outputs something different.
And I've always been amazed that grammar checkers couldn't capture that.
And you know, spell checkers came along and they were amazing.
They really...
I don't know how many people around remember days with four spell checkers, but it was
a huge advance.
Yeah.
And, you know, I always told my teachers, I'm not going to have to know how to spell.
And it turns out, I was right about one thing, very few times was I right, but that
I was right.
So, the grammar checkers always, you know, they were in word for a long time and they
just would miss obvious wrong stuff, and it really bugged me.
And I always thought machine learning would be the way to go.
And so, I started working with endgrams, which is sequences of tokens, this is a few years
ago.
And it turns out, if you think about it for five minutes, it turns out that, you know,
for endgrams of like sequences of three or four, if you take the, you know, I went to
the, that's like four words.
And then you have a distribution over the next word.
And so, if you write a word that is not in that distribution, or not, you know, doesn't
have a lot of weight in that distribution like donkey, but it's similar to a word that
should have high weight like store, although donkey and store are similar, then you've
probably made a mistake.
So, if it's a, I went to the store, right, that's a mistake.
It should be obvious.
Stored is very different, very similar to store, and store is going to have a very
low probability.
The problem with endgrams is you can't, it's that similarity problem talked about before,
because I went to this, I went to the store is, you know, I went, I drove, I meandered,
I walked, all those are very different to an endgram probability thing.
And so, in order to train such a thing, you would have to have seen all these things.
And you'd have to store the vocabulary size to the fourth to store all this probability.
And so, when I started working in deep learning, and I said, oh, sequence of sequences, the
way to go for this.
You're in code this thing, and then you decode it, and you get the power of deep learning
that power we talked about before, that similar words are going to have similar vectors.
And similar sentences are going to have similar vectors to other similar sentences.
You know, the first thing is, oh, okay, I got to write a patent on this, this is, this
is going to be how we're going to do grammar checking, and that, that's how we got started.
Yeah.
And so, what we do is we encode it, and then we decode, and if the thing we decode is different
from what you wrote, then there's a problem, especially if what you wrote is different
than it's similar to something that would have high probability.
Okay.
And then we also sort of, how do you capture that similarity?
We do, it's just typical, typical, you know, the easiest thing you can do is like, what
Levinstein distance, which is edit distance on the letters, you can do that with similarity,
but there's also a bunch of other little similarity things we do, okay, that we take advantage
of a lot of the acquired knowledge over the years and grammar.
So we, yeah, we have a kind of sophisticated similarity measure, and then in addition
to sequence sequence, we throw the kitchen sink of deep learning at it, a bunch of different
CNNs and stuff, and so we've got it pretty good now.
So sometimes it still fails in a way that's disturbing, but if you make a mistake like
the wrong version of there or 222 is really good at that, it can catch it better than anything.
Oh, wow.
So, there's a lot of different markets, so the biggest market, as you might imagine, is
English as a second language, but, and we get people all the time emailing me, please
get to sing going, please, please, please, you know.
The English as a second language is particularly challenging, because sometimes when you're
not familiar with language, which you write is so far from correct, that the machine just
can't, it just doesn't know where to start, and so that's a particular challenge.
And then sometimes there's even a bigger fundamental challenge that the whole sentence has
to be rewritten.
And the only way to do that is to understand what the person said, and we've been talking
this whole whole interview about how computers just can't do that, right?
So that's going to be a problem for a lot of, a lot of time to come, but we can finally,
these dumb mistakes I look at them, I can't believe that the grammar tracker didn't catch
that.
Now it can catch those.
So, so that's really exciting.
Do you offer this as a service for folks, like I use a service for writing called Grammarly,
which you may be familiar with, that does a decent job for some things, some aspects of
their implementation are kind of bad, I don't, just they use it, the UI, user experience
is kind of wonky.
But I can imagine that as a go-to-market model, I can imagine more of a platform-ish approach
where you're offering APIs to developers to build things around, how are you guys going
at it?
We are in the process of trying to decide where exactly we're going to focus because Grammarly
is now really big, they've got a lot of smart people working, and it's going to be hard
to go head-to-head with them.
I think we've got some ideas that are really good, and I think we do some things better,
but they're just hiring like crazy.
You can start by plugging into any of the editing apps on the Mac, which they don't support,
or I don't think that they plug into Google Docs or anything like that.
So, there are some holes there, now, what kind of a note that gives you, that's another
issue.
And also, Grammarly is pretty expensive.
It thinks like $10 a month, and there's a lot of people around the world who really need
this, but $10 a month is a lot of money.
So we can, if we have a service that's really good, better than things that used to be
around before, but maybe we don't have all the bells and whistles at Grammarly, especially
if we can fix those things that are really hard for a computer to catch.
So Grammarly, looking at what they've done, it looks like they spend a lot of time implementing
a lot of rules, like calmer rules, but we can catch the subtle things that just pure learning
can find.
And that's what a lot of people need, because when you're English as a second language,
you don't have the ear that we do, the ear for the language.
And the ESL market is really interesting.
Language is a hobby of mine, and one of the apps that I've been using recently that I really
enjoy is this app called Tandem.
That basically allows you to, it's kind of a global language learning community, and there
have been a bunch of these, but it's the best implemented by far.
You basically go on this app, you tell it what language is your learning, and it'll match
you with people, the people that speak those languages natively that are trying to learn
your languages that you know.
But you'll get in these conversations with folks, and depending on their level, I think
the interesting conversations are when folks are beyond the, hey, I'm going to Google
translate everything I want to say, because you know the failure mode, like you can spot
those, you know, really quickly.
But then there are folks that know enough English that they're just typing what they
think is right, and sometimes it's a little hard to decipher, but most of the time you
can kind of get what they're trying to say, they're just not saying it wrong.
And if your stuff was plugged into this process, as like a kind of a side channel trainer
or coach or something like that, I think it would, you know, the big challenge for language
learners is like decreasing the cycle time of, you know, learning and iteration and accelerating
the process.
Something like that could be really interesting.
Yeah.
No, I hadn't thought of that as like a coach, you know, that you said this and maybe change
it to this other thing.
No, that's a good idea.
Another place that's like video transcription is big now, and a lot of times you have to
pay a human to do it.
Well, it's done automatically, but then you need to pay a human to make sure it's done
right.
Right?
Because you get this text out and sometimes it doesn't quite hear.
And so that's very much like a grammar correction problem.
So we could do that.
Yeah, we're trying to say what exactly, what niche, you know, we should go make it cheap
or make it an API, do transcription, maybe there's some publishers that have reached out
to us, they say, look, we write, we send out all these books and we have to pay people
to go in and read each one.
And so if we use you guys, then we have to pay them, you know, we can have them do more
books per person because they would have less, you know, less tedious stuff to do.
You'd catch the idea of stuff.
And so that's another option.
So we're kind of standing at the crossroads right now, trying to figure out what we're
going to do with it.
Does the technology get into or give you the ability to address stylistic issues as
opposed to correctness?
It kind of does both at the same time, but it doesn't help you rewrite things.
So it's basically going to help you write the way it was trained.
So we started out training on Wikipedia, but then there was everything that it wanted
to fix everything to be very Wikipedia.
Well, you know, the thought that came to me was, you know, the artistic style transfer
stuff where, you know, take this picture and make it Picasso-esque.
Yeah.
Like, you know, I'd love to take my writing and, you know, make it, you know, the form
of some other author.
Right?
That would be cool.
Now, so it doesn't work as well in languages, it doesn't vision because in language,
you're making a set of discrete decisions.
And so in vision, you have pixels which are much more amenable to small gradients.
And that's why they've had such huge success with vision.
And language is harder.
They're starting to get some work in that area.
So someone a new stuff is applying GANs to sequence of sequence models.
And so what you do is instead of using the cost of generating each token while you're
training in the decoder, you use some other measure of the sentence.
And so, and again, it would be the probability based on some discriminator function, the
probability that this is generated by the computer or by a human.
And then you have to back prop, or well, you have to get that answer back into the system
so it can learn.
And that's usually done like with reinforcement learning.
And that's not very efficient right now for language and it kind of works and there's
a lot of advancements, but still got a ways to go.
Okay.
I just finished a report on industrial applications of AI, and ended up being like 30 pages.
And I'd love to put that through like the Hemingway translator, or that will be awesome.
Yeah.
I think we'll get there.
I can assure you the sentences, I think for Hemingway will be a lot shorter, I think
shorter.
Yeah.
Or run on sentence kind of guy.
Yeah, that would be great.
And there is some of that.
So if you, you know, you train the system on Hemingway, it's going to want to generate
tokens that are Hemingwayish.
So you feed your sentence in and it's going to translate it into be shorter and something
about a fish probably.
Mm-hmm.
Nice.
Awesome.
Well, what's the best way for folks to kind of keep tabs on what you're up to and you
know, follow along as you guys iterate on this model and figure stuff out.
Yeah.
So we have a website, dogrammer.com.
On that website, you can try it out, type in a sentence.
Only does one sentence at a time right now, just because we have a cheap server up on
Amazon.
And then you can join our mailing list.
And I tweet my life out at Twitter, at J-M-U-G-A-N.
Okay.
Awesome.
Well, thanks so much.
It was great chatting with you.
Oh, thanks.
It's been fun.
Awesome.
All right, everyone, that's our show for today.
Thank you so much for listening.
And of course, for your ongoing feedback and support.
For more information on Jonathan and the topics we covered in this episode, head on over
to twimmolai.com slash talk slash 49.
If you liked this episode or you've been a listener for a while and haven't yet done
so, please take a moment to jump on over to Apple Podcasts or your favorite podcast
app and leave us that five star review.
We love to read these and it lets others know that the podcast is worth tuning into.
If you've already done this, then thank you so much.
We greatly appreciate it.
One last note, you've probably heard me mention Strange Loop, a great technical conference
held each year right here in St. Louis.
I'll be attending later this week and I encourage you to check it out.
Also, the following week, on October 3rd and 4th, I'll be at the Gartner Symposium IT
Expo in Orlando, where I'll be on a panel on how to get started with AI.
If you plan on being there, send me a shout.
Thanks once again for listening and catch you next time.
