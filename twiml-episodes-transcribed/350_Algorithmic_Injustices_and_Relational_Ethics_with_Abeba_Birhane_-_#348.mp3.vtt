WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:15.920
I'm your host, Sam Charrington.

00:15.920 --> 00:24.240
Hey, what's up everyone?

00:24.240 --> 00:28.440
Last time I mentioned the study group, I'm hosting around IBM's AI Enterprise Workflow

00:28.440 --> 00:33.880
courses and the webinar I'll be doing this Saturday with Ray Lopez, the courses instructor.

00:33.880 --> 00:38.760
It's been awesome to see the strong community response to this new program.

00:38.760 --> 00:43.600
These courses complement the modeling focus courses you may have taken and drill in on the

00:43.600 --> 00:49.000
end-to-end process of building, deploying, and managing real-world enterprise AI and machine

00:49.000 --> 00:52.000
learning projects.

00:52.000 --> 00:55.840
Unique to this program is the full immersion experience it offers.

00:55.840 --> 00:56.840
Consider this.

00:56.840 --> 01:01.520
You've just been hired as a data scientist working for a streaming media company.

01:01.520 --> 01:05.880
Your mission, should you choose to accept it, is to apply the tools of data science and

01:05.880 --> 01:10.280
machine learning to drive engagement and growth at the company.

01:10.280 --> 01:15.120
The examples, business problems, data sets, and tools you'll work with throughout the

01:15.120 --> 01:18.640
course are all woven around this scenario.

01:18.640 --> 01:25.120
The webinar will be this Saturday, February 15th, at 9.30 a.m. Pacific time.

01:25.120 --> 01:31.360
To register or learn more, visit twimlai.com slash AI Workflow.

01:31.360 --> 01:35.760
And now on to the show.

01:35.760 --> 01:38.640
All right, everyone.

01:38.640 --> 01:41.880
I am on the line with Ababa Barhani.

01:41.880 --> 01:46.800
Ababa is a PhD student at University College Dublin.

01:46.800 --> 01:50.360
Ababa, welcome to the Twimlai podcast.

01:50.360 --> 01:52.840
Thank you so much for having me, Sam.

01:52.840 --> 01:55.120
I'm really excited about this conversation.

01:55.120 --> 02:03.080
We had an opportunity to meet in person after a long while interacting on Twitter at the

02:03.080 --> 02:09.480
most recent Nureps conference, in particular, the Black and AI workshop, where you not

02:09.480 --> 02:16.120
only presented your paper algorithmic injustices toward a relational ethics, but you won

02:16.120 --> 02:17.560
best paper there.

02:17.560 --> 02:22.640
And so I'm looking forward to digging into that and some other topics.

02:22.640 --> 02:29.560
But before we do that, I would love to hear you kind of share a little bit about your

02:29.560 --> 02:30.560
background.

02:30.560 --> 02:35.040
And I will mention for folks that are hearing the sirens in the background, while I mentioned

02:35.040 --> 02:43.600
that you are from University College Dublin, you happen to be in New York now at the AIES

02:43.600 --> 02:50.940
conference in association with Triple AI, and as folks might know, it's hard to avoid

02:50.940 --> 02:53.920
sirens and construction in New York City.

02:53.920 --> 03:00.560
So just consider that background or mood ambiance background sounds.

03:00.560 --> 03:02.560
So you're background.

03:02.560 --> 03:03.560
Yes.

03:03.560 --> 03:04.560
Yes.

03:04.560 --> 03:06.160
How did you get started working in AI ethics?

03:06.160 --> 03:15.120
So my background is in cognitive science and particularly a part of cognitive science

03:15.120 --> 03:23.840
called embodied cognitive science, which has roots in cybernetics and systems thinking.

03:23.840 --> 03:32.360
The idea is to focus on the social, on the cultural, on the historical, and kind of to

03:32.360 --> 03:39.680
view cognition in continuity with the world, with historical backgrounds and all that's

03:39.680 --> 03:46.800
as opposed to your traditional approach to cognition, which just treats cognition as

03:46.800 --> 03:54.400
something located in the brain or something formalizable, something that can be computed.

03:54.400 --> 03:55.400
So yeah.

03:55.400 --> 03:57.400
So that's my background.

03:57.400 --> 04:07.520
And during my masters, I lean towards the AI side of cognitive science the more I delve

04:07.520 --> 04:16.560
into it, the more I, much more attracted to the ethics side, to injustices, to the social

04:16.560 --> 04:26.400
issues, and so the more the PhD goes on, the more I find myself in the ethics side.

04:26.400 --> 04:33.000
Was there a particular point that you realized that you were really excited about the ethics

04:33.000 --> 04:36.800
part in particular, or did it just evolve for you?

04:36.800 --> 04:39.040
I think it just evolved.

04:39.040 --> 04:46.080
So when I started out, I attained of my masters and at the start of the PhD, my idea is

04:46.080 --> 04:53.360
that, you know, we have this new, relatively new school thing, way of thinking, which is

04:53.360 --> 05:00.080
embodied cogsai, which I quite like very much, because it emphasizes, you know, ambiguities

05:00.080 --> 05:08.160
and messiness and contingencies as opposed to, you know, drawing green boundaries.

05:08.160 --> 05:17.000
And so the idea is yes, I like the idea of redefining cognition as something relational,

05:17.000 --> 05:23.880
something inherently social, and something that is continually impacted and influenced

05:23.880 --> 05:28.280
by other people and the technologies we use.

05:28.280 --> 05:33.320
So the technology aspect, the technology end was my interest.

05:33.320 --> 05:42.640
So initially, the idea is yes, technology is constitutes aspect of our cognition.

05:42.640 --> 05:51.160
You have the famous 1998 thesis by Andy Clarke and David Chalmers, the extended mind,

05:51.160 --> 05:56.080
where they claimed, you know, the iPhone is an extension of your mind.

05:56.080 --> 06:04.520
So you can think of it that way, and I was kind of advancing the same line of thought.

06:04.520 --> 06:12.440
But the more I dealt into it, the more ISO, yes, digital technology, whether it's, you

06:12.440 --> 06:19.280
know, ubiquitous computing, such as face recognition systems on the street, or your phone,

06:19.280 --> 06:25.440
whatever, yes, it does impact, and it does continually shape and reshape our cognition

06:25.440 --> 06:28.400
and what it means to exist in the world.

06:28.400 --> 06:35.680
But what became more and more clear to me is that not everybody is impacted equally.

06:35.680 --> 06:45.760
The more privileged you are, the more in control of you are asked to, you know, what can

06:45.760 --> 06:49.840
influence you and what you can avoid.

06:49.840 --> 06:59.600
So that's where I become more and more involved with the ethics of computation and its impact

06:59.600 --> 07:01.120
on cognition.

07:01.120 --> 07:10.160
The notion of privilege is something that flows throughout the work that you presented

07:10.160 --> 07:18.360
at Black and AI, the algorithmic injustices paper, and this idea, this construct of relational

07:18.360 --> 07:19.440
ethics.

07:19.440 --> 07:22.760
What is relational ethics, and what are you getting at with it?

07:22.760 --> 07:23.760
Yeah.

07:23.760 --> 07:29.920
So relational ethics is actually not a new thing, a lot of people have theorized about it

07:29.920 --> 07:32.200
and have written about it.

07:32.200 --> 07:40.240
But the way I'm approaching it, the way I'm using it is, it's, I guess it kind of springs

07:40.240 --> 07:51.120
from this frustration that for many folk who talk about AI ethics or fairness or justice,

07:51.120 --> 07:59.880
most of it comes down to, you know, constructing this neat formulation of fairness or mathematical

07:59.880 --> 08:06.680
calculation of who you should be included and who should be excluded.

08:06.680 --> 08:09.120
What kind of data do we need, that sort of stuff.

08:09.120 --> 08:15.800
So for me, relational ethics is kind of, let's leave that for a little bit and let's zoom

08:15.800 --> 08:19.920
out and see the bigger picture.

08:19.920 --> 08:27.000
And instead of using technology to solve the problems that emerge from technology itself,

08:27.000 --> 08:35.400
so which means centering technology, let's instead center the people that are, especially

08:35.400 --> 08:42.320
people that are disproportionately impacted by, you know, the limitations or the problems

08:42.320 --> 08:47.000
that arise with the development and implementation of technology.

08:47.000 --> 08:55.880
So there is a robust research in, you can call it AI fairness or algorithmic injustice.

08:55.880 --> 09:02.680
And the pattern is that the more you are at the, at the bottom of the intersectional level,

09:02.680 --> 09:08.280
that means the further away from you are from, you know, your stereotypical white cisgender

09:08.280 --> 09:18.000
domain, the more, the bigger the negative impacts are on you, whether it's classification

09:18.000 --> 09:28.000
or categorization or whether it's being scaled and scored by hiring algorithms or looking

09:28.000 --> 09:36.040
for housing or anything like that, the more you move away from that stereotypical category,

09:36.040 --> 09:40.040
you know, the status quo, the more the heavy the impact is on you.

09:40.040 --> 09:49.120
So the idea of relational ethics is kind of to think from that perspective to take that

09:49.120 --> 09:50.960
as a starting point.

09:50.960 --> 09:57.960
So these are the groups or these are the individuals that are much more likely to be impacted.

09:57.960 --> 10:05.160
So in order to put them at advantage or in order to protect their welfare, what do we need

10:05.160 --> 10:06.160
to do?

10:06.160 --> 10:10.960
So the idea is to start from there and then ask for their question.

10:10.960 --> 10:19.960
Instead of, you know, saying here, we have this technology or we have this set of algorithms

10:19.960 --> 10:27.920
or calculations, how do we apply them or how do we then use them to, you know, for a

10:27.920 --> 10:30.520
better or a fairer outcome?

10:30.520 --> 10:37.720
And sometimes the answer you arrive at is that a particular technology shouldn't exist

10:37.720 --> 10:39.520
in a given form.

10:39.520 --> 10:42.160
Yeah, exactly, exactly.

10:42.160 --> 10:53.200
So I think one of the downsides of obsessively working on some matrices or some equations

10:53.200 --> 11:00.280
on fairness is that you forgot to ask in the first place, do we, should we even do

11:00.280 --> 11:02.880
this in the first place?

11:02.880 --> 11:07.160
And I think some people have articulated this really well.

11:07.160 --> 11:13.200
You can think of this in terms of the, you know, face recognition systems that are becoming

11:13.200 --> 11:17.520
very normalized and common, especially in the states.

11:17.520 --> 11:26.440
Do you feed your face recognition algorithm with diverse data in order so that it recognizes

11:26.440 --> 11:33.000
everybody equally or do you stop and think, do we actually need face recognition systems

11:33.000 --> 11:35.680
in the first place, you know what I mean?

11:35.680 --> 11:36.680
Yeah.

11:36.680 --> 11:43.120
And that's a question that, you know, honestly, I have trouble with in a lot of ways because

11:43.120 --> 11:52.160
I think there are certainly problematic uses of facial recognition, but often the question

11:52.160 --> 11:57.600
is posed or the assertion is made that, you know, we shouldn't use the technology or we

11:57.600 --> 12:02.640
should, you know, I guess, you know, it's not uncommon to hear people kind of take this

12:02.640 --> 12:05.880
position of, hey, we can't put the genie back in the bottle.

12:05.880 --> 12:11.120
And, you know, I think on some levels, I get that that, you know, maybe that's a copout,

12:11.120 --> 12:14.040
but in other ways, it's like pragmatic.

12:14.040 --> 12:22.400
How do you balance the idealism that I think is probably core to the approach you're trying

12:22.400 --> 12:31.280
to take with pragmatism that recognizes what is already happening and the way technology

12:31.280 --> 12:34.280
tends to develop and evolve?

12:34.280 --> 12:43.400
I guess my approach, at least in the paper I presented at Neurips is that, and if you're

12:43.400 --> 12:52.000
starting point really is the welfare of, you know, the most disadvantaged, then I don't

12:52.000 --> 13:01.920
know how that cash is out with pragmatism or even with the whole idea of fairness because

13:01.920 --> 13:09.360
for most approaches to fairness, whether it's explicitly laid out or whether it's implicitly

13:09.360 --> 13:19.240
implied, the idea is, it's very utilitarian, in a sense, you have, you aspire to arrive

13:19.240 --> 13:24.320
at, you know, the greatest happiness for the greatest number of people.

13:24.320 --> 13:31.840
So, which really doesn't work for, if you are from a disadvantaged group or if you

13:31.840 --> 13:38.920
are a minority because you will never, you are not a minority, you are not a majority

13:38.920 --> 13:39.920
in the first place.

13:39.920 --> 13:47.920
So, any solutions that aspire to please the majority will always have negative consequences

13:47.920 --> 13:49.840
and it just doesn't work.

13:49.840 --> 13:59.880
So, that's the struggle with when you want to prioritize the needs and the welfare of

13:59.880 --> 14:08.880
the least privileged and on the other hand, some form of pragmatism or what's based for

14:08.880 --> 14:15.240
the majority or, you know, the greater the whole society, that's the tension that's

14:15.240 --> 14:18.400
probably, that will always exist probably.

14:18.400 --> 14:25.440
And so, that's at the heart of this idea of relational in a sense, it's, you know, pragmatic

14:25.440 --> 14:27.440
relative to who.

14:27.440 --> 14:33.760
Exactly, but also, you have other, other face recognition might still be, still arrived,

14:33.760 --> 14:37.040
still open to so much controversy.

14:37.040 --> 14:45.800
But you have other examples such as, you know, Facebook recently got a patent for socio-economic

14:45.800 --> 14:54.080
group classification of their users and they haven't said much about how, where they

14:54.080 --> 14:57.600
are going to apply it or how they are going to use it.

14:57.600 --> 15:04.880
But, you know, tools like that, you can see it's insidious and anything, it's very, very

15:04.880 --> 15:10.760
unlikely, anything positive or anything good will come out of it, especially for users,

15:10.760 --> 15:20.760
for people who's socio-economic status is, you know, from a really poor background.

15:20.760 --> 15:29.480
So, the idea of, I guess, relational ethics as well as questioning, do we need these tools

15:29.480 --> 15:31.560
in the first place?

15:31.560 --> 15:39.800
It's also thinking about, you know, the bigger picture of what automation, whether it's

15:39.800 --> 15:49.960
job applications or whether it's housing or whether it's insurance, it's what it's doing

15:49.960 --> 15:52.680
to society.

15:52.680 --> 15:59.480
And what kind of values are we prioritizing and embracing in the process?

15:59.480 --> 16:09.000
So, it's kind of thinking of ethics more of, more as a habit, as kind of constantly thinking

16:09.000 --> 16:15.360
of what kind of society we want to live in as opposed to thinking of, I have this piece

16:15.360 --> 16:24.360
of tool or this piece of equipment and how do I make it fair or how do I twitch with

16:24.360 --> 16:29.520
it or work it to find that balance?

16:29.520 --> 16:39.520
You mentioned earlier the, you know, that a lot of fairness is thinking about data bias

16:39.520 --> 16:47.040
and accommodating for data bias, you know, setting aside the issue of whether the thing that

16:47.040 --> 16:53.800
you're trying to address, you know, something that should be done at all, that, you know,

16:53.800 --> 17:00.880
the issue of data bias is kind of just one small piece of the overall fairness puzzle.

17:00.880 --> 17:08.080
How do you think broadly about kind of the different aspects of AI fairness and AI ethics?

17:08.080 --> 17:14.160
Do you have a categorization or framework or, you know, way of thinking about it that you

17:14.160 --> 17:15.160
found helpful?

17:15.160 --> 17:16.160
Yeah.

17:16.160 --> 17:23.320
So, this is actually at the heart of the whole relational ethics trying to reframe the

17:23.320 --> 17:25.960
whole idea of what ethics is.

17:25.960 --> 17:34.960
So, because as you said, a lot of people working on AI ethics really are about, you know, whether

17:34.960 --> 17:42.640
it's explainability or calculating fairness or justice, it really is usually lost in

17:42.640 --> 17:44.200
the fine-grained details.

17:44.200 --> 17:51.280
So, it's not something implementable that I provide, but it's about kind of really zooming

17:51.280 --> 17:59.680
out and thinking, you know, what are we doing, what are we prioritizing, how are we defining

17:59.680 --> 18:07.800
bias, how are we defining ethics, how are we approaching these concepts in general.

18:07.800 --> 18:17.480
And one of the aspects that I've read that as part of the paper that I emphasized is,

18:17.480 --> 18:23.520
I guess it's the nature, the inherent nature of machine learning, which is that we are

18:23.520 --> 18:31.400
continually predicting whether it's health issues, whether it's socio-economic issues, whether

18:31.400 --> 18:37.720
it's who is going to be, you know, the best employee, it's all about making predictions

18:37.720 --> 18:41.000
based on whatever data we get our hands on.

18:41.000 --> 18:48.840
So, the idea of relational thinking is kind of rethinking the whole idea of predicting

18:48.840 --> 18:54.160
as something we need to stop and pause and think.

18:54.160 --> 19:01.920
Instead of continually predicting how about we kind of take it easy and first analyze

19:01.920 --> 19:09.160
and think about the patterns that we are getting, trying to understand the reality and things

19:09.160 --> 19:17.760
as they are, as opposed to using whatever data we have as evidence for our prediction

19:17.760 --> 19:23.080
or as input into our predicting tool.

19:23.080 --> 19:30.000
So the one of the examples I give is Cati O'Neill in Weapons of Destruction also mentions

19:30.000 --> 19:41.160
this is if you take algorithms used in policing in the legal system instead of, say, recidivism

19:41.160 --> 19:47.720
algorithms instead of striving to predict who is likely to reoffend or who is likely

19:47.720 --> 19:56.360
to commit crime or what area should be policed more, we use our tools or we develop our tools

19:56.360 --> 20:05.120
in a way that lend themselves for us to understand why are this group of, this demographic

20:05.120 --> 20:10.280
or this group of people coming up as higher risks?

20:10.280 --> 20:11.280
What can we do?

20:11.280 --> 20:17.440
How do we rehabilitate, say, prisoners instead of how do we catch them when they reoffend?

20:17.440 --> 20:19.960
So it's really switching mentality.

20:19.960 --> 20:26.200
It's thinking about how do we make the society or the better, the world a better place?

20:26.200 --> 20:32.800
How do we help people get back on their foot rather than how do we, you know, rather

20:32.800 --> 20:36.760
than playing a gacha rather than how do we catch them again?

20:36.760 --> 20:43.480
But so thinking of kind of prioritizing understanding, you know, questioning why do we find

20:43.480 --> 20:50.200
these patterns that we are finding and how do we improve that really kind of aligns with

20:50.200 --> 20:56.080
this relational thinking I've been, I've been talking about as opposed to, you know, creating

20:56.080 --> 20:59.480
and building these predictive tools.

20:59.480 --> 21:05.920
Yeah, and how it was interesting that there's multiple levels to this idea of prioritizing

21:05.920 --> 21:06.920
understanding.

21:06.920 --> 21:13.680
There's, you know, as, you know, individuals working in these areas, we should prioritize

21:13.680 --> 21:22.760
our understanding of the people involved in the scenarios that were involved in and how

21:22.760 --> 21:26.760
the people are interacting and affected in these various scenarios.

21:26.760 --> 21:33.160
But also, you're also suggesting that we should prioritize, you know, the tools that we

21:33.160 --> 21:38.600
build to enhance our understanding as opposed to, you know, just spitting out more and more

21:38.600 --> 21:39.600
predictions.

21:39.600 --> 21:40.600
Yeah, yeah.

21:40.600 --> 21:41.600
Exactly.

21:41.600 --> 21:50.360
I haven't yet seen many tools that aim to understand so much of what I come across is

21:50.360 --> 21:52.920
always predictive tools.

21:52.920 --> 22:00.360
And I think prioritizing understanding really will contribute to, you know, the larger,

22:00.360 --> 22:03.480
greater, greaterness of society.

22:03.480 --> 22:10.800
Again, this is not something you can formulate or you can come up with a set of steps that

22:10.800 --> 22:11.800
you can implement.

22:11.800 --> 22:19.240
It's, it's more of kind of changing your habit, developing a different set of habits.

22:19.240 --> 22:27.240
It's something you continually keep in the back of your mind, whether you are an ecosystem

22:27.240 --> 22:31.680
or an engineer or a data scientist.

22:31.680 --> 22:38.560
So it's really zooming out and looking at the larger picture.

22:38.560 --> 22:49.080
But also, it's not to oppose that we should throw out all implementable tools we have on

22:49.080 --> 22:57.720
whether it's fairness or accountability or explaining, explainability, and it's just

22:57.720 --> 23:01.800
that we have to also look at the larger picture.

23:01.800 --> 23:09.960
And I guess another aspect of relational ethics is you might have these implementable tools,

23:09.960 --> 23:18.280
you might have the set of tools to make your system better, but the idea is if you think

23:18.280 --> 23:28.000
of these concepts such as fairness or ethics or even your own set of solutions as something

23:28.000 --> 23:30.200
that are continually changing.

23:30.200 --> 23:35.800
So this is at the, I guess this goes back to at the start I was talking about how the

23:35.800 --> 23:45.760
idea of embodied cognitive science at its core comes from systems thinking and cybernetics

23:45.760 --> 23:54.360
and the social sciences, and at the heart of it is that not only can you define cognition

23:54.360 --> 24:01.160
in isolation from others or in isolation from the tools you use or in isolation from

24:01.160 --> 24:07.320
the environment, it's also that whatever your definition of cognition or whatever your

24:07.320 --> 24:15.440
understanding of the person has to account for the nature of reality, which is that its

24:15.440 --> 24:22.640
never stable, it's never fixable, it's constantly changing, so and it's very contextual.

24:22.640 --> 24:31.240
So you are some a certain type of personality at the moment with certain expected norms

24:31.240 --> 24:37.760
talking to me on the on here, but some other time in some different contexts, in some

24:37.760 --> 24:42.080
different environments, you are also slightly different person.

24:42.080 --> 24:49.080
So the underlying idea is whatever concepts we are dealing with, whatever solutions we

24:49.080 --> 24:57.640
have, they cannot claim to to finalize things, they cannot stabilize this continually moving

24:57.640 --> 25:04.840
nature of being and whatever is ethical in this context might not be ethical in other contexts.

25:04.840 --> 25:11.840
So I think relational ethics helps you leave whatever solution you have somewhat partially

25:11.840 --> 25:19.760
open, so that you can reiterate, so you can revise and change with whatever new evidence

25:19.760 --> 25:25.600
or new data comes up your way the next day or the next year.

25:25.600 --> 25:33.880
So this treating of things as moving and changing really is fundamental, it helps us realize

25:33.880 --> 25:43.600
our solution now is only for now with an unlimited context, with an unlimited environment.

25:43.600 --> 25:49.800
And I think that's a really important thing we can all pay attention to.

25:49.800 --> 25:55.440
The example you gave of who is Sam in these different contexts makes me think a little

25:55.440 --> 26:04.280
bit about and linguistics, the idea of code switching and I may speak in a particular

26:04.280 --> 26:08.920
way when I'm on the podcast and then when I'm at home, I may speak in a slightly different

26:08.920 --> 26:14.680
way and when I'm out in the neighborhood, I might speak in a slightly different way.

26:14.680 --> 26:19.920
And I haven't seen much in machine learning or NLP that tries to capture that or take

26:19.920 --> 26:28.400
a account of that. Do you have some examples of examples of how you might envision machine

26:28.400 --> 26:34.160
learning systems, you know, if they were to follow this aspect of relational.

26:34.160 --> 26:46.160
Yeah, so this is really difficult and I guess at the heart of a lot of issues and so

26:46.160 --> 26:56.440
when you can assume things are stable and somewhat, you know, you can grasp them with whatever

26:56.440 --> 27:04.880
tools or language you have, it's much easier to construct theories or to construct some

27:04.880 --> 27:13.280
sort of tool which is why I mean that this stability, you know, translates to the,

27:13.280 --> 27:17.920
you know, everything's coming from a identical distribution, which is at the foundation

27:17.920 --> 27:21.120
of most of what we do in machine learning.

27:21.120 --> 27:27.040
Yeah, again, I'm not really a computer scientist, as I say at the beginning, I'm a cognitive

27:27.040 --> 27:35.800
scientist and I think about cognition in persons and I don't know any NLP tools or machine

27:35.800 --> 27:45.720
learning approaches that account for this continual change and contexts.

27:45.720 --> 27:51.920
But also even within the cognitive science movement, especially embodied cognitive science

27:51.920 --> 28:02.520
which is trying to push the importance of these change, you know, language and context

28:02.520 --> 28:11.000
is one of the things it struggles with is because it's difficult to formalize and make

28:11.000 --> 28:20.520
up, provide something conclusive, but when you are underlying change, it ends up dealing

28:20.520 --> 28:27.000
with a lot of theorizing as opposed to producing something, something you can model or something

28:27.000 --> 28:34.120
you can formalize. So I guess it's an existing tension.

28:34.120 --> 28:42.840
Again, it's much better to think of it as a habit and to acknowledge this continual

28:42.840 --> 28:51.720
changing nature of things in a sense that acknowledgement makes you aware that your

28:51.720 --> 28:59.720
tool or your solution or your theory is only as good as the, it's a specification and

28:59.720 --> 29:10.440
the context and that acknowledgement further encourages you to live not to conclude your

29:10.440 --> 29:17.320
solution or your tool as something finalizable, something that will be good all the times

29:17.320 --> 29:24.560
for all contexts, but something that you have to leave a little open partially open, something

29:24.560 --> 29:34.120
that needs revision continually. So that's again, for me, for me at the moment, the

29:34.120 --> 29:43.560
best one can do is acknowledge this change and context and leave this partial openness

29:43.560 --> 29:55.320
and embrace reiteration and revision. One of the other ideas in the paper is that, or at

29:55.320 --> 30:01.880
least I, you know, I interpret it as that along the lines of the idea of prioritizing understanding

30:01.880 --> 30:07.160
over prediction, one of the ideas in the paper is that, you know, when we predict, it's

30:07.160 --> 30:13.480
often based on these very reductive labels that we're applying to things. The examples

30:13.480 --> 30:20.120
you gave are successful versus not criminal versus not, and you kind of point out that that

30:20.120 --> 30:29.640
is inherently problematic in many cases. Exactly. You can also look at a lot of algorithms

30:29.640 --> 30:38.440
with them trying to categorize or identify gender identities and I think that's one of the

30:38.440 --> 30:49.400
most obvious cases where the harm of doing so, the harm of categorization becomes very starkly clear

30:50.360 --> 30:56.920
because usually stereotypically and in most societies, you would categorize

30:56.920 --> 31:09.160
gender as, you know, a male or female. Sometimes you might have bisexuals, but as we know, gender

31:09.160 --> 31:18.360
identities are much more, more than just those categories and not only they are larger in number

31:18.360 --> 31:28.840
those categories, but we all know that they are fluid. Someone that was bisexual or take a

31:28.840 --> 31:36.600
trans person, for example, people change their sexual and gender identities. How do you then?

31:36.600 --> 31:43.800
Then it becomes easy to see how difficult it is for whatever algorithmic tools we are developing

31:43.800 --> 31:52.760
to account for that change. But also, as we developed that tool in category, kind of come up with

31:52.760 --> 31:58.840
these categories, in a sense where this advantage and excluding anybody that doesn't

31:59.560 --> 32:06.440
belong in those categories that we have created, this is where, again, the most vulnerable are,

32:06.440 --> 32:13.800
you know, impacted the most. Yeah, so it's problematic in that regard.

32:15.080 --> 32:17.960
I'm curious what kind of reaction you've seen to the paper.

32:18.920 --> 32:29.000
So, at Nureps, it was overwhelmingly positive. It was my first time in Nureps and I went in thinking,

32:29.000 --> 32:33.160
oh, this is, you know, a machine learning AI conference. I'm just, you know, the

32:33.160 --> 32:42.280
outlier cognitive scientist, slash atheists. So I went in feeling I'm not going to fit very well.

32:43.080 --> 32:50.600
But it was really, really positive. And I was, even when I was, when the announcement that

32:50.600 --> 32:58.680
my paper had won the base paper came, I just could not believe it. You know, as a grad student,

32:58.680 --> 33:05.000
you go to conferences, you present a poster or whatever. And some parts of you sometimes,

33:05.000 --> 33:09.640
you know, deep down, you think, oh, I might win, you know, I might have a chance for a

33:09.640 --> 33:15.400
base poster or something like that. But I went into Nureps, like, there is no chance, I'm just

33:15.400 --> 33:23.800
going to relax. Enjoy the dinner. It was dinner party. And I was really shocked. And so it's,

33:23.800 --> 33:32.200
it's been really positive. I have presented similar ideas previously to very exclusively,

33:33.080 --> 33:40.840
kind of very software engineer, machine learning, deep learning, researchers. And

33:41.960 --> 33:47.320
people really are not interested in my ideas because people want something implementable,

33:47.320 --> 33:55.000
something they can code into, you know, something formal, something they can use. So what I'm asking

33:55.000 --> 34:04.360
is a reframing, a rethinking and in a sense, a changing of habits. And it's almost like an

34:04.360 --> 34:09.960
activism. It's like asking, what kind of society do you want to live in? So for some people,

34:09.960 --> 34:17.880
that's really difficult and something they would rather not get involved in. But the more I

34:17.880 --> 34:25.560
interact with people, but and also on Twitter, it's, it's really, really encouraging. People seem

34:26.600 --> 34:35.240
to like what I have to say. So I'm happy. Really quickly before we wind down, you are in New York

34:35.240 --> 34:41.640
to present a more recent paper. I believe it's more recent paper that you have worked on robot

34:41.640 --> 34:48.680
rights. Can you talk a little bit about that paper? Yes, it's unfortunate that the title has

34:48.680 --> 34:56.120
robot rights because it really is not about robot rights. It's robot rights question mark.

34:56.120 --> 35:03.640
Let's talk about human welfare instead. So this paper, I worked on it with my colleague,

35:03.640 --> 35:10.760
Yeli Van Dyke from University of Twenth. He also, he also comes from a distributed

35:10.760 --> 35:16.920
cognition in body cognitive science background. And we talk about this a lot on Twitter.

35:17.800 --> 35:25.640
And we are constantly getting caught up in this Twitter debates whether, you know,

35:25.640 --> 35:32.920
machines can be sentient or whether robots should be given rights, blah, blah, it goes on.

35:32.920 --> 35:40.600
And it's the same kind of pattern of interaction. So very and over and over again. And I think about

35:40.600 --> 35:46.040
five, four, four, five months ago, I asked him on Twitter, how about we write a paper on this?

35:46.600 --> 35:53.320
And writing the paper came really, really easy because we have the same background. We think I like

35:53.320 --> 36:01.880
the idea of the paper is it has in a sense, it's twofold. The first one is kind of philosophical.

36:03.080 --> 36:12.360
So we lay out how robots are not the type of beings that can either be granted or denied rights.

36:12.920 --> 36:21.640
We lean on a lot of, you know, embodied coxay as I was saying earlier, this notion of cognition

36:21.640 --> 36:28.600
as inherently social, inherently relational, people inherently, you know, value-laden,

36:28.600 --> 36:36.840
constantly striving to make meaning of the world. So we use that post-cartesian approach to

36:37.800 --> 36:44.120
to get at the heart of how philosophically speaking robots or animation learning tools

36:44.120 --> 36:53.160
or any missions at all are not the same beings as humans or even animals. And then the second part

36:53.160 --> 37:02.040
we get at the urgent questions that AI ethics really need to focus. Because sometimes, not sometimes,

37:02.040 --> 37:10.120
most times, it's really frustrating to hear robot ethics classified as part of AI ethics. And

37:10.120 --> 37:18.440
for me, personally, it comes across as worrying about future, may happen, may not happen, may become

37:18.440 --> 37:27.720
sentient. A lot of it is really contemplation and thinking ahead about the future. And it's

37:28.440 --> 37:35.560
all that contemplation and philosophical musing, taking so much of the AI ethics space is just unfair.

37:35.560 --> 37:44.760
So a lot of the second part of our paper deals with how the very idea of AI itself, whether it's,

37:44.760 --> 37:51.480
you know, computer vision, whether it's autonomous systems, it's never autonomous. There is

37:51.480 --> 37:58.760
always humans in the loop. And not only that, it's not possible without the exploitive human

37:58.760 --> 38:06.440
labor, whether it's tagging your old data for that's going to be part of an autonomous system,

38:06.440 --> 38:13.000
or some sort of image recognition. Even when you do recaptures, you are in a sense

38:15.080 --> 38:22.600
kind of being, you are putting in your own unpaid labor into making mission space.

38:22.600 --> 38:30.440
So the argument we made, we make there is that AI systems are never autonomous and they never

38:30.440 --> 38:37.800
will be. But in the argument of whether they are autonomous or not, we lose sight of the people

38:37.800 --> 38:46.520
who are underpaid, such as the mechanical Turk or micro workers, they never enter into the

38:46.520 --> 38:55.000
debates. We also teach upon, you know, how the robot systems, such as, you know, Roomba or whatever

38:55.000 --> 39:02.200
are invading private spaces as they roam around our houses and how that should be more

39:03.000 --> 39:11.640
urgent and crucial as opposed to some stereotypical humanoid, such as Rob, what's your name, Sophia.

39:11.640 --> 39:20.360
And yeah, and we get a little bit on algorithmic injustice as well, how the least privileged,

39:21.240 --> 39:28.520
the most disenfranchised are the most impacted and how that should be the focus of AI ethics

39:28.520 --> 39:34.040
as opposed to, you know, hypothetical sindian things. So that's the core of the paper.

39:36.600 --> 39:41.080
And there's been quite a bit of discussion about this one on Twitter. In fact, we're not going to

39:41.080 --> 39:49.640
be able to get into it very deeply, but I would encourage folks to take their reactions to Twitter.

39:49.640 --> 39:53.160
Is it fair to say that this one has been more controversial than the previous one?

39:53.160 --> 40:00.760
It appears to be so. And to be honest, when we wrote it, we wrote it as, when we have those

40:01.960 --> 40:07.640
never ending conversations about rights again on Twitter, instead of repeating the conversation,

40:07.640 --> 40:16.360
we'll just have a paper to point to. But it has provoked a lot of very strong reaction from people

40:16.360 --> 40:24.440
both defending rights for robots and both thinking it's really idiotic to even discuss rights for

40:24.440 --> 40:31.400
robots. So apparently it's controversial. Interesting. People love their robots, I guess.

40:31.400 --> 40:41.800
Yeah, I guess. Yeah. Yeah. Well, Ababa, it has been so great to have a chance to chat with you

40:42.520 --> 40:47.560
in more detail about what you're up to. Thanks so much for taking the time to share with us.

40:47.560 --> 40:50.200
Thank you so much. It's been great. Thank you.

40:52.200 --> 40:57.080
All right, everyone. That's our show for today. For more information about today's guest,

40:57.080 --> 41:04.120
visit twommalai.com slash shows. To learn more about the AI enterprise workflow study group

41:04.120 --> 41:10.680
I'll be leading, visit twommalai.com slash AI workflow. Of course, if you like what you hear on

41:10.680 --> 41:16.600
this podcast, please take a moment to subscribe, rate, and review the show on your favorite

41:16.600 --> 41:30.120
pod catcher. Thanks so much for listening and catch you next time.

