WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.040
I'm your host Sam Charrington. Today we're joined by Nasreen Mastifazade, senior AI research

00:35.040 --> 00:41.360
scientist at New York-based Elemental Cognition. Our conversation focuses on Nasreen's work

00:41.360 --> 00:46.800
in event-centric contextual modeling and language and vision, which she sees as a means of giving

00:46.800 --> 00:52.600
AI systems a bit of common sense. We discuss Nasreen's work on the story closed tests, which

00:52.600 --> 00:58.360
is a reasoning framework for evaluating story understanding and generation. We explore the details

00:58.360 --> 01:03.200
of this task, including what constitutes a story, and some of the challenges it presents and

01:03.200 --> 01:09.120
approaches for solving it. We also discuss how you model what a computer understands, building

01:09.120 --> 01:14.920
semantic representation algorithms, different ways to approach explainability, and multimodal

01:14.920 --> 01:19.720
extensions to her contextual modeling work. Enjoy.

01:19.720 --> 01:25.720
Alright everyone, I am on the line with Nasreen Mastifazade. Nasreen is a senior AI research

01:25.720 --> 01:31.120
scientist at Elemental Cognition in New York City. Nasreen, welcome to this week in machine

01:31.120 --> 01:34.800
learning and AI. Thanks Sam, thanks for having me.

01:34.800 --> 01:39.920
So I hear that you started working on AI in high school. Tell us a little bit about that

01:39.920 --> 01:48.240
story. Sure, yeah. So I was into computer science and computer engineering in general.

01:48.240 --> 01:53.920
Like, when I was a kid, you know, I basically, that was the toy that I had. And I always

01:53.920 --> 01:58.680
loved the idea of doing something meaningful, but in a sense that, okay, I'm spending

01:58.680 --> 02:04.320
a lot of time in front of computer doing different stuff. What is the best thing that you

02:04.320 --> 02:09.160
can accomplish? And somehow I was introduced to the notion of programming and the fact

02:09.160 --> 02:15.000
that you can build like pieces of software to do things for you. And to me, the idea of

02:15.000 --> 02:20.200
building a piece of software that kind of automates what you do was really intriguing,

02:20.200 --> 02:27.520
which is how I got into robotics initially. So, you know, I started working on robotics

02:27.520 --> 02:32.480
for the couple of my really great friends back in high school that, you know, we then

02:32.480 --> 02:39.800
ended up competing in Robocop competitions, which is this, you know, annual worldwide competitions

02:39.800 --> 02:47.120
among different roboticists to, you know, accomplish different tasks, basically, with robots.

02:47.120 --> 02:51.920
And yeah, so the story, you know, we'll go forward with like how I did that for like a

02:51.920 --> 02:56.960
year and a half, and it was fantastic and we accomplished a lot. And I was always under

02:56.960 --> 03:03.840
impression that it's actually a very complex task, right? You build these multi-agent systems

03:03.840 --> 03:08.160
that are capable of integrating their world model with like different communications

03:08.160 --> 03:13.480
that then the agents and like control aspects, like mechanics, imagined electronics, and

03:13.480 --> 03:19.920
all those sorts of things are really complex. And then somehow through some things that

03:19.920 --> 03:25.360
I'm not going to tell the story of today, I got introduced to the idea that National Language

03:25.360 --> 03:30.320
Understanding and National Language Processing as a field in general is much more complex

03:30.320 --> 03:37.280
than robotics. And that kind of blew my mind like, as I said, I thought I'm working on something

03:37.280 --> 03:42.720
that is, you know, super high tech, which it was, but turned out that at the time, this

03:42.720 --> 03:48.480
is like 12 years ago, there were no systems that could do National Language Understanding

03:48.480 --> 03:54.080
at the level of a four-year-old kit. So, this was an interesting challenge for me to take,

03:54.080 --> 03:59.280
which is how I kind of switched gears and started working on National Language Understanding

03:59.280 --> 04:06.400
and National Language Processing since then. And so, the focus of your work since then has been

04:06.400 --> 04:12.720
largely centered around this idea of event-centric contextual modeling in language and vision.

04:12.720 --> 04:21.360
What exactly does that mean? Yeah, so I think we can start with the events first, right? So,

04:21.360 --> 04:27.600
I've always been interested in looking at the world, basically, through the lens of events,

04:27.600 --> 04:33.920
because events are such central entities of the world through which we, you know,

04:33.920 --> 04:40.160
go about our daily lives, we see things that happen as a result of some stuff that happen,

04:40.160 --> 04:45.760
which is an event. So, an event can cause something else. And then our understanding of the

04:45.760 --> 04:51.840
dynamics of such events and how they shape our world on a day-to-day basis is really, you know,

04:51.840 --> 04:58.560
a crucial part of any kind of cognitive ability. And that makes, you know, me and a lot of other AI

04:58.560 --> 05:04.400
scientists very interested in modeling events. So, that's the event-centric part. And then the

05:04.400 --> 05:10.880
context modeling is about, okay, I have an AI system, I want to build an AI system that can get

05:10.880 --> 05:15.680
an input and then produce some sort of an output, right? That input could be something super complex,

05:15.680 --> 05:22.800
imagine an entire world being perceived by a situated robot, or it could be a piece of text,

05:22.800 --> 05:28.960
right? Or it could be a piece of, you know, an input, which is multimodal, so both text and an

05:28.960 --> 05:35.440
image. So, that's where what context means in my work, basically. So, contextual modeling means

05:35.440 --> 05:42.000
how would the AI system deal with, representing and understanding the input that is provided with.

05:42.000 --> 05:47.040
And then the language on vision part is basically where I've applied this set of, you know,

05:48.000 --> 05:51.280
you know, different pieces of work on event-centric contextual modeling.

05:51.280 --> 05:57.200
My am in nature, I'm in natural language processing researcher, but I've worked on applying

05:57.200 --> 06:03.840
different, you know, AI, you know, methodologies, basically, in the world of vision and language,

06:03.840 --> 06:08.560
which has become, you know, more hot, basically, in the past three years or so.

06:08.560 --> 06:14.800
So, what are some of the types of problems that you come up with and try to solve in this

06:14.800 --> 06:21.920
context of event-centric contextual modeling? So, there are different aspects of the world that

06:21.920 --> 06:28.160
we can basically see through the lens of events, right? One major piece of work that I've done

06:28.160 --> 06:33.760
is on understanding stories, right? As you can imagine, narratives or stories are these

06:33.760 --> 06:41.600
sequence of really eventful sentences that we, you know, basically have to go through on a daily

06:41.600 --> 06:47.280
basis because as humans, we tell stories all the time, right? When we communicate stories,

06:47.280 --> 06:52.480
our major part of how we communicate with each other. And so, one of the, you know, lines of work

06:52.480 --> 06:57.040
that I've invested on in the past couple of years has been on narrative understanding or story

06:57.040 --> 07:02.720
understanding where you want to build the AI systems. They can read, you know, a coherent

07:02.720 --> 07:08.960
sequence of sentences which are event-centric in, you know, in nature. And then it should understand

07:08.960 --> 07:15.120
it in a way that it can answer questions about it. And more so that it could also be able to

07:15.120 --> 07:21.200
generate meaningful stories. So, this work, you know, basically involves not only understanding

07:21.200 --> 07:27.280
a piece of text which happens to be narrative, but also be able to generate, you know, a sequence

07:27.280 --> 07:33.520
of sentences that go together coherently as a meaningful story. And so, how do you tend to approach

07:33.520 --> 07:41.680
that type of problem? Yeah, so, you know, there are different things that you should take care of,

07:41.680 --> 07:47.280
right? If you want to build an AI system that can, say, understand stories. First and foremost is,

07:47.280 --> 07:53.920
okay, what is an event in the story, right? Are we talking about, like, you know, novel by Shakespeare

07:53.920 --> 08:01.200
or are we talking about, like, a sequence of, I don't know, four words? There are people who call

08:01.200 --> 08:07.760
sequence of five words also stories. So, in my work, I particularly, you know, focused on

08:07.760 --> 08:14.960
understanding a sequence of five sentences stories. So, these are, we call it common-sense stories

08:14.960 --> 08:22.080
in a way that we are basically interested in understanding the most, like, daily things that

08:22.080 --> 08:28.640
happen to anyone, you know, in general. That's why we call it common-sense. And we go about

08:28.640 --> 08:32.080
understanding them. So, that's the first thing you have to address, right? What is an event in

08:32.080 --> 08:37.200
the story? After you address that, then you should think about, okay, now I want to build a system

08:37.200 --> 08:42.960
that can understand this, just kind of an input. And then it goes about different, you know, steps

08:42.960 --> 08:49.040
that you should take. So, one is, okay, how do I even represent this piece of, you know, input,

08:49.040 --> 08:55.040
which is a narrative? So, it goes, you know, hand in hand with notions of knowledge representation.

08:55.040 --> 09:00.640
So, in the AI community, from, like, you know, back decades ago, people invested a lot of time

09:00.640 --> 09:06.640
and effort on knowledge representation. And in the NLP community, in particular, we have a lot

09:06.640 --> 09:11.760
of literature on semantic representation and meaning representation, right? So, in my work,

09:11.760 --> 09:17.760
I'm mainly interested in extracting events, right? So, what is, in particular, important about

09:17.760 --> 09:23.200
this story, let's call those events and let's extract those and call it the kind of representation

09:23.200 --> 09:28.160
that I'm interested in. So, that will be basically the second thing you do. And then the third

09:28.160 --> 09:33.520
thing is, okay, now I want to, you know, basically connect the dots. How do I know what are the

09:33.520 --> 09:40.560
stereotypical relations that exist between these events in a story? And then that will give you

09:40.560 --> 09:46.640
this so-called narrative structure of the story that you can basically get in as an input to any

09:46.640 --> 09:52.240
other, you know, system that wants to, say, answer questions about this story. So, that's more or

09:52.240 --> 09:56.160
less, you know, in big picture, how you would go about a story modeling?

09:57.680 --> 10:03.200
So, you started out with talking about how the first thing to understand is, what is a story?

10:03.760 --> 10:07.920
But it strikes me that there's also this question, you know, what do we even mean by understand,

10:07.920 --> 10:15.680
when we're talking about a computer understanding a story? And there's more to it necessarily than,

10:15.680 --> 10:22.720
you know, answering questions. How do you model or assess a computer's ability to understand

10:22.720 --> 10:27.600
a story? Or is that even, you know, part of what you're trying to get at, or is it more performance

10:27.600 --> 10:33.520
on individual tasks? Yeah, that's a very good question, right? And I would say a fundamental question

10:33.520 --> 10:39.520
for the entire AI community. So, the way we go about defining what understanding even is and

10:39.520 --> 10:45.280
the field these days is through these benchmarks that we define, right? So, actually, I personally,

10:46.000 --> 10:50.960
you know, contributed to a benchmark for story understanding, which is a story closed test.

10:51.600 --> 10:58.720
So, the kind of benchmark we defined in particular was as follows. So, the AI system is supposed

10:58.720 --> 11:05.680
to read a sequence of four sentences, which is the context of the story, basically. And then the

11:05.680 --> 11:10.640
task is to predict the ending to that story. So, in the five sentences, the stories that I told you,

11:10.640 --> 11:15.200
you basically imagine that you dropped the last sentence, and the task for the AI system is to

11:15.200 --> 11:22.400
predict that. So, I personally believe that is a good kind of a proxy for modeling, whether or not

11:22.400 --> 11:27.920
a system is understanding the story that it has read. That's one way of putting it, but as it goes

11:27.920 --> 11:36.400
with many benchmarks in the AI world, whenever you have a task and you make it into a benchmark and

11:36.400 --> 11:42.720
then you collect a very particular narrow, in a sense, a test set for it could be hacked, right?

11:42.720 --> 11:49.200
And it goes to say that maybe we shouldn't just define understanding in terms of beating a

11:49.200 --> 11:55.440
particular benchmark, but more so deploying systems in the wild. So, I would say that there are

11:55.440 --> 12:01.120
different ways that you can define what true understanding even is. Having benchmarks is a good

12:01.120 --> 12:06.960
way of making, you know, having proxies, right? And having basically evaluating ourselves as we move

12:06.960 --> 12:12.560
forward, but they're not the ultimate. This should not be the ultimate end goal. I would say of

12:13.200 --> 12:20.400
what we call true understanding. So, I will go to, you know, to add that. So, understanding

12:20.400 --> 12:25.440
could be through the lens of answering questions, right? So, I ask the system, what is the ending to

12:25.440 --> 12:30.560
this story? And if it answers it correctly, that means it's understood, right? But as it goes,

12:31.360 --> 12:36.400
it could be hacked, meaning that without true understanding, maybe a system, a black box system

12:36.400 --> 12:42.560
can actually specify the right ending. If you build systems that can explain themselves,

12:43.280 --> 12:48.240
that could be a win, I would say. So, which is a focus that I have at Elemental Cognition now,

12:48.240 --> 12:54.240
where we are building AI systems, in particular, a story understanding systems that can explain

12:54.240 --> 13:01.760
the decisions they make, which is a better way of kind of knowing what's behind the scenes,

13:01.760 --> 13:05.360
what's, you know, under the hood for the system that is making a decision.

13:06.080 --> 13:10.960
Before we dive into that, I want to make sure I understand the story closed test. You

13:10.960 --> 13:21.680
are training your system, presumably, on some corpus of five sentence stories. And then you're giving it

13:22.320 --> 13:29.120
four sentences that form some new unseen story and asking it to complete the sentence.

13:29.760 --> 13:33.520
And it's to complete the story, yeah. To complete the story, right? So, to provide the

13:33.520 --> 13:43.920
exact three. The final sentence. And in doing so, you demonstrate that it can draw out entities

13:43.920 --> 13:53.360
and contexts from the story and present them in some way that makes sense. Is it, you know,

13:53.360 --> 13:59.440
does, is a human grading the, the responses? I guess the, the origin of that question

13:59.440 --> 14:03.680
is strikes me that generally speaking for this kind of task, you could have multiple correct

14:03.680 --> 14:10.720
output answers. So how does that work? Exactly. So that's actually what we did. We ended up collecting

14:10.720 --> 14:15.520
alternative endings and then the AI, you know, system is posed with two alternatives. One is a

14:15.520 --> 14:19.920
wrong ending, one is the right ending, and then the task is to choose the right one. So that's

14:19.920 --> 14:24.160
actually the kind of a classification task that we ended up doing. But as you can imagine,

14:24.160 --> 14:30.320
the task could be generation, right? The agent could be just posed with the four sentences and then

14:30.320 --> 14:35.040
the task is to generate the ending, basically open ended, right? As opposed to multi-choice

14:35.520 --> 14:41.360
question. But yeah, for the actual story closed, as we ended up collecting the right and wrong

14:42.000 --> 14:47.440
endings by crowdsourcing and then at the end of the day, it becomes a classification task.

14:47.440 --> 15:00.080
And sourcing these answers, were you targeting specific mechanisms of correctness, meaning were

15:00.080 --> 15:07.840
you trying to test specific aspects of the algorithm? So for example, I forget whose work this is,

15:07.840 --> 15:14.400
maybe I've seen it in the context of Josh Tenenbaum's work, but this, the idea that, you know, within

15:14.400 --> 15:20.960
this concept of context, there's so much that's unsaid. You know, the cup is on the table,

15:20.960 --> 15:26.160
you know, there are physical forces that, you know, keep the cup on the table, it's not going to

15:26.160 --> 15:31.680
fall unless some other force pushes it off. That kind of thing, like, are you, I imagine you could

15:31.680 --> 15:37.360
create sentences that kind of test that common sense context, or there are other things that you can

15:37.360 --> 15:45.040
target to test with your sentences. Right. Yeah. So there are, you know, you can, I can imagine having

15:45.040 --> 15:52.480
stories that will, you know, kind of evaluate such cognitive abilities of a system. But the point

15:52.480 --> 15:59.120
about the story closed test is that it's about generic daily life events that happen to anyone,

15:59.120 --> 16:03.920
which is why they're called common sense. So I can give you an example. So this is an example,

16:03.920 --> 16:09.280
you know, a story closed test. So the context is as follows. Karen was assigned a roommate, her

16:09.280 --> 16:14.800
first year of college, and then the story continues that her roommate asked her to go to a nearby

16:14.800 --> 16:22.000
city for a concert, and then Karen agreed happily. The show was absolutely exhilarating,

16:22.000 --> 16:26.640
and then there are two alternative endings to the story. One is Karen became good friends with

16:26.640 --> 16:31.760
her roommate, and the wrong ending, the second ending, which I gave it away is the wrong ending,

16:31.760 --> 16:38.640
is Karen hated her roommate. Right. So this is the level of complexity and generality, actually,

16:38.640 --> 16:46.480
the VR tackling for the story closed test. So it's really about a daily story that has happened,

16:46.480 --> 16:51.920
can happen to anyone. And in particular, we have crowd sources, as I said, by very generic prompts

16:51.920 --> 16:57.600
to the workers. So it's more about predicting the next event, which happens to be the,

16:57.600 --> 17:02.640
you know, ending to the story, as opposed to the kind of phenomena that, yeah, you describe.

17:03.200 --> 17:09.760
Was the algorithm trained only on similar stories, or was it trained on other

17:10.720 --> 17:17.600
information that wasn't in that same story format? Yeah. So he kind of set up this

17:18.320 --> 17:24.240
challenge being the story closed test, so that people can do whatever they can to accomplish the

17:24.240 --> 17:32.880
task. So we did provide a corpus of 100,000 stories called rockestories. These are full five

17:32.880 --> 17:39.440
sentences stories that actually people can use kind of as a positive examples. They can

17:39.440 --> 17:44.640
mine narrative structures from them. They can build like models that predict what happens next

17:44.640 --> 17:50.960
in the story and so on, but it's not directly labeled data for the task of story closed test.

17:50.960 --> 17:56.320
And given that, we've left people, you know, free to use whatever corporate out there,

17:56.320 --> 18:02.640
whatever resources, common sense knowledge bases, or modeling techniques that they have to tackle

18:02.640 --> 18:07.360
the task. So that's basically on our end. We wanted people to, you know, the research community

18:07.360 --> 18:14.560
to basically improvise and be creative and bring in their own resources. So yeah, that's about it.

18:14.560 --> 18:21.840
What kind of engagement or uptake have you seen on this test? Have you seen any interesting

18:21.840 --> 18:31.440
approaches to building out algorithms to? Yeah. Yeah. So it happens that then, so the reason I made

18:31.440 --> 18:37.680
story closed test in the first place is that there wasn't any, you know, systematic way of

18:37.680 --> 18:43.840
evaluating story understanding in the field. So, you know, to get rid of my colleagues, we made

18:43.840 --> 18:50.800
this benchmark so that we have a way of evaluating our progressies moving forward in the field.

18:50.800 --> 18:56.400
And because, as I said, there wasn't any such benchmarking place, what we made actually got a lot

18:56.400 --> 19:02.960
of attention, specifically because we showed that human does 100% on this task. So we actually made

19:02.960 --> 19:08.640
sure that the test set that we put out there is like, you know, doubly human verify so that there

19:08.640 --> 19:15.840
are no boundary cases before choosing the right versus the wrong ending. And also the best,

19:16.560 --> 19:22.160
you know, state-of-the-art model that we trade that we tried on the benchmark was getting some

19:22.160 --> 19:29.040
very around like 59%. And as you can imagine, random baseline would do 50% human was doing 100%,

19:29.040 --> 19:34.960
so there was like more than 40% gap between the best system results and the human performance.

19:34.960 --> 19:39.520
So, you know, these two characteristics, I would say, really contributed to the task getting a lot

19:39.520 --> 19:44.240
of attention. And, you know, since then, this was released about two years ago, a lot of teams,

19:44.240 --> 19:48.800
different teams, you know, from academia and industry, however, have, you know, tackled the task,

19:48.800 --> 19:53.920
there has been so many results. But I go back to the point that I made about hacking, right? So

19:53.920 --> 19:59.520
after we released this data set for many months, maybe, you know, eight, nine months, ten months,

19:59.520 --> 20:04.320
there wasn't any significant improvement. And then we made it into a share task, meaning that, you

20:04.320 --> 20:11.120
know, we had a challenge and this workshop and there was a prize for the winner up. And then,

20:11.120 --> 20:16.640
when, whenever you do that, there are different approaches that will come at you, which is actually

20:16.640 --> 20:21.760
very healthy for a benchmark so that you know exactly what works and what doesn't. And in that,

20:22.480 --> 20:28.480
you know, challenge that we ran basically, that challenge day, there was a team submission from

20:28.480 --> 20:34.560
UDUB that had found out that actually without reading the context, which is the whole point about

20:34.560 --> 20:39.680
this national language understanding framework, without reading the context, you can leverage some,

20:40.800 --> 20:48.160
you know, stylistic features isolated in the endings, just alone that. Yeah, to find the right

20:48.160 --> 20:53.840
ending. And the interesting fact about this group is that so that, you know, the guy that actually

20:53.840 --> 21:00.480
contributes to this model used to work on detecting a fake eout previews and like detecting

21:00.480 --> 21:05.760
notions such as like age or gender from, from a piece of text. And it happened that turns out

21:05.760 --> 21:11.840
our wrong endings actually without our knowledge. Our wrong endings had some, had similar features

21:11.840 --> 21:18.640
as with fake reviews. So they had made this, you know, kind of a very interesting, you know,

21:18.640 --> 21:22.960
observation and they had leveraged that and turned out that you could do, you know, really much

21:22.960 --> 21:29.600
better if you just just train a classifier that just detects the such features. So that was a very

21:29.600 --> 21:33.840
interesting outcome of that challenge that we ran. And it's still, you know, there was a still a

21:33.840 --> 21:39.120
huge gap between human performance and their performance, which was around 70, like two, I think,

21:39.120 --> 21:46.240
or 76%. But it still was such a good example of what can go wrong when you collect data, right?

21:46.240 --> 21:51.120
For a narrow test, as I said, in AI. And this isn't something that has happened only to story

21:51.120 --> 21:57.920
close test, like other benchmarks in the field such as natural language inference, NLI or VQA

21:57.920 --> 22:05.040
visual question answering have basically showcased similar patterns that when we go about collecting

22:05.040 --> 22:12.960
data sets for testing purposes, they could be really biased. And often those biases are not even

22:12.960 --> 22:17.520
revealed to us. If you're lucky, we'll catch some of them. But often they're not revealed. And then,

22:17.520 --> 22:21.680
you know, we can go forward without knowing them and then patting yourselves on a shoulder that

22:21.680 --> 22:26.320
we are doing, you know, deep language understanding and like, you know, just, you know,

22:27.040 --> 22:31.200
claim victory that we are surpassing, I don't know, things like human performance, like

22:31.920 --> 22:39.600
but the truth is that a narrow benchmarks are narrow in the sense that they don't really represent

22:39.600 --> 22:43.520
the world. And we should be really careful with the way that we collect our data.

22:43.520 --> 22:49.520
Right. And I'm sorry I wanted this that we were very careful actually with the way we collected

22:49.520 --> 22:54.560
the story close this and still there were biases that there was no way we would have known ahead

22:54.560 --> 22:59.840
of time. So again, goes about saying how much more care as a community, as a research community,

22:59.840 --> 23:07.280
we should give to exploring, you know, different ways of collecting data and vetting them. And then,

23:07.280 --> 23:14.880
as I said, more so, I believe in the in the power of testing your systems on multiple frameworks

23:14.880 --> 23:19.600
and making sure that they scale beyond the particular training data that they're trained on

23:19.600 --> 23:24.480
or overfitted on basically. You created this benchmark. Have you also

23:25.600 --> 23:32.800
participated on the the other side of it building out models to try to improve on the state of the

23:32.800 --> 23:40.400
art? Yeah. So actually, so the one, you know, I worked on this. We had many different systems,

23:40.400 --> 23:46.400
as I said, but they weren't none of them were doing fantastically. They were basically kept at

23:46.400 --> 23:54.080
60-some percent. And then actually the most recent thing that we did after realizing that with the,

23:54.080 --> 24:00.320
you know, new results that are particularly shared in the challenge day was to come up with

24:00.320 --> 24:09.040
another dataset so that these hidden biases are kind of, you know, taking care of. And so now, you

24:09.040 --> 24:14.560
know, I research the research team that I'm working on right now at Elemental Cognition.

24:14.560 --> 24:20.960
We're working on different kinds of stories that are not exactly rocket stories, but technically,

24:20.960 --> 24:28.800
you know, you can test and, you know, evaluate our system on rocket stories as well. But it's just a

24:28.800 --> 24:33.520
more challenging task that we are tackling right now than the story closed test.

24:34.240 --> 24:40.720
Tell me about the types of approaches that you would take for, you know, either of these types of

24:40.720 --> 24:45.440
tasks. I guess when I hear semantic representation, the first thing that I think of from a deep learning

24:45.440 --> 24:51.920
perspective is like embeddings. Are there, does that come in the play or are there other, what are

24:51.920 --> 24:57.120
the kinds of techniques that come in the play? Yeah, I will tell you about the best model that we had,

24:57.120 --> 25:04.000
which was doing, you know, 59% when we released the dataset, basically. So that was a model that was

25:04.000 --> 25:10.960
basically, as imagine, a sentence embedding model. All it did was that you want to build a model

25:10.960 --> 25:20.400
that embeds the context, which is four sentences, in the shared semantic space with the right ending.

25:20.400 --> 25:25.920
So basically, we trained these two parallel neural nets, one of which will embed the context,

25:25.920 --> 25:30.720
one of which will embed the ending. And at the end of the day, you wanted the, you know, encoding

25:30.720 --> 25:36.960
of the right ending to be closer to the context than the wrong ending. So this was called DSSM,

25:36.960 --> 25:43.520
a deep semantic structure model that did like the best within the, you know, multiple models that

25:43.520 --> 25:49.680
we had initially tried for this story closed test. And then actually, I will go about saying that

25:49.680 --> 25:57.920
the core anti-state of the art, as it is about like, you know, released data points about a

25:57.920 --> 26:05.360
month ago, was from OpenAI where they had, they have this nice piece of work that is, you know,

26:05.360 --> 26:11.200
transformers that are also pre-trained on like a large language model that are doing actually

26:11.200 --> 26:17.280
really good job, 86% on the story closed test. So that's, that goes about telling you that

26:17.280 --> 26:22.160
there's, there are a lot of regularities that you can leverage by just, you know,

26:22.160 --> 26:28.320
reading a lot of corporate out there and building a language model. So that's sort of not exactly

26:28.320 --> 26:36.000
what I would have predicted in terms of what would be the best model that can tackle story

26:36.000 --> 26:41.440
understanding. But the recent results show that actually a very strong language model can do a

26:41.440 --> 26:48.240
good job in predicting the ending. With the language model that they trained, was it trained

26:48.240 --> 26:54.080
specifically for this task or was it trained generally like training on embedding space generally?

26:54.080 --> 27:01.600
Yeah. So the interesting thing about that work actually was that which kind of makes it,

27:01.600 --> 27:06.240
you know, sets it apart from the other systems that have submitted on our story closed test

27:06.240 --> 27:11.680
benchmark was that it was actually a generic language model, was this very big language model that is pre-trained

27:11.680 --> 27:18.080
and then tuned for our particular story closed test. So story closed test comes with a validation

27:18.080 --> 27:24.640
and a test set. So validation is what people usually use for for tuning purposes. But the nice,

27:24.640 --> 27:29.680
as I said, the nice characteristic of OpenAI system was that it was a, you know, pre-trained

27:29.680 --> 27:35.280
generic language model. That is actually, I will add this that I told you that we have now,

27:35.280 --> 27:42.240
we are working on releasing a new story closed test version, the new version of the data set

27:42.240 --> 27:48.400
that kind of bypasses and helps with, you know, improving the kind of biases that people had

27:48.400 --> 27:55.280
found out about the data set. And we tested OpenAI system and it was the only system that was

27:55.280 --> 28:00.480
still having a very high performance on the new data set, which goes about saying that

28:00.480 --> 28:05.520
turns out the other, you know, submit assistance for actually leveraging the biases as opposed to

28:05.520 --> 28:14.480
doing true language understanding. Is there a way to characterize whether the OpenAI system is

28:14.480 --> 28:21.120
just better at leveraging biases, you know, as opposed to kind of true understanding? Yeah, that's

28:21.120 --> 28:26.960
a very good point. And I actually, I do think that there's no way for us that there are not

28:26.960 --> 28:32.720
there some hidden biases in any of the data sets that we are benchmarking our progress on.

28:32.720 --> 28:39.120
And until we get to a point that we can really deploy a system in the wild and see that they can

28:39.120 --> 28:44.880
basically model different kind of complex inputs and then generate different kinds of complex

28:44.880 --> 28:50.000
outposts, there's no way of guaranteeing that, right? I'm sure that our new data set that we're

28:50.000 --> 28:55.280
about to release also has new biases that we are not aware of, right? And there's a very good

28:55.280 --> 29:00.800
chance that the OpenAI system, as an example, is just doing a better job at, you know, again,

29:00.800 --> 29:07.600
like memorizing different sorts of irregularities that are hidden. So I think there's just really

29:07.600 --> 29:13.120
no way of knowing that. But I guess as long as at least we have better practices in place for,

29:14.080 --> 29:20.160
you know, testing and evaluating your systems on a variety of benchmarks, VR in a better shape.

29:20.160 --> 29:28.320
One of the points you raised earlier was the idea of systems that can explain themselves. Now,

29:28.320 --> 29:33.360
that can mean a lot of different things. But one of the things that it can mean is that a system

29:33.360 --> 29:42.160
like this OpenAI system or some other system with this capability can describe, you know, why it is

29:43.360 --> 29:48.880
producing the sentence that it's producing or choosing the sentence that it's choosing.

29:48.880 --> 29:54.400
Is that when you say explain, is that the focus of your work?

29:54.400 --> 30:02.560
Yeah, exactly. So as I said, the outcome of the challenge that we ran on the story close test

30:02.560 --> 30:08.720
was kind of an eye-opening for me personally to think beyond classification tasks. And,

30:08.720 --> 30:13.040
you know, with a few of my colleagues, we had a lot of back and forth. And the consensus was that

30:13.040 --> 30:20.160
if we build AI systems that not only choose the right, whatever their, you know, space they

30:20.160 --> 30:27.120
provided this, choose the right ending in the context of story close test, but also explain why

30:27.120 --> 30:32.160
they did that so that we can basically probe them. We can see whether or not their decision

30:32.160 --> 30:38.720
makes sense. Because as I said, for instance, in the case of the team, the UW team that was

30:38.720 --> 30:44.640
leveraging the features, the, you know, stylistic features, the system can't explain that I chose

30:44.640 --> 30:49.680
this because it had more number of adjectives versus adverbs or it had like, I don't know, like

30:49.680 --> 30:55.040
eight words as opposed to five, right? It should say something that is logically sound to a human

30:55.040 --> 31:01.200
reader. So that's, yeah, back to your point, that's exactly what I mean by explanation. And I do

31:01.200 --> 31:09.920
think that explanation is this cognitive ability that us humans we have and working on the next

31:09.920 --> 31:15.360
generation of AI systems that can explain themselves is not really only for the sake of say

31:15.360 --> 31:21.360
evaluation or for the sake of, I don't know, like they're now these regulations and say EU that

31:21.360 --> 31:27.440
push for AI systems not to be black boxes, but it's really about modeling this human capability

31:27.440 --> 31:33.200
and this cognitive ability that we have as humans because explaining is a way that we showcase

31:33.200 --> 31:38.240
our intelligence often and we have to have AI systems that can just portray that.

31:39.040 --> 31:46.880
And so there are different ways of approaching that. Some take the explanations from an intrinsic

31:46.880 --> 31:55.520
perspective and try to introspect on the actual model that's doing the deciding and put what

31:55.520 --> 32:01.680
it sees there into words and others take more of an external perspective and try to apply some

32:01.680 --> 32:09.680
other model to the primary model to generate explanations. How does your work in the space

32:09.680 --> 32:17.440
approach that distinction? Yeah, so I personally, this is my personal view. I personally,

32:17.440 --> 32:21.600
personally, personally, foremost, I believe that the explanation should be in national language

32:21.600 --> 32:26.880
form, right? So there are different pieces of work off a lot also in the vision and language

32:26.880 --> 32:35.920
community that they basically count your attention features as the way of explaining why a system,

32:35.920 --> 32:41.920
you know, say chooses a particular, you know, outcome versus another, but I count national

32:41.920 --> 32:48.800
language explanations as a reasonable way of interfacing with human. So back to your question

32:48.800 --> 32:56.240
about there being two disjoint modules that one of which will do prediction. Let's say one of

32:56.240 --> 33:01.760
which will do the generation. I would say that I'm not against either of those. It could be a

33:01.760 --> 33:08.640
disjoint model. It could be a joint model, but as long as the system can actually somehow provide

33:08.640 --> 33:16.160
the explanation that makes sense to the human that is judging whether or not makes sense,

33:16.160 --> 33:21.840
it could be still useful. So there are different ways of looking at explanation, right? Is it for

33:21.840 --> 33:28.800
basically trying to diagnose, diagnose like why a system makes a particular decision? Is it

33:28.800 --> 33:34.880
for improving the system? Is it what is the purpose, right? And I would say that at the end of the day

33:34.880 --> 33:40.960
as long as the system can improve, for instance, through the interaction it has with human for

33:40.960 --> 33:48.240
explaining themselves itself, why do we care if there are two different modules that one of

33:48.240 --> 33:54.480
which is generating the explanation and one of which is doing the prediction. But at the end

33:54.480 --> 34:02.560
of the day, as I said, I think we can take explanation as a way of knowing whether or not the system

34:02.560 --> 34:09.520
can tell us what's going on there to hood. And if a system chooses to have two different modules

34:09.520 --> 34:16.880
that's just basically an implementation decision, but I can't imagine a successful system that

34:16.880 --> 34:22.560
doesn't have those two modules communicated with each other. So again, but I'm not opposed to

34:22.560 --> 34:31.040
the idea. So thus far we've talked primarily about natural language understanding aspects of

34:31.040 --> 34:38.320
contextual modeling, but you recently presented at CVPR on some multimodal work incorporating both

34:38.320 --> 34:44.960
language and vision. Can you talk a little bit about that? Sure, absolutely. So as I said, the work

34:44.960 --> 34:50.880
on event-centric contextual modeling has different applications. So we talked about story understanding

34:50.880 --> 34:57.440
another application of it for my work has been on language and vision. So the language and vision

34:57.440 --> 35:03.520
community has seen a lot of interest, has received a lot of interest in the past like a couple of

35:03.520 --> 35:11.120
years, basically after deep learning growth, which is because we can just basically have better

35:11.120 --> 35:19.600
ways of encoding images, which is very nicely something that you can nicely feed into a

35:19.600 --> 35:25.280
recurrent model with which you can generate language. So that's more so what my work has been

35:25.280 --> 35:32.960
around as well. So image captioning has been the most popular application in vision and language

35:32.960 --> 35:38.800
since the beginning, which is about how would you build an AI system that will caption it a given

35:38.800 --> 35:45.040
image very literally. So how would you literally describe what you see in a photo? So my work has

35:45.040 --> 35:51.920
been mainly focused on going beyond literal description and getting more so towards kind of

35:51.920 --> 35:59.120
vision language tasks that require some degree of common sense reasoning. So to give you an example,

35:59.120 --> 36:06.000
the very first work I started on vision language was about this static image. Imagine I'm just

36:06.000 --> 36:13.440
describing it basically now. Imagine this aesthetic photo you see of two policemen with a fallen

36:13.440 --> 36:20.480
motorcycle on the ground. And as human beings, when we see this, right, the tree status of

36:20.480 --> 36:27.840
aesthetic objects, right, two policemen and a fallen motorcycle. From these three static objects,

36:27.840 --> 36:32.960
we go beyond, we connect the dots and we infer that, oh, there should be a notion of injury,

36:32.960 --> 36:38.080
or oh, there should have been a motorcyclist, right, or oh, like there should have been an accident

36:38.080 --> 36:43.760
here. So these kind of connections that we make are really interesting and it wasn't something that

36:43.760 --> 36:49.680
was already explored in the research community. So we basically defined this task called visual

36:49.680 --> 36:58.320
question generation that what it does is it focuses on building an AI system that given a static

36:58.320 --> 37:03.120
image that happens to be event-centric, meaning that something's happening in the image.

37:03.120 --> 37:09.200
What is the most natural first question that pops to your mind, given that image? So that's basically

37:09.200 --> 37:14.800
the VQG work that I did that started off a series of other vision and language projects that I

37:14.800 --> 37:20.000
worked on. And, you know, I will go about asking you. So what would be the most natural question

37:20.000 --> 37:24.960
that you would ask, given the image that I just described to you? At the highest levels,

37:24.960 --> 37:30.240
like what happened? Yeah, what happened exactly? So we understand that something should have

37:30.240 --> 37:36.160
gone wrong or should have occurred that caused that scene. Yeah, what happened is the

37:36.160 --> 37:41.440
motorcycle is still alive. How like serious was the injury, stuff like that are the most common

37:41.440 --> 37:46.560
things that people ask and it was the kind of a, you know, task that we wanted to push for to go

37:46.560 --> 37:54.160
beyond literal description. And so how do you go about tackling that? Yeah, so we basically built

37:54.160 --> 38:00.080
it for first and foremost, you need data, right? So we collected our own data set called VQG,

38:00.080 --> 38:06.800
which was on more, more so on event-centric images, we queried like being search engine to get

38:06.800 --> 38:12.800
images that have something happening in them, save fire, earthquake, injury, stuff like that

38:12.800 --> 38:19.520
are an accident. And then using that data, we built this model that gets as an input, the feature

38:19.520 --> 38:25.440
vector of an image, which, you know, could be a convolutional neural net. And then given that,

38:25.440 --> 38:31.440
just train a language model, basically a conditional language model that will just generate the text

38:31.440 --> 38:38.560
description, which just happens to be a question here. So we actually leveraged existing image

38:38.560 --> 38:45.120
captioning models and just basically tuned them and retrained them on our own data. And it was,

38:45.120 --> 38:50.400
you know, semi-successful in asking relevant questions given an eventful image.

38:50.400 --> 38:59.360
And so with the data set you curated, you had these images of events, things that happened,

38:59.360 --> 39:07.680
and then you had a single caption for each image. Our data actually comes with five questions per

39:07.680 --> 39:13.040
given image, but you know, it could have been anything, right? It just happens that ours was five,

39:14.160 --> 39:22.240
just for the training purposes. We had, we collected 15,000 of such images, each paired with five

39:22.240 --> 39:31.040
questions. And is the task structured in such a way that it's a classification or a generation

39:31.040 --> 39:36.800
of a new sentence based on it? Yeah, that's a very good question. It is generation here. And as it

39:36.800 --> 39:43.120
goes with any generation task, you have the problem of, okay, how would you evaluate, which is a

39:43.120 --> 39:50.240
major, you know, problem in the community. So here, we just, you know, try different metrics that

39:50.240 --> 39:54.560
are classically used in the division of the language community. They're different methods,

39:54.560 --> 40:00.960
you know, you can use blue, you can use media or we just use Delta Blue, which happened to correlate

40:00.960 --> 40:05.840
the best with the human judgment. So can you elaborate on that? What is Delta Blue? What are the

40:05.840 --> 40:11.920
inputs to that? Of course. So these are metrics, right? There's no like, it's not no AI, like

40:11.920 --> 40:19.440
nothing is being trained. It's just a metric for evaluating a generated output versus some gold

40:19.440 --> 40:26.080
standards, which is an inherent problem because for so many taskers, no limited set of possible,

40:26.640 --> 40:31.600
you know, gold answers as it goes for this, right? There's so many different questions you can

40:31.600 --> 40:36.960
ask given an image. But at the end of the day, you, you know, as in research community, at least,

40:36.960 --> 40:44.720
what you have to resort to is to define a set of predefined, you know, limited set of predefined

40:44.720 --> 40:51.360
questions here. Even there, there are so many ways to ask the same question. Let alone

40:51.360 --> 40:58.560
different types of questions that you could ask for about a situation. Exactly, which goes to

40:58.560 --> 41:04.240
saying that to what I just said, that it's just inherently problematic. I would say that we

41:04.240 --> 41:09.440
yet don't know how to evaluate language generation, which is not machine translation. So in machine

41:09.440 --> 41:15.680
translation, even although even a machine translation, blue, which is the metric of how would you

41:15.680 --> 41:21.360
evaluate how good of a job you've done at translation, there even people use blue, which,

41:22.400 --> 41:26.880
which, you know, there are different pieces of work that show that even that doesn't correlate

41:26.880 --> 41:33.520
with human judgment that is strongly, but even in machine translation, the task is much more

41:33.520 --> 41:39.840
defined, right? The semantic content of the source language is really close to the semantic content of

41:39.840 --> 41:45.680
the, you know, the language that you're going to, whereas in dialogue or in story generation,

41:45.680 --> 41:51.600
or in this vision of language test that I just talked about, it's not, nothing is, you know,

41:51.600 --> 41:58.160
set predefined. So we have this major problem, which goes, again, goes back to the story I was

41:58.160 --> 42:03.920
telling you for the story close test that we decided to go with the classification task because

42:03.920 --> 42:10.560
generation inherently is hard to evaluate and classification gives us this ability to systematically

42:10.560 --> 42:18.560
evaluate. So yeah, anyways, we've, you know, the language and vision work that I told you about,

42:18.560 --> 42:25.600
we ended up using Delta Blue, which is this metric for basically counting the number of words that

42:25.600 --> 42:33.920
occur in the generated output versus the gold few human authored, you know, questions that you

42:33.920 --> 42:42.160
have in hand. So you generate a question based on the image and you're evaluating the performance

42:42.160 --> 42:49.760
of, or you're evaluating that question based on Delta Blue, and presumably you're crowdsourcing

42:49.760 --> 42:56.960
the gold standard answers. Exactly. Yes, it just comes from the data set, right? So the 15,000

42:56.960 --> 43:02.160
data, you know, points to be collected, we set aside a portion of that for test, where that

43:02.160 --> 43:12.240
that's a blue, you know, human authored gold questions come from. And so for your implementation

43:12.240 --> 43:18.000
of a system to do this, what was the general approach you took? To general approach, as I said,

43:18.000 --> 43:23.840
was this model that encodes the image using a convolutional neural net and then generates the,

43:23.840 --> 43:32.880
yeah, and then generates a sequence of words using a recurrent neural net. And these are,

43:32.880 --> 43:38.640
you know, there's this recurrent neural net language models are really strong in generating

43:38.640 --> 43:45.680
grammatical outputs, meaning local coherency, but they're not really good at capturing

43:45.680 --> 43:51.680
intricacies and generating basically contentful sentences. As you may have seen, you know,

43:51.680 --> 43:57.040
if you look at the kind of language, the chat box generator, a lot of such original language,

43:58.080 --> 44:03.520
work pieces of work, often the generations are bland, meaning that they're, you know, they're

44:03.520 --> 44:09.760
usually safer on the safer side. You don't have a lot of contentful words or events and stuff

44:09.760 --> 44:18.560
like that, but they do a really good job in generating coherent sentences. And did your work try to

44:18.560 --> 44:23.040
address that? Yeah, so there are different, so we did, you know, we did try different things,

44:23.040 --> 44:30.080
just the, you know, the question is, okay, I have this system that generates different kinds of

44:30.080 --> 44:36.320
output. People usually have this end best list that they rerang. So basically, when you are

44:36.320 --> 44:42.160
generating at the end of the day, you can have a, you know, search, right? And then the question

44:42.160 --> 44:47.680
is how to do that search better, so that you hit the ones that are more contentful. So they're

44:47.680 --> 44:53.200
like different little tricks in the paper we had at the, for the VQG work, we use your question

44:53.200 --> 44:58.720
generation that I just told you about. We had this very simple heuristic that if, if in your

44:58.720 --> 45:05.440
end best list, you have a sentence that has verbs in it, give it a higher rank, or if you have a

45:05.440 --> 45:10.320
longer sentence, is most possibly a better sentence. And then you put all these different,

45:10.320 --> 45:14.800
yeah, features together, and then you tune your model, you use merch, but they're different,

45:14.800 --> 45:18.960
kind of, you know, rankers you can use out there. But, you know, it's not a solve, it's a,

45:18.960 --> 45:24.480
it's a serious problem we have for the, you know, see, basically these kind of recurrent

45:24.480 --> 45:29.920
neural net generations. And when you describe this earlier, you, I think you described it as a

45:29.920 --> 45:35.200
conditional language model. Yeah, I mean, yeah, so you have to have a way of

45:35.200 --> 45:39.760
conditioning on the input right here, the input as the image, the input could have been anything,

45:39.760 --> 45:45.520
right? In the case of story generation, the input could be the previous sentence. Here, we want to

45:45.520 --> 45:51.440
generate using the image, so we condition the generation of the language on the feature vector

45:51.440 --> 45:59.680
of the image. I'm trying to, to visualize what that looks like, or how that is implemented.

45:59.680 --> 46:03.360
You know, when I think about RNNs and time steps and all that kind of stuff, where does the,

46:03.360 --> 46:08.640
the feature vector of the image come into play, or is that the input at these time steps?

46:09.600 --> 46:18.560
Yeah, so the, the very last model we tried for the VQG task, actually, what it got as the input

46:18.560 --> 46:24.000
was the FC7 feature of the, you know, looking the convolutional neural net, one of the, you know,

46:24.000 --> 46:32.080
slices that you can get is like FC7. So this fully connected layer is what we fed into the RNN,

46:32.080 --> 46:37.840
as the, just for the very first step. You can actually feed that in for all the steps. That's

46:37.840 --> 46:43.360
something we tried. We just got worse results. We just conditioned the very first time step on

46:43.360 --> 46:49.280
that. But as I said, that's a, that's a decision that you can make by just, just trial and error.

46:50.240 --> 46:57.360
Awesome. We have covered a ton of ground. Are there any other things that you might want to

46:57.360 --> 47:03.760
mention about your current research areas? Yeah, I think it we covered a lot. I think that

47:04.560 --> 47:12.640
what I would conclude this, this, you know, today's time path is to just mention that I,

47:12.640 --> 47:18.720
I've chosen to work on the topics in AI that I found to be really challenging in terms of

47:19.840 --> 47:26.320
the amount of work that still is needed to be done to even, you know, scratch the surface.

47:26.320 --> 47:31.920
So commonsense reasoning happens to be one of them. There's a consensus in the field these days

47:32.480 --> 47:38.240
that we do yet don't have an AI system that can even have the commonsense understanding of a,

47:38.240 --> 47:44.000
you know, four or five-year-old kid, let alone an actual human like adult. And I think that

47:44.560 --> 47:49.600
the kinds of work on like story understanding, story generation, or division language tasks that

47:49.600 --> 47:56.720
are event-centric, go about, you know, at least going one step beyond the existing

47:57.840 --> 48:02.640
efforts for doing something that is a little bit more challenging. And I think it's important

48:02.640 --> 48:10.000
to be mindful of how far we've come, which is to tackle a lot of, you know, previously challenging,

48:10.000 --> 48:18.240
I would say, you know, kind of perception tasks, but we really have a long way going forward

48:18.240 --> 48:24.240
doing more of your reasoning and cognitive tasks, which is my, my personal research interest,

48:24.240 --> 48:28.480
and I think a lot more into, you know, people in the community should pay attention to it.

48:28.480 --> 48:32.240
Well, Neswin, thank you so much for taking the time to chat with us.

48:32.880 --> 48:35.360
Thank you. Thank you so much for having me.

48:39.840 --> 48:44.880
All right, everyone. That's our show for today. For more information on Nesreen,

48:44.880 --> 48:51.680
or any of the topics covered in this episode, visit twimmelai.com slash talk slash 174.

48:52.560 --> 48:57.360
If you're a fan of the podcast, please pop open your Apple or Google podcast app,

48:57.360 --> 49:02.400
and leave us a five-star rating and review. Your ratings are a great way to help new listeners

49:02.400 --> 49:15.120
find the show. As always, thanks so much for listening and catch you next time.

