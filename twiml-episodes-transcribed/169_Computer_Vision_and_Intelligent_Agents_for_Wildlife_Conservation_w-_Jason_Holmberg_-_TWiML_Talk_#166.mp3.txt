Hey everybody, Sam here. We've got some great news to share, and also a favorite
to ask. We're in the running for this year's People's Choice Podcast Awards, in both the
People's Choice and Technology categories, and we would really appreciate your support.
To nominate us, you'll just head over to twimlai.com slash nominate, where we've linked to and
embedded the nomination form from the award site. There, you'll need to input your information
and create a listener nomination account. Once you get to the ballot, just find and select
this week in machine learning and AI on the nomination list for both the Adam Curry People's
Choice Award and the this week in tech technology category. As you know, we really, really appreciate
each listener and would love to share in this accomplishment with you. Remember that URL
is twimlai.com slash nominate. Feel free to hit pause and take a moment to nominate us now.
Hello and welcome to another episode of Twimla Talk, the podcast why interview interesting people,
doing interesting things in machine learning and artificial intelligence. I'm your host Sam
Charrington. In this episode, I'm joined by Jason Holmberg, Executive Director and Director
of Engineering at Wild Me. Wild Me's Wild Book and Wild Book for Whale Sharks are both open source
computer vision based conservation projects that have been compared to a Facebook for wildlife.
Jason kicks us off with the really interesting story of how Wild Book came to be and its eventual
expansion from a focus on whale sharks to include giant manta rays, humpback whales, zebras and giraffes.
Jason and I explore the evolution of these projects use of computer vision and deep learning,
the unique characteristics of the models they're building and how they're ultimately enabling a
new kind of citizen science. Finally, we take a look at a cool new intelligent agent project that
Jason is working on which mines YouTube for wildlife sightings and automatically engages with
the relevant individuals and scientists on Wild Book's behalf. And now onto the show.
All right, everyone. I am on the line with Jason Holmberg. Jason is the Director of Engineering
for Wild Me. Jason, welcome to this week in machine learning and AI. Hey, thanks for having me on.
Jason, why don't you start out by telling us a little bit about your background and how you got
involved in ML and AI. So I have a very unusual background and journey into running an organization
that is focused on applying machine learning to wildlife conservation. I am a scuba diver.
My background is in chemical engineering and in Arab studies. I did a master's degree at Georgetown.
So neither of these would be suggestive of a journey to machine learning except maybe the
scuba diving because wildlife is an area where we see a lot of application potential for machine
learning. But the journey is a little bit interesting in that as I got this Arab studies degree,
I found myself living at various times in the Middle East, in Egypt, in Tunisia, in Lebanon.
And on one of those opportunities, I, one of those trips, I was able to travel down to Djibouti
there in East Africa, which is an amazing melting pot of cultures just sitting there.
And I was able to go scuba diving and I was excited to see the world's biggest fish, the whale shark.
There was no guarantee I would see one and we did this live aboard sailboat going out to sail
and dive amongst the set frayers islands, which means seven brothers. There are coincidentally
only six islands, so I'm not sure who did the math, but there I was. It embedded sort of in a group
of French tourists on a sailboat and in fact I went scuba diving and as we were coming up,
there was a whale shark. No, whale sharks grow up to 60 feet in length and this was an eight foot
juvenile. So quite small, but actually that's extremely rare. Most of the whale sharks we see in
the wild are about 15 feet and larger. So this was a very rare sighting of this juvenile and I was
just enamored. And the interesting thing about a small whale shark is that I was proportional and
sized to it. So this animal looked back at me, this giant fish and we had a moment of just swimming
together underwater. It was curious about me. Later in life, I was able to swim with some 40-foot
whale sharks in the Galapagos Islands and interacting with a giant, a Leviathan like that is a very
different experience. I'm nothing to a Leviathan. I'm just a passing speck of dust in the water
column. But this young juvenile sort of gave me the time of day if you will and we swam together
for a bit and then it disappeared and that just sparked something. This was, this was in the spring
of 2002. So flash forward, fall of 2002, I decided to go on a whale shark research expedition and
I spent a week down in Baja California on this small boat as this micro light plane circles overhead,
you know, trying to spot the whale shark and call down to us. And the sad story is that we saw
absolutely no whale sharks while we were baking in the Sun for an entire week. But I had the second
spark, which is as I sat there in the boat next to this field biologist who's got a spear and a
plastic tag that's almost like, you know, the somebody's title on their desk, you know,
those sliding plastic tags that they might have saying their name and their title. It's about
that size and often it will have letters and numbers on it indicating the individual that they
tagged. So I'm sitting next to this biologist for a full week, nothing to do. And he's got this
spear in this tag and I ask him, you know, what percentage of the time do you, you know,
recite this tag? You're about to spear on this side of a whale shark and he said, oh, you know,
about 1% of the time. And I said, wait a second, 1% because I have an engineering background and
I guarantee you anytime you have a process that's 1% efficient, I can get you to two, right? Just
just out of rock curiosity and labor. And that was the second kick in the pants. I happened to
have graduated Georgetown with my Arab studies degree was actually working at a company called
Softworks EMC, then EMC, then now it's called Dell EMC. And I had, I was learning to code and I had
all this extra time in my hands. I was young and single and I started thinking about the pattern
of spots on the whale shark and whale shark can have blue or brownish skin, but it has all these
white spots like constellations just rippling across both sides and the top of the animal.
And I started thinking about using it as fingerprints and and I wasn't the first person to think
of that. It had been theorized, but nobody had actually sort of implemented a pattern recognition
system for whale sharks. And here you see the hints of of computer vision emerging, right?
So this is this is fall 2002 and I began creating my own, you know, basic trigonometry type
spot pattern recognition system where I would take a photograph and I would map the spots and I
would take those sets of coordinates and have, you know, 10 comparison sets in my list and then
my N plus one set of coordinates. And I would try to come up with some complex trigonometry.
And and really wasn't getting anywhere, you know, by about the 11th or the 12th pattern,
my accuracy would just start falling apart. And so I remember working late one night, not
not on work, but actually on this. It was a Friday night and I had no plans and a friend of mine,
who we now have a really long term friendship. He is Dr. Zauvin Erzemanian at NASA Greenbelt.
He called me out for a beer and as some discoveries go, they happen over a beer. And so I sort of
dejectedly say, well, yeah, sure, why not? I'm not making any progress here on Friday night.
So I go downtown Washington, DC and I sit down with Zauvin and order a beer and he has another
astronomer with him there coming back from a convention. This is an optical astronomer.
And Zauvin says, hey, why don't you tell my friend about what you're trying to do with with the sharks?
I said, well, you know, I'm trying to map the spots on the side of a whale shark and compare the
patterns across photographs. And casual as can be this optical astronomer whose name I've forgotten
at the moment just turns to me and goes, oh, yeah, we do that. What are you talking about? Hold on a
second. What are you talking about? And he said, well, in optical astronomy, we take multiple pictures
of the night sky and we want to create large composite images. And sure enough, what we need to do
is triangulate on different constellations. And that allows us to get key points and create master
image sets. And I was just blown away. And so, you know, he references me to the right papers.
And then Zauvin really sees that I'm going to put in the effort. And so he sits down as an astronomer
to help me, even though he's a pulsar astronomer, he, you know, moon lighting and whale shark research
is just something you might do at that level, I guess. I'm not sure I saw the connection coming
when you referred to the spot patterns as constellations. Yeah, exactly. And this just sort of all
unfolds in this, you know, amazing way. And so Zauvin and I sit down and translate this 1984
arcane paper by, I believe it's Edward growth, who it turns out also as, you know, as these things
do, sometimes that Zauvin used to play baseball with. But nonetheless, this Edward growth is a
professor who's funded under the Hubble Space Telescope. He develops this algorithm for mapping
and comparing star patterns across photographs. And it works. We modify it a little bit. You know,
the photographs of the night sky can have, you know, need to be rotationally independent,
depending on, depending on where the observer is, but whale sharks have a top and a bottom. So
we make these changes. And voila, we have this highly accurate whale shark spot pattern comparison
system. And we can use the patterns on the left and the right side as a pair of tags, like a left
and a right thumbprint. So now we have the inklings of a, you know, an early 2000s computer vision
matching system and one that's actually fairly well-defined and proven. Here's the other problem,
though. Where do I get all these photographs from? So at this point, I reach out to a biologist
named Brad Norman in Western Australia. He had worked for years swimming with the whale sharks
that congregate off X-Muth and Coral Bay up in the northwest part of Australia. And it's a
really good tourism destination, very reliable place to go and swim with these whale sharks from about,
let's say, late March to late July every year. And he'd been collecting these photographs on the
off chance that somebody came up with this algorithm. So I blindly reach out to Brad Norman via
email and he responds very skeptically and I say, hey, you know, I think I might have this and you
might have the other piece, which is the data. And so we begin this long-distance collaboration
showing that this computer vision system actually worked. And we could reliably across years,
across data sets, across photographs, compare individual whale sharks and determine without
physically tagging the animal. That in fact, years later, this was the same whale shark we might
have seen years before. So then we hit what we sort of thought that was the big problem, like
creating the pattern recognition system. But what we actually found is there was simply no
database content management system for wildlife at all, not even a data model that really could
be used out of the box. And so it turns out that 90% of the work was creating the precursors to what
we now call wild book, which is an open source platform to marry good data management,
computer vision, and citizen science altogether to create a platform for wildlife biologists to
collect more data, process it faster, especially in the context of what's called marker capture.
So if you've ever seen the tag on a deer's ear hanging off, or if you've ever seen the band
on a bird's foot with a number on it, those are the tags. And for whale sharks, it was the spear
and the plastic tag. But those are used to repeatedly identify individuals in a population
in what's called mark and recapture studies. And out the other end, you can either get an abundance
estimate how many of these critically endangered animals do we have, or at least a population
trajectory? Are there numbers relatively getting bigger or smaller from when we started sampling?
And so it turns out that photographic data is now so ubiquitous that harnessing it means that
wildlife biologists who tend to still use Microsoft access and excel on a daily basis, you know,
never mind cloud computing, which is not even a new concept anymore, that they could suddenly
have a platform to utilize this much richer source of data. And it turns out it's much better for
the animals. We're not physically harming them or getting in the way of their daily life.
And so our success applying this to whale sharks, building a cloud platform that allowed
citizen scientists, divers, and snarklers to submit their photographs, the computer vision,
which was the carrot that researchers needed to drop their competitive boundaries and begin
collaborating for what is a massively migratory fish. All of this just sort of melded nicely
into what is now whaleshark.org, the wild book for whale sharks. And whaleshark.org has over 150
researchers and volunteers all over the globe who log in. And about close to 6,000 people from
the public have contributed photographs. That model has proved so successful and so cost-effective
that we've started replicating it for giant manta rays for humpback whales and for zebras and
giraffes now. And as we started achieving success, that's when a new cadre of researchers showed up.
Those researchers are now my copy eyes on the wild book project, their
Professor Tony Burger Wolf, University of Illinois Chicago. She's the overall project leader.
She does data science. We work with Professor Chuck Stewart, Rensselaer Polytechnic Institute.
He has this amazing lab almost exclusively dedicated to wildlife computer vision.
And I actually was able to hire one of his students earlier this year. That's another story related
to an anonymous Bitcoin millionaire. If you've ever heard of the Pineapple Fund, we can talk about
that in a bit. And I had no. It's an interesting story. And then Dr. Dan Rubenstein, professor of
ecology at Princeton University. And somehow, and I feel as I've told you this story that how much
luck has played, I lucked into a group of four collaborative PIs. And at one point,
we all just looked at each other via Skype or whatever mechanism it was at the time. And we
said, you know, this is the project of a lifetime. We all sort of nodded. And there's this
general sense that funded or not funded somehow, we're just going to get this done. And amazingly,
we are now a group of specialists working together before it was me sort of trying to hack
through a computer vision system and apply the data management. Now we have this wonderful pathway
of original research for computer vision done at Rensselaer Polytechnic advised by Tanya at
UIC. And then we transitioned that over to software engineering team at Wild Me. So Wild Me is
executive director. I manage four staff, one machine learning expert, Jason Parham. And then
three software developers, Zhu Blunt, John Vandost, and Colin Kingan, all of whom are nine to five
professional software developers working on wildlife conservation. And just the fact that for
however long we can afford them, we can afford this dedicated of a team is it is a giant coup in
conservation. I'm I'm just excited at where we are. And if you've ever heard of Microsoft's AI
for Earth program, they're investing about 50 million over the next five years into applying AI
for Earth conservation. We just signed to deal with them and they are our biggest supporters at
the moment. And we're excited about working with them and Azure cognitive services and a growing
relationship with them as well. What a great story. What a great story. You mentioned in there
that computer vision started to affect this shift in the I guess the academic culture almost
within this community to make researchers more collaborative. Can you elaborate on that a little
bit? Sure. So for some animals that don't move very far, it's it's very cost effective for
a researcher to go into the field regularly to observe them, especially if they're located
near the animal and to collect as much data as possible about a population. These in these
researchers can almost do a complete census. Many animals are not like that. They are migratory,
especially in the marine environment. They can dive deep and have many unobservable states.
And this is where mark or capture modeling showing up at a field site observing who's there and
who isn't repeatedly comes into play. But in the marine environment or in remote environments,
it's just not cost effective to have a researcher co-located there all the time. Sometimes the
animals have migrated away. Sometimes they've simply moved to an area that's remote and difficult
to get to. But all of that is extremely expensive both to support the researcher and to just keep
them in the field. With with marker capture, the idea is you show up and sub sample the population
at certain times. And really a lot of the marker capture models that are used to estimate these
critically endangered and endangered populations are sort of based around human limitations.
There's this assumption inside the models as you as you model capture probability as you mount
model survival rates to ultimately arrive at abundance or population trajectory that the amount
of time that is passed between your sampling sessions far outweighs the size of your sampling
sessions. So they're not meant for continuous monitoring. And also research collaborations
themselves and the competitiveness in academia really create these silos inside what could be
thought of as a master data set. You know, a zebra does not know where the geographic border is
where one research team begins and another ends saying for a whale shark or a humpback whale is
they migrate. We're sub sampling data based on really human limitations of funding and time
and competitiveness rather than sort of getting the superset of data, which is what are these animals
doing all the time? Where are they? When are they there? Who are they interacting with? You know,
what behaviors are they undertaking in these locations? So the idea is how do we how do we push out
this human limited data collection into something much more scalable? And that's where citizen
science comes in. You know, can we get a minimum high quality volume of data from citizen scientists
or a minimum quality of good data from citizen scientists from the public from people on whale
watching boats from people doing safaris in Kenya? Can their photos be used as data? And that
amplifies the data collection of a researcher. Now we start at that point also getting into things
like bias, you know, inconsistent effort in the data collection and the sampling. But importantly,
the largest volume of data that a researcher can get is visual at this point. Everyone's out now
in this modern world where we all have cameras, multiple cameras attached to us, especially when we
go traveling. That volume of data is sort of yet unharnessed and especially for animals that are
individually identifiable. Getting those photos is really getting a data point for researchers.
The problem is is if that if you're still in access and excel on a desktop and have your photos
sort of sitting in folder drives, the flood of data that you can get is far outpaces the ability
of a research team to curate it. Let me give you an example. We work with Sarah Soda Dolphin
Research Project and they identify individual dolphins, bottled nose dolphins by taking pictures
of their dorsal fin as it pops up out of the surface. So when they have a huge collection of dolphins,
thousands of individuals, when they get that N plus one photo, that new photo and they decide,
okay, what, which dolphin is this? It takes them approximately nine hours to match that one photo.
Similarly, if you go to Cascadia Research Collective in Olympia, Washington and they maintain this
master set of humpback whale imagery, if you stand in the middle of their offices and you look
around you in 360 degree view, you see interns looking at images side by side. And so there's a lot
of inefficiency in manually reviewing all this visual data that the world can provide and that
as a sort of long-winded answer to your question is why computer vision is just so critical.
It is a literal time and cost savings to these researchers whose funding just doesn't scale very well.
I guess I'm thinking of the way that computer vision is kind of evolved in this application since
your initial models, clearly deep learning and convolutional neural nets. Well, I'm imagining
or playing a huge role here as they are in other places. I'm wondering particularly if there are
any unique characteristics of this type of data that change what you can or what you can't do or
the way the types of models that you use to identify these specimens. Sure. And you're absolutely
right. The convolutional neural networks are changing this and that's why our partnership with
Rensselord Polytechnic is so important. The crude algorithm we still use for whale sharks has
been replaced for other species. For humpback whales, we use deep convolutional neural networks
to identify humpback flukes that the backside, the underside of their tail, if you will, in two
different ways. These tools are also proving much more scalable, but let me take a step back here.
A lot of times the challenges that I read about in the literature in computer vision are about
categorization. Maybe identify the species. Maybe there's tens of thousands. Or more
ideally, you're looking at categorizing things. It might be a binary, you know, is this a failed part
or a past part? It might be, you know, put these images into 10 different buckets. The interesting
thing about computer vision for wildlife, especially for identifying individuals, is that
by nature, we're searching for an individual, we're searching for a needle in a haystack.
Let me give you an example of what the problem. So my first attempt at humpback whales and doing
a computer vision system, I sat down and started playing with a fairly basic computer vision algorithm.
And my first competent pass, I was 99.8% successful in categorizing my photos. And I thought,
wait a second, I must be a genius. I must be good at this. This is my calling in life.
And then I looked at what the algorithm had produced. And I had created a machine learning agent
that predicted false 100% of the time. Because when you have, yeah, it cheated. And you know,
what? It was accurate 99.8% of the time. Because when you have one photograph and you're looking
through 50,000 photographs for an individual you may have only seen once before, your probability
that it's false is almost always 100%. It's so close to 100%. So, you know, applying computer
vision to identify individual animals in a field of many thousands of humpback whales, over 9,000,
you know, among 50, 60,000 photographs, it's a needle in a haystack problem. And that's why
deep learning is so incredibly important here. I'll give you an example of some of the research
that's coming out of RPI that we're adapting. My team is adapting a wild me. There's a PhD candidate
Hendrick Weedeman at Rensselaer Polytechnic. He's creating an algorithm called curve rank.
And sort of tying in with that Sarasota dolphin research project. The idea of curve rank is that
that the edge of a dolphin fin is individually identified. Now, previous attempts for edge
identification have absolutely occurred. They've used, you know, for example, dynamic time warping as
a basic, you know, point pattern matching or line matching algorithm. The problem is is that
not every bit of that fin is actually indicative of individually identifiable information. In fact,
most of it isn't. Most of it's fairly flat and generic. So, what's important about Hendrick's work
is given this catalog of multiple photos per marked individual in the dolphin population
that the Sarasota dolphin research project provided. He learns which section of the fin
contains that individually identifiable information. And that then allows us to only compare those
small sections of the fin to give us the ability to, you know, go through that haystack and find
that needle. I guess that there have been research efforts around what's called fine grain
classification problems. Are you familiar with that work and does that apply? Here is it a
different formulation of the problem when you're trying to identify the individual. Do you even
think of it as a classification problem or something different? And this may be that I'm somewhat
of an outsider coming into the machine learning community. I generally don't think of it as a
classification problem. I do have intelligent agent classification problems that I'd like to go
into, but in terms of the individual identification, I generally don't think of it as a classification
problem. Okay. And I don't think we, you know, for each individual, I don't think we train up an
individual classifier, if you will, although I'm aware of others. Let's see. There's Ben Hughes
out of the UK. He's been working on white shark fin identification, and he absolutely builds a
classifier per marked individual. So I believe that is more of a classification, traditional classification
problem. Okay. So you alluded to some new work that you're doing to build intelligent agents around
Wild Book. Can you talk a little bit about that? Sure. So I'm building what is I believe a combination
of a model based reflex agent and a learning agent to data mine YouTube for wildlife sightings that
get missed. And so there's a short story that I'll get into here that describes why I'm going
about that. And this is my individual work at Wild Me when being executive director gives me a
little bit of breathing room. This is what I like to work on. So flashback to 2014 and 2015,
I'm running whale shark dot org, published a couple of papers on marker capture populations
off Mingulu reef in Western Australia. I've just had children and I'm most of the time being
a good parent sitting at home in front and Saturday nights, feeling like my contribution to wildlife
conservation is sort of falling away to just programming and maintenance. So I really felt like,
I can't be in the field right now. My family needs me. I can't be scuba diving, but
isn't there some data that I could collect? Something that other people generally don't have access
to other researchers in the community. So I started while watching TV just surfing YouTube for
whale shark videos and started downloading them, capturing key frames, mapping the spots on the
whale shark, submitting it in a wild book, running identification. And it turns out that in 2014
in 2015, just doing that, I collected more data than any individual researcher in the field and
identified more marked individuals. And that really got me thinking, okay, that's great. Except
I don't want to keep doing this. This is actually kind of boring, but I'd like to automate this.
And so piece by piece, as this wild book collaboration evolved, and as Rensseler Polytechnic
trained up a convolutional neural network to detect whale sharks in imagery, then I was able to
bring that in and say, okay, if I download all the videos, can you tell me, are there clusters of
key frames with a high enough confidence that we can say there's a whale shark in it? And then as I
started canvassing the literature for other things, like, you know, how would I determine based on
freeform text in a YouTube video when this video occurred and where it occurred? I started seeing
the different machine learning components that were needed, both computer vision and in the natural
language processing sphere, that would be needed to replicate my actions. Essentially, I was trying
to put myself out of a job with machine learning. Now, it wasn't a paying job. It was just a volunteer
gig, but the idea is, you know, is potentially this the new way of working, of building yourself
a set of intelligent agents that do work for you as part of your job. And in this case,
if we considered my job trying to find whale shark data, then we've absolutely built that.
And the way the system works is every night at 10 p.m. on our server, a little agent wakes up,
and it asks YouTube, you know, tell me everything that's been, or give me a list of videos that have
been uploaded in the past 24 hours that are tagged or titled or described with the word whale shark,
or in Spanish, Tiber on Bayana. And we want to expand this to other languages. We then get a list
from YouTube back of, you know, in JSON, of the titles, the tags, and the descriptions. And
based on how I historically curated that data from 2014 and 2015, I'm serving as the critic
in the learning-based model, it then will make a prediction on true or false. Does this,
the based on the way the video is described, does it contain a wildlife sighting? And that's
important because it's really acting as a very powerful filter. And I retrain it periodically,
so as I make decisions, ultimate decisions on what it collects, it then learns on what I said,
yeah, that was right, or know that was wrong, and read is able to make better predictions.
So it first makes this true false, is this a wildlife sighting whale shark. And that's
important because the word whale shark will show up in grand theft auto videos and all kinds of
random places you wouldn't expect, right? And also, you know, just things like the Georgia Aquarium
as a whale shark. So it sounds like though that you had the foresight to collect all of that
descriptive information and your ultimate decision when you first started classifying these
these videos. Absolutely. And it's made me really think about, you know, what are the other things
in my job that I could start logging now, what decisions I made that all eventually I could train
machine learning on. And because making a prediction, you know, it's a very human thing that
really cross applies to machine learning quite well. So this agent makes that initial prediction.
And it says, all right, based on how this is tag titled and described, I think this is a whale
shark sighting in the wild. And then it goes on to the next step, which is it will then download
the video, sample the video every two seconds, and then take those key frames and look for high
confidence clusters using a trained convolutional neural network. And we fed data from whale shark.org
in. We did a mechanical Turk process to find the whale shark in the imagery, trained up a detector,
and now that detector gives us confidence scores. And if across the video, at least one of the
frames rises above a confidence threshold, we then go on. If not, then we leave. And the interaction
of the predictor and the computer vision piece is important because, for example, in the vision
piece, there's a whale shark somewhere in the world painted on the side of a Chinese airplane.
And that video periodically shows up. So we need the vision to find whale sharks, but then we need
the oftentimes that predictor to throw it out and say, okay, no, no, no, we're not describing,
you know, a whale shark off the Philippines. We're talking about an airplane, so ignore that. So
those two interplay quite well and serve as a very strong filter. At this point, once we've
gotten past the predictor and computer vision, we're on to thinking, okay, this is probably a whale
shark in a YouTube video. And it's at this point that I will take the title tags in description and
send them up to Azure Cognitive Services and say, all right, do language detection, pretty common
natural language processing task. If any of them are not in English, I will then re-ask Cognitive
Services to use neural machine translation, translate back to English. And then we'll take that
master string that's, I'll concatenate all of it. And then I will run a Stanford package called
SU Time, which uses named entity recognition to tell me when this occurred. And importantly,
I feed in the date that the video was posted, and that contextualizes the description. So if
somebody says, you know, I saw this whale shark last week, or if they gave a formal date,
2018, dash 06, dash 24, what have you? It's able to resolve that and give me back an ISO 8601 date.
Now I know when. So I've got visual confirmation of a whale shark or high confidence visual
confirmation of a whale shark. I know when based on using natural language processing. And then I do
some just simple string mining data mining to figure out the where. Eventually, I want to train
up machine learning to also detect where. But right now, we just look for keywords.
On that, when part to what degree is that a probabilistic determination? If you've got multiple
possible dates, or if it's, you know, about a week and a half ago, does it just, does it pick
something? Or do you get some weighted set of dates? I'm wondering, I guess, where in the
in the process does the final determination of a candidate date happen?
That's a really good point. And definitely an area for improvement. So what we get back out of
SU time is a ray of, is an array of all of the dates. And then that are present. And it's not
probabilistic. It's anything that it, it determines it finds. Now, there may be some probability threshold
built in internally that I'm not aware of. But it will, it will simply, simply give me a list
back of everything it finds. I will then, because it's giving me a standardized format, I will then
do my best to take the most detailed date. So it might find last year and say, okay, 2017,
it might find, you know, last month. And then it might actually find June 25th, 2008.
I will go for the date that contains the most amount of information, including starting with year
then month, then day. But learning which date should be used is actually something that we could
train machine learning on. Absolutely. And give a probabilistic ranking on that. So it's a great
really great point. We don't do that. But importantly, if we don't have the where or the when
determined, we actually will have the agent post back to YouTube, again, trying to post back
in the, the poster's original language. So we might engage in neural machine translation again
if we detected Spanish, ask them the question, when did you see this well chart or where or where
and when? Meaning in the comments? Yeah. That's pretty cool. It is. And then we just added recently
the listener that will then asynchronously just pop in occasionally and look for the response.
So it's going to look through all the YouTube videos that we have curated, look for those missing
date and location, go out and then look for response, feeding the replies to our comments back
through the machine learning to do location and date detection. And then ultimately in this vein
of that we like to call rewarding the gift of data for the gift of knowledge, we win a researcher
finally approves and says, okay, this is whale shark A001 stumpy often in glue reef. When we make
that ID of the YouTube derived data, we post back the link to the wild book that says, did you know
that we found whale shark A001 in your video? And here's a link to everything we know about it.
And we have to our questions, we have about a 45% response rate, which I think in the world of
marketing is awesome. But when we ask questions, people do reply, the agent listens for the reply
and then tries to inform them of the conclusions we've made based on their data. And the great
thing about this is if we think about citizen science, public participation, there's an upper
limit to the number of people who are participate that's related to the amount of outreach. How much
have we gone out to the public and told them that they can even, you know, contribute data about
zeroes or giraffes or whale sharks. And then there's a subset of those people who are informed that
we'll choose to participate. And so we're always up against this. Well, not enough people know
about this. How do we get the word out, et cetera? The interesting thing about running an intelligent
agent this way is it flips that model. We go to where people are already choosing to participate.
We show up randomly, curate their data under fair use and then inform them what we found.
And all of that is automated. And so, you know, retraining that agent and proving its quality is
always an ongoing mission. But now we're looking to cross apply that to other species. You know,
how do we start doing this from homeback whales and giant mantas, et cetera?
Have you also thought at all about other domains more broadly that you might be able to apply
this approach, meaning beyond wildlife conservation? I've thought about data mining for marketing.
I would be surprised if there wasn't a company already doing this whereby you could monitor
social media using a complex interaction of machine learning and intelligent agent. And, you know,
figure out who is talking about your product, who's having a bad time with it, who's having
a good time with it, who are your influencers, et cetera. But, you know, the combination of computer
vision and natural language processing to really find the product in the imagery is my product
appearing. And then, you know, look for sentiment analysis. What are people saying? Is it generally
good or bad? Look at the comments. Is are those good or bad? And then get a sense of, you know,
how a product is doing in the marketplace. That struck me as a sort of for-profit cross application.
This is really interesting. It just, when I think about the traditional AI, a big part of
traditional AI, maybe in the 80s or something, was research around this whole idea of intelligent
agents. And we'd all have these avatars that, you know, while we sleeper out doing our bidding,
this is one of the best examples I've heard thus far of something along those lines. I mean,
they're probably our others and maybe folks will, you know, write in and tell me about
all the other great examples of this. But it seems like a really interesting
approach to, as you say, curate content, but also engage with folks and, you know, peak
their curiosity, allow them to contribute to this broad effort without, you know, them needed,
needing actively seek it out. It's very cool. Yeah, and it's a little bit mind bending too.
You know, I think about whale shark.org and how we have something like 152 researchers.
And then this one inhuman agent participating in this, you know, global study of the world's
biggest fish. You know, what does that mean in terms of having autonomous agents collect data
and participate? One of the things I want to do is start having the agent not just participate or
interact with the public, but interact with the researchers who are logging into wild book and
curating data. The agent absolutely should show up and tell them it found new data from their study
sites. And then, you know, what can it analyze? What can it inform them of as part of their experience?
You know, we're pretty far from Jarvis and Iron Man, but, you know, we are at least a baby step
in that direction. Right. Right. Very cool. One of the things that you mentioned in the process here
is active learning. So kind of correcting this agent periodically and allowing it to update its
its model, its view of the world. How have you built that into the process? Is it? Do you kind of
just retrain periodically? Or do you have something more sophisticated happening than there?
I retrain periodically. It's basically a dance I do with the agent. So the agent collects data.
I will do the finishing curation on it. Basically deciding, did you get it right or not?
And then that will be a part of the next training set. And in the early stages, you know,
we're talking about thousands of data points. So I'm using a simple random forest algorithm
using trigrams, not especially sophisticated. We're using Weka, the open source package in Java.
But, you know, there's a lot of room for improvement there. And then especially as the agent,
you know, as we get more data and get a little more mature in using this agent, I might
allow it to periodically just retrain itself based on my curation. But then ultimately,
I want to expand it to the other, you know, human researchers participating in the system and
their decisions around the data that it collected. Did you choose Weka based on a preference
for Java or was there some other driver? My background is through Java. I'm rapidly converting
to Python. But, you know, as you've heard from my story, my route into machine learning is so
non-traditional that Weka was a very accessible machine learning tool. It's, you know, the algorithms
inside of it. And these are older techniques, obviously. And for newer techniques, I really value
my staff member, Jason Parham, who is just finishing up his PhD, as well as, you know, our
collaboration with Chuck Stewart, Hendrick Weedeman, over at Rensseler Polytechnic. They're pushing
the boundaries of pattern recognition for wildlife. And then, you know, it's my professional
engineering team's job to implement those paper-worthy algorithms as user-worthy software products,
essentially. Wildbook is open source, but we very much think of it as a product because
ultimately, these different wildlife biologists who are in the field coming back with photographs
of their study populations need to use computer vision successfully. You know, one challenge we
have right now, for example, is humpback whales. We actually have two computer vision algorithms
that operate in the 75 to 80 percent success range, identifying the correct whale in the top one.
But they have different failure modes. So, teaching a field biologist who has no computer vision
experience, how to interpret two different computer vision ranking results, and potentially even
boosting those or using an SVM to create a metascore that is interpretable to them is one of the
usability challenges we still have ahead of us. Right now, we literally show two lists. You know,
computer vision found this and the other computer vision algorithm curve rank found this,
and it's up to them to sort of interpret that. How do we give them more confidence and reduce the
amount of interpretation they have to do? And then, if you push it forward, I'm really excited about
the marriage of statistics with computer vision. You know, a PhD now professor, former PhD candidate
now professor at Eastern Kentucky University named Amanda Ellis, took some of our whale shark work
and said, all right, a human aided by computer vision ultimately decided the IDs of these
different whale sharks and these images. What if we took the cloud of photographs? We use the
computer vision algorithm to create pairwise scores between every image in the cloud. And what if
the whole concept of animal identity was not left to a human being, but it was left to the computer
to find its pathway through the different pairwise relationships and create animal identity
within this cloud of photographs and ultimately provide a population estimate with no human
curation. And interestingly enough, some of the work that Chuck Stewart's lab has been doing at
RPI is related to building the similar data constructs. So I'm trying to put these two groups
together because if we have a cloud of photographs and all the pairwise computer vision relationships
that have been mined among them and were able to pass that over to a statistician, what it means
is we could go from years between population estimates for critically endangered species to weeks,
which means that we can evaluate conservation strategies for these animals faster, right?
Most population papers that come out for a particular population at a study site are generally,
you know, five, six, ten years between population estimates. That's just too slow for these animals
that are critically endangered and declining rapidly. With machine learning, we're able to rapidly
reduce that. It's going to be a lot more research, a lot more development effort, but you know,
it's just crazy to me how many applications for computer vision and machine learning there are
in wildlife research and how much of that skill set is out there untapped for conservation.
I'm curious. You've mentioned on several occasions citizen science as regards the engagement of,
you know, ordinary folks taking pictures, posting them, or maybe labeling things through
wild book. I'm wondering if you have, if you see a role for the analog among data scientists,
you know, as data science and machine learning technologies become more accessible as the data
science community grows and wants to engage in the areas around which they've, they have personal
passions. Do you engage with those communities? Do you see roles for them to support projects
like this? Oh, absolutely. I'm really fascinated about new data science techniques for managing and
increasing volume of data for wildlife conservation and Tanya Burger Wolf, my collaborator at UIC
is investigating this as well. You know, what are the biases in crowdsourced visual data?
Can we create population estimates on that that are as accurate or better than traditionally
collected research data? You know, what are, how big are some of these populations? How can somebody
with data science skills help a researcher parse the statistics? How can they, can they help them
curate their data in such a way that it's easy to get them into the population modeling packages?
Can they come out with the next population model? I really feel like the next generation of
conservation tools needs to be driven by data scientists and machine learning experts. It's the
only way that wildlife conservation, which is still, you know, so unfortunately done on the desktop
can leap 20 years ahead and catch up to some of the other fields that are using machine learning.
It really needs to make that evolutionary leap and it's going to take data scientists,
machine learning experts to help people leap 20 years ahead.
And so if someone who's listening is interested in this area and like to get involved,
how would you recommend they get started? So, you know, the first place to look is locally.
What are some of the wildlife conservation efforts going on locally? And is there a local
researcher you can reach out to? I have had, and no others who have had the same experience,
which is wildlife conservationists, biologists actually tend to really appreciate
IT and data experience. I've gotten nothing but warm welcomes when I've reached out to them
and said, hey, can I help with your data? Can we try to solve a problem for you?
And so, you know, look locally for that. Otherwise, you know, look at Wild Book,
we are an open source package and would love for contribution. And we can absolutely put
you in touch with wildlife biologists who we might not currently have time to do work with,
but who could use Wild Book and might need a new machine learning algorithm for their data.
So, we'd absolutely love to talk to you. Awesome. Awesome. Well, we'll link to Wild Book and to
Wild Me on the show notes page before we wrap up. Is there anything else that you'd like to share?
No, thanks for the opportunity to talk about the Wild Book project, our mission at Wild Me,
and I really encourage your listeners to apply their skills to help with wildlife conservation.
We really need machine learning and data science skills to help out. Fantastic. Thanks so much,
Jason. Thanks, Jim. All right, everyone. That's our show for today. For more information on Jason
or any of the topics covered in this episode, head over to twimlai.com slash talk slash 166.
Don't forget to visit twimlai.com slash nominate to cast your vote for us in the People's Choice
Podcast Awards. And as always, thanks so much for listening and catch you next time.
