Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other
smart devices.
You name it?
It was there.
Of course, I was also able to sit down with some really interesting folks working on some
pretty cool AI-enabled products.
Head on over to our YouTube channel to check out some behind-the-scenes footage from my
interviews and other quick takes from the show.
And beyond the look up for our AI and consumer electronics series right here on the podcast,
coming soon.
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning
Summit in Montreal back in October.
This was a great event and in fact their next event, the Deep Learning Summit San Francisco,
is right around the corner on January 25th and 26th and will feature more leading researchers
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow
of Google Brain and Daphne Kohler of Calico Labs and more.
Definitely check out the event and use the code TwimmelAI for 20% off of registration.
In today's show, I sit down with Eric Humphrey, research scientist in the Music Understanding
Group at Spotify.
Eric was at the Deep Learning Summit to give a talk on advances in deep architectures
and methods for separating vocals and recorded music.
We discuss this talk, including how Spotify's large music catalog enables such an experiment
to even take place, the methods they use to train algorithms to isolate and remove vocals
from music, and how neural network architectures like UNET and Pix2Pix come in the play.
We also hit on the idea of creative AI in general, Spotify's attempts at understanding
music content at scale, optical music recognition, and much more.
And now on to the show.
All right, Eric, so why don't you tell us a little bit about your background and how
you got interested in machine learning and AI?
Sure.
I guess it all started in high school.
I always played music.
My dad was an engineer.
And the compromise of that when I was going into college was electroengineering.
I really wanted to make guitar pedals.
I wanted to learn about how amplifiers worked, and then that kind of got into more signal
processing and algorithms, which led to a lot of parameter tuning by hand.
If you want a delay pedal, you know, you have all these different knobs and whatnot.
And I started doing some more stuff around beat tracking and tempo tracking for running
to music.
And it was like, I spent so much time designing these algorithms by hand.
It's like a lot of the tradition for a lot of digital signal processing things.
And that was right around the time that machine learning really started to pick up.
And it's like collecting data for these things and then training an algorithm.
It just simplified the process so much.
And like, well, this is a no-brainer.
I can have the system that I want to do all these cool things, but leveraging these kind
of like elegant data-driven solutions that freed me up to go running and play music and
do all these other things and just, you know, have these systems work.
And this is all why you're in school?
Yeah, so I did a master's down at University Miami in the really great music tech program
down there and got to do a little bit more with running music and worked with the music
therapy department and then parlayed that into a PhD at New York University, where I got
to work with Juan Pablo Beo and Jan Lecun a little bit, which really kind of pivoted it
even further.
Okay.
So that was really when it was, I took a pretty formational machine learning class with
Jan, my first semester at NYU, and kind of put me off on that.
That's the way to get started.
Yeah.
Yeah.
It was pretty, pretty certain dipitus.
I didn't fully appreciate what I was getting myself into at the time.
It was right around 2009-2010.
But for me, it was always much more about computer visions, doing a lot of things with images
and audio.
It had some stuff around speech recognition, but music was always kind of, you know, lagging
behind.
So for me, I always wanted to say, you know, to be similar to computer vision or ASR, recognize
chords and music.
I'm like a tourist.
Can you show me how to play this song automatically?
Can you show me where the beats, the bars, the chords are?
Can I have a playlist of just choruses, these kinds of things?
So when you really start to think about the opportunities around music, leveraging things
like deep learning and machine learning, you can, you know, the imagination can run wild,
the kinds of things you can do with that.
Awesome.
Yeah.
Tell us a little bit about what you're up to nowadays at Spotify.
Did you go to Spotify right after your grad school?
No, I spent some time at a small music ed tech startup that was trying to do things around
optical music recognition.
So in the same way that you could do document scanning, one of the core pieces of technology
that we were working on was if you took a picture of sheet music, could you turn it into
MIDI so that kids could learn how to play any piece of music at their disposal and then
be able to, you know, on the longer arc gives them feedback about how they're doing and
kind of taking it from there.
It seems like that should be fairly straightforward.
So you'd think that one of the really, really interesting things about music in all of its
forms, and I'll mention this tomorrow is that it's so fundamentally intelligent that,
you know, when you have even just a piece of music for a single voice, monophonic instrument,
you don't have polyphony or these other things.
You actually have, instead of OCR for being generally one-dimensional, it's linear and
you'll span a vertical axis.
But music, you actually have a two-dimensional grid that moves linearly with these really
complicated links backward and forward.
So if you have DSL signals or quotas or repetitions or multiple endings, it turns into death
by a million paper cuts because one of the things you'll find is triplets will be
notated in the first measure and then known after because it's cheaper that way to not
print these additional threes on all these things.
But there, any musician would kind of know that it's there.
Any intelligent musician would know that I was there, got it.
Or you'll run into these other really interesting common cases, there aren't even educations
where for children's music, they won't notate rests because they're trying to simplify
the musical surface.
Okay.
So it's in four, but you'll have these two notes here and then those two notes and the
upper staff and a machine is like, I guess they're the same.
So having all of these and then it doesn't have a ton of data for supervised training and
it creates this really, really interesting, challenging, but fundamentally intelligent
problem.
Interesting.
Yeah.
It's one of those things that, you know, to crack from a very like human interest level,
there's a ton of music that just hasn't been brought into the 21st century yet, you know.
Something that was notated from Gregorian Chants all the way up to kind of now where we've
started to shift away from notated music to more recorded music or digital audio workstations,
like Ableton style project files where there isn't really an artifact of the music except
the recording.
There's all this older stuff that could be brought into the future for creative purposes,
musicology and kind of more anthropological consideration.
Okay.
Interesting.
So I did an interview with the dog at Google Brain Project Magenta.
He had an interesting presentation at another conference a few months ago that talked about
even the step beyond what you're just describing, like once you have this sheet music or you
can describe the music to the computer accurately, how do you then get it to play expressively?
And they've been doing some interesting things there.
I don't know if you're familiar with it, but it's interesting stuff.
Yeah, I mean, expressivity and music creation and composition, it's so interesting to really
dig into because I think it cuts to the core of humanity.
There's been so much amazing work around game playing and AI recently, AlphaGo, Atari,
but you have these well-defined objective functions, right?
Make the score high, win the game, that so you have these extrinsic motivators that fit
pretty well into a reinforcement learning formulation.
And music, the most interesting things are internal, they're intrinsic.
It's the novelty and the surprise of, I didn't see that chord coming.
You know, when you're sitting in your room when Hendrix was really just digging in and just
sinking his teeth in deep to a solo is for him, this sort of is hard.
And you could think about having these feedback loops for listens on Spotify or revenue
generated, but these aren't really what cuts to the heart of creativity and expression.
Like, what are you, what are you getting after?
And I think that's going to be one of the really interesting challenges as we start to move
into that next stage of kind of like really autonomous or, you know, things that are self-directed
in the AI space.
Why is it doing it?
What motivates it?
Does it have this notion of self?
So when you think about elements of, I think music, humor, sarcasm, these kinds of things
kind of start to encroach on that in a way that a lot of the recent history of machine
learning hasn't gotten to yet.
Interesting.
Interesting that you put sarcasm in that bucket as a New Yorker living in the Midwest.
I find that sarcasm is underappreciated in a lot of places and I would love to dig deep
into AI and sarcasm.
I think it's an, I think it's a East Coast Northeast kind of thing and I joke off and back,
nothing to back this up that it's probably related to just, you know, local climates and
it's a way to kind of deal with it, a great deal, great weather we're having, right?
And it's just gray snow and there's sludge and all the street corners are backed up and
whatnot, but you don't have that same thing on the West Coast where it's beautiful every
day.
sarcasm doesn't land in the same way with native Californians.
So true.
So at Spotify, you worked, what aspect of this problem are you working on there?
Sure.
So I think at a higher level to the extent that I can kind of delve into, I work on a team
of researchers where we are building algorithms that can understand music content at scale.
So some of the obvious applications would be to fit into, can we better understand users?
Can we help provide better recommendations?
A lot of recommender systems right now have gotten very far by looking at how users,
consumers, listeners interact with content, whether it's purchased at the same time or
in the same catalog, or they've been grouped into the same basket or playlist or things
like that.
You can get really far without having to look inside the box.
So the metaphor I like to make is algorithms for content work as well as they do for say
Amazon when you can kind of just, you don't have to look inside the box, but when you really
want to say more deeply understand a user and what they're after, it's like what color
is the shoe?
Is it felt?
Is it vinyl?
Is it interact with music, do they really gravitate toward an artist or is it lyrical content?
Is it about the harmonic content?
Because one of the things that's really interesting about music in particular is your ability
to enjoy it is tightly coupled to how well you can understand it and to what extent you're
surprised by it.
So for example, there's a one artist called Marsbo and for the untrained listener, it's
going to sound a little bit like noise art, but if you go to a Marsbo concert, there will
be people that are just totally rocking out and they're in it and they get it because
they have a model and an understanding for what's going on.
So you can use how people interact with content as a proxy for what they understand and what
they can relate to.
How explicit is that understanding for them though?
A lot of people can't, well, I can't speak for Marsbo fans.
But one of the really interesting things about music is that a lot of ways that we describe
it are so personal and occasionally they're cultural or they're niche.
So you'll find that in certain subgenres, certain words are used certain ways and you'll
see things pop up.
Oh, this playlist is on fleek and it's like you'll find that some of those language and
the semantics, there are course intermediaries to describe the thing you actually mean.
You're like, I love the part of this song where it really just makes my heart pump.
You're like, well, what is it?
I have no idea.
I have no language to describe these things.
Which makes education and visualization and interaction with the content, a really
interesting area for some of this content understanding at scale.
Botify.
Why do I like this song?
Are there other songs out there that you don't know about that would make you feel
have such a strong physiological reaction in a similar way?
Am I going to be able to ask botify that?
Why do I like this song?
Wouldn't that be a great question?
That would be an answer.
That would be awesome.
I mean, especially because as I hear you describe this, I am not a music kind of sore by
any stretch of the imagination, but it resonates for me that a lot of that is not really having.
I guess when we talked about understanding right there's, you know, the impact that music
has on you and kind of its ability to move you.
And then your ability to understand it moving you and how and why and when.
And then the next level is like your ability to articulate all that.
And I feel like out of for me, if I was able to maybe come at it from the back and be
able to articulate and understand kind of these things at a conceptual level, that might
help me to connect to other types of music.
And it would be really cool if I could ask if Spotify could basically teach me this.
And I would actually take to the logical conclusion because beyond that, then you could say all
this music was composed by another person.
So generally when you're composing as a composer, you're pulling upon all of your experience
and your own surprise and novelty, but that's going to relate and land with an audience
in a very particular way.
So when you think about an artist composing for their fan base or in a genre or a style
or trying to achieve a certain outcome, you know, you might say, you know, I really want
to drop, I want to a verse that's minor so we can step into a major chorus because people
will feel that as this release intention.
But the only way that that can be conveyed appropriately is if everyone has that similar
expectation.
So you're playing off of certain kind of musical behaviors that are in culture in certain
ways, which also makes music at the level of like a global culture really interesting.
Or micro culture as you start to be able to connect the dots across really neat genres
that, you know, didn't have any bandwidth in more of a mainstream music era.
And are you a musician personally?
Do you play what do you play?
I play everything I can get my hands on.
I grew up playing saxophone for about a decade, switched over to guitar and then guitar
and voice.
And I've been learning drums for the last couple of years.
And anyway, that I can kind of express myself with sound is a good time.
And when you're expressing yourself with sound, do you think about it in the way that you
previously described, like, I'm going to try and hit this chorus and I don't even have
my, I can't even, my ability with the words is so poor.
I can't even repeat what you just said.
But I get at least the impression that I have from, you know, pop to immediate TV, whatever
is it, you know, they just go into a room and then music comes out and not like, oh,
I'm going to nail them with this crescendo right here.
I think everyone's process is different.
And I mean, as my PhD was in a music program, so I had to take some graduate level theory
courses, which actually gave me not a perfect vocabulary for it, but a better understanding
of the things that I had developed an intuitive feel for.
And I certainly, I don't think about it that way in the moment.
When I'm playing music, I'm very much in it.
You may have heard of like the idea of like creative flow, where it's like time flies
and whatnot.
So I think for myself kind of coming back at it as an editor, I can think about it like,
oh, you know, like, this is a really good raw idea.
And I can massage this in a way, you know, if I piece these things together, here's a
really interesting musical pun.
I don't think I'm more in the moment.
It's a little bit more like I surprised myself.
You know, that was neat.
I have to record everything and then go back through with a little bit more of a higher
planning process.
So I may be getting away here and turning this into this week in music and, uh, they're
related.
And the arts.
I contend they're related.
But you're speaking here at the conference.
What's your talk on?
I'm talking about one project that we've recently published out of our newly minted music
understanding group at Spotify around primarily singer vocal separation from recorded music.
I lovingly refer to it, hat tip to a colleague from Miami, but it's unbaking the cake in
a way.
So in recorded music, you have these, this is basically acapella from any song, any track
or instrumental from any song.
So you can isolate the vocalist, you can remove the vocalist.
It's a little bit of like the audio processing wizardry that, you know, if you look to computer
vision, there have been some amazing, really interesting things with style transfer or texture
mapping.
So we took some recent advances with the picks to picks and the unit architecture and have
adapted that to music processing.
But the thing that really kind of made it work for us is that, you know, we have this really
the large music catalog and one of the big bottlenecks for source separation for a long
time has been data.
And we were brainstorming one day, it's like, you know, deep warning is great when you
have data for training and you have two options when it comes to data.
You can curate it or you can get clever and try to harvest it.
And a lot of computer vision stuff has gotten really far by using text around images and
leveraging these other kind of serendipitous signals that occurs a byproduct of other
kinds of things.
It's bought a similar thing with play listing.
So one of those signals we were able to harvest is that instrumental versions actually occur
with a non negligible frequency.
So if you have, say, a web scale music collection, you can actually end up with about a month
or two's worth of straight audio for training these kinds of algorithms.
So we were able to do some work out to like a percentage basis, like what percentage of
your catalog has instrumental versions, a very small portion that I probably couldn't
give you a number, either from memory or other reasons.
But it's small, but when you have a large enough catalog, it becomes sufficient.
So we end up with a couple of months of audio and we're able to train these algorithms
to both isolate and remove vocals from the mix.
So we nudge the state of the art a little bit versus some other models that have been
published.
Well, let's jump into that.
So you mentioned, you mentioned picks to picks and another one unit.
Yeah.
So the unit architecture preceded the picks to picks work.
Okay.
What about those two?
Sure.
So the unit architecture, actually taking a step back, one of the ways that a lot of, you
can generally call it signal processing in machine learning has worked for a while as
the idea that if you could have some kind of compressing auto encoder, reducing the
dimensionality, you'll preserve the attributes that are most important.
You can back it out and then by minimizing some loss over the reconstruction, then you
can start doing some interesting things, fiddling with your intermediary representations.
But what ends up happening is especially for audio, a lot of the high frequency detail
in the outputs is just the loss.
So it works pretty well for general shapes, but sharp edges, these kind of things fall
away in these auto encoder and butterfly style architectures.
The unit takes this butterfly style architecture, folds it over into a V or U, however you want.
And butterfly, like as the visual, you got this wide input, you're kind of compressing
down the dimensionality and you're fanning it back out.
Exactly.
They're like a bow tie.
But if you're going to take the butterfly, the bow tie, and fold the wings on itself,
what you can do is you can take the, this is a convolutional architecture, you can take
the feature maps from the forward path and concatenate them with the feature maps from
the reverse path or the inverse path.
And what that ends up doing is providing a lot more detail and just like more fine-grained
granularity so that when you, in source separation, what you generally try to do is produce
a mask and then you apply that mask over say an input, time frequency representation,
like a spectrogram, spectrograms are kind of like an equalizer curve drawn out over time.
So you can wait the relative contribution of each frequency been in time, just as a zero
to one.
So we use this unit architecture to produce a mask over the input representation.
Let's back up a second.
Sure.
Because I'm losing, what does it mean to map the, what does it mean to take the feature
map and circle it back on itself?
I think that's the way you said it.
So generally in a convolutional architecture, you'll have a bank of kernels, you'll take
each kernel and you'll confolve it with an input representation and you'll get out generally
a three-dimensional tensor of feature maps.
If your input is 2D, you'll have k kernels by x by y.
And on the inverse path, you can, you're also producing these feature map tensors.
So with the corresponding layer in the inverse path, you can concatenate the input feature
maps with the reverse feature maps along that case dimension, which becomes that in input,
you can convolve another kernel matrix or kernel tensor, I guess.
So it's kind of like you have your forward path features and your inverse path features
being processed at the same time.
So it allows the kind of like the composition of parts intuition for a deep architecture
that information's propagating all the way to what is effectively this higher level representation
of the network while preserving some of that fine grain detail on the way back.
And what we find is that the masks that are produced by this architecture, do you have
a lot more detail in what they're able to pinpoint in terms of the frequencies?
So it's able to really dial in the components that are contributing to say singing voice.
So the way this works is you can train one of these unit architectures for pinpointing
the voice.
So you can train in a separate one for isolating the voice.
If you had different data, you could imagine doing something similar for say drums or
other source specific architectures.
When you're training to pinpoint the voice, are you using like an acapella version as
you're training data, or are you still using your instrumental and somehow like inverting
it or something like that?
That's a great question.
And for the work that we published and we'll be presenting at the Izmir conference in Suzhou,
China and the near future, what we actually did was there were far more instrumental versions
than acapella versions.
So we kind of estimate what the vocals would be by looking at the positive difference
between a full mix and then the corresponding instrumental.
So it's kind of like a proxy for what the voice would be.
Which is not surprising why we get much better vocal removal results because we're training
on the actual instrumental spectra than the vocal which is estimated.
But it's a really interesting point for future work to say what other kinds of content
do we have at our disposal that we can kind of fold in to this kind of work.
So we've got some really encouraging results, we've got some demos that I'll be sharing
a little bit later.
I imagine there'll be out on the internet in not too long.
And so what else are there any other things that you talked about during your talk or
that you're planning to talk about during your talk you haven't talked yet?
I guess the only other thing I would mention is being that we're in a really pivotal
time for a lot of machine learning and artificial intelligence research as we really start to
frame this conversation.
It's been an interest of mine for a while now that like what we discussed earlier, this
idea of creative AI.
And I do want to take the time tomorrow to just have that shameless plug, music is a really,
really interesting domain.
It doesn't necessarily have the same societal impacts that autonomous vehicles could have
in terms of saving lines, but in a lot of ways music does have that human power in much
more of a emotional and kind of cultural way.
So as we start to think about like what other ways do we want to tackle artificial intelligence?
Like can we really study music without inherently studying humanity and kind of the intelligence
that we've been in doubt with?
And one of the questions that comes up for me in this conversation is a little bit out
of left field, but what in your view is the kind of open source as a concept has like swept
over a bunch of different industries and areas of human activity, right?
Is there an open source?
What is the open source for music?
And the thought that preceded that was it'd be kind of interesting if in some period of
time a musician delivered not this like one final thing, but you know there was like almost
like a project file for your digital workstation it had.
You can pull out the drums if you wanted to, you can pull out, you can isolate all different
kinds of things because it followed some kind of open source, you know, ideology or something
like that.
Is there an analog like that for music or what's the closest we get?
I think that's a really, really fascinating idea.
And you know, I have to smile a little bit when you say what's the analog to that?
In a lot of ways playing music together is that analog, you know, for a while ever since
it was music publishing or sound recording, it's been really interesting what sampling,
remixing, reuse means for music because everything that you compose, create, share is an
amalgamation of all your prior experiences.
So Larry Lessig.
So Larry Lessig.
A little bit of an inherent open source to music from that perspective.
Exactly.
Larry Lessig wrote a really great book called Remix and that touches on a lot of these
interesting things, both from a legal but also a philosophical and kind of cultural perspective.
A focus particularly on music or more generally on touches on, touches on things.
So there have been some famous mashup artists like Greg Gillis and Girl Talk.
I'm testing my memory at this point, but there's a famous composer who decried the rise
of sound recordings and that it was going to like change what reuse and remixing really
meant.
We've seen that get a little bit murky as copyright laws change or whatnot.
But yeah, it's music wants to be open source by nature.
So I think the analog is analog music, it's the acoustic signal.
Interesting.
It'll be interesting to see how music evolves along with machine learning and AI.
Yeah.
I think they have a bright future together.
Absolutely.
Well, thanks so much Eric.
Thank you so much Sam.
All right everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple
podcasts.
My producer says that one of his goals this year is to crack the top 10 and to do that,
we will need your help.
Please head on over to the podcast app, rate the show.
Hopefully we've earned your five stars.
Give us a glowing review and share it with your friends, family, co-workers, Starbucks
baristas, Uber drivers, everyone.
Every review and rating goes a long way.
So thanks in advance.
For more information on Eric or any of the topics covered in this episode, head on over
to twomlai.com slash talk slash 98.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter at at twomlai.
Thanks once again for listening and catch you next time.
