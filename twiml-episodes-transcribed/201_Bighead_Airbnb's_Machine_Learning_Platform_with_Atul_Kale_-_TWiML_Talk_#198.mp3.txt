Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
As many of you already know, one of the exciting parts of my work involves keeping tabs on
the way large companies are adopting machine learning, deep learning, and AI.
While it's still fairly early in the game, we're at a really interesting time for many
companies.
With the first wave of ML projects at early adopter enterprises starting to mature, many
organizations are now asking how they can scale up their efforts to support more projects.
Part of the answer to successfully scaling ML is supporting data scientists and machine
learning engineers with modern processes, tooling, and platforms.
This is the topic that we're super excited to address here on the podcast with the AI
Platforms podcast series that you're currently listening to, as well as a series of ebooks
that we'll be publishing on the topic.
The first of these ebooks takes a bottoms up look at AI platforms and is focused on the
open source Kubernetes project, which is used to deliver scalable ML infrastructure at
places like Airbnb, booking.com, and open AI.
The second book in the series, which looks at scaling data science and ML engineering
from the top down, explores the internal platforms companies like Airbnb, Facebook, and Uber
have built, and what enterprises can learn from them.
If this is the topic you're interested in, I'd encourage you to visit twimmelai.com slash
AI platforms and sign up to be notified as soon as these books are published.
In this episode of our AI Platforms series, we're joined by Atul Kale, engineering manager
on the machine learning infrastructure team at Airbnb.
Atul and I met at the Stradded Data Conference a while back to discuss Airbnb's internal
machine learning platform, Big Head.
In our conversation, Atul outlines the ML lifecycle at Airbnb and how the various components
of Big Head support it.
We then dig into those components, which include Red Spot, their supercharged Jupyter
Notebook service, Deep Thought, their real-time inference environment, Zipline, their data management
platform, and quite a few others.
We also take a look at some of Atul's best practices for scaling machine learning and
discuss a special announcement that he and his team made at Stradda.
And now on to the show.
All right, everyone.
I am in New York City here for the Stradda Conference, and I've got the pleasure of being
seated with Atul Kale.
Atul is an engineering manager at Airbnb on the machine learning infrastructure team.
Atul, welcome to this week in machine learning and AI.
Thanks, Sam.
Thanks for having me.
It's a pleasure to be here.
So yesterday you did a presentation on Airbnb's machine learning platform, which is called
Big Head.
It was a great presentation, and I'm really looking forward to diving into some of the
details with you.
But before we do that, how did you get into ML and AI?
Yeah, sure.
So I'm trained as a computer engineer.
I went to the University of Illinois, and right out of college, I actually started my
career in the trading industry at a firm called DRW Trading.
And DRW, they're proprietary trading firms, so they trade their own money for their own
profit.
They don't have any customers.
And you can imagine for a trading firm like DRW, it's really important to have a data
warehouse that's stockful of information about the financial markets.
So imagine you're working on a particular trading strategy, and you need something kind
of like Tivo for the markets, right?
So you need to be able to understand that a very fine granularity exactly what was going
on on a particular exchange or in the markets for a particular instrument.
So my first job at DRW was to create that data warehouse, you know, everything from kind
of the basic ETL and even like the data capture and all that sort of stuff, as well as
like that back end database that's required.
But then after I completed working on that, I made a transition towards working on trading
strategies, and that's kind of where I got more into machine learning.
So we went through kind of two different trading strategies.
One was kind of light on the machine learning, and one was really heavy on machine learning.
And in that latter one, I started really focusing on infrastructure.
I think in general, my background is in back end work and infrastructure engineering.
So that's really, you know, where my passion is.
And for the trading strategies that we were working on, we really needed a high degree
of automation and sophistication around training our models.
So I spent quite a bit of time working on building infrastructure for my individual team.
And then, you know, about a year and a half ago, I was kind of looking to make a transition,
and I got a good opportunity at Airbnb, you know, working on their machine learning infrastructure
team, kind of combining this passion for infrastructure with the interests I have in machine learning.
So I've been working there for about the last year or so, and I'm currently leading the
team.
Where you explained the motivation for big head in your presentation, I thought was very
well put.
You talked about this notion of inherent complexity and incidental complexity.
What are those?
Right.
Yeah.
So I think the term that we use is intrinsic complexity and incidental complexity.
We kind of just maybe use terms up, but the idea is pretty simple that the intrinsic
complexity with machine learning is all about, you know, kind of understanding the latest
modeling techniques, picking the right model, picking the right features for your model.
And then really fine tuning that, you know, machine learning in some ways is kind of an
art of, you know, really understanding your problem domain and fitting the right models
to it.
And that's really what we found that our ML practitioners at Airbnb, by that I mean,
now in the data scientists and engineers that are working on deploying ML in our product,
we found that they're really interested in solving those problems.
But on the other hand, there's this other side of getting a machine learning project
off the ground.
And that's incidental complexity.
That's what we call it.
And that has to do with all like the messy details of, you know, getting access to your
data warehouse and then making sure that what you prototype with is, you know, what is
consistent with what you actually go into production with.
And, you know, just having consistency also between your, your trained model and what
you're going to actually see in production at inference time, for example.
So there's a lot of like messy details there, you know, you also have to deal with managing
all the various experiments that you've got, the versions of your model.
And you know, all of that just adds up to a lot.
And we, what we found is that when teams were having to deal with intrinsic complexity
and incidental complexity together, that's just, you know, overwhelming for them.
And a lot of teams at Airbnb were just not even that interested in machine learning just
because the cost of it was so high.
So part of the goal is to reduce the barrier to entry so that a greater proportion of
teams could incorporate machine learning into what they're doing.
Exactly.
Yeah.
I mean, our goal as a team, the machine learning instructor team is really to scale ML
at Airbnb.
And in order to reach that goal, of course, you know, we're going to need to reduce the
barrier to entry.
So that's really important to us.
What did it evolve? Was it initially a collection of specific tools that solve specific problems
or was it architected from the beginning as kind of a broad platform?
Oh, yeah.
Definitely.
We did not get this right from the start.
I think that, yeah, as with many products, it's just like, you know, cycle of iteration
and, you know, lessons learned and then applying those lessons learned and building something
new.
The history for us specifically is that, you know, big heads components, sort of the
subcomponents of big head are their pieces that Airbnb had kind of collected over the
years, all to solve, you know, really specific problems with machine learning, with getting
machine learning into production.
And we found that that was kind of piecemeal, you know, of course, those problems were
solved, sometimes not in the most sustainable way, but it solved nonetheless.
But then, you know, when we looked at the entire system and we looked at the shortcomings,
the problem was really that it wasn't cohesive, all these piecemeal components didn't really
fit well together.
So big head was an attempt to kind of make one more pass over the entire system, the
entire architecture, and really focus this time on cohesiveness and end time consistency.
Maybe let's walk through the various components of the system.
Where do you start?
Oh, you went through this in your presentation yesterday.
What was the first component that you presented?
Yeah, sure.
So we start with where our users start, right?
So our users need to prototype.
They need kind of a development environment for machine learning and for that we have
a red spot, which is really just a Jupyter Notebook service.
It's a Jupyter Notebooks as a service, but you know, they're really not just, you know,
the same as running Notebook on your laptop.
It's pretty high-powered.
You can plug it into more or less, you know, any EC2 instance type on AWS that we have
available.
So you can get access to things like GPUs and really high-memory instances.
So you can be kind of unblocked to get your work done.
But then in addition, it's really cleanly integrated into our data warehouse.
So you have access to all the wealth of data that's going to actually make your machine
learning project powerful.
Yeah, one of the things that I've run into just with personal experimentation is you
choose an instance type, you start running some experiment and then you realize that you
get to a point where you wish you had more hardware.
Like, have you managed to decouple the notebook itself from the underlying infrastructure?
Yeah, yeah, and that's a great point.
I think this is actually something that we're looking to address in the future.
It's not just that you might not know exactly what hardware you need, but also it's pretty
efficient or inefficient rather for every user that might need to train a deep neural network
to fire up a GPU machine and have it sitting around for weeks maybe while they tinker
around with their model, right?
So that's definitely an area of concern.
You can't vertically scale individuals' development environment for long.
So what we're doing, and this is kind of on our roadmap, is to build out what we're calling
Big Q right now.
The idea is that your notebook environment can then become really thin and instead what
you do is you just submit your job for training your model off to a work queue that's
powered by GPUs or potentially the latest in distributed training algorithm, something
like that.
This is sort of a follow-on to the rest of our infrastructure where the goal is to really
neatly encapsulate models.
Once we have that powerful abstraction, now we can create these high-power tools like
Big Q where we understand enough about your model that we can train it really efficiently.
So it's definitely kind of out there on our roadmap.
I think it was one of those things where we took an initial stab and we looked at our
user base and we said, well, yeah, this kind of works for most people, and so it's definitely
kind of minimally viable there, but we have a lot of plans to make that more efficient.
And then underlying the notebooks, all of the code is stored in Git repositories, and
is that transparent to the user, or is the user kind of manually checking things and
checking things out?
The user is indicating that they want to check things in, but have you built kind of
in UI into the notebook experience where they're doing that or they're doing that kind
of traditionally?
They're doing that more traditionally.
I think that we can try to hide away some of the details of version control, but ultimately
it's kind of just a complicated messy process, version control in general.
So we don't really want to try to reinvent the wheel on that.
What we focus on is just making sure that it's easy to get your notebook or Python code
into production, and that it's neatly version just like any other production code.
And now Python code typically has, or the notebook code typically has annotations and
other types of artifacts that you don't necessarily want to, you know, they're there for experimentation,
but they're not there for, you know, when you're actually trying to put a model into production,
do you separate those somehow?
Yeah, yeah.
So I think just to start, I would say that we don't necessarily focus on notebooks.
We really just treat notebooks as Python code, and what we do is pre-process those notebooks
to kind of strip out the annotations and strip out some of the exploratory work that
you might be doing while you're prototyping.
And you know, right now, of course, that's pretty simple.
You just have to tag your notebook cells as prototype or not, and we just strip them
out.
Maybe who knows?
We'll come up with a machine learning model to do that automatically or something.
But yeah, it's pretty much manually tagged for now, and we've actually been doing that
for a while, and it seems to work reasonably well.
But I think that one thing to note is that notebooks are kind of a mixed bag.
On the one hand, you've got this ability to annotate things, to show your work, and
to, you know, really show the process of experimentation.
And that's great for machine learning projects where, especially when you get to kind of code
review time, what you're reviewing when you look at somebody else's project isn't really
just their code.
You're reviewing kind of their thought process, their experimentation process.
So it's almost kind of more like a research paper or something like that than it is just,
you know, a normal code review.
So notebooks are great for that.
But they're not great for is kind of the traditional software engineering process.
Composition is in great, you know, of course, you can jam all of your code right into the
notebook, but, you know, a lot of times we find people repeating themselves.
And testing, unit testing, this is super important for software engineering, but it's equal
important for some of this machine learning code.
And I think we're trying to strike the right balance between having those really solid
software engineering practices as well as having that flexibility of the notebook.
So what's the next component?
So next up in sort of the lifecycle for getting your model into production, that would be
Big Head Service.
So the idea with Big Head Service is to manage all of the different versions of your model,
maybe the different experiments that you have, or you can imagine that say you have a model
that needs to be trained weekly.
All of those versions of that model are stored in Big Head Service.
And it's got this nice convenient UI where you can go in and click deploy on your model
and voila, it's in production.
And kind of, you know, for a lot of users, we're hoping that, you know, they start with
their notebook, they go into Big Head Service, they hit deploy, and end of story.
That's sort of the happy path, of course.
You know, and maybe you don't have to go even further than that, but, of course, things
do go wrong.
And when they do, Big Head Service also kind of serves as a portal into some of our model
debugging tools.
And those are more related to how we run our models in production.
So the next pieces in the process are to actually get your model into production.
So say, you've got a model that takes an image of, I don't know, like a listing photo,
and predicts whether it's a bathroom or not.
And you need to be able to do this live in real time in production.
Maybe it's hooked up to the Airbnb website, and you want to, when you, when a host uploads
a listing photo, you want to see if it's a bathroom to ask them something like, you
know, how many towels are there, do you have a hair dryer, you know, little details
like that.
So say there's some kind of feature, and you need just sort of a live prediction.
That's where our component deep dot comes in.
It's just designed to be scalable, offer like a rest endpoint for your model.
And that's, that's something I think that's pretty standard we've seen across the industry.
And then we've also got another productionization component that's called ML Automator.
The idea here is to automate batch training and batch inference.
And I think at Airbnb, maybe more than other, other places batch inference is actually a
pretty common thing.
So there's a lot of times where you don't really need a live score.
You just need to kind of backfill a lot of predictions or scores across your data.
And so that's where ML Automator comes in.
And so when you're, when the developer is working in the big head service interface and
they click deploy, that's kind of pushing the model to deep thought deploying it out on
some actual machines and exposing the rest endpoint.
And then they can build that into whatever they're trying to use the model for.
Yeah, exactly.
If they have their model set up for live inference, then when you click deploy, it's just like
deploying a service at Airbnb, right?
It's kind of a new version of that service that's going out there and that endpoint is now
available.
And you know, we do some things to try to make sure that that endpoint stays available for,
you know, spikes in traffic or, you know, potentially if that endpoint, that particular model needs
a lot of memory, maybe to load a bunch of parameters into memory.
So we do a lot of fine tuning there to make sure that more or less we kind of hide that
away from our users who don't necessarily want to deal with the details of scaling their
model.
Right.
And hopefully, you know, since we're leveraging Kubernetes for that, we're hoping that
we can really get some of the benefits of potentially auto scaling algorithms there.
And has that, has that worked well so far?
Yeah, Kubernetes has been definitely working well so far.
I think auto scaling is a little bit trickier, but that's something that right now we're
just kind of getting into.
And are you using, are you kind of finally managing GPU requirements and things like that
that a given model might have and placing those on specific pods or workers in the Kubernetes
environments?
Yeah, yeah, definitely.
So when we ask our users to specify their model, we ask them to specify their requirements
for a computation up front.
So that includes like memory requirements or a number of cores or potentially GPU.
And then we use that throughout the rest of our system to make sure that their model
runs an environment suitable for their resource requirements in production.
I think earlier in the conversation, you've mentioned experiment management and things like
that.
But I don't think that came up in big head service deep thought or ML automator.
Is it some place else in the system?
No, actually, that's right.
And that's very built into big head service.
So yeah, I think that like this whole process of getting your model from prototype and
clicking the deploy button and its introduction, that's great.
But it's actually kind of scary, you know, just getting your model in a production with just
a quick deploy, you know, is it, is it ready?
Is it predictive?
Do you know whether it even works or not?
So that's where big head service offers a whole lot of kind of introspection tools into
your model.
And you know, it's like some of the examples of that, you know, if we wrap a particular
library called, like let's say, XGBoost, we're able to produce a lot of details about the
train model that you have right from XGBoost, you can of course, like, get details like
ROC or PR curves, and you can also, you know, for XGBoost, get feature importance information.
And both of those types of information, both the actual performance of your model and
the, you know, explainability of your model, that's really important before you click
deploy.
And you can kind of surface that to our users in big head service.
And of course, all of these tools that we have, particularly visualizations around this,
they're all available in your notebook environment.
So we kind of use the visualizations that we see people doing in their notebook environment.
And the most common ones, we kind of elevate them to the status of being incorporated into
our infrastructure.
If they kind of reach that point where we really see everybody doing this all the time,
or we might be thinking, you know what, feature importance, that's probably something useful
that everybody should see.
So we'll just automatically incorporate it.
So this is all built right into the UI.
Are you doing anything in big head service or ML automator that is doing automated hyperparameter
optimization?
So that's kind of an area that we're hoping to get into.
Right now, the training phase is, and especially hyperparameter optimization, that's sort of
left to the user way at prototype time.
But one of the things that we want to do with Big Q, which I mentioned before, is offer
hyperparameter optimization.
And I think that part of what we need to do here is just have people specify their model
to us in a little bit more structured of a way than, all right, here's a Python function
that trains my model.
For us to be able to do something like hyperparameter optimization.
So once we have kind of better adoption of that, then we can move on to that next level
of abstraction on top of this, where we can automatically train your models.
And that includes hyperparameter optimization, but also like I mentioned before, using
like advanced training algorithms, whatever you need.
So one of the other platforms in the Big Head ecosystem is Zipline, which handles a lot
of the training data, pipeline data acquisition.
Can you talk a little bit about that?
Yeah, yeah, sure.
So this entire process that I mentioned, you know, going from prototype into production,
there's a lot of ways that it can just go wrong on you.
And what we found is like consistency across the board is really key.
So in addition to the four components I mentioned already, we've got three other components
whose job is purely just to maintain consistency.
Some of that is around consistency of your environment, and maybe the code that you're
executing, the actual logic of your model, and some of it is around data.
Now data is super important for machine learning projects, having the right feature data.
And so we created Zipline as sort of a solution for keeping consistency across the board.
And I think that, you know, this is probably one of the areas I think that we see sometimes
overlooked, and it's definitely a piece that's really hard to get right.
But when you do it, when you have a single system that owns getting feature data for the prototype
phase when you're just kind of playing around, and you don't, you know, want to wait three
days for your backfill job to generate all your data, but also having that same system
be responsible for materializing your data when you need to train, you know, the entire
large scale model, and maybe perform batch inference on the model, or even real time inference
with the model, having one system that is responsible for features that's really key.
And it's a pretty hard goal to get to, but we're taking a stab at it with Zipline.
Yeah, I think one of the things that I remember picking up on in your talk, maybe
it was another talk, I don't know if I should attribute this to you or if it was someone
else, but there was a comment that someone made at this event about data scientists not
wanting to share their features.
Do you run into that?
That's interesting.
You know, I guess maybe the culture is very from company to company, but at least at
Airbnb, we've gotten a lot of value out of sharing.
Now of course, there's situations where, you know, it's not so much about wanting to
share your features, but you know, machine learning and features in general, it's all
really tied together.
You can't really separate exactly how your features are calculated from how your model
works.
And let's say you create some features and I want to use those features and maybe you're
not quite happy with that implementation of those features and you want to change them
down the line.
Well, now I've already got my machine learning model in production.
That's going to be really disruptive to my flow.
So I think that maybe some data scientists might be a little bit more cautious about sharing
what they're working on because, you know, as soon as you share something, you kind of
need to keep it a little bit more constant.
But what we found is that, you know, as long as we're able to appropriately version our
feature definitions and kind of deal with it in the same way that you might have a library
that you depend on.
You know, maybe you depend on TensorFlow version, you know, X, but TensorFlow version Y has
been released.
And as long as you don't upgrade, you know, you're fine.
And I think that's a pretty important part of our system.
And the way that works, you know, every single time there's a new version of a particular
feature, we had to go and back the level of a bunch of data up front before you could
use it.
That would be pretty painful.
So this only works when you kind of lazily evaluate your feature data sets, just kind
of on demand.
And that's something that's like a key focus for zipline.
So how are the features defined?
The basic way that you define a feature in zipline, you'll define kind of some information
about where to fetch the events for that feature and potentially some aggregation that you're
going to apply over those events.
And we kind of try to model roughly everything as events, but sometimes that's a little bit
of a tough fit.
And then, you know, so now you've kind of told zipline how your feature needs to be calculated
at any given point in time.
But zipline doesn't know what points in time, you know, you actually need your feature
information.
So what it does at that point is not much.
It just kind of keeps track of that.
But then when you come to it and you say, all right, I need a training data set.
And, you know, I need to be able to see my features over the last year.
And here's all the particular timestamps I need, my features calculated at.
At that point, zipline gets to work, gathering all the information it needs and efficiently
aggregating everything and materializing your data set.
So you can imagine, you know, let's say that you have a page on the Airbnb website and
you're trying to predict whether a user visiting that page is likely to book.
And you want to know maybe the like last seven days of history in the last seven days,
how many bookings did that user make?
Maybe that's like a feature into your model.
So specifying, you know, where you get the information about bookings and that you want
a seven day sum or average or something like that, that's sort of the feature definition
side of it.
And then the next side of it is when you actually need to materialize your training data set,
you actually give zipline a series of timestamps and say, okay, for these users, at these
times, give me the information about what their seven day booking history was.
So there's kind of that two step approach and that lets us materialize the data only when
we need to.
And where are those timestamps coming from?
Is this so that you can compare models generated at different times by kind of rolling back
the clock and looking at the training data that the models saw when they were trained,
or are there other use cases?
So I think the way to think about it is the training data set that we're creating, the
columns in that training data set are the features that you might define.
Maybe like your seven day booking sum or something like that.
And the rows are really all of your data points for your machine learning model, right?
So say that a user goes and views a particular page and at that point in time, we want to
know what the state of all these various features were.
And we know at another point in time in the future, what actually happened with that user,
right?
Like did they book or were they just viewing the page?
So each of those data points, you can think of like imagine you have like millions of
page views over the course of, you know, some time period, each of those data points ends
up being an input into your machine learning model, right?
So does the event that actually happened plus all of these features that you think are
relevant to that?
Exactly.
Exactly.
So you can think of it as zipline materializing that data set for you.
And there is of course kind of like the separate question of machine learning model evolution
over time and how training data is impacting that.
And I think that's sort of somewhat of a higher level problem than just directly the
future data that involves the model as a whole as well.
And that's something that we're hoping that the UI visualization tools and think that
service help you do is to kind of see how your model is trending over time.
And it sounds like zipline's ability to access training data relative to these timestamps
could feed into the ability to, you know, if he's into reproducibility, like can I,
you know, can I reproduce the steps that went into creating this model, you know, if I'm
training it at a different time and it sounds like the having an ability to have just having
the flexibility to manipulate training data with respect to time is feet would feed into
that.
Yeah.
Yeah.
Exactly.
And I think like one way to explain zipline is we like to use this analogy.
It's like a time machine for your data warehouse, right?
You know, you have all these events that are data points into your machine learning model
and they happen, you know, all throughout the continuous space of time that you have in
your data warehouse.
But your data warehouse isn't really suited for time in as much of a continuous sense.
You know, it's more suited for kind of large level, large scale aggregation.
And at least at Airbnb, our data warehouse is very much oriented in like sort of a day
by day snapshot of the world.
And that works really well for humans.
You know, humans can't really process things at a millisecond or but second by second level
anyways.
So we of course need to aggregate it.
But for machines, you really need that high precision at every single point in time for
your feature data to actually be useful.
So having that ability to kind of like rewind time and go to any particular point and say
what was the exact state of the world at that moment that my machine learning algorithm
would have seen when it's trying to make this prediction, that's really, really valuable.
It's actually something that kind of is parallel to what I worked on before in the trading
industry where you want to be able to rewind the clock and go exactly to a point in time
and see the state of the market.
And then of course, there's that other part of using your model and that is actually
performing inference live, right?
So if you have a great system set up to rewind to any particular point in time, that same
system, if it's responsible for calculating your features live right now, then you have
that absolute consistency between getting gathering all your trading data and actually
running your model in production and creating live predictions.
And what we found is that when people don't have one system to do both those things,
they end up kind of doing a little bit of manual ETL in their data warehouse and then maybe
in their live service, they try to replicate that.
And that leads to a lot of problems.
Now you've got two code bases doing basically the same thing, maintained by potentially
different people.
So there's a lot of opportunity for those things to become inconsistent and besides, it's
just kind of wasted effort.
So having that kind of single system that encompasses both your offline data warehouse work and
your online live service work, that's really important.
You mentioned in constructing these features, you're specifying aggregations that are happening
over the underlying events, in the case of, for example, a deep learning pipeline, you
may also want to do some stand that I said of data augmentation or data transformations,
things like that, image transformations, does big head or zip line support those types
of things?
Yeah, yeah.
And this is actually an interesting question.
In general, when it comes to forming our feature data, like before we actually go and submit
it into a machine learning algorithm, we do a lot of pre-processing to it.
And some of that pre-processing is things like I've talked about before, potentially doing
aggregations, maybe sums across several days of data, maybe if you're trying to get a
history of seven day bookings, like I mentioned before, but then some of it is just like
extra-processing that we're doing, imputation or normalization.
And we're trying to find the right place to draw the line where that actually happens.
Does it happen in zip line where you're materializing your feature data data set, or does it
happen in what we call your model's pipeline?
And that means what's built into your model directly before your data enters into a machine
learning algorithm.
So that's a tricky balance, but I think that one lesson we've learned there is that if
your model is seeing those same features and doing that same imputation or normalization
in production, maybe on live queries that it's getting, then that needs to be more
built into sort of the model pipeline.
And if in production, and if you're trying to reproduce something that in your backend
data warehouse that maybe the model would have just seen directly from the queries it's
getting from a live service, then that kind of belongs more in zip line.
So it's sort of a balance between kind of doing some computation upfront in feature
calculation and doing some of it as part of the model.
And I think we're still kind of trying to figure that out.
And so for those types of transformations that should happen in the model, we've talked
about this Jupyter Notebook experience is there also some kind of workflow or pipeline
engine, or have you thought about and decided against that?
Do you are there templates or a library of sorts that data scientists and ML engineers
have access to?
How do you address those kind of abstraction types of issues?
Yeah, this is an excellent question.
So in the early days for us, we did something kind of really lightweight in terms of the
interface that we provided our users.
We asked them to define basically a Python function that trains your model and another
one that kind of performs inference with your model.
It's just like, hey, there's two Python functions, fill these in, you're good to go.
And there's some trade-offs with that.
So on the one hand, it's great with the flexibility.
You can really almost plug in almost anything.
And it's really simple for people to take what they had already been doing likely in their
notebook and just get it more or less straight into production by just kind of filling out
these functions.
But what we found was that it was kind of lacking.
We found that people were reinventing the wheel a lot when they were writing these models
for production.
They're really redoing a lot of work that had been done before by themselves in another
model or by somebody else.
So we decided to go kind of add a little bit more structure to that workflow and have
them instead create what we call an ML pipeline.
It's really kind of just a pipeline of transformations that happened to your input data before it's
output.
And a model you can think of the output, sometimes it's a prediction, but really it's just
a function that's transforming some input.
So what we've done is we've tried to create a really generic interface for just specifying
these transformation pieces and plugging them in together in kind of this like really easy
to compose way that you can just create a entire end to end pipeline to do some stuff
like image preprocessing or imputation or normalization and then have that be just
enough structure that we can actually understand a little bit more about your model and do
some more intelligent things than we could if we just had a function to call for training.
So for example, if we know that your model is a deep neural network and maybe on the
back end we've enabled this feature that gives us maybe distributed training or something
like that, just having a little bit extra information from our users that slightly more
onerous interface for them to specify their model, that's really useful, that's powerful.
But that's definitely a trade off because our users are looking at that interface and
they're wondering why do I need to use this.
So I want to just, I already have my model or this is really complicated, I know TensorFlow
or I know Keras, can I just use that.
So we have to be really careful about adding value in that interface and keeping it really
lightweight.
And we try to add value of course through our infrastructure but you then end the prototyping
phase.
We need our users to adopt these tools up front, this is something we've learned and
we found that other companies have also come to this realization that those tools that
you use to productionize your model can't just be incorporated at that point where you're
ready to productionize.
They need to be used up front as much as possible because that way you'll really lower that
barrier to entry into production.
What's the experience or the interface for these pipelines?
Yeah, so you can think of it kind of like plugging in Lego blocks together, you hopefully
the blocks that you need already exist, things like imputation or normalization and you kind
of have an easy way to, I talk about it, we call it a pipeline but it's really kind of
a dag of computation that's happening on your input feature vector before it produces
some output.
So really what you're doing, we're trying to create a lightweight way for you to create
that computation dag, that having to think too much about the details of what dag are.
But we create this pipeline for you and fortunately I can't, the best way to explain it is really
to look at a code sample of how you create a pipeline but I can try to describe it badly.
That answers the question for me, it was really is this something that's defining code
or is there some user interface and is there some notion of a repository of steps that
I might want to plug in that I'm doing visually or do I have to know what those are and
like I'm using a library, a standard library of some sorts and call them from within code.
It sounds like it's the code orientation.
Yeah, definitely, there's several different ways to do things you could prefer a config,
you could prefer code, you could prefer UIs for managing this but what we found is code
seems to be the best way to go.
A lot of times what we see is people doing powerful stuff like programmatically creating
these pipelines or composing them and I think that's a lot easier with code and I think
that to address the piece about sharing, yeah of course, we don't want all of our users
to be reinventing all of these different LEGO blocks so we have our inventory of pipeline
transformations that they can just plug into but they can also kind of share those pieces
that they've created so that they can easily maybe if somebody on another team is working
on a particular model that is sort of relevant, maybe it's an image model and you're also
working on an image model.
You can actually go to the UI and browse and click in and see what pipeline transformation
components they're using and say, hey, oh yeah, you know what, I actually do need a piece
that kind of like resizes my image or something like that so I'll just go ahead and use
what you have.
So that was something really important for us to incorporate into our kind of discoverability
and sharing aspect of our UI.
Okay, awesome, awesome.
So for folks that are maturing in their use of machine learning, AI, but don't have
a platform in place, are there kind of a handful of core principles that they should be
keeping in mind as they start to coalesce different tools into, you know, something that's
more coherent?
Yeah, yeah, absolutely.
So I mean, we've talked to a lot of companies and they're all on this, on various stages
of this journey in scaling ML at their organization, right?
You've got, you know, the big tech giants like Google and Facebook who are really far
along have been doing this for years and, you know, I think they're getting closer to
that ideal of, you know, engineers or data scientists, everybody feeling like they have
ML in their toolkit.
But, you know, and I think Uber is actually coming along on that as well with Michelangelo.
Michelangelo is a major inspiration for us.
And we've talked to other companies as well who are, like, kind of going down that journey.
And, you know, we're seeing a lot of the same problems.
I think that two things that I would mention, one is consistency, be thinking about consistency
in, you know, all aspects of the lifecycle of deploying your machine learning model.
And that's what pieces like the big head library that we talked about and a zip line are
meant to address.
And I think that data is, you know, really probably one of the hardest things to get right.
So, my other point is about, you know, understanding the value of making sure that data and the
entire pipeline leading up to machine learning at your company is really well set up for
machine learning.
I think that, you know, something that we've experienced at Airbnb is that machine learning
is kind of at the tail end of a whole lot of work that's happening upstream.
You know, you've got your production services, the databases they use, maybe the data warehouse,
and then lastly, you've got machine learning tacked on to the end of that.
And so, one of the things we've noticed is that there's sort of a butterfly effect, you
know, upstream changes can have a really outsized impact on machine learning.
And one of the things we're trying to change is just getting more visibility around machine
learning as something that's being used downstream throughout the company.
And I think that that's like kind of a broader organizational type problem that you need
to be able to solve.
Getting awareness about machine learning, what the benefits of it are, and then also having
people incorporate it into their workflow and their understanding of kind of, like, what's
downstream.
So, yeah, data is, of course, like, very critical and very hard to get right, and really
kind of unique to every different organization, the types of data that's important to you,
the pipelines you have set up for it.
But I think there's certainly some tools and best practices that are emerging throughout
the industry.
I think it all kind of boils down to this analogy of using some of, like, the best practices
from software engineering that we've accumulated over the decades and applying them to machine
learning.
I mean, sometimes this analogy doesn't work great, but as with all analogies, but I think
it can kind of go a long way.
So, making sure that you have a good developer environment, making sure that, you know,
you have solid versioning, unit testing, continuous build deployment, monitoring, observability,
these are all things that we've kind of arrived on for software engineering, traditional
software engineering.
And one of the things we're trying to do is just translate those concepts to machine
learning.
Understand, like, what does it mean to unit test your model or integration test your model?
What does it mean to, like, code review a model, right?
This is also a best practice for software engineering, but we need to kind of reinvent
it for machine learning.
And yeah, what does observability look like for a model?
So that's some of, like, the, in kind of a high level way, how we're going about approaching
scaling ML.
And we should probably also mention that you announced in the talk that Airbnb is planning
to open source big head.
Yeah, that's right.
So Airbnb, we've got kind of a culture of being a host to our community.
And I think that's why there's been several open source projects from Airbnb, like Air
Flow and Super Set.
And we're hoping to actually open source big head as well in the coming months.
We're really excited about that.
It's kind of something that our entire team has been thinking about since the start.
It certainly takes a lot of work to actually make happen.
So we're hoping to get to that in Q1 of 2019.
Awesome.
Awesome.
Well, I'll be keeping track of it.
And it's an exciting step.
Thanks so much.
I'll appreciate it.
Yeah.
Thanks for having me, Sam.
All right, everyone.
That's our show for today.
For more information on Atul or any of the topics covered in this podcast, visit twimmelai.com
slash talk slash 198.
To learn more about our AI platform series, or to download our eBooks, visit twimmelai.com
slash AI platforms.
As always, thanks so much for listening and catch you next time.
