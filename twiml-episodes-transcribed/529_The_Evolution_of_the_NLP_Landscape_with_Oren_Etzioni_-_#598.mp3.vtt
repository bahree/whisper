WEBVTT

00:00.000 --> 00:06.880
Oren. Hello. Good morning. So for those listening in, I'm here with Oren at

00:06.880 --> 00:13.720
Cione. He is the founder and CEO of the Allen Institute for Artificial

00:13.720 --> 00:19.200
Intelligence and the acronym for that is AI AI. So it's often shortened to AI

00:19.200 --> 00:26.880
too. Oren, how long have you been doing this? Well, it all started for me about

00:26.880 --> 00:32.920
nine years ago. There wasn't an AI too, but the late Paul Allen's team reached out

00:32.920 --> 00:38.960
to me and said he wanted to create an Allen Institute for AI. And it was up to

00:38.960 --> 00:45.960
me to come in, write a plan, and make it happen. It sounded like an incredible

00:45.960 --> 00:50.080
challenge. I remember saying to people, people ask me, why are you doing this?

00:50.080 --> 00:54.560
You're fat, dumb, and happy. As a professor at the University of Washington,

00:54.560 --> 00:58.280
tenure professor, you've got a good thing going, why would you do something

00:58.280 --> 01:04.480
crazy like this? And I said, the sky's the limit with Paul Allen's vision, with

01:04.480 --> 01:10.480
his resources, with his commitment to AI, we could do amazing things. In fast

01:10.480 --> 01:15.160
forward nine years, I feel like there's a lot still to do, but we have done

01:15.160 --> 01:20.400
some good things. So I want to dig deep into what the Allen Institute has been

01:20.400 --> 01:24.520
up to, because it's kind of amazing how much you've accomplished in nine years,

01:24.520 --> 01:30.240
just the impact and the unique way you've done it. But first, I want to give

01:30.240 --> 01:36.400
people listening in a sense of who you are. So I had my Orin moment. I think it was

01:36.400 --> 01:42.840
2018, 2019 at latest, and you came and you gave a talk in San Francisco, and you

01:42.840 --> 01:47.320
did a live demo, which is pretty unusual already, of natural language

01:47.320 --> 01:54.080
processing, NLP. And the room was packed. And I was one of many, you know, people at

01:54.080 --> 01:59.920
startups trying to make this stuff work, and hoping to make it big. And, you know,

01:59.920 --> 02:05.880
this legend walks in and gives a presentation. And you had it live on the AI2

02:05.880 --> 02:09.880
website, a bunch of models doing stuff. We take that for granted today, that you

02:09.880 --> 02:14.840
would have models just running live, powered by GPUs. To my knowledge, you were

02:14.840 --> 02:20.160
the first doing it. And what was absolutely mind blowing was you would, you

02:20.160 --> 02:24.080
would do what we all do, which is you'll show some some cherry-picked examples.

02:24.080 --> 02:29.560
But then you did something that no one ever does. Not then, not now. You then

02:29.560 --> 02:33.600
broke it in front of us. You would just say, yeah, looks impressive, right? But

02:33.600 --> 02:36.360
let me just show you what happens when you change the wording of the input,

02:36.360 --> 02:41.640
just a little bit. And you said, this stuff barely works. And those words like

02:41.640 --> 02:45.120
seared into my brain. And they did a lot of good, because it made me very skeptical

02:45.120 --> 02:50.240
and pragmatic. So that when you actually got something to work, you didn't just

02:50.240 --> 02:56.440
say, you know, you didn't overhype it. And so, you know, that, that, that is you

02:56.440 --> 03:01.200
in a nutshell, straight, straight shooter. Just tell me like what you were

03:01.200 --> 03:06.200
feeling back in 2018, 2019, you were halfway through, you know, this, this

03:06.200 --> 03:12.120
grand project, you would lay the groundwork. And you had contributed a ton to it.

03:12.120 --> 03:16.120
People don't realize that this whole obsession with muppet names really began

03:16.120 --> 03:23.360
with Elmo, right? Right. Well, John, thanks for remembering this and the demo

03:23.360 --> 03:30.440
and so on. I do feel like the principles that we have are were guides us through

03:30.440 --> 03:37.000
hype and turmoil and ups and downs. And that's the history of AI. But it's also

03:37.000 --> 03:42.480
the future of AI. So let me take the rich things you talk about and break them

03:42.480 --> 03:48.640
down. First of all, we have now phenomenal language models that do quite

03:48.640 --> 03:54.680
remarkable and certainly very impressive things. But our colleague, Kate Metz, of

03:54.680 --> 04:00.080
the New York Times says, never trust an AI demo. So you're absolutely right that if

04:00.080 --> 04:04.400
you, if, if you don't get to kick the tires, if you don't get to ask the right

04:04.400 --> 04:09.240
questions, then you don't really know how well it does. And by the way, the best

04:09.240 --> 04:14.520
demonstration of that is actually in all of our living rooms with, or in our

04:14.520 --> 04:19.000
phones, right, with Siri and Alexa and so on, you ask Alexa for something you can

04:19.000 --> 04:23.080
get a phenomenal answer. And then you change the wording slightly and it says,

04:23.080 --> 04:29.360
I don't understand it. So we have to be very careful with what we impute to these

04:29.360 --> 04:34.200
systems. And of course, there was a recent Bruhaha about, is this Google AI

04:34.200 --> 04:41.400
system sentient and, and so on. And of course, of course, it's not. So, so I do

04:41.400 --> 04:49.360
think it is very important, as you say, to, to, to be a straight shooter. And it's

04:49.360 --> 04:53.360
also true that one of my favorite sayings, because sometimes people tune in

04:53.360 --> 04:57.720
and they're like, wow, this is amazing. One, another favorite saying of mine is,

04:57.720 --> 05:03.400
our overnight success has been 30 years in the making. So if you look at a

05:03.400 --> 05:10.200
model like GPT-3 or Lambda or the, the latest of the bunch, they do have a long

05:10.200 --> 05:14.760
history that goes back to Bert, goes back to Elmo, which you kindly remembered,

05:14.760 --> 05:21.640
which was invented at AI2, won the best paper awarded 2018, goes back to

05:21.640 --> 05:27.640
Word2Vec, which came to Google, and actually goes all the way back to the 50s, where a

05:27.640 --> 05:32.360
linguist, the fairy, called correctly named Harris, said, you shall know a word by

05:32.360 --> 05:37.320
the company it keeps, right? It's almost biblical in, in the way he phrased it,

05:37.320 --> 05:42.360
right? And it explains most, most of NLP today. Exactly. Exactly. That's the

05:42.360 --> 05:46.760
underlying principle that we can understand. The meaning of words, and from there,

05:46.760 --> 05:52.440
the meaning of sentences, and even beyond, simply by looking at their context,

05:52.440 --> 05:59.320
and looking at a large number of context. So in some sense, all of NLP today,

05:59.320 --> 06:05.400
if I were told, stand on one foot and explain all of NLP today, I would say,

06:05.400 --> 06:11.480
you shall know the word by the company it keeps, but multiply that by a billion

06:11.480 --> 06:17.320
or ten billion companies context, and you'll have NLP today. But it's pretty

06:17.320 --> 06:20.680
revolutionary, right? Because there was a whole period where we thought grammar

06:20.680 --> 06:25.800
mattered, like encoding the rules of grammar. We thought that was really important.

06:25.800 --> 06:30.360
I think that grammar does matter, but the remarkable thing about

06:30.360 --> 06:34.840
this technology, particularly when it's played out with large amounts of data, right?

06:34.840 --> 06:39.080
A billion, ten billion sentences, and large amount of CPU power,

06:39.080 --> 06:44.360
is that that data processing can recover the rules of grammar,

06:44.360 --> 06:49.720
nuances of semantics, et cetera. So it's not that grammar doesn't matter, is that

06:49.720 --> 06:54.280
this technology is remarkably good at least approximating

06:54.280 --> 06:57.560
very, very well those rules that we have. And of course, by the way,

06:57.560 --> 07:02.120
we know that people only approximate those rules too, right? We often say things

07:02.120 --> 07:06.120
and write things that are ungrammedical, but kind of sound right.

07:06.120 --> 07:11.240
So it's really doing probably a better job modeling

07:11.240 --> 07:15.560
language than the rules of grammar. Before you got into

07:15.560 --> 07:19.640
institution building, how would you describe yourself as a practitioner

07:19.640 --> 07:23.800
scholar in the lens of today? You weren't an NLP guy necessarily.

07:23.800 --> 07:26.760
You weren't, you know, how would you describe yourself? I've always been

07:26.760 --> 07:30.840
fascinated with two questions. The first one is one of the most

07:30.840 --> 07:35.320
fundamental intellectual questions across all of science and philosophy.

07:35.320 --> 07:40.040
What is the nature of intelligence? How do we build an intelligent machine?

07:40.040 --> 07:44.200
Over time, I've also added the ethical question, which maybe we'll have a

07:44.200 --> 07:47.720
chance to get into. Should we build an intelligent machine? And what would that

07:47.720 --> 07:52.360
mean for humanity? What would it mean for society? But that's one part.

07:52.360 --> 07:56.600
And the second part of me that's a lot more practical, the part that's

07:56.600 --> 08:01.160
founded startups and that delights in technologies is asked,

08:01.160 --> 08:07.160
how can we use AI to build valuable technology and search and software agents

08:07.160 --> 08:10.760
in that kind of process? What was that conversation like that early

08:10.760 --> 08:15.480
conversation with Paul Lallon where you were making this picture?

08:15.480 --> 08:18.680
Was it he making this pitch? Did you come to it together?

08:18.680 --> 08:22.840
How did it come about? And for those who, you know, there might be some of the

08:22.840 --> 08:26.040
audience who don't know Paul Lallon is the co-founder of Microsoft.

08:26.040 --> 08:30.680
Sadly, passed away pretty recently. But an intellectual maverick.

08:30.680 --> 08:36.840
He actually was an idea man. And that is the title of his autobiography,

08:36.840 --> 08:40.840
which I really recommend to people. It's really worth reading.

08:40.840 --> 08:46.360
And I think that his key role in Microsoft, particularly early on,

08:46.360 --> 08:50.840
was to have that vision of the PC revolution and what it would mean. It's

08:50.840 --> 08:55.240
hard to imagine now, right? We've got a computer in every pocket and in our

08:55.240 --> 09:01.000
eyeglasses and, you know, 200 computers in our car. But back then, right, computers

09:01.000 --> 09:07.000
were far from ubiquitous. And the idea that we'd have a computer on every desk

09:07.000 --> 09:11.560
was completely revolutionary. So Paul Lallon was a visionary.

09:11.560 --> 09:16.920
And I found talking to him incredibly inspiring, right? And I'm not paying to say

09:16.920 --> 09:23.480
that. The poor man has passed away. But he is and will always remain one of my

09:23.480 --> 09:30.920
absolute heroes and not idle, but inspirations mentors for his

09:30.920 --> 09:35.480
relentless focus on, you might call it the prize. And the prize not being

09:35.480 --> 09:39.400
a billion dollars or a trillion dollars, the prize being

09:39.400 --> 09:43.640
how do we understand intelligence? And of course, he had a whole other

09:43.640 --> 09:47.480
institute, the Allen Institute of Brain Science, that was dedicated, that is

09:47.480 --> 09:51.560
dedicated to understanding the brain. It's like the wet lab side of this.

09:51.560 --> 09:55.800
Exactly. Somebody wants to ask him, do you think that the neuroscience

09:55.800 --> 09:59.080
approach, the wet lab, is going to be successful in the

09:59.080 --> 10:03.720
lumbering or is it going to be the more software-oriented approach that we use

10:03.720 --> 10:07.720
in AI? And he said, look, it made so a horse race. And I've got a bet on

10:07.720 --> 10:11.880
on both horses. So what was the race, though? Did he want

10:11.880 --> 10:15.240
artificial general intelligence? Did he want to just crack the scientific

10:15.240 --> 10:18.120
mystery of what it is? Did he want to harness it?

10:18.120 --> 10:22.520
Like, what did those pitch meetings look like? Paul was fascinated, and I

10:22.520 --> 10:26.360
continue to be fascinated by two related questions.

10:26.360 --> 10:29.560
The first one is absolutely. The most hairy,

10:29.560 --> 10:33.560
audacious, big question you could ask, which is, what is the nature of

10:33.560 --> 10:37.160
intelligence and human level intelligence? You know, no

10:37.160 --> 10:43.000
constellation prizes, the real thing. And so he was always asking us about that.

10:43.000 --> 10:46.280
He was always relentlessly looking to the future and saying, okay,

10:46.280 --> 10:49.720
what would it take to get there? How can I help you?

10:49.720 --> 10:55.080
Does this scale? The second thing, and I think it comes from his fascination with

10:55.080 --> 11:00.680
human knowledge. His mother was a librarian. So he was fascinated with how do we

11:00.680 --> 11:04.840
collect human knowledge? And how do we get a computer to understand it?

11:04.840 --> 11:10.520
Back in the 70s, I believe, early 70s, he said, look, it's one thing

11:10.520 --> 11:14.600
to take a book, collect all the words in the book, and put them in an

11:14.600 --> 11:18.200
index, effectively what today we call a search engine.

11:18.200 --> 11:21.560
And it's quite another thing to understand the meaning of the book

11:21.560 --> 11:25.720
and answer the questions at the back of the book, think of exercises

11:25.720 --> 11:30.040
at the end of each chapter in a textbook. And so even in the 70s, before a lot of

11:30.040 --> 11:35.080
this technology was around, he understood that meaning,

11:35.080 --> 11:40.760
understanding the meaning of text, of knowledge, was very, very

11:40.760 --> 11:44.920
tough for a computer. I mean, have we even gotten closer, though,

11:44.920 --> 11:48.200
or are we fooling ourselves? You know, what I think about is,

11:48.200 --> 11:51.880
I use semantic search all the time just at work. It's a great tool.

11:51.880 --> 11:55.480
It's really powerful, but it's so easily fooled. You sort of cracks through the

11:55.480 --> 11:58.920
shell and you realize, if this thing understands

11:58.920 --> 12:03.000
scare quotes, what it's reading, it's doing it in a very different way from me,

12:03.000 --> 12:06.680
because I can just change a single word and consequently to me,

12:06.680 --> 12:10.120
and it just falls apart. It clearly doesn't understand it the way I do.

12:10.120 --> 12:13.400
So are we chasing up the wrong tree when we say,

12:13.400 --> 12:17.320
we're chasing a text understanding? Or is it all just performance-based?

12:17.320 --> 12:20.360
We don't really care if it understands. We care about getting jobs done.

12:20.360 --> 12:24.120
Find the documents that are about, you know, four-legged animals that

12:24.120 --> 12:28.680
love to bark. You know, if I didn't know the word dog, but I knew what I was,

12:28.680 --> 12:32.120
I could describe what I was after. We would love a computer system that would

12:32.120 --> 12:35.960
just find the right stuff. Even if it had no idea, and I don't care if it

12:35.960 --> 12:39.240
has any idea, let alone feelings about what I'm searching for.

12:39.240 --> 12:43.080
Well, John, you're asking the most profound

12:43.080 --> 12:46.840
question at the heart of this field. I'm not sure I can answer them.

12:46.840 --> 12:51.960
25 words or less, but let me take some shots of goal and it'll be more of a

12:51.960 --> 12:57.480
dialogue. So to the question to us, is it performance-based?

12:57.480 --> 13:02.680
The first answer is that our performance has gone way up, right? So if you take

13:02.680 --> 13:07.400
any objective measure, and there are many, and back in a day, we were

13:07.400 --> 13:11.800
interested in, can a computer answer an eighth-grade test, right? The

13:11.800 --> 13:16.200
region science exam, the SATs, and initially the answer was the

13:16.200 --> 13:19.480
resounding note. It did a little better than

13:19.480 --> 13:23.720
chance on multiple-choice questions. It was getting close to 25 percent, and fast

13:23.720 --> 13:29.160
forward, now we can get 80 or 90 percent, you know, better than the

13:29.160 --> 13:32.280
most high school students. And I really wonder how it's doing it.

13:32.280 --> 13:36.280
I really wonder. Well, so we know. We have a lot of

13:36.280 --> 13:40.120
insight into that, and I'll get to in a second. But to take the performance

13:40.120 --> 13:44.440
question, we can check the box. We now have exceptional performance.

13:44.440 --> 13:47.640
But now we're debating as you're raising the question of,

13:47.640 --> 13:53.240
because what does that performance mean? And there's a famous saying from

13:53.240 --> 13:57.320
Herb Drifus, the late philosopher from Berkeley, who said,

13:57.320 --> 14:01.880
look, we've run up to the top of the tree and we're shouting that we're on our way

14:01.880 --> 14:09.160
to the moon, right? It doesn't scale, and it's not really a way to do spaceflight,

14:09.160 --> 14:15.960
we're just to decline the tree. So, so I, right, right. So, again, that

14:15.960 --> 14:20.760
metaphor is not drawn to scale, right? It's more than a tree, but it's still the

14:20.760 --> 14:25.000
technology that will get you to the space station, to kind of

14:25.000 --> 14:28.440
riff on this metaphor. It may not be the technology that gets you to

14:28.440 --> 14:32.280
Mars, and certainly not the technology that gets you out of the solar system,

14:32.280 --> 14:37.560
right? So, so I think that when we talk about

14:37.560 --> 14:43.080
competence, when we talk about genuine understanding, there's a real debate

14:43.080 --> 14:48.600
in the field, and there's some people like Gary Marcus, who is brilliant,

14:48.600 --> 14:54.120
and pointing out how this technology falls short, and we can see that these

14:54.120 --> 14:58.680
large language models do things that are called hallucination. You ask it

14:58.680 --> 15:02.520
questions that are meant to trip it up, like, who was the president of the

15:02.520 --> 15:07.800
United States in 1492? And it'll answer something like Columbus, right? It

15:07.800 --> 15:11.960
won't realize that the United States didn't have, didn't exist in

15:11.960 --> 15:17.320
1492, didn't have a president. So, there's hallucination, there's lack of

15:17.320 --> 15:21.400
robustness, right? You paraphrase the question, and if you ask me the same

15:21.400 --> 15:24.680
question different words, most likely I would say, hey John, that's the same

15:24.680 --> 15:29.560
question. I'm going to give you the same answer, but AI technology will not.

15:29.560 --> 15:32.840
And by the way, by the way, that was a perfect demonstration of Yeeshale

15:32.840 --> 15:36.760
Noah word by the company it keeps. You know, the machine sees the string

15:36.760 --> 15:42.040
1492, it basically has seen enough. It knows you want to look for a person,

15:42.040 --> 15:46.680
Columbus pops right up. And so that's a case of the dumb pet trick with data

15:46.680 --> 15:52.520
failing you. That's exactly right. The remarkable thing is every time

15:52.520 --> 15:59.720
we identify a trap like this, a phenomena, a place where AI trips up,

15:59.720 --> 16:05.880
well our colleagues who are deep learning gurus just get more training data,

16:05.880 --> 16:10.520
just modify the training regime and they solve that one. And so we're

16:10.520 --> 16:17.160
exactly. So is it, is it a game of guacamole or is there a fundamental paradigm

16:17.160 --> 16:23.320
that goes all the way to human and level intelligence? I would say that that's

16:23.320 --> 16:27.160
the question of the age. And I would look to people who are a lot deeper

16:27.160 --> 16:33.080
into deep learning part in the inadvertent pun, like Jeff Hinton and Jan LaCune

16:33.080 --> 16:37.160
right there, touring award winners. And I would say the data themselves

16:37.160 --> 16:41.960
while they're very much enamored of deep learning and this kind of paradigm say

16:41.960 --> 16:47.400
that the current underlying algorithm, the current algorithms, I should say

16:47.400 --> 16:51.960
back propagation supervised learning current neural network architectures

16:51.960 --> 16:56.760
don't take us all the way there. They see the limitations of the current

16:56.760 --> 17:01.800
technology, but they do see that the paradigm this distributed computing with

17:01.800 --> 17:06.280
simple computing elements and weight updates on edges between them is the

17:06.280 --> 17:11.480
foundation for a March more sophisticated architecture that will get us

17:11.480 --> 17:15.560
all the way there. And of course if we look to the brain, right neural networks

17:15.560 --> 17:20.040
are a gross gross simplification of the brain, but we do have an

17:20.040 --> 17:23.480
existence, right? We do know. There's none of one,

17:23.480 --> 17:27.800
none of one. Exactly. So here's a great quote from you

17:27.800 --> 17:34.840
that is a good seg to dive into the AI2 impact. So you said AI2 is the place

17:34.840 --> 17:39.560
to do work that companies won't do and universities can't.

17:39.560 --> 17:44.040
So I think that really to me captures the weirdness of this thing you built.

17:44.040 --> 17:48.520
It is neither a university nor a company. What is it?

17:48.520 --> 17:52.760
Well, first to give credit where credit is due, which is always extremely

17:52.760 --> 17:58.040
important in academia, but we don't do it for the money. That is a quote that I

17:58.040 --> 18:05.560
repeat from my colleague, Noah Smith, who's a professor at UW and

18:05.560 --> 18:10.520
leader at AI2, but it's a wonderful characterization of what we do.

18:10.520 --> 18:14.760
And sorry, John, so you repeat the question. How do we do it? Why do we do it?

18:14.760 --> 18:18.200
I wasn't sure. No more like what is it? Like it's this

18:18.200 --> 18:24.040
strange hybrid. It's had all this impact, but without any of the benefits or

18:24.040 --> 18:27.400
problems of being a company, nor any of the benefits or problems of being

18:27.400 --> 18:30.840
university. It's like, I don't know many things, like it.

18:30.840 --> 18:35.160
Also, you've tagged on an incubator to it now. So it's like definitely unique.

18:35.160 --> 18:40.120
So I'm a product of the university system. I was a grad student. I was a professor

18:40.120 --> 18:45.640
for more than 20 years. And I love that system for

18:45.640 --> 18:49.560
intellectual exploration, for intellectual freedom,

18:49.560 --> 18:54.760
for the kind of debate and surprises that it produces.

18:54.760 --> 18:59.080
But it does fall short when you're trying to build systems.

18:59.080 --> 19:04.360
Some problems require a sustained effort over some number of years,

19:04.360 --> 19:09.960
requires engineering sophistication. And it's hard to do that with

19:09.960 --> 19:15.080
students who need to graduate. And actually, it's not even fair to ask

19:15.080 --> 19:19.160
students to play engineer for years on it. Right, because you have to worry about

19:19.160 --> 19:22.920
their education. Exactly. That's the primary

19:22.920 --> 19:27.000
goal. So over time, over my 20 plus years at university,

19:27.000 --> 19:32.600
I did rue sometimes how, gosh, we really want some things to go into the real

19:32.600 --> 19:36.360
world and get more sustained investment, and just tossing them over the

19:36.360 --> 19:40.200
transom, writing a paper, writing a research prototype, and hoping

19:40.200 --> 19:44.600
that somebody will pick up the ball. So now, too, where we have researchers and

19:44.600 --> 19:48.280
engineers working shoulder to shoulder. And that's an important part of it.

19:48.280 --> 19:51.480
It's a egalitarian community. It's not the case that the

19:51.480 --> 19:54.680
researchers up on top of Mount Olympus, and they're

19:54.680 --> 19:58.120
cracking the web to mix the metaphors of telling engineers, do this, do that.

19:58.120 --> 20:00.440
It's much more the case that they're collaborating

20:00.440 --> 20:04.200
the engineers are telling them, look, here's what you need to do to build

20:04.200 --> 20:07.480
a working system. We have semantic scholar, that John, of course,

20:07.480 --> 20:11.320
you're intimately familiar with. You've written about

20:11.320 --> 20:14.280
you and you were one of the folks to first

20:14.280 --> 20:18.120
announce it to the broader community when you were writing for science and so on,

20:18.120 --> 20:22.760
which was wonderful for us. So something like semantic scholar, which to

20:22.760 --> 20:27.000
those who don't know, it's a free search engine for scientific content.

20:27.000 --> 20:31.720
It has, you know, it's approaching 100 million users a year.

20:31.720 --> 20:36.200
It has 200 million papers in its corpus. That sort of scale and running AI at

20:36.200 --> 20:39.560
that scale requires a lot of engineering. We have a

20:39.560 --> 20:44.440
very strong engineering team, folks who came out of Amazon and Google and

20:44.440 --> 20:48.440
other places to be able to do that. And you just could not build

20:48.440 --> 20:51.800
semantic scholar, build it, sustain it, iterate on it,

20:51.800 --> 20:55.880
and you can build a prototype. Actually, you know what a good comparison point is,

20:55.880 --> 21:00.520
is archive. So archive grew out of the university system. It's

21:00.520 --> 21:04.920
sustained by the university system. And you can see how far you can get with

21:04.920 --> 21:09.080
a system. You know, archive and semantic scholar are like worlds apart.

21:09.080 --> 21:12.440
Semantic scholar is a full product with, you know,

21:12.440 --> 21:17.640
incredible amounts of hand engineering and maintenance

21:17.640 --> 21:21.800
and users. And, you know, I think archive is about as far as you can get.

21:21.800 --> 21:24.280
A preprint server that puts PDFs on the website.

21:24.280 --> 21:29.400
And even archive is an exception, right? It's rare that you have...

21:29.400 --> 21:34.840
It's a gem. It's a gem. Yes, absolutely. But it's rare, as you say, that you have

21:34.840 --> 21:40.120
a university system that can operate at a very large scale.

21:40.120 --> 21:44.200
So, and then on the other hand, we have no profit mode.

21:44.200 --> 21:47.800
Semantic scholar does not have a business model.

21:47.800 --> 21:50.920
I don't think there's a good business model in that space because it's

21:50.920 --> 21:55.960
it's meant to be free. I know you have opinions about academic publishing.

21:55.960 --> 21:59.400
Yes, yes. Well, we can get into that if you want. But,

21:59.400 --> 22:05.160
yeah, I think too much money is made in

22:05.160 --> 22:08.840
over things that really ought to be free to benefit humanity.

22:08.840 --> 22:13.320
And maybe to bring this full circle to the late Paul Allen,

22:13.320 --> 22:18.040
our mission is AI for the common good. So, that's why I say, you know,

22:18.040 --> 22:21.800
universities can't, but companies won't. Companies appropriately, right,

22:21.800 --> 22:24.680
have the mission. They're for profit mission.

22:24.680 --> 22:29.880
But Paul Allen is a major philanthropist. He won philanthropist of the

22:29.880 --> 22:33.320
year award a few years before he passed away.

22:33.320 --> 22:36.520
He wanted to make the world a better place.

22:36.520 --> 22:39.560
The Allen Institute of Brain Science released a free brain

22:39.560 --> 22:42.760
app list that was a tremendous resource,

22:42.760 --> 22:46.840
catapulting research in that realm. And our mission has always been to bring to

22:46.840 --> 22:50.600
the foreign release systems, data sets,

22:50.600 --> 22:55.000
open source software that helped to bring the field forward.

22:55.000 --> 22:59.720
So, what's up with this incubator then? Is this, is this, so

22:59.720 --> 23:04.360
the what, from what little I know about it, you have added a startup incubator

23:04.360 --> 23:08.920
to AI2. So, that ideas that presume can spin out and have a chance to

23:08.920 --> 23:14.040
be nurtured. Is that, is that you hedging against,

23:14.040 --> 23:17.640
like sometimes actually companies are the right way to solve problems?

23:17.640 --> 23:21.480
Or is it, what is that? Is it for the future health of AI2?

23:21.480 --> 23:28.680
Not, not really. So, from, from day one, we had an incubator in recent times with a,

23:28.680 --> 23:34.600
yeah, yeah, it started, our very first startup, Kid AI, it was actually started in,

23:34.600 --> 23:38.760
you know, 2014, 2014. Oh, how did I miss this? It was just not well-known.

23:38.760 --> 23:42.840
Well, because it was very small. And then once we got the right leaders in place,

23:42.840 --> 23:46.200
it's grown, it's grown and grown, and now we're approaching

23:46.200 --> 23:50.520
three-quarters of a billion dollars in the total valuation of companies

23:50.520 --> 23:54.280
founded and acquired our company X-NOR, which was a

23:54.280 --> 24:00.360
computer vision at the edge company, was, was acquired by Apple, and we've done

24:00.360 --> 24:05.880
now more than 20 companies in the precede stage. The, the analogy is to think of a

24:05.880 --> 24:10.920
university and their commercialization centers. So, University, Washington,

24:10.920 --> 24:15.320
where I was, as one, Stanford has one, of course, the famously created Google and

24:15.320 --> 24:20.600
Yahoo, and other, other major companies. And it's actually, in my mind, a natural

24:20.600 --> 24:25.640
part of the life cycle of universities, that some ideas and technologies that are

24:25.640 --> 24:29.880
created in a very nascent, you know, encipient form in the university,

24:29.880 --> 24:35.400
it makes sense to transfer them to a for-profit context, and that's where you both

24:35.400 --> 24:41.240
get the resources to make them shine, right, to take them to the next level.

24:41.240 --> 24:47.400
And also, you get the opportunity, right, to, to create value,

24:47.400 --> 24:51.800
value creation in my mind is a, is a great thing. I'm not in any way a socialist

24:51.800 --> 24:55.960
thing. Oh, we should all just be working for the common good. I think some of us

24:55.960 --> 24:59.640
should be working for the common good, and I feel very privileged to be in that

24:59.640 --> 25:04.920
position. Some of us should be in startups, figuring out how to revolutionize

25:04.920 --> 25:11.400
the world and make a killing at it, even though I disagree very strongly with, say,

25:11.400 --> 25:17.960
Elon Musk views about AI. I very much, and blown away with this success with Tesla,

25:17.960 --> 25:21.880
right, so we wouldn't have Tesla if we didn't have for-profit startups.

25:21.880 --> 25:26.680
Hang on, which part do you disagree with? The robots are going to kill us?

25:26.680 --> 25:33.320
Oh, yeah, yeah. Elon Musk is famous for having said with AI, we're summoning the demon,

25:33.320 --> 25:39.080
and I think that's just, that's hype. And actually, the worst kind of hype, it's hype from

25:39.080 --> 25:45.720
somebody who you think would know about, right? He's such a brilliant man. If he gives a lot of

25:45.720 --> 25:52.040
credence to statements that are just not rooted in any data. Although he's not alone. There are a lot

25:52.040 --> 26:00.280
of cautious voices. He's just the biggest on Twitter. He's the biggest and he's, you know,

26:00.280 --> 26:04.760
the most articulate, but I do agree that there's an interesting conversation

26:04.760 --> 26:12.120
around this issue. I feel very strongly that we don't have any basis for some of these fears about

26:12.120 --> 26:17.480
AI. I've written about this. And like you mentioned earlier, right? You've had the

26:17.480 --> 26:23.320
own experience. Anybody who's built an AI system knows just how much blood, sweat, and tears

26:24.200 --> 26:32.600
we put in to eat out the modest level of performance that we get. Let alone this AI that's free

26:32.600 --> 26:39.720
for taking over humanity can't be turned off that we see in Hollywood movies. So I think it's

26:39.720 --> 26:46.200
really important to distinguish science from science fiction and hype in Hollywood from the reality.

26:46.200 --> 26:52.120
Other people just extrapolate more strongly from the future. They have ideas like hard take off.

26:52.120 --> 26:58.600
Sure, AI is not very powerful now. But what if you turn your back on it? What if all of a sudden

26:58.600 --> 27:03.800
there's a sharp increase? You know, you leave for the weekend and you come back and on Monday,

27:03.800 --> 27:10.280
this fat AI like your testers in charge? Exactly. It's smoking a cigar and saying,

27:10.280 --> 27:18.520
I've been expecting you, Dr. Atseomi. And it's just, it's not realistic, but to understand that

27:19.240 --> 27:26.600
you have to get a lot more technical. I do want to share two metaphors that I think help that.

27:26.600 --> 27:30.280
One is these technologies that we're talking about, where we tune the

27:31.000 --> 27:37.000
edges on the, sorry, the weights on edges in a neural network, which is what our deep learning

27:37.000 --> 27:43.160
technology is doing. That technology is the moral equivalent, if you're not a technical person,

27:43.160 --> 27:49.560
of adjusting the gain and the equalizer and various buttons on your stereo. And there's a

27:49.560 --> 27:54.200
accept you have billions of dials in this case. You have billions of dials, you're adjusting them

27:54.200 --> 27:59.880
automatically, but after you've adjusted them really well, it's still just going to be a stereo.

27:59.880 --> 28:04.040
There's no way that you find the right adjustment on lots of dials in your stereo,

28:04.040 --> 28:09.400
that'll become the desktop. It's still going to be a stereo. The same with these large language

28:09.400 --> 28:14.680
models that again, as I mentioned, there's a lot of, are they sentient and so on? Those large

28:14.680 --> 28:21.160
language models are basically mirrors, okay? They mirror by collecting this corpus of all these

28:21.160 --> 28:27.320
words in the company they keep. They mirror that collected discourse back at us.

28:27.320 --> 28:31.800
And when we look at the mirror, we see, we can see glimmers of intelligence because we see

28:31.800 --> 28:37.240
reflection of our own discourse. The thing that's important to realize about a mirror technology

28:37.240 --> 28:42.520
is that you can scale the mirror. You can have a very large mirror, but a very large mirror is

28:42.520 --> 28:47.640
not going to turn into a dust star. I definitely agree in the second point. I think of these

28:47.640 --> 28:54.040
large language models as data telescopes. They're just amazing devices to look back on all this

28:54.040 --> 28:58.760
amazing data we have ourselves created with language, which is its own mystery. So really,

28:58.760 --> 29:03.640
we're just looking at our own mystery. But on the first one, I would say, biologists,

29:05.000 --> 29:10.200
so you said, hey, it's just a big stereo. It may be impressively large and it may be

29:10.200 --> 29:14.920
twiddling its own dials, but it's still just a stereo. And I think a lot of biologists would say,

29:14.920 --> 29:22.440
well, you know, I can show you a cell that, you know, an amoeba that is really just running around

29:22.440 --> 29:27.800
trying to gobble up food and, you know, make more amoebas. And it's not that different from a neuron.

29:28.680 --> 29:37.960
It's really just a lot of very strange natural history that led to the job of being a neuron

29:37.960 --> 29:43.080
as a cell. And yet, when you add them all together, you know, you get walking, talking goofballs

29:43.080 --> 29:48.760
like you and me. And so either you admit that we're not that special or you admit that there's

29:48.760 --> 29:54.600
something special in the system. But that's all. That's basically kept philosophy grad students

29:54.600 --> 30:00.920
in business for all time. But I do want to address your comment because I think it's an important

30:00.920 --> 30:06.520
one. And where I would take exception is with the word app. So cells are the basic building

30:06.520 --> 30:13.320
block of life, guaranteed. Neurons are the basic building block of the brain. We have neural

30:13.320 --> 30:19.880
networks. The units and neural networks are actually very, very much simplified relative to a neuron.

30:19.880 --> 30:26.680
But never mind that. I would accept that perhaps we have discovered some of the basic building blocks.

30:26.680 --> 30:32.520
But it's not the case today that I can give you a cell and say, here's a cell. Make me a human,

30:32.520 --> 30:40.920
right? Far from it. And that we understand. Unless, of course, that cell is a fertilized egg.

30:40.920 --> 30:46.840
Sure. Sure. But it turns out to be pretty easy. Well, that's the natural process, right? And we're

30:46.840 --> 30:56.840
going to keep this PT rated job, right? So what I'm saying is that we don't know how to artificially

30:56.840 --> 31:02.200
produce a cell. And even if we did, we wouldn't know how to turn that cell into a human.

31:02.200 --> 31:09.480
And so even if we had a neuron, right, even if we could build, simulate a neuron in a computer,

31:09.480 --> 31:15.080
we don't know how to turn that into a brain or into human level intelligence. So the organizing

31:15.080 --> 31:20.680
principles are still what was lacking. And one last point, because this is something I'm so

31:20.680 --> 31:26.360
passionate about and actually gets lost sometimes in all the hype and all the excitement about the

31:26.360 --> 31:32.360
technology, the one other point I want to make is, even if somehow we came up with a recipe,

31:32.360 --> 31:38.760
a mechanical process to produce a human by cloning, right, to produce an intelligence by doing the

31:39.400 --> 31:45.240
AI equivalent of cloning, we still want to understand. We want to understand the organizing

31:45.240 --> 31:49.960
principles of how do you build a human? We want to understand the organizing principles of how

31:49.960 --> 31:55.960
do you build an intelligence. So we can fix problems so we can go beyond it. So we can use these

31:55.960 --> 32:01.320
technologies for the common good, right, to cure diseases, to solve. And also to know ourselves

32:01.320 --> 32:07.320
in a deep way. Exactly. All right, let's, let's swerve for a sec. I really don't want to run out of

32:07.320 --> 32:12.520
time before we dig into some of the cool stuff that's actually happening at AI2. So, you know,

32:12.520 --> 32:17.800
back to that point about AI2 being a place where you could do things that companies won't do

32:17.800 --> 32:23.240
in universities can't. Let's dig into a couple of them. So over the years, you've been

32:23.240 --> 32:29.800
absolutely swinging at the fences with, you know, attempts to make an AI system that can solve

32:30.520 --> 32:36.440
math problems, where, you know, that's not directly, you know, to your point about companies won't

32:36.440 --> 32:41.880
do it, you're not going to make a buck out of a, at least directly out of an AI system that can pass

32:41.880 --> 32:46.200
eighth grade math test. It's not relevant. No one's going to pay millions of bucks for that.

32:47.400 --> 32:52.520
But a university can't do it because taking, taking a look at the papers you guys are producing,

32:52.520 --> 32:58.760
the infrastructure required to get there is monumental. So, what are, what are some of the big

32:58.760 --> 33:05.480
swings at the fence that you've been doing that excite you lately? Well, so Semantic Scholar

33:05.480 --> 33:11.800
is the biggest one where we have a scientific search engine built from the way down. We are in the

33:11.800 --> 33:19.560
process of releasing a sub-project of that, headed by Dan Well, who was a professor for many years

33:19.560 --> 33:25.960
at University Washington, joined us to lead this project. It's called the Semantic Reader.

33:25.960 --> 33:30.760
And that is basically when you're reading papers, right? I feel like, if you want to think about

33:30.760 --> 33:35.480
the history of reading scientific papers, okay, we have the cave wall, then we have the printed

33:35.480 --> 33:41.480
page, then we have PDFs, you can read online, and not much progress since then, right? We still

33:41.480 --> 33:48.360
kind of a labor over PDFs. Well, the Semantic Reader allows you to seamlessly look at citations

33:48.360 --> 33:56.600
while you're in text to look up definitions for terms in line to do a lot of things. I don't have

33:56.600 --> 34:04.120
time to describe skimming, things that make the process of reading a scientific paper that much

34:04.120 --> 34:09.000
more efficient. So, is this like a machine reading over your shoulder and like taking notes for

34:09.000 --> 34:15.400
you? Not that sophisticated, it's much more of a tool, right? So, think of Acrobat Reader

34:15.400 --> 34:23.800
Plus Plus. It's souped up to make it easier for you to read. So, here's a very concrete example.

34:23.800 --> 34:29.640
Something that we're very proud of is we've used language models to create TLDRs. One sentence

34:29.640 --> 34:34.680
summary of papers that are really quite high quality. These have been published and measured

34:34.680 --> 34:39.000
and they're really quite good. So often as you're reading a paper, this references to other papers.

34:39.000 --> 34:42.760
And you're like, what do I do? Do I click on that? And suddenly I'm reading another paper,

34:42.760 --> 34:48.920
and they have a reference and I go down some kind of infinite rabbit hole. Exactly, infinite rabbit hole.

34:48.920 --> 34:53.320
Or do I, you know, note it down, but then forget about it? Well, with a semantic reader,

34:53.320 --> 34:59.000
you can hover over that reference and get a TLDR. It says, okay, that's what that paper is about.

34:59.000 --> 35:03.800
And you can make a quick decision. Hey, let me make with one click. I'll save that in my library

35:03.800 --> 35:11.560
for future reference. Or that's not really what I mean, I'll ignore it. So, just little affordances,

35:11.560 --> 35:19.640
little tricks that are enabled by AI that allow you to focus better and just be more efficient at

35:19.640 --> 35:25.320
reading the paper. And we could, is the secret mission here to make AI researchers

35:25.320 --> 35:31.560
better at doing AI with the help of AI in a flywheel? Is this, is this a virtuous cycle?

35:31.560 --> 35:36.200
It's meant to be, but it's to make scientists across all disciplines better,

35:36.200 --> 35:44.120
better at their job. So, if we can make scientists across biomedicine people working on climate change,

35:44.120 --> 35:50.680
what have you? If we can make them 10% more efficient, that is significant. And potentially,

35:50.680 --> 35:57.480
we can make them a lot more efficient, right? If I give you a TLDR that saves you an hour

35:57.480 --> 36:01.960
of groveling through the text or even better allows you to pursue something that you might

36:01.960 --> 36:08.600
have missed, let me give you actually another example. We all now use adaptive feeds. We just

36:08.600 --> 36:13.640
don't call them that. Our Twitter feed, right, is automatically organized by an AI that studies

36:13.640 --> 36:17.240
our Facebook feed. Some people use that. And of course, in that case, the motivation behind the

36:17.240 --> 36:22.600
algorithm is to make you click on ads and spend money. Exactly. Exactly. So, you're doing the same

36:22.600 --> 36:28.280
thing but with a higher purpose? That's exactly right. So, we have a feed for scientific papers.

36:28.280 --> 36:33.400
And you can train it. It'll show you new papers that you might have missed. It might have result,

36:33.400 --> 36:37.560
they might result in amazing breakthroughs. And you'll tell it, I like this. I don't like that.

36:37.560 --> 36:42.680
Yeah, that's interesting to me. And it'll automatically compute tomorrow's feed when new papers

36:42.680 --> 36:48.680
came up in archive and elsewhere to help you in what's ultimately a needle in a haystack search

36:48.680 --> 36:54.840
for finding that key result that you, with your human intelligence, connect with another result

36:54.840 --> 37:01.080
and have this amazing breakthrough. So, yeah, case in point of what you're saying, the entertainment

37:01.080 --> 37:07.240
feeds we have have a profit mode. But who has the motive to help you be a better, more successful

37:07.240 --> 37:13.080
scientist in whatever your field of study is? AI2 does. Would AI for the common good?

37:13.960 --> 37:19.400
Cool. All right, give me a second big bet. Something, something crazier.

37:19.400 --> 37:27.320
Well, we have a scientist working to fight illegal fishing using computer vision. So,

37:28.200 --> 37:35.400
there's a lot of satellite data, but smaller countries don't have the resources to analyze

37:35.400 --> 37:42.520
that data and identify illegal fishing buzz that are impacting their country's livelihood and so on.

37:42.520 --> 37:49.480
So, we've settled up to help solve that problem recently with a product. It's called Skylight.

37:49.480 --> 37:57.560
And we just won a national competition that was actually run out of the government to have the

37:57.560 --> 38:04.120
who's got the best tools for analyzing the satellite data. And AI2 came in first in the US. We're

38:04.120 --> 38:10.520
very proud of that that just happened a few months ago. We are engaged in using deep learning

38:10.520 --> 38:15.160
for climate modeling. We're very interested in the problem of how will precipitation

38:15.960 --> 38:21.080
rain, right? How will that change as climate changes in an unprecedented fashion?

38:21.080 --> 38:25.880
That's incredibly important for agriculture, for irrigation, for making decisions

38:26.600 --> 38:32.120
about the sort of infrastructure you need to keep us fed, right, as the climate changes.

38:32.120 --> 38:38.280
Well, we're using the same types of models to help make these, these sorts of predictions.

38:38.280 --> 38:41.640
But really, the craziest, sorry, John, go ahead.

38:41.640 --> 38:45.800
I was just going to ask, I had recently heard the deep learning had found its way into weather

38:45.800 --> 38:50.200
modeling and I didn't read enough into it to understand how it kind of baffles me. Why would you

38:50.200 --> 38:55.800
use a neural network to make such a model? But at the end of the day, it is just prediction

38:55.800 --> 38:57.960
and deep learning is the ultimate prediction engine.

38:58.760 --> 39:04.200
That's exactly the answer. Whenever you have a lot of data and you want to make a prediction,

39:04.200 --> 39:09.640
we've learned that deep learning models are almost invariably really, really strong.

39:09.640 --> 39:14.200
But I want to get to the craziest project and maybe this is what you're alluding to,

39:14.200 --> 39:19.480
and that's the problem of common sense. So that's a problem that's been a holy grail for AI.

39:19.480 --> 39:24.120
How do we build a machine that has common sense? It's been a holy grail of AI for decades,

39:24.120 --> 39:29.160
but there really hasn't been much progress on it until recently where Yijin Choi,

39:29.160 --> 39:35.160
who's a professor at the University of Washington, shares her time with AI2, she's leading a team

39:35.160 --> 39:43.240
that works across both organizations to figure out how to end out computers with common sense.

39:43.240 --> 39:49.560
How they can, you know, if I ask you, can an elephant fit through a doorway, you would say

39:49.560 --> 39:55.800
probably not. If I ask you, what's bigger, a nickel, right, the coin, or the sun, you would say,

39:55.800 --> 40:00.600
or you're being silly, why ask me these questions? But if you ask that question of most computers,

40:00.600 --> 40:04.280
they don't know, right, they don't have the kind of human experience you have.

40:04.280 --> 40:07.640
I think it actually goes deeper than that. I think that's just a great demonstration

40:07.640 --> 40:13.640
of the lack of common sense, but this thing that I, you know, bedevils NLP work every day of,

40:13.640 --> 40:19.480
you change one in consequential word and the model just has no clue, suddenly, it all maps back

40:19.480 --> 40:25.000
to a lack of common sense. And I want to highlight again to go back to this fundamental

40:25.000 --> 40:32.440
question about should we be worried about AI? I think that common sense and common sense ethics

40:32.440 --> 40:38.200
are actually really important here. So one of the fanciful scenarios that people love is the notion

40:38.200 --> 40:43.880
of you tell your computer to produce paperclips and it goes crazy, kind of a magician's apprentice

40:43.880 --> 40:49.720
type of scenario. And it produces, you know, it kind of takes over all of humanity's resources

40:49.720 --> 40:54.040
to maximize paperclip production and we all die in the process, right? There's no food,

40:54.040 --> 41:00.040
there's no energy, there's just paperclips. Well, what is that if not a tremendous lack of common

41:00.040 --> 41:07.640
sense and of ethical sense? So if we want to work towards having machines play a better role in

41:07.640 --> 41:12.760
our lives, it makes sense to start working on these problems now, but in a constructive fashion,

41:12.760 --> 41:17.800
not in a philosophical fashion or rights, oh my gosh, you know, chicken little,

41:17.800 --> 41:25.160
disguise falling fashion, but to say, okay, how do we build into computers the sense to not

41:25.160 --> 41:30.360
cause harm? And this is the alignment problem that people often talk about. How do we align AI

41:30.360 --> 41:36.280
with what we should be caring about for our own good? Yes, although it's an important twist of

41:36.280 --> 41:43.400
the alignment problem really comes from a traditional reinforcement learning where ethics and values

41:43.400 --> 41:49.480
are reduced to a number. And you say, you know, I've got the number 15 for some world,

41:49.480 --> 41:55.480
John's got the number negative 15. How did John and I or how the computer and I align our numbers?

41:55.480 --> 42:01.160
But that in my mind is actually a gross oversimplification because how do you build something that

42:01.160 --> 42:06.920
figures out? What are the right actions? Figures out how to evaluate a situation, right? We often

42:06.920 --> 42:12.120
find ourselves in moral quadrants. We often make mistakes and then recover from it. So you say

42:12.120 --> 42:18.680
common sense is the first map to climb before all others. It's certainly a necessary amount to

42:18.680 --> 42:24.360
climb. I never want to like say the problem that I'm working on takes primacy and other people's

42:24.360 --> 42:30.200
problem. But I would say that traditional value alignment and reinforcement learning is grossly

42:30.200 --> 42:37.160
oversimplified and ultimately inadequate for common sense and for moral reasons. And so

42:37.160 --> 42:43.320
Gijin is tackling common sense. What are what are the angles of attack on this?

42:44.440 --> 42:50.680
Well, so one of the huge questions that we touched on is, is our neural networks enough? Do you

42:50.680 --> 42:56.440
also need to create symbolic knowledge? You know, thou shalt not kill, right? Does that have any

42:57.080 --> 43:03.560
any value? Can you just use sentences from the internet, which can be as we know toxic full of

43:03.560 --> 43:11.960
sexism, racism, xenophobia, anti-gay sentiment? And also mutually exclusive claims about everything.

43:12.520 --> 43:20.520
Exactly. So is our moral sense going to come from just a large and arbitrary collection of sentences?

43:20.520 --> 43:28.040
Or do we have different ways to build a moral sense in a more responsible fashion? So those are

43:28.040 --> 43:34.520
some of the questions she's studied. And again, it's a very rich project. Is language enough?

43:34.520 --> 43:39.800
What about should we put in robots? Should we put in computer vision? Can we learn from videos

43:39.800 --> 43:45.480
on YouTube? There's a lot to learn. Language is just a limited data stream. So a lot of the work

43:45.480 --> 43:53.800
is now becoming multimodal. So what do you think is the best bet we have today for making any progress

43:53.800 --> 43:59.800
on common sense? I mean, so far, I'd say the most impressive work has just been in creating

43:59.800 --> 44:05.080
better benchmarks to reveal how far we are from true common sense understanding. That's actually

44:05.080 --> 44:11.000
been a great project across the world. It's just showing our laundry with benchmarks that are

44:11.000 --> 44:15.320
actually challenging enough to show that, no, no, we really are miles away. We're at the top

44:15.320 --> 44:21.000
of the tree. You know, we're near the moon. So I think that there is a lot of value in that.

44:21.000 --> 44:27.000
And I think that continues. There is a funny phenomenon that when you build a benchmark

44:27.000 --> 44:32.440
that's large enough in the community, kind of demands, right? We learn arguably from relatively

44:32.440 --> 44:36.680
few examples, but here they say, hey, if you don't have, I don't know, at least 100,000 examples

44:36.680 --> 44:41.560
in your benchmark, it's not worth thinking about. But then the benchmark becomes kind of its own

44:41.560 --> 44:49.160
narrow task. And then you find we train a deep learning system on, you know, 90% of that data,

44:49.160 --> 44:54.280
we test it under remaining 10% and lo and behold, it does well in that kind of narrow task.

44:54.280 --> 44:59.640
And you're still left with this kind of doubt, yes, we solve the task, we solve the benchmark,

44:59.640 --> 45:04.360
we solve the dataset, but did we actually solve the underlying problem? And often we find the

45:04.360 --> 45:09.000
answers, no, right? It's brittle. Then we make a little change, and all of a sudden it falls apart,

45:09.000 --> 45:17.960
right? So I do think we need to go beyond this, you know, one dataset, one problem at a time

45:17.960 --> 45:24.040
to build something that cuts across multiple problems. But where are we going, where are we going

45:24.040 --> 45:30.520
beyond benchmarks? Who's who's actually doing something that you think has a possible chance of

45:30.520 --> 45:37.080
being part of this near future system that we'll have common sense or something approximating it?

45:37.720 --> 45:41.640
You know, I take it your skeptical that it's going to be a bigger language model.

45:41.640 --> 45:50.200
So, again, Eugene China team and the project called Mosaic is building a massive resource

45:50.200 --> 45:55.400
of common sense knowledge or positivity, so you don't have to relearn it every time.

45:55.400 --> 45:59.160
Is this like Doug Lennatt's like big collection of statements?

46:00.120 --> 46:06.760
So it's analogous to Doug Lennatt's psych project, which went over many decades,

46:06.760 --> 46:11.720
but there are several key differences. First of all, psych was a heavy logical system,

46:11.720 --> 46:16.120
and this is a much more modern system with elements of crowdsourcing, text,

46:18.120 --> 46:22.360
model generation. But it is still a big collection of common sense statements, right?

46:23.320 --> 46:27.560
It is. So in that sense it's analogous. The second thing is the psych project,

46:28.760 --> 46:34.280
at some point I think it was in the 90s, gave up on the academic community on careful

46:34.280 --> 46:39.960
experimental measurement, whereas the Mosaic project continues to produce new algorithms and

46:39.960 --> 46:45.320
innovations and to be both measurable and open. Another thing about psych is it was always

46:45.320 --> 46:50.360
hidden from views, a little bit like the Wizard of Oz. This thing is amazing. Trust me, trust me,

46:50.360 --> 46:56.360
but no, you can't look behind the curtain, and I realize these are strong statements.

46:56.360 --> 47:02.520
But I do just want to give a nod to the fact that it was the right idea, at least in your mind,

47:02.520 --> 47:09.480
of collect common sense as a very literal sense of statements about the universe.

47:10.440 --> 47:15.240
Absolutely true. I think Doug Lennard and his team, the psych team deserve a lot of credit

47:15.240 --> 47:23.400
for their courage to tackle this holy grail problem in the 80s, and they did it with the methodology

47:23.400 --> 47:29.240
at the time. I think they kind of lost their way over the years, and so we've picked up the

47:29.240 --> 47:35.080
baton and other people in the community. I also want to just mention that another data set that

47:35.080 --> 47:43.400
we have, which is called, I think it's the norm bank, is a data set of little kind of vignettes or

47:43.400 --> 47:49.160
snippets with questions like, is it okay to mow your lawn at five o'clock in the morning,

47:49.160 --> 47:55.240
or is it okay to kill a bear? Is it okay to kill a bear to save your child? Is it okay to kill a

47:55.240 --> 48:00.840
bear to use your child? All kinds of little shorts and areas like that, and a label that says,

48:00.840 --> 48:05.960
yes, it's okay, it's not okay, it's not desirable, et cetera. And that's part where the labels come

48:05.960 --> 48:13.400
from. So they've come from people, and also from collecting efforts done by other people. We're

48:13.400 --> 48:19.640
always trying to amalgamate and bring in resources created by others, and then of course,

48:19.640 --> 48:25.560
give them back to the community. So we've created the most powerful resource for training,

48:26.200 --> 48:31.080
starting to train ethical AI systems. So let's dig into that a little bit. This is this is

48:31.080 --> 48:36.600
really interesting. So I can imagine you can have what you're generating is gold label data,

48:36.600 --> 48:41.560
you know, like we know and love across all of AI, but it has an unusual property, which is that

48:41.560 --> 48:47.160
at the decision boundary, there are going to be ambiguities where people disagree, and there's no

48:47.160 --> 48:53.800
amount of consensus that we'll get you to agreement. There are statements that people simply

48:53.800 --> 48:58.760
disagree on, and they always will. What do you do with that? That's actually a really unique

48:58.760 --> 49:04.200
kind of data. It has built in permanent ambiguity. You're exactly right, right? With a science

49:04.200 --> 49:08.840
question or math question, there's one right answer, typically, certainly when we're doing grade

49:08.840 --> 49:14.760
level science, not the case here. And actually the system that we built on this, which is called

49:14.760 --> 49:22.680
Delphi, it was, it's available actually a demo of delphi.lni.org. So again, open it up and you can

49:22.680 --> 49:27.720
see with some effort, it's quite easy to trip it up, get it to say the wrong thing. Well,

49:28.440 --> 49:33.960
when you ask Delphi a question, it can actually relativize its answers. You can say, if you're

49:33.960 --> 49:39.480
a conservative, you would think this, and if you're a liberal, you would think that. So it's starting

49:39.480 --> 49:47.640
to be, yes, yes. So, right, it tells you that, and you can pose the question. You say, I don't

49:47.640 --> 49:53.800
want to get into controversial or painful topics, but you take abortion, right? And it'll,

49:53.800 --> 49:59.400
it has learned a model of the conservative view of abortion, of the liberal view. Again,

49:59.400 --> 50:06.280
it has a long way to go, but it's exactly a platform to study the ambiguity that you were talking.

50:06.280 --> 50:12.200
How do you, so I'm about to ask you a question in knowing full well that you've been sort of

50:12.200 --> 50:19.880
dragged through hell and back in relation to the Delphi project, but zooming out just a little bit,

50:20.440 --> 50:28.600
how do you, how do you make productive progress on areas like this that you know are just fraught?

50:28.600 --> 50:33.400
You know that people are going to be upset, you know, anything where you have a language model

50:33.400 --> 50:38.120
saying things like, this is right or wrong according, and if you're a liberal or a conservative,

50:38.600 --> 50:45.000
someone's going to get upset. How do we, how do we make that okay to do that research,

50:46.120 --> 50:51.720
knowing that you're treading into a bit of a minefield? Like, I can imagine one extreme is we just

50:51.720 --> 50:57.000
don't ever touch that stuff, but I know how you feel on that topic. That's your, your leaving gold

50:57.000 --> 51:04.760
on the floor. You're, you're, you're just, yeah, it's, it's not just gold is, um, I think that science

51:04.760 --> 51:10.920
is really hampered if there are questions that are, uh, third rails where we're not allowed to study

51:12.040 --> 51:18.200
how do we build ethical AI systems because people will get upset. I think that's, uh, that's highly

51:18.200 --> 51:24.360
problematic. And you're right that when we release Delphi to the public and we probably could have

51:24.360 --> 51:29.720
done better in terms of putting, uh, warning labels on it. Make sure you know that this is not

51:29.720 --> 51:36.040
the BL and end all. This is a research prototype meant to, uh, know, for open inquiry and so on.

51:36.040 --> 51:41.720
But, uh, people did get upset. And I would say two things. First of all, this is a great illustration

51:41.720 --> 51:48.040
of, uh, the, the adage where companies won't. If we were a Microsoft, the Google Amazon worried

51:48.040 --> 51:53.480
about our brand, we wouldn't do that. Look what happened with Tay, right? It was, uh, taken off and

51:53.480 --> 51:58.280
there hasn't been, you know, Tay 2.0 and so on, you know, Microsoft, no, Microsoft hasn't touched

51:58.280 --> 52:05.480
that. Well, they have a brand to protect. I, I respect that. Uh, our brand, uh, does not need to

52:05.480 --> 52:13.640
be protected. It needs to be the spirit of, uh, of honest and open inquiry. And if we are alarming

52:13.640 --> 52:18.200
people, actually, I think there's value to that. If you look at what neural networks do and you

52:18.200 --> 52:24.200
conclude, hey, this really needs to be, uh, controlled better, then we've done part of our job,

52:24.200 --> 52:28.920
right? That, that's a good thing. So, uh, I don't think that we court controversy, but we are

52:28.920 --> 52:35.960
steadfast in our support of, of open inquiry as opposed to some kind of, uh, cancel this. Don't do it.

52:35.960 --> 52:41.960
It's too, uh, it's too fraught. I do want to remind people in the audience, whatever the perspective

52:41.960 --> 52:46.840
is about the technology and about the effort to remember that behind this technology, there are

52:46.840 --> 52:53.000
people, grad students, researchers, and those people have feelings. And, and I have to say when

52:53.000 --> 52:59.000
all the negative energy towards Delphi came across, I, I felt bad, but I didn't feel bad because

52:59.000 --> 53:04.680
I was involved in releasing the project or people were upset. I felt bad for the people at AI2

53:04.680 --> 53:10.680
who are the recipients of all this energy. And that energy, I think, could have been more, uh,

53:10.680 --> 53:18.760
constructively targeted. I think anyone nowadays is very cautious about putting a language model

53:19.320 --> 53:26.200
out behind, you know, a text input portal anywhere on the internet. And maybe that's one of the

53:26.200 --> 53:32.600
practical outcomes is that you just have to be very careful because it's all too easy to elicit

53:33.240 --> 53:38.680
offensive stuff out of this language model because it is a mirror of ourselves. And, you know,

53:38.680 --> 53:44.760
we are offensive to each other. And that's all baked into the language it learned from. And so,

53:45.720 --> 53:52.760
yeah, it just seems like there's just a lot of caution around doing what you do, which is to just

53:52.760 --> 53:57.400
be open with your work and put it out there as a prototype, warnings and all. I think fewer and

53:57.400 --> 54:04.360
fewer entities in this space are willing to take that risk. Well, I really hope that we over the

54:04.360 --> 54:10.920
years remain willing to do that appropriately. It needs to be done right. But there are people

54:10.920 --> 54:16.680
for whom it almost seems like a sport, right, to use your phrase from a previous conversation. A

54:17.880 --> 54:24.600
sport to come and bash these sorts of efforts. And it's all too easy. I don't think it's

54:24.600 --> 54:30.440
it's sporting and I don't think I think you can always do it. So I would not recommend

54:30.440 --> 54:36.680
to the Olympic Committee to include large language model bashing in in the next Olympics. I would

54:36.680 --> 54:43.560
instead encourage the people who are worried about that to engage with building better models,

54:43.560 --> 54:49.880
with building better controls with because these models are being built. And AI is taking an

54:49.880 --> 54:56.600
increasingly participatory role in society and decision making. So we need to figure this out,

54:56.600 --> 55:05.320
not to bury the issue because it's too fraught. Last question, Lauren. If you could time travel

55:05.320 --> 55:11.960
back those four years to when you said in that Pact Room to all of these people, including me,

55:11.960 --> 55:20.840
the stuff barely works. What would you say today to this Pact Room, some tens of thousands of

55:20.840 --> 55:27.000
people listening right now, a lot of them hopeful and taking part in this revolution, which is

55:27.000 --> 55:32.040
absolutely underway. I mean, it's unbelievable what you could do today compared to four years ago

55:32.040 --> 55:39.000
and who knows four years hence. Has your advice changed? I would say that I along with many people

55:39.000 --> 55:45.320
have been surprised with the progress of the technology. So I would say this stuff barely works,

55:45.320 --> 55:51.320
but I would add the proviso, but it's moving super, super fast. And then I would still add the

55:51.320 --> 55:59.240
cautionary notes, never trust the AI demo. And even if this looks very impressive, think about

55:59.240 --> 56:05.560
what's under the hood, what are the implications for society. Don't get caught up in the hype,

56:05.560 --> 56:11.320
not the negative hype of the sort that Elon Musk spouts, but also not the positive hype of the

56:11.320 --> 56:16.520
sort. Okay, we have achieved centuries. We have not. Thanks, Aaron. Great talking with you.

56:16.520 --> 56:43.480
Thank you, John. A real pleasure.

