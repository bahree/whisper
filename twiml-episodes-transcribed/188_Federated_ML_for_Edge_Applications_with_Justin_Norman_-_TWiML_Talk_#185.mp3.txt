Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode of our Strata Data Conference series, we're joined by Justin Norman, director
of research and data science services at Cloud RF Fast Forward Labs.
Fast Forward Labs was an applied AI research firm and consultancy founded by Hillary
Mason, whose Twimble Talk episode remains an all-time fan favorite.
My chat with Justin took place on the one-year anniversary of Fast Forward Labs' acquisition
by Cloudera.
So we start with a bit of an update before diving into a look at some of their recent
and upcoming research projects.
Specifically, we discussed their recent report on multitask learning and their upcoming
research into federated machine learning.
To learn more about Cloudera and CFFL, visit Cloudera's machine learning resource center
at cladera.com slash ML.
A huge thanks to them for sponsoring this series.
Thanks also to our second sponsor, Capital One.
At the Nip's conference in Montreal this December, they'll be co-hosting a workshop focused
on challenges and opportunities for AI and financial services and the impact of fairness,
explainability, accuracy and privacy.
The call for papers is open now through October 25th.
For more information or submissions, visit twimlai.com slash C1 Nips.
If you love this show, you've got to love our sponsors because they help make it possible.
So please take a look at what they're up to and tell them Twimble sent you.
And now on to the show.
All right, everyone, I am here in New York City at the Stratoc conference and I'm seated
with Justin Norman.
Justin is the director of research and data science services at cladera fast forward
labs.
Justin, welcome to this weekend machine learning and AI.
Thank you very much.
Glad to be here.
Awesome.
So Justin, you started your career in data as an officer with the US Marine Corps.
Tell us a little bit about your background.
Yeah, that's absolutely right.
So I actually got a chance to study computer science and focus on mathematical optimization
at the Naval Academy.
So naturally, when I made the decision to join the Marine Corps, I was asked to go into
a technical role, I'm pretty much straight off the bat, which was just fine by me.
But over the course of about seven years, I got a chance to both locally and also overseas
participate in a lot of large scale network enterprises, which had massive data challenges.
Not the least of which were that time for over 30,000 connected devices under management.
And just understanding what traffic flows looked like normal, what traffic looks like that
were anomalous.
And being able to react dynamically to that was a large portion of my job.
So machine learning was embedded quite frankly in the sort of a soul of what we were doing.
But at the time, we didn't really have terminology for data science and machine learning.
We just sort of did what we needed to do.
But naturally that would help me to sort of progress into industry as the field emerged
a bit further later in my lifetime.
You went on to found a startup in machine learning, is that right?
I did.
So I worked with a few people that I knew quite well to try to work on the problem of business,
predictive analytics within sports.
And so we weren't so focused on the saber matrix that you see from the sort of traditional
sports analytics, but we're a lot more interested in trying to find ways to solve some of the
corporate challenges that we saw in a lot of other industries, but applying them to industries
which really didn't have that kind of background in skill set and helping them to accelerate
from a financial perspective.
So a lot of our focus was actually in the beautiful game in football, but the right football.
And we did a lot of work in Central and South America to help some of those smaller clubs
actually be able to take advantage of the business value of their players.
For example, transferring or trading a player that had a higher social media impact than
perhaps another player that was equally skilled to try to impact things like ticket sales
and retention of a season ticket holders, things like that.
Interesting.
I did an interview with a guy named Minoa gift that was doing some very similar things
in terms of looking at how to score player, social media impact and the ultimate impact
on the game.
So you went on to, among other things, do data science at Cisco and now at Cloud or a
Fast Forward Labs, tell us a little bit about your role and your focus at CFFL.
Great.
Yeah.
So as people may or may not be aware, we're actually exactly at one year of the acquisition
of Fast Forward Labs at Cloud Air.
So in a nutshell, my role is to take Fast Forward Labs kind of from where it is, which
is actually very well integrated and quite successful with its existing clients and some additional
quite our clients and really scale that globally.
And I mean that from not just a physical standpoint.
So yes, we want to expand to media and into Asia, but actually also from a data perspective.
So can we take the research that we've done, the products that we've developed and start
to do those at scale and the enterprise and be really known as not just leaders in the
research field, but leaders in how to apply that research across both corporate organizational
governmental entities.
And that is from a technical perspective, a challenge, but also from a human and strategic
perspective, a challenge, which is why I think Fast Forward Labs is really uniquely positioned.
Because as you know, our research not just doesn't just focus on the technical and technique
aspect of things that we have quite a bit of research in that area, but also in how to
layer the people in process and skills, conversations into the application of that technology so that
you can get the outcome when you actually expect to have it.
And so for those that don't know, the research that you're referring to is a series of reports,
can you talk a little bit about some of the topics that you've covered in the report
just as context?
Sure.
The best way I think to talk about the reports is to start with a process that we go through
to actually decide what we're going to do.
And I think it's really powerful.
So the entire team gets together and really calls out, I'd say, with the intent to find
some bad ideas of types of topics that might be relevant within the next 12 to 24 months
within machine learning.
And the intent is that we're going to look at topics that are not just papers or research
that is out there in scholarly journals, but actually something that is starting to be
applied in industry, or maybe we have the ability to bring that last step.
And so topics in the past, like natural language generation, symmetric recommendations,
image analysis with deep learning, and then most recently multi-task learning, have sort
of become of the topic that has gone that we've narrowed down in that process and really
have felt that is most relevant to the widest group of consumers that exist within industry,
academia, and beyond.
So we've actually just launched a report as I just referred to called multi-task learning,
which focuses on actually executing multiple machine learning tasks at the same time.
And so we're really excited about that.
And there's actually a number of people this week at Strata who are going to be speaking
about that in greater depth.
And so very excited to hear what they have to say about that.
But we're not stopping and actually have two more reports in the queue.
One of them we actually know the research, but the research is going to be focused on
and that's on federated learning as well.
So we're really excited to be able to share this because we think that now we're beginning
to come in influence not just by the community that we had through Fast Forward Labs, which
was extensive, but also because we're at Clavera, we now have a focus along the enterprise
with a much, much larger scale.
It's helping us to understand what's relevant to that community as well.
And that definitely has directly influenced what we've been able to perform in the research
space as well.
On that multi-task learning paper, I haven't had a chance to take a look at that one,
but around the time it was being that initiative was being kicked off someone at CFFL reached
out to me to CFI knew anyone who was working in that space.
And I had recently done an interview with Ryan Poplin at Google.
I'm pretty sure that's who it was who had made an interesting comment that they were looking
at these retinal, fundus images and using those to determine a variety of demographic
information about the people who the images came from.
And they noticed that when they asked their model to do to determine a single thing and
compare the performance relative to asking the model to do multiple things, the model
performed better when it had multiple things to do.
And his theory was that that kind of forced the model to generalize better.
And so I kind of made the connection.
I hope that connections are good connection.
That's a very good connection.
And I think you'll see that in the report even that we recreated results on that specific
use case, but on other use cases that was very similar.
And in some areas, it's very well explainable to your point why that happens.
And in some areas, it's not.
And I think that's one of the most powerful parts of the research component of what that
report covers.
So you certainly can recreate that experience where you do have model performance that's
a bit better on one or multiple, excuse me, multiple or even three or four or even
more learning tasks at the same time.
But the type of data is highly relevant to whether or not you're going to see that performance.
And I'm one of the things that we spend a lot of time talking about.
And so is the, what's the kind of core thesis of that, of that paper?
Well, so that for, like I said, for certain types of data sets and for certain learning
tasks, you should definitely do it.
So it is the core thesis.
But I think the most important part is that it is possible to actually execute these
multiple learning tasks and actually get them at scale, utilizing some of the more now
commoditizable learning functions.
So things like deep learning models are highly relevant to this as well.
So the focus is on really making sure that people understand which techniques exist and
then when to apply them.
And are there a specific set of techniques for multitask learning or is it just the same
stuff but with multiple objectives there, but a little bit of both.
So there are definitely certain sets of techniques, but it depends on the objective.
Can you give me an overview of the specific types of techniques that were you would need
Frederica for?
Okay.
That was the report that recently came out going forward.
You are looking at, you mentioned that the next one is going to be on federated learning.
That's right.
What's the, what's the thinking around that?
So one of the things that was always a theme at fast-forward labs.
And I think again, back to my earlier point is something that's coming in front of mine
because of the types of customers that we've been working with at Clara is the idea of
really trying to execute and scale machine learning at the edge.
So by the edge, I mean, at devices that are, you know, the last device before a human
or last device before a sensor, you know, executes its function to read data from an environment.
And there's a number of reasons to want to you machine learning at the edge.
I think one of the biggest ones is if the device, and now they do, have enough computational
power to execute that, you get returns that results back that you can, you know, influence
outcomes with directly without having to report back to some kind of central model or
sometimes central computing store.
So this is most relevant in IoT applications.
I think you can even think of it as a human with your cell phone.
There's obviously multiple machine learning models that are powering very important functions
on your phone.
But one of the challenges that comes into play right away with that is that ultimately,
there's a lot of people who want to have the power of machine learning and AI enabled
capabilities, but without necessarily exposing the underlying data structure.
And the data that, that data structure that might be influenced by a person's personal
choices, by, for example, on a commercial application, competitive IP applications that
don't want to be shared with the industry, but you still want the benefit of being able
to train and to share, for example, parameters, hyperparameters that settings that are relevant
across the scope of problems without necessarily exposing how you got there.
And so federated learning is a set of techniques for an approach to be able to do just that,
where you potentially have a centralized model or a model that serves as a baseline for
one of multiple devices that are deployed and over multiple workflows.
But you still get a chance to benefit from that without exposing each one of the individual
data underlying data structures that are there.
And that's very important from an ethical perspective, very important from a commercial
perspective.
And also it does have a performance impact, a positive performance impact, as you might
know from starting from, you're essentially starting from, you know, a little bit farther
down the races that are that that's starting line.
So is the issue that you're kind of framing around not wanting to disclose model parameters
is the issue that you're looking to address the idea that if you push a model out to
the edge, you're exposing, you know, that model, the parameters of that model are IP and
you're exposing that IP.
Kind of the other way around.
It's the data we don't want to expose.
So, for example, if you're using a phone, the types of text messages or images that
you have on your phone are very private, you don't want that to be shared.
But being able to do natural language processing or analysis of your text messages in order
to respond better, provide applications on top of that is something of course you want
to have that.
So if you are a provider of software, you want to be able to train models on the widest
set of information you possibly can, but you may not have the benefit of reading everybody's
text messages nor should you.
So is there a way that potentially you can get the benefit of a more precise model that
fits better to a larger set of data, generalizes to a larger set of data without actually having
access to that data?
And so being able to perform these multiple learning functions over time and a very broad
set of workers or workflows that are existing in near real time and then giving that information
back the potential parameter settings of a model or the sort of things that shape a fit
to have a model be more generalizable without exposing the underlying data is what we're
looking for.
And so the idea is that I've got access to some set of data at the edge.
I don't want to transcend, send all of that data back to some central location, for privacy
reasons, bandwidth reasons, whatever, all of the above.
So what can I do on the edge to presumably, we talk a lot about the edge for inference,
but what we're talking about here is to train or partially train on the edge and then use
that to, it sounds like the idea of federated is use that to enhance a centralized model
that can maybe even be like distributed or some aspect of that model or what that model
is learned, distribute out to the other edge devices.
How do you do that?
Well, I mean, you'll have to wait for the report.
But the longest short of it is, it's now possible from a computational perspective to execute
these multiple workers when you're actually looking at thinking about a model that's
being deployed anywhere.
It's being done by some automation workflow, just like any other software engineering component.
And so now that we can do this, not just like with one or two on a laptop, but with 1,000
or 1,000, 100,000, and actually have not just a bit like, we don't have been going straight
but also about computational constraints.
That's a large part of what we'll be exposing is how to actually execute this.
And then what are some of the constraints of that from a technology perspective?
That's one aspect of it.
And then from a sharing of model parameter information or model fit information, how actually
to bring that back to a centralized location, to average and generalize that across the
full schemes that it's useful to be to your point, distribute it back into that matrix
of learners is also focused on.
So there are a couple of techniques that we've developed and a few that we've adopted
and written about that you'll hear about very soon.
And the past, my impression of the reports has been that the focus is on kind of going
out, exploring what's out there in important areas and writing about those.
One of my favorites was the natural language summarization, and it did kind of a survey
of all the different techniques that have been used to do summarization, LDA, lots of
other things.
It sounds like here you're starting a tiptoe into developing some technology or algorithms.
Yeah, and I think that happened largely because we recognize that this is a real and really
important need for industry and for government.
But the techniques that we did research on weren't as mature as they might have again
for other areas of research that we've explored.
So if they were out there, we'd certainly tell you that these are the ones you should
use.
I think we've taken a good survey of what we've found for the multi, for excuse me, for
a federated learning.
But this time around, it became necessary for us to bridge the gap a little bit.
And so, of course, the team has that capability and it's more than happy to do that when we
think it's relevant.
I mean, in a lot of ways that is, it kind of pokes at the thesis of the research, which
is, I've always interpreted it as, this is stuff that's right at the edge and hate this
cool thing happens someplace which kind of flipped it and people don't really know yet.
Right.
I'm trying to remember what the thing was in the summarization paper, but there was, there
was some new, I think it was, it was word vectors.
Right.
So there was a bunch of LDA stuff and language modeling that people were using for this and
it was kind of worked a little bit, but it wasn't great.
But hey, word vectors came around and now we can really do interesting summarization stuff.
The thesis was, hey, we're kind of scanning the world looking for these opportunities
where external change has enabled some new way of some new capability.
You know, here, you're kind of more being that external change.
Yeah.
It's a pretty, it strikes me as a pretty significant shift.
It is a significant shift and I think it goes back to, you know, how we started the
podcast, which is, you know, Cladera gives us a platform now to have, I think, a perspective
on what's happening in industry from the top layer, right?
So I think one of the things that's very powerful that we've maintained about fast forward
lab is that we're talking to academia, we're talking to startups, we're talking to, you
know, organizations which are focused on machine learning and AI.
And that's a particularly powerful perspective that we've always had, but we didn't have
as much exposure to the, you know, industry players, but the people who are doing machine
learning and AI at scale in the edge and the petabyte scale, right?
So I think what we're finding now is we're being asked by that community to solve a different
set of problems and those problems may not have been commoditized yet.
So we have the ability to connect what they're looking at strategically and from a business
or a mission relevance perspective and then apply that cutting edge research, which was
always sort of there with a layer of, you know, of interface to use it to use a word.
So I think that's what what's occurring as a result of that one year being at the company.
Okay.
Okay.
Interesting.
And now this whole area of devices and the edge is not new to you.
No.
And we didn't talk about it in your background, but you spent some time at Fitbit.
I did.
How has that informed your thinking about this whole space?
It definitely has been shaped by thinking about this space and made it quite frankly
top of mind for me anytime that I recommend building a model factory, AI system.
And so just to get a little bit deeper about that at companies that have an edge device
that's being delivered to consumer or is being used to gather information from the environment
like a sensor, you end up having this really poignant real time challenge.
And so you need to and you want to do machine learning right at that device or near that
device because it's important to be able to serve back either to your human or to the
environment.
The results of a model or the score of a model in some very, some very poignant way.
And you know, the example that you get on a device like a fitness tracker is that a person
might go through some fitness activity and want to see, you know, projections of where
they might be, had certain variables changed, had they run faster, you know, what their
heart rate might be, if they were to continue the next set of their exercise.
And for example, you know, something very famously, you know, the number of steps that
you're doing, that's not a static number for any human being and their performance upwards
or downwards on that, you know, depends on a variety of factors, a variety of features
to use that machine learning language.
So of course, you want to be able to provide that back as quickly as possible and you know,
latency and incurred by going back to a more robust off-mind data store.
Sometimes it's just not, it's just not for a moment enough to provide that.
And so you end up wanting to do this sort of challenging activity, which is to do, you
know, training of a model and batch scoring of a model within your data store, where all
the data actually lives on a larger, on a larger platform, like, for example, the
chip, and you want to be able to somehow take the results of that train model and put it
somewhere that's much closer to the user itself, which is probably connected near to some
connected device at the edge.
And it actually means that you need a couple of different environments to be able to do
a scoring of machine learning models or AI models.
And then your production environment doesn't become something that's running, you know,
within a 24 hours of latency, but something that may be running several times a minute
or even a second.
And if you say a couple of different environments, what do you mean?
I mean, I mean, deployment environments from machine learning models.
So for example, historically deployment might look something like, I have a machine learning
model that's written in Python and I'm going to use a cron job or something like that to
schedule a run of the model.
It'll just pull a representative sample from, you know, from some interface layer on top
of the doop and then I'll return the results of that and dump it in a table somewhere
else also to do.
And then people can query that with whatever they want, you know, let's say SQL, right?
So that's a, that's a, that's a, a point of workflow.
And then if you want from an application perspective to return results, you could put some kind
of restful API layer on top of that and then you listen to that endpoint or hit that endpoint
whenever you, you have a request, that's, that's an idea.
Not super performant, right?
Yeah.
So if I was a, you know, a cell phone or a fitness tracker and there were 17 million
of me, right?
And I wanted to do that, that, that same function that I was querying that API directly
from there.
It's pretty challenging, right?
So you actually end up needing a different environment, actually completely different
soft software engineering workloads to deliver that come from performance, but you still
want access to the underlying data because it's still saving the doop and that's how
you train, right?
So this is, you know, sort of an extension of the problem that we were talking about
before, where you actually do, you know, want to have access to the data.
And so having a separate environment that's closer to the closer to the consumer, whether
that be a device or a person, allows you to actually do some pre-processing and treat
pre-printing of features actually at the edge or near the edge.
And so for that, you know, common set of information that are features that I like power these
models.
And wanting to, you know, to do that pre-processing almost as the data streamed itself, right?
And then allow the model that's sitting on that device to pull from that source of data
where it needs to, and then return results much more quickly.
So that's one of the things that I learned while working at a device company is that sometimes
the machine learning challenges are not necessarily from a technical algorithm perspective, but
actually how you apply it and how you apply it to get the outcome that you're looking
for from, you know, from a consumer perspective and sometimes near real time or step second
time.
It sounds like when you describe this architecture, it's in the different environments, you
know, we've talked about kind of the central capability we've talked about the edge, but
it almost sounds like you're laying out a hierarchical environment where you've got,
you know, maybe you've got regions and something that's, you know, immediately behind the
edge, like this is talks about often in an IoT environment, you've got your edge devices
and then you've got your plant device that's kind of a, you know, it's, I don't think
it, I don't know that it's clear what the exact function of that is today.
It's kind of doing some training and intermediate model development or at least that's the people
aspire to that.
Sure.
Today, it's more doing data collection and centralizing the data transfer to, you know,
some mothership or some kind of intermediate mothership.
How do you see all that evolving?
Yeah.
So what I think is starting to happen is because there's a much clearer definition of what
actually you're going to need from a, especially for commoditized models.
So if we're talking about like ensemble models, like random forests are actually boost.
We know it's, you know, we're very, very good at understanding what these models are
going to need from an input layer perspective to be successful, right?
So when you begin to pre-process or pre-trained features or pre-trained data and put that
post to the edge, the interviewer that you're talking about actually can be responsible
for promoting these feature sets and, you know, accomplishing this migration of data with
the very precise scale as opposed to just dumping or translating everything when we think
we might need it.
And I think that's where MLOPS, AIOPS is actually going to start to take a bigger role.
And, you know, really even applying machine learning to, you know, the workflow itself
and what necessarily needs to be accomplished during that phase.
And you start to see the, you know, technologies like airflow and Jenkins, you know, be the control
layer for it, but underlying it, there's actually a machine learning functions that are
powering what we translate into the, you know, the online feature store.
Which I think is something that's really exciting.
Now, airflow is kind of a data workflow open-source project developed by Airbnb.
Airbnb?
I think the original developer came from Facebook and then worked at Airbnb, but yeah,
so airflow is an example of what I'd call an AIOPS or AIOPS tool.
It definitely does scheduling and more flamaticant, just like any other tool like in that category
would do.
But I think what makes it specific is that it's really tuned for the types of techniques
that you'll need to perform in a machine learning workflow specifically.
And so I think that category of, you know, tools is starting to become not just needed,
but also up here in multiple, you know, software options, which shows that it's validated
in the market.
Airflow is an open-source approach to it, but there are certainly a lot of others who
are taking a more commercial approach to it.
And I think it's going to be necessary and is a lot of these companies and, you know,
project scale.
And you've mentioned Jenkins, which, you know, has been around forever, kind of in the
Java community for those bills and stuff.
Right.
How does that fit into this whole?
Well, I think we're, so now we're, like, getting into the idea of how we're going to,
or what is a model, or what is a deployed model.
Yeah.
And so, you know, a deployed model, as we mentioned before, could be simply a table somewhere
that we've scored or we use a function to score, you know, some fitted, some model to
score.
So, you know, the data, the results of data, and put it into a table.
That could be, you know, what we consider deployed.
But I think more and more we're starting to see deployed, meaning it is the model, the
model architecture itself, and the ability to query that, that model architecture for
the result that we're looking for in some kind of performant or way that we can fit into
an SLI.
I think that's what we're starting to see.
And so, when containerization and microservices, you know, are layered together to try to
produce this result holistically, so, like, you know, a model, you know, becomes the
container itself, and the services that it takes to be able to interface with it, tools
like that, you know, automation, workflow management, tools become critical.
Because when you're actually interfacing with a model, you actually might be spending
the whole workflow of a machine, the data, workflow that the machine needs to interact
with, and then performing the scoring, and then actually returning that result all
of us.
And so, it's no longer about just having the skill set to be able to produce a nicely
fitted model against some set of data.
It's actually being able to put, to return that result, and the way that it needs to
be consumed, either by an API or directly to an application.
And so, now we're in this space where we're thinking about things a bit more holistically
than we have to.
Maybe going back a little bit to this kind of federated model, federated idea, and
just kind of in the back of my head thinking through like how this might work.
And at, you know, one thought is at a low level, you know, people are doing things around
distributed training, and that's all centered, or frequently centered around this idea of
like exchanging gradient updates from one machine to another, or, you know, having something
that's tracking all of these gradient updates that the distributed workers involved in training
can access.
Like is that involved in, is that part of what we're talking about, or what we're talking
about, you know, when you talk about federated training and federated ML, is it something
at a higher level?
Well, so I think it's certainly a component of what we're talking about, but I think
we have to be really careful there not to slip into the space of, of training the model
of leaking, essentially, across.
And that's exactly what we don't want to accomplish with federated.
So I think the, and a library on what you mean by that.
So the idea of, you know, even though individual workers or individual models that are, that
are being deployed by workers, don't have access directly to other, to other data stores.
The fact that they could learn the underlying patterns or structures and perhaps in for
what's there, we really have to be careful about that.
And so the idea of a federated sort of abstracts, I think a little bit more, you know, how
to accomplish that, though we're sharing, you know, not just graded information, but really
a larger set of parameters that are, that are, you know, available depending on the
type of model.
And actually, because it's a, it's much more oriented around the techniques, the specific
algorithms that we're using underneath that shouldn't matter as much.
But rather, you know, it is the mechanism for which these distributed workers are able
to share two centralized repository and then that centralized repository performs better,
you know, then in each individual one could do that we're really focused on.
So then it sounds like it is, it is operating at a higher level and the, the goal is less
about, you know, sharing these gradient updates or whatever is a low level kind of information
sharing for the model in question to allow this distributed model to converge and more
like training a model individually at different places and sending key information about that
model, sharing key information about that model to make everyone's models better.
Right.
And so, again, the sort of central idea, if you were asked to summarize that would be
that, you know, if you have, you know, the ability to take a random of these techniques,
which might be, you know, ultimately computationally expensive, it's in to do across such a large
set of independent workers, you would achieve an outcome which would be impossible without
that centralized sharing information, without that federated sharing information.
So the other thing that this makes me think a little bit about is differential privacy.
Is that something that you've looked at previously in other roles or that you are looking
at in the context of, of this federated learning?
Yeah, definitely.
I think it's going to, we're going to definitely try to write about that, but the research
is ongoing.
So I wouldn't be able to tell you what we've uncovered yet because we're literally looking
at some of these things right now.
Okay.
You talked a little bit about online versus offline learning often when the whole online
learning comes up, it comes up in the context of spark and streaming and pipelines.
How do you see that fitting into this?
Yeah.
So when I was describing this area earlier where, you know, where you might want to respond
to, you know, to an activity or something that has occurred, like, for example, with
a fitness tracker, those exchanges of data are happening with real-time streams.
So, you know, I don't think it's a secret to anyone that's, you know, that's spark streaming
and really, you know, things like Kafka, you know, interfaces between these devices and
our key component of how this works.
So what I would say is that's the piping behind this process is that we've been talking
about.
What makes it a little bit more challenging is the sort of potentially asynchronous nature
of how data works in that area, and this is back to the sort of Cisco days, and the, you
know, for lack of a better term, the UDPness of it, where you may receive some or all the
data, some of it may be at order, probably will be, and, you know, you still have to find
a way to coherently respond to that.
And I think that component of it becomes a data engineering challenge, which is at the
edge, combined with a machine learning challenge, which is at the edge, combined with a software
engineering challenge at that the edge.
And so we have to solve all these problems very, very quickly.
And so what I've seen in industry so far is I've worked on it is that everyone's architecting
slightly different workflows are on this, and it's probably time that we and others begin
to standardize that a bit to make it more accessible for the community.
And so we're specifically talking about workflows.
Are you talking about the whole, the federated concepts specifically, or we're not, we're
talking about the workforce for managing these edge devices and software engineering for
edge devices.
Yeah.
I mean, you asked about the, about, you know, how a premium instrument plays into it.
And I think it, like I said before, it's the piping that regardless of what application
you're trying, whether it's a federated learning mechanism, or simply you're just trying
to deliver online services, machine learning services, you're going to have to interact
with that environment.
And the ecosystem right now is pretty complex.
And I think we've got to do some work to, you know, to at least abstract some of it so
that we're more focused on solving the problem than dealing with engineering workflows, which
is where we currently are.
And how close do you think we are to being able to do that?
We've advanced engineering practices, you know, with DevOps, for example, significantly
over the past few years, and we've got a growing base of experience with mobile.
Although for, you know, from the device perspective, the rate of change of, you know, what's
happening in the device tends to be a lot slower than what we'd want in a kind of an IoT
enterprise, more agile environment.
But with, you know, with those as kind of background, you know, then we've got these two, you
know, IoT, which is changing quickly, and we're just kind of wrapping our heads around
and trying to figure out men to layer onto that, the machine learning and AI stuff.
Like how do you think we as an industry like put together?
You mentioned standards.
It seems like we're so far from standards, you know, just a way that works that consistently
to manage all of this stuff.
Yeah.
I don't think that would be a smart thing to do, but I will say that it might be useful
for us to maybe go a little bit away from segregating ourselves by community.
There are really well, very well understood, very powerful practices and networks that
have existed for years that might be really helpful for us as some of these challenges.
They've been dealing with real-time streaming since the beginning of computers sharing
information with each other, right?
I mean, I spent a lot of time at Cisco, and let me tell you, there's some people there
who have gotten down to the algorithm level and digester shortest path and can tell you
exactly how messages respond to that, you know, from device to device when you're talking
about the context of a router.
And those standards exist.
They're actually really, they're very well documented, they're published, they're public.
So I think that we have examples of how to accomplish this, you know, sort of standards-based
practice out there, but it may not come from the community we're familiar with, right?
So as a machine learning practitioner or a data scientist, you're probably a lot more
familiar with the software engineering community, and you're probably a lot more familiar with
people who are publishing standards by way of, you know, GitHub or standards by way, you
know, of library, but we may not have access to or may not have thought about, you know,
applications for handling messaging or handling workflows, you know, even a lower layer
of the stack and the computing and computing, and I think it's going to be this interface
of communities that are happening that helps us to build out these standards and practices
that make sense because we are not talking about one community, we're actually talking,
we just talked in this conversation about multiple different architectures converging.
So I think we need to expand our view a little bit and start to look for places and people
that have experiences, you know, potentially a different part of the stack that can help
us be successful where we are here in the real time of online and mental conversation.
You know, you talked about Spark as being core to this streaming environment.
Is there an analogous environment or platform on the Python side that's gained any traction?
Yeah, not that I'm aware of, and I think largely that's happening because of the way that
Python tends to interact with data in general.
I think data scientists, data scientists, we've tended, and this might just be a community
issue.
We've tended to use it on the, you know, the research and dev side.
And so interacting with data, you know, that the more abstracted, I mean, the less abstracted
layer really at that point has not been something I've seen a lot of research poured into,
but I could be wrong about that.
So, you know, I think I haven't seen it, but, you know, that's not to say it doesn't
exist.
All right.
So you, you mentioned a term AI ops, or you may have said ML ops a couple of times I've
done some writing about that term, but as applied to AI enabled IT operations, so like managing
the data center with AI, are you starting to see, or are you starting to evangelize the
use of that term to specifically refer to ops as it relates to AI, meaning managing the
AI infrastructure of an organization?
Yeah.
So, I think that's where I would hope we would go.
I mean, you know, who knows what these terminologies are going to end up, but I mean, we're
talking about AI when, you know, 10 years ago we've been talking about, you know, applied
statistics.
So, you could go anywhere, but I do think that that's a really powerful way to use that
term, because I think in a lot of ways, I actually saw a tweet the other day that was really
great.
You know, that was talking about, you know, asking, you know, software engineers and IT
people to deploy models, being just like, you know, trying to have, see world, take care
of your drafts, just because they take care of other large scale mammals, right?
Like, it's really completely different set of skills, yeah.
And so, when we think about software engineering, we have a term, we have a terminology, we
have a framework, we actually have tools and entire category of software that's devoted
to taking care of the operations necessary to manage the software infrastructure that runs
the company, right?
It's very, it's a huge business and it's actually really important.
But from an AI perspective, historically, and I've experienced this personally, you're
usually either asked to do it yourself or you're, you're translating or giving, you know,
your baby, so to speak, to IT or engineering organization and telling them to scale that.
And what almost universally happens, they have no idea why you made those decisions.
They disagree with them, sometimes for really good reason, about the way that you approach
it from a software engineering perspective.
And so, they bring engineer the whole thing and miss a lot of the core components of
what you were trying to accomplish.
Do you have a specific example of that?
Yeah, so I can tell you that one of the first models that I actually developed myself
in industry, you know, was written in NAR, I think like a lot of us were doing it.
It was a simple logistic progression, or turned a binary result and I felt like it was quite
generalizable for the problems that we're looking at, so I delivered it to my IT organization
to be applied back into one of the products that was actually already online for one of our
internal tools and they wrote the entire thing in Java.
And so the challenge with that was, you know, Java has a lot of, it's very verbose.
And so the performance that I had sort of expected out of my small, like, you know, seven-line
Python code, now had a whole bunch of other dependencies that I'd never planned for.
And so the application, the AI application, so to speak, that returned, you know, sub-second
or very close to second results for the data set size I was looking at now took, you
know, five to ten minutes to run.
Probably we think of going from, you know, our Python to Java is a step forward in performance,
right?
Totally.
If only that's all you're doing, but in the context of it being run in a much more robust
application, which had not quite a few other dependencies that I never cared about, you
know, that that's isn't necessarily the case.
And so, you know, I think if we'd had a much more robust AI workflow in place, I would
be deploying my model, so to speak, you know, and the workflow or even the AI tool kit
itself would decide the best way to implement that or to return that rather than, you know,
the IT organization myself hadn't had the negotiation for every single product that we
developed, right?
And so that, you see that all the time when someone checks in code, you know, to, you
know, to a DevOps workflow, they have no idea how it might be integrated at the end.
It doesn't matter.
It's that the core component of what the application that they've developed gets integrated
into the overall.
And so we just got to get there from an AI perspective.
And I think we're starting to make a lot of progress on that.
Years ago, maybe five years ago, let's say, you know, we would talk about data scientists
as this unicorn, you know, did everything, right?
And now more often than not, we're starting to see, you know, specifically in internet
companies, right?
You've got this very well-defined role now of machine learning engineer that works, you
know, tends to work hand in hand with data science.
Sure.
People are even changing, you know, data scientists is becoming a bit passe and there's
like applied research scientists and these other roles, but they're, it's much more
of a teaming kind of relationship between these two and the ML engineer than inherits, you
know, because they are an engineer, they kind of inherit everything that's been developed,
you know, in terms of DevOps processes.
Yep.
And it seems to be that a lot of that is as much smoother, you know, for organizations
that kind of have that worked out.
Sure.
This tends to be internet companies, not enterprises, there's still a lot of work that
needs to be done, I think, to kind of translate these practices.
I think that's right.
But we're getting there.
And I think you're right about that.
This is a good story.
I'm not, you hear that, like, to sell, you know, doom and gloom, I think we're absolutely
getting there.
I would expand your list to data first companies.
Companies then, like, use, you know, information that they either generate or derive from their
customers as the main product that they then, you know, present back to their customers.
That's a great distinction, yeah.
And so, you know, there are many there in that category, but the ride sharing, you know,
companies, you know, anyone that's doing consumer pricing, you know, like hotel companies,
come to mind is the ones that are really innovating in this area.
It's a little slower for utility companies.
It's a little slower for telecoms and it's a little bit slower, you know, for governments.
And I think these are some of the organizations which actually have the data scale that we
really want to interact with.
And I haven't seen as much of that there, but we're on the way.
And to your point, if we can get the tooling, you know, ready or closer to being, you know,
being mature, as we build those roles in those organizations, I think that's a really
good marriage.
Yeah.
Yeah.
Awesome.
Well, Justin, thanks so much for taking the time to chat with us.
It's been great to meet you.
Welcome to CFFL here to like 90 days.
This is day 90.
Awesome.
Awesome.
Well, I enjoyed the rest of the conference and thank you.
All right, everyone, that's our show for today.
For more information on Justin or any of the topics we covered in this show, visit
twomolei.com slash talk slash 185.
For more information on the entire strata data series, visit twomolei.com slash strata
and why 2018.
Once again, a big thanks to our sponsors, Cladera and Capital One for their sponsorship
of this series.
As always, thanks so much for listening and catch you next time.
