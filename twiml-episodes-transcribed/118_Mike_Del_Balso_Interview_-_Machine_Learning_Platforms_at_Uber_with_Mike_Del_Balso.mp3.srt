1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,240
I'm your host Sam Charrington.

4
00:00:32,240 --> 00:00:37,320
In this episode I speak with Mike Delbalso, product manager for machine learning platforms

5
00:00:37,320 --> 00:00:38,920
at Uber.

6
00:00:38,920 --> 00:00:43,520
Mike and I sat down last fall at the Georgian Partners portfolio conference to discuss

7
00:00:43,520 --> 00:00:48,560
his presentation, finding success with machine learning in your company.

8
00:00:48,560 --> 00:00:53,080
In our discussion, Mike shares some great advice for organizations looking to get value

9
00:00:53,080 --> 00:00:54,840
out of machine learning.

10
00:00:54,840 --> 00:00:59,200
He also details some of the pitfalls that companies run into, such as not having the proper

11
00:00:59,200 --> 00:01:04,480
infrastructure in place for maintenance and monitoring, not managing their expectations,

12
00:01:04,480 --> 00:01:09,080
and not putting the right tools in place for data science and development teams.

13
00:01:09,080 --> 00:01:14,560
On this last point, we touch on the Michelangelo platform, which Uber uses internally to build,

14
00:01:14,560 --> 00:01:19,240
deploy and maintain machine learning systems at scale, and the open source distributed

15
00:01:19,240 --> 00:01:22,800
TensorFlow system they've created, Horavod.

16
00:01:22,800 --> 00:01:29,600
This was a very insightful interview, so get your notepad ready.

17
00:01:29,600 --> 00:01:33,760
Before we jump in, over the last few weeks, you've heard me talk quite a bit about our

18
00:01:33,760 --> 00:01:40,000
MyAI contest, which explores the role we see for AI in our personal lives.

19
00:01:40,000 --> 00:01:44,760
We receive some outstanding entries, and now it's your turn to check them out and vote

20
00:01:44,760 --> 00:01:46,280
for a winner.

21
00:01:46,280 --> 00:01:51,880
Do this by visiting our contest page at twimmolai.com slash MyAI.

22
00:01:51,880 --> 00:01:57,640
Voting remains open through Sunday March 4th at 11.59 pm Eastern time.

23
00:01:57,640 --> 00:01:59,520
One more quick announcement.

24
00:01:59,520 --> 00:02:05,720
Join us on Tuesday, March 13th for the next Twimmol Online Meetup, in which our presenter,

25
00:02:05,720 --> 00:02:10,840
Sean Devlin, will be doing an in-depth overview of reinforcement learning and presenting

26
00:02:10,840 --> 00:02:16,360
the Google Deep Mind paper, playing Atari with deep reinforcement learning.

27
00:02:16,360 --> 00:02:21,560
Head on over to twimmolai.com slash Meetup for full details.

28
00:02:21,560 --> 00:02:26,280
And now on to the interview.

29
00:02:26,280 --> 00:02:33,080
Hey everyone, so I am here at the Georgian Partners Conference, and I have the pleasure

30
00:02:33,080 --> 00:02:39,160
of being here with Mike DelBalso, Mike is the product manager for machine learning platforms

31
00:02:39,160 --> 00:02:40,160
at Uber.

32
00:02:40,160 --> 00:02:42,400
Yep, Mike, welcome to this week in machine learning and AI.

33
00:02:42,400 --> 00:02:44,400
Thanks, happy to be here.

34
00:02:44,400 --> 00:02:49,360
It's awesome to have you, and I especially love it when folks tell me that they actually

35
00:02:49,360 --> 00:02:51,880
listen to the show before I'm about to interview them.

36
00:02:51,880 --> 00:02:54,280
Yeah, I listen to almost every episode at the gym.

37
00:02:54,280 --> 00:02:56,240
Oh wow, nice, nice.

38
00:02:56,240 --> 00:02:59,360
Do you like, do you put it on like 1.5x or my guy?

39
00:02:59,360 --> 00:03:00,360
I actually do.

40
00:03:00,360 --> 00:03:05,080
I have some thing where other people can't stand it, like when I play podcasts for my

41
00:03:05,080 --> 00:03:11,320
friends, but it's got some setting to skip spaces and to just go 1.5x, so it just sounds

42
00:03:11,320 --> 00:03:14,120
really weird, but I got you so used to what it doesn't bother me.

43
00:03:14,120 --> 00:03:15,120
That's fun.

44
00:03:15,120 --> 00:03:19,400
I listen to a ton of audio books and podcasts, and I tend to listen to it one and a half

45
00:03:19,400 --> 00:03:24,800
x, and you know, it's still the point where like if I hear the person slow down, they

46
00:03:24,800 --> 00:03:25,800
sound all weird.

47
00:03:25,800 --> 00:03:26,800
Yeah.

48
00:03:26,800 --> 00:03:30,920
I'm like really weird and slow talking, and it's not the point I was expecting.

49
00:03:30,920 --> 00:03:35,920
But yeah, there's actually like little jingles in front of different podcasts that when

50
00:03:35,920 --> 00:03:40,920
I listen to it at 1x, it just sounds weird, like because it's all down pitched or whatever,

51
00:03:40,920 --> 00:03:42,520
I don't know, it's weird.

52
00:03:42,520 --> 00:03:44,560
So I guess this podcast has that too, right?

53
00:03:44,560 --> 00:03:45,560
There's a little jingled.

54
00:03:45,560 --> 00:03:48,200
Yeah, I'm not even going to try to repeat this.

55
00:03:48,200 --> 00:03:49,200
Don't do it.

56
00:03:49,200 --> 00:03:52,200
It's called robot race.

57
00:03:52,200 --> 00:03:53,280
But yeah.

58
00:03:53,280 --> 00:03:58,480
So you know the routine, right, how do you get started in machine learning and AI?

59
00:03:58,480 --> 00:04:05,840
Yeah, so I'm a trained electrical engineer and right on, and so we're in Toronto right

60
00:04:05,840 --> 00:04:12,520
now, and I went to University of Toronto, and when I graduated, I got hired as an associate

61
00:04:12,520 --> 00:04:20,080
product manager at Google, and the associate product management program is the way that

62
00:04:20,080 --> 00:04:24,880
they hire product managers directly out of school, and it's a rotational program.

63
00:04:24,880 --> 00:04:29,120
So they put you on one team for about a year, and then you switch, and they put you on

64
00:04:29,120 --> 00:04:32,120
another team for a year, and then you can switch again if you want.

65
00:04:32,120 --> 00:04:36,200
And so my first role was on maps, and I worked on maps, data, kind of stuff, and then my

66
00:04:36,200 --> 00:04:39,840
second role was on Google's ads auction team.

67
00:04:39,840 --> 00:04:46,600
And I was the product manager for some of the teams that generate a lot of the machine learning

68
00:04:46,600 --> 00:04:50,920
predictions for the ads auction.

69
00:04:50,920 --> 00:04:55,480
And so the kinds of predictions you'd want to make are, you know, how relevant is this

70
00:04:55,480 --> 00:05:00,840
ad to the query that somebody is searching for, or how likely do we think someone is to

71
00:05:00,840 --> 00:05:05,200
click this ad, so just a tangential little product at Google.

72
00:05:05,200 --> 00:05:06,200
It was super important.

73
00:05:06,200 --> 00:05:11,320
It was really interesting because it was super important, and we would make changes that

74
00:05:11,320 --> 00:05:18,960
would have gigantic financial impacts, but you know, it's a lot of really big numbers

75
00:05:18,960 --> 00:05:22,640
and dollars that you work with, and you could be hundreds of millions of dollars change

76
00:05:22,640 --> 00:05:23,840
you'd make.

77
00:05:23,840 --> 00:05:30,440
But at the same time, you work with really small numbers where you are working on a project

78
00:05:30,440 --> 00:05:35,520
that you think is going to improve something or some other metric by 0.01 percent, and

79
00:05:35,520 --> 00:05:39,120
you'll be happy that you squeezed out another 0.1 percent, but really it translates to

80
00:05:39,120 --> 00:05:40,120
a lot of money.

81
00:05:40,120 --> 00:05:41,120
Wow.

82
00:05:41,120 --> 00:05:46,400
It's like really interesting place to work, and there's some pretty unique things about

83
00:05:46,400 --> 00:05:48,960
working on that team.

84
00:05:48,960 --> 00:05:55,920
One, those machine learning models that we ran were, I think, had some of the strictest

85
00:05:55,920 --> 00:05:59,720
requirements for stability in the industry, so, you know, we didn't want any downtime

86
00:05:59,720 --> 00:06:03,840
in the ad system, so we had a whole bunch of infrastructure to support that.

87
00:06:03,840 --> 00:06:04,840
Okay.

88
00:06:04,840 --> 00:06:07,760
Also, super large scale, super real time.

89
00:06:07,760 --> 00:06:13,680
We have to score a whole bunch of ads at query time and return to all these scores and

90
00:06:13,680 --> 00:06:20,120
run a whole crazy auction really quickly, and it's just a complex problem to try to predict

91
00:06:20,120 --> 00:06:25,800
if you're going to click on an ad, and so we tried to do a really good job of it, but

92
00:06:25,800 --> 00:06:27,360
it's always like you can always do more.

93
00:06:27,360 --> 00:06:31,920
So that, I learned a lot about machine learning on those teams, and probably set you up to

94
00:06:31,920 --> 00:06:35,280
learn a ton about the importance of platforms for machine learning.

95
00:06:35,280 --> 00:06:41,360
Yeah, we had our own infrastructure to support us in the machine learning space that helped

96
00:06:41,360 --> 00:06:45,520
us train models and evaluate them in real time and all that kind of stuff.

97
00:06:45,520 --> 00:06:51,720
So I think most importantly though, is I learned a lot about best practices on machine learning,

98
00:06:51,720 --> 00:06:57,760
and I learned that at a time when machine learning was still pretty new for a lot of people,

99
00:06:57,760 --> 00:07:05,480
and I think even today, we're at a point where best practices for machine learning aren't

100
00:07:05,480 --> 00:07:13,400
as widely established as best practices for regular software development, like checking

101
00:07:13,400 --> 00:07:18,200
your code and use Git and run tests before you submit stuff, and et cetera.

102
00:07:18,200 --> 00:07:21,200
And that kind of stuff is just not as well established for machine learning.

103
00:07:21,200 --> 00:07:27,680
So it's part of those best practices are kind of what I learned on those teams.

104
00:07:27,680 --> 00:07:31,560
And so I've been there for a while, and then I joined, about two years ago, I joined

105
00:07:31,560 --> 00:07:37,800
Uber as the product manager for what was pretty much the only machine learning team at the

106
00:07:37,800 --> 00:07:45,160
time, and we began building a machine learning platform, which today is known as Michelangelo,

107
00:07:45,160 --> 00:07:52,160
and we recently wrote a blog post and published it to explain what Michelangelo is all about.

108
00:07:52,160 --> 00:07:58,960
Yeah, basically that's a system to allow internal people within Uber to build machine learning

109
00:07:58,960 --> 00:08:06,560
systems, and deploy them, and monitor them, and maintain them at scale within Uber.

110
00:08:06,560 --> 00:08:14,160
And so our customers are teams like the team who's trying to predict ETAs, like how long

111
00:08:14,160 --> 00:08:18,000
it will take a car to get to you, or the Uber Eats team.

112
00:08:18,000 --> 00:08:20,800
Can you tell them I think they need to work on that a little bit more?

113
00:08:20,800 --> 00:08:25,760
I'm working on it, man, and we've got a whole roadmap to improve this stuff.

114
00:08:25,760 --> 00:08:35,120
But I mean, that really touches on real kind of product implications of using machine

115
00:08:35,120 --> 00:08:42,360
learning, which is like how do you communicate and design for uncertainty, because you know

116
00:08:42,360 --> 00:08:47,480
machine learning, you're inherently, you're making a prediction, and often a lot of use

117
00:08:47,480 --> 00:08:54,440
cases a system will ask for, give me your best prediction, give me one number back.

118
00:08:54,440 --> 00:08:58,040
But often a system will kind of know like a distribution, like I think it's going to

119
00:08:58,040 --> 00:09:06,040
be in this range, and so we try to, when we can, provide like a range of our confidence

120
00:09:06,040 --> 00:09:10,240
intervals for how long we think it will take some process to happen, and I think we do

121
00:09:10,240 --> 00:09:16,840
that well in Uber Eats, but the product requires a single number in ETAs, and it's hard

122
00:09:16,840 --> 00:09:17,840
to, right?

123
00:09:17,840 --> 00:09:22,760
It's funny how much of this ends up being just kind of UX UI, right?

124
00:09:22,760 --> 00:09:28,480
Like if it said, you know, four to eight minutes, it would be, I think a lot more palatable

125
00:09:28,480 --> 00:09:31,560
than four, and then eight the next time you look at it.

126
00:09:31,560 --> 00:09:32,560
Yeah, right.

127
00:09:32,560 --> 00:09:40,120
Yeah, it's also the kind of thing where we have to work with designers a lot to explain

128
00:09:40,120 --> 00:09:46,160
to them the uncertainty that comes with these machine learning systems and help them

129
00:09:46,160 --> 00:09:52,400
understand what level of confidence we actually have with these numbers, so that they can understand

130
00:09:52,400 --> 00:09:58,040
what the user experience will be from, like once they get these values, once they get

131
00:09:58,040 --> 00:10:03,160
the predictions back, but ETAs are inherently a really challenging thing.

132
00:10:03,160 --> 00:10:07,880
Like there will be construction on the street, and then your model has to adapt really

133
00:10:07,880 --> 00:10:10,720
quickly, and it's a tough engineering problem to solve.

134
00:10:10,720 --> 00:10:12,720
Yeah, yeah.

135
00:10:12,720 --> 00:10:17,880
If only users thought about how tough the engineering problems were, when they were waiting

136
00:10:17,880 --> 00:10:18,880
for their Uber.

137
00:10:18,880 --> 00:10:21,560
I think they shouldn't worry about it, and we should just make it a good experience,

138
00:10:21,560 --> 00:10:24,840
and they don't have to even consider anything about implementation.

139
00:10:24,840 --> 00:10:29,920
Should be magical, and that's kind of like what we're trying to do at Uber is make transportation

140
00:10:29,920 --> 00:10:33,880
as reliable as running water, so you don't have to worry about anything, you just open

141
00:10:33,880 --> 00:10:37,400
up the app and call it right, and the ride comes to you.

142
00:10:37,400 --> 00:10:38,400
Right, right.

143
00:10:38,400 --> 00:10:41,520
I want to talk about Michelangelo, I also want to talk about the presentation you did

144
00:10:41,520 --> 00:10:47,080
here, because you spoke here at the George and Partners conference.

145
00:10:47,080 --> 00:10:49,840
Do you talk about Michelangelo in your presentation?

146
00:10:49,840 --> 00:10:55,160
I think I intended to mention it, plug it, but I don't think I actually got to it in the

147
00:10:55,160 --> 00:10:56,160
presentation.

148
00:10:56,160 --> 00:10:57,160
Okay.

149
00:10:57,160 --> 00:11:00,080
I mean, I'm happy to chat about what that's about.

150
00:11:00,080 --> 00:11:07,760
We'll put that on the stack, also HoraVod, you've just announced HoraVod, and that came

151
00:11:07,760 --> 00:11:14,880
up in our last, the Toma Long Island meetup, someone mentioned that they saw the news,

152
00:11:14,880 --> 00:11:19,880
and we discussed that for a little bit, so I'd love to hear kind of you riff on that for

153
00:11:19,880 --> 00:11:24,280
a bit, but let's start with kind of the presentation, and we'll see where that takes us.

154
00:11:24,280 --> 00:11:25,280
Yeah, okay.

155
00:11:25,280 --> 00:11:36,720
So, I gave a 20, 30 minute talk at this conference today, and I tried to get across the

156
00:11:36,720 --> 00:11:43,920
main idea that I tried to get across in this talk, is that to actually get enterprise

157
00:11:43,920 --> 00:11:49,080
value out of using machine learning systems, a building of applying machine learning in

158
00:11:49,080 --> 00:11:56,560
your company, there's a lot more to do, and a lot more to get right than just choosing

159
00:11:56,560 --> 00:12:02,600
the right algorithm, and a lot of people focus on what's the right algorithm to use when

160
00:12:02,600 --> 00:12:09,960
you see most of the news we see about machine learning is, so and so came up with an advancement

161
00:12:09,960 --> 00:12:15,640
in AI that lets us beat this other thing, a new accuracy metric, and stuff like that,

162
00:12:15,640 --> 00:12:21,240
and that's great, and it's totally like a research frontier that's super valuable to

163
00:12:21,240 --> 00:12:27,920
have, but what practically I deal with, and a lot of the basic machine learning problems

164
00:12:27,920 --> 00:12:35,920
that we deal with day to day, in most use cases in a company, are issues that can be adequately

165
00:12:35,920 --> 00:12:42,280
handled with a very few algorithms, we can apply like grading boosted decision trees

166
00:12:42,280 --> 00:12:52,320
or random forests to a large number of classification problems, and receive acceptable results.

167
00:12:52,320 --> 00:12:57,840
And so I forget the stat, but some astounding number of Kaggle competitions are

168
00:12:57,840 --> 00:13:10,520
basically just unsumbles of them, and so if you're trying to extract value practically

169
00:13:10,520 --> 00:13:14,320
in your company and make systems that really use this stuff, the bottlenecks usually

170
00:13:14,320 --> 00:13:19,400
are not there in which algorithm you're choosing, maybe if you're building self-driving

171
00:13:19,400 --> 00:13:24,120
cars and stuff where you're really pushing the limits of what AI can do today, then

172
00:13:24,120 --> 00:13:30,160
you're dealing with not a lot more, but we deal with a lot more challenges related

173
00:13:30,160 --> 00:13:37,560
to infrastructure and data, and things like that that are closer to typical engineering

174
00:13:37,560 --> 00:13:43,360
problems, that people usually have to put a little bit more effort to think about how

175
00:13:43,360 --> 00:13:49,960
to do that correctly in the machine learning paradigm, so in my talk, I was talking about

176
00:13:49,960 --> 00:13:58,560
a few different areas in which there are pitfalls where people typically don't, there could

177
00:13:58,560 --> 00:14:03,040
be things that if you don't have experience, you don't think about this ahead of time

178
00:14:03,040 --> 00:14:09,000
and you're not planning on dedicating time to solving these problems, or it could just

179
00:14:09,000 --> 00:14:13,440
be tricky things to get right even if you know what you're doing, and so one area is

180
00:14:13,440 --> 00:14:18,280
technically just infrastructure, solving the infrastructure that goes around your machine

181
00:14:18,280 --> 00:14:22,280
learning system, so if you're in grad school and you're learning machine learning, you're

182
00:14:22,280 --> 00:14:29,520
a data scientist, as that is, you get really good at training a Python model, or an R model

183
00:14:29,520 --> 00:14:37,000
here, output you're deliverable from that often is a scikit-learn Python object, or maybe

184
00:14:37,000 --> 00:14:44,400
something you can export to, like a PMML format, but that's just one part of the story.

185
00:14:44,400 --> 00:14:49,800
Even if you have a great model, there's a lot more infrastructure around your machine

186
00:14:49,800 --> 00:14:54,080
learning system that you need to integrate well, integrate with wealth.

187
00:14:54,080 --> 00:14:59,280
For example, you need a logging system that is aware of the machine learning use case

188
00:14:59,280 --> 00:15:04,520
that can store historical data in a way that's compatible with what your machine learning

189
00:15:04,520 --> 00:15:08,200
system wants to do with that data, and the future one is training a model on it, or you

190
00:15:08,200 --> 00:15:15,960
need a monitoring system to be able to accurately monitor and evaluate your model's accuracy

191
00:15:15,960 --> 00:15:20,240
over time, so you can determine if your model that you're using in production becomes

192
00:15:20,240 --> 00:15:24,840
stale, and you need to either retrain it, or if you have some other system that's automatically

193
00:15:24,840 --> 00:15:29,080
retraining things, you need a whole bunch of infrastructure to manage that, automatically

194
00:15:29,080 --> 00:15:36,800
train, evaluate, deploy a lot of that stuff, and so my point there is that there's a large

195
00:15:36,800 --> 00:15:41,960
engineering investment to go alongside the basic work on the algorithms, that a lot

196
00:15:41,960 --> 00:15:46,480
of people overlook, and when people think like machine learning is magic, it's really

197
00:15:46,480 --> 00:15:51,800
not magic, you still got to put a lot of work into it, so that's kind of one area, is

198
00:15:51,800 --> 00:16:02,600
that lead you to, like is there for a mature company, or a company who's doing machine learning

199
00:16:02,600 --> 00:16:07,840
a scale, is there like a magic ratio of data scientists to engineers?

200
00:16:07,840 --> 00:16:15,560
That's a good question, well, you know, I'm building Michelangelo, which is, we have

201
00:16:15,560 --> 00:16:23,440
almost exclusively engineers building that, and our customers are a mix of data scientists

202
00:16:23,440 --> 00:16:28,600
and engineers, so the teams that are using our system to actually build these models and

203
00:16:28,600 --> 00:16:34,120
run them in production, it's usually like half and half engineers in data scientists,

204
00:16:34,120 --> 00:16:40,160
and there's a blurry line between engineer and data scientists sometimes, and so there's

205
00:16:40,160 --> 00:16:44,680
often a lot of engineers who just know enough about machine learning that they don't need

206
00:16:44,680 --> 00:16:48,560
to consult data scientists and they can understand enough about these models to be confident

207
00:16:48,560 --> 00:16:54,800
about them, and it doesn't frequently, I don't see it going the other way too much, there's

208
00:16:54,800 --> 00:17:00,320
not often like a data scientist who's running like a production system, but I mean, that

209
00:17:00,320 --> 00:17:05,960
can happen, that wouldn't surprise me if I saw that, so it seems to be that they'll

210
00:17:05,960 --> 00:17:10,360
be paired, so you have data scientists working with kind of this new breed of engineer called

211
00:17:10,360 --> 00:17:15,240
the machine learning engineer that has kind of that base level understanding of models.

212
00:17:15,240 --> 00:17:16,240
Are you?

213
00:17:16,240 --> 00:17:23,480
So what we have at Uber is kind of like the nucleus of a team is an engineer, the team

214
00:17:23,480 --> 00:17:32,080
of engineers, usually one product manager, and a couple of data scientists, and being

215
00:17:32,080 --> 00:17:36,000
an engineer is not just one skill set, and so there are engineers who are very data-focused

216
00:17:36,000 --> 00:17:41,080
engineers or there's UI engineers and the whole spectrum of it, and so usually the folks

217
00:17:41,080 --> 00:17:48,280
that partner most closely with data scientists are the more data-minded engineers, but they're

218
00:17:48,280 --> 00:17:53,400
not necessarily folks with machine learning specialties, and even on my team on a Michelangelo

219
00:17:53,400 --> 00:17:59,320
team, most of the work that you end up doing to run a production machine learning system

220
00:17:59,320 --> 00:18:05,160
is data pipeline, work floor stuff, and so I would say probably even most of our engineers

221
00:18:05,160 --> 00:18:11,760
are just like data engineers, infrastructure engineers, systems engineers, folks that have

222
00:18:11,760 --> 00:18:17,120
picked up a lot of important machine learning concepts, but do not have like PhDs in machine

223
00:18:17,120 --> 00:18:24,240
learning or something like that. And so the idea behind Michelangelo is that you can invest

224
00:18:24,240 --> 00:18:30,360
in building this platform, and then the data scientists can focus on the machine learning

225
00:18:30,360 --> 00:18:34,720
and not have to think about the logging and the monitoring and all of these, you know,

226
00:18:34,720 --> 00:18:37,960
the life cycle of it deploying and managing a model of production.

227
00:18:37,960 --> 00:18:44,720
Yeah, exactly. So, you know, about two years ago when we started Michelangelo, there were

228
00:18:44,720 --> 00:18:50,880
a lot of teams who either were trying to put machine learning systems into production

229
00:18:50,880 --> 00:18:56,760
and building their own production stack for it, and different teams were doing the similar

230
00:18:56,760 --> 00:19:02,840
thing, but they were all these bespoke solutions that we're not, you know, if you're building

231
00:19:02,840 --> 00:19:09,640
something for a specific use case, it's unlikely to be supported with a proper engineering

232
00:19:09,640 --> 00:19:13,120
investment, and it's unlikely to generalize well when you want to expand your use case

233
00:19:13,120 --> 00:19:21,400
and somewhat brittle and stuff like that. So there was a clear, a clear need for a platform

234
00:19:21,400 --> 00:19:27,320
to help these teams put something into production and manage something in production, but, you

235
00:19:27,320 --> 00:19:33,960
know, our goal is to support everything from the exploration side of the data science

236
00:19:33,960 --> 00:19:39,400
workflow all the way to production, managing something, maintaining something in production

237
00:19:39,400 --> 00:19:45,760
and a whole operational side of it. I would say that we, our particular focus is putting

238
00:19:45,760 --> 00:19:53,760
things into production. It's a much harder problem to solve that helping the teams would

239
00:19:53,760 --> 00:19:57,480
like the model exploration stuff. You know, data scientists have their own tools that

240
00:19:57,480 --> 00:20:05,600
they like using, and it's hard to do much better than psychic learn and just letting

241
00:20:05,600 --> 00:20:10,280
a data scientist just iterate really rapidly with the tools that they're used to, but when

242
00:20:10,280 --> 00:20:16,280
it comes time for them to like begin using that system in production, the, and usually

243
00:20:16,280 --> 00:20:20,360
need to rely on an engineering solution, and that's where our platform comes in.

244
00:20:20,360 --> 00:20:24,560
Okay. And so you mentioned logging, you mentioned monitoring, are there other kind of main

245
00:20:24,560 --> 00:20:29,880
features of the platform? Well, I was suggesting that there's, there's actually like all kinds

246
00:20:29,880 --> 00:20:35,840
of infrastructure, whether it's a machine learning platform or not, that you want to have

247
00:20:35,840 --> 00:20:41,280
aware of the machine learning use case. And there's also types of infrastructure that you

248
00:20:41,280 --> 00:20:45,480
would, that is machine learning specific that you might need to build. So there's a thing

249
00:20:45,480 --> 00:20:51,080
that we built, which is a feature store. And so that allows teams to share and discover

250
00:20:51,080 --> 00:20:58,280
features. So if you are building the model that predicts how long a restaurant's going

251
00:20:58,280 --> 00:21:04,960
to take to prepare meal for Uber Eats, maybe the model that I'm building would be able

252
00:21:04,960 --> 00:21:09,200
to benefit from some of the features that you're using in your model. And so it took

253
00:21:09,200 --> 00:21:12,840
way for us to share features. So I don't have to duplicate all the data pipelines to create

254
00:21:12,840 --> 00:21:18,880
those features. And is that, does something like that exist as like a kind of a metadata

255
00:21:18,880 --> 00:21:25,760
catalog that is, you know, mirrors a data store like a HDFS or something like that?

256
00:21:25,760 --> 00:21:31,160
Yeah. So for us, practically, we have, you can do a few things, but basically we manage

257
00:21:31,160 --> 00:21:35,640
metadata, metadata layer to understand where these features are stored. And it is all

258
00:21:35,640 --> 00:21:40,400
in like high HDFS and other people can contribute to it. Or we have this other way where we

259
00:21:40,400 --> 00:21:47,960
manage some core set of features. And, and yeah, we're still figuring out like how we can

260
00:21:47,960 --> 00:21:54,680
make that most usable to other teams. Right. And really nail the, the sharing of this

261
00:21:54,680 --> 00:22:00,760
data use case because there's, it's, it's, I mean, like it's super tricky because if

262
00:22:00,760 --> 00:22:05,840
you're building a production system, you are going to be hesitant to rely on a data

263
00:22:05,840 --> 00:22:10,560
set that you don't own. Right. And you don't know if that person is going to change that

264
00:22:10,560 --> 00:22:14,120
how it's calculated the next day and it messes up your system. So we're trying to like

265
00:22:14,120 --> 00:22:20,240
figure out the right contracts are to guarantee certain data stability and quality and stuff

266
00:22:20,240 --> 00:22:26,120
like that. Like how far along are you on? We have what it even means to have a contract.

267
00:22:26,120 --> 00:22:32,640
Like it's, it's hard enough for services, which have a fairly, you know, small surface

268
00:22:32,640 --> 00:22:40,040
area. Right. Of the, the API, but the, it strikes me that data has a much more kind

269
00:22:40,040 --> 00:22:44,520
of expensive surface area around what you need to try to define a contract. Yeah. I,

270
00:22:44,520 --> 00:22:51,840
like, I, I haven't spent enough time on this area to, to have really figured out where,

271
00:22:51,840 --> 00:22:56,040
where we actually want it to be. So I can't even say if we're 20% there or 50% there,

272
00:22:56,040 --> 00:23:00,040
you know, like, because I really don't know how much you want to do in this space. So

273
00:23:00,040 --> 00:23:03,840
that's, that's tricky. I think it's an area that we probably want to prioritize more.

274
00:23:03,840 --> 00:23:07,920
Yeah. And like another thing that comes along with that is, you know, that's HDFS hive.

275
00:23:07,920 --> 00:23:12,280
That's all offline stuff that's ready for batch processing. Right. But there's a whole

276
00:23:12,280 --> 00:23:18,000
other side to this where if you're running a online prediction model, you want these features

277
00:23:18,000 --> 00:23:24,800
to be generated in real time. And so you might have some real time stream processing

278
00:23:24,800 --> 00:23:29,480
stuff that will calculate those same features. And you want them to be calculated in the

279
00:23:29,480 --> 00:23:35,560
same way as you calculated your offline batch data that you trained your model on. But

280
00:23:35,560 --> 00:23:38,560
you're calculating these features in real time and then making them available to your

281
00:23:38,560 --> 00:23:43,200
model so your model can score. Right. You know, and this could be like understanding how

282
00:23:43,200 --> 00:23:49,360
many, a good example is how many meals has this restaurant? How many orders does this restaurant

283
00:23:49,360 --> 00:23:54,280
gotten in the past 30 minutes? And that might be like a window figure out how you're going

284
00:23:54,280 --> 00:24:00,240
to accumulate. And it's like you, you've got your, this repository that says what your

285
00:24:00,240 --> 00:24:05,040
features are. And then you've got your, you know, you're kind of your meta meta that's

286
00:24:05,040 --> 00:24:12,320
like how you derive the features from the underlying stream of data. Yeah. It's, it's a, it's

287
00:24:12,320 --> 00:24:18,840
and so you can imagine there's a large activation energy to set up a system like that. And so

288
00:24:18,840 --> 00:24:24,440
that's part of the value of having a platform that builds all this infrastructure and plugs

289
00:24:24,440 --> 00:24:28,160
it all together nicely. So you just need to provide a configuration and say, hey, use this

290
00:24:28,160 --> 00:24:33,360
data and give me this real time feature. Right. And that, you know, we found that that

291
00:24:33,360 --> 00:24:38,840
kind of infrastructure has really lowered the activation energy. So we've allowed like

292
00:24:38,840 --> 00:24:42,360
those teams were building these spoke solutions. We built a good system for them and they've

293
00:24:42,360 --> 00:24:47,880
kind of pivoted onto our system and begun using our system. But also there's teams that

294
00:24:47,880 --> 00:24:51,920
before didn't have the resources to even take on a machine learning project, but they've

295
00:24:51,920 --> 00:24:57,920
known that they want to have some kind of predictive solution in their product. And, and the

296
00:24:57,920 --> 00:25:02,640
lower activation energy to get started has kind of helped them unlock that and they've been

297
00:25:02,640 --> 00:25:06,840
able to fund a product because the, or such a project because the cost has come down

298
00:25:06,840 --> 00:25:12,840
so much. Nice. Nice. Now, so we've talked primarily around what I kind of roughly think

299
00:25:12,840 --> 00:25:19,200
of as engineering facing features and infrastructure. But there's a whole set of operational I, you

300
00:25:19,200 --> 00:25:26,160
know, operationalizing features. You may remember me getting into part of this conversation

301
00:25:26,160 --> 00:25:30,400
in an interview with Jennifer, Jennifer Prinky, who is at Walmart Labs at the time, not

302
00:25:30,400 --> 00:25:35,360
too long ago. And kind of the direction that they were heading. And I could see Michelangelo

303
00:25:35,360 --> 00:25:40,080
going in this direction. If it's not already is, you know, you've got a model in production.

304
00:25:40,080 --> 00:25:47,080
Like, are you tracking kind of either statistical drift of the inputs or, you know, model accuracy,

305
00:25:47,080 --> 00:25:52,440
you know, decreasing over time. And then like automatically triggering or at least like

306
00:25:52,440 --> 00:25:57,880
setting a ticket or something like that to reevaluate the model. So the kind of the

307
00:25:57,880 --> 00:26:03,280
full like service level impact of these models. Do you get into that at all? So this is,

308
00:26:03,280 --> 00:26:07,600
we do get kind of into it. And this is an area that we're really trying to focus on over

309
00:26:07,600 --> 00:26:12,920
the next couple of months is build out a lot of, there's a lot of things you would want

310
00:26:12,920 --> 00:26:18,600
to do to operationalize and maintain such a model. So things like tracking the data quality

311
00:26:18,600 --> 00:26:24,560
both in and out of your model. So you might want to do things like track feature distributions

312
00:26:24,560 --> 00:26:29,280
over time and real time. So all the data is coming into your model. Does it look different

313
00:26:29,280 --> 00:26:35,560
in this five minutes than it looked in right past? And you might compare that to the distribution

314
00:26:35,560 --> 00:26:39,760
of data that you saw an hour ago, but also like the historical distribution of like the past

315
00:26:39,760 --> 00:26:44,800
two years of data like that. And right. Has that changed? I mean, you just start

316
00:26:44,800 --> 00:26:50,040
in a rough way. I go ahead. When we were talking about, you know, data and contracts

317
00:26:50,040 --> 00:26:54,960
around data and models like one of my thoughts was, you know, is part of that contract like

318
00:26:54,960 --> 00:26:59,640
a statistical distribution of the data at a point in time so that you can refer back

319
00:26:59,640 --> 00:27:03,720
to, you know, what you thought that data looked like when you built the model. Right. So

320
00:27:03,720 --> 00:27:09,200
you can, so that's not something that we would necessarily want the customer to explicitly

321
00:27:09,200 --> 00:27:14,880
provide, but it would be something that we may be able to extract from the training data.

322
00:27:14,880 --> 00:27:18,640
So we might be able to say like, look, this is the distribution of the data we saw at

323
00:27:18,640 --> 00:27:25,000
training time. Right. It's kind of save a snapshot of the summer summary statistics and then

324
00:27:25,000 --> 00:27:30,240
compare our normal like the data we see in real time to the statistics. And so sometimes

325
00:27:30,240 --> 00:27:35,320
like, you know, your training data, you might rebalance classes and reweight things and stuff

326
00:27:35,320 --> 00:27:40,240
like that. And so there's some complication to take that stuff into account. And so this

327
00:27:40,240 --> 00:27:43,320
is some of the challenges that we're working through right now. Okay. But then there's also

328
00:27:43,320 --> 00:27:48,240
the other side of like doing the same data quality checks on the output of your model.

329
00:27:48,240 --> 00:27:55,040
Like the predictions and, you know, doing the same statistics tracking stuff. And you would

330
00:27:55,040 --> 00:28:02,520
also want to have alerting happen on that kind of thing. And that's all part of this like

331
00:28:02,520 --> 00:28:09,000
larger infrastructure integration story because your company should have a way to, you

332
00:28:09,000 --> 00:28:13,400
know, like set up alerts for production systems. And so you just want to integrate with the

333
00:28:13,400 --> 00:28:21,360
basic stuff that you have. Cool. So we didn't get very far in your presentation. So I was

334
00:28:21,360 --> 00:28:26,560
talking a lot about like infrastructure side, which is the summary is you got to do a lot

335
00:28:26,560 --> 00:28:33,280
of engineering work before you get into to put one of these models into production properly.

336
00:28:33,280 --> 00:28:38,800
There's also another part of it, which is, you know, building tools to help the data

337
00:28:38,800 --> 00:28:45,200
scientists do their job properly, both correctly and productively. And so, you know, there's

338
00:28:45,200 --> 00:28:52,200
many parts of the data science workflow. And, you know, if you leave data scientists,

339
00:28:52,200 --> 00:28:57,480
if you don't support them with the proper tools, then they may be building systems in a

340
00:28:57,480 --> 00:29:01,280
non reproducible way. So there's not a way to like recreate the model. If you need to

341
00:29:01,280 --> 00:29:06,120
retrain it, it may not be all version and like checked in and source control and stuff

342
00:29:06,120 --> 00:29:15,240
like that. And you may not even have standardized ways to evaluate these models. So you may just

343
00:29:15,240 --> 00:29:22,040
waste a lot of cycles comparing data scientists as evaluations to data scientists, bees evaluations.

344
00:29:22,040 --> 00:29:26,040
Because there's not a common set of metrics that everybody's agreed on ahead of time. So

345
00:29:26,040 --> 00:29:31,160
there's some work to provide some infrastructure for them as well. And so the future store is

346
00:29:31,160 --> 00:29:37,480
one example. It gives a kind of like common source of truth. But beyond the technical stuff,

347
00:29:38,440 --> 00:29:46,520
there's a lot of organizational considerations when you're trying to make the most out of

348
00:29:46,520 --> 00:29:53,000
machine learning in your company. So, you know, one thing that's important to understand is that

349
00:29:53,720 --> 00:29:59,720
not everyone's an expert at machine learning. And people come with different levels of

350
00:29:59,720 --> 00:30:06,760
abilities and background knowledge of machine learning. So that leads to a lot of times when

351
00:30:07,480 --> 00:30:11,720
people have false conclusions about how machine learning can be applied to their problem. So

352
00:30:11,720 --> 00:30:15,720
some people will tell me, well, I don't think machine learning can help with my problem at all.

353
00:30:15,720 --> 00:30:20,760
So we don't need to like have a collaboration. And then when we look into it, we realize, oh,

354
00:30:20,760 --> 00:30:25,160
well, actually the main problem you're trying to solve is pretty appropriate for a machine learning

355
00:30:25,160 --> 00:30:31,560
solution. Or there's folks who have told me have kind of conveyed that they believe machine learning

356
00:30:31,560 --> 00:30:35,320
is basically magic. And they think it can solve all of their problems. And that's a kind of thing

357
00:30:35,320 --> 00:30:42,040
where you really want to understand those expectations and adjust those expectations to be

358
00:30:42,040 --> 00:30:48,920
much more realistic. So you don't have a problem down the line of completely missed expectations.

359
00:30:48,920 --> 00:30:55,800
But then there's also people who are almost like overconfident about what they know about

360
00:30:55,800 --> 00:31:01,640
machine learning and they request specific algorithms or specific implementation. And the

361
00:31:01,640 --> 00:31:07,560
example I get in the talk with someone once asked me mentioned that they need unsupervised

362
00:31:09,080 --> 00:31:14,600
online, deep reinforcement warning or something like that. And I was like, okay,

363
00:31:14,600 --> 00:31:19,720
it's like, I don't know if that's a thing, but we should understand where your problem is. So

364
00:31:19,720 --> 00:31:24,120
so like the way to handle most of these situations is to not really focus on the machine learning

365
00:31:24,120 --> 00:31:28,600
part of it, but to really just like talk to these people and understand what the business

366
00:31:28,600 --> 00:31:33,720
problem they're trying to solve is and not involve machine learning vocabulary. And you know,

367
00:31:33,720 --> 00:31:38,600
I'm the product manager of our team. So part of my role is to understand the business problem

368
00:31:38,600 --> 00:31:45,160
and find a way to translate it into a machine learning problem, something that we would be able

369
00:31:45,160 --> 00:31:49,560
to solve and understand also the need to be able to solve or something that you'd be able to support

370
00:31:49,560 --> 00:31:55,720
with both. So we have we also have an applied machine learning team that we that we

371
00:31:56,920 --> 00:32:01,480
is like a tiger team that supports the different engineering groups. Yeah, it's kind of like a

372
00:32:01,480 --> 00:32:09,160
set of machine learning specific data scientists that these folks may be able to like put on loan

373
00:32:09,160 --> 00:32:13,800
to a team to help them solve a machine learning problem if they don't have the resources when it's

374
00:32:13,800 --> 00:32:18,760
a particularly relevant and important machine learning problem. So yeah, so like I hope kind of

375
00:32:18,760 --> 00:32:23,800
coordinate these these types of problems. And yeah, so we figure out like if it's a relevant

376
00:32:23,800 --> 00:32:29,240
problem and how we might be able to solve it. And ultimately I'm trying to understand the

377
00:32:29,240 --> 00:32:35,160
the business metrics that these people are trying to optimize for and understand the mechanisms

378
00:32:35,160 --> 00:32:39,080
that the whole product works. So what do they think actually affects those business metrics?

379
00:32:39,080 --> 00:32:45,240
And then I'm trying to translate that problem into for for our data scientists or for even for

380
00:32:45,240 --> 00:32:50,600
their data scientists who might need help with it. Like a machine learning problem that

381
00:32:51,640 --> 00:32:58,840
where if a data scientist is optimizing for precision recall or all the other kind of data science

382
00:32:58,840 --> 00:33:03,560
metrics that they would be used to that are related to machine learning as they optimize for those

383
00:33:03,560 --> 00:33:09,640
those would likely be an appropriate proxy for optimizing for these end business metrics. And so

384
00:33:09,640 --> 00:33:15,960
it doesn't always work out perfectly. But it's a good way to like start things off. And ultimately

385
00:33:15,960 --> 00:33:21,560
you want to have people who understand the whole problem end to end to really like think things

386
00:33:21,560 --> 00:33:29,160
through think things through. But it's a good way to get started in that kind of area. So like talking

387
00:33:29,160 --> 00:33:38,040
to people in the right at the right level of like vocabulary, you know focusing on the business

388
00:33:38,040 --> 00:33:43,240
problems, expectations making sure they don't think that machine learning's magic setting that

389
00:33:43,240 --> 00:33:52,200
properly. Another thing that I found useful is going out of my way to find a senior person in

390
00:33:52,200 --> 00:33:57,800
the company who really understands machine learning or like taking the time out to educate them

391
00:33:57,800 --> 00:34:03,320
about machine learning. So I can always have them on my side if I really need to ever need to defer

392
00:34:03,320 --> 00:34:09,000
to someone more senior. I know that there's someone there who will will either be sufficiently

393
00:34:09,000 --> 00:34:13,720
technically competent to be able to make the right call or that I've spent enough time with so

394
00:34:13,720 --> 00:34:18,840
that they'll trust me to make the call when it comes to that. So that I found that to be useful.

395
00:34:19,560 --> 00:34:26,040
And then another thing that is also useful for figuring out how to make the most out of machine

396
00:34:26,040 --> 00:34:30,200
learning is realizing that you're not going to have a machine learning, you're not going to have

397
00:34:30,200 --> 00:34:35,080
infinite machine learning experts. And so you have to find a way to make the most out of the limited

398
00:34:35,080 --> 00:34:39,880
number of machine learning experts you have. And what's the answer to that challenge? It's

399
00:34:39,880 --> 00:34:43,560
like how do you set up your org, right? It's like how do you how do you distribute your machine

400
00:34:43,560 --> 00:34:48,760
learning folks across the different teams? And there's different ways you can do it. So I mentioned

401
00:34:48,760 --> 00:34:57,320
that we have this applied group which is kind of like a solutions team in some sense where they

402
00:34:57,320 --> 00:35:04,920
they're a team of a few data scientists who are experts at machine learning and they kind of

403
00:35:05,720 --> 00:35:10,280
are out on loan to different teams that need help that don't have the machine learning expert

404
00:35:10,280 --> 00:35:14,360
keys. And so you can imagine there's these different engineering teams and then there's one

405
00:35:14,360 --> 00:35:19,320
kind of cluster of machine learning people on a separate team that loosely interacted those teams.

406
00:35:19,320 --> 00:35:24,040
But another model is you could just embed one machine learning person or one or two on each

407
00:35:24,040 --> 00:35:29,160
of those teams. And so that model is slightly more difficult to scale because you have a new

408
00:35:29,160 --> 00:35:34,600
team and then you need another machine learning person. And so I think we've experimented with

409
00:35:34,600 --> 00:35:41,320
different models. Different models will work well for different size companies. Often it's not

410
00:35:41,320 --> 00:35:47,240
just a binary thing but there could be like a hybrid where you might lean more to one side or another

411
00:35:47,240 --> 00:35:52,360
and in practice we might have things where our applied team might start off a project but then

412
00:35:52,360 --> 00:35:56,840
hand off a project to a data scientist on a product team at some point. So like thinking those

413
00:35:56,840 --> 00:36:02,520
things through explicitly ahead of time and being realistic about how many resources related to

414
00:36:02,520 --> 00:36:07,880
machine learning that you have that's pretty valuable. And so that kind of covers the organizational

415
00:36:08,520 --> 00:36:12,680
things you want to think about when you're launching machine learning systems. And then

416
00:36:13,640 --> 00:36:21,000
finally just like team focused things which are like how do you when you're running a machine

417
00:36:21,000 --> 00:36:27,080
learning project. What can you do to like organize your machine learning project to be set up for

418
00:36:27,080 --> 00:36:31,960
success. And there's a few things that are kind of just like questions you want to ask yourself

419
00:36:31,960 --> 00:36:37,880
and like rules of thumb you want to apply. So one is about like not artificially encapsulating

420
00:36:37,880 --> 00:36:44,040
the machine learning specific part separately from the non machine learning specific part of your

421
00:36:44,040 --> 00:36:49,080
project. Like you don't want the person who's building the model who's generating these scores

422
00:36:49,080 --> 00:36:53,400
to not know anything about how these scores are being used. And so there's a lot of

423
00:36:54,840 --> 00:36:59,640
like there's a lot of ways that these scores can be misused and a lot of like nuance and how

424
00:36:59,640 --> 00:37:06,280
these scores are generated. And we talked about the the inverse of that which is the designers

425
00:37:06,280 --> 00:37:11,640
and the app teams not really understanding the scores and the probabilistic nature of the scores.

426
00:37:11,640 --> 00:37:20,360
Yeah. So that's that's part of like how you would design a project to to a product rather to

427
00:37:20,360 --> 00:37:26,280
take make actually relevant use of the machine learning scores that you're providing. And then

428
00:37:26,280 --> 00:37:32,680
there's other examples of this which are more like you know your scores can be used in a way

429
00:37:32,680 --> 00:37:39,800
that they they were not intended to be used. And when they can even be used in a way that can

430
00:37:39,800 --> 00:37:43,480
harm your machine learning model. If you have a certain feedback loop where you know if you're

431
00:37:43,480 --> 00:37:50,840
trying to predict churn for example and then someone uses these scores to to provide some

432
00:37:50,840 --> 00:37:55,080
treatment to the people who are most likely to churn. And then they solve the churn problem for

433
00:37:55,080 --> 00:38:01,000
the 10% most likely to churn people. Then those people didn't churn. And then when you go to

434
00:38:01,000 --> 00:38:06,360
retrain your model next month or two months from now or whenever you'll not have those labels

435
00:38:06,360 --> 00:38:11,080
that those people churned. And so your training data is going to be kind of messed up. So that's not

436
00:38:11,080 --> 00:38:16,280
a it's not that's not like a particularly hard problem. But you just have to if you don't think

437
00:38:16,280 --> 00:38:19,960
about that ahead of time if you're not thinking this problem through end to end and you're not aware

438
00:38:19,960 --> 00:38:24,920
of how these scores are being used. Then you may not include that in your design of the whole system.

439
00:38:24,920 --> 00:38:28,360
And so your whole project might be flawed from the beginning. And there's a solution there to

440
00:38:28,360 --> 00:38:33,320
flush your your labels when you make changes like that. There's other things you could do. One could

441
00:38:33,320 --> 00:38:41,320
be just have a holdout set. So you just don't you never apply treatments to some x% of the people

442
00:38:41,320 --> 00:38:46,040
that you're making predictions for. So then you can always reserve them as like the control,

443
00:38:46,040 --> 00:38:52,840
the untouched people that you can train your model on in the future. Or you can even like a more

444
00:38:52,840 --> 00:38:59,240
advanced way is find a way to include that treatment in your modeling in the future. So your model

445
00:38:59,240 --> 00:39:03,880
is aware that maybe these people didn't turn. But it was because they got this treatment. And so

446
00:39:04,680 --> 00:39:08,520
this kind of that has more of a feedback loop. And this is more complicated. You need to think

447
00:39:08,520 --> 00:39:15,960
about it more deeply. But overall like I'm on the team side, it's you really want your folks that

448
00:39:15,960 --> 00:39:22,600
who have who really have an attitude of ownership of these systems to be the most involved here.

449
00:39:22,600 --> 00:39:28,360
You want them to be kind of almost like borderline paranoid about the the operation of these systems

450
00:39:28,360 --> 00:39:34,280
and asking questions like if the world changes, is my model going to be able to react to it?

451
00:39:34,280 --> 00:39:41,320
And how will my model react to it? Or like if how a feature is computed by the logging people

452
00:39:42,440 --> 00:39:47,480
or if it's how it's recorded changes slightly like the user different format to the store int now

453
00:39:47,480 --> 00:39:52,680
instead of a double. Is that going to mess up my number or my predictions? And how can I prevent that

454
00:39:52,680 --> 00:39:58,760
to happen from happening? Or like how will I notice if something's wrong or any number of things

455
00:39:58,760 --> 00:40:04,840
or even even like on these different dimensions like legal and cultural things like based on how

456
00:40:04,840 --> 00:40:11,080
these predictions are being used. Is there any bias that's inherent in this model? And will that have

457
00:40:11,080 --> 00:40:17,880
some impact in how the model is perceived like legally or culturally? And that's always a tricky

458
00:40:17,880 --> 00:40:22,520
problem. And often not just one person can think through all of these problems and you want

459
00:40:26,040 --> 00:40:30,040
like a kind of like a good citizenship attitude because no one's particularly

460
00:40:30,040 --> 00:40:33,960
singularly responsible for these issues. We just want people to really think things through

461
00:40:33,960 --> 00:40:38,840
and really own the system. So they are always feeling like they're doing the right thing here as well.

462
00:40:38,840 --> 00:40:44,040
Okay. So we covered I think the topics in your presentation. I wanted to make sure we hit

463
00:40:44,040 --> 00:40:52,360
on Horavod before we move on. What's that all about? So we have a lot of data over and we've been

464
00:40:52,360 --> 00:41:00,040
trying to get started running distributed TensorFlow. And we ran into a lot of issues. I'm just setting

465
00:41:00,040 --> 00:41:08,040
up the parameter server constellation and connecting everything together in the proper way. And

466
00:41:08,040 --> 00:41:15,320
we've got an engineer, Alex Sergeev, who's awesome. He's a genius. And he found a way to kind of

467
00:41:15,320 --> 00:41:22,360
improve upon the Bidews, a paper that Bidew published where they open source some code to

468
00:41:23,080 --> 00:41:31,400
do a different way to distribute the work in TensorFlow. And basically ripping out the guts of

469
00:41:31,400 --> 00:41:36,760
the distributed stuff and replacing it with open MPI. Exactly. Hearkens back to the great computing

470
00:41:36,760 --> 00:41:44,600
days. Yeah. So it uses like the ring all reduced methods. And you have, you know, you have, if you

471
00:41:44,600 --> 00:41:52,360
have n nodes, each node is only sending data to one other node and receiving from one other node.

472
00:41:52,360 --> 00:41:59,720
And there's no parameter servers. Only all the workers are communicating with other nodes and

473
00:41:59,720 --> 00:42:08,200
are averaging up gradients in that node itself. And so you kind of set up this ring of connections

474
00:42:08,200 --> 00:42:12,920
between like a circle, like an online them all in a circle. And so then if you pass one message

475
00:42:12,920 --> 00:42:20,680
through it will, and you keep iterating your message passing eventually, one signal will get

476
00:42:20,680 --> 00:42:25,000
all the way around and be distributed to all the nodes. It's much easier to understand visually.

477
00:42:25,000 --> 00:42:34,120
We run into that problem. But Alex built this awesome system that is called Horavod. Horavod is

478
00:42:34,120 --> 00:42:40,200
a Russian dance that people link arms in the circle. And so it's kind of like the right imagery

479
00:42:40,200 --> 00:42:48,920
there. And so he's been applying it to a lot of our distributed learning problems. And he's found

480
00:42:48,920 --> 00:42:56,200
that it's much easier to set up a distributed learning job in TensorFlow. So, you know, there's a

481
00:42:56,200 --> 00:43:02,200
lot of boilerplate code that you need to have in a existing parameter server TensorFlow paradigm

482
00:43:02,920 --> 00:43:08,920
to like allow it to all be distributed. And he's figured out a way to have only four lines of code

483
00:43:08,920 --> 00:43:14,840
that you need to add to a TensorFlow script to distribute it. And I've heard not great things about

484
00:43:14,840 --> 00:43:22,280
trying to distribute out of the box TensorFlow. It's pretty challenging. I don't know exactly what

485
00:43:22,280 --> 00:43:28,040
Google does internally. But I think they're probably working on an improvement.

486
00:43:28,040 --> 00:43:33,480
Lewis, from what I've heard, they're trying to, or at least, you know, someone is thinking that at

487
00:43:33,480 --> 00:43:37,880
some point, you know, they've got this Kubernetes thing. It's kind of good at distributed compute.

488
00:43:37,880 --> 00:43:42,120
They've got this TensorFlow thing. Hey, you know, somehow we can kind of get the chocolate and

489
00:43:42,120 --> 00:43:48,440
then the peanut butter together. Yeah, yeah. So I mean, I'm sure there's a whole team working on

490
00:43:48,440 --> 00:43:54,200
that in Google. And so this was just like Alex on our team needed to solve a problem for himself

491
00:43:54,200 --> 00:43:59,560
immediately. And so he built this system. And it's really easy. If you check out the blog posts we

492
00:43:59,560 --> 00:44:06,680
have to search Horavod. And this blog post is open source. And it's open source. You can just

493
00:44:06,680 --> 00:44:11,320
go download it and check out the sample code. And it's literally just add four lines to your TensorFlow

494
00:44:11,320 --> 00:44:18,520
script. And you're good to go. And here you describe it. It goes higher up and ripping out the

495
00:44:18,520 --> 00:44:24,360
guts than I even thought. Like I thought it was just like using kind of MPIs like a low,

496
00:44:24,360 --> 00:44:29,640
low level message passing thing. But it's also kind of changing the way gradients are distributed

497
00:44:30,360 --> 00:44:35,640
across the system. Yeah, it touches various layers there. So Alex has been able to use it too.

498
00:44:35,640 --> 00:44:43,240
Let me see. I think there was some model that took maybe like 10 days to train. And on our cluster

499
00:44:44,040 --> 00:44:48,760
with I don't remember the details. But in some real practical application, he was able to train

500
00:44:48,760 --> 00:44:54,280
this model in seven hours instead. Instead of how many nodes. I don't know how many he used in

501
00:44:54,280 --> 00:44:59,480
this particular thing. But we've trained stuff up to maybe 128 GPUs. That's right. Another that

502
00:44:59,480 --> 00:45:06,120
I mentioned that I remember the the graph that was in the blog post. And you get pretty darn

503
00:45:06,120 --> 00:45:12,200
close to ideal multipliers on the scale. It was pretty impressive. Yeah, it's pretty, I mean,

504
00:45:12,200 --> 00:45:20,520
it's a very large speed up for really big applications. And you know, you can imagine if you take

505
00:45:20,520 --> 00:45:23,880
10 days to train your model and suddenly you can train a model in seven hours, it changes

506
00:45:23,880 --> 00:45:29,080
your whole workflow. It changes like what your job is. Right. And so that team's productivity has

507
00:45:29,080 --> 00:45:35,320
been changed dramatically. And there's not that many use cases that absolutely need like so much

508
00:45:35,320 --> 00:45:42,360
data. So we're still trying to figure out if there's ways that it can be useful to speed up like

509
00:45:42,360 --> 00:45:48,840
smaller work a smaller use cases as well. But it's always something good to have because you know,

510
00:45:48,840 --> 00:45:52,120
people aren't going to begin training on less data. People are always adding more data. And so

511
00:45:52,120 --> 00:45:56,040
that's what's happening to our use cases. And is this an example of something that

512
00:45:56,040 --> 00:46:04,200
would get integrated into the Michelangelo platform or yeah, this is part of the Michelangelo

513
00:46:04,200 --> 00:46:09,000
platform. Yeah. And so I wasn't mentioned in our Michelangelo blog post, but we're working on

514
00:46:10,360 --> 00:46:18,200
kind of like a separate development, X model exploration, deep learning specific IDE kind of

515
00:46:18,200 --> 00:46:25,400
or IDs the wrong word. Like a framework or yeah, kind of like a framework for people to

516
00:46:25,400 --> 00:46:30,680
easily run containers and tends to flow and iterate on TensorFlow models and get a bunch of

517
00:46:31,240 --> 00:46:35,640
machines that they can like GPU machines so they can run these big distributed jobs.

518
00:46:35,640 --> 00:46:39,160
Okay. And then have ultimately have those models appear at Michelangelo so they can push them

519
00:46:39,160 --> 00:46:46,360
to production and evaluate stuff like that. And that's so that's how Horavod fits into Michelangelo.

520
00:46:46,360 --> 00:46:52,040
And I think we're just going to see a tighter and tighter integration in the future.

521
00:46:52,040 --> 00:46:58,200
So now Horavod is open source. Michelangelo is not. Michelangelo is not. I'm still trying to

522
00:46:59,080 --> 00:47:05,240
prioritize that in our roadmap. It's a lot of work to open source something. So don't blame me.

523
00:47:06,680 --> 00:47:12,280
A lot of people have gotten a lot of emails like, hey, so I couldn't find Michelangelo and GitHub.

524
00:47:12,280 --> 00:47:19,400
And I was like, oh, nice medium post. Sorry guys. But yeah, I hope to be able to open source

525
00:47:19,400 --> 00:47:25,320
at sometime. It's just not something we're working on right now. Well, it's I think

526
00:47:26,920 --> 00:47:31,720
as people deploy, you know, as people kind of productionalize machine learning more and more,

527
00:47:31,720 --> 00:47:38,200
like they run into this. They have to run into this. And at every place like, you know, the problem

528
00:47:38,200 --> 00:47:43,400
that you're trying to solve with Michelangelo, preventing data scientists and engineers at Uber

529
00:47:43,400 --> 00:47:47,480
from building this over and over again, we're doing this like on the scale of the industry.

530
00:47:47,480 --> 00:47:54,840
And, you know, there are like a handful of proprietary, you know, kind of all-in-one platforms that

531
00:47:54,840 --> 00:48:00,120
kind of solve some of it. And I forget the name of the company, but I recently came across a

532
00:48:00,120 --> 00:48:07,800
company that is, you know, their focus is trying to solve the kind of model lifecycle and production

533
00:48:07,800 --> 00:48:14,120
thing. But, you know, I think ultimately, you know, it's infrastructure, right? And infrastructure

534
00:48:14,120 --> 00:48:19,560
wants to be open source. Yeah, right. So, you got to shut open source it. I think it would be

535
00:48:19,560 --> 00:48:24,920
cool if we did. What's up with that? I'll bring that to feedback back. I'm arguing for it too.

536
00:48:26,280 --> 00:48:31,080
So, yeah, I think it would be cool if we did. And I would definitely love to see like an industry-wide

537
00:48:31,080 --> 00:48:37,480
adoption of Michelangelo. I feel like we have a lot of high priority stuff. We got to add to it

538
00:48:37,480 --> 00:48:43,000
internally. But there's definitely, I mean, there's a lot of benefits to open sourcing stuff.

539
00:48:43,000 --> 00:48:49,240
And I mean, that's why I think that's why Google open sourced TensorFlow from the beginning.

540
00:48:49,240 --> 00:48:53,320
I mean, they worked on it a lot internally, but then when it was good enough, they open sourced it.

541
00:48:53,320 --> 00:48:58,520
And as far as I understand is because they didn't want to have the same thing happen to them as

542
00:48:58,520 --> 00:49:04,120
what happened with MapReduce, where they didn't open sourced it at first. And then it took,

543
00:49:04,120 --> 00:49:09,240
and then there was a big deviation from the open sourced stuff. Then what happened when they

544
00:49:09,240 --> 00:49:14,440
made someone else open sourced it. Yeah, and they were kind of left out. Yeah. So, awesome.

545
00:49:14,440 --> 00:49:16,680
Yeah, there's a lot of benefits. We'd love to do it sometime.

546
00:49:16,680 --> 00:49:21,720
Right. Oh, Mike, I really enjoyed the conversation. Thanks so much for taking the time to sit down

547
00:49:21,720 --> 00:49:27,480
with us and share a little bit about what you and over up to in the realm of ML platforms.

548
00:49:27,480 --> 00:49:29,160
Awesome. It was a pleasure being here. Thanks a lot.

549
00:49:29,160 --> 00:49:29,720
Right. Thanks.

550
00:49:32,840 --> 00:49:39,000
All right, everyone. That's our show for today. For more information on Mike or any of the topics

551
00:49:39,000 --> 00:49:44,680
covered in this episode, head on over to twimmalai.com slash talk slash 115.

552
00:49:45,640 --> 00:49:52,840
Definitely remember to vote on your favorite MyAI video at twimmalai.com slash MyAI.

553
00:49:52,840 --> 00:50:09,400
And, of course, thanks so much for listening and catch you next time.

