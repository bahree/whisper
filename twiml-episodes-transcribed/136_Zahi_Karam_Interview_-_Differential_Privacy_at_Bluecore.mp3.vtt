WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:36.480
This week on the podcast I am excited to present a series of interviews exploring the emerging

00:36.480 --> 00:39.280
field of differential privacy.

00:39.280 --> 00:43.040
Over the course of the week we'll dig into some of the very exciting research and application

00:43.040 --> 00:46.160
work happening right now in the field.

00:46.160 --> 00:51.600
In this episode I'm joined by Zahi Karam, director of data science at Bluecore, whose

00:51.600 --> 00:57.000
retail marketing platform specializes in personalized email marketing.

00:57.000 --> 01:01.720
I sat down with Zahi at the George and Partners portfolio conference last year, where he gave

01:01.720 --> 01:07.120
me my initial exposure to the field of differential privacy, ultimately leading to this podcast

01:07.120 --> 01:08.800
series.

01:08.800 --> 01:13.280
Zahi shared his insights into how differential privacy can be deployed in the real world

01:13.280 --> 01:17.680
and some of the technical and cultural challenges to doing so.

01:17.680 --> 01:23.080
We discussed the Bluecore use case in depth, including why and for whom they build differentially

01:23.080 --> 01:25.800
private machine learning models.

01:25.800 --> 01:29.840
Thanks to George and Partners for their continued support of the podcast and for sponsoring

01:29.840 --> 01:32.400
this series.

01:32.400 --> 01:37.200
George and Partners is a venture capital firm that invests in growth stage business software

01:37.200 --> 01:40.440
companies in the US and Canada.

01:40.440 --> 01:44.920
Most investment, George and works closely with portfolio companies to accelerate adoption

01:44.920 --> 01:50.120
of key technologies, including machine learning and differential privacy.

01:50.120 --> 01:54.680
To help their portfolio companies provide privacy guarantees to their customers, George

01:54.680 --> 02:00.440
and recently launched its first software product, Epsilon, which is a differentially private

02:00.440 --> 02:02.720
machine learning solution.

02:02.720 --> 02:07.600
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,

02:07.600 --> 02:11.600
and if you find this field interesting, I'd encourage you to visit the differential

02:11.600 --> 02:20.320
privacy resource center they've set up at gptrs.vc slash twimmelai.

02:20.320 --> 02:25.440
And now on to the show.

02:25.440 --> 02:32.240
Alright everyone, I am here at the George and Partners portfolio conference and I am

02:32.240 --> 02:38.800
with Zahi Karam, Zahi is the director of data science at BluCore.

02:38.800 --> 02:41.600
Zahi, welcome to this week in machine learning and AI.

02:41.600 --> 02:44.240
It's great to be here, great, great.

02:44.240 --> 02:48.040
Why don't we get started by having you tell us a little bit about your background and

02:48.040 --> 02:52.200
how you got involved in data science and machine learning.

02:52.200 --> 02:59.240
So my background is a bit of an interesting one for somebody in machine learning and data

02:59.240 --> 03:02.920
science, I started off in electrical engineering.

03:02.920 --> 03:03.920
That was my undergrad.

03:03.920 --> 03:04.920
Go AA.

03:04.920 --> 03:05.920
AA?

03:05.920 --> 03:08.680
Yes, amazing.

03:08.680 --> 03:15.000
So I was in AA and I got very excited about digital signal processing and so that was

03:15.000 --> 03:18.040
my focus for my undergrad.

03:18.040 --> 03:24.840
Then for my masters, I stuck with digital signal processing, which is still very exciting.

03:24.840 --> 03:31.680
And then I got into speech, speech is an interesting field because it started off purely in the

03:31.680 --> 03:37.400
electrical engineering realm and now it has gotten into the computer science realm with

03:37.400 --> 03:44.600
the advent of algorithms such as deep learning.

03:44.600 --> 03:51.640
So I got into speech and that's how I got into machine learning, specifically my PhD was

03:51.640 --> 04:02.440
focused on semi-supervised methods to improve on speaker identification, okay.

04:02.440 --> 04:07.000
Meaning identifying or differentiating one speaker from another in a noisy kind of

04:07.000 --> 04:14.520
the party scenario or not in a noisy environment, specifically how can we, specifically the

04:14.520 --> 04:22.840
problem we were trying to solve is, can I tell given a 30 second speech utterance, can

04:22.840 --> 04:25.520
I tell who it is, okay.

04:25.520 --> 04:31.640
And in a setting like that, you've got somewhat, you've got maybe two or three utterances

04:31.640 --> 04:36.320
from the target speaker that you cover, but you also have a lot of additional data that

04:36.320 --> 04:41.320
is unlabeled, right, because it's not hard to collect speech, right, it's expensive to

04:41.320 --> 04:43.360
label speech, right, right.

04:43.360 --> 04:49.880
So how do we leverage that additional data to do a better job at speaker identification

04:49.880 --> 04:54.080
and really it's, it extends beyond speaker identification, that's just the application.

04:54.080 --> 04:55.080
Okay.

04:55.080 --> 04:59.640
Well, if you're gotten pretty good at that, now you can talk to your Google home or your

04:59.640 --> 05:04.880
Alexa and the training is, you know, just saying, okay, Google like three or four times

05:04.880 --> 05:08.560
and then it can differentiate the different people in the family.

05:08.560 --> 05:18.440
This work in general is, has benefited from these competitions that provide a data set,

05:18.440 --> 05:22.760
that companies and research labs from around the world compete.

05:22.760 --> 05:25.520
So it's very easy to get started with it.

05:25.520 --> 05:30.880
You've got the data, just usually a tough part, right, and then you've got a highly collaborative

05:30.880 --> 05:36.360
environment where everybody is, you know, there's yearly conferences and really competitions.

05:36.360 --> 05:39.680
So it makes for a field that evolves very quickly.

05:39.680 --> 05:40.680
Yeah.

05:40.680 --> 05:43.200
So I was fortunate to be part of that experience.

05:43.200 --> 05:44.200
That's great.

05:44.200 --> 05:46.880
Yeah, and it was a highly collaborative experience.

05:46.880 --> 05:55.960
So that was my PhD, then I went to University of Michigan for a postdoc and there I wanted

05:55.960 --> 06:02.880
to stay in the machine learning world but move away from speech and get into medical applications

06:02.880 --> 06:11.600
unfortunately, speech pulled me back and it turns out there's an interesting application

06:11.600 --> 06:20.840
in psychiatry where bipolar patients, if you are a person with bipolar disorder, you

06:20.840 --> 06:27.640
can be, have, be normal most of the time but you could also go manic and when you go

06:27.640 --> 06:37.720
manic, you'll do crazy stuff like why Mercedes, quit your job, that's naked on tables and

06:37.720 --> 06:41.440
you're not aware that you're doing these things, you know.

06:41.440 --> 06:47.640
And so that could have a very negative effect on your life, you could, it takes on average

06:47.640 --> 06:53.960
two years to get back to your normal life after a manic episode and on the others and you

06:53.960 --> 07:01.600
could also go get depressed and then we all know the negative side of depression.

07:01.600 --> 07:06.480
So you've got this spectrum and the problem with bipolar disorder, when you're depressed,

07:06.480 --> 07:09.240
you don't want to seek help because you're depressed and when you're manic, you don't

07:09.240 --> 07:11.920
know you're manic, so you don't seek help.

07:11.920 --> 07:17.000
So how do we identify ahead of time that somebody's going manic with the press, so that's

07:17.000 --> 07:24.200
a problem and it turns out that speech is an interesting, let's say, biomarker for that.

07:24.200 --> 07:31.320
The psychiatry we worked with left to say that I'll have family members of people with

07:31.320 --> 07:37.520
bipolar disorder tell me, doctor, I knew he was going to go manic because I heard it in

07:37.520 --> 07:42.880
his voice or I called her on the phone and she sounded depressed.

07:42.880 --> 07:43.880
So I knew she was good.

07:43.880 --> 07:47.760
Not just an indicator, but a predictive indicator.

07:47.760 --> 07:54.640
So that's what we based the research on and so we spent about three years between writing

07:54.640 --> 07:59.760
an application, a cell phone application that recorded only your side of the conversation

07:59.760 --> 08:04.520
because it's illegal in some states to record both sides of that consent.

08:04.520 --> 08:10.280
So only record your side of the conversation and analyze it for mood symptoms and then

08:10.280 --> 08:19.040
we gave those cell phones to bipolar patients and tracked the phone calls over a year and

08:19.040 --> 08:25.760
then also tracked the mood symptoms over that year and then build a system that could

08:25.760 --> 08:29.760
recognize when somebody was going manic or depressive.

08:29.760 --> 08:30.760
Interesting.

08:30.760 --> 08:32.440
So that was that work.

08:32.440 --> 08:39.680
And are the, you know, we're still like not at the core topic yet, but I'm super interested

08:39.680 --> 08:40.680
in this.

08:40.680 --> 08:47.200
Is this where the signals, the things that they said or things like pitch in tempo and

08:47.200 --> 08:53.520
kind of vocal characteristics or something that's like even, you know, lower level than

08:53.520 --> 08:54.520
that?

08:54.520 --> 08:55.520
Yeah.

08:55.520 --> 09:02.680
So it's very hard to design experiments that protect the patients.

09:02.680 --> 09:07.240
And that is the most important thing that you have to do when you design an experiment.

09:07.240 --> 09:12.920
Make sure that you're protecting the patient, especially a vulnerable population, like

09:12.920 --> 09:14.400
people with bipolar disorder.

09:14.400 --> 09:23.400
And so we had to make sure that the features that came out of the system could not be used

09:23.400 --> 09:28.400
to reconstruct what was said in these phone calls.

09:28.400 --> 09:34.000
So our feature extraction was designed in a way to make sure that you couldn't recover

09:34.000 --> 09:35.240
what was said.

09:35.240 --> 09:42.920
And that's important because let's say somebody goes manic and does something, then we have

09:42.920 --> 09:46.880
all the phone recordings and that's right.

09:46.880 --> 09:50.840
And what if we knew ahead of time that something was going to happen?

09:50.840 --> 09:56.040
It gets into a very first, you're not protecting the patient, but there's also the whole legal

09:56.040 --> 09:57.040
amplification.

09:57.040 --> 09:58.040
Right.

09:58.040 --> 09:59.040
Right.

09:59.040 --> 10:00.040
Interesting.

10:00.040 --> 10:01.040
Interesting.

10:01.040 --> 10:06.040
Okay.

10:06.040 --> 10:07.040
Nice.

10:07.040 --> 10:08.520
And so what does BluCore do?

10:08.520 --> 10:12.400
BluCore is a decisioning platform for commerce.

10:12.400 --> 10:18.120
We help marketers find their best customers and keep them for life.

10:18.120 --> 10:27.600
We are a SaaS company, so essentially we provide service for commerce companies and essentially

10:27.600 --> 10:34.520
we help their marketing team identify who to send a message to and what is the best message

10:34.520 --> 10:38.680
and at what time to send it and on which channel?

10:38.680 --> 10:49.080
And the experience, it sounds like the experience you had working on the postdoc in terms of the

10:49.080 --> 10:56.440
focus that needed to be placed on ensuring the privacy of the patients has had kind of

10:56.440 --> 11:03.080
immediate ramifications on your work at BluCore and has led you to doing some pretty interesting

11:03.080 --> 11:05.280
things around privacy.

11:05.280 --> 11:06.280
Yes.

11:06.280 --> 11:13.160
I wouldn't say there's a direct connection between the two, but certainly I think that we

11:13.160 --> 11:19.000
should always be thinking about how can we make sure that we're preserving privacy in

11:19.000 --> 11:26.000
the data that we collect in a way that protects our customers, commerce companies and protects

11:26.000 --> 11:27.800
their customers, right?

11:27.800 --> 11:33.960
And in the medical field, it's obviously even you could argue more important to protect

11:33.960 --> 11:37.880
the patients and the vulnerable population that we are trying to help.

11:37.880 --> 11:40.840
So yes, there's definitely that parallel there.

11:40.840 --> 11:43.960
The approaches are very different in each of them.

11:43.960 --> 11:49.440
In the medical field, we were using a lot of encryption, a lot of feature design that

11:49.440 --> 11:58.520
protected the content, what we said, at BluCore, we are using the financial privacy to try

11:58.520 --> 12:08.160
and build models that do not leak any information about the customers that we use to build those

12:08.160 --> 12:09.160
models with.

12:09.160 --> 12:10.160
Okay.

12:10.160 --> 12:15.320
And when you say customers, you're referring to your customers or your customers' customers

12:15.320 --> 12:16.320
are both.

12:16.320 --> 12:17.320
Both.

12:17.320 --> 12:18.320
Okay.

12:18.320 --> 12:19.320
Both.

12:19.320 --> 12:25.440
But it's a great question and it's a very interesting question and one that we are figuring

12:25.440 --> 12:31.680
out as we go through this process, but ultimately we want to protect both, right?

12:31.680 --> 12:40.520
Where we really see the value of the financial privacy at BluCore is that we have multiple

12:40.520 --> 12:41.520
partners.

12:41.520 --> 12:46.640
We've got about 400 partners to be clear partners are our customers are the commerce

12:46.640 --> 12:49.200
companies that buy us.

12:49.200 --> 12:57.040
And each of these partners, we silo their data so that we are essentially protecting each

12:57.040 --> 13:02.800
commerce company's data from the other commerce company's data, right?

13:02.800 --> 13:09.760
Because we don't want necessarily to give, you know, if you had, let's say, to apparel

13:09.760 --> 13:16.080
companies that are competitors, you don't want to give, I'm going to throw out names

13:16.080 --> 13:21.520
like, gap, you don't want to give it expresses data, right, and vice versa, right?

13:21.520 --> 13:28.120
So we keep all of those kind of, there's, you know, like this base level security of you

13:28.120 --> 13:35.880
don't want the data leaking over from one customer to the next, but you also don't, unless

13:35.880 --> 13:40.560
you're, this is part of your value proposition to these customers and it's what they're buying.

13:40.560 --> 13:45.000
You don't want to give their competitors an advantage based on data that you've collected

13:45.000 --> 13:47.080
from the one partner.

13:47.080 --> 13:48.080
Exactly.

13:48.080 --> 13:50.400
We want to be very careful.

13:50.400 --> 13:54.880
We don't want to give their competitors an advantage throughout platform, right?

13:54.880 --> 13:55.880
Right.

13:55.880 --> 13:59.280
Essentially, you want to be fair at all of our partners, right?

13:59.280 --> 14:05.360
And obviously, in the contracts we have with our partners, all of that is in there, right?

14:05.360 --> 14:11.240
What gets interesting though is, when you're building machine learning models or data science

14:11.240 --> 14:17.320
models or whatever you want to call it, the more data you have, the better the model is.

14:17.320 --> 14:25.960
And you could argue that there is an advantage if you could break down these silos.

14:25.960 --> 14:33.280
And there is an advantage that data asset becomes much more powerful if you can break down

14:33.280 --> 14:34.280
these silos.

14:34.280 --> 14:43.000
Now, how can we do it and is there a way to do it in a way that doesn't give gap up on

14:43.000 --> 14:46.200
express, right, or express a leg up on gap?

14:46.200 --> 14:54.320
Is there a way to break down these silos and get better models for everyone?

14:54.320 --> 15:03.280
But in a way where if you took out one of the partners' data sets from that pool of data,

15:03.280 --> 15:05.960
the model would not be affected.

15:05.960 --> 15:14.200
So that means now, each partner doesn't contribute enough to that aggregate model for them

15:14.200 --> 15:18.000
to be leaking any of the information to the others, right?

15:18.000 --> 15:19.000
Interesting.

15:19.000 --> 15:27.720
So it's a way to think about the maybe exemplar of the problem as being, you know, if the

15:27.720 --> 15:35.000
model is impacted proportionally by the amount of data I'm bringing to the table, then the

15:35.000 --> 15:41.760
larger customers or partners are, you know, basically subsidizing the model performance

15:41.760 --> 15:43.880
of the smaller ones, right?

15:43.880 --> 15:50.360
And so is what you're saying that differential privacy solves that, helps you solve that

15:50.360 --> 15:51.360
problem?

15:51.360 --> 15:56.480
That is what we hope, differential privacy will help us solve.

15:56.480 --> 16:01.840
You're very right, if you think about, because the other way of doing this is to get everybody

16:01.840 --> 16:04.760
to sign up for a data co-op, right?

16:04.760 --> 16:08.360
Where you explicitly saying, I'm going to share my data with everybody else, and I get

16:08.360 --> 16:09.600
everybody else's data.

16:09.600 --> 16:12.400
And that makes a lot of sense if you've got a lot of small players.

16:12.400 --> 16:17.840
But at BlueCore, we've got massive enterprise customers, and then we've got smaller SMB

16:17.840 --> 16:18.840
customers.

16:18.840 --> 16:24.640
So the enterprise customer might not want to share that data and give an advantage to

16:24.640 --> 16:26.720
SMB customers, right?

16:26.720 --> 16:34.000
So how can we make it so that that is a non-issue, right?

16:34.000 --> 16:35.640
And it's very tricky.

16:35.640 --> 16:40.640
And there's this whole concept of, well, you could do data anonymization, right?

16:40.640 --> 16:44.760
This is how it's typically been done in the past, where you obfuscate some part of the

16:44.760 --> 16:45.760
data.

16:45.760 --> 16:47.720
And that's not quite enough, right?

16:47.720 --> 16:52.040
Well, this is the problem, anonymization doesn't guarantee privacy, right?

16:52.040 --> 16:56.800
And we've seen that over and over with examples like the release of the Netflix data and people

16:56.800 --> 17:01.600
were able to, even though the Netflix data had removed, are you familiar with the Netflix

17:01.600 --> 17:02.600
data?

17:02.600 --> 17:04.760
I am vaguely familiar.

17:04.760 --> 17:12.640
So there was a data set that was, quote unquote, anonymized, but you can infer individually

17:12.640 --> 17:17.160
identifiable information from the data set, was it there?

17:17.160 --> 17:18.160
Yeah.

17:18.160 --> 17:22.320
Netflix had this grand challenge that said we'll give them a million dollars to the first

17:22.320 --> 17:28.920
percent that can improve our recommendations by X, right?

17:28.920 --> 17:34.120
And so they released this data set where they had some data set of people waiting real

17:34.120 --> 17:43.640
data of people, their customers waiting movies, and they removed the identifier of the customer,

17:43.640 --> 17:44.640
right?

17:44.640 --> 17:47.560
So you didn't know which customer waited for that movie, right?

17:47.560 --> 17:51.640
So they released that and that's an anonymized data set.

17:51.640 --> 17:57.760
The thing is some clever researchers said, well, you know what, that's not really anonymous

17:57.760 --> 18:04.360
because I can use this external data source, which is IMDB, and correlate the ratings

18:04.360 --> 18:06.680
from IMDB with the Netflix data.

18:06.680 --> 18:11.560
And they were able to identify certain people on the Netflix data, right?

18:11.560 --> 18:14.960
So anonymization doesn't guarantee privacy, got it?

18:14.960 --> 18:20.120
So this recent field coming up, that's been coming up in the last 10 years, that is around

18:20.120 --> 18:29.120
differential privacy, whereas how can you mathematically prove privacy guarantees, right?

18:29.120 --> 18:35.120
And the basic concept behind this whole field is essentially you add noise.

18:35.120 --> 18:42.160
You add some noise that makes it hard to recover the underlying data that trained the model.

18:42.160 --> 18:49.840
And the basic idea behind it is, I need to be able to have two data sets that differ

18:49.840 --> 18:52.800
by one data point.

18:52.800 --> 18:56.440
One data set has that data point, the other data set does not have that data point.

18:56.440 --> 19:02.120
And I should not be able to tell from the resultant models that I trained using these two separate

19:02.120 --> 19:08.520
data sets, which of the two data sets the model was generated from, right?

19:08.520 --> 19:16.280
So if I have a model A, let's say logistic regression, and I use the data set that is missing

19:16.280 --> 19:22.120
at that point of interest, and I train logistic regression model with that, and then I use the

19:22.120 --> 19:25.880
data set with the point of interest, and I train logistic regression model with that.

19:25.880 --> 19:30.520
I should not be able to tell which of the two data sets I used, right?

19:30.520 --> 19:33.760
And that's the basic idea behind the financial privacy.

19:33.760 --> 19:41.280
When I hear a differential privacy explained, folks usually come at it from the opposite direction,

19:41.280 --> 19:47.480
which is, you don't want me to be able to interrogate a model and determine that individual

19:47.480 --> 19:50.040
X was somehow used in its training, right?

19:50.040 --> 19:53.240
It's kind of two sides of the same coin.

19:53.240 --> 19:54.240
Exactly.

19:54.240 --> 19:55.240
Exactly.

19:55.240 --> 19:58.240
It's just another way of explaining, yeah.

19:58.240 --> 20:03.920
So the basic premise is you inject noise at some point in the model training, whether that's

20:03.920 --> 20:09.960
at the feature level, at the model training level, at the output of the models.

20:09.960 --> 20:15.600
So that's essentially the basic concept at a very high level behind the financial privacy.

20:15.600 --> 20:20.480
Yeah, conceptually, I'm struggling with the idea that tens or hundreds of millions of

20:20.480 --> 20:26.480
people are going to have their data points in this model, and that even without differential

20:26.480 --> 20:33.320
privacy, I'd be able to learn anything from about one individual person with how huge the

20:33.320 --> 20:34.320
data set is.

20:34.320 --> 20:37.160
It's kind of counterintuitive for me.

20:37.160 --> 20:41.560
It is, I agree that it's counterintuitive, but a lot of the models that are being used

20:41.560 --> 20:46.720
right now have thousands of parameters in them that get tweaked.

20:46.720 --> 20:50.560
So for example, if you look at deep learning models between all the layers and all the

20:50.560 --> 20:53.360
weights, you end up with thousands of parameters, right?

20:53.360 --> 21:02.440
And so it is possible, for example, if you build an image recognition system and then using

21:02.440 --> 21:09.080
images to train that model, it is possible for some of these images that used to train

21:09.080 --> 21:15.880
to be for the model parameters to actually capture that image and you would be able to

21:15.880 --> 21:24.320
interrogate that model and get back that image or exactly the model parameters and get back

21:24.320 --> 21:25.320
that image.

21:25.320 --> 21:27.000
So while the likelihood of that is low, it's a risk.

21:27.000 --> 21:28.000
It's a risk.

21:28.000 --> 21:36.400
And the only way to gain T that can't happen is through this approach of the financial

21:36.400 --> 21:42.480
privacy where you inject noise and how much noise you inject and where you inject it

21:42.480 --> 21:49.800
is the eye of it and then you can prove to a certain bound, right?

21:49.800 --> 21:56.680
That this model is eventually private and it will not give up your data.

21:56.680 --> 22:05.040
The other thing that's a little counterintuitive for me is generally these models are, the models

22:05.040 --> 22:06.040
are closed.

22:06.040 --> 22:09.960
I don't have access to the models, the model parameters, things like that.

22:09.960 --> 22:19.880
Is it that and maybe it's both of these that AI can learn by interrogating the model

22:19.880 --> 22:28.840
just via regular inference and or B that the model parameters are leakier than I might

22:28.840 --> 22:29.840
otherwise think.

22:29.840 --> 22:33.880
I can look in the memory of my phone and see the model parameters or something like that.

22:33.880 --> 22:39.560
So the leakiness of the model parameters really comes up when people open source a lot

22:39.560 --> 22:40.880
of these models, right?

22:40.880 --> 22:46.840
Like now when Google might open source the image recognition stack, right?

22:46.840 --> 22:50.280
And then that deep learning model is essentially what the open source thing is, is the way

22:50.280 --> 22:51.280
to.

22:51.280 --> 22:52.280
Okay.

22:52.280 --> 22:53.280
And someone's picture might be in there.

22:53.280 --> 22:54.280
Yeah.

22:54.280 --> 22:55.280
Okay.

22:55.280 --> 23:01.440
So, but the alternative is somebody could also interrogate that model and through that

23:01.440 --> 23:09.520
interrogation, identify, uncover some of the training data that was used.

23:09.520 --> 23:16.600
Being a sophisticated enough approach that interrogates the model enough and you know

23:16.600 --> 23:23.760
with a, you can design a coverage, I guess, of a way to interrogate the model to isolate.

23:23.760 --> 23:25.280
Exactly.

23:25.280 --> 23:30.960
And you can also, the other goal of the French privacy is it's not just interrogating that

23:30.960 --> 23:42.040
model but also a malicious party might use external data side information to know how to design

23:42.040 --> 23:48.160
a way to interrogate that model to figure out some, if somebody was in the data set or

23:48.160 --> 23:51.720
what that person's features were in that data set.

23:51.720 --> 23:53.640
So it's, it's three ways.

23:53.640 --> 23:58.040
You can either look under the hood and the parameters might leak some information.

23:58.040 --> 24:03.360
You can just directly interrogate the model or you can leverage side data, right?

24:03.360 --> 24:07.760
So for example, in the Netflix example, if you just looked at the Netflix data by itself,

24:07.760 --> 24:09.640
you couldn't uncover anybody.

24:09.640 --> 24:16.520
But then when you, a malicious agent could go to IMDB and then use that to figure out,

24:16.520 --> 24:17.520
right?

24:17.520 --> 24:21.080
So there's that aspect too.

24:21.080 --> 24:22.080
Okay.

24:22.080 --> 24:26.440
And this is the tricky part and you're asking the right questions, right?

24:26.440 --> 24:32.840
We can't protect against everything and we can't foresee all the ways that a malicious

24:32.840 --> 24:37.040
agent could try and uncover this information.

24:37.040 --> 24:43.200
So all these protections we try and put, whether it's anonymization or black boxing

24:43.200 --> 24:44.200
get, right?

24:44.200 --> 24:45.840
So you can't look at the parameters.

24:45.840 --> 24:52.480
All of these are good, they're useful, but they don't guarantee and the way to guarantee

24:52.480 --> 24:55.280
is again through this concept of the French privacy, right?

24:55.280 --> 24:58.320
So you are asking the right questions.

24:58.320 --> 25:03.920
In most cases, especially smaller models with fewer parameters and if you have a lot of

25:03.920 --> 25:10.200
training data, the likelihood that somebody could get at the individual training samples,

25:10.200 --> 25:15.040
this is pretty low, sure, but the French privacy gives us a guarantee.

25:15.040 --> 25:17.520
So now we are, we are comfortable.

25:17.520 --> 25:22.120
We can say I am comfortable that this model is not going to be information.

25:22.120 --> 25:29.040
Is differential privacy or at least the kind of the mathematical framework that has been

25:29.040 --> 25:37.240
developed around it also allow us to assess a model and put some bounds around kind of

25:37.240 --> 25:41.600
the risk for a model that for what, you know, pre differential privacy?

25:41.600 --> 25:47.520
It's, it's very much focused on the, once you decide how and where you want to inject

25:47.520 --> 25:50.800
noise, what are the, what are the guarantees after that?

25:50.800 --> 25:51.800
Okay.

25:51.800 --> 25:57.320
So how do you see that fitting into the mix at BluCore?

25:57.320 --> 26:02.160
So at BluCore, we have a, there's a bit of a difference between how we're trying to

26:02.160 --> 26:07.560
use differential privacy and how it's typically thought of, right?

26:07.560 --> 26:14.720
Because there is, our goal is to make sure that we can, or to find a way to aggregate data

26:14.720 --> 26:21.360
across these different silos in a way that protects each of our partners data, right?

26:21.360 --> 26:27.720
And, and that's where, that's why it's, it's a bit interesting, right?

26:27.720 --> 26:30.760
It's, it's a space that isn't that explored.

26:30.760 --> 26:35.960
All of the whole differential privacy space is young and then this is an application of

26:35.960 --> 26:39.840
the differential privacy that is a bit novel, right?

26:39.840 --> 26:46.200
So it's a, it's an interesting one and, and if, and if we are able to do aggregation

26:46.200 --> 26:52.240
in a differential private manner, then that will make our models that much better.

26:52.240 --> 26:57.560
It will make the value of our data that much higher and not at the expense of any individual

26:57.560 --> 26:58.560
partner.

26:58.560 --> 26:59.560
Exactly.

26:59.560 --> 27:03.720
And ultimately our partners will benefit, you know, and in the end, we exist to serve

27:03.720 --> 27:04.720
our partners.

27:04.720 --> 27:05.720
Right.

27:05.720 --> 27:08.240
That's why they do business with us.

27:08.240 --> 27:14.480
And so how can we make our models better, provide them value without violating any, any

27:14.480 --> 27:20.960
contracts, we have them, any, any, whether they are written contracts or, or, or trust,

27:20.960 --> 27:21.960
right?

27:21.960 --> 27:26.240
In the end, there's a legal aspect of what you can do with the data, but there's also

27:26.240 --> 27:30.560
the partner's willingness to, to share that data, right?

27:30.560 --> 27:35.080
And, and in the end, we want to be forthcoming with our partners with how we use their data

27:35.080 --> 27:38.960
and, and how we combine it with other data sets, right?

27:38.960 --> 27:42.200
So this is, this is very much an exploratory phase.

27:42.200 --> 27:47.960
We are trying to understand what we can do, what we can guarantee and what are the benefits

27:47.960 --> 27:49.440
of it.

27:49.440 --> 27:55.720
And through that, we will understand what the value of it is and then go to our partners

27:55.720 --> 28:01.760
with that value and articulate that value and get them excited about what differential

28:01.760 --> 28:04.840
privacy can bring to them, right?

28:04.840 --> 28:06.400
And how it can protect them.

28:06.400 --> 28:07.400
Yeah.

28:07.400 --> 28:13.440
Are you, have you engaged with any of your partners around the topic yet?

28:13.440 --> 28:19.520
I'm imagining the, you know, getting a partner up to speed on kind of the, you know, even

28:19.520 --> 28:20.840
what you're trying to do.

28:20.840 --> 28:24.000
Like there's, you're trying to do something that's pretty innovative and you're trying

28:24.000 --> 28:30.320
to do something that's, you know, trying to do that thing using, you know, innovative

28:30.320 --> 28:37.200
and, I mean, not to overuse that word, but like it, it's cutting edge stuff.

28:37.200 --> 28:39.880
I guess replace one cliche with another.

28:39.880 --> 28:48.120
Well, it is cutting edge and this is where our partnership with Georgian has been extremely

28:48.120 --> 28:55.200
valuable because we are a startup, we're growing fast, we have products we have to build.

28:55.200 --> 29:02.000
And yet we have to make time to explore these, these, these novel and cutting edge avenues

29:02.000 --> 29:06.560
that could down the line provide us significant value, right?

29:06.560 --> 29:12.520
And our team is growing, but we're not necessarily big enough to be able to support all of these

29:12.520 --> 29:18.480
pie in the sky or, you know, futuristic models and ideas alone.

29:18.480 --> 29:25.840
So the partnership with Georgian, partners has been crucial for us exploring this space.

29:25.840 --> 29:27.440
The partnership is, it's twofold.

29:27.440 --> 29:35.120
The partnership is from a technical aspect where we've been working with great researchers

29:35.120 --> 29:44.840
on the Georgian impact team and they are providing a lot of technical guidance, running a lot

29:44.840 --> 29:45.840
of experiments.

29:45.840 --> 29:51.360
It's a full on engagement, it's not engagement where one party is removed, so one party

29:51.360 --> 29:52.800
is removed from the equation, right?

29:52.800 --> 29:56.000
It's a full on engagement, it's down in the weeds.

29:56.000 --> 30:02.240
We've had some of our data scientists come and spend time in Toronto and embed with

30:02.240 --> 30:04.040
the team for a few days.

30:04.040 --> 30:10.480
So it's a full on engagement with the Georgian partners team from a technical standpoint,

30:10.480 --> 30:19.040
but then there's also the legal standpoint of it all and the communication standpoint.

30:19.040 --> 30:24.120
How do you understand this landscape of what you can do with the data, what you can do

30:24.120 --> 30:29.920
with the data given the constraints of the contracts you have, and most importantly

30:29.920 --> 30:36.520
is how do you message what the financial privacy is to your partners, to your customers,

30:36.520 --> 30:41.680
so that they buy into the idea and sign off on the idea, right?

30:41.680 --> 30:48.040
Because even if your contracts allow for the concept of the financial private aggregation

30:48.040 --> 30:55.480
of data and your partners don't necessarily buy into this idea, then we can't go live

30:55.480 --> 30:56.480
with that.

30:56.480 --> 30:57.480
Right.

30:57.480 --> 31:01.360
So we don't want to do our partners, right?

31:01.360 --> 31:06.760
Our goal is to create something that's good for them, not create something that would

31:06.760 --> 31:10.640
lose us our trust.

31:10.640 --> 31:19.400
So just to run that off, Georgian has been working hard on understanding that legal landscape

31:19.400 --> 31:28.480
and understanding how to communicate what the financial privacy is to partners that might

31:28.480 --> 31:34.080
not have the technical depth to fully understand what it is and how it's different from anonymization.

31:34.080 --> 31:35.080
Right.

31:35.080 --> 31:39.640
People understand anonymization, they don't understand anonymization and they might not

31:39.640 --> 31:42.240
understand the risk of anonymization, right?

31:42.240 --> 31:45.200
And then you're telling them, well, there's this other thing called the financial privacy,

31:45.200 --> 31:46.200
right?

31:46.200 --> 31:53.600
And it's just, it took us a while to understand what it brings to the table and so you can

31:53.600 --> 31:59.840
imagine people that aren't technically versed in the space, it's hard to explain these things,

31:59.840 --> 32:00.840
right?

32:00.840 --> 32:08.280
I'm trying to envision kind of the process that you're pursuing to explore this.

32:08.280 --> 32:13.720
Are you, I mean, it sounds like it's kind of going straight from, you know, academic

32:13.720 --> 32:21.280
research papers to running experiments locally to, yeah, this is, this is it, we're, we're

32:21.280 --> 32:25.800
essentially, in everything we do, we try not to reinvent the wheel, right?

32:25.800 --> 32:29.440
There's, there's a lot of great academic literature out there.

32:29.440 --> 32:34.800
Unfortunately, the financial privacy is, is a weird one because it's a relatively young

32:34.800 --> 32:35.800
field.

32:35.800 --> 32:39.680
Ten years old is relatively young and most of the work that's happening in that field

32:39.680 --> 32:41.320
is happening now.

32:41.320 --> 32:46.360
So a lot of the papers that come out, you know, it's, it's, we have to constantly keep

32:46.360 --> 32:51.200
checking the literature to make sure we're up to date on, on the latest and greatest.

32:51.200 --> 32:55.880
And at the same time, a lot of the papers are coming out from companies and so they might

32:55.880 --> 33:01.320
not be fully transparent in everything they have done, right, to allow us to replicate

33:01.320 --> 33:02.320
what's going on.

33:02.320 --> 33:03.320
Okay.

33:03.320 --> 33:09.240
So it's, it's an interesting world where we're trying to follow what's out there, but at the

33:09.240 --> 33:16.680
same time, figure out where we need to innovate to make it work for us, right?

33:16.680 --> 33:22.600
Which is again, somewhat of a different use case than the traditional differential privacy

33:22.600 --> 33:25.320
use case, or at least different concerns.

33:25.320 --> 33:30.280
Well, there's, there's that, but there's also differential privacy is one of these things

33:30.280 --> 33:35.240
where, especially if you're, if you're applying, you know, I said you're, you're injecting

33:35.240 --> 33:36.800
noise in certain places, right?

33:36.800 --> 33:39.920
If you're applying, if you're injecting that noise at the model level, at the output of

33:39.920 --> 33:46.640
the model, then the choice of the model directly affects how you bring differential privacy

33:46.640 --> 33:47.640
into it.

33:47.640 --> 33:52.080
So while the literature covers some models, it doesn't cover all models.

33:52.080 --> 33:53.080
Okay.

33:53.080 --> 33:55.240
So finding out what is the best way to do that.

33:55.240 --> 34:00.800
And then the, the, the ultimate challenge that we have is that just because we can get

34:00.800 --> 34:05.920
a given model to be defensively private, so logistic regression, there's some good literature

34:05.920 --> 34:11.440
about how to get that to be defensive private, that doesn't necessarily mean that it will

34:11.440 --> 34:16.720
provide value when you aggregate data across multiple partners.

34:16.720 --> 34:22.880
So that's where we found that we've had where there isn't much precedent around it.

34:22.880 --> 34:25.040
So we've figuring it out as we go along.

34:25.040 --> 34:29.320
So how do you take a model that you can make defensive private through through what exists

34:29.320 --> 34:34.560
in the literature, but then get value when you aggregate data across multiple partners?

34:34.560 --> 34:37.800
Yes, I think where that's where it's been exciting for us, right?

34:37.800 --> 34:43.360
And that's where the engagement with Georgian partners becomes extremely valuable.

34:43.360 --> 34:49.520
And you plan to publish on any of this or is your focus more kind of on the commercial

34:49.520 --> 34:54.560
application and you'll kind of let the research catch up if you in the areas you may have

34:54.560 --> 34:56.520
jumped ahead?

34:56.520 --> 34:58.480
That's a very good question.

34:58.480 --> 35:04.360
We, I think if there is something that is worthwhile to publish.

35:04.360 --> 35:10.320
I think it would be a great opportunity to share that with the community because it's

35:10.320 --> 35:12.920
a community that's growing fast, right?

35:12.920 --> 35:17.800
It's a community that's still young, so it would be good to give back to that community.

35:17.800 --> 35:24.760
And at the same time, I think it would be good to highlight the work that Georgian is doing

35:24.760 --> 35:33.160
and that we are doing in this space because I do think it's exciting that we are able

35:33.160 --> 35:42.520
to, as a startup, do this sort of cutting edge work and find some time and especially with

35:42.520 --> 35:49.160
the help of Georgian, gather, combine those resources and make progress on a problem that

35:49.160 --> 35:54.680
is at the sort of cutting edge of what's happening right now.

35:54.680 --> 36:01.480
So it's nice coming from an academic background and the people we work with at Georgian

36:01.480 --> 36:07.160
also come from an academic background, it's nice to be able to keep playing in that

36:07.160 --> 36:09.080
cutting edge space, right?

36:09.080 --> 36:17.240
Even when you're in a high growth startup that needs to deliver products and sell products

36:17.240 --> 36:18.240
and right now.

36:18.240 --> 36:19.240
All right.

36:19.240 --> 36:24.240
So to wrap things up for folks that want to dig into differential privacy a bit more

36:24.240 --> 36:30.000
is there canonical reference or two or three that they should be looking for or start

36:30.000 --> 36:32.960
with a Google search?

36:32.960 --> 36:37.200
There isn't a good book on the subject matter yet.

36:37.200 --> 36:40.320
I wish there was, it would make our life a lot easier.

36:40.320 --> 36:46.200
There's a Google search, Google's color search would get you, you'd pretty much find everything

36:46.200 --> 36:47.200
that's out there.

36:47.200 --> 36:50.480
You know, there's not, the thing is there isn't that much out there.

36:50.480 --> 36:55.400
So it doesn't take much digging to get to the canonical papers, man.

36:55.400 --> 36:56.400
Got it.

36:56.400 --> 36:57.400
Yeah.

36:57.400 --> 36:58.400
Awesome.

36:58.400 --> 36:59.400
Anything else you'd like to leave us with?

36:59.400 --> 37:03.240
No, I guess nothing on the technical side.

37:03.240 --> 37:09.560
I think that it's very interesting, this is my first experience at a startup.

37:09.560 --> 37:16.800
It's very exciting for me that we have partners that we can collaborate with, you know, in

37:16.800 --> 37:22.360
academia, you know, you always try and find other research you can collaborate with, that

37:22.360 --> 37:24.560
brings something new to the equation.

37:24.560 --> 37:32.280
And so it was very refreshing to be at the startup, at BluCore and find partners with our

37:32.280 --> 37:40.760
VCs, within our VCs that we can collaborate with on a problem that is very exciting, right?

37:40.760 --> 37:47.000
And while I'm new to the space, I don't think this is a very common theme.

37:47.000 --> 37:53.200
So I think it's, you know, it's a great opportunity for us as BluCore.

37:53.200 --> 38:01.480
And I think it's also a great opportunity for Georgian partners to work on this very

38:01.480 --> 38:09.440
exciting work together and explore it from the legal aspect of it, the customer sentiment

38:09.440 --> 38:13.640
side, the technical side, and ultimately the application.

38:13.640 --> 38:14.640
Right.

38:14.640 --> 38:15.640
Well, Zahee, thank you so much.

38:15.640 --> 38:20.720
I really enjoyed the conversation and learned a ton about differential privacy and how

38:20.720 --> 38:24.000
you're looking to use it.

38:24.000 --> 38:25.000
It was a pleasure.

38:25.000 --> 38:26.000
Thank you.

38:26.000 --> 38:32.360
All right, everyone, that's our show for today.

38:32.360 --> 38:37.880
For more information on Zahee or any of the topics covered in this episode, head on over

38:37.880 --> 38:43.400
to twimla.com slash talk slash 133.

38:43.400 --> 38:47.760
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit

38:47.760 --> 38:55.560
their differential privacy resource center at gptrs.vc slash twimla for more information

38:55.560 --> 38:58.760
on the field and what they're up to.

38:58.760 --> 39:26.760
Of course, thanks so much for listening and catch you next time.

