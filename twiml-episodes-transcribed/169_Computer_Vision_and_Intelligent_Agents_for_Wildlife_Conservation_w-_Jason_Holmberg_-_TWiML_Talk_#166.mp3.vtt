WEBVTT

00:00.000 --> 00:06.080
Hey everybody, Sam here. We've got some great news to share, and also a favorite

00:06.080 --> 00:12.360
to ask. We're in the running for this year's People's Choice Podcast Awards, in both the

00:12.360 --> 00:17.960
People's Choice and Technology categories, and we would really appreciate your support.

00:17.960 --> 00:24.000
To nominate us, you'll just head over to twimlai.com slash nominate, where we've linked to and

00:24.000 --> 00:29.720
embedded the nomination form from the award site. There, you'll need to input your information

00:29.720 --> 00:35.480
and create a listener nomination account. Once you get to the ballot, just find and select

00:35.480 --> 00:41.560
this week in machine learning and AI on the nomination list for both the Adam Curry People's

00:41.560 --> 00:49.400
Choice Award and the this week in tech technology category. As you know, we really, really appreciate

00:49.400 --> 00:55.320
each listener and would love to share in this accomplishment with you. Remember that URL

00:55.320 --> 01:02.520
is twimlai.com slash nominate. Feel free to hit pause and take a moment to nominate us now.

01:14.360 --> 01:20.520
Hello and welcome to another episode of Twimla Talk, the podcast why interview interesting people,

01:20.520 --> 01:26.440
doing interesting things in machine learning and artificial intelligence. I'm your host Sam

01:26.440 --> 01:40.440
Charrington. In this episode, I'm joined by Jason Holmberg, Executive Director and Director

01:40.440 --> 01:47.240
of Engineering at Wild Me. Wild Me's Wild Book and Wild Book for Whale Sharks are both open source

01:47.240 --> 01:52.840
computer vision based conservation projects that have been compared to a Facebook for wildlife.

01:53.720 --> 01:59.160
Jason kicks us off with the really interesting story of how Wild Book came to be and its eventual

01:59.160 --> 02:06.440
expansion from a focus on whale sharks to include giant manta rays, humpback whales, zebras and giraffes.

02:07.560 --> 02:12.360
Jason and I explore the evolution of these projects use of computer vision and deep learning,

02:12.360 --> 02:17.400
the unique characteristics of the models they're building and how they're ultimately enabling a

02:17.400 --> 02:22.920
new kind of citizen science. Finally, we take a look at a cool new intelligent agent project that

02:22.920 --> 02:28.360
Jason is working on which mines YouTube for wildlife sightings and automatically engages with

02:28.360 --> 02:34.120
the relevant individuals and scientists on Wild Book's behalf. And now onto the show.

02:34.120 --> 02:44.280
All right, everyone. I am on the line with Jason Holmberg. Jason is the Director of Engineering

02:44.280 --> 02:49.480
for Wild Me. Jason, welcome to this week in machine learning and AI. Hey, thanks for having me on.

02:49.480 --> 02:53.480
Jason, why don't you start out by telling us a little bit about your background and how you got

02:53.480 --> 03:02.280
involved in ML and AI. So I have a very unusual background and journey into running an organization

03:02.280 --> 03:07.320
that is focused on applying machine learning to wildlife conservation. I am a scuba diver.

03:07.320 --> 03:14.040
My background is in chemical engineering and in Arab studies. I did a master's degree at Georgetown.

03:14.040 --> 03:20.280
So neither of these would be suggestive of a journey to machine learning except maybe the

03:20.280 --> 03:26.840
scuba diving because wildlife is an area where we see a lot of application potential for machine

03:26.840 --> 03:31.560
learning. But the journey is a little bit interesting in that as I got this Arab studies degree,

03:31.560 --> 03:39.000
I found myself living at various times in the Middle East, in Egypt, in Tunisia, in Lebanon.

03:39.000 --> 03:44.440
And on one of those opportunities, I, one of those trips, I was able to travel down to Djibouti

03:44.440 --> 03:49.800
there in East Africa, which is an amazing melting pot of cultures just sitting there.

03:50.440 --> 03:55.720
And I was able to go scuba diving and I was excited to see the world's biggest fish, the whale shark.

03:55.720 --> 04:01.800
There was no guarantee I would see one and we did this live aboard sailboat going out to sail

04:01.800 --> 04:07.240
and dive amongst the set frayers islands, which means seven brothers. There are coincidentally

04:07.240 --> 04:13.880
only six islands, so I'm not sure who did the math, but there I was. It embedded sort of in a group

04:13.880 --> 04:19.240
of French tourists on a sailboat and in fact I went scuba diving and as we were coming up,

04:19.240 --> 04:26.680
there was a whale shark. No, whale sharks grow up to 60 feet in length and this was an eight foot

04:26.680 --> 04:31.480
juvenile. So quite small, but actually that's extremely rare. Most of the whale sharks we see in

04:31.480 --> 04:36.840
the wild are about 15 feet and larger. So this was a very rare sighting of this juvenile and I was

04:36.840 --> 04:42.200
just enamored. And the interesting thing about a small whale shark is that I was proportional and

04:42.200 --> 04:48.760
sized to it. So this animal looked back at me, this giant fish and we had a moment of just swimming

04:48.760 --> 04:54.680
together underwater. It was curious about me. Later in life, I was able to swim with some 40-foot

04:54.680 --> 04:59.800
whale sharks in the Galapagos Islands and interacting with a giant, a Leviathan like that is a very

04:59.800 --> 05:06.440
different experience. I'm nothing to a Leviathan. I'm just a passing speck of dust in the water

05:06.440 --> 05:11.480
column. But this young juvenile sort of gave me the time of day if you will and we swam together

05:11.480 --> 05:17.960
for a bit and then it disappeared and that just sparked something. This was, this was in the spring

05:17.960 --> 05:25.000
of 2002. So flash forward, fall of 2002, I decided to go on a whale shark research expedition and

05:25.000 --> 05:30.600
I spent a week down in Baja California on this small boat as this micro light plane circles overhead,

05:30.600 --> 05:35.240
you know, trying to spot the whale shark and call down to us. And the sad story is that we saw

05:35.240 --> 05:40.040
absolutely no whale sharks while we were baking in the Sun for an entire week. But I had the second

05:40.040 --> 05:46.200
spark, which is as I sat there in the boat next to this field biologist who's got a spear and a

05:46.200 --> 05:53.160
plastic tag that's almost like, you know, the somebody's title on their desk, you know,

05:53.160 --> 05:57.480
those sliding plastic tags that they might have saying their name and their title. It's about

05:57.480 --> 06:02.680
that size and often it will have letters and numbers on it indicating the individual that they

06:02.680 --> 06:07.320
tagged. So I'm sitting next to this biologist for a full week, nothing to do. And he's got this

06:07.320 --> 06:12.120
spear in this tag and I ask him, you know, what percentage of the time do you, you know,

06:12.120 --> 06:17.080
recite this tag? You're about to spear on this side of a whale shark and he said, oh, you know,

06:17.080 --> 06:22.280
about 1% of the time. And I said, wait a second, 1% because I have an engineering background and

06:22.280 --> 06:27.480
I guarantee you anytime you have a process that's 1% efficient, I can get you to two, right? Just

06:27.480 --> 06:34.840
just out of rock curiosity and labor. And that was the second kick in the pants. I happened to

06:34.840 --> 06:40.520
have graduated Georgetown with my Arab studies degree was actually working at a company called

06:40.520 --> 06:47.720
Softworks EMC, then EMC, then now it's called Dell EMC. And I had, I was learning to code and I had

06:47.720 --> 06:52.920
all this extra time in my hands. I was young and single and I started thinking about the pattern

06:52.920 --> 06:58.520
of spots on the whale shark and whale shark can have blue or brownish skin, but it has all these

06:58.520 --> 07:03.720
white spots like constellations just rippling across both sides and the top of the animal.

07:04.360 --> 07:08.520
And I started thinking about using it as fingerprints and and I wasn't the first person to think

07:08.520 --> 07:14.040
of that. It had been theorized, but nobody had actually sort of implemented a pattern recognition

07:14.040 --> 07:20.280
system for whale sharks. And here you see the hints of of computer vision emerging, right?

07:20.920 --> 07:27.960
So this is this is fall 2002 and I began creating my own, you know, basic trigonometry type

07:27.960 --> 07:32.520
spot pattern recognition system where I would take a photograph and I would map the spots and I

07:32.520 --> 07:37.560
would take those sets of coordinates and have, you know, 10 comparison sets in my list and then

07:37.560 --> 07:42.600
my N plus one set of coordinates. And I would try to come up with some complex trigonometry.

07:43.480 --> 07:48.120
And and really wasn't getting anywhere, you know, by about the 11th or the 12th pattern,

07:48.120 --> 07:53.480
my accuracy would just start falling apart. And so I remember working late one night, not

07:53.480 --> 07:58.120
not on work, but actually on this. It was a Friday night and I had no plans and a friend of mine,

07:58.840 --> 08:03.960
who we now have a really long term friendship. He is Dr. Zauvin Erzemanian at NASA Greenbelt.

08:03.960 --> 08:11.320
He called me out for a beer and as some discoveries go, they happen over a beer. And so I sort of

08:11.320 --> 08:16.440
dejectedly say, well, yeah, sure, why not? I'm not making any progress here on Friday night.

08:16.440 --> 08:22.840
So I go downtown Washington, DC and I sit down with Zauvin and order a beer and he has another

08:22.840 --> 08:26.920
astronomer with him there coming back from a convention. This is an optical astronomer.

08:26.920 --> 08:31.960
And Zauvin says, hey, why don't you tell my friend about what you're trying to do with with the sharks?

08:31.960 --> 08:37.000
I said, well, you know, I'm trying to map the spots on the side of a whale shark and compare the

08:37.000 --> 08:42.760
patterns across photographs. And casual as can be this optical astronomer whose name I've forgotten

08:42.760 --> 08:48.120
at the moment just turns to me and goes, oh, yeah, we do that. What are you talking about? Hold on a

08:48.120 --> 08:52.120
second. What are you talking about? And he said, well, in optical astronomy, we take multiple pictures

08:52.120 --> 08:57.640
of the night sky and we want to create large composite images. And sure enough, what we need to do

08:57.640 --> 09:02.040
is triangulate on different constellations. And that allows us to get key points and create master

09:02.040 --> 09:07.880
image sets. And I was just blown away. And so, you know, he references me to the right papers.

09:07.880 --> 09:12.120
And then Zauvin really sees that I'm going to put in the effort. And so he sits down as an astronomer

09:12.120 --> 09:16.120
to help me, even though he's a pulsar astronomer, he, you know, moon lighting and whale shark research

09:16.120 --> 09:20.680
is just something you might do at that level, I guess. I'm not sure I saw the connection coming

09:20.680 --> 09:26.920
when you referred to the spot patterns as constellations. Yeah, exactly. And this just sort of all

09:26.920 --> 09:33.480
unfolds in this, you know, amazing way. And so Zauvin and I sit down and translate this 1984

09:33.480 --> 09:39.880
arcane paper by, I believe it's Edward growth, who it turns out also as, you know, as these things

09:39.880 --> 09:45.080
do, sometimes that Zauvin used to play baseball with. But nonetheless, this Edward growth is a

09:45.080 --> 09:49.960
professor who's funded under the Hubble Space Telescope. He develops this algorithm for mapping

09:49.960 --> 09:56.440
and comparing star patterns across photographs. And it works. We modify it a little bit. You know,

09:56.440 --> 10:00.280
the photographs of the night sky can have, you know, need to be rotationally independent,

10:00.280 --> 10:03.720
depending on, depending on where the observer is, but whale sharks have a top and a bottom. So

10:03.720 --> 10:10.440
we make these changes. And voila, we have this highly accurate whale shark spot pattern comparison

10:10.440 --> 10:15.320
system. And we can use the patterns on the left and the right side as a pair of tags, like a left

10:15.320 --> 10:21.960
and a right thumbprint. So now we have the inklings of a, you know, an early 2000s computer vision

10:21.960 --> 10:27.400
matching system and one that's actually fairly well-defined and proven. Here's the other problem,

10:27.400 --> 10:32.280
though. Where do I get all these photographs from? So at this point, I reach out to a biologist

10:32.280 --> 10:39.240
named Brad Norman in Western Australia. He had worked for years swimming with the whale sharks

10:39.240 --> 10:45.080
that congregate off X-Muth and Coral Bay up in the northwest part of Australia. And it's a

10:45.080 --> 10:51.000
really good tourism destination, very reliable place to go and swim with these whale sharks from about,

10:51.000 --> 10:57.160
let's say, late March to late July every year. And he'd been collecting these photographs on the

10:57.160 --> 11:02.920
off chance that somebody came up with this algorithm. So I blindly reach out to Brad Norman via

11:02.920 --> 11:07.720
email and he responds very skeptically and I say, hey, you know, I think I might have this and you

11:07.720 --> 11:13.960
might have the other piece, which is the data. And so we begin this long-distance collaboration

11:13.960 --> 11:20.040
showing that this computer vision system actually worked. And we could reliably across years,

11:20.040 --> 11:25.640
across data sets, across photographs, compare individual whale sharks and determine without

11:25.640 --> 11:29.800
physically tagging the animal. That in fact, years later, this was the same whale shark we might

11:29.800 --> 11:35.320
have seen years before. So then we hit what we sort of thought that was the big problem, like

11:35.320 --> 11:39.240
creating the pattern recognition system. But what we actually found is there was simply no

11:39.880 --> 11:45.400
database content management system for wildlife at all, not even a data model that really could

11:45.400 --> 11:51.080
be used out of the box. And so it turns out that 90% of the work was creating the precursors to what

11:51.080 --> 11:58.200
we now call wild book, which is an open source platform to marry good data management,

11:58.840 --> 12:06.680
computer vision, and citizen science altogether to create a platform for wildlife biologists to

12:06.680 --> 12:13.000
collect more data, process it faster, especially in the context of what's called marker capture.

12:13.000 --> 12:20.840
So if you've ever seen the tag on a deer's ear hanging off, or if you've ever seen the band

12:20.840 --> 12:26.360
on a bird's foot with a number on it, those are the tags. And for whale sharks, it was the spear

12:26.360 --> 12:31.880
and the plastic tag. But those are used to repeatedly identify individuals in a population

12:31.880 --> 12:36.760
in what's called mark and recapture studies. And out the other end, you can either get an abundance

12:36.760 --> 12:41.480
estimate how many of these critically endangered animals do we have, or at least a population

12:41.480 --> 12:45.960
trajectory? Are there numbers relatively getting bigger or smaller from when we started sampling?

12:46.680 --> 12:53.640
And so it turns out that photographic data is now so ubiquitous that harnessing it means that

12:53.640 --> 12:59.400
wildlife biologists who tend to still use Microsoft access and excel on a daily basis, you know,

12:59.400 --> 13:04.840
never mind cloud computing, which is not even a new concept anymore, that they could suddenly

13:04.840 --> 13:11.000
have a platform to utilize this much richer source of data. And it turns out it's much better for

13:11.000 --> 13:15.880
the animals. We're not physically harming them or getting in the way of their daily life.

13:17.320 --> 13:22.440
And so our success applying this to whale sharks, building a cloud platform that allowed

13:22.440 --> 13:27.000
citizen scientists, divers, and snarklers to submit their photographs, the computer vision,

13:27.000 --> 13:32.680
which was the carrot that researchers needed to drop their competitive boundaries and begin

13:32.680 --> 13:37.960
collaborating for what is a massively migratory fish. All of this just sort of melded nicely

13:37.960 --> 13:43.960
into what is now whaleshark.org, the wild book for whale sharks. And whaleshark.org has over 150

13:43.960 --> 13:49.320
researchers and volunteers all over the globe who log in. And about close to 6,000 people from

13:49.320 --> 13:55.960
the public have contributed photographs. That model has proved so successful and so cost-effective

13:55.960 --> 14:03.560
that we've started replicating it for giant manta rays for humpback whales and for zebras and

14:03.560 --> 14:10.440
giraffes now. And as we started achieving success, that's when a new cadre of researchers showed up.

14:11.320 --> 14:15.160
Those researchers are now my copy eyes on the wild book project, their

14:15.800 --> 14:21.080
Professor Tony Burger Wolf, University of Illinois Chicago. She's the overall project leader.

14:21.080 --> 14:26.440
She does data science. We work with Professor Chuck Stewart, Rensselaer Polytechnic Institute.

14:26.440 --> 14:31.400
He has this amazing lab almost exclusively dedicated to wildlife computer vision.

14:31.400 --> 14:36.200
And I actually was able to hire one of his students earlier this year. That's another story related

14:36.200 --> 14:41.480
to an anonymous Bitcoin millionaire. If you've ever heard of the Pineapple Fund, we can talk about

14:41.480 --> 14:48.920
that in a bit. And I had no. It's an interesting story. And then Dr. Dan Rubenstein, professor of

14:48.920 --> 14:57.240
ecology at Princeton University. And somehow, and I feel as I've told you this story that how much

14:57.240 --> 15:02.360
luck has played, I lucked into a group of four collaborative PIs. And at one point,

15:02.360 --> 15:07.000
we all just looked at each other via Skype or whatever mechanism it was at the time. And we

15:07.000 --> 15:10.920
said, you know, this is the project of a lifetime. We all sort of nodded. And there's this

15:10.920 --> 15:17.640
general sense that funded or not funded somehow, we're just going to get this done. And amazingly,

15:17.640 --> 15:22.440
we are now a group of specialists working together before it was me sort of trying to hack

15:22.440 --> 15:27.160
through a computer vision system and apply the data management. Now we have this wonderful pathway

15:27.160 --> 15:31.640
of original research for computer vision done at Rensselaer Polytechnic advised by Tanya at

15:32.200 --> 15:38.040
UIC. And then we transitioned that over to software engineering team at Wild Me. So Wild Me is

15:38.040 --> 15:43.160
executive director. I manage four staff, one machine learning expert, Jason Parham. And then

15:43.800 --> 15:50.760
three software developers, Zhu Blunt, John Vandost, and Colin Kingan, all of whom are nine to five

15:50.760 --> 15:55.800
professional software developers working on wildlife conservation. And just the fact that for

15:55.800 --> 16:01.160
however long we can afford them, we can afford this dedicated of a team is it is a giant coup in

16:01.160 --> 16:06.920
conservation. I'm I'm just excited at where we are. And if you've ever heard of Microsoft's AI

16:06.920 --> 16:13.320
for Earth program, they're investing about 50 million over the next five years into applying AI

16:13.320 --> 16:18.200
for Earth conservation. We just signed to deal with them and they are our biggest supporters at

16:18.200 --> 16:23.480
the moment. And we're excited about working with them and Azure cognitive services and a growing

16:23.480 --> 16:30.040
relationship with them as well. What a great story. What a great story. You mentioned in there

16:30.040 --> 16:38.200
that computer vision started to affect this shift in the I guess the academic culture almost

16:38.920 --> 16:45.240
within this community to make researchers more collaborative. Can you elaborate on that a little

16:45.240 --> 16:53.080
bit? Sure. So for some animals that don't move very far, it's it's very cost effective for

16:53.080 --> 16:57.640
a researcher to go into the field regularly to observe them, especially if they're located

16:57.640 --> 17:03.560
near the animal and to collect as much data as possible about a population. These in these

17:03.560 --> 17:09.960
researchers can almost do a complete census. Many animals are not like that. They are migratory,

17:10.520 --> 17:15.000
especially in the marine environment. They can dive deep and have many unobservable states.

17:15.000 --> 17:20.120
And this is where mark or capture modeling showing up at a field site observing who's there and

17:20.120 --> 17:25.880
who isn't repeatedly comes into play. But in the marine environment or in remote environments,

17:25.880 --> 17:30.600
it's just not cost effective to have a researcher co-located there all the time. Sometimes the

17:30.600 --> 17:35.880
animals have migrated away. Sometimes they've simply moved to an area that's remote and difficult

17:35.880 --> 17:42.040
to get to. But all of that is extremely expensive both to support the researcher and to just keep

17:42.040 --> 17:48.680
them in the field. With with marker capture, the idea is you show up and sub sample the population

17:48.680 --> 17:54.440
at certain times. And really a lot of the marker capture models that are used to estimate these

17:54.440 --> 17:58.840
critically endangered and endangered populations are sort of based around human limitations.

17:58.840 --> 18:03.720
There's this assumption inside the models as you as you model capture probability as you mount

18:03.720 --> 18:09.240
model survival rates to ultimately arrive at abundance or population trajectory that the amount

18:09.240 --> 18:13.720
of time that is passed between your sampling sessions far outweighs the size of your sampling

18:13.720 --> 18:19.400
sessions. So they're not meant for continuous monitoring. And also research collaborations

18:19.400 --> 18:27.720
themselves and the competitiveness in academia really create these silos inside what could be

18:27.720 --> 18:33.080
thought of as a master data set. You know, a zebra does not know where the geographic border is

18:33.080 --> 18:38.440
where one research team begins and another ends saying for a whale shark or a humpback whale is

18:38.440 --> 18:45.160
they migrate. We're sub sampling data based on really human limitations of funding and time

18:45.160 --> 18:50.600
and competitiveness rather than sort of getting the superset of data, which is what are these animals

18:50.600 --> 18:56.040
doing all the time? Where are they? When are they there? Who are they interacting with? You know,

18:56.040 --> 19:02.120
what behaviors are they undertaking in these locations? So the idea is how do we how do we push out

19:02.120 --> 19:07.800
this human limited data collection into something much more scalable? And that's where citizen

19:07.800 --> 19:15.000
science comes in. You know, can we get a minimum high quality volume of data from citizen scientists

19:15.000 --> 19:19.560
or a minimum quality of good data from citizen scientists from the public from people on whale

19:19.560 --> 19:27.640
watching boats from people doing safaris in Kenya? Can their photos be used as data? And that

19:27.640 --> 19:32.200
amplifies the data collection of a researcher. Now we start at that point also getting into things

19:32.200 --> 19:39.480
like bias, you know, inconsistent effort in the data collection and the sampling. But importantly,

19:39.480 --> 19:44.440
the largest volume of data that a researcher can get is visual at this point. Everyone's out now

19:44.440 --> 19:50.040
in this modern world where we all have cameras, multiple cameras attached to us, especially when we

19:50.040 --> 19:56.280
go traveling. That volume of data is sort of yet unharnessed and especially for animals that are

19:56.280 --> 20:02.680
individually identifiable. Getting those photos is really getting a data point for researchers.

20:02.680 --> 20:08.680
The problem is is if that if you're still in access and excel on a desktop and have your photos

20:08.680 --> 20:15.240
sort of sitting in folder drives, the flood of data that you can get is far outpaces the ability

20:15.240 --> 20:21.160
of a research team to curate it. Let me give you an example. We work with Sarah Soda Dolphin

20:21.160 --> 20:26.600
Research Project and they identify individual dolphins, bottled nose dolphins by taking pictures

20:26.600 --> 20:32.680
of their dorsal fin as it pops up out of the surface. So when they have a huge collection of dolphins,

20:32.680 --> 20:37.480
thousands of individuals, when they get that N plus one photo, that new photo and they decide,

20:37.480 --> 20:44.680
okay, what, which dolphin is this? It takes them approximately nine hours to match that one photo.

20:44.680 --> 20:51.480
Similarly, if you go to Cascadia Research Collective in Olympia, Washington and they maintain this

20:51.480 --> 20:57.880
master set of humpback whale imagery, if you stand in the middle of their offices and you look

20:57.880 --> 21:04.920
around you in 360 degree view, you see interns looking at images side by side. And so there's a lot

21:04.920 --> 21:10.760
of inefficiency in manually reviewing all this visual data that the world can provide and that

21:10.760 --> 21:15.800
as a sort of long-winded answer to your question is why computer vision is just so critical.

21:15.800 --> 21:21.560
It is a literal time and cost savings to these researchers whose funding just doesn't scale very well.

21:22.440 --> 21:30.680
I guess I'm thinking of the way that computer vision is kind of evolved in this application since

21:30.680 --> 21:38.200
your initial models, clearly deep learning and convolutional neural nets. Well, I'm imagining

21:38.200 --> 21:44.440
or playing a huge role here as they are in other places. I'm wondering particularly if there are

21:44.440 --> 21:52.280
any unique characteristics of this type of data that change what you can or what you can't do or

21:52.280 --> 21:59.000
the way the types of models that you use to identify these specimens. Sure. And you're absolutely

21:59.000 --> 22:04.760
right. The convolutional neural networks are changing this and that's why our partnership with

22:04.760 --> 22:11.480
Rensselord Polytechnic is so important. The crude algorithm we still use for whale sharks has

22:11.480 --> 22:16.120
been replaced for other species. For humpback whales, we use deep convolutional neural networks

22:16.840 --> 22:23.560
to identify humpback flukes that the backside, the underside of their tail, if you will, in two

22:23.560 --> 22:31.640
different ways. These tools are also proving much more scalable, but let me take a step back here.

22:31.640 --> 22:37.320
A lot of times the challenges that I read about in the literature in computer vision are about

22:37.320 --> 22:44.680
categorization. Maybe identify the species. Maybe there's tens of thousands. Or more

22:44.680 --> 22:49.400
ideally, you're looking at categorizing things. It might be a binary, you know, is this a failed part

22:49.400 --> 22:54.680
or a past part? It might be, you know, put these images into 10 different buckets. The interesting

22:54.680 --> 23:01.240
thing about computer vision for wildlife, especially for identifying individuals, is that

23:01.240 --> 23:05.880
by nature, we're searching for an individual, we're searching for a needle in a haystack.

23:05.880 --> 23:10.360
Let me give you an example of what the problem. So my first attempt at humpback whales and doing

23:10.360 --> 23:17.240
a computer vision system, I sat down and started playing with a fairly basic computer vision algorithm.

23:17.240 --> 23:24.760
And my first competent pass, I was 99.8% successful in categorizing my photos. And I thought,

23:24.760 --> 23:28.920
wait a second, I must be a genius. I must be good at this. This is my calling in life.

23:28.920 --> 23:37.240
And then I looked at what the algorithm had produced. And I had created a machine learning agent

23:37.240 --> 23:42.440
that predicted false 100% of the time. Because when you have, yeah, it cheated. And you know,

23:42.440 --> 23:47.400
what? It was accurate 99.8% of the time. Because when you have one photograph and you're looking

23:47.400 --> 23:53.560
through 50,000 photographs for an individual you may have only seen once before, your probability

23:53.560 --> 24:00.760
that it's false is almost always 100%. It's so close to 100%. So, you know, applying computer

24:00.760 --> 24:07.240
vision to identify individual animals in a field of many thousands of humpback whales, over 9,000,

24:07.240 --> 24:13.160
you know, among 50, 60,000 photographs, it's a needle in a haystack problem. And that's why

24:13.160 --> 24:19.160
deep learning is so incredibly important here. I'll give you an example of some of the research

24:19.160 --> 24:25.240
that's coming out of RPI that we're adapting. My team is adapting a wild me. There's a PhD candidate

24:25.240 --> 24:30.120
Hendrick Weedeman at Rensselaer Polytechnic. He's creating an algorithm called curve rank.

24:30.120 --> 24:36.280
And sort of tying in with that Sarasota dolphin research project. The idea of curve rank is that

24:36.280 --> 24:43.480
that the edge of a dolphin fin is individually identified. Now, previous attempts for edge

24:43.480 --> 24:48.920
identification have absolutely occurred. They've used, you know, for example, dynamic time warping as

24:48.920 --> 24:55.400
a basic, you know, point pattern matching or line matching algorithm. The problem is is that

24:55.400 --> 25:01.480
not every bit of that fin is actually indicative of individually identifiable information. In fact,

25:01.480 --> 25:06.520
most of it isn't. Most of it's fairly flat and generic. So, what's important about Hendrick's work

25:06.520 --> 25:12.920
is given this catalog of multiple photos per marked individual in the dolphin population

25:12.920 --> 25:18.120
that the Sarasota dolphin research project provided. He learns which section of the fin

25:18.120 --> 25:23.960
contains that individually identifiable information. And that then allows us to only compare those

25:23.960 --> 25:29.800
small sections of the fin to give us the ability to, you know, go through that haystack and find

25:29.800 --> 25:36.840
that needle. I guess that there have been research efforts around what's called fine grain

25:36.840 --> 25:42.600
classification problems. Are you familiar with that work and does that apply? Here is it a

25:42.600 --> 25:49.800
different formulation of the problem when you're trying to identify the individual. Do you even

25:49.800 --> 25:56.120
think of it as a classification problem or something different? And this may be that I'm somewhat

25:56.120 --> 26:00.520
of an outsider coming into the machine learning community. I generally don't think of it as a

26:00.520 --> 26:06.840
classification problem. I do have intelligent agent classification problems that I'd like to go

26:06.840 --> 26:13.000
into, but in terms of the individual identification, I generally don't think of it as a classification

26:13.000 --> 26:18.520
problem. Okay. And I don't think we, you know, for each individual, I don't think we train up an

26:18.520 --> 26:27.000
individual classifier, if you will, although I'm aware of others. Let's see. There's Ben Hughes

26:27.000 --> 26:31.720
out of the UK. He's been working on white shark fin identification, and he absolutely builds a

26:31.720 --> 26:36.760
classifier per marked individual. So I believe that is more of a classification, traditional classification

26:36.760 --> 26:45.000
problem. Okay. So you alluded to some new work that you're doing to build intelligent agents around

26:45.000 --> 26:51.000
Wild Book. Can you talk a little bit about that? Sure. So I'm building what is I believe a combination

26:51.000 --> 26:59.320
of a model based reflex agent and a learning agent to data mine YouTube for wildlife sightings that

26:59.320 --> 27:05.480
get missed. And so there's a short story that I'll get into here that describes why I'm going

27:05.480 --> 27:10.120
about that. And this is my individual work at Wild Me when being executive director gives me a

27:10.120 --> 27:16.840
little bit of breathing room. This is what I like to work on. So flashback to 2014 and 2015,

27:16.840 --> 27:22.520
I'm running whale shark dot org, published a couple of papers on marker capture populations

27:22.520 --> 27:27.640
off Mingulu reef in Western Australia. I've just had children and I'm most of the time being

27:27.640 --> 27:33.960
a good parent sitting at home in front and Saturday nights, feeling like my contribution to wildlife

27:33.960 --> 27:39.160
conservation is sort of falling away to just programming and maintenance. So I really felt like,

27:40.040 --> 27:43.320
I can't be in the field right now. My family needs me. I can't be scuba diving, but

27:44.680 --> 27:49.480
isn't there some data that I could collect? Something that other people generally don't have access

27:49.480 --> 27:55.960
to other researchers in the community. So I started while watching TV just surfing YouTube for

27:55.960 --> 28:02.040
whale shark videos and started downloading them, capturing key frames, mapping the spots on the

28:02.040 --> 28:07.400
whale shark, submitting it in a wild book, running identification. And it turns out that in 2014

28:07.400 --> 28:13.880
in 2015, just doing that, I collected more data than any individual researcher in the field and

28:13.880 --> 28:18.520
identified more marked individuals. And that really got me thinking, okay, that's great. Except

28:18.520 --> 28:23.240
I don't want to keep doing this. This is actually kind of boring, but I'd like to automate this.

28:23.240 --> 28:29.320
And so piece by piece, as this wild book collaboration evolved, and as Rensseler Polytechnic

28:29.320 --> 28:36.760
trained up a convolutional neural network to detect whale sharks in imagery, then I was able to

28:36.760 --> 28:40.840
bring that in and say, okay, if I download all the videos, can you tell me, are there clusters of

28:40.840 --> 28:45.240
key frames with a high enough confidence that we can say there's a whale shark in it? And then as I

28:45.240 --> 28:49.160
started canvassing the literature for other things, like, you know, how would I determine based on

28:49.160 --> 28:54.280
freeform text in a YouTube video when this video occurred and where it occurred? I started seeing

28:54.280 --> 28:59.960
the different machine learning components that were needed, both computer vision and in the natural

28:59.960 --> 29:05.640
language processing sphere, that would be needed to replicate my actions. Essentially, I was trying

29:05.640 --> 29:09.960
to put myself out of a job with machine learning. Now, it wasn't a paying job. It was just a volunteer

29:09.960 --> 29:15.800
gig, but the idea is, you know, is potentially this the new way of working, of building yourself

29:15.800 --> 29:21.880
a set of intelligent agents that do work for you as part of your job. And in this case,

29:22.680 --> 29:27.320
if we considered my job trying to find whale shark data, then we've absolutely built that.

29:27.320 --> 29:34.120
And the way the system works is every night at 10 p.m. on our server, a little agent wakes up,

29:34.120 --> 29:41.000
and it asks YouTube, you know, tell me everything that's been, or give me a list of videos that have

29:41.000 --> 29:45.560
been uploaded in the past 24 hours that are tagged or titled or described with the word whale shark,

29:45.560 --> 29:50.280
or in Spanish, Tiber on Bayana. And we want to expand this to other languages. We then get a list

29:50.280 --> 29:56.600
from YouTube back of, you know, in JSON, of the titles, the tags, and the descriptions. And

29:56.600 --> 30:04.360
based on how I historically curated that data from 2014 and 2015, I'm serving as the critic

30:04.360 --> 30:11.240
in the learning-based model, it then will make a prediction on true or false. Does this,

30:11.240 --> 30:16.440
the based on the way the video is described, does it contain a wildlife sighting? And that's

30:16.440 --> 30:22.040
important because it's really acting as a very powerful filter. And I retrain it periodically,

30:22.040 --> 30:28.280
so as I make decisions, ultimate decisions on what it collects, it then learns on what I said,

30:28.280 --> 30:32.200
yeah, that was right, or know that was wrong, and read is able to make better predictions.

30:32.200 --> 30:36.200
So it first makes this true false, is this a wildlife sighting whale shark. And that's

30:36.200 --> 30:41.320
important because the word whale shark will show up in grand theft auto videos and all kinds of

30:41.960 --> 30:46.280
random places you wouldn't expect, right? And also, you know, just things like the Georgia Aquarium

30:46.280 --> 30:51.320
as a whale shark. So it sounds like though that you had the foresight to collect all of that

30:51.320 --> 30:57.640
descriptive information and your ultimate decision when you first started classifying these

30:57.640 --> 31:02.120
these videos. Absolutely. And it's made me really think about, you know, what are the other things

31:02.120 --> 31:07.960
in my job that I could start logging now, what decisions I made that all eventually I could train

31:07.960 --> 31:13.160
machine learning on. And because making a prediction, you know, it's a very human thing that

31:13.160 --> 31:18.680
really cross applies to machine learning quite well. So this agent makes that initial prediction.

31:18.680 --> 31:23.240
And it says, all right, based on how this is tag titled and described, I think this is a whale

31:23.240 --> 31:29.080
shark sighting in the wild. And then it goes on to the next step, which is it will then download

31:29.080 --> 31:34.840
the video, sample the video every two seconds, and then take those key frames and look for high

31:34.840 --> 31:42.360
confidence clusters using a trained convolutional neural network. And we fed data from whale shark.org

31:42.360 --> 31:47.160
in. We did a mechanical Turk process to find the whale shark in the imagery, trained up a detector,

31:47.160 --> 31:52.760
and now that detector gives us confidence scores. And if across the video, at least one of the

31:52.760 --> 31:59.880
frames rises above a confidence threshold, we then go on. If not, then we leave. And the interaction

31:59.880 --> 32:05.160
of the predictor and the computer vision piece is important because, for example, in the vision

32:05.160 --> 32:09.480
piece, there's a whale shark somewhere in the world painted on the side of a Chinese airplane.

32:09.480 --> 32:13.640
And that video periodically shows up. So we need the vision to find whale sharks, but then we need

32:13.640 --> 32:19.320
the oftentimes that predictor to throw it out and say, okay, no, no, no, we're not describing,

32:19.320 --> 32:23.000
you know, a whale shark off the Philippines. We're talking about an airplane, so ignore that. So

32:23.000 --> 32:27.800
those two interplay quite well and serve as a very strong filter. At this point, once we've

32:27.800 --> 32:33.800
gotten past the predictor and computer vision, we're on to thinking, okay, this is probably a whale

32:33.800 --> 32:40.280
shark in a YouTube video. And it's at this point that I will take the title tags in description and

32:40.280 --> 32:46.920
send them up to Azure Cognitive Services and say, all right, do language detection, pretty common

32:46.920 --> 32:52.760
natural language processing task. If any of them are not in English, I will then re-ask Cognitive

32:52.760 --> 32:57.640
Services to use neural machine translation, translate back to English. And then we'll take that

32:57.640 --> 33:03.800
master string that's, I'll concatenate all of it. And then I will run a Stanford package called

33:03.800 --> 33:10.840
SU Time, which uses named entity recognition to tell me when this occurred. And importantly,

33:10.840 --> 33:15.880
I feed in the date that the video was posted, and that contextualizes the description. So if

33:15.880 --> 33:20.280
somebody says, you know, I saw this whale shark last week, or if they gave a formal date,

33:20.280 --> 33:28.040
2018, dash 06, dash 24, what have you? It's able to resolve that and give me back an ISO 8601 date.

33:28.040 --> 33:32.760
Now I know when. So I've got visual confirmation of a whale shark or high confidence visual

33:32.760 --> 33:37.960
confirmation of a whale shark. I know when based on using natural language processing. And then I do

33:37.960 --> 33:44.120
some just simple string mining data mining to figure out the where. Eventually, I want to train

33:44.120 --> 33:47.560
up machine learning to also detect where. But right now, we just look for keywords.

33:48.440 --> 33:54.040
On that, when part to what degree is that a probabilistic determination? If you've got multiple

33:54.040 --> 34:00.040
possible dates, or if it's, you know, about a week and a half ago, does it just, does it pick

34:00.040 --> 34:05.400
something? Or do you get some weighted set of dates? I'm wondering, I guess, where in the

34:06.200 --> 34:11.080
in the process does the final determination of a candidate date happen?

34:11.080 --> 34:17.960
That's a really good point. And definitely an area for improvement. So what we get back out of

34:17.960 --> 34:24.520
SU time is a ray of, is an array of all of the dates. And then that are present. And it's not

34:24.520 --> 34:29.560
probabilistic. It's anything that it, it determines it finds. Now, there may be some probability threshold

34:29.560 --> 34:34.600
built in internally that I'm not aware of. But it will, it will simply, simply give me a list

34:34.600 --> 34:41.080
back of everything it finds. I will then, because it's giving me a standardized format, I will then

34:41.080 --> 34:47.960
do my best to take the most detailed date. So it might find last year and say, okay, 2017,

34:47.960 --> 34:54.600
it might find, you know, last month. And then it might actually find June 25th, 2008.

34:54.600 --> 35:00.440
I will go for the date that contains the most amount of information, including starting with year

35:00.440 --> 35:07.960
then month, then day. But learning which date should be used is actually something that we could

35:07.960 --> 35:14.520
train machine learning on. Absolutely. And give a probabilistic ranking on that. So it's a great

35:14.520 --> 35:20.440
really great point. We don't do that. But importantly, if we don't have the where or the when

35:20.440 --> 35:26.200
determined, we actually will have the agent post back to YouTube, again, trying to post back

35:26.200 --> 35:29.960
in the, the poster's original language. So we might engage in neural machine translation again

35:29.960 --> 35:35.240
if we detected Spanish, ask them the question, when did you see this well chart or where or where

35:35.240 --> 35:41.480
and when? Meaning in the comments? Yeah. That's pretty cool. It is. And then we just added recently

35:41.480 --> 35:46.680
the listener that will then asynchronously just pop in occasionally and look for the response.

35:46.680 --> 35:51.320
So it's going to look through all the YouTube videos that we have curated, look for those missing

35:51.320 --> 35:58.040
date and location, go out and then look for response, feeding the replies to our comments back

35:58.040 --> 36:03.560
through the machine learning to do location and date detection. And then ultimately in this vein

36:03.560 --> 36:09.080
of that we like to call rewarding the gift of data for the gift of knowledge, we win a researcher

36:09.080 --> 36:14.840
finally approves and says, okay, this is whale shark A001 stumpy often in glue reef. When we make

36:14.840 --> 36:21.240
that ID of the YouTube derived data, we post back the link to the wild book that says, did you know

36:21.720 --> 36:27.080
that we found whale shark A001 in your video? And here's a link to everything we know about it.

36:27.080 --> 36:31.880
And we have to our questions, we have about a 45% response rate, which I think in the world of

36:31.880 --> 36:37.960
marketing is awesome. But when we ask questions, people do reply, the agent listens for the reply

36:37.960 --> 36:44.600
and then tries to inform them of the conclusions we've made based on their data. And the great

36:44.600 --> 36:49.160
thing about this is if we think about citizen science, public participation, there's an upper

36:49.160 --> 36:54.120
limit to the number of people who are participate that's related to the amount of outreach. How much

36:54.120 --> 36:57.640
have we gone out to the public and told them that they can even, you know, contribute data about

36:57.640 --> 37:02.520
zeroes or giraffes or whale sharks. And then there's a subset of those people who are informed that

37:02.520 --> 37:07.080
we'll choose to participate. And so we're always up against this. Well, not enough people know

37:07.080 --> 37:11.960
about this. How do we get the word out, et cetera? The interesting thing about running an intelligent

37:11.960 --> 37:16.840
agent this way is it flips that model. We go to where people are already choosing to participate.

37:16.840 --> 37:23.640
We show up randomly, curate their data under fair use and then inform them what we found.

37:23.640 --> 37:30.040
And all of that is automated. And so, you know, retraining that agent and proving its quality is

37:30.040 --> 37:35.320
always an ongoing mission. But now we're looking to cross apply that to other species. You know,

37:35.320 --> 37:38.600
how do we start doing this from homeback whales and giant mantas, et cetera?

37:39.480 --> 37:46.120
Have you also thought at all about other domains more broadly that you might be able to apply

37:46.120 --> 37:52.520
this approach, meaning beyond wildlife conservation? I've thought about data mining for marketing.

37:52.520 --> 37:57.480
I would be surprised if there wasn't a company already doing this whereby you could monitor

37:57.480 --> 38:03.400
social media using a complex interaction of machine learning and intelligent agent. And, you know,

38:03.400 --> 38:08.840
figure out who is talking about your product, who's having a bad time with it, who's having

38:08.840 --> 38:14.040
a good time with it, who are your influencers, et cetera. But, you know, the combination of computer

38:14.040 --> 38:18.520
vision and natural language processing to really find the product in the imagery is my product

38:18.520 --> 38:23.880
appearing. And then, you know, look for sentiment analysis. What are people saying? Is it generally

38:23.880 --> 38:29.080
good or bad? Look at the comments. Is are those good or bad? And then get a sense of, you know,

38:29.080 --> 38:35.480
how a product is doing in the marketplace. That struck me as a sort of for-profit cross application.

38:36.280 --> 38:42.280
This is really interesting. It just, when I think about the traditional AI, a big part of

38:42.280 --> 38:47.880
traditional AI, maybe in the 80s or something, was research around this whole idea of intelligent

38:47.880 --> 38:53.240
agents. And we'd all have these avatars that, you know, while we sleeper out doing our bidding,

38:53.240 --> 38:58.520
this is one of the best examples I've heard thus far of something along those lines. I mean,

38:58.520 --> 39:03.080
they're probably our others and maybe folks will, you know, write in and tell me about

39:03.080 --> 39:06.520
all the other great examples of this. But it seems like a really interesting

39:07.320 --> 39:14.200
approach to, as you say, curate content, but also engage with folks and, you know, peak

39:14.200 --> 39:22.120
their curiosity, allow them to contribute to this broad effort without, you know, them needed,

39:22.120 --> 39:28.600
needing actively seek it out. It's very cool. Yeah, and it's a little bit mind bending too.

39:28.600 --> 39:33.800
You know, I think about whale shark.org and how we have something like 152 researchers.

39:33.800 --> 39:39.400
And then this one inhuman agent participating in this, you know, global study of the world's

39:39.400 --> 39:45.800
biggest fish. You know, what does that mean in terms of having autonomous agents collect data

39:45.800 --> 39:50.760
and participate? One of the things I want to do is start having the agent not just participate or

39:50.760 --> 39:55.400
interact with the public, but interact with the researchers who are logging into wild book and

39:55.400 --> 40:00.840
curating data. The agent absolutely should show up and tell them it found new data from their study

40:00.840 --> 40:08.360
sites. And then, you know, what can it analyze? What can it inform them of as part of their experience?

40:08.360 --> 40:13.240
You know, we're pretty far from Jarvis and Iron Man, but, you know, we are at least a baby step

40:13.240 --> 40:20.280
in that direction. Right. Right. Very cool. One of the things that you mentioned in the process here

40:20.280 --> 40:27.160
is active learning. So kind of correcting this agent periodically and allowing it to update its

40:28.360 --> 40:34.440
its model, its view of the world. How have you built that into the process? Is it? Do you kind of

40:34.440 --> 40:38.280
just retrain periodically? Or do you have something more sophisticated happening than there?

40:39.000 --> 40:45.640
I retrain periodically. It's basically a dance I do with the agent. So the agent collects data.

40:45.640 --> 40:51.240
I will do the finishing curation on it. Basically deciding, did you get it right or not?

40:52.200 --> 40:59.400
And then that will be a part of the next training set. And in the early stages, you know,

40:59.400 --> 41:04.520
we're talking about thousands of data points. So I'm using a simple random forest algorithm

41:05.240 --> 41:12.520
using trigrams, not especially sophisticated. We're using Weka, the open source package in Java.

41:13.400 --> 41:18.600
But, you know, there's a lot of room for improvement there. And then especially as the agent,

41:19.560 --> 41:25.560
you know, as we get more data and get a little more mature in using this agent, I might

41:25.560 --> 41:31.320
allow it to periodically just retrain itself based on my curation. But then ultimately,

41:31.320 --> 41:35.800
I want to expand it to the other, you know, human researchers participating in the system and

41:35.800 --> 41:43.000
their decisions around the data that it collected. Did you choose Weka based on a preference

41:43.000 --> 41:51.080
for Java or was there some other driver? My background is through Java. I'm rapidly converting

41:51.080 --> 41:57.480
to Python. But, you know, as you've heard from my story, my route into machine learning is so

41:57.480 --> 42:04.920
non-traditional that Weka was a very accessible machine learning tool. It's, you know, the algorithms

42:04.920 --> 42:11.400
inside of it. And these are older techniques, obviously. And for newer techniques, I really value

42:11.400 --> 42:16.600
my staff member, Jason Parham, who is just finishing up his PhD, as well as, you know, our

42:16.600 --> 42:22.360
collaboration with Chuck Stewart, Hendrick Weedeman, over at Rensseler Polytechnic. They're pushing

42:22.360 --> 42:28.280
the boundaries of pattern recognition for wildlife. And then, you know, it's my professional

42:28.280 --> 42:36.360
engineering team's job to implement those paper-worthy algorithms as user-worthy software products,

42:36.360 --> 42:40.360
essentially. Wildbook is open source, but we very much think of it as a product because

42:40.360 --> 42:45.400
ultimately, these different wildlife biologists who are in the field coming back with photographs

42:45.400 --> 42:52.200
of their study populations need to use computer vision successfully. You know, one challenge we

42:52.200 --> 42:56.680
have right now, for example, is humpback whales. We actually have two computer vision algorithms

42:56.680 --> 43:03.080
that operate in the 75 to 80 percent success range, identifying the correct whale in the top one.

43:03.080 --> 43:07.960
But they have different failure modes. So, teaching a field biologist who has no computer vision

43:07.960 --> 43:13.880
experience, how to interpret two different computer vision ranking results, and potentially even

43:13.880 --> 43:19.960
boosting those or using an SVM to create a metascore that is interpretable to them is one of the

43:19.960 --> 43:24.440
usability challenges we still have ahead of us. Right now, we literally show two lists. You know,

43:24.440 --> 43:29.480
computer vision found this and the other computer vision algorithm curve rank found this,

43:29.480 --> 43:33.880
and it's up to them to sort of interpret that. How do we give them more confidence and reduce the

43:33.880 --> 43:39.560
amount of interpretation they have to do? And then, if you push it forward, I'm really excited about

43:39.560 --> 43:47.960
the marriage of statistics with computer vision. You know, a PhD now professor, former PhD candidate

43:47.960 --> 43:53.080
now professor at Eastern Kentucky University named Amanda Ellis, took some of our whale shark work

43:53.080 --> 43:58.520
and said, all right, a human aided by computer vision ultimately decided the IDs of these

43:58.520 --> 44:03.800
different whale sharks and these images. What if we took the cloud of photographs? We use the

44:03.800 --> 44:09.240
computer vision algorithm to create pairwise scores between every image in the cloud. And what if

44:09.240 --> 44:14.680
the whole concept of animal identity was not left to a human being, but it was left to the computer

44:14.680 --> 44:20.840
to find its pathway through the different pairwise relationships and create animal identity

44:20.840 --> 44:26.040
within this cloud of photographs and ultimately provide a population estimate with no human

44:26.040 --> 44:30.360
curation. And interestingly enough, some of the work that Chuck Stewart's lab has been doing at

44:30.360 --> 44:35.160
RPI is related to building the similar data constructs. So I'm trying to put these two groups

44:35.160 --> 44:40.920
together because if we have a cloud of photographs and all the pairwise computer vision relationships

44:40.920 --> 44:45.800
that have been mined among them and were able to pass that over to a statistician, what it means

44:45.800 --> 44:52.600
is we could go from years between population estimates for critically endangered species to weeks,

44:52.600 --> 44:58.280
which means that we can evaluate conservation strategies for these animals faster, right?

44:58.280 --> 45:03.000
Most population papers that come out for a particular population at a study site are generally,

45:03.000 --> 45:08.040
you know, five, six, ten years between population estimates. That's just too slow for these animals

45:08.040 --> 45:13.480
that are critically endangered and declining rapidly. With machine learning, we're able to rapidly

45:13.480 --> 45:19.160
reduce that. It's going to be a lot more research, a lot more development effort, but you know,

45:19.160 --> 45:24.520
it's just crazy to me how many applications for computer vision and machine learning there are

45:24.520 --> 45:30.360
in wildlife research and how much of that skill set is out there untapped for conservation.

45:30.360 --> 45:39.320
I'm curious. You've mentioned on several occasions citizen science as regards the engagement of,

45:39.320 --> 45:44.920
you know, ordinary folks taking pictures, posting them, or maybe labeling things through

45:45.720 --> 45:55.400
wild book. I'm wondering if you have, if you see a role for the analog among data scientists,

45:55.400 --> 46:00.600
you know, as data science and machine learning technologies become more accessible as the data

46:00.600 --> 46:07.560
science community grows and wants to engage in the areas around which they've, they have personal

46:07.560 --> 46:12.840
passions. Do you engage with those communities? Do you see roles for them to support projects

46:12.840 --> 46:19.640
like this? Oh, absolutely. I'm really fascinated about new data science techniques for managing and

46:19.640 --> 46:26.360
increasing volume of data for wildlife conservation and Tanya Burger Wolf, my collaborator at UIC

46:26.360 --> 46:33.480
is investigating this as well. You know, what are the biases in crowdsourced visual data?

46:33.480 --> 46:38.200
Can we create population estimates on that that are as accurate or better than traditionally

46:38.200 --> 46:45.160
collected research data? You know, what are, how big are some of these populations? How can somebody

46:45.160 --> 46:52.360
with data science skills help a researcher parse the statistics? How can they, can they help them

46:52.360 --> 46:57.400
curate their data in such a way that it's easy to get them into the population modeling packages?

46:57.400 --> 47:04.920
Can they come out with the next population model? I really feel like the next generation of

47:04.920 --> 47:12.120
conservation tools needs to be driven by data scientists and machine learning experts. It's the

47:12.120 --> 47:18.520
only way that wildlife conservation, which is still, you know, so unfortunately done on the desktop

47:18.520 --> 47:25.640
can leap 20 years ahead and catch up to some of the other fields that are using machine learning.

47:26.600 --> 47:30.440
It really needs to make that evolutionary leap and it's going to take data scientists,

47:30.440 --> 47:34.280
machine learning experts to help people leap 20 years ahead.

47:34.280 --> 47:39.240
And so if someone who's listening is interested in this area and like to get involved,

47:39.240 --> 47:44.840
how would you recommend they get started? So, you know, the first place to look is locally.

47:44.840 --> 47:49.720
What are some of the wildlife conservation efforts going on locally? And is there a local

47:49.720 --> 47:55.480
researcher you can reach out to? I have had, and no others who have had the same experience,

47:55.480 --> 48:00.920
which is wildlife conservationists, biologists actually tend to really appreciate

48:01.720 --> 48:06.440
IT and data experience. I've gotten nothing but warm welcomes when I've reached out to them

48:06.440 --> 48:10.200
and said, hey, can I help with your data? Can we try to solve a problem for you?

48:10.920 --> 48:15.320
And so, you know, look locally for that. Otherwise, you know, look at Wild Book,

48:15.320 --> 48:20.360
we are an open source package and would love for contribution. And we can absolutely put

48:20.360 --> 48:25.720
you in touch with wildlife biologists who we might not currently have time to do work with,

48:25.720 --> 48:31.160
but who could use Wild Book and might need a new machine learning algorithm for their data.

48:31.160 --> 48:37.560
So, we'd absolutely love to talk to you. Awesome. Awesome. Well, we'll link to Wild Book and to

48:38.600 --> 48:44.840
Wild Me on the show notes page before we wrap up. Is there anything else that you'd like to share?

48:45.640 --> 48:51.160
No, thanks for the opportunity to talk about the Wild Book project, our mission at Wild Me,

48:51.160 --> 48:56.600
and I really encourage your listeners to apply their skills to help with wildlife conservation.

48:56.600 --> 49:02.520
We really need machine learning and data science skills to help out. Fantastic. Thanks so much,

49:02.520 --> 49:12.040
Jason. Thanks, Jim. All right, everyone. That's our show for today. For more information on Jason

49:12.040 --> 49:19.080
or any of the topics covered in this episode, head over to twimlai.com slash talk slash 166.

49:19.080 --> 49:26.760
Don't forget to visit twimlai.com slash nominate to cast your vote for us in the People's Choice

49:26.760 --> 49:56.600
Podcast Awards. And as always, thanks so much for listening and catch you next time.

