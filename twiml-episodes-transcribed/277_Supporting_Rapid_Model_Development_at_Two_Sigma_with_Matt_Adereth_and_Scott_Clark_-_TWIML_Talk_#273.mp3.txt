Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Two weeks ago we celebrated the show's third birthday
and a major listenership milestone. And last week we kicked off the second volume of our
listener favorite AI platform series, sharing more stories of teams working to scale and
industrialize data science and machine learning at their companies.
We've been teasing that there's more to come and today I am super excited to announce
the launch of our inaugural conference, Twimblecon AI Platforms.
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices
necessary to scale the delivery of machine learning and AI in the enterprise.
Now you know Twimble for bringing you dynamic practical conversations via the podcast and
we're creating our Twimblecon events to build on that tradition.
The event will feature two full days of community oriented discussions, live podcast interviews
and practical presentations by great presenters sharing concrete examples from their own experiences.
By creating a space where data science, machine learning, platform engineering and MLOPS
practitioners and leaders can share, learn and connect, the event aspires to help see
the development of an informed and sustainable community of technologists that is well equipped
to meet the current and future needs of their organizations.
Some of the topics that we plan to cover include overcoming the barriers to getting machine
learning and deep learning models into production, how to apply MLOPS and DevOps to your machine
learning workflow, experiences and lessons learned in delivering platform and infrastructure
support for data management, experiment management and model deployment, the latest approaches
platforms and tools for accelerating and scaling the delivery of ML and DL and the enterprise,
platform deployment stories from leading companies like Google, Facebook, Airbnb as well
as traditional enterprises like Comcast and Shell and organizational and cultural best
practices for success.
The two day event will be held on October 1st and 2nd in San Francisco and I would really
love to meet you there.
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first 10 listeners
who register the amazing opportunity to get their ticket for 75% off using the discount
code TwimbleFirst.
Again, the conference site is Twimblecon.com and the code is TwimbleFirst.
I am really grateful to our friends over at Sigopt who stepped up to support this project
in a big way.
In addition to supporting our AI platform's podcast series and next ebook, they've made
a huge commitment to this community by signing on as the first founding sponsor for the event.
Sigopt Software is used by enterprise teams to standardize and scale machine learning experimentation
and optimization across any combination of modeling frameworks, libraries, computing
infrastructure and environment.
Teams like Two Sigma rely on Sigopt software to realize better modeling results much faster
than previously possible.
Of course, to fully grasp its potential, it's best to try it yourself.
And this is why Sigopt is offering you an exclusive opportunity to try their product on some
of your toughest modeling problems for free.
To learn about and take advantage of this offer, visit twimblei.com slash Sigopt.
And now on to the show.
Right everyone, I am here in San Francisco and I am with Matt Adarath, who is a managing
director at Two Sigma Investments and Scott Clark.
Scott is the founder and CEO at Sigopt and has been on this show previously.
Matt and Scott, welcome to this week in machine learning and AI.
Thanks for having us.
Thank you.
Awesome.
So, I'm really excited about diving into this conversation.
We'll be talking about Two Sigma's journey with regard to building out its modeling and
machine learning platform.
Matt, you've not been on the show before.
We'd love to hear a little bit about your background, how you got started working in machine
learning and at this intersection of engineering and modeling.
Yeah, it's always been an area that I've been interested in, the intersection of computer
science and math, that's what I studied at Carnegie Mellon.
And I always wanted to find some place where I could apply both of those disciplines and
learn and grow in those.
So after school, I went to Microsoft where I was there for about four years, working
on office, which there wasn't so much math there, unfortunately, but it was very interesting.
And then I left Microsoft to join a startup that was spun out of Microsoft Research where
I was doing a lot of analytics on social networks.
And I realized that I wanted to be at a place where I'd be able to do the math that I had
learned and loved.
And the thing that occurred to me was that finance would be a great place to do that because
they exist at that intersection of math and computer science.
And then the whole world of finance was also very interesting to me.
So about 12 years ago, I joined Two Sigma, it was much smaller than.
And I started working on tools and infrastructure there for analyzing data and really helped
build out the platform that we have today.
Awesome.
We'll definitely be coming back to that.
But before we do, Scott, give us a refresher about your background and what Sigopt is
up to.
Yeah, definitely.
So Sigopt provides an experimentation and optimization platform that can bolt on top
of platforms like we're talking about today.
And really help amplify and accelerate the way that you get to impact from these models.
I came to this via grad school where basically when I was doing my PhD at Cornell, I saw that
experimentation and optimization was a core part of every project anyone in my department
was doing.
That was reinforced when I went to Yelp and worked on the advertising system.
And so decided to build Sigopt to help solve this problem in an enterprise way for universities,
government agencies and firms like Two Sigma around the world today.
So Matt, you started Two Sigma 12 years ago, you said.
Where was Two Sigma kind of in the journey of a platform?
You started there working on tools and infrastructure as opposed to coming over from data science
where you, you know, one of the first people working on tools in emperor was there already
an established group focused on that.
So there were teams working on it, but it was really more spread across the teams like
our data, we had a data engineering team and there was a modeling engineering team.
But they would each build out their own infrastructure of varying qualities and it was a little
later that we started really trying to find the solution that would work for everyone.
But since the beginning, the company was founded in 2001 and it was started as a quantitative
investment manager with a focus on building out platforms for the things that we were trying
to do with the knowledge that we would try to expand into different businesses and want
to be able to leverage the solutions that we had already built.
So it was already in play when I got there 12 years ago and it was more just like picking
up the torch on certain areas that needed more focus.
Okay.
Okay.
And so maybe talk a little bit about the various constituents you serve or you, do you
have a single type of data scientist or a single type of skill set there?
Do you have engineers as well as data scientists as well as other types of quantitative folks
that are using the tools that you're building?
Yeah. So it really is a wide variety of skill sets that we're trying to serve.
Even within our modeling discipline, there are some folks who are much more technical and
are looking at things from a like a technique perspective and then there are other folks
who are much more focused on understanding their data sets and trying to figure out what
the predictive value is based off of a deep understanding of where the data comes from
and what it means.
And roughly how many users does your group support?
So the company is roughly a third modeling.
So talking around 500.
Wow.
Okay.
And so I guess what I'm curious most about is how the thinking about building out tools
and platforms to support modeling has evolved over the time that you've been there.
So one of the things that we try to do is not be prescriptive about the tools that our
modelers use.
We want to hire the best and we want them to be able to apply the tools and techniques
that they are familiar with and that they are able to leverage the most.
So that's a challenge when we're trying to build up platforms because if we're not being
prescriptive, we can't really limit them.
But what we try to do is identify the common threads that will benefit the most number of
people.
We also try to identify what we think are going to be the winners in terms of technologies
where we can give them a little bit more of a push or support them a little bit more
to make those extra easy to drive people to them without being prescriptive.
How did Sigop come into play?
How did you find one another?
You know, was there a search around sigmas and what companies can we find?
We definitely ran into each other at a lot of the similar academic conferences that
we go to in Publish, whether it's ICML, NERIPS, that kind of thing.
But I think a lot of it, after seeing each other, what drove us together was this shared
desire to be very modeling driven and really help augment and amplify these experts.
So it's not about, again, being prescriptive, like Matt said, it's more about giving them
the tools to run as fast as they can once they've picked the direction.
And so you want to be able to unify and standardize on things that make sense to standardize
on, but then don't do it in a constraining way.
So you want to provide optionality so that they can use their expertise, that context
awareness, that domain expertise.
But at the end of the day, really run as fast as possible.
And I think modeling platforms help with that and agnostic optimization helps with that
and good infrastructure helps with that, but it's really about really empowering these people,
which is that shared vision I think we both have.
Matt, when you think about the end-to-end modeling platform that your team is offering,
how do you articulate the various components of that?
When I think about these kinds of platforms, I tend to think of them in terms of data management
and transformation in those kinds of things, experiment management and production or model
deployment.
Do you have a similar view of the landscape or do you organize things differently?
At a high level, that's how we look at things.
I would break it down a little bit more.
Even at the data management stage, we have a finer breakdown where there are things around
like data ingestion is a big thing for us because we have so many data sets that we take
in that's what we do is we try to bring in as many data sets as possible and find the
value in all of them.
So bring ingesting at scale is a challenge, cleaning at scale and making sure that that
data is not just available for doing data science, but making sure that it's available for
our real-time trading systems, which is an additional challenge because there is this
timeliness aspect to how soon we process the data.
Then on the research side, we break things down even further around not just accessing
the data, but transforming it, sharing transformations is a big concern for us and then modeling.
Okay.
Then on the modeling side, do you have further distinctions within that set of capabilities?
Yeah. So the first part of the modeling capabilities is just doing the preliminary analysis of
the data, like explore to our data analysis that everybody does.
We have a lot of focus on the time seriousness of everything.
We treat everything as a time series.
We kind of have this belief that everything is a time series and if you have something
that you think isn't a time series, you're probably wrong and you're going to regret
not trading it as a time series because it changes and we need to be able to do point
and time simulations and back testing for everything.
So there's this whole phase of analysis and modeling and then once you have something
that you think has predictive value, there's this second stage of seeing how it trades and
that is really a different style of analysis than typical data science outside of
finance.
You mentioned previously that some of the things that you're doing are particularly challenging
at the scale that you're doing.
Can you give us a sense for the scale?
So I mentioned that we have hundreds of researchers and they're each doing lots of different
kinds of analysis at scale, whether they're doing the exploratory data analysis on large
data sets or they're testing out their models and seeing how different parameters perform.
In some of those cases, they may be running thousands or tens of thousands of very expensive
simulations.
So we have a huge demand for compute and a lot of challenges around scaling up to those
managing that amount of compute.
Scott, when you think about the challenges that Matt's talked about from a modeling perspective
and curious from your perspective, having seen this play out at a number of customers, what
of what he described would you say is unique to Sigma and what do you see broadly in the
industry?
Great question.
So I think broadly, we definitely see people doing unique modeling, they're doing differentiated
modeling.
I think special about what they bring to the table, whether it's on the data side or whether
it's the end application, and that's really, again, where domain expertise and contextual
awareness plays a huge role.
And whether that's in the asset management space or whether it's the work that we're doing
with tech companies or the US intelligence community, everybody has something different
they're trying to solve.
And what we've found is more and more companies are starting to invest in platforms.
Because of course, you've seen with the series that you're running and are continuing to run.
And a lot of that comes down to people who are doing this already, they just might not
have been doing it the right way, or they might not have been doing it in a way that
could amplify across the entire organization.
We see the same thing with experimentation.
So there's a lot of parallels in terms of just the core concept of we have data, we're
trying to solve a problem.
It might be making trades in a market, it might be trying to make a recommendation for a
streaming service.
But at the end of the day, it's somebody trying to solve this very specific problem.
And then teams like Matt and teams like Sigopt are trying to really just give them the tools
to do those jobs better, without forcing them into a sandbox or without trying to give
them a one size fits all solution.
When I talk to folks that are trying to provide these types of tools, the goals that they
have are all over the map.
Some folks have these broad, I kind of asked about the different types of users that you're
trying to support.
Some folks, their primary goal is to make machine learning more accessible so that more teams
can make models, can build models, that kind of thing.
Other folks are driving towards their whole existence is around achieving some level of
scale or kind of compressing the innovation cycle, things like that.
How do you think about your prime directives, if you will, what is really driving to
Sigma to continue to invest in this, if you had to stack rank them?
Yeah, so the two things are we want to get better answers and we want to get them faster.
We have lots of modelers, like I mentioned before, so any multiplier on their speed really
adds up.
Not just because there are so many of them, but because of what each of them is doing
is so high value.
The two things that we're looking at are getting better answers and getting them faster.
The better answers can come from applying better techniques that may discover things that
couldn't be discovered otherwise.
The speed comes not just from the speed of the algorithms that are maybe doing these
optimizations, but also the usability of the tool is really critical for us.
It's very easy to lose a day to trying to figuring out some error message or some weird
API that wasn't well designed, so that's something that we're also really focused on.
A big part of that is around these kind of different elements of the platform that we've
talked about, making sure the data is available to folks, making sure they can iterate on experiments
very quickly.
Do you have challenges on the production-alizing side as well, or your users mostly focused
on analytical results in inquiry and not so much deploy models out?
They do deploy models.
We do have our modelers own their models end-to-end.
That's something that we think there's a lot of value in doing, and there are challenges
in writing a production model versus doing something in research that is another focus
for my team, making sure that that transition is as seamless as possible.
A lot of the benefits that you described in terms of increasing the pace of innovation
and getting better results is focused on those researchers and their ability to turn through
the possible solution space, if you will, for these problems that they're trying to solve.
When you think about the things that you've done to address or attack the experimentation
challenge, can you give us a spectrum of the types of things you've done from a platforming
perspective to narrow in on those goals?
Sure.
I can give a couple examples.
One is we focus a lot on the languages that people use, making sure that we have domain-specific
languages that allow our modelers to express their problems really naturally, while still
having the flexibility to cover all of the possible ideas that they might have.
What's the native modeling environment for your folks?
Is it primarily notebooks or something else?
Over the last few years, there's been a big push towards notebooks.
We've open sourced some of the stuff that we've built.
Beaker X is one plugin that runs on top of Jupiter that allows people to work in a really
seamless polyglot environment and has some internal things that are bespoke to our environment
that make it just really easy for people to do the things that they need to do.
Another example of the kinds of things that we focus on to make it easy for them is
around running lots and lots of jobs, specifically back test simulations is a big focus for us.
That's where a significant chunk of our compute is dedicated.
Making that easy for modelers to interact with, to launch things at scale, to monitor and
manage those things and to easily deploy to different cloud providers or reusing our
in-house data centers, we just want to make that all as easy as possible so that they
can get their jobs done quickly.
What's the interface between the researchers and the Sigopt product and its capability?
That's a great question.
We have a few different solutions.
Many folks who use the Sigopt API directly, it solves a problem that lots of people
have and where they know that they have this problem, I have parameters I want to tune
them.
We also have a number of tools that are for solving specific problems that modelers
have that use Sigopt under the hood.
We really like that it works well as just a component in other tools where the model
may not even know that they're using Sigopt.
Scott, is that a pretty common experience among the folks that use Sigopt?
Yeah, I would say there's different tools for different jobs and it's about sometimes
to end platform that allows some flexibility for the modeler and sometimes it's something
where it's a very point solution for a very specific problem.
I guess I have one quick question if that's okay for me.
As you start to build up these platforms, obviously you've been there for 12 years.
You've seen a lot of different iterations of this over time.
How do you like measure the efficacy of some of your efforts?
You guys have been on the bleeding edge for a long time but I imagine there's been several
iterations of infrastructure, of experimentation.
How do you make sure you're making progress or how do you make decisions of what's worth
continuing to invest in, what's worth building versus buying?
We do have a lot of modelers.
It is also few enough that we're able to talk to them and have conversations about what
they find useful.
That's something that we're always doing is just interfacing directly with our users and
understanding what they like and what they don't like.
We do have tons of metrics.
We're a quantitative investment manager so we like to look at the numbers.
We always are looking at things like usage, seeing how that's tracking to help identify
where we should focus.
In some cases, it's not driven by numbers.
It's driven by just keeping an eye out on what's going on in the industry, in the broader
ecosystem of data science and seeing what's applicable to us, and then we evaluate those
things through experiments.
When we were looking at Sigopt, we compared it to not just the in-house solutions that
we were already using but also some other open source things like we looked heavily at
GPIOP as one of the big alternatives and saw how it worked on a variety of optimization
problems that we had identified.
When you were looking at it relative to those other things, just curious, was there any
particular?
Was it a performance motivation that led you towards Sigopt or was it more of a user
experience thing and how do you weight those things?
Yeah.
It's performance, both the quality of the solutions that it finds and how quickly it
finds it, but also the usability and not just is the API sane, but things like how much
tuning do you need to do of your hyperparameter tuning?
We found that for a lot of other solutions, GPIOP in particular, it was very sensitive
to its parameters that you would set it with.
The fact that it wasn't something that we could just run and reliably get answers and
that it was something that you had to fuss with a lot was another qualitative measure
that kind of turned us off from pursuing those and made us go with Sigopt.
You need a hyperparameter optimizer for your hyperparameter optimizer?
Yes.
It sounds like a lot of turtles.
Yeah, the number one piece of feedback we got after I opened source Mo at Yelp was this
is great, but you've taken my optimization problem and turned it into another optimization
problem.
But I guess, I mean, so you have 500 incredibly intelligent modelers, some of which have
very deep expertise in mathematics.
How do you decide for something like this?
Obviously, you tried a bunch of in-house solutions, you looked at a bunch of different things.
How do you make that trade off of, this is worth us spending up a team of 12 people and
attacking it for years versus taking kind of a best in class solution?
How do you make that trade off internally of, this is worth us solving internally versus
this is worth us partnering?
Yeah, well, we looked at the opportunity cost.
We have these folks who we've hired who are very talented, but we want them to be working
on specific problems with the best tools that are available.
So if there's something that already exists and it's a lot cheaper than us paying 12
folks to a year to build out, of course, we're going to go with it.
Yeah, I'm curious about, certainly on the modeling side, a lot of Barton Parcel to experimentation
is, you know, faux, you go down a path, it doesn't work out, you go down another path,
and that's kind of inherent to the modeling process.
I'm curious if there are any things that you can share from a platform perspective, things
that you tried, you were very excited about, they didn't really work out.
They didn't give you the kind of performance or the acceleration that you were looking
for.
One thing that I can cite that's relevant to this is we've been looking at black box
optimization for a very long time.
It's always been an interest of mine.
I knew that it would be applicable to us.
So it was about seven years ago, I started an initiative to make some more black box
optimization algorithms broadly available at the company that, in ways that leveraged
the rest of our platform and were just as easy as possible.
And one of the issues that we ran into very quickly was when we started running things
at a totally new scale that was required for doing these black box searches, a lot of
the small pain points around having like long running simulations fail, all sorts of
things that just go wrong when you have distributed systems.
A lot of those things just broke the user experience completely.
And we had to like table a lot of those initiatives around doing black box optimization to revisit
our core platform for how we run all of these jobs.
And what the interface was, how we deal with things like failures and just how do we make
it work at scale.
So it was kind of a pivot, we took this thing that we knew was a good idea but really kind
of failed because the environment wasn't right and then focused on the environment instead.
And then kind of tabled the black box optimization as a service because it was such a hard problem
to fit your at the platform, which took us years to build out and get right.
So I kind of view that as something that didn't work out, it had these additional benefits
but it also set us up for success later on when something likes it comes along that we
couldn't have leveraged if we didn't already experience our own failures trying to solve
the same problem.
Okay.
And at the infrastructure level, are you now having gone through that process?
Are you using something like a Kubernetes or some other open source or is it a proprietary
distributed computing solution?
Yeah.
So when we started this, it was before Kubernetes was really even a thing.
Okay.
For a lot of the things that we're talking about, we were doing it before it really was
a thing.
Even the term data science wasn't really happening 12 years ago when I started and big
data wasn't a hot phrase that everybody knew.
So our solution was built on top of Apache Mace OS, which we built a framework on top
of it.
It has this nice, pluggable, like framework interface.
So we were able to build something that handled our bespoke scheduling needs.
We open sourced it, our scheduler that we use because we do know that there are other
folks who have kind of similar problems and there are benefits to us to open sourcing
it.
Yeah.
So you started before something like Kubernetes, you were using Mace OS, you know, in that
kind of 12 years ago, time frame, Mace OS was the Kubernetes of distributed compute
in the sense that it was very popular for these kinds of applications and we are investing
heavily in Kubernetes now over the last few years.
Okay.
Yeah.
Okay.
Interesting.
And so you went down this path to build out some distributed optimization stuff.
You ran into limitations of the infrastructure and it sounds like you spent a few years building
out the infrastructure to support kind of where you wanted the platform to go or the kinds
of things that you knew you needed to be able to do.
Can you characterize how much of your efforts are spent kind of at the low level infrastructure
versus the higher level tools and services that are more researcher facing?
Yeah.
So we do have our engineering work is also roughly about a third of the company and it's
broken up into several teams.
I work on modeling engineering where we focus on building up the tools and infrastructure
that are targeting our modelers.
There is a separate team called platform engineering which is providing the platforms
for all engineering and all of the company.
So they're about the same size.
I think modeling engineering might be a little larger.
So what we find is that sometimes we will build out platform solutions to solve a specific
modeling problem and then realize, oh, actually this is more broadly applicable and do a
handoff to another team to the platform engineering folks which has happened with a lot
of our compute specifically.
Did your team kind of identify this problem or Kubernetes in particular and kind of stand
that up and then hand that off to the platform team later or were they already kind of ahead
of that curve or you know, you got the same point in the curve?
So the initiative to adopt MESOS came out of modeling engineering specifically when
we were trying to solve this modeling problem of doing black box optimization at scale
on like simulations.
And then it turned out, yes, this is a workable solution and it is more broadly applicable
so we should transition it over to another work.
Okay, got it and Scott, SIGUP has been doing some things with Kubernetes as well.
Can you talk about that and the relationship between the work you're doing on the optimization
side and the infrastructure do you see in particular do you see folks, other folks kind
of express that same challenge where there's one, there are things they want to do to
empower their modelers and their researchers, but they run into infrastructure challenges?
Definitely.
We see this all the time where especially if you're transitioning from a world where
you're maybe doing manual tuning of a model where it's an expert trying to do it sequentially
in a notebook environment or something like that doing 10-dimensional optimization in
your head is hard enough but trying to do that maybe across a hundred different workers
is maybe impossible.
But in addition to that being a hard optimization problem, it becomes just a hard like resource
management problem of SSHing into a hundred different machines and making sure that they
don't fail like Matt said and everything like that.
So we're seeing more folks really doing it like that.
We've definitely had users where they want to take advantage of our ability to do high
parallelism as part of our optimization suite when we support up to a hundred individual
workers.
I've seen users, yeah, literally SSHing doing a handful of different machines and trying
to like, they're using screen to keep the sessions alive and they're trying to do everything
like that.
And at the end of the day you could be an expert in deep learning and expert modeling,
but then all of a sudden there's this massive barrier of DevOps and doing this right.
So we try to be active members in this community, we're contributors to Kubeflow, we've developed
our own solution called orchestrate that handles all of this for you as well.
Take something that you might have written in a notebook and then very easily you can
containerize it up, pass it off to a Kubernetes cluster, and then sigopt acts as this distributed
scheduler for all of your training and tuning jobs.
And we see this as again helping just amplify what these modelers are already doing.
Like again, you could be an expert in defining the model, understanding the data and things
like that.
But you don't want that barrier of parallelism, that barrier of distribution to be what
prevents you from really getting to that best possible answer.
So we see this as something that some people are building into their platforms.
We see this as some individual researchers just want to be able to have this superpower,
but we're continuing to invest in that, both in actively contributing to the open source
community and building specific tools tailored to the enterprise that we can serve our customers
with.
And across the folks you talk to, how do folks know when they need to transition from this,
I'm going to do optimization, man, I guess there's one, there's the step zero I'm doing
in my head and I need to do it programmatically or I need to do it manually.
And then I need to do it in a more automated fashion.
I'm trying to get a how do folks come to terms with, folks that aren't heavily investing
in platforms or optimization or some type of automation, what are the things that you
see like clicking for them that motivate them to start investing?
Yeah.
So I think it goes back to what Matt was saying around opportunity cost and you can define
opportunity costs in a variety of different ways.
So for some firms we work with, it's about expert productivity.
So we're working with a global technology consulting firm where when they rolled us out
globally, every single team that was using SIGOP was able to complete their client engagements
30% faster.
We're also working with a small startup that has five PhDs doing really cutting edge AI
and their founder says they think of SIGOP is just another member of the team because
it's taken this burden of doing this manual tuning off of them.
And so that expert time can be a massive opportunity cost.
But also, just if the performance of your models matters, that's more opportunity cost
as well.
Because usually optimization of architectures, feature transformations, hyper parameters,
it takes time, but it's usually also orthogonal to any of the feature engineering you're doing.
So it's added benefit that's otherwise being left on the table.
It's not about replacing what your data science was doing.
It's about just adding to what they're doing.
So that's another big opportunity cost.
And then finally, it's time to market its compute costs, it's things like that.
If you're spinning up GPUs and waiting around for sometimes weeks to get the results of
something being tuned, cutting that down to days or hours can really change the way you
do iterative modeling.
And so I think it's really about seeing if you run into one of these pain points or if
there's just a lot of opportunity being left on the table, expert time, compute time to
market, or just the value of these models that you're investing so heavily in.
And are there particular types of models that are prevalent among your researchers?
And from a platform perspective, are you building out specific platform capabilities
to support specific kind of models?
Or do you think about the world more model agnostic?
So like I said before, we try not to be prescriptive about what our modelers do and we really
do everything.
And obviously, over the last few years, there's been a lot of buzz around deep learning and
it is something that we have been looking at really deeply.
No, put on bombs.
But that is one technique where it really did turn out that we need to build some things
that are specific to supporting that technique.
So it is a pretty broad class of models or a broadly applicable technique.
Right.
So we have had to invest into things that are targeting deep learning scenarios.
Just to even test out whether it works on our types of problems.
And as we all know, hyper parameter tuning there is also a big challenge.
So that's another place where we are playing Sigopt.
And was that something that just worked out of the box like, was it, did you have to
do anything special to apply Sigopt or any other other things in your tool chain to deep
learning or was it just work?
The challenge is around getting the machines that have the GPUs that we want in order
to use it effectively.
So that was one thing that we had to change how we do some stuff.
That's me sounds like infrastructure like Kubernetes and scheduling and that kind of thing.
Yes.
Okay.
But as far as Sigopt goes, one of the things that we love about it is how well it works
out of the box and how easily it integrates with so many of our existing platform solutions.
Kind of looking back over the past 12 years are there things that you had approached
very differently knowing what you know now?
That's a great question.
So the one thing that immediately comes to mind is Python and how dominant Python has
become in the data science modeling space.
That is something that certainly wasn't obvious back in 2001, it wasn't obvious when I had
started.
We've been a JVM shot since the beginning.
So built out a lot of tools and infrastructure on the JVM for doing not just for building
or systems, but for doing the analysis that we need to do.
So I would say investing more in Python earlier on, getting on that train would have been
something that I would do differently, but it really has taken over for us, our modelers
come in.
Everybody higher comes in with that as their language of choice, the best library, open source
libraries are out there and obviously the deep learning world is dominated by Python as
well.
So having had that experience, how do you apply that forward?
Does that lead you to wanting to touch and try everything or are there specific things
that you now see were clear in Python and Python's rise that you can apply that pattern
matching to other things?
I don't think so, but I think that the one change is that it's made us approach things
in a more language and like platform agnostic way or not relying on everything happening
on the JVM, so you can't predict what's going to win.
So you have to build something that's flexible and that can support some different things.
It's interesting.
Folks, that end up on both sides of this, it takes a lot more resources to support
something that supports everything, right?
Then it does to build a very targeted solution and be prescriptive.
But this is a great argument for not doing that because you don't know what we all pronounced
in TensorFlow, the winner without any challenges and all of a sudden PyTorch popped up, right?
And now it's totally reasonable to allow your modelers to use that if that's their preferred
kind of interface.
That's just one example of how stuff you think it's a game over and stuff just pops up
out of nowhere.
Yeah, but that said, some of the platform agnostic choices that we make are actually just
the right choice to make any way, or they have additional benefits aside from being somewhat
future-proof.
For instance, data access and doing that through services where we're not having any expectation
of what the consumer is, is an example of a way that we're adapting based off of what
we've observed.
And so if you were talking to a team that has been doing modeling, has been doing data
science, has achieved a level of success there, but is early on in thinking about formalizing
a platform effort, what kind of advice would you give to them?
Well, the world is so different now and also a lot of those places if they're just starting,
they're operating at a very different scale from us.
So the cloud providers and their solutions are wonderful, especially at smaller scales,
but even in some cases for folks at our size.
So picking one and really taking advantage of it and all of the services that they offer,
while being mindful about vendor lock-in is probably what I would say to focus on.
Yeah, when you think about the cloud providers or generally about any kind of package offering,
they often have kind of gaps that get in your way that can be hard to feel like, how
do you balance that concern versus, you know, kind of the ease or just kind of getting
everything all at once, or it is, first of all, is that a challenge that you've had
to kind of engineer around in the past and if so, how do you kind of think through all
that?
It hasn't been so much of a challenge for us because we have been running our own data
centers for so long.
So we've already got things working really well without all of the cloud provider-specific
offerings.
So it made leveraging the cloud provider's compute really easy because we weren't dependent
on any of their bespoke solutions.
More generally than cloud, there are a lot of folks that are a lot of startups and even
mature companies that are trying to offer NN1-size-fits-all platform in a box kind of solutions.
And the route that you've gone is to at least in the case of optimization, identify kind
of a best of breed, you know, focused, targeted thing and kind of plug it into some broader
thing that you're building.
Like when you think about kind of the role of that end-to-end, you know, versus kind of
building based on, you know, individual things that you like, like how do you kind of parse
all of that?
Because we've already been doing it with components, separate components and haven't
been leveraging one end-to-end solution because nobody really does everything that we
need.
I mean, we like to think deeply about every step of the process and find the right solution
for each step.
So that's one of the things that really appealed to us about Sigopt was that it really did target
this one problem and solved it in a way that didn't, that wasn't locking us into any particular
technology before or after that stage, that really was a huge benefit for us.
And then on top of that, the fact that it could be used on any of the cloud providers,
it wasn't just one solution that was like trying to keep us locked in somewhere, it was
also a big benefit.
And Scott, I'm curious if you have any perspective on that, you certainly, I'm sure you get asked
that a lot.
Yeah, definitely.
And I think, again, it's going back to standardizing where it makes sense, but doing so in a way
that's non-constraining.
So as Matt was saying, you don't want to spend all your time being future-proof, but
you definitely need to recognize that the flavor of the month is going to change.
Four years ago, a lot of people using Sigopt were using Psychic Learn primarily, and now
we see a lot of TensorFlow and PyTorch, and we're already starting to see quite a bit
to pick up of reinforcement learning, even in production, with firms like OpenAI that
use us.
And two years from now, it's all probably going to be different, like the landscape continues
to change, and it's not just the landscape of tooling, it's the landscape of infrastructure
providers as well.
Again, four years ago, AWS was so far in front of everybody else that you didn't really
think twice, but now there's other providers out there, people use hybrid solutions, firms
like Dropbox are moving back off of the cloud for cost-saving measures, and it's really
about saying, what can I bring to the problem, what do I differentiate, and then leave optionality
everywhere else?
And that's something that's core to us, being agnostic to the tooling, the infrastructure,
whatever it is.
We want to meet you where you're at and augment what you're doing, not try to fit you into
a sandbox or lock you into something in particular.
Well, Scott and Matt, thanks so much for taking the time to chat about this very, very interesting
topic.
I appreciate having you on the show.
Thanks so much, Sam.
Thank you.
All right, everyone, that's our show for today.
For more information about today's guest, or to follow along with AI Platform Volume
2, visit twemalai.com slash AI Platforms 2.
Make sure you visit twemalcon.com for more information or to register for Twemalcon AI
Platforms.
Thanks again to Sigout for their sponsorship of this series, to check out what they're
up to and take advantage of their exclusive offer for Twemal listeners, visit twemalai.com
slash Sigout.
As always, thanks so much for listening and catch you next time.
