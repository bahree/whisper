Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Once again, the recording you're about to hear is part of a series of interviews
I recorded live from the O'Reilly AI and Stratoconferences in New York City.
My guess this time is Pascal Fung, professor of electrical and computer engineering at
Hong Kong University of Science and Technology.
Pascal gave a really interesting presentation at the AI conference, focused on how we teach
computers and robots to understand human emotion and be empathetic.
She also had some really interesting things to say about the theoretical foundations of
the various modern approaches to speech understanding.
And we dig into all of this in our conversation.
As always, I'll be linking to Pascal and her research in the show notes, which you'll
be able to find at twimlai.com slash talk slash nine.
As is unfortunately the case with my other field recordings, there's a bit of unavoidable
background noise, but it's not too bad.
And now on to the show.
Alright, hey everyone, I'm here at the O'Reilly AI conference and I'm with Pascal Fung,
who is a professor of electrical engineering at Hong Kong University of Science and Technology.
And I sat in on her talk earlier on emotions and AI and how do we enable computers to recognize
emotions.
And she graciously agreed to spend a few minutes with us to tell us a little bit about
what she's working on.
So welcome Pascal.
Thank you.
How about we start with talking a little bit about your background and the kinds of things
you work on?
Sure.
So my background, I am an electrical engineer and computer scientist.
And I've been working on speech recognition since 1988 and then move on to statistical
machine translation in the 90s.
And after I became a professor at the HKUST, I lead a team working on speech language
and more recently on emotion and mood recognition or sort of using statistical modeling and machine
learning methods.
That's my technical background.
I worked at different places before.
I was a student and while working on my PhD thesis at Bell Labs, I was very lucky to work
with some of the best people in the area and in the early 90s when I started my thesis,
it was when the field of natural language processing was transforming from a heavily
knowledge-based, linguistics-based field to statistical modeling.
And at the time, statistical modeling for language was very controversial.
Right.
People actually didn't believe you could learn language or study language or understand
language with just statistics.
But 20 years later, now that is the mainstream approach or everything we do is with machine
learning statistical modeling in natural language processing.
One of the slides you put up had a quote from a professor who I think you mentioned you
work with that said, for every language, like fire, or anything like that.
He's sort of the father of statistical speech recognition.
So the field holds a lot to him.
He passed away a few years ago.
And then this quote of his was controversial, but what was the quote?
The quote was that every time I fire a linguist, the speech recognition accuracy goes up.
So it came from at the time at IBM, he was leading the IBM group with a group of mathematicians
and information theoreticians to work on the problems of speech recognition, which was
previously worked on using knowledge-based approach in AI.
And actually, his group wasn't allowed to use that approach.
They somehow did it anyways.
So there was always a little bit of conflict between the knowledge-based AI community and
the at the time, the statistical-minded people.
So there were papers written about the empiricist versus rationalists in the 90s.
And I remember my first conference presentation on a statistical-based natural language processing
paper.
I was yelled at by some of the senior people in the field.
Yeah, those were the days.
But now it's totally non-controversial at all, because you can see machine learnings everywhere.
And yeah, so he said that because the approach he proposed, his group proposed, was very
radical at the time, which is not looking at how to imitate human at all, but just looking
at the input and output of the task.
So for example, for speech recognition, the input is speech waveforms and the output should
be words.
And for machine translation, the input could be French and output should be English.
And they basically treated all these problems as the kind of information theoretic problems
to solve.
So a different mathematical approach from the traditional knowledge-based AI approach
at the time.
Can you maybe give a 30,000-foot background of information theory and how it plays into
all this?
Okay.
I'll try.
So information theory was invented by the entire field, came from a paper written by Claude
Shannon in 1948 called the Mathematical Theory of Communication.
So it basically looks at, you know, the information coding.
For example, if you want to transmit telephone signal through a telephone cable, there's
only that much information you can transmit.
And how many simultaneous calls can you transmit in one cable is limited by physics, actually.
Right.
And so the information theory really is talking about, you know, how do you encode on transmit
and decode information.
So the earliest application was, of course, telephone systems.
So no coincidence.
So there was no accident that Claude Shannon was also at the labs.
And then later on, information theory was then applied, actually, at the time when Claude
Shannon came up with this information theory, he already had the paper on the information
of language.
Oh, right.
Yes, yes.
He actually wrote a paper very early about how language can be encoded and learned.
And so he was one of the earlier pioneers of AI.
Even though people don't think of him all in that way, he had no idea.
Yes.
And he actually also had a cent of American paper on the first chess playing game, chess
playing algorithm in the 60s.
So, yeah.
So information theory became an entire field of research.
And it's applied widely in many, many different areas, most importantly telecommunications communications.
And then, of course, in all the statistical learning field, we also use information theory.
For example, look at how we can learn to model a language from information theory at the
point of view.
So it's more like a, you can think of as message encoding kind of way, rather than linguistically
motivated way.
So it's a different way of looking at mathematical approach of looking at problems.
Right.
You mentioned that in your talk and I found that fascinating and it never occurred to
me.
I think you, you pose the idea of thinking of the machine translation problem as you've
got this, you've got this message that is, you're trying to translate French to English.
You've got this message that's in noisy English.
Yeah.
Exactly.
You clean up the noise and get to the true English.
So for example, it's exactly that.
So for example, the word order are different in different languages.
So it can be, you can think of that word reordering as a kind of a distortion.
And then some of the words actually in French English, some of the words are the same.
Like 40%.
Yeah.
Right.
But other words are different.
Right.
So you can, again, think of a word that's different, a different language being a kind of noise.
I mean, a kind of distortion, I'm sorry.
But if you can learn that distortion, then you learn the translation.
So and that is some information theoretic approach.
That's what Google Translate is still based on.
So interesting.
So how, starting from what are some of the, you know, the algorithms or approaches that
kind of come out of the information theory background, like how is it applied more concretely
to that problem?
Okay.
So for example, all modern speech recognition software is based on the noisy channel model.
So the whole idea of you can train a speech recognizer with lots of data.
So let's say voice search and all that.
It's all based on what other people have said and it's based on these days, millions
of hours of speech data like Siri, for example, it's trained on this data.
And then it uses a different kind of machine learning method.
But all these speech recognition methodologies based on one big paradigm is still the
noisy channel model, which is speech inwards out through this channel.
Now the latest approaches has turned part of these methods into using deep learning to
replace some modules, for example, replacing how phonemes can be modeled or replacing part
of the predicting what will come after which word.
So some of this is now enabled by deep learning, but the whole paradigm is still an information
theory approach.
Similarly for things like Google Translate, it's still based on the, what I just talked
about the noisy channel model.
And recently there's some research work on using Neuronat, but we haven't seen any commercial
application that claims to be using Neuronat for, so end to end Neuronat for formation translation
yet.
Okay.
Okay.
So how did you, how did you kind of get it?
It's all to mention all the encryption software, all that is based on information theory.
Right.
Yeah.
Okay.
That is not my field, but that's actually a main application for this kind of work.
Yeah.
Okay.
Awesome.
Awesome.
How did you get into the study of emotion?
How did I get into the study of emotion?
Basically we noticed that for, so I've been working on spoken language understanding for
a long time.
And I've participated on different efforts from different generations of virtual agents,
what we call virtual assistants today.
Okay.
So the earliest system in the 80s was funded by DARPA, which was a ticket booking system.
Why you call and say I want to go from here to there and then it's trying to book a ticket
for you.
And from that time on, we have seen different generations of dialos systems up until today
we have Siri and Cortana and working on these dialos systems, I've noticed that we've
always sort of just look at literal meaning of a user query.
So the machine just pays attention to what is the destination, the origin city, how
I mean text you want, what kind of restaurant you're looking for.
So very sort of literal interpretation of your query, which means that your query has
to be very clear.
These days people always complain, Siri doesn't work well and all that.
To us, researchers, we can see why it doesn't work well because the system assumes users
to be very clear and say explicitly what they really want to get.
And unless the users are lawyers, I mean, very few people talk that way.
We talk naturally, we expect you to understand what I mean.
And for example, you just laughed because you know I was trying to be funny.
And that kind of information is completely lost in our dialos systems of previous generations.
But it is very, very important for true communication.
If we want to go past the current sort of plateau of understanding, we have to also incorporate
the understanding of emotion intent and all that in addition to understanding the meaning
of the words.
So that's when I started working on incorporating.
So I do not recognize, I don't work on recognizing emotion for emotion sake.
It's really recognizing emotion for communications.
So that's what I call empathy module.
And then when I look at the future applications, what we do, the sum of the most immediate
applications that immediate in the sense that in the next 20 to 30 years we'll see why
desperate application will be healthcare and elderly care, because by the year 2050 there
will be more old people than young children in the world.
And so elderly care is a big area and governments will be running out of resources, human resources
to take care of these elderly.
So we're going to need a lot of machine assistance.
Now my mother spends a lot of time alone.
She's very independent, she's in her ceremonies.
She doesn't want to live with me and she wants to be left alone to do her own thing.
And I'm always worried and in her independence, I worry about her health.
She seems healthy when I see her, but at her age I want to make sure that she's fine
right now for example.
I want to know she's fine right.
So this kind of, I want to know her health conditions but also her mental conditions.
So if she doesn't want to live with me, now how about if I have, if we build a home
robot who can be there at her, you know, at her home or be around her and converse with
her, sometimes just to get a sense of how she's doing simple things, right.
And then sends me a message so I know how she, I mean, all sends me a curve of her vital
signs in addition to her emotional state, then that will help the guilty children, busy
guilty children, but also help the elderly because in a lot of people have emergency problems
like a heart attack or something that could have been, they could have been saved if someone
knows.
And then there are others who can get lonely and depressed and then they can also be helped
by machine to some extent, not completely by machines, you know, living with just machines
is also very sad, but but when there are not enough humans around people around then
the machines can help.
So this is why I want to work on empathetic robots, in the case, in the case of a crisis
situation like a heart attack, where does empathy and what does that come in?
I think heart attack is basically its emergency, then the robot has to basically alert, call
the ambulance, right?
That's the first thing, but what empathy comes into play is that so daily reminder to take
medicine.
In some of the aging studies, people have found that a lot of elderly, they don't want
to take medicine unless somebody talks to them, like sometimes you have to sort of entice
them, I mean some elderly select children.
So in that case, the machine doesn't just say take your medicine and repeatedly insisting
that you take your medicine, like with the same command, that will be extremely annoying
and it will have the opposite effect, right?
So the machine needs to know that the elderly is hesitant or resisting and how is the person's
patient's emotional state and to know whether now the machine can insist or it's time to
call a doctor or nurse or whether telling the patient a bedtime story will suit them.
So that requires empathy, if you think about all the nurses in the hospital, a lot of
their job, a lot of their work and their tasks are very repetitive and sometimes the better
nurses are the ones who really have a very good bedside manner and what is the bedside
manner other than being empathetic, just being empathetic, you know, for both doctors
and nurses.
So if you want doctors and nurses to be empathetic, obviously you want the healthcare robots
to be empathetic.
Right, right.
You talk, I interpreted its focus on empathy recognition, but your description is also
talking about what you might call generative empathy.
Right, right.
So empathy has two sides, it's the emotional recognition and then the appropriate emotional
response.
Okay.
So empathy, I only, in my talk, I focus mostly on the emotion recognition part because that
is hard until we can recognize the emotion, we don't know how to react, right?
So I focus on that.
But the response part, I talked a little bit at the end that we're trying to learn the
appropriate response as well from data, so also using deep learning.
You mentioned healthcare as a use case, I think there's often, for a while now, people
have talked about a customer service use case.
Right, right.
Sure, sure.
You know, the, the, the whole line will recognize when you're getting frustrated.
Sure, sure.
In fact, at AT&T Bell Labs in the 90s already, they, they, a group worked, came out with
the system called How May I Help You?
So when you call the AT&T line, it's a, it's a virtual system, virtual operator that
talks to you first and says, How May I Help You?
And you basically say whatever you want and then it goes to different categories.
Yeah.
And that is the intention classifier.
Okay.
And also at the time, AT&T, AT&T had internal programs to analyze all this call center
data to see whether people upset, they're happy or not.
So that is already the beginning of emotion recognition.
That was my first contact with emotion recognition.
And it was for, for customer service, indeed, it is a big area.
But I don't get the sense that it's widely deployed or at least not in a way that I would
see as an every person, maybe it's more at, yeah, it's at the back end.
So this is the thing because it's not consumer facing.
It's really for, it's really in the, in the area, the realm of data analytics.
So it's more of a tool used by corporations to improve the efficiency of their call centers.
And a performance management.
Performance, yeah, they do do that.
There are companies that provide technology to customer service.
Okay.
Yeah.
Yeah.
Yeah.
So you also talked about the use of convolutional neural nets and recognizing emotion and kind
of drew some interesting correlations across, you basically that the CNNs were able to functionally
approximate the cochlear functionality.
Yeah.
And some perception system, indeed, that's what we found.
So I think my talk, I started out by saying, okay, traditionally speech recognition, look
at these human, human, some perception system and try to imitate that.
But we sort of hit the bottleneck and we had to move away from that and go with the information
theoretic approach where we actually try, we don't try to imitate human model at all.
And what's interesting with CNN is that a lot of times CNN or other deep neural net are
being used as a black box.
So we know it works, we don't know why, and humans being humans are always interesting
knowing why.
Right.
And in fact, there's a practical reason is that if you ever want to commercialize a technology
like that, to provide to your customer, they want to know why, what it's learning.
So we then took a look at the CNN, different layers of CNN and saw that it was actually,
as I mentioned in my talk, that it's basically approximating the future bank in our cochlea
that's connected to the auditory system.
And then we also saw that it's picking up on the amplitude, the peaks in the amplitude
that correspond to different emotions.
Now we thought that was very interesting that we could actually see what's going on.
For example, it was always, CNN was first applied to image recognition.
They used like seven, eight, nine layers of CNN to achieve that purpose.
And in image recognition, they were able to see that each layer, so for example, one
layer is recognizing the edges of image and the other layer is looking nice, maybe something
else, some features on your face and all that.
So it's all very obvious and really nice.
And we were never able to explain how the neural networks on speech and language.
So it was interesting to see why works on emotion.
We still are not able to figure out what it's doing on languages.
So each layer is recognizing.
We were hoping that each layer will correspond to some linguistic functions, such as syntax
or we haven't seen anything that neat yet.
So.
Interesting.
Yeah.
So it's not blinding learning something.
It's learning something which has a physical meaning.
But you made the point that it's also an error to correlate it too tightly to brain function
because that doesn't really work.
It doesn't.
It's not.
Because no.
So even though I said it approximates human perception system, it's really the hearing
system.
Right?
It's not the understanding part.
It's the hearing system.
And we know exactly how our hearing system function.
We know very, very well.
We don't guess.
But how our mind's functioning, understanding the meaning, we don't know.
We're activating our, you know, I mentioned 100 billion neurons and 100 trillion synopsis
to get that until we have neural network of their size.
It's hard to come up with something similar to human cognitive ability.
So it's not.
We don't have that.
So that's why I say it's no coincidence that we can use.
We can't explain what CNN is doing for perception.
So speech recognition and emotional recognition are both perception problems.
Perception is actually easy because we understand human perception, how our skin feels the
temperature.
We actually understand the physics of that very, very well.
But once you go into understanding, which is language understanding, which is the trans,
you know, if I want to use a noisy channel model, it would be from words to meaning.
Once we're getting to the realm of that, we are kind of clueless to how humans.
So there are a lot of linguistic theories about how humans think, how humans understand.
But you know, for every linguistic theory, there are other people who say no.
So there's no, no, there's no scientific truth that we all share right now about how human
mind understands language or understands anything else.
How do you know a video of a cat is funny or not?
How does our mind interpret humor?
We actually don't know.
A lot of marketers would like me to be able to.
So yeah, how do you predict, for example, one thing we were asked since we could classify
music, we were asked by a company and say, hey, can you predict whether a song is going
to be a hit or not?
Maybe we could predict that, wouldn't that be amazing?
No, you know, because you know, even we look at big data of all the past hit songs, it
can learn, it cannot predict what the next one, not yet, maybe.
So all we can do right now is use engineering models to approximate input and output.
You know what I mean, it's really a mimicry, like just looking at this input, can we come
out with output that's similar to the truth?
So we're not, we're not in any way near to imitating human minds and that will be, even
though, so even the term deep learning is a new term for something that has been around
for a while.
So that's a particular kind of machine learning.
Right.
It is, it is no deeper, let's say that are the kind of machine learning methods, it's
a terminology and also neural network, it's a very, very rudimentary kind of neural network,
you know, for speech recognition that might be tens of thousands of neurons and for emotional
recognition, much, much fewer.
So that cannot compare to the human brain.
One of the things that I noticed in your presentation is that when you're doing
the emotional, emotional recognition, you're mapping it to kind of the, you know, these
names, common names we have for emotion, angry sad, whatever.
Is that model even too simple or is there an underlying more nuanced model for emotion?
I'm not sure they're.
Yeah, well, so there's a lot of research done on the underlying model for emotion, such
as valence arousal, you know, and there are models that try to predict that first before
they predict the final label.
And to interrupt you, because I saw the slide, but these folks haven't, valence arousal is
valence is like the strength of the emotion and-
No arousal is the strength.
Valence is the positive and negative of the emotion.
And so the various, you know, angry sad, cool, it's a combination of, yeah, they are a
combination of different values of valence and arousal, this is one emotion theory.
So these are models that psychologists came up with to try to organize what we know
about emotion.
And so it's just human minds, we're symbolic animals, you know, we need to have names,
we give names to everything.
So it's just easier for us to give a name to emotion, so we know what we're talking
about, rather than give this vague, you know, number, valence arousal, if I tell you, oh,
this is valence arousal, this is this number, you wouldn't know what I'm talking about,
if I say he's, you know, he's showing happiness on his face, you kind of, kind of, no.
So it's kind of emotional recognition, it's kind of like speech recognition when it was
only recognizing isolated words.
It's oversimplified for sure.
We're not good at all with emotion recognition.
You saw my slides, the performance is nowhere near the ability, the performance of recognizing
words, recognizing emotion is much harder right now.
One reason you pointed out is that it's hard to define where emotion is, you know, for
example, maybe it's easy to see if somebody's happy, but is he smiling because he's happy
as he's smiling because he's trying to be polite.
And also, what about emotions like frustration?
How can you tell?
Sure, some things are obvious, you know, when someone's frustrated, if they're rolling
their eyes or something, but there are other times, you know, from the voice, from the tone
of your voice, we can tell a lot of things, but we cannot tell everything all the time.
So it is along with the goal, you saw the accuracy, the accuracy these days, even the
best commercial systems, it's just like 60 percent, you know, and most 70 percent.
And speech recognition, we're talking about above 90 percent, right?
So there's a long way to go for emotional recognition, and especially some more complex
emotions, like humor, sarcasm, yeah, sarcasm, humor and, and deceit, you know, is this
person lying?
And there are colleagues in the field who have come up with systems that, that has 70
some percent accuracy in detecting deceit and actually performs better than humans, turns
out we are now very good at detecting deceit, we're not good at all.
And humans are not very good in recognizing emotions.
Yeah.
You know what we found with, when we did some human subject studies, is that we found
that women are better, women are more empathetic than men, there was actually quantifiable.
And then women can detect emotion across languages, languages, in languages they don't know.
Interesting.
Also better than men in the same language.
So you can talk about why the reason, you know, we're programmed to be mothers, we must
recognize the emotion of a baby early on.
And I think there's some merit to that.
So, although I'm not a anthropologist, I cannot prove that, but, yeah, so, interesting.
And then the reasons, you know, you see there are a lot of women who are nurses, doesn't
mean men cannot be, but just happen that way, right.
A lot of the caregivers, women, you know, kindergarten teachers, you know, just nannies, you know.
So if we want to build robots, we want them to be more like that, more empathetic.
Right.
Great.
Well, thanks so much for taking the time to sit down with me.
It was a great discussion.
I really appreciate it.
Thank you.
Thank you.
Would you like to share how folks can find your research, or are you on any of the social
media networks?
Well, it's easy to Google my name, which is Pascal Fung, P-A-S-E-A-L-E, Fung is F-U-N-G.
If you Google my name, you come to my website, which lists all my our publications, our projects
and all that.
And they can, they can email me via that website as well.
Great.
Great.
Well, thanks so much.
Thank you.
All right, everyone, that's it for today's show.
If you enjoyed this show or have something to add to the discussion, please leave a comment
on the show notes page at twimmaleigh.com slash talk slash nine.
Or tweet to me at at Sam Charrington or at twimmaleigh to let me know what you think.
Thanks so much for listening and catch you next time.
