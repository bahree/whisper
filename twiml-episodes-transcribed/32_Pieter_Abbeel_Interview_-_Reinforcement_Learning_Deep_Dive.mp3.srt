1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,960
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,960 --> 00:00:23,760
I'm your host Sam Charrington.

4
00:00:23,760 --> 00:00:28,320
This week we continue our exploration into industrial AI.

5
00:00:28,320 --> 00:00:33,600
Is that you ask, well in my forthcoming paper on the topic, I define industrial AI as

6
00:00:33,600 --> 00:00:40,080
any application of AI relating to the physical operations or systems of an enterprise.

7
00:00:40,080 --> 00:00:45,040
I go on to note that the focus of industrial AI is on helping enterprises monitor, optimize

8
00:00:45,040 --> 00:00:51,200
or control the behavior of these operations and systems to improve their efficiency and performance.

9
00:00:51,200 --> 00:00:54,320
When people hear the phrase industrial, they quickly jump to manufacturing, but it's

10
00:00:54,320 --> 00:01:00,320
more than that. Industrial AI includes manufacturing applications like robotics and using computer

11
00:01:00,320 --> 00:01:05,440
vision for quality control, but also applications like supply chain optimization and risk management,

12
00:01:05,440 --> 00:01:11,120
warehouse automation, the monitoring and operation of building HVAC systems and much more.

13
00:01:11,120 --> 00:01:18,000
For more information about industrial AI or the report, visit twimlai.com slash industrial AI.

14
00:01:18,000 --> 00:01:25,840
This week our guest is Peter Rabiel, Assistant Professor at UC Berkeley, Research Scientist at Open AI

15
00:01:25,840 --> 00:01:31,280
and Co-Founder of Great Scoop. Peter has an extensive background in AI research,

16
00:01:31,280 --> 00:01:36,320
going way back to his days as Andrew Ng's first PhD student at Stanford.

17
00:01:36,320 --> 00:01:39,200
His work today is focused on deep learning for robotics.

18
00:01:39,840 --> 00:01:43,920
During this conversation, Peter and I really dig into reinforcement learning,

19
00:01:43,920 --> 00:01:49,280
which is a technique for allowing robots or other AI's to learn through their own trial and error.

20
00:01:50,080 --> 00:01:54,960
Before we jump in, a quick nerd alert. This conversation explores cutting edge research with

21
00:01:54,960 --> 00:02:00,640
one of the leading researchers in the field, and as a result, it gets pretty technical at times.

22
00:02:00,640 --> 00:02:05,600
I try to up level it when I can keep up myself, so hang in there. I promise that you'll learn

23
00:02:05,600 --> 00:02:11,200
a ton if you keep with it. I could also use your feedback here. You want more or fewer of these

24
00:02:11,200 --> 00:02:16,080
kinds of conversations. Let me know in the comments, along with any feedback, comments,

25
00:02:16,080 --> 00:02:22,320
or questions you have about this episode. Before we jump in, a word about our sponsor.

26
00:02:22,320 --> 00:02:26,640
I introduced Bonsai last week, and once again, I'd like to thank the team over there

27
00:02:26,640 --> 00:02:31,360
for sponsoring this podcast series, as well as my forthcoming industrial AI research.

28
00:02:32,240 --> 00:02:38,000
Bonsai offers an AI platform that empowers enterprises to build and deploy intelligent systems.

29
00:02:38,000 --> 00:02:42,720
If you're trying to build AI-powered applications focused on optimizing and controlling the systems

30
00:02:42,720 --> 00:02:47,440
in your enterprise, you should take a look at what they're up to. They've got a unique approach to

31
00:02:47,440 --> 00:02:52,560
building AI models that let you use high-level code to model real-world concepts in your application,

32
00:02:53,200 --> 00:02:57,200
automatically generate, train, and evaluate low-level models for your project,

33
00:02:57,200 --> 00:03:03,120
using reinforcement learning and other technologies, and then easily integrate those models into your

34
00:03:03,120 --> 00:03:10,320
applications and systems using APIs. You should check them out at bonds.ai, bons.ai, bons.ai,

35
00:03:10,320 --> 00:03:15,920
and definitely let them know you appreciate their support of the podcast. And now on to the show.

36
00:03:24,560 --> 00:03:30,800
All right, hello, everyone. I have got Peter Abil on the line. Peter is an associate professor

37
00:03:30,800 --> 00:03:38,240
at UC Berkeley, a research scientist at OpenAI, and a co-founder at GradeScope. Peter, how are you?

38
00:03:39,200 --> 00:03:45,360
Doing pretty well. How about you, Sam? I'm doing very well and excited to jump into this conversation.

39
00:03:45,360 --> 00:03:52,720
In addition to all of those positions you hold, you are also at the cutting edge of a very interesting

40
00:03:52,720 --> 00:03:59,120
field in machine learning called deep reinforcement learning. And I'm really looking forward to

41
00:03:59,120 --> 00:04:05,200
learning a bunch about that, talking with you today. Why don't we get started by

42
00:04:05,200 --> 00:04:09,840
having you tell us a little bit about your background and kind of how you got to where you are now

43
00:04:09,840 --> 00:04:15,920
in your area of research? Sure, yeah. If I go pretty far back, actually, as a high school,

44
00:04:15,920 --> 00:04:21,200
my excitement was mostly about physics and math, and from there engineering was a natural

45
00:04:21,200 --> 00:04:26,640
next pick. And when I was wrapping up my bachelor's in engineering, I just found so many topics so

46
00:04:26,640 --> 00:04:31,360
exciting and it was hard to choose, but ultimately it seemed artificial intelligence was the area

47
00:04:31,360 --> 00:04:37,600
that could drive almost all other areas. It is by making progress in AI, it might be possible to help

48
00:04:37,600 --> 00:04:43,200
know how to do many, many other things. And so that's kind of what drove me into AI and then got

49
00:04:43,200 --> 00:04:48,480
me started on my masters and then PhD on the artificial intelligence with Andrew at Stanford.

50
00:04:48,480 --> 00:04:52,880
And then from there I became professor at Berkeley and research scientist at OpenAI.

51
00:04:52,880 --> 00:04:57,680
Well, working with Andrew, that's an impressive credential. He's done a lot of amazing things in

52
00:04:57,680 --> 00:05:02,880
the field. It couldn't agree more. It's very fortunate that actually his first PhD students,

53
00:05:02,880 --> 00:05:10,320
I saw him start from ground zero, which was amazing. Oh wow. Wow. And so what do you focus on today?

54
00:05:11,200 --> 00:05:16,800
So a lot of what drives my work is trying to get robots out in the real world,

55
00:05:16,800 --> 00:05:21,200
meaning beyond repetitive tasks, as you would see in current robot deployments.

56
00:05:21,200 --> 00:05:27,760
And so what I think is key to get robots out in the real world and doing more flexible things is

57
00:05:27,760 --> 00:05:32,640
to give them more intelligence and key to that will be for them to be able to learn rather than

58
00:05:32,640 --> 00:05:35,760
us having to program them for every possible scenario they could encounter.

59
00:05:36,480 --> 00:05:42,560
And so reinforcement learning obviously comes up in that. Is that just one of a number of techniques

60
00:05:42,560 --> 00:05:50,160
that you're looking into to make robots smarter or what's kind of the landscape of things that you're

61
00:05:50,160 --> 00:05:55,280
pursuing there? All right. So there's there's many ways to learn. If you look at the kind of

62
00:05:55,280 --> 00:06:00,400
machine learning landscape, there is supervised learning, which is recognizing a pattern between

63
00:06:00,400 --> 00:06:05,120
inputs and outputs. There is unsurface learning, which is trying to make sense of just data doesn't

64
00:06:05,120 --> 00:06:10,640
have any labels. There's reinforcement learning, which looks at trying to optimize the reward,

65
00:06:10,640 --> 00:06:16,160
which is a really good fit for robotics. So let's say you have a robot and maybe you want it to

66
00:06:16,160 --> 00:06:22,640
clean your home. You could define a reward function that says the cleaner my home, the higher the

67
00:06:22,640 --> 00:06:28,800
reward. And then a reinforcement algorithm deployed on a robot would try to optimize that reward

68
00:06:28,800 --> 00:06:32,880
than as a consequence, optimize the cleanliness of your home if it's successful.

69
00:06:34,240 --> 00:06:40,800
So we've been hearing a lot about deep reinforcement learning of late, but is reinforcement learning

70
00:06:40,800 --> 00:06:47,600
as a whole? Is it new or has it been in use for a while? And how has it done prior to deep learning?

71
00:06:48,240 --> 00:06:53,600
Yeah. So reinforcement learning has been around for a long time. Ever since people start thinking

72
00:06:53,600 --> 00:06:58,160
about artificial intelligence, they were thinking about things like reinforcement learning. Because

73
00:06:58,160 --> 00:07:05,200
when you think about AI, you think about some kind of intelligent system supposedly required to

74
00:07:05,200 --> 00:07:10,160
make a decision. And then after making that decision, there will be consequences. And then it will

75
00:07:10,160 --> 00:07:15,200
need to deal with the consequences of the action. And this will repeat over and over and over.

76
00:07:15,200 --> 00:07:19,200
And that is exactly the reinforcement learning setting where some systems, some AI systems

77
00:07:19,200 --> 00:07:23,440
are supposed to make decisions that have impact over time and then adjust over it.

78
00:07:24,320 --> 00:07:28,800
And so it's been around for a very long time. Actually, even before deep reinforcement,

79
00:07:28,800 --> 00:07:34,160
there were quite a few interesting success stories. For example, Andrew, his autonomous helicopter,

80
00:07:34,160 --> 00:07:38,720
I was part of the group working on that. But the helicopter at Stanford was largely driven by

81
00:07:38,720 --> 00:07:43,360
a reinforcement learning. And this was just regular reinforcement learning, no deep neural nets

82
00:07:43,360 --> 00:07:51,920
behind it. Russ Tedrick, professor at MIT, but during his PhD at MIT also, he build a biped walker

83
00:07:51,920 --> 00:07:56,800
that learned to walk with regular reinforcement learning, no deep networks involved.

84
00:07:57,760 --> 00:08:03,360
But what characterized those early successes is that it required a combination of a lot of

85
00:08:03,360 --> 00:08:09,200
demand expertise with expertise in reinforcement learning. So you would carefully think about how

86
00:08:09,200 --> 00:08:14,320
does the helicopter work? What are the relevant aspects of controlling helicopter? Talk to experts

87
00:08:14,320 --> 00:08:19,200
in helicopter piloting what they pay attention to, what they think about. And then you would condense

88
00:08:19,200 --> 00:08:24,480
that into some representation. You'd define to decide what it means to be a good helicopter

89
00:08:24,480 --> 00:08:28,240
control policy. You'd say, well, helicopter control policy would look at these and these and these

90
00:08:28,240 --> 00:08:34,400
aspects of helicopter state and then make a decision based on that. And leave a few free parameters

91
00:08:34,400 --> 00:08:38,960
in there. Stanford, a helicopter control kit, there were 12 parameters that were not determined,

92
00:08:38,960 --> 00:08:43,360
there were just real numbers that were hard to come up with by hand. But then the reinforcement

93
00:08:43,360 --> 00:08:48,640
learning algorithm would find those 12 parameters better than a person could find them by hand.

94
00:08:48,640 --> 00:08:54,880
And that would lead to extremely reliable helicopter flight. But the big difference now with

95
00:08:54,880 --> 00:09:01,920
a deeper reinforcement learning is that it largely takes away the need for domain expertise.

96
00:09:02,560 --> 00:09:09,680
So if you look at the results on Atari go or the results in robotics that we, for example,

97
00:09:09,680 --> 00:09:16,320
got on Berkeley with learning assembly, you look at those and what goes into the thing that's

98
00:09:16,320 --> 00:09:20,880
learning is raw sensory percepts. So it'll get raw pixels from the Atari game, it'll get

99
00:09:20,880 --> 00:09:26,640
it just the raw board configuration, not some new encoding of strengths or weaknesses

100
00:09:26,640 --> 00:09:31,440
about the board configuration, just the raw configuration for learning assembly. We'll get raw

101
00:09:31,440 --> 00:09:37,280
pixels of what the robot is seeing and we'll need to make decisions based on that. And so

102
00:09:38,240 --> 00:09:43,680
now it's the deep network that somehow makes sense of this raw sensory information and turns

103
00:09:43,680 --> 00:09:50,320
it then into after a long computation into meaningful control commands, which is a very different

104
00:09:50,320 --> 00:09:54,160
setup from this previous successes where you would have had to analyze ahead of time.

105
00:09:54,160 --> 00:09:58,960
What is it that we need to pay attention to? How can we extract that with a separate piece of code

106
00:09:58,960 --> 00:10:06,640
that then is fed into the reinforcement learner? Are we limited by our ability to incorporate

107
00:10:06,640 --> 00:10:12,160
that domain expertise into the deep neural networks? And the background for that question is I'm

108
00:10:12,160 --> 00:10:18,000
just imagining that if we were able to incorporate that domain expertise in, the models would be

109
00:10:18,000 --> 00:10:22,880
even smarter and more accurate. This is a very interesting question. How can you kind of get the

110
00:10:22,880 --> 00:10:28,160
best of both worlds? Exactly, exactly. I think it's like, how do you get both the domain expertise

111
00:10:28,160 --> 00:10:35,520
and the flexibility of learning things from raw sensory information? Like, for example,

112
00:10:35,520 --> 00:10:41,360
I had a conversation with Stefano Irman who you may know over at Stanford. And one of the things

113
00:10:41,360 --> 00:10:48,160
that we talked about was some of his work on incorporating, for example, physics models into

114
00:10:49,280 --> 00:10:56,400
machine learning models and how they were able to dramatically increase the accuracy of a

115
00:10:57,440 --> 00:11:04,480
projectile trajectory model by telling a little bit about the parabolic trajectory that

116
00:11:04,480 --> 00:11:09,600
things take when they're moving through free space to be a physics. And it strikes me that if we

117
00:11:09,600 --> 00:11:16,640
could, again, get the best of both worlds, that would help us here. Absolutely. So that is a perfect

118
00:11:16,640 --> 00:11:22,880
example. So let me maybe expand the scope of this a little bit. So if you look at prior knowledge,

119
00:11:22,880 --> 00:11:28,000
I can come in in many different formats. So one thing is, for example, you might know that

120
00:11:28,000 --> 00:11:35,200
physics is involved and that the laws of physics could help you in making decisions. It might also be

121
00:11:35,200 --> 00:11:40,000
that certain existing algorithms could be relevant. Maybe you know that a common filter which was

122
00:11:40,000 --> 00:11:46,960
actually used to track the first rocket that went to the moon, that that idea is going to be relevant

123
00:11:46,960 --> 00:11:52,320
because you want to track maybe the position of your self-driving car. Or maybe you know that

124
00:11:52,320 --> 00:11:57,440
a planning competition, a competition that doesn't just reactively look at the current

125
00:11:57,440 --> 00:12:03,440
sensory inputs and the spit tide and action, but actually thinks ahead simulates what would happen

126
00:12:03,440 --> 00:12:08,320
if you were to take a certain sequence of actions. And then based on that makes a decision.

127
00:12:08,320 --> 00:12:14,720
And so there's many of those ideas out there that we know can play a role in decision-making.

128
00:12:15,600 --> 00:12:21,040
And if a deep neural net is supposed to figure it out all from scratch, the data needs might be

129
00:12:21,040 --> 00:12:27,280
prohibitive to get to a practical application anytime soon. And so you can do it there, which is

130
00:12:27,280 --> 00:12:32,640
actually really interesting, which maybe transcends deep networks a little bit, which is actually the

131
00:12:32,640 --> 00:12:39,120
automatic differentiation frameworks, such as TensorFlow, Theano, PyTorch, and so forth. These

132
00:12:39,120 --> 00:12:45,120
frameworks actually can differentiate to anything. They're not specific to neural nets. And so we

133
00:12:45,120 --> 00:12:51,120
can do is you can set up something more complex than a neural net, more generalized the competition

134
00:12:51,120 --> 00:12:55,600
graph. What you can do with that is you can tend to set up a competition graph that encodes

135
00:12:56,720 --> 00:13:00,560
the algorithm or the prior knowledge that you have. So for example, if you thought you wanted to

136
00:13:00,560 --> 00:13:05,840
use a common filter, you could set up the equations of the common filter inside these frameworks.

137
00:13:06,960 --> 00:13:11,040
Now typical would happen is you still need to deal with raw pixels. So you would

138
00:13:11,840 --> 00:13:16,880
take your raw pixels, feed them through a deep network that feeds into these common filter

139
00:13:16,880 --> 00:13:23,520
equations that then leads to some output. And so what you get then if you train this is that

140
00:13:23,520 --> 00:13:28,640
you're training this big competition graph that has the flexibility of a neural net,

141
00:13:28,640 --> 00:13:33,520
namely the ability to adapt to how you should process raw sensory information. But then also the

142
00:13:34,080 --> 00:13:39,120
advantage of knowing that a certain competition will matter and have it built into it. And you can

143
00:13:39,120 --> 00:13:44,640
optimize this all in one optimization. You don't need to separately find a neural net that you then

144
00:13:44,640 --> 00:13:49,680
plug into a common filter. The same is true for the physics equations that you were referring to

145
00:13:49,680 --> 00:13:53,680
that could be, for example, inside of a planner. So you could have your neural net processing

146
00:13:53,680 --> 00:14:00,400
raw sensory information, feed it into another competition that has a planner in it and the planner

147
00:14:00,400 --> 00:14:05,920
can rely on physics equations. And you don't fully wire up the details of how it's going to rely

148
00:14:05,920 --> 00:14:10,960
on that. You let it figure that out, but it's a component that sits there ready for it to use

149
00:14:10,960 --> 00:14:16,000
such that it can be more effective at learning from whatever data it's getting, what the right thing

150
00:14:16,000 --> 00:14:22,000
is to do. It sounds like there's definitely a lot in there to unpack, but it sounds like in a

151
00:14:22,000 --> 00:14:28,160
nutshell, what you're saying is that these, you know, our prior knowledge, whether it's a form of

152
00:14:28,160 --> 00:14:33,360
in the form of governing equations or models or subject matter expertise or what have you,

153
00:14:33,360 --> 00:14:39,120
you can almost think of them as like features that you're eventually going to be

154
00:14:39,120 --> 00:14:45,040
using as inputs to your neural networks. And in fact, you know, these features, we've got a

155
00:14:45,040 --> 00:14:49,600
tremendous amount of depth that we can express through, you know, matrix math and differential

156
00:14:49,600 --> 00:14:54,320
equations, you know, they can go through the same infrastructure that we're using to train our

157
00:14:54,320 --> 00:15:00,480
reinforcement learning models vis-a-vis TensorFlow and the like. Absolutely correct. And one thing

158
00:15:00,480 --> 00:15:06,000
that this also reminds me of is when you think about all these equations, they've been developed

159
00:15:06,000 --> 00:15:11,920
over time. Mathematics is been developed over time. If you think of, you know, there's a lot of

160
00:15:11,920 --> 00:15:19,280
benefits to reinforcement learning in and of itself. And it's really powerful to be able to

161
00:15:19,280 --> 00:15:27,440
learn from scratch. But the way humans often learn is actually by imitation, which is just as

162
00:15:27,440 --> 00:15:33,680
important an area of deep learning for action, for systems that can take action. Because if you

163
00:15:33,680 --> 00:15:38,880
think about it, I mean, imagine, you know, our intelligence now, our reinforcement and capabilities

164
00:15:38,880 --> 00:15:43,040
now as humans are probably not that different from what our reinforcement and capabilities

165
00:15:43,040 --> 00:15:48,240
were 100,000 years ago or even 200,000 years ago. But we live a very different life. And the

166
00:15:48,240 --> 00:15:53,440
reason we live a very different life is because we don't know start from scratch. We actually build

167
00:15:53,440 --> 00:15:58,720
upon what previous generations have developed, have built, and then we learn from that much more

168
00:15:58,720 --> 00:16:05,120
effectively than if we had to start from scratch. And so a lot of applications of reinforcement

169
00:16:05,120 --> 00:16:10,080
learning actually will rely on a combination of imitation learning and reinforcement learning.

170
00:16:10,080 --> 00:16:14,800
So typical setting would be something where you say, well, I want my robot maybe to, I don't know,

171
00:16:14,800 --> 00:16:21,440
one maybe stack some dishes. And then a natural thing to do would be to instead of starting from

172
00:16:21,440 --> 00:16:25,600
scratch with reinforcement learning, which of course, in some sense is beautiful intellectually,

173
00:16:25,600 --> 00:16:30,160
but at the same time is very ineffective given that you know what stacking dishes looks like.

174
00:16:30,640 --> 00:16:36,480
The more natural thing to do would be to show how to stack a few dishes and have the system

175
00:16:36,480 --> 00:16:43,440
watch you do that. And then use that as a guide to then learn its own motor skills that will

176
00:16:43,440 --> 00:16:49,680
allow it to match up with what you just did. That should poses a lot of challenges. In the simplest

177
00:16:49,680 --> 00:16:54,320
version, you would actually just move the robot's arms around and make the robot experience everything.

178
00:16:54,320 --> 00:16:58,400
But that is not how you would want to do it in the longer and longer and you want to just do it

179
00:16:58,400 --> 00:17:04,880
yourself has the robot watch you and understand what is the essence namely the objects that are being

180
00:17:04,880 --> 00:17:10,080
moved around. And what is not essence namely that it's your hands versus robot hands or that maybe

181
00:17:10,080 --> 00:17:14,880
you are moving your head in certain ways that are irrelevant to the task because you just might

182
00:17:14,880 --> 00:17:20,400
be looking because somebody comes by and checking out what they're doing. And so a robot understanding

183
00:17:20,400 --> 00:17:26,000
from watching a human what is the essence and distilling that into then understanding how to do

184
00:17:26,000 --> 00:17:30,320
something themselves in their own body which is different from the human's body.

185
00:17:31,440 --> 00:17:34,400
There's a lot of interesting challenges that actually will go a long way in terms of

186
00:17:34,400 --> 00:17:40,960
seeding the robot's capabilities to then bring in reinforcement learning to really get fine-tuned

187
00:17:40,960 --> 00:17:46,880
skills. When we're doing imitation learning what what's the underlying mechanism or what are some

188
00:17:46,880 --> 00:17:52,160
of the underlying mechanisms that we're using to kind of capture what the you know what we're

189
00:17:52,160 --> 00:17:57,360
training on that what we're imitating is it like we're constraining the the state space of the

190
00:17:57,360 --> 00:18:01,760
ultimate solution. And so like we don't have to train a bunch of things or are we like building

191
00:18:01,760 --> 00:18:06,080
representations of what the robot's seeing or some combination of all that plus other stuff.

192
00:18:06,720 --> 00:18:10,800
Yeah so there's been quite a few interesting things happening over the past year that I think

193
00:18:11,600 --> 00:18:18,320
are changing what's possible with imitation learning. So one piece of work that happened to come

194
00:18:18,320 --> 00:18:26,400
out of OpenAI about a half year ago was a third person imitation learning. And so what that considers

195
00:18:26,400 --> 00:18:33,120
is it it considers the specific problem of how to learn when you are watching from a third

196
00:18:33,120 --> 00:18:37,840
person point of view what it is that you should do but then later you should do it yourself

197
00:18:37,840 --> 00:18:42,400
in which case it'll look very different because it's now you with your own hands from your own

198
00:18:42,400 --> 00:18:48,480
viewpoints, first person viewpoints. And so some of the ideas we we put out play there were

199
00:18:49,120 --> 00:18:55,040
actually quite related to some work Stefano Armand did at Stanford who you just mentioned

200
00:18:55,040 --> 00:19:02,640
on imitation learning through generative adversarial networks. And so the idea there was to

201
00:19:02,640 --> 00:19:08,400
say in the original paper from Stefano's group, this was John Fennel, Stefano's student,

202
00:19:08,400 --> 00:19:15,200
they looked at how can we generate a behavior that an adversarial network cannot distinguish

203
00:19:15,200 --> 00:19:20,400
from the demonstration behavior because if an adversarial neural net cannot distinguish the robot

204
00:19:20,400 --> 00:19:26,960
behavior from the demonstrations, then that means the robot might have captured what matters about

205
00:19:26,960 --> 00:19:33,200
those demonstrations. Now the tricky part is what does behavior mean here is it is it the movements

206
00:19:33,200 --> 00:19:40,400
or the outcomes? So those are things that the organ designer has a little bit of an open

207
00:19:40,400 --> 00:19:44,240
and a choice there. Do you just let it look at the outcome? Do you let it look at the entire

208
00:19:44,240 --> 00:19:48,800
trajectory? Most of the work let's again look at the entire trajectory and based on that make a

209
00:19:48,800 --> 00:19:56,480
decision of whether this is the same or not the same as the expert. Now when when you do first

210
00:19:56,480 --> 00:20:02,000
person demonstrations where you're inside the robot, this will work. But now if you have a third

211
00:20:02,000 --> 00:20:06,640
person view on the demonstration, this will never work. This is always very easy to distinguish

212
00:20:06,640 --> 00:20:11,280
the demonstration from the robot's execution because while the demonstration is going to be this

213
00:20:11,280 --> 00:20:16,320
this human doing something and then the robot's going to do it and it's obviously there's a human

214
00:20:16,320 --> 00:20:21,520
or a robot and so you can't directly apply this in a third person setting. And when you say inside

215
00:20:21,520 --> 00:20:27,200
the robot, you mean controlling a robot remotely using some controller? Yeah, we're using some

216
00:20:27,200 --> 00:20:33,040
controllers, but then the robot experiences it themselves as if they're doing it. And so

217
00:20:34,320 --> 00:20:40,320
what you need then is something extra, something that says, I want the GAN not to be able to distinguish

218
00:20:40,320 --> 00:20:45,840
between the two, but what it's looking at the GAN should not have access to certain pieces of

219
00:20:45,840 --> 00:20:50,960
information. She not have access to essentially what identifies the human versus robot because then

220
00:20:50,960 --> 00:20:54,400
it's obvious and it's not actually paying attention to the essence of what you care about.

221
00:20:54,400 --> 00:20:58,960
Right. So in the third person imitation work, we brought everybody in another actually GAN

222
00:20:58,960 --> 00:21:05,120
related idea, domain confusion. And what you do there is you you process information through

223
00:21:05,120 --> 00:21:11,360
your neural net and then at some layer in the neural net, you decide that it should not be possible

224
00:21:11,360 --> 00:21:18,080
to distinguish between two things. In this case, between whether it was a robot or a human doing it.

225
00:21:18,960 --> 00:21:22,560
And if you're not allowed to distinguish in that layer, that means that layer, the features that

226
00:21:22,560 --> 00:21:28,320
live in that layer are not allowed to contain information anymore about human versus robot,

227
00:21:28,320 --> 00:21:32,800
but only about other things like the objects in the scene, which are shared between the

228
00:21:32,800 --> 00:21:39,360
demonstration and the robot. And so that's a way to make sure that that information is removed

229
00:21:39,360 --> 00:21:44,080
and then you start paying attention to the essence when you're trying to imitate.

230
00:21:44,800 --> 00:21:50,160
That shares other applications too. For example, Richard Semmel's group at the University of Toronto

231
00:21:50,160 --> 00:21:55,520
has looked at this in terms of understanding things like fairness in machine learning.

232
00:21:55,520 --> 00:21:59,040
So you can imagine that you don't want to make a decision based on certain features.

233
00:21:59,040 --> 00:22:02,720
Maybe you don't want to make a decision with your machine learning system based on race.

234
00:22:02,720 --> 00:22:07,440
But if you just remove race from your feature set, that's not enough because the zip code might

235
00:22:07,440 --> 00:22:11,920
curl it with race or anything else might curl it with race. And so as you know in the processes

236
00:22:11,920 --> 00:22:17,360
things, you could decide that at some layer, the information should not be extractible anymore,

237
00:22:17,360 --> 00:22:22,960
what the race was of the person being processed. And then at that point, you know that the

238
00:22:22,960 --> 00:22:27,920
decisions being made by your machine learning system, while not depending on the feature that you

239
00:22:27,920 --> 00:22:33,040
didn't want to depend on, not just not directly depending on it, but also implicitly not depending

240
00:22:33,040 --> 00:22:38,240
on it through how it could have figured it out from other features. So that's the domain

241
00:22:38,240 --> 00:22:47,120
confusion idea. And another application domain. And now how are we identifying the layers that

242
00:22:47,120 --> 00:22:52,000
are encoding this feature that we don't want to be able to use?

243
00:22:54,000 --> 00:22:58,000
So that's a good question and often requires a little bit of trial and error. But essentially,

244
00:22:58,000 --> 00:23:02,720
you take a deep network and somewhere in the middle of that network, you pick a layer and decide

245
00:23:02,720 --> 00:23:08,240
this layer is the one I'm going to use. And at this point, it's not allowed to be distinguishable

246
00:23:08,240 --> 00:23:14,240
anymore, let's say between human and robot, or you know, can't recover race or yet, you know,

247
00:23:14,240 --> 00:23:20,240
sometimes people would do it between simulated environment and real world environment and so forth.

248
00:23:20,240 --> 00:23:24,960
And what does it mean to not allow the network to use that layer? Does it mean you're not propagating

249
00:23:25,760 --> 00:23:29,040
things from that layer forward or you're not, you're changing weights or

250
00:23:29,040 --> 00:23:34,880
so what that means is that at that layer, there is a network that continues that will try to make

251
00:23:34,880 --> 00:23:39,600
a decision of maybe what action to take or whether it's experts versus non expert and so forth.

252
00:23:39,600 --> 00:23:45,600
But then you branch off a second head of the neural network. It'll get a second head and

253
00:23:45,600 --> 00:23:50,640
it could be a pretty deep head that can do a lot of competition. And that second head is on

254
00:23:50,640 --> 00:23:58,080
its output classifies, let's say, between robot and person. And then instead of maximizing the

255
00:23:58,080 --> 00:24:02,960
accuracy of that head, you minimize the accuracy. You make it maximally confused.

256
00:24:04,000 --> 00:24:09,920
So you're basically training your network to forget about or obfuscate that information in

257
00:24:09,920 --> 00:24:16,480
that network and that layer rather. Exactly. So if you wanted to be that that network can do well

258
00:24:16,480 --> 00:24:21,920
and the head is trying to be accurate, but the layers before the split need to do something to

259
00:24:21,920 --> 00:24:27,520
ensure that that head cannot be accurate. So the early set of layers needs to lose the information

260
00:24:27,520 --> 00:24:32,720
so that that head cannot achieve what it's trying to do. That's incredible.

261
00:24:34,320 --> 00:24:41,040
Interesting, interesting. So we've gotten pretty deep here. I want to maybe take a step back

262
00:24:41,040 --> 00:24:50,000
to kind of RL deep reinforcement learning and maybe address, I think when most folks come into

263
00:24:50,000 --> 00:24:56,400
contact with reinforcement learning, it's probably in the context of games like Atari video games

264
00:24:56,400 --> 00:25:05,040
and other games that people are training RL models to try to play. Well, first, I'm wondering if

265
00:25:05,040 --> 00:25:12,080
you can speak to, what's the significance of games to RLY? Are they so popular as training

266
00:25:12,080 --> 00:25:19,840
vehicles? And then maybe we can dig into some of the techniques that folks are using these

267
00:25:19,840 --> 00:25:23,840
are policy gradients and Q learning and what those mean and how they're applied.

268
00:25:23,840 --> 00:25:29,120
Sure. So there are a few things that make games very interesting as the research environment.

269
00:25:29,120 --> 00:25:36,880
So one aspect is that games are designed by humans for other humans. And so they're designed with

270
00:25:36,880 --> 00:25:40,800
some kind of intelligence in mind. And so it's very interesting to see if we can build

271
00:25:40,800 --> 00:25:46,080
an artificial intelligence that can also play those games. Now related to that, they're designed by

272
00:25:46,080 --> 00:25:51,440
humans for other humans, but they were designed not with artificial intelligence in mind. So

273
00:25:51,440 --> 00:25:56,240
it's not something where you get to design a game once you have your argument in mind. The

274
00:25:56,240 --> 00:26:01,120
games already exist. And we need to see if our algorithms can tackle these existing games.

275
00:26:02,720 --> 00:26:08,080
I will say there are also a few downsides to games. So one of their big upsides is of course

276
00:26:08,080 --> 00:26:15,360
that simulation is easier than real world experimentation. Far safer, you can parallelize more easily

277
00:26:15,360 --> 00:26:20,400
and you can often get more self-contained environments to make it easier to run large-scale

278
00:26:20,400 --> 00:26:25,120
experiments through the size of the environment. And so there's a lot of benefits to simulation,

279
00:26:25,120 --> 00:26:30,800
including also simulation of other things such as simulated robots. Where games can get a little

280
00:26:30,800 --> 00:26:37,040
tricky is that it's not always obvious if you start looking at more advanced things like

281
00:26:37,040 --> 00:26:41,520
transfer. If you learn in one game, can you learn more quickly in another game? It's not always

282
00:26:41,520 --> 00:26:46,560
obvious that whether this should be possible or not, whereas if you look at things like, for example,

283
00:26:46,560 --> 00:26:52,160
simulated or real robotic manipulation, it's more natural to expect that if you learn to pick up one

284
00:26:52,160 --> 00:26:58,320
object, it should help you learn to pick up another object. And if your algorithm is not able

285
00:26:58,320 --> 00:27:03,440
to get that kind of transfer, it's probably all of them that's that fault. Or maybe it didn't see

286
00:27:03,440 --> 00:27:08,960
enough data yet. Whereas if you learn to play, let's say, Pong, and then you're supposed to learn

287
00:27:08,960 --> 00:27:13,120
Montezuma as a revenge, it's not immediately obvious that there should be any transfer between the

288
00:27:13,120 --> 00:27:21,200
two games. And so one of the big questions, I think, when you think about RL, are you learning

289
00:27:21,200 --> 00:27:28,720
for mastery or you're learning for generalization? And so, masteries where you stay within one environment,

290
00:27:28,720 --> 00:27:33,840
one game. You say, I want to master Montezuma's revenge, which probably no system has done yet.

291
00:27:33,840 --> 00:27:41,440
It's one of the harder Atari games in that suite that's researched a lot. But it's still a different

292
00:27:41,440 --> 00:27:46,400
question. Can you master one game versus can you learn something that then can be helping you

293
00:27:46,400 --> 00:27:50,800
in the future to learn something else more quickly? Right. And so that's that's where

294
00:27:51,840 --> 00:27:56,880
games can get a little trickier unless you're very careful about maybe which set of games you choose.

295
00:27:56,880 --> 00:28:00,960
And at the risk of kind of going kind of arguing down into another detail.

296
00:28:02,880 --> 00:28:07,600
With regards to transfer learning, is there any work looking at transfer learning?

297
00:28:07,600 --> 00:28:15,520
Is transfer learning only done kind of at the level of an entire deep network? Or can you transfer

298
00:28:15,520 --> 00:28:23,280
learn specific layers or architectural subsets of a network? That's a good question. I think

299
00:28:23,280 --> 00:28:29,040
transfer learning is still very much an open problem to claim anybody's found like a full solution

300
00:28:29,040 --> 00:28:32,960
to it. There's definitely been some progress and people have done very interesting things.

301
00:28:32,960 --> 00:28:39,760
So for example, one type of transfer that's been very successful is training on ImageNet

302
00:28:39,760 --> 00:28:44,720
and then fine tuning on a new data set. So this would be for computer vision. You want to do a

303
00:28:44,720 --> 00:28:49,280
good computer vision on a new task where you have a small amount of data. You first train on ImageNet,

304
00:28:49,280 --> 00:28:55,120
which is a data set with many, many labeled images, a thousand categories. And it turns out if

305
00:28:55,120 --> 00:29:00,800
you train to be good at recognizing those thousand categories, the later layers of this deep network

306
00:29:00,800 --> 00:29:04,320
contain features that are quite good. In fact, the entire network contains information that's

307
00:29:04,320 --> 00:29:10,160
quite good to then reuse to train another data set. Still a vision data set of course, but one

308
00:29:10,160 --> 00:29:15,600
that might not have as many labels, it might have completely different categories. So that's one

309
00:29:15,600 --> 00:29:21,600
example. The idea being that the training on ImageNet teaches your network, things like edges

310
00:29:21,600 --> 00:29:26,320
and textures and things like that that are transferable to other vision related tasks.

311
00:29:26,320 --> 00:29:32,960
Exactly. And so I think some of the most exciting work in terms of transfer has actually been

312
00:29:32,960 --> 00:29:40,160
inspired by those results. And it falls under the category of a few shot learning. And the idea

313
00:29:40,160 --> 00:29:45,120
there is that at training time, you might see a lot of data that you can do all kinds of things with.

314
00:29:45,120 --> 00:29:51,360
But then at test time, you'll get to see new data that has different categories. And you're

315
00:29:51,360 --> 00:29:57,120
supposed to learn very quickly what to do for those new categories. For example, a standard

316
00:29:57,120 --> 00:30:00,960
thing could be maybe have ImageNet, which is a thousand categories that training time you only

317
00:30:00,960 --> 00:30:06,240
get to train on 800 categories. At test time, the new categories get presented to you. I need to

318
00:30:06,240 --> 00:30:11,360
adapt very, very quickly. And so that's the few shot learning setup. People do for other data sets

319
00:30:11,360 --> 00:30:17,200
like Omnigloth, which is a handwritten character data set. And so some of the ideas there essentially,

320
00:30:17,200 --> 00:30:23,200
one example that we worked on recently, this was led by Chelsea Finnipi, as she's doing that

321
00:30:23,200 --> 00:30:29,360
Berkeley was to see if it's possible to also apply this to reinforcement learning. So people had

322
00:30:29,360 --> 00:30:34,400
had some success in supervised learning, but can you in reinforcement learning train in training

323
00:30:34,400 --> 00:30:41,280
environments, but then somehow reuse what you learn there in new test environments that are

324
00:30:41,280 --> 00:30:49,360
related. So maybe you learn to control a and simulated robot to do certain things, like maybe

325
00:30:49,360 --> 00:30:53,920
running at certain speeds. But then at test time, it needs to run at a very different speed.

326
00:30:53,920 --> 00:30:58,160
And the question is how quickly can it learn to run at that different speed? Can it do it with a

327
00:30:58,160 --> 00:31:02,880
very small number of policy-gradient updates? And indeed, the experience found that it is possible

328
00:31:02,880 --> 00:31:09,120
with a very small number of updates to adjust to a new task at test time compared to typical RL

329
00:31:09,120 --> 00:31:15,200
if you were to learn from scratch would need a very large number of iterations.

330
00:31:15,200 --> 00:31:21,600
Which brings us back to policy-gradients and Q learning. Yes, absolutely. So let's start with

331
00:31:21,600 --> 00:31:28,240
Q learning. So what's the idea behind Q learning? What's a Q value? A Q value is the Q value of a

332
00:31:28,240 --> 00:31:34,720
current state and current action is how much reward you expect to get when you start in that

333
00:31:34,720 --> 00:31:42,400
current state, take that action and from then onwards act optimally. So if you have the Q

334
00:31:42,400 --> 00:31:46,880
values, it's very easy to decide what to do. You look at the Q values in your current state and

335
00:31:46,880 --> 00:31:51,440
you just choose the action that maximizes the Q value in that current state and that's the best

336
00:31:51,440 --> 00:31:56,560
action to take. Now of course, the tricky part is how do you find your Q values? You need to

337
00:31:56,560 --> 00:32:01,600
somehow know what they are for every state that might be in the world. And so there's a lot of

338
00:32:01,600 --> 00:32:07,280
states. And so building just a table that tells you whatever Q value is is not practical unless

339
00:32:07,280 --> 00:32:13,200
your environment is really, really tiny and not of practical interest. So for realist

340
00:32:13,200 --> 00:32:19,040
environments or even just for larger, not that realist environment, like just some simple video

341
00:32:19,040 --> 00:32:23,840
games, typically the Q values are represented by deep neural nets this day. And so input to the

342
00:32:23,840 --> 00:32:28,320
neural net would be, let's say pixel values, what's currently on the screen and output would be

343
00:32:28,320 --> 00:32:33,520
for each action that you can take the Q value of that action for the current screen configuration.

344
00:32:34,080 --> 00:32:38,400
And so for the initial Atari results from DeepMind, they trained a Q network.

345
00:32:39,840 --> 00:32:44,800
And from its own tron error, this network was trained to take on the right values or good enough

346
00:32:44,800 --> 00:32:50,560
values such that if you use the trained Q network to choose your actions, you actually perform quite

347
00:32:50,560 --> 00:32:56,480
well in the game. The intuition on how you train this Q network is as follows. You say, okay,

348
00:32:56,480 --> 00:33:02,000
what does it mean to be a Q value? It's the value of current state and action, okay? That is

349
00:33:02,000 --> 00:33:07,600
actually equal to the reward you're going to get in the first transition that you encounter from

350
00:33:07,600 --> 00:33:14,000
current time to next time, plus the Q value at the next state that you landed. Because

351
00:33:14,640 --> 00:33:19,040
how well you do from current state is how well you do in the first step, plus then how well you do

352
00:33:19,040 --> 00:33:24,000
in all future steps. And so that, that then gives you actually a self consistent set of equations.

353
00:33:24,000 --> 00:33:29,040
That says Q value equals reward plus Q value at next state. And so what Q learning algorithms

354
00:33:29,040 --> 00:33:35,120
do is they solve this self consistent set of equations. And the way to solve it is by collecting

355
00:33:35,120 --> 00:33:40,080
a lot of data from running trials in the environment. And on the states and actions that are

356
00:33:40,080 --> 00:33:47,040
experienced, trying to enforce the self consistency of that set of equations. And once you've

357
00:33:47,040 --> 00:33:52,480
enforced the self consistency, you end up with, if it's fully enforcing up with the correct Q

358
00:33:52,480 --> 00:33:57,120
values and that will prescribe your actions. And also tell you how good it is to be in a certain

359
00:33:57,120 --> 00:34:02,960
state and take a certain action. And practice the one before we made self consistent. It's a very

360
00:34:02,960 --> 00:34:07,680
large set of equations. And it's not easy to make that self consistency true. But stochastic

361
00:34:07,680 --> 00:34:13,840
gradient taking updates will get you closer to self consistency. And as you get closer, acting

362
00:34:13,840 --> 00:34:21,200
based on those Q values will will typically lead to pretty good behavior. And so our policy

363
00:34:21,200 --> 00:34:27,280
gradients, an enhancement of that basic technique, or is it a different technique altogether?

364
00:34:27,280 --> 00:34:32,320
So it's very interesting. So policy gradients are a different family of techniques. But I'll get

365
00:34:32,320 --> 00:34:36,560
back to how they might actually be quite similar. Most I've explained it because actually there

366
00:34:36,560 --> 00:34:39,760
is some recent work showing that there might be stronger connections than people might have

367
00:34:39,760 --> 00:34:46,400
initially thought. Okay. But so what policy gradients do in some senses is much simpler to explain.

368
00:34:46,400 --> 00:34:51,520
What's a policy? A policy is in these days, it's a function. And these days it's typically a deep

369
00:34:51,520 --> 00:34:56,160
neural net. So it's a deep neural net that takes in, let's say current pixels and outputs the

370
00:34:56,160 --> 00:35:01,600
action that you're going to take. Or a distribution over actions is what often is used. So input,

371
00:35:01,600 --> 00:35:06,720
current situation, output distribution over actions. So once you have a policy, you can follow that

372
00:35:06,720 --> 00:35:11,600
policy by sampling from that distribution over actions. And then you're executing the policy.

373
00:35:11,600 --> 00:35:16,960
Of course, most policies are not good policies. So you need to do some work to find a good policy.

374
00:35:16,960 --> 00:35:22,720
And so a policy grand approach is a very, use a very simple idea. Let's say you have a current

375
00:35:22,720 --> 00:35:27,920
policy, you execute a few times, you see what happens, you see how much reward you got,

376
00:35:27,920 --> 00:35:32,240
and now you can use, you can perturb your policy. You can say, let me use a slightly different

377
00:35:32,240 --> 00:35:38,480
policy and execute again and see what happens. Now you can compare how well did my original

378
00:35:38,480 --> 00:35:43,920
policy do, how well did my new policy do in terms of how much reward they collected. Which

379
00:35:43,920 --> 00:35:47,920
ever is the better one you retain and you repeat, that would be a simple way to do it.

380
00:35:48,960 --> 00:35:55,040
And so that way you're gradually improving your policy as you iterate in your algorithm.

381
00:35:55,040 --> 00:36:00,960
And so you said a policy is for all intents and purposes a deep neural network. When we

382
00:36:00,960 --> 00:36:05,840
perturb the policy, are we changing the weights or are we randomly changing the weights,

383
00:36:05,840 --> 00:36:09,120
so what does that mean specifically? So there's different ways, that's a really good

384
00:36:09,120 --> 00:36:15,120
question. Different strategies to do policy perturbation. So one way can perturb the policies by

385
00:36:15,120 --> 00:36:21,440
just randomly preserving the weights in the neural net. Another way you can get variation to get

386
00:36:21,440 --> 00:36:26,080
gradient signal from is by ensuring that your distribution, that you have a distribution of

387
00:36:26,080 --> 00:36:30,560
our actions. So a non-deterministic policy. And then what happens is you sample your actions

388
00:36:30,560 --> 00:36:36,400
from that distribution. And so every rollout will lead to different behavior. And it turns out

389
00:36:36,400 --> 00:36:39,920
that you can compute policy gradients from that too using something called the likelihood ratio

390
00:36:39,920 --> 00:36:45,680
of policy gradient. And that effectively pieces apart and then makes actions that led to better

391
00:36:45,680 --> 00:36:51,840
reward more likely and actions that led to less reward less likely and it doesn't object to your

392
00:36:51,840 --> 00:36:57,040
policy that way. You can also do something like finite differences where you go per coordinate.

393
00:36:57,040 --> 00:37:01,440
Of course, in high dimensions, that's a little tricky. You could go per coordinate, increase the

394
00:37:01,440 --> 00:37:05,600
value of that coordinate. Cornet here is a single weight in your neural net. For each thing,

395
00:37:05,600 --> 00:37:10,160
each weight in your neural net, you can increase the weight, decrease it, and just use like a high

396
00:37:10,160 --> 00:37:15,600
school type derivative calculation. With finite difference ways, say I increase the x value,

397
00:37:15,600 --> 00:37:22,000
decrease the x value. Now look at f of x plus minus f of x minus divided by the size of the

398
00:37:22,000 --> 00:37:28,320
perturbation and that gives me my derivative. That's probably a baseline that you could check,

399
00:37:28,320 --> 00:37:33,760
but that would be somewhat sampling efficient if you had a high dimensional policy.

400
00:37:34,560 --> 00:37:39,040
Okay, so the summary on Q learning and policy gradients is

401
00:37:40,240 --> 00:37:47,040
Q learning, you've got a neural network that's representing essentially a sequence of consistent

402
00:37:47,040 --> 00:37:54,320
equations and you solve that. You solve for your model by enforcing that consistency and

403
00:37:54,320 --> 00:38:03,840
coming up with a set of weights that kind of maximizes the score, if you will. Maximize the

404
00:38:03,840 --> 00:38:08,800
consistency. Okay, okay. And then with policy gradients, you've got this policy that is,

405
00:38:09,600 --> 00:38:13,360
well, explain for me actually the relationship between the policy neural network and the

406
00:38:13,360 --> 00:38:18,160
broader neural network is one subset of the other or they just two totally different things.

407
00:38:18,160 --> 00:38:23,600
So in policy gradients, the policy network is the only network that you might use and it just

408
00:38:23,600 --> 00:38:29,280
represents a policy directly. Okay. Now what's interesting is that some recent words both from

409
00:38:29,280 --> 00:38:34,320
DeepMind and from OpenAI. Here, John Schillman was lead author here at OpenAI.

410
00:38:34,320 --> 00:38:39,440
Show that there's a very close connection between policy gradients and Q learning.

411
00:38:39,440 --> 00:38:45,520
Ask Q learning is typically used. So if you look at how Q learning is typically used in practice,

412
00:38:45,520 --> 00:38:49,760
look at the details because there's there's various incarnations you can have of Q and with this

413
00:38:49,760 --> 00:38:55,280
typical way of using it in practice is that you collect data based on what your current

414
00:38:55,280 --> 00:39:01,200
Q function prescribes. Once you do that, it becomes a lot closer to a policy gradement because

415
00:39:01,200 --> 00:39:05,040
the policy gradient method also says I have a current policy at collect data and I improved the

416
00:39:05,040 --> 00:39:09,280
policy based on that. If you have a Q function, a current Q function which of course doesn't

417
00:39:09,280 --> 00:39:14,880
satisfy the self-consisting equations yet, we collect data based on that current Q function

418
00:39:14,880 --> 00:39:19,600
and then try to get the self-consistency to be more satisfied. It turns out that that update

419
00:39:20,160 --> 00:39:25,360
is extremely similar and under some additional specific assumptions that are quite practical and

420
00:39:25,360 --> 00:39:32,880
people often have algorithms that match those, the two become unified and Q learning and policy

421
00:39:32,880 --> 00:39:39,440
gradients end up using the same update equations as each other which is very intriguing and actually

422
00:39:39,440 --> 00:39:44,080
explains in many ways some of the mystique that has been behind Q learning in the sense that

423
00:39:44,800 --> 00:39:49,920
if you look at Q learning the self-consistency set of equations, it turns out that

424
00:39:51,520 --> 00:39:58,160
what ends up being found doesn't really fully satisfy the self-consistency and also the values

425
00:39:58,160 --> 00:40:03,280
that you find running Q learning, which are supposed to be how much reward you'll get going

426
00:40:03,280 --> 00:40:10,160
forward from that state and action. The values are often way way off. They're not precise at all

427
00:40:10,160 --> 00:40:17,120
and nevertheless somehow this Q learning algorithm leads to a good policy and so this connection

428
00:40:17,120 --> 00:40:25,520
between the two that John Shulman figured out essentially shows why it might be that Q learning

429
00:40:25,520 --> 00:40:30,800
leads to good policies, namely that it's secretly running something like a policy-grant algorithm

430
00:40:30,800 --> 00:40:37,760
underneath or something very close to it. Is the idea then when you talked about collecting

431
00:40:37,760 --> 00:40:45,120
additional data is that in both Q learning and policy gradients you're training some agent

432
00:40:45,120 --> 00:40:52,000
to kind of navigate an environment and the agent in either case tends to perform behaviors

433
00:40:52,000 --> 00:40:58,240
in a rough neighborhood of what it has previously seen and done and that's kind of the cause

434
00:40:58,240 --> 00:41:06,400
of the one approximating the other. That's exactly right. The space is so big that the learning tends

435
00:41:06,400 --> 00:41:13,840
to focus on where you currently would be going with what you have learned so far and once you do

436
00:41:13,840 --> 00:41:18,320
that in Q learning because it's natural to restrict attention to that because why try to learn

437
00:41:18,320 --> 00:41:23,280
about everything there's so much that you might be busy for too long. Once you mostly pay attention

438
00:41:23,280 --> 00:41:28,640
to what your current Q function prescribes you start being extremely similar to a policy-grating

439
00:41:28,640 --> 00:41:34,160
method. You've mentioned pixels a few times in your descriptions is reinforcement learning only

440
00:41:34,160 --> 00:41:41,440
applied to applications that have some vision component? So reinforcement learning can take in

441
00:41:41,440 --> 00:41:48,800
any type of sensory input. So raw sensory information could be pixels but it could be something else

442
00:41:48,800 --> 00:41:54,720
in robotics. We also take in joint angles and joint velocities because while the motors that

443
00:41:54,720 --> 00:41:59,680
are part of the robot know those values and they're very informative about what situation the robot

444
00:41:59,680 --> 00:42:07,120
is currently in. So that's being fed in too. Looking ahead things that are very interesting to me are

445
00:42:07,120 --> 00:42:13,280
things like tactile sensing. If you have a robot hand if you can have tactile sensing on that robot hand

446
00:42:14,320 --> 00:42:20,000
that should amplify what this robot hand is capable of doing but now how do you process that

447
00:42:20,000 --> 00:42:26,240
information? How do you turn this raw sensory tactile information into an understanding of how you're

448
00:42:26,240 --> 00:42:29,680
holding the object? Well there are some object properties of the object that you're holding and

449
00:42:29,680 --> 00:42:36,320
so forth. Those are challenging problems but also the kind of problems that I suspect deep learning

450
00:42:36,320 --> 00:42:43,600
could help solve because it is the same flavor of problem as the image processing type problems.

451
00:42:43,600 --> 00:42:48,160
You have high dimensional sensory information. The information is intrinsically in there but it

452
00:42:48,160 --> 00:42:54,080
just somehow needs to be teased apart from these raw sensory inputs and so it should be learnable

453
00:42:54,080 --> 00:42:59,680
if we set up the data collection for that if we have some supervision or if there's some reward

454
00:42:59,680 --> 00:43:07,200
related to if you had a reward such that to be successful on that reward tactile would matter

455
00:43:07,200 --> 00:43:12,560
then presumably that robot hand would learn how to process tactile information because that would be

456
00:43:12,560 --> 00:43:20,960
the way to maximize reward. So we've talked about games we've talked about robots

457
00:43:22,000 --> 00:43:27,920
focusing in on the industrial applications of reinforcement learning in and around robots and

458
00:43:27,920 --> 00:43:35,680
other other use cases that would appear within an enterprise context. What use cases that we

459
00:43:35,680 --> 00:43:41,360
seen success with and where are we kind of getting close? Okay that that's a great question that

460
00:43:41,920 --> 00:43:48,400
actually first I think a lot of reinforcement learning right now is happening in still research

461
00:43:48,400 --> 00:43:54,320
environments. So if you look at a lot of the big success tours of reinforcement learning and

462
00:43:54,320 --> 00:43:59,440
that are very well known learning to play Atari games. Learning to play go which by the way there's

463
00:43:59,440 --> 00:44:04,320
a combination of imitation and reinforcement learning in that case. Learning simulated locomotion

464
00:44:04,320 --> 00:44:11,760
skills that was some of the work John Schumer and Sergey Leven did at at at Berkeley or learning

465
00:44:11,760 --> 00:44:18,160
robot motor control but still for very simple tasks was Sergey Leven and Chelsea Finna Berkeley

466
00:44:18,160 --> 00:44:23,840
how how to assemble let's say toys all of those are are still a little removed from what you

467
00:44:23,840 --> 00:44:30,080
would think of as real world deployment. I think there are a few reasons for that. I think one reason

468
00:44:30,080 --> 00:44:37,120
is that these algorithms are only recently have become like really something that people are

469
00:44:37,120 --> 00:44:42,800
able to get to work five years ago people didn't think those things are possible but now they're

470
00:44:42,800 --> 00:44:48,880
starting to work and it takes some time to transition that into applications especially since for

471
00:44:48,880 --> 00:44:54,720
now it still requires a little bit of well I would say a substantial amount of reinforcement

472
00:44:54,720 --> 00:45:00,960
learning expertise to make sure these things work out and so the number of people who can put this

473
00:45:00,960 --> 00:45:06,320
to use is still a little limited and a lot of those people are actually excited about expanding

474
00:45:06,320 --> 00:45:11,600
the research frontier rather than necessarily putting it into application so there's maybe a slight

475
00:45:11,600 --> 00:45:18,080
shortage of reinforcement learning experts that are taking their expertise then and and try to

476
00:45:18,080 --> 00:45:22,320
deploy them in the real world because the real world has many so man's where it could be applied

477
00:45:23,680 --> 00:45:29,600
anything where you make decisions over time reinforcement learning is going to matter this could be

478
00:45:29,600 --> 00:45:37,280
for your HFAC system this could be for let's say servicing demand in cues where maybe you're

479
00:45:37,280 --> 00:45:41,520
providing support longer run once language understanding is better this could be part of

480
00:45:41,520 --> 00:45:47,920
dialogue because the goal and dialogue is not just to spit out a statistically reasonable sentence

481
00:45:47,920 --> 00:45:52,720
and reply to the previous one the goal and dialogue tends to be figuring out what the other person

482
00:45:52,720 --> 00:45:57,840
wants to achieve and helping them achieve what they're trying to achieve and so in that scenario

483
00:45:57,840 --> 00:46:02,480
there is a reinforcement learning problem in terms of maximizing reward is maximizing happiness

484
00:46:02,480 --> 00:46:07,360
of the other side in terms of what they get out of this conversation and so forth so one of the

485
00:46:07,360 --> 00:46:13,360
things we're actually doing in late august is this need to get it with a few other people so also

486
00:46:13,360 --> 00:46:18,880
Vladimir from from deep mind under her path from open AI serving 11 from Berkeley Chelsea Finn

487
00:46:18,880 --> 00:46:24,480
from Berkeley and then John Schillman rocket one and Peter Chen from open AI is organizing a

488
00:46:24,480 --> 00:46:30,960
deep reinforcement learning boot camp in late august the incentive here is that it seems

489
00:46:30,960 --> 00:46:37,040
reinforcement learning is getting ready to be deployed in various application domains in industry

490
00:46:37,040 --> 00:46:44,080
but it will require more experts and so to educate experts to then start you know they they will

491
00:46:44,080 --> 00:46:48,560
see the applications once their experts they'll see the applications and latch onto them and start

492
00:46:48,560 --> 00:46:54,560
deploying things and so the hope is to kind of accelerate that a little bit by having a weekend

493
00:46:54,560 --> 00:47:01,440
a very intense weekend with lectures but at least for about 50% lab sessions where people really

494
00:47:01,440 --> 00:47:07,680
you know get things working in old environments we talked about simulated robots video games and so

495
00:47:07,680 --> 00:47:12,960
forth get the essence down but then with the hope that they can take it back to their companies

496
00:47:12,960 --> 00:47:19,200
or to their research efforts because it's also useful for researchers to be more productive and

497
00:47:19,200 --> 00:47:26,640
get things done with reinforcement learning that sounds great I will get the URL to that from you

498
00:47:26,640 --> 00:47:31,520
and we'll make sure to include it in the show notes it sounds like the summary is that the technology

499
00:47:31,520 --> 00:47:39,200
is ready but there is still a shortage of expertise to help folks build out these applications and

500
00:47:40,000 --> 00:47:46,160
you know start so that we can start to see you know proliferation of success stories out in

501
00:47:46,160 --> 00:47:54,400
the commercial world yeah am I am I say the technology is at the cost of being ready I wouldn't say

502
00:47:54,400 --> 00:48:00,400
it's like very mature it's like it's it's getting there and the first application should become

503
00:48:00,400 --> 00:48:05,200
possible in the future but the technology shouldn't also still be improved a lot over the next few

504
00:48:05,200 --> 00:48:13,120
years and in your experience is reinforcement learning often a an alternative to some other

505
00:48:13,120 --> 00:48:20,320
technique that you know might work or is reinforcement learning kind of the only way to solve

506
00:48:20,320 --> 00:48:25,120
the problems that reinforcement learning is good at and you know is there a general way to

507
00:48:25,120 --> 00:48:30,960
characterize like the benefits or advantages of our own relative to you know some alternative

508
00:48:30,960 --> 00:48:36,320
approaches so the typical starting point when you need to make decisions over time like

509
00:48:36,320 --> 00:48:43,200
it reinforcement learning would be imitation learning because imitation learning is simpler

510
00:48:43,200 --> 00:48:48,800
it's it's like supervised learning you would demonstrate what needs to happen and then you would

511
00:48:48,800 --> 00:48:54,240
try to learn something that matches what you did during the demonstrations now when it's hard to

512
00:48:54,240 --> 00:49:00,320
demonstrate that could be an issue or it could be that it's not too hard to demonstrate but it's

513
00:49:00,320 --> 00:49:06,240
hard to demonstrate at the scale that you need to learn from and that's where reinforcement

514
00:49:06,240 --> 00:49:10,880
can do autonomous data collection and so it might be able to collect a much larger amount of data

515
00:49:10,880 --> 00:49:15,120
than you can get from demonstrations it could also be that you can demonstrate in the format that

516
00:49:15,120 --> 00:49:21,920
you need so maybe you want a robot to do something that maybe load your dishwasher but it's very hard

517
00:49:21,920 --> 00:49:27,120
to make the robot do it you can do it yes by hand yourself but that's not the exact form factor

518
00:49:27,120 --> 00:49:31,040
that's easiest to learn from and it would be the third person imitation again which is

519
00:49:31,040 --> 00:49:36,400
still a hard problem even though some progress has been made so the first shot typically

520
00:49:36,400 --> 00:49:42,880
I would argue is you try and find a way to get imitation in place see how far you can get with that

521
00:49:42,880 --> 00:49:46,800
and then take it from there and typically you'll you'll build reinforcement on top of that

522
00:49:46,800 --> 00:49:51,440
that will fine tune or in some cases imitation will just not be workable because you can get

523
00:49:52,080 --> 00:49:57,680
the demonstrations that that you need yeah I would imagine even if you can kind of demonstrate

524
00:49:57,680 --> 00:50:04,720
the activity in a perfect environment reinforcement learning has still advantages in being somewhat

525
00:50:04,720 --> 00:50:10,480
more robust to the position of the object that you're picking relative to just a pure imitation

526
00:50:10,480 --> 00:50:16,560
learning is that true in general that capability of adaptation once deployed if you let your

527
00:50:16,560 --> 00:50:21,520
reinforcement learner continue to learn once is deployed is very different from what you would get

528
00:50:21,520 --> 00:50:26,320
from a standard imitation learning setup and so yes that's absolutely a big part that also

529
00:50:26,320 --> 00:50:31,920
reminds me of the I think automation provides big opportunities and one I'm personally particularly

530
00:50:31,920 --> 00:50:37,920
interested in is how to get reinforcement learning and imitation learning to start playing big

531
00:50:37,920 --> 00:50:44,800
roles in how automation works for manufacturing to make that a lot more flexible than the way

532
00:50:44,800 --> 00:50:48,240
things tend to be right now which is a lot more rigid than how you need to set things up

533
00:50:48,240 --> 00:50:55,200
and the idea being there that as opposed to a particular you know an agent like a robot

534
00:50:55,200 --> 00:51:00,160
manipulating something we're talking about now sequences of steps are we talking about kind of

535
00:51:00,160 --> 00:51:04,320
the big picture manufacturing are we talking about you know still individual devices

536
00:51:04,320 --> 00:51:11,040
it could be both so at the individual level it could be that instead of you know setting up a robot

537
00:51:11,040 --> 00:51:17,840
for taking multiple days or weeks to set up a robot for an individual manipulation skill you just

538
00:51:17,840 --> 00:51:22,880
demonstrate a few times it learns from that reinforcement learns on top of that and based on that

539
00:51:22,880 --> 00:51:28,320
maybe can be deployed within hours rather than days or weeks in the bigger picture I think it's

540
00:51:28,320 --> 00:51:33,600
very fascinating I mean big it wouldn't be that easy to execute on a small scale with a bigger

541
00:51:33,600 --> 00:51:40,000
scale if you say I want a factory that can take in any raw materials and output any goods and the

542
00:51:40,000 --> 00:51:45,120
system can just adapt itself to whatever the needs are just send your design files and outcomes

543
00:51:45,120 --> 00:51:50,960
you know in goes raw materials outcomes the product I think I mean that I'm not saying that that's

544
00:51:50,960 --> 00:51:59,760
going to happen tomorrow but that kind of flexibility would be really amazing and it seems it requires

545
00:51:59,760 --> 00:52:04,800
a lot of flexibility a lot of adaptation the robots in such a system need to be able to do a wide

546
00:52:04,800 --> 00:52:08,880
range of things you can just set them up for one thing because then something new needs to

547
00:52:08,880 --> 00:52:14,400
manufactures and they need to reconfigure themselves take up a new skill and work together to get the

548
00:52:14,400 --> 00:52:19,360
product made right right well that's a super compelling vision and maybe a great place to

549
00:52:19,360 --> 00:52:24,400
leave off here anything else that you'd like to leave the audience with we'll definitely share

550
00:52:24,400 --> 00:52:27,840
that link to the deep learning bootcamp but anything else you'd like to share

551
00:52:27,840 --> 00:52:34,320
um thanks for listening uh thanks for having me Sam this is a great and fun chat yeah great well

552
00:52:34,320 --> 00:52:41,840
thanks so much Peter I really appreciate it bye bye all right everyone that's our show for today

553
00:52:41,840 --> 00:52:47,920
thanks so much for listening and for your continued support comments and feedback we're excited to

554
00:52:47,920 --> 00:52:53,520
hear what you guys think about this show and the series I'd also like to thank our sponsor

555
00:52:53,520 --> 00:53:02,320
Banzai once again be sure to check out what they're doing at Banz.ai B-O-M-S.ai another reminder

556
00:53:02,320 --> 00:53:08,160
there are less than two weeks left into the O'Reilly AI conference in New York City and I just saw

557
00:53:08,160 --> 00:53:14,720
an email this morning that prices go up today if you'd like to attend you can save 20% on registration

558
00:53:14,720 --> 00:53:21,600
using our discount code which is PC Twimble PCT W-I-M-L we'll link to the registration page

559
00:53:21,600 --> 00:53:27,200
in the show notes I'd love to meet up with listeners at the conference and as I mentioned last time

560
00:53:27,200 --> 00:53:31,840
I'm planning a meetup during the event I'll share details as soon as they've been ironed up

561
00:53:32,880 --> 00:53:38,160
the notes for this episode can be found at Twimble AI dot com slash talk slash 28

562
00:53:38,880 --> 00:53:45,280
for more information on industrial AI my report or the industrial AI podcast series visit

563
00:53:45,280 --> 00:53:52,080
Twimble AI dot com slash industrial AI as always remember to post your favorite quote or take away

564
00:53:52,080 --> 00:53:57,840
from this episode or any other and we'll send you a laptop sticker you can post them as comments

565
00:53:57,840 --> 00:54:05,120
to the show notes page via Twitter at Twimble AI or via our Facebook page once again thanks so

566
00:54:05,120 --> 00:54:15,120
much for listening and catch you next time

