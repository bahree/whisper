1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,000
I'm your host Sam Charrington.

4
00:00:32,000 --> 00:00:37,680
With today's show we conclude our TensorFlow Dev Summit series which was recorded on location

5
00:00:37,680 --> 00:00:40,320
during the summit a few weeks back.

6
00:00:40,320 --> 00:00:45,040
This overview features my conversation with Alfredo Luquet, a software engineer on the machine

7
00:00:45,040 --> 00:00:48,480
learning infrastructure team at Airbnb.

8
00:00:48,480 --> 00:00:53,800
If you're among the many Twimble fans interested in AI platforms and machine learning infrastructure,

9
00:00:53,800 --> 00:00:59,240
you probably remember my interview with Alfredo's colleague, Airbnb's Otto Kale, in which we

10
00:00:59,240 --> 00:01:01,680
discussed their big head platform.

11
00:01:01,680 --> 00:01:07,040
In my conversation with Alfredo, we dig a bit deeper into big head support for TensorFlow.

12
00:01:07,040 --> 00:01:11,320
Discuss a recent image categorization challenge they solved with the framework and explore

13
00:01:11,320 --> 00:01:15,840
what the new 2.0 release means for their users.

14
00:01:15,840 --> 00:01:20,080
If you're interested in topics like these, TensorFlow, deep learning, you'll probably

15
00:01:20,080 --> 00:01:25,200
also be interested in the awesome swag box that Google gave out to some of the attendees.

16
00:01:25,200 --> 00:01:30,000
They included their new Coral Edge TPU device and the Spark Fund Edge development board

17
00:01:30,000 --> 00:01:32,400
as well as some other really cool goodies.

18
00:01:32,400 --> 00:01:37,360
And I'm excited to give you an opportunity to win one in our latest giveaway.

19
00:01:37,360 --> 00:01:42,800
To enter, just visit Twimbleai.com slash TF giveaway and let us know what you would do

20
00:01:42,800 --> 00:01:46,280
with this kit if you got your hands on it.

21
00:01:46,280 --> 00:01:50,560
We'd like to send a huge thanks to the TensorFlow team for helping us bring you this podcast

22
00:01:50,560 --> 00:01:52,520
series and giveaway.

23
00:01:52,520 --> 00:01:57,080
With all the great announcements coming out of the Dev Summit, including the 2.0 Alpha,

24
00:01:57,080 --> 00:02:01,320
you should definitely check out the latest and greatest over at TensorFlow.org, where

25
00:02:01,320 --> 00:02:05,000
you can also download and of course start building with the framework.

26
00:02:05,000 --> 00:02:09,520
And now on to the show.

27
00:02:09,520 --> 00:02:16,640
All right, everyone, I am here at the TensorFlow Developer Summit with Alfredo Lukay.

28
00:02:16,640 --> 00:02:19,360
Alfredo is a software engineer at Airbnb.

29
00:02:19,360 --> 00:02:22,120
Alfredo, welcome to this week in machine learning and AI.

30
00:02:22,120 --> 00:02:23,120
Hi there.

31
00:02:23,120 --> 00:02:28,800
Let's get started by having you share a little bit about your background and how you came

32
00:02:28,800 --> 00:02:35,000
to work in machine learning and AI and in particular on machine learning platforms and frameworks.

33
00:02:35,000 --> 00:02:36,000
Sure.

34
00:02:36,000 --> 00:02:42,040
So I guess my interesting machine learning started in college actually well before, you know,

35
00:02:42,040 --> 00:02:46,400
there were a lot of applications to do this.

36
00:02:46,400 --> 00:02:52,560
Started with writings of my own like kernels for GPUs by hand.

37
00:02:52,560 --> 00:02:57,480
And exploring some of the really primitive ways we could do ML back then.

38
00:02:57,480 --> 00:02:59,080
Since then a lot of things have changed.

39
00:02:59,080 --> 00:03:03,680
I mean, since, you know, the last five or six years we had a lot of new developments.

40
00:03:03,680 --> 00:03:07,840
What interest drove you to write your own kernels?

41
00:03:07,840 --> 00:03:11,840
It was just very curious about, you know, what could be done there.

42
00:03:11,840 --> 00:03:17,840
There have been a lot of interesting and promising research around image recognition.

43
00:03:17,840 --> 00:03:21,440
First the convolutional neural net papers have been starting to drop.

44
00:03:21,440 --> 00:03:26,400
And there was a lot of really great applications that actually tried to set yourself outside

45
00:03:26,400 --> 00:03:28,600
of a research setting.

46
00:03:28,600 --> 00:03:31,360
So that's what really got me into it first.

47
00:03:31,360 --> 00:03:36,560
I went on to work in the online advertising real-time bidding.

48
00:03:36,560 --> 00:03:42,640
And later to work on my own hedge funds and then we really, we're just a lot of techniques

49
00:03:42,640 --> 00:03:47,640
in like natural language processing, which further kind of drove me into the field.

50
00:03:47,640 --> 00:03:48,640
Okay.

51
00:03:48,640 --> 00:03:49,640
Yeah.

52
00:03:49,640 --> 00:03:50,640
Interesting.

53
00:03:50,640 --> 00:03:53,960
I don't think I've talked to someone that started their own hedge fund before.

54
00:03:53,960 --> 00:03:55,760
Yeah.

55
00:03:55,760 --> 00:03:57,560
So you're at Airbnb now.

56
00:03:57,560 --> 00:04:00,320
What's your focus at Airbnb?

57
00:04:00,320 --> 00:04:04,800
So at Airbnb, I work on our machine learning infrastructure team.

58
00:04:04,800 --> 00:04:10,920
You actually had our engineering manager on here a couple of episodes ago.

59
00:04:10,920 --> 00:04:15,560
Basically we build tooling to make machine learning really easy.

60
00:04:15,560 --> 00:04:20,160
Developing models and productionizing them should be something that everybody can do.

61
00:04:20,160 --> 00:04:22,760
It's kind of our belief.

62
00:04:22,760 --> 00:04:28,800
And I specifically focus on aspects involving the actual creation of models.

63
00:04:28,800 --> 00:04:34,200
You know, actually developing which techniques you're going to be using, making sure that

64
00:04:34,200 --> 00:04:39,320
it's really easy to assemble those things and making sure there's no additional difficulty

65
00:04:39,320 --> 00:04:42,680
in actually making it production ready.

66
00:04:42,680 --> 00:04:49,560
So we have our own libraries that wrap around most of the major ML frameworks and make this

67
00:04:49,560 --> 00:04:51,880
a really, really easy thing to do.

68
00:04:51,880 --> 00:04:56,640
So TensorFlow is one of those, but we support most of the major ML frameworks.

69
00:04:56,640 --> 00:05:02,320
Yeah, that was one of the aspects of what you're doing with Big Head that struck me in that

70
00:05:02,320 --> 00:05:11,280
initial conversation with Atul, the extent to which you've invested in building the platform

71
00:05:11,280 --> 00:05:16,840
and be framework agnostic and supporting many different frameworks that the data scientists

72
00:05:16,840 --> 00:05:19,400
and software engineers want to use.

73
00:05:19,400 --> 00:05:24,200
Is that responsibility falls on you among others?

74
00:05:24,200 --> 00:05:28,520
But yeah, how do you kind of manage the burden of doing that?

75
00:05:28,520 --> 00:05:33,320
Yeah, so we kind of dove in head first as the answer.

76
00:05:33,320 --> 00:05:38,640
The really two approaches that you see to handling this sort of agnostic behavior.

77
00:05:38,640 --> 00:05:42,800
One of them is you basically treat a machine learning model like a black box, right?

78
00:05:42,800 --> 00:05:47,080
There is some interface, you know, you feed it some data and it's going to spit something

79
00:05:47,080 --> 00:05:50,720
out and then you don't really care what happens inside.

80
00:05:50,720 --> 00:05:55,280
And that definitely does make your platform pretty agnostic, but it also means you don't

81
00:05:55,280 --> 00:05:57,680
really have a really nice integration.

82
00:05:57,680 --> 00:06:02,840
So as a data scientist from ML engineer, if I'm actually trying to develop a model, it

83
00:06:02,840 --> 00:06:07,200
becomes much more difficult if all I hand off is just this big black box.

84
00:06:07,200 --> 00:06:10,360
We don't know if it's going to run efficiently.

85
00:06:10,360 --> 00:06:14,680
We don't know if I want a different environment, it's going to behave the same way.

86
00:06:14,680 --> 00:06:18,800
And I'm not really developing using the same tooling that's actually going to be running

87
00:06:18,800 --> 00:06:20,320
my model.

88
00:06:20,320 --> 00:06:25,720
So metronome frameworks have started really working on this issue of integrating everything

89
00:06:25,720 --> 00:06:26,720
nicely.

90
00:06:26,720 --> 00:06:31,000
I think in 2.0 tons of flows are really making strides there.

91
00:06:31,000 --> 00:06:34,520
And there's other frameworks as well that has really started focusing on this.

92
00:06:34,520 --> 00:06:40,240
But our goal was to really build these libraries that cleanly wrap around the major features

93
00:06:40,240 --> 00:06:42,480
of each framework.

94
00:06:42,480 --> 00:06:47,160
That doesn't mean a high support load, but it also does mean that our users, when they

95
00:06:47,160 --> 00:06:50,880
write something really feel like they're interacting directly with the framework, can

96
00:06:50,880 --> 00:06:53,440
use all its features.

97
00:06:53,440 --> 00:06:58,720
And when they go to production lines, their model, then they get exactly what they wrote.

98
00:06:58,720 --> 00:07:01,360
And they have a lot of predictability.

99
00:07:01,360 --> 00:07:06,000
And more importantly, they can just swap out tons of flow for actually boost or something

100
00:07:06,000 --> 00:07:11,520
else and get the exact same results in the earlier parts of their preprocessing.

101
00:07:11,520 --> 00:07:17,440
So that was a really important aspect of Big Head for us was working on that.

102
00:07:17,440 --> 00:07:25,640
I would imagine that swap ability kind of forced you down and approach other than the one

103
00:07:25,640 --> 00:07:31,360
you've taken where it's more black boxy, more kind of least common denominator.

104
00:07:31,360 --> 00:07:37,720
How do you expose the full functionality, but yet still a lot of people that do swap

105
00:07:37,720 --> 00:07:39,560
outs?

106
00:07:39,560 --> 00:07:46,560
We basically dive into the internals of each framework and try to find the interface

107
00:07:46,560 --> 00:07:52,680
point that sort of makes sense to let users develop their model.

108
00:07:52,680 --> 00:07:57,680
But then encapsulate it in a way where we know how to now feed data into that model.

109
00:07:57,680 --> 00:08:03,120
So once we really establish the method, but we can feed data into a model of a given

110
00:08:03,120 --> 00:08:08,640
framework, then whatever is on the other side, we already know how to deal with essentially

111
00:08:08,640 --> 00:08:10,480
what's in the model.

112
00:08:10,480 --> 00:08:14,920
And what comes before we build ourselves or the user route.

113
00:08:14,920 --> 00:08:19,640
So we have a really, really good way of essentially converting all these data types and making

114
00:08:19,640 --> 00:08:22,280
sure the flow of the data works well.

115
00:08:22,280 --> 00:08:27,400
Another aspect we're exploring as well is once you write what we call a Big Head pipeline,

116
00:08:27,400 --> 00:08:31,640
essentially your entire workflow graph of all the preprocessing steps in your machine

117
00:08:31,640 --> 00:08:37,840
learning models, we can actually potentially convert that into, if you're using one framework,

118
00:08:37,840 --> 00:08:41,200
a framework's native operations.

119
00:08:41,200 --> 00:08:46,520
So if you're using purely tenser flow, you can pile it into a tenser flow graph.

120
00:08:46,520 --> 00:08:51,080
And I think this approach is neat because in that case, you'd get basically no loss in

121
00:08:51,080 --> 00:08:52,080
performance.

122
00:08:52,080 --> 00:08:56,440
But again, it does require diving down into the internals of each framework.

123
00:08:56,440 --> 00:09:01,000
I mean, that is a lot of work, but we think it's worth it.

124
00:09:01,000 --> 00:09:06,480
We've also been collaborating directly looking to, as we open source, and agree engineers

125
00:09:06,480 --> 00:09:12,840
from those different platforms and supporting some of these wrappers and contributing directly.

126
00:09:12,840 --> 00:09:13,840
So.

127
00:09:13,840 --> 00:09:20,680
One of the things that was featured here at the summit was a case study video about a

128
00:09:20,680 --> 00:09:25,480
project at Airbnb focused on categorizing listing images.

129
00:09:25,480 --> 00:09:26,480
What's that project about?

130
00:09:26,480 --> 00:09:29,800
Yes, at Airbnb, we have a lot of listing images.

131
00:09:29,800 --> 00:09:36,320
Basically, every listing you see might have between five to maybe more images.

132
00:09:36,320 --> 00:09:41,720
And one of the challenges is showing the relevant images, right?

133
00:09:41,720 --> 00:09:47,400
So when you go try to book an Airbnb, typically the first thing you want to see is, you know,

134
00:09:47,400 --> 00:09:54,800
put you into the living room or the bedroom and not like the bathroom ear.

135
00:09:54,800 --> 00:10:00,880
So it becomes really important to get accurate classification of those images so that we can

136
00:10:00,880 --> 00:10:02,120
choose what to present.

137
00:10:02,120 --> 00:10:07,200
And for host, we can also assist them potentially in reordering them.

138
00:10:07,200 --> 00:10:13,160
So it was a pretty big project because while image classification is a well-known solved

139
00:10:13,160 --> 00:10:17,000
problem, the issue is the skill.

140
00:10:17,000 --> 00:10:21,480
We have almost half a billion images to sit through.

141
00:10:21,480 --> 00:10:26,800
And if you try out a new model and you want to backfill it, that's a lot of time to backfill

142
00:10:26,800 --> 00:10:27,800
it.

143
00:10:27,800 --> 00:10:32,920
So traditional approaches, just kind of running this on, there's things like Spark, you can

144
00:10:32,920 --> 00:10:39,040
run this on, so the CPUs would have taken months with any framework.

145
00:10:39,040 --> 00:10:46,320
And so one of the things we did was really focus on optimizing that and just taking a pre-trained

146
00:10:46,320 --> 00:10:52,240
model on Keras with some of the optimizations we made earlier in the pre-processing, we

147
00:10:52,240 --> 00:10:55,000
were able to get that to a few days.

148
00:10:55,000 --> 00:11:00,640
It's running on a few machines with GPUs, and so that was a pretty big one for us.

149
00:11:00,640 --> 00:11:05,000
And there are many other projects like this that can use a similar approach.

150
00:11:05,000 --> 00:11:09,840
What was it specifically that allowed you to get that to a few days from weeks or months?

151
00:11:09,840 --> 00:11:16,760
Yeah, so I mean, when you have image data, unfortunately, it's never exactly in the format

152
00:11:16,760 --> 00:11:18,760
that you need.

153
00:11:18,760 --> 00:11:24,920
Yeah, an ideal world you have, you know, perfectly resized data sets, everything's like

154
00:11:24,920 --> 00:11:31,280
nice and clean, but we have some images that might just, you know, they might be corrupted,

155
00:11:31,280 --> 00:11:33,880
they're the wrong size.

156
00:11:33,880 --> 00:11:38,880
If you're using something like ResNet 50, you have to do some scaling and kind of normalize

157
00:11:38,880 --> 00:11:39,880
the values.

158
00:11:39,880 --> 00:11:40,880
Get to the T24 by T24 image.

159
00:11:40,880 --> 00:11:46,400
Yeah, and then normalize it to zero and range and reorder the color channels.

160
00:11:46,400 --> 00:11:50,840
A lot of this stuff was written in Python, especially if you use Carrots, there's a lot

161
00:11:50,840 --> 00:11:57,560
of Python in there, and that becomes very, very slow, that ultimately limits, even if

162
00:11:57,560 --> 00:12:04,200
you have the fastest GPUs out there, they can't get images quickly enough to actually want

163
00:12:04,200 --> 00:12:05,720
inference on your model.

164
00:12:05,720 --> 00:12:10,960
Is the primary time savings inference focused or is it the training?

165
00:12:10,960 --> 00:12:11,960
An inference.

166
00:12:11,960 --> 00:12:12,960
Okay.

167
00:12:12,960 --> 00:12:19,760
Maybe in B, most of our scaling issues are always with inference on a really training.

168
00:12:19,760 --> 00:12:25,200
We've found at least that, typically with, you know, the full-todd GPUs or if you use

169
00:12:25,200 --> 00:12:34,600
GPUs, a couple of machines are plenty to reasonably train whatever you have, but for inference,

170
00:12:34,600 --> 00:12:37,040
that's the real key.

171
00:12:37,040 --> 00:12:38,520
If you try to backhole something there.

172
00:12:38,520 --> 00:12:44,080
Every day at huge scale, yeah, and if you try out something new, you know, every day

173
00:12:44,080 --> 00:12:48,960
you get more images, so now it's going to take even longer to backfill all of history.

174
00:12:48,960 --> 00:12:54,360
So we find ourselves backfilling all of history a lot, basically every time we try out something

175
00:12:54,360 --> 00:12:55,360
new.

176
00:12:55,360 --> 00:12:58,800
Let's dig into that particular point a little bit more.

177
00:12:58,800 --> 00:13:06,400
This was something that I don't recall if Attila and I went really deep into this, but

178
00:13:06,400 --> 00:13:10,960
Airbnb has spent a lot of time, like backfilling is a big part of the way you think about

179
00:13:10,960 --> 00:13:14,480
the problems that you're solving, like zipline, a big part of what it's doing is trying

180
00:13:14,480 --> 00:13:23,160
to manage these point in time, features and backfills, and like, yeah, walk us through

181
00:13:23,160 --> 00:13:24,160
like that whole space.

182
00:13:24,160 --> 00:13:29,080
What's the, is backfilling and how does it, where does it arise in your workflows?

183
00:13:29,080 --> 00:13:33,880
Yeah, so I mean, a really simple definition of backfilling would be, you know, if you have

184
00:13:33,880 --> 00:13:41,120
data that's collected every day and saved every day, then, you know, we might train our

185
00:13:41,120 --> 00:13:46,680
model on a couple of days, but if we want to apply our model to all the data that we've

186
00:13:46,680 --> 00:13:50,760
ever collected, we have to go through all of history.

187
00:13:50,760 --> 00:13:53,760
So it might have started being collected two or three years ago.

188
00:13:53,760 --> 00:13:58,720
So what's the specific example where you might want to apply the model to all of history?

189
00:13:58,720 --> 00:14:02,040
So this was actually, that project is actually a great example.

190
00:14:02,040 --> 00:14:04,360
It's categorizing the languages, okay?

191
00:14:04,360 --> 00:14:10,240
Yeah, so if you just categorized last month, it's not too useful because frankly, the

192
00:14:10,240 --> 00:14:15,000
user could click on a listing that hasn't been categorized, right?

193
00:14:15,000 --> 00:14:20,560
Doing that in real time ends up being a lot of wasted effort because Airbnb has, you

194
00:14:20,560 --> 00:14:24,480
know, a limited number of listings, it has many, but, you know, it's a finite number.

195
00:14:24,480 --> 00:14:27,360
And it's a pretty manageable number.

196
00:14:27,360 --> 00:14:29,640
And many users will be viewing the same listing.

197
00:14:29,640 --> 00:14:35,320
So you have a lot of duplicated work there and it's a pretty expensive computation to go

198
00:14:35,320 --> 00:14:38,560
through 15 images every time somebody clicks on it, right?

199
00:14:38,560 --> 00:14:39,560
Okay.

200
00:14:39,560 --> 00:14:43,720
So if we want to categorize these images, we need to do it for all listings, right?

201
00:14:43,720 --> 00:14:44,720
Got it.

202
00:14:44,720 --> 00:14:49,400
And if you want to run an experiment then and see by providing these better labels and

203
00:14:49,400 --> 00:14:53,480
these reordered images on the sites, do we have a revenue increase?

204
00:14:53,480 --> 00:14:55,120
You really need to go through everything.

205
00:14:55,120 --> 00:14:56,120
Okay.

206
00:14:56,120 --> 00:14:58,520
So that's kind of a rationale for it.

207
00:14:58,520 --> 00:15:06,240
And so it's, again, backfilling is, does it only occur in inference scenarios or is

208
00:15:06,240 --> 00:15:10,160
it also relevant to training scenarios?

209
00:15:10,160 --> 00:15:16,120
It could be relevant to training scenarios if you need to generate training data.

210
00:15:16,120 --> 00:15:19,920
So one example of this is there are models that use embedding.

211
00:15:19,920 --> 00:15:28,320
So essentially, instead of a single value, outputting a chunk of your network, right?

212
00:15:28,320 --> 00:15:32,440
And any of these models do need these embeddings and so you have to go run the embeddings model

213
00:15:32,440 --> 00:15:34,840
on all your data.

214
00:15:34,840 --> 00:15:40,120
If that embeddings model changes or something downstream changes, you might need a backfill

215
00:15:40,120 --> 00:15:41,120
everything.

216
00:15:41,120 --> 00:15:42,120
Okay.

217
00:15:42,120 --> 00:15:47,320
So there are scenarios like this where for training, you might need to backfill something.

218
00:15:47,320 --> 00:15:49,920
I'd say the majority are inference.

219
00:15:49,920 --> 00:15:53,880
For most enterprises, it's pretty similar to what we've heard as well.

220
00:15:53,880 --> 00:15:58,280
It's inference where we didn't chuck through a lot of data, right?

221
00:15:58,280 --> 00:16:07,040
You kind of set up this backfill process, well, the big task was to reduce the amount

222
00:16:07,040 --> 00:16:11,960
of time it took to categorize these images so that when you're doing the actual backfilling,

223
00:16:11,960 --> 00:16:14,280
it's a manageable time.

224
00:16:14,280 --> 00:16:17,520
You mentioned kind of a big part of what you do is like digging deep into these frameworks

225
00:16:17,520 --> 00:16:21,080
did, to what extent was that required in this project?

226
00:16:21,080 --> 00:16:22,080
Yeah.

227
00:16:22,080 --> 00:16:28,240
So we actually done that work already in this case for essentially wrapping Keras and

228
00:16:28,240 --> 00:16:32,840
there are different backends for it, but we had specifically also wrapped TensorFlow specific

229
00:16:32,840 --> 00:16:34,800
implementation.

230
00:16:34,800 --> 00:16:41,720
The investment here was also largely in building these reusable primitives that we could use,

231
00:16:41,720 --> 00:16:47,320
whether we were using TensorFlow or not for pre-processing these images.

232
00:16:47,320 --> 00:16:52,920
And we believe this time investment was largely worth it because we have many other models

233
00:16:52,920 --> 00:16:55,240
leveraging these things now.

234
00:16:55,240 --> 00:17:02,160
We did enable our data scientists to experiment using a variety of models and they would

235
00:17:02,160 --> 00:17:07,440
always get the exact same data set after pre-processing.

236
00:17:07,440 --> 00:17:13,960
So we actually ended up writing a lot of these things in C++ largely because we wanted

237
00:17:13,960 --> 00:17:20,400
to use, this is getting a little into the weeds, but we wanted to use CPUs for the pre-processing

238
00:17:20,400 --> 00:17:27,320
because the models then can use all the remaining GPUs, and this provided a pretty significant

239
00:17:27,320 --> 00:17:34,360
performance boost, and it also meant that anything else, you know, like a tree model,

240
00:17:34,360 --> 00:17:40,880
leverage it, so that was kind of the rationale for investing the time there and digging

241
00:17:40,880 --> 00:17:41,880
in.

242
00:17:41,880 --> 00:17:48,560
And so far as Keras' concern, the investment there was really making multi-GPU inference

243
00:17:48,560 --> 00:17:52,520
very easy, our users don't have to think about these things.

244
00:17:52,520 --> 00:17:57,640
Essentially, whatever machine you're running on, we figure out what's on there, configure

245
00:17:57,640 --> 00:18:01,840
your model to run it in optimized fashion.

246
00:18:01,840 --> 00:18:07,920
And that's one thing I think framework authors are starting to really focus on is, you

247
00:18:07,920 --> 00:18:12,720
don't need to think so much about the level of mechanics of how you're running it.

248
00:18:12,720 --> 00:18:20,840
And I'm trying to think of like if you are, if you're kind of starting out here and

249
00:18:20,840 --> 00:18:27,200
you're trying to, you know, say you've got a team that you are supporting, and you've

250
00:18:27,200 --> 00:18:34,200
got folks that want to work in different frameworks, like where do you start, or where would

251
00:18:34,200 --> 00:18:39,320
you start if you were starting all over again, and like providing them some tooling that

252
00:18:39,320 --> 00:18:43,720
would allow them to be most efficient, where do you think like the most value is in the

253
00:18:43,720 --> 00:18:47,200
kind of things that you've been working on?

254
00:18:47,200 --> 00:18:53,400
You know, I think about this a lot, but I think we've made more or less the right calls.

255
00:18:53,400 --> 00:18:58,240
There are two areas that we really identified where people waste a lot of time.

256
00:18:58,240 --> 00:19:05,320
One of those is simply generating your features to train on and to use for inference.

257
00:19:05,320 --> 00:19:11,120
This is a huge time-waster because oftentimes it means a lot of really, really hacky querying

258
00:19:11,120 --> 00:19:13,320
of a database.

259
00:19:13,320 --> 00:19:17,400
You have no guarantees of point-and-time correctness or anything.

260
00:19:17,400 --> 00:19:21,160
If you do want to guarantee that ends up being a lot of work.

261
00:19:21,160 --> 00:19:26,400
And typically when you're building a model, you're going to iterate through a lot of different

262
00:19:26,400 --> 00:19:30,920
data sets, a lot of different variations of features.

263
00:19:30,920 --> 00:19:36,120
So that was one really, really huge area where people were wasting time.

264
00:19:36,120 --> 00:19:41,680
And this has basically been productized at Airbnb in Zipline.

265
00:19:41,680 --> 00:19:43,920
Yeah, that's right.

266
00:19:43,920 --> 00:19:50,280
We like to call it the engineers working on it called a time machine for your data warehouse.

267
00:19:50,280 --> 00:19:51,280
Right.

268
00:19:51,280 --> 00:19:58,160
So actually, let's remember the second one, and let's just dig into this because I think

269
00:19:58,160 --> 00:20:05,560
as I've kind of studied what you've done with Zipline, I mostly get it.

270
00:20:05,560 --> 00:20:10,440
But like the point-and-time correctness, I don't feel like I fully, fully get exactly

271
00:20:10,440 --> 00:20:14,080
the situations where you need that.

272
00:20:14,080 --> 00:20:18,920
And so maybe walk through kind of where the various places that comes up and why it's

273
00:20:18,920 --> 00:20:22,160
important and how you get there.

274
00:20:22,160 --> 00:20:23,160
Yeah.

275
00:20:23,160 --> 00:20:31,040
There's a lot of models that, for instance, will use aggregations, or some metric that's

276
00:20:31,040 --> 00:20:37,360
calculated every minute, or every hour, or even every day.

277
00:20:37,360 --> 00:20:42,640
But the problem is how a data warehouse works is often at midnight, you start feeding

278
00:20:42,640 --> 00:20:44,440
in data.

279
00:20:44,440 --> 00:20:47,840
And at the other midnight, you basically cut it off.

280
00:20:47,840 --> 00:20:53,680
And there may have been updates in that window that you miss.

281
00:20:53,680 --> 00:20:57,920
There may be things that have an afterwards that actually belong to that day.

282
00:20:57,920 --> 00:21:01,440
There are a lot of things that can go wrong, and technically your data is not perfectly

283
00:21:01,440 --> 00:21:02,440
accurate.

284
00:21:02,440 --> 00:21:03,440
Right.

285
00:21:03,440 --> 00:21:10,320
Zipline actually worries about the mutations that happen to a database, and it will actually

286
00:21:10,320 --> 00:21:16,120
look at, for a given window of time, for a given aggregation, how do I do this correctly?

287
00:21:16,120 --> 00:21:22,000
So that the numbers I give you for your model are reproducible, and they're perfectly

288
00:21:22,000 --> 00:21:23,320
accurate.

289
00:21:23,320 --> 00:21:25,840
So this reproducibility is really, really key.

290
00:21:25,840 --> 00:21:31,800
I think that's for somebody building a model, one of the more interesting aspects.

291
00:21:31,800 --> 00:21:35,960
If I'm going to get the same data later on in time, or earlier on in time, I know I'm

292
00:21:35,960 --> 00:21:38,760
always going to get the same thing.

293
00:21:38,760 --> 00:21:42,400
And that really just gives you a lot of ease of mind.

294
00:21:42,400 --> 00:21:48,240
So what's the specific example of a model that needs this point in time correctness?

295
00:21:48,240 --> 00:21:53,040
Yeah, so I can get to you in the specifics, but for many of the fraud models, you really

296
00:21:53,040 --> 00:21:58,440
do care about actions that you use your friends that may have taken during a certain window

297
00:21:58,440 --> 00:21:59,440
of time.

298
00:21:59,440 --> 00:22:02,000
Those windows can get very, very granular.

299
00:22:02,000 --> 00:22:09,960
So if you're off, or if you're missing data, or even worse, if pulling the data to consecutive

300
00:22:09,960 --> 00:22:12,360
times you get different results, that's really

301
00:22:12,360 --> 00:22:16,760
going to throw off your model and the consequences there can be pretty disastrous.

302
00:22:16,760 --> 00:22:20,440
So you really care about the precision.

303
00:22:20,440 --> 00:22:24,560
That's one case where you might have very, very granular time windows, and it's very likely

304
00:22:24,560 --> 00:22:28,360
that you'll miss data just because of how data warehousing works.

305
00:22:28,360 --> 00:22:29,360
Right.

306
00:22:29,360 --> 00:22:34,280
So the data warehouse might have these daily aggregates or something, and you need to say,

307
00:22:34,280 --> 00:22:40,120
tell me the number of times a user has tried to use this credit card in the past five minutes

308
00:22:40,120 --> 00:22:42,320
from point X in time.

309
00:22:42,320 --> 00:22:43,320
Yeah.

310
00:22:43,320 --> 00:22:44,320
I kind of think.

311
00:22:44,320 --> 00:22:45,320
Yeah, but it's aggregated daily.

312
00:22:45,320 --> 00:22:48,640
So if you try to do it yourself, if you have daily aggregates, you're never going to be

313
00:22:48,640 --> 00:22:49,640
quite right.

314
00:22:49,640 --> 00:22:50,640
Right.

315
00:22:50,640 --> 00:22:51,640
Right.

316
00:22:51,640 --> 00:22:55,400
And the other really big feature is that ZIPLINE gives you this same data streaming for

317
00:22:55,400 --> 00:22:56,640
free.

318
00:22:56,640 --> 00:23:01,440
So once you've backfilled your model, and you want to actually use this in production,

319
00:23:01,440 --> 00:23:07,760
when you have real data flowing in, you can actually get that same data feed in the

320
00:23:07,760 --> 00:23:15,960
same exact format streaming, and it'll guarantee that that streaming data, more or less, will

321
00:23:15,960 --> 00:23:21,720
match exactly what you would get at the end of the day, just using ZIPLINE offline.

322
00:23:21,720 --> 00:23:26,520
So users don't really have to duplicate any of this work.

323
00:23:26,520 --> 00:23:31,440
For many models, they end up being hosted both online, so in production, using the website

324
00:23:31,440 --> 00:23:37,320
and offline for analysis, so it's a pretty nice thing to be able to use.

325
00:23:37,320 --> 00:23:44,400
And so the second kind of big category beyond the point in time correctness is what?

326
00:23:44,400 --> 00:23:48,400
Users write a lot of boilerplate, a lot of boilerplate code in general.

327
00:23:48,400 --> 00:23:53,760
Like I said, data is rarely exactly in a format you need, and there's oftentimes a lot of

328
00:23:53,760 --> 00:23:56,680
pre-processing that's done.

329
00:23:56,680 --> 00:23:59,320
For images, it's usually not too bad.

330
00:23:59,320 --> 00:24:04,000
It might be an impact on performance, but there's usually not too much code.

331
00:24:04,000 --> 00:24:09,400
If you're dealing with tags, you have to strip a lot of punctuation outs, and code

332
00:24:09,400 --> 00:24:12,040
that tags a certain way.

333
00:24:12,040 --> 00:24:16,800
Sometimes there's aggregation of features like you might do, if this feature exists and

334
00:24:16,800 --> 00:24:21,080
this one doesn't, then I want to admit both or something like that.

335
00:24:21,080 --> 00:24:24,960
There's a lot of business logic that people in code is a result.

336
00:24:24,960 --> 00:24:29,080
And historically, we just saw that this was copied and pasted.

337
00:24:29,080 --> 00:24:33,960
Basically, you might have 500 lines of copied and pasted code.

338
00:24:33,960 --> 00:24:40,520
With tweaks, so you can't just always copy, paste it, and expect it to work.

339
00:24:40,520 --> 00:24:44,400
This became a really big problem because people can't share this stuff.

340
00:24:44,400 --> 00:24:50,080
They can't compose it very easily for a new model, and it's very, very error-prone.

341
00:24:50,080 --> 00:24:57,880
So instead, we really focus on modularizing this, so we have a psychic learn-like interface

342
00:24:57,880 --> 00:25:04,160
where people can wrap these unique operations that they can define, or they can use our

343
00:25:04,160 --> 00:25:08,680
built-ins, and compose them to build the entire workflow of the model.

344
00:25:08,680 --> 00:25:12,880
And then if they want to share just one piece of that with their teammates, their teammates

345
00:25:12,880 --> 00:25:16,760
can use that chunk as the basis for another model.

346
00:25:16,760 --> 00:25:23,000
This reusability was a pretty big bet, but it's something that we found was a huge win

347
00:25:23,000 --> 00:25:25,200
for many of our users.

348
00:25:25,200 --> 00:25:28,800
And it enables us to do these nice things.

349
00:25:28,800 --> 00:25:33,840
At the end of my model, I'm using XGBoost, but I want to use now a convolutional neural

350
00:25:33,840 --> 00:25:34,840
network.

351
00:25:34,840 --> 00:25:38,920
And on one line of code, I can just swap it out and it works.

352
00:25:38,920 --> 00:25:42,920
And so where does this manifest itself in the system?

353
00:25:42,920 --> 00:25:46,920
Is this the feature store aspect of?

354
00:25:46,920 --> 00:25:48,920
This is a library that these are used.

355
00:25:48,920 --> 00:25:49,920
The big library?

356
00:25:49,920 --> 00:25:50,920
Yeah.

357
00:25:50,920 --> 00:25:51,920
Okay.

358
00:25:51,920 --> 00:25:56,440
If they're working in Python, they just import our library and can search just writing

359
00:25:56,440 --> 00:25:59,320
a model using the big head libraries.

360
00:25:59,320 --> 00:26:06,040
And if you want to use a TensorFlow model, we have Keras and T.O. estimators built in.

361
00:26:06,040 --> 00:26:10,760
And then they can just use their normal TensorFlow syntax to build out their model.

362
00:26:10,760 --> 00:26:14,520
But it now clearly plugs into the rest of their workflow.

363
00:26:14,520 --> 00:26:19,400
And so at the very end, they end up with a pipeline, basically just an object that contains

364
00:26:19,400 --> 00:26:22,960
all of their logic, and they can just save this thing.

365
00:26:22,960 --> 00:26:25,120
And that'll include everything they need to run it.

366
00:26:25,120 --> 00:26:27,680
And we run it for beta and production.

367
00:26:27,680 --> 00:26:28,680
So.

368
00:26:28,680 --> 00:26:29,680
Okay.

369
00:26:29,680 --> 00:26:30,680
Yeah.

370
00:26:30,680 --> 00:26:40,040
This is tie into, do you run it via a graph and the airflow and that as the orchestrator

371
00:26:40,040 --> 00:26:41,880
or how do you?

372
00:26:41,880 --> 00:26:45,760
It used to, no, it has its own graph scheduler internally.

373
00:26:45,760 --> 00:26:49,000
So this little run in a single machine.

374
00:26:49,000 --> 00:26:53,760
So it'll execute its components, you know, possibly a multiple threads.

375
00:26:53,760 --> 00:26:58,000
There's schedulers for different steps they might take.

376
00:26:58,000 --> 00:27:00,440
But it's meant to be real time.

377
00:27:00,440 --> 00:27:04,440
So I mean, the latency is sub-nilosecond in many cases for using this thing.

378
00:27:04,440 --> 00:27:05,440
Okay.

379
00:27:05,440 --> 00:27:10,040
Whereas airflow might be for huge, you know, back full jobs.

380
00:27:10,040 --> 00:27:14,360
So this entire graph, I mean, we'll run and online inference as well.

381
00:27:14,360 --> 00:27:18,480
So every time you submit a request, it's running the whole graph.

382
00:27:18,480 --> 00:27:24,160
And yeah, I mean, in the future, there's plans to add components where we can have steps

383
00:27:24,160 --> 00:27:29,480
run on different machines and use one about GPUs and things like that.

384
00:27:29,480 --> 00:27:32,880
But for now, we've found that even on a single machine, it's sufficient.

385
00:27:32,880 --> 00:27:33,880
Okay.

386
00:27:33,880 --> 00:27:34,880
Okay.

387
00:27:34,880 --> 00:27:39,240
So there's two pieces then summarizing or effectively kind of managing the data and figuring

388
00:27:39,240 --> 00:27:44,680
out how to deal with some of these trickier issues like point-and-time correctness and

389
00:27:44,680 --> 00:27:53,040
efficient backfills and things like that and kind of, I guess that summarizes like raising

390
00:27:53,040 --> 00:27:58,880
the level of abstraction maybe so that the user has to write less boilerplate and can

391
00:27:58,880 --> 00:28:04,680
compose pipelines from things that are already provided for them.

392
00:28:04,680 --> 00:28:05,680
Yeah.

393
00:28:05,680 --> 00:28:07,680
And also whatever they choose to write.

394
00:28:07,680 --> 00:28:08,680
Right.

395
00:28:08,680 --> 00:28:14,640
I think composability of their own code is something that we really care about as well.

396
00:28:14,640 --> 00:28:20,080
Especially if you're just hacking something out in a couple of hours, typically it's not

397
00:28:20,080 --> 00:28:23,240
going to be the nicest code, so at least encapsulating it, right?

398
00:28:23,240 --> 00:28:24,240
Yeah.

399
00:28:24,240 --> 00:28:27,040
And it's a pretty good way to guarantee you can use that in the future.

400
00:28:27,040 --> 00:28:28,040
Okay.

401
00:28:28,040 --> 00:28:29,040
So yeah.

402
00:28:29,040 --> 00:28:30,040
Okay, cool.

403
00:28:30,040 --> 00:28:35,440
So have you started looking at the TensorFlow 2.0 and like how that impacts the way, you know,

404
00:28:35,440 --> 00:28:40,840
some of the things you've already built and the way that your users will be using TensorFlow

405
00:28:40,840 --> 00:28:41,840
in the future?

406
00:28:41,840 --> 00:28:42,840
Yeah.

407
00:28:42,840 --> 00:28:43,840
I mean, it makes our lives a lot easier.

408
00:28:43,840 --> 00:28:44,840
I think so.

409
00:28:44,840 --> 00:28:45,840
I think so.

410
00:28:45,840 --> 00:28:49,840
One of the key things he did in T.Point now is deprecate a lot of really old APIs.

411
00:28:49,840 --> 00:28:53,680
I think that was, that was initially we ran into with, there was a lot of, there was a

412
00:28:53,680 --> 00:29:00,400
lot of incompatibility within the one point X releases, you know, certain APIs that only

413
00:29:00,400 --> 00:29:04,920
work with certain features and that proved to be a really big source of frustration.

414
00:29:04,920 --> 00:29:10,880
I think by removing those, those old APIs and really focusing on kind of starting more

415
00:29:10,880 --> 00:29:11,880
of a clean slate.

416
00:29:11,880 --> 00:29:19,080
It means that developers using TensorFlow will usually, I would expect an easier time.

417
00:29:19,080 --> 00:29:21,680
And I think there are some other neat features in there as well.

418
00:29:21,680 --> 00:29:28,040
I think the fact they've moved to mostly Eager mode by default is something that people

419
00:29:28,040 --> 00:29:30,080
would like to be able to use.

420
00:29:30,080 --> 00:29:36,920
If I'm most of your users prefer Eager mode to kind of the estimator of layers, API?

421
00:29:36,920 --> 00:29:41,360
The users that care about Eager mode, it's historically been using PyTorch.

422
00:29:41,360 --> 00:29:46,400
Just because it, by design, it kind of works in that way.

423
00:29:46,400 --> 00:29:51,880
But I think they would consider TensorFlow as an option now that Eager mode kind of works

424
00:29:51,880 --> 00:29:57,400
in a more easy fashion and they don't have these other sort of static graph complexities

425
00:29:57,400 --> 00:29:59,480
that deal with the most part.

426
00:29:59,480 --> 00:30:03,680
So yeah, I think it's going to emerge as a pretty viable option for, for many users just

427
00:30:03,680 --> 00:30:09,000
looking to prototype, you know, even in Jupyter, it's really nice to just be able to run

428
00:30:09,000 --> 00:30:11,400
a couple of styles into a result.

429
00:30:11,400 --> 00:30:16,520
So yeah, I think that's, it's really open up those features to people that I think would

430
00:30:16,520 --> 00:30:22,000
have not considered them in the past because of just the difficulty of using them.

431
00:30:22,000 --> 00:30:28,640
They've done a lot around 2.0 and recently with integration with Colab and Jupyter Notebooks.

432
00:30:28,640 --> 00:30:34,480
Your team has built quite a bit of infrastructure around Notebooks, like how do you see those

433
00:30:34,480 --> 00:30:35,480
playing together?

434
00:30:35,480 --> 00:30:42,200
Yeah, I mean, there are very much two different approaches, I think.

435
00:30:42,200 --> 00:30:46,080
I can't blame them, but Google does want to integrate with their own offerings.

436
00:30:46,080 --> 00:30:51,560
And we generally try to be really, really agnostic about what you're using.

437
00:30:51,560 --> 00:30:57,200
So while integrating with, you know, their own, they have this really nice Jupyter, a

438
00:30:57,200 --> 00:31:01,760
collaborative editor, using Google Cloud, yeah.

439
00:31:01,760 --> 00:31:04,880
But I think, you know, if users can leverage that, I mean, it's a great product.

440
00:31:04,880 --> 00:31:09,760
I've tried it out myself and it's, I'm sure, I'm sure it's a great option.

441
00:31:09,760 --> 00:31:18,600
Do any of the new capabilities allow you to do more better things in your own notebook

442
00:31:18,600 --> 00:31:19,600
implementation?

443
00:31:19,600 --> 00:31:22,720
Or are we already doing everything you needed to do anyway?

444
00:31:22,720 --> 00:31:25,360
We definitely weren't doing everything we needed.

445
00:31:25,360 --> 00:31:29,600
We belong to features that people like.

446
00:31:29,600 --> 00:31:34,840
One of the things I saw in the demo were, so the ability to better show these types

447
00:31:34,840 --> 00:31:39,080
of photographs and kind of expose parts of your model inside a Jupyter notebook.

448
00:31:39,080 --> 00:31:44,280
I think that would be useful for us, but I would need to take a closer look at what that

449
00:31:44,280 --> 00:31:45,280
offering entails.

450
00:31:45,280 --> 00:31:50,320
So, yeah, I mean, anything they can do to better surface visualizations, I think that's

451
00:31:50,320 --> 00:31:56,880
one area where generally users are very happy if you give them better ways to sort of see

452
00:31:56,880 --> 00:31:59,160
what's happening under the hood.

453
00:31:59,160 --> 00:32:04,880
One sort of board was one really, I think it was basically the only mainstream option

454
00:32:04,880 --> 00:32:08,880
that you could kind of do for visualizing models.

455
00:32:08,880 --> 00:32:13,000
And there just wasn't a very good way to integrate that nicely with Jupyter.

456
00:32:13,000 --> 00:32:19,280
There was a lot of caveat, so I think it'll be really exciting if they start making that

457
00:32:19,280 --> 00:32:23,880
first class and see what they do.

458
00:32:23,880 --> 00:32:29,600
And you use within Big Head TensorBoard pretty extensively, right?

459
00:32:29,600 --> 00:32:30,600
I wouldn't see.

460
00:32:30,600 --> 00:32:31,600
Remembering that extensively.

461
00:32:31,600 --> 00:32:35,320
But you have your own visualization stuff.

462
00:32:35,320 --> 00:32:44,720
We support TensorBoard, not super well right now, but it's on a roadmap.

463
00:32:44,720 --> 00:32:48,320
There's a lot of internal visualizations that we have.

464
00:32:48,320 --> 00:32:53,720
We've run visualization library for data and visualizing these pipelines.

465
00:32:53,720 --> 00:32:57,440
And we generally like to have a cohesive look whenever possible.

466
00:32:57,440 --> 00:33:00,960
So we'll probably continue to use that.

467
00:33:00,960 --> 00:33:06,200
But if a model exposes its own visualizations, we want to just leverage that out of the

468
00:33:06,200 --> 00:33:07,200
box.

469
00:33:07,200 --> 00:33:09,640
We don't want to reinvent the wheel or anything.

470
00:33:09,640 --> 00:33:16,800
So as an example, one thing we did, XGBoost has a feature importance, kind of really nicely

471
00:33:16,800 --> 00:33:17,800
built in.

472
00:33:17,800 --> 00:33:21,680
We can just expose that plot and people are loved using that.

473
00:33:21,680 --> 00:33:28,440
So yeah, I mean, using TensorBoard or whatever the framework builds in is something that we

474
00:33:28,440 --> 00:33:30,120
going forward want to use.

475
00:33:30,120 --> 00:33:36,800
And do you use any of the other kind of components of the TensorFlow, the evolving TensorFlow

476
00:33:36,800 --> 00:33:44,240
family, like the probabilistic programming or TFX or any of those other components?

477
00:33:44,240 --> 00:33:49,800
Yeah, I mean, we would like to be able to use more of the components, but essentially

478
00:33:49,800 --> 00:33:56,040
how we operate is also largely based on what our users need the most right now.

479
00:33:56,040 --> 00:33:57,760
There are other focuses at the moment.

480
00:33:57,760 --> 00:34:03,480
But I think as we plan on going and open sourcing this, it's going to be easier for other

481
00:34:03,480 --> 00:34:07,440
people to add support for these different features that they need.

482
00:34:07,440 --> 00:34:09,880
Yeah, probabilistic programming.

483
00:34:09,880 --> 00:34:13,080
There are a couple of frameworks that are kind of looking into this.

484
00:34:13,080 --> 00:34:15,360
I think Uber released one as well.

485
00:34:15,360 --> 00:34:16,360
Yeah, Pyro.

486
00:34:16,360 --> 00:34:22,440
That'd be really cool to add, but we just haven't had any tangible use case that immediately

487
00:34:22,440 --> 00:34:25,280
needed it right now.

488
00:34:25,280 --> 00:34:34,880
It sounds like your focus is on enabling kind of the core use cases and you haven't had

489
00:34:34,880 --> 00:34:38,680
much of a need yet to support some of these other ones.

490
00:34:38,680 --> 00:34:43,880
Yeah, I mean, that's the kind of thing that if we offered it, you would find use cases

491
00:34:43,880 --> 00:34:44,880
for it.

492
00:34:44,880 --> 00:34:51,440
But yeah, you have to start somewhere and I think giving users really, really good fundamental

493
00:34:51,440 --> 00:34:55,920
building blocks that work first or a good area where we can do that.

494
00:34:55,920 --> 00:35:00,280
I think estimators in TensorFlow are one area where they're really investing heavily

495
00:35:00,280 --> 00:35:06,440
in that and we'd like to have that also be really, really well supported as well.

496
00:35:06,440 --> 00:35:08,840
And is it not currently?

497
00:35:08,840 --> 00:35:13,560
We support some really basic use cases with estimators, so they have some pre-built ones

498
00:35:13,560 --> 00:35:16,440
and you can define your own custom estimator.

499
00:35:16,440 --> 00:35:20,600
And maybe take a second to explain what those are and how they're used.

500
00:35:20,600 --> 00:35:27,200
Yeah, so estimators are basically TensorFlow's, I think, approached as something that's

501
00:35:27,200 --> 00:35:28,920
like it learned like.

502
00:35:28,920 --> 00:35:36,240
We have a very, very simple API, essentially trained and run inference and the model is

503
00:35:36,240 --> 00:35:37,240
very encapsulated.

504
00:35:37,240 --> 00:35:38,240
Right?

505
00:35:38,240 --> 00:35:41,240
You don't have to worry about sessions and graphs and all these things.

506
00:35:41,240 --> 00:35:46,680
And basically define this structure of your model and what its input data looks like.

507
00:35:46,680 --> 00:35:51,560
And then you have something you can just play with.

508
00:35:51,560 --> 00:35:56,760
They provide a lot of built-ins, so like deep neural networks, you can kind of choose

509
00:35:56,760 --> 00:36:00,360
what those look like, linear models.

510
00:36:00,360 --> 00:36:04,600
I think they have a couple of other ones as well, but I'm not mentioning.

511
00:36:04,600 --> 00:36:05,600
And you can also write your own.

512
00:36:05,600 --> 00:36:11,120
So you can also just take TensorFlow code and kind of wrap it in one of these estimators.

513
00:36:11,120 --> 00:36:14,840
This fits really nicely with what we're doing in Big Head, where we're trying to encapsulate

514
00:36:14,840 --> 00:36:17,320
these models.

515
00:36:17,320 --> 00:36:25,200
So being able to allow users to use these more easily and not have to worry about anything

516
00:36:25,200 --> 00:36:30,600
other than the actual design of the model, that would be beneficial for them.

517
00:36:30,600 --> 00:36:36,480
So like I mentioned before, worrying about GPUs or where things are run or that's one

518
00:36:36,480 --> 00:36:39,800
question that we don't want our users thinking about.

519
00:36:39,800 --> 00:36:45,080
That's because we can almost always make more informed decisions at runtime about what

520
00:36:45,080 --> 00:36:50,760
that should look like, so they can get the best performance out of their models.

521
00:36:50,760 --> 00:36:55,480
So yeah, that's something I'm pretty excited to see what they do with.

522
00:36:55,480 --> 00:37:01,800
I think it was already in a pretty nice state, not too long ago, but now that they clean

523
00:37:01,800 --> 00:37:09,240
up TensorFlow more and 2.0, it's probably going to be the defector choice for a lot of people.

524
00:37:09,240 --> 00:37:18,720
We kind of taking a step back and kind of thinking about frameworks and the framework space

525
00:37:18,720 --> 00:37:25,200
from the perspective of someone who has to provide these to a set of users.

526
00:37:25,200 --> 00:37:34,640
Do you have like a wish list or predictions or where you see it all going?

527
00:37:34,640 --> 00:37:42,280
How do you see this evolving or what would you want to see to better support your users?

528
00:37:42,280 --> 00:37:47,640
Yeah, I think focus on developers and focus on the enterprise space is going to be one

529
00:37:47,640 --> 00:37:51,960
area that's pretty interesting.

530
00:37:51,960 --> 00:37:58,280
So I mean Google has been investing a lot in TFX and it is TensorFlow's specific, so

531
00:37:58,280 --> 00:38:00,440
I mean you have to use TensorFlow.

532
00:38:00,440 --> 00:38:06,560
But it's also largely their attempt at handling the entire end end, data management, model

533
00:38:06,560 --> 00:38:09,280
management, all of this.

534
00:38:09,280 --> 00:38:14,280
We haven't seen that too much from other framework providers, but one thing we have seen outside

535
00:38:14,280 --> 00:38:18,160
of TensorFlow is a focus on modularity alone.

536
00:38:18,160 --> 00:38:23,680
So another framework we've been exploring a lot is MxNet and they've had a lot of focus

537
00:38:23,680 --> 00:38:26,120
as well on this.

538
00:38:26,120 --> 00:38:31,280
So I really see, I mean you're probably going to have only a few big frameworks left

539
00:38:31,280 --> 00:38:34,240
after a while.

540
00:38:34,240 --> 00:38:39,520
For general use cases, it's just very, very hard to develop a new framework.

541
00:38:39,520 --> 00:38:45,320
So yeah, I mean I see the main player still has probably TensorFlow and PyTorch and MxNet

542
00:38:45,320 --> 00:38:48,680
with specific frameworks kind of lacking behind.

543
00:38:48,680 --> 00:38:53,920
I mean you have things like SpaceC and you know you'll have things like Pyro for niche

544
00:38:53,920 --> 00:39:00,800
use cases or for NLP specific use cases, but yeah, I mean a consolidation is very likely

545
00:39:00,800 --> 00:39:01,800
to happen.

546
00:39:01,800 --> 00:39:08,480
It's just not sustainable to have 10 of these and you know, developers putting their time

547
00:39:08,480 --> 00:39:14,600
between them, but it's, I think it's healthy to have competition, you know, an example

548
00:39:14,600 --> 00:39:20,600
of that was PyTorch was kind of the first to have the ability to just run these computations

549
00:39:20,600 --> 00:39:27,000
on a fly and TensorFlow then came out with eGermode and MxNet came out with its own like

550
00:39:27,000 --> 00:39:28,000
comparative APIs.

551
00:39:28,000 --> 00:39:34,320
I think that's a really good back and forth that will really help everyone.

552
00:39:34,320 --> 00:39:38,520
I think there's been a lot of focus on the research space too historically.

553
00:39:38,520 --> 00:39:46,160
So making it really easy to do research is nice, but for most people they generally have

554
00:39:46,160 --> 00:39:50,520
a rough idea of where they're going to start and they want really nice higher level APIs

555
00:39:50,520 --> 00:39:57,960
and then when they have those APIs, the other issue right now is usually when you use one

556
00:39:57,960 --> 00:40:03,760
of those things there's such a performance that moving it to production is going to require

557
00:40:03,760 --> 00:40:08,280
more engineers and that's something we at Airbnb don't really have.

558
00:40:08,280 --> 00:40:13,800
I mean we try to basically only have people writing models and maybe a team working on

559
00:40:13,800 --> 00:40:20,320
the infrastructure but nobody actually dedicated purely to converting these models into a performance

560
00:40:20,320 --> 00:40:21,320
version, right?

561
00:40:21,320 --> 00:40:22,320
Okay.

562
00:40:22,320 --> 00:40:24,640
That's something that we think computers can do better.

563
00:40:24,640 --> 00:40:28,640
Well, Afraidio, thanks so much for taking the time to chat with me.

564
00:40:28,640 --> 00:40:30,280
Thank you very much.

565
00:40:30,280 --> 00:40:36,040
Alright everyone that's our show for today.

566
00:40:36,040 --> 00:40:40,720
For more information on Afraidio or any of the topics we covered in this show visit twimmel

567
00:40:40,720 --> 00:40:44,960
AI dot com slash talk slash 244.

568
00:40:44,960 --> 00:40:53,360
As always, thanks so much for listening and catch you next time.

