All right, everyone. I am on the line with Errol Coolmaster. Errol is head of AI foundation
at H&M Group. Errol, welcome to the Twomel AI podcast.
Thank you so much. It's a pleasure to be here.
Hey, it's great to have you on the show and I'm looking forward to digging into our conversation.
I'd love to have you start us off by sharing a bit about your journey.
How did you come to work in AI?
Yeah, thank you so much. My journey really started about 15 years ago.
I was still in school studying finance.
I got the opportunity to join one of the largest bank here in the Nordic region of Europe.
So I joined Nordia Bank, basically as a fraud analyst in the beginning.
Keep in mind, this was 15 years ago.
We weren't into deep tech back then. Basically, I was set at the position where they said,
can you help us fix the fraud? So immediately, I was exposed to real-time problems,
trying to in real-time work with the credit card companies to stop it.
In the past, it was just rule-based, given that I had a finance background studying math
at the time. I started doing statistical analysis, improving the results significantly,
back then, my love for data, machine learning, and algorithms were born.
So basically, I just took it from there. I then, of course, graduated.
Went on to be a part of building up the fraud unit in the bank.
Did that for a few years when I went on to actually anti-monolondering?
So I was a part of building up the first detection systems in the bank as well,
which was really interesting. I did that for about three years.
And algorithms, screening, I couldn't get enough of it.
I had my first child, and my second child during that period,
when I realized I need some more exposure to data problems.
I felt like this field is moving so fast. So I got to opportunity within the bank to have
open roles, so to say. So I moved around a little bit in the bank every day.
Was it the children that gave you the insight that you needed more exposure to data?
The data problems?
Definitely. Here is Sweden. We are very, we are very luxurious.
We get a lot of free time. We get children. We get about a year off.
I didn't take all the time, but it makes you think a little bit about what should I focus on.
Okay.
I realized I want to go broader, for all of these folks in and of me.
So at that time, I had the opportunity to start playing around with Hadoop installations, Vanilla.
Of course, this was our early days still. I had the opportunity to integrate the SaaS platform
with Hadoop, distributed computing, early days of Spark, and I just was hooked.
So I deep-dived into the technical aspect of data science.
And I just couldn't get enough. I really love this field, and I have a strong passion for it.
After a year or so, in that relatively free role, I got a phone call.
And it was from a headhunter working at London, and they were recruiting for lead data scientists
role for Vodafone Group. So basically, the second largest cell phone provider in the world.
And I said, this is a great opportunity. So I decided to take my kids and wife and move to the UK,
where I worked out to their global office, setting up their AI department as one of their
lead data scientists. So, still, early days for me and early days for Vodafone. So I had the
opportunity to work with some very talented individuals, and being a part of building out that team
that I think is now over 500 people. We still, I would say, had set up infrastructure that I had
some problems with today's perspective on the premium, large clusters, distributed computing,
and very long lead times to actually rolling out models. So I got a little bit of frustrated.
I like living in the fast lane, especially when it comes to extracting value.
So I spent about a year and a half in London when I had the opportunity to actually start
my own business. So I spent a bit of a lot of time in airports moving back and forth between
Sweden and London, servicing clients, primarily banks and startups, with AI implementations.
But after less than a year, my partner decided to pull out, and my good friends that think
big analytics that was purchased by Territate, gave me a call and asked why not to join us,
not to partners pull them. So I decided to do so. I came in as a director of data science for
the Nordics Eastern Europe, Russia, managing a team of 25 people approximately.
And what I did was traveling around in this region, advising the large corporations on how to
implement AI and data. I did so for about a year. Then I get a really interesting phone call
from one of the largest companies, not just here in the Nordics, but also in the world.
H&M Group said, Hey, we are investing heavily in AI. We want to build out this internal capability.
Can you help us do it? I at that point I said, okay, so do you have buying from senior stakeholders?
Do you have budget? And do you have data? And I got a positive answer to all three of those.
And I said, okay, let's do it. And that was two and a half years ago. And I haven't stopped
running since. That's fantastic. So for those who may not be familiar with H&M Group,
tell us a little bit about the company. So the H&M Group, of course,
come from the original H&M brand that would started in 1947 here in Sweden in a small town called
Vesteros. It was a very successful business model in the beginning. The founder,
Aling Passion said he wanted to democratize fashion. Fashion in the pre-war era was relatively
expensive. So he basically cut prices and made it available for everyone. In the off-the-war
Europe, everybody wanted to kind of get the economy started. So this was a spot on.
The model was so successful. So in around year 2000, they had 2000 stores when we opened in
Manhattan. And now today, 20 years later, we have 5,000 stores globally. H&M, of course,
the group consists of more than the H&M brand, which is very well known for most people.
We have other brands like weekday, monkey, another story, H&M home, etc. So we as a group,
we serve all of them. And most additionally, these initiatives are still being centrally managed
out of Sweden. Okay, nice. And what you mentioned, you got that call that said that they wanted to
invest heavily in AI. What stage was H&M at when they gave that call?
So I mean, so the listeners listening to this, you probably met the CEOs or the senior managers
who says, hey, AI, I've heard about it at a conference somewhere. What is it? Shouldn't we be
investing in it? H&M came to that conclusion in around 2016. That meant that the CEO at the
time called you on passion, the grandson of the founder, bent to business development, and he gave
that line pretty much. What are we doing within AI? So business development went out into the
business and said, we got some bad news. We're not doing much. We do have a CRM department.
We do have some central initiatives here and there, but hey, we're H&M. We're not doing it a
scale. We're not taking advantage of our economy of scale economy we could be doing here.
So business development, they engage in external consultancy firm for the first proof of
concepts in around 2017, with the aim to go to production if they were good. They were good,
their first indicative results. So they went to production. They started to see payback
immediately. The return on investment was less than a year in most of the business cases they
started. It was so successful. So in 2018, when I got that call, they decided to establish AI
as its own function. And a function in H&M is kind of a big deal. One private act is sustainability
department. If you know H&M a little bit sustainability is something very important for us,
then you can imagine how important AI is now. So maybe tell us a little bit about various
use cases for AI at H&M. How do you organize them? Do you think about them by business unit or
by functional area? I'm imagining that or based on our previous conversation that there's
quite broad use at this point. Yeah. So H&M covers the entire value chain of a retail company,
except actually manufacturing the clothes. So for us, AI needs to cover the entire value chain.
We have an end-to-end responsibility here. So when we started assessment of what use cases to
actually do, we looked on value and feasibility. So we wanted to pick the low hanging fruits first,
which was a very good strategy. So we looked on, okay, in the beginning of the cycle,
when we're starting to, before we start manufacturing the clothes, when we actually do the design,
can we detect the trends? They're going to be interesting relevant in eight months when
we're going to get the clothes back. So we started one of the use cases was fashion forecasting,
very interesting looking on social media, whether the trends. So our designers could go in
with their hypotheses and actually validate them in this tool. How the parts of this was,
of course, demand forecasting. So when we do have some of these things, how much should we buy?
That's also very important for us. In the past, when we had hyper growth in the physical retail,
we could just say we're going to grow 10% next year. If we didn't reach that target,
we could open a new store and put all the garments there and just sell them out anyway.
Now we needed to be more granular in our predictions. So demand forecasting for the company was,
of course, something we focused quite a lot on. Then on the other side, we needed to a way to
negotiate with different production plants. So we applied AI into different pricing algorithms,
in negotiations, and also vendor selection algorithms for the production part. On top of that,
when everything is produced, we need to allocate them to warehouses. So allocation became a
relatively good use case for us as well. What garments should go ware and to what stores should
again. It's not a one-size-fits-all. It needs to be individual depending on the market as well.
And then, of course, in the end of the sales cycle, we need to handle pricing. So markdowns for
the online store was one of our more successful use cases. And then, of course, covering everything
is the personalization, recommendation engines, and tailored offering for our customers.
So those were our main relatively high-level use cases that we focused on most recently.
So you joined H&M at a time where the company had completed several
proof-of-concept efforts, had demonstrated some value. I believe didn't have much of a team
relied primarily on external resources at this point. What were your first steps in
taking on the challenge to scale this more broadly within the organization?
It's a very good question. And I think my first reaction when joining H&M is what have I given
myself into? It was relatively what it's nested with some really talented consultants,
but they were focused so much on value. And I came in with the perspective of course with value,
but how do I scale these type of efforts? I started thinking how do we actually recruit the
amount of people that we need to recruit to replace these consultants? How do we streamline the
technology and architecture? So one of the first things we started doing was communicating a lot
more around the topics we were working on to make the market aware of that we were doing this
type of investments to be able to attract talent into some of the very challenging
problems we were working on. So we started crafting a story line that we wanted to introduce
to the general market as well. You should never underestimate communication to create a bit of bus
in the Nordics because that's primarily where we started out, but also then more globally as well.
Then of course popping the hood of all the use cases, looking at the technical architecture,
trying to understand what the external consultant's firm had actually built. So some of the first
thing I did was going out and hiring more technical people that could understand the engineering aspect
is one thing to build a model, but it's another thing to push it to production and keep it there,
which requires a lot of engineering effort. So my first hire was some really senior machine learning
engineers and we created something we call a reference architecture because we wanted to maintain
autonomy of the actual data science teams that were building the algorithms and the products
to be able to run as fast as possible but to be able to get as much central support as possible.
It didn't make sense for us to make 10 different technical choices when they were trying to solve
the same problem. So we started streamlining some of these choices to be able to speed up deliveries,
started to harmonize the technical choices, and then of course started taking the use case over
and rolling them out at a faster pace. At that point we're there existing data scientists within
different business units and you were supporting them centrally or were you also staffing up
across the business in the various business units or departments?
It's a very relevant question. I've seen different companies do it differently. The
DGNM approach was to do it centrally from the start, basically to incubate the capability
rather to spread it out, what we realized and what I realized from my experience as well is
if you put a data science in a team as the only technical and algorithmic person,
they're going to have a problem delivering not because they are a pad at delivering
but they're not in the right context. So you tend to see a relatively high turnover of data
scientists being set as an isolated island in a business unit. So we decided to initially
start with a central incubation. When you kind of pull back the covers on these proof of
concepts, you were looking for the, you're trying to understand them technically,
but what did you also learn from a value perspective? Like often there's misalignment between
the needs of a proof of concept meaning to show something flashy or fancy and the ultimate
needs of the business and how you align in a sustainable long-term way. Did that kind of issue
come up for you? I think that some of the really good things with the external firm that we
worked, but they were really good at understanding the business requirements. They worked very closely
and I think a strategy from H&M side given that they have had over the technical development
to an external firm was to actually have an integrated person from the business in each use case.
So the alignment around the value and the delivery of these things, I would say we're very well
aligned. The problems were more on the technical side because showing something shiny
doesn't always scale. My learnings and experiences start with the simplest thing that's better than
random that gives some sort of initial uplift, but you can scale it better. You don't go with the
most complex technology or algorithm from start because you don't know how that will scale.
So it's better to create a stable and scalable infrastructure rather than focusing on the modeling
from day one if you're serious about these things. And so what were some of the things that you
built into that infrastructure as you started to pull these projects together? So some of the things
that we did is we looked at the process and what I realized is if you understand what you're building,
then you can map the processes and then you can actually automate most of the things.
vis-à-vis the MLOPS type of setup. So what we did was basically to map out the development
process and look on where are some of the major pain points that we have today. What I started
realizing is that we had a lot of stale clusters on the cloud that weren't used 70% of the time.
So we started moving over to a more scalable infrastructure. So we moved most of our development
away from Jupyter notebooks into Databricks, given that then we could have on-demand compute,
and we could have more virtual control over the infrastructure. So we standardized on how we
actually did model development. With those things we also started looking on the the orchestration.
So we standardized it a lot around the work for orchestration, went to going more into to airflow,
building new pipelines into that things, and also looking on how do we production lies and how do
we follow up on these things. So for each of these process step we took a few design decisions
and created supporting infrastructure for that. And then did you go to the actual
maybe taking a step back, were you building infrastructure and kind of promoting these proof of
concepts into a more production capability at the same time, or did one come before the other?
Did you pause on the PSE, build infrastructure, and then return to the PSEs to make them scale? How did
you approach all of that? Yeah, I mean my general comment is super hard. But it's only concept of
technical depth. So these PSEs, some of them were in production already delivering value and
if you are using production delivering value, it's hard to pull back because the business is
going to go crazy. But what we started doing was we slowly worked with the different products,
identifying their major pain points, and gave them boilerplate code to start migrating towards.
So some of the engineers they went in, they did a small proof of concept for instance moving
into air flowing Kubernetes in some of the use cases and showed on the benefits and then we had
them paid down the technical depth. In our OCRs, I even formulated goals for the product area to
actually have no technical depth in their use cases or product in this case. So we mandated them
that not all your efforts would go into new feature development and you roll out to new markets,
it should be paying down the depth that we still owe on the proof of concepts.
Can you walk us through a particular use case kind of before you had this infrastructure and
you know once you establish the infrastructure and how that use case evolved along with
the platform technologies that you put in the place?
Yeah. I can tell you about the first use case we started as the H&M team,
solely based on the reference architecture and how we actually developed that.
So we started a sort of quantification for online, which is demand for causing for the online
store, from scratch. So we wanted a green field with all the experience of all the proof of
concepts you can consider in mind based on the reference architecture. Basically what we started
of course with was creating this blueprint, the reference architecture, and then started building
the different building blocks out of that. So from scratch, we of course made a modeling on
Databricks. We created the the Airflow orchestration, the integration, the scoring, the
hyper parameter tuning, the interference part and all of that. It took M2M to build out this
infrastructure approximately 12 months. And I'm not going to say this was super false but given
the complexity and dependencies that we had and the team was relatively new to building this,
I think it was relatively good. And were you building the infrastructure for this particular
product or were you building the infrastructure with many products in mind and this was kind of
a tracer bullet? That's exactly the question I wanted to get to because we had all the
use cases in mind. So this time we didn't just build it for one, we built it for many.
And the learning that we made from that is the second use case that we started which was
balancing a warehouse that took six months. Basically the same prerequisites, of course,
not the same data and the same business output but the process and everything associated with it
is very similar. So we could reuse much of the infrastructure because we built it in such a way.
So all of a sudden we had saved 50% of development time in the new use case.
And our new strategic approach is to even reduce that further with 50% and hopefully getting
it down to three months in the ones that we're starting right now. So we see enormous productivity
gain with designing an infrastructure that's reusable, raw data and building independent use
cases at this scale because it doesn't make sense. Yeah. You've talked about some of the trade-offs
between simplicity and complexity and modeling. I'm wondering if you can talk a little bit about
what the portfolio looks like from that perspective, what kinds of models are you using
and the way you're approaching it today now that you've built some of the infrastructure?
Yeah. So most of the things that we have taken to production the last year or so are all
relatively simple. Most of our models are like GBM. Like GBM is sort of a house model these days.
I mean it produces good results and its computational power isn't that expensive.
So we get the results that we need and it's also relatively easy to integrate into the use
cases and the infrastructure that we have. When we start it out and somebody consults the early
use cases and some of our early use cases, many of the people in the teams throw themselves
directly into the latest research, wanted to do neural networks, wanted to get just a CR.1 uplift.
But what we realize as well is this isn't a cagle competition. It is not about optimizing
that metrics and that then you're done. It's about carrying it over into production
into infrastructure as well. So these days we tend to start with something relatively simple
as a benchmark. It can be anything from from like GBM, XGBoost and some other regression models
for instance. And now we're slowly moving towards more advanced tools. So looking at
the graph neural networks, et cetera, in our more latest modeling because we then see an extra
need for it. So it is a relatively wide range. We have experts in all of these areas,
we have optimization experts as well in some of our use cases. So we try to keep it as simple as
possible but we're not 100% there yet. You talked about one of your biggest challenges being
scaling your organization from zero to many employees, data scientists. Do you run into the issue
of the folks that you're trying to attract wanting to play with cool, fancy, shiny toys,
and here you're working with XGBoost and light GBM. How do you address that?
Yeah, it's a it's a super hard. I think it also comes with seniority and I think it also comes
with establishing different types of teams. What we're saying is the most valuable thing for us
right now when we look on scale are the simpler models that they produce a good value and the
reason to maintain and and put into production. But we also have established research teams
which have as a purpose to look on how can we prove what we have today. How can we introduce more
more research into these product teams but they have another time perspective. So what we tend
to talk to our data scientists and machine learning engineers is that you have to consider
timing into this. We have to have a positive ROI in our investment today to actually get the
funding to be able to scale this but at the same time we are investing in the research
for the research needs to be heavily associated with what you're doing with as well.
So we are trying to have a long and a short term perspective. Can we can we attract everyone
that wants to work with the latest and the shiniest thing? No, of course not. Hopefully we can get
some of them and that will be enough. And hopefully one day we are advanced enough so that most
of the things we're working on are the latest. But we are a company with a lot of legacy and it's
going to take us a bit of time before we are there. You just touched on a topic that I hear fairly
frequently when talking to folks that are in enterprise leadership positions and that is the
importance of managing the project portfolio. You know needing to have some kind of swing for
the fences, big goals in mind to keep executives excited and keep the funding coming in but also
needing to have small wins and pick off low hanging fruit, that kind of thing. I'm wondering if
you see that as well and how you approach that portfolio management aspect of the role.
Yeah, I would like to put it a little bit more context as well. So we had a major reorganization
recently integration AI with IT department and business development into one large unit called
business tech. So these days we are enabling domain as well which means we we serve all of the
the business with enabling capabilities. So the majority of the use cases actually is prioritized
by other units than ourself these days and we support them with building them out. This put us
in a tricky situation trying to identify how should we accelerate AI now given that we are
enabling domain. So we sat down and we formulated a strategy which we called the Fountainhead which
is the encapsulation of all AI capabilities. So one of the cornerstones of that is going from vertical
capabilities which is basically use case by use case because we see that's not a scalable business
wallet into a horizontal scaling. Basically what that does it gives us another factor of
prioritizing. If we in the past prioritized our use cases in our portfolio by value and feasibility
we have now added the notion of reusability. And the notion of reusability basically is because
take the move box balancing of warehouses use case that we had that's in season demand for
costing. If we build it just for move box this is not an infrastructure question but if we build
that specifically demand for costing just for move box that means that we created once and we use
it once. But if we prioritize our use cases for all the use cases now that can do in season demand
for costing because we just created that capability and we prioritize them that means we reach value
much faster. So we scale smarter not harder by just doing more use cases. So we have flipped
the roadmap over in our portfolio of AI use cases work together with the domain and created that
type of roadmap to prioritize times the value which gives us a relatively good ROI quickly
within the first year is our estimate rather than a sequence with small value lumps along the
way that the next few years. So we have we have spent quite a lot of time recently
and are seeing that the first results already know.
Maybe to replay that a little bit to make sure that I'm catching what you're saying.
It sounds like you started out you had a bunch of proof of concepts and you built some
fairly low level infrastructure things like Databricks and Airflow and other things to enable you
to scale those proof of concepts and more recently what you're seeing is you're kind of pushing up
the level of abstraction and so you're saying okay instead of you know building out this
vertically integrated demand forecasting app for a particular use case. Hey well we need to do
forecasting across lots of different use cases potentially. Let's build another level of platform
kind of at the forecasting level or the application level. You know at the risk of throwing around
too many buzzwords here. You're spot on and that's exactly what we're doing. We increased the
abstraction level so I think there's a very good summary of what we're actually doing.
Focusing more on the reusability effect notches on technology but on the output as well.
Because we are right now obsessed with scaling the value aspect. I did the math that is not
complicated. If we are 100 people today or 120 issue working on the AI use cases we have around
10 use cases in production right now. If we're going to have all our core operational decisions
amplified by AI by 2025 which is our tech clip that we're aiming towards. We're going to need
thousands of people if we're scaling it vertically. So we need not the way of assessing the talent
gap and the time to value assessment as well. You mentioned that part of this recent reorg
put you in the same organization as IT. I'm curious about the implications of that or the impacts
of that and how you work together with IT. What ways has it impacted your workflow in the way you
approach your test? I think one of the good decisions that was taken a few years back was that we
were going to start the AI efforts all on cloud with basically no limitations around the technology.
We could pick pretty much whatever we wanted. It was a curse and a blessing. But if you look
on a traditional IT on the prem traditional waterfall approaches relatively long cycles,
deployment, secretations, not so agile. Going into the new business tech organization where we
established around 250 product teams also supposed to work agile. The IT department coming down from
a monolithic type of situation trying to break that down into product teams relatively small.
Of course they have a lot of legacy. So we are running really fast. We are able to ship
with every delivery. The integration with IT is still relatively few. But what we are
experienced now is that we are encountering some of the traditional processes from that type of
organization. So I would say it's good for the company because we are getting everybody up to
the same stand that on how you deliver in a modern software organization or more than tech
organization. But on our side of course the lead times become a little bit longer when we're
trying to integrate. But it sounds like you are still operating. Well is AI operating primarily
cloud first but the rest of IT is not primarily cloud first. Is that part of what I am hearing?
The strategy is cloud first for the company. Then of course the migration takes a bit of time
for the rest of the organization. So we are 100% cloud first which makes life a lot of easier.
And so when you get a proof of concept or a project to production does that stay within your org
for operational responsibility or is there a handoff to IT? How have you structured around
ops long time? So IT doesn't exist in a longer as a notion. It's just business techno.
But what it is on the upside is that we have the notion of you run it, you build it.
What we said try to do is work together with the domains. So our domains like sourcing and
production, customer fulfillment, etc. They are responsible for the actual juice case. We are
responsible for the capability, the best practice, and the algorithms. So what we do is we work
together with the domain. They are going to run the product long term. But we are going to make
sure that they have the specialist to solve the problem. So take in season demand for a casting
for instance. We are running the algorithmic, the part that actually produces. They will handle
the integration and the actual usage of the product. So we can specialize in being specialist
in demand for casting. And they can integrate with the business and the change management
and run it. So it's going to be two product teams working together.
Okay. And so does the business units that sounds like have their own technical capability
to operate these applications long term. But they rely on business tech as and enable it to help
them build in emerging technologies. Is that the idea? Yeah. So the business units, they are
actually once running the business. And then business tech is a traditional old IT organization,
but being a more agile organization these days. So we serve the needs of the business units
in the group. They don't run, but they use the products that we develop. So for instance,
so for instance on the customer side, what we're building now is a platform to integrate with
hnm.com to make it fully personalized. We can deploy microservices to get interact and
create the personalized experience. Of course, those that actually are working on the use cases,
working with the sales and everything that's our online sales unit. And that sits in the business
units of the hnm brand and business tech is creating the technology to enable them to do their job
the best possible way. Got it. So you know, in all of this in your journey there at hnm group
growing the team from zero to I think you said 120 employees you've gone through, you know,
reorgs and transformations like what what are some of your your biggest learnings and all that
around how to scale NAI team from, you know, from the start. I think one of my biggest learnings
is perfection is the enemy of done. I think if you wait around and want to create the best product
or want to create the best technology, you're never going to to reach that end state. And if
you're serious about running fast and scaling, then you have to start now. It takes time. Even
though we have run extremely fast and lost 12 half years, we have just started. We are extracting
value. Many of the companies I worked with and seen doing these things, they wait too long. They
want to build the perfect infrastructure, they want to build the perfect model, they want to evaluate,
they want to get everybody on board. Of course, they call the management, management is extremely
important. Communication is extremely important, but at some point you just have to say, let's just
do it. The start up mentality, if there's value on the table, let's go out and get it. Scaling is
being an entrepreneur, where you start something from from zero, you have to have a bit of guts as
well and just go with the notion. And just to get tactical on that particular point,
often with these machine learning projects in particular, the looming perfection or the
small percent of increase in whatever your target metric is, always an temptation out there.
How do you and your teams stay focused and know when done is, where in the process do you know
that and how do you define it? It's super hard in case by case, but at some point you have to have
a baseline, you have to have a target, but it's like you say in software, they shift with every
delivery. This is not the research lab. We do have one of those as well, and they have different
prerequisites and different targets. What we want to get to here is we want to be able to
ship and show something within every sprint. Every PEI needs to be able to ship something,
so don't expect to be able to sit and fiddle around and try to optimize for too long.
And you have to work with your stakeholders as well and agree on a level and have an open
dialogue. What we don't want are unicorn data scientists that solves everything themselves.
What we want is talented individuals to work together with the team and have a transparent
dialogue around their process and what's hard, what's not working, so that everybody can take
an informed decision if we should go to production with what they have or if we should wait in other
sprits. You've talked a bit about thinking like a startup entrepreneur. You've mentioned
terms that we know from Silicon Valley companies, OKRs and technical debt and the like.
You know, clearly you're trying to operate like a startup within H&M Group. Does that create
friction for you in a large established enterprise and how have you addressed those kinds of issues
if you've encountered them? Definitely. I've seen people looking at us and saying why can they
do these type of things without actually looking at themselves and try it. I think what it comes
to is having a good story and communication around these topics. What I try to do in my communication
is that I'm doing this for the entire company. This is for the better good of this company. It's
not for just the success of the AI department. So the story is really around how we as a company
come around and it's not about threatening other people. It's about amplifying other people
to make sure that the people in this organization get to stay away from the boring long tail type
of problems and get to focus on the really interesting stuff. AI is really good at the long tail
type of problems and let's be humans that can focus on the creative things, the development of
the organization and the business. So we try to position ourselves not as a threat, but as an
enabler for people in this organization to be their best themself. Awesome. Is there
is part of the way you talk about what your group is doing? Do you have some moonshot
project or vision that kind of captures what you're looking to create over the long term?
Yeah, I think it goes back to the tech clip we want to do. We want to have all core operational
decisions amplified by AI by 2025. That's where we want to be and I think it's possible with the
strategy that we have created and I truly believe that if we're able to onboard more resources
of talent and grow the president we will be able to transform this company to a truly an AI
first type of company. Awesome, awesome. Well, Ariel, thanks so much for taking the time to share a bit
about what you're up to. Very cool stuff. Thank you so much. That's your beer. My pleasure, thanks.
