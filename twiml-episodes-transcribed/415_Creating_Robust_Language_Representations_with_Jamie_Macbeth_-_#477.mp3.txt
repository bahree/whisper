All right, everyone. I am here with Jamie McBeth. Jamie is an assistant professor in the Department
of Computer Science at Smith College. Jamie, welcome to the Twoma AI podcast.
Thank you. Thanks for having me. Hey, I'm super excited to dig into our conversation and learn
a bit about your research and what you're up to. Let's get started by having you share a bit
about your background with our audience. How did you come to work in AI and cognitive systems in
particular? Sure. So originally I was, I would say a physicist actually a mathematician and a
physicist as an undergraduate and also sometime as a grad student. I then fell in love with
computer science. I studied computer science in graduate school. And towards the end of my
career in graduate school, I also fell in love with the specific topic that I work on now,
which is artificial intelligence systems and cognitive systems for performing natural
language understanding and the the issues associated with that. That's great. That's great.
When we were chatting earlier, you spoke a little bit more about your, the way you think about
cognitive systems and kind of how that's different from a lot of the contemporary application
of machine learning and AI. I'd love to hear you elaborate on that a bit for our audience.
Sure. Sure. Yeah. So those of us in the cognitive systems community,
where we're a part of the artificial intelligence community, but people are focused, people in the
cognitive systems community are focused quite a bit more on using artificial intelligence as a
vehicle for a better understanding of human intelligence and not particularly of using AI to
just score well at particular tasks and do well on the leaderboard. I think some of the negative
things that have been associated with artificial intelligence these days, such as biases and things
like that, have to do with there being a little bit too much hype around the systems that people
are building and the way you're able to show good numbers at these test problems and focusing
less on the actual science. Okay. What really can these systems do? So yeah, in the cognitive systems
community, or I care much more about building systems that have a human-like intelligence.
Yeah. It's an interesting contrast. I think we see all the time in popular media nowadays,
you know, research or publishes a paper about AI scoring well against some benchmark,
and then journalist writes article, AI learns to understand x, y, z, and you must be like,
no way, not understand. Yeah, yeah. Yeah, I agree. So I think that the, I think things that have
happened in the machine learning community, I think we've, we've, machine learning our
artificial intelligence have come a long way in the past couple of decades, but I think another
thing that's happened is that that we've begun or not begun, but there has evolved a particular
kind of machine learning research that's just focused on doing doing well, scoring higher than
a little bit higher than the last person did on that one particular data set. And then what I
found in my work is that if you scratch the service a little bit, you find that there are important
issues with things like the metrics that people are using, and the possibility that you, you,
you have systems that are fairly overfit to particular data sets. You know, you throw adversarial
examples at a system and you see it kind of crumble. It's unable to really do the things that,
that you thought it was doing because it scored well in that data set.
So when you, when you talk about AI as a vehicle for better understanding human intelligence,
what are some examples of, of that, you know, ways that we understand human intelligence
now better because of AI research. Oh, yeah, that's a, that's a great question.
You know, I, my examples for this come from the kind of decades ago,
traditions of AI. For example, Roger Shank and Robert Abelson's original work on
scripts, goals, plans, and understanding. And that's a, that's a, that's a text or those are ideas
that have kind of permeated a large, a large path throughout, throughout, throughout,
through many different, throughout many different communities, including the cognitive psychology
community, the cognitive linguistics community, even people performing social science,
quote, this idea of scripts, which basically that means that there are knowledge structures that
people use for navigating a commonly encountered social situation. So that's, that's a, I think
that's a really good example that book from 1977 is still cited. It, you know, it's got tens of
thousands of citations and people still cited over and over again. And when I talk to social scientists
who know about this concept of scripts, they don't even, they don't even realize that it comes from
artificial intelligence research, but it's important concept that people use across many disciplines.
So that, that, that's a good example, I think.
Got it. Yeah. And great to hear you mentioned that book. It doesn't get mentioned on the podcast
very often. I think it's come up once or twice before. I went to grad school at Northwestern,
and I think that's where Roger Shank was for quite a bit of time. And that was the first book
that I ever came across about AI. In fact, I picked it up to like a use, you know, either like the,
you know, use bookshelf at Barnes and Nobles or like a flea market or something like that.
The, you also talk a little bit about, well, you mentioned that a big focus
from an application perspective for you is on natural language understanding.
How does that tie into the broader goal of your cognitive systems work?
Yeah. What you, what you end up realizing, when I started studying this, I knew what,
what lots of people can see as natural language processing, what I understood was, oh, you know,
you just have these systems that are calculating statistics having to do with words or
parsing to figure out what the grammatical structure is of a sentence and maybe doing some things
like that. What you eventually realize is that if you, if you want to build systems and build
systems that are performing something like the way humans understand language and produce language,
because language expresses all, expresses ideas, you realize that your systems need to be able to
represent ideas and manipulate ideas and basically have thought representations. So,
your systems that, that at first you think, natural language processing is just about messing
with words and grammar, you eventually come to the conclusion that natural language understanding
is about building systems machines that can really think like people if you're really going to
get it to understand that text and say, answer a question about that text.
And so, are the tasks that you're focused on, the same tasks that we see,
you know, traditional tasks in the NLU community like question answering?
Yes, yeah, that's one important task, not the only thing. I mean, you can think of others
such as translation or summarization and things along those lines, but more recently,
folks in the machine learning, natural language processing community have started to generate
large data sets that have those tasks in them that have things like, you know, a data set that
has paragraphs and lots of questions and answers. But when you scratch the surface of the machine
learning deep learning systems that are working on those tasks, what you, what you find out basically
is that they're taking advantage of patterns in the texts and in the questions in order to come
up with correct answers to the questions according to what's in the test answers for the data set.
And if you change things around just a little bit, all of a sudden the system doesn't really
understand. But yeah, those are things like, you know, reading a story and answering questions
about the story, summarizing a story, paraphrasing a story, those, those, those are the kinds of
things that I work on. Okay, and so in the, I don't think traditional is the right word here,
but in the typical NLU setup, you've got this task of let's say question answering and you're just
trying to perform well on the question answering data set. You're not necessarily trying to perform
while you're trying to gain deeper insights into something. What are the things that you're trying
to gain deeper insights to and like, what are, how do you approach that problem or what's your
problem set up and where does your learning, you know, come out of thinking about these kinds of
problems? Yeah, one of the deeper, deeper insights I want to get to by looking at, say,
story understanding or question answering is what are the, how, how do people form a mental
picture of what the text describes? How do people create that mental picture and then how do the
people or how to systems manipulate that mental picture in order to be able to reason about the text
and also the questions that were asked about the text? And the main thing I found in my work,
the most important thing to me is that it turns out ironically that to represent
knowledge thought meaning well, you have to have a significant part of your representation that
really has nothing to do with the language at all. It really has to do with these imagery,
mental model pictures that you create that represent the meaning and then those get mapped
onto language. They get mapped from language in understanding to that representation and then when
people say things or produce language, the idea which is this non-linguistic conceptual structure
then gets mapped back onto language and that's one of the important things I've found in my research
and an important thing I'm trying to get to by looking at tasks like question answering or paraphrasing
because in many ways that's what paraphrasing is, you realize that to say, well, there are many
different ways of saying the same thing. That same thing stuff must be this other thing that's
different from the language, the ways of saying it. Yeah, I'm wondering what that means in practice.
Like, I think of a deep learning model is building some representation that is some vector space,
is what you're saying that are you adding additional constraints to that representation that says
that it has more image-like properties that should I be taking what you're saying about building
a picture literally or is that more figurative? It's in some senses, literal in some senses,
figurative. Yeah, when it comes to say, for example, artificial neural networks or deep learning
systems that are professed to be able to build their own representations, the problem that I see
is that I haven't seen people demonstrate very well, particularly when it comes to natural
language processing problems, demonstrate well that the representations these systems are building
are like the ones that I think we should be building to build natural language understanding
systems. So, you're supposed to be building your own, the system's supposed to be building its own
representations. It's not obvious based on the inputs that deep learning systems are being given,
say, for example, examples of paragraphs and examples of questions and answers to those
relevant to those paragraphs. It's not clear that the deep learning systems are building
that they're building representations that are like the ones that I think probably would be good
for representing things. In my opinion, the representation systems that I work on, for the time
being, I'm building them by hand, and these representation systems try to decompose meaning into
complex combinations of conceptual primitive structures. For example, if I said something like
Mary kicked the ball on one hand, and I said something on the other hand, like Mary moved
her foot towards the ball and struck it. I didn't use the word kick in that other expression,
but you're still in that other expression able to compose a picture of what happened,
and see that it's equivalent to the first sentence that I gave. So, I've been building systems by
hand that can start with a conceptual structure that looks like this. It's got these decomposed
conceptual primitives, like, for example, for kick. One conceptual primitive meaning that
Mary moved her foot, another primitive act, meaning that the foot struck the ball, and another
primitive act, meaning that the ball took off and went somewhere. Then I have these other systems
that actually can generate lots of paraphrases of this conceptual structure. It can generate
language from it. Then what I've been doing in some ways is feeding these paraphrases into deep
learning systems that supposedly can understand language and finding that they don't really.
So, when you talk about this representation that you're hand crafting and you're
creating equivalences between kicking and moving feet is that within the
natural language domain, or is that in some vector space, or are you mapping to some kind of
image-based representation of these things, or something totally different?
Yeah. So, these kinds of representations have been studied by AI people before we were talking
about Roger Shank, and we were talking about script plans, goals, and understanding. If you
read that book, you might have remembered the original restaurant script had all these conceptual
primitive acts in it, such as the waiter, P. Francis, his or herself, themselves to the customer,
and then leads the customer to the table, and then the customer, M. Francis, their order to the
server, and things like that. So, Shank had the system called conceptual dependency,
and the idea was that try to reduce the number of primitive acts that you have to represent
things, and then to represent more complex things, just add more primitive acts to make it more specific.
So, for the time being, those are the kinds of representations that I work on.
They have primitive, such as some object moving through space, some object moving to the inside
of another object that was called ingest, some object moving from the inside to the outside of
another object, expel, and then other primitive acts like that, trying to break things down in
kind of molecular structure kind of way. There are other systems that people have developed in the
cognitive linguistics community, specifically these systems called image schemas that were popularized
by Lake Hoff and Johnson. And in research, we're only just now over the past few years figuring out
some of the correspondences between a Shank's old system of conceptual dependency,
and these other systems that were developed by people in the cognitive linguistics community,
and trying to learn from them. Sorry, if that's kind of too long. That's great.
I'm wondering about you kind of, as part of the setup you talked about how deep learning systems,
as an example, are often overfitted on a given benchmark, and there's a lot of competition
in the research community to one up the next and achieving state-of-the-art performance on a
given benchmark. Is your research, are you like saying, hey, I'm not going to play that game,
I'm trying to get understanding, and if you aren't, if you're opting out of that game,
how do you evaluate the performance of your representations?
Yeah, that's a great question. In some ways, I am not playing that game, and in some ways,
I am playing that game. In recent work that was published at the Advanced
Executive Systems Conference last summer, what I did was basically, and I think I described
this a little bit earlier, there are deep learning systems that can do paraphrase recognition,
and there are data sets that are devoted to training and evaluating systems that can
read two sentences and determine to some degree whether those sentences are paraphrases or not.
What I did is I took one of those systems, and then I took a system that Shank and his students
worked on at Stanford in the early 70s and I enhanced it, and what the system does is the system
was called Margie, and what it could do was you could input some natural language, it would
translate it over to this non-linguistic language-free conceptual representation, conceptual dependency,
and then it could, based on its language-free thought representation of the original sentence,
it could generate paraphrases of the original sentence. I enhanced the system so that it
could generate a lot more paraphrases than it could in the 70s because in the 70s, the computers
that people had and the systems that people had were very limited, and so I generated a set of
tens of thousands of paraphrased pairs that meant the same thing, and I also tested that these
pairs meant the same thing because I sent them to crowd workers and asked, okay, do these sentences,
you think these sentences mean the same thing, and overwhelmingly they did. Then what I did is I
took these pairs of sentences that all meant the same thing, and then I sent them to one of these,
I didn't send them, but I used one of these deep learning paraphrase detection paraphrase
recognition systems, and I asked this paraphrase recognition system, okay, do you think these sentences
mean the same thing, and what was interesting about the sentences that my system generated was that
they encompassed a wide range of linguistic variation, they were able to express the same idea
using lots of different words and different syntactic structures. So you had sentences that
meant the same thing that maybe only had one word in common, and syntactically they're very different,
and humans looked at these pairs of sentences and said, yeah, those mean the same thing,
the deep learning systems that were trained to do paraphrase recognition and detection,
they fell off a cliff, when the sentences stopped using similar words, and you got to the point
where the sentences were not using the same words at all, meant the same thing, the deep learning system
was basically like a coin flip, half of the time I would say they were paraphrases, half of the time
they would not, it was basically just guessing because it wasn't really understanding
what the sentences meant. So in that way, I play the game a little bit.
I was just going to push a little further on that and try to have you explain in that context,
what are you benchmarking your systems results against, like is it?
It's not necessarily trying to, well, maybe the goal of this research was to identify the
deficiencies of the deep learning systems, and your metric was the number of paraphrases that they
couldn't recognize or the performance, but I'm curious, do you ever compare the performance of
your representations against the performance of a deep learning type of system,
or are there other ways for you to understand whether the systems that you're able to create
with your representations are more robust. It also does strike me that some ways maybe the metrics
aren't rich enough, like envisioning a scenario where you're creating these paraphrases that
are so much richer than what a deep learning model might create. And is the metric,
have we created the metric for richness and what is that expressiveness? Maybe the traditional
competitions aren't really judging the things that your representations are better at,
but I'm also wondering broadly, if you opt out like how do you compare and how do you know
when you're on the right track and when your research is getting you closer to understanding?
Yeah, you raise a bunch of important questions. Let me try to address each of them in turn.
Yeah, so in the study that I was describing, there are metrics that you can apply,
but these are metrics that, well, not metrics, but we did statistical tests to determine
that basically humans were better at recognizing that the sentences in this set of paraphrase
pairs, that these paraphrase pairs, humans were better at seeing that these sentences meant the
same thing, and we were able to perform statistical tests showing that humans saw that they meant
the same thing, whereas the deep learning systems that we were testing, they basically declined
in performance with greater degree of linguistic variation. You raise the important issue of,
well, if linguistic variation is important, what are the measures for doing that?
And I've developed some measures for linguistic variation such as you can do things like
if you've got a corpus of text, if you've got two different corpore of text, you can do things like
parse all the content, and then look at things like what were the part of speech tags that were given?
What is the distribution of the part of speech tags that were given in this data set versus that
data set? And I did that in a previous study where I was trying to better understand the differences
between human caption generation and captions that were generated by the typical deep learning
systems that people have used for caption generation problems. But I think it's something that needs
studying these kinds of metrics is something that I want to work on in the future.
There are projects that I'm working on now where I'm hoping I'll be able to actually build systems
that, so I talked to you about this paraphrase generation system. I'm working on systems now that
ideally can do the opposite. They can read and understand language and bringing into this
bringing into this non-linguistic form. And we're using a data set from a data set called
ProPara from the Allen AI Institute, I believe, and this data set has these texts that are about
ProPara short for process paragraphs, and these texts are about things like geological processes
and physical and chemical processes, and we're going to treat those like their stories. And that
data set has tests that you can use to see if your system is really measuring up. So that is part
of our plan. But I think the larger issue of having the right metrics is important.
When it comes to judging these systems that perform paraphrase recognition, the metrics that
are usually used are these bag of words metrics, like, for example, the blue metric. Basically,
what they do is they take, they want to compare one text to another's text to see if they are
kind of the same, and they just treat each text like a bag of words. The metric doesn't care
about the order in which words come in and things like that, or you could have rearranged the words
and totally, it could have meant something different. You're basically just trying to get your system
to throw the right words in there that are relevant to the thing. You can say the same thing
with different words, and then your metric doesn't really work anymore. But generally, this is an
issue with the kind of research that people are doing where they just want to score better than
the last person on that data set. And so they don't really care what the metric is very much,
or whether the metric means anything. All they care about is their score.
In some ways, your research makes me think of folks like Josh Tenenbaum at MIT.
His is, I think, less natural language focused and more kind of visual. At least, I think about
it like that. And a lot of what he'll talk about is trying to kind of capture and understand
this idea of common sense, and, you know, external opportunity knowledge or things like that.
Is that part of what you're looking to understand in your research and the language domain?
Yeah, yeah. I'm flattered that you mentioned Josh Tenenbaum when talking about my work,
and then you think of Josh Tenenbaum. I'm very honored that you would say that.
And, yeah, I think that we are interested in many of the same issues.
I'm trying to remember the term that Josh Tenenbaum uses. I can't remember it at the moment,
but I do believe that from what I've heard him talk about, that he is interested in
trying to figure out what the most abstract conceptual primitives or
structures that people are using to kind of break things down and understand them.
I think he's what I've seen. I've seen him do some interesting and important work involving
children and their interactions with people around certain kinds of tasks that involve
reasoning and social interaction and things like that. Yeah, I think it's a great important work.
And so that's something I'm interested in, too, is if you believe like me that there are some
structures that some cognitive structures that people evolve or, let's say, people develop
at a young age before they even learn language. If you're going to say that they're these kind of
pre-linguistic representations that people are using in their building language on top of those
representations, you'd be interested to know what those are. And I think there are folks like
Josh Tannenbaum who are trying to get it what that is or what those things are.
But perhaps not so much doing it through language. You can see I'm trying to do it
through these tasks that have to do largely to do with language and texts.
Yeah. In the kind of traditional deep learning approach, like the knowledge of the system is
kind of stored in like weights and embeddings and things like that. It's kind of inherent in the
model. Sounds like your representations are more external. I wonder if there's anything you can
elaborate on there. Does your work have this kind of traditional view of a model? How does that
relate when you're building systems around your representations? What do they look
like? Are they similar to what we might be used to with deep learning and machine learning?
Or are they very different? In many ways they would look like structures,
kinds of models and systems and structures that were way more popular perhaps in the 1970s
in 1980s when people would call symbolic artificial intelligence, good old-fashioned artificial
intelligence. Some people characterize them also as rule-based systems. I've been building
a lot of systems by hand. Are you okay with all of those terminologies or are you
air quoting it because you don't really like those terms? Well, I'm air quoting it because
if you say symbolic AI, I have no problem with that. I guess the reason why air
quoted is because people have negative associations with those kinds of technologies where you are
building things by hand. Historically, people know that there have been ups and downs in artificial
intelligence. AI winters, as they're often referred to, and there were AI winters that were
associated with people building rule-based systems, symbolic systems where they were actually
coding things by hand instead of using machine learning. My view is that there's a general
negative association with doing any sort of building systems by hand. I think part of it is because
the old AI tradition people thought about this idea that you'd have expert systems that were
able to replace people completely and so what people came to realize is man, there's so much
that people know and so much that I would need to encode in my own that there's no way I would
ever be able to, if people were working around the clock, lots of them just encoding this knowledge
by hand, it's impossible that we'd ever be able to build a real system. Notwithstanding
double-enit and psych and projects like that, but my point of view is different. I'm not building
systems by hand in the hopes that I would eventually be able to build an artificial general
intelligence that I was coding by hand and I would eventually be able to encode all of the knowledge
that people have, but I'm using symbolic systems, building systems by hand, or you can say,
you can call them rule-based systems if you like to, if it's not too pejorative. I mean,
the reason why I build these systems is to help try to understand what's going on, what the
representation should be so that when we turn to say, okay, we're going to now use machine learning
to try to build systems to do these tasks, we have better ideas. Instead of kind of what I see
today is, there's deep learning, right? And it feels people, it seems like people think that
deep learning can do anything as long as you supply enough data to it. But then the question is
what kind of data should we supply to it? If we just supply text, is there stuff that we know
that helps us understand text that is outside of the text itself? And do we also need to supply that?
No. And so those, that's why I'm not opposed to building systems by hand. And in some of these
studies, what I've done is I've built systems by hand that are able to say, for example, generate
lots of paraphrases as an example and create a data set like that that demonstrates, okay,
maybe these are the kinds of representations we should be aiming for in our deep learning,
machine learning systems. I think that the new target, so right now the way things are is that people,
people build these data sets say, for example, paragraphs, questions, and answers,
and the target is just try to get the deep learning system to give the right answer.
In my opinion, the new target should be, should be that you should be supplying data sets that
help deep learning machine learning systems build the right representations. And those representations
might be non-linguistic or language free. And in turn, those representations help you get the
right answers. And then you can see, oh yeah, this thing is really kind of thinking the way I want
it to think. Whereas right now it feels like deep learning is just giving us a black box
where we can't quite see inside and see whether the representations it's building to solve the
task, making sense. Yeah, that the way you describe that resonates really strongly with the
conversation I had just the other day with Peter Abiel, a research out of Berkeley focused on
robotics. And we were kind of comparing and contrasting his views as an academic thinking about
kind of end-to-end deep learning. And you know, as you said, just throw enough data at a problem.
And his more evolved views as an entrepreneur and a roboticist that's trying to solve problems
for for companies. And the need to try to capture and incorporate knowledge that we have about
these these problems. And he made this really interesting point that, you know, one, you know,
if you say, okay, we want to incorporate, you know, this knowledge that we have about a problem,
you know, one way to do it is to, you know, build rules into your system, you know, build your
if-then statements or whatever that looks like. But what he's said that struck me as really interesting
was another way to do that is to use your rules to generate more data for your deep learning systems.
And so in that way, you're kind of training them on the cases that you know a lot about. But still
not having to, you know, not having to take on the technical debt, if you will, of having a lot
of rules, you know, the brittleness of rules, that kind of thing. And it strikes me that maybe your
systems could be used in a simpler way. Like you've demonstrated the ability to create these really
robust paraphrases that, you know, could be really interesting augmented data for a paraphrasing
system that you might want to train. Maybe that is the kind of glue between your world and the deep
learning world. Yeah, that is that is one way. I mean, so anytime you come up with an adversary,
anytime someone comes up with a with anything that shows that the hey, the deep learning system
is really isn't really doing what you want. Then the natural, you know, what the what the folks
who are doing big data, the machine learning will just say, I think many of them will probably
say is, well, okay, just give me the data that you just created that made my system. Give me those
examples and I'll just feed them in. And then my system will, well, then be able to handle those
kinds of will be able to handle those adversaries and will be better as a result. That's that's one
way of looking at it. But then from from my standpoint, it's it's, you know, I can I can continue
doing work on on these kinds, these these representational issues. And, you know, perhaps generating
generating data. But I think I think generally, yeah, I think I think generally it's it's one one
way to interface between those communities. I should say, though, also that I think part of the
reason part of the reasons why the the in the export systems era that the rule based systems
people had had a lot of difficulty was that they were using in many cases, in many cases, perhaps
they're using certain kinds of logical inference engines. And I think the issue with logic is that
people what people mostly tend to do is they create logical symbols that correspond to words. They
don't necessarily create symbols in those systems that correspond to non linguistic conceptual
representations that I think we have. And so I think if we're if we're creating knowledge structures
and reasoning systems that have more of that non linguistic language free abstract primitive
decomposition stuff that that we could we could do much better even even if we're even if we're
not going to use machine learning at all. If we were just back to building export systems like we
were in the early, well, I suppose in earlier decades, if we're back to building those
export systems, if we're using better representations, those systems could have been better too. You know,
maybe we would not have failed and had a had AI winters the way we way we did.
What are, you know, kind of looking forward? What are you most excited about in terms of
directions for your research? Yeah, so the systems that the these representational systems that
they keep talking about for the time being they're we find that things are fairly straight much
more straightforward when you're talking about things that are happening in the physical world
like Mary kicking the ball being decomposed into someone's you know Mary's foot moving and striking
the ball and then the ball moving and things like that. One thing that even even in earlier decades
where they were working on this research of de you know trying to represent trying to come up with
these non linguistic representations that they never I in my opinion they didn't really get a good
handle on and and this is also true if you read scripts plans goals and understanding was well
how do you how do you decompose the idea that someone should have a goal or how do you decompose
the idea that someone should have a plan? How do you decompose a decision or or or other kinds
of activities that involve thoughts? And they had a I feel like they got stuck back in those days
and didn't make much progress in in figuring those things out. They in many cases created
more and more diverse kinds of structures without doing the decomposition into primitive thing
that I think made their work in the early 70s more cool versus their work in the mid mid 80s
or so. And so that's something that I'm in future work that I'm then curious about interested in
again about scripts plans goals and understanding one of the one of the important basic ideas from
that book is that some of or some important reasoning involves from your episodic memories.
So remember the restaurant script the idea that you tell this story about Mary going into the
restaurant or you know she she eats the lobster and leaves and you're able to reason that
the she ordered the lobster from the server and all these other things and those knowledge
structures are built out of or at least theoretically the theoretical idea is that those
knowledge structures are built out of your episodic memories of your experiences with restaurants.
So you start wondering well you know what if what if other kinds of reasoning actually work that
way where you can you can build lots of script structures representing people's common experiences
and that's how people do a lot of their reasoning and a lot of their reasoning may involve scripts
combining with each other to reason about unusual events such as you know what happens when you
have a birthday party at the restaurant you combine the birthday party script with the restaurant
script and then you start reasoning about things like well if so and so pays for my dinner is that
considered a birthday gift at the restaurant something like that. And so that's that's something
I'm really excited about is learning more about general general reasoning through these structures
that are meant to represent people's episodic memories just people's experiences rather than saying
well it must be first-order logic or or I guess on another you know if you take it the other way
it must be deep learning is the only way or something like that so you know can you can you
start building databases of scripts or or databases of episodic memories and start using those as
the basis for structures for reasoning and things like that so those are those are just a couple
a couple of examples of things that I'm really excited about in the future.
Awesome awesome well as always we will link to your website and some of your recent work on the
show notes page that is will be available when the episode is published. Thanks so much for
taking the time to share a bit about what you're up to. Thank you Sam thank you so much everybody
for having me.
