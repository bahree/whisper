Hey, what's up everyone? Sam Charrington here, host of this week in Machine Learning
and AI Podcast, bringing you a look at my recent trip to the Consumer Electronic Show
and Las Vegas.
CES is one of those things that's hard to fully comprehend without having seen, so I thought
it'd be fun to give you a look at it from my vantage point.
In this special YouTube exclusive, we're going to check out some of the interesting examples
of Machine Learning and AI that I found at the event.
As you'd expect, there were a ton of robots on display at CES this year, but this enormous
combine harvester in the John Deere booth might be one of the biggest bots you'll ever
see.
I spent some time speaking with Dears Tavanga, Ciavora, learning about the autonomy and
computer vision tech built into this bad boy.
Check it out.
I'm here at the John Deere booth with Tavanga, who is telling me that there is a long history
of autonomy in these foreign vehicles, you said you've been doing it for how long?
Certainly.
John Deere has been doing a self-driving for over 15 years now, and we actually have the
accuracy level with our equipment down to plus or minus 2.5 centimeters of accuracy
for a piece of equipment of this size and scale.
So, yeah, here at CES this year we're excited about telling that story, and also many of
the other technologies we continue to evolve.
Here at the stand, we've been talking through some of our advancements in machine learning
and computer vision, and also looking at we're going next with artificial intelligence
or recent purchase of blue river technologies, and really how John Deere is trying to help
farmers continue to feed the world.
And you mentioned that there's computer vision and work at work in this combine.
So, what's a lot of that?
So, one thing we're really excited about with the new S700 series, Combine, what you're
seeing here.
We have some camera technology that's now embedded into the key aspects of the Combine.
So, what you're seeing here is the grain elevator, right?
Think of the Combine as a factory on wheels that's going through that corn field harvesting,
and what's coming out up here is actually the desired product, Dipper, a harvesting corn
for example, there's going to be kernels coming up here, and what's coming out the back
of the Combine is going to be compost, right?
So, it's going to be chopped up pieces of stock leaves, and all of that, and stuff you don't
want.
Well, yes, actually farmers actually do want to keep that on the field because then it
puts nutrients back into the soil, right?
So, hey, they're here, they're just done for.
So, what I was talking through earlier is, as that material is coming up, now our equipment
is able to allow the operator of the equipment to also program, and in essence have the machine
make those adjustments for them, which is an essence taken care of what is the farmer's
payday, right?
So, they've been investing quite a bit, and so that crop cycle, so it's making those adjustments
for them on the fly.
These days, the average farmer for the U.S. farmer is about 5,000 acres, and for these guys,
it's a very narrow window of time, so there are times working, you know, 12 to 16 hour
days.
So, both through driving or self-driving equipment, through this computer vision and automation,
this and our equipment today, it actually helps them do so more productively.
Right, and with the computer vision in particular, the result is higher yield and less foreign
matter in the...
Certainly, certainly.
So, you know, think about this way.
As we mentioned earlier, as they're going through harvesting, that's the farmer's payday,
right?
So, we're trying to help them maintain the best quality grain that's collected, and
also make sure that with those tailings or what's coming out of the back of the combine,
they aren't losing any of that grain also.
Awesome.
Thank you.
Pretty cool, right?
Of course, there were also a ton of drones here at CES.
While they're getting smarter and smarter each year, one startup is attempting to give
drones superhero-like powers.
All right, cool.
I'm here with Leah with AstroAR.
What's AstroAR up to?
Oh, man.
Okay.
So, we build drones that stop bullets.
We've developed a sensor array.
We're performing sensor fusion directly on the drone.
It's all edge.
And Vidya Inception partner, that's how we're able to do this.
That's how we're able to get the type of hardware necessary to run those machine learning
algorithms directly on the drone, doing all of this edge millimeter wave radio frequency,
combined with computer vision, and other sensors I can't really delve into.
But I assure you the ability to detect the difference between a fake gun and a real gun,
a loaded gun and an unloaded gun, we can read the writing on the facts in the shell casings
loaded into the gun.
Wow.
And now when I hear a drone that stops bullets, I think of like Wonder Woman bracelets and
like getting in front of the bullets in real time, that's not really what this is doing.
Not exactly.
Because the drone is able to identify the gun, the presence of a firearm in advance, we
identify this as an anomaly, not a firearm.
It's just an object of interest for which a police officer is made aware of.
We're not trying to take anybody's guns, but you can't shoot people.
The drone will take up a position that we refer to as guardian angel mode.
We'll get ready to get involved.
Now, it assesses threat based on the proximity of a hand to a gun, any hand, any gun.
So yes, they are absolutely, they're absolutely capable of getting in the way from that vantage
point.
And also, there is a human reflex that we are exploiting in order to ensure that this
works.
Do you remember, do you remember how Sarah Connor, she's running at that security guy?
He knows how dangerous she is and yet, when she tosses the keys to him, his hands come
up.
Yeah.
Yeah.
That is a human reflex.
It is universal.
The drones will rush you.
They will make themselves the most interesting thing in your world for about the next 30
seconds.
They will be extremely difficult to ignore.
It's like ignoring a flying blender.
If you're familiar with the canine technique of bark and hold, it's a canine can be trained
to rush someone, they'll jump up and down and bark.
Yeah.
Bark, bark, bark, bark.
Yeah.
You're getting a live pseudo live demonstration of bark and hold here.
Yeah.
I'm a bark and hold.
It's what that's called, when police do it.
And it's impossible to ignore that dog.
Exactly the same principle.
It is impossible to ignore those rotors.
Those rotors are, it's loud, it's making noise.
We have red and blue.
You're absolutely right.
Yeah.
So you've got millimeter wave technology that sounds like it can see through walls and
objects and determined.
The sensors that we use are actually capable of detecting cancer over the air.
We repurpose them for this.
Okay.
And so you're just.
They originally designed to perform a, it's basically a handheld MRI.
Okay.
Yeah.
And so what's the range?
10 meters.
Okay.
It's about 30 feet.
Now these work best in pairs.
So about 60 feet.
Yeah.
Okay.
And so it detects that there's a gun present and kind of takes a guardian angel position.
And it also sounds like you're doing some kind of pose detection to deter, like pose detection.
Do it to a certain extent, to a certain extent.
Yes.
This.
It's probably.
It's predicted by.
Yeah.
The city of hand to gun.
Okay.
That hand touches that gun and it's your program activates.
The only thing getting shot is a drone.
Yeah.
Right.
Yeah.
That's this is the armor.
Oh, this is one of the armors.
This is the raw armor.
Raw.
The produce armor has a backing of ceramic or Kevlar or you know, it's built up with you.
Build a proof of acrylic.
Okay.
It's a composite.
It is a composite.
And I mean, believe it or not, we actually have bulletproof panions.
I know, right?
I'm not even kidding.
I'm not kidding.
It works.
What can I say?
So I mentioned the.
Come on over.
Come on over.
What's that?
Move out of the way.
Dude, you're blocking our.
Yeah.
I mentioned the trajectory thing.
The pose detection.
Yeah.
Because you mentioned trajectory tracking.
Yeah, trajectory tracking.
The has has most.
It's most focused on what is the gun doing in the hand.
So it kind of estimates the departure angle of the bullet from the gun and gets in the way based on that.
Yeah.
I mean, you're going to.
It's also trying to provoke you to aim at it.
Okay.
Yeah.
So this is very much an active process that it is trying to get you to, hey, I'm over here.
Hey, I'm over here.
Pay attention to me.
Um, for God's sakes, we make the armor shiny.
Okay.
And so it sounds like it's pretty, uh, inference heavy.
You're not kind of sending information to a cloud and waiting for.
We're not.
We absolutely.
If we tried to send this over the cloud, someone would get killed.
So what are you using on device?
Uh, no, we're like I said, we're an Nvidia inception startup.
We are, uh, we're also arrow partners and we are.
It's like a Jetson or.
Uh, what's the Xavier?
Okay.
Yeah, we're using Xavier's.
I know Xavier's.
These are TX 2's.
We're.
We're not going to bring.
We're not going to bring the Xavier's.
Well, taking that down out the block.
Sorry, guys.
Um, yeah.
And this is the model of the.
Uh, yeah, that's what's going on.
That's being too arrow.
Let's throw that we use.
As you can see, we also incorporate.
These are, this is a D415.
Um, that we've incorporated.
We have an excellent relationship with Intel.
An excellent relationship with Nvidia.
An excellent relationship with arrow.
Um, and we are producing some custom hardware, some custom
silicon in order to achieve these purposes.
So what are the key things you've had to learn from a machine learning
and AI perspective or deep learning perspective?
A lot easier and a lot more straightforward than people seem to think.
Okay.
Yeah.
I mean, quite frankly, this has much.
That and the FBI doesn't like it when I talk about this too much.
I mean, I want to, I want to deep dive this.
I love it.
So I mean.
But there's only so much I can say.
Okay.
Yeah.
Got it.
Well, very cool application.
I hope this works out.
Absolutely, absolutely.
Oh, there's one last thing I would like to say.
No, we do not weaponize the drones ever under any circumstances.
We will not add pepper spray.
We will not add a taser.
We will not add anything that will make this into a weapon.
It is a shield that is its job.
Yes, you can defeat it with a t-shirt.
You go right ahead.
That's like messing with the cops canine.
You see what happens.
Yeah.
Now, you wouldn't know it by the name, but CES continues to showcase
more and more enterprise-oriented innovations each year.
One enterprise robotics vendor, Omron, uses a ping pong playing robot
that's actually not for sale to demonstrate their industrial chops.
How did this work?
So this is forpious.
Our fifth generation AI equipped table tennis shooter
and essentially it uses machine vision to monitor the player
to watch the player's body language also how they swing the paddle
and then also how the where the ball is.
And use AI to compare that to how it was taught from an expert player.
So it'll use that and actually try to teach the player to get better.
So it uses the vision to analyze, you know, to see where the ball is.
It uses the AI to analyze how the player is playing against the other
and how to help them improve.
And it also uses IT control to be able to get the robotics in the right place
to be able to actually play and keep the rallies going.
Okay, and how many cameras are involved?
There's five cameras.
Five.
Yeah.
Where are they located?
So they're actually in the front.
So if you look in front, they're all facing the player looking down
and again watching the player, watching the paddle, watching the ball.
And this isn't a product you sell.
What's the, why do you show this here?
Yeah, even though we do get offers to sell it,
it is actually just a demonstration of our technology.
We actually, we use the same technology in industrial automation,
healthcare, in mobility, and also energy management.
So this is, this is kind of fun, but those other places are what it does for a living.
Okay.
So here's an industrial automation application of some of the same technology you've seen before.
If you look at the robot behind me, it's doing a pick and place application
which is very typical in a factory automation setting.
Kind of pick and place.
So basically picking up something from one conveyor to one place
and sorting it and putting it somewhere else.
Okay.
So it's a job that is not really good for humans to do because it's repetitive.
So it's an example of being able to take an unsafe job
and even be able to do it with a robot.
And it's using the same technologies in terms of computer vision
and path planning and the like.
Right.
Exactly.
So if the, there is, there's vision similar to what's being used in portbius
looking at the conveyor line to find and locate the items that are on the conveyor line.
And then the same robot that's usually used for the paddle is also being used
in a pick and place application here.
If you were a fan of our AI in sports series last year,
you would love CES.
There were a bunch of really exciting sports tech applications on display there.
One interesting one was this AI powered boxing trainer,
bot boxer, which can be yours for just $20,000.
This is Bob Boxer.
It's an intelligent luching bag.
It can move.
But you can also see your shots.
So it tries to attract multiple points to your body
and tries to predict where the shot is going to come from.
So it's using computer vision to do that?
It's using computer vision.
Yeah.
And with time, it also learns your style.
Okay.
And so it can look at you and it can try and tease you.
It can, you know, it can see.
It can see you winding up a shot.
Okay. It's going to predict where the shot is heading from.
And how's it doing that?
We're tracking the upper body angles
and using that to predict what shot.
We use multiple track points.
You can see you doing this.
Telegraphing a shot.
And it also understands what you're trying to trick.
Like, good boxes, good boxes.
They strategize.
They would come up with one angle and then they struck it.
It's trying to get another.
So, buy boxes.
You can also understand these things.
Center for different levels.
You can try it.
You can try it.
You know, dodge the shot.
But here, you know, you can, you can work on your,
you can work on your boxing technique,
but it's also an awesome upper body cardio workout technique.
We have different modes.
We have training modes.
You can work on your different punches.
We'll give you standard boxing combination.
Combination of jab is across the hooks.
And it'll build you a workout routine.
Awesome.
Yeah, thank you.
One of the product categories I was most excited about at CES
was health and fitness technology.
Here, we speak with Frank McGillan of Quell,
a wearable task with delivering chronic pain relief
using AI and neuro technology.
I'm here with Frank McGillan with Quell.
What's Quell doing?
So Quell is the first pain relief solution
to integrate neuro technology plus AI
to deliver personalized approach to treating chronic pain.
All right, so let's break that apart.
So neuro technology.
Neuro technology.
Yeah, yeah.
What is the neuro technology aspect of the problem?
So neuro technology is stimulating sensory nerves
with high frequency, high power, electrical stimulation.
And it triggers the body's natural pain blocking mechanism.
Okay.
After first generation product in 2015,
and this year we brought AI to make this product even smarter
and react better to the patient's needs.
And so how specifically are you using AI?
Yeah, well, we're tapped into.
We have 70,000 users from our first generation product
who volunteered their health profile information.
So we're able to look at everything from,
you know, where do they have pain?
How long have they had pain?
Other health conditions.
It's body mass, age, all those factors.
They were applied machine learning to that data,
to look at clusters,
to find similarities in clusters.
And we've built that into a smart algorithm
into the growl device.
So when a new user gets quell,
they're benefiting from all the learning we have
from those 70,000 people.
Okay.
So what's an example of something
that machine learning helps you do better?
Yeah.
So we found that, you know, certain phenotypes of pain sufferers
that are normal calibration may misjudge
what their dosage should be.
They say that their dose should be lower or higher,
for example, than what they really need.
And my dosage, we're talking about the amount of electrical
which is an elation.
Yeah, exactly.
Exactly.
So in the key to getting quality relief
is getting the dosage right.
You know, if you think about it,
if you're taking pain medication,
if you're taking 10 milligrams and you need 100,
you're not going to see relief.
Well, the same thing with neuro stimulation
or electrical therapy.
So based on our smart algorithm,
we're now actually factoring in, you know,
hundreds of variables in addition to this
nervous sensitivity test we do when you first use the product.
Okay, awesome.
And do you have, it looks like a band that you wear.
You don't happen to have one on your knee.
I actually do.
So, you know, it's born under your leg here.
Okay.
And it's controlled through the app.
So we have a swell app here which lets you
help you manage the device.
We provide a lot of tracking information.
So we're not only collecting some of this profile data,
but we're also collecting things like activity,
gate, sleep.
So we provide a lot of tracking information.
And again, we're using this data not only to make the device smarter,
but also helping improve in the real world
how to achieve recovery pain relief.
Okay. And are there other examples of where machine learning
AI have come into play beyond the calibration?
So that's our first step.
Okay.
You know, we're really, again, trying to make sure we benefit
from all the data we've collected from our current users.
Yeah, what we're looking at for next step is how to apply that,
not only in that initial calibration,
but on an ongoing basis.
Okay.
Awesome. Thanks so much, Frank.
Cool.
Thank you.
Perhaps my least favorite category at CES
was so-called smart cities.
Not because they're not smart,
but rather because too often it's just a thinly veiled euphemism
for the surveillance state.
That said, my conversation with Zemanthus was interesting
because they claim to fix something that we all hate.
Traffic.
All right, cool. So I'm here with Alexandros from Zemanthus.
What is Zemanthus up to?
Zemanthus, we are predicting what will happen
in your traffic route before you get there.
So we can give you, for instance, information
about an upcoming traffic jam, 30 minutes or one hour
or two hours into the future.
And the beauty of it is, you can avoid it.
If you know it's going to happen.
Now, in order to do this,
you need some advanced mathematics.
And this is where Xemanthus came in.
We have a patented algorithm already
that combines machine learning and stochastic modeling
in order to produce this advanced forecasting
of your future routes to work or home.
So you can actually get there in time before anyone else.
But of course it's not just for just private users.
The municipality is gained because there is reduction in fuel.
There is reduction in congestion.
There is reduction in pollutants for the environment.
So everybody gains by avoiding a congestion, for instance.
So the idea is that you're combining statistical models
with more model-based approaches.
Exactly.
So machine learning is, as you know,
useful to predict patterns in the usual historic information.
So machine learning finds out what is usually going to happen
in traffic.
But to figure out what will happen in the future,
the patterns is not always very accurate
because traffic is chaotic phenomena.
So to figure out what happens in the future,
we use our own stochastic algorithms.
Which is like predicting what will happen in the stock market
together with the machine learning.
And those two, combined together,
allow us to have amazing accuracy for future events
before they occur.
Okay. And so these are like your kind of jamming machine learning
models and like Markov processes,
MM1 cues and MMN cues and all that kind of stuff.
So it is Markov modeling together with Monte Carlo
for the simulations.
Exactly.
Okay. So primarily simulation-based.
Okay.
Because once you simulate based on the real data,
what will happen, then you can start avoiding the problems.
Okay.
Because you know where they will occur.
In that location of the road in 30 minutes,
I'll have a traffic jam.
I know what to do now.
Okay.
You've got some of the use cases here
and you're also displaying some of the results you've seen.
This is an example in Sweden.
We got real data in blue versus green are predictions.
Which is right on top of each other.
Yeah.
And this is over several days we can do that.
Okay.
An amazing thing, which is really important,
is this graph in the bottom.
So down here, you have the usual traffic jam,
but then on the 23rd of November,
there is no traffic congestion.
Yet we predicted that.
They won't be any.
Okay.
You know what happened on the 23rd of November?
Looks like black Friday.
It's a black Friday.
Okay.
And you know why this is amazing?
If you go around,
any machine learning algorithm will predict the pattern.
Every, you know, five o'clock,
you have a traffic jam.
But they won't predict it's a black Friday.
We predicted it was a black Friday.
Not traffic jam.
It seems like it would depend on what kind of features
you're using, like if you're,
if your feature engineering kind of tags historical data
with, you know, events like back Friday
would do a better job of predicting it.
So what allowed your approach to do a better?
A stochastic pattern, the algorithm is what allows that.
Because it's not only looking at the patterns,
which are real cool,
but says what will happen in the future based
on the current information.
And then the stochastic algorithm takes over for the future.
And never, ever hit the congestion
because it was a black Friday.
I don't know how it happened.
Because I don't control the stochastic.
It's a market process.
It's a random thing.
You will do its own thing.
But the figure did it out on its own.
That's why we saw this.
It was an amazing result.
And so the data sources that you've played around with
are primarily traffic-based or there are others.
So we use whatever is provided.
In this case, we have cameras and video.
So we use that.
But we can use RFID to detect as you name it.
Whatever we are provided, we use it.
It can all go in as input in the algorithm.
It doesn't care.
And what was the origin of the technology?
The origin is my own because I'm a mathematician.
So I actually created my own stochastic model
because I work in traffic.
To simulate traffic.
And because it's a chaotic phenomenon,
it was research for me.
But I saw the obvious advantage here.
So I created a company.
Okay.
And so have you published on this at all?
Yes.
After we did the pattern, then we published it.
Okay. And where can we find that?
What's the name of the paper that folks should look at?
There are several papers now.
Okay.
I've been giving different conferences,
the information.
So I've published a number of papers by now.
Any particular one we can start out with?
Yeah.
Asymmetric simple exclusion processes.
Asymmetric simple exclusion processes.
For traffic dynamics, yes.
Okay. Awesome.
Yeah.
Alexandra, thank you so much.
Autonomous vehicles come in all different shapes, sizes,
form factors at CES.
And one of the most impactful applications
targets people with limited mobility.
Check out this clip for a look at a computer vision equipped wheelchair
developed by HuBox Robotics,
a startup partnering with Intel.
You can see all my facial expressions
and all the data I can capture.
All the points.
So this is what is running inside the Intel Nug.
So in real time, you can capture facial expressions in a more precise way
than any other technology you do.
So which allows us to use the kids to stop the wheelchair,
open mouth to go to one side,
let me just turn them.
Turn them off.
Nice.
And a kids to stop.
Raise that eyebrow.
Kids to stop.
Open mouth.
And kids to stop.
And a quick smile.
And move forward.
And even a tongue out.
Sometimes I can hold back.
So you see the precision.
You see the quality of the capture and facial expressions
in the technology.
So this is what we are delivering to the market
using terrorism scammer and also the Intel Nug.
It's our own work computer that I have here
underneath the wheelchair.
So we are processing everything locally
to capture those facial expressions.
So far we can capture 10 facial expressions.
You can use five of them to control your wheelchair.
And the other ones you can talk to Alexa by Amazon for example.
So from AI perspective in this based on a supervised type of model
did you train it on?
It's already pre-trained.
So what we did was we create a huge data set of 3D faces
because we are using 3D technology.
We pre-trained it to classify 10 facial expressions.
I'll give you an example.
We are able to classify a full smile, a half smile,
and a shy smile for example.
So this is huge, right?
And using 3D technology we are able to do this
regardless of the light condition.
So what we did was we create two layers.
This is 2TAC, right?
But we create two layers.
One, using machine learning.
So we can classify those facial expressions.
And the second layer, we use some eristics.
So I have never met before, but if we smile right now,
I can tell that you are smiling, right?
So we just touched algorithm to get back there.
So we have two layers.
One machine learning.
And now we have our eristics.
So we start to optimize this technology
using the intention of open Vino.
So now we can open Vino.
So now we can have a deep learning layer.
So 3 layers, machine learning, eristics,
and also deep learning.
And we can do this all offline, all offline,
just using the knock here underneath the wheelchair.
So here I can perform some facial expressions.
Just turn on.
I can't remember the numbers.
I can't remember, however,
you know, the numbers are there.
They think like a background.
It's left on the ground.
A pretty smile to move forward.
It's right there.
I can't remember the numbers.
Raised eyebrow.
I can't remember.
And even a dongle.
I can't move backwards.
So they're just taking the...
Cool.
Thank you.
All right, everyone, that's our show for today.
Be sure to like this video
and smash that subscribe button down below.
For more information on any of the awesome companies
or technologies you saw today,
visit twimmelai.com slash talk slash 2-2-2.
As always, thanks so much for watching
and catch you next time.
