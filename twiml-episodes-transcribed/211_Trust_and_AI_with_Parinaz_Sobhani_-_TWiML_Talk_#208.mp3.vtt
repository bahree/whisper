WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.320
I'm your host Sam Charrington.

00:31.320 --> 00:36.040
This week on the podcast, we explore perspectives on trust in AI.

00:36.040 --> 00:40.600
In today's episode, we're joined by Paranaz Sabani, director of machine learning and

00:40.600 --> 00:42.600
Georgian partners.

00:42.600 --> 00:47.300
In our conversation, Paranaz and I discuss some of the key issues falling under the trust

00:47.300 --> 00:51.580
umbrella, such as transparency, fairness, and accountability.

00:51.580 --> 00:55.580
We also explore some of the trust-related projects she and her team at Georgian are working

00:55.580 --> 01:00.820
on, as well as some of the interesting trust and privacy papers coming out of the NURPS

01:00.820 --> 01:02.700
conference.

01:02.700 --> 01:06.480
This week's series is sponsored by our friends at Georgian Partners.

01:06.480 --> 01:10.580
Georgian Partners is a venture capital firm that invests in growth-stage business software

01:10.580 --> 01:13.180
companies in the US and Canada.

01:13.180 --> 01:17.800
Most investment, Georgian works closely with portfolio companies to accelerate adoption

01:17.800 --> 01:22.780
of key technologies, including machine learning and differential privacy.

01:22.780 --> 01:27.740
To learn more about Georgian, visit Twimbleai.com slash Georgian.

01:27.740 --> 01:35.380
And now on to the show.

01:35.380 --> 01:40.460
Alright everyone, I am in Montreal and I've got the pleasure of being with Paranaz Sabani.

01:40.460 --> 01:44.220
Paranaz is the director of machine learning at Georgian Partners.

01:44.220 --> 01:47.100
Paranaz, welcome to this week in machine learning and AI.

01:47.100 --> 01:48.100
Thank you Sam.

01:48.100 --> 01:51.540
It's great to see you here and talk to you.

01:51.540 --> 01:52.540
Absolutely.

01:52.540 --> 01:57.460
It's been a while since we've seen one another and I'm looking forward to diving into the

01:57.460 --> 01:58.460
conversation.

01:58.460 --> 02:01.300
But before we do that, can you share a little bit about your background?

02:01.300 --> 02:04.100
Had you start working on machine learning?

02:04.100 --> 02:05.100
Long story.

02:05.100 --> 02:06.100
It's always a long story.

02:06.100 --> 02:07.100
I know.

02:07.100 --> 02:09.100
It's always a long story.

02:09.100 --> 02:15.220
I started working in AI and machine learning when I was in third year of my bachelor and

02:15.220 --> 02:17.540
I joined the Robocop team.

02:17.540 --> 02:20.900
I don't know if you know what does it mean, Robocop.

02:20.900 --> 02:25.860
Robocop, I just spoke to someone who also did Robocop and that was their kind of entree

02:25.860 --> 02:26.860
into ML&A.

02:26.860 --> 02:27.860
Oh, so cool.

02:27.860 --> 02:28.860
Exactly.

02:28.860 --> 02:35.380
He's a kind of simulation for soccer and in back home in Iran, they were all like the

02:35.380 --> 02:42.860
big fan of soccer and that's why we wanted to do something different, maybe simulation

02:42.860 --> 02:49.180
with robots and then I really found it amazing.

02:49.180 --> 02:55.500
And then in my master, I worked also in machine learning, I worked on data stream classification

02:55.500 --> 03:03.700
and concept rift detection and then I came to Canada to do my PhD and I found one of

03:03.700 --> 03:11.300
the most challenging problems in AI is like natural language understanding.

03:11.300 --> 03:17.020
If you look at the other problems that we are solving using machine learning technologies,

03:17.020 --> 03:27.820
speech recognition, vision and image kind of processing, I feel like out of all NLP and

03:27.820 --> 03:32.260
natural language understanding is one of the most difficult one and it's mainly because

03:32.260 --> 03:38.260
over time, over the course of human evolution, natural languages has adopted all the kind

03:38.260 --> 03:41.300
of human complexities.

03:41.300 --> 03:46.740
So if all I've got to be one of the last things that AI can achieve, truly, I mean like

03:46.740 --> 03:53.820
actual natural language understanding and generation, not simple mapping, sequence models

03:53.820 --> 03:55.820
here.

03:55.820 --> 04:02.300
So and then in my PhD, I've worked on sentiment analysis and stance classification and these

04:02.300 --> 04:08.580
are like kind of baby steps towards natural language understanding, mainly takes classification

04:08.580 --> 04:17.380
for social media and then I've worked for Microsoft research, working on machine translation

04:17.380 --> 04:24.500
and also for National Research Council of Canada, working on natural language understanding

04:24.500 --> 04:30.220
using deep neural networks, mostly three base LSTMs, that is structural LSTMs and stuff

04:30.220 --> 04:31.940
like this.

04:31.940 --> 04:40.500
And then were you working at Microsoft research kind of in the hey day of the neural machine

04:40.500 --> 04:44.460
translation when they were exactly that was a Google starter that they really wanted

04:44.460 --> 04:48.780
to have something similar and they wanted to ship the product as soon as possible.

04:48.780 --> 04:49.780
Okay.

04:49.780 --> 04:50.780
Yeah, exactly.

04:50.780 --> 04:51.780
Oh, wow, exciting.

04:51.780 --> 04:52.780
Exciting.

04:52.780 --> 04:54.300
It was a lot of that happening in Toronto.

04:54.300 --> 04:55.300
No, in Seattle.

04:55.300 --> 04:56.300
Oh, in Seattle?

04:56.300 --> 04:57.300
Yeah, in Redmond, actually.

04:57.300 --> 04:58.300
Okay.

04:58.300 --> 04:59.300
Yeah.

04:59.300 --> 05:00.300
Okay.

05:00.300 --> 05:01.300
The actual headquarter.

05:01.300 --> 05:02.300
Yeah.

05:02.300 --> 05:10.620
And then I joined Georgian Partners two years ago, which was kind of like big decision because

05:10.620 --> 05:15.860
like nobody can, nobody in my area, nobody in Michelinink kind of community can think

05:15.860 --> 05:22.380
of like venture capital as a carrier path and it was, I found, I didn't know about a lot

05:22.380 --> 05:28.540
about venture capital as well, but I found the opportunity pretty unique and the kind of

05:28.540 --> 05:34.220
opportunity that is like you're going to have more exposure to multiple businesses and

05:34.220 --> 05:36.740
you can work with several different startups.

05:36.740 --> 05:40.580
Actually, at some point, I decided I don't want to stay with big corporations.

05:40.580 --> 05:41.580
Okay.

05:41.580 --> 05:45.460
And I was so interested in a startup ecosystem, but I didn't know anything about startup

05:45.460 --> 05:46.460
ecosystem.

05:46.460 --> 05:47.940
So I thought, it's going to be fun.

05:47.940 --> 05:51.700
I'm going to learn a lot and actually I learned a lot.

05:51.700 --> 05:56.060
And at the same time, I was, it was more meaningful for me because I was helping startups

05:56.060 --> 06:01.380
that they didn't have resources or skulls like this cause I had.

06:01.380 --> 06:07.580
And I thought, okay, Google has so many smart people, Microsoft has so many smart people.

06:07.580 --> 06:13.060
So let's work for some places or work on some problems that if I don't work on them,

06:13.060 --> 06:15.700
there is nobody else to work on them.

06:15.700 --> 06:23.340
And so how did you get started working in the domain of trust and fairness in AI?

06:23.340 --> 06:28.260
I guess like most of other machine learning like machine learning centers that are working

06:28.260 --> 06:31.340
on real world problems.

06:31.340 --> 06:36.180
In one of our projects, we realized, oh my god, it's such an important topic.

06:36.180 --> 06:38.900
And I actually realized I don't know anything about it.

06:38.900 --> 06:43.980
So I have focused a lot on the optimization and the choice of machine learning models

06:43.980 --> 06:51.980
on like data processing, future engineering, but I never thought about these kind of fundamental

06:51.980 --> 06:53.660
problems.

06:53.660 --> 06:59.740
And again, it's kind of interesting at least for most of like products that I worked on.

06:59.740 --> 07:06.260
It's not full automation, it's more like a human in the loop that you provide recommendation

07:06.260 --> 07:12.540
and you have to build this trust with the human so that they will act under on, they will

07:12.540 --> 07:15.260
act based on your recommendations.

07:15.260 --> 07:20.060
And I thought, oh my god, even if I build the best model, if I can't build that trust

07:20.060 --> 07:26.060
by having better user interfaces or interpretability and shrink the value of your time and shrink

07:26.060 --> 07:31.300
maybe like for some applications, more critical decision making.

07:31.300 --> 07:34.980
It's also important to show the model is fair.

07:34.980 --> 07:41.940
Then actually the product would not be adopted and all my efforts going to be useless.

07:41.940 --> 07:44.380
So that was the story.

07:44.380 --> 07:53.620
And again, it was working with one of our companies and I can't share a lot about the company

07:53.620 --> 07:59.300
and the project, but it was a pretty important decision making, automation for a pretty important

07:59.300 --> 08:09.260
decision making that could have impacted the individuals and realized that we never evaluated

08:09.260 --> 08:14.820
the performance of the model against fairness, against bias.

08:14.820 --> 08:20.300
And we started to think of like what would be the sensitive attributes for their problem.

08:20.300 --> 08:24.940
And looking at the literature, it was more about gender ethnicity, but then we realized

08:24.940 --> 08:29.380
it's not only limited to gender and ethnicity or religion.

08:29.380 --> 08:34.260
And for every application, we might have a different set of protected attributes that

08:34.260 --> 08:37.940
we have to consider.

08:37.940 --> 08:44.180
And then, so that was the fairness story and then we decided, okay, how we can let's

08:44.180 --> 08:48.420
first of all quantify the bias in the model.

08:48.420 --> 08:50.740
And we used a fair test.

08:50.740 --> 08:56.180
There is a kind of open source tool which is great, very good user interface.

08:56.180 --> 08:57.740
And what's it called?

08:57.740 --> 08:58.740
Fair test.

08:58.740 --> 08:59.740
Fair test?

08:59.740 --> 09:00.740
Yes.

09:00.740 --> 09:01.740
Okay.

09:01.740 --> 09:07.340
I can't remember the authors of the paper, can't check, but I can't remember anyhow.

09:07.340 --> 09:15.860
But then, and we could kind of identify and quantify a couple of sources of the bias.

09:15.860 --> 09:24.420
And for their problem, we could solve this issue by having better sampling strategy.

09:24.420 --> 09:29.020
To as far as, as soon as we identified what are the kind of under-representatives in this

09:29.020 --> 09:35.500
data sets, then we could have kind of, it was only a single iteration, it was several

09:35.500 --> 09:40.540
kind of back-end-forced testing the model, improving the strategy, sampling the strategy,

09:40.540 --> 09:42.300
and then testing it again.

09:42.300 --> 09:48.740
One of the things that you mentioned that came up in the context of trust was user interface,

09:48.740 --> 09:51.820
which is not something that often comes up.

09:51.820 --> 09:56.980
What are the, when you think about kind of the broad issues that fall under trust?

09:56.980 --> 10:00.620
What are the kind of main things that you're thinking about?

10:00.620 --> 10:07.940
When I'm talking about trust, first of all, I'm thinking about transparency.

10:07.940 --> 10:15.460
And it's hard, and again, all these kinds of problems are hard to define, at least in

10:15.460 --> 10:17.340
a mathematical way.

10:17.340 --> 10:25.740
So by transparency, I mean, is it clear for the kind of users or customers, how are you

10:25.740 --> 10:32.580
using their data for what kind of processes you are using their data, and how the data

10:32.580 --> 10:37.820
has been shared across several different kind of use cases or processes?

10:37.820 --> 10:44.060
And then, interpretability, explainability is also, is really, really important.

10:44.060 --> 10:48.900
And again, I remember one, two years ago, when we were talking about interpretability

10:48.900 --> 10:53.260
in machine learning community, everybody were like, why should we focus on interpretability?

10:53.260 --> 10:58.100
We should focus on improving the performance of our machine learning models.

10:58.100 --> 11:03.180
And exactly in one of the projects that we were working on, we realized that users are

11:03.180 --> 11:08.900
not looking for that single probability or prediction.

11:08.900 --> 11:15.420
They are asking for more evidences, and it can help them to combine the knowledge of

11:15.420 --> 11:21.100
the machine or models with their own knowledge and having smarter decisions.

11:21.100 --> 11:25.380
So there are several different reasons that we might need interpretability, that's one

11:25.380 --> 11:26.380
of them.

11:26.380 --> 11:31.940
There are lots of discussion if interpretability can help causality or identifying if the

11:31.940 --> 11:37.140
model is biased or for scientific purposes.

11:37.140 --> 11:40.740
So there are several different reasons that we might need interpretability, that was one

11:40.740 --> 11:47.020
of the examples that I encountered in my kind of experience.

11:47.020 --> 11:51.900
And privacy also is really important.

11:51.900 --> 11:58.060
Again, it's not a secret anymore that machine learning models, they can memorize data.

11:58.060 --> 12:03.220
And especially if you use sensitive data to build your machine learning models, these

12:03.220 --> 12:08.980
models can remember the training in senses and can be reversed engineered to get access

12:08.980 --> 12:12.980
to those sensitive attributes of the customers or users.

12:12.980 --> 12:18.620
As we are using machine learning models for more kind of sensitive applications like

12:18.620 --> 12:25.580
healthcare, the importance of privacy is even higher and higher.

12:25.580 --> 12:30.220
Security is actually more important than any of these because it's actually the fundamental

12:30.220 --> 12:31.220
layer.

12:31.220 --> 12:39.020
If you don't have the secure kind of computation or if you don't encrypt your data in transition

12:39.020 --> 12:47.220
or in rest, and if you have a data bridge, nothing else can work, you know what I mean?

12:47.220 --> 12:50.180
You have lost the trust.

12:50.180 --> 12:51.180
Right.

12:51.180 --> 12:56.980
So it sounds like a lot of these issues that we sometimes think of as disconnected issues

12:56.980 --> 13:02.820
are all kind of come together when it comes to creating a trustful relationship between

13:02.820 --> 13:07.060
the user of an AI system and the system itself.

13:07.060 --> 13:08.060
Exactly.

13:08.060 --> 13:09.060
Yeah, exactly.

13:09.060 --> 13:13.660
And like there are, every day there are new issues, for example, adversarial attacks say

13:13.660 --> 13:18.900
are new issues completely specific to machine learning models.

13:18.900 --> 13:24.500
And like AI safety, like now these days I nips again, there are a lot of interesting papers

13:24.500 --> 13:27.620
about adversarial attacks on AI safety.

13:27.620 --> 13:35.540
Before we weren't even aware of possible risks of using machine learning models.

13:35.540 --> 13:41.580
Are there specific examples of projects that you can talk us through that, you know,

13:41.580 --> 13:44.380
where you kind of explore these various issues?

13:44.380 --> 13:48.780
Again, for sure talk about differential privacy.

13:48.780 --> 13:53.500
We've worked on several different, differential privacy projects.

13:53.500 --> 14:03.620
And as you know, we try to have the message as you can create value by having more private

14:03.620 --> 14:07.260
differential learning and data mining approaches.

14:07.260 --> 14:15.080
As an example, you can convince the user to share their data and you can aggregate data

14:15.080 --> 14:20.500
across multiple customers, building better performing machine learning models at the same

14:20.500 --> 14:21.500
time.

14:21.500 --> 14:26.220
You're going to have guarantees in terms of privacy of the users.

14:26.220 --> 14:31.340
So that's the kind of like one of the most kind of inspiring projects that I've worked

14:31.340 --> 14:36.860
on because again, it's not only about these are the risks and it's also about creating

14:36.860 --> 14:37.860
value.

14:37.860 --> 14:42.380
It's also about win-win kind of like you're going to have private models you shouldn't

14:42.380 --> 14:47.460
and at the same time you don't need to sacrifice the performance of the machine learning model.

14:47.460 --> 14:48.460
Okay.

14:48.460 --> 14:53.380
That's the kind of issue we have also with fairness because normally in fairness people believe

14:53.380 --> 14:57.580
that there should be a trade-off between accuracy and fairness.

14:57.580 --> 14:59.980
Why they really don't believe that's the case.

14:59.980 --> 15:07.180
And at least in our case, when we improved the kind of sampling strategy, we could improve

15:07.180 --> 15:09.980
both the performance of the model at the same time.

15:09.980 --> 15:19.420
We could have root out the bias and kind of discrimination in the system.

15:19.420 --> 15:23.660
So this example where you had to work with improving the sampling strategy.

15:23.660 --> 15:27.740
What was wrong with the model before you did that?

15:27.740 --> 15:33.220
So like we didn't, again, there are several different definitions for fairness.

15:33.220 --> 15:36.300
So we weren't mostly looking at group fairness.

15:36.300 --> 15:42.540
So for protected groups or for microseconds of the population, there was a huge difference

15:42.540 --> 15:51.380
between performance of the model and that's why, again, I know lots of people argued

15:51.380 --> 15:56.740
that should we have individual fairness or group fairness, but what we achieved by having

15:56.740 --> 16:02.740
better sampling strategy was group fairness for protected groups.

16:02.740 --> 16:07.020
And so can you walk us through those two different definitions of fairness?

16:07.020 --> 16:10.860
Like how are people thinking about this problem?

16:10.860 --> 16:14.660
So like, again, like some people argue that even if you have group fairness, you might

16:14.660 --> 16:16.260
not have individual fairness.

16:16.260 --> 16:24.380
For example, me and you having the same qualifications for the application, for the application X, we

16:24.380 --> 16:28.300
might be discriminated, we might not get the same utility out of the machine learning

16:28.300 --> 16:29.300
models.

16:29.300 --> 16:34.820
But at the same time, the model might have a similar performance for microseconds that

16:34.820 --> 16:37.900
we belong to.

16:37.900 --> 16:44.980
So there's a kind of intuition behind group fairness versus individual fairness.

16:44.980 --> 16:50.020
In a space like this where you mentioned that we don't have good definitions for this

16:50.020 --> 16:54.660
stuff in math, it's even like we don't have good definitions for this stuff in English,

16:54.660 --> 16:55.660
let alone math.

16:55.660 --> 16:59.340
There's so many different ways to define all these things.

16:59.340 --> 17:05.500
How does one go about creating, charting a path through such a marquee territory?

17:05.500 --> 17:06.980
It's really difficult.

17:06.980 --> 17:07.980
And you're right.

17:07.980 --> 17:13.460
Every time I'm giving a talk about fairness and masking, has anyone have a good definition

17:13.460 --> 17:14.460
for fairness?

17:14.460 --> 17:19.700
I'm really willing to listen and learn more about it, but still there is no concrete

17:19.700 --> 17:22.180
kind of definition or metric.

17:22.180 --> 17:26.180
And again, we are talking about like we have to quantify the bias, but then if we don't

17:26.180 --> 17:31.100
have any kind of metric that everybody agrees, that's a metric that we have to use.

17:31.100 --> 17:35.060
It's not like if one scored at everyone using machine learning now, we need to have

17:35.060 --> 17:36.060
similar score.

17:36.060 --> 17:39.100
But it doesn't mean there is no research.

17:39.100 --> 17:42.260
I'm really encouraged that we are in the right direction.

17:42.260 --> 17:46.340
In the machine learning community, even again, last night I saw a couple of papers that

17:46.340 --> 17:51.700
they were talking about different metrics about for fairness in Newrips.

17:51.700 --> 17:54.940
So like again, the community is in the right direction.

17:54.940 --> 18:01.180
We are thinking about these problems and we are and the other kind of encouraging thing

18:01.180 --> 18:07.180
that is happening is that we are kind of engaging with other communities as well.

18:07.180 --> 18:11.900
So now in the conferences, every time I'm giving a talk, I saw a couple of people from

18:11.900 --> 18:18.180
legal domain and they are kind of challenging me and asking me questions and I learn a lot

18:18.180 --> 18:19.180
from them.

18:19.180 --> 18:25.340
Also, there are also people from sociology interested in these kinds of problems.

18:25.340 --> 18:30.300
So we are in the right path, but it's still a long way to go.

18:30.300 --> 18:36.500
Yeah, I was at the kind of the exhibit floor here in Newrips earlier and was getting

18:36.500 --> 18:43.580
a demo of some project that IBM research was working on as Fairness 360 Toolkit, I think

18:43.580 --> 18:46.100
is what they called it.

18:46.100 --> 18:53.020
They were going through some examples and they had like six different half a dozen different

18:53.020 --> 18:54.340
fairness metrics.

18:54.340 --> 19:01.140
They touched on this kind of group versus individual and some others and then they had several

19:01.140 --> 19:09.300
different ways that this kit could help someone address these kinds of issues, just like

19:09.300 --> 19:14.380
statistically working with biases that might be in their data.

19:14.380 --> 19:19.620
But even with six, they're like, oh, and it's open, other people could add their own.

19:19.620 --> 19:22.940
There are just so many different definitions and ways that you might want to approach

19:22.940 --> 19:23.940
this.

19:23.940 --> 19:24.940
Yeah, that's right.

19:24.940 --> 19:34.020
One of my favorite papers this year in Newrips is my classifier discriminatory.

19:34.020 --> 19:39.560
It's a really good paper because it also talks about let's root out the bias and let's

19:39.560 --> 19:42.420
find where this bias is coming from.

19:42.420 --> 19:49.980
It's not enough maybe to play with the optimization functional objective function.

19:49.980 --> 19:56.260
It might be better to start trying different sampling strategies or even start capturing

19:56.260 --> 19:58.180
more information about the entities.

19:58.180 --> 20:03.180
For example, there might be a strong predictor that we are not using in our data.

20:03.180 --> 20:08.140
So it's kind of like it's not only about optimization, it's not only about mass, it's

20:08.140 --> 20:12.020
also about figuring out what is missing here.

20:12.020 --> 20:17.220
And I guess it was also in my practical experience more important than the mass or the optimization

20:17.220 --> 20:18.220
part.

20:18.220 --> 20:24.020
Quite a couple of approaches in the literature, but none of them actually gave us a pretty

20:24.020 --> 20:25.420
good result.

20:25.420 --> 20:29.820
So what do you mean by finding other predictors?

20:29.820 --> 20:34.900
I guess what I'm hearing is something that any data scientist is always trying to do

20:34.900 --> 20:41.260
find additional features or variables that might have predictive value.

20:41.260 --> 20:43.820
Are you getting at something different?

20:43.820 --> 20:48.860
You know exactly is the same thing, like that you can understand if you can kind of figure

20:48.860 --> 20:56.740
out what are the holes in your system and can you get access and you're right every time

20:56.740 --> 21:01.380
we're asking more information, we are asking for more resources, time and effort.

21:01.380 --> 21:08.500
And the question mark is that it's worth it to invest in kind of capturing more information

21:08.500 --> 21:10.540
and getting access to that data.

21:10.540 --> 21:16.220
And sometimes it's worth it if we kind of realize it has a huge impact both in the performance

21:16.220 --> 21:21.780
of the model and also on the bias and fairness issues we have.

21:21.780 --> 21:31.500
One of the things that seems a bit paradoxical in this space is that I think I have the impression

21:31.500 --> 21:37.240
this isn't particularly well informed, but my impression is that in a lot of highly

21:37.240 --> 21:44.780
regulated industries, you know take for example banking, they historically haven't collected

21:44.780 --> 21:50.020
information about these protected attributes because they don't want to be accused of

21:50.020 --> 21:52.140
using them to make decisions.

21:52.140 --> 21:57.820
And now we're talking about using statistical approaches that require that need that information.

21:57.820 --> 21:58.820
Have you run into that kind of thing?

21:58.820 --> 22:04.780
Yeah, I was in the panel exactly with a couple of other people, VPs and leaders of those

22:04.780 --> 22:09.500
financial institutes and they were, I was kind of arguing and challenging them, but I need

22:09.500 --> 22:10.500
access.

22:10.500 --> 22:14.700
As a data scientist, I need access to those protected attributes and they were like, why

22:14.700 --> 22:16.940
should we give you access?

22:16.940 --> 22:20.620
And I know there might be lots of privacy issues as well.

22:20.620 --> 22:23.620
So I was actively thinking about what would be the solution.

22:23.620 --> 22:28.700
So if you don't want to give access to those protected attributes, at the same time you

22:28.700 --> 22:33.140
want to give the data scientist the opportunity to test their models against different kind

22:33.140 --> 22:38.540
of biases, what would be the solution, the simplest one I could find was like having

22:38.540 --> 22:40.020
an API.

22:40.020 --> 22:45.100
So you don't access the data scientist attributes, but you can have an API that you can send

22:45.100 --> 22:50.180
your requests to a server and you can get the answer.

22:50.180 --> 22:55.220
So that's kind of like simple, simple maybe kind of solutions, maybe we need to do more

22:55.220 --> 23:02.060
research on what's the best kind of framework or process.

23:02.060 --> 23:06.980
But yeah, that's a kind of huge problem we have with the senior leaderships of the companies

23:06.980 --> 23:11.420
that they are like, why do you need access and it's hard to convince them.

23:11.420 --> 23:16.060
We do want to use them to improve the performance of our models, actually when we use them to

23:16.060 --> 23:17.580
test our models.

23:17.580 --> 23:22.020
And without having access to them, there is no way that we can't test our models.

23:22.020 --> 23:28.940
Is it an absolute requirement to be able to test against a particular kind of bias or

23:28.940 --> 23:35.820
bias against a particular attribute to have those attributes available as labels or are

23:35.820 --> 23:40.580
there other ways that you could infer that from your data or kind of explore from your

23:40.580 --> 23:41.580
data?

23:41.580 --> 23:42.860
That's a very good question.

23:42.860 --> 23:50.060
So you normally need to define the protected attributes and then test the models over

23:50.060 --> 23:58.380
different macro segments based on these protected attributes or sensitive features.

23:58.380 --> 24:03.660
But there are always proxies in data that we know they are a kind of good proxies of

24:03.660 --> 24:08.940
for example, even if we don't have access to gender, the other attributes might be very

24:08.940 --> 24:10.460
correlated with the gender.

24:10.460 --> 24:12.220
Okay, just part of the problem.

24:12.220 --> 24:13.220
Exactly.

24:13.220 --> 24:18.020
It's just like, yeah, that's exactly the problem that two years ago when we were talking

24:18.020 --> 24:21.940
about bias and fairness, they were telling us, okay, just remove those sensitive attributes

24:21.940 --> 24:22.940
from your data.

24:22.940 --> 24:28.620
We were like, okay, don't you know about correlations in features?

24:28.620 --> 24:33.100
But at the same time, we can't leverage those kind of correlations if we don't have access

24:33.100 --> 24:34.100
to it.

24:34.100 --> 24:39.220
And in fact, for that project that I was talking about, we didn't have access to lots of

24:39.220 --> 24:40.220
information like this.

24:40.220 --> 24:42.540
So we used lots of proxies.

24:42.540 --> 24:46.740
But you weren't necessarily able to even test your proxies against any kind of reaction.

24:46.740 --> 24:47.740
You're right.

24:47.740 --> 24:49.740
We don't know how accurate these proxies.

24:49.740 --> 24:54.300
The only possibility is this for example, for example, the address might be a proxy for

24:54.300 --> 24:55.300
ethnicity.

24:55.300 --> 24:56.300
Right?

24:56.300 --> 24:57.300
Sure.

24:57.300 --> 25:02.740
So again, then you can just on public data sets, you can kind of test the correlation between

25:02.740 --> 25:04.460
the address and ethnicity.

25:04.460 --> 25:09.740
That's the kind of possibility, but on your data set, yes, you can do anything.

25:09.740 --> 25:10.740
Yeah.

25:10.740 --> 25:17.540
All this is making me wonder if there's some kind of intersection between the differential

25:17.540 --> 25:22.940
privacy and fairness, not, you know, so setting aside the privacy uses for differential privacy

25:22.940 --> 25:30.700
like, are there ways to, I don't know, maybe mixing up streams, but I talked to someone

25:30.700 --> 25:39.300
else who was working on kind of a, like a cryptographic way to allow people to do testing

25:39.300 --> 25:46.580
against data or share data without giving them the data itself, which is kind of basically

25:46.580 --> 25:47.580
like this API.

25:47.580 --> 25:48.580
Exactly.

25:48.580 --> 25:50.940
It's the same basic thing.

25:50.940 --> 25:55.180
And so I don't know if the only thread between those two is, no, that's a very good point.

25:55.180 --> 26:03.340
And I believe two years ago in KDD, Cynthia, she also had a invited talk and she talked

26:03.340 --> 26:08.980
about maybe we should use techniques similar to differential privacy for fairness.

26:08.980 --> 26:13.700
And she talked, okay, like in differential privacy, actually, if you think of like the

26:13.700 --> 26:18.780
kind of data we have as a kind of matrix, and in differential privacy, we want to mask

26:18.780 --> 26:20.820
a row in the matrix.

26:20.820 --> 26:27.260
So which means like we want our models to not be sensitive to any single user information.

26:27.260 --> 26:34.380
But in kind of like when we talk about fairness, we want our models to be exactly, to be vertically

26:34.380 --> 26:37.780
in sensitive to some features.

26:37.780 --> 26:43.620
So there are some commonalities, but I didn't see any actual research in this domain.

26:43.620 --> 26:49.100
It's a great opportunity for researchers to start working on kind of using techniques

26:49.100 --> 26:51.660
like differential privacy for fairness.

26:51.660 --> 26:57.140
So you mentioned one paper that you were excited about here in NURP's, what was the name

26:57.140 --> 26:58.140
of that one again?

26:58.140 --> 27:04.020
Is my classifier discriminatory?

27:04.020 --> 27:09.180
And were there any other observations that jumped out at you from that presentation or

27:09.180 --> 27:10.180
from the paper?

27:10.180 --> 27:17.860
I guess the presentation is going to be tomorrow or in the poster, maybe tomorrow, but a friend

27:17.860 --> 27:24.980
of mine, she just told me about this paper and found it pretty interesting.

27:24.980 --> 27:30.780
There was another paper that I ran into last night in the poster session, but I can't

27:30.780 --> 27:31.780
remember.

27:31.780 --> 27:35.660
I can just Google it if you want.

27:35.660 --> 27:38.580
No, that's fine, that's fine.

27:38.580 --> 27:45.260
When you're working with startups that are trying to bring a product to market, they're

27:45.260 --> 27:51.780
under all kinds of intense pressures to bring a product to market, to satisfy customers

27:51.780 --> 27:58.020
like how do they, or maybe how do you typically have to convince them to pay attention to this

27:58.020 --> 28:00.380
or are they already thinking about it?

28:00.380 --> 28:01.380
It depends.

28:01.380 --> 28:04.860
Like some companies, they have already some data scientists that they have thought about

28:04.860 --> 28:08.940
those problems, some of them know, and we have to convince them.

28:08.940 --> 28:19.060
So, what I found out is that people are normally more familiar with security box and a normally

28:19.060 --> 28:26.340
kind of frame furnace or biased box as kind of unwanted association, but it's a box

28:26.340 --> 28:28.060
as well.

28:28.060 --> 28:33.740
And it's similar to privacy and security box that they are so hard to identify.

28:33.740 --> 28:35.980
But the risks are also very similar.

28:35.980 --> 28:40.260
So, the same risk of data breach, what would be the risk?

28:40.260 --> 28:44.220
What's going to happen to the valuation of the company?

28:44.220 --> 28:50.460
It's going to be very similar kind of impact on your brand if your model proved to be like

28:50.460 --> 28:52.700
discriminatory or biased.

28:52.700 --> 28:59.820
So, that's kind of high frame it, and I try to convince people to think about the importance

28:59.820 --> 29:01.540
of these issues.

29:01.540 --> 29:10.460
When you frame it like that on the security side and the security bugs that lead to breaches,

29:10.460 --> 29:20.580
there are increasing amounts of hard data about the costs of these kinds of breaches and

29:20.580 --> 29:25.260
the implications from leadership change perspective in that business.

29:25.260 --> 29:34.860
Are there similar examples from a breach of fairness or a breach of trust perspective

29:34.860 --> 29:37.300
or are you familiar with any?

29:37.300 --> 29:40.580
Not in the startup ecosystem on slave.

29:40.580 --> 29:48.540
For example, I guess you all know about a kind of blog post that was about Amazon recruiting

29:48.540 --> 29:52.500
software that they start using it.

29:52.500 --> 29:58.060
This was for folks that may not have come across this, an article that actually thought a

29:58.060 --> 29:59.900
lot of the headlines were disingenuous.

29:59.900 --> 30:06.020
It sounded like they were researching some, using some tool in HR and they found the tool

30:06.020 --> 30:12.700
to have a bias against women in the hiring process and they decided not to use the thing,

30:12.700 --> 30:15.420
which that sounds like what you're supposed to do.

30:15.420 --> 30:16.420
Exactly.

30:16.420 --> 30:17.420
Yeah.

30:17.420 --> 30:20.660
If it wasn't the case, you wouldn't even hear about it.

30:20.660 --> 30:23.180
Right.

30:23.180 --> 30:29.020
So there's that and I guess the Netflix prizes, the one that comes along, that's more privacy.

30:29.020 --> 30:30.020
It was privacy.

30:30.020 --> 30:31.020
Yeah.

30:31.020 --> 30:32.020
It was privacy.

30:32.020 --> 30:34.380
You simple the identification and it failed.

30:34.380 --> 30:37.820
For example, Google example, so that lots of people use it.

30:37.820 --> 30:43.540
I don't know if they fix it or not, but I remember six months ago, if you search for images

30:43.540 --> 30:47.100
of CEO, it was all white male.

30:47.100 --> 30:52.780
And it's a hard question and it's a kind of question that even for Michelle and Ingrid

30:52.780 --> 30:54.980
data scientists can't answer.

30:54.980 --> 31:00.900
It's a kind of question that you have to ask in the community of interdisciplinary researchers

31:00.900 --> 31:08.460
because actually, if you look at the statistics, what's the percentage of non-white male

31:08.460 --> 31:15.380
CEOs and it's a kind of, if you do random sampling, what's the likelihood of having one

31:15.380 --> 31:19.500
of those kind of, for example, women's deals?

31:19.500 --> 31:24.980
Because right now, but it's not about equality, it's more about equity and like that's kind

31:24.980 --> 31:30.860
of like hard to see, that's kind of, as a kind of scientist, you're working on this problem

31:30.860 --> 31:32.380
and you have a random sampling.

31:32.380 --> 31:33.380
You don't know.

31:33.380 --> 31:40.660
So I would take that kind of restate that as part of the challenge is that we want our

31:40.660 --> 31:46.260
models to reflect our values more so than the data that we have to work with to create

31:46.260 --> 31:47.260
the models.

31:47.260 --> 31:52.980
And so model may be performing accurately relative to the data, but that's not necessarily

31:52.980 --> 31:53.980
what.

31:53.980 --> 31:59.980
Yeah, it is reflecting the data, which is reflective of many, many years of biases.

31:59.980 --> 32:00.980
Exactly.

32:00.980 --> 32:01.980
Right.

32:01.980 --> 32:06.620
And that's why we need to have more awareness that the main assumption of any machine learning

32:06.620 --> 32:11.700
model is that future is going to be similar to the past.

32:11.700 --> 32:17.460
So for any reason, if we don't want future to be the same as past, then we shouldn't

32:17.460 --> 32:23.660
rely on data and we shouldn't rely only on machine learning technologies.

32:23.660 --> 32:25.420
So what should we rely on?

32:25.420 --> 32:26.420
Do we know yet?

32:26.420 --> 32:27.420
No.

32:27.420 --> 32:36.540
I was like, again, like in Norepse yesterday, there was a talk, I can't even remember.

32:36.540 --> 32:43.980
There was like a tutorial, actually, Norepse Tutorials.

32:43.980 --> 32:44.980
It was very interesting.

32:44.980 --> 32:45.980
I didn't.

32:45.980 --> 32:46.980
Were you there for that?

32:46.980 --> 32:47.980
No.

32:47.980 --> 32:50.220
It was a very good one.

32:50.220 --> 32:54.340
And surprisingly, there were very, very few people in the room.

32:54.340 --> 33:00.780
So like most of the tutorials, they were packed, but this one, very, very few people.

33:00.780 --> 33:05.220
And I was like, why, these are like questions that we need to think about.

33:05.220 --> 33:10.300
And unfortunately, there are not so many people in our community in that room.

33:10.300 --> 33:16.300
So like, that was the sad part, but there was a tutorial on common pitfalls for studying

33:16.300 --> 33:20.460
the human side of machine learning.

33:20.460 --> 33:23.900
And they had very, very interesting points.

33:23.900 --> 33:29.420
So they were like, maybe explainability is not enough, maybe it's not only about casting

33:29.420 --> 33:33.100
the furnace as optimization, we should also fix the process.

33:33.100 --> 33:40.420
They had lots of interesting points, and one of the interesting problems, if they point

33:40.420 --> 33:46.620
out was data, data are not the truth.

33:46.620 --> 33:52.260
So also we know that there are lots of noise in capturing the data, and even if we talk

33:52.260 --> 33:59.220
about future, the past problem, we have data or machine learning models, but even sometimes

33:59.220 --> 34:04.460
the data is not even reliable because there are so many kind of noise in recording the

34:04.460 --> 34:05.460
data.

34:05.460 --> 34:11.060
So, but my problem with those kind of talks is that they don't talk about the solutions.

34:11.060 --> 34:16.460
And as a kind of practitioner, I'm always like waiting for, like, give me a solution,

34:16.460 --> 34:17.460
give me a solution.

34:17.460 --> 34:22.340
And it might not be a perfect solution, but it can be a starting point.

34:22.340 --> 34:27.180
And okay, we are, we have a long way to go, but they have to start from somewhere.

34:27.180 --> 34:33.620
Now structured is kind of your solution approach when you're engaging with one of these portfolio

34:33.620 --> 34:38.580
companies that, you know, recognizes that there's an issue or that they don't want there

34:38.580 --> 34:39.580
to be an issue.

34:39.580 --> 34:40.900
That's a very good point.

34:40.900 --> 34:46.620
So like, normally, if they don't, sometimes they also are aware of possible problems with

34:46.620 --> 34:48.020
their data and the problem.

34:48.020 --> 34:49.020
They are solving.

34:49.020 --> 34:53.340
But sometimes it's harder to convince them, so I ask them, can you give me the access

34:53.340 --> 34:57.300
to data and the predictions of your machine learning models?

34:57.300 --> 34:58.500
And then I run some tests.

34:58.500 --> 35:01.100
For example, they told you about this fair test.

35:01.100 --> 35:06.300
It can give you all the statistics about kind of performance of the model, accuracy,

35:06.300 --> 35:07.300
different macro segments.

35:07.300 --> 35:13.500
And it's very different combinations of these protected and non-protected features.

35:13.500 --> 35:19.420
And then I come with some kind of proofs, okay, these are my evidences that your model

35:19.420 --> 35:20.740
is biased.

35:20.740 --> 35:21.740
Okay.

35:21.740 --> 35:27.060
And let's think about the possible solutions.

35:27.060 --> 35:33.700
And if they are using any kind of sampling strategy, we would start with improving the

35:33.700 --> 35:34.860
sampling strategy.

35:34.860 --> 35:39.500
If you have lots of data on your only using parts of your data, how can we improve the

35:39.500 --> 35:46.020
model by leveraging better sampling strategies or like other parts of the data that you are

35:46.020 --> 35:47.020
not using?

35:47.020 --> 35:54.140
Is the implication there that the problems are introduced by their sampling strategy?

35:54.140 --> 36:00.060
Or they can use alternative sampling strategies to overcome problems that are already in

36:00.060 --> 36:01.060
their data?

36:01.060 --> 36:02.060
Exactly.

36:02.060 --> 36:07.100
They can use alternative sampling strategies or iteratively try different sampling strategies,

36:07.100 --> 36:12.340
test the model in that fair test kind of framework, kind of evaluating and observing

36:12.340 --> 36:16.220
the result and having the second iteration.

36:16.220 --> 36:20.580
The other kind of solution is that if they are getting their labels, if they're having

36:20.580 --> 36:24.660
a supervised learning problem, getting the labels from the humans, and if they are using

36:24.660 --> 36:31.100
human annotators, how can you define strategies to minimize the human bias?

36:31.100 --> 36:38.100
So like I remember for my PhD, I used mechanical turics to label my data.

36:38.100 --> 36:45.140
So we had all these kind of quality assurance techniques to make sure that we're getting

36:45.140 --> 36:49.900
kind of accurate labels from our annotators, so we can do similar things.

36:49.900 --> 36:53.940
So like quorum types of approaches and that kind of thing?

36:53.940 --> 36:54.940
Exactly.

36:54.940 --> 36:56.140
That's another one.

36:56.140 --> 37:02.180
The other one is like, for example, one of my other favorite papers is the paper by

37:02.180 --> 37:10.180
Rich Zemol, which is about learning a representation for your data that captures as much as possible

37:10.180 --> 37:16.300
the information, but at the same time masks all the sensitive attributes.

37:16.300 --> 37:22.060
So kind of having two competitive goals when you are learning representations for your

37:22.060 --> 37:23.420
entities.

37:23.420 --> 37:27.980
It might not be possible for some applications because like we don't have enough data to

37:27.980 --> 37:34.100
learn the actual representation, we are not using fancy deep neural networks.

37:34.100 --> 37:37.980
So like we have a couple of solutions, but still sometimes we are in a position that

37:37.980 --> 37:43.380
we don't know what can be the solution and we start researching again.

37:43.380 --> 37:49.980
So first step is playing with sampling strategies and seeing if we can solve it there.

37:49.980 --> 37:58.060
And then the next step is just diving in and it sounds like it's a combination of data

37:58.060 --> 38:03.620
science and research and subject matter expertise and it's, you know, like other aspects

38:03.620 --> 38:04.620
of the science.

38:04.620 --> 38:09.420
It's kind of all these two apart, yeah, exactly like any other problem.

38:09.420 --> 38:16.540
You've mentioned how a lot of this work is interdisciplinary, are there specific disciplines

38:16.540 --> 38:23.340
that you kind of look to first when you're looking for practical approaches that you might

38:23.340 --> 38:27.940
be able to incorporate into solving a company's problem?

38:27.940 --> 38:34.340
Normally what I found really useful is engaging with the product managers.

38:34.340 --> 38:39.100
Because they have, they are the, or the product owners because they have the best understanding

38:39.100 --> 38:43.460
of their user requirements and the addressable markets.

38:43.460 --> 38:50.660
So like even their model might be discriminatory because they are not serving part of the population.

38:50.660 --> 38:55.060
So it's not like cohort of the user might not be in the addressable market.

38:55.060 --> 39:01.660
So it's really good to have their insights before having any kind of further experiments

39:01.660 --> 39:05.660
or exploratory kind of analysis.

39:05.660 --> 39:10.500
So like I found like product people are the best people to talk to.

39:10.500 --> 39:18.180
But what is really missing is a kind of trust officer in our companies.

39:18.180 --> 39:22.900
So and we try to, we just started to think about these problems.

39:22.900 --> 39:27.460
Do we need a trust officer and it's not about the person who can solve every problem,

39:27.460 --> 39:32.020
but this is a person that can collect all the information from different stakeholders.

39:32.020 --> 39:33.020
Right.

39:33.020 --> 39:38.580
And he's like the owner of building trust with the customers.

39:38.580 --> 39:40.340
And again, it's not going to be a single person.

39:40.340 --> 39:45.260
It should be education for the whole company.

39:45.260 --> 39:51.980
And even we suggested you should have, you should add another kind of metric in your performance

39:51.980 --> 39:56.140
evaluation when you're evaluating your employees performance.

39:56.140 --> 40:05.780
You should add the trust component or responsible innovation or responsible AI kind of metric

40:05.780 --> 40:10.580
when you're evaluating your customer, your employees performance.

40:10.580 --> 40:12.180
It's not going to be a single person.

40:12.180 --> 40:18.420
But again, you should engage as many as possible people from the company and even outside

40:18.420 --> 40:19.420
of the company.

40:19.420 --> 40:25.620
Because for example, social sociologists, like we normally don't have anybody with sociology,

40:25.620 --> 40:27.420
background in our companies.

40:27.420 --> 40:31.740
So it's really important to engage those people if you have such question like the Google

40:31.740 --> 40:32.740
problem.

40:32.740 --> 40:33.740
Right.

40:33.740 --> 40:34.740
Yeah.

40:34.740 --> 40:35.740
That's interesting.

40:35.740 --> 40:41.340
I can see at larger companies, I wonder where this will end up living, right?

40:41.340 --> 40:47.980
You've got your, you know, chief data officers that are responsible for, you know, both kind

40:47.980 --> 40:52.780
of protecting and securing data or, you know, sometimes it's a CSO, chief information

40:52.780 --> 40:58.940
security officer who owns that, a CDO might own kind of monetizing data or, you know,

40:58.940 --> 41:00.380
the way that they manage it.

41:00.380 --> 41:06.740
But to your earlier point, a lot of the, you know, the challenge and the risk for larger

41:06.740 --> 41:09.980
companies would fall under someone who owns the brand, right?

41:09.980 --> 41:10.980
Yeah.

41:10.980 --> 41:15.220
I guess it says that it needs to be something that companies think about at the highest

41:15.220 --> 41:19.220
levels and have dialogues about.

41:19.220 --> 41:27.540
Exactly, somebody that can have a decision-power and is in all the decision-making discussions,

41:27.540 --> 41:33.260
somebody in a leadership team like that because you want to make sure that for every decision

41:33.260 --> 41:41.580
you make that can impact your users or customers, you have a person thinking about all these

41:41.580 --> 41:49.140
kinds of problems that might happen later when you're, when you're deploying your

41:49.140 --> 41:50.140
systems.

41:50.140 --> 41:59.460
And so are there, when you're kind of looking for research to incorporate into finding

41:59.460 --> 42:02.380
a solution to a given problem?

42:02.380 --> 42:07.980
Is there anything unique about your approach to that in this space as opposed to trying

42:07.980 --> 42:16.300
to solve a vision problem using unique research or is it kind of the same Google or kind

42:16.300 --> 42:18.860
of search or something?

42:18.860 --> 42:20.780
That's a very good point.

42:20.780 --> 42:27.700
I try to not bias myself by reading only papers from Shalani community.

42:27.700 --> 42:32.980
So it's still, it's still its research and I'm scientist and I have to dig into different

42:32.980 --> 42:41.300
kind of papers and articles and blog posts out there, but at the same time I'm trying

42:41.300 --> 42:46.700
to read more papers from other disciplines as well.

42:46.700 --> 42:52.220
So that's the only thing I can also like, I try to be active in the community and having

42:52.220 --> 42:56.620
lots of conversation with people with different backgrounds.

42:56.620 --> 43:03.460
It also helped me a lot, but I really don't have any good answer to your question that

43:03.460 --> 43:09.020
is there any fundamental difference between this problem and any other kind of problem

43:09.020 --> 43:10.020
we are solving?

43:10.020 --> 43:11.020
Right, right.

43:11.020 --> 43:20.780
Are there particular communities that you've found to be very useful and under-recognized

43:20.780 --> 43:26.340
I guess in terms of contributions to this type of work?

43:26.340 --> 43:33.940
Underrecognized, again, ethics in here is also hot, so that's less of a problem.

43:33.940 --> 43:36.820
Exactly, it's like harder than a couple of years ago.

43:36.820 --> 43:37.820
Yeah, exactly.

43:37.820 --> 43:38.820
It's not hot.

43:38.820 --> 43:41.300
Lots of people interested to work in this area.

43:41.300 --> 43:49.140
Lots of conferences, talks, events, yeah, top of my mind I have no example.

43:49.140 --> 43:53.620
Any other things that you're looking forward to seeing here at NURBS on this particular

43:53.620 --> 43:55.140
topic?

43:55.140 --> 44:02.740
I believe what is missing is that our community at least historically wasn't very welcoming

44:02.740 --> 44:08.500
for people with different backgrounds, so I told you that I in my PhD was working on

44:08.500 --> 44:15.860
NLP and I believe I met lots of people with linguistic kind of backgrounds in NLP conferences

44:15.860 --> 44:23.260
and it was more welcoming, so but here if you don't have mass background, you kind of

44:23.260 --> 44:32.020
full intimidated, so and I don't know how we can encourage people with not necessarily

44:32.020 --> 44:38.260
machine learning background to join our community and also give us comments and challenges and

44:38.260 --> 44:44.180
challenge our kind of mathematical optimization techniques.

44:44.180 --> 44:47.020
That's I guess what I'm fully missing right now.

44:47.020 --> 44:53.580
Yeah, I guess that kind of underscores one of the, we are kind of starting to have this

44:53.580 --> 45:00.940
conversation often about the role of diversity, like on teams and helping create awareness

45:00.940 --> 45:07.940
of these kinds of issues and identify, you know, to help an organization be open to understanding

45:07.940 --> 45:11.620
where these issues might lie and it sounds like you're kind of underscoring that at the

45:11.620 --> 45:13.700
level of the community as well.

45:13.700 --> 45:14.700
Yeah, exactly.

45:14.700 --> 45:21.380
Also, you are right in our teams, you also need to have more diverse kind of opinions.

45:21.380 --> 45:26.380
And again, when we talk about diversity, it's not only about ethnicity or gender, it's

45:26.380 --> 45:29.940
also about having people from different backgrounds.

45:29.940 --> 45:34.260
Well, Paranas, thanks so much for taking the time to chat with me about the stuff, super

45:34.260 --> 45:37.900
interesting stuff and as always wonderful to catch up with you.

45:37.900 --> 45:41.620
It was a pleasure and I also really enjoyed the chat.

45:41.620 --> 45:44.900
It was as simple as Chang told me.

45:44.900 --> 45:54.980
Well, we had a good one on the topic of differential privacy, Chang and I and I would encourage

45:54.980 --> 46:01.940
folks who haven't listened to the former series on differential privacy, they should take

46:01.940 --> 46:04.180
a listen to that one because it's good stuff.

46:04.180 --> 46:05.180
Perfect.

46:05.180 --> 46:06.180
Thank you.

46:06.180 --> 46:12.940
All right, everyone, that's our show for today.

46:12.940 --> 46:19.300
For more information on Paranas or any of the topics covered in this show, visit twimmelai.com

46:19.300 --> 46:22.220
slash talk slash 208.

46:22.220 --> 46:26.340
Thanks once again to the good folks over at Georgian Partners for their sponsorship of this

46:26.340 --> 46:27.740
series.

46:27.740 --> 46:54.500
As always, thanks so much for listening and catch you next time.

