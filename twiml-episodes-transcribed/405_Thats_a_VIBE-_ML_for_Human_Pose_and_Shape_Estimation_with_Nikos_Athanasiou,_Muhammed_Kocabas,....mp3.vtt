WEBVTT

00:00.000 --> 00:15.920
Welcome to the Tumel AI Podcast, I'm your host Sam Charrington.

00:15.920 --> 00:23.560
Hey, what's up everyone?

00:23.560 --> 00:26.560
I hope you all had a wonderful weekend.

00:26.560 --> 00:31.920
As I mentioned last week, the popular causal modeling and machine learning course, hosted

00:31.920 --> 00:37.720
by Tumel, featuring instructor Robert O'Sezua Ness, is back.

00:37.720 --> 00:42.680
We've now held two sessions of this course and student feedback has been amazing.

00:42.680 --> 00:48.440
Students love the in-depth course content, assignments and capstone projects, of course,

00:48.440 --> 00:53.760
but the number one observation has steadily been about Robert's weekly study group sessions

00:53.760 --> 00:57.560
and the one-on-one office hours that students have with him.

00:57.560 --> 01:01.800
You can use this course as an intro to causal modeling by just spending a few hours a week

01:01.800 --> 01:06.120
watching the course videos and attending the study group session, or really dig into

01:06.120 --> 01:10.400
the materials and assignments and develop cutting-edge skills in the field.

01:10.400 --> 01:15.600
Now, no promises, but a couple of former students have gone on to write academic research

01:15.600 --> 01:20.880
papers with Robert based on the materials they learned in this course.

01:20.880 --> 01:26.120
This morning, Robert and I held a great session about causal modeling and the course.

01:26.120 --> 01:30.960
If you're interested in the topic of causal modeling or considering the course, I really

01:30.960 --> 01:38.440
encourage you to check it out by visiting our course page at twimmelai.com slash causal.

01:38.440 --> 01:44.000
As you all know, we are super excited about Twimmelfest, our upcoming AI festival that

01:44.000 --> 01:50.720
is jam-packed with engaging community-oriented and community-driven content.

01:50.720 --> 01:56.000
We are on the lookout for Twimmelfest hosts who will help us create the event.

01:56.000 --> 02:00.920
As a host, you'll get to share your ideas, experiences, expertise, and passions with

02:00.920 --> 02:05.840
the Twimmel community through a session that you propose and organize.

02:05.840 --> 02:11.240
So far, we've had a bunch of wonderful submissions, including an exploration into the impact

02:11.240 --> 02:17.280
that AI will have on the medical field, a quote-unquote rookie playbook for ML beginners,

02:17.280 --> 02:22.880
a code names challenge where AI-powered bots compete, and many more.

02:22.880 --> 02:27.600
There's still time to submit your session ideas by visiting twimmelfest.com, but time

02:27.600 --> 02:32.640
is running out, so put together those great ideas and hit submit.

02:32.640 --> 02:37.760
And even if you're not ready to host, be sure to register for Twimmelfest today.

02:37.760 --> 02:43.640
The event takes place October 13th through 30th is totally free and your registration ensures

02:43.640 --> 02:49.760
that you'll be among the very first to hear about important updates, giveaways, and announcements.

02:49.760 --> 02:53.920
And now on to the show.

02:53.920 --> 03:00.400
All right, everyone.

03:00.400 --> 03:07.280
I am on the line with Mohammed Kojabash, Nikos Athanasu, and Michael Black.

03:07.280 --> 03:14.840
They are with the Max Planck Institute for Intelligent Systems, and we're here to talk about their

03:14.840 --> 03:21.600
most recent paper, a paper called Vib that we will get into in more detail.

03:21.600 --> 03:25.560
But Mohammed, Nikos, and Michael, welcome to the Twimmel AI podcast.

03:25.560 --> 03:26.560
Thanks.

03:26.560 --> 03:28.600
Thanks for having us.

03:28.600 --> 03:29.600
Awesome.

03:29.600 --> 03:35.400
Let's start by having each of you share a little bit about your background and your role

03:35.400 --> 03:41.480
at the Max Planck Institute and what got you, what in your background led you to this work

03:41.480 --> 03:44.720
and what your general research interests are.

03:44.720 --> 03:46.720
Mohammed, why don't we start with you?

03:46.720 --> 03:47.720
Sure.

03:47.720 --> 03:49.040
So I am Mohammed Kojabash.

03:49.040 --> 03:55.160
I'm currently a PhD student in Max Planck Institute for Intelligent Systems in Perseving

03:55.160 --> 04:00.600
Systems Department, doing computer vision and graphics research.

04:00.600 --> 04:06.920
So I did my bachelor's in computer engineering in Istanbul Technical University in Turkey,

04:06.920 --> 04:11.600
and my master's in Middle East Technical University in Turkey again.

04:11.600 --> 04:19.360
So during my bachelor's, so I tried to explore different areas that is interesting for me,

04:19.360 --> 04:24.320
ranging from robotics and LP to computer vision and machine learning.

04:24.320 --> 04:30.120
I continued for a master's degree in computer vision and machine learning, and I started working

04:30.120 --> 04:38.360
on human process estimation, which has broader implications and application areas in the community.

04:38.360 --> 04:42.800
And yeah, and then I started doing a PhD again in the same topic.

04:42.800 --> 04:43.800
Cool.

04:43.800 --> 04:44.800
Cool.

04:44.800 --> 04:45.800
Thank you.

04:45.800 --> 04:46.800
Nikos.

04:46.800 --> 04:47.800
So hi.

04:47.800 --> 04:49.000
I'm Nikos Athanasio.

04:49.000 --> 04:57.480
So I'm also currently a PhD student at Max Planck Institute, I mean my first year.

04:57.480 --> 05:06.040
So a little bit about my background, I completed my bachelor's and master's degree in computer

05:06.040 --> 05:11.000
engineering in Greece in National Technical University.

05:11.000 --> 05:16.520
During the first year of my research, I started doing research basically on natural language

05:16.520 --> 05:24.280
processing, but then I wanted to combine natural language processing, machine learning with

05:24.280 --> 05:29.400
computer vision, so I started my PhD at Max Planck Institute.

05:29.400 --> 05:37.040
I'm currently working on motion, understanding the coding and how we can combine motion

05:37.040 --> 05:40.520
and actions with language semantics.

05:40.520 --> 05:47.200
I would like to combine machine learning and mathematical modeling with human interactions.

05:47.200 --> 05:53.480
I think that's the most interesting part of the machine learning research.

05:53.480 --> 05:58.440
And that's why I followed the PhD degree and started last September.

05:58.440 --> 05:59.440
Awesome.

05:59.440 --> 06:00.440
Awesome.

06:00.440 --> 06:01.440
Thank you.

06:01.440 --> 06:02.440
Michael.

06:02.440 --> 06:03.440
Well, I'm Michael Black.

06:03.440 --> 06:07.760
I'm a director at the Max Planck Institute for Intelligent Systems.

06:07.760 --> 06:14.520
And I've been working on problems in human motion estimation since the early 1990s.

06:14.520 --> 06:20.120
And I became interested in human motion because computers can't really understand us and

06:20.120 --> 06:25.880
be full partners with us until they understand our facial expressions, how we move, how we

06:25.880 --> 06:27.760
interact with the world.

06:27.760 --> 06:32.400
And I'm motivated by Aristotle who said to be ignorant of motion is to be ignorant of

06:32.400 --> 06:33.400
nature.

06:33.400 --> 06:37.880
And I think almost all my research focuses on how things move and trying to get computers

06:37.880 --> 06:40.080
to understand that motion.

06:40.080 --> 06:41.360
A little bit about my background.

06:41.360 --> 06:46.080
I've lived all over the place, educated in the U.S. and Canada.

06:46.080 --> 06:52.360
And before I came to Germany about nine years ago, I was a professor at Brown University.

06:52.360 --> 07:01.480
And I've been in Germany since 2011 and came here to co-found this new institute for intelligent

07:01.480 --> 07:07.320
systems that brings together scientists studying artificial intelligence and robotics and

07:07.320 --> 07:13.680
using techniques all the way from machine learning, like we're talking about here, to physical

07:13.680 --> 07:17.760
systems where they make new materials for soft robotics and so on.

07:17.760 --> 07:21.600
It's a very interdisciplinary and exciting field.

07:21.600 --> 07:28.400
So Mohammed, you mentioned that your area of interest is in pose estimation and affect

07:28.400 --> 07:35.320
the paper that we'll be talking about vibe, which is short for video inference for human

07:35.320 --> 07:43.440
body pose and shape estimation is kind of in the direction of advancing pose estimation.

07:43.440 --> 07:50.560
You share a little bit about kind of the broader landscape of pose estimation and where your

07:50.560 --> 07:52.800
paper is hoping to contribute.

07:52.800 --> 07:58.520
I think we've, you know, folks in the audience have probably seen various images that come

07:58.520 --> 08:06.840
from pose estimation papers and tools that show kind of like a stick figure superimposed

08:06.840 --> 08:09.280
on a picture of a body.

08:09.280 --> 08:14.440
What you're doing is a little different than that and that you're doing kind of 3D pose

08:14.440 --> 08:15.440
estimation.

08:15.440 --> 08:19.400
Talk a little bit about the broad landscape that your paper fits into.

08:19.400 --> 08:20.400
Sure.

08:20.400 --> 08:28.320
So basically the pose estimation is the task to estimate human pose from images, videos

08:28.320 --> 08:31.080
or any kind of sensory data.

08:31.080 --> 08:35.880
But in computer vision specifically, we tackle with the problem of estimating human pose

08:35.880 --> 08:39.440
from images and videos specifically.

08:39.440 --> 08:44.040
And we can define human pose in different kind of ways.

08:44.040 --> 08:49.040
Like as you mentioned, one of them would be to estimate some key points like these are

08:49.040 --> 08:56.400
some parts set of key points only that can be the joint locations or some lean locations

08:56.400 --> 09:04.160
of the human bodies or we can also try to estimate the whole body like in a more structured

09:04.160 --> 09:08.400
and misstructured manner like a human body mesh.

09:08.400 --> 09:13.520
So this kind of representations have some kind of a hierarchy.

09:13.520 --> 09:19.920
So a sparse set of key points are really lacking to explain the real human body.

09:19.920 --> 09:27.040
So and also we can estimate the 2D or 3D sparse set of key points.

09:27.040 --> 09:32.360
But when we proceed to more higher richer representations like human body meshes which

09:32.360 --> 09:39.560
can define the whole body like the whole pose and shape, which has some bleacher information

09:39.560 --> 09:41.920
that explains the human body.

09:41.920 --> 09:49.680
So in this paper specifically, we try to estimate the whole body mesh, which is more explanatory

09:49.680 --> 09:53.280
than on the sparse set of key points.

09:53.280 --> 10:00.760
And also we tackle the problem in 3D space, which makes the problem more difficult to estimate

10:00.760 --> 10:03.760
from the images or videos only.

10:03.760 --> 10:04.760
Right.

10:04.760 --> 10:11.760
And Nika, you mentioned that one of your interests is in applying vision to motion.

10:11.760 --> 10:17.680
Can you talk a little bit about the motion aspect of this and where that comes in?

10:17.680 --> 10:22.000
So we are aiming directly to capture motion.

10:22.000 --> 10:29.880
So we are starting from a plane video, which are multiple images, the sequence of images.

10:29.880 --> 10:37.320
And from those images that we have, we detect the person in the video and we estimate his

10:37.320 --> 10:40.040
or her 3D pose and shape.

10:40.040 --> 10:47.480
So many methods have looked at this problem, but the vast majority focus on single images.

10:47.480 --> 10:53.680
So estimating the human pose in one image and a video is just a sequence of single images.

10:53.680 --> 10:57.560
So you could just apply those techniques one frame at a time.

10:57.560 --> 11:03.520
If you do that, you tend to get a jerky reconstruction that's not moving smoothly and naturally

11:03.520 --> 11:05.920
like a human would.

11:05.920 --> 11:11.320
And so one of the observations here is there's more information in the video sequence in

11:11.320 --> 11:13.120
the continuity of the motion.

11:13.120 --> 11:19.760
And if we can train the computer to exploit that by teaching it what it is to move like

11:19.760 --> 11:25.160
a human, then we can exploit information across a long video sequence and get better results

11:25.160 --> 11:28.680
and you would get by doing it frame by frame.

11:28.680 --> 11:34.640
And as an example of the better nature of the results, you've got a great picture at

11:34.640 --> 11:42.080
the beginning of the paper that shows some areas where the current state of the art method

11:42.080 --> 11:48.160
in a challenging video doesn't seem to track very well, whereas your method performs

11:48.160 --> 11:49.400
better.

11:49.400 --> 11:56.080
What is that kind of reference method that you looked at and what's the data set that

11:56.080 --> 11:59.720
you are building your model with?

11:59.720 --> 12:05.720
Well, the competing method is from one of our very good friends and colleagues, Anju Kanazawa,

12:05.720 --> 12:09.240
who actually did an internship in our institute.

12:09.240 --> 12:13.680
So it's a friendly competition for who's going to have the state of the art.

12:13.680 --> 12:18.400
Anju's amazing and she's a professor at Berkeley.

12:18.400 --> 12:25.600
She also extended one of these single frame methods over time, but in a different way.

12:25.600 --> 12:32.040
And one of the things that distinguishes our approach was actually inspired by something

12:32.040 --> 12:39.440
she did earlier, which is to use a discriminator that knows something about, it's trained

12:39.440 --> 12:44.600
to distinguish between fake human motions and real human motions.

12:44.600 --> 12:50.680
And to enable that, we exploited a data set that we made public back in the fall called

12:50.680 --> 12:51.680
a mass.

12:51.680 --> 12:59.280
And a mass is a data set of about 45 hours worth of human motion capture data that captures

12:59.280 --> 13:03.680
a wide range of how people move.

13:03.680 --> 13:09.000
And then we used that to teach the computer basically what is a good human motion and

13:09.000 --> 13:10.400
what isn't.

13:10.400 --> 13:15.200
And when you say human motion capture, is this kind of regular video or is this the kind

13:15.200 --> 13:20.760
of thing that we will often see with humans in like white suits with like balls around

13:20.760 --> 13:21.760
them?

13:21.760 --> 13:22.760
Yeah, the latter.

13:22.760 --> 13:29.440
There are many, this is commercial motion capture data with markers and infrared reflective

13:29.440 --> 13:31.520
lights and so on.

13:31.520 --> 13:34.320
So very much a lab setup.

13:34.320 --> 13:39.400
And many groups around the world have made small amounts of motion capture available.

13:39.400 --> 13:46.400
Every data set though is in a different format and researchers were struggling to get enough

13:46.400 --> 13:48.920
data to train deep neural networks.

13:48.920 --> 13:53.680
As we all know, deep networks are great and they could do a wonderful job, but they need

13:53.680 --> 13:56.360
a lot of data, they're data hungry.

13:56.360 --> 14:02.280
And so what we did was pool together a dozen different data sets and put them all into

14:02.280 --> 14:09.320
a single format, this representation of the human body we use called simple SMPL.

14:09.320 --> 14:17.120
And by unifying them all, it provides enough training data to really do something bigger.

14:17.120 --> 14:22.760
So Mohammed, you tell us a little bit about your kind of first steps in tackling this

14:22.760 --> 14:23.760
problem.

14:23.760 --> 14:24.760
Sure.

14:24.760 --> 14:29.880
So the first step in general is to create a baseline for your method.

14:29.880 --> 14:36.720
So what we did in the first step is to create a model that can only predict some pause

14:36.720 --> 14:43.520
and shape from a video without using any intricate or complicated architecture.

14:43.520 --> 14:49.880
So at that time, the first thing we tried, so there is a method called human mesh recovery,

14:49.880 --> 14:54.080
which estimates a pause and shape from only a single image.

14:54.080 --> 15:01.000
And we tried to extend this human mesh recovery to estimate a sequence of pauses given a

15:01.000 --> 15:04.800
video using a recurrent neural network architecture.

15:04.800 --> 15:11.720
So a recurrent neural network is basically a type of neural network that lets you integrate

15:11.720 --> 15:17.520
some information from past frames or past data points.

15:17.520 --> 15:24.280
So we take each output of CNNs from human mesh recovery method.

15:24.280 --> 15:31.000
And we try to estimate the sequence of pauses from this recurrent neural network architecture.

15:31.000 --> 15:36.440
This was the first thing that we tried to see how the model performs with the available

15:36.440 --> 15:37.960
data.

15:37.960 --> 15:45.440
And in the next step, we tried to integrate Amaz using a genitive error cell network approach.

15:45.440 --> 15:52.320
So imagine we have this baseline method that produces some noisy or inaccurate estimations

15:52.320 --> 15:54.640
of the motion happening.

15:54.640 --> 16:01.240
And if we integrate a discriminator to the training stage, which tells if the predicted

16:01.240 --> 16:09.640
motion is plausible or similar to real life motion, then we can supervise this baseline

16:09.640 --> 16:14.000
method to produce better looking motions.

16:14.000 --> 16:18.960
And this is what we did in the final stages, where we introduced Amaz data to set and

16:18.960 --> 16:23.800
the discriminator, which supervises this baseline generator we had.

16:23.800 --> 16:28.000
So let's go back to the initial stage.

16:28.000 --> 16:34.160
You've got a CNN whose output you're feeding into an RNN.

16:34.160 --> 16:37.200
What part of the CNN are you feeding into the RNN?

16:37.200 --> 16:43.760
Is it kind of the output of a final classifier stage or is it some earlier layer in the

16:43.760 --> 16:48.280
CNN that you're using as input to your RNN?

16:48.280 --> 16:52.760
So let me first summarize the human mesh recovery architecture.

16:52.760 --> 16:59.160
So we have a CNN that extracts the image features and then a regressor, which is a couple

16:59.160 --> 17:03.440
of NLP layers that estimates simple body pose and shape parameters.

17:03.440 --> 17:07.960
So the CNN part is fully convolution, a fully convolutional model.

17:07.960 --> 17:13.840
And so we get rid of the regressor part to train the wipe and we only use the CNN part

17:13.840 --> 17:17.280
to extract the image features of the paper frame.

17:17.280 --> 17:22.000
And we feed these image features to recurrent neural network we have.

17:22.000 --> 17:29.120
And you talked about the discriminator is the discriminator trained separately or kind

17:29.120 --> 17:33.240
of end to end as part of training the CNN.

17:33.240 --> 17:38.480
Yeah, so actually the generator generates some predictions and we train the whole thing

17:38.480 --> 17:39.800
end to end.

17:39.800 --> 17:47.840
So the discriminator takes as inputs the generators predictions of the pose and shape and it takes

17:47.840 --> 17:50.000
also samples from a mass.

17:50.000 --> 17:57.160
So a mass is exactly what Michael previously explained as the unified motion dataset.

17:57.160 --> 18:03.840
So we use the discriminator to validate if the sequence of poses that we have is plausible.

18:03.840 --> 18:08.680
It can actually be a real human motion.

18:08.680 --> 18:15.640
So the discriminator's job is by taking those two different pose sequences to compare

18:15.640 --> 18:20.640
them and refine the generators output sequence.

18:20.640 --> 18:25.960
So we train this whole thing end to end and the discriminators takes the real fake samples

18:25.960 --> 18:28.600
from a mass and that was generator.

18:28.600 --> 18:34.000
When I think about the in this picture that you created for me Michael with the folks

18:34.000 --> 18:41.880
and suits with the balls that process and those images, I think of those as kind of fairly

18:41.880 --> 18:48.000
sparse points on the body that are being captured and yet these models are, you know, they

18:48.000 --> 18:53.400
look to be like fairly high, you know, resolution, fairly fine grained.

18:53.400 --> 19:02.240
How does the model go from kind of this sparse motion capture to more robust looking mesh?

19:02.240 --> 19:05.440
Yeah, great question.

19:05.440 --> 19:06.640
That's a whole nother paper.

19:06.640 --> 19:13.880
So indeed, when you capture a motion capture lab, you've got a bunch of sparse markers.

19:13.880 --> 19:21.040
And then the traditional method solves for a skeleton that explains those markers.

19:21.040 --> 19:25.960
Now we replace that traditional solve with a different kind of solve.

19:25.960 --> 19:29.160
We actually take our 3D body model.

19:29.160 --> 19:34.280
This is the simple body model and fit it to the marker data in a using a process called

19:34.280 --> 19:37.600
Mosh for motion and shape capture.

19:37.600 --> 19:43.720
And this was a paper that appeared at Cigraf or Cigraf Asia a few years ago and does a really

19:43.720 --> 19:49.880
good job of taking sparse motion capture markers and fitting the most likely body surface

19:49.880 --> 19:51.880
that could explain them.

19:51.880 --> 19:59.960
It turns out that from 20 to 60 sparse markers on the body, you can really figure out what

19:59.960 --> 20:01.080
a person looks like.

20:01.080 --> 20:05.840
You could think about it as a very, very sparse 3D scan of the person.

20:05.840 --> 20:11.680
And with a powerful statistical model of the body, it's about how body shape varies and

20:11.680 --> 20:17.080
how pose varies, you can fit a sequence of these very sparse markers and get out this

20:17.080 --> 20:21.800
kind of realistic detail that we can then use to train other methods.

20:21.800 --> 20:28.720
Maybe, Mohammed, you can help me get to the like the core element of this paper that

20:28.720 --> 20:33.120
helps it perform beyond the previous method.

20:33.120 --> 20:39.400
What's kind of that kernel of contribution here, innovation here that helps get to that

20:39.400 --> 20:43.680
incremental performance and how did you arrive at that?

20:43.680 --> 20:47.960
Was that obvious setting out at this project and you just kind of put the pieces in place

20:47.960 --> 20:53.800
and it all worked or was there a evolutionary process here that helped you arrive at the

20:53.800 --> 20:55.680
final architecture?

20:55.680 --> 21:02.360
So the major contribution in this paper is this motion discriminator, which tells if the

21:02.360 --> 21:05.480
predicted sequences are fake or real.

21:05.480 --> 21:12.480
So this is the major contribution we have and this is the one that we get the most improvement

21:12.480 --> 21:15.600
in the results in the results.

21:15.600 --> 21:23.680
So actually, it is quite, so from the human mesh recovery paper from Anuju Kanazawa, we

21:23.680 --> 21:30.760
already know that using this kind of a discriminator for single image pose estimation helps a lot

21:30.760 --> 21:33.680
to have plausible bodies.

21:33.680 --> 21:40.080
So and we thought that intuitively this should help for a sequence of poses.

21:40.080 --> 21:45.280
And we already know that Amaz is a very large scale data set, which has different kind

21:45.280 --> 21:51.360
of motions, which happens in real time and performed by the real human actors.

21:51.360 --> 21:57.960
And this intuitive reasoning led us to create this architecture.

21:57.960 --> 22:04.800
So yeah, during the development phase, we had a lot of technical difficulties and we just

22:04.800 --> 22:11.280
get through them by doing like fine-grained experiments on which part improves, which

22:11.280 --> 22:14.040
is led us to architecture we have.

22:14.040 --> 22:19.400
What were some of those technical difficulties and experiments that helped you overcome them?

22:19.400 --> 22:27.080
Yeah, for example, tuning the, we have this discriminator and generator and we have this

22:27.080 --> 22:34.960
discriminator losses that tells if this sequence is real or fake and also some other losses

22:34.960 --> 22:42.480
that tells the 2D joint locations and 3D joint locations of a given input video.

22:42.480 --> 22:51.760
So we had to tune these 2 kind of losses to every on an optimal solution.

22:51.760 --> 22:56.600
This was one of the things that we really take.

22:56.600 --> 23:01.800
Maybe it's worth telling the audience also that one of the problems here is we want to

23:01.800 --> 23:08.480
get 3D human poses in detail and yet there's not much ground truth training data where

23:08.480 --> 23:16.800
you have video sequences with the true 3D human pose in correspondence.

23:16.800 --> 23:23.600
And there are large data sets of labeled 2D human poses, but not 3D.

23:23.600 --> 23:30.600
And so this is one way to combine sort of weak 2D information with very strong 3D information.

23:30.600 --> 23:31.800
They're not paired.

23:31.800 --> 23:34.080
So a mass doesn't have any images.

23:34.080 --> 23:36.560
It's not associated with any real videos.

23:36.560 --> 23:41.320
But the discriminator learns what it is to be a motion and then the generator learns

23:41.320 --> 23:44.880
how to go from images to 3D.

23:44.880 --> 23:46.360
Yeah, exactly.

23:46.360 --> 23:52.560
Actually, that's what the high overview of the paper is we get the 2D data.

23:52.560 --> 23:59.320
We output some pose sequences and then we use that huge data set a mass that has various

23:59.320 --> 24:05.560
motions to refine those predictions to be able to not, between the two sequences, one

24:05.560 --> 24:13.640
from a mass and the hour sequence, our generated sequence, the discriminator ideally should

24:13.640 --> 24:15.960
not distinguish between those two.

24:15.960 --> 24:22.920
So the discriminator should be 50% sure about which one is the real and which one is the

24:22.920 --> 24:23.920
fake.

24:23.920 --> 24:24.920
And that's a great point.

24:24.920 --> 24:30.560
So the discriminator is operating purely in the motion domain.

24:30.560 --> 24:36.800
It doesn't know anything about images or the frames of the video.

24:36.800 --> 24:42.120
It's just answering the question, does this seem like the kind of motion that the human

24:42.120 --> 24:49.160
body does based on the mass data set and the CNN and the other parts of the architecture

24:49.160 --> 24:54.360
are more focused on extracting motion from the video frames?

24:54.360 --> 24:57.000
Yeah, exactly.

24:57.000 --> 24:58.000
Cool.

24:58.000 --> 25:07.400
And so I think one of the points that you make early on in the paper is that the motion

25:07.400 --> 25:12.440
that, and even the examples that you give, it's not like normal motion.

25:12.440 --> 25:21.200
It's kind of complex motion that is not something that you see every day is that was the

25:21.200 --> 25:30.840
mass data set kind of curated to identify and include lots of examples of complex motion

25:30.840 --> 25:36.160
in it or is there something in the process that is kind of extrapolating to that kind

25:36.160 --> 25:37.360
of complex motion?

25:37.360 --> 25:38.360
Okay.

25:38.360 --> 25:40.160
It's a good question.

25:40.160 --> 25:41.680
There's a mixture in there.

25:41.680 --> 25:44.240
There's a lot of mundane things like walking.

25:44.240 --> 25:48.760
It seems like in motion capture labs, people capture a lot of walking.

25:48.760 --> 25:54.200
So, but there are some data sets that are a little bit more extreme, have more interesting

25:54.200 --> 25:55.200
poses.

25:55.200 --> 26:01.440
And in fact, we captured something we call extreme poses, which we hired some gymnasts

26:01.440 --> 26:07.640
to come in and do basically the wildest things that the human body can do, just to really

26:07.640 --> 26:10.800
flesh out the space of what's possible.

26:10.800 --> 26:14.920
And there's one other thing that happens in motion capture a lot for video games, people

26:14.920 --> 26:22.520
often capture something like kicking a ball and turning left or turning right in very discreet

26:22.520 --> 26:24.320
movements.

26:24.320 --> 26:28.440
And so we have something called a transition's data set in there as well that captures

26:28.440 --> 26:32.280
people doing one movement and then transitioning to another movement.

26:32.280 --> 26:38.480
So this is again trying to expand these transitions are things that you might not think to capture

26:38.480 --> 26:42.080
on your own, but are really part of the natural human behavior.

26:42.080 --> 26:49.480
So there's a mixture in there and it's rich enough for this task.

26:49.480 --> 26:54.720
Are there specific things that Mohamed Arniko having worked with this data set for this

26:54.720 --> 27:02.600
particular task jump out at you as, you know, I wish the data set had more of, you know,

27:02.600 --> 27:07.840
kind of category X because it would help you build more robust models?

27:07.840 --> 27:15.440
Yeah, basically that a mass data set has almost all of the available mocap data by different

27:15.440 --> 27:21.280
labs captured and in different conditions, but it doesn't have a lot of in the wild data.

27:21.280 --> 27:27.640
That's the thing that it's missing the extreme poses and the in the wild kind of data.

27:27.640 --> 27:37.280
So someone doing extreme gymnastic exercises or having like motion without a constant pace

27:37.280 --> 27:43.920
and having different poses and like opening the hands or the feet extremely quickly or

27:43.920 --> 27:52.000
slowly or I mean, we could always benefit from more variation of extreme and quick or fast

27:52.000 --> 27:53.000
motions.

27:53.000 --> 27:54.000
Yeah.

27:54.000 --> 27:59.160
Now that we are starting to have, you know, I'm thinking like sporting events that are

27:59.160 --> 28:05.720
captured with many cameras from lots of different angles, does that ever able to take the

28:05.720 --> 28:11.920
place of motion capture types of information or are we not kind of sophisticated enough

28:11.920 --> 28:19.320
to correlate all of that and and produce the kind of data sets that kind of traditional

28:19.320 --> 28:21.320
motion capture is able to provide.

28:21.320 --> 28:23.760
Have you looked at that Michael at all?

28:23.760 --> 28:29.040
Yeah, so it's a let's come back to this idea that, you know, traditional motion captures

28:29.040 --> 28:34.040
a bunch of sparse points on the body, not very detailed.

28:34.040 --> 28:39.320
With the images, we've got thousands, hundreds of thousands of points on the body.

28:39.320 --> 28:43.640
It just happened to be in 2D, but they are much richer in some sense.

28:43.640 --> 28:49.200
We have all this facial expression information, details of the hands, subtle, you know,

28:49.200 --> 28:51.320
subtle motions of the body breathing and so on.

28:51.320 --> 28:53.520
You can see all that in a video.

28:53.520 --> 29:00.880
So our hope is, you know, one day that motion capture from video will be more accurate

29:00.880 --> 29:06.000
and more detailed than motion capture from a traditional mocap system today.

29:06.000 --> 29:07.000
We're not there yet.

29:07.000 --> 29:09.240
This is a step in that direction.

29:09.240 --> 29:13.280
I think your intuition about combining multiple views is a good one and it's something

29:13.280 --> 29:18.080
that we're we've looked at in the past and continue to look at.

29:18.080 --> 29:25.680
Can we combine multiple 2D views of a person and get, you know, really high quality output?

29:25.680 --> 29:26.920
I think the answer is yes.

29:26.920 --> 29:33.480
I haven't seen a system yet that competes with the most accurate mocap systems.

29:33.480 --> 29:37.840
But then the most accurate mocap systems, you know, cost hundreds of thousands of dollars

29:37.840 --> 29:40.440
and have dozens of cameras.

29:40.440 --> 29:48.000
I think if you built a video based mocap system of sort of that sophistication, I think

29:48.000 --> 29:50.120
we might be getting close.

29:50.120 --> 29:54.760
And does light are types of systems or like what we've seen with the connect, does that

29:54.760 --> 29:56.360
play into this as well?

29:56.360 --> 29:57.360
Yeah.

29:57.360 --> 29:58.360
Absolutely.

29:58.360 --> 29:59.360
We also work with connect.

29:59.360 --> 30:06.200
The new Microsoft connect for Azure is allows you to put multiple connects together.

30:06.200 --> 30:09.120
So you can actually have several of them looking at a person.

30:09.120 --> 30:13.760
That's a really promising direction for a lightweight, easy to set up system.

30:13.760 --> 30:19.720
We don't have anything to show yet, but yeah, cool, cool.

30:19.720 --> 30:26.280
So Muhammad again, kind of going back to this opening picture in the paper, you know, it's

30:26.280 --> 30:33.720
easy to see where your model is outperforming the traditional model and kind of eyeball the

30:33.720 --> 30:34.720
delta.

30:34.720 --> 30:42.160
How do you evaluate that performance more concretely and mathematically in the paper?

30:42.160 --> 30:48.400
So we have several datasets that has ground through 3D joint positions.

30:48.400 --> 30:54.880
And one of them is a dataset called 3D pauses in the wild again, that is developed in our

30:54.880 --> 30:57.240
group in person systems group.

30:57.240 --> 31:02.440
So this dataset is captured in the wild.

31:02.440 --> 31:09.120
And there are some subjects varying some I am I am you sensors on their body.

31:09.120 --> 31:12.960
And so this I am you sensors can be placed under the cloth.

31:12.960 --> 31:17.160
So it doesn't affect the image quality.

31:17.160 --> 31:22.640
And so you can get the 3D joint positions from those I am you sensors.

31:22.640 --> 31:29.920
And we the major that the most prominent dataset we try is this 3D PW 3D people in the wild

31:29.920 --> 31:31.400
dataset.

31:31.400 --> 31:39.040
And so basically what we do is we take the sequences from the dataset, produces our results,

31:39.040 --> 31:44.560
and also the other methods results and compares what is the distance between the ground through

31:44.560 --> 31:49.080
the joint positions and the predicted ground to the joint positions.

31:49.080 --> 31:54.840
And also there are some other indoor 3D datasets we also use.

31:54.840 --> 32:01.240
One of them is called human 3.6M and the other one is called MPI informatics 3D human

32:01.240 --> 32:03.320
position data set.

32:03.320 --> 32:09.520
The 3D poses in the wild dataset is that you mentioned it uses a particular kind of

32:09.520 --> 32:15.680
sensor is that sensor and the kind of sparsity of that sensor similar to what you see in

32:15.680 --> 32:23.760
a mocap system or is it you know somehow more more dense I'm curious about the relationship

32:23.760 --> 32:31.720
between the primary way your comparing performance is based on kind of these sparse representations

32:31.720 --> 32:39.320
I wonder if that leaves something to be desired or an opportunity in terms of the

32:39.320 --> 32:45.240
ability to kind of capture what's happening in complex motions with these sparse data

32:45.240 --> 32:46.560
points on the body.

32:46.560 --> 32:48.240
It's a good question.

32:48.240 --> 32:54.400
So we use their commercial mocap suit called it from accents uses these inertial measurement

32:54.400 --> 32:55.400
units.

32:55.400 --> 33:01.680
I think there's 10 or 12 on the body, but we don't just use that.

33:01.680 --> 33:09.040
We also use a camera tracking the people around and we use 2D measurements about the joint

33:09.040 --> 33:17.480
locations in those images to precisely align the estimated 3D body pose with the images.

33:17.480 --> 33:26.320
And so we have a technique that fits again our simple SMPL body model to these multiple

33:26.320 --> 33:30.240
kinds of measurements the IMU measurements and the 2D measurements.

33:30.240 --> 33:35.160
We also have to solve for the camera translation and rotation and things like that.

33:35.160 --> 33:39.920
But putting this all together gives reasonable we don't call it ground truth we call it reference

33:39.920 --> 33:40.920
data.

33:40.920 --> 33:45.720
We're a little careful about that you know it's not the same level as a motion capture

33:45.720 --> 33:47.440
lab.

33:47.440 --> 33:52.440
But you get the benefit of being outside in in the wild with all the complex lighting

33:52.440 --> 33:58.160
and occlusion and things that go on in you know in natural scenarios.

33:58.160 --> 33:59.160
Okay.

33:59.160 --> 34:03.520
It's a bit of a trade off it's the holy grail is to have perfect ground truth where you

34:03.520 --> 34:09.880
know every single bit of the motion with no crazy sensors to get in the way outdoors in

34:09.880 --> 34:14.200
the wild but it you know it doesn't exist.

34:14.200 --> 34:19.680
So mom and mentioned earlier you know this process of kind of experimenting and identifying

34:19.680 --> 34:24.760
you know things that didn't work with particular emphasis on the discriminator Nika someone

34:24.760 --> 34:30.000
ring you know from your perspective you know if you can share another example of something

34:30.000 --> 34:36.120
that maybe thought was gonna work and didn't and how you had to adjust the approach to accommodate

34:36.120 --> 34:37.120
it.

34:37.120 --> 34:44.440
Yeah the experimentation procedure was was tough but yeah there are some a few points

34:44.440 --> 34:52.440
so we tried one minor point was that we tried the difficult CNN feature extractors and

34:52.440 --> 34:58.520
that was a difficult part to experiment with but we quickly converts to an optimal one

34:58.520 --> 35:07.320
and the other one is that we decided to use a self attention mechanism which is like

35:07.320 --> 35:13.600
a major kind of a major contribution because actually the self attention mechanism is not

35:13.600 --> 35:19.840
so commonly used in 3D modeling or modeling of human motion so we use the self attention

35:19.840 --> 35:28.480
mechanism to get even better results on the discriminator part which is basically imagine

35:28.480 --> 35:34.840
that we are processing the human motion sequence and we are extracting some features using

35:34.840 --> 35:40.520
2G or U layers which is a type of very current neural network as Mohamed said before and

35:40.520 --> 35:46.040
instead of hard pulling or combining these features statically we are using a weighted

35:46.040 --> 35:53.880
schema to combine those two those features so the features that we have for every pose

35:53.880 --> 36:00.240
in the sequence we combine them as a weighted sum in order to amplify the contribution

36:00.240 --> 36:07.040
of most important frames of the sequence more for the discriminator's performance to improve

36:07.040 --> 36:14.680
so that's what was too key technical difficulties that we have.

36:14.680 --> 36:22.240
Were you able to develop any kind of intuition over why that worked and what the attention

36:22.240 --> 36:26.080
mechanism ended up attending over?

36:26.080 --> 36:33.320
We have made a population experiments that we tested the self attention mechanism so

36:33.320 --> 36:39.200
we tried two different things so instead of using self attention we trying pulling the

36:39.200 --> 36:46.680
features in a static way so concatenating the average and the max of the features and

36:46.680 --> 36:53.120
using that for the discriminator to decide whether the motion is fake or real and we

36:53.120 --> 37:00.280
compared with that case with our case and the performance seemed to improve.

37:00.280 --> 37:06.400
It didn't improve in such a margin that the results were extremely visible but for sure

37:06.400 --> 37:12.440
attention pointed out the frames that they should be corrected in the pose sequence and

37:12.440 --> 37:15.240
that they were the more plausible ones.

37:15.240 --> 37:23.520
I think about the various pieces of the model that you've built here, you've got CNN,

37:23.520 --> 37:31.160
you've got Narin and you've got this attention mechanism, you mentioned VAE in the paper.

37:31.160 --> 37:37.000
It seems like a lot of moving pieces and I'm wondering how that impacted the process

37:37.000 --> 37:44.640
and also from a computational perspective if the training runs were very expensive.

37:44.640 --> 37:48.280
You're also working with video which is a lot of data.

37:48.280 --> 37:55.480
How was the process of working on this from a just computational perspective?

37:55.480 --> 37:59.680
Did that have a lot of impact on the way you approach things, Mohammed?

37:59.680 --> 38:06.080
So surprisingly the model we have both in training time and testing time is quite lightweight

38:06.080 --> 38:09.200
even though we have lots of components.

38:09.200 --> 38:14.320
So the heaviest part in our model is the CNN future extraction part.

38:14.320 --> 38:20.400
And what we do during training, we pre-compure the CNN features and we don't update this

38:20.400 --> 38:26.520
CNN module and we only train the recurrent neural network and since the number of parameters

38:26.520 --> 38:36.200
is quite low with the simple body representation, our neural network is quite lightweight.

38:36.200 --> 38:46.440
And we don't use very deep layers, only a couple of a stack of GRU layers is enough for us

38:46.440 --> 38:50.960
and it wasn't that difficult for us to train the model.

38:50.960 --> 38:57.520
So actually during training time for example, like in half an hour or in one hour we can

38:57.520 --> 39:04.600
converge to a good solution since we use this already pre-computed CNN features.

39:04.600 --> 39:10.760
And during training time we also get rid of the discriminator and we only use the generator

39:10.760 --> 39:15.120
part and it makes the testing much faster.

39:15.120 --> 39:20.160
Okay and then what about from an inference perspective, how expensive is that?

39:20.160 --> 39:27.720
Again, it is not quite expensive since we only have the CNN which is resident 50 in our

39:27.720 --> 39:34.080
case, which is again lightweight and it can run in real time in GPUs.

39:34.080 --> 39:38.800
And the GRU layers we add, it is not again that expensive.

39:38.800 --> 39:47.480
So in a commercial GPU, we can get almost real time speeds during the experience.

39:47.480 --> 39:48.480
Very cool.

39:48.480 --> 39:56.320
Maybe start to wrap things up, I love for each of you to kind of put this paper and

39:56.320 --> 40:00.920
maybe talk about the things that you're either your top lessons learned or the thing

40:00.920 --> 40:04.640
that you're most excited about with this paper and its contributions and kind of how

40:04.640 --> 40:09.040
you see it moving forward in your own research.

40:09.040 --> 40:11.040
Nikos, do you want to start us off with that?

40:11.040 --> 40:12.480
Yeah, I can start.

40:12.480 --> 40:19.880
So I joined the MBI in September and I didn't know a lot of things for 3D human modeling.

40:19.880 --> 40:30.880
So and this paper is a very good first experience and I think one of the most important

40:30.880 --> 40:34.040
features of the paper is that it is clear.

40:34.040 --> 40:43.080
I mean the contributions are clear, we use a lot of unpaired 3D data to refine 2D predictions

40:43.080 --> 40:50.880
actually to the predictions, actually 3D meshes that were predicted mostly from 2D key

40:50.880 --> 40:52.680
points.

40:52.680 --> 41:03.040
And our implementation is converges fast, it can create high quality and state of the

41:03.040 --> 41:12.080
art, the predictions are high quality and state of the art.

41:12.080 --> 41:21.960
And also one important thing is that we've made all the code and data publicly available

41:21.960 --> 41:29.480
so we have detailed instructions for everyone so it's pretty easy for anyone if you want

41:29.480 --> 41:37.080
or see wants to run and use our model and methods, we have clear instructions to do

41:37.080 --> 41:38.080
it.

41:38.080 --> 41:39.080
Great.

41:39.080 --> 41:40.080
Mohammed?

41:40.080 --> 41:46.200
So it seems that why we're working quite well on the restricted data sets for evaluation

41:46.200 --> 41:53.840
we have, but in real life or in the wild videos there are a lot of difficult situations

41:53.840 --> 42:00.960
and a human motion is quite difficult to model because humans are capable of doing lots

42:00.960 --> 42:08.320
of different kind of movements and also in real life situations we have like objects

42:08.320 --> 42:14.920
and scenes around us, we have occlusions caused by again objects or other people.

42:14.920 --> 42:21.680
These kind of problems are still waiting to be tackled with and also we show with

42:21.680 --> 42:33.680
wipe, we cannot perform well in these kind of settings and in the future there are lots

42:33.680 --> 42:40.200
of things that we have to do to model human motion and also track and capture human motion

42:40.200 --> 42:45.560
from videos and this is what I learned in the process even though we have the state of

42:45.560 --> 42:50.680
the art model we cannot solve even some basic scenarios still.

42:50.680 --> 42:51.680
Great.

42:51.680 --> 42:52.680
Thank you.

42:52.680 --> 42:53.680
Michael?

42:53.680 --> 42:59.680
Well, I'm really excited about this paper and we're taking this but I'm always thinking

42:59.680 --> 43:07.800
about the next thing and humans don't move for no reason, they don't move in a vacuum,

43:07.800 --> 43:13.480
they move in the 3D world to solve tasks and to change that world to have an influence

43:13.480 --> 43:14.480
on it.

43:14.480 --> 43:17.880
You cross the room to open the door, you open the door to move through it, you move

43:17.880 --> 43:22.800
through it to get to the next room, you're solving tasks all the time and in solving those

43:22.800 --> 43:26.520
tasks you're interacting with the environment.

43:26.520 --> 43:32.680
Even if you're interacting with another person you're moving your face and you're gesturing

43:32.680 --> 43:37.720
in ways that try to have an influence on the other person, maybe without any contact

43:37.720 --> 43:44.640
but you're still using your body to influence people or the world and this work at the

43:44.640 --> 43:47.360
moment just treats the body in isolation.

43:47.360 --> 43:52.720
The body isn't embedded in the environment, the CNN doesn't really know anything about

43:52.720 --> 43:56.960
the environment, it's a single person not interacting with other people.

43:56.960 --> 44:01.600
I think there's a tremendous amount we still have to do to get computers to really understand

44:01.600 --> 44:09.400
us and those next steps will be about human-human interaction and about human-object-human-seeing

44:09.400 --> 44:10.400
interaction.

44:10.400 --> 44:11.400
Great.

44:11.400 --> 44:18.440
Well, Mohamed Niko's, Michael, thanks so much for taking the time to share with us your

44:18.440 --> 44:20.840
paper and what you're up to.

44:20.840 --> 44:21.840
Thanks a lot.

44:21.840 --> 44:22.840
Thanks.

44:22.840 --> 44:28.520
All right, everyone, that's our show for today.

44:28.520 --> 44:34.480
To learn more about today's guest or the topics mentioned in this interview, visit twimmolai.com.

44:34.480 --> 44:39.480
Of course, if you like what you hear on the podcast, please subscribe, rate, and review

44:39.480 --> 44:42.200
the show on your favorite pod catcher.

44:42.200 --> 45:08.960
Thanks so much for listening and catch you next time.

