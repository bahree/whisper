All right, everyone, I am here with Joseph Soriaga.
Joseph is Senior Director of Technology at Qualcomm.
Joseph, welcome to the Twinmore AI podcast.
Great. Yeah. Thank you for having me. It's great to be here.
Looking forward to jumping into our conversation.
We'll be talking about a couple of very interesting papers that you and
your team have that will be published later in the year at Globecom.
Before we dig into those, though, I'd love to have you share a bit about your
background. Tell us how you came to work in the field of machine learning.
Yeah, sure. I'd be happy to. So I've actually been working for a while and
wireless. And when I started, I actually started in coding and
information theory a while ago, even and well before in the deep learning revolution.
But there were some interesting parallels, I'd say, with machine learning.
Because at the time, it was really exciting where belief propagation was
very popular. And it was essentially being used to achieve shanty capacity
and a lot of different use cases. And the trick was trying to find the right
graphical model for the particular problem to achieve that.
This is coming from TurboCodes and rediscovered LDPC.
Another nice parallel was that our kind of community was getting used to
and embracing a lot of like experiment-driven research.
Like really extensive coding up of experiments to prove something could work.
Because it was very difficult to prove rigorously and mathematically that
this belief propagation would achieve shanty capacity.
But through a lot of extensive experiments, you could go and simulate it and
design things and then for all practical purposes show that you were achieving it.
There were a lot of relationships to machine learning of belief propagation at
the time, but I was really more on the communication side.
And then coming out of school, my PhD, I joined a Qualcomm corporate R&D.
And that was where I was really excited and exposed to this industrial
research. And one of the great things about Qualcomm is that they not only
invest substantially in research, but it's a very open, collaborative environment
where we would really focus on taking a fundamental research idea
and then proving it out.
See how far you can go and making the system more complicated, more realistic.
And to the point where you're actually going to build it and test it and
demonstrate it.
And that's been kind of a core philosophy that they've continued to do
through, I don't know, my 15 plus years at the company.
So that's kind of been a great thing.
Through that course, I touched a lot of different things on the system,
like the base station, the modem, the entire air interface system.
And then most recently, at 5G, I worked closely with the
research and then coming to participate in standards.
So I did a lot of work in that.
And through the course of that, I'd say, you know, it's 5G's, of course, very
amazing things have opened up in this technology, new frequencies, new
spectrums, new paradigms for how we do things, new services, of course.
And through that, we started to see that we have this very large bandwidth.
We have lots of directionality now with the millimeter wave.
And the system itself, becoming lower latency more precise, it actually is
very sophisticated for understanding the environment too.
So positioning was one of the first, you know, interesting use cases of
trying to understand the environment that the 5G system was able to
demonstrate with these really large antenna rays and wide bandwidths.
And so, you know, as the system gets more complicated and the capability
of the system also grows, it just became more obvious that, like, there's a
lot that can happen with AI and wireless and so, and so I kind of, over time,
transition to doing much more AI in this space as opposed to being more
wireless driven.
And that was kind of one reason to moving more directly into Qualcomm AI
research.
Another, you know, important, you know, motivation I had was that this
interface, it actually does help accelerate, you know, the developments in AI and
the ability of certain applications.
So, because, you know, at the start, it just is a better link to the cloud.
Things can be more interactive and more direct.
But then over time, as the devices also become more capable and we start
moving more of the, you know, lower level perception tests, for instance, locally
to the devices, then it enables, like, the next stage of AI, the cloud can
concentrate on the more complicated and emerging research advances, like
understanding reasoning and generalizing beyond what has been trained on.
And so, it's very natural to think, okay, let's get
more involved in this and write the kind of the next wave of where things are.
Yeah, what I'm really looking forward to digging into in this discussion is what I
think of as the other side of the AI 5G coin, specifically, I spoke not too
long ago with your colleagues, he had about how 5G can be used to accelerate
distributed AI applications that was episode 489 back in June.
But we'll be talking about here, again, the flip side, the ability for machine
learning and AI to help enable 5G and make it more efficient.
You mentioned in your introductions, Shannon capacity, that's kind of a key
concept that flows throughout your work, would you like to take a second to explain
that?
Yeah, sure.
And the Shannon capacity is one of these amazing results from cloud Shannon is being
able to characterize how well, for a particular channel, you know,
conveying information from a transmitter receiver, how well can you hope to
achieve, you know, throughput without actually, you know, building a system to do
it.
And then it becomes a question of like exactly what do we do to build a system to
achieve that.
And one of the really important advances from Shannon was that we know when to give
up and to focus on different things, because if it's beyond the capacity, we
shouldn't, you know, bother and move to the next problem.
And if it's well within the capacity, then we know that we're what potential is
there and what we have to realize.
So yeah, so kind of the theoretical bandwidth limit, let's say between the
between two communication devices or entities, I guess, exactly.
As the channels become more complicated to characterize, some of these principles,
you know, they could become more difficult to evaluate to some level.
You start doing Monte Carlo integration and then some level, you know, this is
where machine learning can start to come in and help us to understand what's the
best representation for this particular system, which has now become so
complicated to really get the right heuristic and the right abstraction.
So that's, that's, yeah, that's how things have evolved.
One of the papers that will dig into, or the first of the papers that will dig
into is one on deep learning based network augmentation.
Can you introduce that topic to us?
We spoke to your former colleague, Max Walling, about that about a year ago.
At the time, it was kind of this interesting idea that was being pursued in
research.
And it sounds like it's matured quite a bit in a year and now you've got a
paper about some of the results you're seeing.
Yeah, yeah, I'm glad you brought it up.
So yeah, last year, Max did bring up this topic and the importance of it in
communication is that, so the idea of neural augmentation is you want to,
whenever a particular problem has some abstraction where we have a good
algorithm that's perfectly matched to that abstraction, there's no reason to
change that algorithm anymore.
But if the abstraction itself is a little mismatched with reality, we can
augment that algorithm with some neural networks so that we can make the
right adaptation of the algorithm for the mismatch instead of kind of
redesigning the whole thing.
And there's still ways to make that say end to end trainable with the real world.
So one really nice benefit of this that plays well with the
communication, you know, academic community and industry is that there's a
long history of abstracting different problems in wireless and domain where
we know that we're doing exactly the right thing and it's just a matter of,
how do we adapt our domain knowledge now that the channels are becoming even
wider bandwidth or even higher frequency and there's more non-linearities
being introduced or more more sophistication in the model itself that we
weren't really able to capture properly in our abstraction.
And so this is where that idea of the paper is coming in.
And the idea there is that, you know, we often see in academic papers
relative to kind of real world deployment in many fields is that the
papers out of necessity in order to get clean mathematical results will
simplify the real world ignoring any number of factors.
And what this work is suggesting is that deep learning can be used to kind
of map the simple models to what's seen in the real world to make them
more, to make them match better and ultimately kind of more predictive.
Is that the idea?
Yeah, that's right.
To make it more predictive.
Actually, in the paper what's interesting is, you know, initially we were
just naturally it's thinking that, okay, this is, so we were trying to track
a channel and then the speed at which you move, you know, affects the
Doppler of that channel, how fast it's changing.
And if we know the Doppler, then the dynamics are well suited to the
common filter in one particular example.
So it's just a matter of making sure we have the right, you know,
parameters for a common filter for that particular speed that we're at.
And, and so when we're talking about Doppler, we're talking about the
communication between like a fixed base station and a moving communicator
like, you know, someone on a mobile device in a car and you're using the
Doppler shift to, to kind of localize that, that device.
Yeah, exactly.
Is that the general idea?
That's one, one example.
Surprisingly, Doppler is everywhere.
So wireless, a nice thing about wireless is you get great coverage because it
bounces off a lot of things and sometimes transmitter and receiver aren't
moving, but they're bouncing off of things that are moving.
And so then we get Doppler again there.
So, so it is a very ubiquitous problem that we do with even, even when I'm
talking to you today, if we're on a, on a cellular system, we'd have to do
with that to some degree.
And so when you talk about channel tracking and Doppler and all that stuff,
like why do, why do we care?
Why do you care?
As a, you know, someone who's building systems to help people communicate?
Yeah, good question.
So we always want to, like coming back to that shannan capacity, we always want to
operate at that shannan capacity.
And so it's evolved in our system to track this channel so that we know, don't
put too much information if the channel is not very good.
And then if the channel is really good, you know, let's put as much information as we
can and opportunistically, you know, ride the wave of when it's good and when it's bad.
And this is really.
And how does that tie to Doppler?
Doppler, the dynamic of the channel helps us to be able to predict how well.
And then when we're actually trying to decode the signals themselves, we'll be able to
resolve some of that signal better once we know how the channel has rotated through
the course of time.
And so those are the two aspects of it.
And in this particular paper, we were deciding, we were looking more at the tracking the
rotation so that we can resolve the signals that have been degraded.
And when you say rotation in this instance, are you talking like phase shift or something
like that?
Yeah, probably using a lot of jargon, but you know, we'll send our communication signal
usually, you know, your message that you want to convey goes through a very sophisticated
remapping of bits into constellation and a MIMO transmission across antennas.
And that's the signal that we're trying to get to the receiver.
And then we want to be able to decode that original, you know, set of points that convey
the message.
And if that gets through, then we can always remap that back to the original information
that was to convey it.
Yeah, so maybe to kind of try to recap the setting here, going back to this Shannon model,
you've got this transmitter, the transmitter is pumping information into this channel.
There's a receiver that's trying to capture as much of that information as possible and
kind of maximize the bandwidth or throughput in this channel.
And there's inherently this assumption of a noise source that's like disrupting the communication
between the two.
I think we kind of think of, you know, as lay people, a straight line between the, you
know, a cell tower and a mobile, but you know, as you've talked about the signals bouncing
all over the place, you know, even if it wasn't bouncing, you just talked about MIMO and
these other technologies, really the, there's multiple signals or channels kind of between
the cell tower and the device and, you know, there's noise being inserted.
The signals are all going different directions and the ideas that you have to figure all
that you, the more you can kind of take the information you're getting about the network
and the devices, Doppler and all this stuff, you can use that to ultimately kind of statistically
predict what was sent at the device and vice versa.
And that's how you, you maximize the, your bandwidth or throughput.
Yes.
I think you put it well.
Yeah.
Exactly.
Got it.
Got it.
You know, common filter, where does common filter come into play in all this?
Sure.
So, the way the, the channel is evolving, this, you know, you fade up and you fade down,
it's very well modeled in some cases, in a lot of cases, by an autoregressive model.
And then common filters are very well studied and optimal way of tracking that dynamic
system.
And so, so that's, that's where kind of that solution is coming to play.
Got it.
And so, kind of going back to this idea of neural augmentation, talk about the kind of the gap
between the common filter and what's observed in real world.
Sure.
Sure.
So, in the real world, you're going to move, or things around, you're going to move at
different speeds, which implies that your common filter should be using parameters for
each speed, you know, nominally, that would be the best thing to do.
And so, so like a very natural, you know, first attempt at this problem is you, you have
a bank of common filters and you have a bank of speeds and then you, you have a two-part
problem.
What speed are you at?
You figure that out, what common filter should you use.
This is a, you know, it's a very simple, nice intuitive, but it's a bit, it's a two-part
problem and it's also, doesn't necessarily scale as well because you have to go and train
all this, this whole bank and, and then you have to figure out, exactly, you can't have
an infinitely, you know, resume, resolve banks, so you, to figure out what kind of bins
are you going to have to cover, what you need to cover.
Then the other, so it's, it's very natural and, and you look at how people can just have
deep learning, try to solve the system, so you can remove the common filter and say, okay,
we have to deal with Doppler and common filter changing, why don't we just put an RNN
in the system and then train it on all this data of the channel changing and then have
it figure out what it should be doing.
And it is a universal approximator, so if we give it the right amount of data, it should
figure out to do what the common filter does, right?
And specifically, would you be using the RNN to predict, to predict the, the channel characteristics
or to predict the ultimate output bits, for example?
So just the, just the channel itself, and then we would feed that into the rest of the,
demodulator that would get the bits.
Yeah.
Okay.
Got it.
And so again, kind of universal approximator, given enough data and RNN should work, but,
but, I think is where you were going.
What, what we, the other thing we tried was let's just have an RNN to take the observations
and then tell us what kind of common filter we should use.
And then, you know, the nice thing now is we have somewhat of a continuum of parameters
for the common filter and then we also can still have a nice way of making it end to
end trainable.
So unlike the two-stage approach where you have to go and have some kind of, you're sick
of, of training things and building up different parts of the system, here we have one that
we just give data to it and then give, you know, let it train.
And, and so we, we tried all of these and, you know, the nice, there were some interesting
things that actually came up.
So, you know, naturally we, we want to see that we're doing better than everything.
And, you know, we were able to show that this, what we're calling this neural augmented
common filter, specifically we're calling it a hypernet common filter, since we have
an RNN that's setting the parameters, it's kind of a hypernet.
And then, you know, interestingly, so, so, you know, the benefit over the traditional
approach is that it's end to end trainable, so it's much more straightforward.
You can just expose it to data and then it learns and then, you know, it's ready to go.
It makes the offline process much simpler.
And it ends up being also, because there's a continuum of settings, you're not limited
by the coarseness of binning that typically comes up from, you know, practical reduction
of the problem of the solution.
Now when we look at how it compares with the LSTM, which is what we used in this particular
solution, one thing that was really interesting was that our hypernet common filter actually
addressed unseen cases a lot better.
And so, one important detail I didn't going to get to you earlier, but what happens when
we're actually tracking the channel is that we don't, we don't get to, we usually use
pilot symbols, so something that we know at the transmitter and the receiver, we send
it.
And then we use that pilot to figure out what the channel looks like, the fade and the
phase shift and so forth.
And that, you know, the frequency at which we send a pilot is a trade-off, we always have
to deal with.
Like if we send more, then we have a great look at the channel, but we don't have much
information that we can send, and if we send less, the opposite.
And so, one interesting thing was when we had the LSTM train on one particular pilot density
or frequency at which we send these pilots, and we gave it a different density, one that
was higher or one that was lower, it didn't just perform worse, it actually in some cases
was somewhat significantly lower in performance.
And so it was, you know, seemed like it was probably learning some things that would
help it to track the particular case it was given for the training set.
But yeah, but maybe you was using maybe the wrong features or is over-fitting in such
a way that it was not good at generalizing.
Whereas when we constrained it to be this hypernet common filter, it actually performed
well, like it gracefully degraded when it went to actually did well when it went to this
lower density or this higher density without having seen these in the training set.
And so the two cases we're comparing here are the LSTM alone to predict the channel
versus the model that's predicting the common filter characteristics.
And the idea maybe is that in a sense, it kind of relates to this conversation I have
in the podcast.
A lot like we know a lot about the physical world and its behavior, and we've developed
these models that, you know, are representative, that in this case, the common filter.
So how do we couple kind of statistical approaches and physics-based approaches so that we're
not, you know, kind of throwing out the baby with the bath water, right?
And in this case, it sounds like the common filters providing some constraints or guardrails
to the RNN that allow it to perform better even when it's or to generalize better.
Yeah, exactly. I think it as well put exactly the point of what we're doing.
And we tried other situations where not just the pilot density change, but even the strength
of the signal to noise change.
So we, you know, we could train it at one signal to noise ratio and then test it at a lower
one or test it at a higher one.
And we saw, again, a similar robustness from having a hypernet common filter versus just
a pure neural network.
And this idea of, you know, what you're calling neural augmentation or hypernet augmentation,
are you applying it to, or have you applied it to other types of models or other problems?
Yeah, I think that it's a great that you brought that up.
I like to think of this particular paper and, you know, all the things also from the
podcast a year ago with Max.
This is kind of a seed idea and we wanted to really go deep in one particular problem
so that people can see, you know, the different trade-offs that we were trying to understand
and see how things were doing.
But then it's not that a common filter is going to solve every problem, but there are
going to be these other core algorithms in different problems.
And we want this philosophy of, you know, don't forsake all the domain knowledge you've
built at this point.
It's really just a little bit of mismatch.
You have to figure out what that is and put the neural network such that we give it,
you know, the right amount of flexibility to learn how to adapt, but not too much that
it might, you know, learn the wrong things and kind of adapt the solution the wrong way.
Before we jump into the next paper, can you speak specifically to the results you saw
with this?
You kind of spoke generally to it, it performed better than the LSTM alone, but how did
it perform relative to the original common filter implementation?
The original, like the classical solution where you can classify it, that one, the short
coming is more in how coarse the bins are.
And so it's a very nice smooth degradation now, or I should say smooth interpolation across
these bins, because we can't, in the classical approach, we can't quite just, you know, have
a very fine resolution of speeds from 0 to, you know, 500 kilometers per hour.
We have to somehow make that practical and do, you know, a dozen bins or less.
And so there's no, there's a lot of trial and error or a lot of heuristic behind solving
that, and the hypernet actually is able to give us a different way of having, like, almost
a continuous interpolation there.
That's really where we saw a nice, healthy gain over that.
Got it.
Got it.
Awesome.
Awesome.
So the next paper we want to talk about is focused on RF sensing and communication.
Introduce us to the setup here and the problem that you're trying to solve.
Yeah, yeah.
This is another, this is kind of a fun area, I would say, because, so we've, I kind of
walked you through the idea that signals degrade environment, interacts with the signal
going from the transmitter receiver, and we have to be able to track that well and deal
with it so that we get the information through efficiently.
One interesting, you know, the dual of this kind of is that, the environment interacts
and it, you know, can degrade our signal, it hurts the information that we send.
But at the same time, there's a lot of information about the environment when it's degraded
the signal.
And so, so then, oh, how much can we learn about the environment whenever it degrades the
signal, becomes like a question that we want to get to.
And now when we go to 5G, we have like a much larger antenna raise, much wider bandwidth.
It's like a richer signal set now that we have to really understand how things are being
interfered with from the environment to the communication system.
And so we flip this around and we say, well, instead of trying to get the information
through, let's try to learn what's in, what's in the environment as much as we can to see,
you know, what, what can we, how far can we go at this, essentially?
And so, you know, a very simple, you know, the very starting point where they both interact,
you know, is that can we try to predict when the fate is going to come and go and then
you know, very simply like that.
But as you may have seen, even industries are like small companies are starting to introduce
this.
There are different other levels of more sophisticated inference that can be done, like understanding
the presence of a person in a particular space or the activity of detecting an activity
in a particular space.
There has been gesture recognition, I believe that you've probably seen this on phones
and other places.
And then a location in position is another very interesting use case where it's, you know,
called RF fingerprinting sometimes where you actually try to pinpoint where the activity
is happening relative to the environment.
I assume the gesture, all the gesture stuff was based on sensors in the phone, like,
you know, camera or a light or something.
It never occurred to me that they might be using RF interference to, to track gestures.
Yeah, that's more recent, I would say, relatively speaking, but there's a technology.
As you move higher in frequency, you can actually separate the transmitter from the receiver
in a very small space.
So a phone can transmit and receive without jamming it so too much.
And that's the technology behind some of the recent gesture recognition.
So a phone will transmit at a millimeter wave frequency and listen at the same time.
And then from that, it can see signatures of things moving.
And then, of course, by machine learning to figure out, is that signature, you know,
a swipe left or a swipe right or?
Oh, wow.
Wow.
Yeah.
And you mentioned some other companies working on this a couple of years ago, back in February
18, I did an interview with a company called Ariel that is in this space, like using Wi-Fi
and home or commercial buildings to detect presence and motion and, you know, in an elder
care scenario, if someone falls, that kind of thing.
It sounds like that's the same kind of problem or same kind of result you're tackling.
It's different about the work that you, that's in this paper.
Yeah.
Great point.
Yeah.
So a lot of really nice use cases, our knowledge are supervised and very densely labeled.
And so, which is a great solution, but it's not necessarily something that scales.
So what we did here was the team, it came back to the problem and they asked, is there
any way we can reduce the requirement to labeling?
If we can make it completely unsupervised with the proper loss function or if we can have
some weak labeling.
And that's, and we looked at, you know, instead of looking at, you know, all the problems
in the space, we looked at positioning.
So this seemed like a pretty challenging problem, but at the same time, a really nice one that
has a lot of commonality with other applications at bar, since, and so, and so, you know, there
was a prior result where, since you know that what you're tracking is moving in time, they
looked at using a triplet loss to try to space things out so that in the latent space,
so in, in the real space, we can kind of assume that samples are close together because
they're close in time.
And so in the latent space, we, it should be that they're also close in the latent space
because they're close in time.
And then, and then in these previous results, they had a, you know, a nice first step
in showing that, okay, we can get a reasonably good similarity between the latent space
distance and in the real distance in, in the real world.
But it wasn't, you know, perfect because it wasn't geometrically consistent.
So a floor plan may not look like a floor plan, even though you are moving around and close
distances were close.
The, the part that was missing is that, you know, sometimes you may be close in distance
but not necessarily close in time because you may have walked in a circle or you may have
come back somehow to the point that you were at.
So our team looked at, um, clustering, a lot of deep cluster, you know, really interesting
technology that's been used for other things and self-supervised learning.
And, and looked and say, what if we put triplet loss together with this deep cluster so
that we can address this problem when we do come back to the same space that we know
that, okay, they actually should be close together and not for our part because of their,
their cluster in another dimension.
And so, uh, and so putting those two together, uh, now our latent space, which, um, uh, looks
a lot better, a lot closer to reality of, of distance and position than, um, then before
and then, you know, previous methods before.
And so now you can go and just, in theory, you just walk around a room and then you're
just listening to all the RF signals and with the loss functions and so forth training
it, you come back and you see, okay, the latent space looks a lot like the room and where
you were in the room.
And it's not perfect, but it's a lot better than, than previous by doing this, both this
triplet loss and then this clustering.
And then our team won a, you know, one step further and they said, um, what if we just,
what if we just coarsely told you, okay, you're in a room and the room has this shape and
then you, you moved to another room and the room has this shape.
So it's, it's much, it's still, it now becomes labeling, but it's a much lighter labeling
instead of having, uh, some way of annotating exactly where this, um, person was moving around
the room.
You just tell it, okay, I'm in a room, I'm moving around, but the room has this boundary
and that's all I'm going to know.
And then, you know, the really cool thing was that now they actually could get to, um,
you know, within one or two meters, which was as good as the supervised case where we
did have a very precise tracking of the user and the labeling.
And so by putting all these things together, then the idea is that, um, it's very simple
to go in and, and build up, um, I know that they can infer where you are from the interference
in the RF, uh, that it sees, um, and so the, the two parts of that are the, the triplet
loss and the clustering, the deep clustering, that's fully unsupervised and then the, the,
what you described there towards the end of, um, oh, that's like some labeling, localizing
in the room.
Yeah.
It's done labeling.
That's the weak labeling part.
That's right.
Yeah.
Got it.
And, uh, were those, uh, were those done successively or were those done, was the system
trained with kind of two components to, to the, the training process, this unsupervised
part and the weak supervised part.
It was done together.
So there, there is one, I should say, if you read the paper a little bit further, there
are some things we do to initialize one other loss function that we put in.
It's a jumpstart it, but overall those two key, key parts that we're doing together.
I should mention that there's a, uh, a video of this system, um, that will post a link
to in the show notes page.
It was demonstrated at, uh, MWC, uh, Mobile World Congress.
Is that right?
Yeah, yeah.
Thank you for reminding me.
That's right.
Um, you know, one of the really exciting things about this was, um, we had a whole team
that could give us real world data and then different locations and, um, and we can really
prove out these ideas is this, how robust is this an approach?
And although the, the paper and the demo probably showed two particular types of layouts, um,
we did test them on more and also the situations within each layout will also play around with
like, you know, someone holding a, a metal sheet, for instance, and, and so if you go to
that, uh, MWC demo, you can, you can get a nice, um, feel for, you know, what it, how does
it look?
You know, they have a nice animation of a person walking around, which is the ground truth
and then you can see where, like, a dot to tell you where should you be?
And, uh, it's pretty amazing thinking that, unfortunately, the MWC doesn't quite tell
you that it's unsupervised or weekly supervised, which I personally feel like even more amazing
than even the idea is, like, you're being able to be tracked with just RF signals is amazing
in its own right, I think.
But then when you go a little deeper and you realize, oh, this is something that they've
gone and made it, you know, much more, um, scalable in a sense, um, that's, that's really
a, uh, amazing that the team was able to do that.
Yeah.
I've wondered in thinking about these applications, like, do you, uh, to what degree do you need
kind of inside access to the, to the devices or the, the Wi-Fi access point in the case
if Wi-Fi is the, the signal that you're using in order to do the, or is there some, you
know, net flow or logging or something, I can turn on in my, my unify system that will,
uh, collect all the data and I could try to replicate what you're doing and build some
system that tracks presence within, within my building here.
There are some standards activities happening to, to make this somewhat, um, I guess like
a standard that's accessible.
I think we mentioned that a little bit in our paper.
Um, I, I know that, uh, so I can't speak on, like, our product and other products in
the field.
It's, uh, I don't know that very well, but I do know that there are these capabilities
and, you know, we've had the benefit of working with the, those, those, um, like we have counter
parts on like a wireless research team and they really help us to enable us to get the
right data and, and clean up the signal and make sure.
So, um, you know, in theory, you should be able to do some of these, I, I think, but
I, you know, the, the actual execution, uh, I have to have another show with some other
experts.
Maybe that's awesome, yeah, awesome, you know, maybe to kind of wrap, wrap things up, um,
you know, these two papers are, again, kind of under this theme of, you know, how machine
learning can be used to enable, you know, the more effective delivery of network services
like 5G, um, and in some ways, a lot of that is enabled by increasing sophistication on,
on the devices themselves, uh, and, um, machine learning capabilities on device.
Uh, I'd love to hear you kind of riff on, you know, where you see all that going and what's
possible.
Yeah.
Um, uh, I should say that, like, when, when you see this demo, MWC, it's part of, uh,
I would say like a series of four or five demos in machine learning, uh, and 5G.
And there's a lot of, uh, really, you know, cool applications, um, across, uh, like,
for instance, like, there's outdoor positioning, there's, um, milling away beam forming.
There's, um, you know, efficient channel state feedback.
A lot of these have core designs enabled from deep learning and, um, they are also relying
on, uh, the device having a very capable AI, um, and, you know, that's been, uh, a core
you've probably seen on the show, uh, from, uh, many of our, um, you know, Qualcomm
participants were focused on making sure that, uh, what AI can do, it becomes ubiquitous
so we can bring it to mobile and, and have it in as many places as we can, and it, um,
not only are the use cases that we come to associate with AI, like, you know, not language
processing or computer vision, perception, but even making, you know, the wireless system
better also is now benefiting, or potentially going to benefit from having, uh, a very capable
device. And, um, and in this, um, this area of, say, you know, joint RF sensing are using
the COM system for RF sensing, um, this is, uh, like, like, like other things I talk about,
it's this very simple, um, example, and it's a seed idea, but there's certainly more things
that can come from it, and we didn't demonstrate it on devices, mobile devices, we actually
demonstrate on base stations, but you can imagine that, um, there are other things that
you can have the mobile device get involved. And, you know, building up an awareness of
the environment, here where we're showing that you can track people, uh, you can imagine
over, uh, over the course of this that you can, you can start, um, understanding environment
and, um, making the whole communication system more efficient, because now you don't have
to serve, for instance, you don't have to blindly search for, whereas the best, um,
where's the base station coming from, or where's the best direction to have a communication
link, because, uh, you've established an awareness of where strong reflectors are, where blockers
are, and, and where you're moving in the environment. So that's something we're really excited
about moving forward to see, um, this, um, joint interaction between, you know, sensing
helping to communicate AI, and sensing helping communication, and, you know, AI on device
helping this, this, um, enable this, mm-hmm. Awesome, awesome. Well, Joseph, thanks so much
for joining us and sharing a bit about what you're up to, uh, and walking through these
two papers with us. Great, um, I hope, uh, hope you enjoyed it, and, uh, hope you look
forward to seeing more stuff that we're going to do, because we have a lot more coming.
Awesome. Thanks so much. I certainly did.
