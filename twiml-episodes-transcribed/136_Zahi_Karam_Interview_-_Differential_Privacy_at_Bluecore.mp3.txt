Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast I am excited to present a series of interviews exploring the emerging
field of differential privacy.
Over the course of the week we'll dig into some of the very exciting research and application
work happening right now in the field.
In this episode I'm joined by Zahi Karam, director of data science at Bluecore, whose
retail marketing platform specializes in personalized email marketing.
I sat down with Zahi at the George and Partners portfolio conference last year, where he gave
me my initial exposure to the field of differential privacy, ultimately leading to this podcast
series.
Zahi shared his insights into how differential privacy can be deployed in the real world
and some of the technical and cultural challenges to doing so.
We discussed the Bluecore use case in depth, including why and for whom they build differentially
private machine learning models.
Thanks to George and Partners for their continued support of the podcast and for sponsoring
this series.
George and Partners is a venture capital firm that invests in growth stage business software
companies in the US and Canada.
Most investment, George and works closely with portfolio companies to accelerate adoption
of key technologies, including machine learning and differential privacy.
To help their portfolio companies provide privacy guarantees to their customers, George
and recently launched its first software product, Epsilon, which is a differentially private
machine learning solution.
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,
and if you find this field interesting, I'd encourage you to visit the differential
privacy resource center they've set up at gptrs.vc slash twimmelai.
And now on to the show.
Alright everyone, I am here at the George and Partners portfolio conference and I am
with Zahi Karam, Zahi is the director of data science at BluCore.
Zahi, welcome to this week in machine learning and AI.
It's great to be here, great, great.
Why don't we get started by having you tell us a little bit about your background and
how you got involved in data science and machine learning.
So my background is a bit of an interesting one for somebody in machine learning and data
science, I started off in electrical engineering.
That was my undergrad.
Go AA.
AA?
Yes, amazing.
So I was in AA and I got very excited about digital signal processing and so that was
my focus for my undergrad.
Then for my masters, I stuck with digital signal processing, which is still very exciting.
And then I got into speech, speech is an interesting field because it started off purely in the
electrical engineering realm and now it has gotten into the computer science realm with
the advent of algorithms such as deep learning.
So I got into speech and that's how I got into machine learning, specifically my PhD was
focused on semi-supervised methods to improve on speaker identification, okay.
Meaning identifying or differentiating one speaker from another in a noisy kind of
the party scenario or not in a noisy environment, specifically how can we, specifically the
problem we were trying to solve is, can I tell given a 30 second speech utterance, can
I tell who it is, okay.
And in a setting like that, you've got somewhat, you've got maybe two or three utterances
from the target speaker that you cover, but you also have a lot of additional data that
is unlabeled, right, because it's not hard to collect speech, right, it's expensive to
label speech, right, right.
So how do we leverage that additional data to do a better job at speaker identification
and really it's, it extends beyond speaker identification, that's just the application.
Okay.
Well, if you're gotten pretty good at that, now you can talk to your Google home or your
Alexa and the training is, you know, just saying, okay, Google like three or four times
and then it can differentiate the different people in the family.
This work in general is, has benefited from these competitions that provide a data set,
that companies and research labs from around the world compete.
So it's very easy to get started with it.
You've got the data, just usually a tough part, right, and then you've got a highly collaborative
environment where everybody is, you know, there's yearly conferences and really competitions.
So it makes for a field that evolves very quickly.
Yeah.
So I was fortunate to be part of that experience.
That's great.
Yeah, and it was a highly collaborative experience.
So that was my PhD, then I went to University of Michigan for a postdoc and there I wanted
to stay in the machine learning world but move away from speech and get into medical applications
unfortunately, speech pulled me back and it turns out there's an interesting application
in psychiatry where bipolar patients, if you are a person with bipolar disorder, you
can be, have, be normal most of the time but you could also go manic and when you go
manic, you'll do crazy stuff like why Mercedes, quit your job, that's naked on tables and
you're not aware that you're doing these things, you know.
And so that could have a very negative effect on your life, you could, it takes on average
two years to get back to your normal life after a manic episode and on the others and you
could also go get depressed and then we all know the negative side of depression.
So you've got this spectrum and the problem with bipolar disorder, when you're depressed,
you don't want to seek help because you're depressed and when you're manic, you don't
know you're manic, so you don't seek help.
So how do we identify ahead of time that somebody's going manic with the press, so that's
a problem and it turns out that speech is an interesting, let's say, biomarker for that.
The psychiatry we worked with left to say that I'll have family members of people with
bipolar disorder tell me, doctor, I knew he was going to go manic because I heard it in
his voice or I called her on the phone and she sounded depressed.
So I knew she was good.
Not just an indicator, but a predictive indicator.
So that's what we based the research on and so we spent about three years between writing
an application, a cell phone application that recorded only your side of the conversation
because it's illegal in some states to record both sides of that consent.
So only record your side of the conversation and analyze it for mood symptoms and then
we gave those cell phones to bipolar patients and tracked the phone calls over a year and
then also tracked the mood symptoms over that year and then build a system that could
recognize when somebody was going manic or depressive.
Interesting.
So that was that work.
And are the, you know, we're still like not at the core topic yet, but I'm super interested
in this.
Is this where the signals, the things that they said or things like pitch in tempo and
kind of vocal characteristics or something that's like even, you know, lower level than
that?
Yeah.
So it's very hard to design experiments that protect the patients.
And that is the most important thing that you have to do when you design an experiment.
Make sure that you're protecting the patient, especially a vulnerable population, like
people with bipolar disorder.
And so we had to make sure that the features that came out of the system could not be used
to reconstruct what was said in these phone calls.
So our feature extraction was designed in a way to make sure that you couldn't recover
what was said.
And that's important because let's say somebody goes manic and does something, then we have
all the phone recordings and that's right.
And what if we knew ahead of time that something was going to happen?
It gets into a very first, you're not protecting the patient, but there's also the whole legal
amplification.
Right.
Right.
Interesting.
Interesting.
Okay.
Nice.
And so what does BluCore do?
BluCore is a decisioning platform for commerce.
We help marketers find their best customers and keep them for life.
We are a SaaS company, so essentially we provide service for commerce companies and essentially
we help their marketing team identify who to send a message to and what is the best message
and at what time to send it and on which channel?
And the experience, it sounds like the experience you had working on the postdoc in terms of the
focus that needed to be placed on ensuring the privacy of the patients has had kind of
immediate ramifications on your work at BluCore and has led you to doing some pretty interesting
things around privacy.
Yes.
I wouldn't say there's a direct connection between the two, but certainly I think that we
should always be thinking about how can we make sure that we're preserving privacy in
the data that we collect in a way that protects our customers, commerce companies and protects
their customers, right?
And in the medical field, it's obviously even you could argue more important to protect
the patients and the vulnerable population that we are trying to help.
So yes, there's definitely that parallel there.
The approaches are very different in each of them.
In the medical field, we were using a lot of encryption, a lot of feature design that
protected the content, what we said, at BluCore, we are using the financial privacy to try
and build models that do not leak any information about the customers that we use to build those
models with.
Okay.
And when you say customers, you're referring to your customers or your customers' customers
are both.
Both.
Okay.
Both.
But it's a great question and it's a very interesting question and one that we are figuring
out as we go through this process, but ultimately we want to protect both, right?
Where we really see the value of the financial privacy at BluCore is that we have multiple
partners.
We've got about 400 partners to be clear partners are our customers are the commerce
companies that buy us.
And each of these partners, we silo their data so that we are essentially protecting each
commerce company's data from the other commerce company's data, right?
Because we don't want necessarily to give, you know, if you had, let's say, to apparel
companies that are competitors, you don't want to give, I'm going to throw out names
like, gap, you don't want to give it expresses data, right, and vice versa, right?
So we keep all of those kind of, there's, you know, like this base level security of you
don't want the data leaking over from one customer to the next, but you also don't, unless
you're, this is part of your value proposition to these customers and it's what they're buying.
You don't want to give their competitors an advantage based on data that you've collected
from the one partner.
Exactly.
We want to be very careful.
We don't want to give their competitors an advantage throughout platform, right?
Right.
Essentially, you want to be fair at all of our partners, right?
And obviously, in the contracts we have with our partners, all of that is in there, right?
What gets interesting though is, when you're building machine learning models or data science
models or whatever you want to call it, the more data you have, the better the model is.
And you could argue that there is an advantage if you could break down these silos.
And there is an advantage that data asset becomes much more powerful if you can break down
these silos.
Now, how can we do it and is there a way to do it in a way that doesn't give gap up on
express, right, or express a leg up on gap?
Is there a way to break down these silos and get better models for everyone?
But in a way where if you took out one of the partners' data sets from that pool of data,
the model would not be affected.
So that means now, each partner doesn't contribute enough to that aggregate model for them
to be leaking any of the information to the others, right?
Interesting.
So it's a way to think about the maybe exemplar of the problem as being, you know, if the
model is impacted proportionally by the amount of data I'm bringing to the table, then the
larger customers or partners are, you know, basically subsidizing the model performance
of the smaller ones, right?
And so is what you're saying that differential privacy solves that, helps you solve that
problem?
That is what we hope, differential privacy will help us solve.
You're very right, if you think about, because the other way of doing this is to get everybody
to sign up for a data co-op, right?
Where you explicitly saying, I'm going to share my data with everybody else, and I get
everybody else's data.
And that makes a lot of sense if you've got a lot of small players.
But at BlueCore, we've got massive enterprise customers, and then we've got smaller SMB
customers.
So the enterprise customer might not want to share that data and give an advantage to
SMB customers, right?
So how can we make it so that that is a non-issue, right?
And it's very tricky.
And there's this whole concept of, well, you could do data anonymization, right?
This is how it's typically been done in the past, where you obfuscate some part of the
data.
And that's not quite enough, right?
Well, this is the problem, anonymization doesn't guarantee privacy, right?
And we've seen that over and over with examples like the release of the Netflix data and people
were able to, even though the Netflix data had removed, are you familiar with the Netflix
data?
I am vaguely familiar.
So there was a data set that was, quote unquote, anonymized, but you can infer individually
identifiable information from the data set, was it there?
Yeah.
Netflix had this grand challenge that said we'll give them a million dollars to the first
percent that can improve our recommendations by X, right?
And so they released this data set where they had some data set of people waiting real
data of people, their customers waiting movies, and they removed the identifier of the customer,
right?
So you didn't know which customer waited for that movie, right?
So they released that and that's an anonymized data set.
The thing is some clever researchers said, well, you know what, that's not really anonymous
because I can use this external data source, which is IMDB, and correlate the ratings
from IMDB with the Netflix data.
And they were able to identify certain people on the Netflix data, right?
So anonymization doesn't guarantee privacy, got it?
So this recent field coming up, that's been coming up in the last 10 years, that is around
differential privacy, whereas how can you mathematically prove privacy guarantees, right?
And the basic concept behind this whole field is essentially you add noise.
You add some noise that makes it hard to recover the underlying data that trained the model.
And the basic idea behind it is, I need to be able to have two data sets that differ
by one data point.
One data set has that data point, the other data set does not have that data point.
And I should not be able to tell from the resultant models that I trained using these two separate
data sets, which of the two data sets the model was generated from, right?
So if I have a model A, let's say logistic regression, and I use the data set that is missing
at that point of interest, and I train logistic regression model with that, and then I use the
data set with the point of interest, and I train logistic regression model with that.
I should not be able to tell which of the two data sets I used, right?
And that's the basic idea behind the financial privacy.
When I hear a differential privacy explained, folks usually come at it from the opposite direction,
which is, you don't want me to be able to interrogate a model and determine that individual
X was somehow used in its training, right?
It's kind of two sides of the same coin.
Exactly.
Exactly.
It's just another way of explaining, yeah.
So the basic premise is you inject noise at some point in the model training, whether that's
at the feature level, at the model training level, at the output of the models.
So that's essentially the basic concept at a very high level behind the financial privacy.
Yeah, conceptually, I'm struggling with the idea that tens or hundreds of millions of
people are going to have their data points in this model, and that even without differential
privacy, I'd be able to learn anything from about one individual person with how huge the
data set is.
It's kind of counterintuitive for me.
It is, I agree that it's counterintuitive, but a lot of the models that are being used
right now have thousands of parameters in them that get tweaked.
So for example, if you look at deep learning models between all the layers and all the
weights, you end up with thousands of parameters, right?
And so it is possible, for example, if you build an image recognition system and then using
images to train that model, it is possible for some of these images that used to train
to be for the model parameters to actually capture that image and you would be able to
interrogate that model and get back that image or exactly the model parameters and get back
that image.
So while the likelihood of that is low, it's a risk.
It's a risk.
And the only way to gain T that can't happen is through this approach of the financial
privacy where you inject noise and how much noise you inject and where you inject it
is the eye of it and then you can prove to a certain bound, right?
That this model is eventually private and it will not give up your data.
The other thing that's a little counterintuitive for me is generally these models are, the models
are closed.
I don't have access to the models, the model parameters, things like that.
Is it that and maybe it's both of these that AI can learn by interrogating the model
just via regular inference and or B that the model parameters are leakier than I might
otherwise think.
I can look in the memory of my phone and see the model parameters or something like that.
So the leakiness of the model parameters really comes up when people open source a lot
of these models, right?
Like now when Google might open source the image recognition stack, right?
And then that deep learning model is essentially what the open source thing is, is the way
to.
Okay.
And someone's picture might be in there.
Yeah.
Okay.
So, but the alternative is somebody could also interrogate that model and through that
interrogation, identify, uncover some of the training data that was used.
Being a sophisticated enough approach that interrogates the model enough and you know
with a, you can design a coverage, I guess, of a way to interrogate the model to isolate.
Exactly.
And you can also, the other goal of the French privacy is it's not just interrogating that
model but also a malicious party might use external data side information to know how to design
a way to interrogate that model to figure out some, if somebody was in the data set or
what that person's features were in that data set.
So it's, it's three ways.
You can either look under the hood and the parameters might leak some information.
You can just directly interrogate the model or you can leverage side data, right?
So for example, in the Netflix example, if you just looked at the Netflix data by itself,
you couldn't uncover anybody.
But then when you, a malicious agent could go to IMDB and then use that to figure out,
right?
So there's that aspect too.
Okay.
And this is the tricky part and you're asking the right questions, right?
We can't protect against everything and we can't foresee all the ways that a malicious
agent could try and uncover this information.
So all these protections we try and put, whether it's anonymization or black boxing
get, right?
So you can't look at the parameters.
All of these are good, they're useful, but they don't guarantee and the way to guarantee
is again through this concept of the French privacy, right?
So you are asking the right questions.
In most cases, especially smaller models with fewer parameters and if you have a lot of
training data, the likelihood that somebody could get at the individual training samples,
this is pretty low, sure, but the French privacy gives us a guarantee.
So now we are, we are comfortable.
We can say I am comfortable that this model is not going to be information.
Is differential privacy or at least the kind of the mathematical framework that has been
developed around it also allow us to assess a model and put some bounds around kind of
the risk for a model that for what, you know, pre differential privacy?
It's, it's very much focused on the, once you decide how and where you want to inject
noise, what are the, what are the guarantees after that?
Okay.
So how do you see that fitting into the mix at BluCore?
So at BluCore, we have a, there's a bit of a difference between how we're trying to
use differential privacy and how it's typically thought of, right?
Because there is, our goal is to make sure that we can, or to find a way to aggregate data
across these different silos in a way that protects each of our partners data, right?
And, and that's where, that's why it's, it's a bit interesting, right?
It's, it's a space that isn't that explored.
All of the whole differential privacy space is young and then this is an application of
the differential privacy that is a bit novel, right?
So it's a, it's an interesting one and, and if, and if we are able to do aggregation
in a differential private manner, then that will make our models that much better.
It will make the value of our data that much higher and not at the expense of any individual
partner.
Exactly.
And ultimately our partners will benefit, you know, and in the end, we exist to serve
our partners.
Right.
That's why they do business with us.
And so how can we make our models better, provide them value without violating any, any
contracts, we have them, any, any, whether they are written contracts or, or, or trust,
right?
In the end, there's a legal aspect of what you can do with the data, but there's also
the partner's willingness to, to share that data, right?
And, and in the end, we want to be forthcoming with our partners with how we use their data
and, and how we combine it with other data sets, right?
So this is, this is very much an exploratory phase.
We are trying to understand what we can do, what we can guarantee and what are the benefits
of it.
And through that, we will understand what the value of it is and then go to our partners
with that value and articulate that value and get them excited about what differential
privacy can bring to them, right?
And how it can protect them.
Yeah.
Are you, have you engaged with any of your partners around the topic yet?
I'm imagining the, you know, getting a partner up to speed on kind of the, you know, even
what you're trying to do.
Like there's, you're trying to do something that's pretty innovative and you're trying
to do something that's, you know, trying to do that thing using, you know, innovative
and, I mean, not to overuse that word, but like it, it's cutting edge stuff.
I guess replace one cliche with another.
Well, it is cutting edge and this is where our partnership with Georgian has been extremely
valuable because we are a startup, we're growing fast, we have products we have to build.
And yet we have to make time to explore these, these, these novel and cutting edge avenues
that could down the line provide us significant value, right?
And our team is growing, but we're not necessarily big enough to be able to support all of these
pie in the sky or, you know, futuristic models and ideas alone.
So the partnership with Georgian, partners has been crucial for us exploring this space.
The partnership is, it's twofold.
The partnership is from a technical aspect where we've been working with great researchers
on the Georgian impact team and they are providing a lot of technical guidance, running a lot
of experiments.
It's a full on engagement, it's not engagement where one party is removed, so one party
is removed from the equation, right?
It's a full on engagement, it's down in the weeds.
We've had some of our data scientists come and spend time in Toronto and embed with
the team for a few days.
So it's a full on engagement with the Georgian partners team from a technical standpoint,
but then there's also the legal standpoint of it all and the communication standpoint.
How do you understand this landscape of what you can do with the data, what you can do
with the data given the constraints of the contracts you have, and most importantly
is how do you message what the financial privacy is to your partners, to your customers,
so that they buy into the idea and sign off on the idea, right?
Because even if your contracts allow for the concept of the financial private aggregation
of data and your partners don't necessarily buy into this idea, then we can't go live
with that.
Right.
So we don't want to do our partners, right?
Our goal is to create something that's good for them, not create something that would
lose us our trust.
So just to run that off, Georgian has been working hard on understanding that legal landscape
and understanding how to communicate what the financial privacy is to partners that might
not have the technical depth to fully understand what it is and how it's different from anonymization.
Right.
People understand anonymization, they don't understand anonymization and they might not
understand the risk of anonymization, right?
And then you're telling them, well, there's this other thing called the financial privacy,
right?
And it's just, it took us a while to understand what it brings to the table and so you can
imagine people that aren't technically versed in the space, it's hard to explain these things,
right?
I'm trying to envision kind of the process that you're pursuing to explore this.
Are you, I mean, it sounds like it's kind of going straight from, you know, academic
research papers to running experiments locally to, yeah, this is, this is it, we're, we're
essentially, in everything we do, we try not to reinvent the wheel, right?
There's, there's a lot of great academic literature out there.
Unfortunately, the financial privacy is, is a weird one because it's a relatively young
field.
Ten years old is relatively young and most of the work that's happening in that field
is happening now.
So a lot of the papers that come out, you know, it's, it's, we have to constantly keep
checking the literature to make sure we're up to date on, on the latest and greatest.
And at the same time, a lot of the papers are coming out from companies and so they might
not be fully transparent in everything they have done, right, to allow us to replicate
what's going on.
Okay.
So it's, it's an interesting world where we're trying to follow what's out there, but at the
same time, figure out where we need to innovate to make it work for us, right?
Which is again, somewhat of a different use case than the traditional differential privacy
use case, or at least different concerns.
Well, there's, there's that, but there's also differential privacy is one of these things
where, especially if you're, if you're applying, you know, I said you're, you're injecting
noise in certain places, right?
If you're applying, if you're injecting that noise at the model level, at the output of
the model, then the choice of the model directly affects how you bring differential privacy
into it.
So while the literature covers some models, it doesn't cover all models.
Okay.
So finding out what is the best way to do that.
And then the, the, the ultimate challenge that we have is that just because we can get
a given model to be defensively private, so logistic regression, there's some good literature
about how to get that to be defensive private, that doesn't necessarily mean that it will
provide value when you aggregate data across multiple partners.
So that's where we found that we've had where there isn't much precedent around it.
So we've figuring it out as we go along.
So how do you take a model that you can make defensive private through through what exists
in the literature, but then get value when you aggregate data across multiple partners?
Yes, I think where that's where it's been exciting for us, right?
And that's where the engagement with Georgian partners becomes extremely valuable.
And you plan to publish on any of this or is your focus more kind of on the commercial
application and you'll kind of let the research catch up if you in the areas you may have
jumped ahead?
That's a very good question.
We, I think if there is something that is worthwhile to publish.
I think it would be a great opportunity to share that with the community because it's
a community that's growing fast, right?
It's a community that's still young, so it would be good to give back to that community.
And at the same time, I think it would be good to highlight the work that Georgian is doing
and that we are doing in this space because I do think it's exciting that we are able
to, as a startup, do this sort of cutting edge work and find some time and especially with
the help of Georgian, gather, combine those resources and make progress on a problem that
is at the sort of cutting edge of what's happening right now.
So it's nice coming from an academic background and the people we work with at Georgian
also come from an academic background, it's nice to be able to keep playing in that
cutting edge space, right?
Even when you're in a high growth startup that needs to deliver products and sell products
and right now.
All right.
So to wrap things up for folks that want to dig into differential privacy a bit more
is there canonical reference or two or three that they should be looking for or start
with a Google search?
There isn't a good book on the subject matter yet.
I wish there was, it would make our life a lot easier.
There's a Google search, Google's color search would get you, you'd pretty much find everything
that's out there.
You know, there's not, the thing is there isn't that much out there.
So it doesn't take much digging to get to the canonical papers, man.
Got it.
Yeah.
Awesome.
Anything else you'd like to leave us with?
No, I guess nothing on the technical side.
I think that it's very interesting, this is my first experience at a startup.
It's very exciting for me that we have partners that we can collaborate with, you know, in
academia, you know, you always try and find other research you can collaborate with, that
brings something new to the equation.
And so it was very refreshing to be at the startup, at BluCore and find partners with our
VCs, within our VCs that we can collaborate with on a problem that is very exciting, right?
And while I'm new to the space, I don't think this is a very common theme.
So I think it's, you know, it's a great opportunity for us as BluCore.
And I think it's also a great opportunity for Georgian partners to work on this very
exciting work together and explore it from the legal aspect of it, the customer sentiment
side, the technical side, and ultimately the application.
Right.
Well, Zahee, thank you so much.
I really enjoyed the conversation and learned a ton about differential privacy and how
you're looking to use it.
It was a pleasure.
Thank you.
All right, everyone, that's our show for today.
For more information on Zahee or any of the topics covered in this episode, head on over
to twimla.com slash talk slash 133.
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit
their differential privacy resource center at gptrs.vc slash twimla for more information
on the field and what they're up to.
Of course, thanks so much for listening and catch you next time.
