WEBVTT

00:00.000 --> 00:14.480
Welcome to the Tumul AI Podcast.

00:14.480 --> 00:23.840
Hey, what's up everyone?

00:23.840 --> 00:27.680
One of the questions we asked in our recent machine learning development and deployment

00:27.680 --> 00:33.040
survey was the extent to which respondents' organizations had established a platform

00:33.040 --> 00:36.520
to facilitate the delivery of machine learning models.

00:36.520 --> 00:41.680
Nearly 85% reported being somewhere on this journey, from not yet having established

00:41.680 --> 00:47.480
a platform but working on it to having one or more environments already in place.

00:47.480 --> 00:51.480
With so many organizations investing in building out their machine learning tooling, it's

00:51.480 --> 00:57.000
no wonder that the top-voted topic at last year's Tumul Khan Unconference was a discussion

00:57.000 --> 01:01.480
on whether you should build or buy your data science platform.

01:01.480 --> 01:06.640
I'm looking forward to taking on this topic with Kenny Daniel, the CTO of Algorithmia

01:06.640 --> 01:12.000
in an upcoming webinar on Tuesday, June 9 at 10am Pacific Time.

01:12.000 --> 01:15.720
I'll be sharing some more insights from our survey, and Kenny and I will be discussing

01:15.720 --> 01:19.720
what goes into building a machine learning management platform.

01:19.720 --> 01:24.960
How to make the business case for MLOPS at your company, and how to evaluate off-the-shelf

01:24.960 --> 01:27.560
machine learning management solutions.

01:27.560 --> 01:29.080
I hope you can join us.

01:29.080 --> 01:34.960
To learn more or register, go to www.tumulai.com slash Algorithmia.

01:34.960 --> 01:38.800
And now on to the show.

01:38.800 --> 01:41.440
Hey everyone, I am here with Andreas Tolyos.

01:41.440 --> 01:46.200
Andreas is a professor of neuroscience at the Baylor College of Medicine.

01:46.200 --> 01:49.360
Andreas, welcome to the Tumul AI podcast.

01:49.360 --> 01:50.360
Thank you.

01:50.360 --> 01:52.400
How are you making out down there in Houston?

01:52.400 --> 01:59.840
It's good, we're trying to survive, trying to maintain our social distancing, but so

01:59.840 --> 02:01.240
far so good.

02:01.240 --> 02:03.040
Good, good, good.

02:03.040 --> 02:08.040
Why don't we get started by having you share a little bit about your background and

02:08.040 --> 02:14.800
in particular how you came to work in the nexus of neuroscience in AI.

02:14.800 --> 02:19.520
Did you come at it from the medical side or in biology side or from the computational

02:19.520 --> 02:21.680
side or a little bit of both?

02:21.680 --> 02:27.320
Yeah, mostly from the neuroscience side, the scientific side.

02:27.320 --> 02:33.720
Although I did my PhD at MIT in systems and computational neuroscience, and I always

02:33.720 --> 02:42.280
had an interest in how we can use our understanding of the brain to, you know, or test our understanding

02:42.280 --> 02:49.520
of the brain by mimicking its capabilities in behavioral, such as visual perception,

02:49.520 --> 02:53.520
maybe motor control and other things and decision-making.

02:53.520 --> 03:00.120
And so I've always had this interest in how do we bridge neuroscience and AI together.

03:00.120 --> 03:05.720
And at the same time, how do we use tools from machine learning, which are just, you

03:05.720 --> 03:12.200
know, other form of statistical tools to analyze the huge amount of data that we are right

03:12.200 --> 03:14.800
now are capable of getting from the brain.

03:14.800 --> 03:19.880
So the sort of the information goes both ways in the sense of we use tools from machine

03:19.880 --> 03:25.040
learning and AI to study the brain and by understanding the brain, we hope to advance

03:25.040 --> 03:27.000
this field of AI.

03:27.000 --> 03:32.840
So that's sort of my background in tweet, but I myself, most of my training is in systems

03:32.840 --> 03:36.200
neuroscience and neurophysiology.

03:36.200 --> 03:41.600
You mentioned that we have access to lots of data coming from the brain.

03:41.600 --> 03:48.080
Where is that coming from? Is this like MRI type of information or that many, many, I

03:48.080 --> 03:55.280
would say in the last 10 years, maybe a little bit longer, we have the capability because

03:55.280 --> 04:03.800
of new technologies and also, you know, things that we can parallelize, record a huge amount

04:03.800 --> 04:09.640
of data from the human brain, starting from the human using a MRI, but now also like

04:09.640 --> 04:16.560
maybe high density, also human recordings from neurosurgical patients.

04:16.560 --> 04:22.680
But in particular, also in the animal, we in different animal species, we have new technologies

04:22.680 --> 04:28.200
to record large, very large numbers of neurons, either using electrophysiological methods

04:28.200 --> 04:35.440
or imaging, in particular two photon imaging and calcium imaging has enabled us to record,

04:35.440 --> 04:41.280
you know, there are now, data sets are in order of 10,000 neurons recorded simultaneously

04:41.280 --> 04:44.960
from the brain of an animal doing a task.

04:44.960 --> 04:53.600
So, and these data are huge in terms of both size and complexity and, you know, we cannot

04:53.600 --> 04:58.320
just look at them to understand them, we have to analyze them and one of the methods that

04:58.320 --> 05:05.800
we use to analyze them is actually machine learning and in particular, my team and my collaborators

05:05.800 --> 05:11.640
and also other people around the world are using deep learning to model the brain or model

05:11.640 --> 05:12.640
this data.

05:12.640 --> 05:17.680
So, on the one hand, you can think of it like we have the technologies to record a huge

05:17.680 --> 05:24.760
amount of data, we have the tools now from machine learning and in particular, deep learning

05:24.760 --> 05:28.880
to model this data and build predictive models of this data.

05:28.880 --> 05:34.040
And given that we have these predictive models, what we can do is that we can analyze the

05:34.040 --> 05:41.200
model in silico of the brain and run an unlimited almost number of experiments that we could

05:41.200 --> 05:45.520
not have done in the brain by itself in the first place.

05:45.520 --> 05:52.040
For the paradigm, we call inception loops that where we start from an in vivo experiment

05:52.040 --> 05:56.640
will build an in silico model of that system we want to study.

05:56.640 --> 06:00.920
In our case, it was the visual system of the mouse or the macac.

06:00.920 --> 06:05.800
And then having this in silico model, we can ask questions that would have been very

06:05.800 --> 06:08.880
hard to ask without the model in the first place.

06:08.880 --> 06:13.680
So, we can ask the model, for example, to find whether the optimal stimuli that these

06:13.680 --> 06:16.240
neurons that were recorded lie.

06:16.240 --> 06:21.160
And then we call it a loop because then we go back in vivo and test the predictions

06:21.160 --> 06:26.680
of the model and to falsify them or verify them.

06:26.680 --> 06:31.920
And that we can also, in principle, run this in multiple loops where we can improve the

06:31.920 --> 06:32.920
model.

06:32.920 --> 06:41.080
And so, that's kind of where deep learning, one aspect where deep learning, which is

06:41.080 --> 06:47.080
a subset of machine learning, is helping us analyze and understand the brain.

06:47.080 --> 06:53.640
So we've used this and we had a paper come out last year where we've shown that a very

06:53.640 --> 07:01.080
old, you know, understanding of the mouse visual system in this case, we found new principles

07:01.080 --> 07:02.080
of how it's organized.

07:02.080 --> 07:08.080
And in particular, we found that this early visual areas like primary visual cortex had

07:08.080 --> 07:12.440
preferences that seemed much more complex than people had previously thought.

07:12.440 --> 07:19.000
So that's one area where deep learning and AI has helped our understanding of the brain.

07:19.000 --> 07:26.760
You mentioned earlier, you kind of used modeling the data and modeling the brain interchangeably.

07:26.760 --> 07:32.280
You know, is that always the case that when you're modeling the data that you have somehow

07:32.280 --> 07:36.160
gotten that kind of describes things that are going off in a brain that you're building

07:36.160 --> 07:38.680
a model of the brain itself?

07:38.680 --> 07:41.440
Or does that data always capture?

07:41.440 --> 07:45.640
I think of the latter, like modeling the brain is kind of modeling its structure, whereas

07:45.640 --> 07:48.880
the data could be kind of absent of that structure.

07:48.880 --> 07:51.000
Yeah, well, it's both, right?

07:51.000 --> 07:57.000
So the brain, modeling the brain, you can model it at different levels, right?

07:57.000 --> 08:00.040
You can, if we start on the top, you have the behavioral level.

08:00.040 --> 08:01.040
Okay.

08:01.040 --> 08:10.200
So you model behavior without necessarily trying to model their presentations or faithfully

08:10.200 --> 08:14.360
model their presentations that are happening inside the brain, either neurons.

08:14.360 --> 08:20.760
So you could let a build a model of human behavior and given a certain input to predict what

08:20.760 --> 08:26.600
how humans are going to behave and cognitive neuroscientists and psychologists have been

08:26.600 --> 08:31.600
doing this for many, many years, many cases very, very successfully.

08:31.600 --> 08:37.000
So you could let a build a model that tries to predict, as I said, to human behavior

08:37.000 --> 08:43.760
if you want, but with, but big, agnostic or not trying to faithfully capture the neural

08:43.760 --> 08:45.600
representations.

08:45.600 --> 08:51.080
Then if you go a level down, you could say, okay, I'm going to build a model that captures

08:51.080 --> 08:55.680
the behavior, but I'm going to, I'm going to ask the model to also capture the neural

08:55.680 --> 08:59.600
representations of how that behavior came about.

08:59.600 --> 09:05.320
And in this interesting debate in the field because there are some people or ideas out there

09:05.320 --> 09:11.640
that say that once we impose that constraint, the behavioral constraint, then maybe of these

09:11.640 --> 09:16.880
models without even trying to hard, they will start capturing these neural representations.

09:16.880 --> 09:23.280
Alternatively, there's many ways to solve a problem and the brain happens to be one specific

09:23.280 --> 09:24.760
solution.

09:24.760 --> 09:30.280
And for example, we know that, you know, we take the field of let's say object recognition.

09:30.280 --> 09:35.440
There are other species that can do object recognition like birds, but they don't necessarily

09:35.440 --> 09:39.480
have the same architecture or the same structure as our brain.

09:39.480 --> 09:43.880
So they're, but they have some similarities and differences, so that becomes interesting.

09:43.880 --> 09:49.720
So you could say, I'm going to model the brain and David Mar is famous for it because he came

09:49.720 --> 09:55.120
with these different levels of analysis where you can model it at the behavioral level

09:55.120 --> 10:01.000
or we can model it more at the representational level, which is by recording these thousands

10:01.000 --> 10:06.880
of neurons and trying to build the model that is not only achieving the right behavioral

10:06.880 --> 10:11.480
goals, but it's achieving them through the right representations.

10:11.480 --> 10:16.320
And then you could even go at the incorporate another level where is the one you were talking

10:16.320 --> 10:23.200
about, which is the structure you're saying the brain has a specific wet wear architecture

10:23.200 --> 10:29.520
and is relying on this specific architecture to achieve those representations, which in

10:29.520 --> 10:31.880
turn will achieve those behaviors.

10:31.880 --> 10:36.400
So if I want to fully understand the system in some ways, and in particular, if I want

10:36.400 --> 10:40.720
to be able to fix it if it breaks down, you know, psychiatric diseases, I have to go

10:40.720 --> 10:44.240
the way to the implementation level details.

10:44.240 --> 10:48.680
Like for example, I have to take into account that the brain has, you know, different

10:48.680 --> 10:52.840
layers, let's say, you know, the cortex has different layers, different cell types, because

10:52.840 --> 10:58.040
there's evidence from, you know, work done in animal models of diseases and in human diseases

10:58.040 --> 11:06.120
that a lot of these diseases, mental diseases affect specific circuits and specific parts

11:06.120 --> 11:11.520
of the brain or specific cell types because of often they are genetic underpinning.

11:11.520 --> 11:18.640
So if you want to build a model to understand how the brain achieves the behavior that

11:18.640 --> 11:23.880
it achieves, let's say I was talking now, you know, I saw you a couple of weeks ago, we

11:23.880 --> 11:27.160
still remember your face stuff like that.

11:27.160 --> 11:32.320
If we want to understand it and be able to fix it in case it breaks down, we need to go

11:32.320 --> 11:34.720
down to the implementation level details.

11:34.720 --> 11:39.720
It's not enough to stay at the very high level because not fixing.

11:39.720 --> 11:40.720
Okay.

11:40.720 --> 11:47.800
So you've got these different kind of levels of modeling the brain and, you know, essentially

11:47.800 --> 11:54.400
we're taking, you know, data from the brain and using them to build these models at various

11:54.400 --> 12:02.160
levels to kind of ultimately inform our understanding of how, you know, either, I guess it depends

12:02.160 --> 12:03.160
on what level you're at.

12:03.160 --> 12:10.120
How humans work or how the brain is working and we're using machine learning and deep

12:10.120 --> 12:14.640
learning to kind of further that understanding, but you've also talked about going the other

12:14.640 --> 12:22.840
way where we're using what we know about the brain to enhance our understanding of machine

12:22.840 --> 12:25.080
learning and deep neural networks.

12:25.080 --> 12:30.720
And you recently wrote a survey paper on that work, engineering a less artificial intelligence.

12:30.720 --> 12:33.720
Can you kind of introduce that work to us?

12:33.720 --> 12:34.720
Yes.

12:34.720 --> 12:42.720
So what, you know, with my colleagues and I we wrote is a perspective on how or what some

12:42.720 --> 12:51.880
ideas or some approaches for neuroscientists and AI scientists and machine learning engineers

12:51.880 --> 12:56.720
to try and use neuroscience to advance AI and in particular, deep learning.

12:56.720 --> 13:04.280
So just to give you to put this into perspective, in this paper first, we give an introduction

13:04.280 --> 13:09.880
into very brief historical introduction into what is now called deep learning.

13:09.880 --> 13:16.120
In particular, underscoring the point that there are many approaches to solving AI or

13:16.120 --> 13:22.440
to from an engineering point of view, there's probably many different ways to achieve AI.

13:22.440 --> 13:28.400
But what seems right now, one of the most promising ones, which is old in the sense that

13:28.400 --> 13:35.080
people have been thinking about is since the 50s or 60s and 70s and so on, is by building

13:35.080 --> 13:36.880
what's called artificial neural networks.

13:36.880 --> 13:43.520
So it's saying that our brain, its key computational ingredients in neuron and the signups, that

13:43.520 --> 13:44.520
is plastic.

13:44.520 --> 13:46.000
And that's how we learn, right?

13:46.000 --> 13:51.800
So if I have a neural network and it performs some input output non-linear transformation

13:51.800 --> 13:54.480
and I change the signups, is I can achieve computation.

13:54.480 --> 13:57.400
So the idea is that that's how the brain does it.

13:57.400 --> 14:03.800
So let's try and mimic that into a revert, you know, engineer systems that do something

14:03.800 --> 14:04.800
like that.

14:04.800 --> 14:09.280
This has been the big revolution or success in the last 10 years in deep learning.

14:09.280 --> 14:16.480
That's how these networks are trained to play chess, go, do voice recognition, how Alexa

14:16.480 --> 14:17.800
works, Siri.

14:17.800 --> 14:22.840
You have some sort of, at least part of it is some aspect of a neural network with a lot

14:22.840 --> 14:27.880
of engineering put into it to optimize it and make it work.

14:27.880 --> 14:34.360
And what we've learned also in the last 10 years, because we have very strong, you know,

14:34.360 --> 14:41.120
very good, you know, fast computers and we can parallelize this, is that if you have

14:41.120 --> 14:47.240
a scenario, and I'll give you, let's say, an example, let's say you chess and you allow

14:47.240 --> 14:53.920
to computers to play against each other and they essentially play millions or maybe tens

14:53.920 --> 14:57.280
of millions of games that two humans could never play.

14:57.280 --> 15:03.280
And in this type of well-defined problems, humans can not out-compete computers.

15:03.280 --> 15:09.120
And if these problems seem very, very difficult to us, like playing chess, go, you know, things

15:09.120 --> 15:15.680
that are or doing very fine discriminations, as long as the computer has the ability to

15:15.680 --> 15:18.880
be exposed to a lot of training examples.

15:18.880 --> 15:22.040
And because it's running on silicone, it never gets tired.

15:22.040 --> 15:25.680
You know, you can parallelize it and run, you know, on a super computer and it can play

15:25.680 --> 15:30.840
day and night for, you know, equivalent to probably thousands of years of what it would

15:30.840 --> 15:32.160
take a human.

15:32.160 --> 15:33.600
It's game over, right?

15:33.600 --> 15:40.000
They cannot, no human, I mean, it's very hard for humans to compete where the computers

15:40.000 --> 15:47.360
or deep learning fails right now is when you change the testing distribution, even slightly

15:47.360 --> 15:50.360
from the training distribution, okay.

15:50.360 --> 15:54.680
And my colleague Matthias Betke, who is a coder on that paper, has done a lot of interesting

15:54.680 --> 15:59.800
work on this and I'll give you one and others, of course, but I'll give you a simple example.

15:59.800 --> 16:04.560
And one of the poster trials of deep learning is object recognition.

16:04.560 --> 16:07.480
People have trained, and the famous example is image net.

16:07.480 --> 16:13.400
So there was a heroic effort where people sat down, they took maybe a million images,

16:13.400 --> 16:18.200
they labeled them into a thousand classes or more, you know, and then they trained in

16:18.200 --> 16:22.760
your own network to say, this is a bird, this is a dog, this is a cat, this is a car,

16:22.760 --> 16:25.880
this is a bus, this is a spoon and so on.

16:25.880 --> 16:29.320
And when you train these, you find that these networks become pretty good.

16:29.320 --> 16:34.480
I mean, not as good maybe as a human, but in some classes actually even better, like

16:34.480 --> 16:38.960
things that, you know, what type of bird it is maybe, and because most humans are not

16:38.960 --> 16:42.320
experts, a network can learn that even better.

16:42.320 --> 16:47.080
But they're very good, okay, and we can use this and, you know, Google, if you go and

16:47.080 --> 16:49.600
do a Google search, they use algorithms like this.

16:49.600 --> 16:57.320
But if now the network has only seen clean images with no noise added into it.

16:57.320 --> 17:04.400
If you add noise onto these images, even random noise, that to a human, it would not deteriorate

17:04.400 --> 17:05.560
that perception at all.

17:05.560 --> 17:10.200
We would still see this as a cat or as a dog, this network is completely confused.

17:10.200 --> 17:13.000
And they quickly got a chance performance.

17:13.000 --> 17:18.240
And one of the sort of puzzling thing that was discovered early on in the early days

17:18.240 --> 17:22.200
of deep learning is something called the serial attacks.

17:22.200 --> 17:28.120
So if you take this image, if you take an image of a dog in the network, you can change

17:28.120 --> 17:34.480
a few pixels in a specific way that you can fool the network, thinking that right now,

17:34.480 --> 17:38.040
that now it's a cat, and this is called the serial attack.

17:38.040 --> 17:43.200
And these by itself, and these changes are imperceptible to a human.

17:43.200 --> 17:45.200
So they are so small changes.

17:45.200 --> 17:50.320
So the fact that these networks can get confused and be very confident, that something that

17:50.320 --> 17:55.840
it's imperceptible to a human, which you can think of it as, if I change a few pixels

17:55.840 --> 18:00.880
in an image, there's no way that the physical objects out there are changes, right?

18:00.880 --> 18:07.880
Because ultimately intelligence is about inferring what's the correct causes of the images

18:07.880 --> 18:09.640
that we experience, let's say in vision, right?

18:09.640 --> 18:18.320
So if I am in the jungle and I see a lion, I need to perceive it as a lion and notice

18:18.320 --> 18:21.520
my child, right, that I may be hiking with.

18:21.520 --> 18:26.560
So the fact that these things can get fooled so easily is a testimony that they're doing

18:26.560 --> 18:33.720
something qualitatively very different than the way human and animal brains work, okay?

18:33.720 --> 18:37.880
To summarize it, it has to do with the way they get trained, and they have to do with

18:37.880 --> 18:44.120
the learning algorithm in particular, you know, but they're basically, they are, you

18:44.120 --> 18:50.040
can think of it, they are trained by a brute force approach, where for an network to learn

18:50.040 --> 18:56.200
that this is a cat, it has to, the way it learns it, that this is a cat, it's definition

18:56.200 --> 19:03.480
of a cat inside its hidden or latent representations, seems to be very different than our representations

19:03.480 --> 19:05.320
because of this example.

19:05.320 --> 19:07.400
And that is why it seems to fail.

19:07.400 --> 19:12.080
Now there are similarities in this very nice work, that by a bunch of my colleagues, both

19:12.080 --> 19:17.040
in neuroscience and in AI, that show there is quite a lot of similarities between our

19:17.040 --> 19:21.320
representations in neural networks, but they also find, and we, and others find a lot of

19:21.320 --> 19:22.320
differences.

19:22.320 --> 19:27.120
And I think there is a qualitative difference, that is strong enough, that makes them so

19:27.120 --> 19:31.040
prone to things like adversarial attack.

19:31.040 --> 19:36.240
Dollar runs for one set, there is the known nodes, the known unknowns, and the unknown unknowns.

19:36.240 --> 19:40.880
These networks are very bad in the unknown unknowns, if they're faced with a completely

19:40.880 --> 19:47.680
new situation, they do not know what to do with it, whereas we humans are much more flexible,

19:47.680 --> 19:52.360
you know, and that is what gives us, right, and animals are also like that, there's also

19:52.360 --> 19:54.760
working animal behavioral work that they can do it.

19:54.760 --> 20:00.480
So that's what gives biological brains a superiority in terms of their ability to

20:00.480 --> 20:04.040
generalize outside their training distribution.

20:04.040 --> 20:11.600
And the paper, the perspective that we wrote is trying to make a proposition if you want

20:11.600 --> 20:19.520
or what are the areas that neuroscientists can study the brain to extract what we call

20:19.520 --> 20:26.200
an inductive or also a model bias, to then try and advance this field, you know, try

20:26.200 --> 20:29.480
and make machines smart.

20:29.480 --> 20:33.920
And we have some propositions, and it actually goes along, again, these three levels of David

20:33.920 --> 20:41.720
Mark, including a relearning rule, we know that right now the successes in deep learning

20:41.720 --> 20:48.560
have been with what we call supervised learning, some with reinforcement learning, but we humans

20:48.560 --> 20:55.160
and animals rely a lot on self supervision, active learning or active learning where we manipulate

20:55.160 --> 21:00.400
objects in the world, we do cause a leave you on manipulations in the world to try and

21:00.400 --> 21:05.400
understand how the vision works in the world so we can perceive it.

21:05.400 --> 21:10.840
And you know, so you could imagine things at the behavioral level where you change, you

21:10.840 --> 21:16.200
build the CERABOT or you build some agent that tries to mimic a child the way they learn

21:16.200 --> 21:20.560
at this behavioral level and people working on that, or you can even go a step further

21:20.560 --> 21:25.800
down into this representational level and try and understand whether the principles,

21:25.800 --> 21:28.760
the way information is represented in the brain.

21:28.760 --> 21:34.760
And then you can go into the learning rule itself and also put even more architectural

21:34.760 --> 21:35.760
constraints.

21:35.760 --> 21:40.920
Now, another thing that is very interesting is like when we are bored, in particular other

21:40.920 --> 21:47.720
animals, you know, a horse is bored and you can immediately start galloping or not galloping

21:47.720 --> 21:49.360
but walking, right?

21:49.360 --> 21:56.240
So we, we've learned at multiple times because it could have the evolutionary timescale.

21:56.240 --> 22:03.240
So we do not come out as a blank slate necessarily like we have some capabilities and different

22:03.240 --> 22:08.160
animals have different capabilities depending on their, you know, environment.

22:08.160 --> 22:12.320
So that's kind of the overall idea is like how can we learn what we call the model bias

22:12.320 --> 22:17.880
or the inductive bias from the brain, although those are not exactly the same but they are

22:17.880 --> 22:25.520
related, to then transfer that information into a neural network of a device that we

22:25.520 --> 22:29.480
be able to generalize outside this training distribution better.

22:29.480 --> 22:31.000
And this is still a major challenge.

22:31.000 --> 22:35.600
I would say in the last ten years, one area that deep learning has advanced very little

22:35.600 --> 22:40.560
is this outside of the training set generalization and the way that people do this by a brute force

22:40.560 --> 22:41.800
approach right now.

22:41.800 --> 22:42.800
Sure.

22:42.800 --> 22:46.520
Data augmentation and domain adaptation and things like that.

22:46.520 --> 22:52.000
Kind of throwing more data into the training set, whether it's real or artificial.

22:52.000 --> 22:58.200
Exactly. And that may work again, if you are an engineer that works in Google or Uber

22:58.200 --> 23:02.760
and you want to build autonomous driving, you know, they are approaching the problem

23:02.760 --> 23:08.000
by that, okay, I'm going to like collect data and all kinds of conditions, rains, snow,

23:08.000 --> 23:10.200
clouds, you know, whatever, right.

23:10.200 --> 23:16.240
And then and I think although it may end up working in the end, it's not very energetically

23:16.240 --> 23:20.520
efficient because you're not really understanding three intelligence.

23:20.520 --> 23:24.320
You're basically just getting humans to level more and more and more data until you cover

23:24.320 --> 23:25.320
the whole distribution.

23:25.320 --> 23:27.040
You're never going to be surprised.

23:27.040 --> 23:33.120
But that's not I think the most exciting area of AI, the most exciting area of AI is to

23:33.120 --> 23:38.600
figure out how to do this with a few examples and by trying to understand how the brain can

23:38.600 --> 23:39.600
achieve this.

23:39.600 --> 23:40.600
Yeah.

23:40.600 --> 23:41.600
Yeah.

23:41.600 --> 23:44.920
You were careful to point out that there's difference between inductive bias and model

23:44.920 --> 23:45.920
bias.

23:45.920 --> 23:50.080
Can you elaborate on that difference and how it plays out in this scenario?

23:50.080 --> 23:51.080
Yeah.

23:51.080 --> 24:00.480
So the inductive bias or a learning bias comes from the, you can think of let's say two

24:00.480 --> 24:05.880
networks that have a different architecture and different long linearities and they will

24:05.880 --> 24:12.960
have a different inductive bias, okay, because they will in a simpler way, you can think of

24:12.960 --> 24:13.960
it.

24:13.960 --> 24:14.960
I'm trying to fit some data.

24:14.960 --> 24:21.560
And if I have to rely if my architecture gives rise to a particular order of a polynomial

24:21.560 --> 24:25.720
or another order of a polynomial, then that's an inductive bias.

24:25.720 --> 24:32.680
Then once you learn from the data, you also learn a model bias because if in a good example

24:32.680 --> 24:39.000
is what you just said, when you do data documentation, you know, you're basically learning maybe

24:39.000 --> 24:43.720
a better model bias because you're getting trained now on an adversarial training is one

24:43.720 --> 24:44.720
example, right?

24:44.720 --> 24:53.440
Where you're now getting trained on many different modifications or augmentations of this data.

24:53.440 --> 24:57.880
And now you are better at generalizing and people have shown that the best way to defend

24:57.880 --> 25:04.160
against adversarial attack is by, you know, training in a adversarial scenario, which is

25:04.160 --> 25:10.080
basically a better model bias, even if inductive bias, the architecture may be exactly the

25:10.080 --> 25:11.080
same.

25:11.080 --> 25:15.600
The brain is very interesting because it does have a very strong inductive bias because

25:15.600 --> 25:23.480
as I said, the horse is bored and it has some, that network has not experienced the world.

25:23.480 --> 25:28.920
There is information is genetic code that wires it and then you get a network that does

25:28.920 --> 25:29.920
something.

25:29.920 --> 25:35.280
So, you know, and then the experience fine tunes it and improves on it, but it has a very

25:35.280 --> 25:37.760
strong inductive bias.

25:37.760 --> 25:45.720
As a proof that they are easy, we were to understand the how to read out this inductive bias.

25:45.720 --> 25:48.880
You know, then we will be able to build smarter machines.

25:48.880 --> 25:55.520
Now, there is another complexity to this that the learning algorithm may be tightly interwined

25:55.520 --> 25:56.920
with this architecture.

25:56.920 --> 26:01.920
So you may have the perfect architecture, but if you use a different objective function

26:01.920 --> 26:06.960
on different learning algorithm, you will not get now the right performance or the right

26:06.960 --> 26:07.960
model bias.

26:07.960 --> 26:09.960
So, that's how these things are related.

26:09.960 --> 26:17.960
And so, how are we doing on this task of learning from the brain and applying it to deep

26:17.960 --> 26:18.960
learning?

26:18.960 --> 26:19.960
Yeah.

26:19.960 --> 26:25.920
I don't get the sense that a lot of the most important things that we've learned about

26:25.920 --> 26:31.920
making deep learning work like, you know, drop out and, you know, learning rate tricks

26:31.920 --> 26:35.440
and things like that came from biological inspiration.

26:35.440 --> 26:36.440
Yeah.

26:36.440 --> 26:37.440
That is true.

26:37.440 --> 26:41.440
And this is, so it's very interesting because I think that's a very interesting question

26:41.440 --> 26:47.960
also from a, you know, the larger scale if you want and I'll elaborate.

26:47.960 --> 26:52.200
So if you, if you try to understand the brain, right, and you say, you know, in our case,

26:52.200 --> 26:55.400
we try to understand vision and visual perception, right?

26:55.400 --> 27:01.440
In some ways, a good test of it would be able to build a system based on what we think

27:01.440 --> 27:04.000
we understood that does vision, right?

27:04.000 --> 27:08.840
Because if I say, I understand how object vision works in the brain, I've done some experiments

27:08.840 --> 27:14.360
in, you know, in humans, animals, whatever, and I've studied these principles, then my

27:14.360 --> 27:21.200
long-term goal or as a field should be, can we now assemble this principle into a system

27:21.200 --> 27:26.600
that mimics the behavior of the system that I'm trying to understand, right?

27:26.600 --> 27:29.000
And that is essentially where the AI's goal is.

27:29.000 --> 27:34.520
You're saying from a neuroscience perspective, you know, we've got these models of, you

27:34.520 --> 27:38.880
know, cones and layers and all these things, you know, forget about this deep learning

27:38.880 --> 27:39.880
stuff.

27:39.880 --> 27:43.360
We should just take our models and kind of implement them and they should be better

27:43.360 --> 27:44.360
in theory.

27:44.360 --> 27:46.520
In theory, in theory, we understand it.

27:46.520 --> 27:53.080
I mean, if we really understand, once we fully understand how vision works, right,

27:53.080 --> 27:57.880
we should be able to reverse engineering, we should be able to take all these principles

27:57.880 --> 28:03.000
and put them together and it should perform the task that the human visual system does.

28:03.000 --> 28:07.440
I mean, that's sort of the, it's a very stringent test over hypothesis, right?

28:07.440 --> 28:09.280
You have to test them like that.

28:09.280 --> 28:13.760
Now, I'm not saying it's going to be achieved in our lifetime, maybe, but who knows, or

28:13.760 --> 28:17.600
it could take many, many years, but that's the goal of neuroscience, right?

28:17.600 --> 28:22.280
Ultimately, not necessarily now, but we are far away from that goal, right?

28:22.280 --> 28:26.680
I mean, it's very, you know, there's very, there's only sort of toy examples where we've

28:26.680 --> 28:32.480
taken these principles and we've shown that we can achieve robust vision, you know, that

28:32.480 --> 28:36.040
we can achieve the zero-bust optical recognition.

28:36.040 --> 28:41.640
Is that based on gaps in our understanding or our ability to implement what we understand?

28:41.640 --> 28:44.120
No, I think it's based on two things.

28:44.120 --> 28:48.760
I think, and this is a more pessimistic view.

28:48.760 --> 28:55.640
It's based on our gab of understanding principles.

28:55.640 --> 29:03.360
I mean, right now, we've been doing more, and this is maybe, you know, it's a very difficult

29:03.360 --> 29:04.360
problem, right?

29:04.360 --> 29:08.960
And there's very, there's a lot of technology, I said earlier, that gives us incredible

29:08.960 --> 29:13.080
capabilities to understand the brain.

29:13.080 --> 29:17.800
But new principles are much harder to understand, fundamental principles.

29:17.800 --> 29:22.720
And then we get lost in details and we humans don't know how to, I mean, you know, we don't

29:22.720 --> 29:26.880
just copy it because you see there's a difference also between understanding and, I mean, if

29:26.880 --> 29:31.840
we could take the brain and copy it piece by piece, we would, maybe we would achieve what

29:31.840 --> 29:36.360
I just said, but it would still not be satisfying from a scientific point of view because

29:36.360 --> 29:38.600
it wouldn't be to understand it, right?

29:38.600 --> 29:44.440
If we just categorize the cones, the receptors, the wiring, and we just like copy it over,

29:44.440 --> 29:50.040
and it works, what have we understood is like, I don't know, like, it's like me copying

29:50.040 --> 29:54.240
a foreign language, I faithfully copied it by understood nothing.

29:54.240 --> 29:55.560
So we don't want to do that either.

29:55.560 --> 29:57.040
So that's why it's more complicated, right?

29:57.040 --> 30:01.680
We need to understand the principles that guide these organizations.

30:01.680 --> 30:06.600
And then we may not even have to worry, hopefully we'll not worry about all the details, you

30:06.600 --> 30:09.880
know, the way we understand the principles of flying and we build aeroplanes.

30:09.880 --> 30:10.880
That's kind of the idea.

30:10.880 --> 30:13.600
We don't try and copy every single feather of a bird.

30:13.600 --> 30:18.080
Now, on the other side, my children has been much more successful in building systems

30:18.080 --> 30:23.080
that work, right, like, you know, if you use an AI system, it's built by machine learning

30:23.080 --> 30:25.880
engineers and not by neuroscientists.

30:25.880 --> 30:31.520
And you're right that a lot of it is brute force engineering and often, or most cases,

30:31.520 --> 30:33.120
is also there's no understanding.

30:33.120 --> 30:35.960
It's not like people that do the engineering of the machine learning side.

30:35.960 --> 30:41.920
I mean, there is some understanding, but it's very, it's a lot of black magic, you know,

30:41.920 --> 30:46.320
it's like a black box approach, you tune things, and then it makes it work.

30:46.320 --> 30:48.040
We do understand some principles.

30:48.040 --> 30:51.480
And in some ways, that's sort of maybe how the brain works too, like, for example, a deep

30:51.480 --> 30:55.880
learning has the deep layers, it has the bug propagation, it's a principle.

30:55.880 --> 31:01.760
So I can, we can write down a recipe of, you know, a few things that then when you train

31:01.760 --> 31:03.680
it, it will do stuff.

31:03.680 --> 31:09.280
But you're right that we haven't been very successful in translating neuroscientific

31:09.280 --> 31:12.280
understanding to machine learning in a direct way.

31:12.280 --> 31:16.960
It's mostly an inspirational way, like if you talk to AI people, they say, oh, yeah, I

31:16.960 --> 31:19.160
use reinforcement learning as an inspiration.

31:19.160 --> 31:23.760
I use, we call them deep networks comes from neuroscience because we know that in the

31:23.760 --> 31:29.720
brain, there's many layers, you know, the first deep learning model was completely inspired

31:29.720 --> 31:36.120
by Fukushima, who was a neuroscient, I mean, not a new computational neuroscience person

31:36.120 --> 31:42.920
or inspired person that, so there are fundamental ingredients in deep learning, but a lot of

31:42.920 --> 31:47.120
it is right now brute force engineering, okay, with a lot of ideas.

31:47.120 --> 31:50.320
So I do think that the dialogue is very fruitful.

31:50.320 --> 31:56.360
I mean, we ourselves have some ideas on how we are trying to do this, and we had some

31:56.360 --> 32:03.000
papers that are mentioned in that review too at the representational level, but it hasn't

32:03.000 --> 32:10.080
been, is not a, you know, I think the big breakthroughs let's say in this field of taking,

32:10.080 --> 32:16.640
of understanding neuroscience principles, A, and B, translating them in a way that you

32:16.640 --> 32:22.520
show a qualitatively different performance in a machine learning case has not been demonstrated.

32:22.520 --> 32:23.520
Yeah.

32:23.520 --> 32:27.400
I mean, I hope it will be and hopefully soon, but I think that's going to be a major

32:27.400 --> 32:31.920
breakthrough because it will give us, if you want the bridge on how to go from one to

32:31.920 --> 32:35.360
the other, but it's not easy, I agree with you.

32:35.360 --> 32:41.640
And a lot of it, I think, comes from our inability to extract principles from neuroscience.

32:41.640 --> 32:48.320
Such as your paper, help us understand where we should be looking, you know, framework

32:48.320 --> 32:53.000
for the different places where we might be likely to find this kind of insight.

32:53.000 --> 32:54.000
Yeah.

32:54.000 --> 33:02.560
I mean, there we are again, drawing on three levels plus the learning on the behavioral level,

33:02.560 --> 33:08.480
or on the cognitive level, you know, you can, and a lot, I mean, you know, this is not

33:08.480 --> 33:13.080
a new idea and a lot of people in the field in machine learning are very interested in

33:13.080 --> 33:19.720
this and exploring it is when you, you try and change the learning objective if you want.

33:19.720 --> 33:25.440
I mean, let me give you an example, instead of let's say just learning object recognition

33:25.440 --> 33:30.600
by just giving labels, you could maybe give a few labels, but you could have an agent manipulate

33:30.600 --> 33:32.280
the objects, right?

33:32.280 --> 33:36.920
And maybe move them around and maybe break some every now and then.

33:36.920 --> 33:42.480
So you get an idea of the physics in the world on the other level.

33:42.480 --> 33:47.040
And this is an area that we might team and my collaborators were doing a lot of work on

33:47.040 --> 33:50.920
this is trying to understand it at the representational level.

33:50.920 --> 33:55.520
We know that let's say the Maccag or the human visual system was in the order of 30 visual

33:55.520 --> 33:56.520
areas.

33:56.520 --> 34:03.480
And although we know a lot about it, we also lack a fundamental, let's say something

34:03.480 --> 34:07.440
that everybody agrees on, what is the role of all these even 30 areas?

34:07.440 --> 34:12.360
Why do we have 30 and not five or why, you know, there's this idea of eventual and

34:12.360 --> 34:17.160
dorsal stream, but then some people say, well, that's not quite true, maybe it's like

34:17.160 --> 34:21.840
what more and more than, you know, like it's more of an action stream versus a perception

34:21.840 --> 34:22.840
stream.

34:22.840 --> 34:27.080
So to understand how the brain does it, we have to record from individual neurons and

34:27.080 --> 34:32.280
we have to have ways to and we've developed some metal based on the learning the system

34:32.280 --> 34:36.760
identification, the assumption loops to try and decipher what are the representations

34:36.760 --> 34:39.120
of the tuning functions of this neuron.

34:39.120 --> 34:43.400
And these are some mathematical principles that dictate these tuning functions or do they

34:43.400 --> 34:46.400
look like they are not interpretable.

34:46.400 --> 34:51.920
And I think one key thing here that we find both in neuroscience and in AI is they do

34:51.920 --> 34:55.320
have interpretable AI or interpretable.

34:55.320 --> 34:58.960
And that's a struggle both for neuroscientists and machine learning people.

34:58.960 --> 35:05.880
One theory in the field in both sides is that if you do not have interpretable a colleague

35:05.880 --> 35:11.520
of mine actually has a paper he's writing where he's trying to relate interpretability

35:11.520 --> 35:12.920
with robustness.

35:12.920 --> 35:18.040
So this idea that if we do not have interpretable systems and the representations are not

35:18.040 --> 35:23.240
interpretable, then a is going to be very hard to trust them necessarily.

35:23.240 --> 35:28.560
And this is an issue of deploying AI technologies, let's say, for radiology readings.

35:28.560 --> 35:30.560
Although others say, what do you care about?

35:30.560 --> 35:37.000
The human level interpretability is not possible even in the brain and we've kind of been

35:37.000 --> 35:44.480
fooling ourselves by saying, we understand this area, does this and this area, does that

35:44.480 --> 35:46.320
that is humanly interpretable.

35:46.320 --> 35:52.200
But I think that's an interesting conversation and debate that we will see more of it

35:52.200 --> 35:53.720
of interpretability.

35:53.720 --> 35:59.160
And I think if there is such a thing as interpretability and we understand it in the brain, then

35:59.160 --> 36:04.280
it's going to be easier for engineers to like say, okay, now we know that thing exists.

36:04.280 --> 36:06.720
Let's try and figure out how we can put it in the brain.

36:06.720 --> 36:10.080
And that's at the representational level where we'll call from yours and this is what

36:10.080 --> 36:16.320
with most of the perspective paper focuses on this representational level.

36:16.320 --> 36:21.480
If we need to figure out interpretability in the brain before we're able to get to interpretability

36:21.480 --> 36:25.640
or explainability in machine learning models, I think we're probably in trouble.

36:25.640 --> 36:26.640
Yeah.

36:26.640 --> 36:27.640
That is true.

36:27.640 --> 36:33.560
But it's interesting though, because the tools that people in machine learning are developing

36:33.560 --> 36:38.880
to gain interpretability, it's exactly the tools that we and others are trying to implement

36:38.880 --> 36:39.880
in the brain.

36:39.880 --> 36:45.200
So that's where there's a very strong link again between the two fields.

36:45.200 --> 36:55.240
Are there examples that come to mind of successes that we've seen already in, you know, pulling

36:55.240 --> 37:00.720
over understanding that we've gained on the machine learning deep learning side to the

37:00.720 --> 37:01.720
neuroscience side?

37:01.720 --> 37:02.720
Yeah.

37:02.720 --> 37:10.640
So we had a paper in NIPs last year, Julie was the first author and what Julie did is he

37:10.640 --> 37:18.680
took recordings in this case from the mouse visual system and built, we built a model actually

37:18.680 --> 37:20.920
this inception loop model.

37:20.920 --> 37:26.640
And then we generated a similarity matrix, basically we showed, let's say, 1000 natural

37:26.640 --> 37:34.200
images to the model and we computed the similarity matrix between image I and image J in the

37:34.200 --> 37:37.880
models, the brain models neural space.

37:37.880 --> 37:38.880
Okay.

37:38.880 --> 37:43.600
And then we took a neural network and we tried instead of just teaching it to say cats

37:43.600 --> 37:52.640
and dogs, we also tried to make its representations to look more similar in terms of these representational

37:52.640 --> 37:55.240
similar metrics to the ones we measure in the brain.

37:55.240 --> 38:00.440
So in some ways, you can think of it like if you have a deep neural network, you have

38:00.440 --> 38:05.880
some laws on the top layer, which is what's the class, but all this stuff of this latent variables

38:05.880 --> 38:08.360
are let free to do whatever they want, right?

38:08.360 --> 38:12.200
So we tried to put some constraints in these middle layers and say, let's try in one of

38:12.200 --> 38:16.280
these layers to make it look a little bit more like the brain of a mouse.

38:16.280 --> 38:17.280
Okay.

38:17.280 --> 38:25.120
And what we found was that that model became more robust to perturbations, it was interesting

38:25.120 --> 38:32.040
because it became more robust to high frequency noise perturbations, which we are now studying

38:32.040 --> 38:36.800
and we can explain in a simpler way that it may be like, you know, the mouse is more sensitive

38:36.800 --> 38:42.920
to low spatial frequencies, but this is an at least a proof of concept that, you know,

38:42.920 --> 38:49.280
even if this case, it may be a simpler scenario that you should, if you know, that you could

38:49.280 --> 38:56.120
learn something at the representational level in a system and then translate it over.

38:56.120 --> 39:00.600
We have the tools to sort of move it over because if we have a model of a brain, there

39:00.600 --> 39:05.800
is a deep neural network and it's trained on data and then we have a neural network model

39:05.800 --> 39:08.040
that tries to do object recognition.

39:08.040 --> 39:13.360
They're both models, they're both built of the same ingredient, which is neurons with

39:13.360 --> 39:20.000
synapses in silicon, so we can try and now transfer information from one to the other.

39:20.000 --> 39:25.480
Now that's still not, even if we're in my personal taste, even if we were to do this and

39:25.480 --> 39:30.040
we would achieve, you know, something incredible, let's say, so that the serial attack, it would

39:30.040 --> 39:33.640
still not be very satisfying if we don't understand why it's solving.

39:33.640 --> 39:41.840
You know, the mere co-being operation itself, you know, it may work, but for me personally

39:41.840 --> 39:47.320
to be satisfied, I would like to understand why it's working and which again goes back

39:47.320 --> 39:51.640
to the some aspect of interpretability in some ways.

39:51.640 --> 39:59.960
It almost kind of points to this, at least you're your limited experiment, you know, kind

39:59.960 --> 40:06.600
of calls to mind this future where instead of pulling a, you know, building a network

40:06.600 --> 40:11.680
up from convolutional layers and pooling layers where, you know, I'll, you know, use a mouse

40:11.680 --> 40:16.400
layer and a flatworm layer and a cat layer and that kind of thing.

40:16.400 --> 40:17.400
Yeah, yeah.

40:17.400 --> 40:23.360
And I think the other thing is to show the utility of these systems, we need to come also

40:23.360 --> 40:26.200
with the right machine learning benchmark test.

40:26.200 --> 40:31.760
And right now, the focus in machine learning in the last 10 years has been, again, to create

40:31.760 --> 40:37.720
benchmark tests that were very difficult in the 90s and in the 80s for computers, like

40:37.720 --> 40:40.160
optical cognition, right?

40:40.160 --> 40:45.760
But there hasn't been as much or there's more now, but where you train, you know, you

40:45.760 --> 40:49.960
want to build a visual system that has all the possible visual tasks, right?

40:49.960 --> 40:54.960
It's a segmentation, optical cognition, motion tracking, action recognition, it can work

40:54.960 --> 40:59.800
with video, you can work with study game, like the way biological system works, right?

40:59.800 --> 41:04.600
And from a, any male sort of to do with who is working on the problem, like if you have

41:04.600 --> 41:09.120
engineers, especially working in a company that they want to build autonomous driving, they

41:09.120 --> 41:14.840
don't care if that system, that static optical cognition, because they'll never be tested

41:14.840 --> 41:16.600
on static images, right?

41:16.600 --> 41:21.880
You know, all that same system is not the one that's going to talk to you to the driver,

41:21.880 --> 41:22.880
right?

41:22.880 --> 41:30.200
So, but from a, if you're interested from the AI, more scientific, AI as a science and

41:30.200 --> 41:35.360
not as an engineering field, you want to understand which is the same as neuroscience in some

41:35.360 --> 41:41.320
ways, what are the principles that enable the same network to perform all this task and

41:41.320 --> 41:44.800
also do transfer learning from one task to the other very fast?

41:44.800 --> 41:51.480
And one of the areas that seeing some interesting research on the machine learning side is the

41:51.480 --> 41:53.320
idea of multitask learning.

41:53.320 --> 41:59.400
So, you know, we can train these networks to do two things at the same time instead of

41:59.400 --> 42:05.520
one or end things at the same time instead of one and there, you know, that somehow has

42:05.520 --> 42:10.080
this kind of regularizing, you know, a thing that makes them perform better.

42:10.080 --> 42:14.760
Is there any kind of biological inspiration to that or biological parallel to that?

42:14.760 --> 42:15.760
Yeah.

42:15.760 --> 42:23.640
Actually, we discussed that in the paper also multitask training and when we see the world,

42:23.640 --> 42:30.040
we don't just do object recognition, we know the curvature of an object, the distance

42:30.040 --> 42:38.960
of the object, we can grasp it, we have some idea about the texture, you know, we can,

42:38.960 --> 42:44.760
to multi-scale perception, like we can look at the individual, you know, eyebrow on someone's

42:44.760 --> 42:53.400
face and also recognize their face, we can, you know, do, you know, figure out if someone

42:53.400 --> 43:02.720
is sad, so we do a multitask, multi-futural embedding and if you want, the same brain is

43:02.720 --> 43:04.600
doing all these things.

43:04.600 --> 43:09.600
So that's one of the areas where we think the brain is very different than a machine.

43:09.600 --> 43:13.480
And I think there are people that are saying, okay, maybe we should like, we train one

43:13.480 --> 43:18.400
everywhere on all the tasks, maybe we'll generalize also better, it will be more robust,

43:18.400 --> 43:24.960
it will be, you know, so that's sort of an interesting direction that we and others

43:24.960 --> 43:30.320
are interested in, you kind of, you're absolutely right, that's very important.

43:30.320 --> 43:35.560
The problem again becomes how do you train this because if you rely on human labeling,

43:35.560 --> 43:39.760
all these data, then you need to like have humans labeled the color, detect, and then

43:39.760 --> 43:46.920
you're sort of again limited by, you know, yeah, but you could like approach it in a brute

43:46.920 --> 43:51.640
force approach at least in toy examples to show that this is the right direction.

43:51.640 --> 43:58.680
Is there any one particular direction in kind of this entire space, biological systems

43:58.680 --> 44:03.800
to deep learning, deep learning to biological systems that you're most excited about?

44:03.800 --> 44:10.800
The good question, I think that I'm mostly, I think right now in the next few years,

44:10.800 --> 44:18.800
the most exciting directions are at the cognitive or behavioral level and at the representational

44:18.800 --> 44:27.520
level because of, you know, like, and it may be a practical thing because we have good

44:27.520 --> 44:32.920
baseline models that relay, they have a cost function, what they're training and they

44:32.920 --> 44:35.720
have representations that we can measure, right?

44:35.720 --> 44:42.360
And I'm mostly excited about these two higher levels in terms of building models that are

44:42.360 --> 44:49.960
going to get information at best or inspiration, you know, more, in a less maybe ambitious

44:49.960 --> 44:53.360
way from the brain to advanced AI.

44:53.360 --> 44:59.920
If you go down to the implementation level, although I think that's, you know, it may happen

44:59.920 --> 45:05.160
at some point, there's too much complexity down there and we don't understand why there

45:05.160 --> 45:06.160
is this complexity.

45:06.160 --> 45:11.760
It could be implementation, it could be biological way we are constraints, for example, energy

45:11.760 --> 45:16.400
constraints like, you know, the brain has chemistry and it doesn't have silicon and then

45:16.400 --> 45:22.400
synapses need neurotransmitters because of biology and then you build this complexity.

45:22.400 --> 45:27.400
So trying to like understand, you know, without a clear understanding and let me, the classical

45:27.400 --> 45:30.800
argument is even the spike, right, neurons have spikes, neural networks don't, they're

45:30.800 --> 45:36.080
not spiking, you know, so people are still debating and there hasn't been let's say golden

45:36.080 --> 45:41.480
success in spiking networks that can do something that, you know, the standard neural networks

45:41.480 --> 45:42.480
cannot do.

45:42.480 --> 45:47.400
So I would say the first sky or two levels, at least right now in the next few years

45:47.400 --> 45:53.280
are more promising in my mind to see these transfer of knowledge.

45:53.280 --> 45:59.760
Well, Andreas, thanks so much for sharing with us what you're working on, really great

45:59.760 --> 46:01.080
to speak with you.

46:01.080 --> 46:03.280
Thank you for having me and have a nice day.

46:03.280 --> 46:04.760
Thanks Andreas.

46:04.760 --> 46:10.680
Alright everyone, that's our show for today.

46:10.680 --> 46:16.480
For more information on today's show, visit twomolai.com slash shows.

46:16.480 --> 46:31.480
As always, thanks so much for listening and catch you next time.

