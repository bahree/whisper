WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twomel AI Podcast.

00:13.400 --> 00:21.320
I'm your host Sam Charrington.

00:21.320 --> 00:26.960
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded

00:26.960 --> 00:31.800
earlier this month at the 33rd annual NURRIPS conference.

00:31.800 --> 00:35.920
If you've been waiting for the Twomel pendulum to swing from workflow and deployment back

00:35.920 --> 00:39.920
over to AI and ML research, this is your time.

00:39.920 --> 00:45.200
We've got some great interviews in store for you over the upcoming weeks.

00:45.200 --> 00:49.280
Before we move on, I want to send a huge thanks to our friends at Shell for their support

00:49.280 --> 00:53.840
of the podcast and their sponsorship of this NURRIPS series.

00:53.840 --> 00:58.600
Shell has been an early adopter of a wide variety of AI technologies to support use cases

00:58.600 --> 01:04.920
across retail, trading, new energies, refineries, exploration, and many more, and is doing

01:04.920 --> 01:08.680
some really interesting things, but don't take it from me.

01:08.680 --> 01:14.720
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.

01:14.720 --> 01:19.600
They have a very deliberate strategy of using AI right across their operation from the drilling

01:19.600 --> 01:22.800
operations to safety.

01:22.800 --> 01:27.800
Last year, the company established the Shell.AI Residency Program, a two-year full-time

01:27.800 --> 01:32.680
program, which allows data scientists and AI engineers to gain experience working on

01:32.680 --> 01:37.080
a variety of AI projects across all Shell businesses.

01:37.080 --> 01:40.960
If you're in a position to take advantage of an opportunity like this, I'd encourage

01:40.960 --> 01:49.400
you to hit pause now and head over to Shell.AI to learn more, once again, that Shell.AI.

01:49.400 --> 01:53.320
And now on to the show.

01:53.320 --> 01:58.320
All right, everyone here in Vancouver for NURRIPS.

01:58.320 --> 02:00.280
And I am with Blake Richards.

02:00.280 --> 02:05.120
Blake is an assistant professor in the School of Computer Science and the Montreal Neurological

02:05.120 --> 02:11.800
Institute at McGill University, as well as a core faculty member at Miele.

02:11.800 --> 02:13.880
And you've also got an affiliation with C-FAR.

02:13.880 --> 02:18.160
Yes, I'm a candidate C-FAR AI Chair and a member of C-FAR is Learning and Machines

02:18.160 --> 02:19.640
and Brains Program.

02:19.640 --> 02:20.640
Fantastic, fantastic.

02:20.640 --> 02:22.880
Well, Blake, welcome to the C-FAR AI Podcast.

02:22.880 --> 02:24.680
Thank you very much for having me.

02:24.680 --> 02:25.680
Awesome.

02:25.680 --> 02:30.320
So you are doing a talk here on sensory prediction error signals in the Neo Cortex.

02:30.320 --> 02:31.320
Yes.

02:31.320 --> 02:33.320
Let's just jump right into that.

02:33.320 --> 02:35.080
What's the talk about?

02:35.080 --> 02:36.080
Sure.

02:36.080 --> 02:42.280
So, a lot of people have postulated for a long time that our brains, and in particular,

02:42.280 --> 02:48.720
the Neo Cortex, the region concerned with higher order thought or functions, if you will,

02:48.720 --> 02:52.520
is effectively an unsupervised learning machine.

02:52.520 --> 03:01.520
It is there to make predictions about incoming stimuli and that it would use differences

03:01.520 --> 03:06.200
between those predictions and the actual data that it receives in order to learn about

03:06.200 --> 03:09.720
the structure of the world and develop a good internal model.

03:09.720 --> 03:15.680
And although there have been many computational studies that have postulated this and this

03:15.680 --> 03:21.480
idea has also informed artificial intelligence a great deal, the fact is that there isn't

03:21.480 --> 03:24.480
a lot of direct evidence for it in the brain.

03:24.480 --> 03:26.720
There are a few initial studies.

03:26.720 --> 03:33.280
But myself and my collaborators, Joel Zalberberg at York University, Joshua Bengeo, also at

03:33.280 --> 03:38.480
Milla in University of Malayal, and Tim Lilly crap that glue deep mind.

03:38.480 --> 03:44.680
We put together a proposal to the Allen Brain Institute a couple of years ago to run some

03:44.680 --> 03:51.480
experiments to explicitly look for some of the sorts of prediction error signals that

03:51.480 --> 03:56.880
these kinds of models of unsupervised learning in the brain predict would be there.

03:56.880 --> 04:02.040
So the Allen Institute has been running a series of studies doing what's called two photon

04:02.040 --> 04:04.200
calcium imaging in mice.

04:04.200 --> 04:09.080
This is basically a way to record the activity of many hundreds of neurons at once, as well

04:09.080 --> 04:13.240
as their dendritic processes in a live animal.

04:13.240 --> 04:19.040
And so we've got recordings of the brains of mice, primary visual cortex.

04:19.040 --> 04:24.520
Well we expose them to stimuli that follow particular statistical patterns, which we then

04:24.520 --> 04:26.480
violate occasionally.

04:26.480 --> 04:34.360
And we have found evidence for very clear and really strong responses to those violations

04:34.360 --> 04:37.800
of the expected stimulus.

04:37.800 --> 04:41.760
And additionally, there are some interesting kind of breakdowns in terms of where those

04:41.760 --> 04:44.960
signals appear in the cortical circuit.

04:44.960 --> 04:49.240
And also some interesting data in terms of the way that it seems to be something that the

04:49.240 --> 04:53.400
animals actually learn over multiple exposures to the stimuli.

04:53.400 --> 04:57.800
So you're conditioning mice to expect some kind of response, then you kind of take a

04:57.800 --> 05:02.400
left turn when they're expecting a right so to speak and you're observing what's going

05:02.400 --> 05:05.240
on in their brains as a result.

05:05.240 --> 05:07.480
And so what you found is what?

05:07.480 --> 05:12.800
So what we've found is we've examined two different types of stimuli, one which is where

05:12.800 --> 05:18.560
you've got a consistent visual flow in the screen as it were.

05:18.560 --> 05:22.800
So there's these bricks that kind of drift across the screen in a particular direction.

05:22.800 --> 05:25.200
And they're always consistent in that movement.

05:25.200 --> 05:29.280
And then occasionally some of the bricks will start moving in the different direction than

05:29.280 --> 05:32.120
the expected one.

05:32.120 --> 05:37.640
Suttily or like pretty clearly, and like you notice it when you watch the stimuli yourself

05:37.640 --> 05:39.960
very much so.

05:39.960 --> 05:45.040
And for those stimuli, we see a massive response in a particular part of the neocortical

05:45.040 --> 05:46.120
micro circuit.

05:46.120 --> 05:51.560
The cells go nuts in response to this stimulus and this seems to happen right off the

05:51.560 --> 05:54.000
bat with no training.

05:54.000 --> 05:58.880
So that suggests that this particular type of violation of expected stimuli is something

05:58.880 --> 06:02.280
that the circuit is hardwired to detect.

06:02.280 --> 06:07.800
But we also have another set of stimuli where we present basically these random patches

06:07.800 --> 06:14.480
of edges that are all sampled from where the orientation of the edges are all sampled

06:14.480 --> 06:16.400
from a particular distribution.

06:16.400 --> 06:21.760
And then occasionally we violate that expectation by sampling from a different distribution for

06:21.760 --> 06:23.440
the orientations.

06:23.440 --> 06:29.480
And when we present these stimuli to the animal, at first we don't see any responses to the

06:29.480 --> 06:32.400
unexpected orientations.

06:32.400 --> 06:37.000
But over multiple recording sessions, we start to see huge responses to the unexpected

06:37.000 --> 06:38.640
orientations.

06:38.640 --> 06:45.600
So what's interesting about this is it suggests that the circuit is able to learn as it were

06:45.600 --> 06:48.680
to be surprised to particular types of stimuli.

06:48.680 --> 06:55.920
And it might, at the same time, be hard-coded to respond to particular other types of violations.

06:55.920 --> 07:00.960
We hypothesize that this might have to do with ultimately the evolutionary purpose of most

07:00.960 --> 07:06.320
visual cortex, one of which would obviously be to help the most avoid predators.

07:06.320 --> 07:11.560
So it's very important that you detect violations of visual flow in your visual field if you're

07:11.560 --> 07:15.160
trying to avoid predators, because that's something you want to avoid potentially.

07:15.160 --> 07:19.480
You want to see that hawk flying above or that's right, exactly.

07:19.480 --> 07:22.760
But for the other type of stimuli that we're showing them, where it's these oriented

07:22.760 --> 07:27.520
edges that can violate these patterns, what's interesting is that they don't show that response

07:27.520 --> 07:29.760
right away, but they learn to show it.

07:29.760 --> 07:36.400
And so that is some evidence that their neocortex is, in fact, a sort of generative model that

07:36.400 --> 07:42.040
can learn the data distribution over time and learn to be surprised when data doesn't

07:42.040 --> 07:44.520
actually adhere to that distribution.

07:44.520 --> 07:51.400
In the first case, where you've got more of a stark difference in the visual pattern,

07:51.400 --> 07:55.360
do they become desensitized to it over time?

07:55.360 --> 07:59.560
We don't actually see any evidence for desensitization, which is interesting.

07:59.560 --> 08:04.960
The signal continues to be very robust over three different days of recording sessions,

08:04.960 --> 08:06.880
and each session is an hour long.

08:06.880 --> 08:12.640
So even after many reputed exposures of this, they still seem to signal this very strongly,

08:12.640 --> 08:17.440
which again suggests that for that particular type of stimulus, this is a hard-wired component

08:17.440 --> 08:18.440
to certain extent.

08:18.440 --> 08:22.240
Which is very consistent with the evolutionary, like if you've got desensitized hawks,

08:22.240 --> 08:26.600
that'd probably be a bad thing if you're not exactly.

08:26.600 --> 08:27.600
Exactly.

08:27.600 --> 08:31.760
And so there was another element of this work, or at least one that you haven't gone

08:31.760 --> 08:37.200
into this level of detail yet, that is talking about the hierarchical nature of inference

08:37.200 --> 08:44.360
in these circuits, is that a kind of an ancillary result, or is that core to the model that

08:44.360 --> 08:46.040
you've developed to understand this stuff?

08:46.040 --> 08:52.240
Yeah, so the thing that I didn't mention is that what's interesting is that that second

08:52.240 --> 08:57.960
type of surprise signal that we see, that the animals learn to be surprised to the orientation

08:57.960 --> 09:05.240
of edges that occur in an unexpected way, we actually see that signal not in the neurons

09:05.240 --> 09:10.800
themselves, but in the dendritic trees of the neurons, and in particular part of the

09:10.800 --> 09:13.560
dendritic trees, that is the area...

09:13.560 --> 09:17.320
That's right, it's being like the fingers that we see in our depiction of the nerves.

09:17.320 --> 09:18.840
Yes, that's right, exactly.

09:18.840 --> 09:22.800
All those little branches that come out of the dendrite out of the neurons, those are

09:22.800 --> 09:28.640
dendrites, and those are the sites of synaptic inputs to real neurons.

09:28.640 --> 09:35.200
But real, pure middle neurons in the neocortex, which is a particular type of neuron that comprises

09:35.200 --> 09:43.840
75% to 80% of the neurons in the neocortex, and it's the key information processing cell

09:43.840 --> 09:52.240
type in this circuit, these cells have one unique dendritic process called the apical dendrite,

09:52.240 --> 09:53.240
which they send up...

09:53.240 --> 09:54.240
Apical?

09:54.240 --> 09:55.240
Yeah, apical.

09:55.240 --> 10:00.200
And they kind of like a tree, because what they do is they send it up to the top, the surface

10:00.200 --> 10:06.080
of the brain, almost like what the trunk of a tree does to get the leaves up to the sunlight.

10:06.080 --> 10:09.880
But in this case, they send it up to the surface of the brain, and what they receive at this

10:09.880 --> 10:17.600
location are the top-down inputs, so higher order information from other parts of the brain.

10:17.600 --> 10:19.520
And our data suggests...

10:19.520 --> 10:25.000
So that is actually where we see those surprise signals that are learned.

10:25.000 --> 10:26.000
And from some additional analysis...

10:26.000 --> 10:29.120
At the tops of the brain, or in this structure overall.

10:29.120 --> 10:34.600
In this dendritic structure that is up at the top of the brain, and what that data suggests

10:34.600 --> 10:42.040
and some of our other analyses suggest is that this surprise signal, that violated my

10:42.040 --> 10:48.400
expectations signal that the animals learn, is being driven by top-down inputs.

10:48.400 --> 10:52.880
So that suggests that the entire model that they have for the world that they're learning

10:52.880 --> 10:59.080
is a hierarchical model, where it's actually the higher order parts of the network, if you

10:59.080 --> 11:03.960
will, for machine learning people, you can think of it as the upper layers of the network

11:03.960 --> 11:08.360
that are actually detecting the violation of the expected statistics, and then they are

11:08.360 --> 11:13.160
communicating that back down the hierarchy to the lower layers of the network.

11:13.160 --> 11:20.880
Is there a relationship between this hierarchical nature of inference that you've observed

11:20.880 --> 11:30.080
and the idea of spiking in neural nets, where a signal becomes significant after a certain

11:30.080 --> 11:35.440
number of times for lack of a scientific one?

11:35.440 --> 11:40.000
You know where there's like a build-up of the signal and it crosses some threshold?

11:40.000 --> 11:41.000
Potentially.

11:41.000 --> 11:49.600
I mean, we do find that the surprise response that we see in this circuit takes around 300

11:49.600 --> 11:54.360
milliseconds to appear after the initial surprising stimulus.

11:54.360 --> 11:58.440
And to put that into perspective, that's a pretty long time.

11:58.440 --> 12:03.840
Usually it takes around 50 milliseconds or so for visual stimuli to get into visual cortex.

12:03.840 --> 12:04.840
So that means...

12:04.840 --> 12:06.440
And this is the second case, as opposed to...

12:06.440 --> 12:10.760
This is the second case where the animals learn this signal, that's right.

12:10.760 --> 12:16.600
And what that suggests is that there is a whole build-up of activity.

12:16.600 --> 12:21.960
There's additional stuff going on before the surprising stimulus gets noticed by the

12:21.960 --> 12:23.720
circuit as it were.

12:23.720 --> 12:29.880
Now that might be a build-up of spikes, but it also might just be because the information

12:29.880 --> 12:33.320
has to be propagated up the hierarchy and then back down the hierarchy.

12:33.320 --> 12:38.600
And each synaptic step takes an extra 10 milliseconds kind of thing.

12:38.600 --> 12:43.840
So it suggests that there's potentially a fairly large hierarchy or very recurrent

12:43.840 --> 12:49.720
circuits that are responsible for calculating this surprise signal.

12:49.720 --> 12:58.880
Are you distinguishing or able to distinguish in your work something akin to learning versus

12:58.880 --> 13:03.800
realizing or paying attention to or other things that may be happening?

13:03.800 --> 13:07.280
So that's an interesting question.

13:07.280 --> 13:11.960
The question of whether or not they're just paying more attention to it, we try to

13:11.960 --> 13:17.280
get at by measuring a variety of behavioral signals, including, say, for example, the

13:17.280 --> 13:19.440
dilation of their pupils.

13:19.440 --> 13:24.120
And we don't have any evidence to suggest that they are more attentive when we see these

13:24.120 --> 13:25.120
signals.

13:25.120 --> 13:29.400
Their pupil dilation is no different from the other times that it occurred and we didn't

13:29.400 --> 13:30.880
see the signal.

13:30.880 --> 13:34.440
We also look at their running speed and we don't see any change in their running speed,

13:34.440 --> 13:36.760
so it suggests that it's not like a treadmill kind of thing.

13:36.760 --> 13:37.760
They're on a treadmill.

13:37.760 --> 13:38.760
That's right.

13:38.760 --> 13:44.400
So just that they're not like, I don't know, just like kind of wigged out and they respond

13:44.400 --> 13:47.360
differently and this drives the response.

13:47.360 --> 13:53.440
Instead, there seems to be some kind of build up of the calculation that implies learning.

13:53.440 --> 13:56.480
But you may have just answered this.

13:56.480 --> 13:57.480
Yes.

13:57.480 --> 14:03.240
But is the amplitude of the response the same as the in the first case?

14:03.240 --> 14:04.240
It's not surprised.

14:04.240 --> 14:05.240
It's just they.

14:05.240 --> 14:06.240
Right.

14:06.240 --> 14:11.920
So the amplitude is the same as in the first case where we have that violation of visual

14:11.920 --> 14:12.920
flow.

14:12.920 --> 14:13.920
Okay.

14:13.920 --> 14:15.160
But it's in a different part of the circuit.

14:15.160 --> 14:21.520
So in the first case, it's actually the cell bodies that are signalling this, a particular

14:21.520 --> 14:23.800
type of cell body.

14:23.800 --> 14:28.440
And that suggests that in that first case, that's a calculation that's happening potentially

14:28.440 --> 14:31.120
locally within the circuit.

14:31.120 --> 14:36.840
Whereas in the second case, this is, as I said, the signal is in these dendrites that

14:36.840 --> 14:42.040
are receiving information from higher up, the cortical hierarchy.

14:42.040 --> 14:46.920
So suggesting that it's something that's part of the hierarchical calculations rather

14:46.920 --> 14:48.960
than a local calculation.

14:48.960 --> 14:53.200
Now that you've made this discovery, like, what's next in this line of research, what

14:53.200 --> 14:54.520
does it tell you?

14:54.520 --> 15:00.920
So an example of the kind of thing that this tells us and, you know, obviously one of

15:00.920 --> 15:03.480
the things we're interested in is just understanding the brain.

15:03.480 --> 15:11.560
And so, you know, this is evidence that indeed the brain has a hierarchical predictive model,

15:11.560 --> 15:14.200
a generative model as it were.

15:14.200 --> 15:18.720
And, you know, next steps will be trying to understand how this generative model works,

15:18.720 --> 15:24.440
more fully explore new stimulus space, understand the type, the role of different cell types

15:24.440 --> 15:27.600
and modulators in this model, et cetera.

15:27.600 --> 15:32.400
But as well, one of the other things that we're very interested in is using the lessons that

15:32.400 --> 15:37.480
we gain from neurobiology to inform new machine learning designs.

15:37.480 --> 15:45.680
So one of the things that we're interested in exploring further is developing new unsupervised

15:45.680 --> 15:49.520
learning models based on some of what we learn here.

15:49.520 --> 15:53.000
And there are a few different avenues that we're interested in exploring this way.

15:53.000 --> 15:58.880
So one is we think, and it's a little bit too complicated to go into, I think, here,

15:58.880 --> 16:01.840
but we've got to at least try.

16:01.840 --> 16:02.840
Okay.

16:02.840 --> 16:12.800
So there is another type of cell in the circuit that is responsible for sending inhibitory

16:12.800 --> 16:17.880
signals to these dendrites where we see these surprise responses.

16:17.880 --> 16:22.120
And we have been trying to rack our brains about, okay.

16:22.120 --> 16:24.360
So what is happening in a circuit?

16:24.360 --> 16:26.640
Why would it learn this surprise?

16:26.640 --> 16:29.920
And we wondered if one of the reasons it might learn this surprise is because there were

16:29.920 --> 16:35.200
changes in this other cell type that were inhibiting the dendrites.

16:35.200 --> 16:40.200
And in particular, what we've started to get interested in is that there might actually

16:40.200 --> 16:45.120
be a connection with these results with a class of unsupervised learning models known

16:45.120 --> 16:47.680
as contrast of predictive coding.

16:47.680 --> 16:55.120
So contrast of predictive coding is one approach to doing these sorts of next frame prediction

16:55.120 --> 16:56.520
models.

16:56.520 --> 17:01.640
And what they do in contrast of predictive coding is technically they're trying to maximize

17:01.640 --> 17:07.960
the mutual information between a higher order context variable and incoming stimuli.

17:07.960 --> 17:14.480
But the way that they do this is basically they compare a top down prediction from up the

17:14.480 --> 17:17.080
hierarchy with the data.

17:17.080 --> 17:24.280
And then they compare that match to the extent to which the prediction matches other data

17:24.280 --> 17:28.200
samples from the same distribution, but not the appropriate frame.

17:28.200 --> 17:30.680
These are called negative samples.

17:30.680 --> 17:35.400
And from a machine learning perspective, this has all sorts of advantages.

17:35.400 --> 17:39.160
But one of the things that we got thinking about from a neuroscience perspective is what

17:39.160 --> 17:43.040
if is potentially happening in the circuit is basically that this other cell type that

17:43.040 --> 17:49.840
normally inhibits these dendrites is having to actually learn that this is no longer a

17:49.840 --> 17:52.320
sort of negative sample.

17:52.320 --> 17:58.440
And so we're starting to build ML models where what we do is we take a sort of contrast

17:58.440 --> 18:03.880
of predictive approach, but we have alternative circuits that are learning the data distribution

18:03.880 --> 18:09.400
for the negative samples and using that to compare to the positive samples.

18:09.400 --> 18:13.600
And this has potential interesting applications for online learning because you could do this

18:13.600 --> 18:17.840
now in a totally online manner real time having to live.

18:17.840 --> 18:18.840
Yes.

18:18.840 --> 18:23.800
And without having to store previous examples or anything like that, which might be interesting

18:23.800 --> 18:32.400
for kind of later extensions of this work to edge computing in the field kind of applications.

18:32.400 --> 18:39.200
Is this related to like the concept of forgetfulness in a sense, thinking of like forget

18:39.200 --> 18:42.760
gates and things like that that have come up in conversations in the past?

18:42.760 --> 18:43.760
Right.

18:43.760 --> 18:44.760
That's interesting.

18:44.760 --> 18:47.400
I mean, it's not directly related.

18:47.400 --> 18:51.960
There have been people who have postulated that these other types of cells that I mentioned,

18:51.960 --> 18:57.960
these inhibitory cells might be involved in something like forget gates and output gates.

18:57.960 --> 19:00.080
And I think that's possible.

19:00.080 --> 19:05.760
What we're exploring here in particular is the idea that these cells are as it were,

19:05.760 --> 19:08.760
you can think of it more like a GAN scenario.

19:08.760 --> 19:16.560
So the idea is that you're going to generate predictions from up in your hierarchy.

19:16.560 --> 19:21.840
And then what you want to do to train your predictions is you want to not only have the

19:21.840 --> 19:25.800
data that you're trying to predict, but also some negative samples that are going to

19:25.800 --> 19:28.280
try to fool your predictions as it were.

19:28.280 --> 19:32.800
And so then what you want to do is you want to train simultaneously your predictions to

19:32.800 --> 19:38.720
get better at differentiating the true data from the false data that you're generating,

19:38.720 --> 19:43.320
but you also want to train that false data generator to generate better, better, better

19:43.320 --> 19:44.320
false data.

19:44.320 --> 19:45.320
That's the GAN analogy.

19:45.320 --> 19:46.320
Yeah.

19:46.320 --> 19:47.320
That's right.

19:47.320 --> 19:52.680
So we're thinking of this as more like that these inhibitory cells are effectively trying

19:52.680 --> 19:57.680
to produce false stimuli to fool the predictive system.

19:57.680 --> 20:02.160
And then the predictive systems job is to try to learn that this is false and what's

20:02.160 --> 20:03.160
reality.

20:03.160 --> 20:06.800
It's kind of amazing to relate all of the machinery that goes into a GAN to a single

20:06.800 --> 20:07.800
cell.

20:07.800 --> 20:08.800
Yeah.

20:08.800 --> 20:11.640
Or a population of cells, but this is my work.

20:11.640 --> 20:12.640
That's right.

20:12.640 --> 20:13.640
Yes, quite.

20:13.640 --> 20:14.640
Quite.

20:14.640 --> 20:22.000
Is there an element of what's interesting here that in kind of traditional inference if

20:22.000 --> 20:28.600
that makes any sense like that you have only positive signal and here you've got kind

20:28.600 --> 20:30.400
of both positive and negative signal.

20:30.400 --> 20:31.400
That's right.

20:31.400 --> 20:35.320
That serves a really useful purpose for training.

20:35.320 --> 20:40.720
And this is part of the reason that contrastive predictive coding works well.

20:40.720 --> 20:45.760
In contrasting the positive samples to the negative samples, you can end up doing a lot

20:45.760 --> 20:53.280
better at capturing more interesting distributions in your predictive system.

20:53.280 --> 20:55.440
No, why is that?

20:55.440 --> 20:58.000
Well, so there's a variety of reasons.

20:58.000 --> 21:04.960
One reason is there are these, so the classic predictive models in neuroscience and also

21:04.960 --> 21:07.560
some of the predictive models in machine learning.

21:07.560 --> 21:09.960
What you do is you generate a prediction.

21:09.960 --> 21:15.040
You compare the prediction to your incoming data and you take the difference.

21:15.040 --> 21:21.560
And then that difference is your error signal and you can pass that back up the hierarchy

21:21.560 --> 21:24.640
in order to do more training of your prediction.

21:24.640 --> 21:29.720
So these are called predictive coding models and it's based on a particular theory from

21:29.720 --> 21:35.600
some guy's name Rowan Ballard that goes back to 1999, but which a lot of neuroscientists

21:35.600 --> 21:40.040
are interested in and a few machine learning people, you know, David Cox's group at IBM

21:40.040 --> 21:45.680
has built the prednet, which is basically just a stacked predictive coding system.

21:45.680 --> 21:49.480
And these systems can do interesting things and they can do frame prediction.

21:49.480 --> 21:53.560
But arguably there are some difficulties with frame as in an image.

21:53.560 --> 21:54.560
Yes, sorry.

21:54.560 --> 21:58.920
So like you've got a movie and you feed in a bunch of frames of the movie and then the

21:58.920 --> 22:02.640
job of the system is to predict the next frame of the movie.

22:02.640 --> 22:09.160
And do you mention that to say that that's one of the things that they're good at or that's

22:09.160 --> 22:11.600
one of the things that they're better at than other things?

22:11.600 --> 22:16.080
Well, so this is what they're designed to do, these sorts of predictive coding models.

22:16.080 --> 22:17.080
Got it.

22:17.080 --> 22:23.320
But there's a slight problem with them, which is that you have this situation where if you've

22:23.320 --> 22:29.120
got a, so if you have a unimodal probability distribution over your data, these models

22:29.120 --> 22:31.200
can work very well.

22:31.200 --> 22:37.560
But once you have complicated multimodal distributions, it starts to break down a little bit.

22:37.560 --> 22:42.760
And the reason is that if you think about it this way, okay, so let's say I have a complicated

22:42.760 --> 22:49.520
multimodal data distribution, I make a prediction and I get, I make a prediction that it's going

22:49.520 --> 22:53.960
to be on a particular mode and then I get data from another mode.

22:53.960 --> 22:59.400
And what I'm going to do then in these standard systems is I'm going to try to reduce the difference

22:59.400 --> 23:01.640
between my prediction and reality.

23:01.640 --> 23:06.440
But what that's going to do is that's going to bring me off the original mode that I predicted

23:06.440 --> 23:11.640
into some in between space that might actually have very low probability.

23:11.640 --> 23:14.120
So that's not actually what I want to do.

23:14.120 --> 23:18.640
Instead what I want to understand is that I just sampled from the wrong mode there effectively

23:18.640 --> 23:23.400
and I should have been predicting the other mode for that data.

23:23.400 --> 23:30.160
So potentially what you want is you want to be able to contrast your predictions to not

23:30.160 --> 23:36.600
only the incoming data you get, but to all the other negative data distributions from the

23:36.600 --> 23:42.120
data that you have received to know basically whether or not the problem was that you were

23:42.120 --> 23:50.840
in fact predicting a wrong part of the data as opposed to simply just not the right piece

23:50.840 --> 23:52.320
of data.

23:52.320 --> 23:59.920
And so by actually doing this contrastive thing you can end up doing a much better job

23:59.920 --> 24:04.800
of making predictions with complicated probability distributions than you can with a standard

24:04.800 --> 24:09.840
kind of just like take the difference and learn on that model.

24:09.840 --> 24:15.200
So is that also imply that it's better when the distributions itself are shifting or

24:15.200 --> 24:18.680
themselves are shifting as opposed to static distributions?

24:18.680 --> 24:21.280
Potentially that's an interesting question.

24:21.280 --> 24:26.960
I think that the, now obviously you have to be able to deal with shifting distributions

24:26.960 --> 24:33.160
for doing movie prediction because necessarily the distribution for a frame is distributed

24:33.160 --> 24:37.200
according to a conditional probability distribution based on the other frames and that's

24:37.200 --> 24:38.880
what you're trying to learn.

24:38.880 --> 24:46.400
I think there's the other interesting question though which is if you have a shifting dynamics.

24:46.400 --> 24:51.800
So the way in which frames depend on past frames changes over time.

24:51.800 --> 24:53.160
Can you learn that?

24:53.160 --> 24:58.320
I am not sure that some of these standard systems could deal with that that might be require

24:58.320 --> 25:01.120
additional machinery for such networks.

25:01.120 --> 25:06.400
It's also an interesting question as to whether or not animals can do that because arguably,

25:06.400 --> 25:10.080
I mean probably animals could but generally test for that.

25:10.080 --> 25:14.360
Well, yeah, you see the funny thing is how would you test for that because generally

25:14.360 --> 25:21.440
our world though it can be changing and though the actual kind of distributions of data

25:21.440 --> 25:28.160
that we receive are hardly stationary, the dynamics in the world are relatively stationary.

25:28.160 --> 25:35.200
So objects tend to move in consistent ways and it's not like the laws of physics occasionally

25:35.200 --> 25:37.680
switch on us.

25:37.680 --> 25:42.720
And so as a result, it's not clear that the brain has to be able to deal with shifting dynamics

25:42.720 --> 25:43.720
necessarily.

25:43.720 --> 25:49.760
But at the same time, things will happen in certain types of movies that are unexpected

25:49.760 --> 25:52.440
in a real world but we still kind of expect them.

25:52.440 --> 25:53.800
Yes, and we can get used to them.

25:53.800 --> 25:55.120
And we get used to them, right?

25:55.120 --> 25:56.120
That's right.

25:56.120 --> 25:59.560
And so that's why I was about to correct myself as even though it seems like maybe animals

25:59.560 --> 26:05.080
don't have to deal with that, our own intuition would be that we seem to be able to deal

26:05.080 --> 26:09.520
with that because let's say if we play a video game where the laws of physics are indeed

26:09.520 --> 26:14.440
a bit different, over time we will get used to that video game and we will kind of understand

26:14.440 --> 26:15.440
its dynamics.

26:15.440 --> 26:16.440
Right.

26:16.440 --> 26:19.640
And so this gets back to the idea that, you know, and this is a discussion that we have

26:19.640 --> 26:25.200
in the sort of neural AI intersection a lot is how much is hard-coded, how much is

26:25.200 --> 26:32.120
innate, and what can we learn and what can we not learn, and even though it is like

26:32.120 --> 26:38.600
indeniable that evolution endows us with a massive innate structure to help us understand

26:38.600 --> 26:39.600
the world.

26:39.600 --> 26:45.720
A lot of our core facilities also seem to be learnable and tunable.

26:45.720 --> 26:50.920
And so even something as simple as kind of being able to predict what stimuli you're

26:50.920 --> 26:53.360
going to receive in the near future.

26:53.360 --> 26:57.360
Our data, for example, suggests that that's something tunable and it's possible that

26:57.360 --> 27:03.320
also the dynamics of these of the world is tunable for our models.

27:03.320 --> 27:07.960
Kind of going back to this particular research and the hierarchical inference result and

27:07.960 --> 27:09.920
some of these other results.

27:09.920 --> 27:15.240
How close are you to having an implementation of that in the machine learning world?

27:15.240 --> 27:20.520
So it's something that we're working on and I've got a couple of students who are trying

27:20.520 --> 27:24.920
to build these systems where the negative examples are being learned by an additional

27:24.920 --> 27:31.440
network that's doing the off-distribution estimation.

27:31.440 --> 27:34.400
And I think that we've made some progress on that.

27:34.400 --> 27:37.240
We're still probably a ways away from it.

27:37.240 --> 27:43.720
We're in the grand scheme of things, a small academic lab compared to the big ML outfits

27:43.720 --> 27:46.280
in industry and stuff like that.

27:46.280 --> 27:52.080
So it'll probably take us longer if I could take them, even just because of the compute

27:52.080 --> 27:54.600
resources that we have to compete for.

27:54.600 --> 28:00.200
But I think that hopefully within the year we'll have a working model of the basic thing

28:00.200 --> 28:05.600
and we'll be able to say whether or not this actually works well in practice or if it's

28:05.600 --> 28:09.160
just a bad idea.

28:09.160 --> 28:15.480
Separating this particular result with the negative examples from the concept of hierarchical

28:15.480 --> 28:18.400
instance, like how well has that idea been explored?

28:18.400 --> 28:25.000
And I've got to imagine that there are examples of that or prior research into that idea or

28:25.000 --> 28:26.240
is that novel in some way.

28:26.240 --> 28:28.040
I think it's actually reasonably novel.

28:28.040 --> 28:31.760
I haven't seen a lot of stuff this way for predictive models.

28:31.760 --> 28:37.480
Obviously, there's a lot of work now in training adversarial systems where you generate sort

28:37.480 --> 28:44.440
of negative data that you have to discriminate from positive data and that's just across

28:44.440 --> 28:52.000
the amount of tons of this stuff, but using this specifically to do the predictions in

28:52.000 --> 28:57.360
time series, I haven't seen yet, but I can't keep on top of all literature.

28:57.360 --> 29:02.200
And so as I say this, I'm sure someone's publishing exactly this.

29:02.200 --> 29:03.200
Right, right.

29:03.200 --> 29:04.800
There's a talk happening right now.

29:04.800 --> 29:06.800
Yeah, that's right.

29:06.800 --> 29:07.800
That's right.

29:07.800 --> 29:08.800
Yeah.

29:08.800 --> 29:09.800
Nice.

29:09.800 --> 29:13.080
So what are some of the other things that your live is working on?

29:13.080 --> 29:18.840
So another area that we're really interested in is the use of memory systems for reinforcement

29:18.840 --> 29:20.160
learning.

29:20.160 --> 29:23.560
And this is again, motivated by the neuroscience literature.

29:23.560 --> 29:27.120
So one of the systems like attention or...

29:27.120 --> 29:31.840
So attention and memory are, you know, kind of intimately related, but let's maybe keep

29:31.840 --> 29:36.320
it simple for a moment and just focus on classical memory systems.

29:36.320 --> 29:39.320
And what I mean by that is to give you a sense.

29:39.320 --> 29:45.160
So the region of the brain that is typically associated with what we call episodic memories

29:45.160 --> 29:47.840
is the hippocampus.

29:47.840 --> 29:54.280
Episodic memories just refer to actual memories of events from your life where you can say

29:54.280 --> 29:55.280
stuff that happened.

29:55.280 --> 30:01.520
You can say X occurred in this location with these people there at this time.

30:01.520 --> 30:04.480
And that depends on this brain region, the hippocampus.

30:04.480 --> 30:09.440
We know that from, you know, decades of neurology research.

30:09.440 --> 30:15.160
People who have their hippocampus damaged can't form new episodic memories.

30:15.160 --> 30:21.400
What the idea in applying reinforcement learning is, if in addition to remembering a kind

30:21.400 --> 30:26.880
of a current cumulative reward, I can remember stuff that I did and stuff that happened.

30:26.880 --> 30:27.880
That's right.

30:27.880 --> 30:30.760
I should be able to converge faster to some kind of...

30:30.760 --> 30:32.080
Yes, right.

30:32.080 --> 30:35.720
So what's interesting is in the neuroscience field, there's been more and more evidence

30:35.720 --> 30:39.800
suggesting that the hippocampus is involved in reinforcement learning in animals.

30:39.800 --> 30:45.200
And indeed the hippocampus makes direct projections to some of the core circuits in a region

30:45.200 --> 30:50.560
called the basal ganglia that seem to be key for reinforcement learning.

30:50.560 --> 30:56.680
And various theoreticians have posited that the hippocampus in providing this sort of episodic

30:56.680 --> 31:00.800
record could be used for bootstrapping reinforcement learning.

31:00.800 --> 31:01.800
Basically...

31:01.800 --> 31:07.400
The hippocampus predicts or does it just serve up experiences?

31:07.400 --> 31:10.480
Is it just memory or is there some inference in there too?

31:10.480 --> 31:12.200
That's a great question.

31:12.200 --> 31:18.360
So the original proposal, which was made by Peter Dianne and Matti Lengiel back in the

31:18.360 --> 31:24.160
mid-2000s, was that it just provides you with a record of explicit trajectories that

31:24.160 --> 31:27.040
you've done through the state space, basically.

31:27.040 --> 31:33.680
And they posted it by taking an explicit record of trajectories you've done, then you can

31:33.680 --> 31:38.800
just rely upon like, okay, well, I did this last time and I got something okay so I can

31:38.800 --> 31:39.800
just do it again.

31:39.800 --> 31:44.240
And they showed that this could speed up reinforcement learning in the kind of early phases or just

31:44.240 --> 31:46.840
after a change occurs.

31:46.840 --> 31:53.280
But subsequent work in neuroscience has suggested that the hippocampus is really key for in

31:53.280 --> 31:59.560
fact making predictions and making inferences and doing imagination, kind of doing forward

31:59.560 --> 32:02.840
rollouts in your mind.

32:02.840 --> 32:04.680
And so I think that...

32:04.680 --> 32:07.120
How do you test for that?

32:07.120 --> 32:13.320
Well, so the way that they test for this, you know, there's a bunch of different ways,

32:13.320 --> 32:17.400
but fundamentally what you do is you record from the hippocampus well an animal or a person

32:17.400 --> 32:24.120
is doing something and you train a decoder to take hippocampal activity and say what the

32:24.120 --> 32:28.320
person was looking at or where the animal was in the environment.

32:28.320 --> 32:33.440
And then you can show that when you then put the person back in this task or the animal

32:33.440 --> 32:36.880
back in this environment and you have them go around and you're recording from their

32:36.880 --> 32:44.320
hippocampus, there will be moments where your decoder says, oh, what's actually happening

32:44.320 --> 32:49.520
right now is that the person is looking a few frames ahead in the sequence or the animal

32:49.520 --> 32:53.440
is down the hallway to the right kind of thing.

32:53.440 --> 32:58.720
And so that suggests that the activity in the hippocampus is basically following a forward

32:58.720 --> 33:04.760
trajectory through space or time, which is pretty cool.

33:04.760 --> 33:08.520
Yeah, that's amazing, totally.

33:08.520 --> 33:14.600
And so there's also other evidence suggesting that the representations in the hippocampus

33:14.600 --> 33:17.080
themselves are in fact predictive.

33:17.080 --> 33:21.120
There's within reinforcement learning that this was another contribution from Peter

33:21.120 --> 33:22.120
Diane.

33:22.120 --> 33:28.000
There's a particular type of representation called successor representations, the successor

33:28.000 --> 33:29.000
successor.

33:29.000 --> 33:30.000
That's right.

33:30.000 --> 33:36.760
And there are a way of representing state space that allows you to do one shot adaptation

33:36.760 --> 33:39.720
to changes in the reward function.

33:39.720 --> 33:44.720
And what's, the way they do that is basically they pull out the transition dynamics from

33:44.720 --> 33:52.400
the MDP out of the expectation for your value calculation or rather pulls the reward out

33:52.400 --> 33:53.400
of the expectation.

33:53.400 --> 33:57.360
And that means that now when you redo your reward calculations, you don't need to worry

33:57.360 --> 34:02.080
about averages, which speeds things up.

34:02.080 --> 34:06.120
And so, so anyway, what's interesting is Kim Stockenfeld and Sam Gershman showed in

34:06.120 --> 34:12.000
a paper, I guess, either two years ago or one year ago now, that the representations

34:12.000 --> 34:18.520
in the hippocampus seem to adhere in many ways to successor representation type dynamics

34:18.520 --> 34:21.960
and predictions.

34:21.960 --> 34:27.720
And so what's interesting is successor representations are basically a way of representing the environment

34:27.720 --> 34:32.600
in a predictive manner because how they do this is they represent each state according

34:32.600 --> 34:38.840
to the expected future occupancy of that state from your current state.

34:38.840 --> 34:43.920
So it's kind of saying that there's evidence that fundamentally the hippocampus isn't

34:43.920 --> 34:51.080
just a kind of snapshot of scenes, but it's an inherently predictive representation

34:51.080 --> 34:57.200
of this scene kind of happened and this thing kind of resulted from whatever action you

34:57.200 --> 34:58.200
took.

34:58.200 --> 34:59.200
Right.

34:59.200 --> 35:02.360
So, so what's interesting is that that's roughly right, but I would rephrase it slightly.

35:02.360 --> 35:07.160
I would say that it looks like the hippocampus is not just a record of what you've seen

35:07.160 --> 35:12.960
or done, but it is a record that is encoded in a manner that also predicts what you are

35:12.960 --> 35:15.120
going to now do in the future.

35:15.120 --> 35:20.280
It's like taking your past to predict your future at a very high level representation

35:20.280 --> 35:22.600
of the environment.

35:22.600 --> 35:26.960
And this successor was a successor representation.

35:26.960 --> 35:32.480
That's kind of the machine learning approximate of that or analog.

35:32.480 --> 35:33.480
That's right.

35:33.480 --> 35:34.480
Okay.

35:34.480 --> 35:37.080
So this, you know, the successor representation is just an idea from machine learning.

35:37.080 --> 35:38.080
Yeah.

35:38.080 --> 35:42.280
And what's interesting is that it seems to predict a lot of data in the hippocampus.

35:42.280 --> 35:45.320
Now, is the hippocampus actually a successor representation?

35:45.320 --> 35:46.320
Sure.

35:46.320 --> 35:51.440
I don't know, but I think the core idea that part of what the hippocampus is doing is providing

35:51.440 --> 35:56.560
a sort of predictive map for the environment and something that would also allow you to

35:56.560 --> 36:02.120
do more so that it's not just like functionally predicting, but it's representationally

36:02.120 --> 36:03.120
predicting.

36:03.120 --> 36:04.120
That's right.

36:04.120 --> 36:05.120
It's representational.

36:05.120 --> 36:06.120
That's pretty cool.

36:06.120 --> 36:10.680
And so my lab is working on a variety of implementations of reinforcement learning that

36:10.680 --> 36:17.280
takes advantage of these sorts of systems to then do much faster adaptation to changes

36:17.280 --> 36:18.280
in the environment.

36:18.280 --> 36:19.280
Okay.

36:19.280 --> 36:22.880
So in particular, what we're building, because what's interesting right is you've got,

36:22.880 --> 36:27.640
so the successor representation had been developed previously in machine learning, but then there's

36:27.640 --> 36:32.440
a variety of interesting questions in terms of, well, okay, what if now you are explicitly

36:32.440 --> 36:38.640
storing different memories of different successor representations for different contexts?

36:38.640 --> 36:42.520
And furthermore, what if you can use those memories of different successor representations

36:42.520 --> 36:50.440
for different environments in order to sort of load up your predictions for any new context?

36:50.440 --> 36:54.240
We think that this should be able to help you to do reinforcement learning much more rapidly

36:54.240 --> 36:59.560
and we have some initial data showing that that is indeed the case.

36:59.560 --> 37:05.960
So kind of broadly, the idea is that we want to take the lessons that we're learning about

37:05.960 --> 37:12.640
how memory systems in the brain work and use those to inform the design of new RL systems

37:12.640 --> 37:15.680
that are better at adapting to changes in the environment.

37:15.680 --> 37:16.680
Huh.

37:16.680 --> 37:17.680
Interesting.

37:17.680 --> 37:18.680
Interesting.

37:18.680 --> 37:20.200
Sounds like it's still a little early or do you have an implementation?

37:20.200 --> 37:24.600
So we do have an implementation in toy tasks that works really well and we can basically

37:24.600 --> 37:28.040
take a specific thing or just toy tasks.

37:28.040 --> 37:32.440
Toy tasks, meaning just all the standard kind of stuff.

37:32.440 --> 37:38.440
So grid worlds, Atari games, these sorts of very simple tasks, right?

37:38.440 --> 37:44.240
And what we find is that in very simple tasks, this approach of storing multiple different

37:44.240 --> 37:50.120
successor representations for different contexts and also utilizing what we call

37:50.120 --> 37:55.640
a sort of evolutionary initialization where we learn a sort of average successor representation

37:55.640 --> 38:00.360
across contexts that we use to initialize all the memories, combining these two ideas

38:00.360 --> 38:07.640
leads to RL systems that can do very rapid, just kind of like one-shot adaptation to changes

38:07.640 --> 38:10.640
in the environment, which is cool.

38:10.640 --> 38:16.640
But we need to see if changes are step-to-step or changes to the reward function, critical

38:16.640 --> 38:17.640
changes to the reward.

38:17.640 --> 38:20.200
Also changes to the task.

38:20.200 --> 38:26.200
So if you drop the agent in a new task or within a given task, if you move where the rewards

38:26.200 --> 38:31.880
are located or change the reward function in any way, that it can adapt quickly to this.

38:31.880 --> 38:39.400
And it changes to the reward function inherent to kind of your typical Atari game or is

38:39.400 --> 38:40.400
it...

38:40.400 --> 38:41.400
So that's not something...

38:41.400 --> 38:42.400
Yeah.

38:42.400 --> 38:47.120
So typically we do this, I should say, when we look at changes to the reward function, we're

38:47.120 --> 38:49.720
doing this in little environments that we've made.

38:49.720 --> 38:55.520
And typically it's like foraging tasks where the agent has to go through a little maze

38:55.520 --> 39:00.960
to find rewards and what we do is we change where the rewards are located.

39:00.960 --> 39:05.960
And if you just take a standing where the reward is located, it doesn't necessarily dictate

39:05.960 --> 39:07.600
a new reward function, does it?

39:07.600 --> 39:10.040
Will it die about that correctly?

39:10.040 --> 39:14.440
Well it does because the reward function is typically defined as, okay, for any particular

39:14.440 --> 39:18.760
state and action pair, you're going to get a particular reward.

39:18.760 --> 39:23.240
And so if you change to the location of the reward now, the state action pair that gets

39:23.240 --> 39:26.720
you a reward is different than the one that it was previously.

39:26.720 --> 39:27.720
And if you...

39:27.720 --> 39:29.280
There are a variety of different ways to do this.

39:29.280 --> 39:33.600
So it should be said that this result in and of itself is not super exciting.

39:33.600 --> 39:37.360
Other people have shown ways to adapt to changes in the reward function.

39:37.360 --> 39:40.680
But what we're hoping is that we'll be able to show...

39:40.680 --> 39:45.200
It is early days, but you know, as we scale up, there will be able to show that in more

39:45.200 --> 39:50.160
complicated environments, as you change the context and you drop the agent down into

39:50.160 --> 39:55.520
different contexts, they will always be able to adapt very rapidly by using these sorts

39:55.520 --> 39:58.480
of predictive memories to guide their updates.

39:58.480 --> 39:59.480
Okay.

39:59.480 --> 40:03.840
Yeah, I thought you were talking about like structural changes to the reward function

40:03.840 --> 40:04.840
as opposed to...

40:04.840 --> 40:05.840
I see.

40:05.840 --> 40:06.840
Kind of parameterize.

40:06.840 --> 40:07.840
No, just parameterize.

40:07.840 --> 40:08.840
Okay.

40:08.840 --> 40:09.840
Okay.

40:09.840 --> 40:13.040
And that's what the successor representation is good at dealing with.

40:13.040 --> 40:14.040
Got it.

40:14.040 --> 40:18.120
You can update its parameters to the reward function in one shot very quickly with just a single

40:18.120 --> 40:19.720
multiplicative calculation.

40:19.720 --> 40:20.720
Okay.

40:20.720 --> 40:21.720
Well, sounds like really interesting work.

40:21.720 --> 40:22.720
Thank you.

40:22.720 --> 40:26.520
Yeah, thanks for taking the time to share a bit about what you're up to.

40:26.520 --> 40:27.520
My pleasure.

40:27.520 --> 40:28.520
Thanks very much for having me.

40:28.520 --> 40:29.520
Thank you.

40:29.520 --> 40:32.520
All right, everyone.

40:32.520 --> 40:34.320
That's our show for today.

40:34.320 --> 40:39.760
For more information on today's guest or our NURPS podcast series, head over to Twemal

40:39.760 --> 40:44.040
AI.com slash NURPS 2019.

40:44.040 --> 40:47.960
Thanks once again to Shell for sponsoring this week's series.

40:47.960 --> 40:54.040
Check out the Shell.AI Residency program by typing Shell.AI into your browser's address

40:54.040 --> 40:55.040
bar.

40:55.040 --> 40:57.280
Thanks so much for listening.

40:57.280 --> 41:15.480
Happy Holidays and catch you next time.

