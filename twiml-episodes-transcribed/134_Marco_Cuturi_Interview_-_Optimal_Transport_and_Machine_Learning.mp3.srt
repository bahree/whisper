1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,440
I'm your host Sam Charrington.

4
00:00:31,440 --> 00:00:37,960
Alright Twimble listeners, this is your last chance to register for my upcoming AI Summit.

5
00:00:37,960 --> 00:00:43,880
The event takes place next week, April 30th and May 1st in Las Vegas, Nevada, in conjunction

6
00:00:43,880 --> 00:00:46,920
with the Interop ITX Conference.

7
00:00:46,920 --> 00:00:51,600
I'll be presenting an introductory session on ML and AI Fundamentals and we'll have

8
00:00:51,600 --> 00:00:57,880
deep dive sessions on AI for NLP and conversational applications, computer vision and Internet

9
00:00:57,880 --> 00:00:59,200
of Things.

10
00:00:59,200 --> 00:01:04,320
Then we'll be discussing more strategic topics including data collection and annotation,

11
00:01:04,320 --> 00:01:11,320
operationalizing and managing AI and building out your organization's AI culture and strategy.

12
00:01:11,320 --> 00:01:15,680
Attendees will also have a chance to discuss their organization's unique AI opportunities

13
00:01:15,680 --> 00:01:20,040
and challenges with our panel of distinguished experts.

14
00:01:20,040 --> 00:01:26,120
To learn more about the event, head on over to Twimbleai.com slash AI Summit for Details

15
00:01:26,120 --> 00:01:29,600
and a Discount Code.

16
00:01:29,600 --> 00:01:34,920
In this episode, I'm joined by Marco Couturi, professor of Statistics at University Paris

17
00:01:34,920 --> 00:01:36,240
Saclay.

18
00:01:36,240 --> 00:01:40,640
Marco and I spent some time discussing his work on Optimal Transport Theory at NIPS last

19
00:01:40,640 --> 00:01:41,960
year.

20
00:01:41,960 --> 00:01:46,560
In our discussion, Marco explains Optimal Transport which provides a way for us to compare

21
00:01:46,560 --> 00:01:48,680
probability measures.

22
00:01:48,680 --> 00:01:53,480
We look at ways Optimal Transport can be used across machine learning applications including

23
00:01:53,480 --> 00:01:57,000
graphical NLP and image examples.

24
00:01:57,000 --> 00:02:02,080
We also touch on GANs or Generative Adversarial Networks and some of the challenges they

25
00:02:02,080 --> 00:02:05,040
represent to the research community.

26
00:02:05,040 --> 00:02:07,080
And now on to the show.

27
00:02:07,080 --> 00:02:14,720
All right, everyone, I am here in Long Beach, California at the NIPS conference and I am

28
00:02:14,720 --> 00:02:17,240
with Marco Couturi.

29
00:02:17,240 --> 00:02:22,400
Marco is a professor of Statistics at University Paris Saclay.

30
00:02:22,400 --> 00:02:23,400
Ah.

31
00:02:23,400 --> 00:02:27,200
University Paris Saclay is how I should say it.

32
00:02:27,200 --> 00:02:30,200
You're perfectly fine.

33
00:02:30,200 --> 00:02:31,800
Marco, welcome to the podcast.

34
00:02:31,800 --> 00:02:32,800
Thanks.

35
00:02:32,800 --> 00:02:34,200
Thanks for having me.

36
00:02:34,200 --> 00:02:38,280
So why don't we get started by having you tell us a little bit about your background.

37
00:02:38,280 --> 00:02:44,080
Are you coming at things from a stats perspective, but applying them to the world of machine learning

38
00:02:44,080 --> 00:02:45,080
in AI?

39
00:02:45,080 --> 00:02:47,280
How did you combine these interests?

40
00:02:47,280 --> 00:02:48,280
Yeah, exactly.

41
00:02:48,280 --> 00:02:52,080
So I'm here at NIPS, so I might maybe start with that.

42
00:02:52,080 --> 00:02:59,600
I've been coming at NIPS for about, I think, 12 years maybe since I was a PhD student.

43
00:02:59,600 --> 00:03:01,080
And you haven't missed any?

44
00:03:01,080 --> 00:03:05,680
Oh, no, I have missed a few right in the middle, but let's say I've probably been here more

45
00:03:05,680 --> 00:03:07,560
often than not during those years.

46
00:03:07,560 --> 00:03:08,560
Okay.

47
00:03:08,560 --> 00:03:11,520
So probably maybe seven times, something like that.

48
00:03:11,520 --> 00:03:18,160
And my background is, so as you mentioned, I have more of a mathematical training in statistics,

49
00:03:18,160 --> 00:03:23,960
maybe a little bit of CS, to blend of everything because I come from a French chronicle, so

50
00:03:23,960 --> 00:03:28,080
those are universities that try to give you a pretty general training.

51
00:03:28,080 --> 00:03:32,120
And when I was a PhD student, I was interested in machine learning already, always doing

52
00:03:32,120 --> 00:03:33,120
bioinformatics.

53
00:03:33,120 --> 00:03:37,200
I mean, trying to do bioinformatics with some mathematical tools.

54
00:03:37,200 --> 00:03:39,840
And then I kept from there, and I did a bit of finance.

55
00:03:39,840 --> 00:03:45,160
I worked also in the financial industry for a short while and worked as a lecturer in

56
00:03:45,160 --> 00:03:47,160
the US for a while as well.

57
00:03:47,160 --> 00:03:48,160
You said an actor?

58
00:03:48,160 --> 00:03:49,160
I'm sorry.

59
00:03:49,160 --> 00:03:50,160
As a lecturer?

60
00:03:50,160 --> 00:03:51,160
Lecturer.

61
00:03:51,160 --> 00:03:52,160
I'm sorry.

62
00:03:52,160 --> 00:03:53,160
No.

63
00:03:53,160 --> 00:03:54,160
That is exotic.

64
00:03:54,160 --> 00:03:55,160
No, no.

65
00:03:55,160 --> 00:03:56,160
Definitely more.

66
00:03:56,160 --> 00:04:01,520
And I was in Princeton for a while, and then I worked in Japan as well.

67
00:04:01,520 --> 00:04:05,480
For a long time, I was a professor there for six years, and then just one year ago, actually,

68
00:04:05,480 --> 00:04:06,480
I came back to France.

69
00:04:06,480 --> 00:04:07,480
Okay.

70
00:04:07,480 --> 00:04:09,440
So I looked back to France.

71
00:04:09,440 --> 00:04:15,440
And yes, as you said, my training is more math, so I'm more interested in how can you

72
00:04:15,440 --> 00:04:22,440
say, well-behaved, well-posed problems, mathematical problems that we can think of, and try to

73
00:04:22,440 --> 00:04:28,520
reply them, and make them work, and actually the messy setting of the real data, et cetera.

74
00:04:28,520 --> 00:04:29,520
Okay.

75
00:04:29,520 --> 00:04:36,080
And so one of those mathematical tools that you're working on now is called Optimal Transport.

76
00:04:36,080 --> 00:04:41,680
And in fact, you're doing a, or you did, a tutorial on that here at Nibb's.

77
00:04:41,680 --> 00:04:43,000
What is Optimal Transport?

78
00:04:43,000 --> 00:04:44,000
Yeah.

79
00:04:44,000 --> 00:04:50,480
So it's been a bit my, I couldn't say my main subject of research for me, for about maybe

80
00:04:50,480 --> 00:04:58,560
three, four years now, before that I was, so maybe if you see Nibb's through years, there's

81
00:04:58,560 --> 00:05:03,960
waves of technologies and ideas and methods that come by, and so one of them was current

82
00:05:03,960 --> 00:05:06,040
methods, and that was when I was a PhD student.

83
00:05:06,040 --> 00:05:07,040
Okay.

84
00:05:07,040 --> 00:05:13,680
And then it started receiving when deep learning exploded, and then I, well, I had this,

85
00:05:13,680 --> 00:05:19,600
an interest for this theory for a while, the theory dates back to the 18th century.

86
00:05:19,600 --> 00:05:21,880
And it was developed by a French mathematician.

87
00:05:21,880 --> 00:05:27,800
It's very intuitive concepts, but for a while, this was not, there was no real progress.

88
00:05:27,800 --> 00:05:32,120
And then this was really discovered in the 20th century by a, by a Russian mathematician

89
00:05:32,120 --> 00:05:37,360
called Kantorovich, and he got a Nobel Prize, actually, 70s in economics, for, for work

90
00:05:37,360 --> 00:05:40,000
on this, in this theory.

91
00:05:40,000 --> 00:05:43,920
And then more recently, there's a also very famous French mathematician.

92
00:05:43,920 --> 00:05:49,280
You might know him, he's now, he was recently elected, like in the Congress, in the French

93
00:05:49,280 --> 00:05:53,320
Congress, and he also got the Fiat Medal, which, you know, is like the, the biggest

94
00:05:53,320 --> 00:05:57,040
in the prize you could get in mathematics is Nibb's, like, Vylanie.

95
00:05:57,040 --> 00:06:01,120
And so this is a really beautiful theory that's, that's, that the many mathematicians

96
00:06:01,120 --> 00:06:04,240
have been investigating for, for the last decades.

97
00:06:04,240 --> 00:06:08,600
And yeah, I think recently we made some progress on the numerical side, computational

98
00:06:08,600 --> 00:06:12,520
side, and this can become very relevant in machine learning.

99
00:06:12,520 --> 00:06:18,960
So across those three, four years, we, we've seen a wave of papers that have, I think,

100
00:06:18,960 --> 00:06:23,480
brought forward, proved a bit that this was useful, and now the tutorial was a way to,

101
00:06:23,480 --> 00:06:26,040
to showcase a bit, all, all that progress.

102
00:06:26,040 --> 00:06:27,040
Okay.

103
00:06:27,040 --> 00:06:33,040
So for someone who's not familiar with optimal transport, what is it, and what is it trying

104
00:06:33,040 --> 00:06:34,040
to achieve?

105
00:06:34,040 --> 00:06:37,040
Yeah, so let's just break it down and, you know, two words, right?

106
00:06:37,040 --> 00:06:38,040
Okay.

107
00:06:38,040 --> 00:06:39,040
It's optimal and transposed.

108
00:06:39,040 --> 00:06:41,440
So why do we need, like, transport?

109
00:06:41,440 --> 00:06:46,880
Transport is actually, it really dates back to very simple idea of, of transporting goods

110
00:06:46,880 --> 00:06:49,120
from where we produce them to where we need them.

111
00:06:49,120 --> 00:06:50,120
Okay.

112
00:06:50,120 --> 00:06:54,320
And typically, of course, if you only have, let's say, one mine and one factory, then there's

113
00:06:54,320 --> 00:06:56,160
no, not many questions you should ask.

114
00:06:56,160 --> 00:07:00,720
I mean, you just bring, whatever the mine produces, and you bring it to the factory, to,

115
00:07:00,720 --> 00:07:02,920
to transform it into some other goods.

116
00:07:02,920 --> 00:07:06,520
But now suppose that you have a map, and you have a lot of mines everywhere in the, on

117
00:07:06,520 --> 00:07:12,960
the map, and they are, they all produce this, you know, a raw resource in different quantities,

118
00:07:12,960 --> 00:07:16,600
and then you have factories all over the map as well, and they all need that resource

119
00:07:16,600 --> 00:07:19,880
to, to, to, for them to keep on producing final goods.

120
00:07:19,880 --> 00:07:24,520
And the question is, how you move, you know, those, those resources around in an optimal

121
00:07:24,520 --> 00:07:25,520
way, in a cheap way.

122
00:07:25,520 --> 00:07:29,280
So it's a, it's really related to a deep problem, you know, in operations research, or just

123
00:07:29,280 --> 00:07:33,320
industrial problem, which is, uh, optimally move things.

124
00:07:33,320 --> 00:07:39,600
And this was, the reason why this really boomed as a theory in World War II, is that this

125
00:07:39,600 --> 00:07:45,400
is probably when really, we had a lot of stuff to move, and really, we really wanted

126
00:07:45,400 --> 00:07:46,400
to be optimal.

127
00:07:46,400 --> 00:07:51,040
But not just a matter of profits, or trying to squeeze more money, or anything, this was

128
00:07:51,040 --> 00:07:52,040
just vital.

129
00:07:52,040 --> 00:07:56,960
You'd have to, there were some, you know, front lines, you would need 10,000 soldiers there,

130
00:07:56,960 --> 00:08:01,120
20,000 soldiers there, 20,000 soldiers there, etc., and then you had them in, in, another

131
00:08:01,120 --> 00:08:04,520
place, another location, and you really had to find out very quickly, where was the

132
00:08:04,520 --> 00:08:05,520
best way to do that.

133
00:08:05,520 --> 00:08:09,520
And so, unfortunately, I mean, during World War II, I guess, none of those techniques

134
00:08:09,520 --> 00:08:14,360
actually were used, but they, that situation triggered a lot of thinking, and that's why

135
00:08:14,360 --> 00:08:18,200
you control it, and actually other people here in the US, as well, Hitchcock, Kupmans,

136
00:08:18,200 --> 00:08:22,120
there's lots of names that are associated with this problem.

137
00:08:22,120 --> 00:08:28,000
And then, this was really a topic that was mostly investigated by people in operations

138
00:08:28,000 --> 00:08:29,000
we search.

139
00:08:29,000 --> 00:08:35,560
But then gradually, this idea of transporting things, in an optimal way, also found applications

140
00:08:35,560 --> 00:08:42,240
and, for instance, in physics, like you would try to guess how a distribution of gas would

141
00:08:42,240 --> 00:08:45,800
slowly evolve towards another distribution of gas.

142
00:08:45,800 --> 00:08:50,600
I mean, there's many, many different settings where you're basically trying to compare those

143
00:08:50,600 --> 00:08:56,320
two things, like the probability distribution of what, what you have, and the probability

144
00:08:56,320 --> 00:09:01,680
distribution of the, what you would like to get, and how to transform it optimally.

145
00:09:01,680 --> 00:09:05,800
And so, this is why mathematicians got interested in that.

146
00:09:05,800 --> 00:09:10,480
And from our point of view, basically, it boils down to the following thing.

147
00:09:10,480 --> 00:09:15,960
This idea of optimal transport provides a fresh way to look at what we call probability

148
00:09:15,960 --> 00:09:16,960
distributions.

149
00:09:16,960 --> 00:09:22,280
Probability distributions basically quantifying how things are likely to happen.

150
00:09:22,280 --> 00:09:26,920
And they also quantify, basically, where we observe data.

151
00:09:26,920 --> 00:09:29,320
What kind of images do we observe in the real world, right?

152
00:09:29,320 --> 00:09:31,160
We don't observe just any kind of image.

153
00:09:31,160 --> 00:09:33,480
We observe images with a certain structure.

154
00:09:33,480 --> 00:09:40,560
And so those observations form a probability distribution, and we try to come up with

155
00:09:40,560 --> 00:09:46,080
probability distributions of the world, which are, we, as modellers, is what we can do.

156
00:09:46,080 --> 00:09:51,560
And we try to compare them, and this is where the theory is actually useful.

157
00:09:51,560 --> 00:09:56,960
And so, I'll stick with this background question.

158
00:09:56,960 --> 00:10:01,600
Is the implication then in the context of World War II?

159
00:10:01,600 --> 00:10:06,440
Like they were probably, probabilistically modeling troop movements, as opposed to determining,

160
00:10:06,440 --> 00:10:08,440
I would imagine that to be more deterministic.

161
00:10:08,440 --> 00:10:11,440
So, I'm going to move these troops from A to B as opposed to-

162
00:10:11,440 --> 00:10:12,440
Yeah, very good point.

163
00:10:12,440 --> 00:10:20,400
So, here, let me just tell you how you could see, imagine probabilistic troop movements.

164
00:10:20,400 --> 00:10:27,600
Imagine you have front-line A and front-line B, and you need 10,000 soldiers at A and NB,

165
00:10:27,600 --> 00:10:28,600
okay?

166
00:10:28,600 --> 00:10:33,080
Because somewhat the enemy is coming, so we really need 10,000 guys there, and 10,000

167
00:10:33,080 --> 00:10:34,080
guys there.

168
00:10:34,080 --> 00:10:38,600
So, suppose you have two barracks, each with 10,000 soldiers.

169
00:10:38,600 --> 00:10:45,160
So, a very naive way would be to, let's say, send all the 10,000 soldiers of the back

170
00:10:45,160 --> 00:10:50,880
one to front-line A, 10,000 soldiers from back two to front-line B, okay?

171
00:10:50,880 --> 00:10:53,680
So there's no real probabilistic thing, right?

172
00:10:53,680 --> 00:10:57,320
It's just everyone goes there, and then everyone goes there.

173
00:10:57,320 --> 00:11:01,920
It sounds great, but imagine that, for some reason, you're a bit suspicious that maybe

174
00:11:01,920 --> 00:11:08,240
something might happen to a 10,000 soldiers that are sent through just one road.

175
00:11:08,240 --> 00:11:14,200
So one thing you might try to do is split those 10,000s into five and five, and the other

176
00:11:14,200 --> 00:11:19,000
barrack also is split into five and five, and so you send simultaneously 5,000 soldiers

177
00:11:19,000 --> 00:11:21,000
to each of the front-line B.

178
00:11:21,000 --> 00:11:22,000
You see what I mean?

179
00:11:22,000 --> 00:11:23,000
Yeah, so now you've got four routes.

180
00:11:23,000 --> 00:11:24,000
Exactly.

181
00:11:24,000 --> 00:11:28,720
Four people, four groups of people, five thousand people moving around.

182
00:11:28,720 --> 00:11:31,880
So it's not clear you gain a lot by doing that, right?

183
00:11:31,880 --> 00:11:36,960
Because somewhat maybe if barrack one was very close to front-line A, and barrack two

184
00:11:36,960 --> 00:11:41,760
was very close to front-line B, then you're actually asking people to cross basically over

185
00:11:41,760 --> 00:11:44,480
and have this much longer trip, right?

186
00:11:44,480 --> 00:11:49,280
But what you've gained is that you have something, maybe this is where maybe the probabilistic

187
00:11:49,280 --> 00:11:55,240
aspect can be seen like, each soldier basically from back one had chanced one over two to

188
00:11:55,240 --> 00:12:00,760
go to A or B, or on the other hand, if you want to just stick to the optimal plan, you

189
00:12:00,760 --> 00:12:03,640
will just send them directly to the closest to front-line, right?

190
00:12:03,640 --> 00:12:09,440
Is what you're trying to model probabilistically the chance that they'd actually get there,

191
00:12:09,440 --> 00:12:11,880
like the danger of their route or some kind of...

192
00:12:11,880 --> 00:12:16,480
Well, this is something I might have in mind to actually choose the one where they split

193
00:12:16,480 --> 00:12:22,000
into two instead of just going directly to the best possible, I mean, the shortest basically

194
00:12:22,000 --> 00:12:23,000
front-line point.

195
00:12:23,000 --> 00:12:31,400
But what I'm saying is that when you have to distribute resources in a network like this

196
00:12:31,400 --> 00:12:38,280
from a starting point to an ending point, well, sometimes you can find that the best possible

197
00:12:38,280 --> 00:12:45,360
way, the cheapest possible way is to do this a bit deterministically as you imply, there's

198
00:12:45,360 --> 00:12:49,480
no probability you just send everyone to the best possible route.

199
00:12:49,480 --> 00:12:54,600
But you could add some little fuzziness, so you know what, maybe in case there's a problem

200
00:12:54,600 --> 00:12:59,080
with that route, maybe this breaks down, this bridge breaks down, if this bridge breaks

201
00:12:59,080 --> 00:13:03,800
down, maybe no one will get front-line B and then I'm really in a mess, right?

202
00:13:03,800 --> 00:13:08,080
So to hedge up it yourself, you might want to choose something that's a bit more fuzzy,

203
00:13:08,080 --> 00:13:10,000
a bit more probabilistic.

204
00:13:10,000 --> 00:13:15,600
And on the other hand, if you really go towards the fuzziest possible way to do this, where

205
00:13:15,600 --> 00:13:19,680
you split everyone equally, then you see that there's a problem because you're paying

206
00:13:19,680 --> 00:13:26,480
very high cost usually, in terms of travel, because maybe front-line B is like 100 miles

207
00:13:26,480 --> 00:13:31,720
away, from like A's, maybe 10 miles away from the back one, and you're actually asking

208
00:13:31,720 --> 00:13:35,720
for the half of your troops to walk those hour and a half.

209
00:13:35,720 --> 00:13:41,760
So see, there's this, there's a trade-off somewhat, in terms of robustness, on the one hand,

210
00:13:41,760 --> 00:13:46,200
you would like to do what's cheap, on the other hand, I didn't need to be a fuzziness,

211
00:13:46,200 --> 00:13:49,560
makes it a bit more robust against any problem that might occur.

212
00:13:49,560 --> 00:13:55,080
So here, I'm using this, you know, vocabulary of maybe there's a risk or there's something

213
00:13:55,080 --> 00:14:00,120
that I beg down, etc, but in, from mathematical perspective, what this really boils down to

214
00:14:00,120 --> 00:14:07,120
is more of something, which is, you know, the stability of the solution is, you know,

215
00:14:07,120 --> 00:14:12,640
having a little bit of fuzzier things makes it a bit, makes it a bit more stable, having

216
00:14:12,640 --> 00:14:18,120
something that's really routing everyone to a, just one in one location makes things a

217
00:14:18,120 --> 00:14:19,120
bit unstable.

218
00:14:19,120 --> 00:14:20,120
And so why-

219
00:14:20,120 --> 00:14:23,920
Make you think of, if you're familiar with Nassim Taleb's anti-fragility?

220
00:14:23,920 --> 00:14:29,640
Yeah, exactly, you could think of this, so we're going a bit away from basically my exact

221
00:14:29,640 --> 00:14:32,720
technical contribution, but actually the idea is exactly that.

222
00:14:32,720 --> 00:14:39,640
And my, all my, part of my talk, so first I explain this problem of moving around optimally

223
00:14:39,640 --> 00:14:40,640
things.

224
00:14:40,640 --> 00:14:44,640
This moving around optimally things helps you figure out whether two things are close,

225
00:14:44,640 --> 00:14:46,040
whether two distributions are close.

226
00:14:46,040 --> 00:14:50,720
If there's a way, if there's a cheap way to move all the troops to all the frontlines,

227
00:14:50,720 --> 00:14:54,000
then it means that the two distributions are very close.

228
00:14:54,000 --> 00:14:59,680
But if you write down the equations, and this is why Kantorovich called the Nobel Prize,

229
00:14:59,680 --> 00:15:02,920
this yields something called a linear program.

230
00:15:02,920 --> 00:15:08,280
It's one major, very important, family of optimization problems.

231
00:15:08,280 --> 00:15:13,680
And the way this was formalized 50 years ago was just linear programming, so let's go

232
00:15:13,680 --> 00:15:17,080
for the cheapest possible distribution.

233
00:15:17,080 --> 00:15:21,400
Now what I was arguing a bit during the tutorial is that you might trade off a little bit of

234
00:15:21,400 --> 00:15:26,520
that optimality in terms of the cost, and accept a little bit of fuzziness.

235
00:15:26,520 --> 00:15:31,600
And this makes things much better behave computationally when you run all those algorithms.

236
00:15:31,600 --> 00:15:38,280
So yeah, actually this is a meaning, meaning that introducing the randomness not only has

237
00:15:38,280 --> 00:15:43,600
these properties, these beneficial properties, and kind of the physical world we're talking

238
00:15:43,600 --> 00:15:50,680
about troop movements, but also introducing probability and randomness allows you to give

239
00:15:50,680 --> 00:15:54,040
you a more computationally efficient solution in solving the linear program?

240
00:15:54,040 --> 00:16:00,840
Yes, like one million times faster, like in some relevant dimensionalities, let's

241
00:16:00,840 --> 00:16:06,400
say you have, let's say 10,000 barracks, 10,000 frontlines, and you want to compute this

242
00:16:06,400 --> 00:16:11,600
optimal transport, where it turns out that if you use, if you're willing to take a little

243
00:16:11,600 --> 00:16:16,760
bit of fuzziness, not that many, but just a little bit, then it can actually, you can

244
00:16:16,760 --> 00:16:23,760
compute this transport, and maybe let's say, let's say one second, okay, for the sake

245
00:16:23,760 --> 00:16:28,160
of the argument, but if you were to solve it with a linear program, that would be like

246
00:16:28,160 --> 00:16:32,840
one million times slower, that would really be much longer.

247
00:16:32,840 --> 00:16:37,920
And the other thing that's also very relevant I think in this NIPPS conference is that computing

248
00:16:37,920 --> 00:16:43,760
transport with that little fuzziness, not so much, but just a little bit, allows you to

249
00:16:43,760 --> 00:16:50,360
compute everything with GPUs, so everything becomes a parallel, very embarrassingly parallel

250
00:16:50,360 --> 00:16:55,640
algorithm, and this is one of the also big sources of speed up, whereas doing it the linear

251
00:16:55,640 --> 00:17:00,720
programming way, the way that was proposed in the 50s or 60s by such great names as

252
00:17:00,720 --> 00:17:06,160
George Tansik, people that invented all those algorithms in 50s or 60s, those algorithms

253
00:17:06,160 --> 00:17:11,600
don't parallelize well with GPUs, and this is the main reason why we're trying to get

254
00:17:11,600 --> 00:17:17,080
a bit away from them, and a bit more towards those algorithms, but because they include

255
00:17:17,080 --> 00:17:22,280
some fuzziness, a little bit of fuzziness, for some reason, you go down in the math, you

256
00:17:22,280 --> 00:17:28,680
observe that you can cast them as matrix products, and so everything works well on GPUs.

257
00:17:28,680 --> 00:17:34,200
So if I want to push a bit the analogy, this field of optimal transport has been around

258
00:17:34,200 --> 00:17:41,760
for many years, I propose this little, to add this little fuzziness, and everything all

259
00:17:41,760 --> 00:17:47,520
of a sudden can be executed on GPUs, so it looks a bit like someone that comes at a machine

260
00:17:47,520 --> 00:17:52,280
learning conference maybe 10 years ago, and says, oh, I have deep learning algorithms,

261
00:17:52,280 --> 00:17:56,240
not only are they great, actually they run on GPUs, and because they run on GPUs they

262
00:17:56,240 --> 00:18:01,360
are going to get much better performance, because we know this was one of the main drivers

263
00:18:01,360 --> 00:18:09,520
of innovation for deep learning was the fact that they could really well exploit GPUs.

264
00:18:09,520 --> 00:18:18,600
So then are you also applying optimal transport to machine learning problems, or are you,

265
00:18:18,600 --> 00:18:23,600
you know, is this a method for doing better work with traditional types of problems that

266
00:18:23,600 --> 00:18:26,040
we think of as optimal transport problems?

267
00:18:26,040 --> 00:18:31,240
So yeah, it's a very good question, so my point of view right now is that,

268
00:18:31,240 --> 00:18:34,640
so maybe I didn't stress this enough, but what really optimal transport allows you to

269
00:18:34,640 --> 00:18:40,320
do is to compute a distance between probability distributions, and by this I mean, let me

270
00:18:40,320 --> 00:18:45,560
just give you a very concrete example, and this was actually presented at NIP last year

271
00:18:45,560 --> 00:18:51,920
by another team, so you have, suppose you want to compare two text documents, so very

272
00:18:51,920 --> 00:18:58,360
standard approach is to see those documents as bags of words, right, just a long list

273
00:18:58,360 --> 00:19:03,640
of words and another long list of words, right, so I think it's a very relevant question

274
00:19:03,640 --> 00:19:09,240
to say how can we compute the distance between those two bags of words, right, so there

275
00:19:09,240 --> 00:19:15,040
are a few approaches that exist, and one of them is using optimal transport, so basically

276
00:19:15,040 --> 00:19:20,720
what this optimal transport approach requires is that you are able to define a distance

277
00:19:20,720 --> 00:19:27,840
between pairs of words, so basically if you can define a distance between pairs of words,

278
00:19:27,840 --> 00:19:31,720
optimal transport allows you to define a distance between histograms of words, it's

279
00:19:31,720 --> 00:19:39,040
like a meta distance, so you have a distance on just, I don't know, the word cat is that

280
00:19:39,040 --> 00:19:44,880
meters away or miles away from the word, let's say dog, and then very close to TT, for

281
00:19:44,880 --> 00:19:49,080
instance, if you can actually quantify that distance for every pair of words, I will be

282
00:19:49,080 --> 00:19:52,680
able to come up with a distance that can quantify the distance between histograms of words,

283
00:19:52,680 --> 00:19:57,160
cloud, point clouds, you know, those point clouds that we see very often on, yeah, word

284
00:19:57,160 --> 00:20:02,840
clouds, so optimal transport allows you to compute that, that very accurate distance between

285
00:20:02,840 --> 00:20:11,840
word clouds, and now what I'm trying to use is, suppose that my goal is not only to compute

286
00:20:11,840 --> 00:20:16,640
the distance between two things, but also to use that distance as what is called a loss

287
00:20:16,640 --> 00:20:23,840
function, something that allows you to quantify whether your prediction kind of matches what

288
00:20:23,840 --> 00:20:28,280
you should observe, so imagine that, imagine, so this is another example I was taking

289
00:20:28,280 --> 00:20:35,240
for a nips paper two years ago, imagine that your task is to predict from an image cloud

290
00:20:35,240 --> 00:20:43,000
of tags, okay, you just see an image, there's a tree, sun, bridge, etcetera, imagine now

291
00:20:43,000 --> 00:20:48,200
that you have a predictor that takes that image and says that, on that image actually

292
00:20:48,200 --> 00:20:54,080
it's a forest, there's sunlight, and there is a river, so as you can see I've managed

293
00:20:54,080 --> 00:21:01,080
to find a few tags that are somewhat overlapping the original tags that are the ground truth,

294
00:21:01,080 --> 00:21:05,480
but it would be really useful if I could accurately quantify how different those two histograms

295
00:21:05,480 --> 00:21:11,760
of words are, and that's where optimal transport can be used, and so what I've shown in the

296
00:21:11,760 --> 00:21:16,320
tutorial yesterday is that you can not only use optimal transport as a distance just

297
00:21:16,320 --> 00:21:22,680
to put a geometry on the space of histograms, but you can use that as a loss in the learning

298
00:21:22,680 --> 00:21:27,520
algorithm, so whenever, whenever I will, so very often for instance an algorithm just

299
00:21:27,520 --> 00:21:34,200
produces one guess, one guess, well just think about a meta algorithm that would not produce

300
00:21:34,200 --> 00:21:39,480
one guess, but the histogram of guesses, the probability distribution, some confidence

301
00:21:39,480 --> 00:21:44,320
on basically several possible guesses, well then it's useful to use optimal transport

302
00:21:44,320 --> 00:21:49,440
theory to compare those histograms of guesses, instead of maybe you've heard about this,

303
00:21:49,440 --> 00:21:55,840
a coolback library divergence, or other, those are, that there is a very important block

304
00:21:55,840 --> 00:22:00,720
in many machine learning algorithms, including deep learning algorithms, which is this cross-centropy

305
00:22:00,720 --> 00:22:06,080
loss, or coolback library loss, well this is another loss that has been around for centuries

306
00:22:06,080 --> 00:22:12,920
to compare to probability distributions, basically my main agenda is to maybe remove that loss

307
00:22:12,920 --> 00:22:20,440
and replace it by this optimal transport loss, okay, and is the primary, what are the advantages

308
00:22:20,440 --> 00:22:27,400
in doing, so you mentioned the computational advantage with our ability to do these in parallel,

309
00:22:27,400 --> 00:22:32,040
is that something that you can't do with cross-entropy loss, actually it's the other

310
00:22:32,040 --> 00:22:37,840
way around, cross-entropy is very popular because it's very, very simple, it's very, very

311
00:22:37,840 --> 00:22:43,760
easy to compute, but it uses very little information, the sense that, imagine that you have two

312
00:22:43,760 --> 00:22:49,400
histograms of words, and I compute the cross-entropy between them, all the coolback library

313
00:22:49,400 --> 00:22:57,600
divergence, what that won't be able to detect is whether words are synonyms or not, in

314
00:22:57,600 --> 00:23:02,240
the sense that if, for instance, one of my histograms, the one that I want to predict

315
00:23:02,240 --> 00:23:08,200
as a word cut, and for some reason my machine predicted the word kitty, in the cross-entropy

316
00:23:08,200 --> 00:23:13,840
world, those will be two completely different coordinates, and there's no, there will,

317
00:23:13,840 --> 00:23:20,360
three relative, there's no, basically the cross-entropy formula will just say, okay cut appeared

318
00:23:20,360 --> 00:23:27,400
here in the, in the ground truth, my guess was kitty, so I have put a zero on cut, then

319
00:23:27,400 --> 00:23:30,800
you pay a price, and then on the other hand, the ground truth didn't have the word kitty,

320
00:23:30,800 --> 00:23:34,920
but I predicted it, so you also pay the price, you pay the price twice if you want, whereas

321
00:23:34,920 --> 00:23:40,200
this optimal transport loss allows you to say, oh, come on, actually, the ground truth

322
00:23:40,200 --> 00:23:45,400
is cut, and my prediction is kitty, but I know that cutting kitty are very close, I mean,

323
00:23:45,400 --> 00:23:49,480
they're almost the same, they're almost synonyms, so I'm paying the very small price, because

324
00:23:49,480 --> 00:23:55,840
I'm able to somewhat transport the word cut onto the word kitty, and that's the main

325
00:23:55,840 --> 00:23:59,360
idea where this gives a loss, which is more flexible if you want.

326
00:23:59,360 --> 00:24:05,120
Now, it sounds like, in this case, it's calling them on, like, word embeddings, and word

327
00:24:05,120 --> 00:24:08,320
spaces, and so do you need to do all that in order to use it?

328
00:24:08,320 --> 00:24:10,920
Exactly, so the thing is, is that an alternative, or are they complementary?

329
00:24:10,920 --> 00:24:17,480
No, so exactly, so basically, so in those papers, those people are called the word movers

330
00:24:17,480 --> 00:24:21,920
distance, because the idea of a tumultranspactory has many names, it's called Vassarstein distance,

331
00:24:21,920 --> 00:24:26,600
earth movers distance, and so it was recently discovered by this word, word movers distance,

332
00:24:26,600 --> 00:24:31,560
where you're actually computing the distance between two point clouds of word embeddings.

333
00:24:31,560 --> 00:24:32,560
Okay.

334
00:24:32,560 --> 00:24:36,560
So you can imagine that one text is just a bunch of points in dimension 100, those are the

335
00:24:36,560 --> 00:24:41,840
embedding, these embedding dimensions of the words, and another point cloud is another

336
00:24:41,840 --> 00:24:46,920
text in dimension 100, and then you're trying to transport them optimally, and what you

337
00:24:46,920 --> 00:24:51,520
need to do to get there is a notion of distance between every pair of words, right?

338
00:24:51,520 --> 00:24:53,760
So that's what the word embeddings give us.

339
00:24:53,760 --> 00:24:58,200
So what embeddings are just the fuel for this, I mean, just the main ingredient that allows

340
00:24:58,200 --> 00:25:00,560
us to put a bit of tumultransport.

341
00:25:00,560 --> 00:25:01,800
Okay, interesting.

342
00:25:01,800 --> 00:25:06,400
So there's an application in NLP, what are some of our machine learning applications?

343
00:25:06,400 --> 00:25:11,520
So we have, so let me just list you what the application where things really worked very,

344
00:25:11,520 --> 00:25:12,520
very well.

345
00:25:12,520 --> 00:25:13,520
Okay.

346
00:25:13,520 --> 00:25:15,480
So the ones where it works really nicely is graphics.

347
00:25:15,480 --> 00:25:16,480
Okay.

348
00:25:16,480 --> 00:25:20,040
So it's a bit away from machine learning, because graphics people are mostly concerned with

349
00:25:20,040 --> 00:25:22,000
low dimensional shapes.

350
00:25:22,000 --> 00:25:29,840
So we've had a few successes, you know, doing interpolations of shapes, and I mean, we have

351
00:25:29,840 --> 00:25:34,160
a few C-graph papers, and what are the specific examples?

352
00:25:34,160 --> 00:25:38,160
So when there's the C-V, well, imagine that you have a probability distribution, which

353
00:25:38,160 --> 00:25:42,720
is a shape, and another probability distribution that's a shape, what optimal transport allows

354
00:25:42,720 --> 00:25:48,480
you to do is to do interpolations between them, like more things, without very, very little

355
00:25:48,480 --> 00:25:51,000
assumptions on the data.

356
00:25:51,000 --> 00:25:53,080
So what we've seen is that this can be used.

357
00:25:53,080 --> 00:25:57,440
So what can you give me a more kind of concrete example of where you'd have these probabilistic

358
00:25:57,440 --> 00:25:58,440
shapes?

359
00:25:58,440 --> 00:25:59,440
Yeah.

360
00:25:59,440 --> 00:26:04,640
So it's one of those cases where a picture actually is worth a thousand words, but we have

361
00:26:04,640 --> 00:26:10,920
those pictures in the C-graph papers where you would see, for instance, a duck, like

362
00:26:10,920 --> 00:26:17,160
a duck, like a kid having a bath, that would gradually morph into some cow or any other

363
00:26:17,160 --> 00:26:18,160
shape.

364
00:26:18,160 --> 00:26:23,880
So you can actually compute interpolations between tens of shapes, one another, the application

365
00:26:23,880 --> 00:26:25,400
of the brain imaging.

366
00:26:25,400 --> 00:26:31,000
We can try to morph one brain into another without making any assumption on actually the

367
00:26:31,000 --> 00:26:33,800
parametric assumption of the shape of the brain, for instance.

368
00:26:33,800 --> 00:26:37,400
And so optimal transport is very natural for that.

369
00:26:37,400 --> 00:26:38,400
Right, right.

370
00:26:38,400 --> 00:26:41,680
So you've got your beginning distribution, your end distribution, and now this allows

371
00:26:41,680 --> 00:26:46,880
you to compute intermediate states at any point in time or point in, which makes sense

372
00:26:46,880 --> 00:26:47,880
geometrically.

373
00:26:47,880 --> 00:26:51,480
So it's very easy to compute intermediate probability distribution.

374
00:26:51,480 --> 00:26:56,920
You just add one to the other, divide by two, then you will probably get those weird

375
00:26:56,920 --> 00:26:57,920
effects.

376
00:26:57,920 --> 00:26:58,920
Yeah.

377
00:26:58,920 --> 00:27:01,800
So optimal transport actually allows you to morph continuously from one to the other.

378
00:27:01,800 --> 00:27:02,800
Okay.

379
00:27:02,800 --> 00:27:04,640
And this was discovered in the late 90s.

380
00:27:04,640 --> 00:27:09,760
For many years, we had no idea how to compute it, because it was actually very, very heavy.

381
00:27:09,760 --> 00:27:13,560
And those ideas that I told you about about adding a little bit of fuzziness, exploiting

382
00:27:13,560 --> 00:27:16,560
GPUs, basically make it possible now.

383
00:27:16,560 --> 00:27:21,800
And so you can average not only two, but maybe let's say 10, 20 brains and things like

384
00:27:21,800 --> 00:27:22,800
that.

385
00:27:22,800 --> 00:27:24,280
So that can help you from templates.

386
00:27:24,280 --> 00:27:30,520
So this is what the few, those are the examples where there are actually visually a bit

387
00:27:30,520 --> 00:27:31,520
more striking.

388
00:27:31,520 --> 00:27:32,520
Okay.

389
00:27:32,520 --> 00:27:36,280
So dimension, we can, we as humans can get a few of what's going on.

390
00:27:36,280 --> 00:27:39,680
Then the NLP applications, I think, are pretty exciting.

391
00:27:39,680 --> 00:27:44,080
And then there has been the water application, the NLP, the NLP, the NLP, the dimension.

392
00:27:44,080 --> 00:27:48,520
And then we had a, there's a lot of imaging also applications with a distribution of color

393
00:27:48,520 --> 00:27:54,160
experiences, like image palettes, more thing one image with a given palette, changing the

394
00:27:54,160 --> 00:28:00,360
palette of an image, for instance, you want to, yeah, the palette is, has colors reminiscent

395
00:28:00,360 --> 00:28:01,360
of autumn.

396
00:28:01,360 --> 00:28:03,320
And you want them to be all of a certain kind of blueish, etc.

397
00:28:03,320 --> 00:28:06,000
So there's lots of applications like this.

398
00:28:06,000 --> 00:28:10,080
But what we've seen in machine learning recently, and which is pretty exciting, is more

399
00:28:10,080 --> 00:28:17,480
applications to a generative models, those models that are able to create images.

400
00:28:17,480 --> 00:28:23,960
And so there's been a few papers in the last year, actually, in just one year from now.

401
00:28:23,960 --> 00:28:25,920
And one of them is called the Vassarstein Gan.

402
00:28:25,920 --> 00:28:27,680
It has attracted a lot of attention.

403
00:28:27,680 --> 00:28:30,280
We also tried to write a few papers in that line.

404
00:28:30,280 --> 00:28:34,280
So here in that case, the, the story is as follows.

405
00:28:34,280 --> 00:28:38,880
You can see the distribution of natural images in the world, let's say, as a probability

406
00:28:38,880 --> 00:28:42,720
distribution of the space of all possible images, right?

407
00:28:42,720 --> 00:28:48,040
We think of an image as a three channels, and a hundred times a hundred pixels, it's

408
00:28:48,040 --> 00:28:52,000
basically a vector of dimension, 30,000.

409
00:28:52,000 --> 00:28:56,120
But there's very few actually in that space of dimension, 30,000.

410
00:28:56,120 --> 00:28:59,240
There's not that many images that are natural in there, right?

411
00:28:59,240 --> 00:29:04,120
You were to basically write a map of every image that can be natural.

412
00:29:04,120 --> 00:29:05,120
Right.

413
00:29:05,120 --> 00:29:06,440
It would be very thin there, right?

414
00:29:06,440 --> 00:29:10,240
So the problem of generative model is to create a probabilistic model, something that can

415
00:29:10,240 --> 00:29:16,240
generate images that fall not too far from that manifold of natural images.

416
00:29:16,240 --> 00:29:19,360
And this is a statistically very, very messy problem.

417
00:29:19,360 --> 00:29:24,840
And so what you want to do is imagine that I have a model that generates images.

418
00:29:24,840 --> 00:29:29,080
It will have its own manifold in that space of dimension, 30,000.

419
00:29:29,080 --> 00:29:33,720
And then I collect data that's another manifold in dimension, 30,000.

420
00:29:33,720 --> 00:29:40,200
And so the point is I want my manifold to look like the manifold of natural images, right?

421
00:29:40,200 --> 00:29:44,200
And I need to be able to say how far I am, right?

422
00:29:44,200 --> 00:29:46,720
That's the whole point of doing gradient descent.

423
00:29:46,720 --> 00:29:51,600
It's quantifying how far my manifold is from the real manifold, and trying to make that

424
00:29:51,600 --> 00:29:53,960
distance smaller and smaller.

425
00:29:53,960 --> 00:30:02,400
And what people found out just about one year ago, is that many usual distances like

426
00:30:02,400 --> 00:30:08,080
cross-entropy, you know, kubak library, et cetera, were not well suited for that problem.

427
00:30:08,080 --> 00:30:11,800
Because we are really talking about very degenerate manifolds.

428
00:30:11,800 --> 00:30:17,960
And what we use kubak library for is typically to compare gaussians, for instance, so to compare

429
00:30:17,960 --> 00:30:19,880
things that are very smooth.

430
00:30:19,880 --> 00:30:23,640
And so when you have this problem of trying to quantify the distance between one manifold

431
00:30:23,640 --> 00:30:27,040
and another, optimal transport can be useful.

432
00:30:27,040 --> 00:30:29,840
And so there's been a lot of interest for that.

433
00:30:29,840 --> 00:30:36,600
There's Vassarstein Gaan has burned a lot of papers, and there's many, many tricks that

434
00:30:36,600 --> 00:30:41,600
people are trying to use to make this work, because it's computationally challenging.

435
00:30:41,600 --> 00:30:44,400
But then again, in the world of Gaan's, it's very difficult to measure performance.

436
00:30:44,400 --> 00:30:45,400
Right.

437
00:30:45,400 --> 00:30:49,320
Right now, everybody's a bit frustrated, but I think there's still a bit of progress there.

438
00:30:49,320 --> 00:30:52,400
I mean, still progress that's being made now.

439
00:30:52,400 --> 00:30:57,080
I haven't picked up on the frustration around Gaan, as much as the enthusiasm.

440
00:30:57,080 --> 00:31:02,280
Where did you talk to frustration specifically around the ability to measure performance?

441
00:31:02,280 --> 00:31:03,280
Yeah, exactly.

442
00:31:03,280 --> 00:31:08,200
I mean, if you look at the papers, most of them end up being a collection of the images

443
00:31:08,200 --> 00:31:17,040
that are being generated, and that's not a very scientifically correct way of, yeah.

444
00:31:17,040 --> 00:31:21,000
You want some more objective measure of performance, but that in itself is already a big

445
00:31:21,000 --> 00:31:22,520
scientific problem, right?

446
00:31:22,520 --> 00:31:24,880
Trying to find a good measure of performance there.

447
00:31:24,880 --> 00:31:25,880
Okay.

448
00:31:25,880 --> 00:31:26,880
Interesting.

449
00:31:26,880 --> 00:31:28,920
I really enjoyed learning about optimal transport.

450
00:31:28,920 --> 00:31:30,920
Any final words or...

451
00:31:30,920 --> 00:31:31,920
Oh, yes.

452
00:31:31,920 --> 00:31:36,480
So we've written a small, I'm just doing it to put some little advertisement for a survey

453
00:31:36,480 --> 00:31:38,160
that we've written recently.

454
00:31:38,160 --> 00:31:46,640
So if you go to Optimal Transport and just one word, Optimal Transport.github.io, you

455
00:31:46,640 --> 00:31:50,760
will find the slides of the tutorial and you will find also a survey, a 200 pages survey

456
00:31:50,760 --> 00:31:53,800
that I've written with my colleague Gaby Alperin.

457
00:31:53,800 --> 00:31:54,800
Okay.

458
00:31:54,800 --> 00:31:58,080
Interesting. And is there...

459
00:31:58,080 --> 00:31:59,080
In terms of...

460
00:31:59,080 --> 00:32:01,080
You mentioned Github and I'm thinking implementation.

461
00:32:01,080 --> 00:32:02,080
Yes, it is.

462
00:32:02,080 --> 00:32:03,080
It could be available.

463
00:32:03,080 --> 00:32:04,080
Yes, of course.

464
00:32:04,080 --> 00:32:05,080
Okay.

465
00:32:05,080 --> 00:32:11,080
The good thing is that, as I was telling you earlier, all of this eventually is actually

466
00:32:11,080 --> 00:32:12,080
very simple to code.

467
00:32:12,080 --> 00:32:17,680
I mean, I wish I could tell you we have really a great toolbox that, but actually, if

468
00:32:17,680 --> 00:32:20,160
you really look a bit at the papers, it's just...

469
00:32:20,160 --> 00:32:21,960
The code is just a few lines.

470
00:32:21,960 --> 00:32:22,960
It's just...

471
00:32:22,960 --> 00:32:23,960
Okay.

472
00:32:23,960 --> 00:32:24,960
Put that loss...

473
00:32:24,960 --> 00:32:29,400
This magic Optimal Transport loss in your learning problem just requires you writing

474
00:32:29,400 --> 00:32:32,480
maybe a wild loop and three lines in the wild loop.

475
00:32:32,480 --> 00:32:33,480
Okay.

476
00:32:33,480 --> 00:32:34,480
So it's more...

477
00:32:34,480 --> 00:32:37,960
What I mean, this right up your alley, it's more about the math that gave you those three

478
00:32:37,960 --> 00:32:38,960
lines and the three lines themselves.

479
00:32:38,960 --> 00:32:39,960
Exactly.

480
00:32:39,960 --> 00:32:40,960
There's not much in there involved, yes.

481
00:32:40,960 --> 00:32:41,960
Okay.

482
00:32:41,960 --> 00:32:42,960
Awesome.

483
00:32:42,960 --> 00:32:45,200
Well, Marco, thanks so much for taking the time to chat with me.

484
00:32:45,200 --> 00:32:46,200
Thank you so much.

485
00:32:46,200 --> 00:32:47,200
It was really a pleasure.

486
00:32:47,200 --> 00:32:53,360
All right, everyone, that's our show for today.

487
00:32:53,360 --> 00:32:58,400
For more information on Marco or any of the topics covered in this episode, head on over

488
00:32:58,400 --> 00:33:02,840
to twimmaleye.com slash talks slash 131.

489
00:33:02,840 --> 00:33:20,000
And of course, thanks so much for listening and catch you next time.

