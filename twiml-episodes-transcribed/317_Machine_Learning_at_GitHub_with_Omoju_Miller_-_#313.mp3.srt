1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:22,960
I'm your host Sam Charrington.

3
00:00:22,960 --> 00:00:25,880
Sam is back on the road for the next few weeks.

4
00:00:25,880 --> 00:00:30,020
If you're hearing this on October 31st, you can catch him at TensorFlow World in Santa

5
00:00:30,020 --> 00:00:31,620
Clara.

6
00:00:31,620 --> 00:00:35,800
Next week he'll be at Microsoft Ignite in Orlando, and the week after that, a cube

7
00:00:35,800 --> 00:00:37,520
con in San Diego.

8
00:00:37,520 --> 00:00:40,920
If you see him wandering around, please pull up on him.

9
00:00:40,920 --> 00:00:41,920
Say what's up.

10
00:00:41,920 --> 00:00:45,640
Grab one of our awesome new stickers, or maybe even snap a selfie.

11
00:00:45,640 --> 00:00:49,800
He loves that kind of stuff.

12
00:00:49,800 --> 00:00:54,880
But before we move on, last week we published a show featuring an interview with Phoebe

13
00:00:54,880 --> 00:01:00,480
Devries and Brendan Meade about their paper, deep learning of aftershock patterns following

14
00:01:00,480 --> 00:01:02,520
large earthquakes.

15
00:01:02,520 --> 00:01:06,440
It turns out that since that interview was recorded, some questions have been raised

16
00:01:06,440 --> 00:01:09,560
about the research methodology used in the paper.

17
00:01:09,560 --> 00:01:13,680
This was brought to our attention by Raju Shah, a longtime listener and friend of the

18
00:01:13,680 --> 00:01:17,240
show, who initially raised the concerns.

19
00:01:17,240 --> 00:01:21,680
The issue has been covered in numerous articles, links to a couple of which will add to a

20
00:01:21,680 --> 00:01:23,680
note on the show notes page.

21
00:01:23,680 --> 00:01:26,800
Sorry, we missed this one before publishing.

22
00:01:26,800 --> 00:01:30,680
Happy Halloween, enjoy the show.

23
00:01:30,680 --> 00:01:33,120
All right, everyone.

24
00:01:33,120 --> 00:01:39,120
I am here in Santa Clara for the TensorFlow World Conference, and I've got the pleasure of

25
00:01:39,120 --> 00:01:41,760
being seated with Omoju Miller.

26
00:01:41,760 --> 00:01:45,360
Omoju is a senior machine learning engineer at GitHub.

27
00:01:45,360 --> 00:01:48,680
Omoju, welcome to the Tomel AI podcast.

28
00:01:48,680 --> 00:01:50,320
Thank you for having me.

29
00:01:50,320 --> 00:01:55,640
It's great that we finally got this opportunity to have this chat for folks that are listening

30
00:01:55,640 --> 00:01:56,640
in.

31
00:01:56,640 --> 00:02:02,800
We've been trying to connect on this chat for quite some time, maybe about a year, yeah.

32
00:02:02,800 --> 00:02:06,760
And not only do we get to finally have it, but we get to finally have it in person here

33
00:02:06,760 --> 00:02:08,560
in sunny California.

34
00:02:08,560 --> 00:02:12,880
I'm not in my head, yes.

35
00:02:12,880 --> 00:02:17,920
As is typical for us, why don't we get started by talking a little bit about your background?

36
00:02:17,920 --> 00:02:21,120
How did you get started and interested in machine learning?

37
00:02:21,120 --> 00:02:24,080
When I got started in machine learning, it wasn't called machine learning.

38
00:02:24,080 --> 00:02:29,120
It was just computer science, graduate work.

39
00:02:29,120 --> 00:02:37,120
I think this was around 1999, 2000, and this is C++, there were no packages, you have

40
00:02:37,120 --> 00:02:39,080
to write everything yourself.

41
00:02:39,080 --> 00:02:42,360
I was taking graduate classes, I was a graduate researcher.

42
00:02:42,360 --> 00:02:43,360
This is what I walked on.

43
00:02:43,360 --> 00:02:47,720
I used MATLAB, but then I realized that this is too academic.

44
00:02:47,720 --> 00:02:49,360
Now we're going to apply this thing.

45
00:02:49,360 --> 00:02:54,360
So I pivoted to more knowledge representation on the semantic web.

46
00:02:54,360 --> 00:02:58,920
We had a more press and realistic project to work with the Department of Defense.

47
00:02:58,920 --> 00:03:00,920
And so that's what I did.

48
00:03:00,920 --> 00:03:01,920
2012.

49
00:03:01,920 --> 00:03:03,920
This is like owl ontologies and I can't stop.

50
00:03:03,920 --> 00:03:04,920
Yeah, owl ontologies, yes.

51
00:03:04,920 --> 00:03:09,120
I will have you know, we built one of the first sensor ontologies on owl.

52
00:03:09,120 --> 00:03:10,120
Oh really?

53
00:03:10,120 --> 00:03:13,360
Yes, and I think of it by still one of the most cited in that area.

54
00:03:13,360 --> 00:03:14,360
Oh wow.

55
00:03:14,360 --> 00:03:15,360
But I like that kind of stuff.

56
00:03:15,360 --> 00:03:21,120
I like to do like motors, buttons, motors, torrents, first other predicate logic.

57
00:03:21,120 --> 00:03:22,120
I like that.

58
00:03:22,120 --> 00:03:23,120
It's very clean and clear.

59
00:03:23,120 --> 00:03:25,120
It never quite happened for the web the way it was supposed to do.

60
00:03:25,120 --> 00:03:26,120
It never did.

61
00:03:26,120 --> 00:03:27,120
I don't know what happened.

62
00:03:27,120 --> 00:03:28,640
Well, actually, I do know what happened.

63
00:03:28,640 --> 00:03:30,640
Probabilistic models happened.

64
00:03:30,640 --> 00:03:32,400
Probabilistic models happen.

65
00:03:32,400 --> 00:03:33,640
We have lots more data.

66
00:03:33,640 --> 00:03:38,000
We don't have to do that logical reasoning with enough data.

67
00:03:38,000 --> 00:03:41,240
We can understand like the norms of things and it's good enough.

68
00:03:41,240 --> 00:03:44,320
Humans, I good enough first fit model.

69
00:03:44,320 --> 00:03:48,120
We don't need to have the entire thing, but a good enough solution we can take it there.

70
00:03:48,120 --> 00:03:50,400
So probabilistic models were good enough.

71
00:03:50,400 --> 00:03:53,600
And then Google came and existed.

72
00:03:53,600 --> 00:03:55,080
And that was the end of that.

73
00:03:55,080 --> 00:03:56,080
And it works.

74
00:03:56,080 --> 00:03:57,080
Right.

75
00:03:57,080 --> 00:04:02,360
So I went to parental leave and by time I came back, all the packages had been created.

76
00:04:02,360 --> 00:04:05,400
There was a real ecosystem now for machine learning.

77
00:04:05,400 --> 00:04:10,440
And I actually wanted to stop to study more about how to actually acquire technical knowledge.

78
00:04:10,440 --> 00:04:15,040
Luckily enough, I went to Berkeley where they actually had programs where I could do this.

79
00:04:15,040 --> 00:04:16,040
And I created it.

80
00:04:16,040 --> 00:04:17,040
You need to enter the program.

81
00:04:17,040 --> 00:04:18,040
Yeah, at Berkeley.

82
00:04:18,040 --> 00:04:19,040
Okay.

83
00:04:19,040 --> 00:04:20,040
You know, they had this little small program.

84
00:04:20,040 --> 00:04:21,120
I could do that at Berkeley.

85
00:04:21,120 --> 00:04:25,120
And I was, I also was listening to a lot of JZNB on say, to be honest.

86
00:04:25,120 --> 00:04:27,120
So I was like, I need to spark you too.

87
00:04:27,120 --> 00:04:28,120
Yeah.

88
00:04:28,120 --> 00:04:29,120
I like language.

89
00:04:29,120 --> 00:04:30,120
I love rap music.

90
00:04:30,120 --> 00:04:32,120
So I'm just like, you know what?

91
00:04:32,120 --> 00:04:36,400
Wouldn't it be cool if they actually had like an NLP rap unit?

92
00:04:36,400 --> 00:04:38,480
Like why we always didn't poke in and all this stuff?

93
00:04:38,480 --> 00:04:39,480
Like I like rap music.

94
00:04:39,480 --> 00:04:42,840
A lot of what JZ is thinking, that's what I want to do.

95
00:04:42,840 --> 00:04:45,040
My NLP on is the rap corpus.

96
00:04:45,040 --> 00:04:46,040
Okay.

97
00:04:46,040 --> 00:04:48,920
So we were able to like do that unit in classes.

98
00:04:48,920 --> 00:04:54,080
And I just wanted to investigate like cultural approaches to actually acquiring technical

99
00:04:54,080 --> 00:04:56,280
knowledge.

100
00:04:56,280 --> 00:05:00,960
If people are not, if people are either passionate or extremely dispassionate, but there's

101
00:05:00,960 --> 00:05:07,360
some kind of emotional response versus just like nothing boring, and if people already

102
00:05:07,360 --> 00:05:12,120
come into an environment with mathematics and reasoning that they already have in their

103
00:05:12,120 --> 00:05:17,760
heads, then they're able to actually truly understand the machine learning is a statistical

104
00:05:17,760 --> 00:05:21,200
based approach to solving human problems.

105
00:05:21,200 --> 00:05:26,200
It's not some kind of, it's not just derivatives and partial differential equations.

106
00:05:26,200 --> 00:05:29,800
It's not, yes, we use that, but it's not math.

107
00:05:29,800 --> 00:05:35,920
It's math applied to human context and modeling that.

108
00:05:35,920 --> 00:05:37,760
That's what I wanted people to get at.

109
00:05:37,760 --> 00:05:42,360
So I did that, and I was really excited about it, I wanted to continue.

110
00:05:42,360 --> 00:05:49,280
And so wait, the connection between that and the rap music in J.C., the specific project

111
00:05:49,280 --> 00:05:50,280
that you...

112
00:05:50,280 --> 00:05:51,280
Yeah, I called it hip-hoppathy.

113
00:05:51,280 --> 00:05:52,280
It was a very small thing.

114
00:05:52,280 --> 00:05:53,280
Hip-hoppathy.

115
00:05:53,280 --> 00:05:54,280
Okay.

116
00:05:54,280 --> 00:05:57,000
I made a little thing, just like a little amiz-bush of such a work, you know?

117
00:05:57,000 --> 00:06:01,400
Because you have to do a PhD, you have to get out of there, you have to do something,

118
00:06:01,400 --> 00:06:03,960
study it, and get out of there.

119
00:06:03,960 --> 00:06:09,000
And I wanted to continue working around how humans actually acquire technical knowledge,

120
00:06:09,000 --> 00:06:15,400
and better yet, helping humans acquire that knowledge faster and solving their problems

121
00:06:15,400 --> 00:06:16,400
faster.

122
00:06:16,400 --> 00:06:23,640
So I was like, who would, who has the largest data on humans and computation?

123
00:06:23,640 --> 00:06:24,640
GitHub.

124
00:06:24,640 --> 00:06:30,440
Oh, it will also be in their interest to actually solve this problem too, because people are

125
00:06:30,440 --> 00:06:35,600
not on GitHub, technical people are on GitHub, a lot of people come on GitHub also to learn.

126
00:06:35,600 --> 00:06:39,320
And GitHub is where we build that technology that helps us move forward.

127
00:06:39,320 --> 00:06:42,760
So it would be a great place if you want to study that and actually do work in that area

128
00:06:42,760 --> 00:06:46,760
to go to GitHub machine learning, and they could build things to actually help people learn

129
00:06:46,760 --> 00:06:49,480
better and get their work done faster.

130
00:06:49,480 --> 00:06:53,240
So I reached out to people like GitHub, became friends with them.

131
00:06:53,240 --> 00:06:56,520
Luckily for me, they were creating a machine learning team.

132
00:06:56,520 --> 00:06:59,480
They asked me to interview, the rest is history.

133
00:06:59,480 --> 00:07:00,480
Here we are.

134
00:07:00,480 --> 00:07:01,480
Attensive flow.

135
00:07:01,480 --> 00:07:02,480
In Santa Clara.

136
00:07:02,480 --> 00:07:03,480
Right, right, right.

137
00:07:03,480 --> 00:07:07,480
So when you first had the idea of doing this at GitHub, there was no machine learning team

138
00:07:07,480 --> 00:07:08,480
at GitHub.

139
00:07:08,480 --> 00:07:09,480
There was no machine learning team.

140
00:07:09,480 --> 00:07:10,480
I was just like, hey, GitHub.

141
00:07:10,480 --> 00:07:11,480
Hi, it's me.

142
00:07:11,480 --> 00:07:12,480
What are you all doing?

143
00:07:12,480 --> 00:07:13,480
You must have data.

144
00:07:13,480 --> 00:07:14,480
What do you have?

145
00:07:14,480 --> 00:07:15,480
What are the plans for you?

146
00:07:15,480 --> 00:07:23,480
If people must have data now, because this is 2017, you existed since 2009.

147
00:07:23,480 --> 00:07:24,480
That's a little years of data.

148
00:07:24,480 --> 00:07:25,480
What are your plans for this data?

149
00:07:25,480 --> 00:07:30,480
I think you should have enough now that are to start using machine learning to help accelerate

150
00:07:30,480 --> 00:07:32,480
certain things.

151
00:07:32,480 --> 00:07:38,880
So coincidentally, or maybe I think would have it, the company in a couple of months decided

152
00:07:38,880 --> 00:07:40,480
that that's also what they wanted to do.

153
00:07:40,480 --> 00:07:43,480
They said they were like, oh, well, I already was already interested in this.

154
00:07:43,480 --> 00:07:46,640
But you just have to come and interview and see if you can help.

155
00:07:46,640 --> 00:07:48,480
She can help us be part of this thing.

156
00:07:48,480 --> 00:07:50,480
That's awesome.

157
00:07:50,480 --> 00:07:57,480
What's your, what's the scope of the mission of the ML team at GitHub and your role in particular?

158
00:07:57,480 --> 00:07:58,480
So I'm a machine learning engineer.

159
00:07:58,480 --> 00:08:00,480
I'm a senior machine learning engineer.

160
00:08:00,480 --> 00:08:02,480
I'm part of the inaugural machine learning team.

161
00:08:02,480 --> 00:08:09,480
Our mission is to use GitHub data to accelerate the transaction cost of developer.

162
00:08:09,480 --> 00:08:12,480
To reduce the transaction cost of developer corporation.

163
00:08:12,480 --> 00:08:13,480
And how do we do that?

164
00:08:13,480 --> 00:08:17,480
We do that by super charging GitHub features with data.

165
00:08:17,480 --> 00:08:20,480
Because everything that people are doing in GitHub, we have a lot of data.

166
00:08:20,480 --> 00:08:24,480
So if you've written issues, we've seen a lot of those issues.

167
00:08:24,480 --> 00:08:27,480
So we know what issues are going to get close faster than others.

168
00:08:27,480 --> 00:08:31,480
If you're looking at the same repos as I am, you're seeing a lot of the same issues too.

169
00:08:31,480 --> 00:08:34,480
Yeah, we're seeing the same issues over and over again, like bugs.

170
00:08:34,480 --> 00:08:37,480
Maybe things live all as bugs and get treated fast enough.

171
00:08:37,480 --> 00:08:40,480
Maybe, okay, how can we like uproot this things like that?

172
00:08:40,480 --> 00:08:41,480
Right.

173
00:08:41,480 --> 00:08:42,480
Yeah.

174
00:08:42,480 --> 00:08:48,480
And so you're building primarily kind of internally focused.

175
00:08:48,480 --> 00:08:51,480
What's the way to think of it?

176
00:08:51,480 --> 00:08:52,480
Is it GitHub features?

177
00:08:52,480 --> 00:08:53,480
Yes, GitHub features.

178
00:08:53,480 --> 00:08:54,480
They're not internal to GitHub.

179
00:08:54,480 --> 00:08:58,480
They are product that are put on GitHub itself.

180
00:08:58,480 --> 00:08:59,480
Okay.

181
00:08:59,480 --> 00:09:05,480
So one of the products we have is when you open a GitHub repo, create a new GitHub repo.

182
00:09:05,480 --> 00:09:09,480
Sometimes they give you suggestions of labels.

183
00:09:09,480 --> 00:09:11,480
That is a machine learning product.

184
00:09:11,480 --> 00:09:14,480
Like, oh, we see that you open this repo.

185
00:09:14,480 --> 00:09:17,480
Maybe you want to add this level of deep learning to it.

186
00:09:17,480 --> 00:09:18,480
Mm-hmm.

187
00:09:18,480 --> 00:09:19,480
Yeah, things like that.

188
00:09:19,480 --> 00:09:20,480
That is an example.

189
00:09:20,480 --> 00:09:21,480
Like, oh, it's a topic.

190
00:09:21,480 --> 00:09:22,480
Add this topic to it.

191
00:09:22,480 --> 00:09:24,480
Oh, this things look like Azure, but are notebooks in it.

192
00:09:24,480 --> 00:09:25,480
It has this in it.

193
00:09:25,480 --> 00:09:26,480
It has that.

194
00:09:26,480 --> 00:09:29,480
Maybe it's like the rest of everything that has all that stuff.

195
00:09:29,480 --> 00:09:30,480
That is all machine learning.

196
00:09:30,480 --> 00:09:31,480
Maybe you want to put this tag on it.

197
00:09:31,480 --> 00:09:32,480
Things like that.

198
00:09:32,480 --> 00:09:36,480
So imagine when you were a part of getting this effort started,

199
00:09:36,480 --> 00:09:40,480
the first thing to do was like look across, you know, GitHub and figure out

200
00:09:40,480 --> 00:09:43,480
where are the places that you inject machine learning.

201
00:09:43,480 --> 00:09:45,480
You mentioned the tags, like discoverability.

202
00:09:45,480 --> 00:09:47,480
It's got to be a challenge.

203
00:09:47,480 --> 00:09:51,480
So, you know, tags and search are probably a place to start thinking about machine learning.

204
00:09:51,480 --> 00:09:52,480
Yes.

205
00:09:52,480 --> 00:09:53,480
Search as a natural one.

206
00:09:53,480 --> 00:09:55,480
Discoverability is a natural one.

207
00:09:55,480 --> 00:09:56,480
Mm-hmm.

208
00:09:56,480 --> 00:10:00,480
So when I came in, I was, yeah, I think it was the fourth person.

209
00:10:00,480 --> 00:10:01,480
Okay.

210
00:10:01,480 --> 00:10:02,480
And that came to the ML team.

211
00:10:02,480 --> 00:10:06,480
So my predecessors, all everybody is still within months of each other.

212
00:10:06,480 --> 00:10:13,480
But like, okay, one of the first things we wanted to do was around helping people find open source

213
00:10:13,480 --> 00:10:17,480
communities where they want to participate because we're an open source platform.

214
00:10:17,480 --> 00:10:19,480
And one of the problems is you have all these skills.

215
00:10:19,480 --> 00:10:20,480
You come and GitHub.

216
00:10:20,480 --> 00:10:22,480
You want to give back some time.

217
00:10:22,480 --> 00:10:24,480
How do you even figure out what open source community you want to join?

218
00:10:24,480 --> 00:10:25,480
Right.

219
00:10:25,480 --> 00:10:27,480
So understanding user interest.

220
00:10:27,480 --> 00:10:32,480
And using that user interest, do recommendations for like open source communities.

221
00:10:32,480 --> 00:10:33,480
Mm-hmm.

222
00:10:33,480 --> 00:10:37,480
And the ideas are like, ah, we see you do a lot of ML stuff in here.

223
00:10:37,480 --> 00:10:39,480
Maybe you're interested in TensorFlow.

224
00:10:39,480 --> 00:10:40,480
Right.

225
00:10:40,480 --> 00:10:45,480
Well, even beyond that look, they have issues that we think you might be interested in.

226
00:10:45,480 --> 00:10:46,480
Right.

227
00:10:46,480 --> 00:10:47,480
And smicking those kinds of suggestions.

228
00:10:47,480 --> 00:10:51,480
So that's discoverability is one matching with open source is another one.

229
00:10:51,480 --> 00:10:53,480
Another one is triaging.

230
00:10:53,480 --> 00:10:57,480
Like, you know, putting labels on issues, maybe helping that with notifications.

231
00:10:57,480 --> 00:10:58,480
Security.

232
00:10:58,480 --> 00:11:02,480
There are so many places that machine learning fits in.

233
00:11:02,480 --> 00:11:03,480
Mm-hmm.

234
00:11:03,480 --> 00:11:07,480
Natural language, understanding of all the readmys and this and that.

235
00:11:07,480 --> 00:11:09,480
And GitHub is a global platform.

236
00:11:09,480 --> 00:11:13,480
So there are things that hasn't been built yet, like localization.

237
00:11:13,480 --> 00:11:15,480
Perhaps machine learning could actually help.

238
00:11:15,480 --> 00:11:18,480
If I'm reading a repo that is mostly written in Cantonese and Mandarin.

239
00:11:18,480 --> 00:11:20,480
And I want to extract that knowledge.

240
00:11:20,480 --> 00:11:25,480
Maybe there's a way we can actually help to do some kind of machine translation of some of the readmys.

241
00:11:25,480 --> 00:11:28,480
So I have some idea of what is in this repo that I could use.

242
00:11:28,480 --> 00:11:29,480
Okay.

243
00:11:29,480 --> 00:11:30,480
Things like that.

244
00:11:30,480 --> 00:11:31,480
Interesting, interesting.

245
00:11:31,480 --> 00:11:33,480
Maybe steps, which is beginning.

246
00:11:33,480 --> 00:11:39,480
And so when I think about kind of the scope of those types of problems and applying those problems.

247
00:11:39,480 --> 00:11:44,480
I'm imagining a lot of, but not everything that you're doing is looking at the code.

248
00:11:44,480 --> 00:11:50,480
And I'm wondering the types of tools that you use to enable that.

249
00:11:50,480 --> 00:11:55,480
Is it all kind of natural language processing or there are other things that.

250
00:11:55,480 --> 00:11:57,480
It doesn't sound like a solve solve problem.

251
00:11:57,480 --> 00:11:59,480
No, it's not.

252
00:11:59,480 --> 00:12:09,480
What are the types of techniques that you're kind of constantly using when you're thinking about building models based off of code?

253
00:12:09,480 --> 00:12:11,480
Machine learning on code.

254
00:12:11,480 --> 00:12:16,480
We are lucky enough at the machine learning team that we have sister team called semantic code team.

255
00:12:16,480 --> 00:12:17,480
Okay.

256
00:12:17,480 --> 00:12:24,480
And these are the folks that actually build parsers and build representations of code as abstract syntax trees.

257
00:12:24,480 --> 00:12:29,480
It's very exciting to work at GitHub because you actually get to do the computer science you learn.

258
00:12:29,480 --> 00:12:31,480
Like, you learn ASTs.

259
00:12:31,480 --> 00:12:34,480
And then you go into the role of the software engineer.

260
00:12:34,480 --> 00:12:37,480
If you're not writing compilers off when you're not using ASTs.

261
00:12:37,480 --> 00:12:40,480
So we have a team that actually does those kind of representations.

262
00:12:40,480 --> 00:12:49,480
And we can then ingest the things that create to actually model and build them representation and embedding of an AST.

263
00:12:49,480 --> 00:12:50,480
Okay.

264
00:12:50,480 --> 00:12:55,480
So that's something that is experimental now is not something that we actually have inside the product.

265
00:12:55,480 --> 00:12:57,480
But it's these are the kinds of things that we're working on.

266
00:12:57,480 --> 00:13:00,480
Like will ASTs help us get there faster?

267
00:13:00,480 --> 00:13:06,480
Or should we model code as an AST or should we model code and treat it like natural language?

268
00:13:06,480 --> 00:13:08,480
These are all these things you have to think about.

269
00:13:08,480 --> 00:13:09,480
Right.

270
00:13:09,480 --> 00:13:11,480
Because it's not natural language.

271
00:13:11,480 --> 00:13:12,480
Right.

272
00:13:12,480 --> 00:13:13,480
Clearly it's not.

273
00:13:13,480 --> 00:13:15,480
But it's a language and it's rules are different.

274
00:13:15,480 --> 00:13:19,480
So maybe then the question is how do you model a graph as an embedding?

275
00:13:19,480 --> 00:13:21,480
So they're all these different kinds of questions.

276
00:13:21,480 --> 00:13:27,480
And that is now a slow little subset of people in the world who do that kind of research.

277
00:13:27,480 --> 00:13:38,480
And the question is is there research in that area that is fine enough that can be brought into a production environment and applied environment and can scale.

278
00:13:38,480 --> 00:13:42,480
They can scale very, very well gracefully.

279
00:13:42,480 --> 00:13:46,480
So these are all the paths we actually have to now pave.

280
00:13:46,480 --> 00:13:50,480
Because we're not necessarily and it has to also happen very, very fast.

281
00:13:50,480 --> 00:13:53,480
Because we're not building something bundled in an IDE.

282
00:13:53,480 --> 00:13:54,480
Right.

283
00:13:54,480 --> 00:13:57,480
So these are all the paths we're learning how to pave.

284
00:13:57,480 --> 00:14:05,480
It sounds like at least a part of your team's charter is keeping an eye on that research horizon

285
00:14:05,480 --> 00:14:10,480
and what's happening there and trying to figure out which of those things are worth playing around with,

286
00:14:10,480 --> 00:14:13,480
far enough along to play around with.

287
00:14:13,480 --> 00:14:16,480
Yeah. And we have people who help us do that. We have our product managers,

288
00:14:16,480 --> 00:14:21,480
who help us, you know, the product managers help to also shape what we're going to pursue.

289
00:14:21,480 --> 00:14:22,480
Yeah.

290
00:14:22,480 --> 00:14:25,480
We have the ML team itself saying these are things that we think are,

291
00:14:25,480 --> 00:14:27,480
these are no longer on the horizon.

292
00:14:27,480 --> 00:14:29,480
I think we can do this now.

293
00:14:29,480 --> 00:14:35,480
Let's walk with our product managers to actually see if we can all have this joint vision and bring this to the product.

294
00:14:35,480 --> 00:14:39,480
Then we have our like, you know, infrastructure team saying,

295
00:14:39,480 --> 00:14:43,480
the infrastructure is solid enough or robust enough that we think we can scale this.

296
00:14:43,480 --> 00:14:48,480
So it's like a multiple stakeholders that we have our managers, our VPs,

297
00:14:48,480 --> 00:14:52,480
who have the longer term vision and does this actually fall in line with the longer term vision

298
00:14:52,480 --> 00:14:56,480
of what we want machine learning to really help with the product.

299
00:14:56,480 --> 00:15:00,480
So we are a small part of a big hole and we have multiple stakeholders,

300
00:15:00,480 --> 00:15:05,480
but luckily not wearing a place that we can all work together as one finely oiled machine

301
00:15:05,480 --> 00:15:10,480
to bring things to reality with maybe like a 12 month horizon.

302
00:15:10,480 --> 00:15:15,480
So here at the conference, you've already delivered a couple of presentations.

303
00:15:15,480 --> 00:15:16,480
Yes.

304
00:15:16,480 --> 00:15:17,480
Yeah.

305
00:15:17,480 --> 00:15:18,480
So what were those?

306
00:15:18,480 --> 00:15:19,480
What was the first one?

307
00:15:19,480 --> 00:15:24,480
So the first one was understanding what's happening to machine learning communities

308
00:15:24,480 --> 00:15:26,480
because machine learning and GitHub.

309
00:15:26,480 --> 00:15:29,480
So I had this idea for this talk like two years ago,

310
00:15:29,480 --> 00:15:32,480
a year and a half ago, one of my colleagues on an analytics team

311
00:15:32,480 --> 00:15:35,480
and a pink bench was like, well, what do you think about doing this?

312
00:15:35,480 --> 00:15:36,480
And she pushed me, pushed me.

313
00:15:36,480 --> 00:15:40,480
And I wrote half of it and then work calls,

314
00:15:40,480 --> 00:15:43,480
because it's not actually work.

315
00:15:43,480 --> 00:15:45,480
This is just like brain twiddling.

316
00:15:45,480 --> 00:15:46,480
Like, ah, let's see.

317
00:15:46,480 --> 00:15:47,480
Yeah.

318
00:15:47,480 --> 00:15:53,480
Because we realized that machine learning has a field as existed for a very long time.

319
00:15:53,480 --> 00:15:54,480
Right.

320
00:15:54,480 --> 00:16:00,480
But as a somewhat commercially viable field with real tooling around it,

321
00:16:00,480 --> 00:16:04,480
started its ascendancy around the same time of GitHub's invention.

322
00:16:04,480 --> 00:16:08,480
And so did GitHub help ML grow?

323
00:16:08,480 --> 00:16:09,480
Who knows?

324
00:16:09,480 --> 00:16:10,480
Or maybe it's the other way around.

325
00:16:10,480 --> 00:16:11,480
We don't know.

326
00:16:11,480 --> 00:16:15,480
But we do know that a lot of the stuff around GitHub machine learning communities

327
00:16:15,480 --> 00:16:16,480
have happened in GitHub.

328
00:16:16,480 --> 00:16:18,480
And we have that data going back 10 years.

329
00:16:18,480 --> 00:16:23,480
So we can actually do like a longitudinal retrospective study

330
00:16:23,480 --> 00:16:26,480
of the evolution of machine learning communities in GitHub.

331
00:16:26,480 --> 00:16:27,480
That was the first talk.

332
00:16:27,480 --> 00:16:31,480
And that talk was for the contributors themselves to TensorFlow.

333
00:16:31,480 --> 00:16:32,480
OK.

334
00:16:32,480 --> 00:16:35,480
So quick takeaways on that talk.

335
00:16:35,480 --> 00:16:38,480
What did you find in that study?

336
00:16:38,480 --> 00:16:40,480
Two major things happen.

337
00:16:40,480 --> 00:16:45,480
We have 2010, second learn, pandas, ipathun, Kaggle,

338
00:16:45,480 --> 00:16:47,480
all these things were created.

339
00:16:47,480 --> 00:16:52,480
2011 I think is that it might be 2010 to the Stanford AI course around that time.

340
00:16:52,480 --> 00:16:53,480
Yeah.

341
00:16:53,480 --> 00:16:57,480
100,000 people log on to one of them on machine learning.

342
00:16:57,480 --> 00:16:58,480
Including yours truly.

343
00:16:58,480 --> 00:16:59,480
Oh, good.

344
00:16:59,480 --> 00:16:59,480
Yeah.

345
00:16:59,480 --> 00:17:03,480
Because Sebastian Trump, I remember when did I do that Mass Rover Challenge?

346
00:17:03,480 --> 00:17:04,480
Uh-huh.

347
00:17:04,480 --> 00:17:05,480
I did the Andering one.

348
00:17:05,480 --> 00:17:07,480
So the Sebastian was different.

349
00:17:07,480 --> 00:17:08,480
Yeah.

350
00:17:08,480 --> 00:17:10,480
So why do I do this Mass Rover Challenge?

351
00:17:10,480 --> 00:17:11,480
Yeah.

352
00:17:11,480 --> 00:17:12,480
I was also a student.

353
00:17:12,480 --> 00:17:16,480
My advisor, my teacher, that professor of that time was also doing the Mass Rover Challenge.

354
00:17:16,480 --> 00:17:18,480
So it was like an academic thing.

355
00:17:18,480 --> 00:17:19,480
Yeah.

356
00:17:19,480 --> 00:17:22,480
So even I did nothing 100,000 people.

357
00:17:22,480 --> 00:17:23,480
What is this?

358
00:17:23,480 --> 00:17:24,480
Right.

359
00:17:24,480 --> 00:17:25,480
And most people did that.

360
00:17:25,480 --> 00:17:27,480
The following yet, Udacity was created.

361
00:17:27,480 --> 00:17:28,480
Coursera was created.

362
00:17:28,480 --> 00:17:29,480
Right.

363
00:17:29,480 --> 00:17:32,480
So then you're starting to see machine learning become a commercial,

364
00:17:32,480 --> 00:17:35,480
a viable business enterprise.

365
00:17:35,480 --> 00:17:36,480
Mm-hmm.

366
00:17:36,480 --> 00:17:38,480
No one could just academia and NASA.

367
00:17:38,480 --> 00:17:39,480
Mm-hmm.

368
00:17:39,480 --> 00:17:41,480
They're actually human VCs.

369
00:17:41,480 --> 00:17:43,480
Putting money into this thing.

370
00:17:43,480 --> 00:17:44,480
Right.

371
00:17:44,480 --> 00:17:48,480
So it was the first sign of the AI we until we had had thought.

372
00:17:48,480 --> 00:17:49,480
Right.

373
00:17:49,480 --> 00:17:50,480
Now it's a real thing.

374
00:17:50,480 --> 00:17:53,480
And so it started picking up, started picking up, started picking up, picking up, picking up.

375
00:17:53,480 --> 00:17:57,480
Then Andrew Ying does the cat thing at Google, Google X was it?

376
00:17:57,480 --> 00:18:01,480
Where they trained those neural networks to do it, to learn a representation of cats on YouTube.

377
00:18:01,480 --> 00:18:02,480
Right.

378
00:18:02,480 --> 00:18:04,480
I think there was a TED talk and all this.

379
00:18:04,480 --> 00:18:07,480
And everybody was like, oh my god, it's coming.

380
00:18:07,480 --> 00:18:09,480
Then we see another great rise.

381
00:18:09,480 --> 00:18:10,480
And they are no cafe.

382
00:18:10,480 --> 00:18:11,480
All these things are created.

383
00:18:11,480 --> 00:18:12,480
And then Google.

384
00:18:12,480 --> 00:18:19,480
And they got all of these, these, the rise and these inflections that you're speaking to is based on.

385
00:18:19,480 --> 00:18:20,480
Stars and get up.

386
00:18:20,480 --> 00:18:21,480
Right.

387
00:18:21,480 --> 00:18:28,480
So this is using GitHub stars as a proxy of human interest in these packages.

388
00:18:28,480 --> 00:18:29,480
Mm-hmm.

389
00:18:29,480 --> 00:18:34,480
And I did an analysis of quarter of a quarter growth of stars on GitHub repose.

390
00:18:34,480 --> 00:18:35,480
Okay.

391
00:18:35,480 --> 00:18:39,480
Of all these packages from 2019, all the way to 2019.

392
00:18:39,480 --> 00:18:45,480
To actually see if they're, what is the growth of stars on these packages from a quarter to quarter?

393
00:18:45,480 --> 00:18:46,480
Mm-hmm.

394
00:18:46,480 --> 00:18:49,480
And we notice a trend.

395
00:18:49,480 --> 00:18:50,480
I notice a trend.

396
00:18:50,480 --> 00:18:55,480
How big is just three cycles from 2012 Q4 to Q1 2013.

397
00:18:55,480 --> 00:18:58,480
There is a massive spike.

398
00:18:58,480 --> 00:19:07,480
We see the same thing I think around 2015 or in that 2015 to 2016 or 2016 to 2017.

399
00:19:07,480 --> 00:19:10,480
Another massive spike.

400
00:19:10,480 --> 00:19:16,480
The third cycle, which will be around 2019, no spike.

401
00:19:16,480 --> 00:19:18,480
So it's only 10 years worth of data.

402
00:19:18,480 --> 00:19:19,480
Right.

403
00:19:19,480 --> 00:19:20,480
No spike.

404
00:19:20,480 --> 00:19:26,480
And for the first time, if you look at all the stars of all the major repos in machine learning,

405
00:19:26,480 --> 00:19:30,480
we're talking about your TensorFlow's pie torch, cafe.

406
00:19:30,480 --> 00:19:31,480
Right.

407
00:19:31,480 --> 00:19:43,480
You just boost, cycle learn, Jupyter, and so on and so forth.

408
00:19:43,480 --> 00:19:44,480
Interesting.

409
00:19:44,480 --> 00:19:46,480
So the question is why.

410
00:19:46,480 --> 00:19:49,480
Is this the beginning of another AI winter?

411
00:19:49,480 --> 00:19:51,480
I don't think so because you look at VC funding.

412
00:19:51,480 --> 00:19:54,480
I don't think it has accelerated at all.

413
00:19:54,480 --> 00:19:57,480
And if you look at the dot AI domain,

414
00:19:57,480 --> 00:20:01,480
these things are just going up. One more people are going to nerves.

415
00:20:01,480 --> 00:20:03,480
Nerves are longer and academic pursuits.

416
00:20:03,480 --> 00:20:04,480
Right.

417
00:20:04,480 --> 00:20:07,480
It's not becoming like, I don't even know what it is.

418
00:20:07,480 --> 00:20:09,480
So it's not necessarily slowing down.

419
00:20:09,480 --> 00:20:10,480
So somebody asked a question.

420
00:20:10,480 --> 00:20:15,480
I said, did you look at the production framework repos?

421
00:20:15,480 --> 00:20:18,480
Things like Kubernetes, cube flow, things TF serving.

422
00:20:18,480 --> 00:20:19,480
We looked at those ones.

423
00:20:19,480 --> 00:20:21,480
And I realized I hadn't done that.

424
00:20:21,480 --> 00:20:25,480
I'm going to go back to the office and look at that to see maybe those is where that's where we're putting our energy

425
00:20:25,480 --> 00:20:27,480
because we're actually putting things into production now.

426
00:20:27,480 --> 00:20:28,480
Right. Right.

427
00:20:28,480 --> 00:20:35,480
Or is it that it's longer trended because it has become a permanent part of software engineering?

428
00:20:35,480 --> 00:20:36,480
Right.

429
00:20:36,480 --> 00:20:37,480
It's longer trend.

430
00:20:37,480 --> 00:20:38,480
It's just is.

431
00:20:38,480 --> 00:20:41,480
Or it could be that we've solved all the problems.

432
00:20:41,480 --> 00:20:42,480
We've not solved any problem.

433
00:20:42,480 --> 00:20:44,480
Did you come here and I'll tell you my vehicle?

434
00:20:44,480 --> 00:20:46,480
I don't think so.

435
00:20:46,480 --> 00:20:47,480
Exactly.

436
00:20:47,480 --> 00:20:57,480
And what's interesting about these three phases is that they map very cleanly or directly, I guess,

437
00:20:57,480 --> 00:20:59,480
is the better word to my own experience.

438
00:20:59,480 --> 00:21:00,480
Right.

439
00:21:00,480 --> 00:21:08,480
So in that, I think it was 2011 time frame when Andrew did his course.

440
00:21:08,480 --> 00:21:09,480
That was that first wave.

441
00:21:09,480 --> 00:21:10,480
He's first wave.

442
00:21:10,480 --> 00:21:18,480
I was in that wave doing that course and then in the 2015, 2016 wave,

443
00:21:18,480 --> 00:21:24,480
this podcast was started out of kind of feeling the energy that, you know,

444
00:21:24,480 --> 00:21:27,480
is this inflection point that you're talking about?

445
00:21:27,480 --> 00:21:35,480
And, you know, maybe the silent echo, we call it like this last, you know, these last couple of years.

446
00:21:35,480 --> 00:21:36,480
Now, right?

447
00:21:36,480 --> 00:21:41,480
It's this, yeah, Q4 2019 going to cute.

448
00:21:41,480 --> 00:21:43,480
Maybe, maybe something's going to happen over Christmas.

449
00:21:43,480 --> 00:21:44,480
Right.

450
00:21:44,480 --> 00:21:50,480
Well, I think what's happening from my perspective, what's happening is that a lot of the energy and investment

451
00:21:50,480 --> 00:21:57,480
is going into making it real and productionalizing, operationalizing, a lot of the experimentation

452
00:21:57,480 --> 00:21:59,480
that's been happening.

453
00:21:59,480 --> 00:22:06,480
And so I'm very curious to see what you find around these frameworks.

454
00:22:06,480 --> 00:22:11,480
But even then, I think, and what I mean is the higher level like the production framework.

455
00:22:11,480 --> 00:22:13,480
Yeah, the production frameworks.

456
00:22:13,480 --> 00:22:14,480
Yeah, the production frameworks.

457
00:22:14,480 --> 00:22:20,480
But even then, a lot of it is people like building their own stuff off of the low level things

458
00:22:20,480 --> 00:22:27,480
and just kind of demonstrating value of first kind of building that kind of smooth glide path

459
00:22:27,480 --> 00:22:30,480
and then using that to demonstrate value.

460
00:22:30,480 --> 00:22:45,480
And it could be that, you know, what we're seeing now is, you know, ultimately the growth in kind of the broader market

461
00:22:45,480 --> 00:22:48,480
is people kind of reaping value.

462
00:22:48,480 --> 00:22:50,480
And a lot of that value is internal and not shared.

463
00:22:50,480 --> 00:22:52,480
So that's what I was also thinking about.

464
00:22:52,480 --> 00:22:57,480
Maybe it's actually going to private repose in enterprises.

465
00:22:57,480 --> 00:22:59,480
And so we don't look at any private repose.

466
00:22:59,480 --> 00:23:02,480
But one thing, I don't think we're going into a winter.

467
00:23:02,480 --> 00:23:09,480
But I do think we have a solid five to ten years to say that machine learning is not a trend though.

468
00:23:09,480 --> 00:23:16,480
Because some of the challenges are our lovely friends at the media have written all these stories,

469
00:23:16,480 --> 00:23:23,480
this wonderful stories, everybody has high expectation.

470
00:23:23,480 --> 00:23:26,480
In five years, I expect to, I am hoping the car I have now is the last vehicle I will ever own.

471
00:23:26,480 --> 00:23:33,480
Because I'm hoping that by that time there should be an autonomous vehicle that I hope car ownership would actually go away.

472
00:23:33,480 --> 00:23:37,480
Maybe I just buy some Uber credits or some company of the future thing.

473
00:23:37,480 --> 00:23:39,480
And I just have transportation solved.

474
00:23:39,480 --> 00:23:45,480
And the idea of me as a human being owning a car driving it becomes a hobby like polo.

475
00:23:45,480 --> 00:23:47,480
If I want to drive, I want to track.

476
00:23:47,480 --> 00:23:50,480
But it's not like a transportation thing.

477
00:23:50,480 --> 00:23:54,480
That is my idea of what the next five to ten years is.

478
00:23:54,480 --> 00:23:59,480
If that doesn't happen though, machine learning might then just have become truly a fad.

479
00:23:59,480 --> 00:24:01,480
That'll be the downside.

480
00:24:01,480 --> 00:24:02,480
But I doubt that will happen.

481
00:24:02,480 --> 00:24:04,480
Because there's too much.

482
00:24:04,480 --> 00:24:06,480
Look at Google search.

483
00:24:06,480 --> 00:24:08,480
Burp is already helping with Google search.

484
00:24:08,480 --> 00:24:10,480
There's a real real there.

485
00:24:10,480 --> 00:24:13,480
But I want to see other companies outside of the major Silicon Valley.

486
00:24:13,480 --> 00:24:18,480
The big ones really have a real win with machine learning.

487
00:24:18,480 --> 00:24:21,480
Not Google, not Facebook, not Microsoft, not GitHub.

488
00:24:21,480 --> 00:24:23,480
I want to see a company that doesn't exist today.

489
00:24:23,480 --> 00:24:26,480
And in five years from now revolutionized the world.

490
00:24:26,480 --> 00:24:29,480
That for me will be when we've truly arrived.

491
00:24:29,480 --> 00:24:30,480
Yeah, yeah.

492
00:24:30,480 --> 00:24:33,480
I am very confident that we will see that.

493
00:24:33,480 --> 00:24:36,480
And we probably already know some of those companies.

494
00:24:36,480 --> 00:24:41,480
And if they, you know, they're probably already starting these projects that are going to do that.

495
00:24:41,480 --> 00:24:46,480
The other thing that you, the other thing that what you said made me think of is this

496
00:24:46,480 --> 00:24:51,480
kind of the hype cycle technology adoption hype cycle that I think Gartner popularized

497
00:24:51,480 --> 00:24:52,480
but or created.

498
00:24:52,480 --> 00:24:56,480
But it might have come before them or been someone else.

499
00:24:56,480 --> 00:25:00,480
But he had an inevitable part of this hype cycle is the trough of disillusionment.

500
00:25:00,480 --> 00:25:01,480
Yes.

501
00:25:01,480 --> 00:25:02,480
Right.

502
00:25:02,480 --> 00:25:09,480
So the peak of inflated expectations is kind of maybe where we were in 1516 or where we are today.

503
00:25:09,480 --> 00:25:11,480
Who knows.

504
00:25:11,480 --> 00:25:16,480
But at some point, you know, the hype kind of catches up to the market.

505
00:25:16,480 --> 00:25:17,480
Yeah.

506
00:25:17,480 --> 00:25:18,480
Right.

507
00:25:18,480 --> 00:25:20,480
And then, you know, there's a bit of a retrenchment, not necessarily a winter.

508
00:25:20,480 --> 00:25:22,480
But it's, you know, people have a contraction.

509
00:25:22,480 --> 00:25:23,480
A bit of a contraction.

510
00:25:23,480 --> 00:25:24,480
A bit of a contraction.

511
00:25:24,480 --> 00:25:26,480
People working hard to kind of create value.

512
00:25:26,480 --> 00:25:29,480
And then I always forget what the next one is.

513
00:25:29,480 --> 00:25:31,480
Plateau of productivity, I think.

514
00:25:31,480 --> 00:25:33,480
That's where you get value out of it.

515
00:25:33,480 --> 00:25:38,480
And I think we're kind of, you know, the different,

516
00:25:38,480 --> 00:25:43,480
the different sub technologies of ML&AR in different places on this curve.

517
00:25:43,480 --> 00:25:44,480
Yeah.

518
00:25:44,480 --> 00:25:49,480
But I don't think we should be too surprised that there is a trough, you know,

519
00:25:49,480 --> 00:25:54,480
and that we come out of that trough and, you know, ultimately give value out of the stuff.

520
00:25:54,480 --> 00:25:55,480
I hope so.

521
00:25:55,480 --> 00:25:56,480
I really hope so.

522
00:25:56,480 --> 00:25:57,480
Yeah.

523
00:25:57,480 --> 00:25:58,480
So that was the first presentation.

524
00:25:58,480 --> 00:25:59,480
That was the first presentation.

525
00:25:59,480 --> 00:26:01,480
And what was the second presentation?

526
00:26:01,480 --> 00:26:05,480
The second presentation is about machine learning as a product itself.

527
00:26:05,480 --> 00:26:06,480
Okay.

528
00:26:06,480 --> 00:26:10,480
One thing we've noticed is there's so many hobbyists and people really, really getting

529
00:26:10,480 --> 00:26:12,480
the feet wet into machine learning.

530
00:26:12,480 --> 00:26:14,480
And they don't have any path to production.

531
00:26:14,480 --> 00:26:16,480
They're not inside of me company.

532
00:26:16,480 --> 00:26:20,480
They're an individual contributor inside their home right in the model.

533
00:26:20,480 --> 00:26:21,480
And they want to move forward.

534
00:26:21,480 --> 00:26:28,480
And so we realize that we actually have a path to production within GitHub data app ecosystem.

535
00:26:28,480 --> 00:26:31,480
Because we have something called the GitHub marketplace.

536
00:26:31,480 --> 00:26:36,480
And in GitHub marketplace, you can put an unverified GitHub app on the marketplace.

537
00:26:36,480 --> 00:26:41,480
And so my colleague, how I'm all realize that, whoa, I could actually build an ML model

538
00:26:41,480 --> 00:26:44,480
and put it in a GitHub app and push it to marketplace.

539
00:26:44,480 --> 00:26:49,480
And people can actually use it to solve problems and get up itself, especially for open source.

540
00:26:49,480 --> 00:26:55,480
So independent of ML, what are the kinds of things that I might find in the GitHub marketplace?

541
00:26:55,480 --> 00:26:56,480
There are lots of apps.

542
00:26:56,480 --> 00:26:58,480
I think they can ban apps.

543
00:26:58,480 --> 00:27:00,480
But there are lots of app around software engineering.

544
00:27:00,480 --> 00:27:01,480
Yeah.

545
00:27:01,480 --> 00:27:03,480
Just the software engineering process workflows.

546
00:27:03,480 --> 00:27:04,480
There's all kinds of apps.

547
00:27:04,480 --> 00:27:05,480
Okay.

548
00:27:05,480 --> 00:27:09,480
Like maybe you open an issue or a new pull request and something says, hi, or something like that.

549
00:27:09,480 --> 00:27:13,480
They're just any kind of thing they can hook into a GitHub web hook.

550
00:27:13,480 --> 00:27:14,480
You can build an app around.

551
00:27:14,480 --> 00:27:17,480
So there are people who use a lot of stuff.

552
00:27:17,480 --> 00:27:18,480
Okay.

553
00:27:18,480 --> 00:27:19,480
Got it.

554
00:27:19,480 --> 00:27:22,480
And so we realize that we care a lot about open source.

555
00:27:22,480 --> 00:27:25,480
We also do a lot of work around natural language processing.

556
00:27:25,480 --> 00:27:31,480
The kinds of stuff that we are tinkering with, experimenting with within GitHub itself.

557
00:27:31,480 --> 00:27:36,480
One of the ways we can move our experiments forward is actually maybe something is not

558
00:27:36,480 --> 00:27:39,480
all the way quite ready to be a full-on product yet.

559
00:27:39,480 --> 00:27:42,480
You need to work out some of the kinks of it.

560
00:27:42,480 --> 00:27:49,480
One thing you can do, a look-a-metement way to get it continued to go, is to actually push it on the marketplace

561
00:27:49,480 --> 00:27:51,480
and have users use it.

562
00:27:51,480 --> 00:27:57,480
So you can learn the utility of it, you can iterate on it, and anybody can do that.

563
00:27:57,480 --> 00:28:01,480
Especially when it comes to machine learning itself as a community on GitHub.

564
00:28:01,480 --> 00:28:03,480
Like we've been talking about the hype cycles.

565
00:28:03,480 --> 00:28:07,480
What those things actually mean is people's weekends and weeknights.

566
00:28:07,480 --> 00:28:15,480
The maintainers of this of the same frameworks, spending inordinate amount of hours working for no pay

567
00:28:15,480 --> 00:28:21,480
often to build the things that we all use, we take for granted, somebody built pandas.

568
00:28:21,480 --> 00:28:23,480
Somebody built non-party.

569
00:28:23,480 --> 00:28:25,480
Was kidding in that guess.

570
00:28:25,480 --> 00:28:29,480
And sometimes they built this before they joined the corporation.

571
00:28:29,480 --> 00:28:34,480
We write a Python thing, you put your import pandas as NPT.

572
00:28:34,480 --> 00:28:39,480
Somebody's put time for free often to make that happen for you.

573
00:28:39,480 --> 00:28:44,480
And that person is one person having to deal with 600 contributors.

574
00:28:44,480 --> 00:28:50,480
So the amount of maintainers to contribute or flow, it's just not tractable.

575
00:28:50,480 --> 00:28:53,480
So sometimes you don't get the kind of response you want in open source,

576
00:28:53,480 --> 00:28:56,480
which affects the health of the open source community itself.

577
00:28:56,480 --> 00:29:02,480
One thing we realized very, very early on is that machine learning can actually automate a lot of those challenges away.

578
00:29:02,480 --> 00:29:08,480
Machine learning could actually triage some of your issues for you by putting levels that are pushing it to the right person.

579
00:29:08,480 --> 00:29:15,480
And you as a person in the ecosystem can get access to get up data to get up archive,

580
00:29:15,480 --> 00:29:19,480
which is actually inside of Google BigQuery.

581
00:29:19,480 --> 00:29:21,480
You can extract information.

582
00:29:21,480 --> 00:29:26,480
Meaning the code data or the lifecycle data or the behavioral data.

583
00:29:26,480 --> 00:29:27,480
Right.

584
00:29:27,480 --> 00:29:28,480
The behavioral data.

585
00:29:28,480 --> 00:29:30,480
You can use that data to tray models.

586
00:29:30,480 --> 00:29:32,480
We now have code data.

587
00:29:32,480 --> 00:29:35,480
We publish code data in something called code search net.

588
00:29:35,480 --> 00:29:42,480
We just did I think last month. So we have like six million snippets of functions that we've cleaned for you.

589
00:29:42,480 --> 00:29:44,480
We've parsed in multiple languages.

590
00:29:44,480 --> 00:29:45,480
We have JavaScript.

591
00:29:45,480 --> 00:29:46,480
We have PHP.

592
00:29:46,480 --> 00:29:47,480
We have Python.

593
00:29:47,480 --> 00:29:48,480
We have Java.

594
00:29:48,480 --> 00:29:52,480
So we have all these data sets that you can actually start using to build models.

595
00:29:52,480 --> 00:29:57,480
And then better yet we have get up apps that you can then push your app into production.

596
00:29:57,480 --> 00:30:00,480
And people can use it and give you feedback.

597
00:30:00,480 --> 00:30:06,480
And you can be a virtual circle cycle for helping keeping the health of our ecosystem alive.

598
00:30:06,480 --> 00:30:11,480
And we had our partner, Jeremy, who works at CUPLU, who's one of the lead developers of CUPLU,

599
00:30:11,480 --> 00:30:16,480
who is a customer of the issue level bot that, how my colleague wrote.

600
00:30:16,480 --> 00:30:19,480
And so we're talking about that entire virtual cycle.

601
00:30:19,480 --> 00:30:22,480
Like you want to really start contributing to machine learning.

602
00:30:22,480 --> 00:30:28,480
Start by building this apps to help keep the health of the machine learning communities themselves good.

603
00:30:28,480 --> 00:30:30,480
And we're using TensorFlow to do that.

604
00:30:30,480 --> 00:30:36,480
You've got these data sets that you make available both about code,

605
00:30:36,480 --> 00:30:43,480
not to mention the code that's on GitHub itself, but these data sets as well as the interaction information.

606
00:30:43,480 --> 00:30:55,480
And you provide the ability for folks to create applications that interfaces with or interacts with rather the GitHub users and repos,

607
00:30:55,480 --> 00:31:00,480
you know, things like, you know, tagging, uh, tagging issues and.

608
00:31:00,480 --> 00:31:03,480
Notifications. Any kind of event on GitHub.

609
00:31:03,480 --> 00:31:04,480
Yeah.

610
00:31:04,480 --> 00:31:05,480
Whatever it is.

611
00:31:05,480 --> 00:31:06,480
Somebody start a repo.

612
00:31:06,480 --> 00:31:09,480
They could write something to just interact with that.

613
00:31:09,480 --> 00:31:10,480
Somebody forked your repo.

614
00:31:10,480 --> 00:31:13,480
You could just say, oh, thank you. What do you want to use it for?

615
00:31:13,480 --> 00:31:14,480
You could write a bot to do that.

616
00:31:14,480 --> 00:31:16,480
Yeah, right. So it's, I was going to say it sounds a lot like a bot.

617
00:31:16,480 --> 00:31:19,480
Like do you think of them as bots essentially or are their bots?

618
00:31:19,480 --> 00:31:20,480
Yeah.

619
00:31:20,480 --> 00:31:22,480
Bots can have ML in them.

620
00:31:22,480 --> 00:31:23,480
Right.

621
00:31:23,480 --> 00:31:26,480
It's a way to get something to use this.

622
00:31:26,480 --> 00:31:28,480
It's a, it's a path to build their products.

623
00:31:28,480 --> 00:31:29,480
Yeah.

624
00:31:29,480 --> 00:31:36,480
So it's, so as opposed to, you know, if you, if you are playing around with machine learning and want to do something interesting,

625
00:31:36,480 --> 00:31:40,480
as opposed to building your own website or building a UI, something with a UI and having to deal with all that,

626
00:31:40,480 --> 00:31:45,480
you can build a GitHub bot and you've got this marketplace of people that you,

627
00:31:45,480 --> 00:31:48,480
that can connect to your GitHub bot and kind of take advantage of it.

628
00:31:48,480 --> 00:31:49,480
Yeah.

629
00:31:49,480 --> 00:31:53,480
And if you want to, you can get a verified, I don't want to make money.

630
00:31:53,480 --> 00:31:54,480
Yeah.

631
00:31:54,480 --> 00:31:57,480
Nothing stops you, but you can actually start adding value.

632
00:31:57,480 --> 00:32:00,480
You can understand that machine learning is not some delitant pursuit.

633
00:32:00,480 --> 00:32:01,480
Right.

634
00:32:01,480 --> 00:32:04,480
That it is a set of technologies to solve real human problems.

635
00:32:04,480 --> 00:32:07,480
And you can start doing that immediately.

636
00:32:07,480 --> 00:32:13,480
You don't have to wait to be part of a company and then do your ML within that company and then push it to users.

637
00:32:13,480 --> 00:32:16,480
You have the power now to do end to end.

638
00:32:16,480 --> 00:32:20,480
And so did you go into, like, how technical was the presentation?

639
00:32:20,480 --> 00:32:24,480
Did you go into some of the, you know, the details of how you would go about doing that?

640
00:32:24,480 --> 00:32:26,480
Not really, but we have lots.

641
00:32:26,480 --> 00:32:27,480
We're going to release the slides.

642
00:32:27,480 --> 00:32:32,480
We have, if you don't know how to write in flask, there's a course that we recommend.

643
00:32:32,480 --> 00:32:33,480
We put all the events there.

644
00:32:33,480 --> 00:32:36,480
We put snippets of the queries that you can use in BigQuery.

645
00:32:36,480 --> 00:32:38,480
So we get everything that you would need.

646
00:32:38,480 --> 00:32:43,480
We pointed to all the right sources and tutorials that you can actually recreate this.

647
00:32:43,480 --> 00:32:47,480
And they should labor about itself as open source. So you can see every single thing.

648
00:32:47,480 --> 00:32:50,480
So let's talk a little bit about the issue label bot is.

649
00:32:50,480 --> 00:32:51,480
It's a bot.

650
00:32:51,480 --> 00:32:56,480
It's responding to an issues created and it's, you know, has access to the content of that issue.

651
00:32:56,480 --> 00:33:00,480
Yeah. And it's liberated as a bug feature enhancement question.

652
00:33:00,480 --> 00:33:02,480
It's just adding labels to it.

653
00:33:02,480 --> 00:33:09,480
And something as simple as that is so powerful in giving maintenance back hours.

654
00:33:09,480 --> 00:33:17,480
And so the idea is, so GitHub has its own kind of automated issue labeling that it's doing.

655
00:33:17,480 --> 00:33:19,480
Correct or no?

656
00:33:19,480 --> 00:33:20,480
No, not really.

657
00:33:20,480 --> 00:33:22,480
We don't have the automated issue labeling just yet.

658
00:33:22,480 --> 00:33:23,480
Okay.

659
00:33:23,480 --> 00:33:24,480
We're starting to do the automated issue level.

660
00:33:24,480 --> 00:33:30,480
So you think your group, you mentioned earlier, your group is kind of thinking about how you might do that at the scale of GitHub.

661
00:33:30,480 --> 00:33:37,480
But if someone wants, you're not doing it because you can't do it.

662
00:33:37,480 --> 00:33:41,480
It's, you have to deal with the scale, but someone could take this issue label bot.

663
00:33:41,480 --> 00:33:45,480
And are they, what does that mean to take that?

664
00:33:45,480 --> 00:33:49,480
Did they have to take that and then plug their own model in or what are they doing?

665
00:33:49,480 --> 00:33:50,480
No, they don't plug their model.

666
00:33:50,480 --> 00:33:51,480
They just take it.

667
00:33:51,480 --> 00:33:52,480
It's a connection.

668
00:33:52,480 --> 00:33:55,480
They get a repo and the budget starts working.

669
00:33:55,480 --> 00:33:58,480
And what is the bot able to do?

670
00:33:58,480 --> 00:34:00,480
What labels is it able to, is it?

671
00:34:00,480 --> 00:34:01,480
Those three papers.

672
00:34:01,480 --> 00:34:02,480
Really labeling?

673
00:34:02,480 --> 00:34:03,480
Yes.

674
00:34:03,480 --> 00:34:08,480
The issue, it says, is this a bug, it labels it as a bug, if it's a bug, if it records nice, it predicts.

675
00:34:08,480 --> 00:34:09,480
Oh, this looks like a bug.

676
00:34:09,480 --> 00:34:10,480
Okay.

677
00:34:10,480 --> 00:34:11,480
Somebody said not working.

678
00:34:11,480 --> 00:34:12,480
Yeah.

679
00:34:12,480 --> 00:34:16,480
Then it opens it, it puts an issue, it puts another comment in the issue.

680
00:34:16,480 --> 00:34:17,480
This looks like a bug.

681
00:34:17,480 --> 00:34:21,480
If you think it's a bug, say thumbs, thumbs up, or thumbs down.

682
00:34:21,480 --> 00:34:24,480
If you say thumbs up, it adds the label bug to it.

683
00:34:24,480 --> 00:34:27,480
So it does its prediction and asks you to verify.

684
00:34:27,480 --> 00:34:29,480
And you can say, yes, it's a bug.

685
00:34:29,480 --> 00:34:31,480
And that just reinforces it learns.

686
00:34:31,480 --> 00:34:32,480
So it just helps to triage for you.

687
00:34:32,480 --> 00:34:33,480
Got it.

688
00:34:33,480 --> 00:34:35,480
With those simple labels.

689
00:34:35,480 --> 00:34:36,480
Something as simple as that.

690
00:34:36,480 --> 00:34:37,480
The use of the bug and whatever.

691
00:34:37,480 --> 00:34:39,480
The use of the bug, enhancements, or question.

692
00:34:39,480 --> 00:34:40,480
Got it.

693
00:34:40,480 --> 00:34:42,480
So maybe it's a feature request.

694
00:34:42,480 --> 00:34:44,480
I would like this to do that.

695
00:34:44,480 --> 00:34:47,480
Or how do I, whatever, question?

696
00:34:47,480 --> 00:34:49,480
Those three simple things.

697
00:34:49,480 --> 00:34:53,480
And that gives maintenance back hours.

698
00:34:53,480 --> 00:34:56,480
Because this is what humans have to read.

699
00:34:56,480 --> 00:34:59,480
And say, okay, I'm going to, somebody's job was just...

700
00:34:59,480 --> 00:35:00,480
Tagging news.

701
00:35:00,480 --> 00:35:02,480
Yeah, yeah.

702
00:35:02,480 --> 00:35:03,480
This is what everything that we do.

703
00:35:03,480 --> 00:35:04,480
This is how these things get done.

704
00:35:04,480 --> 00:35:05,480
Right.

705
00:35:05,480 --> 00:35:06,480
It's somebody doing all this kind of work.

706
00:35:06,480 --> 00:35:07,480
Right.

707
00:35:07,480 --> 00:35:10,480
But that person, if we can give them back their hours, it's great.

708
00:35:10,480 --> 00:35:11,480
Right.

709
00:35:11,480 --> 00:35:15,480
And especially for building a robust ecosystem of machine learning,

710
00:35:15,480 --> 00:35:19,480
GitHub is the home of all developers, including machine learning developers.

711
00:35:19,480 --> 00:35:25,480
The virtual cycle is you can actually do things to make the ecosystem more robust.

712
00:35:25,480 --> 00:35:26,480
Mm-hmm.

713
00:35:26,480 --> 00:35:28,480
And solve our own problems by leveraging.

714
00:35:28,480 --> 00:35:31,480
If you, by thinking of GitHub as a platform,

715
00:35:31,480 --> 00:35:33,480
instead of just could repository hosting,

716
00:35:33,480 --> 00:35:36,480
it's also a platform to plug and build things on top of.

717
00:35:36,480 --> 00:35:37,480
Got it.

718
00:35:37,480 --> 00:35:38,480
So we ourselves and the ML team,

719
00:35:38,480 --> 00:35:41,480
we build things on top of that platform as open source.

720
00:35:41,480 --> 00:35:45,480
And it's just a way to also just play with things they're experimenting with.

721
00:35:45,480 --> 00:35:47,480
Like, if you're writing a little experiment,

722
00:35:47,480 --> 00:35:49,480
why not open source it to see if it's actually useful?

723
00:35:49,480 --> 00:35:52,480
And maybe it's this useful, they add more time to it,

724
00:35:52,480 --> 00:35:54,480
and you actually build it all the way up.

725
00:35:54,480 --> 00:35:55,480
Yeah, yeah, yeah.

726
00:35:55,480 --> 00:35:59,480
With this issue label bought, it's something that you can immediately start using

727
00:35:59,480 --> 00:36:00,480
and get some value out of.

728
00:36:00,480 --> 00:36:02,480
And it sounds like the cube flow team is using that.

729
00:36:02,480 --> 00:36:03,480
The cube flow team is using it.

730
00:36:03,480 --> 00:36:04,480
Okay.

731
00:36:04,480 --> 00:36:06,480
And they're getting a lot of value out of it.

732
00:36:06,480 --> 00:36:08,480
The person that wrote the bot is downstairs, Hammer.

733
00:36:08,480 --> 00:36:09,480
Yeah.

734
00:36:09,480 --> 00:36:11,480
Here at the bot, Jeremy is also with us in the talk.

735
00:36:11,480 --> 00:36:13,480
He's using that bot to triage.

736
00:36:13,480 --> 00:36:14,480
Yeah.

737
00:36:14,480 --> 00:36:16,480
He's environment, he's a maintenance, he's project,

738
00:36:16,480 --> 00:36:19,480
because they want to keep the health of that project very high.

739
00:36:19,480 --> 00:36:20,480
Right.

740
00:36:20,480 --> 00:36:22,480
They want people to continue to contribute.

741
00:36:22,480 --> 00:36:24,480
And they want to turn around to actually getting things done.

742
00:36:24,480 --> 00:36:25,480
Right.

743
00:36:25,480 --> 00:36:28,480
Nobody wants to go to an issues, listen, see a bunch of issues

744
00:36:28,480 --> 00:36:29,480
that haven't been responded to.

745
00:36:29,480 --> 00:36:30,480
Exactly.

746
00:36:30,480 --> 00:36:31,480
And tagging is the first step in.

747
00:36:31,480 --> 00:36:34,480
That's the first step to figure out where should he go.

748
00:36:34,480 --> 00:36:35,480
Right.

749
00:36:35,480 --> 00:36:36,480
Right.

750
00:36:36,480 --> 00:36:39,480
And so, but this is not just a tool.

751
00:36:39,480 --> 00:36:43,480
It's also an example for other folks that might want to build their own tools.

752
00:36:43,480 --> 00:36:44,480
Absolutely.

753
00:36:44,480 --> 00:36:45,480
Using machine learning.

754
00:36:45,480 --> 00:36:46,480
Yes.

755
00:36:46,480 --> 00:36:48,480
That connect into the GitHub experience.

756
00:36:48,480 --> 00:36:49,480
Yeah.

757
00:36:49,480 --> 00:36:51,480
This was just saying, hey, you want to write code.

758
00:36:51,480 --> 00:36:55,480
You're doing ML and you care about developer productivity.

759
00:36:55,480 --> 00:36:59,480
Or you care about doing stuff around ML on code itself.

760
00:36:59,480 --> 00:37:04,480
Maybe you want to write that plan or have an idea of automated pull request review.

761
00:37:04,480 --> 00:37:08,480
Here's a code base for you that we've cleaned up.

762
00:37:08,480 --> 00:37:12,480
We've built some things ourselves wherever is baseline of what we think the model is.

763
00:37:12,480 --> 00:37:14,480
We've done something here is the baseline we got.

764
00:37:14,480 --> 00:37:17,480
Maybe you want to work on that and beat that baseline.

765
00:37:17,480 --> 00:37:18,480
Right.

766
00:37:18,480 --> 00:37:19,480
And then publish it.

767
00:37:19,480 --> 00:37:21,480
Like get the things down faster.

768
00:37:21,480 --> 00:37:24,480
So I'd be remiss if you just mentioned ML on code.

769
00:37:24,480 --> 00:37:25,480
I'm curious about.

770
00:37:25,480 --> 00:37:28,480
I'm sure others are curious about the generative side of this.

771
00:37:28,480 --> 00:37:31,480
Is that something that you're exploring in your group?

772
00:37:31,480 --> 00:37:33,480
So I am not exploring generative side of it.

773
00:37:33,480 --> 00:37:36,480
I think it's something that we've had ideas on.

774
00:37:36,480 --> 00:37:39,480
To me, that would be the ultimate moonshot.

775
00:37:39,480 --> 00:37:40,480
Yeah.

776
00:37:40,480 --> 00:37:43,480
And part of that code challenge was the GPT2 for things like that.

777
00:37:43,480 --> 00:37:44,480
Yeah.

778
00:37:44,480 --> 00:37:45,480
Exactly.

779
00:37:45,480 --> 00:37:46,480
That is the ultimate moonshot.

780
00:37:46,480 --> 00:37:48,480
So people have said I do work around that.

781
00:37:48,480 --> 00:37:51,480
The thing has to be good enough to be able to compile.

782
00:37:51,480 --> 00:37:54,480
So there's a version of this.

783
00:37:54,480 --> 00:37:56,480
I mean, but you've got that's like an adversarial thing.

784
00:37:56,480 --> 00:37:57,480
Yeah.

785
00:37:57,480 --> 00:37:58,480
But a version of this could just be.

786
00:37:58,480 --> 00:37:59,480
Could completion.

787
00:37:59,480 --> 00:38:00,480
Yeah.

788
00:38:00,480 --> 00:38:04,480
The challenge is so we are not necessarily an IDE.

789
00:38:04,480 --> 00:38:05,480
Right.

790
00:38:05,480 --> 00:38:09,480
From a get up perspective, if I put on a hat of like a get up product manager,

791
00:38:09,480 --> 00:38:11,480
we're not necessarily an IDE.

792
00:38:11,480 --> 00:38:12,480
Right.

793
00:38:12,480 --> 00:38:16,480
The person that would really, really rock and roll on that stuff is the person that is doing an IDE.

794
00:38:16,480 --> 00:38:18,480
Yeah.

795
00:38:18,480 --> 00:38:22,480
Because who is the, sometimes as technologists, we fall in love with the technology.

796
00:38:22,480 --> 00:38:25,480
And now we're trying to shoe horn it in.

797
00:38:25,480 --> 00:38:28,480
Who is the customer for that technology?

798
00:38:28,480 --> 00:38:30,480
Everybody ever sucks about.

799
00:38:30,480 --> 00:38:33,480
I would like to just say I want to do this and it writes the code for me.

800
00:38:33,480 --> 00:38:36,480
Even if that's where you want to get at, who's the customer for that?

801
00:38:36,480 --> 00:38:41,480
Is the person that has like an IDE or some environment where you write code?

802
00:38:41,480 --> 00:38:45,480
We don't necessarily, maybe in the future, we'll right now write code within GitHub.

803
00:38:45,480 --> 00:38:51,480
So from a generated perspective, I guess I can, I can see that from our managers.

804
00:38:51,480 --> 00:38:55,480
I'm not going to give us like the thumbs up to grab the chasing something that we can't,

805
00:38:55,480 --> 00:38:57,480
like that's not our job.

806
00:38:57,480 --> 00:39:00,480
We should be doing other things that are, we are not researchers.

807
00:39:00,480 --> 00:39:02,480
We are not a research.

808
00:39:02,480 --> 00:39:03,480
Right.

809
00:39:03,480 --> 00:39:09,480
Our shop is built something within a 12 month horizon to help our customers today.

810
00:39:09,480 --> 00:39:11,480
It's very, very quick.

811
00:39:11,480 --> 00:39:22,480
We have that those that you are in a position that not many organizations are in terms of the volume of code that you have access to.

812
00:39:22,480 --> 00:39:23,480
Indeed.

813
00:39:23,480 --> 00:39:26,480
Google also has code.

814
00:39:26,480 --> 00:39:31,480
But we don't have as much of all the different languages as GitHub does.

815
00:39:31,480 --> 00:39:32,480
Right.

816
00:39:32,480 --> 00:39:36,480
And structured metadata about the code.

817
00:39:36,480 --> 00:39:44,480
So there is, it does seem that there is a certain, part of this is, do you have the market?

818
00:39:44,480 --> 00:39:49,480
And maybe the IDE has the market for this feature.

819
00:39:49,480 --> 00:39:53,480
But they don't necessarily have the code, right?

820
00:39:53,480 --> 00:39:54,480
They couldn't.

821
00:39:54,480 --> 00:39:55,480
Yes.

822
00:39:55,480 --> 00:39:56,480
Right.

823
00:39:56,480 --> 00:39:58,480
We don't have the code.

824
00:39:58,480 --> 00:40:00,480
So where does GitHub fit?

825
00:40:00,480 --> 00:40:01,480
Right.

826
00:40:01,480 --> 00:40:03,480
I think this is the beginnings of code search net.

827
00:40:03,480 --> 00:40:08,480
If they don't have the code, could we provide some of the code for them?

828
00:40:08,480 --> 00:40:10,480
And so this is code search net is the.

829
00:40:10,480 --> 00:40:13,480
It's a code challenge.

830
00:40:13,480 --> 00:40:14,480
It's a code challenge.

831
00:40:14,480 --> 00:40:15,480
Okay.

832
00:40:15,480 --> 00:40:19,480
Replete with a data set filled with code itself.

833
00:40:19,480 --> 00:40:24,480
Past snippets of code and the sequences of code.

834
00:40:24,480 --> 00:40:27,480
And they're docks strings in natural language.

835
00:40:27,480 --> 00:40:32,480
Put out there to accelerate development on this issues.

836
00:40:32,480 --> 00:40:36,480
Understanding that we actually have customers today that we need to be building stuff for.

837
00:40:36,480 --> 00:40:37,480
Right.

838
00:40:37,480 --> 00:40:39,480
But we also want this long term horizon.

839
00:40:39,480 --> 00:40:40,480
So maybe we have this.

840
00:40:40,480 --> 00:40:44,480
Let's just see if anything interesting happens in this petri dish that we put out there.

841
00:40:44,480 --> 00:40:49,480
But I will make sure that when I get off this talk and I talk to my manager and our VP,

842
00:40:49,480 --> 00:40:54,480
I will tell them that our community might like us to start doing this.

843
00:40:54,480 --> 00:41:00,480
And so if they give us the thumbs up to chase that, then perhaps there's a GitHub research that is created.

844
00:41:00,480 --> 00:41:04,480
And get a research that is tossed with doing this kinds of work.

845
00:41:04,480 --> 00:41:05,480
Yeah.

846
00:41:05,480 --> 00:41:07,480
I mean, maybe this is the beginning of GitHub research.

847
00:41:07,480 --> 00:41:16,480
Yeah, maybe it does seem like it does seem like there's this asset available in open source code

848
00:41:16,480 --> 00:41:19,480
that you're uniquely positioned to take advantage of.

849
00:41:19,480 --> 00:41:24,480
And who knows what the, yeah, it's kind of classic innovators dilemma.

850
00:41:24,480 --> 00:41:26,480
Like this is our current business.

851
00:41:26,480 --> 00:41:28,480
So we kind of have the blinders on or focused on his current business.

852
00:41:28,480 --> 00:41:36,480
But you also are in this very unique position to kind of leapfrog that and do something potentially interesting.

853
00:41:36,480 --> 00:41:37,480
Yeah.

854
00:41:37,480 --> 00:41:41,480
And GitHub machine learning is all of almost three years old.

855
00:41:41,480 --> 00:41:42,480
Yeah.

856
00:41:42,480 --> 00:41:43,480
January 2017.

857
00:41:43,480 --> 00:41:44,480
Yeah.

858
00:41:44,480 --> 00:41:45,480
So that's also what you have to remember.

859
00:41:45,480 --> 00:41:46,480
Right.

860
00:41:46,480 --> 00:41:47,480
Right.

861
00:41:47,480 --> 00:41:48,480
So very different.

862
00:41:48,480 --> 00:41:49,480
I was just curious.

863
00:41:49,480 --> 00:41:50,480
Very different problem for sure.

864
00:41:50,480 --> 00:41:56,480
With the code challenge for those that don't have a team that does ASTs sitting right next to them.

865
00:41:56,480 --> 00:42:02,480
Are there kind of other kind of off the shelf tools that folks are using a play around with that kind of stuff?

866
00:42:02,480 --> 00:42:08,480
There are many parsers that are available that have their own version of ASTs that can parse ASTs for you.

867
00:42:08,480 --> 00:42:11,480
So I can mention any of them off the top of my cough, but they are available.

868
00:42:11,480 --> 00:42:12,480
But that's the thing.

869
00:42:12,480 --> 00:42:16,480
Like look for ASTs and parsers and kind of mashed up.

870
00:42:16,480 --> 00:42:17,480
But we don't know yet.

871
00:42:17,480 --> 00:42:20,480
We don't know is this is research.

872
00:42:20,480 --> 00:42:22,480
So get all the representations.

873
00:42:22,480 --> 00:42:27,480
If I were doing this, I'll get the code represented as an AST.

874
00:42:27,480 --> 00:42:29,480
I will represent the code as natural language.

875
00:42:29,480 --> 00:42:30,480
I'll just do everything.

876
00:42:30,480 --> 00:42:31,480
Yeah.

877
00:42:31,480 --> 00:42:34,480
And then do the science and see which one gets you further.

878
00:42:34,480 --> 00:42:35,480
Yeah.

879
00:42:35,480 --> 00:42:36,480
Yeah.

880
00:42:36,480 --> 00:42:38,480
And then, you know, put your.

881
00:42:38,480 --> 00:42:42,480
Join the leaderboard and submit your model and join the leaderboard.

882
00:42:42,480 --> 00:42:44,480
And I actually see how far you perform.

883
00:42:44,480 --> 00:42:47,480
Like a Netflix prize kind of thing where you've got a million bucks.

884
00:42:47,480 --> 00:42:49,480
I don't think there's a million bucks.

885
00:42:49,480 --> 00:42:51,480
I think there's just bracket rights of I did this.

886
00:42:51,480 --> 00:42:52,480
Okay.

887
00:42:52,480 --> 00:42:53,480
You're prizes.

888
00:42:53,480 --> 00:42:54,480
You're helping your community.

889
00:42:54,480 --> 00:42:55,480
Right.

890
00:42:55,480 --> 00:42:56,480
Awesome.

891
00:42:56,480 --> 00:42:57,480
Well, I'll mold you.

892
00:42:57,480 --> 00:43:01,480
It was so wonderful to finally get a chance to chat with you.

893
00:43:01,480 --> 00:43:03,480
Thanks so much for taking the time to be on the show.

894
00:43:03,480 --> 00:43:05,480
Thank you for having me.

895
00:43:05,480 --> 00:43:10,480
That's our show for today.

896
00:43:10,480 --> 00:43:15,480
To learn more about today's episode, visit twomalead.com slash shows.

897
00:43:15,480 --> 00:43:19,480
If you missed twomalcon or want to share what you learned with your team,

898
00:43:19,480 --> 00:43:24,480
be sure you visit twomalcon.com slash videos for more information about

899
00:43:24,480 --> 00:43:26,480
twomalcon video packages.

900
00:43:26,480 --> 00:43:52,480
Thanks so much for listening.

