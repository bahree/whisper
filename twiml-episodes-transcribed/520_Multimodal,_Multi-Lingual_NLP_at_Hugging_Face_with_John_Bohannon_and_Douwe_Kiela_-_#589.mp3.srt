1
00:00:00,000 --> 00:00:14,480
All right. Let's get this started. Hi, Dawa. Hi. So I'm talking to you from my home in San Francisco. Where are you?

2
00:00:14,480 --> 00:00:20,320
I'm in Palo Alto, not too far away from you actually. Also at your home. Also at my home, yes.

3
00:00:22,080 --> 00:00:28,000
The new office. The new office, yeah. So we do actually have an office hugging face Silicon Valley

4
00:00:28,000 --> 00:00:34,560
office in Palo Alto, not too far away from here, which we opened recently. But yeah, it's I'm still

5
00:00:34,560 --> 00:00:40,560
getting used to going to an actual office. I really like my home office. Yeah, it's kind of here

6
00:00:40,560 --> 00:00:49,200
to stay. So this is really exciting for me because for a number of reasons, one hugging face is one of

7
00:00:49,200 --> 00:00:55,760
the most interesting companies today. So especially in the machine learning space, but most especially in

8
00:00:55,760 --> 00:01:02,960
the natural language processing space, which is where I work. And yeah, I saw the tweet in January

9
00:01:02,960 --> 00:01:09,120
that you sent out announcing that you were the new head of research at hugging face. And I've

10
00:01:09,120 --> 00:01:14,240
been dying to talk to you ever since. And it's been a good six months. So you've had times to

11
00:01:14,240 --> 00:01:20,240
settle in, find your feet, get up to speed, actually maybe make an agenda and plan for yourself

12
00:01:20,240 --> 00:01:25,920
at hugging face. So it seems like a great time to catch up. And also a lot of the listeners of this

13
00:01:25,920 --> 00:01:35,440
podcast will have heard Tomas Wolf from Hugging Face, one of the founders. Is that right? How would

14
00:01:35,440 --> 00:01:42,880
you describe Tomas? He's one of the three co-founders. And he's our chief science officer.

15
00:01:42,880 --> 00:01:49,600
So many on this in listening to us right now will have heard Tomas interviewed by Sam three months

16
00:01:49,600 --> 00:01:57,120
ago. And so this is and he had a lot to say about research. And so it's a perfect time to dig deeper

17
00:01:57,120 --> 00:02:02,000
into some of the things that he got into. And also to just open up new territory, find out what's

18
00:02:02,000 --> 00:02:07,920
on your mind. How's that sound? Yeah, for sure. Yeah, I thought it was the podcast with Tom was

19
00:02:07,920 --> 00:02:12,640
really amazing. So if people haven't listened to that, I highly recommend people listen to that too.

20
00:02:12,640 --> 00:02:19,200
Yeah. You and I spoke briefly a week or two back. And I took some notes. And I want to give you the

21
00:02:19,200 --> 00:02:25,360
and the listeners kind of the menu of things that came to mind for me that we could touch on. So

22
00:02:26,000 --> 00:02:32,720
big themes I would love to know more about you as a human. And you and Hugging Face, I think a lot

23
00:02:32,720 --> 00:02:38,720
of people probably have a name recognition for Hugging Face, but probably don't know really what

24
00:02:38,720 --> 00:02:44,720
it is. So it'd be good to dig into that a little bit. And then the main dish of the course,

25
00:02:44,720 --> 00:02:51,600
let's dig into the future of NLP. Yeah, one thing I'd like to emphasize is that Hugging Face is

26
00:02:51,600 --> 00:02:59,040
no longer an NLP company per se. So we are doing a lot of very interesting work in computer vision

27
00:02:59,040 --> 00:03:05,040
and speech and other areas of AI. So I like to think of Hugging Face as an AI company.

28
00:03:05,040 --> 00:03:09,680
Yeah. And so that's a perfect seg. Let's dig into that. So Hugging Face used to be an NLP

29
00:03:09,680 --> 00:03:15,520
company. I think it's safe to say. And it's really been expanding. I looked on CrunchBase

30
00:03:15,520 --> 00:03:20,880
just to see what the basic stats are these days. It's like somewhere between a hundred and 200

31
00:03:20,880 --> 00:03:29,760
people, Series C, and based in New York officially, although quite remote now like the rest of us.

32
00:03:30,480 --> 00:03:38,720
Yeah. And so when you joined, it was already transitioning into something bigger than NLP.

33
00:03:38,720 --> 00:03:43,680
Yeah. What was your perception of Hugging Face? How would you have described it like before

34
00:03:43,680 --> 00:03:51,600
you joined and now that you've joined? Yeah. So I've always been impressed by Hugging Face and how

35
00:03:51,600 --> 00:03:56,160
it presents itself to the outside world. It's a very open and transparent organization

36
00:03:57,760 --> 00:04:04,000
where it really is about a community effort to democratize a lot of the tools that everybody

37
00:04:04,000 --> 00:04:10,800
uses. So from data sets to models, so Transformers Library, of course, also the Hub, which is really

38
00:04:10,800 --> 00:04:17,520
a crucial part of the AI ecosystem these days, I think. So I've just always been very impressed

39
00:04:17,520 --> 00:04:25,600
by it. And so that's why I chose to join this company. I think it really is a special

40
00:04:25,600 --> 00:04:30,480
special place and it really plays a special role in the community. So I don't think that a company

41
00:04:30,480 --> 00:04:36,400
like Google or Meta could play the same role that Hugging Face plays in this ecosystem.

42
00:04:37,200 --> 00:04:43,920
I agree. I agree. It's a pioneer with open source for sure. So something else that I really

43
00:04:43,920 --> 00:04:48,160
like about Hugging Face is how European it is and now actually very international. The people

44
00:04:48,160 --> 00:04:53,600
are just, they come from all over the place. Did you know any of the core Hugging Face people

45
00:04:53,600 --> 00:04:59,920
before you joined? Yeah. So I mean, I met Tom a few times before and I knew Victor and a bunch

46
00:04:59,920 --> 00:05:07,200
of others Victor San. So it's funny actually that you mentioned the Europeanness. So I'm a European

47
00:05:07,200 --> 00:05:12,560
as you can tell from my accent. I'm originally from Holland, but I live in California and I spent

48
00:05:12,560 --> 00:05:18,320
some time in the UK and in New York before I moved to California. But Tom, my boss, actually lives

49
00:05:18,320 --> 00:05:24,880
in Utrecht in the Netherlands, which is where I studied for my undergrad. So and Tom is not

50
00:05:24,880 --> 00:05:31,120
Dutch. But you didn't cross paths during your years in the Netherlands? No, no, no. I left

51
00:05:31,120 --> 00:05:36,000
Holland more than 10 years ago. So I don't think Tom's been living here for 10 years. So it's not

52
00:05:36,000 --> 00:05:41,120
a Dutch mafia. It's a coincidence. It's a French mafia if anything. So the founders are French.

53
00:05:44,320 --> 00:05:50,080
So in terms of the company at large, what I find fascinating is that we have people I think in

54
00:05:50,080 --> 00:05:55,200
Morgan 25 countries all over the world. So in the science team, we have people on the west coast,

55
00:05:55,200 --> 00:06:02,160
on the east coast of the US and in Canada and lots of different places in Europe and in South

56
00:06:02,160 --> 00:06:11,840
Korea. And Turkey as well. I have a friend based in Istanbul. Yeah. Let's see. What is your job?

57
00:06:11,840 --> 00:06:26,240
Good question. I wish I knew. So broadly speaking, I'm just trying to help the team realize this

58
00:06:26,240 --> 00:06:32,640
very ambitious vision that the founders have for the company and for the science team inside the

59
00:06:32,640 --> 00:06:40,880
company. So yeah, it's not really a well-defined role. I think it also kind of depends on what stage

60
00:06:40,880 --> 00:06:47,280
we're in in a given research project, for example. So I'm kind of discovering that as I go along.

61
00:06:47,280 --> 00:06:51,840
So the official title is Head of Research. That's right. And so then comes the question,

62
00:06:51,840 --> 00:06:56,880
what is research at Hugging Face? How is it different from research at a university or research

63
00:06:56,880 --> 00:07:00,960
at a big company like Facebook slash meta, which is where you came from before this?

64
00:07:01,680 --> 00:07:06,080
Yeah. So we're trying to go for a bit of a different model. I think if you want to compare it to

65
00:07:06,080 --> 00:07:11,760
to a single place, then maybe something like DeepMind or OpenAI is closer to what we're trying to do

66
00:07:11,760 --> 00:07:19,120
than meta. So yeah, as you mentioned, I've been at Fair for five years and it was a wonderful time.

67
00:07:20,480 --> 00:07:26,720
But one of the things that was difficult at Fair was that it's very bottom up, which in theory

68
00:07:26,720 --> 00:07:32,640
sounds really nice, but it makes it very difficult to do very big ambitious projects. So if you really

69
00:07:32,640 --> 00:07:39,440
want to create step change research artifacts, which is what we're trying to do, then you need to

70
00:07:39,440 --> 00:07:44,000
pull together big groups of people and then make sure that they're all aligned in realizing this

71
00:07:44,000 --> 00:07:51,760
vision. And in a bottom up research organization, that's very difficult to do. So what we're trying

72
00:07:51,760 --> 00:07:57,120
to do is find the optimal place between the bottom up approach that Fair and Google Brain and

73
00:07:57,120 --> 00:08:03,440
places like that have and the top down approach, which are DeepMind and OpenAI have, where they have

74
00:08:03,440 --> 00:08:08,960
a benevolent dictator like Demis or Ilya, basically telling people what to do and what the

75
00:08:08,960 --> 00:08:13,040
vision is. And we're trying to occupy the middle ground a little bit there and really try to use

76
00:08:13,040 --> 00:08:19,280
the things that make us special. So that's the ability to move fast, the ability to work with

77
00:08:19,280 --> 00:08:26,240
the community, like we've been doing with projects like big science, and to really to exploit the

78
00:08:26,240 --> 00:08:33,120
things that make us unique. What's the difference between big science, which is a project involving

79
00:08:33,120 --> 00:08:38,080
lots of external people, as many as a thousand or signed up from what I heard from Thomas,

80
00:08:39,360 --> 00:08:44,480
probably more like hundreds that are active participants on a daily basis, but that's big.

81
00:08:45,280 --> 00:08:51,520
And then the research team at Hugging Face, describe your actual, what would you call the

82
00:08:51,520 --> 00:08:58,640
actual research team at Hugging Face? Is it like 10 people, 20? So I think last count was 30,

83
00:08:58,640 --> 00:09:04,960
35 people actually. Okay, big group. Science is one of the projects we have going on. So I can tell

84
00:09:04,960 --> 00:09:10,400
you a bit about the other projects we have going on. So one of the advantages of being at Hugging

85
00:09:10,400 --> 00:09:14,560
Face is that it's a super transparent and open company. So I can just tell you everything that

86
00:09:14,560 --> 00:09:24,880
we're doing without feeling bad about it. So no secret sauce revealed. So we have a project around

87
00:09:25,600 --> 00:09:32,000
multimodal models. So multimodality, I think everyone agrees is very important for the future

88
00:09:32,000 --> 00:09:37,120
of AI. And when you say multimodality, for those listening in, you're referring to more than

89
00:09:37,120 --> 00:09:43,280
just text, more than just images, all kinds of sensory, what we would think of as sensory modalities

90
00:09:43,280 --> 00:09:47,680
or information modalities for humans, you're trying to capture that for models, but all at once.

91
00:09:48,320 --> 00:09:53,840
Yeah, all at once. So I think if you look at more recent multimodal work, it's very often just

92
00:09:53,840 --> 00:09:59,760
text and images, but there are all kinds of different modalities that you all might want to

93
00:09:59,760 --> 00:10:03,760
integrate into one single model. So how many modalities are you stuffing in?

94
00:10:05,040 --> 00:10:12,640
So right now, it's images, text, videos, and audio, because those are the main ones. And then once

95
00:10:12,640 --> 00:10:18,080
you have those, then you can start thinking about other specific modalities, maybe sort of submodalities,

96
00:10:18,080 --> 00:10:23,520
right? So it's unclear whether code as a modality is a part of text or if it's something else.

97
00:10:24,320 --> 00:10:29,440
So there's all kinds of interesting questions about what the modality really is. So my PhD thesis

98
00:10:29,440 --> 00:10:34,960
actually was about grounding meaning in perceptual modalities, where I also incorporated all

99
00:10:34,960 --> 00:10:41,600
factory semantics. So you can build a bag of chemical compounds model and build smell vectors,

100
00:10:41,600 --> 00:10:48,080
essentially, and do interesting things with that. So that's a long time ago, but yeah, there's

101
00:10:48,080 --> 00:10:53,120
a lot of potential there. What does the word grounded mean in this context? So let's use NLP.

102
00:10:53,120 --> 00:10:58,480
Let's use an example like you have a model that, you know, like GPT-3. So it's learned how to

103
00:10:58,480 --> 00:11:04,480
generate text. What does it mean for that model to be grounded? Yeah, so I was going to say,

104
00:11:04,480 --> 00:11:10,240
I think the word grounded isn't pretty well defined, but I'm a philosopher by training originally,

105
00:11:10,240 --> 00:11:15,200
so I would argue that most things are not well defined. But in my thesis, I make an explicit

106
00:11:15,200 --> 00:11:20,480
distinction between referential grounding and representational grounding. And so I think

107
00:11:20,480 --> 00:11:26,480
referential grounding is what people often think about with like referral data sets. So those

108
00:11:26,480 --> 00:11:31,440
exist in computer vision, for example, where you have to pick out the object. So when someone says

109
00:11:31,440 --> 00:11:36,320
banana, then you have to be able to point into image where the banana is. But I think the

110
00:11:36,320 --> 00:11:41,520
much more interesting type of grounding is representational grounding where you have a holistic

111
00:11:41,520 --> 00:11:46,560
meaning representation of a concept like elephant and you or violin, maybe it's a better example.

112
00:11:46,560 --> 00:11:52,160
And so you know the semantic meaning of violin, you can go to Wikipedia and look up what violin is,

113
00:11:52,160 --> 00:11:56,400
what that means. But you also have a visual representation of it and you know what it looks like,

114
00:11:56,400 --> 00:12:00,400
you know, what it sounds like, maybe you know what it smells like, what it feels like, what it's

115
00:12:00,400 --> 00:12:06,880
like to play it, all of these different modalities are a part of your overarching meaning representation

116
00:12:06,880 --> 00:12:12,240
of the concept of violin. And I think that is the much more interesting type of meaning representation.

117
00:12:12,240 --> 00:12:16,960
And so that's the meaning we should try to get into machines if we want them to be able to

118
00:12:16,960 --> 00:12:21,680
really understand humans. So what are the, what are some of the problems you see with today's

119
00:12:22,240 --> 00:12:29,520
models that reveal that they're insufficiently grounded? Yeah, so I don't know if we're sure

120
00:12:29,520 --> 00:12:33,840
that models are insufficiently grounded. I think that's still an empirical question,

121
00:12:33,840 --> 00:12:37,920
but my hunch and I think a lot of people in the field share that hunch is that you need to have

122
00:12:37,920 --> 00:12:43,040
some understanding of the world as humans perceive it if you really want to understand humans.

123
00:12:43,920 --> 00:12:51,040
And so there's a lot of communication that happens between humans that never really

124
00:12:51,760 --> 00:12:56,400
becomes explicit. So people call this common sense, for example. So the example I always use is

125
00:12:56,400 --> 00:13:01,520
coffee and what coffee smells like. Everybody knows what coffee smells like. So I never have to

126
00:13:01,520 --> 00:13:08,000
explain that to anyone. And so for that reason, I also just have no idea how to describe the smell of

127
00:13:08,000 --> 00:13:12,800
coffee. I don't know if you can try that or describe the smell of a banana in one sentence.

128
00:13:12,800 --> 00:13:17,040
Like you've never had to do that because you know that everybody knows what banana smell like.

129
00:13:17,040 --> 00:13:22,400
And if you could pull it off, we would call you a poet. Yeah, exactly. So I think you're

130
00:13:22,400 --> 00:13:27,520
totally right. So you have to fall back to associations then because there is no descriptive language

131
00:13:27,520 --> 00:13:33,360
for this sort of stuff. And I think this happens all over the place in natural language communication

132
00:13:33,360 --> 00:13:39,840
between humans. And that makes it very hard for machines to learn this stuff just from reading Wikipedia

133
00:13:39,840 --> 00:13:44,080
or whatever corpus they're trained on. It's funny. You're very much coming at this as a philosopher,

134
00:13:44,080 --> 00:13:49,520
I could see. There's another angle, which is where I'm coming from. So you know, I'm at a company

135
00:13:49,520 --> 00:13:55,840
that is on the applied side. So we're using NLP to try and solve problems. And where I see what

136
00:13:55,840 --> 00:14:00,560
seems to be the grounded problem is the model clearly, if you just poke a little bit, it clearly

137
00:14:00,560 --> 00:14:06,640
doesn't understand what it's talking about. You know, it'll say all the right things. And then

138
00:14:06,640 --> 00:14:12,960
it reveals that it actually has no common sense understanding of what coffee is. Because it'll say

139
00:14:12,960 --> 00:14:18,480
something that's a human would find crazy. Yeah, but so I think the word understanding,

140
00:14:18,480 --> 00:14:23,200
what does understanding even mean there? I mean, so I think what you're maybe talking about.

141
00:14:23,200 --> 00:14:27,360
And that's so I think there are two main things missing in our current paradigm. One is

142
00:14:28,240 --> 00:14:34,560
multimodal understanding of concepts. And the other is the intentionality with a T of language.

143
00:14:34,560 --> 00:14:40,400
So the fact that we use language with an intent to change the mental state of whoever we're

144
00:14:40,400 --> 00:14:47,360
talking to, right? So I'm using my voice now to change your brain essentially. And so that intent

145
00:14:47,360 --> 00:14:52,080
is is crucial for real meaning and real understanding. And it's something that doesn't exist

146
00:14:52,080 --> 00:14:59,920
in language models. Do you reckon that we have to give real agency to systems to achieve that?

147
00:14:59,920 --> 00:15:04,560
To like have them care about something? And maybe with reinforcement learning or other paradigms?

148
00:15:05,200 --> 00:15:11,840
I think so. Yeah. So I don't know if agency, I mean, I don't want to keep like going on the

149
00:15:11,840 --> 00:15:18,800
definitions, but so agency is also a bit unclear, I think. So it's more, yeah, you can model the

150
00:15:18,800 --> 00:15:25,280
intent of communication when you're trying to model human communication. You can try to model

151
00:15:25,280 --> 00:15:30,560
the intent as a part of the interaction. So you could think of so the two things I just talked

152
00:15:30,560 --> 00:15:36,080
about you could integrate them in language models pretty easily, right? So you could have a language

153
00:15:36,080 --> 00:15:41,280
model that also has a multimodal input. Maybe you can put it in an embodied environment where it

154
00:15:41,280 --> 00:15:48,160
can walk around. And then maybe you can have multiple of these language models walking around

155
00:15:48,160 --> 00:15:52,880
in that world and interacting with each other and other humans. So if you put all of that together,

156
00:15:52,880 --> 00:15:58,240
then I think you get something very close to how humans learn language. Is this where you think

157
00:15:58,240 --> 00:16:03,040
Hugging Face is headed? Is this one of the grand directions? This is definitely one of the grand

158
00:16:03,040 --> 00:16:07,200
directions. Yeah. So one of our projects is multimodal. As I said, another one is about

159
00:16:07,200 --> 00:16:12,800
embodied learning. Thomas also talked about this when he spoke on this podcast. Yeah, the way he

160
00:16:12,800 --> 00:16:18,320
described it was, maybe we need to teach models language more like we teach humans language,

161
00:16:18,320 --> 00:16:24,000
which is in the world trying to get things done. Exactly. Yeah. So and that's because we want

162
00:16:24,000 --> 00:16:29,840
the models to use the kind of language that's useful for interacting with humans. So people sort

163
00:16:29,840 --> 00:16:36,160
of gloss over it, but the reason we want to have natural language understanding and natural

164
00:16:36,160 --> 00:16:40,560
language generation capabilities in these models because we want them to interact with humans.

165
00:16:41,440 --> 00:16:46,800
And so I mean, one of the other things I've been pushing a lot for is a more holistic evaluation

166
00:16:46,800 --> 00:16:51,920
of these models where rather than just evaluating them on static test sets, we actually expose them

167
00:16:51,920 --> 00:16:56,160
to real humans and we see how well they do in that setting. And as you as you mentioned,

168
00:16:57,360 --> 00:17:00,800
those models very quickly break down if you try to actually do that.

169
00:17:00,800 --> 00:17:06,640
All right. So a different question. I was really curious. So I consider you a very multilingual

170
00:17:06,640 --> 00:17:11,680
person. I mean, all Dutch people are. If you've ever met a Dutch person, you've met multilingual

171
00:17:11,680 --> 00:17:19,120
people. And here you are in NLP and adjacent. You know, you're you're definitely expanding

172
00:17:19,120 --> 00:17:25,520
beyond that. But you would consider yourself an NLP practitioner. Yeah. I think so. Yeah, kind of.

173
00:17:25,520 --> 00:17:31,440
I mean, I've been branching out for a long time. So I would consider myself an AI person like

174
00:17:31,440 --> 00:17:36,160
so a lot of my work is multimodal, but this is language first. Yeah, language is my my main

175
00:17:37,520 --> 00:17:47,280
interest. How frustrating or bizarre has it felt to be a deeply multi-lingual person in like

176
00:17:47,280 --> 00:17:53,520
a time and science where it's just so English dominated, the research itself, the tools down to

177
00:17:53,520 --> 00:17:57,040
the very data that we're training these things on. And I'm asking this as an obvious

178
00:17:57,040 --> 00:18:02,640
seg to this really exciting, you know, project that's underway to perhaps create the first truly

179
00:18:02,640 --> 00:18:07,280
multilingual based language model as that's my understanding of the project. But I first wanted

180
00:18:07,280 --> 00:18:13,760
to hear just like you, Dawa, like as a deeply multilingual person, you know, like what does it feel

181
00:18:13,760 --> 00:18:19,280
like? What has it felt like to be in this weirdly accidentally English dominated space?

182
00:18:19,280 --> 00:18:24,000
Yeah, so that's a very interesting question, but I don't know if I'm the right person to ask it

183
00:18:24,000 --> 00:18:32,480
because I moved to the UK for my PhD and then I moved to the US and so most Dutch people speak

184
00:18:32,480 --> 00:18:40,640
pretty decent English, I think. So I think where the accessibility of language models and the

185
00:18:40,640 --> 00:18:45,200
multilinguality of language models where that really matters is for people who are

186
00:18:45,200 --> 00:18:52,960
monolingual and who don't speak English. So people who can't easily access this technology

187
00:18:52,960 --> 00:18:57,120
because it's limited only to English. But I think that doesn't really apply to

188
00:18:57,120 --> 00:19:01,840
most Dutch people because they go very easily switch over as you mentioned.

189
00:19:01,840 --> 00:19:05,920
But also like using these things to make sense of the world that's not written in English.

190
00:19:05,920 --> 00:19:10,400
Like I could tell you how hard it is because that's my day to day is like dealing with Chinese,

191
00:19:10,400 --> 00:19:17,280
Russian or other languages like the tools and the data is far, far weaker.

192
00:19:17,840 --> 00:19:23,680
Oh yeah, yeah for sure and I think there's also very interesting underlying questions there about

193
00:19:25,120 --> 00:19:32,880
the cultural differences that manifest themselves in languages. So English as a language is very

194
00:19:32,880 --> 00:19:39,040
explicit so you can be relatively low context in how you communicate. So you're just very explicit

195
00:19:39,040 --> 00:19:44,000
or you know some people would consider Americans relatively blunt. I think in how they communicate

196
00:19:44,000 --> 00:19:49,680
same for Dutch people anyway. But if you think about like Japanese language which is very sort of

197
00:19:49,680 --> 00:19:57,360
indirect and very different in a sense from English I think that also manifests itself in the culture.

198
00:19:57,360 --> 00:20:02,080
So maybe there are just things that you can really capture about Japanese culture because you have

199
00:20:02,080 --> 00:20:08,160
a specific type of language model. So tell us a bit about the ongoing experiment to make a truly

200
00:20:08,160 --> 00:20:13,520
multilingual model. Yeah so this is the big science model. It has a name now it's called Bloom

201
00:20:14,240 --> 00:20:20,320
which I think is a really nice name because the logo of big science has also always been a flower.

202
00:20:20,880 --> 00:20:27,440
So the flower is starting to bloom and so this language model it's as you said the first

203
00:20:27,440 --> 00:20:35,840
big multilingual language model and it is only a few weeks away from being done training so

204
00:20:35,840 --> 00:20:42,240
it's been very cool you can just follow it on Twitter. There's a regular Twitter update whereas

205
00:20:42,240 --> 00:20:48,720
like we're at like 87% or something now and so have you been playing with checkpoints?

206
00:20:49,520 --> 00:20:54,880
Yeah so there's something called the Bloom book where people have been able to just submit

207
00:20:54,880 --> 00:21:01,120
problems and then someone would run them and store their output somewhere for people to inspect

208
00:21:01,120 --> 00:21:05,680
and so we're releasing some checkpoints soon as well for people to talk to and then when the final

209
00:21:05,680 --> 00:21:10,560
model comes out it's also going to be released so that people can play with it themselves.

210
00:21:10,560 --> 00:21:16,880
Cool. Is it a basic text-to-text autogressive model? Same architecture as your typical big text-to-text

211
00:21:16,880 --> 00:21:24,560
models? Yeah basically yeah so it's I think by design that there hasn't been too much divergence

212
00:21:24,560 --> 00:21:30,400
from the sort of standard language model that people are used to but there are some nifty new things

213
00:21:30,400 --> 00:21:37,840
in there so it uses like a LMI for like how to do the token embeddings and things like that so there

214
00:21:37,840 --> 00:21:42,560
are a couple of nice different things in there but yeah the main architecture is exactly what you

215
00:21:42,560 --> 00:21:48,160
would expect. Let's dig into that. A lot of people on this call won't really even know what a token

216
00:21:48,160 --> 00:21:53,440
or a tokenizer is. I think this is a really neat part of NLP. It's just very much like the tools

217
00:21:53,440 --> 00:21:59,280
you use kind of talk but let's just like take a moment. Tell us what is a token, what is a tokenizer

218
00:21:59,280 --> 00:22:05,680
and then like how did you do it differently with this this big bloom model and why did you have to?

219
00:22:06,400 --> 00:22:12,400
Yeah so I'm not I'm not the the right person to really answer detailed questions about the tokenization

220
00:22:12,400 --> 00:22:19,440
of the language model but that so I can explain what what tokenization is so it's basically just how

221
00:22:19,440 --> 00:22:25,680
do you cut up your your text so you know a sentence consists of words so you could just cut it up

222
00:22:25,680 --> 00:22:34,960
in the white space and and just every word is a token but that is inefficient so what people have

223
00:22:34,960 --> 00:22:40,160
been doing is trying to chunk it up in smarter ways because then you'd have like a vocabulary of

224
00:22:40,160 --> 00:22:46,400
millions right and with multiple languages it could be huge. Yeah so especially if it's multilingual

225
00:22:46,400 --> 00:22:51,120
maybe you just don't see words often enough to really have a very good understanding of their

226
00:22:51,120 --> 00:22:56,960
meeting so a good representation of their meeting and so what you can do is you can chunk

227
00:22:57,760 --> 00:23:03,760
different segments of words together in smart ways so so this is BPE by parent coding and things

228
00:23:03,760 --> 00:23:11,680
like that and so there has been a working group in the big science workshop so it's like a one-year

229
00:23:11,680 --> 00:23:17,200
workshop is how we're thinking about it and so I think there are 40 50 different working groups

230
00:23:17,200 --> 00:23:22,080
and there was one working group working on tokenization they wrote a very nice survey paper

231
00:23:22,080 --> 00:23:27,680
they did a big analysis of what the right tokenization is and one of the things that they found

232
00:23:27,680 --> 00:23:33,040
I think also together with under like the main model working group is that these alibi

233
00:23:33,840 --> 00:23:40,240
positional embeddings that really help so this was just an empirical finding and and so so

234
00:23:40,240 --> 00:23:45,920
you know there's just a lot of this small research that went into this this whole endeavor.

235
00:23:45,920 --> 00:23:50,800
So why not just go all the way down to the individual character? Why mess with tokens at all?

236
00:23:50,800 --> 00:23:55,840
Yeah it's a good question I mean there are admin efforts in this direction or like

237
00:23:56,800 --> 00:24:02,800
so back in the days there were like character RNNs before Transformers and people were trying

238
00:24:02,800 --> 00:24:07,600
to get this to work it sort of worked but it didn't really really work. It was a great way to

239
00:24:07,600 --> 00:24:14,320
generate made-up silly words. Yeah yeah for sure yeah and so yeah I think there's also an

240
00:24:14,320 --> 00:24:19,840
interesting possibility there where we reduce everything to the byte level and so when you think

241
00:24:19,840 --> 00:24:26,800
about like Unicode or or UTF-8 or things like that like in theory every single character can just

242
00:24:26,800 --> 00:24:31,440
be be modeled at the byte level and then maybe that's the future and then maybe you could even like

243
00:24:32,640 --> 00:24:37,920
put images and audio and everything is just bytes and so basically you can just have a

244
00:24:37,920 --> 00:24:44,240
pre-trained byte level model so I think that's an interesting research direction and there's

245
00:24:44,240 --> 00:24:49,600
been some work on that but it so far it hasn't really proven to be better than just smart ways

246
00:24:49,600 --> 00:24:55,520
of tokenizing your data. So maybe the real explanation for it not working yet is that we haven't

247
00:24:56,080 --> 00:25:02,480
used enough data yet so maybe we just need even more data as always Ben. Thomas mentioned 800

248
00:25:02,480 --> 00:25:07,520
gigabytes. What does that actually translate to in terms of like how much of the internet did you

249
00:25:07,520 --> 00:25:15,200
grab for this? I understand you crowdsourced it. Yeah so it was crowdsourced with a big community

250
00:25:15,200 --> 00:25:22,800
of collaborators who were part of this big science effort and so it's not really a crawl so it's

251
00:25:22,800 --> 00:25:27,680
very hard to say like what percentage of the internet is this it really depends on the language

252
00:25:27,680 --> 00:25:33,920
and the folks who contributed the data for their own language. I think some of them also had

253
00:25:33,920 --> 00:25:41,760
different approaches so it's a very kind of targeted way of collecting data and that's one of the

254
00:25:41,760 --> 00:25:46,240
beauties of this big science effort. So I think there's a lot of emphasis on this bloom model

255
00:25:46,960 --> 00:25:52,720
but what's also very interesting about the overarching endeavor is that we have this data set which

256
00:25:52,720 --> 00:25:59,920
is really beautiful and curated by experts in those languages. It has a very interesting coverage

257
00:25:59,920 --> 00:26:07,040
of different languages geographically over the whole world. I don't know what the latest numbers

258
00:26:07,040 --> 00:26:13,840
know in the 40s or 50s I think 45. Wow so this is a huge collection of languages and it includes

259
00:26:13,840 --> 00:26:18,960
like low resource African languages and things like that so I think that's really great and so

260
00:26:18,960 --> 00:26:23,760
there's the data effort but then like the the legal side of this like how do you distribute the

261
00:26:23,760 --> 00:26:29,600
model the governance side of the data itself. All of these the super intriguing questions have just

262
00:26:29,600 --> 00:26:35,280
been explored by the community completely in the open so it's just fascinating for me I'm sort

263
00:26:35,280 --> 00:26:42,160
of an outsider right so just yeah so I mean me too in a way like this started about a year ago

264
00:26:42,160 --> 00:26:48,720
I think or more than that and so I've just been following it from the sidelines and I'm still

265
00:26:48,720 --> 00:26:53,040
kind of like not directly involved in it that much and it's just amazing to see.

266
00:26:53,040 --> 00:27:00,720
So okay now back to you so here you are six months into your new role at Hugging Face.

267
00:27:02,000 --> 00:27:09,040
Give us a sense of like what you thought your job would be when you started and now six months

268
00:27:09,040 --> 00:27:14,640
later like what has changed what's the newest thing that you've learned about yourself and Hugging

269
00:27:14,640 --> 00:27:19,040
Face and the mission you know like what gets you out of bed in the morning that's changed.

270
00:27:19,040 --> 00:27:29,840
Yeah no that's a very interesting question I mean I think the job has been what I expected sort of

271
00:27:29,840 --> 00:27:35,280
so I knew going into this that it's just an amazing team and like we really have some some

272
00:27:35,280 --> 00:27:41,520
brilliant researchers in this team so I was very excited about getting to work with those folks

273
00:27:41,520 --> 00:27:49,040
and so that's been really awesome. I think one thing that I maybe didn't really expect is

274
00:27:50,400 --> 00:27:55,520
when you're a company like Hugging Face and you're this distributed all across the globe

275
00:27:55,520 --> 00:28:00,320
you have to be very decentralized so a lot of the communication happens asynchronously

276
00:28:00,320 --> 00:28:07,360
on Slack in public channels which I think is great and so Hugging Face really has a unique culture

277
00:28:07,360 --> 00:28:13,280
that supports this way of working together but if you come from a different working culture like

278
00:28:13,280 --> 00:28:20,800
me coming from Mehta that is quite the transition to make and so especially you can't just go to

279
00:28:20,800 --> 00:28:26,400
a whiteboard with people. Yeah so everything is remote but it's not even just remote where you're

280
00:28:26,400 --> 00:28:33,680
both like talking to your computer or resume it's like it's remote also in time so one of the things

281
00:28:33,680 --> 00:28:39,840
I'm struggling with is just time zone I think so I'm in California right so I'm sort of trailing

282
00:28:39,840 --> 00:28:48,320
the world and and so when when I wake up or when my son wakes me up at around 7 a.m. then I check

283
00:28:48,320 --> 00:28:53,200
my phone and I have like a million Slack messages and emails and things to read through and then

284
00:28:53,200 --> 00:28:58,800
usually my meetings start at 8 a.m. because I need to make sure I can talk to the Europeans

285
00:28:58,800 --> 00:29:04,640
and then they stop working soon after that and so I'm always kind of like trailing in time

286
00:29:05,360 --> 00:29:10,320
which is which is not easy. So you didn't see that coming? I was not prepared for that yeah I'm

287
00:29:10,320 --> 00:29:15,120
I'm still still adjusting but I mean it's an interesting learning experience and it's just

288
00:29:15,120 --> 00:29:20,400
fascinating I think to see like where the world is going with remote work and so this is the

289
00:29:20,400 --> 00:29:26,000
future way I think in which a lot of companies are going to be doing this. So what's it like

290
00:29:26,000 --> 00:29:30,720
running and building and nurturing a research team at a startup? I think that's something that people

291
00:29:30,720 --> 00:29:35,360
will be really curious about. I think a lot of a lot of people will be familiar directly or

292
00:29:35,360 --> 00:29:41,040
indirectly with how a research group even 30 strong like you said at a university works you know

293
00:29:41,040 --> 00:29:46,320
you've got a PI and that PI's job is mostly to get grant money and then you've got the postdocs

294
00:29:46,320 --> 00:29:51,040
who actually run the show and then you've got like grad students who are ranging from miserable to

295
00:29:51,040 --> 00:29:56,400
pretty happy and then you've got like interns and undergrads. Does it have anything like that

296
00:29:56,400 --> 00:30:01,840
structure? Is it just a totally different beast? Yeah and it's very different so I am definitely not

297
00:30:01,840 --> 00:30:11,520
a PI so I'm more a facilitator I think or a coordinator and so we have a very flat non-hierarchical

298
00:30:11,520 --> 00:30:18,080
organization. We do have team leads so those those would be closer to PIs I think so we have a

299
00:30:18,080 --> 00:30:23,600
multimodal project and it has very clear team leads and you know things like that. So my role

300
00:30:24,800 --> 00:30:30,960
is it's more like a sort of serving leader where I just try to to help people the best way I

301
00:30:30,960 --> 00:30:35,680
possibly can and to make sure they don't have roadblocks and and that people are talking to

302
00:30:35,680 --> 00:30:40,480
each other and that I'm aware of what's going on and I try to connect people to the right people

303
00:30:40,480 --> 00:30:45,280
and connect ideas to the right ideas. So it sounds like pretty normal management actually.

304
00:30:45,280 --> 00:30:51,600
Yeah but yeah I guess you could say that but it's very different from normal management at the

305
00:30:51,600 --> 00:30:56,080
same time I think because of how decentralized the company is and because of all of the other

306
00:30:56,080 --> 00:31:00,320
things that are just very different from from a traditional management role it like a big tech

307
00:31:00,320 --> 00:31:04,960
company. Well and also the fact that there's like a thousand strong group of people outside the

308
00:31:04,960 --> 00:31:08,880
company that you actually have to work with and coordinate with. Yeah but that's just a big

309
00:31:08,880 --> 00:31:14,800
science project right so I think I mean you make an interesting point that one of the things that

310
00:31:14,800 --> 00:31:21,200
makes hugging phase so special is that the community plays such a big role in the company and that's

311
00:31:21,200 --> 00:31:27,120
not just big science right so like if you look at Transformers the library and the open source ecosystem

312
00:31:27,120 --> 00:31:32,320
and data sets and things like that that's a huge community and all of these people are also

313
00:31:32,320 --> 00:31:38,480
contributing actively to making these tools so awesome. Yeah no I remember the day we first

314
00:31:38,480 --> 00:31:45,680
started using your Transformers library at my company primer it was a revelation you just like I

315
00:31:45,680 --> 00:31:54,320
can't under it I can't say enough about how positive the open sourcing of Transformer language

316
00:31:54,320 --> 00:32:01,520
models was and I think hugging phase deserves most of the credit just like yeah. I think one of the

317
00:32:01,520 --> 00:32:07,600
the the reasons that Burke became so popular so quickly was because of the Transformers Library

318
00:32:07,600 --> 00:32:13,520
or the predecessor right so Pythorch pre-trained bird I remember I was at a workshop at this

319
00:32:13,520 --> 00:32:17,600
Santa Fe Institute they do these workshops where they invite a bunch of people and they talk

320
00:32:17,600 --> 00:32:24,160
about some stuff and Fernando Pereira was there the the Google director of research I think

321
00:32:24,960 --> 00:32:30,400
and he was saying like we have this thing coming out and it's going to like blow everything out

322
00:32:30,400 --> 00:32:34,560
of the water it's amazing it's going to revolutionize NLP and like I've heard people say that before

323
00:32:34,560 --> 00:32:42,000
and I never really believed it but in this case he was right so so birds yeah so it dropped like

324
00:32:42,000 --> 00:32:46,320
I think two weeks later or something and then so everyone wanted to play with it and being in

325
00:32:46,320 --> 00:32:51,680
fair obviously Pythorch was the preferred framework and and it took like I don't know like a week or

326
00:32:51,680 --> 00:32:56,960
two before there was this Pythorch pre-trained bird model that everyone was playing with so it's

327
00:32:56,960 --> 00:33:02,720
amazing and so I did some snooping your most cited paper at least according to Google scholar

328
00:33:02,720 --> 00:33:09,200
is this 2017 paper on sentence representations why I think that's so so notable is that that's

329
00:33:09,200 --> 00:33:18,080
like just on the before side of Bert so you know Bert comes out in October 2018 something like that

330
00:33:18,080 --> 00:33:23,600
and so like well a full year before that you were deep in NLP solving hard NLP problems

331
00:33:24,400 --> 00:33:30,160
do you remember how crazy it was when suddenly like on the other side of that line when we had

332
00:33:30,160 --> 00:33:35,840
language models all the things in NLP that were really hard and tedious and you needed so much

333
00:33:35,840 --> 00:33:43,360
data to even barely get some performance suddenly became kind of routine and fun and easy like

334
00:33:43,360 --> 00:33:48,800
I'm not hiding I'm not hiding the reality that like tons of stuff doesn't work and tons of stuff

335
00:33:48,800 --> 00:33:56,240
is still hard but the the things that are hard are new things largely yeah so I agree with with

336
00:33:56,240 --> 00:34:01,600
that but so so to me as a researcher it didn't feel like a very abrupt transition actually so

337
00:34:02,320 --> 00:34:07,040
I think that was much more the case for NLP practitioners that more applied people trying to

338
00:34:07,040 --> 00:34:13,760
use the tools yeah but so for me as a researcher I think like the transition was was actually very

339
00:34:13,760 --> 00:34:19,360
natural and so we were doing things with LSTMs and then okay transformers so LSTMs didn't really

340
00:34:19,360 --> 00:34:24,560
work so you needed attention and so there were so even in infrecent we were also experimenting

341
00:34:24,560 --> 00:34:29,040
with self-attention and things like that and then what the transformers paper did is it basically

342
00:34:29,040 --> 00:34:34,720
removed the recurrent so rather than having an LSTM we did it forward just a normal NLP feed

343
00:34:34,720 --> 00:34:40,720
forward network and so it turned out that attention on its own is actually okay right so

344
00:34:42,320 --> 00:34:48,160
from that it became natural to try to do this on just language modeling test so that's GPT

345
00:34:48,160 --> 00:34:52,000
and then if you can do language modeling why not do it bi-directionally because we were playing

346
00:34:52,000 --> 00:34:57,600
with bi-directional LSTMs all the time infrecent is a bi-directional LSTM so birth is just a bi-directional

347
00:34:57,600 --> 00:35:06,720
GPT so it all was very natural I think when it came up so it felt it felt naturally from the point

348
00:35:06,720 --> 00:35:11,440
of view of like no understanding the science but I can tell you from the point of view of people

349
00:35:11,440 --> 00:35:17,520
trying to solve pay solve problems that people will pay you money for no yeah for sure it changed

350
00:35:17,520 --> 00:35:24,960
everything what there is an aspect though scientifically that is new right I was delighted when

351
00:35:24,960 --> 00:35:30,320
this little cottage industry of Bertology suddenly kind of sprouted out of nowhere so here's the

352
00:35:30,320 --> 00:35:36,960
thing you know it struck me that deep learning used to be very much like a branch of mathematics

353
00:35:36,960 --> 00:35:43,040
right because it was part of statistics you know so like all of ML was just math and it felt like

354
00:35:43,040 --> 00:35:48,560
the math world and then suddenly here we are today with models that are so complicated they're more

355
00:35:48,560 --> 00:35:53,920
like biology artifacts we're like kind of prodding them and probing them and trying to understand

356
00:35:53,920 --> 00:36:01,040
things like how how the heck does Bert you know does it understand grammar to what extent does it

357
00:36:01,040 --> 00:36:06,160
do it differently than us suddenly it's feeling more like an empirical science and less like a

358
00:36:06,160 --> 00:36:13,520
branch of math yeah I'm not I'm not sure I'm happy with that actually I also think that that is this

359
00:36:13,520 --> 00:36:17,680
so yeah I have a couple of things to say about that actually so I think this cottage industry of

360
00:36:17,680 --> 00:36:26,000
Bertology is interesting because a few years before that we had a cottage industry in Wortevek

361
00:36:26,000 --> 00:36:30,800
right so Wortevek kind of blew everyone away and then there were a couple of ACLs in the

362
00:36:30,800 --> 00:36:37,120
EMNOPs where just everything was something to veck and it was all just trying to analyze what

363
00:36:37,120 --> 00:36:41,600
Wortevek really did and so I think that's just kind of the progression of science where you have

364
00:36:41,600 --> 00:36:46,720
a big breakthrough model and then there's some consolidation right in the sort of Thomas Kuhn

365
00:36:47,440 --> 00:36:52,240
paradigm shift so there's a real paradigm shifting artifact like Wortevek or Bert and then

366
00:36:52,240 --> 00:36:57,120
there's a lot of consolidation where people try to understand this better so I think that's just

367
00:36:57,120 --> 00:37:02,480
the natural progression and that's just going to continue happening but about Bert specifically

368
00:37:03,600 --> 00:37:09,840
so so we just don't have the correct mathematical tools I think to really understand what it's

369
00:37:09,840 --> 00:37:14,320
learning and so there are some efforts now from like Chris Ola and trying to understand better

370
00:37:14,320 --> 00:37:19,760
what transformers are really learning but so we have a very interesting paper called

371
00:37:19,760 --> 00:37:26,400
Mass Language Modeling and a distributional hypothesis order work does not matter much or something

372
00:37:26,400 --> 00:37:32,240
like that so what we basically show is that you shuffle if you shuffle a corpus and so all of the

373
00:37:32,240 --> 00:37:37,760
senses are not in the right order anymore and you train a Bert model on it it just does just

374
00:37:37,760 --> 00:37:43,600
as well as a regular Bert so you mentioned like does Bert learn grammar which seem like weirdly

375
00:37:43,600 --> 00:37:49,120
seems to suggest that it doesn't matter like that it's clearly doing something differently than

376
00:37:49,120 --> 00:37:54,480
humans do because if you imagine trying to learn language with shuffled language it'd be a nightmare

377
00:37:54,480 --> 00:38:04,480
exactly yeah so so I think yeah maybe we're also over yeah I don't know like oh thinking that

378
00:38:04,480 --> 00:38:08,160
Bert is better than it really is or these sorts of models that they're actually better than

379
00:38:08,160 --> 00:38:14,720
they really are and I think like when you think about GPT-3 and how much of a splash that made

380
00:38:14,720 --> 00:38:20,080
there's also this this element that people just have a natural tendency to anthropomorphize

381
00:38:20,080 --> 00:38:25,600
everything like you do this to like your robot vacuum cleaner and things that you give it a name

382
00:38:25,600 --> 00:38:30,800
and right so that in the the words of Daniel Dennett the philosopher you're ascribing intentionality

383
00:38:31,760 --> 00:38:37,840
and and so I think we do that all the time through everything and we do it especially to

384
00:38:37,840 --> 00:38:42,400
things that produce language because language is essentially the only thing we know that is really

385
00:38:42,400 --> 00:38:48,320
really human only and and so when something produces language we just go out there has to please

386
00:38:48,320 --> 00:38:54,160
something brilliant behind that but very often it's just a higher order distributional statistics

387
00:38:55,040 --> 00:39:01,680
it's just clever hans that's basically we we create these benchmark tests and we watch the

388
00:39:01,680 --> 00:39:07,360
performance on these tests going up up up up and we attribute you know a model like GPT-3

389
00:39:07,360 --> 00:39:11,600
on these language tests as getting truly more clever it truly has a deeper quote-unquote

390
00:39:11,600 --> 00:39:15,840
understanding of the task at hand but then you do these clever experiments like the one you

391
00:39:15,840 --> 00:39:21,440
described with scrambling and it reveals well surely actually it's just using distributional tricks

392
00:39:21,440 --> 00:39:26,560
yeah but so I don't know I think the jury still I wouldn't put a debt strongly I think the jury's

393
00:39:26,560 --> 00:39:34,160
still out so I definitely think that there's an evaluation crisis in NLP and I mean I've been

394
00:39:34,160 --> 00:39:40,320
doing a lot of work with lots of folks in trying to improve that through things like Dynabench where

395
00:39:40,320 --> 00:39:47,440
we do a different we try to rethink benchmarking essentially but I mean it's undeniable that

396
00:39:47,440 --> 00:39:55,120
progress in NLP has just been insane so if you look at like what GPT-3 can do compared to GPT-1

397
00:39:56,080 --> 00:40:03,120
or what we can do now with Dali too compared to I don't know the earlier text to image synthesis models

398
00:40:03,120 --> 00:40:08,080
it's just crazy how fast we're yeah so the progress is real but we should be careful to not

399
00:40:08,080 --> 00:40:13,280
not kind of over-interpret what we're seeing so there's still a lot of stuff that

400
00:40:13,840 --> 00:40:18,560
so there's a headline there's a headline going around just this week about

401
00:40:19,360 --> 00:40:26,080
researcher from Google essentially attributing sentience to the Lambda language model

402
00:40:27,600 --> 00:40:33,040
and I think that's really to your point like what we're talking about it's these things actually

403
00:40:33,040 --> 00:40:39,200
know how to work with language and we humans are language machines we're like completely geared

404
00:40:39,200 --> 00:40:45,600
towards understanding and transmitting receiving and sending information with language that is the

405
00:40:45,600 --> 00:40:51,680
most human information you know intentionality and understand the world around us getting stuff

406
00:40:51,680 --> 00:40:58,800
done and so when some mathematical object is doing it it's I feel it too I can't help it have

407
00:40:58,800 --> 00:41:03,680
you ever kind of like had that feeling that you you just like have to push away of like man I'm

408
00:41:03,680 --> 00:41:08,160
talking to this thing but what do you mean by a mathematical object I mean I think you can argue

409
00:41:08,160 --> 00:41:12,560
that your brain is also a mathematical object or at least you can write your brain

410
00:41:13,200 --> 00:41:18,000
you took the bait I was hoping you shake the bait this is like philosopher catnip

411
00:41:20,320 --> 00:41:26,480
well yeah I don't know so I think this it's very interesting because there's a very nice

412
00:41:26,480 --> 00:41:31,760
theory of consciousness that says that we take the intentional stance towards ourselves as

413
00:41:31,760 --> 00:41:37,280
rational agents and that's what consciousness is there's a strange loop as Douglas Hofstetter calls

414
00:41:37,280 --> 00:41:43,920
it so yeah maybe we're we're evolutionarily hardwired to take an intentional stance towards

415
00:41:43,920 --> 00:41:48,880
things and that's why we're so confused by the NLP progress we've been seeing does it also hint

416
00:41:48,880 --> 00:41:57,600
that a way to achieve artificial intelligence of a more AGI flavor oh yeah so I yeah again like

417
00:41:57,600 --> 00:42:03,360
I don't really know what AGI even means and I think it's very premature to to start thinking

418
00:42:03,360 --> 00:42:09,280
about it and so so we have a philosopher slash ethicist who joined having faced recently

419
00:42:09,280 --> 00:42:17,280
Jado Pestili so she has has made this point on Twitter too I think where there are real problems

420
00:42:17,280 --> 00:42:22,800
right now with the deployment of AI and and so when we're thinking about the applied ethics of

421
00:42:22,800 --> 00:42:28,960
these systems there are just real things we need to fix right now and they are much more

422
00:42:28,960 --> 00:42:34,400
salient and much more important right now than thinking about AGI and paperclip maximizers

423
00:42:34,400 --> 00:42:40,320
and things like that I agree I agree completely there's just I think actually the biggest problem

424
00:42:40,320 --> 00:42:46,320
to solve from an ethics point of view is these systems not working that well on narrow tasks and

425
00:42:46,320 --> 00:42:51,280
people over trusting them that's where a lot of harm can come from not from bias even that's

426
00:42:51,280 --> 00:42:55,840
another level of problem just like over trusting systems misunderstanding their limits

427
00:42:56,560 --> 00:43:01,760
yeah true but so there's a trade-off here too right so we shouldn't hype them up too much

428
00:43:01,760 --> 00:43:06,400
because people will just misunderstand what these systems are capable of but we also shouldn't

429
00:43:06,400 --> 00:43:12,400
underhyped them too much so Sam Bowman professor at NYU is a very nice paper where he talks about

430
00:43:12,400 --> 00:43:17,520
the dangers of underhyping so if we all just pretend that there is no progress at all then at

431
00:43:17,520 --> 00:43:22,880
some point we are going to be very surprised when AGI suddenly emerges and we have sent

432
00:43:22,880 --> 00:43:29,920
you into AI so we definitely should think about this stuff and so AI alignment is a very active

433
00:43:29,920 --> 00:43:35,360
research area and it's a very important research area but it's all about finding that balance I think

434
00:43:35,360 --> 00:43:44,000
sure okay lightning round most exciting things on the horizon for research in NLP that you're

435
00:43:44,000 --> 00:43:49,120
either working towards now or you'd like to go a little bit further than what you're you know

436
00:43:49,120 --> 00:43:54,800
just about to ship just about to publish yeah so I'm very excited in multi-modality obviously

437
00:43:55,680 --> 00:44:01,600
I think that there's a lot of interesting work coming out in semi-parametric models where you

438
00:44:01,600 --> 00:44:08,160
have retriever components and some sort of lightweight reader model on top of that retriever so there

439
00:44:08,160 --> 00:44:13,040
is a paper from DeepMind coming out a couple days it came out a couple days ago the idea and a

440
00:44:13,040 --> 00:44:19,680
nutshell there is it that these language models are basically frozen in time based on the data you

441
00:44:19,680 --> 00:44:24,960
give them and so we need some way to help them keep refreshing what they understand about the world

442
00:44:24,960 --> 00:44:30,400
oh yeah that's just one application I think it's much much more so so it's I think it's much

443
00:44:30,400 --> 00:44:36,720
more about how you learn different things so so as humans we have different kinds of memory we have

444
00:44:36,720 --> 00:44:43,360
a semantic memory and an episodic memory and so we can we also have a library and the internet where

445
00:44:43,360 --> 00:44:49,840
we can look up stuff right so we don't have to store all of it in our parameters our brain so I

446
00:44:49,840 --> 00:44:55,520
think if you do this with models too where you have a big index where you can invest a lot of heavy

447
00:44:55,520 --> 00:45:00,800
compute in having a very high quality index then you can have lots of lighter weight reader models

448
00:45:00,800 --> 00:45:05,920
on top of this so this is also going to have lots of repercussions I think for industry where if

449
00:45:05,920 --> 00:45:11,360
you're a company like Facebook you want to have a million classifiers from all of these different

450
00:45:11,360 --> 00:45:15,760
teams all trying to do cool stuff with their classifier if they have a big index that they can

451
00:45:15,760 --> 00:45:22,560
rely on then you kind of do the computing in a much more intelligent way I think so so it's about

452
00:45:22,560 --> 00:45:28,160
finding the mix between the retriever which will be a big big sort of language model and the

453
00:45:28,160 --> 00:45:32,800
reader model on top which will also be a big sort of language model and just for people who are

454
00:45:32,800 --> 00:45:39,200
unfamiliar with the current status quo you what we have right now is a basically just two kinds

455
00:45:39,200 --> 00:45:44,160
of of information storage you've got the model itself which has been pre-baked with just an

456
00:45:44,160 --> 00:45:49,920
understanding of language and whatever emerges from that just basically predicting missing words

457
00:45:49,920 --> 00:45:54,880
and then you've got what people usually call the prompt which is whatever you can cram into the

458
00:45:54,880 --> 00:45:59,600
attention window at inference time so you can actually put a whole conversation there you can put

459
00:45:59,600 --> 00:46:04,000
example problems you could do a lot of neat things in the prompt but it's pretty darn limited right

460
00:46:04,000 --> 00:46:09,840
it's it's aside from your pre-baked knowledge that's crystallized all these things can do is whatever

461
00:46:09,840 --> 00:46:14,480
you can cram into the prompt and what you're suggesting is hey maybe we could actually build a

462
00:46:14,480 --> 00:46:23,040
whole separate system where they could retrieve knowledge at game time yeah so so there are some

463
00:46:23,040 --> 00:46:27,760
interesting so I've been involved in a model called rag retrieval augmented generation and there's

464
00:46:27,760 --> 00:46:33,920
also realm from from Google and yeah so the basic ideas that you can so you have your your

465
00:46:33,920 --> 00:46:39,440
language model which would be parametric and then you have your K&N like a nearest neighbor search

466
00:46:39,440 --> 00:46:44,080
algorithm essentially which is non-parametric and if you put those two approaches together you get

467
00:46:44,080 --> 00:46:49,760
a semi-parametric model and I think there's there's a lot of potential applications for that

468
00:46:49,760 --> 00:46:54,800
down the line so that's one thing and then the other thing so I said multimodal semi-parametric

469
00:46:54,800 --> 00:46:59,040
and I think the other thing that's going to be interesting and there's a lot of attraction

470
00:46:59,040 --> 00:47:06,080
happening there now too is around data-centric AI so I'm still rooting for things like active learning

471
00:47:06,080 --> 00:47:12,240
becoming much more mainstream measuring our data much more carefully so so we have folks like

472
00:47:12,240 --> 00:47:17,760
make Mitchell in hugging face working on data measurement tools and things like that so really trying

473
00:47:17,760 --> 00:47:22,640
to understand much better what's really happening in our data and trying to do things to the data

474
00:47:22,640 --> 00:47:29,120
or curate the data in different ways so that we can have better models in yet yeah I I quickly before

475
00:47:29,760 --> 00:47:37,760
calling you I actually refreshed my knowledge of what this data tool looks like it's kind of like

476
00:47:37,760 --> 00:47:42,240
an x-ray for data sets and it's a really beautiful idea I'm surprised that no one you know you know

477
00:47:42,240 --> 00:47:46,560
what an idea is a good one when you're like why haven't we been doing this for ages it's just like

478
00:47:46,560 --> 00:47:52,720
all the automatic obvious things you can measure about a data set that that's composed of language

479
00:47:52,720 --> 00:47:56,880
like let's put that all in one toolkit and then you can keep adding to it and make it more and

480
00:47:56,880 --> 00:48:02,320
more sophisticated that's the basic idea right yeah that that's exactly it and and so I think that

481
00:48:02,320 --> 00:48:07,920
it's just nice that so this isn't in a space so it has a graphical user interface it just

482
00:48:07,920 --> 00:48:13,120
exists maybe in the longer term it will be a natural part of the hugging face hub where you can

483
00:48:13,120 --> 00:48:19,040
just upload any data set to the hub and start measuring what's actually in your data and you have

484
00:48:19,040 --> 00:48:24,480
a nice interface where you can just inspect it on the fly so we have have a lot of interesting

485
00:48:24,480 --> 00:48:30,400
things going on in the direction of evaluation and measurement so so I think what's really crucial

486
00:48:30,400 --> 00:48:35,120
when you think about the AI pipeline of the future is that you have raw data which you turn into

487
00:48:35,120 --> 00:48:39,760
data sets and those data sets you turn into models which you want to measure your data sets and

488
00:48:39,760 --> 00:48:43,360
your models and you want to understand what's in there and how well they perform and then you want

489
00:48:43,360 --> 00:48:49,680
to based on your measurement deployed the best model to production so for making predictions with

490
00:48:49,680 --> 00:48:55,600
your model and so so measurement is really absolutely crucial in all of the decisions that you're

491
00:48:55,600 --> 00:49:01,200
making there but measurement is also very difficult and and so that's something that we're we're

492
00:49:01,200 --> 00:49:06,240
trying to address so we have an evaluate library that came up a couple of weeks ago we have some

493
00:49:06,240 --> 00:49:11,200
very exciting announcements coming up soon I don't know when this podcast comes out it might

494
00:49:11,200 --> 00:49:17,200
already be out by that time but we're working on evaluation on the hub so that you can essentially

495
00:49:17,200 --> 00:49:22,320
evaluate any model on any data set using any metric just at the click of a button so you don't

496
00:49:22,320 --> 00:49:28,080
have to do any manual stuff there well surely people are going to miss having to go and copy-paste

497
00:49:28,080 --> 00:49:36,480
massive chunks of scikit-learn code into their Jupyter notebooks come on dour yeah yeah no I think

498
00:49:37,040 --> 00:49:44,320
like lower lowering the barrier to doing proper evaluation according to best practices I think

499
00:49:44,320 --> 00:49:48,720
that's that's a hugely impactful thing to do and that's something that hugging faces uniquely

500
00:49:48,720 --> 00:49:53,920
equipped to do so so that's one of the things I've been excited about also in the science team

501
00:49:53,920 --> 00:49:57,760
you mentioned active learning I'd love to dig into that a little bit that's something I've

502
00:49:57,760 --> 00:50:03,920
worked with myself and it's an enticing idea just for those listening at home who aren't familiar

503
00:50:04,800 --> 00:50:10,080
you know usually when you want to make a data set to train a model you the human have to select

504
00:50:10,080 --> 00:50:16,240
the samples of data from your raw data pool that you're going to gold label and you know train

505
00:50:16,240 --> 00:50:20,400
your model on the idea of active learning in a nutshell is let's put a model between you and the

506
00:50:20,400 --> 00:50:26,960
data sometimes the model you're training and it will decide which ones to put in front of you the

507
00:50:26,960 --> 00:50:33,040
human whose time is expensive and you know whose site is limited and make good choices and try

508
00:50:33,040 --> 00:50:38,320
and find the most instructive examples from the data to labels so that you get the most bang for

509
00:50:38,320 --> 00:50:42,880
buck because no one wants to spend their whole life labeling data in fact there's there's some

510
00:50:42,880 --> 00:50:47,600
problems that that's just prohibitive you literally just can't do it true positives are too rare

511
00:50:47,600 --> 00:50:54,480
etc etc so is there some like really exciting new developments in active learning I feel like

512
00:50:55,360 --> 00:50:59,520
like it's kind of like a done deal is there is there something new and exciting on the horizon you think

513
00:51:00,240 --> 00:51:09,920
yeah I'm not so I think it just has a lot of potential and so what you described is a specific

514
00:51:09,920 --> 00:51:14,320
kind of active learning I think where you have an acquisition function that scores examples and

515
00:51:14,320 --> 00:51:20,320
you just decide which example you want to label but I think there are extensions of these algorithms

516
00:51:20,320 --> 00:51:25,520
where you can think not about the labeling part but about the pre-training part so maybe I can

517
00:51:25,520 --> 00:51:31,280
pick parts of a large corpus that I should be pre-training on now because I know what downstream

518
00:51:31,280 --> 00:51:37,360
task I care about in the end and so so there's a mismatch currently between the pre-training phase where

519
00:51:37,360 --> 00:51:42,400
we just do language modeling or so causal or mess language modeling and then we fine tune it on

520
00:51:42,400 --> 00:51:46,560
something that might be very different from what we're training on so we're we don't really know

521
00:51:46,560 --> 00:51:51,680
what to train on so I think if we can connect the pre-training phase to what we know we are going

522
00:51:51,680 --> 00:51:57,840
to care about then you can do very interesting things and so the way to to do that selection so

523
00:51:57,840 --> 00:52:03,440
data selection is true in acquisition function type of things so that's why it's related to active

524
00:52:03,440 --> 00:52:08,480
learning another interesting thing that I've been working on as well with some folks is dynamic

525
00:52:08,480 --> 00:52:13,680
adversarial data collection where you have a model in the loop and and a human is trying to

526
00:52:13,680 --> 00:52:19,120
fool the model and so if you take the the model fooling examples or or so if you take all the

527
00:52:19,120 --> 00:52:23,920
examples including the ones that didn't fool the model but that are still kind of intended to

528
00:52:23,920 --> 00:52:29,120
try to probe the model for a weakness if you train on that data and you keep updating the model as

529
00:52:29,120 --> 00:52:35,040
you're doing the training so that's the dynamic component then you get a much better model out

530
00:52:35,040 --> 00:52:39,920
in the end so it's really like 10 percent better so we have a nice paper where we try to do this

531
00:52:39,920 --> 00:52:44,560
in the limit over like 20 rounds of natural language inference and you just get a much much

532
00:52:44,560 --> 00:52:49,360
better model out in the end so I think the future of data collection the way we think about it now

533
00:52:49,360 --> 00:52:55,040
in the field is going to be changed a little bit where everything is just always going to be

534
00:52:55,040 --> 00:53:00,160
with models in the loop and that maybe ties back to this long term vision of having language

535
00:53:00,160 --> 00:53:05,360
models interacting with each other in some environment but if you can have humans and models

536
00:53:05,360 --> 00:53:09,760
together interacting with each other and learning from each other and maybe trying to also kind

537
00:53:09,760 --> 00:53:15,200
of probe each other and and be on the decision boundaries of certain things then you can learn

538
00:53:15,200 --> 00:53:19,680
much more efficiently I think it sounds like you're describing education like a school

539
00:53:19,680 --> 00:53:26,960
exactly yeah yeah so yeah so so in terms of education one of the important things is also a

540
00:53:26,960 --> 00:53:33,440
curriculum right so I think one thing you could do with this pre-training like the active learning

541
00:53:33,440 --> 00:53:37,760
of pre-training and connecting that to fine tuning is you could try to have a smarter curriculum

542
00:53:37,760 --> 00:53:43,600
so your if your acquisition function changes over time essentially you're designing a curriculum

543
00:53:43,600 --> 00:53:48,800
or you're learning a curriculum on the fly that helps your language model be as good as possible

544
00:53:48,800 --> 00:53:55,120
on the downstream test as you might care about okay and final question at least that I can think of

545
00:53:55,840 --> 00:54:03,200
is something that Thomas mentioned that intrigued me which is he said that there seems to be

546
00:54:03,200 --> 00:54:09,680
something missing with NLP you know like we making these language models bigger and bigger and bigger

547
00:54:09,680 --> 00:54:16,480
and yes we're improving the data and we're getting more data centric but he gave the impression

548
00:54:16,480 --> 00:54:20,640
that he really believes that there's something fundamentally missing it's not just more data

549
00:54:20,640 --> 00:54:25,840
it's not just more text and you've hinted at least yourself with multimodality and interaction

550
00:54:25,840 --> 00:54:33,840
agency or whatever that means so yeah if you if you had to make a guess 10 years from now

551
00:54:35,280 --> 00:54:41,200
like what do you think the paradigm is going to be will we even talk about NLP I suspect NLP

552
00:54:41,200 --> 00:54:48,000
will be a historical footnote they'll just be AI right and text will be just one of the many

553
00:54:48,720 --> 00:54:56,320
crucial developmental raw material for for artificial intelligence yeah but I still think

554
00:54:56,320 --> 00:55:02,880
that text will just remain a dominant modality so even as it is with humans

555
00:55:02,880 --> 00:55:09,280
exactly right so so language really is very crucial so I don't think NLP itself is going

556
00:55:09,280 --> 00:55:16,080
away but I think yeah in order to get to real meaning all of these other fields are probably going

557
00:55:16,080 --> 00:55:24,560
to be subsumed into NLP when it comes to language understanding so yeah I don't know in 10 years

558
00:55:24,560 --> 00:55:31,520
or now I think we're going to have very very different models in a sense but I think a lot of the

559
00:55:31,520 --> 00:55:35,600
the building blocks that we have now are still going to exist in those models do you think it'll

560
00:55:35,600 --> 00:55:43,920
just be all eventually robotics either real world and or virtual world robots like taking in all

561
00:55:43,920 --> 00:55:50,400
the sensory information virtual robots I definitely buy but so I think for actual physical robots

562
00:55:51,440 --> 00:55:56,880
I think like learning from that doesn't really scale that well so I think one of the big problems

563
00:55:56,880 --> 00:56:01,680
in robotics is similar to real right how do you transfer from a simulation to a real environment

564
00:56:01,680 --> 00:56:07,680
and so as our simulations become reader and reader that problem is going to become smaller and

565
00:56:07,680 --> 00:56:13,920
smaller so I don't think we need physical embodiment in any real sense in order to get to meaning but

566
00:56:13,920 --> 00:56:18,560
we'll probably definitely need virtual embodiment where you have an environment where you can interact

567
00:56:18,560 --> 00:56:23,680
with each other but maybe that environment already exists right so it maybe that environment is

568
00:56:23,680 --> 00:56:29,920
just the internet or maybe that environment will come into existence very soon in the form of

569
00:56:29,920 --> 00:56:34,960
the metaverse or whatever you want to call it any final thoughts you want to share with tens of

570
00:56:34,960 --> 00:56:42,240
thousands of people I think so I think one of the things that I think is important and that's

571
00:56:42,240 --> 00:56:49,600
kind of what hugging face also stands for is just open source and open science and so if I were to

572
00:56:49,600 --> 00:56:55,840
give any parting thoughts I would encourage people to always embrace openness because that really

573
00:56:55,840 --> 00:57:01,360
is crucial to making progress but also making sure that the progress that we make doesn't end up

574
00:57:01,360 --> 00:57:07,120
in the wrong hands or go in the wrong direction so just for those listening at home Daoah where can

575
00:57:07,120 --> 00:57:14,320
we find out more about you and what you do yeah so I have a website it's daoakila.github.io it has

576
00:57:14,320 --> 00:57:21,040
a couple of links to relevant social media profiles also have a Twitter account so that's Daoakila

577
00:57:21,040 --> 00:57:28,400
my name so D-O-U-W-E-K-I-E-L-A and so I'm trying to be more active on Twitter I'm still working

578
00:57:28,400 --> 00:57:33,760
on that and I'm bohan and bot on Twitter and you'll see me probably asking follow up questions

579
00:57:34,400 --> 00:57:39,920
out in the open following your advice Daoah thank you so much this was this was a blast thank you

580
00:57:39,920 --> 00:57:51,200
thanks for having me

