1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,320
I'm your host Sam Charrington.

4
00:00:23,320 --> 00:00:28,320
Once again, let's start the show by sending some love out to you, the listeners for your

5
00:00:28,320 --> 00:00:32,440
continued support over the last few weeks and months.

6
00:00:32,440 --> 00:00:38,480
This community continues to amaze us, continues to grow and to engage with us, which we love

7
00:00:38,480 --> 00:00:39,480
to see.

8
00:00:39,480 --> 00:00:44,360
We've said it before, but please don't hesitate to reach out to us with any questions,

9
00:00:44,360 --> 00:00:50,320
comments, guest or topic requests or just a friendly hello via any of our various channels.

10
00:00:50,320 --> 00:00:54,360
You can reach us on our Facebook page or Twitter at Twimble AI.

11
00:00:54,360 --> 00:01:02,240
You can reach me directly at Sam Charrington on Twitter or you can email us at teamattwimbleia.com.

12
00:01:02,240 --> 00:01:07,920
Speaking of community, please take note, the next Twimble online meetup is coming up soon.

13
00:01:07,920 --> 00:01:15,360
On Tuesday, November 14, at 3pm Pacific time, we'll be joined by Kevin T, who will be presenting

14
00:01:15,360 --> 00:01:20,920
his paper active preference learning for personalized portfolio construction.

15
00:01:20,920 --> 00:01:24,920
If you've already registered for the meetup, you should have received an invitation with

16
00:01:24,920 --> 00:01:26,560
all the details.

17
00:01:26,560 --> 00:01:32,520
If you still need to sign up, just head on over to twimbleia.com slash meetup to do so.

18
00:01:32,520 --> 00:01:34,040
We hope to see you there.

19
00:01:34,040 --> 00:01:39,280
Now, as you may know, a few weeks ago we spent some time in New York City hosted by our

20
00:01:39,280 --> 00:01:42,400
friends at NYU Future Labs.

21
00:01:42,400 --> 00:01:47,000
About six months ago, we covered their inaugural AI Summit, an event they hosted to showcase

22
00:01:47,000 --> 00:01:52,360
the startups in their first batch of their AI Nexus Lab accelerator program, as well as

23
00:01:52,360 --> 00:01:55,840
the impressive AI talent in the New York City ecosystem.

24
00:01:55,840 --> 00:02:00,520
Well, this time we had the pleasure of interviewing the four startups from the second AI Nexus

25
00:02:00,520 --> 00:02:07,040
Lab batch, Mount Cleverist, Bite AI, Second Mind, and Bowtie Labs.

26
00:02:07,040 --> 00:02:11,000
We also interviewed some of the great speakers from the event, and we're presenting a couple

27
00:02:11,000 --> 00:02:14,000
of those interviews to you this week.

28
00:02:14,000 --> 00:02:20,360
If you missed any of the shows in the series, visit twimbleia.com slash AI Nexus Lab 2 to

29
00:02:20,360 --> 00:02:21,760
get caught up.

30
00:02:21,760 --> 00:02:29,000
My guess this time is Dennis Mortensen, founder and CEO of x.ai, a company whose AI-based

31
00:02:29,000 --> 00:02:33,480
personal assistant Amy helps users with scheduling meetings.

32
00:02:33,480 --> 00:02:38,160
I caught up with Dennis backstage at the Future Labs event a few weeks ago, right before

33
00:02:38,160 --> 00:02:43,280
he went on stage to talk about investing in AI from the startup point of view.

34
00:02:43,280 --> 00:02:48,480
Dennis shares some great insight into building an AI first company, not to mention his vision

35
00:02:48,480 --> 00:02:53,040
for the future of scheduling, something no one actually enjoys doing, and his thoughts

36
00:02:53,040 --> 00:02:55,960
on the future of human AI interaction.

37
00:02:55,960 --> 00:02:59,720
This was a really fun interview, which I'm sure you'll enjoy.

38
00:02:59,720 --> 00:03:05,040
A quick warning though, this might not be the show to listen to in the car with the kiddos,

39
00:03:05,040 --> 00:03:08,440
as this episode does contain a few expletives.

40
00:03:08,440 --> 00:03:17,560
And now on to the show.

41
00:03:17,560 --> 00:03:21,920
Alright everyone, I'm here at the NYU Scurple Center, where the Future Labs group is having

42
00:03:21,920 --> 00:03:26,720
their AI Summit, and I've got the pleasure of being backstage with Dennis Mortensen,

43
00:03:26,720 --> 00:03:31,440
the founder and CEO of x.ai, Dennis, welcome to the podcast.

44
00:03:31,440 --> 00:03:32,440
Thanks much for having me.

45
00:03:32,440 --> 00:03:34,080
It's great to have you on the show.

46
00:03:34,080 --> 00:03:38,240
Why don't we get started by having you tell us a little bit about your background and

47
00:03:38,240 --> 00:03:39,800
what the company's up to?

48
00:03:39,800 --> 00:03:40,800
Sure.

49
00:03:40,800 --> 00:03:41,800
Background.

50
00:03:41,800 --> 00:03:43,320
So that's the four hour version, right?

51
00:03:43,320 --> 00:03:44,320
Exactly.

52
00:03:44,320 --> 00:03:47,840
How do you find your way into AI?

53
00:03:47,840 --> 00:03:55,160
So we've, sadly, because we are getting older, been at it for about twenty-three years.

54
00:03:55,160 --> 00:04:02,040
This is our fifth venture, and they've all had really a backdrop in data, and I think

55
00:04:02,040 --> 00:04:07,120
we've been able to massage data from the mid-90s all the way up to this point for

56
00:04:07,120 --> 00:04:08,120
way.

57
00:04:08,120 --> 00:04:12,680
If you really want to massage data in the year 2017, AI is probably the right moniker to

58
00:04:12,680 --> 00:04:14,040
apply to that.

59
00:04:14,040 --> 00:04:19,160
So in our prior venture, we did predictive analytics for media, trying to predict which

60
00:04:19,160 --> 00:04:23,960
stories to carry where, say, on the home page of CNN, and for how long, and when you kill

61
00:04:23,960 --> 00:04:26,880
it, what other story do we put in its place?

62
00:04:26,880 --> 00:04:30,920
And before that, we did an enterprise web analytics company.

63
00:04:30,920 --> 00:04:35,480
So go to four seasons, how do you make sure they sell as many rooms as possible, and you

64
00:04:35,480 --> 00:04:37,200
try to analyze that data?

65
00:04:37,200 --> 00:04:42,000
So certainly, just a lifetime around data, and we're very fond of that.

66
00:04:42,000 --> 00:04:44,880
And you're saying, we, so this is a team of people that have kind of stuck together

67
00:04:44,880 --> 00:04:46,400
across these five companies or so.

68
00:04:46,400 --> 00:04:52,440
So we've certainly carried over team members from one venture to the next, and there's

69
00:04:52,440 --> 00:04:57,840
suddenly some comfort in knowing that when I'm up here with you, the house is not at

70
00:04:57,840 --> 00:04:59,480
fire at home, right?

71
00:04:59,480 --> 00:05:04,000
Because, hey, I've talked to these guys for the last ten years, at least.

72
00:05:04,000 --> 00:05:09,200
And for this particular venture of x.ai, it's not so much that we're an AI company.

73
00:05:09,200 --> 00:05:15,720
I think we are, but it's more that we've latched on to this, I think, very obvious pain

74
00:05:15,720 --> 00:05:16,920
of setting up meetings.

75
00:05:16,920 --> 00:05:19,120
So it's not that we kind of invented that pain.

76
00:05:19,120 --> 00:05:24,200
I think both you and I figured out exactly two hours out of college that if you go to work,

77
00:05:24,200 --> 00:05:25,200
you set up meetings.

78
00:05:25,200 --> 00:05:28,160
And when you set up meetings, you're not going to get a personal assistant.

79
00:05:28,160 --> 00:05:29,160
It's you.

80
00:05:29,160 --> 00:05:30,160
Are you?

81
00:05:30,160 --> 00:05:31,160
You're going to fucking hate it.

82
00:05:31,160 --> 00:05:33,960
And then you do it for 20 years straight.

83
00:05:33,960 --> 00:05:42,000
And that just doesn't ring true to me, as in, do I do that task for the next 20 years?

84
00:05:42,000 --> 00:05:46,200
So I think that was perhaps the catalyst to say, hey, there might be this opening for

85
00:05:46,200 --> 00:05:52,480
where some intelligent agent can come along and just remove this one particular chore.

86
00:05:52,480 --> 00:05:56,560
And we then spent the last four years trying to engineer that intelligent agent, a me at

87
00:05:56,560 --> 00:05:57,560
x.ai.

88
00:05:57,560 --> 00:06:01,080
So then when you email me saying, hey, Dennis, I'm downtown.

89
00:06:01,080 --> 00:06:04,960
You've got time to meet up for a Diet Coke, I can reply back and say, yeah, I'm up for

90
00:06:04,960 --> 00:06:05,960
that.

91
00:06:05,960 --> 00:06:06,960
I have C.C.

92
00:06:06,960 --> 00:06:07,960
didn't.

93
00:06:07,960 --> 00:06:08,960
Amy.

94
00:06:08,960 --> 00:06:09,960
And she can help put something on my calendar.

95
00:06:09,960 --> 00:06:14,520
It is now her job to remove me from the conversation, reach out to you.

96
00:06:14,520 --> 00:06:19,880
Have this very human-like negotiation, really, drive it towards conclusion and upon conclusion

97
00:06:19,880 --> 00:06:20,880
send us an invite.

98
00:06:20,880 --> 00:06:23,120
And it's not that you haven't seen it before.

99
00:06:23,120 --> 00:06:24,520
You can buy it today if you want to.

100
00:06:24,520 --> 00:06:26,440
It's just cost you $60,000 a year.

101
00:06:26,440 --> 00:06:27,440
And it's called Tom.

102
00:06:27,440 --> 00:06:32,120
You should have come to the office on Monday, but if you want to pay $17 instead, you should

103
00:06:32,120 --> 00:06:33,120
hire you.

104
00:06:33,120 --> 00:06:34,120
Yeah.

105
00:06:34,120 --> 00:06:35,120
Nice.

106
00:06:35,120 --> 00:06:36,120
Nice.

107
00:06:36,120 --> 00:06:37,120
And you've been at it for four years.

108
00:06:37,120 --> 00:06:38,120
Can people sign up for it publicly for a while?

109
00:06:38,120 --> 00:06:41,560
It was like in by the only or something along those lines, right?

110
00:06:41,560 --> 00:06:42,560
Very much so.

111
00:06:42,560 --> 00:06:49,800
So we spent perhaps short of the first three years doing core R&D, but this is one of those

112
00:06:49,800 --> 00:06:50,800
products for way.

113
00:06:50,800 --> 00:06:54,280
If there's no pre-existing data set, you're going to have that chicken and egg challenge,

114
00:06:54,280 --> 00:06:55,280
right?

115
00:06:55,280 --> 00:06:58,320
I need the product out there to collect some data, but I can't have the product out there

116
00:06:58,320 --> 00:07:00,000
because I don't have any data.

117
00:07:00,000 --> 00:07:08,400
So we had this suddenly very early, early, early data that we ran with for years that became

118
00:07:08,400 --> 00:07:09,800
more and more robust.

119
00:07:09,800 --> 00:07:14,560
And the whole thing was based on a free wait list and we were so fortunate that people

120
00:07:14,560 --> 00:07:16,880
immediately kind of recognized the pain and signed up for it.

121
00:07:16,880 --> 00:07:19,960
We had a very long wait list and that's always nice.

122
00:07:19,960 --> 00:07:26,560
But early this year, we commercialized that R&D and put it to market and now I've been

123
00:07:26,560 --> 00:07:33,600
in market for short of three quarters and I just about to kind of tune our product tiers

124
00:07:33,600 --> 00:07:39,520
and pricing just ever so slightly, but suddenly, off to the races, trying to pay back our

125
00:07:39,520 --> 00:07:40,520
investors.

126
00:07:40,520 --> 00:07:41,520
Nice.

127
00:07:41,520 --> 00:07:42,520
Very nice.

128
00:07:42,520 --> 00:07:48,160
You know, one of the dirty secrets, if you will, of AI is that at least people outside

129
00:07:48,160 --> 00:07:50,840
of the industry think it's just the computer is doing the work.

130
00:07:50,840 --> 00:07:54,880
I'm imagining there's a significant human and a loop component to what you're doing.

131
00:07:54,880 --> 00:07:57,720
Can you tell me, you know, how much of a row, a row that plays?

132
00:07:57,720 --> 00:07:58,720
Sure.

133
00:07:58,720 --> 00:08:05,400
I think there's a difference between what you're trying to achieve and there's nothing wrong

134
00:08:05,400 --> 00:08:10,560
with a human and loop and there's not even anything wrong with a human and loop forever.

135
00:08:10,560 --> 00:08:15,720
That's called automation or augmenting the humans that they can do a job slightly faster,

136
00:08:15,720 --> 00:08:18,120
slightly more accurate and so on and so forth.

137
00:08:18,120 --> 00:08:20,280
I think you need to decide what you want to be.

138
00:08:20,280 --> 00:08:27,040
Do I want to be a, if you're in the self-driving car space, a BMW with slightly better lane

139
00:08:27,040 --> 00:08:33,280
control or do I want to be way more with a fully autonomous vehicle in place, perhaps,

140
00:08:33,280 --> 00:08:34,280
in a decade from now?

141
00:08:34,280 --> 00:08:39,040
But I don't think you can work on both at the same time though, because they're somewhat

142
00:08:39,040 --> 00:08:47,240
in conflict and we set out to create the fully autonomous agent from day one or die-trying.

143
00:08:47,240 --> 00:08:52,040
Investors fucking hate it when I say that out loud, that's in, there is no plan B here.

144
00:08:52,040 --> 00:08:56,520
And I think the difference is between one of you having a fallback for where there's

145
00:08:56,520 --> 00:09:01,080
something here which you didn't understand or didn't predict at a level of accuracy

146
00:09:01,080 --> 00:09:03,880
for where you're willing to move forward in a fully autonomous way.

147
00:09:03,880 --> 00:09:08,360
So you now send it back to a human, we need to kind of resolve that.

148
00:09:08,360 --> 00:09:15,400
Or you have not a fallback, but a willingness to make errors and simply just label the

149
00:09:15,400 --> 00:09:19,200
data and then upon those errors figure out how can I make that prediction slightly more

150
00:09:19,200 --> 00:09:20,840
accurate tomorrow?

151
00:09:20,840 --> 00:09:24,640
So we're not a self-driving car, that means self-driving cars probably don't have much room

152
00:09:24,640 --> 00:09:26,240
for errors.

153
00:09:26,240 --> 00:09:32,400
Perhaps some versions of an error which is that you stop or go to the side or anything

154
00:09:32,400 --> 00:09:33,400
that rhymes with that.

155
00:09:33,400 --> 00:09:35,480
What are you going to do as mess up my lunch tomorrow?

156
00:09:35,480 --> 00:09:36,480
Yeah, exactly.

157
00:09:36,480 --> 00:09:41,560
And I might even churn you as a customer and I would hate for that to kind of happen,

158
00:09:41,560 --> 00:09:44,800
but I'm allowed to make mistakes here.

159
00:09:44,800 --> 00:09:48,680
So we've always hunted the idea of the fully autonomous agent.

160
00:09:48,680 --> 00:09:54,640
And that means of that 150 people we have in place for the team right now, about a hundred

161
00:09:54,640 --> 00:09:55,640
to answer your question.

162
00:09:55,640 --> 00:09:57,360
So I'm not trying to avoid it.

163
00:09:57,360 --> 00:10:02,640
About a hundred of that 150 does nothing but label data.

164
00:10:02,640 --> 00:10:04,120
But I think there's a distinction here between-

165
00:10:04,120 --> 00:10:07,880
But they're not labeling exceptions in the loop of the customer query.

166
00:10:07,880 --> 00:10:12,040
It's something that happens later and Amy does what she can to figure out.

167
00:10:12,040 --> 00:10:18,280
It's certainly happening in real time as well and it's happening as double annotation

168
00:10:18,280 --> 00:10:22,040
and it's happening with golden data sets where they label things that aren't even part

169
00:10:22,040 --> 00:10:23,880
of a real customer query.

170
00:10:23,880 --> 00:10:30,000
But all of this for this purpose of being able to come out tomorrow with a slightly more

171
00:10:30,000 --> 00:10:31,520
accurate set of predictions.

172
00:10:31,520 --> 00:10:32,520
Sure.

173
00:10:32,520 --> 00:10:33,520
Sure.

174
00:10:33,520 --> 00:10:38,640
And that's going to be going from zero data to millions of emails annotated over the

175
00:10:38,640 --> 00:10:45,600
last three years and that becomes that corpus for where we can wake up one day and essentially

176
00:10:45,600 --> 00:10:50,640
have this all margin type of software which we can then be in market with.

177
00:10:50,640 --> 00:10:51,640
Yeah.

178
00:10:51,640 --> 00:10:56,280
So what's the- I mean if you do have two thirds of your team doing annotation and some

179
00:10:56,280 --> 00:11:01,200
of that is real time just like a company that might describe that as human in the loop,

180
00:11:01,200 --> 00:11:06,760
what are the key distinctions between you know building a company kind of with the idea

181
00:11:06,760 --> 00:11:11,160
that you're going to do that and building a company that you know does that but at

182
00:11:11,160 --> 00:11:13,640
horse every minute of it?

183
00:11:13,640 --> 00:11:20,160
I think certainly the difference is that if you have kind of this split setting for

184
00:11:20,160 --> 00:11:22,000
where you have a human in the loop.

185
00:11:22,000 --> 00:11:27,200
The human then many times is tasked to make a perfect outcome.

186
00:11:27,200 --> 00:11:29,720
Whatever the implication might be on your data set.

187
00:11:29,720 --> 00:11:30,880
Just make the perfect outcome.

188
00:11:30,880 --> 00:11:33,920
I said right now just swing the card to the left.

189
00:11:33,920 --> 00:11:35,840
I don't really care what that means to our data set.

190
00:11:35,840 --> 00:11:38,120
Just swing the card to the left.

191
00:11:38,120 --> 00:11:43,080
When we label data there'll be a set of intents, there'll be some entities that we try to

192
00:11:43,080 --> 00:11:49,520
extract but even as they see those entities either not being there or being labeled

193
00:11:49,520 --> 00:11:52,440
so that the outcome is wrong they still move forward.

194
00:11:52,440 --> 00:11:56,920
I said now I'll drive that car into a wall and I'm putting this in the air force and

195
00:11:56,920 --> 00:12:00,600
nobody can see this but their job is not to swing the car to the left.

196
00:12:00,600 --> 00:12:05,440
That is to drive it into the wall and say I labeled it as we've agreed.

197
00:12:05,440 --> 00:12:09,520
And the machinery still took a decision which was not optimal but the only way we can

198
00:12:09,520 --> 00:12:14,080
kind of learn from that is if I label it per the guidelines.

199
00:12:14,080 --> 00:12:20,000
So that's the difference between going all McKinsey style on automating a workforce to

200
00:12:20,000 --> 00:12:26,280
do a job much faster and that of trying to train or create a corpus of data where you

201
00:12:26,280 --> 00:12:29,680
can have this autonomous agent kind of operate.

202
00:12:29,680 --> 00:12:35,320
So I'm hearing that as there's a difference between labeling and having a CSR jump in

203
00:12:35,320 --> 00:12:37,280
and fix a situation and get it right.

204
00:12:37,280 --> 00:12:42,280
And labeling is kind of you know you're positioning it as the longer term approach but certainly

205
00:12:42,280 --> 00:12:50,280
is one that contributes more towards generating a bigger you know better data set whereas

206
00:12:50,280 --> 00:12:55,440
having the CSR jump in and you know provide the right answer when the AI gets stuck might

207
00:12:55,440 --> 00:12:59,080
not necessarily contribute to the long term solution.

208
00:12:59,080 --> 00:13:02,800
Couldn't agree more and it's not that there's anything wrong with that.

209
00:13:02,800 --> 00:13:06,960
I said you can create a formidable business on humans in the loop I said that exists

210
00:13:06,960 --> 00:13:11,920
everywhere in the world call a taxi and there's a human in that car right so that exists

211
00:13:11,920 --> 00:13:14,000
today all over the place.

212
00:13:14,000 --> 00:13:18,080
My point from very early on was just that I think you have to make up your mind whether

213
00:13:18,080 --> 00:13:23,120
you want to be one or the other the day you start because they are in conflict and it's

214
00:13:23,120 --> 00:13:30,120
very easy to fall in love with the perfect outcome because it hurts less on the way there

215
00:13:30,120 --> 00:13:35,800
so I've suddenly had emails in my inbox that have disappointment included into them.

216
00:13:35,800 --> 00:13:40,000
That's the most polite way I can put it where Amy made a mistake she shouldn't have made

217
00:13:40,000 --> 00:13:43,960
and the funny thing about machine mistakes and you know this obviously is that machine

218
00:13:43,960 --> 00:13:48,200
mistakes don't look like human mistakes so if you email me and you and I have a dialogue

219
00:13:48,200 --> 00:13:52,720
back and forth and there's a little bit of ambiguity in the way you put it you can empathize

220
00:13:52,720 --> 00:13:57,560
with my decision I said oh right I see where Dennis came from on that decision but machines

221
00:13:57,560 --> 00:14:03,080
make different type of mistakes for where it is much harder to empathize that's it that

222
00:14:03,080 --> 00:14:07,480
is just so obvious I said why didn't you get that right because there's a difference

223
00:14:07,480 --> 00:14:13,000
in the machine and you trying to kind of resolve what's being said here and that means

224
00:14:13,000 --> 00:14:19,560
little things for where tonight at 1 a.m. you send me an email saying hey Dennis I got something

225
00:14:19,560 --> 00:14:25,080
super important which you meet up tomorrow and talk it through the machine might just really

226
00:14:25,080 --> 00:14:30,600
do that tomorrow but that's not what you're trying to do you wanted today you're just so excited

227
00:14:30,600 --> 00:14:34,840
that you stayed up late and then you want to meet Dennis in eight hours from now right and that

228
00:14:34,840 --> 00:14:39,800
seems like a silly machine mistake I said you saw the importance couldn't you feel the importance

229
00:14:39,800 --> 00:14:46,120
in that email and then I wanted to meet with you in eight hours yeah so that is kind of the

230
00:14:46,120 --> 00:14:50,920
interesting dilemma the way but we've been taking those punches to the face for three years

231
00:14:50,920 --> 00:14:56,280
and we stick into it yeah and so I am imagining your response to this but based on the

232
00:14:56,280 --> 00:15:02,120
the previous conversation around human and loop but for that particular type of error and those

233
00:15:02,120 --> 00:15:08,840
like it is the answer labeling more data or is the answer developing some kind of front-end

234
00:15:08,840 --> 00:15:15,960
set of human heuristics that can help guide the AI down the right path so that's a really good

235
00:15:15,960 --> 00:15:21,960
question so I think people see this as a single problem but it's probably kind of a set-up

236
00:15:21,960 --> 00:15:28,520
problem and I think if you were to simplify it perhaps there's the initial natural language

237
00:15:28,520 --> 00:15:34,600
understanding challenge which is not kind of a solved science and the only hope we have is that we

238
00:15:34,600 --> 00:15:39,640
picked a space so narrow that we might be able to understand everything which is being talked about

239
00:15:39,640 --> 00:15:45,160
when we talk about meetings right but even as you saw that that is certainly a place for where

240
00:15:45,160 --> 00:15:53,880
many times what we just need is more data as in there's things in our little universe here that

241
00:15:53,880 --> 00:15:59,400
happens so rare that the data set is still so sparse and give you an example so a new meeting

242
00:15:59,400 --> 00:16:04,440
intent for the very definition that a meeting is about to happen that happens in every single

243
00:16:04,440 --> 00:16:09,480
meeting that we set up so that means we have a ton of new meeting intent data as in you can say

244
00:16:09,480 --> 00:16:14,680
pretty much what you want hedonist let's do the hoki boki calm early next week we understand that

245
00:16:14,680 --> 00:16:19,160
being you wanting to set up a new meeting but you're trying to change the pin code to the conference

246
00:16:19,160 --> 00:16:24,600
call happens one out of 10,000 meetings so do a hundred thousand meetings and I have 10 data points

247
00:16:25,240 --> 00:16:31,400
as in that is so sparse it's not about any type of model which I might put in place as in there's

248
00:16:31,400 --> 00:16:35,720
just not enough data really to take any good decisions here so that is something for we can certainly

249
00:16:35,720 --> 00:16:41,320
see we just need to keep turning through more meetings and we can see that kind of the level of

250
00:16:41,320 --> 00:16:46,280
accuracy continues to increase so that's the one challenge and then if you do understand what's

251
00:16:46,280 --> 00:16:52,440
being said I said that NLU engine you put in place is robust and backed by a very large data set

252
00:16:52,440 --> 00:16:58,760
you need to have some sort of reasoning engine in place for where you email Amy at x3a and say hey

253
00:16:58,760 --> 00:17:03,640
I'm going to be running five minutes late if I understand it that doesn't mean I know what to do

254
00:17:03,640 --> 00:17:09,400
with it do I do nothing do I do something if I do something what is that something what does that

255
00:17:09,400 --> 00:17:16,200
look like that is where as you alluded to there's a lot of design where I can help

256
00:17:17,080 --> 00:17:23,080
take you down a dialogue path which is more likely to end up in a successful outcome versus

257
00:17:23,080 --> 00:17:28,280
another dialogue path for where it is less likely to end up in a successful outcome and we can

258
00:17:28,280 --> 00:17:35,480
certainly see and this sounds devious and it's not that if we help direct people down one avenue

259
00:17:35,480 --> 00:17:39,560
we're both going to end up slightly happier certainly more likely to end up slightly happier

260
00:17:40,280 --> 00:17:45,720
and then in the end if you take some action in your reasoning engine that is some sort of

261
00:17:45,720 --> 00:17:51,480
computational outcome then you need some NLG engine that can take that computational outcome

262
00:17:51,480 --> 00:17:57,080
and turn it into language so we can communicate clearly to all the constituents that is also a

263
00:17:57,080 --> 00:18:03,320
place where we found that we thought it was clear what we just communicated but given that the

264
00:18:03,320 --> 00:18:09,960
conversational UI is somewhat of a new UI perhaps not to you and I who started on the command line

265
00:18:09,960 --> 00:18:14,040
but certainly to many people in the middle of a way they grew up in the graphical use interface

266
00:18:14,040 --> 00:18:24,200
they don't have some inner connection to the conversation UI and that have been just a long

267
00:18:24,200 --> 00:18:29,800
optimization path trying to figure out exactly how to put it and you describe that last step as

268
00:18:29,800 --> 00:18:36,200
NLG to what extent is it real kind of NLG versus picking from a list of predefined things

269
00:18:37,080 --> 00:18:43,560
we don't think there's a decision tree of sorts for where if I just do every single branch

270
00:18:43,560 --> 00:18:49,960
and have enough templates in place I can find that template the matches that particular setting

271
00:18:49,960 --> 00:18:55,560
that I ended up on so we've tried to create and I'm certainly not saying that we solved it

272
00:18:55,560 --> 00:19:00,520
but the only way that we can be so ambitious is again we picked a single vertical

273
00:19:00,520 --> 00:19:04,680
meaning that you can talk all you want about Chelsea Football Club winning the premiership

274
00:19:04,680 --> 00:19:10,120
next year we don't have any idea but if you talk about meetings we can generate

275
00:19:10,120 --> 00:19:15,560
tricky fluid responses that are created on the fly and the reason that we need that is that

276
00:19:16,280 --> 00:19:22,200
even though meeting sounds sounds almost simple they're just not because you talk about

277
00:19:22,200 --> 00:19:29,160
multiple times in multiple ways well multiple participants some mandatory some optional some assistance

278
00:19:29,720 --> 00:19:35,480
sure and not to get it even get into all the rescheduling and the moving locations and all of that

279
00:19:35,480 --> 00:19:39,560
and you don't follow the path that we sit forth and that means sometimes we need to talk about

280
00:19:39,560 --> 00:19:44,840
fewer things sometimes I need to talk about multiple things at the same time so we assemble those

281
00:19:44,840 --> 00:19:52,280
on the fly and I've been kind of forced to build this kind of design setting so if you want

282
00:19:52,280 --> 00:19:57,880
some sort of visual output you know you and me will go install Photoshop and for other tools

283
00:19:57,880 --> 00:20:03,720
and we'll end up with some pallet of little sprites that we can use for that kind of output

284
00:20:04,280 --> 00:20:12,040
where do you design conversational UIs I said not in word but where right now you're building

285
00:20:12,040 --> 00:20:16,360
your own thing to do it right you're building your own thing and the same as he goes for the

286
00:20:16,360 --> 00:20:21,960
front of it here on the labeling and where do you label your data I said hopefully not in Excel

287
00:20:21,960 --> 00:20:26,520
so you're going to label it somewhere else so you build your own kind of labeling piece of software

288
00:20:27,000 --> 00:20:35,160
so those NLG scripts and scripting that we've invented was some sort of amalgamation of raw text

289
00:20:35,160 --> 00:20:40,600
and JavaScript and our own little version of JavaScript is how we can generate these responses

290
00:20:40,600 --> 00:20:48,680
hmm interesting but even they can given that they are programmatic end up sometimes not sound obvious

291
00:20:48,680 --> 00:20:54,520
I said why does he say that I said that's not even a proper sentence I know I'm also sorry

292
00:20:54,520 --> 00:20:57,960
but it's because we're trying to generate on the fly we actually don't know what it looks like

293
00:20:57,960 --> 00:21:04,360
until she starts talking and you English only currently or multiple languages English only okay

294
00:21:04,360 --> 00:21:12,440
okay we are you're you're kind of sighing back in the chair so no that is a big problem it sounds

295
00:21:12,440 --> 00:21:17,240
like so we have really three dimensions for where we're going to go expand and the reason I do the

296
00:21:17,240 --> 00:21:26,120
kind of slight sign because it was so visual is that whenever we raised another $2 in capital we

297
00:21:26,120 --> 00:21:32,040
immediately get the hey when are you going to do more languages we could talk about the challenge

298
00:21:32,040 --> 00:21:37,000
in that to when are you going to do more communication channels and we can talk about the challenge

299
00:21:37,000 --> 00:21:41,880
in that we will most certainly do both of them we want the agent to be multilingual

300
00:21:42,680 --> 00:21:47,480
not just so we can attack other markets but so that we can better serve the guest

301
00:21:48,200 --> 00:21:52,920
so we set up meetings at about 190 countries today but we really only have customers

302
00:21:53,640 --> 00:21:59,080
in English speaking nations but they meet up with people all over the world so for me up with

303
00:21:59,080 --> 00:22:05,560
somebody in Germany I could actually remove some of the ambiguity for spoken in his own language

304
00:22:06,360 --> 00:22:11,720
so we most certainly will do that and then the last one is that we want to make those obvious

305
00:22:11,720 --> 00:22:18,200
integrations into things that revolve around the event itself say whenever you meet up with

306
00:22:18,200 --> 00:22:24,440
somebody in midtown you use Uber one she set up the meeting two she know where is that three

307
00:22:24,440 --> 00:22:29,480
do you know why you office is that why do I have to kind of click for the Uber you can just make

308
00:22:29,480 --> 00:22:33,240
it happen or even better why do I have to spend an hour and yelp trying to figure out where we're

309
00:22:33,240 --> 00:22:38,360
going to meet those little things where hey you know I eat at Harasushi whenever I meet up with

310
00:22:38,360 --> 00:22:43,080
people just book the table it's not rocket science right right so those are certainly three

311
00:22:43,080 --> 00:22:50,520
dimensions but to say it's all email all English very few integrations okay and you mentioned

312
00:22:50,520 --> 00:22:56,440
some of the challenges on the language side is it all like doing it all over again certainly

313
00:22:56,440 --> 00:23:02,280
there are some economies of scale and you have to see the face that associates this question in

314
00:23:02,280 --> 00:23:08,840
particular this not you but certainly other people are from the outside if they haven't thought

315
00:23:08,840 --> 00:23:13,480
about it for more than a few minutes immediately just let's on to oh so it's just about kind of

316
00:23:13,480 --> 00:23:21,000
translating your templates first of all search and replace yeah yes sorry you did not respond to

317
00:23:22,600 --> 00:23:30,760
Google translation yeah so I think there's two parts to it that's certainly the fact that we might

318
00:23:31,960 --> 00:23:41,560
have to train on a local data set I said the way you set up meetings in northern Europe versus

319
00:23:41,560 --> 00:23:49,000
southern Europe or the Caribbean or Asia might actually just be slightly different it could just

320
00:23:49,000 --> 00:23:55,240
be that northern Europe we are super direct we press slightly more casual and I'm saying that as

321
00:23:55,240 --> 00:24:01,080
an insults if we go to southern Europe we might just have little cues in Asia that we don't have

322
00:24:01,080 --> 00:24:09,000
in the US and if I don't pick up on that I might not have the intelligent agent I had hope for

323
00:24:09,000 --> 00:24:15,000
in a different language so that's certainly should yes we might have to train on a local

324
00:24:15,000 --> 00:24:20,520
data set I'm imagining a country where you know the level of politeness maybe Japan might be an

325
00:24:20,520 --> 00:24:26,040
example this is so high that you know someone says okay that means they don't really want that time

326
00:24:26,040 --> 00:24:30,680
that's just their way of not so not to offend the Japanese we might even cut this whole segment

327
00:24:32,360 --> 00:24:36,520
so now I'll give you something where both you and I are involved so it's only you and I we are

328
00:24:36,520 --> 00:24:42,760
insulting now yeah so I'm Danish I assume you're American yeah and here's the thing we don't do

329
00:24:42,760 --> 00:24:50,200
in northern Europe so you and me can set up a meeting for May 17th next year at my office and you'll

330
00:24:50,200 --> 00:24:56,200
never hear from me I will just assume you turn up at my office and it's all good here's what most

331
00:24:56,200 --> 00:25:03,560
Americans do confirm triple confer so you knew it already three weeks prior to the meeting hey Dennis

332
00:25:03,560 --> 00:25:08,120
just checking in I'll see you in about three weeks the day before Dennis see you tomorrow one

333
00:25:08,120 --> 00:25:14,280
o'clock the first time I'll do the yeah I know it's on my calendar the third time I have yeah

334
00:25:15,000 --> 00:25:20,040
I fucking know which I'm on this like four times now and the funny thing is that we've actually had

335
00:25:20,040 --> 00:25:27,960
to engineer for that in our solution because as you double confirm there's many things which you

336
00:25:27,960 --> 00:25:33,800
say in that that rhyme with the reschedule so we need to be very good at picking up on the fact that

337
00:25:33,800 --> 00:25:38,920
all you're doing here it's just giving me a thumbs up one of the signs that we've done to kind of

338
00:25:38,920 --> 00:25:44,440
protect against that is that Amy learned this skill as well which is that she will reach out knowing

339
00:25:44,440 --> 00:25:49,560
that we set up the meeting a long time ago and you are American so prior to the meeting happening

340
00:25:49,560 --> 00:25:54,600
she'll reach out and say hey just give me your thumbs up the meetings for tomorrow at one o'clock

341
00:25:54,600 --> 00:25:58,920
if there's no changes I'll assume you're both all good and set she'll have a better language than

342
00:25:58,920 --> 00:26:04,200
that so that is one of those interesting things for me just do that for the American and

343
00:26:04,200 --> 00:26:10,520
seems that the Northern European is good so right now she does it for everybody and we haven't

344
00:26:10,520 --> 00:26:16,920
had any complaint for where hey don't be so overly anal they're just taking it you know that's

345
00:26:16,920 --> 00:26:21,400
that but I can totally imagine and that brings me to the second part of the language challenge

346
00:26:21,400 --> 00:26:25,720
outside of being able to train a new data set which is what I alluded to here is that there's

347
00:26:25,720 --> 00:26:31,560
probably some product design choices that you need to make for the particular market that you're in

348
00:26:31,560 --> 00:26:38,280
example take our reminder logic so I see see an Amy to set up a meeting between you and I say

349
00:26:38,280 --> 00:26:44,040
for Friday you are slightly tidy or busy you're here today right doing all sorts of things

350
00:26:44,040 --> 00:26:50,920
how quick can Amy not you so we can certainly see East Coast US you can be reasonably aggressive

351
00:26:51,880 --> 00:26:57,000
we can certainly also see that's even just within the US there's other places for where

352
00:26:57,000 --> 00:27:02,440
people are not as comfortable in her reasoning out as quick as she's doing I said she's a little

353
00:27:02,440 --> 00:27:10,680
bit too bossy for them yeah and certainly people in New York sure still let's do this but there's

354
00:27:10,680 --> 00:27:15,400
certainly other places where that is not the case do you find it all that people that people try to

355
00:27:15,400 --> 00:27:21,000
give Amy the kind of advice that they might give an actual assistant like hey Amy you need to tone

356
00:27:21,000 --> 00:27:27,400
it down a little bit or that kind of thing or to what extent does Amy blur the lines between a

357
00:27:27,400 --> 00:27:34,280
virtual assistant like virtual assistants overloaded but yeah an AI and a human that's a very

358
00:27:34,280 --> 00:27:41,560
interesting question now I'm not sure you me or anybody really got the answer just yet what I

359
00:27:41,560 --> 00:27:47,480
don't think you should do and certainly not something which we're trying to do is to play a game

360
00:27:47,480 --> 00:27:53,560
of daily twining tests for where you try to fool people into believing that this is a human

361
00:27:53,560 --> 00:27:58,840
I'm finding it hard to figure out what you win on each one of those tests sure you can have a

362
00:27:58,840 --> 00:28:04,200
little bit of a ha ha moment and a little bit of social fun and that's that really probably just make

363
00:28:04,200 --> 00:28:10,120
your make the job harder for yourself the next time that is exactly what is happening so we try

364
00:28:10,120 --> 00:28:15,640
very hard to be upfront and honest about the fact that this is machinery but do the job so well

365
00:28:16,280 --> 00:28:23,160
that you kind of forget or don't care now let's give you one stat here that we did early on

366
00:28:23,160 --> 00:28:30,120
so in 11% of all the meetings which we do at least one of the emails in that dialogue will have

367
00:28:30,120 --> 00:28:36,680
one intent only gratitude as in somebody emailed Amy back saying thank you or appreciate you setting

368
00:28:36,680 --> 00:28:43,800
this up for Friday so sorry for not getting back to you earlier even people like me I bloody work

369
00:28:43,800 --> 00:28:51,720
there will start out my kind of handovers with Amy would you be so kind and it's not that I don't

370
00:28:51,720 --> 00:28:58,280
know what's going on here but that is interesting and we're still so early that that's probably

371
00:28:58,280 --> 00:29:04,440
going to be a half decade for we fumble a little bit until we figure out what is the right design

372
00:29:04,440 --> 00:29:10,840
for this new setting for we have kind of mixed agents some human some machine have you ever thought

373
00:29:10,840 --> 00:29:16,280
about whether in that particular example you're doing that for Amy or for the human that's on the

374
00:29:16,280 --> 00:29:24,680
other side of the email so there's some research that suggests in any master slave relationship

375
00:29:24,680 --> 00:29:31,400
if the master is acting in a aggressively demeaning way towards the slave it's actually not the

376
00:29:31,400 --> 00:29:36,840
slave who's losing it's the master and that's plenty of traditional research on that for where

377
00:29:36,840 --> 00:29:44,600
the more rude you turn over time the Saturday things really become for you and that's also why we

378
00:29:44,600 --> 00:29:49,800
have these early suggestions for where you should probably be kind to the Alexis and the series

379
00:29:49,800 --> 00:29:55,960
and the katanas of the world especially if you have kids around the house because you are in one

380
00:29:55,960 --> 00:30:01,160
way certainly asking a question but you're also teaching some other human being about how to behave

381
00:30:01,160 --> 00:30:07,160
in the world and there's certainly a thing missing right now for where they will not learn it otherwise

382
00:30:07,800 --> 00:30:12,760
if they don't learn it from you because there's no real penalties built into these systems yet

383
00:30:12,760 --> 00:30:18,840
which I think we need to have penalties for where like Amy talking back I don't think that's the

384
00:30:18,840 --> 00:30:24,600
right design but yes so I don't know who you think you're talking to Buster but I'm not

385
00:30:24,600 --> 00:30:29,720
scheduling anything for you if you talk to me like that well I couldn't get the point across

386
00:30:30,280 --> 00:30:35,400
that would get the point across I think if you walked into the team of AI interaction designers

387
00:30:35,400 --> 00:30:40,760
we have they would kind of say I hear you let's massage that a little bit and kind of see what we

388
00:30:40,760 --> 00:30:47,080
can do here but my point is certainly what aligns with what you said here for where say we pick a

389
00:30:47,080 --> 00:30:54,360
slightly more refined example for where you are really a super kind person but you kind of late

390
00:30:54,360 --> 00:31:00,680
all the time you're a little bit tardy still nice though that means as Amy is about to suggest

391
00:31:00,680 --> 00:31:06,280
you and me meeting up tomorrow if she knows that you're probably not gonna really be there for the

392
00:31:06,280 --> 00:31:13,160
8 a.m. like a third of the meetings you do you reschedule it's just who you are nice but tardy

393
00:31:13,160 --> 00:31:17,320
perhaps you should really just start out by suggesting how about we meet up at one worst case

394
00:31:17,320 --> 00:31:20,600
then you can just continue to work as inbox you didn't have to kind of get up early and be

395
00:31:20,600 --> 00:31:25,640
at the office for only to kind of sit there alone because you didn't get there and that is us

396
00:31:25,640 --> 00:31:33,080
taking into consideration that people are different here and what I want is even if you kind of

397
00:31:33,080 --> 00:31:40,520
perhaps even turn into an asshole perhaps the response speed just slows down as in she's super

398
00:31:40,520 --> 00:31:46,040
speedy you know machine speedy right but perhaps we'll put this on the cool a little bit

399
00:31:46,040 --> 00:31:49,880
I'll respond to you in half an hour and I'm not sure what those designs look like but I actually

400
00:31:49,880 --> 00:31:57,080
do think they eventually will have to emerge in these systems interesting interesting before

401
00:31:57,080 --> 00:32:01,880
we wrap up I want to go back to a comment that you made earlier about just about the different

402
00:32:01,880 --> 00:32:06,520
machinery the different tools that you've had to build on your own I've talked to several

403
00:32:07,080 --> 00:32:12,040
companies in the conversational space over the past few days and everyone's building the same

404
00:32:12,040 --> 00:32:19,960
things right everyone has you know they started off they tried wit.ai api.ai you know the kind of

405
00:32:19,960 --> 00:32:25,800
the black box nature of those platforms didn't work out for them you know you're saying similar

406
00:32:25,800 --> 00:32:31,000
things you know the platform itself plus all of the tooling that goes around you know labeling

407
00:32:31,000 --> 00:32:37,400
annotation is this all stuff that you think that everyone is doomed to reinvent for themselves

408
00:32:37,400 --> 00:32:43,000
or you know or is it the nature of the problem says that you know folks will want to create these

409
00:32:43,000 --> 00:32:50,120
things over and over or do you think the problem will eventually lend itself to a more of a platform

410
00:32:50,120 --> 00:32:58,920
type of an approach it depends on how loaded the word platform is I suddenly believe that

411
00:32:58,920 --> 00:33:09,160
the tooling will disappear as a task for the individual companies that doesn't ring true to me and

412
00:33:10,360 --> 00:33:18,680
I've been around long enough to see how the first mover were forced to make all sorts of choices

413
00:33:18,680 --> 00:33:23,240
for where they would go and implement things for where had they been a tool out there I wouldn't

414
00:33:23,240 --> 00:33:30,760
have been implementing that but there was no tool out there so that goes from any type of labeling

415
00:33:30,760 --> 00:33:38,520
or even any kind of NLG type design you would have to do I expect that type of tooling to arrive

416
00:33:38,520 --> 00:33:44,200
I'm actually even surprised that more people are not trying to attack the kind of AI space

417
00:33:45,160 --> 00:33:50,440
from that angle yeah and I haven't really seen anybody do anything but just do it for themselves

418
00:33:50,440 --> 00:33:55,560
for where well we might even want to when they spin that out and say hey here's a tool

419
00:33:56,280 --> 00:34:03,400
for where somebody else might be able to take advantage of that and go back say 30 years pretty

420
00:34:03,400 --> 00:34:11,160
much any Fortune 2000 would pretty much implement their own ERP system if you did that in 2017

421
00:34:11,160 --> 00:34:18,760
you'll be crazy you would install you know some Oracle people solve whatever type ERP and

422
00:34:18,760 --> 00:34:24,040
hopefully be happier with that so tooling I agree should and will be commercialized

423
00:34:24,760 --> 00:34:31,800
now on the generalized ability to kind of make predictions I think there's a difference

424
00:34:31,800 --> 00:34:38,840
between whether you being in a high accuracy or low accuracy space higher accuracy meaning that

425
00:34:38,840 --> 00:34:46,840
your product can't exist without a very high degree of accuracy in its predictions so a self-driving

426
00:34:46,840 --> 00:34:52,520
car cannot live with a footnote that suggests for every one thousand miles we hit a pedestrian

427
00:34:52,520 --> 00:34:57,320
even though that is a fantastic piece of software it just can't exist in market but there's

428
00:34:57,320 --> 00:35:03,960
only plenty of software for where hey I pick up 80% of the faces in all of the photos that you

429
00:35:03,960 --> 00:35:09,560
upload nice I said that's not too shabby I said that's really just you helping me out for where I

430
00:35:09,560 --> 00:35:14,520
don't need to kind of attack those faces in most of the images which I upload that's good and for

431
00:35:14,520 --> 00:35:20,600
that you should probably just go use clarify and that becomes I think a good platform play but I

432
00:35:20,600 --> 00:35:28,840
think right now if you cannot live with a kind of degree of error you probably have to forget

433
00:35:28,840 --> 00:35:34,680
how do I then go engineer my own high accuracy backdrop and the only way you can beat those

434
00:35:34,680 --> 00:35:40,280
platforms is by being super focused on some vertical way I'm just the guy who scheduled meetings

435
00:35:40,280 --> 00:35:45,800
right as we've optimized everything for that particular use case and there's not that we're

436
00:35:45,800 --> 00:35:52,120
necessarily smarter than the next guys just hyper focused so yes I do think that will arrive

437
00:35:52,920 --> 00:35:58,680
what I don't think will arrive is that you want to build something on Facebook messenger the tools

438
00:35:58,680 --> 00:36:08,360
which they provide will be all you need sure for some nifty few basic things but not for anything

439
00:36:08,360 --> 00:36:15,000
serious okay great well I really enjoy this conversation Dennis thanks so much for joining us

440
00:36:15,000 --> 00:36:18,200
this was fun we should do it again awesome thank you see you

441
00:36:23,080 --> 00:36:29,160
all right everyone that's our show for today thanks so much for listening and for your

442
00:36:29,160 --> 00:36:37,320
continued feedback and support for more information on Dennis x.ai or any of the topics covered in

443
00:36:37,320 --> 00:36:44,920
this episode head on over to twimmolai.com slash talk slash 67 we hope you've enjoyed our NYU

444
00:36:44,920 --> 00:36:51,880
future labs ai summit series if you need to catch up on any of the episodes visit twimmolai.com

445
00:36:51,880 --> 00:37:00,040
slash ai nexus lab 2 of course you can send along your feedback or questions via twitter to act

446
00:37:00,040 --> 00:37:07,400
twimmolai or at sam charrington or leave a comment or write on the show notes or series pages thanks

447
00:37:07,400 --> 00:37:12,680
again to future labs for their sponsorship of this series for more information on the program

448
00:37:12,680 --> 00:37:29,480
visit futurelabs.nyc and of course thank you once again for listening and catch you next time

