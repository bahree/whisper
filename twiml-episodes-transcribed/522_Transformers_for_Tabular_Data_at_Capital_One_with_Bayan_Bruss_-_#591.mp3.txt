All right, everyone. Welcome to another episode of the Twomble AI podcast. I'm your host,
Sam Charrington. And today I'm joined by Bayon Bruce, a senior director of applied ML research
at Capital One. Bayon, welcome to the podcast. Thanks, Sam. It's great to be here.
It's great to have you on the show. I'm looking forward to digging into our conversation. We'll be
talking about some of your work on deep learning for tabular data. Before we get to that subject,
or to get us to that subject, I'd love to have you share a little bit about your background
and how you came to work in the field. Sure, happy to. I'm really excited to be here. I love this
podcast, by the way. I've learned a tremendous amount over the years. So in my current role,
I lead applied machine learning research at Capital One. Capital One, of course, is a major
financial institution in the United States and Canada and the UK. And we provide a broad range
of financial services products to consumers. And within that, we use machine learning for a number
of our applications in how we provide the services to our customers. And it's really embedded
across the ecosystem, across the business. And when we stood up several years ago,
a team of applied machine learning researchers, which I lead, with the goal of looking at the field
of machine learning. And as we know, and as you've brought on many guests, the field itself is
advancing quite rapidly. There's a major breakthrough. It feels like every six months.
And so the idea behind applied research was, okay, what of that realm of all possible advancements?
Is it all relevant to us as a financial services institution? How do we distill that down into
something that is useful for the kinds of problems that we try to solve as a company? And then,
you know, once you've kind of narrowed the focus a little bit, narrowed even further by saying
which of those techniques, which of those breakthroughs actually work on our data,
which of them actually work for specific use cases and problems within the within the company.
And that's a, you know, that's a hard problem because a lot of the data that gets used in
many of the publications is out there. You know, a lot of the benchmark data is very pristine.
It's very static. It's used for, you know, time and time again in a variety of different experiments.
And so it's been picked over by a number of different researchers. When you compare that to the
kind of data that we use inside the company, it's messy, it's noisy, it's complicated.
You may only have a few people that have really rolled up their sleeves and worked with it for a
number of years. And so there's a big gap then between the breakthrough and then what actually
works when you try it out on the noisy messy data. So our team's goal then is to narrow the
scope down, figure out what actually works, test it on our use cases, and then take it a step
further. Can we actually generalize that, build it into some of our production systems and tools
and platforms and make it available for data scientists across the company to then use it in a
variety of our use cases. And so the ultimate objective is to shrink the time from when something's
discovered or some innovation or some breakthroughs made to when it can be used in servicing our
customers in a unique way. And so that's that's kind of the mandate of my team. I've been with
Capital One for five years now doing that. Before Capital One, I had a mixture of background in
academia and startups and consulting. And when we look at the areas that we could be focused on,
I mean, there's obviously quite a lot of the space of machine learning is fairly massive.
Just fairly. It's just fairly. I mean, I can hardly read everything. If you spent all your time
just reading all the papers that were out there, but we try to like organize ourselves thematically
around topic areas within machine learning. And then that changes over time depending on,
you know, whether there's something, you know, some new advancement that's really exciting and
we're like, okay, we need to really focus on this. But they at the moment, what they are for us is
we have a very strong interest in graph machine learning as a company. As a financial services
company, we basically work with data that is derived from financial networks. When you swipe a
credit card, you are establishing an edge between yourself and emergent. And so every time our
customers process a payment, they are doing so on a network, a financial network at a, you know,
national global scale. And so that network then becomes quite useful in a lot of our applications
if you can have machine learning that can handle the kind of cardinality and sparsity that comes
with that kind of network. And so we've been working for a number of years on taking a lot of the
advancements that we've seen in graph convolutional networks and even other more traditional graph
mining algorithms and applying them to some of our financial services applications. So that's one
of the topic areas that's kind of been a long time for us and continues to be of high interest.
Another one is explainability and interpretability. As a, you know,
highly regulated financial institution, we have a very high bar for understanding the soundness of
our models, understanding the way that our models are making decisions. And so as we use more
machine learning across the company, we've thought it very important to invest in figuring out
of all the advancements in model explainability and interpretability, which ones can be most useful
in helping us manage our risks and manage the way we deploy models as a company better.
A third one is anomaly detection. So, you know, financial services, we are kind of under attack
from fraudsters on a daily basis. Those attacks are extremely creative. We have a lot of
different ways of defending against those attacks. Some of those include supervised machine learning
where we've built these massive models that look at all of our credit card transaction data and
and can predict with fairly high accuracy whether a given transaction is fraudulent or not.
Sometimes they're, they're rule-based systems, a combination of, of both of these and in
heuristics. But in addition to that, you know, we realize that there's a broad range of
anomaly detection algorithms and advancements, particularly as you start to look at how do you
scale anomaly detection to the kinds of scale that we're working with that would allow us to
capture emerging trends of fraudulent behavior or other kinds of nefarious behavior that might be
escaping what a supervised model has seen in the past, right? Your supervised models can only
generalize based on what they know. And if it's a new attack pattern, you have to have something that
is not conditioned on that distribution in order to to capture it and respond to it effectively.
And so that's where we see the promise in anomaly detection. And then the final area that we're
actively focused on kind of internally focused is around privacy. We've worked for a number of years
in how do you generate synthetic data so that you can provide people with an understanding of the
the data that might be in production without actually giving them access to that data.
We have more recently started to explore federated learning as a domain where we could potentially
train models at the edge. Now, beyond those four areas, which kind of are very like pressing
and we're focused on at the moment, there's also some areas that we're looking out into the future
and saying, okay, this could be a potential game changer. And oftentimes when we're looking at
those areas, we partner with major academic institutions to help us flesh out the ideas,
help do some of the more experimental research, publish papers, engage with the community.
It has a couple of really nice benefits. One is that if it's an area that's under-explored
in the research community by funding it and getting a community of researchers, it kind of has
these like cascading effects where more researchers say, oh, that's actually an interesting topic.
Let's do some more research. And then you fund one thing and three more papers come out of
from others on the topic because it's become something of interest to the community. And so
within that realm, and what we were going to talk about today is this domain of deep learning
for tabular data. Tabular data, you know, as we look to our use cases as a financial services
company, you know, many of our data problems are formulated within this realm of tabular data.
So, you know, and for those who don't know, tabular data is essentially structured data as
another term for it. It's a data that comes in a table as opposed to an image which, you know,
you have a grid or text where you have a sequence. Tabular data is, you can think of it as this
mixed type data set where you have some numerical features, some categorical features,
some discrete integers. And usually they're like compiled from a variety of source systems into a
single snapshot of the population you're trying to build a model on top of. And then you,
and then you build and train a machine learning model to make some kind of prediction.
You mentioned earlier kind of this flood of innovation that's been happening in the field.
A lot of the flashiest innovations in machine learning have been focused on
images and NLP to name a couple of examples of graphs as well.
There's been a bit of work on tabular data, but it doesn't seem like nearly as much,
especially considering its prevalence in the broader industry, right? Banks run on tabular data,
most businesses run on tabular data. Any takes on why that is?
I think there's a couple of reasons. And it really boils down to the quality of the baselines.
I think if you look back two decades ago, the quality of the baselines in language,
and then not to disparage the language researchers from 20 years ago, like they just weren't
that great or computer vision. The room for growth was humongous. And so then when you're
starting at a very low performing model, and you can see every year exponential improvement,
well, that gets everybody excited and more research and funding goes towards that.
Whereas if we look at the evolution of machine learning for tabular data, you start with
very simple linear and logistic models, and then you kind of advance into your support vector
machines. And then you see these non-parametric tree-based models and ensembles of tree-based
models like random forests and gradient booster machines. Those have, they do really well.
And it's hard to beat them. And there's another piece to it, which is that not only do they do well,
they do well on a wide variety of problems. And the tooling ecosystem that's been built around them,
tools like XG Boost, and the whole ecosystem of Python for data science has really exploded
in the last decade. Make it really, really easy to use those techniques. And so there's not a whole
lot of incentive to ask, well, what's outside of that paradigm? And I think there's a third factor
in it. And that's primarily that like a lot of the big public benchmark data sets are in computer
vision and in NLP. There hasn't historically existed kind of these big challenge type data sets
for tabular data, where you can see that benchmark improvement year over year at, you know,
some of the big conferences like CVPR. It's the other piece that's missing, I think, from the field.
I think the biggest one is the fact that they just, they're really good models. But what you've seen
as a result of that, as you said, like all of this research has focused on computer vision NLP
and more recently graphs, leaving a huge application space of tabular models outside of the main line
of machine learning research. Still a ton of research that happens in the statistical literature,
but the kind of the stuff you see at ICLR and NUREPs and ICML hasn't primarily focused on tabular
data for the last few years with some research here and there. But a couple of things that are
interesting. One is that we haven't fully explored how the advances that we see in computer vision
and NLP apply to tabular data. I think historically we would have said, oh, there's no, there's no
relationship, right? It's a completely different domain. There's no way to reuse those components.
That was actually originally said between computer vision and NLP, right? Like, there was this
page. You couldn't use language models for vision and vice versa. And then more recently,
we've seen, yeah, exactly. Transformers can be used for everything. You can also, you know,
certain instances of language can be modeled quite well with, you know, image, image models.
And so I think the same thing could be said for tabular data. It's just a matter of, you know,
asking the questions and doing the research and doing the exploration. The other piece that
is really, really critical is that, you know, all of the research that surrounds
computer vision and NLP models, primarily around answering questions of how do we make these
models robust? How do we make these models interpretable and explainable? All of those are often
predicated on the model itself being a deep learning model, right? Many of these techniques require
a differentiable model. And so in a lot of cases, we can't take those techniques and then apply them
to your XGBoost model. You've created this bifurcation in what is possible if you use a neural network
and what is possible if you don't. And by maybe closing that gap and asking, can we do the
tabular data in the same paradigm as we're using for computer vision and NLP? Can we then take all
of that, you know, support that's been built around those deep learning models in computer vision
and NLP and use them for our tabular data? And so that's one of the exciting things about bridging
the gap between the two fields. I think, like, candidly, because the baselines are so strong in
for methods like graded boosted trees and random forests, it's that ecosystem of functionality
that makes it compelling more than, you know, incremental improvements, marginal gains in the
in the overall performance. I mean, everybody gets excited when you have a table where you're
you're performing the best on all the data sets you're testing on. But you know, from a practitioner's
perspective, it's much more exciting to be able to use, you know, the broad suite of capabilities
that come with deep learning. I'm curious, are there specific things that come to mind there?
I generally get the idea of, hey, we've got, you know, this broad set of tooling that, you know,
has been built up around deep learning. And some of that, some of that I can see, for example,
you know, hey, you use TensorFlow or PyTorch, you want to use the same tool chain for everything,
just for efficiencies and, you know, learning curves, all that kind of stuff. But some of the other
things that I'm not sure if you specifically mentioned, like some of the explainability methods
that are based around deep learning do, kind of strike a little bit of, you know, hey, we've got
these tools to solve these problems that were created by the methods that we're using to solve
problems that, you know, we can solve otherwise with better performance that don't have the same
opacity. Yeah, that's a good point. Many of these tools were developed to, as you say,
solve some of the problems that come with deep learning. However, many of the models that you will
find used in large industrial systems will be equally opaque. You know, there's, it's just as hard
to interpret or understand a tree-based model that has, you know, 10 splits per tree and that
has several thousand trees and that has several thousand features that's being used. It's
equally hard to explain a single decision of that type of model as it is to explain a neural network.
It's no less complex a model. It's not capturing any fewer interactions. It's just modeling the
data in a different way. And so I think we're seeing a growth and complexity of machine learning
models, regardless of what you're using, whether it's a deep learning model or a tree-based model,
but I think what we're seeing is that for deep learning, there's just been this huge investment
in trying to understand how they work because I think maybe partially because they've been so
effective and people want to know why. They want to know what they're learning. You know, as we
start to make claims about intelligence, people want to pinpoint factors in the decisioning of
these systems. And so that spurred all of this research. It's available. It just doesn't work for
this other tools, for these other tools that we use. But I think that it's really, really,
I mean, it's important not to just say like, oh, because it's good for deep learning, it'll work
for these other systems, but there are certain problems that we've been able to see the methods
that were developed for deep learning to be very, very powerful for tabular data. If, particularly
in the realm of explainability, and I'll give you an example, there's a subdomain of model
explanations, local explanations called counterfactual explanations. I don't know if you're familiar
with counterfactual explanations. Sure, counterfactual explanation essentially is asking the question for
a given input to a model. What would have had to have been different about this input in order for
the model to have made a different prediction? And so you think about a binary classification task,
like whether or not a credit card transaction is fraudulent. Well, we can say, okay,
given that this transaction was classified fraudulent by this model, what would have had been
different about that transaction for this model to not have thought that would have fraudulent.
So what feature changes would you need? And those feature changes, the difference between
the actual features and what would have had to have been different becomes the explanation
of why that model made a decision. And there's a lot of different ways you can do this, and there's been,
you know, many, many papers on the different techniques to do this. A very, very simple way to do
this is to take the inputs to your model and project them into a lower dimensional space.
And then search within that lower dimensional space, which is now a continuous space, right?
This is the standard way that a neural network works. The shortest path to another data point
where the prediction is different. And then come back out from that lower dimensional space
using like a decoder to original feature space. And now you have input to a model in the original
feature space that if you had used the original model to predict it would have resulted in a
different classification. Now, in order to do that, you need a, you need a differentiable model.
You need to have the ability to train a model such that it can project into that lower dimensional
space and use the, use the gradients with respect to the model prediction in order to do that.
There are other ways to solve that problem. There are ways that don't require you to
use the differentiable model in order to come up with the counterfactual. But that's a very simple
and very efficient way to do it. And if you had a deep learning model, you'd be able to do that.
So that's an example where there are solutions out there. You don't have to use deep learning,
but a lot of the paradigm makes it a lot simpler once you adopt it.
I do. I do. I think that's one of the really exciting things is this
deep learning is compositional, right? You can take pieces of deep learning systems and build
them together and then train them in an end to end fashion. Multimodality allows us to do that
in ways that we would never be able to do or we would have to do in very, very complex ways
historically. A good example of that would be within the domain of graph machine learning.
So we have these complex financial networks. We want to build models that help us predict
individual entities, status within that network. Those individual nodes will often have a lot
of tabular data associated with them in addition to the edges that connect them on the graph.
And so knowing what the right way to encode that tabular data is and how to build that into
broader, differentiable model, a deep learning model on the entirety of the graph,
it becomes really, really powerful. And I think the things that are most exciting that we're all
getting very excited about things like Dolly and stable diffusion are situations where people
have figured out how do we fuse together different domains of data in ways that allow us to
interact with that data in entirely new ways. And quite frankly, I'm not entirely certain.
All the different ways we're going to be able to use multi-modality within financial services,
but I think once we've proven how you can do it, we're going to see a lot of really exciting
ways to do it.
Yeah.
I think fundamentally the data is different. The data tends to come
from the process that generates the data is usually not actually a single process.
Oftentimes, in tabular domains, it's multiple processes. You might be looking at
some combination of customers' payment history along with information about where they spend
their money. And so those are two completely different systems. They're not actually fundamentally
related in any other way other than their with regards to a specific customer. You take those
data sets, you engineer the features, you combine them together, and now you have a tabular data set.
Unlike an image where you have this continuous distribution, at least within a specific domain,
all the images come from the same distribution. They are fairly well structured in the fact that
there's strong local correlations within an image. Nearby pixels are very highly likely to be
similar to the one next to them. Columns in a data set have no inherent structure to them.
There's nothing other than the peculiarities of the data scientists who put that column
next to the other column that determine their proximity to one another.
A lot of that inherent structure is missing, which is one of the reasons why
transformers are starting to be the thing that's bridging that gap, because transformers can
look across the entirety of the data set and determine what context is important. They don't have
to rely on individual proximities that are hard-coded into the architecture to figure that out.
That's one is the mixed type and the lack of inherent structure. I think there's been some
really interesting work. It's primarily been focused on not necessarily
architecturally how do we handle tabular data, but it's focused on even a level before that,
which is how do we encode the data in a way that a deep learning model can utilize that
information more effectively? A number of researchers have started to point out that varying
encoding schemes have a tremendous impact on the quality of a deep learning model for tabular
data. These encoding schemes can be anything from simple linear projections. Yeah, exactly,
exactly, piecewise, linear projections, binning, etc. All of those have a very strong effect.
I think there's going to be more research just into how do we encode data better.
I would also be very curious, and I don't know if anybody's done this yet. If those encoding
strategies also benefit models like XGBoost and random forest models, it might be that rather than
doing simple one-hot encoding, like some of these more complex encoding, both for numerical
and categorical features, benefits, all models. That would be great. Another thing that's been
really profound and some of the researchers that have pointed this out, I think doing great work,
is that the way that you regularize the model has a profound impact, and that's not surprising.
Regularization kind of rules everything in machine learning, and that goes back to computer vision
and LP as well. Interestingly enough, if you go back and look at the original research online,
XGBoost, one of the key things they introduced in that model was novel ways of regularizing
the decision trees. So regularization is kind of one of those foundation steps that if you're
ever going to walk into a new domain of machine learning, you have to figure out what regularization
works for this particular domain. It was a paper a while ago that just found that
if you did hyperparameter search over a group of possible regularization techniques
for any given model, and you selected the optimal subset of regularization techniques,
very simple models like MLPs could perform outstanding on tabular data. It was just the right,
they called it a cocktail of regularization techniques. I think that's a really interesting
approach. Interestingly, they also included in what they're calling regularizing a variety of
data augmentation methodologies. This is an area where NLP and more recently computer vision
have seen really interesting research, which is how do we augment the data and use it for self-supervised
pre-training in a way that makes our downstream models more robust. And that is, this data augmentation
is an area that is almost completely lacking in the tabular data domain. We don't know what works
exactly synthetic data. Yeah, but we don't know what the right techniques are. I think
something like cut mix might work well on an image, but do you just apply cut mix to a tabular
data set? Now you have to define a scheme of data augmentation that actually makes sense for
this mixed type domain in tabular data. And so I think that's an area that's really interesting.
There's a final area, so we have encoding, we have regularization, and then there's finally
architecture. What architecture really impacts? And we've been looking at this for a while now,
and we've been partnering with Tom Goldstein at University of Maryland, his grad student,
Gautamie Somapeli, wrote a paper a few years ago called Saint, that looked to take a lot of what
we've learned in transformer architectures and apply them to tabular data. And this is one of the
first papers in this area, and kicked off a lot of the subsequent research. And kind of the novelty
of that research was twofold. One was one of the first papers to look at transformer architecture
across a row within a given data set. So the goal of a model like that is to ask for all of the
features that I'm using to predict this particular outcome, attend to the ones that matter the most
for this particular goal. And that seems fairly straightforward. Transformer architectures are
designed to do that. Interestingly, Saint also uses this notion of intersample attention. And so it
takes subsamples of the training data, and it asks not just to attend to the individual row that
is for a specific data point, but of all the other data points in that sample, which one is most
useful to this prediction task. And so it almost brings together a transformer architecture with
like a canierous neighbor classifier. So you're not just attending to data points that I'm
interested in. You're also attending to similar data points, or maybe even dissimilar data points
depending on what's most useful to the task at hand. And Saint was a very foundational paper in
this space. Since then, there's been a number of different transformer papers that have looked at
how do we apply these architectures. As I mentioned, some people have pointed out that, you know,
given good encodings and good regularization, maybe you don't need a transformer. I think the
question of architecture is still an open question as much as the question of regularization and
encoding is an open question. But I ultimately think that the combination of these three
in whatever the final state will be will be a powerful new new tool system for deep learning
on tabular data. And so how where are we like this? How far does Saint get us or how close does
Saint get us to solving the problem? Is it just kind of demonstrating a particular, you know, a direction?
That's a good question. It far depends on the journey that you're on.
I think if, right, like it all depends on the end destination. If you look at some of the research
recent survey papers on the field, what they found is that on small data sets anywhere from zero to
50,000 training samples, it's hard to beat, you know, a well-trained extibus model to NIPER parameters.
Like that still is the dominant paradigm. You start to see some of these deep learning
methods exceed that when you get above 50,000. And so I think for a while, we're going to see a gap
between small data sets and large data sets in much in the way that for many years we saw within
the field of NLP. If you're working with a small data set, for instance, for sentiment analysis,
it was much more effective to do a TF IDF encoding and a logistic regression model
than it was to use a big language model if you only had 10,000 training samples.
I think we're in that paradigm where NLP eventually went was if you had a large language model
that was trained in an unsupervised way, you could use it on small data sets and fine-tune it
quite effectively. So maybe we'll get there with tabular data. Maybe there will be this notion of
you know, large pre-trained tabular data models that can then be used on small data sets and
effectively transfer there encoding. What does that actually mean? What is the underlying
the underlying relationship between all tabular data that such a thing would exploit?
Yeah, that's a great question. We're starting to make progress on that. I don't have an answer
to your specific question, but we're starting to make progress very simply. We recently submitted a
paper with Roman Levin and also Tom Goldstein, Michael Goldblum, Andrew Gordon Wilson at NYU
and a number of other grad students where we're looking at transfer learning within the realm of
tabular data and the idea there is if you have a high level overlap between the feature space
of any given task. So in that case, we're looking at medical predictions. So we have a variety
of tabular data sets. A lot of them contain a lot of the similar features and some of them contain
distinct features. They're all trying to predict different diseases. And so the idea was can you
pre-train a tabular data set on maybe one of the larger data sets and the predictions for that
for that given disease and then transfer that. Maybe the feature set is slightly different
and we had to come up with a novel way of how do you adjust the feature set within this
pre-training scheme. But then can you use the learning from that much larger data set into that
smaller one. Now again, that that is very different than a foundation model which basically
gobbles up all of the language on the internet pre-trains and then can be used for anything
that is transfer learning in a much smaller scope. But I think it's a starting point to say that
yes, you can use a generic feature encoder from a tabular data set and extend it to other tasks
in a way that retains the original structures that you've learned. I do think that we would have
to answer the fundamental question which you asked, which I think is still not clear in my head,
which is what does it mean if you were to go and build a model that included every tabular data
set in the world? Like what would it mean for a combination of a healthcare data set with a
financial services data set with like a wine classification data set. And when we've seen all
of the data sets that are out there, they're fundamentally different. So what is it learning?
On the other hand, maybe kind of rolling back a little bit of my
disbelief. I think we're interested in tabular data because tabular data is kind of this fundamental
currency of business like it's all over the place. But I suspect that there's a lot of the same
thing happening in lots of places. Like there are a lot of churn models, like there are a lot of
fraud models, like there's, you know, you can, I don't know how many kind of super classes, you know,
if we were to try to taxonomize, you know, all of the tabular data sets, what that looks like. But,
you know, if you could get access to a whole bunch of, you know, churn,
you have data sets that related to churn models, I could envision like some kind of foundational
churn model that, you know, understands propensities to do something based on other things.
I don't know. I think that's right. I think that's right. Even if the, even if the inputs are
fairly heterogeneous, I think that is, and luckily within industry, you know, there's not, they're
not, there are not very many companies that like span multiple distinct domains where, you know,
they're trying to predict credit card fraud and disease outcomes at the same time. Like you don't
see that kind of conglomeration. So yes, within an individual company is this notion of transfer
learning or even kind of industry-specific foundation models actually do potentially make sense,
globally across all data sets. That's maybe a little bit extreme. But yeah, I think within
specific domain, specific applications, we could, we could certainly think about how you bring
together all those different types of data sets and tasks into a single modeling framework.
Of course, the task of pulling those data sets together is a very different one from collecting
images or text off of the internet. Yeah, it's not as easy as just like many just the political,
you know, the, you know, the access to the data itself. That's right. And many tabular data sets
because they are collected within company walls are actually quite sensitive. They contain private
customer information that rightly so those companies don't want to share publicly and make
available. Very little exists the way that text and images just exist free, free to grab on the
internet within the domain of tabular data sets. But within specific industries, there might be
some consortiums that emerge to, you know, start to bring together groupings of those types of
data sets if this, if this paradigm seems to be the one that people find promising. Certainly,
I think, you know, when we look internally at some of these, as you said, you know, you're,
whether it's churn or it's marketing or fraud detection a lot of times, you're using,
you know, similar overlapping data sets and variations on a theme when you're talking about your
task. And, you know, traditional industry would build a separate model for each one of those
use cases. And when you're first getting started as a company in machine learning, having a
separate model for every single thing is probably not that big of a deal. You've got 10, 15, 20
models that you're maintaining. When you become a full-fledged adopter of machine learning and
you have hundreds of models that you're running in production and interacting with each other in
ways that you didn't anticipate and they're relying on each other in stacked ways, managing
that overall system complexity becomes really, really critical and a very big challenge. And so
rethinking it as a, you know, a single pre-trained model with a lot of smaller fine tuning, you know,
that actually changes how you do business, that changes the tech stack that you work with,
that changes how you think about the overall machine learning ecosystem. And so I do think like
long-term it could potentially help a lot of companies reduce their overall machine learning
complexity if they think about it that way. What do we know about solving tabular data problems?
How do you, what's most important there? And how do you think about approaching them today for,
you know, for the real problems that you're trying to solve today? Yeah, I think that the, the
biggest gap right now if it's standing there between some of this research that we've been
talking about and deep learning for tabular data and what we've actually used internally is not
necessarily kind of at this point a novel architecture, a novel encoding scheme or novel
regularization technique. I think each of those has more research and there's going to be some more
work that figures out like which is the best or which are the sets of best. I think the biggest
gap is tooling is do we have, you know, tools that are as easy to use as scikit-learn or XG boost
that you can fit and deploy a tabular data model using, you know, using deep learning as you can
for a tree-based methodologies. And when I say easy to use it's, you know, everything from
not having to configure a thousand hyper parameters just to figure out what which one's going to work
best to is it hardened and, you know, well documented does it have good logging like the whole
software engineering side of it is actually I think what's missing at the moment. There's been a
lot of great research but now there just needs to be some really good quality engineering that
takes that and figures out, okay, can we build libraries or, you know, packages that make this
so that it's not a data scientist figuring out how to apply research. It's a data scientist
using a tool, right? Like that's that's the gap. That sounds like a fairly well-stated problem and
I know Capital One loves open source. Like, is that, are you working on that? Yeah, that is that's
most data problem because it's something that we're thinking about and working on, yes, but I do
think that's the biggest gap at the moment. Maybe this is a tangent but you mentioned graph
learning, deep learning on graphs that whole direction. Do you think that that plays
into working with tabular data in a big way? Like, is each of the rows in a, you know, tabular
data set kind of a node in a graph and, you know, that's going to help us figure all this out?
Yeah, that's a really interesting question and I think one that's not well studied enough. I
mentioned in saint how the saint architecture takes a transformer and applies it to each of the rows
and then it takes that same transformer and looks across, across rows in a sub sample of the data.
If you think about what that's doing is it's essentially treating the data set
like a similarity graph. It's saying, okay, I've got all these data points. Each one of them is a
node in this data set or at least within the sample of it. Attend to the neighborhood around me
that is most useful for this task and so it transforms it into a graph but I don't think even when
we worked on the paper, we didn't really dive into that element of it and looked at the connection
between, okay, now that you've structured it as a graph or you're even thinking about as a graph,
how does that change how you approach the learning task? I definitely think this question of
how do we create graphs from tabular data as an important one? As we look to
leverage more graph machine learning, there is a big question of how do we
take and it's not always tabular in the sense of like features in a model. Sometimes it's just
tabular data of records about a customer that we want to build into an actual network but there's
a fundamental challenge and how do you go from that structure of a data table into something that
you can use a graph convolutional network or even something simpler like belief propagation.
It's a really hard problem oftentimes to go from data tables into graphs and I think that's one
that we're actively looking at at the moment. Where would you say the kind of research frontier is and
is kind of heading around this problem deep learning for our tabular data? I think
when we mentioned, which is self-supervised pre-training, what are the transformers and so yeah.
And particularly, what is the right way to augment data going into that? What's the right way to
build a transformer backbones for tabular data? I think there's going to be continued study on
encoders and regularizers and how you do the optimal combination of those. I don't think that's
going away. And then I think from a theory perspective, we have to understand a little bit more
about particularly as you start to combine data sets and as those data sets get more and more
heterogeneous, what exactly the architecture is doing in that domain that it makes it at all useful
for any of the downstream tasks. So as we start to push into larger pre-training,
maybe this idea of foundation models for tabular data,
there's open questions not just on how do you do it, but what does it even mean? And why would
you do it? Because I think most people, if you mention that to them, we'll have a very similar
to response to the one you did, which is why would you even do that? That doesn't make any sense
for tabular data. And I think that's a very reasonable response. It doesn't make any sense.
And so if we're going to do that research, which I do believe we will, and we have to, because
that's the direction of, that we're seeing many of the domains of machine learning research go,
we need to answer that question. And what is doing that research mean? Is it just collecting more
and more and more data sets and seeing what happens when we try to train models on them?
Yeah, collecting more and more data sets, seeing what happens when we try to chain models on them,
understanding how to combine those data sets, how to train across multiple domains, whether,
I mean, we talked about multi-modality, but even how do you build a
transformer architecture that can then take in a variety of different tabular data sets,
some might combine a bunch of categorical features, some might be very, very small data sets,
other might be very large data sets, the scale and complexity of the diversity within the
tabular domain is much greater than it is in computer vision and LP. And so those are the
kinds of questions we have to answer is, is how do we build things that can generalize using all
of that diverse data? Yeah, so we kind of set this up in talking about there are all these benefits
that you get from the deep learning ecosystem and tooling. The disadvantage is that you,
you know, we're not kind of at performance parity with the, you know, established methods,
XG boost and the like, but you've also kind of highlighted that there are key gaps in terms
of the tooling. Do you need to get to performance parity in order for the benefits of the deep learning
ecosystem to actually think that we are already at performance parity. If you look at a lot of the
research, many of these models are performing as well. Sometimes we're sometimes better,
but on average as well as XG boost. And so that's why I was suggesting that it's the tooling
ecosystem that needs to improve. I think, you know, many data scientists love the benefit of
well-defined APIs, like I could learn the next to boost and that they don't have to spend a lot
of time thinking about how do I get this piece of software to work for me? They just know that the
piece of software has a fit function and a predict function and maybe six or seven hyperparameters
that they have to know about and they have to know what those six or seven hyperparameters do
with respect to how well that model operates. Many data scientists aren't used to, particularly
ones who aren't working on computer vision and NLP, having to answer 400 different questions around
whether I use this particular type of encoder, whether I use these, these sets of regularizers,
how deep does it need to be? What are the dimensions of each of my, you know, different encoder,
transformer encoders, et cetera? You know, there's so many choices in deep learning.
It makes it really, really hard to, you know, plug and play the way that you can plug and play with like
an XG boost model or a random forest. And so I think as we start to learn what things work the
best, we'll get to a place within the deep learning, uh, tabular data ecosystem in which
there are some tools. They have very simple APIs and a lot of that decision-making has already
been made in the way that the, the tool has been built. And it reduces the complexity and the
decisions that the data scientist has to make when they're, when they're choosing to use that model.
And I think that's when we'll finally see widespread adoption. And once that's in place,
then all of the other stuff, like the explainability, the ability to quantify uncertainty,
all the things that have been developed around deep learning will start to be added on top of that.
But that, that very simple interface, that very simple API I think is the, is the major hurdle
that a lot of people face when they try to start using deep learning for tabular data. There's
just too much for them to figure out. Well, Bion, this is, this has been a wonderful chat.
Um, it's been a while since I've, uh, had a guest on the show talking about the, the topic,
surprising as important as it is. Um, but this has been a great chat to get caught up on it.
Thank you so much. It was an absolute pleasure. I really enjoyed talking to you,
Dave Sam. Uh, my pleasure. Thank you.
