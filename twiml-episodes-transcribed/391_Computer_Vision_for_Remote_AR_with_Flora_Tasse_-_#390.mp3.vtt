WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:25.680
I'm your host Sam Charrington.

00:25.680 --> 00:29.920
Before we get to today's episode, I'd like to send a huge thank you to our friends

00:29.920 --> 00:36.280
at Qualcomm for their support of the podcast and their sponsorship of this series.

00:36.280 --> 00:42.640
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,

00:42.640 --> 00:47.440
reasoning, and action ubiquitous across devices.

00:47.440 --> 00:52.080
Their work makes it possible for billions of users around the world to have AI enhanced

00:52.080 --> 00:56.680
experiences on Qualcomm technology's powered devices.

00:56.680 --> 01:07.120
To learn more about what Qualcomm is up to on the research front, visit twimalai.com-qualcomm-qalcomm-al-c-o-m-m.

01:07.120 --> 01:10.280
And now onto the show.

01:10.280 --> 01:12.760
Alright everyone, I am here with Flora Tase.

01:12.760 --> 01:18.880
Flora is head of computer vision and AI research at Stream, a company that she joined

01:18.880 --> 01:23.840
through the acquisition of another company which she co-founded, Solario.

01:23.840 --> 01:26.680
Flora, welcome to the Twimal AI Podcast.

01:26.680 --> 01:28.880
Thank you Sam, it's a pleasure to be here.

01:28.880 --> 01:29.880
Thanks for the invite.

01:29.880 --> 01:33.720
It's a pleasure to have you on the show and I'm really looking forward to digging into

01:33.720 --> 01:38.160
your recent CVPR presentation.

01:38.160 --> 01:42.200
You did a keynote at the ARVR workshop at CVPR.

01:42.200 --> 01:49.120
I was excited to see that there's enough happening at the kind of intersection of ARVR and

01:49.120 --> 01:55.040
machine learning to, you know, that there's a workshop at CVPR on that topic.

01:55.040 --> 01:57.040
These tons happening.

01:57.040 --> 01:58.040
It was awesome.

01:58.040 --> 01:59.040
These tons happening.

01:59.040 --> 02:00.040
That's face.

02:00.040 --> 02:01.440
So it was definitely a good workshop.

02:01.440 --> 02:02.440
Yeah.

02:02.440 --> 02:07.320
Well, you can tell us all about that, but before you do, please share a little bit about

02:07.320 --> 02:14.400
your background and how you got started in computer vision, how you came to found Solario,

02:14.400 --> 02:16.160
how you ended up at Stream.

02:16.160 --> 02:17.760
I'd love to hear all of it.

02:17.760 --> 02:22.280
That's like a, that would be a long story, but I would try to be the shot.

02:22.280 --> 02:26.240
So I was born and raised in Cameroon.

02:26.240 --> 02:33.080
So Cameroon is in Central Africa and more precisely in the city of Duala.

02:33.080 --> 02:38.920
And so I was raised in the French speaking part of the country, so you might notice that

02:38.920 --> 02:39.920
for my accent.

02:39.920 --> 02:40.920
Okay.

02:40.920 --> 02:47.120
There's some French in there and yeah, so from a very early age, I was very much into

02:47.120 --> 02:53.240
special effects and movies and more precisely Jurassic Park.

02:53.240 --> 02:58.600
So I was a big fan of the, of the movies because of the dinosaurs.

02:58.600 --> 03:02.480
I would just like, I would sit like this close to the TV.

03:02.480 --> 03:07.760
I would introduce dinosaurs and wondering, how can it be like, why is this so real?

03:07.760 --> 03:13.120
And then I asked my dad, like, how can they make these extinct creatures look so realistic?

03:13.120 --> 03:18.000
And then he said, graphics, it's like, okay, yeah, that's what I'm going to do.

03:18.000 --> 03:25.280
That seems like my perfect dream, you know, making the impossible become possible.

03:25.280 --> 03:32.240
So fast forward a few years, I did my bachelor in maths in the English speaking part.

03:32.240 --> 03:38.200
So I had to learn English and then move and did maths because I couldn't, they had no

03:38.200 --> 03:40.320
course in computer science.

03:40.320 --> 03:47.760
So then move out of the country to South Africa, where I did a master in, in Cape Town, beautiful

03:47.760 --> 03:48.920
city in South Africa.

03:48.920 --> 03:55.920
Yeah, it's amazing, I recommend it's a great vacation spot.

03:55.920 --> 04:00.880
So I did my, my master's there then came to Cambridge for my PhD.

04:00.880 --> 04:08.560
So 2012, I arrived in the UK already to make my dreams come true.

04:08.560 --> 04:11.480
And so that was definitely like a good experience.

04:11.480 --> 04:18.000
So at the Cambridge, I was looking at how do you take real things in images and turn them

04:18.000 --> 04:19.840
into 3D content.

04:19.840 --> 04:23.880
So I was doing some 3D shape retrieval, shape analysis from images.

04:23.880 --> 04:30.160
I mean, you have a 2D image and you want to turn that into a 3D shape.

04:30.160 --> 04:31.160
Exactly.

04:31.160 --> 04:33.680
Yeah, that's exactly what I was doing.

04:33.680 --> 04:39.920
So for years or three and a half years, I stumbled upon like a great discovery that if you

04:39.920 --> 04:47.040
actually incorporate NLP, so if you're in corporate language information, you can basically

04:47.040 --> 04:52.680
get really, really accurate results going from 2D to 3D.

04:52.680 --> 04:59.840
And so the addition of that, yeah, so the concept was that rather than just looking at images

04:59.840 --> 05:05.600
as just, you know, pixels that you can then turn into some features and then use classification

05:05.600 --> 05:10.680
on them, it was a concept that those features can actually be semantic features.

05:10.680 --> 05:15.600
So there's some semantic meaning attached to those descriptors that you are, that you

05:15.600 --> 05:16.600
are generating.

05:16.600 --> 05:17.600
Okay.

05:17.600 --> 05:23.880
And so that means that of the text that would associate or would be associated with an image.

05:23.880 --> 05:29.880
So yeah, the very early version was just, so if I look at the chair, I know it's a chair.

05:29.880 --> 05:34.600
So I have like my data, my data set has labels like this is an image of a chair and this

05:34.600 --> 05:35.600
is a chair.

05:35.600 --> 05:40.480
And what we are trying to do is create a representation that capture both of those information.

05:40.480 --> 05:45.880
So it captures both the visual information and the text information, which in this case

05:45.880 --> 05:51.560
is just the semantic meaning of what the word share is.

05:51.560 --> 05:56.040
So in language processing, they have a lot of language models.

05:56.040 --> 06:01.880
So that's what they use for translation and other NLP tasks and they do this by representing

06:01.880 --> 06:06.640
those words as multi-dimensional vectors.

06:06.640 --> 06:12.160
And so what we were doing was creating a representation that would capture both the visual

06:12.160 --> 06:16.360
data and that language based representation.

06:16.360 --> 06:21.160
In that, it would capture semantic data inside the representation.

06:21.160 --> 06:26.600
And since that is actually, you have the same concept in 3D shapes, in hand drawn sketches,

06:26.600 --> 06:29.960
you can use that as a way of bridging the garbage in all of these domains.

06:29.960 --> 06:32.520
That was my PhD, basically.

06:32.520 --> 06:38.320
And so we had really good results, really increased the bench, the baseline by like 30% in some

06:38.320 --> 06:40.400
of the use cases.

06:40.400 --> 06:42.040
And so that was great.

06:42.040 --> 06:48.000
And the question is what happens next, what do we do with this?

06:48.000 --> 06:52.960
So there were some offers on the table, like big companies could go and work for, but

06:52.960 --> 06:58.360
I was really passionate about that specific discovery and how you could scale that to not

06:58.360 --> 07:01.920
just one object, but multiple scenes.

07:01.920 --> 07:08.360
Can I then take a picture of my living room and then turn each object into a hidden shape

07:08.360 --> 07:11.920
and build a scene graph of my surrounding?

07:11.920 --> 07:16.320
And so that became Celerio, that's what Celerio was doing.

07:16.320 --> 07:22.320
So it's fun, now that work out of the university created a startup called Celerio.

07:22.320 --> 07:28.360
That was looking at how do you take the real world and turn that into a digital format?

07:28.360 --> 07:33.080
Yeah, the story of that for how long before stream came along.

07:33.080 --> 07:34.400
Two years and a half.

07:34.400 --> 07:35.400
Okay.

07:35.400 --> 07:36.400
Yes.

07:36.400 --> 07:37.400
So it was a great journey.

07:37.400 --> 07:43.680
Like journey, we got to like build something I was running on the phone, had it tested by

07:43.680 --> 07:49.800
a bunch of like AR developers, it was a really cool and interesting journey.

07:49.800 --> 07:55.080
And during that testing phase, stream was one of the people who were testing our platform,

07:55.080 --> 07:57.400
looking to integrate that into their product.

07:57.400 --> 08:02.600
And we had you had met stream like a year ago because we have a common investor.

08:02.600 --> 08:04.360
And so we really like what they were doing.

08:04.360 --> 08:10.480
So stream is all about remote collaboration and how do you add value to that using open

08:10.480 --> 08:11.480
technology?

08:11.480 --> 08:16.760
So you take you take a video collaboration platform and then you incorporate it at a

08:16.760 --> 08:19.040
very core of it.

08:19.040 --> 08:23.680
And so they were doing that for like really amazing use cases like repairing, customer support,

08:23.680 --> 08:25.520
home repairs and so on.

08:25.520 --> 08:31.040
And so they came with us with an offer because they saw what we're doing as very close to

08:31.040 --> 08:36.760
like the vision of them of their product.

08:36.760 --> 08:39.760
And we were very interested in like bringing this tech to the real world.

08:39.760 --> 08:45.120
So we had been hard at work building the technology and we're really happy about the results,

08:45.120 --> 08:50.960
but we couldn't wait to actually see that being used by real customers and stream hello

08:50.960 --> 08:53.120
one of the best use cases for AR.

08:53.120 --> 08:56.520
And so that's how the acquisition happened.

08:56.520 --> 08:57.520
That's awesome.

08:57.520 --> 08:58.520
Yeah, go.

08:58.520 --> 08:59.520
Thank you.

08:59.520 --> 09:00.520
Thank you.

09:00.520 --> 09:07.960
So I'll say before we get very far, we will link in the show notes to your slides from

09:07.960 --> 09:10.840
the AR VR workshop.

09:10.840 --> 09:14.280
And folks who are listening should definitely check them out because there are a lot of

09:14.280 --> 09:20.440
cool videos and demos and things like that in the slides.

09:20.440 --> 09:27.760
So you know, maybe tell us a little bit about your talk and kind of what some of the main

09:27.760 --> 09:30.440
points that you're presenting.

09:30.440 --> 09:36.040
So the talk was entitled computer vision for remote AR.

09:36.040 --> 09:41.080
And so a lot of time people think of AR VR, they think of a mobile phone, they think

09:41.080 --> 09:48.600
of a headset, they think of, you know, like, yes, basically.

09:48.600 --> 09:53.640
And oh, like they would think of like an experience where you are looking at something and then

09:53.640 --> 09:56.800
you have something augmented on top of that.

09:56.800 --> 10:03.960
So remote AI is very different because someone else across the ocean is kind of experiencing

10:03.960 --> 10:08.320
what you are experiencing, but they are not in that same physical location.

10:08.320 --> 10:13.480
And so then I was talking about like, yes, what remote AR is and how computer vision can

10:13.480 --> 10:19.480
then help solve some of the issues that take place in that kind of scenario.

10:19.480 --> 10:23.960
And more precisely, I was looking at the stream use case, what we do and how we are using

10:23.960 --> 10:27.080
computer vision to solve some of the issues.

10:27.080 --> 10:30.800
One of them is something like measuring the environment.

10:30.800 --> 10:37.640
So a quick example, if I'm here in my living room and I have an issue with my fridge and

10:37.640 --> 10:43.480
I call one of these companies, I call their customer support and say I have this issue.

10:43.480 --> 10:49.480
Now they can use stream and the augmented reality platform that we build so that the expert

10:49.480 --> 10:54.520
we call it a pro on the other side can have visibility into my environment and what I'm

10:54.520 --> 10:55.920
looking at.

10:55.920 --> 11:01.520
And so sometimes one of the main use case, main question that they have is can we measure

11:01.520 --> 11:02.520
things?

11:02.520 --> 11:06.400
We want to be able to measure something so that if we send, if something is broken and

11:06.400 --> 11:10.320
we have to send something over, you know, you know, we know what size it should be.

11:10.320 --> 11:12.040
How can you make measure and more accurate?

11:12.040 --> 11:14.800
And it's really hard to do that on an image.

11:14.800 --> 11:19.520
So one thing that we do is being able to build a 3D mesh of the environment.

11:19.520 --> 11:24.880
And so once you have a 3D mesh, then the pro can just say from point A to point B, what

11:24.880 --> 11:27.400
is the distance?

11:27.400 --> 11:32.280
And so things that are important is measuring like the geometric data, but also the texture

11:32.280 --> 11:35.880
data because once you have texture, it gives you context.

11:35.880 --> 11:38.480
And so I was, yeah.

11:38.480 --> 11:46.040
The meshes that you build or are working with are these based solely on 2D data or like

11:46.040 --> 11:50.680
the iPad now has a lidar sensor in it, it's got to make the job easier.

11:50.680 --> 11:51.680
Yeah, definitely.

11:51.680 --> 11:56.360
That has been, yeah, that's, that's, I look forward to more devices have interesting.

11:56.360 --> 12:01.040
So we definitely have a framework that we build in house for doing meshing from only to

12:01.040 --> 12:02.040
the images.

12:02.040 --> 12:03.040
Okay.

12:03.040 --> 12:07.000
But that wouldn't be needed as accurate as using depth sensing, definitely.

12:07.000 --> 12:12.840
So we have different solutions, depending on the capability of the device that the customer

12:12.840 --> 12:13.840
is using.

12:13.840 --> 12:14.840
Okay.

12:14.840 --> 12:15.840
Yeah.

12:15.840 --> 12:20.840
And it's building the 3D mesh from the 2D image.

12:20.840 --> 12:25.320
Talk a little bit about, you know, is that a largely solved problem?

12:25.320 --> 12:26.960
Is it still like really hard?

12:26.960 --> 12:32.360
Like where is it on the spectrum of, you know, computer vision, AR, types of challenges?

12:32.360 --> 12:37.000
Yeah, it's still, it's still really hard, so it's a solved problem.

12:37.000 --> 12:43.120
As solved as it can be, what people are not doing is adding AI for predicting depth, but

12:43.120 --> 12:47.920
I see some challenges around using predicted depth for meshing because you need temporal

12:47.920 --> 12:48.920
coherence.

12:48.920 --> 12:52.800
So as you go from A to 3D, you need things to be kind of correct.

12:52.800 --> 12:56.320
You need a depth to be current across the different frames.

12:56.320 --> 13:00.440
So that's still a, that's still some of the people are looking at.

13:00.440 --> 13:03.800
So for the vast majority, multiple are using motion stereo.

13:03.800 --> 13:10.880
So you look at consecutive frames and you are trying to find the values that are consistent

13:10.880 --> 13:12.720
across multiple frames.

13:12.720 --> 13:18.560
And so that is going to be harder if you have frames that don't have much texture, depending

13:18.560 --> 13:22.120
on the change of lighting, that still would be tricky.

13:22.120 --> 13:28.000
So people are now looking towards like stereo systems or, you know, devices that have

13:28.000 --> 13:29.480
their same thing.

13:29.480 --> 13:35.480
So there's not much work going into, you know, meshing from 2D images.

13:35.480 --> 13:39.080
There's not that much work going in there because there will always be some limitations

13:39.080 --> 13:42.080
because of just the nature of the, of the problem.

13:42.080 --> 13:46.160
And the stereo use case that you're describing is that assuming multiple cameras?

13:46.160 --> 13:47.160
Yeah.

13:47.160 --> 13:52.440
So they, one of the, some of the techniques that people are using from 2D images is like

13:52.440 --> 13:58.640
using motion stereo, so you kind of look at two images from the same camera and if they

13:58.640 --> 14:01.040
are close enough, you can simulate stereo.

14:01.040 --> 14:04.920
And obviously you also have like phones that have left and right images.

14:04.920 --> 14:09.400
In that sense, you can just like make the problem easier because you don't have to try to

14:09.400 --> 14:14.480
rectify, you know, images that are happening at different time frames, time stamps.

14:14.480 --> 14:19.360
You, I'm sorry, I interrupted you, you're going through these use cases that you see and

14:19.360 --> 14:20.960
one of them is measuring things.

14:20.960 --> 14:21.960
Yeah, measuring things.

14:21.960 --> 14:28.360
Yeah, I'm just making the point that for measuring things, meshing and texture are very important.

14:28.360 --> 14:34.560
So part of my talk was, how do you attach texture to a mesh that is changing all the time

14:34.560 --> 14:39.520
and how do you do that in a way that's real time so that it doesn't interrupt the customer

14:39.520 --> 14:40.520
experience.

14:40.520 --> 14:45.560
So that was one of the big points I talk about in the presentation.

14:45.560 --> 14:46.560
Okay.

14:46.560 --> 14:47.560
Yeah.

14:47.560 --> 14:48.560
It was drilling to that.

14:48.560 --> 14:52.560
Is that something that your research is focusing on at a stream?

14:52.560 --> 14:53.560
Yeah.

14:53.560 --> 14:56.840
So that's one thing that you are actively working on is.

14:56.840 --> 15:01.560
So a lot of prior work, if you look at some of the mesh reconstruction frameworks out

15:01.560 --> 15:05.920
there, they will give you a color per vertex.

15:05.920 --> 15:09.800
So they don't really have, so the colors kind of look really washed out and blurry.

15:09.800 --> 15:16.040
And so it's kind of hard to like make out the edges of, you know, a countertop in the

15:16.040 --> 15:17.040
mesh.

15:17.040 --> 15:22.800
And so we see texture as a big part of, you know, making that easier, making is easier

15:22.800 --> 15:27.840
for people to be able to like make out the context and the edges.

15:27.840 --> 15:33.120
And so it becomes really tricky to like most of the time this, this is happening.

15:33.120 --> 15:36.480
The testing process happens after the meshing.

15:36.480 --> 15:40.920
We are trying to do that in real time as the mesh gets updated all the time.

15:40.920 --> 15:44.400
So that's, that is what makes it very challenging.

15:44.400 --> 15:46.880
And that's the research problem that you are working on.

15:46.880 --> 15:51.680
And so the textures that we're talking about also like at some kind of texture estimate

15:51.680 --> 15:53.600
or something per vertex.

15:53.600 --> 15:54.600
Yeah.

15:54.600 --> 15:59.280
So you have, you have like a mesh that you obtain from, it doesn't matter where you got

15:59.280 --> 16:00.280
it from.

16:00.280 --> 16:04.640
And you have some video and you're trying to kind of, and you know where your camera is.

16:04.640 --> 16:09.880
So I mean, time you can say, I'm at this point looking into this direction, how can I use

16:09.880 --> 16:14.080
that information and apply that to my mesh.

16:14.080 --> 16:17.800
And the camera is moving all the time, the video has blur.

16:17.800 --> 16:24.480
So how do you then like kind of try to make a mesh that looks good, at least like with

16:24.480 --> 16:26.160
no artifacts.

16:26.160 --> 16:30.320
So that's, yeah, that's a problem that you look at.

16:30.320 --> 16:37.400
And is the end application in trying to apply the colors and textures to the mesh?

16:37.400 --> 16:43.000
Are you essentially trying to create a, like a 3D model of whatever the camera is seeing

16:43.000 --> 16:44.480
someplace else?

16:44.480 --> 16:49.200
Yes, because yeah, not only you are doing that, you are trying to, so it, this is remote

16:49.200 --> 16:50.200
AR, right?

16:50.200 --> 16:54.320
So this could be shared with one person, multiple people who are looking at the same scene and

16:54.320 --> 16:59.640
they actually can like, they can insert 3D elements and things like that.

16:59.640 --> 17:03.360
So not only you are trying to do this, meshing this texturing, you are also trying to sync

17:03.360 --> 17:05.560
it across devices.

17:05.560 --> 17:10.520
So there's a lot of networking challenges that come into place when you start doing these

17:10.520 --> 17:11.520
things.

17:11.520 --> 17:19.280
So I'm wondering why I need the mesh to be very accurate if I've got the video stream.

17:19.280 --> 17:23.400
So because it's really hard to do measurements in a video stream because the camera is always

17:23.400 --> 17:24.400
moving.

17:24.400 --> 17:29.280
So if you're a professional on the other end, you pretty much have to say, oh, like, can

17:29.280 --> 17:33.200
you just be still for like the next 20 seconds?

17:33.200 --> 17:38.040
Well, I take this measurement and the measurement would be accurate because you might be clicking

17:38.040 --> 17:46.760
at some place, but actually, it's not, it's the pixel or the vertex data at that location

17:46.760 --> 17:47.760
is not that correct.

17:47.760 --> 17:52.040
You actually look at something behind, let's say behind the fridge instead of like on the

17:52.040 --> 17:53.040
edge of the fridge.

17:53.040 --> 17:56.560
And so when you have 3D information, it gives you just more context.

17:56.560 --> 18:03.000
You can rotate the mesh and then make sure that you're actually clicking the right end

18:03.000 --> 18:04.000
point.

18:04.000 --> 18:08.960
And so then having the color and the texture information associated with the mesh allows

18:08.960 --> 18:15.320
the person who's remote to make sure that they're clicking on the right things, exactly,

18:15.320 --> 18:16.320
exactly.

18:16.320 --> 18:18.800
So they're the experts so that they know how to take this measurement.

18:18.800 --> 18:22.040
Usually they have to come into the home to do this.

18:22.040 --> 18:26.280
And in the home, they are in 3D because they're in the location.

18:26.280 --> 18:30.920
So how can we kind of replicate that experience in a remote setting where they're not physically

18:30.920 --> 18:37.400
there, so that's where like 3D information is so, so important.

18:37.400 --> 18:41.560
And if you fast forward like in a setting where you have headsets, then obviously you can

18:41.560 --> 18:46.040
put on your headset and you can actually walk around the environment and feel like you

18:46.040 --> 18:48.080
are in the physical location.

18:48.080 --> 18:53.800
And again, with the color and texture, you want, certainly with the headset, you want

18:53.800 --> 18:58.600
to have a situation where you're kind of walking around navigating the world and the

18:58.600 --> 19:02.440
video might not be there anymore or the video might be going in in a different direction

19:02.440 --> 19:03.440
or exactly.

19:03.440 --> 19:04.440
Exactly.

19:04.440 --> 19:05.440
Yeah.

19:05.440 --> 19:06.440
Okay.

19:06.440 --> 19:07.440
Cool.

19:07.440 --> 19:16.240
So texture as a big element of measurement was one of the points that you went into.

19:16.240 --> 19:19.320
What were some of the other cases you covered?

19:19.320 --> 19:26.320
So one thing that we don't see more shared computer vision conference, like even ARV are,

19:26.320 --> 19:33.720
you know, applications, it's just been able to know is extract metadata from the environment.

19:33.720 --> 19:38.440
So more precisely, if you are looking at see a washing machine and a place looking to

19:38.440 --> 19:42.960
fix this washing machine, it's very important for them to know what is the serial number,

19:42.960 --> 19:44.920
what model are we looking at.

19:44.920 --> 19:48.920
So that's that's like the bread and bread of customer support.

19:48.920 --> 19:54.760
And how do we, and then the customer really don't know this information or had you kind

19:54.760 --> 20:00.560
of like they are focused on the fixing part of it, not necessarily on the data collection

20:00.560 --> 20:02.080
part of the process.

20:02.080 --> 20:07.720
So one of the computer vision problems that you have is how do you look at a device and

20:07.720 --> 20:16.200
be able to extract information about model number, serial number, especially from labels.

20:16.200 --> 20:20.400
So if some of these devices, they make it easy for you to have, they have labels.

20:20.400 --> 20:26.200
And then can you look at text and know what text is relevant and what text is associated

20:26.200 --> 20:29.560
to a serial number or to a model number.

20:29.560 --> 20:35.720
So that's the metadata extraction part of what we do.

20:35.720 --> 20:41.680
Having repaired a bunch of home appliances or at least attempted to, you know, and

20:41.680 --> 20:45.160
seeing a lot of these labels, they have all kinds of numbers on them and trying to figure

20:45.160 --> 20:47.400
out which ones are the right ones.

20:47.400 --> 20:55.600
It sounds similar to looking at some other kind of structured or semi structured text,

20:55.600 --> 20:59.960
like a document, like an invoice and trying to figure out what's the invoice number

20:59.960 --> 21:02.840
and what are the other items and that kind of thing.

21:02.840 --> 21:03.840
Yeah, yeah.

21:03.840 --> 21:05.440
And it can be so hard, right?

21:05.440 --> 21:08.680
The orientation is probably going to be weird and a perspective is probably going to

21:08.680 --> 21:09.680
be weird.

21:09.680 --> 21:10.680
Yeah.

21:10.680 --> 21:12.080
The lighting is probably going to be bad.

21:12.080 --> 21:13.080
Definitely.

21:13.080 --> 21:15.920
Like we do a lot with like skewed angles, like depending on where you are looking from,

21:15.920 --> 21:18.000
it can be very distorted.

21:18.000 --> 21:21.400
Adding to the fact that like manufacturers have very different standards.

21:21.400 --> 21:25.240
So you can't have like a rule-based system that says, oh, like we'll always be looking

21:25.240 --> 21:29.600
at, you know, this area of the label.

21:29.600 --> 21:37.000
So AI, this is where like AI really comes, comes handy because so yeah, we just basically

21:37.000 --> 21:40.320
look at it as a two-part problem.

21:40.320 --> 21:42.040
First is an OCR problem.

21:42.040 --> 21:47.320
So how can I just extract text only data from this image?

21:47.320 --> 21:51.880
And then it's, you know, which part are relevant to what we care about.

21:51.880 --> 21:56.680
And so that becomes a classification problem based on the textual data, based on ways

21:56.680 --> 22:01.720
located, train over a lot of labels and a lot of a lot of data.

22:01.720 --> 22:06.720
We can now kind of look at the label and tell the custom, the pro that this is a serial

22:06.720 --> 22:07.720
number.

22:07.720 --> 22:11.200
This is the model number of these appliance.

22:11.200 --> 22:13.200
What are the OCR part of that?

22:13.200 --> 22:19.520
Are you kind of taking the image as is and pulling the letters out?

22:19.520 --> 22:26.000
You know, however they are, are you trying to reorient that kind of as a, you know, ortho-rectification

22:26.000 --> 22:29.200
process so that it's straight?

22:29.200 --> 22:34.760
So we, so we leverage a lot of like the existing frameworks for OCR because OCR is just about

22:34.760 --> 22:37.920
taking some image, getting text out of it.

22:37.920 --> 22:44.360
So we can do, you can do a lot of, like a lot of data augmentation that happens in this,

22:44.360 --> 22:48.520
so these type of models have done a lot of data pre-processing to get ready.

22:48.520 --> 22:53.400
So they're very good at, you know, at, at extracts and text, it's not straight.

22:53.400 --> 22:54.400
Yeah.

22:54.400 --> 22:55.400
Yeah.

22:55.400 --> 22:57.440
So we don't have to worry about that part that part.

22:57.440 --> 23:01.680
So the biggest, so in this case, the biggest effort is like building a model that is

23:01.680 --> 23:07.280
going to do the, the classification right because obviously like OCR just gives you

23:07.280 --> 23:12.520
random text and so, and in the case of Cian, these numbers, they might occur in different

23:12.520 --> 23:15.760
places, might be like some of the tops, some of the buttons.

23:15.760 --> 23:20.720
And then the OCR might give you groups, but those groups could be like very disjointed.

23:20.720 --> 23:24.720
So how do you combine this information into something that makes sense?

23:24.720 --> 23:32.160
That's where our focus is on, less on the OCR, more like making sense of the OCR output.

23:32.160 --> 23:39.080
And are you using as a feature in that part of the problem, the actual text that might

23:39.080 --> 23:44.520
say, you know, model no or serial no, or are you kind of ignoring that and just looking

23:44.520 --> 23:48.400
at trying to do it based on the number itself?

23:48.400 --> 23:50.640
So we do it based on the whole text.

23:50.640 --> 23:55.040
So from the whole text, we would basically classify, we say, okay, we kept by the Cian

23:55.040 --> 24:00.520
number, what bits of this text represent the Cian number.

24:00.520 --> 24:03.600
And we do that by training a model that does that.

24:03.600 --> 24:09.520
So implicitly, the model is using, is using all of this like, you know, implicit information

24:09.520 --> 24:16.880
around like where this is a like Cian number on top, behind before and so on.

24:16.880 --> 24:20.720
We don't explicitly do it, but I think it's implicitly different.

24:20.720 --> 24:24.000
It's a fun problem.

24:24.000 --> 24:27.400
There were some other use cases that you walked through?

24:27.400 --> 24:28.400
Oh, yeah.

24:28.400 --> 24:32.960
Well, my favorite is always, so when you talk, so going back to what I was seeing around

24:32.960 --> 24:40.000
like blending, what's real, what's virtual and having like this immersive environment,

24:40.000 --> 24:47.360
one of the key things that I'm very excited about is walk through like tutorials in AR.

24:47.360 --> 24:53.680
So you get a new appliance, you want to know how to get started, we can have an AR tutorial

24:53.680 --> 24:59.520
experience around that machine to tell you how to like use it, like it's broken and then

24:59.520 --> 25:04.880
you can have a troubleshooting walk through around that appliance.

25:04.880 --> 25:09.000
And so for me, that's like one of the most fun use case because it gives the power back

25:09.000 --> 25:10.800
to the customer.

25:10.800 --> 25:16.080
And and one of the key problem in being able to do that is being able to identify the

25:16.080 --> 25:21.720
six-duff pose of whatever appliance you are looking at or whatever object you're looking

25:21.720 --> 25:28.200
at, we need to be able to know what is translation, rotation, so we can then impose data like

25:28.200 --> 25:30.800
virtual data around it.

25:30.800 --> 25:36.440
So I talk a bit about how we do six-duff pose estimation for objects like that.

25:36.440 --> 25:40.680
And elaborate on on what that means because I saw that in a presentation, you know, you

25:40.680 --> 25:45.640
get right as 60 and it's like 60, what it was, the six.

25:45.640 --> 25:50.440
Six-duff, well, six-duff, a six degrees of freedom.

25:50.440 --> 25:57.880
So three for translation, three for rotation, so that makes six variables that you have

25:57.880 --> 26:03.600
to estimate in order to like get a post-transform for where your object is respect to your camera.

26:03.600 --> 26:06.520
Oh, got it, got it, got it, got it.

26:06.520 --> 26:12.000
So you've got your you're located in an object in 3D space and it's got some rotation and

26:12.000 --> 26:14.480
that engine is next position.

26:14.480 --> 26:15.480
Yeah.

26:15.480 --> 26:16.480
Okay.

26:16.480 --> 26:23.040
So at any point in time, we are trying to figure out what are the six values so that you can

26:23.040 --> 26:28.560
build a transform from that and then you can use it to impose your digital content around

26:28.560 --> 26:29.560
it.

26:29.560 --> 26:30.560
Okay.

26:30.560 --> 26:31.560
Yeah.

26:31.560 --> 26:37.640
So maybe what are the challenging parts of that problem?

26:37.640 --> 26:44.280
And it's one of, even today, I think for the past, as far as I can remember, people have

26:44.280 --> 26:47.840
always been working on post-destimation.

26:47.840 --> 26:53.200
Even DCI-CDPR, there are so many papers around that because it's a very hard problem to solve.

26:53.200 --> 26:57.000
For us, it's even harder because you are actually not looking at the image, you are looking at

26:57.000 --> 26:58.160
a video.

26:58.160 --> 27:03.800
So your camera is changing and you expect that post to be correct across multiple frames.

27:03.800 --> 27:07.760
So for us, we use a lot of, we use a combination of techniques.

27:07.760 --> 27:16.880
So at DCI-CDPR, you see a lot of work around pure MLB techniques where you can just give

27:16.880 --> 27:22.520
an image to a model and it will tell you what are those six values or it will tell you

27:22.520 --> 27:24.360
what are the key points.

27:24.360 --> 27:33.160
So if you have the key points of your object, you can recover the post-information.

27:33.160 --> 27:37.960
So what we saw with those technologies that are not nearly accurate enough, like typically

27:37.960 --> 27:44.800
you have like a 10 degrees margin of error, which is for us not good enough because we are

27:44.800 --> 27:50.840
going to be superimposing content on top of this in AR so that we need like super accurate

27:50.840 --> 27:52.440
post.

27:52.440 --> 27:59.880
So what we end up doing is we have a combination of both like getting some MLB techniques to

27:59.880 --> 28:04.680
identify key points or even just asking users to select those key points.

28:04.680 --> 28:10.000
And then we focus on like from that initial post, how can we refine it further to really

28:10.000 --> 28:14.160
fit the edges of what is currently in the image.

28:14.160 --> 28:20.400
And so these are called region based approaches where it's looking at what is a background, what

28:20.400 --> 28:25.760
is the foreground, and how can we find a post that best differentiate between the two.

28:25.760 --> 28:30.080
So those are very accurate for this type of use case.

28:30.080 --> 28:31.080
Okay.

28:31.080 --> 28:37.960
It sounds like in this case a big part of the way you improve your results is by kind of

28:37.960 --> 28:41.480
manipulating the user interface so that you can get more information with that.

28:41.480 --> 28:42.480
Exactly.

28:42.480 --> 28:46.200
Overly burdensome on the part of the person is using it.

28:46.200 --> 28:47.200
Yeah.

28:47.200 --> 28:51.000
So that's so we try because accuracy is so important for us.

28:51.000 --> 28:56.840
And if we need some user input, we need to leverage some user input to get to that level

28:56.840 --> 28:57.840
of accuracy.

28:57.840 --> 29:00.360
And that's something that we are happy to do.

29:00.360 --> 29:05.840
And in a sense, it also helps us collect more data so that we can also improve the accuracy

29:05.840 --> 29:06.840
of our models.

29:06.840 --> 29:12.800
Have you seen many products out on the market that are kind of putting this kind of AR in

29:12.800 --> 29:14.040
the hands of the consumer?

29:14.040 --> 29:19.000
Like you mentioned this training use case for washing machine or something.

29:19.000 --> 29:20.240
Is anyone doing that?

29:20.240 --> 29:24.320
I haven't seen anyone doing that.

29:24.320 --> 29:30.240
So I think like streaming is in a very good position because we have like an existing

29:30.240 --> 29:34.760
customer base who are actually asking for these kind of features because they see that

29:34.760 --> 29:35.760
as a big need.

29:35.760 --> 29:40.200
And so that's kind of driving the problems you are working on.

29:40.200 --> 29:43.560
So I think I haven't seen it out there yet.

29:43.560 --> 29:47.080
So maybe people are doing working on that like behind the scenes.

29:47.080 --> 29:53.080
I was just going to say, I've seen a lot of demos of it in a kind of industrial type

29:53.080 --> 30:02.920
of use case like at one of the Microsoft conferences a year ago, the CIO, Sachin

30:02.920 --> 30:11.360
Adela demonstrated a kind of remote repair, you know, a worker goes into a telecom closet,

30:11.360 --> 30:14.920
you know, out in the middle of nowhere and they're trying to figure out how to, you know,

30:14.920 --> 30:17.880
get the 5G working again or something.

30:17.880 --> 30:25.640
There's a, you know, they're supported by someone who's pushing screens, overlay screens

30:25.640 --> 30:29.120
on top of what they're seeing telling you a lot of fixed stuff.

30:29.120 --> 30:35.360
The closest I've seen to it in something, consumery is I don't know how it's kind of

30:35.360 --> 30:38.880
prosumer slash low end enterprise.

30:38.880 --> 30:42.000
And it's actual, I think it's real as opposed to a demo.

30:42.000 --> 30:50.720
But there's a networking company ubiquity networks that makes these again, kind of prosumer

30:50.720 --> 30:56.760
networking gear and their latest line of switches in some of their promotional materials.

30:56.760 --> 31:04.120
They're showing you kind of holding your phone over the, these switches and they're

31:04.120 --> 31:07.840
showing you like what's connected to what in the different ports.

31:07.840 --> 31:08.840
Yes.

31:08.840 --> 31:13.720
But that's probably the only example I've seen of that actually in the wild.

31:13.720 --> 31:14.720
Yeah.

31:14.720 --> 31:15.720
It's a pretty cool.

31:15.720 --> 31:16.720
Yeah.

31:16.720 --> 31:17.720
Definitely.

31:17.720 --> 31:18.720
Yeah.

31:18.720 --> 31:24.280
And I've seen obviously like AI Kit has some demos around like being able to, if you

31:24.280 --> 31:29.520
have like a Lego set and then you, you're able to kind of recognize that and then in

31:29.520 --> 31:33.120
people some 3D content around that.

31:33.120 --> 31:41.240
So there's definitely been some use some demos around this type of use cases.

31:41.240 --> 31:45.200
So when we mentioned Microsoft, I was just thinking, oh, it has a depth sensor.

31:45.200 --> 31:47.200
That would make our life easier.

31:47.200 --> 31:48.200
Yes.

31:48.200 --> 31:49.200
Yes.

31:49.200 --> 31:50.200
Yes.

31:50.200 --> 31:51.200
Yes.

31:51.200 --> 31:57.240
That's why I can't wait for more their sensing to come to phones, but assuming just a phone.

31:57.240 --> 32:01.840
And so you're only able to use things that are commonly available on phones.

32:01.840 --> 32:02.840
Yeah.

32:02.840 --> 32:03.840
Yeah.

32:03.840 --> 32:08.360
So definitely the world where what is going to happen is that like on phones that don't

32:08.360 --> 32:13.160
have the sensing, we have these techniques on phones that do have extra capabilities, then

32:13.160 --> 32:15.520
we can offer.

32:15.520 --> 32:23.280
We can use that data that's sensor data to even push further the accuracy of our methods.

32:23.280 --> 32:26.640
So the more devices of those like the better.

32:26.640 --> 32:30.800
What are some of the other big challenges that you see in the space?

32:30.800 --> 32:31.800
In our space.

32:31.800 --> 32:32.800
Okay.

32:32.800 --> 32:33.800
So one of the big.

32:33.800 --> 32:41.160
So at the end of representation towards the end, I mentioned spatial AI and the yeah, and

32:41.160 --> 32:43.240
what it meant for us.

32:43.240 --> 32:48.960
And for us, it just means that going beyond trying to understand these tasks at the granular

32:48.960 --> 32:51.840
level, but like look at a big picture.

32:51.840 --> 32:56.520
So at the end of the day, what you want to know is in a video, in a collaboration between

32:56.520 --> 32:58.720
two people, what is the problem?

32:58.720 --> 32:59.720
What is the solution?

32:59.720 --> 33:03.880
We're talking about, and how do you capture this information in a way that you can then

33:03.880 --> 33:06.560
reproduce it somewhere else?

33:06.560 --> 33:09.960
And so yeah, you have this idea of both chat boards.

33:09.960 --> 33:14.680
You have this everywhere, like in the banking systems where you have an issue, and then they

33:14.680 --> 33:18.480
can just kind of based on your question, they will suggest you something.

33:18.480 --> 33:23.360
And if they can't fix that, then it will escalate to a human being.

33:23.360 --> 33:29.400
And so I take that AI and I add special AI on top of that, the sense that now this AI

33:29.400 --> 33:35.440
both actually they understand, they can look at what you're looking at, and they can make

33:35.440 --> 33:36.880
sense of it.

33:36.880 --> 33:44.720
So ultimately, can we create agents that are able to solve, like to help you solve these

33:44.720 --> 33:49.280
issues that you're needing to actually talk to a human being?

33:49.280 --> 33:56.120
That's kind of like the big moonshot problem that you are looking at, beyond this individual

33:56.120 --> 33:57.560
computer vision test.

33:57.560 --> 33:59.560
Yeah.

33:59.560 --> 34:07.480
Something as an example there, the, you know, the cliched scene we see in the movies where

34:07.480 --> 34:11.840
the bomb squad comes in and defuse the bomb, you know, someone's holding their phone

34:11.840 --> 34:16.120
over it in the system, as opposed to some remote expert saying, oh, it's definitely the

34:16.120 --> 34:17.120
yellow cable.

34:17.120 --> 34:18.120
Clip the yellow cable.

34:18.120 --> 34:19.120
Exactly.

34:19.120 --> 34:20.120
Exactly.

34:20.120 --> 34:21.120
Yeah.

34:21.120 --> 34:23.120
The bit I'd be right.

34:23.120 --> 34:24.120
Yeah.

34:24.120 --> 34:25.120
The bit I'd be right.

34:25.120 --> 34:28.120
Yeah.

34:28.120 --> 34:31.800
But yeah, that's what some of the things that we are thinking of now is how do we start

34:31.800 --> 34:37.600
getting there by maybe like starting by making sense of the video data, like can we start

34:37.600 --> 34:42.960
clustering videos by, you know, the functionality.

34:42.960 --> 34:48.680
So not just similarities in a sense, yeah, they have similar scenes, but they're trying

34:48.680 --> 34:51.120
to solve similar problems.

34:51.120 --> 34:56.400
So how can you capture that out of a video and be able to start creating these clusters,

34:56.400 --> 35:01.360
which can be very useful for just like training for a company to understand, like what are

35:01.360 --> 35:07.720
the most common issues and capture, you know, what are the solutions to these issues so

35:07.720 --> 35:11.240
that they can replicate that in other environments.

35:11.240 --> 35:12.240
Mm-hmm.

35:12.240 --> 35:18.840
Sounds a little bit like you're describing kind of going back to expert systems types of

35:18.840 --> 35:24.440
approaches for, you know, but coupling that with information that we've pulled using

35:24.440 --> 35:29.800
modern AI techniques, you know, neural networks and the like, am I hearing that correctly?

35:29.800 --> 35:30.800
Yeah.

35:30.800 --> 35:32.800
Special expert systems.

35:32.800 --> 35:35.000
Special expert systems.

35:35.000 --> 35:40.400
The our mission at stream is making the world expertise more accessible.

35:40.400 --> 35:41.400
Okay.

35:41.400 --> 35:46.240
And so how do you open access to like, you know, to everyone.

35:46.240 --> 35:49.320
So that's that's part of how we get to that vision.

35:49.320 --> 35:50.320
Okay.

35:50.320 --> 35:51.320
Yeah.

35:51.320 --> 35:56.880
Do you follow the computer vision space more broadly or is it what do you find interesting

35:56.880 --> 35:59.040
at CVPR this time around?

35:59.040 --> 36:00.040
Oh, man.

36:00.040 --> 36:06.720
Like obviously I'm always kind of looking at more like problems that we are already looking

36:06.720 --> 36:07.720
at.

36:07.720 --> 36:15.800
So one of the work that I read recently was normal assisted death estimation.

36:15.800 --> 36:19.520
And so in the sense, you're not looking at one image to create to predict death, you're

36:19.520 --> 36:25.160
actually looking at a couple of images, which for me is really interesting because then

36:25.160 --> 36:29.840
you just get as far information and you make sure that whatever your outputting is consistent

36:29.840 --> 36:32.040
across multiple views.

36:32.040 --> 36:39.880
And so for me, that's like the stereo problem that you were just using AI, which is good

36:39.880 --> 36:45.520
because you can now scale across like different type of scenes and you can be robustness

36:45.520 --> 36:50.880
to like letting issues and so on, which you can't do with like traditional methods.

36:50.880 --> 36:52.880
So that's that's very interesting.

36:52.880 --> 36:57.080
I'm still like reading through, there's a lot of content at CVPR so I'm still working

36:57.080 --> 37:05.440
my way through everything that was presented, but definitely that's one of them.

37:05.440 --> 37:09.760
And just in a general sense, I'm very excited about machine learning with no without needing

37:09.760 --> 37:16.280
global training data, I think that's a big thing people are still looking at that.

37:16.280 --> 37:22.240
So I'm less interested in being able to like generate photo realistic images, there's

37:22.240 --> 37:28.160
a lot of focus on that, but I really think of like machine learning and computer vision.

37:28.160 --> 37:33.480
And you look at all of these models, they need tons and tons of data to train them.

37:33.480 --> 37:40.480
So there's definitely a new line, some line of work around how do you basically do self-training

37:40.480 --> 37:46.800
systems, how do you do that without needing like a bunch of data, data, so that's something

37:46.800 --> 37:50.320
that I'm very interested about.

37:50.320 --> 37:51.320
Nice.

37:51.320 --> 37:56.400
The workshop that you presented at, is there something that you've been participating

37:56.400 --> 37:59.920
in for a long time or is it new at CVPR?

37:59.920 --> 38:03.160
So this is the second year, the workshop is taking place.

38:03.160 --> 38:10.440
So last year I was attending and we were presenting this scene generation from images work that

38:10.440 --> 38:11.440
I mentioned earlier.

38:11.440 --> 38:18.760
We presented that at the workshop and I was invited for this year keynote, for one of

38:18.760 --> 38:19.760
the keynote this year.

38:19.760 --> 38:25.360
So I was definitely like, I was glad to have that from the talk about our work.

38:25.360 --> 38:26.360
Yeah.

38:26.360 --> 38:28.720
It's a great workshop.

38:28.720 --> 38:32.880
I kind of encourage everyone to go and check the other speakers because there was a lot

38:32.880 --> 38:38.480
of work around just like the vision of AR VR, what's stopping us from, you know, from

38:38.480 --> 38:45.160
everyone doing conferences, you know, through VR, like, what are we not doing that today?

38:45.160 --> 38:46.160
And things like that.

38:46.160 --> 38:48.120
So that was the thing you also.

38:48.120 --> 38:49.120
Interesting.

38:49.120 --> 38:50.120
Yeah.

38:50.120 --> 38:57.000
I did organize the conference a few years ago and we had a presenter on AR VR and kind

38:57.000 --> 39:01.680
of some of the implications for machine learning, but it was super early.

39:01.680 --> 39:06.400
Like, there was nothing really happening.

39:06.400 --> 39:11.480
And so it's exciting for me to hear that, you know, the field is advancing and, you

39:11.480 --> 39:18.880
know, there's a particularly, and is it, is it fair to say that AR in particular because

39:18.880 --> 39:23.000
the scenes need to be understood more.

39:23.000 --> 39:27.520
There's more implications for machine learning than VR, which tends to be more kind of

39:27.520 --> 39:28.520
generative and scripted.

39:28.520 --> 39:29.520
Yeah.

39:29.520 --> 39:30.520
Yeah.

39:30.520 --> 39:31.520
Definitely.

39:31.520 --> 39:37.360
So in a lot of these tasks in augmented reality, we just, we need machine learning because

39:37.360 --> 39:42.000
no one is going to, there's no time to sit down and try and do like, like, mind reality,

39:42.000 --> 39:45.360
take a real scene and then create a digital.

39:45.360 --> 39:49.520
So if you're going to do like physics interact, if you're going to interact with the environment,

39:49.520 --> 39:52.680
then you need to understand that environment and that's computer vision.

39:52.680 --> 39:57.640
That's what computer is all about, taking reality and making sense of it.

39:57.640 --> 40:01.960
So it's very crucial to augmenting your reality.

40:01.960 --> 40:05.800
Well, Flora, thanks so much for taking the time to share what you're up to.

40:05.800 --> 40:06.800
Very cool.

40:06.800 --> 40:07.800
Thank you.

40:07.800 --> 40:10.000
Always happy to talk about these things.

40:10.000 --> 40:11.000
Yeah.

40:11.000 --> 40:12.000
Thanks.

40:12.000 --> 40:13.000
Nice.

40:13.000 --> 40:14.000
Thank you.

40:14.000 --> 40:19.840
All right, everyone, that's our show for today.

40:19.840 --> 40:25.640
For more information on today's show, visit twomolai.com slash shows.

40:25.640 --> 40:29.240
As always, thanks so much for listening and catch you next time.

