Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, I'm excited to present a series of interviews exploring the emerging
field of differential privacy.
Over the course of the week, we'll dig into some of the very exciting research and application
work happening right now in the field.
In this, our first episode of the series, I'm joined by Aaron Roth, Associate Professor
of Computer Science and Information Science at the University of Pennsylvania.
Aaron is first and foremost a theoretician, and our conversation starts with him helping
us understand the context and theory behind differential privacy, a research area he
was fortunate to begin pursuing at its very inception.
We explore the application of differential privacy to machine learning systems, including
the costs and challenges of doing so.
Aaron discusses as well quite a few examples of differential privacy in action, including
work being done at Google, Apple, and the US Census Bureau, along with some of the major
research directions currently being pursued in the field.
Thanks to George and Partners for their continued support of the podcast and for sponsoring
this series.
George and Partners is a venture capital firm that invests in growth stage companies in
the US and Canada.
Post investment, George and works closely with portfolio companies to accelerate adoption
of key technologies, including machine learning and differential privacy.
To help their portfolio companies provide privacy guarantees to their customers, George
and recently launched its first software product, Epsilon, which is a differentially private
machine learning solution.
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,
but if you find this field interesting, I'd encourage you to visit the differential
privacy resource center they've set up at gptrs.vc slash twimmel ai.
And now on to the show.
Alright everyone, I am on the line with Aaron Roth.
Aaron is Associate Professor of Computer Science and Information Science at the University of
Pennsylvania.
Aaron, welcome to this week in machine learning and AI.
Thanks, I'm glad to be here.
Why don't we get started by having you tell us a little bit about your background and
how you got involved in the machine learning field?
Sure, so I'm a computer scientist and I guess I would describe my background as really
coming from theoretical computer science.
So as someone who sits down and tries to understand things by thinking mathematically and proving
theorems and the way I came to machine learning in general is well from a background in learning
theory and in particular the flavor of problems that I've studied both sort of historically
and now have to do with the way that machine learning as a technology interacts with things
that you might more normally think of as societal concerns.
So things like privacy, things like fairness and things that maybe more typically an economist
would think about like how do machine learning algorithms work in strategic situations?
You're also very involved in the work happening around the application of differential privacy
to machine learning.
How did you get started down that route?
Well, so I started my PhD in 2006 which is the same year that the first paper on differential
privacy was published by Cynthia Dwork and Cobine Sim and Adam Smith and Frank McCherry.
And so this was a very new topic at the time that I was just starting to think about
research and it struck me as timely and important and at the same time when I was just starting
to think about it, not much was known.
So it was sort of at the sweet spot for PhD thesis where you can study an important problem
and have a lot of impact without having to do anything to clever.
Well, maybe a good place to start in our exploration of differential privacy and machine learning
would be to have you define differential privacy and tell us a little bit about the context
in which it was created.
Sure, so privacy has a relatively long history in computer science, but until people started
thinking about differential privacy, what people meant when they said privacy, with some
kind of syntactic constraints on what the output of a computation could look like.
And it turns out these kinds of syntactic privacy guarantees don't have a strong meaning
in terms of privacy and there was sort of a cat and mouse game in which people would
attempt to share data sets with some kind of privacy protections and then some clever
person would come around and figure out how to get around those privacy protections and
this would iterate.
Can you give us an example of those types of syntactic constraints and a little bit of
how that cat and mouse game evolved?
Sure, yeah, so maybe the simplest thing that you might imagine is you might think to
yourself, well, if I just remove any identifying attributes from a data set.
So for example, if I've got a data set of medical records, if I just remove things like
name and zip code and maybe a few others, that'll anonymize the data and it'll be safe
to release the data set in the clear.
And unfortunately, that turns out not to be the case.
So there's a bunch of examples of the sort, but maybe the first one was a demonstration
by Latanya Sweeney.
At the time she was a PhD student at MIT, now she's a professor at Harvard.
And the state of Massachusetts had released a supposedly anonymized data set of patient
medical records.
So it didn't have people's names attached.
But what Latanya figured out was that there was another data set that was publicly available.
That was the voter registration records in Cambridge, Massachusetts.
And when you've got two data sets and you know something about an individual, for example,
Latanya knew that the governor of Massachusetts at the time, William Weld, lived in Cambridge.
I knew a few other things about him.
You can basically cross-reference these two data sets and reattach names.
So that's sort of the simplest example for why attempting to remove identifying attributes
doesn't work.
You know, it seems like a good idea in isolation, but in the real world, there's all of this
information out there that you can attempt to cross-reference with existing data sets.
I think another example along those lines was when Netflix released their anonymized
recommendation data set for I think it was the Netflix prize, someone or a set of people
cross-reference that to IMDB and found that they were able to de-anonymize a pretty significant
portion of those records.
Yeah, that was another high-profile example using a more sophisticated technique that
was done by Arvind Narayanaan, who is now a professor at Princeton and Videlish Mottikov,
who's at Cornell Tech.
And that was another example where names were removed, so all that was made available was
sort of a big data set where for each person now identified by supposedly anonymous numeric
identifier, all you saw about them were which movies they watched, what they rated them,
and approximately when they watched them.
And as you say, by cross-referencing this data set with IMDB, they were able to reattach
names.
So differential privacy came about kind of in the wake of the broader realization of
the failure of anonymization, it sounds like?
Exactly.
So I think the key insight that the creators of differential privacy had was that, you
know, if you want to speak rigorously about what someone can infer about an individual
and given what they observe about an algorithm, you shouldn't be trying to put syntactic
constraints on the output of that algorithm, but rather you should be putting information
theoretic constraints on the algorithmic process itself on the computation.
And so that's exactly what differential privacy does.
What differential privacy means, informally, it's a constraint on an algorithm, and it
says small changes in the input to an algorithm, for example, adding or removing the record
of a single individual should have only small changes on the distribution of outputs that
the algorithm produces.
So if I remove your record entirely from a data set, that shouldn't cause any observable
event, anything that the algorithm might do when computing on the data set to become too
much more or less likely.
And this kind of probabilistic information theoretic constraint turns out to have a really strong
semantics about what, you know, an adversary, an attacker can infer about your data.
One of the subtleties in the way you describe that is that different, and maybe it's not
so subtle, but differential privacy isn't an algorithm itself, it's a constraint on an
algorithm.
Am I hearing that correctly?
That's right.
Yeah.
So differential privacy is a property that an algorithm might or might not have, any particular
algorithm might be differentially private or might not be.
And a lot of the definition of the constraint, it's relatively simple, but a lot of the
science that goes into studying differential privacy asks the question, you know, if I've
tied my hands in this way in what kinds of algorithms I can use, what type of algorithm
tasks can I still perform?
And as you say, differential privacy is a family of, you know, it's a constraint, it's
not an algorithm.
So to show that you can do something subject to differential privacy, it's sufficient
to exhibit a differentially private algorithm that does that thing, but to prove lower bounds
to show that for some problem, you cannot solve it subject to differential privacy.
That's another matter entirely.
You have to write down a mathematical proof that no algorithm could solve it subject to
the constraint.
Hmm.
So how does that relatively simple sounding constraint lead to the benefits of privacy?
I guess most basically it provides a guarantee of plausible deniability.
So let me, maybe to make things less abstract, let me give you an example of a very simple,
intuitive, differentially private algorithm.
So suppose that I want to conduct a poll and I want to find out amongst all of the citizens
in Philadelphia, how many of them voted for Donald Trump in the last election?
Okay.
You know, the most straightforward way to do this is I would call up some random sample
of individuals on the phone.
And I'd ask them, you know, did you vote for Donald Trump in the 2016 election?
And I'd write down their answer on a piece of paper.
And then when I was all done, when I'd called, you know, sufficiently many people, I'd
tally up their answers.
I'd find that, you know, 15% of people voted for Donald Trump.
I'd do some statistics to attach error bars to that.
And then I'd publish the, uh, publish the statistic.
Okay.
Now, note that like the thing that I wanted to find out was just this property of the
distribution, what fraction of people voted for Donald Trump.
But like incidentally along the way, I accumulated this large body of potentially sensitive
information, what individual people voted for, who, who individual people voted for.
Right.
Okay.
So think about the following alternative polling procedure, which turns out to be differentially
private and will allow us to figure out the distributional statistic we care about,
like fraction of people voted for Donald Trump without having to collect, you know, sensitive
information about individuals.
Okay.
So I'm going to, again, call up some large collection of people.
But now instead of telling them to, instead of instructing them to tell me whether they
voted for Donald Trump, I'll tell them the following thing, I'll say flip a coin.
If it comes up heads, tell me truthfully whether you voted for Donald Trump or not.
If it comes up tails, though, tell me a random answer by which I mean, flip the coin
again and tell me Trump if it came up heads and tell me not Trump if it came up tails.
So and importantly, you're not going to tell me how the coin came out.
Right.
Okay.
So I hear Trump or not Trump, but I don't know how your coins were realized.
So I don't know whether you're telling me the truth or whether you're lying simply because
of how the coins were flipped.
Okay.
So on the one hand, you now have a significant amount of plausible deniability.
Okay.
If all of a sudden the country collapses into a police state and my polling records are
subpoenaed and you're called in front of the truth commission and that's suggested that
you voted in one way or the other, you can reasonably say, no, I didn't.
The answer that you're reading was simply the result of a coin flip because I was following
this randomized protocol.
Right.
Okay.
So you have plausible deniability.
That's the guarantee of privacy.
On the other hand, even though I cannot form strong beliefs about what any individual's
data looks like, in aggregate, I can, I can, I can figure out to a high degree of accuracy
what fraction of people voted for Donald Trump.
And that's because I understand the process by which noise was injected into these measurements.
Okay.
In aggregate, the noise behaves in a very structured easy to understand way and I can subtract
that noise out of the average, even though I cannot figure out for any individual who
they voted for.
That strikes me as somewhat counterintuitive in that, you know, thinking about the coin flip,
you know, almost half of your data could be corrupted or quarter, maybe a quarter exactly.
So if you think about it, right, it's a simple calculation.
If I know that for every individual, three quarters of the time they're telling me the truth
and a quarter of the time they're telling me the opposite of the truth, then say 15%
of people in Philadelphia truly voted for Donald Trump, you know, I can write out on a piece
of paper what percentage of people in expectation should report to me under this protocol voted
for Donald Trump.
Right.
And because I've got a bunch of independent samples, the actual number of people who end up
telling me this will be highly concentrated around its expectation.
Right.
And therefore, you know, what I need to do as the pollster is work backwards.
What I know is the number of people who actually told me they voted for Donald Trump, but
because I can compute this one-to-one mapping between the number of people who really did
and the number of people I expect to tell me this, I can invert the mapping and figure
out what the underlying population statistic was to a high degree of accuracy.
Okay.
Sounds like, you know, the method you're describing could be used both prior to data collection
by instructing your respondents to follow this algorithm, or if you're an organization
collecting data, you could collect the actual responses and then apply this algorithm
before publishing the data.
That's right.
So the interaction that I described to you was what's called the local model of differential
privacy and, you know, in this scenario, people's privacy was protected even from the pollster.
Okay.
He never wrote down the data.
And of course, if that's what you want, then you have to apply these protections when the
data is being gathered.
Right.
But as you say, if I'm an organization that's already got the data, I can apply privacy
protections to the output of my computations to anything I release.
So then obviously, privacy isn't protected from me, the organization that has the data.
I've already got the data in the clear, but assuming there are no data breaches and
no subpoenas, differential privacy can guarantee something about what any outside observer
can learn about individuals in the data set by observing the outcomes of computations.
And you're careful to say guarantee something.
What exactly does differential privacy guarantee and what does it not guarantee?
Yeah.
So one thing that has been alighted in this discussion is that there's a quantitative
parameter that comes with differential privacy called Epsilon.
But what differential privacy guarantees formally is that no event becomes much more likely
if your data is in the data set compared to if it's not.
But what does much more likely mean, that means more likely by some multiplicative factor
that depends on this parameter Epsilon.
So suppose Epsilon is small.
This is a pretty good guarantee because it says whatever it is that you're worried about
from the perspective of privacy, whatever harm you're worried that the use of your data
will cause to befall you, that harm is even though I don't know what you're worried
about, I can promise you that the risk, the increased risk of that harm befalling you can
be bounded as a function of this parameter Epsilon.
But of course, if Epsilon is very big, then that's not a very strong guarantee.
So it doesn't really mean anything if someone tells you that their algorithm is differentially
private, unless they also tell you what this privacy parameter is.
As the privacy parameter goes to infinity, differential privacy is no constraint at all,
as it goes to zero, it becomes a very strong constraint.
Going back to this fundamental constraint, it's that within the bounds of Epsilon, adding
or removing an individual piece of data won't change the statistics of your overall distribution.
Is that correct?
That's right.
It won't change the behavior of your algorithm.
So adding or removing a single data point won't cause your algorithm to do something that
is very different from the point of view of an observer.
And so how does, how do we get from there to, again, the notion of privacy?
And I guess you were setting that up in talking about the plausible deniability example.
Yes.
So there's a couple of interpretations of differential privacy.
And I can walk through a few of them.
So one is this plausible deniability guarantee.
So if someone accuses you of having some particular data record, and the piece of evidence
they have at their disposal is the output of a differentially private computation, you
have plausible deniability in the sense that you can say that the piece of evidence they
have in hand is essentially, as likely to have been observed, again, up to this factor
relating to the privacy parameter, if your data had been different.
Okay.
Another way of saying this is, you know, suppose they've got some prior belief about what
your data point looks like.
And then they observe the output of a computation.
And so they update their belief to some posterior belief using Bayes' rule, for example.
And what differential privacy promises is that they would have performed nearly the
same update, and therefore had nearly the same belief had your data been different if
we hold the rest of the data set fixed.
Another interpretation is this model of harm, and you can view this as a sort of utilitarian
definition of privacy, you know, like how hard is it for me to convince you to contribute
your data to my data set, if I'm going to use it for some statistical analysis?
Well, why wouldn't you want to contribute your data?
There's any number of reasons, and I might not know what they are, but, you know, presumably
you're worried that as the result of the use of your data, something bad is going to happen
to you.
Maybe your health insurance premiums are going to rise, or maybe you're going to start
getting drunk phone calls during dinner.
And what differential privacy can promise is no matter what event that you're worried
about, and I really mean no matter what event, so we can talk about the probability that
your health insurance premiums rise for the probability that you get spam phone calls.
This event will be, will have almost the same probability up to, again, this privacy parameter.
In the following two hypothetical worlds, in the one world, you don't contribute your data
to the computation in the other world you do, and everything else is held constant between
these two worlds.
That's the difference in differential privacy, right?
So if I look at the two different worlds that are identical, except for this one fact,
whether you contributed your data to my analysis or whether you did not, then the events that
you're worried about, whatever they are, become almost no more likely when you contribute
your data.
And does that interpretation, it sounds like that assumes some anonymization?
It doesn't assume anything, that follows sort of directly from the definition of differential
privacy, that if you like, that is the definition of differential privacy.
I guess I think I'm maybe I'm thinking of this in some perversely, but if I include
my data and my data, includes my phone number, how does differential privacy address that?
Oh, well, your data can include anything you like, including your phone number, but a
differentially private algorithm, certainly can't look at your data record and publish your
phone number.
Right.
And so is the idea that we're applying the coin flip, for example, to the publishing, you
know, maybe it would randomly spit out phone numbers or something like that?
Yeah, I mean, I think I'm getting stuck in a rat hole here, but so maybe one thing that's
useful to keep in mind, you know, you can try to write down a differentially private algorithm
for anything you like, but it's only for certain kinds of problems for which differentially
private algorithms are going to be able to do anything useful.
And those are our statistical problems, where the thing that you want to estimate is some
property of the underlying distribution, so it's very good for sort of machine learning
if I want to learn a classifier that on average makes correct predictions.
But if I want to learn what your phone number is, you know, it's all well and good that
I want to learn that, but by design, you know, there is no differentially private algorithm
that will give me a non trivial advantage over essentially random guessing.
Differential privacy isn't compatible with answering the kinds of questions that have
to do with just a single individual, and that's by design.
So that's a great segue to the applications of differential privacy in machine learning.
Can you maybe start by talking about the specific machine learning, you know, problems or examples
that differential privacy is, is trying to address in that application and maybe talk
through some of the specifics of how that's done?
Sure.
So there's a couple of things that you might want to do subject to differential privacy
when you're doing machine learning.
So one is that just that you might want to solve a single machine learning problem subject
to differential privacy.
So maybe you've got some, for example, supervised classification task, you've got some label
data you'd like to learn, you know, a support vector machine or a neural network of some
sort that will minimize some loss function, maybe my classification error, when I apply
it to new data.
Okay.
So that's the standard machine learning problem and differential privacy is extremely
amenable to this kind of problem, essentially, because well, there's several reasons,
but maybe the most fundamental reason is that this is inherently a statistical problem
in the sense that the thing that I already for reasons of overfitting wanted to avoid
when I'm solving a machine learning problem, depending too heavily on any single data
point, right?
So, so machine learning and privacy, they're sort of aligned in the sense that they're
both trying to learn facts about the distribution without overfitting to the particular data
set I have on hand overfitting is closely related to privacy violations and we can talk
more about that.
The connection turns out to go both ways.
Another thing that you might want to do is more ambitious, you might want to construct
a synthetic data set by which I mean, like rather than solving a single machine learning
problem, maybe you want to construct a data set that looks like the real data set with
respect to some large number of statistics or machine learning algorithms, but it's
nevertheless differentially private, so I can construct this synthetic data set with
a private algorithm and then release it to the world and then other people can go and
try to apply their machine learning algorithms to this synthetic data set, the hope being
that insights that they derive classifiers they train on the synthetic data set would
also work well on the real data set.
And then finally, and this relates back to the connections between differential privacy
and overfitting, it might be that you don't care about privacy at all in your application,
but you know, you want to, for example, repeatedly test hypotheses or fit different kinds
of machine learning models while reusing the same holdout set over and over again, maybe
because it's too expensive to get more data.
Now normally this would be a really bad idea, sort of the standard test train methodology
and machine learning entirely falls apart, basically, if you reuse the holdout set as part
of an iterative procedure by which you're choosing your model.
But as it turns out, when you perform your entire pipeline of statistical analyses subject
to the guarantees of differential privacy, you can't overfit, so if you can be accurate.
In sample, you can be guaranteed that you learn an accurate model out of sample, even
if you've repeatedly reused the data.
And when you say perform your entire pipeline subject to the guarantees of differential
privacy, does that mean you are enforcing those constraints at every individual step
or the, you know, relative to the inputs and outputs of the entire pipeline?
It means that, you know, everything you shouldn't have touched the data in any way using a non-differentially
private algorithm.
So differential privacy has a very nice property that it composes.
If I have two algorithms, you know, the first one is epsilon one, differentially private,
and the second one is epsilon two, differentially private.
Then if I run the first one and based on the outcome decide what I want to do at the second
step and then I run the second algorithm, this whole computation is in aggregate, epsilon
one plus epsilon two, differentially private.
So, you know, if at every decision point I'm making my decision about what to do next
as a function only of differentially private access to the data, then you've got these
strong safety guarantees about overfitting.
So maybe to make it a little bit more concrete, I've heard a few examples of scenarios that
pop up in the machine learning world, and I'm vaguely recalling them, maybe you can provide
a bit more detail, but one of them was an example of a, it's almost, it was like reverse
engineering and object detector to determine whether an individual or a face detector to
determine whether an individual face was in the training data set.
Okay, so you're talking about maybe an attack on a classifier that wasn't trained in
a differentially private way and the kind of thing that you might, the kind of reason
why you might want to have privacy guarantees when you're training a learning algorithm
that they don't come for free, if I'm.
Exactly, exactly.
Yes, so I think there's a couple of these kinds of attacks now, and I don't know the details
of the specific one you're referring to, but you might have, you know, a priori before
you started worrying about privacy, I think that, you know, okay, maybe if I'm, you know,
releasing individual data records like in the Netflix example, I have to worry about privacy
violations, but if I'm, if I'm just training a classifier, how could, how could releasing
only the model parameters, you know, the weights in the neural network possibly violate
anyone's privacy?
Exactly.
Yeah, and that intuition is wrong and you're describing the kind of attacks that, that
show that it's wrong, but the basic, I think, premise underlying these attacks is that when
you train a model without differential privacy, it'll tend to overfit a little bit, even
if, even if this doesn't really affect the model performance.
But what you find is that, you know, when you try to classify a face, for example, that
was in the training data set, the model will tend to have higher confidence in its classification
than when you try to classify an example that was not in the training data set.
Okay.
And it's sort of natural that you would expect that because the model got to update itself
on the examples in the training set.
Right.
And by taking advantage of that, you can therefore figure out whether a particular person's
picture was in the training data set or not by examining what is the confidence in
the model's prediction.
Okay.
Are other examples that come to mind of, you know, where the notion of distributing
models or, you know, more generally, I guess, statistical aggregates can fail to be privacy
preserving?
So, maybe the most obvious example is sort of naive training of a support vector machine.
So the simplest way to, you know, the most concise way for me to describe to you the model
of a trans support vector machine is by communicating to you the support vectors.
But the support vectors are just examples from the training data set.
So the most straightforward way to distribute a trained support vector machine is to transmit
to you some number of examples from the training data set in the clear.
So that's sort of obvious once you realize it, but, you know, it is one of the things that
you might not have thought of initially when you, if you're coming from this position,
but things that represent just aggregate statistics like trained models shouldn't be
disclosed.
Okay.
Okay.
What I'm hearing is, you know, I guess granted some in some classes of problem, maybe privacy
isn't, you know, the greatest concern.
But if differential privacy were free and easy to apply everywhere, you know, I might
do that. What are some of the, you know, the issues or costs of applying differential
privacy that come up when trying to apply it in the machine learning context?
Yeah.
So it definitely doesn't come for free.
And I think there's costs of two sorts.
So the first is sort of, maybe it's difficult to just acquire the expertise to implement
all of these things at the moment, you know, a lot of the knowledge about differential
privacy at the moment is contained in hard to read, you know, academic papers.
There's not that many people who are trained to read these things.
So if you're addressed some random company, it can be hard to even get started.
But maybe the more fundamental thing is that although differential privacy is compatible
with machine learning by which I mean, in principle, anything that you can, any statistical
problem that is susceptible to machine learning absent differential privacy is, you know,
can also be solved with machine learning with differential privacy guarantees.
The cost is that if you want strong differential privacy guarantees, you'll need more data.
And if you want the privacy parameter to be small, this thing that governs the strength
of the privacy guarantee, you might need a lot more data to achieve the same accuracy
guarantees.
So as a result, it can be a tough sell to apply privacy technologies in a setting in which,
you know, developers and researchers already have direct access to the data set because
the data set's not getting any bigger.
So if yesterday you could do your statistical analyses on your data set of 10,000 records
and today I say, now you've got to do it subject to differential privacy.
The accuracy of your analyses is going to degrade the place in which I've seen it successfully
deployed overcoming sort of this kind of objection in industry has been in settings where because
of privacy concerns, developers previously didn't have access to the data at all.
And they're now, you know, once privacy, strong privacy guarantees are built in, are
able to start collecting it, so it's a much easier sell if the privacy guarantees are
going to give you access to new data sets that previously you couldn't touch because
of privacy concerns, then it is to sort of add on X post when previously you were able
to ignore privacy or the cost of privacy will tend to come in the form of less accuracy
in terms of your, you know, costification error, for example.
Okay, some of the known uses of differential privacy are places like Google, Apple, Microsoft,
the US Census Bureau, are you familiar with those examples and what they're doing and
can you talk about the ones that you are?
Sure, so Google and Apple are both using differential privacy in the local model, this model
of the polling agency trying to figure out how many people voted for Donald Trump in the
example that I gave.
Okay, so both of them are collecting statistics, you know, Google in your Chrome web browser
and Apple on your iPhone in which the privacy protections are added on device.
And what they're trying to do are collect simple statistics, population-wide averages.
So if you look at the Apple paper, for example, they're collecting statistics on, you know,
like what are the most frequently used emojis in different countries or for different websites?
What fraction of people like it when videos automatically play as opposed to requiring
some human intervention?
So they're trying to collect population-wide statistics that allow them to improve user
experience or improve things like predictive type and things like that.
The US Census is doing something more ambitious and the US Census collects all the data in
the clear.
So they're not trying to protect the privacy of your data from the census, they're collecting
it.
Instead they're using differential privacy in the centralized model.
But they release huge amounts of statistical information.
So you can go on existing census websites and figure out the answers to questions like,
how many people live in this collection of zip codes and work in this other collection
of zip codes?
Okay.
And they're going to continue releasing these large amounts of statistical information
about the US population, but for the 2020 census, they're going to release it subject
to differential privacy protections.
Interesting.
And so they're releasing not individual data records, but more of these statistical aggregates
subject to differential privacy.
That's right.
So in all of these applications, what's being released or statistics rather than actual
synthetic data sets.
Right.
As far as I know, I don't know the details of what the census plans to do.
And I'm not sure that's even been pinned down.
In an academic setting, I have a former student, Stephen Wu, who's now a postdoc at Microsoft
Research.
But when he was here, he worked with colleagues in the medical school, a professor named
Casey Green, to construct a differentially private synthetic medical data sets.
So medicine is a field that's got a big problem in that there's a lot of data and it's starting
to be susceptible to yielding all sorts of wonderful insights if we apply the latest machine
learning technologies to it.
But medicine is a domain where there are serious privacy concerns and legal regulations.
And so it's very difficult to share data sets.
Ideally, you'd like to share your data sets with other researchers, allow them to reproduce
the kinds of garments you did on the data, combine data sets.
And so what Stephen and his colleagues did was it gave sort of a proof of concept that
you could use techniques for privately training neural networks and combine those with techniques
for generating synthetic data for training gans that would let you create synthetic data
sets that you could release to the public, but that would look like the original data
set with respect to a very large class of machine learning algorithms.
So you could train the algorithms on the synthetic data and then find that when you evaluated
them on the real data, they did pretty well.
Okay.
Interesting.
Interesting.
This is sort of the more ambitious kind of technology that I think, as far as I know,
has so far been the domain of only academic research, but maybe in the coming years
will find industrial and government applications.
Can you maybe share a brief word on the current research areas and around differential privacy
and machine learning?
So there are many in diverse and there are people focused on more practical problems
and more theoretical problems.
I myself, you know, just through my natural activities tend to focus on sort of the more
theoretical problems.
But I think that it remains, even though it's an old problem, it remains an important
and unsolved problem to figure out sort of practical ways to generate useful synthetic
data for large collections of tasks.
We know, we've known for a while, you know, since my PhD thesis that these kinds of problems
are possible in principle, there are inefficient information theoretic, you know, kinds of algorithms
that accomplish them, but we don't get to have practical algorithms.
I think that remains very important.
You know, another important direction is that a lot of the academic literature to date
has really focused on the central model of data privacy where there's a trusted database
curator who gathers all the data in the clear in part because you can do more stuff in
that model.
So it's attractive to study it.
But as we've seen differential privacy move from theory to practice, you know, two
days, it's two largest scale deployments that Google and Apple are both in the local model.
And there's a lot of things, I think, that we understand in the central model of differential
privacy that we still don't understand in the local model.
And that's too bad because the local model's turning out to be very important.
So I think understanding basic tasks in the local model continues to be very important.
And I mentioned briefly this sort of research agenda showing that you can use differential
privacy to avoid false discovery and overfitting even when you don't care about privacy.
I think this is one of the most general promising directions to, you know, attack the statistical
crisis in science.
But so far, we're just in early days, you know, we understand the basic sort of proof
of concept kinds of results for why techniques from differential privacy might be useful.
But we're a pretty far way off from having practical tools that, you know, working data
scientists can use to prevent overfitting and with practically sized data sets.
Okay.
Great.
We will share a link to your website so folks can take a look at some of your recent
work and publications in this area.
Before we close up, would you, is there anything else that you'd like to share with the
audience?
No, thanks for, thanks for listening and, you know, I'm always happy to hear about interesting
new applications of differential privacy.
So feel free to send me emails when I was just getting started and writing my PhD thesis.
You know, all of this was a theoretical abstraction and it's been great fun, you know, hearing
about and consulting with companies that are actually putting this into practice.
So it's been a fun ride and I like to hear what's going on out there.
Fantastic.
Well, thanks so much, Aaron.
Thank you.
All right, everyone, that's our show for today.
For more information on Aaron or any of the topics covered in this episode, head on over
to twimla.com slash talk slash one thirty two.
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit
their differential privacy resource center at gptrs.vc slash twimla for more information
on the field and what they're up to.
And of course, thank you so much for listening and catch you next time.
