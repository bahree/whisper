1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:21,280
people doing interesting things in machine learning and artificial intelligence. I'm your

3
00:00:21,280 --> 00:00:26,440
host Sam Charrington. This week on the podcast, we're featuring a series of conversations

4
00:00:26,440 --> 00:00:32,600
from the AWS Reinvent Conference in Las Vegas. I had a great time at this event, getting caught

5
00:00:32,600 --> 00:00:38,520
up on the new machine learning and AI products and services announced by AWS and its partners.

6
00:00:38,520 --> 00:00:42,760
If you missed the news coming out of Reinvent and want to know more about what one of the

7
00:00:42,760 --> 00:00:48,320
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble Talk

8
00:00:48,320 --> 00:00:54,360
number 83. A roundtable discussion I held with Dave McCrory and Lawrence Chung. We cover

9
00:00:54,360 --> 00:01:01,080
all of AWS's most important news, including the new SageMaker, DeepLens, Recognition Video,

10
00:01:01,080 --> 00:01:07,720
Transcription, Alexa for Business, Greengrass ML inference and more. This week, we're also

11
00:01:07,720 --> 00:01:13,560
running a special listener appreciation contest to celebrate hitting 1 million listens here on

12
00:01:13,560 --> 00:01:21,400
the podcast and to thank you all for being so awesome. Tweet to us using the hashtag Twimble 1

13
00:01:21,400 --> 00:01:27,960
Mill to enter. Every entry gets a fly Twimble 1 Mill sticker, plus a chance to win a limited

14
00:01:27,960 --> 00:01:33,640
run t-shirt commemorating the occasion. We'll be digging into the Magic Twimble swag bag and

15
00:01:33,640 --> 00:01:38,440
giving away some other mystery prizes as well, so you definitely don't want to miss this.

16
00:01:39,320 --> 00:01:46,920
If you're not on Twitter or you want more ways to enter, visit Twimbleai.com slash Twimble 1 Mill

17
00:01:46,920 --> 00:01:53,000
for the full rundown. Before we dive in, I'd like to thank our good friends over at Intel

18
00:01:53,000 --> 00:01:58,760
Nirvana for their sponsorship of this podcast and our reinvent series. One of the big announcements

19
00:01:58,760 --> 00:02:04,520
at reinvent this year was the release of Amazon DeepLens, a fully programmable, deep learning

20
00:02:04,520 --> 00:02:10,520
enabled wireless video camera designed to help developers learn and experiment with AI

21
00:02:10,520 --> 00:02:17,080
both in the cloud and at the edge. DeepLens is powered by an Intel Atom X5 processor, which

22
00:02:17,080 --> 00:02:23,160
delivers up to 100 gigaflops of processing power to onboard applications. To learn more about

23
00:02:23,160 --> 00:02:29,960
DeepLens and the other interesting things Intel's been up to in the AI space, check out intel Nirvana.com.

24
00:02:31,000 --> 00:02:37,960
Okay, in this episode, we're joined by Chris Adzima, senior information analyst for the Washington

25
00:02:37,960 --> 00:02:44,520
County Sheriff's Department. Chris joined me to discuss his very interesting use case for AWS

26
00:02:44,520 --> 00:02:51,080
recognition, their image object detection system, which he uses to help his agency identify criminal

27
00:02:51,080 --> 00:02:57,720
suspects in the Portland area by matching photos to mug shots. We discuss how bias affects the

28
00:02:57,720 --> 00:03:02,920
work he's doing and how they try to remove it from their process, as well as what his next steps

29
00:03:02,920 --> 00:03:07,960
are with the recognition service. This was a pretty interesting discussion and I'm sure you'll enjoy

30
00:03:07,960 --> 00:03:10,520
it. And now on to the show.

31
00:03:18,360 --> 00:03:23,800
All right, everyone. I am here at the AWS Reunment Conference and I've got the pleasure of being

32
00:03:23,800 --> 00:03:29,000
seated with Chris Adzima. Chris is a senior information systems analyst with the Washington

33
00:03:29,000 --> 00:03:33,480
County Sheriff's Office and Washington County, if you don't know is in the Portland area. Is that

34
00:03:33,480 --> 00:03:38,600
right, Chris? Chris, welcome to this week in machine learning and AI. Well, thanks for having me.

35
00:03:38,600 --> 00:03:43,160
It's great to have you here. I am looking forward to learning a little bit about the talk that you

36
00:03:43,160 --> 00:03:48,920
gave yesterday. What was the topic of that one? So it was called the unusual suspects and essentially,

37
00:03:49,640 --> 00:03:58,600
it is about how I used AWS recognition to attempt to identify unknown suspects who committed

38
00:03:58,600 --> 00:04:06,760
crimes. And what I used is previous booking photos or mug shots to upload the recognition

39
00:04:06,760 --> 00:04:13,880
created collection. And now we can search surveillance footage or pictures taken by eyewitnesses

40
00:04:14,600 --> 00:04:19,480
against those booking photos and attempt to identify those people based on if they had stayed

41
00:04:19,480 --> 00:04:24,280
in our jail already. Oh, wow. Oh, wow. Before we get into that, why don't we spend a little bit of

42
00:04:24,280 --> 00:04:29,560
time having you share with us a little bit about your background and how you got interested in

43
00:04:29,560 --> 00:04:34,920
the machine learning and AI stuff to begin with? Sure. I have a pretty extensive background,

44
00:04:34,920 --> 00:04:41,880
goes back about 15 years now where I started at eBay working in fraud detection. And there we

45
00:04:42,520 --> 00:04:47,160
utilized machine learning. I didn't ever make any models because I'm not a data scientist,

46
00:04:47,160 --> 00:04:52,280
but I utilized the models to detect things like people taking over accounts or posting fraudulent

47
00:04:52,280 --> 00:05:01,400
items. Okay. So back then it was very interesting to me. And I kind of grew up with that as part of

48
00:05:01,400 --> 00:05:08,760
my background. When I moved into public service, I started working at the sheriff's office. I wanted

49
00:05:08,760 --> 00:05:15,720
to bring some of that interest into the sheriff's office. And so I started looking at ways we could

50
00:05:15,720 --> 00:05:22,840
utilize machine learning or anything like that in order to help our deputies do their job.

51
00:05:23,320 --> 00:05:28,120
And so one of the biggest areas of opportunity was the fact that we have all of these

52
00:05:28,920 --> 00:05:33,960
videos, all of these pictures of people who committed crimes that we can't identify. And we also

53
00:05:33,960 --> 00:05:38,920
have all of these pictures of people who have been booked into our jail. And I thought there's

54
00:05:38,920 --> 00:05:46,280
got to be a way that we can marry those two situations and come up with a very interesting way to

55
00:05:46,280 --> 00:05:51,560
solve that problem. And so that's kind of where how I grew up and got interested into it.

56
00:05:51,560 --> 00:05:58,760
Okay. Great. Is this the first machine learning project that you've done at the sheriff's office?

57
00:05:58,760 --> 00:06:06,120
Yes. This is the first one. We started looking into other machine learning things and we're

58
00:06:06,120 --> 00:06:12,760
slowly getting into other things like crime detection. And by crime detection, I don't mean

59
00:06:13,960 --> 00:06:18,840
somebody, one specific person committing a crime. I mean, minority report, not like minority report.

60
00:06:20,120 --> 00:06:26,920
More specifically, we noticed that as crime happens in one area, it migrates to another area

61
00:06:26,920 --> 00:06:32,840
over time. And we can utilize that data to predict that there's a possibility that a crime is going

62
00:06:32,840 --> 00:06:39,080
to occur in an area subsequent or bordering. And therefore, we can send out patrols to that area.

63
00:06:39,080 --> 00:06:43,080
Okay. Not to attempt to catch the person, but to attempt to prevent the crime from happening

64
00:06:43,080 --> 00:06:48,040
in the first place. Because that's honestly what our goal is, right? We don't want to catch the

65
00:06:48,040 --> 00:06:51,400
people doing the crime. We want the people to not do the crime in the first place. So that's what

66
00:06:51,400 --> 00:06:56,200
we're hopes are for that. So an application like that is essentially like, you know, how you got this

67
00:06:57,160 --> 00:07:02,200
you know, fleet of cars and officers and other vehicles. Like how do you deploy them?

68
00:07:02,200 --> 00:07:06,840
Right. Right. How do you how do you put them in the in the areas that are going to get the most

69
00:07:08,280 --> 00:07:11,160
prevention for where they are? Okay.

70
00:07:12,680 --> 00:07:16,840
How long have you been at the sheriff's office? Just about two years now. In fact,

71
00:07:16,840 --> 00:07:22,600
is it the 29th? It's the 29th. Yes, it'll be two years tomorrow. Oh wow. Wow.

72
00:07:22,600 --> 00:07:32,440
That'd be anniversary. Yes. Did the I guess I'm curious. Did the you know, so you you used

73
00:07:32,440 --> 00:07:41,080
Amazon's recognition product to do this first application to what degree did you know, having access

74
00:07:41,080 --> 00:07:47,480
to this via an API as opposed to needing to build your own models contribute to your ability to

75
00:07:47,480 --> 00:07:54,520
actually do it? 100%. I guess I kind of do the answer to that question, but I'm not a data scientist.

76
00:07:54,520 --> 00:08:00,920
I don't know how any of the stuff works in the back end. And I'm okay with that. What I what it

77
00:08:00,920 --> 00:08:07,160
really did was allow me somebody who's very big into coding. Some I know I know how to code.

78
00:08:07,880 --> 00:08:11,880
Now I'm able to utilize these these machine learnings, these deep learning techniques

79
00:08:11,880 --> 00:08:19,000
to help me do my job. And I didn't have to engage a data scientist. I didn't have to even

80
00:08:19,000 --> 00:08:25,000
know the model. I just was able to tie into an API and utilize their model that they already

81
00:08:25,000 --> 00:08:30,760
built. And it has been immensely beneficial because of that. Can you talk a little bit about your

82
00:08:30,760 --> 00:08:37,800
process for developing the system? Yeah. So in my talk yesterday, I talked about how quickly

83
00:08:37,800 --> 00:08:42,680
this happened. This was right around this time last year at Reinventing Announced Recognition.

84
00:08:43,480 --> 00:08:52,840
And by mid-November, sorry mid-December, I had had a prototype up and running already,

85
00:08:52,840 --> 00:08:57,640
basically. That's incredible. Yeah, it was really, really amazing. Once I found out

86
00:08:57,640 --> 00:09:03,560
that recognition existed and I did a quick, I think it only took me about a day to go in and

87
00:09:03,560 --> 00:09:08,920
read the documentation because it's not an overly complicated API. I think there's only

88
00:09:08,920 --> 00:09:14,200
something like 20 calls in total. So once I read the documentation, I was able to go in and

89
00:09:14,200 --> 00:09:19,720
realize that, okay, the first thing I need to do is get my mug shots available to recognition,

90
00:09:19,720 --> 00:09:27,960
uploaded 300,000 mug shots into S3. I did it manually. I didn't realize there was an API I could

91
00:09:27,960 --> 00:09:34,120
use. This was when I was really green with AWS. So literally I used the web for my drag and draw

92
00:09:34,120 --> 00:09:42,280
300,000 increments of a thousand out into the web form. Okay. So that took me about five days.

93
00:09:43,400 --> 00:09:48,840
Had I realized now that then that what I know now about the API for S3, I would have just written

94
00:09:48,840 --> 00:09:53,880
the script, uploaded them that way. That's how we keep our mug shots up to date now. I have a daily

95
00:09:53,880 --> 00:10:00,120
script that runs, throws them into S3 and indexes. But at least in mug shots were digital.

96
00:10:00,120 --> 00:10:05,560
In the beginning, in the beginning, it was a manual process. Once I got those up into S3, though,

97
00:10:06,280 --> 00:10:12,040
I did write a script because then I knew that I could wrote a script to loop through all 300,000

98
00:10:12,040 --> 00:10:20,600
indexed them into recognition, which is just one simple API call. Once they were indexed,

99
00:10:20,600 --> 00:10:25,480
I essentially had everything I needed to do to get up and running. Once you have that collection,

100
00:10:25,480 --> 00:10:34,680
you can just do an API call to their search faces API. And you send it the binary data from the

101
00:10:34,680 --> 00:10:39,720
image that you want to search from into the collection and it returns the results.

102
00:10:40,440 --> 00:10:43,800
And what are the images that you want to search from? What are those input images?

103
00:10:43,800 --> 00:10:54,840
So they come from multiple different ways. The web form that I created will post them to S3 and

104
00:10:54,840 --> 00:11:00,680
then S3 will do the search that way. I'll then delete that one because we don't save any of the

105
00:11:00,680 --> 00:11:09,480
images we search for. But then I also have a mobile app. Those will go directly into the API

106
00:11:09,480 --> 00:11:15,000
that way. The source images, though, are these like, is this surveillance video or is this like

107
00:11:15,000 --> 00:11:20,200
an officer on a street with your mobile app taking a picture of someone or in my talk yesterday,

108
00:11:20,200 --> 00:11:24,920
I should three different examples. One of them was from a surveillance. I don't know if people

109
00:11:24,920 --> 00:11:30,040
know this or not, but there are surveillance cameras on those little self-check out things at most

110
00:11:30,040 --> 00:11:36,200
of those department stores. And that was the surveillance camera from that. Yeah. So that was a good

111
00:11:36,200 --> 00:11:41,720
hit. Meaning like the credit card swiper thing? Well, not the self-check out kiosks.

112
00:11:41,720 --> 00:11:46,760
You know, we're going to scan it yourself. So in that case, the guy was scanning the items,

113
00:11:46,760 --> 00:11:50,520
but he didn't actually pay. He just put them back in the cart and walked out, made it seem like

114
00:11:50,520 --> 00:11:56,200
it was legitimate. Okay. So we got a surveillance shot off of that. The second example I gave

115
00:11:56,200 --> 00:12:02,280
was actually a cell phone picture. And I witnessed took a picture of somebody with their cell phone

116
00:12:02,280 --> 00:12:08,120
and then called the deputies. The deputies showed up. And the deputy took a picture of the cell phone

117
00:12:08,120 --> 00:12:13,560
picture. So it was like a second generation with all the glare on the phone and all.

118
00:12:13,560 --> 00:12:19,400
But recognition still found the face and ran that. And then the third example, which is the example

119
00:12:19,400 --> 00:12:27,000
that I am most pleased with, was an artist's rendition from an eyewitness. So a sketch.

120
00:12:27,000 --> 00:12:33,000
Wow. We ran a sketch through recognition and it pulled back a legitimate result. Wow.

121
00:12:33,000 --> 00:12:39,000
So when you say, well, we're running, we're running anything we can. If it has a face on it,

122
00:12:39,000 --> 00:12:46,440
it doesn't matter if it's a drawing or an image of an image of an image, we're trying

123
00:12:46,440 --> 00:12:56,120
because we want to identify these people. And how do you characterize the performance of

124
00:12:56,120 --> 00:13:01,400
recognition for your use case? Do you have specific ways you think about that?

125
00:13:02,200 --> 00:13:09,880
So as far as metrics go not yet because it's still really early. It's only been a year since I

126
00:13:09,880 --> 00:13:14,520
even started. And it's only been about six months that the deputies have been using it in full force.

127
00:13:15,720 --> 00:13:21,400
But I can tell you anecdotally, every single deputy, every single law enforcement officer who's

128
00:13:21,400 --> 00:13:30,120
used this has said, wow, I can't believe that how good this is doing. And it's also difficult

129
00:13:30,120 --> 00:13:35,880
to quantify it because I'd love to get up here and say, you know, this tool has led to

130
00:13:35,880 --> 00:13:41,880
X amount of convictions, right? But it doesn't quite work like that, right? The deputies use it as

131
00:13:41,880 --> 00:13:47,960
a tool. It's not their one and only thing they do. So while a deputy might put something into

132
00:13:47,960 --> 00:13:55,480
the tool, get a result. It may or may not be the person in the picture, but it could lead them

133
00:13:55,480 --> 00:14:01,560
to a relative. Like with facial features being similar, you might get somebody's father return

134
00:14:01,560 --> 00:14:07,320
in the result. You go talk to the father, realize it was the son who was the person you were looking

135
00:14:07,320 --> 00:14:14,760
for. So is that a good hit for us? I don't know how to quantify that yet. But I can tell you that all

136
00:14:14,760 --> 00:14:21,960
of these situations have occurred. So it's working well. Everybody likes it. And I think that's really

137
00:14:21,960 --> 00:14:26,920
all that matters when I'm putting a tool in the hands of a deputy, saving them time so that they can

138
00:14:26,920 --> 00:14:33,480
go out there and keep ourselves safe. When you're thinking about it from the perspective of a developer

139
00:14:33,480 --> 00:14:39,720
or a technologist using this service, how about the performance of recognition itself? Have you?

140
00:14:39,720 --> 00:14:45,000
And you know, I guess you would have to like, you put in an image, I guess you would want to know

141
00:14:45,000 --> 00:14:51,080
like what percent of the time, you know, is the image found when it's in the database. Do you

142
00:14:51,080 --> 00:14:56,600
track that? We did some benchmarking at the very beginning about that because we had images that

143
00:14:56,600 --> 00:15:02,600
were not mug shots, but we knew those people in those images had been booked into our jail at some

144
00:15:02,600 --> 00:15:11,160
point. So we created, I don't want to call it a blind case study, but I didn't know which of the

145
00:15:11,160 --> 00:15:14,440
group had been in our jail and which of the group had not been in our jail. And I didn't know the

146
00:15:14,440 --> 00:15:21,480
identities of the people. Okay. So one of the deputies sent me over about a hundred images.

147
00:15:22,280 --> 00:15:28,200
And of the images of the people who had been in our jail, I was able to correctly identify 75

148
00:15:28,200 --> 00:15:35,560
percent of them. And of the people who had not been in our jail, but results were returned,

149
00:15:35,560 --> 00:15:42,280
I was able to say that if 50 percent of those people, I was able to say, no, we don't have a good

150
00:15:42,280 --> 00:15:46,680
idea for these people. So about 50 percent of the people who had never been in our jail, I said,

151
00:15:46,680 --> 00:15:52,280
oh, I think it's this person and it actually was. But I think when it comes down to a 75 percent

152
00:15:52,280 --> 00:15:56,920
accuracy saying that there's a 75 percent chance that if they had been booked in our jail,

153
00:15:56,920 --> 00:16:02,040
I'm going to be able to tell you who they are. That's tremendous. So yeah, I think it's,

154
00:16:03,080 --> 00:16:08,920
you know, it's like what you said previously, it's a tool, right? And if you think about it in

155
00:16:08,920 --> 00:16:15,480
that context and not like recognition is going to replace policing, then it's helpful.

156
00:16:15,480 --> 00:16:20,920
Yeah. And exactly. And giving the deputies as many tools as possible to go out and do their

157
00:16:20,920 --> 00:16:30,760
jobs is exactly what my job is. And so I don't necessarily want to do their job for them,

158
00:16:30,760 --> 00:16:36,920
although I'd love to do their job. But I don't necessarily want to do their job for them. I want

159
00:16:36,920 --> 00:16:43,240
them to have a tool that they can not have to be sitting in behind a computer doing research,

160
00:16:43,240 --> 00:16:52,040
looking at thousands of mug shots that possibly fit. Instead, they get a likely choice of five or

161
00:16:52,040 --> 00:16:57,800
six and they can go out and take action on those follow up on leads as opposed to being tied to

162
00:16:57,800 --> 00:17:06,280
a computer. Are there things that you need to think about as a developer when using these APIs

163
00:17:06,280 --> 00:17:10,840
that are different from, you know, the traditional ways you might develop applications?

164
00:17:10,840 --> 00:17:19,160
As far as being in law enforcement, yes. I have to concern myself with different levels

165
00:17:19,160 --> 00:17:28,760
of data classifications. So for instance, I, there's laws that prevent us from sending images

166
00:17:28,760 --> 00:17:35,800
of juveniles over the internet without certain security in place. And while those securities do

167
00:17:35,800 --> 00:17:41,480
exist with Amazon, I have to ensure that they are in place before I send a juvenile's image. So

168
00:17:42,120 --> 00:17:46,520
in that case, we just have a policy that we don't run recognition off of juvenile images.

169
00:17:47,080 --> 00:17:53,640
And as far as just a regular old developer that I have been for a while now thinking about these

170
00:17:53,640 --> 00:18:00,920
APIs, I have to think about them in a different way because kind of like what we were just talking

171
00:18:00,920 --> 00:18:08,600
about the effectiveness of the API. Usually if you were to come with me and say this API is 75%

172
00:18:08,600 --> 00:18:15,160
effective, I wouldn't even look at because that's just not beneficial. But when you think about

173
00:18:15,160 --> 00:18:22,840
what it's doing and the fact that 75% beneficial is way better than 10% beneficial, which is where

174
00:18:22,840 --> 00:18:32,440
we were at with scraping through mug shots using a Boolean search essentially. It's way beneficial.

175
00:18:32,440 --> 00:18:35,720
And that's something that you have to wrap your head around as a developer saying that

176
00:18:35,720 --> 00:18:41,480
don't just throw it out because you want to see in high 90s. But elaborate a little bit on

177
00:18:42,200 --> 00:18:45,720
what you were doing before. Previous to having this recognition

178
00:18:47,000 --> 00:18:52,600
product in place, we had a web form where you would go in and you would type in demographics

179
00:18:52,600 --> 00:19:01,560
like I'm looking for a mail between the ages of 4050, height of 5 foot 10, black hair and

180
00:19:01,560 --> 00:19:05,880
down the list of anything that you can search for. And then it would then search for all of the

181
00:19:05,880 --> 00:19:12,360
inmates that meet those requirements and return thousands of mug shots that you would have to

182
00:19:12,360 --> 00:19:21,640
then search through by your eyes to determine. And where those that metadata was all manually

183
00:19:21,640 --> 00:19:27,960
exactly. So it's all based on when you get booked in, ask you questions, how old are you?

184
00:19:28,680 --> 00:19:32,200
Sometimes we have all that information because they give us a driver's license and we can put

185
00:19:32,200 --> 00:19:37,960
all that in. But in some cases, it's all based on what the inmates tell us. And I can tell you

186
00:19:37,960 --> 00:19:42,200
that they foreshore give us different names. They foreshore give us different birthdays.

187
00:19:42,200 --> 00:19:51,000
So yeah, it was very, while it was somewhat accurate, it definitely did have a tilt to it

188
00:19:51,000 --> 00:19:58,200
because of incorrect entered data. Okay. And so you get this list of your thousand search results

189
00:19:58,200 --> 00:20:02,600
and you know, for whatever reason, I guess it's TV. But I'm imagining like these big binders of

190
00:20:02,600 --> 00:20:08,920
mug shots, you're not doing it like that anymore. No, not anymore. Obviously before we had a website

191
00:20:08,920 --> 00:20:13,560
that the deputies could utilize, that's exactly how they did it. And there was binders full of

192
00:20:13,560 --> 00:20:19,480
mug shots. I think it's still like that on TV most years. On TV. And when they when they give

193
00:20:20,280 --> 00:20:26,760
what we call a six pack or a lineup card, it's still right there laid on the table where the

194
00:20:26,760 --> 00:20:32,280
guy points at the picture. Yeah, it's on the computer now too. Oh, really? Yeah. So, but

195
00:20:32,600 --> 00:20:36,600
while it's very different than that, it's actually very similar at the same time where yeah,

196
00:20:36,600 --> 00:20:41,960
you're just slogging through picture after picture. And anybody who knows anything about like

197
00:20:41,960 --> 00:20:47,000
assembly line hypnosis or anything like that, after the fifth page, you're not seeing those

198
00:20:47,000 --> 00:20:52,920
faces like you should anymore. You know, you're just, you know, hoping that there's some huge

199
00:20:52,920 --> 00:20:57,320
like mole on the guy's forehead or something because aside from that, you're not going to see it.

200
00:20:57,320 --> 00:21:07,240
Which is where the you mentioned 10% accuracy rate before where that comes from. So, when I think

201
00:21:07,240 --> 00:21:14,760
about the, you know, some of the issues associated with associated with applying machine learning

202
00:21:14,760 --> 00:21:21,400
in AI to one of the things that comes up for me pretty quickly or some of the, you know, incidents

203
00:21:21,400 --> 00:21:27,880
and stories we've seen around just the, you know, the bias that's injected into these types of

204
00:21:27,880 --> 00:21:34,520
algorithms and how that can kind of play out in, you know, these which are, you know, you know,

205
00:21:34,520 --> 00:21:40,520
really critical, you know, situations that involve human lives. Like, what kind of, how do you

206
00:21:40,520 --> 00:21:46,200
think about that and what kind of experience have you had with those kinds of issues? So, luckily,

207
00:21:46,200 --> 00:21:52,920
we, I'm happy to work for a sheriff who is very conscious about bias in policing.

208
00:21:54,040 --> 00:22:01,560
Well, before any of the other counties around this were thinking about that, he was thinking about

209
00:22:01,560 --> 00:22:06,840
it and running reports to make sure that his deputies weren't biased. I'm happy to say that

210
00:22:06,840 --> 00:22:16,200
all reports look good for us. So, as far as personal or very specific to what I've done,

211
00:22:16,200 --> 00:22:21,480
I don't see it, unfortunately, to say that like, this is how I would handle it or how we do him.

212
00:22:21,480 --> 00:22:29,320
Fortunately, we don't see it, but I do keep that in mind because these algorithms do tend to

213
00:22:29,320 --> 00:22:35,080
to have a little bit of bias. And that bias is based on when the algorithms are trained,

214
00:22:35,080 --> 00:22:40,680
they tend to be trained by the developers. So, the developers are going to, you know,

215
00:22:40,680 --> 00:22:47,800
whoever developed it is going to have their ethnicity, their gender more likely to be detected

216
00:22:47,800 --> 00:22:54,280
than others. So, that's what's the specific effects things that you've seen in your work with

217
00:22:54,280 --> 00:23:02,440
recognition? I haven't necessarily seen a lot of, I don't know, what I would call a misidentity

218
00:23:02,440 --> 00:23:07,800
based on something that I would say that, oh, this is definitely because this person is one

219
00:23:07,800 --> 00:23:12,600
ethnicity, but it's biased towards another ethnicity. I really haven't seen that much.

220
00:23:12,600 --> 00:23:17,320
That being said, we have come across a couple of situations where I would run,

221
00:23:17,320 --> 00:23:23,160
we would run an image through recognition. And it would give us a ethnicity, the results would

222
00:23:23,160 --> 00:23:27,480
be an ethnicity that was contrary to what we personally thought just looking at the image.

223
00:23:27,480 --> 00:23:36,600
But in that case, we still haven't had a situation where we were either proven wrong or right

224
00:23:36,600 --> 00:23:39,720
based on our previous thing, because we haven't identified that person yet.

225
00:23:41,240 --> 00:23:47,240
Do you find that is it equally as effective in identifying women

226
00:23:47,240 --> 00:23:53,960
in, you know, out of the pictures that you've identified or equally as effective in identifying,

227
00:23:53,960 --> 00:23:59,000
you know, all ethnicities or are there kind of shifts and biases in that part?

228
00:23:59,000 --> 00:24:06,040
I would say that when I see results that, for instance, if a result set has multiple

229
00:24:06,040 --> 00:24:13,320
genders in that result set, I will see males put in with a picture of a female, more than I will

230
00:24:13,320 --> 00:24:20,680
see females returned when I give it a picture of a male. So it definitely is more,

231
00:24:20,680 --> 00:24:27,000
I hate to use the word bias because, you know, I don't know how to say this except for that,

232
00:24:27,000 --> 00:24:31,960
when it's all set and done, because it's just a tool that we've created.

233
00:24:31,960 --> 00:24:37,800
If it gives us five results of people who are completely off, then we just kind of

234
00:24:37,800 --> 00:24:46,200
sweep those results aside. But yes, I know you can bias in some ways as a kind of a loaded term,

235
00:24:46,200 --> 00:24:53,880
and especially in my field. But statistically, if we kind of separate those kind of issues

236
00:24:53,880 --> 00:25:01,160
that you are seeing where it is more likely to return male pictures than female pictures,

237
00:25:01,160 --> 00:25:03,560
even when you give it a female picture, for example.

238
00:25:03,560 --> 00:25:10,680
Right. And I find that I've almost looked at those results and saw that the results return.

239
00:25:10,680 --> 00:25:15,960
So if I give recognition of female picture and it returns five results and two of them

240
00:25:15,960 --> 00:25:23,160
are males, I find that those males tend to have more generic features. You know, when you look at

241
00:25:23,160 --> 00:25:28,280
them, you don't see anything that stands out specifically. And so that makes me wonder if that those

242
00:25:28,280 --> 00:25:35,240
people's facial architecture is just simply generic in themselves. And that's why they're getting

243
00:25:35,240 --> 00:25:40,840
returned. I haven't done deep enough into looking at their specific facial analysis, which is one

244
00:25:40,840 --> 00:25:45,320
thing that recognition would allow me to do is I could send that one picture through and it could

245
00:25:45,320 --> 00:25:53,160
tell me what it's likelihood of it being male female, likelihood of it being one age group or

246
00:25:53,160 --> 00:25:59,720
another. I haven't really done that much of a deep dive into those outliers that I've seen

247
00:25:59,720 --> 00:26:04,200
to see if maybe that's what it is, maybe that recognition thinks that this picture of a male

248
00:26:04,200 --> 00:26:09,960
is actually a picture of a female and that's why it's being returned. But I can say that as far as

249
00:26:09,960 --> 00:26:17,720
age goes, I am a 35-year-old guy and every single time I run my picture through it, it thinks I'm 50.

250
00:26:21,800 --> 00:26:29,240
So there's that. And I think that if I maybe lost a beard and maybe didn't have as much

251
00:26:29,240 --> 00:26:40,120
little gray hairs, maybe it wouldn't think of me so old, but yeah, interesting. Are there kind of

252
00:26:40,120 --> 00:26:50,520
continuing on the, are there ways, I'm thinking the right way to get at this question? I'm curious

253
00:26:50,520 --> 00:26:56,840
about you're getting going back to the fact that recognition in a lot of ways is a black box for

254
00:26:56,840 --> 00:27:03,000
you, right? Are there things that you maybe kind of, you know, blind to as a developer that you

255
00:27:03,000 --> 00:27:09,800
might want to have more information about that you've run into? Well, obviously, I'm pretty much

256
00:27:09,800 --> 00:27:17,240
blind to the entire recognition of how it works. I send it faces, it sends me results and I have no

257
00:27:17,240 --> 00:27:27,400
idea how it gets to be, but I would like a way to a train recognition, if not, if they're just for

258
00:27:27,400 --> 00:27:34,040
me, you know, not having training my own collection, but train it by some sort of feedback loop,

259
00:27:34,040 --> 00:27:40,440
say that indeed this was a good hit, but indeed this wasn't a good hit, so that, you know, it can

260
00:27:40,440 --> 00:27:48,280
get smarter. I'd love to see that. I'd love to see a way for me to train it in different ways,

261
00:27:48,280 --> 00:27:55,320
because while I only use one slice of recognition, there are other bits that would be hugely

262
00:27:55,320 --> 00:28:00,760
beneficial to us if they worked in the way we needed them. A great example of that is tattoo

263
00:28:00,760 --> 00:28:09,160
recognition. We have, as well as a catalog of faces, we have a catalog of scars marks and tattoos.

264
00:28:09,160 --> 00:28:14,920
I've seen this on TV also. This is on TV as well. You know, on TV, they're able to say,

265
00:28:14,920 --> 00:28:19,720
oh, here's a picture of his skull tattoo, and here's him over here with his skull tattoo.

266
00:28:19,720 --> 00:28:31,080
Right. That doesn't exist in reality and be in recognition. It doesn't, the API isn't detailed

267
00:28:31,080 --> 00:28:36,520
enough to tell me this is a tattoo of a skull. It can say it's a tattoo, which is great,

268
00:28:36,520 --> 00:28:43,640
but it would be even better if I could take all of my pictures of tattoos, feed them into

269
00:28:44,840 --> 00:28:54,760
the collections in recognition, and then auto-tag those, so that we could have a more standardized

270
00:28:54,760 --> 00:29:01,880
list of tattoos. So if recognition saw a skull, it would always return it as the same spelling of

271
00:29:01,880 --> 00:29:09,880
skull as the same. It would generate some kind of taxonomy of tattoos. Somebody would say skull,

272
00:29:09,880 --> 00:29:15,320
somebody makes a crossbone, somebody may put an eye in there somewhere. So we don't have an easy

273
00:29:15,320 --> 00:29:24,440
way to textually search for if a victim comes in and says their attacker had a skull tattoo on

274
00:29:24,440 --> 00:29:32,280
their chest. We could, if we had recognition already auto-tagging these tattoos, I could go

275
00:29:32,280 --> 00:29:37,880
in search skull and get a list of everybody who has a skull tattoo on their chest that has been

276
00:29:37,880 --> 00:29:44,040
through our jail, possibly ideing them simply by knowing that they had a tattoo, and that would be

277
00:29:44,040 --> 00:29:51,240
obviously immensely, I can see how that'd be powerful, and I can tell you that I was watching a

278
00:29:51,240 --> 00:29:58,440
TV show a couple of days ago, and they took a picture of a guy's side of his head, and they said,

279
00:29:58,440 --> 00:30:05,000
oh, look at this guy's ear, ears are biometrically as identical to fingerprints, so we're just going

280
00:30:05,000 --> 00:30:11,400
to take a picture of his ear and run it through ear recognition. I mean, obviously if they could do

281
00:30:11,400 --> 00:30:17,560
that, I'd love recognition to do that. I brought that up because this is the, again, this is the

282
00:30:17,560 --> 00:30:25,160
thing I come across with the public is they assume we already can do this stuff. They assume, like,

283
00:30:25,160 --> 00:30:33,480
when I talk about recognition with the citizens in our area, half of them didn't realize we couldn't

284
00:30:33,480 --> 00:30:39,000
do facial recognition before. They assume that when we send a picture to the news and say, can you

285
00:30:39,000 --> 00:30:45,320
help me identify this person, that we've already done that part, right? Now luckily we have already

286
00:30:45,320 --> 00:30:53,160
done that part, but before this year, we just didn't exist, and it's pretty funny. In my mind,

287
00:30:53,160 --> 00:30:57,720
like, you know, in my mind, you're still going through these bindings of things, and the public

288
00:30:57,720 --> 00:31:02,920
thinks that you have a, you know, a minority report already established. Exactly. That's interesting.

289
00:31:02,920 --> 00:31:11,160
So that's another thing is I want to, I want to get us to the point where we are at where the

290
00:31:11,160 --> 00:31:16,760
citizens already think we are, right? So that when they watch TV, they watch a TV show like APB,

291
00:31:16,760 --> 00:31:22,280
and they see all of this stuff that doesn't exist yet, that I can at least say, well, I'm working

292
00:31:22,280 --> 00:31:28,360
towards it and getting us there. And that's what, that's my goal. And so what do you,

293
00:31:29,080 --> 00:31:36,200
what's next to get you there? Is it, you know, are you kind of waiting for, are you stuck waiting for

294
00:31:36,200 --> 00:31:41,400
recognition to build out all these features, or is this like inspiration and justification for you

295
00:31:41,400 --> 00:31:46,200
to, you know, go find a data scientist to partner with, or something like that. How do you proceed?

296
00:31:47,000 --> 00:31:53,640
Many different ways. This is definitely not me sitting waiting for something to happen.

297
00:31:53,640 --> 00:31:59,880
This has really energized me and energized everybody on my team to go out and innovate and find

298
00:31:59,880 --> 00:32:04,600
new ways that we can assist the deputies and other law enforcement officers doing their job.

299
00:32:04,600 --> 00:32:09,240
So if it means that we know we want to do something and the only way we're going to be able to do it

300
00:32:09,240 --> 00:32:15,480
is finding a data scientist to partner with, then we'll do that. If it means, you know, talking with

301
00:32:15,480 --> 00:32:21,320
Amazon over and over again until we get something into the product that we need, then that's what it

302
00:32:21,320 --> 00:32:27,720
means. If it means finding an interesting way to use another one of their machine learning tools

303
00:32:28,760 --> 00:32:33,080
that they didn't intend, because I can honestly tell you all of the talks I've had with Amazon,

304
00:32:33,080 --> 00:32:38,520
when they first put out recognition, nobody thought this would be great for recognizing criminals

305
00:32:38,520 --> 00:32:44,680
from mug shots. But now that I do it, it's something that they absolutely think about the law

306
00:32:44,680 --> 00:32:52,360
enforcement applications for all of their future machine learning stuff. So yeah, it's just going

307
00:32:52,360 --> 00:32:57,080
out there and trying to find interesting ways to use the things that we already have access to

308
00:32:57,080 --> 00:33:04,280
and possibly driving that thing into what we need it to be, which is kind of what we've done

309
00:33:04,280 --> 00:33:09,880
with recognition. Now, I won't take any credit for what the AWS guys do because they do great work

310
00:33:09,880 --> 00:33:15,320
and they do it on their own. I'm just saying that once they see that we have a use case there,

311
00:33:15,320 --> 00:33:18,920
they will take that into account and move for us.

312
00:33:18,920 --> 00:33:34,600
When you think about the vast array of services that AWS offers, or when you think about the array

313
00:33:34,600 --> 00:33:41,000
of machine learning and AI services that are offered by not only AWS, but Google and Microsoft,

314
00:33:41,000 --> 00:33:49,560
and a host of other players, do you have a short list of things that you're excited about,

315
00:33:49,560 --> 00:33:56,280
you know, tinkering with and putting the use in your service office?

316
00:33:56,280 --> 00:34:01,960
Absolutely, absolutely. I mean, just this morning, the keynote announced recognition for video.

317
00:34:03,960 --> 00:34:06,920
I don't even know if I'm going to wait till I get home to play with that.

318
00:34:06,920 --> 00:34:16,120
We have so much video in the way of surveillance of crimes and the way of, you know,

319
00:34:17,160 --> 00:34:23,960
video sent in by eyewitnesses that we could utilize to determine, you know, who that person is

320
00:34:23,960 --> 00:34:28,680
and doing that thing. And then there are other use cases that just quickly going through my head,

321
00:34:28,680 --> 00:34:34,360
you know, one of the things that recognition would possibly allow us to do is determine intent.

322
00:34:34,360 --> 00:34:39,000
You know, is somebody intending to do harm? Is somebody intending to do somebody else?

323
00:34:39,000 --> 00:34:45,480
Maybe there's a way to get notifications if a camera sees that.

324
00:34:45,480 --> 00:34:54,840
So just many things I want to play around with that. There's the new, I think it's called deep lens,

325
00:34:54,840 --> 00:35:00,920
which allows you to train a model based on what you show it. Obviously, you can use that

326
00:35:00,920 --> 00:35:07,080
for anything from training it to determine products to training it to determine. I think they showed

327
00:35:08,520 --> 00:35:15,240
record labels, things like that. I think I could train it to do better at finding

328
00:35:16,040 --> 00:35:21,080
the faces of people in those surveillance cameras. Or maybe like we were just talking about,

329
00:35:21,080 --> 00:35:28,360
maybe I can train it to do tattoo detection, right? So that's something I want to play around with.

330
00:35:28,360 --> 00:35:36,680
Then they talked about, I can't remember the name of it now, but the new, they have a new

331
00:35:36,680 --> 00:35:41,000
machine learning platform that allows you to build the models without having to know anything

332
00:35:41,000 --> 00:35:46,440
about data science. And so I'm going to play around with that obviously and see if I can

333
00:35:46,440 --> 00:35:49,720
get a model up and running and maybe talk, like when we were talking about

334
00:35:51,320 --> 00:35:57,000
predictive policing to know where to send deputies because of, you know, maybe on the Fourth of July,

335
00:35:57,000 --> 00:36:01,880
we see a lot of illegal fireworks being set off in a certain area and get a notification. Hey,

336
00:36:01,880 --> 00:36:07,160
why don't you head over to that area? That kind of thing. So lots of stuff I'm excited to play with.

337
00:36:07,800 --> 00:36:12,760
Awesome. Awesome. Well, Chris, thanks so much for taking the time out to chat with me. I enjoyed

338
00:36:12,760 --> 00:36:18,600
learning a bit about your use case and, you know, hearing about this kind of dealing with AI

339
00:36:18,600 --> 00:36:22,840
services from a developer's perspective. I enjoyed it too. Thank you very much for having me.

340
00:36:22,840 --> 00:36:32,520
Thanks. All right, everyone. That's our show for today. Thanks so much for listening and for your

341
00:36:32,520 --> 00:36:38,520
continued feedback and support. For more information on Chris or any of the topics covered in this

342
00:36:38,520 --> 00:36:46,280
episode, head on over to twimmelaii.com slash talk slash 86. To follow along with the AWS

343
00:36:46,280 --> 00:36:54,200
Reinvent series, visit twimmelaii.com slash reinvent. To enter our twimmel one mill contest,

344
00:36:54,200 --> 00:37:01,560
visit twimmelaii.com slash twimmel one mill. Of course, we'd be delighted to hear from you,

345
00:37:01,560 --> 00:37:08,360
either via a comment on the show notes page or via Twitter to at twimmelaii or at Sam Charrington.

346
00:37:09,480 --> 00:37:14,360
Thanks again to until Nirvana for their sponsorship of this series. To learn more about their

347
00:37:14,360 --> 00:37:21,720
role in deep lens and the other things they've been up to, visit intelnervana.com. And of course,

348
00:37:21,720 --> 00:37:51,560
thanks once again to you for listening and catch you next time.

