Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Did you miss TwimalCon AI platforms?
If so, you'll definitely want to check out our TwimalCon Video Packages.
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more
about their experiences automating, accelerating, and scaling machine learning and AI.
In each video package, you'll receive our keynote interviews, the exclusive team Teardown
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.
Once again, visit twimalcon.com slash videos for more information or to secure your advanced
purchase today.
All right everyone, I am on the line with Chavier Amatria, Chavier is the co-founder and CTO of Curai.
Chavier, welcome back to the Twimal AI Podcast.
Yeah, thanks for having me, Sam.
So for those that don't recognize the name, Chavier was actually our third guest after
switching to the interview format.
So this was over three years ago, and so much has happened for both of us.
We last had an opportunity to catch up at the AWS Remarice Conference, almost back in June
or so, and I thought it makes sense to get Chavier back on the show to get a little bit
of an update as to what he's been up to.
So when we last spoke to Chavier, he was leading the engineering team at Cora, doing a ton of
work on recommendation systems and other machine learning use cases.
Prior to that, he led the machine learning algorithms team at Netflix, and again, he's
currently the co-founder of Curai, a startup in the healthcare AI space.
Chavier, why don't we just jump right in and have you bring us up to date on Curai and
what's your up to there?
Sure.
Yeah, so Curai, we are using state-of-the-art AI and machine learning for a very big
and bold mission, which is to basically bring the world's best healthcare to everyone.
And of course, that is, as I said, a very bold and very big mission, and we are making
it concrete by basically focusing first on primary care.
So we want to bring the cost down of providing good, not good, but best quality healthcare
to everyone by using AI and machine learning to bring it down to a place where it can be
affordable and it can be scalable.
And everyone in the world who has a phone can have primary care in a very convenient
accessible and affordable way.
And so when you're talking about allowing people to use their phones for primary care,
are we talking about like turning your phone into a tricorder, or are we talking about
using your phone as a kind of a vehicle for accessing human physicians or something
in between?
Or something totally different?
It's a combination of the above, plus something slightly different, but the realization
is that a lot of what can be solved in primary care and in healthcare, it really boils down
to having conversations between patients and physicians.
And of course providing input to those conversations from different sensors and labs and other
places, right?
But the core of what happens in any kind of like medical visit is a conversation between
the patient and the doctor.
And that's the part that can be really automated and not only automated, but actually brought
to a point where you can do it from anywhere that you have a phone with any kind of connection
and you can start having a conversation and chatting with a doctor.
That's all you need.
And of course, there's always going to be things that you can do over the phone like you
can get your medication over the phone, right?
But that's okay because we can always deliver medication to your home or we can always
refer you to a lab that is nearby and get the results from the lab and whatnot.
But the really key issue in the part that we're focusing on is in that conversation that
happens between patients and doctors.
And we have a service where we employ physicians to basically be on the other end of the line
to have those conversations.
And what we're doing is applying this AI machine learning approaches to automate as much
as possible this conversation so we can augment and scale the doctor.
So an important piece here is we're not replacing the doctors with the AI machine learning.
What we're making them is we're giving them, I usually say we're giving them superpowers
that instead of being able to see say 100 patients a day, they'll be able to see 10,000 of
the conversations and most of the easy stuff will be handled by the AI machine learning
and they'll be able to focus only in the places that they're needed the most.
Is 100 patients a day a typical metric for practicing physician?
It really depends. In primary care, the numbers are roughly about, the average is I think 12 minutes
per patient. So you can, it depends on what the working hours are for for doctors.
That's actually that's in the, in the U.S. Okay, all the places like India, for example,
where it's much less than that and doctors can see way more than 100 patients in a day.
And the reality is that they're not even seeing them, the nurses are taking care of them
before they get to see the doctor, but they count as having seen the doctor.
So it's, yeah, it's not a, it's not a, but you can, obviously, you can imagine that
a doctor with 15 minutes or less to see a patient and to remember, first of all, the
history of the patient, what's going on, ask the right questions, get the right answers,
remember everything they know about medical school and come up with a diagnosis and come
up with a recommendation. It's really hard for any, you know, human being to do that at
that rate, right? And of course, there's a lot of mistakes and a lot of things that happen
because of this. What we're trying to do is say, hey, hand that off as much as possible
to the algorithms and the machines and then make sure that when the doctor comes in,
they come at the right time and they come at the point where they have all that information
laid out for them and they can verify the decisions and make sure that they're saying
the right thing. And at the same time, that's what we mean by augmenting, right, the
doctor. We are, of course, giving them information that is state of the art and based on real
science and they can get that information in a way that they can parse it and they can
say, okay, yeah, this is the right decision. I agree. Instead of sort of like having to
deal with all the messiness of gathering that information, parsing it, remembering things,
going through the electronic health record and then making a decision, all of that in
less than 15 minutes, right?
Now there are aspects of this that sound very much like, you know, from the kind of technology
I'd expect to see like other conversational agents where you've got some backend resources
or team that you want to optimize the use of the time and allow some AI system to handle
the kind of easy, easy responses. I've got to imagine that, you know, things get a lot
more complicated and messier, different, certainly more important and healthcare side of things.
Can you talk about some of the unique challenges associated with applying this kind of technology
in healthcare?
Yeah, yeah, definitely. There's a lot of challenges and you're right. You could think that, you
know, the typical approach to dialogue systems and all the advances that we're having recently
on this kind of chatbots and things like transformers and birds and GPG2s and things like that
are useful and they are. I mean, we are using all of the above in different ways, but the
reality is in a domain like healthcare medicine where the stakes are so high, you cannot
leave things out to, you know, chance or just to a model to rely on this kinds of conversations
to actually following the right path. And there's a lot of examples out there where you
can trick any of these models to say things that seem reasonable for any human being, but
they're medically completely wrong, right? And there's been a few examples of that. And
of course, that's the key issue that we were tackling with is like, how do we combine
prior knowledge about what's correct and incorrect in science and in medicine with some
of this automation, right? We do have a key insight here. A very important thing of what
we're doing is we control the end to end. So we have both sides of the conversation and
we're meaning the patient and the expert, the doctor writing this case. So the interesting
thing in our, in the way that we're applying this technology is that we can deploy the conversation
helpers in both ways, right? We can decide to serve something directly to the user if we
want to, but we can also serve it to the doctor and the doctor can use it in an assistant
and make a call whether that makes sense or not if it's being helpful, right? That's
a really important thing, right? Because then you go, you're basically walking the line
between a chatbot and an assistant, kind of like a Gmail assistant, if you will, when
you get an auto response suggestion, right? And the doctor can decide, okay, yeah, this
makes sense. I'll just take it as it is or this doesn't make sense, but they're still
sort of like making sure that that's medically correct. And at the same time, we are getting
training data on how our model, the model that we're building are accurate or not and
in what way they are or not. We have actually an upcoming publication in one of the works
of Neuribs, which basically talked about this, how to constrain the flexibility of this
sort of like deep neural dialogue systems with expert feedback in order to make sure that
the information is accurate for a domain, particularly in medicine, in this case. So
we need to combine the best of both worlds. And by the way, we do the same thing in other
parts of our modeling strategies like diagnosis. In diagnosis, we're also combining expert
systems, which is, you know, old school day eye with deep learning. And I think you cannot
rely 100% on any of the two, but you could get much better if you combine those two strategies
in some smart ways, which I think it's a key insight for medicine, but it's also something
that will happen. And it's being advocated for by many people in machine learning in general,
right? Like you can't blindly trust models that come only with it from the data with no prior
knowledge or some form of knowledge constraints. And a lot of people are trying to figure out
like how do you combine those two things, right? How do you combine all the power you get from
models that are basically just being trained from lots and lots of data with knowledge that we have
and structuring that knowledge and form of prior into the models? Right, right. That is a theme
that continues to recur here on the podcast and in my conversations. One interesting thought
there is, you know, certainly on the probabilistic side, we've benefited from a huge recent
explosion in available tools and algorithms and the like. You've mentioned a bunch of those
already, Bert, et cetera. And, you know, we've got tools like TensorFlow and PyTorch and many,
many, many others. Whereas expert systems, you know, we think of as kind of a throwback to
pre-winter, you know, AI. And I can't think of, you know, not being deep in that space. I can't
think of kind of what the leading open source expert system software might be. Is there a tools
ecosystem there or is it, you know, are people building, you know, when people have this realization
that they need both and not one or the other, are they kind of building it from scratch?
Yeah, I don't think there is such a thing. I don't think there's a, you know, there's a
next-for-system component for TensorFlow or PyTorch. Or should there be, does that make sense?
You know, would something like that have benefited you or is it, you know, is it basically just kind
of rules that we know how to code them because, you know, it's not probabilistic?
Not really. I mean, and by the way, they can be probabilistic, right? At the end of the day,
what you have with these expert systems is a graph. And then you can do probabilistic
inference on the graph and you can do different things on that graph. So basically,
I'm thinking a generic tool for expert systems would be rather simple in the sense that all you
need is a way to represent sort of like graphs and make inferences on those graphs. So it wouldn't be
that complicated to sort of like have a component for TensorFlow or for PyTorch that basically
does that for you. So the key thing here is those expert systems rely a lot still on sort of like
manual labor. And just to give you an example, in the case of some of the expert systems we're using,
we're using some that have been developed for over 50 years, right? So there's a couple of
expert systems for medical diagnosis that go back 50 years. And we, we're using both of them,
actually. And interestingly, you know, there's a lot of knowledge in there, right? You can think
about, you know, 50 years of a bunch of hundreds of really well trained physicians encoding
knowledge and information about medicine and a graph, right? And that's really valuable. And
and it's really something that if you can then inject it into any learning system,
you get a lot of a lot from it, right? To your question, there's no, you know, there's no
tooling for that. On the other hand, you can do interesting things like one of the things we've
done is use this expert system as a, basically a data generator to generate synthetic data and
train learning models from the data that is generated from the expert system, right? That's an
example of something that I think is very useful and really, really valuable because then you can
you can even merge synthetic data with natural data and you can tweak it in ways that you can
learn a model that actually now has some prior knowledge that has been injected in the form
of a ground truth data, so to speak. Can you speak to that particular point in a little bit more
detail? Yeah, sure. Okay, so in this case, the thought process was like that, right? Like,
we know that, you know, if we have very good data and we train a deep learning neural net,
we could get sort of like a really high accurate diagnosis system, the reality is that high quality
data does not exist. If you go to electronic health records, which we have used ourselves, I mean,
we have a project with Stanford where we have been working with them on using the
electronic health records, and this has been something that others have done, like Google and
DeepMind, and you name it, it's like learning predictive models from electronic health records,
electronic health records, the data quality is really, really poor notoriously so. Yes, yes,
and there's a lot of reason for it, but one of it is, you know, they weren't designed
for the purpose of diagnosis, they were designed for the purpose of billing and to make sure
the insurance company's got their money back, so there's a ton of issues with them. So,
but again, that data is valuable, it's not like it's totally noise, there is something in it.
So how can you generate some kind of data that is more, you know, solid and sort of like,
can treat more as a ground truth? Well, you can go to this expert system, which again,
they're, all they are is, you know, a graph, and you can start activating notes and generate
data from that graph that basically becomes sort of other cases that you use to train your
deep learning model, and that's what we showed in this, this is a paper we published last year,
where we basically generated data from the expert system. We injected noise to that data,
because an interesting and important thing is you want to train a model that is robust to noise,
right? The problem with expert systems, one of the problems, is that they're not capable of
dealing with noise. So in other words, if, you know, if the patient doesn't say exactly the
symptom they have and they make a mistake, because they didn't understand the question, or the doctor
enters a wrong thing, the expert system is basically doomed and and we're going to give you an
incorrect output. That's not the case for, you know, you can train machine learning models are,
you know, relatively robust to noise, because you can even do adversarial training and you can do
a lot of different things to make them robust. So how do you combine both? Well, you can also
inject noise to the expert system, that's basically what we did. So we generated data from the
expert system. We injected different kinds of noise. One example, which I think will be very obvious,
is you can inject noise by saying, hey, I'm just going to randomly inject things, symptoms that
are very common, right? I'll just add coughing to everything, because, you know, coughing is
something that people have in general, no matter whether they have one disease or not, right? It's like,
it's, you know, you always can cough or sneeze or something like that that is very prevalent and
very common. It's a typical thing and can confuse, really confuse an expert system, but it's,
if you train a machine learning model on ignoring cough, because it's something that's very common,
and it's not very, it's not going to have determined what the diagnosis is, well, then you build
a robust model. So we again, generated synthetic data from, from these systems, injected noise
in ways that we made the learn model more robust, and we also combined that synthetic data
with natural data that we had from EHRs and other sources to also prove that you can, you don't
need to constrain yourself to just one single kind of data, right? All you need to do is combine it
in smart ways to sort of like understand, because there's obviously value to training from real world
data. All you need to do is figure out how to combine it with more clean data and data that you
can trust. You mentioned this kind of injecting noise via adding symptoms that are frequently
recurrent. What are some other examples of the kind of noise that you're injecting in and more
broadly? How do you quantify the value of this synthetic data in building out your models?
Yeah, so, okay, so to the first question, I mean, I think that the key insight to adding
noise in a domain like medicine is that you do need, you need to have some domain knowledge,
right? When I give you the example of adding symptoms that are very common, that makes sense,
right? Because it makes sense because we know about medicine like, okay, the explanation makes
sense. Another example is like, well, you can remove symptoms that are very rare or are likely
to be missed, right? That's another thing that makes sense once you explain it, right? But you need
to have some insight and you need to talk to doctors, and that's something we do all the time,
right? This kind of strategies don't come up by sheer imagination. They come up because we talk
to our physicians and we talk to them and say, hey, what's, how do you deal with this issue? Where
are issues that are common and that lead to mistakes in diagnosis? How can we make sure that our model
doesn't make the same mistake? So I think that is a key and important thing is you need to work with
the main experts and that leads me to answer your second question. Let me just pause there because
that's a kind of an interesting point. I think, you know, and I think of noise, at least from a
classical engineering perspective, I think of noises like this junk that's, you know, uncorrelated
from your signal. But what you're suggesting is that at least when you're creating synthetic
data, your noise needs to be correlated with your actual noise that you need to expect. You can't
just have, you know, purely random noise because that won't help your model.
Yeah, that's pretty much it. I mean, here it's slightly different, right? And notion of noise
if you will. But what you have is synthetic data that is strictly true, if you will, because
true in a scientific sense, because it's been generated by kind of like an expert system that
has been designed on science. But what you need to do is inject noise that mimics more the
reality of nature, right? And the messiness, right? But that noise needs to model some of the
natural messiness that you see in real life. And you need to not inject it. Yeah, it's not white
noise, right? In that sense, right? It's noise that tries to sort of turn that synthetic data into
something that is more real, right? If you think about it, I mean, I use some time for the metaphor
of like the self-driving cars, also use synthetic data that is generated from video games. And it's
like, well, you can imagine that you're training your self-driving model on data from, from Grand
Flip Auto, but you need to inject, I don't know, a flock and you need to inject rain and you need to
inject things that are not maybe, you know, in your synthetic data. And they're adding noise to the
capture of the image, but in a way that mimics real life situation, right? Not just white noise.
And that sense, it's a bit like the concept of domain adaptation.
Yeah, I mean, you could consider that for sure. And that's another, it is a very, I mean,
domain adaptation in itself. I mean, we could go into that. It's another important thing that
you need to do in many cases because, and yeah, you're right. It could be seen as that, right? Because
sometimes you are training on ideal data, but then you're going to be faced with real life data that
it's going to have to be interpreted in the context of the ideal data that you use for training. So,
yeah, it is, yeah. Okay, so you're about to take on that second question.
Yeah, the second question was about how do you even, you know, how do you know that the data is good
or even the model that your training is good? And, and, you know, beyond that, the relative
advantage of, you know, how do you compare with and without using this synthetic data, you know,
is it a, is it a training time or is it a, you know, accuracy or some combination of all these things?
It's mostly about accuracy, right? And, and the, the problem is that the definition of the accuracy
is, again, really tricky and, and, and, and not that obvious, right? And accuracy in the context
of medical diagnosis is a very, very tricky thing to define, particularly because you would hope
that by asking physician, you would get a ground truth, but that's not the case, right? There's
a studies out there, for example, the human DX project that published some studies that
the, on their dataset, the average accuracy of a single physician was 60 percent, right?
Which is really low. Now, if you, if you take the consensus of 20 physicians that got up to
over 80 percent, which is much better, but then, of course, you need to have 20 physicians agree,
and you still have to 80 percent, which is a lot better, but not necessarily comforting if
you're the patient. Exactly. And, and, and I think that's, that's a key issue in like, what do we
treat as ground truth? So, in our case, we, we, I mean, we use a combination of a lot of things,
we use a combination of sort of, like, publicly known datasets, which there's not that many,
unfortunately, for, for this domain, and they're just, you know, a few, what's called medical
vignettes that you can use to evaluate. We also use our own physicians to QA, and we make sure that
we have sort of, like, several of them agreeing on the cases, so we know that, that we're right.
And then, at the end, it, there's also this kind of, like, synthetic data, right? It's like,
you need to treat that synthetic data as pseudo ground truth, in the sense that, as I mentioned,
if you think about it, that, that synthetic data is the result of, as I said before, 50 years
of research from hundreds of physicians who have agreed that that's what, you know, that,
particular disease should be defined as, and that's those are the symptoms that are related. So,
it's, it's as good as a ground truth as you can get in many other cases, right? So, again,
it's, I wish I had a, like, a great answer for this, but the reality is, I don't. It's like,
it's a, it's kind of an iterative process where you, like, treat one data as a ground truth,
but then you compare it to your other data, you let your physicians go through it and say,
yeah, this is correct or it is not. And then you feed it back and you keep improving both over time.
And I think that's, that's another very important lesson learned here is that you need to design
all the systems as really learning systems, right? So, it's, it's not only about what's their
accuracy today, it's more about how can you make sure that the accuracy and all the other
methods you care about improve over time, right? And in the meantime, the, the, the, the important
thing is like, we always default to humans, right? It's like, we'll always default to a human
doctor and improve the model over time and, and just tell that human doctor, like, hey,
our model thinks that these three things are important. You want to consider them and the doctor
will say, yes or no, and it's their call. And, you know, we'll be as good as the, as the doctors
are. But over time, we, we are pretty sure. Actually, even in our outline evaluation metrics,
we think that we're already, our models are already at least as good as not better than the average
doctor. But even with that, it's, it's not enough, right? It's like, they need to be better than
the best doctor to even make it feasible to rely on, on, on them. But they're a good assistant
and a good augmentation to the human physician for sure. Do you, have you made any attempts to
benchmark the, the third party expert systems with regard to, you know, some elusive metric around
accuracy or, you know, I guess that the thought is that, you know, even if we were confident that each
of the elements in this expert system, you know, was vetted by the 20 doctors or whatever required
to, you know, have a consensus that, you know, has some sufficient level of accuracy. You know,
medical perspectives have changed significantly over 50 years. We may, I don't know the extent
to which this is tracked in this expert system. But, you know, there are diagnostic practices that
apply not equally across different groups of patients. So you have all the potential for all kinds
of biases within a data set like that. Have you made any attempt at kind of evaluating that?
I mean, we are constantly evaluating that with our data, but it's really hard to come up with a,
you know, something that I, I would dare to publish, right? Because it's, the problem is the same.
It's like there is no, no ground truth. There's a, there's a couple of papers on evaluating
different systems and different online symptom checkers. And those are the ones that everyone
is using as sort of like the benchmark. And there's a paper by semi-gram on evaluating symptom
checkers. And there's some medical vignettes that she published, which are commonly used by a bunch
of people, including some like Babylon in the UK and so on with they published things like,
well, we use these vignettes because that's all we have that at least is commonly available and
you can benchmark against. But they're far from, you know, something that it's that you could
consider sort of like has good coverage of medical conditions and you can trust us as being
comparable. But that being said, again, I think that the reality is as harsh as it may sound,
it's not too hard to be better than the average physician. But again, that's not enough. That's
not convincing. Like if I told you like, oh, I can build a self-driving car that is better than
the average teenage driver, would you be okay? Like, well, probably not. Because the average
teenage driver is not somebody I would trust on an automated driving machine. So I think here
it's pretty much the same. It's not about being better than the average doctor. It's about
being better than the best doctor and being able to augment and always sort of like fall back on
humans. And I think that's exactly, I like that comparison to self-driving cars a lot because I
think what we're trying to build is not completely autonomous vehicle, right? We're trying to build
this AI automation as an assistant to the driver just like many cars do right now. But in this case,
the driver is an expert who is a physician. One more question for you. You mentioned earlier that
among the techniques that you're relying on, you do make some use of transformers,
Bert, GPT-2, that kind of thing. How does that play out in what you're building?
That plays out in many different ways. I mean, there's a lot of great things about
those approaches that the one that I think is probably the most relevant in our case is the fact
that it's all about transfer learning, right? It's about if you have a great model that has learned
in general how to speak, sort of say, you can then fine tune it on some specific domain to become
better about speaking about healthcare, right? So a lot of the approaches we take is we look at some
of these models. We fine tune them on very specific data that we have that is focused on healthcare.
And then we can use it to do a bunch of things. I mean, the output of those models can be used
in the context of a chat bottle or a dialogue system, but you can also use them to generate
features for anything, for a classifier or you name it, right? And I think they build a representation
of language in general, right? So we use them as inputs to many of the things we do,
but more directly, we also use them, as I was mentioned before, to generate
assistance to the physicians as they're chatting and they're talking to the patient, right? So
if you think about, and then that also, I think I dare to say pretty common in many applications
of just customer service in general, like where customer service will have, sort of like assistance.
Actually, there are some papers, I think, for example, from Airbnb, where they've done similar
things for their customer service, where there's basically an assistant that is telling the customer
service and suggesting things they could say, so they can basically accept them or not and decide
whether they want to type them out or just simply select the suggested response. So that's an
example where you can almost, you know, you can take one of these models fine-tuned training on,
training on very specific data that it's more healthcare-oriented and you can generate sort of
like an assistant for a physician or an expert in any given domain. Well, Chavier, it was
absolutely wonderful catching up with you, really excited to learn more about what's your up to
there at CURI and I'll definitely be following along. Okay, yeah, great. I would say that many of
these things that we've, I've mentioned, we are publishing and we're, we have I think four papers
in this machine learning for healthcare, workshop and new ribs and if people are interested in
following up in some of the details of how we use this transformer model or how do we do diagnosis
and so on, that's all, I mean, they can go to archive and find more details on some of these
techniques and how we're using them and trying to solve sort of like this huge healthcare problem
access. So yeah. Fantastic. We'll include some links to those papers on archive in the Shunuts.
Great. So great talking to you. Thank you. That's our show for today. To learn more about today's show,
visit Twomolai.com slash shows. Once again, if you missed Twomolcon or want to share what you learned
with your team, be sure to visit Twomolcon.com slash videos for more information about Twomolcon video
packages. Thanks so much for listening. Peace.
