WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twilmal AI Podcast.

00:13.400 --> 00:15.600
I'm your host Sam Charrington.

00:15.600 --> 00:23.200
Hey, what's up everyone?

00:23.200 --> 00:24.200
This is Sam.

00:24.200 --> 00:29.040
A quick reminder that we've got a bunch of newly formed or forming study groups, including

00:29.040 --> 00:34.800
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders

00:34.800 --> 00:36.760
part one courses.

00:36.760 --> 00:42.960
It's not too late to join us, which you can do by visiting twilmalai.com slash community.

00:42.960 --> 00:47.960
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.

00:47.960 --> 00:50.360
If you're at either event, please reach out.

00:50.360 --> 00:52.360
I'd love to connect.

00:52.360 --> 00:57.860
Alright, this week on the podcast I'm excited to share a series of shows recorded in

00:57.860 --> 01:01.580
Orlando during the Microsoft Ignite Conference.

01:01.580 --> 01:05.820
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship

01:05.820 --> 01:07.140
of this series.

01:07.140 --> 01:11.900
Thanks to decades of breakthrough research and technology, Microsoft is making AI real

01:11.900 --> 01:18.660
for businesses with Azure AI, a set of services that span vision, speech, language processing,

01:18.660 --> 01:21.740
custom machine learning, and more.

01:21.740 --> 01:26.180
Millions of developers and data scientists around the world are using Azure AI to build

01:26.180 --> 01:31.260
innovative applications and machine learning models for their organizations, including

01:31.260 --> 01:35.300
85% of the Fortune 100.

01:35.300 --> 01:41.260
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven

01:41.260 --> 01:47.420
enterprise grade capabilities and innovations, wide range of developer tools and services

01:47.420 --> 01:49.460
and trusted approach.

01:49.460 --> 01:54.500
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS

01:54.500 --> 02:00.260
and DevOps professionals across all skill levels to increase productivity, operationalize

02:00.260 --> 02:06.660
models at scale, and innovate faster and more responsibly with Azure machine learning.

02:06.660 --> 02:11.540
Learn more at aka.ms slash Azure ML.

02:11.540 --> 02:13.980
Alright, onto the show.

02:13.980 --> 02:17.780
Alright, everyone.

02:17.780 --> 02:20.220
I am here in sunny Orlando.

02:20.220 --> 02:26.580
Actually, it's not all that sunny today, it's kind of gray and gray and rainy, but it is

02:26.580 --> 02:28.140
still sunny Orlando, right?

02:28.140 --> 02:31.100
How could it not be at Microsoft Ignite?

02:31.100 --> 02:33.820
And I've got the wonderful pleasure of being seated with Sarah Bird.

02:33.820 --> 02:38.460
Sarah is a Principal Program Manager for Azure Machine Learning Platform.

02:38.460 --> 02:41.060
Sarah, welcome to the Twomo AI Podcast.

02:41.060 --> 02:42.060
Thank you.

02:42.060 --> 02:43.060
I'm excited to be here.

02:43.060 --> 02:44.060
Absolutely.

02:44.060 --> 02:49.540
I am really excited about this conversation we're about to have on Responsible AI, but before

02:49.540 --> 02:52.540
we do that, I'd love to hear a little bit more about your background.

02:52.540 --> 03:00.460
You've got a very enviable position at the nexus of research and product and tech strategy.

03:00.460 --> 03:02.460
How did you create that?

03:02.460 --> 03:06.380
Well, I started my career in research.

03:06.380 --> 03:13.780
I did my PhD in machine learning systems at Berkeley, and I loved creating the basic

03:13.780 --> 03:18.380
technology, but then I wanted to take it to the next step, and I wanted to have people

03:18.380 --> 03:25.220
who really used it, and I found that when you take research into production, there's

03:25.220 --> 03:30.780
a lot more innovation that happens, and so I really sense graduating and have styled

03:30.780 --> 03:35.300
my career around living at that intersection of research and product and taking some

03:35.300 --> 03:38.980
of the great cutting-edge ideas and figuring out how we can get them in the hands of people

03:38.980 --> 03:45.820
as soon as possible, and so my role now is specifically focused on trying to do this for

03:45.820 --> 03:51.060
Azure Machine Learning, and Responsible AI is one of the great new areas that there's

03:51.060 --> 03:56.180
a ton of innovation in research, and people need it right now, and so we're working to

03:56.180 --> 03:58.540
try to make that possible.

03:58.540 --> 04:05.780
That's fantastic, and so between your grad work at Berkeley and Microsoft, what was the

04:05.780 --> 04:07.300
path?

04:07.300 --> 04:13.940
So I was in John Lankford's group in Microsoft Research, and was working on a system

04:13.940 --> 04:18.900
for contextual bandits and trying to make it easier for people to use those in practice,

04:18.900 --> 04:24.340
because a lot of the times when people were trying to deploy that type of algorithm, the

04:24.340 --> 04:26.420
system infrastructure would actually get in the way.

04:26.420 --> 04:30.580
You wouldn't be able to get the features to the point of decision or the logging would

04:30.580 --> 04:35.420
not work, and it would break the algorithm, and so we designed a system that made it

04:35.420 --> 04:39.020
correct by construction, so it was easy for people to go and plug it in, and this is

04:39.020 --> 04:43.780
actually turned into the personalizer cognitive service now.

04:43.780 --> 04:47.940
But through that experience, I learned a lot about actually working with customers and

04:47.940 --> 04:53.340
doing this in production, and so I decided that I wanted to have more of that in my career,

04:53.340 --> 04:59.780
and so I spent a year as a technical advisor, which is a great role in Microsoft, where

04:59.780 --> 05:06.180
you work for an executive and advise them and help work on special projects, and it enables

05:06.180 --> 05:13.540
you to see both kind of the business and the strategy side of things, as well as all

05:13.540 --> 05:17.860
the operational things how you run orgs, and then of course the technical things, and

05:17.860 --> 05:22.900
I realized that I think that makes this very interesting, and so after that I joined

05:22.900 --> 05:29.660
Facebook, and my role was at the intersection of Fair, Facebook AI Research, and AML,

05:29.660 --> 05:33.700
which was the applied machine learning group, with this role of specifically trying to

05:33.700 --> 05:39.420
take research into production and accelerate the rate of innovation.

05:39.420 --> 05:46.420
So I started the Onyx project as a part of that, enabling us to solve a tooling gap where

05:46.420 --> 05:51.900
it was difficult to get models from one framework to another, and then also worked on PyTorch

05:51.900 --> 05:57.420
and enabling us to make that more production ready, and since then I've been working in

05:57.420 --> 05:58.420
AI ethics.

05:58.420 --> 06:04.180
Yeah, if we weren't going to be focused on AI ethics and responsible AI today, we would

06:04.180 --> 06:10.380
be going deep into personalizer, what was Microsoft decision service, and this whole contextual

06:10.380 --> 06:16.940
bandits thing, a really interesting topic, not the least of which because we talk a lot

06:16.940 --> 06:22.540
about reinforcement learning, and if it's useful, and while it's not this deep reinforcement

06:22.540 --> 06:27.540
learning, game playing thing, it's reinforcement learning, and people are getting a lot of

06:27.540 --> 06:29.940
use out of it, in a lot of different contexts.

06:29.940 --> 06:35.420
Yeah, it's actually, when it works, it doesn't work in all cases, but in which it works,

06:35.420 --> 06:36.420
it works really well.

06:36.420 --> 06:40.820
It's the kind of thing where you get the numbers back, and you're like, can this be true?

06:40.820 --> 06:45.900
And so I think it's a really exciting technology going forward, and there's a lot of cases

06:45.900 --> 06:49.540
where people are using it successfully now, but I think there'll be a lot more in the

06:49.540 --> 06:50.540
future.

06:50.540 --> 06:51.540
Awesome.

06:51.540 --> 06:55.500
We'll have to take a rain check on that aspect of the conversation, and kind of segue

06:55.500 --> 07:03.180
over to the responsible AI piece, and I've been thinking a lot about a tweet that I saw

07:03.180 --> 07:10.220
by Rachel Thomas, who is a former guest of the podcast, a longtime friend of the show,

07:10.220 --> 07:16.980
and currently the UCSF Center for Applied Data Ethics head, and she was kind of lamenting

07:16.980 --> 07:22.060
that there are a lot of people out there talking about AI ethics like it's a solve problem.

07:22.060 --> 07:23.740
Do you think it's a solve problem?

07:23.740 --> 07:25.340
No, absolutely not.

07:25.340 --> 07:27.860
I didn't think so.

07:27.860 --> 07:33.860
So I think there are fundamentally hard and difficult problems when we have a new technology,

07:33.860 --> 07:37.340
and so I think we're always going to be having the AI ethics conversation.

07:37.340 --> 07:40.820
This is not something that we're going to solve and go away.

07:40.820 --> 07:46.740
But what I do think we have now is a lot more tools and techniques and best practices

07:46.740 --> 07:52.380
to help people start the journey of doing things responsibly, and so I think the reality

07:52.380 --> 07:57.020
is there's many things people could be doing right now that they're not, and so I feel

07:57.020 --> 08:02.140
like there's an urgency to get some of these tools into people's hands so that we can

08:02.140 --> 08:03.140
do that.

08:03.140 --> 08:06.300
So I think we can quickly go a lot farther than we have right now.

08:06.300 --> 08:14.500
In my conversations with folks that are working on this and thinking about the role that

08:14.500 --> 08:23.020
are responsible, AI plays in the way they do machine learning.

08:23.020 --> 08:26.820
A lot of people get stopped at the very beginning like, who should own this?

08:26.820 --> 08:27.820
Where does it live?

08:27.820 --> 08:35.820
Is it like a research kind of function or is it a product function or is it kind of more

08:35.820 --> 08:42.220
of a compliance kind of thing like a chief data officer or a chief security officer kind

08:42.220 --> 08:48.940
of function like one of those executive functions and oversight or compliance is the better

08:48.940 --> 08:51.540
word?

08:51.540 --> 08:57.020
What do you see folks doing and do you have any thoughts on where successful patterns

08:57.020 --> 08:58.540
of where it should live?

08:58.540 --> 09:05.660
Yeah, I think the models that we've been using are thinking a lot about the transition

09:05.660 --> 09:16.180
to security, for example, and I think the reality is it's not one person's job or one function.

09:16.180 --> 09:19.580
Everybody now has to think about security, even your basic software developers have to

09:19.580 --> 09:22.580
know and think about it when they're designing.

09:22.580 --> 09:27.060
However, there are people who are experts in it and handle the really challenging problems.

09:27.060 --> 09:31.700
There was, of course, legal and compliance pieces in there as well, and so I think we're

09:31.700 --> 09:38.260
seeing the same thing where we really need every role to come together and do this.

09:38.260 --> 09:44.860
So one of the patterns we are seeing is part of the challenge with responsible AI and

09:44.860 --> 09:51.660
technology is that we've designed technology to abstract away things and enable you to

09:51.660 --> 09:56.060
just focus on your little problem and this has led to a ton of innovation.

09:56.060 --> 10:00.820
However, the whole idea of responsible AI is actually you need to pick your head up,

10:00.820 --> 10:04.300
you need to have this larger context, you need to think about the application in the

10:04.300 --> 10:07.940
real world, you need to think about the implications and so we have to break a little bit of our

10:07.940 --> 10:14.900
patterns of my problem as just this little box and so we're finding that like user research

10:14.900 --> 10:21.660
and design, for example, is already trained and equipped to think about the people element

10:21.660 --> 10:27.260
in that and so it's really great to bring them into more conversations as we're developing

10:27.260 --> 10:33.500
the technology, so that's one pattern that we're finding adds a lot of value.

10:33.500 --> 10:40.460
In my conversation with Jordan Edwards, your colleague, many of his answers were all

10:40.460 --> 10:41.460
of the above.

10:41.460 --> 10:44.900
It sounds like this one is in all of the above response as well.

10:44.900 --> 10:49.740
Yeah, I think doing machine learning and practice takes a lot of different roles as Jordan

10:49.740 --> 10:54.980
was talking about in operationalizing things and then responsible AI just adds an extra layer

10:54.980 --> 10:57.780
of more roles on top of that.

10:57.780 --> 11:02.660
It's one of the challenges that kind of naturally evolves when everyone has to be thinking

11:02.660 --> 11:07.380
about something is that it's a lot harder, right?

11:07.380 --> 11:11.860
The developer is trained as a developer and now they have to start thinking about this

11:11.860 --> 11:19.180
security thing and it's changing so quickly and the best practices are evolving all the

11:19.180 --> 11:21.660
time and it's hard to stay on top of that.

11:21.660 --> 11:27.180
Now, if we're to replicate that same kind of model in responsible AI, which sounds like

11:27.180 --> 11:31.620
the right thing to do, how do we support the people that are kind of on the ground trying

11:31.620 --> 11:32.620
to do this?

11:32.620 --> 11:39.580
Yeah, and I think it's definitely a challenge because the end result can't be that every

11:39.580 --> 11:46.060
individual person has to know the state of the art in every area in responsible AI and

11:46.060 --> 11:51.780
so one of the ways that we're trying to do this is as much as possible, build it into our

11:51.780 --> 11:54.460
processes and our tooling, right?

11:54.460 --> 12:01.100
So that you can say, okay, well, you should have a fairness metric for your model and you

12:01.100 --> 12:04.660
can talk to experts about what that fairness metric should be, but you should know the

12:04.660 --> 12:08.540
requirement that you should have a fairness metric, for example.

12:08.540 --> 12:14.380
And so we first are starting with that process layer and then in Azure Machine Learning,

12:14.380 --> 12:17.860
we've built tools that enable you to easily enact that process.

12:17.860 --> 12:23.820
And so the foundational piece is the MLOPS story that Jordan was talking about where we

12:23.820 --> 12:29.660
actually enable you to have a process that's reproducible, that's repeatable.

12:29.660 --> 12:33.700
So you can say, before this model goes into production, I know that it's passed these

12:33.700 --> 12:39.460
validation tests and I know that a human looked at it and said, it looks good.

12:39.460 --> 12:44.500
And if it's out in production and there's an error or there's some sort of issue that rises,

12:44.500 --> 12:48.940
you can go back, you can recreate that model, you can debug the error.

12:48.940 --> 12:53.660
And so that's the real foundational piece for all of it.

12:53.660 --> 12:59.780
And then on top of that, we're trying to give data scientists more tools to analyze the

12:59.780 --> 13:01.700
models themselves.

13:01.700 --> 13:04.060
And there's no magic button in here.

13:04.060 --> 13:08.940
It's not just, oh, we can run a test and we can tell you everything you want to know.

13:08.940 --> 13:13.420
But there's lots of great algorithms out there in research that help you better understand

13:13.420 --> 13:14.940
your model.

13:14.940 --> 13:18.900
Like Shap or Lime are common interpretability ones.

13:18.900 --> 13:24.300
And so we've created a toolkit called Interpret ML where this is an open source toolkit,

13:24.300 --> 13:26.380
you can use it anywhere.

13:26.380 --> 13:32.940
But the idea is, enables you to easily use a variety of these algorithms to explain your

13:32.940 --> 13:37.660
model behavior and explore it and see if there are any issues.

13:37.660 --> 13:44.180
And so we've also built that into our machine learning process so that if I build a model,

13:44.180 --> 13:47.220
I can easily generate explanations for that model.

13:47.220 --> 13:51.260
And when I've deployed it in production, I can also deploy an explainer with it.

13:51.260 --> 13:55.220
So individual predictions can be explained while it's running.

13:55.220 --> 13:59.500
So I can understand if I think it's doing the right thing and if I want to trust it,

13:59.500 --> 14:05.340
for example, it strikes me that there's a bit of a catch 22 here in the sense that the

14:05.340 --> 14:11.100
only way we could possibly do this is by putting tools in the hands of the folks that are

14:11.100 --> 14:15.500
working data scientists and machine learning engineers that are working on these problems.

14:15.500 --> 14:22.460
But the tools in their very nature kind of abstract them away from the problem and allow

14:22.460 --> 14:28.100
them, if not encourage them to think less deeply about what's going on underneath.

14:28.100 --> 14:30.460
How do we address that?

14:30.460 --> 14:37.420
Yeah, I think. Or do you agree with that first of all? No, I completely agree with that.

14:37.420 --> 14:44.860
And it's a challenge that we have in all of these cases where we want to give the tool

14:44.860 --> 14:46.860
to help them and to have more insight.

14:46.860 --> 14:51.380
But it's easily for people then to just use it as a shortcut.

14:51.380 --> 14:57.340
And so in a lot of cases, we're being very thoughtful about the design of the tool and

14:57.340 --> 15:03.420
making sure that it is helping you surface insights, but it's not saying this is the answer

15:03.420 --> 15:09.820
because I think when you start doing that, like if you have some net flags and says this

15:09.820 --> 15:13.460
is a problem, then people really start relying on that.

15:13.460 --> 15:17.940
And maybe someday we will have the techniques where we have that level of confidence and

15:17.940 --> 15:18.940
we could do it.

15:18.940 --> 15:20.780
But right now we really don't.

15:20.780 --> 15:25.740
And so I think a lot of it is making sure that we design the tools that encourages this

15:25.740 --> 15:31.020
mindset of exploration and deeper understanding of your models and what's going on.

15:31.020 --> 15:33.780
And not just, oh, this is just another compliance test.

15:33.780 --> 15:34.780
I have to pass.

15:34.780 --> 15:37.460
I just run this test and it says green and I go.

15:37.460 --> 15:43.540
You alluded to this earlier in the conversation, but it seems appropriate here as well.

15:43.540 --> 15:45.260
And it's maybe a bit of a tangent.

15:45.260 --> 15:54.420
But so much of pulling all these pieces together is kind of a user experience and design.

15:54.420 --> 15:55.420
Any thoughts on that?

15:55.420 --> 16:00.220
Is that something that you've kind of dug into and studied a lot or do other folks worry

16:00.220 --> 16:04.540
about that here?

16:04.540 --> 16:10.020
It's not in my background, but to me, it's an essential part of the function of actually

16:10.020 --> 16:12.580
making these technologies usable.

16:12.580 --> 16:18.300
And particularly when you take something that as complex as an algorithm and then you're

16:18.300 --> 16:23.100
trying to make that abstracted and usable for people, the design is a huge part of this

16:23.100 --> 16:24.100
story.

16:24.100 --> 16:29.020
What we're finding in responsible AI is that we need to think about this even more.

16:29.020 --> 16:36.540
And a lot of our, a lot of the guidelines are saying, you know, be more thoughtful and

16:36.540 --> 16:39.660
include sort of more careful design.

16:39.660 --> 16:43.500
For example, people are very tempted to say, well, this is the data I have.

16:43.500 --> 16:48.300
So this is the model I can build and so I'm going to put it in my application that way.

16:48.300 --> 16:53.980
And then if it has too much inaccuracy, then you spend a lot of resources to try to make

16:53.980 --> 16:59.460
the model more accurate where you could have just had a more elegant UI design, for example,

16:59.460 --> 17:05.300
where you actually get better feedback based on the UI design or the design can tolerate

17:05.300 --> 17:07.900
more errors and you don't need that higher model accuracy.

17:07.900 --> 17:12.500
And so we're really encouraging people to co-design the application in the model and

17:12.500 --> 17:17.100
not just take it for granted that this is what the model does and that's the thing we're

17:17.100 --> 17:18.780
going to focus on.

17:18.780 --> 17:26.860
With the interpret ML tool, what's the kind of user experience like?

17:26.860 --> 17:28.340
It depends on what you're trying to do.

17:28.340 --> 17:33.620
There's two types of interpretability that people think about.

17:33.620 --> 17:36.740
One is where we call glass box models.

17:36.740 --> 17:41.300
And the idea there is I want my model to be inherently interpretable.

17:41.300 --> 17:46.660
So I'm going to pick something like, you know, a linear model or decision trees where

17:46.660 --> 17:52.060
I can actually inspect the model and enable you to build a model of that that you can

17:52.060 --> 17:53.460
actually understand.

17:53.460 --> 18:00.900
And so we support a bunch of different glass box explainer models and then so you can

18:00.900 --> 18:03.460
actually use it then to train your own model.

18:03.460 --> 18:11.580
And the other part is black box explainers where I have a model that I is a black box and

18:11.580 --> 18:16.140
I can't actually inspect it, but I can use these different algorithms to explain the

18:16.140 --> 18:17.700
behavior of the model.

18:17.700 --> 18:22.980
And so in that case, what we've done is made it easy for you to just call an explainer

18:22.980 --> 18:29.180
and ask for global explanations and ask for local explanations and ask for feature importance.

18:29.180 --> 18:34.660
And then all of those are brought together in an interactive dashboard where you can

18:34.660 --> 18:41.780
actually explore the explanations and try to kind of understand the model behavior.

18:41.780 --> 18:50.220
So a lot of the experience hits an SDK and so it's all easy calls to ask for explanations.

18:50.220 --> 18:55.540
But then we expect a lot of people to spend their time kind of in that dashboard exploring

18:55.540 --> 18:57.060
and understanding.

18:57.060 --> 19:03.860
I did a really interesting interview with Cynthia Rudin, who you may know, may know.

19:03.860 --> 19:10.340
She's a Duke professor and the interview was focused on her research essentially says

19:10.340 --> 19:16.660
that we should not be using black box models in I forget the terminology that she used

19:16.660 --> 19:20.860
but something like critical kind of mission critical scenarios or something along those

19:20.860 --> 19:25.700
lines where we're talking about someone's, you know, life or liberty.

19:25.700 --> 19:31.180
That kind of thing does providing kind of interpretability tools that work with black

19:31.180 --> 19:36.620
box models like encourage their use in scenarios that they shouldn't really be used in and are

19:36.620 --> 19:43.300
their ways that you kind of advise folks when and when not, they should be using those

19:43.300 --> 19:44.980
types of models.

19:44.980 --> 19:54.460
So we have people who do publish sort of best practices for interpretability and it's

19:54.460 --> 19:58.820
a very active area of work for the company and we work with the partnership on AI to try

19:58.820 --> 20:02.660
to make sort of industry-wide recommendations for that.

20:02.660 --> 20:07.740
I don't think it's completely decided on this idea that models should be interpretable

20:07.740 --> 20:12.700
in these settings versus, well, we want other mechanisms to make sure that they're doing

20:12.700 --> 20:13.700
the right thing.

20:13.700 --> 20:16.980
Like interpretability is one way that we could be sure that they're doing the right thing

20:16.980 --> 20:20.620
but we also could have more robust testing regimes, right?

20:20.620 --> 20:24.940
There's a lot of technologies where we don't understand every detail of the technology

20:24.940 --> 20:29.540
but we've been able to build safety critical systems on top of it, for example.

20:29.540 --> 20:36.620
So yeah, as a company, we do try to provide guidance but I don't think the industry has

20:36.620 --> 20:42.980
really decided the final word on this and so the mindset of the toolkit is enabling

20:42.980 --> 20:48.540
you to use these techniques if it's right for you but that doesn't specifically say that

20:48.540 --> 20:52.540
you should go use a neural net in a particular setting.

20:52.540 --> 21:00.500
So in addition to the interpret ML toolkit, you also announced this week here from Ignite

21:00.500 --> 21:02.540
a fair-learn toolkit.

21:02.540 --> 21:04.180
What's that all about?

21:04.180 --> 21:10.340
So it's the same spirit as interpret ML where we want to bring together a collection

21:10.340 --> 21:16.060
of fairness techniques that have been published in research and make it easy for people to

21:16.060 --> 21:19.420
use them all in one toolkit.

21:19.420 --> 21:24.900
That's the same spirit that you want to be able to analyze your model and understand

21:24.900 --> 21:29.300
how it's working so that you could make decisions around fairness.

21:29.300 --> 21:36.140
And so the toolkit, there's famously many, many different fairness metrics published.

21:36.140 --> 21:39.860
I think there was a paper, you know, cataloging 21 different fairness metrics.

21:39.860 --> 21:45.420
And so we built many of these common ones into the toolkit and then it makes it easy

21:45.420 --> 21:50.620
for you to compare how well your model works for different groups of people in your data

21:50.620 --> 21:51.620
set.

21:51.620 --> 21:58.100
So for example, I could say does this model have the same accuracy for men and women?

21:58.100 --> 22:02.620
Does this model have the same outcomes for men and women?

22:02.620 --> 22:08.780
And so we have an interactive dashboard that allows you to explore these differences between

22:08.780 --> 22:13.700
groups and model performance through a variety of these metrics that have been published

22:13.700 --> 22:15.220
in research.

22:15.220 --> 22:21.700
Then we've also built in several mitigation techniques so that if you want to do mitigation

22:21.700 --> 22:26.220
via post-processing in your model, then you can do that, for example, setting thresholds

22:26.220 --> 22:27.620
per group.

22:27.620 --> 22:31.900
And in a lot of cases, it might be that you actually want to go and fix the underlying

22:31.900 --> 22:34.180
data or you want to make some different decisions.

22:34.180 --> 22:39.260
So the mitigation techniques aren't always what you would want to do, but they're available

22:39.260 --> 22:41.220
if you want to do that.

22:41.220 --> 22:45.860
And so the name of the toolkit actually comes from one of these mitigation techniques

22:45.860 --> 22:52.540
from Microsoft Research, where the algorithm was originally called Fairlearn.

22:52.540 --> 23:00.300
And the idea is that you say, I want to reduce the difference between groups on a particular

23:00.300 --> 23:01.300
dimension.

23:01.300 --> 23:06.620
So you pick the metric and you pick the groups and the algorithm actually retrain your

23:06.620 --> 23:12.380
model by re-weighting data and iteratively retraining to try to reduce that disparity.

23:12.380 --> 23:14.180
So we've built that into the toolkit.

23:14.180 --> 23:18.900
So now you can actually look at a variety of your versions of your model and see if one

23:18.900 --> 23:24.380
of them has properties that works better for what you're looking for to deploy.

23:24.380 --> 23:27.460
Again, I'm curious about the user experience in doing this.

23:27.460 --> 23:35.300
How much kind of knob turning and tuning does the user need to do when applying that technique

23:35.300 --> 23:40.580
you were describing or is it more, I'm envisioning something like contextual band, it's reinforcement

23:40.580 --> 23:43.140
learning where it's kind of tooling the knobs for you.

23:43.140 --> 23:47.620
Yeah, no, it's like, it is doing the knobs and the retraining.

23:47.620 --> 23:53.060
But what you have to pick is which metric you're trying to minimize.

23:53.060 --> 24:00.540
Like, do I want to reduce the disparity between the outcomes or do I want to reduce the disparity

24:00.540 --> 24:05.500
in accuracy or some other, you know, there's many different metrics you could pick, but you

24:05.500 --> 24:09.500
have to know what the metric is that's right for your problem.

24:09.500 --> 24:13.860
And then you also need to select the groups that you want to do.

24:13.860 --> 24:20.980
So it can work in a single dimension, like as we're saying, making men and women more

24:20.980 --> 24:26.740
equal, but then it would be a totally separate thing to do it for age, for example.

24:26.740 --> 24:31.260
So you have to pick both kind of the sensitive attribute that you are trying to reduce

24:31.260 --> 24:35.300
disparity and you have to pick the metric for disparity.

24:35.300 --> 24:40.740
Were you saying that you're able to do multiple metrics in parallel or you're doing them

24:40.740 --> 24:42.380
seriously?

24:42.380 --> 24:46.940
Right now the techniques work for just one metric.

24:46.940 --> 24:49.860
So it will produce a series of models.

24:49.860 --> 24:52.780
And if you look at the graph, you can actually plot disparity.

24:52.780 --> 24:56.700
My accuracy and you'll have a models that are kind of on that Pareto optimal curve to look

24:56.700 --> 24:57.700
at.

24:57.700 --> 25:01.740
But then if you said, okay, well, now I want to look at that same chart for age.

25:01.740 --> 25:06.740
The models might be all over the place in the space of disparity and accuracy.

25:06.740 --> 25:13.220
So it's not a perfect technique, but there are some settings where it's quite useful.

25:13.220 --> 25:22.380
So kind of going back to this idea of abstraction and tools versus deeply understanding the problem

25:22.380 --> 25:28.340
domain and how to think about it in the context of your problem domain.

25:28.340 --> 25:32.780
I guess the challenge domain and your problem domain I don't know what the right terms are.

25:32.780 --> 25:35.300
But you mentioned the paper.

25:35.300 --> 25:40.580
There's that paper with all of the different disparity metrics and the like, is that

25:40.580 --> 25:45.940
the best way for folks to get up to speed on this or are there other resources that you've

25:45.940 --> 25:47.580
come across that are useful?

25:47.580 --> 25:54.580
Yeah, I think for fairness in particular, it's better to start, I think, with your application

25:54.580 --> 26:01.060
domain and understand, for example, if you're working in an employment setting, how do we

26:01.060 --> 26:04.380
think about fairness and what are the cases?

26:04.380 --> 26:11.180
And so in that case, we actually recommend that you talk to domain experts and even your

26:11.180 --> 26:15.700
legal department to understand what fairness means in that setting.

26:15.700 --> 26:20.180
And then you can go to the academic literature and start saying, okay, well, which metrics

26:20.180 --> 26:25.020
kind of line up with that higher level concept of fairness for my setting.

26:25.020 --> 26:30.980
But if you start with the metrics, it's very, I think it can be very overwhelming.

26:30.980 --> 26:36.660
And there's just many different metrics and a lot of them are quite different in other

26:36.660 --> 26:38.100
ways, they're very similar with each other.

26:38.100 --> 26:43.020
And so I find it much easier to first think, start with the domain expertise and know

26:43.020 --> 26:46.860
what you're trying to achieve in fairness and then start finding the metrics that line

26:46.860 --> 26:47.860
up with that.

26:47.860 --> 26:51.540
You're also starting to do some work in the differential privacy domain, tell me a little bit about

26:51.540 --> 26:52.540
that.

26:52.540 --> 26:59.620
Yeah, we announced a couple weeks ago that we are building an open source privacy platform

26:59.620 --> 27:02.740
with Harvard.

27:02.740 --> 27:07.980
And differential privacy is a really fascinating technology.

27:07.980 --> 27:13.460
It was first published in Microsoft Research in 2006.

27:13.460 --> 27:19.900
And it's a very interesting idea, but it has taken a while for it as an idea to mature

27:19.900 --> 27:23.060
and develop and actually be able to be used in practice.

27:23.060 --> 27:29.380
Harvard, now we're seeing several different companies who are using it in production.

27:29.380 --> 27:35.540
But in every case, the deployment was a very bespoke deployment with experts involved.

27:35.540 --> 27:39.780
And so we're trying to make a platform that makes it much easier for people to use these

27:39.780 --> 27:43.900
techniques without having to understand them as much.

27:43.900 --> 27:49.900
And so the idea is the open source platform can go on top of a data store and able you

27:49.900 --> 27:57.020
to do queries in a differentially private way, which means that actually it adds noise to

27:57.020 --> 28:03.060
the results so that you can't reconstruct the underlying data.

28:03.060 --> 28:07.900
And also then potentially use the same techniques to build simple machine learning models.

28:07.900 --> 28:14.660
And so we think this is particularly important for some of our really societally valuable

28:14.660 --> 28:15.660
data sets.

28:15.660 --> 28:20.340
For example, if I, there are data sets where people would like to do medical research,

28:20.340 --> 28:27.100
but because we're worried about the privacy of individuals, there's limits to what they

28:27.100 --> 28:28.100
can actually do.

28:28.100 --> 28:33.260
And if we use a differential private interface on that, with a lot more privacy guarantees

28:33.260 --> 28:39.860
and so we can unlock a new type of innovation and research and understanding our data.

28:39.860 --> 28:47.060
So I think we're really excited and think this could be the future of privacy in certain

28:47.060 --> 28:48.380
applications.

28:48.380 --> 28:49.980
But the tooling just isn't there.

28:49.980 --> 28:54.460
And so we're working on trying to make it easier for people to do that.

28:54.460 --> 28:59.260
We're building it in the open source because it's important that people can actually,

28:59.260 --> 29:02.900
it's very easy to get the implementation of these algorithms wrong.

29:02.900 --> 29:09.180
And so we want the community and the privacy experts to be able to inspect and test the

29:09.180 --> 29:12.580
implementations and have the confidence that it's there.

29:12.580 --> 29:15.940
And also we think this is such an important problem for the community.

29:15.940 --> 29:20.300
We would like anybody who wants to be rejoining in and working on this.

29:20.300 --> 29:22.980
This is not something that we can solve on our own.

29:22.980 --> 29:31.460
Yeah, differential privacy in general and differentially private machine learning are fascinating

29:31.460 --> 29:35.180
topics and ones that we've covered fairly extensively in the podcast.

29:35.180 --> 29:41.620
We did a series on differential privacy a couple of years ago maybe and it's continuing

29:41.620 --> 29:46.820
to be an interesting topic like the Census Bureau I think is using differential privacy

29:46.820 --> 29:53.460
for the first time next year and it's is both providing the kind of anticipated benefits

29:53.460 --> 29:59.980
but also raising some interesting concerns about an increased opacity I guess by on the

29:59.980 --> 30:03.420
part of researchers to the data that they want to get access to.

30:03.420 --> 30:06.260
Are you familiar with that challenge it?

30:06.260 --> 30:07.740
Yeah, absolutely.

30:07.740 --> 30:12.100
So the reality is, you know, people always want the most accurate data, right?

30:12.100 --> 30:16.300
It doesn't sound great to say, well, we're adding noise in the data is less accurate.

30:16.300 --> 30:17.300
Yeah.

30:17.300 --> 30:22.740
In a lot of cases, it is accurate enough for the tasks that you want to accomplish and

30:22.740 --> 30:29.620
I think we have to recognize that privacy is one of the sort of fundamental values that

30:29.620 --> 30:34.900
we want to uphold and so in some cases it's worth the cost.

30:34.900 --> 30:40.380
For the Census in particular, right, they to motivate the decision to start using this

30:40.380 --> 30:49.860
for the 2020 Census, they did a study where they took the reports from the 1940 Census and

30:49.860 --> 30:55.780
they were able to recreate something like 40% of Americans, you know, data with a result

30:55.780 --> 30:57.420
of just the outputs from the Census.

30:57.420 --> 30:58.420
Wow.

30:58.420 --> 31:01.900
They talked about me personally identify 40% of Americans.

31:01.900 --> 31:07.660
Yeah, that's, they talk, he talks about this in his ICML keynote for last year.

31:07.660 --> 31:10.340
So if you want to learn more, you can watch the keynote.

31:10.340 --> 31:14.500
But yeah, basically they took all the reports and they use some of these privacy attacks

31:14.500 --> 31:20.140
and they could basically recreate a bunch of the underlying data and you know, this is

31:20.140 --> 31:26.940
a real risk and so we have to recognize that yes, the Census results are incredibly important

31:26.940 --> 31:32.500
and they help us make many different decisions but also protecting people's data is important.

31:32.500 --> 31:36.780
And so some of it is education and changing our thinking and some of it is making sure

31:36.780 --> 31:43.220
that we use the techniques in the right way in that domain where you're not losing what

31:43.220 --> 31:46.900
you were trying to achieve in the first place but you are adding these privacy benefits.

31:46.900 --> 31:52.740
There are a couple of different ways that people have been applying differential privacy.

31:52.740 --> 31:58.380
One is a more centralized way where you're applying it to a data store.

31:58.380 --> 32:00.740
It sounds a little bit like that's where your focus is.

32:00.740 --> 32:06.100
Other like apples kind of a noted use case where they're applying differential privacy

32:06.100 --> 32:14.980
in a distributed manner kind of at the handset to keep user data on the iPhone but still

32:14.980 --> 32:21.020
provide information to kind of centrally for analysis.

32:21.020 --> 32:25.540
Am I correct that your focus is on the central wise use case or does the toolkit also support

32:25.540 --> 32:27.660
kind of the distributed use case?

32:27.660 --> 32:30.420
We are focusing on the global model.

32:30.420 --> 32:36.740
The local model works really well and particularly some of these user telemetry settings but it

32:36.740 --> 32:38.940
limits what you can do.

32:38.940 --> 32:45.340
You need like much larger volume to actually get the accuracy for a lot of the queries

32:45.340 --> 32:48.700
that you need and there aren't as many queries that you can do.

32:48.700 --> 32:53.980
And so the global model on the other hand, there's a lot more that you can do and still

32:53.980 --> 33:00.180
have reasonable privacy guarantees and so we felt like as I was saying we were motivated

33:00.180 --> 33:04.500
by these cases where we have the data sets like somebody is trusted to have the data

33:04.500 --> 33:10.020
sets but we can't really use them and so that looks like a global setting and so to start

33:10.020 --> 33:16.220
we're focused on the global piece but there are many cases where the local is promising

33:16.220 --> 33:21.820
and there are cases where we are doing that in our products and so it's certainly a direction

33:21.820 --> 33:23.940
that things could go.

33:23.940 --> 33:29.220
And differential privacy from a data perspective doesn't necessarily get you to a differentially

33:29.220 --> 33:30.580
private machine learning.

33:30.580 --> 33:36.660
Are you doing anything in particular on the differential private ML side of things?

33:36.660 --> 33:44.540
The plan is to do that but the project is pretty new so we haven't built it yet.

33:44.540 --> 33:49.940
And I guess before we wrap up you're involved in a bunch of kind of industry and research

33:49.940 --> 33:57.220
initiatives in the space that you've mentioned, SISML, MLSIS, a bunch of other things.

33:57.220 --> 34:03.300
Can you talk a little bit about some of the broader things that you're doing?

34:03.300 --> 34:12.460
Yeah so I helped found the, now I think named MLSIS systems in machine learning research

34:12.460 --> 34:18.780
conference and that was specifically because I've been working at this intersection for

34:18.780 --> 34:24.260
a while and there was some dark days where it was very hard to publish work because the

34:24.260 --> 34:29.060
machine learning community was like this is a systems result and the systems community

34:29.060 --> 34:35.420
was like this doesn't seem like a systems result and so we started the conference about

34:35.420 --> 34:40.980
two years ago and apparently many other people were feeling the same pain because even

34:40.980 --> 34:46.060
from the first conference we got excellent work, people's kind of top work which is always

34:46.060 --> 34:49.060
a challenge for the research conferences because people don't want to submit their best

34:49.060 --> 34:53.900
work to an unnamed conference, but there was such a gap for the community.

34:53.900 --> 34:59.140
So it's been really exciting to sort of see that community form more and now have a home

34:59.140 --> 35:01.620
where they can put their work in and connect.

35:01.620 --> 35:08.760
So I've also been running the machine learning systems, workshops at NURBS for several

35:08.760 --> 35:14.180
years now and that's been a really fun place because it really has helped us form the community

35:14.180 --> 35:19.020
particularly before we started the conference, but it's also a place where you can kind of

35:19.020 --> 35:23.740
explore new ideas like this last year, we're starting to see a lot more innovation at

35:23.740 --> 35:28.020
the intersection of programming languages and machine learning and so in the workshop

35:28.020 --> 35:32.660
format we can you know have several of those talks highlighted and kind of have a dialogue

35:32.660 --> 35:37.580
and show some of the emerging trends so that's been a really fun thing to be involved

35:37.580 --> 35:38.580
in.

35:38.580 --> 35:39.580
Awesome.

35:39.580 --> 35:40.580
Yeah.

35:40.580 --> 35:48.100
Was it last year that there was both the CISML workshop and ML for systems workshop

35:48.100 --> 35:49.580
and got really confusing?

35:49.580 --> 35:50.580
Yeah.

35:50.580 --> 35:51.580
This year too.

35:51.580 --> 35:52.580
This year too.

35:52.580 --> 35:54.500
Yeah.

35:54.500 --> 35:58.660
And I think you know that's a sign that the field is growing that it used to be that

35:58.660 --> 36:02.820
it felt like we didn't even have enough people for one room at the intersection of machine

36:02.820 --> 36:07.780
learning and systems and I think this last year there was maybe four or five hundred people

36:07.780 --> 36:13.980
in our workshop alone and so that's great now there's definitely room to have workshops

36:13.980 --> 36:20.940
on more focused topics right and so I think machine learning for systems is an area that

36:20.940 --> 36:26.420
people are really excited about now that we've kind of have more depth in understanding

36:26.420 --> 36:27.700
the intersection.

36:27.700 --> 36:33.940
For me it's very funny because that is really kind of the flavor of my thesis and which

36:33.940 --> 36:39.580
was a while ago and so it's fun to see it now starting to become kind of an area that

36:39.580 --> 36:41.260
people are excited about.

36:41.260 --> 36:47.500
The other conference that we didn't talk about ML for systems is all about using machine

36:47.500 --> 36:53.540
learning within computational systems networking systems as a way to optimize them.

36:53.540 --> 37:00.700
So for example ML to do database query optimization also a super interesting topic.

37:00.700 --> 37:01.700
Yeah.

37:01.700 --> 37:02.700
No.

37:02.700 --> 37:08.100
It absolutely is and I actually I really believe in that and I think for several years

37:08.100 --> 37:12.820
people were just trying to replace kind of all of the systems intelligent with one machine

37:12.820 --> 37:16.580
learning algorithm and it was not working very well and I think what we're seeing now

37:16.580 --> 37:22.580
is recognizing that a lot of the algorithms that we use to control systems were designed

37:22.580 --> 37:28.100
for that way and they work actually pretty well but on the other hand there's something

37:28.100 --> 37:34.460
that's dynamic about the world or the workload and so you do want this prediction capability

37:34.460 --> 37:40.460
built in and so a lot of the work now has a more sort of intelligent way of plugging

37:40.460 --> 37:46.060
the algorithms into into the system and so now we're actually starting to see promising

37:46.060 --> 37:48.740
results at this intersection.

37:48.740 --> 37:55.180
So my thesis work actually was a resource allocation that built models in real time in

37:55.180 --> 38:00.260
the operating system and allocated resources and it was exactly this piece where there was

38:00.260 --> 38:06.860
a modeling and a prediction piece but the final resource allocation algorithm was not

38:06.860 --> 38:08.340
purely machine learning.

38:08.340 --> 38:09.340
Awesome.

38:09.340 --> 38:10.340
Awesome.

38:10.340 --> 38:11.340
Wonderful conversation.

38:11.340 --> 38:14.780
Looking forward to catching up with you at NERP's hopefully.

38:14.780 --> 38:17.580
Thanks so much for taking the time to chat with us.

38:17.580 --> 38:21.060
Yes, thanks for having me and I look forward to seeing you at NERP's.

38:21.060 --> 38:22.060
Thank you.

38:22.060 --> 38:31.180
Alright everyone that's our show for today to learn more about this episode visit Twomolei.com.

38:31.180 --> 38:47.380
As always, thanks so much for listening and catch you next time.

