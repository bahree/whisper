1
00:00:00,000 --> 00:00:12,560
Hello everyone and welcome to Twimble Talk.

2
00:00:12,560 --> 00:00:16,460
The podcast where I interview interesting people doing interesting things and machine

3
00:00:16,460 --> 00:00:19,440
learning and artificial intelligence.

4
00:00:19,440 --> 00:00:22,640
I am very excited to share this interview with you.

5
00:00:22,640 --> 00:00:27,940
For this show, my guest is Chavier Amatriye, Chavier is a former researcher who went on

6
00:00:27,940 --> 00:00:33,060
to lead the machine learning recommendations team at Netflix and is now the vice president

7
00:00:33,060 --> 00:00:36,620
of engineering at Quora, the Q&A site.

8
00:00:36,620 --> 00:00:42,900
Chavier and I spend quite a bit of time digging into each of these experiences in the interview.

9
00:00:42,900 --> 00:00:46,140
Here are just a few of the things you'll learn from our discussion.

10
00:00:46,140 --> 00:00:50,300
Why Netflix invested $1 million in a Netflix prize?

11
00:00:50,300 --> 00:00:53,020
It didn't use the winning solution.

12
00:00:53,020 --> 00:00:57,900
What goes into engineering practical machine learning systems anyway?

13
00:00:57,900 --> 00:01:01,900
The problem that Chavier has with the deep learning hype.

14
00:01:01,900 --> 00:01:05,940
And what the heck is a multi-arm bandit and how can it help us?

15
00:01:05,940 --> 00:01:10,020
Of course, I'll be linking to the resources we mentioned in the show notes, which you'll

16
00:01:10,020 --> 00:01:15,180
be able to find at twimlai.com slash talk slash three.

17
00:01:15,180 --> 00:01:22,380
It's twimlai.com slash TALK slash the number three.

18
00:01:22,380 --> 00:01:27,180
A quick note before the interview, you've got just a few days left to enter into my drawing

19
00:01:27,180 --> 00:01:31,660
to win a free ticket to the O'Reilly AI conference.

20
00:01:31,660 --> 00:01:35,660
I'll talk about how to enter after the interview and in the show notes.

21
00:01:35,660 --> 00:01:37,820
And now on to the show.

22
00:01:37,820 --> 00:01:51,180
Hey, everyone, I'm here with Chavier Amatrian and Chavier, why don't we get started by

23
00:01:51,180 --> 00:01:52,700
here at Quora now?

24
00:01:52,700 --> 00:01:56,460
Why don't we have you talk a little bit about what you do there?

25
00:01:56,460 --> 00:01:57,460
Sure.

26
00:01:57,460 --> 00:02:03,980
So, I'm at Quora, I'm the BP of engineering, so I lead the whole engineering organization

27
00:02:03,980 --> 00:02:05,700
right now.

28
00:02:05,700 --> 00:02:08,540
My background, though, is more in machine learning.

29
00:02:08,540 --> 00:02:13,620
Previously to Quora, I was at Netflix and I was leading the machine learning recommendation

30
00:02:13,620 --> 00:02:16,020
steam at Netflix.

31
00:02:16,020 --> 00:02:20,100
And even before that, I was doing research and I was in academia.

32
00:02:20,100 --> 00:02:26,060
And my background, again, is on recommendations, machine learning, and so on, and I've published

33
00:02:26,060 --> 00:02:30,020
papers on that space for some years.

34
00:02:30,020 --> 00:02:35,860
So it's kind of interesting that somebody with this kind of background is now the BP of

35
00:02:35,860 --> 00:02:41,820
engineering of a growing company like Quora, where I need to deal with a lot of different

36
00:02:41,820 --> 00:02:45,860
concerns, not only machine learning, but it also tells you a little bit of story

37
00:02:45,860 --> 00:02:50,060
of what is important for Quora as a company as a product.

38
00:02:50,060 --> 00:02:54,340
And that also aligns with some of the trends that we're seeing in industry, right?

39
00:02:54,340 --> 00:03:01,900
That more and more the machine learning AI people that used to be like close in a room

40
00:03:01,900 --> 00:03:07,140
by a corner and they were like, the weirdos in the lab, now they're having a lot more

41
00:03:07,140 --> 00:03:12,820
influence on decisions that are being made on how to design programs and how to run companies.

42
00:03:12,820 --> 00:03:19,940
And in my case, that's probably like one of the reasons that I'm

43
00:03:19,940 --> 00:03:25,020
in this position now leading the whole engineering organization because for us, machine learning

44
00:03:25,020 --> 00:03:30,660
it's like a big part of our success and how we're growing.

45
00:03:30,660 --> 00:03:34,780
All right, so there's a ton in there and we'd really like to get to know you a little

46
00:03:34,780 --> 00:03:35,780
bit better.

47
00:03:35,780 --> 00:03:37,420
So let's rewind a bit.

48
00:03:37,420 --> 00:03:39,620
You mentioned that you spent some time in academia.

49
00:03:39,620 --> 00:03:41,460
Yeah, how did you learn machine learning?

50
00:03:41,460 --> 00:03:45,420
Where did you go to school and where were you working in academia?

51
00:03:45,420 --> 00:03:48,460
Yeah, that's a good question.

52
00:03:48,460 --> 00:03:57,780
So I'm actually kind of old for what you see right now and I have a long history behind

53
00:03:57,780 --> 00:03:58,780
me.

54
00:03:58,780 --> 00:04:05,260
And I'm saying that because when I did my PhD, which by the way, I did it back in Spain.

55
00:04:05,260 --> 00:04:07,660
I'm originally from Barcelona, Spain.

56
00:04:07,660 --> 00:04:14,180
So when I did my PhD, I was mostly interested in signal processing and particularly in signal

57
00:04:14,180 --> 00:04:19,140
processing and systems design related to audio and music.

58
00:04:19,140 --> 00:04:22,700
Actually, that's what my PhD was based on.

59
00:04:22,700 --> 00:04:31,780
And at that point in time, it was that age when multimedia and signal processing was kind

60
00:04:31,780 --> 00:04:35,300
of like the hot thing and machine learning was not too much.

61
00:04:35,300 --> 00:04:40,540
So I did use some machine learning here and there for different aspects of my research

62
00:04:40,540 --> 00:04:45,980
and particularly for some of the initial recommendation systems that I worked on that

63
00:04:45,980 --> 00:04:48,980
were related to music, but it wasn't my core area.

64
00:04:48,980 --> 00:04:53,340
So I was more into signal processing and systems doing my PhD.

65
00:04:53,340 --> 00:05:02,140
So I would say that I got into machine learning more on the chops and after I left my, you

66
00:05:02,140 --> 00:05:04,220
know, I did my PhD.

67
00:05:04,220 --> 00:05:11,220
I went and did some more multimedia related research in the University of California, Santa

68
00:05:11,220 --> 00:05:12,820
Barbara and UCSB.

69
00:05:12,820 --> 00:05:13,820
So I was there.

70
00:05:13,820 --> 00:05:18,140
I was working on virtual reality and immersive environments.

71
00:05:18,140 --> 00:05:20,340
And that was also very cool.

72
00:05:20,340 --> 00:05:25,380
It's kind of coming back again now, but I was really interested in that space combining

73
00:05:25,380 --> 00:05:30,980
signal processing and multimedia and this kind of immersive and virtual reality environment.

74
00:05:30,980 --> 00:05:35,940
But after that, I became more and more interested on the data side, right?

75
00:05:35,940 --> 00:05:41,260
Like how do we use the data and how do we infer information from the data?

76
00:05:41,260 --> 00:05:45,460
And particularly very interested in how do we understand users from the data, right?

77
00:05:45,460 --> 00:05:50,420
So that's what kind of led me to forget a little bit more about the signals that were

78
00:05:50,420 --> 00:05:55,420
a little bit like, you know, more there's, they're also data, but they're like cold data

79
00:05:55,420 --> 00:06:03,980
that come from systems and focus more on the human generated data and try to build intelligent

80
00:06:03,980 --> 00:06:05,420
systems that understand.

81
00:06:05,420 --> 00:06:12,860
So I did, then I switched my research and went into working for a few years in recommendations

82
00:06:12,860 --> 00:06:17,460
and using machine learning and different kind of approaches, not only machine learning,

83
00:06:17,460 --> 00:06:24,420
but also human computer interaction approaches to build this intelligent sort of like assistance

84
00:06:24,420 --> 00:06:26,420
that tell you what you like and what you don't like.

85
00:06:26,420 --> 00:06:32,700
So that's what actually led me eventually into net present to leading the recommendations

86
00:06:32,700 --> 00:06:33,700
team there.

87
00:06:33,700 --> 00:06:34,700
Okay.

88
00:06:34,700 --> 00:06:39,860
Now, you dangled a big shiny object in front of my eyes and that is signals processing.

89
00:06:39,860 --> 00:06:43,300
That was an area that I studied in grad school as well.

90
00:06:43,300 --> 00:06:48,860
And I'm curious, well, hey, I'm curious if you could explain wavelets to me because that

91
00:06:48,860 --> 00:06:52,700
was one thing that was game be a hard time, but actually, no, we're not going to talk

92
00:06:52,700 --> 00:06:54,700
about that.

93
00:06:54,700 --> 00:06:59,300
I'm wondering if you see any parallels.

94
00:06:59,300 --> 00:07:03,580
I'm wondering if there are any interesting things happening at the intersection of signals

95
00:07:03,580 --> 00:07:07,180
processing and machine learning just out of curiosity.

96
00:07:07,180 --> 00:07:10,980
Do you have you seen anything?

97
00:07:10,980 --> 00:07:15,620
There's actually a ton of those intersections.

98
00:07:15,620 --> 00:07:20,300
There's more of like the principles and how they intersect, but I would say probably

99
00:07:20,300 --> 00:07:26,020
more interesting now there is the intersection at the application side of things, right?

100
00:07:26,020 --> 00:07:36,020
So if you think about it, a lot of the systems that are now being built using machine learning

101
00:07:36,020 --> 00:07:43,780
approaches, particularly deep learning to understand things like speech recognition or image

102
00:07:43,780 --> 00:07:52,580
recognition, those were considered in the past like signal processing applications and

103
00:07:52,580 --> 00:07:58,260
for example, although I didn't professionally focus too much in speech recognition, I did

104
00:07:58,260 --> 00:08:06,660
study quite a lot of that and at that time we were using Hayden Mark of models and these

105
00:08:06,660 --> 00:08:12,060
other techniques that for us in the signal processing world, it wasn't, you know, there

106
00:08:12,060 --> 00:08:16,340
were just tools and means to an end so it wasn't like the most important part of the system

107
00:08:16,340 --> 00:08:19,500
although it was really like the core of it.

108
00:08:19,500 --> 00:08:25,740
But now that's moved towards some deep learning and RNNs and so on.

109
00:08:25,740 --> 00:08:30,100
So there's always been an intersection right between machine learning and signal processing

110
00:08:30,100 --> 00:08:35,940
and there's always a lot to say about how to interpret signals wherever they come from

111
00:08:35,940 --> 00:08:44,700
and those signals again could be audio, could be speech, music, video, images and you need

112
00:08:44,700 --> 00:08:50,860
to build a system that actually either understand those things or even able to generate them

113
00:08:50,860 --> 00:08:56,660
in some way and there's always a, well not always but at some point it's clear that that's

114
00:08:56,660 --> 00:09:01,820
evolved more into having a layer of intelligence in the middle that it's going to be learned

115
00:09:01,820 --> 00:09:06,140
and that comes from a machine learning system that it's sort of like at the heart of any

116
00:09:06,140 --> 00:09:08,140
of those systems.

117
00:09:08,140 --> 00:09:09,140
Great.

118
00:09:09,140 --> 00:09:10,140
Great.

119
00:09:10,140 --> 00:09:16,900
So you made your way from academia and ended up at Netflix immediately prior to where

120
00:09:16,900 --> 00:09:22,260
you are now at Quora and your focus there was on recommendation systems.

121
00:09:22,260 --> 00:09:23,260
Yeah.

122
00:09:23,260 --> 00:09:29,300
I started with a very specific focus on recommendation systems.

123
00:09:29,300 --> 00:09:34,540
You could consider it as a continuation and natural continuation of the Netflix

124
00:09:34,540 --> 00:09:40,140
prize, the famous $1 million Netflix prize which by the way that's what got me connected

125
00:09:40,140 --> 00:09:47,500
to Netflix as I was dabbling with it and also a part of using that data set for some

126
00:09:47,500 --> 00:09:49,100
of my research.

127
00:09:49,100 --> 00:09:50,100
Okay.

128
00:09:50,100 --> 00:09:51,100
So yeah.

129
00:09:51,100 --> 00:09:56,140
So I started with what you could consider like the continuation of that Netflix prize

130
00:09:56,140 --> 00:10:01,620
but already working for Netflix and we eventually grew the team to be more of a core machine

131
00:10:01,620 --> 00:10:06,660
learning algorithms team that was building not only recommendations but algorithms for

132
00:10:06,660 --> 00:10:13,100
search and for different things where they do images and it was it grew to sort of like

133
00:10:13,100 --> 00:10:19,380
being a core machine learning slash algorithms team that was serving different purposes beyond

134
00:10:19,380 --> 00:10:20,380
recommendations.

135
00:10:20,380 --> 00:10:25,140
But recommendations is something that is very important for Netflix right.

136
00:10:25,140 --> 00:10:30,820
So that was really like probably the core of the team at any given time.

137
00:10:30,820 --> 00:10:31,820
Okay.

138
00:10:31,820 --> 00:10:40,020
So in terms of the you mentioned the next the Netflix prize am I correct that the the

139
00:10:40,020 --> 00:10:47,820
winning prize entry was never really implemented at Netflix.

140
00:10:47,820 --> 00:10:55,100
I'm glad you asked this because I get this question all the time and I react to it.

141
00:10:55,100 --> 00:11:00,460
By saying it is correct the final entry was not used.

142
00:11:00,460 --> 00:11:02,660
That doesn't mean that it was useless right.

143
00:11:02,660 --> 00:11:08,620
So there's I'm saying that because people immediately when I say that and we wrote it in

144
00:11:08,620 --> 00:11:13,980
a blog post when I was in Netflix at some point and even though it was very clearly explained

145
00:11:13,980 --> 00:11:18,820
people still took away like oh Netflix wasted a million dollars and they didn't use the

146
00:11:18,820 --> 00:11:19,820
outcome.

147
00:11:19,820 --> 00:11:25,300
That's not true actually Netflix got way more than one million dollar back in research

148
00:11:25,300 --> 00:11:30,420
and in interesting stuff that is being used and was used in different parts of different

149
00:11:30,420 --> 00:11:31,420
systems.

150
00:11:31,420 --> 00:11:37,940
So so if there's a difference between was the final entry use and the answer is no it was

151
00:11:37,940 --> 00:11:38,940
not used.

152
00:11:38,940 --> 00:11:48,740
There were over 130 different machine learning models combined in an ensemble.

153
00:11:48,740 --> 00:11:54,300
Most of the different models that were there were adding just a tiny increase in accuracy

154
00:11:54,300 --> 00:11:56,700
and a lot of complexity and they were not worth it.

155
00:11:56,700 --> 00:12:05,220
So the reality is that two of the models on their own gave like enough accuracy that the

156
00:12:05,220 --> 00:12:12,820
other hundred and thirty some were not needed or they were not worth the R.I.

157
00:12:12,820 --> 00:12:16,780
That's it doesn't mean that they were not useful to understand what they were adding

158
00:12:16,780 --> 00:12:19,460
and how they were adding it.

159
00:12:19,460 --> 00:12:27,540
So again the story is the final prize winning entry with a complex combination of all of

160
00:12:27,540 --> 00:12:34,660
those methods in an ensemble was not used as it was but the learning were worth much

161
00:12:34,660 --> 00:12:40,780
more than what was invested in the prize and part of the final winning entry the most

162
00:12:40,780 --> 00:12:44,420
important method were actually used directly in production.

163
00:12:44,420 --> 00:12:47,260
Okay.

164
00:12:47,260 --> 00:12:53,500
I came across this recently in an interesting blog post by Josh Bloom over at Wise and he

165
00:12:53,500 --> 00:13:02,380
talked about the economics of machine learning basically all of the various trade offs that

166
00:13:02,380 --> 00:13:06,780
get you know that come up when real business is trying to figure out how to put machine

167
00:13:06,780 --> 00:13:11,900
learning into production and that was one of the examples he used about how I forget

168
00:13:11,900 --> 00:13:16,220
how many pages or something the final algorithm was but a hundred and thirty models that's

169
00:13:16,220 --> 00:13:24,860
a huge that's a huge model yeah and I know Josh very well so we're friends and he knows

170
00:13:24,860 --> 00:13:33,580
a lot about what we've talked about this in person and you know the thing is that story

171
00:13:33,580 --> 00:13:41,300
is so juicy that you can spin it in many different ways I actually recently got this

172
00:13:41,300 --> 00:13:47,900
pretty crazy but I did get in my Facebook feed an advertisement from a math works trying

173
00:13:47,900 --> 00:13:52,300
to sell me mad lab that was using that story and saying something like Netflix did not

174
00:13:52,300 --> 00:13:59,540
use their final winning entry we can help you with mad lab what was that have to do with

175
00:13:59,540 --> 00:14:05,660
mad lab right so I don't even I don't get where they're going at all with that well I don't

176
00:14:05,660 --> 00:14:12,580
know but you know that's the point is that yeah the the real story is yes you do need to

177
00:14:12,580 --> 00:14:17,500
be concerned and I'm I always say the same I mean you know you need to be concerned about

178
00:14:17,500 --> 00:14:23,140
system complexity and about making sure that whatever you do in research is actually deployable

179
00:14:23,140 --> 00:14:30,300
and it's and it's good to or easy to build engineering around it but that's very different

180
00:14:30,300 --> 00:14:35,540
from saying that the Netflix prize was a waste of time or money sure so can you maybe

181
00:14:35,540 --> 00:14:43,540
spend some time walking out walking through some of the various factors right so you

182
00:14:43,540 --> 00:14:49,300
mention engineering time and there's you know so there's obviously like an implementability

183
00:14:49,300 --> 00:14:54,900
you know from a complexity perspective you know they're going to be data aspects there's

184
00:14:54,900 --> 00:15:02,260
computational obviously you know when you think about you know practical machine learning

185
00:15:02,260 --> 00:15:07,700
and the the issues that you know you're you're an engineering VP of engineering now not a VP

186
00:15:07,700 --> 00:15:11,940
of machine learning research or something when you think about you know engineering these systems

187
00:15:11,940 --> 00:15:19,780
at large scale what are the things that you need to think about oh there's like a long list

188
00:15:19,780 --> 00:15:28,740
of things and you mentioned a few them system complexity is one which actually spans into

189
00:15:28,740 --> 00:15:35,620
different some areas and different concerns that relate to the system to the complexity of the

190
00:15:35,620 --> 00:15:42,420
system one of them which is soft and overlooked is simply cost right it's like if you can do something

191
00:15:42,420 --> 00:15:50,340
in a single machine which I have this kind of infamous slide that I when I show people some

192
00:15:50,340 --> 00:15:55,380
people don't like very much is I tell people that they can do probably almost everything they need

193
00:15:55,380 --> 00:16:02,020
to do machine learning in a single machine and I have reasons to say that but the point is

194
00:16:02,020 --> 00:16:08,260
that if you add a necessary system complexity first of all you're going to have a lot more cost

195
00:16:08,260 --> 00:16:13,220
so you're going to have this now a huge number of machines of machine that you're going to have

196
00:16:13,220 --> 00:16:19,780
to maintain in a cluster or pay Amazon for the AWS cost right so that's one and it's probably

197
00:16:19,780 --> 00:16:26,580
obvious and it's probably not the most important the most important one is system complexity reduces

198
00:16:26,580 --> 00:16:33,140
your speed of innovation and if you have a system that is really complex from the get go

199
00:16:33,140 --> 00:16:39,460
innovating on it becomes like a huge pain right because then I'm trying to tweak something and it

200
00:16:39,460 --> 00:16:44,260
turns out that that's something it's just one of the 10,000 knob that are in the system and it's

201
00:16:44,260 --> 00:16:50,420
hard to know what it did it's hard to understand whether it improves things and if you keep your

202
00:16:50,420 --> 00:16:55,860
system as simple as possible as long as possible your innovation is going to improve and your

203
00:16:55,860 --> 00:17:02,260
innovation speed because you're going to be in a much better position to then change things

204
00:17:02,260 --> 00:17:07,460
dramatically improve them understand what you're doing and what is improving and at some point

205
00:17:07,460 --> 00:17:14,020
you need to add complexity there's no way around it it's like complexity might add enough

206
00:17:14,900 --> 00:17:21,060
improvement either in accuracy or basically in whatever metric you care about that it's worth adding

207
00:17:21,620 --> 00:17:27,860
but the problem is you don't want arbitrary complexity from the start because that

208
00:17:27,860 --> 00:17:34,180
midterm and longterm is going to impact you're going to be end up in a local optimal sort of speak

209
00:17:34,180 --> 00:17:39,700
and you're never going to reach that global one that you would be getting if you keep your

210
00:17:39,700 --> 00:17:46,660
option simple as much as possible interesting to me the thing that it brought up was the notion of

211
00:17:46,660 --> 00:17:51,780
technical debt that's typically applied to code right code debt is there's anyone have you come

212
00:17:51,780 --> 00:17:59,380
across anyone that's thought this through in terms of algorithmic debt oh yeah there's this

213
00:17:59,380 --> 00:18:06,020
interesting paper that was published actually originally was published in a works of the iCore

214
00:18:06,020 --> 00:18:15,140
organizing nips and it's called a high interest credit card of machine learning debt and it's

215
00:18:15,140 --> 00:18:21,140
a very good read it's by a couple of authors from google by the way so they know what they're

216
00:18:21,140 --> 00:18:28,020
talking about in terms of machine learning debt so it's something that it's been discussed

217
00:18:28,020 --> 00:18:36,020
again even in papers right so right so it's it's it's an something that any organization will

218
00:18:36,020 --> 00:18:41,460
face at some point and it's something that it's really important and it's really important

219
00:18:41,460 --> 00:18:48,100
at many levels not only at the level of the system itself but also and I would go further that

220
00:18:48,100 --> 00:18:54,020
that's part of sort of like the the core of the machine learning algorithm algorithmic design right

221
00:18:54,020 --> 00:19:01,380
it's like it's okam's razor principle of you know if you have a possibility of choosing between

222
00:19:01,380 --> 00:19:07,300
two things always choose the simplest one and part of the reason is because you want to minimize

223
00:19:07,300 --> 00:19:12,820
your debt as long as possible and only make things more complicated when they really need to be

224
00:19:12,820 --> 00:19:18,980
and they're adding up enough so that goes back to the lesson learned from the Netflix prize it like

225
00:19:18,980 --> 00:19:23,940
you know yeah sure you can have you can go for the more complex solution but is the delta

226
00:19:23,940 --> 00:19:29,380
improvement that is adding worth the huge increasing complexity and many times the answer is

227
00:19:29,380 --> 00:19:36,020
going to be no that's an interesting segue to one of the topics that I wanted to chat with you

228
00:19:36,020 --> 00:19:44,020
about you recently tweeted about a natural language processing course and the hashtag you use

229
00:19:44,020 --> 00:19:50,660
was no deep learning and across a number of of your public appearances you've maybe developed

230
00:19:51,540 --> 00:19:56,820
our little reputation for mr hashtag no deep learning and of course I'm being I'm being

231
00:19:56,820 --> 00:20:02,340
artificially you know controversial here yeah I understand that this is you know it's a

232
00:20:02,340 --> 00:20:09,140
it's a tool in the toolbox but some of our earlier discussion about system complexity I think

233
00:20:09,140 --> 00:20:13,540
is one of the issues that you have with deep learning maybe walk us through you know what your

234
00:20:13,540 --> 00:20:18,340
position how you think of your position on deep learning and you know why you bring it up

235
00:20:18,340 --> 00:20:23,940
interesting when I talk about deep learning I always start by having a few slides in my presentation

236
00:20:23,940 --> 00:20:29,460
that explain how deep learning works right so I want to get that out of the way and say hey

237
00:20:29,460 --> 00:20:35,380
I know the deep learning works and it's great for a few things actually particularly for

238
00:20:35,380 --> 00:20:39,060
natural language processing I think that it's getting to a point where it's the default

239
00:20:39,060 --> 00:20:46,900
tool for many things and it's great so the reason I was using the hashtag is just to warn people

240
00:20:46,900 --> 00:20:52,180
that if they were looking for deep learning it wasn't available in that course so I think it is

241
00:20:52,180 --> 00:21:01,140
it's very important for people to understand what is the right tool for the right task and

242
00:21:02,820 --> 00:21:08,500
for example we use deep learning at Quora for several things right we have a lot of text and going

243
00:21:08,500 --> 00:21:17,860
back to the NLP example there's many things now in text processing that RNNs are you know they're

244
00:21:17,860 --> 00:21:23,620
actually the simplest solution there is because you can you can find some of this

245
00:21:25,060 --> 00:21:30,740
ready available open source tool kit that have already been trained and you can even use the

246
00:21:30,740 --> 00:21:35,940
model as it is you don't even need to have your own data set or then you can retrain it but

247
00:21:35,940 --> 00:21:44,100
that basically becomes simple enough that that could be your default approach to an NLP task

248
00:21:44,660 --> 00:21:51,620
that you have in hand but that's very different from saying that that's equally true for

249
00:21:51,620 --> 00:21:57,380
all machine learning applications and you need to understand like what is the complexity you're

250
00:21:57,380 --> 00:22:03,380
paying for defaulting to machine learning for everything you have and I've seen a couple of

251
00:22:03,380 --> 00:22:10,340
examples recently where I think we're you know in a dangerous situation where a lot of people

252
00:22:11,140 --> 00:22:18,820
especially like more junior researchers or engineers that they're you know they've come into

253
00:22:18,820 --> 00:22:24,660
industry right at the cost of the deep learning bubble or wave or whatever we want to call it

254
00:22:25,220 --> 00:22:31,540
and their their mind goes straight into deep learning as the default solution for anything

255
00:22:31,540 --> 00:22:40,420
and I've seen cases where I've had engineers in some companies tell me hey I'm using this

256
00:22:41,700 --> 00:22:50,340
TensorFlow architecture on a problem where I have 10,000 examples and 30 features and I want to

257
00:22:50,340 --> 00:22:56,020
ask you a question and my answer like why are you doing this to yourself I mean if you have

258
00:22:56,020 --> 00:23:04,340
10,000 examples and 30 features do you really think you need a deep learning model with a bunch

259
00:23:04,340 --> 00:23:11,700
of layers and most of the time the answer is no and even if the classifier you're building

260
00:23:12,660 --> 00:23:22,100
with that deep learning architecture is let's say in the best case one percent better than the

261
00:23:22,100 --> 00:23:26,820
one you could be building with a simple logistic regression you're still going to be better off

262
00:23:26,820 --> 00:23:32,420
going for the logistic regression because what going back to what I was saying before your ability to

263
00:23:32,420 --> 00:23:38,740
innovate on that initial model is going to be much bigger than your ability to innovate on a very

264
00:23:38,740 --> 00:23:46,660
complex deep neural net that you don't really understand what's going on inside so I guess my

265
00:23:46,660 --> 00:23:52,500
the point that I'm trying to make when I talk about quote unquote no deep learning is that deep

266
00:23:52,500 --> 00:23:58,420
learning should be another of the tools we have in our toolkit and there's a lot of other very

267
00:23:58,420 --> 00:24:06,580
interesting machine learning tools and even research that is going on that it's we should

268
00:24:07,460 --> 00:24:12,020
still pay attention to there's a problem also in the research world right now with deep learning

269
00:24:12,020 --> 00:24:19,380
is that because it's so new and there's so many so much low-hang improved it feels like you know

270
00:24:19,380 --> 00:24:24,980
that it's the easiest way to get a paper except it is to do an incremental improvement or not

271
00:24:24,980 --> 00:24:30,020
so incrementable an improvement on some deep learning approach and that's why we're seeing all

272
00:24:30,020 --> 00:24:36,980
the conferences now dominated with deep learning things right even when you go to a

273
00:24:36,980 --> 00:24:45,220
computer conference like KDD or the ACM recommender systems conference that I'm going to be attending

274
00:24:45,220 --> 00:24:50,260
in September you start seeing like a bunch of deep learning papers because it's new it's easy to

275
00:24:50,260 --> 00:24:56,900
be innovating using deep learning but we run the risk of like saying oh yeah this is the one thing

276
00:24:56,900 --> 00:25:02,980
that works for everything and we're going to try to find all the nails that apply to this hammer

277
00:25:02,980 --> 00:25:09,060
and we'll think that they're all they all look the same and and I think that's there is a danger

278
00:25:09,060 --> 00:25:15,460
in that so you've touched a little bit on some of the things you're doing at Quora maybe tell us

279
00:25:15,460 --> 00:25:20,740
a little bit about you know tell us a bit about your experiences there and you know what are some

280
00:25:20,740 --> 00:25:30,020
of the interesting problems that you face there yeah sure so I that's a great question one of the

281
00:25:30,020 --> 00:25:36,420
things that I love about Quora and one of the reasons as I said before that we have a VP of

282
00:25:36,420 --> 00:25:40,740
engineering with this kind of background in machine learning and algorithms is that

283
00:25:41,940 --> 00:25:49,060
everywhere I look on our product and our the issues that we're dealing with I see problems that

284
00:25:49,060 --> 00:25:56,980
are solvable and should be solved through machine learning right so I know if I sorry for interrupting

285
00:25:56,980 --> 00:26:03,300
but it's likely that most of the people listening know what Quora is but maybe you can start with

286
00:26:03,300 --> 00:26:11,220
just an explanation of the site and the mission sure that's yeah that's that's a very good point

287
00:26:12,260 --> 00:26:16,580
and it's a very good point because also even people that know us and use this frequently they have

288
00:26:16,580 --> 00:26:24,020
a misconception about what Quora is so Quora is on the surface is a question and answer site and

289
00:26:24,020 --> 00:26:33,060
application but our mission goes beyond that so the mission of Quora is to grow and share the

290
00:26:33,060 --> 00:26:39,620
world's knowledge and we think that the question answer paradigm is really well suited for actually

291
00:26:39,620 --> 00:26:46,980
growing and sharing knowledge just to give a different example of the only other Quora and

292
00:26:46,980 --> 00:26:54,500
quote company that has a similar mission which would be Wikipedia Wikipedia also believes in the spreading

293
00:26:54,500 --> 00:26:59,860
or growing the knowledge but they believe in the encyclopedic format and that leads to a bunch

294
00:26:59,860 --> 00:27:06,340
of different part decisions of course so we feel like question entering and a broader notion of

295
00:27:06,340 --> 00:27:12,180
what knowledge is so Wikipedia is about factual knowledge we think that for example an expert opinion

296
00:27:12,180 --> 00:27:19,140
is also knowledge and should be included in any knowledge base so all of that defines our decisions

297
00:27:19,140 --> 00:27:25,380
and using question answer for now is working really well and we think it's the ideal vehicle

298
00:27:25,380 --> 00:27:30,980
but we are not close to trying different things and actually we do have even different things as

299
00:27:30,980 --> 00:27:37,460
of today in our product that enable that knowledge growing and knowledge sharing so

300
00:27:37,460 --> 00:27:45,620
so another way to look at to understand Quora is the different sort of like networks that overlay

301
00:27:45,620 --> 00:27:53,780
in the product so we do have obviously a knowledge network and even another one that is a topical

302
00:27:53,780 --> 00:27:58,900
network so we have entities of knowledge that are connected to each other topics that are related

303
00:27:58,900 --> 00:28:05,620
to each other and then on top of that we add the social aspect right so then we have people and

304
00:28:05,620 --> 00:28:10,020
we have people that are connected to other people and we have people that are connected to topics

305
00:28:10,020 --> 00:28:16,900
and to knowledge entities and this sort of like different overlays of different graphs at different

306
00:28:16,900 --> 00:28:22,820
levels and the different connections between them is what makes the whole data problem very exciting

307
00:28:22,820 --> 00:28:29,380
because we have a lot of applications that cross the different networks in different directions

308
00:28:29,380 --> 00:28:37,140
and we have for example algorithms that are purely on the content space and they tell us how

309
00:28:38,180 --> 00:28:43,060
good is the quality of a given piece of content we have other algorithms that tell us

310
00:28:43,540 --> 00:28:49,620
how likely is a person to answer a question on a given topic we have different kinds of

311
00:28:50,500 --> 00:28:55,460
machine learning algorithms that their purpose is sort of like trying to understand and predict

312
00:28:55,460 --> 00:29:01,780
different aspects of this dynamic system and the relations between all these different entities

313
00:29:02,420 --> 00:29:06,580
so again examples of things that we do we do a lot of recommendations

314
00:29:08,100 --> 00:29:15,140
you have initially in your home page you'll see a feed of different stories that include questions

315
00:29:15,140 --> 00:29:22,500
and answers that we're optimizing for you to be interested on and that's kind of similar to

316
00:29:22,500 --> 00:29:29,620
the Facebook feed that has other implications and a different objective function so recommendations

317
00:29:29,620 --> 00:29:36,900
like that recommendations that you get through email we optimize the notifications that you get

318
00:29:36,900 --> 00:29:41,620
through the different devices also using machine learning that's all on the personalization side of

319
00:29:41,620 --> 00:29:51,380
things then we have content approaches to infer the quality of a content to do things like ranking

320
00:29:51,380 --> 00:29:58,740
answers according to how good they are we have things related to a lot of the text side of things

321
00:29:59,380 --> 00:30:07,780
automatic topic labeling how to infer a topic out of a given text how to find similarities

322
00:30:07,780 --> 00:30:17,620
in questions and answers how to find duplicates and then also we have the whole abuse side of things

323
00:30:17,620 --> 00:30:23,700
which also uses machine learning we need to one of the things that courage known about for is

324
00:30:23,700 --> 00:30:28,660
you know keeping high quality content and that's the quality piece but also keeping a very healthy

325
00:30:28,660 --> 00:30:36,020
positive community and we do that with very aggressive sort of norms and also algorithms that detect

326
00:30:36,020 --> 00:30:43,140
any form of spam harassment bad actors and so on so forth and each one of them is a different

327
00:30:43,140 --> 00:30:48,180
machine learning algorithm so it's really exciting in that sense because we have covering sort of

328
00:30:48,180 --> 00:30:56,100
like a huge space of applications and data types that go into this applications interesting

329
00:30:56,980 --> 00:31:02,580
can you talk a little bit about the extent to which you use hybrid machine learning plus human

330
00:31:02,580 --> 00:31:08,820
yeah obviously there's a big component of the site that you could argue as hybrid as users are

331
00:31:08,820 --> 00:31:16,020
ranking different answers but are there ways that you're using hybrid approaches behind the scenes

332
00:31:16,900 --> 00:31:27,620
yes we are so so one way to think about it is initially all everything all of this was manual

333
00:31:27,620 --> 00:31:32,340
right then the first initial beta version of kora there were no algorithms in place and all

334
00:31:32,340 --> 00:31:40,900
that needed to be manual so we do have a team of moderators and people that look at content

335
00:31:41,540 --> 00:31:46,260
and there's always a point where algorithms are not going to be sufficient and you need somebody

336
00:31:46,260 --> 00:31:53,780
to look at the nuances of like is this answer about this politician really violating our norms yes

337
00:31:53,780 --> 00:31:58,980
or no and it's like really nuanced and we need to have a person look at it so the way we think about

338
00:31:58,980 --> 00:32:07,300
it is there's if you think about any content moderation issue there's always going to be a high

339
00:32:08,180 --> 00:32:13,780
portion of the stuff that you have on your site that is going to be good and it's going to be good

340
00:32:13,780 --> 00:32:20,100
with no doubt so you can have algorithms that say hey above of this threshold I'm totally positive

341
00:32:20,100 --> 00:32:25,860
this is good stuff we don't need to worry about it there's always going to be a huge not a huge

342
00:32:25,860 --> 00:32:30,660
but some part of your content is going to be really bad and there's no doubt about it so there's

343
00:32:30,660 --> 00:32:35,540
another threshold that tells you below this threshold I'm just going to remove this stuff because

344
00:32:35,540 --> 00:32:41,780
it's basically crap and you don't want that's how you keep the quality of your content

345
00:32:41,780 --> 00:32:46,660
in the site right now there's this gray area in between those two thresholds and that's the

346
00:32:46,660 --> 00:32:52,900
tricky part right so you have to do two things one is there's you know you have to have people

347
00:32:52,900 --> 00:32:57,460
then look at this gray area and decide yeah this is not really that bad it should where it should be

348
00:32:57,460 --> 00:33:04,340
okay with it and at the same time you need to improve your algorithms to get those two thresholds

349
00:33:04,340 --> 00:33:08,260
as close to each other as possible and that's very interesting right because it represents sort of

350
00:33:08,260 --> 00:33:13,780
like a research challenge for us to improve our machine learning algorithms say hey we want the

351
00:33:13,780 --> 00:33:19,940
gray area of the things are uncertain to over time become as small as possible and we're doing

352
00:33:19,940 --> 00:33:26,660
that at the same time the gray area is still there and when when we have things in the gray area we

353
00:33:26,660 --> 00:33:34,260
need to use some humans in the loop to understand what's going on uh-huh so if if Quora were to do

354
00:33:34,900 --> 00:33:41,780
a Quora prize analogous to the Netflix prize what would it be about what are some of the biggest

355
00:33:41,780 --> 00:33:53,300
challenges that you face well there's in each of those dimensions that I mentioned before there's

356
00:33:54,180 --> 00:34:01,540
challenges that are still not resolved but I guess thinking of the Netflix prize and something

357
00:34:01,540 --> 00:34:08,260
that would be kind of similar and I think it's very interesting and probably that's an obvious

358
00:34:08,260 --> 00:34:16,500
direction we would go is that for something like knowledge there is also the problem

359
00:34:16,500 --> 00:34:21,700
which similar to a Netflix prize of how do you get the right piece of content to the right person

360
00:34:22,260 --> 00:34:30,260
and content is expressed in two ways right one is a content that you can consume so that's an answer

361
00:34:30,260 --> 00:34:35,780
that you can read and you can enjoy and you can learn from it and the other one is a question

362
00:34:35,780 --> 00:34:42,420
that you can answer so both of those things how to route them to the right person and how to

363
00:34:42,420 --> 00:34:47,860
optimize algorithm for those two things are at the core of what we're doing and they're very

364
00:34:47,860 --> 00:34:55,060
important for us so I think we could think of like uh again drawing the analogy of the Netflix prize

365
00:34:55,060 --> 00:35:02,740
of like question and answer recommendation uh being like a very interesting topic uh that um for

366
00:35:02,740 --> 00:35:09,780
us it's like a super interesting challenge uh it also connects like many different dimensions on

367
00:35:10,580 --> 00:35:15,620
different overlays that I was talking about because it's not only about personalization but you

368
00:35:15,620 --> 00:35:22,020
also have to care about content quality right and you have to care about those different aspects

369
00:35:22,020 --> 00:35:29,940
and how they uh feed into what what the users are going to be doing and reacting to short term

370
00:35:29,940 --> 00:35:35,700
but more importantly what they're going to be reacting to long term uh I've talked about that in the

371
00:35:35,700 --> 00:35:42,420
path in some of my presentations like this sort of like tension between short term metrics and long

372
00:35:42,420 --> 00:35:48,420
term metrics and that's something that a lot of companies have done the wrong thing and they've

373
00:35:48,420 --> 00:35:53,940
gone downhill because of that and it's really important to understand uh for example in the context

374
00:35:53,940 --> 00:36:00,900
of content how to avoid clickbait right and if you're optimizing for some things you're going to get

375
00:36:00,900 --> 00:36:07,460
clicks sure but those clicks are going to turn into people not visiting your site ever again after

376
00:36:07,460 --> 00:36:13,540
a couple weeks uh so all those things sort of like uh fit into this picture of sort of like

377
00:36:14,420 --> 00:36:17,220
content recommendation or knowledge recommendation

378
00:36:17,220 --> 00:36:27,540
how do you address the short term long term trade off now and maybe even in the context of uh a

379
00:36:27,540 --> 00:36:34,260
clickbait type of application so so there's different things that go into it uh I would say that

380
00:36:34,980 --> 00:36:44,100
that's uh that's one of the most interesting research areas that I don't think it's been really

381
00:36:44,100 --> 00:36:50,180
solved even in uh research literature because there's it's very hard to get enough good quality

382
00:36:50,180 --> 00:36:55,220
datasets to even do something about it if you're if you're a research in academia and in the industry

383
00:36:55,620 --> 00:37:00,420
uh I mean as far as I know from the field that I talk there's obviously different things that we're

384
00:37:00,420 --> 00:37:10,340
all doing but uh holistic approach to it is it's hard um the the one important thing is you do need

385
00:37:10,340 --> 00:37:16,420
to make sure that you're running your AB test with the right sort of metrics right because at the end

386
00:37:16,420 --> 00:37:23,140
of the day you can be optimizing whatever you want in the lab and say oh uh it's a ranking problem

387
00:37:23,140 --> 00:37:30,180
I'm going to be optimizing NDCG but the reality of that metric that you're optimizing in the lab with your

388
00:37:30,180 --> 00:37:36,180
algorithm might not really correlate perfectly to what you want to get in the product in that long

389
00:37:36,180 --> 00:37:42,420
term metric so first you need to make sure that you whatever you tune in your in the lab you run

390
00:37:42,420 --> 00:37:50,100
AB test long enough term with the right metric to understand like what is the met what what what

391
00:37:50,100 --> 00:37:57,620
what are the effects that whatever you're doing have on the users and then you kind of work backwards

392
00:37:57,620 --> 00:38:02,660
from that right once you have the right metric on your AB test you know oh if I do this my users end

393
00:38:02,660 --> 00:38:09,140
up not coming back after two weeks what did I do then you back you you kind of work backwards

394
00:38:09,140 --> 00:38:14,660
from that and try to understand like what are the metrics in the lab that you could have used to

395
00:38:14,660 --> 00:38:21,780
sort of like predict that kind of behavior and the kind of effect right so building regression

396
00:38:21,780 --> 00:38:31,860
models from sort of like your uh easy to compute uh metrics which they're all going to be related to

397
00:38:31,860 --> 00:38:37,860
some kind of error or some kind of uh information retrieval precision recall whatever you will

398
00:38:37,860 --> 00:38:44,180
into the real world of usage I think that's that's very important and then there's a there's a

399
00:38:44,180 --> 00:38:48,580
ton of other things that you can do once you understand those dynamics in trying to

400
00:38:50,500 --> 00:38:57,460
define your training set in a way that actually um defines the problem in the right way

401
00:38:57,460 --> 00:39:07,220
uh and and sometimes I've talked about this also in the past people have this mistake of I need

402
00:39:07,220 --> 00:39:13,380
to use all the data that I have and I need to use the raw data that I have and sometimes that's

403
00:39:13,380 --> 00:39:18,980
not really the answer you might need to use some data and not others because some of the data that

404
00:39:18,980 --> 00:39:24,100
you might be feeding into the into your model might be teaching the model the wrong thing or you

405
00:39:24,100 --> 00:39:29,620
might need to weight your data in a way that some is more important than other because it needs to

406
00:39:29,620 --> 00:39:35,780
longer term effects that you're interested on while other might lead to a click but nothing else

407
00:39:35,780 --> 00:39:42,900
so there's there's a lot of sort of like uh different details going into the recipe uh but again

408
00:39:42,900 --> 00:39:47,380
I don't think there is a very holistic approach to it or not that I'm aware of

409
00:39:47,380 --> 00:39:55,700
okay uh one thing that that came to mind for me was and this is maybe going back to our discussion

410
00:39:55,700 --> 00:40:01,860
around deep learning uh there is some research happening around RNNs and you know

411
00:40:01,860 --> 00:40:07,780
when the the reinforcement or the score you know comes later and how the RNN can optimize for

412
00:40:08,580 --> 00:40:13,860
um you know this delayed gratification so to speak and so you know maybe this is where

413
00:40:13,860 --> 00:40:20,500
you know if this gets sophisticated enough this is where you get some benefit from

414
00:40:20,500 --> 00:40:26,020
introducing the complexity of RNNs where an otherwise simple model might come into play

415
00:40:26,900 --> 00:40:36,340
yeah uh that's definitely true uh so models or approaches that have any sense of sequencing or time

416
00:40:36,340 --> 00:40:44,420
or evolution over time do have some uh that some benefits uh and and and you can use them

417
00:40:44,420 --> 00:40:50,660
it's not only about RNNs another thing that comes to mind it's uh some reinforcement learning

418
00:40:50,660 --> 00:40:57,460
approaches uh I mean the typical what one of the typical ways to deal with this uh to use

419
00:40:57,460 --> 00:41:04,100
and some form of multi-arm banded uh approach uh to deal with the exploration exploitation

420
00:41:04,100 --> 00:41:08,420
tradeoff it's it's more of like yeah you know I know that you're clicking on this but let me try to

421
00:41:08,420 --> 00:41:16,580
explore more things let me try to come up over time uh have uh you know my model converge to something

422
00:41:16,580 --> 00:41:22,980
that is a global optimal rather than getting stuck on that local one where I am right now so yes

423
00:41:23,780 --> 00:41:32,260
you're right I mean and and some of the sequential RNNs with some form of uh memory and and

424
00:41:32,260 --> 00:41:38,740
ability to sort of like uh remember different stages and sort of like end up converging

425
00:41:38,740 --> 00:41:46,980
over time into a better optimal uh they're super interesting yeah before we before we get too far

426
00:41:46,980 --> 00:41:56,420
can you explain simply uh multi-arm banded yeah uh so the idea uh it's pretty simple I mean multi-arm

427
00:41:56,420 --> 00:42:05,380
banded comes from this notion of you have uh the the typical image that people use is the

428
00:42:06,980 --> 00:42:11,540
slot machines in a casino uh you imagine that you're going to casino and you have 10 slot

429
00:42:11,540 --> 00:42:16,340
machines in front of you and you don't know which arm you should pull that's where the multi-arm

430
00:42:16,340 --> 00:42:22,180
banded come from and you start trying one and say oh this one is giving me some interesting

431
00:42:22,180 --> 00:42:26,660
prices but should I try another one because maybe the one that I have next to me is actually

432
00:42:26,660 --> 00:42:32,340
better than this one and how to deal with this dilemma of out of multiple arms that you could be

433
00:42:32,340 --> 00:42:39,860
pulling there's some that you have more information about and you know uh with a degree of certainty

434
00:42:39,860 --> 00:42:43,860
how well they're doing and there are others that you don't really know anything about them

435
00:42:43,860 --> 00:42:48,820
should you risk yourself and going to the ones you don't know anything about them or should you

436
00:42:48,820 --> 00:42:54,820
just stick to the one that kind of works but maybe it's not the optimal one so I think that's the

437
00:42:54,820 --> 00:43:02,660
whole point of the multi-arm banded uh approaches is like they try to define a way in which you can have

438
00:43:02,660 --> 00:43:08,340
an optimal policy to deciding whether you should continue pulling from the same arm or you should

439
00:43:08,340 --> 00:43:20,500
go to a different one and there's a lot of literature on on this and and you can read about it and

440
00:43:21,380 --> 00:43:26,580
I usually joke about it there's a lot of literature about multi-arm banded but there's only one

441
00:43:26,580 --> 00:43:33,060
that actually works in practice but I don't know if I want to give that away I mean it's it's it's

442
00:43:33,060 --> 00:43:42,340
pretty it's pretty uh well-known in in industry that uh Thompson sampling is the easiest and

443
00:43:42,340 --> 00:43:48,500
sort of like more practical approach to multi-arm banded so I think that I'm not giving too much

444
00:43:48,500 --> 00:43:57,620
away by saying that right so what uh what do you find the most exciting about machine learning

445
00:43:57,620 --> 00:44:04,180
right now obviously there's a ton of things going on there's uh deep learning stuff there's

446
00:44:04,180 --> 00:44:09,860
the work that's happening around bots there's applying deep learning to NLP like you know given

447
00:44:09,860 --> 00:44:16,260
everything that's going on like what what's the most exciting and and do you get to apply that in

448
00:44:16,260 --> 00:44:23,860
your work um and what's the most exciting thing that you're actually working on um so I think the

449
00:44:23,860 --> 00:44:33,620
most exciting thing for me it's almost a non-technical thing it's more of a this thing coming from

450
00:44:33,620 --> 00:44:42,500
society as a whole that it's accepted as a given that machine learning and AI is inevitably part

451
00:44:42,500 --> 00:44:48,100
of making a better future right and I think you know there's still so people that were arguing about

452
00:44:48,100 --> 00:44:54,100
dangers and and about robots taking over and so on but I think generally speaking

453
00:44:54,740 --> 00:45:02,020
society is convinced and it's pretty uh much you know all bought in you know self-driving cars

454
00:45:02,020 --> 00:45:07,140
a couple of years ago people thought uh we were crazy about self-driving cars and now they're

455
00:45:07,140 --> 00:45:15,060
already being tested uh with um people writing in them so so I think this sort of like change in

456
00:45:15,060 --> 00:45:22,180
society and in mindset and people realizing that oh machine learning is not really evil it can be

457
00:45:22,180 --> 00:45:28,340
it's a tooling can be used in my benefit and it's something that I expect things to have

458
00:45:28,340 --> 00:45:36,900
to have so not very long ago seeing something that was an algorithm or machine learning was like whoa

459
00:45:36,900 --> 00:45:43,220
what's going on I'm losing control this is not something I like and now it's shifting to the opposite

460
00:45:43,220 --> 00:45:50,180
like you expect applications you expect uh gadgets to have intelligent and to have machine learning

461
00:45:50,180 --> 00:45:55,220
otherwise you're disappointed like oh my gosh I need to tell this phone everything I want the

462
00:45:55,220 --> 00:46:00,260
phone should know what I want right so uh I think that's that's a very very interesting

463
00:46:00,980 --> 00:46:07,620
shift and and it kind of connects a lot with some of the things we're doing at Kora right and Kora

464
00:46:07,620 --> 00:46:14,820
we are very user focused and we want to we want to keep this warm feeling of you're in the community

465
00:46:14,820 --> 00:46:18,740
you're sharing knowledge this is very important for you it's very important for the people

466
00:46:18,740 --> 00:46:23,780
but you're going to be surrounded by all this different algorithms that make your life much better

467
00:46:23,780 --> 00:46:28,500
and they protect you from bad people and they protect you from horrible content that you don't

468
00:46:28,500 --> 00:46:33,300
want to read and they help you get your content to the right people that want to uh read about it

469
00:46:33,300 --> 00:46:40,260
and they're going to be helped by it uh so this combination of so that the warmth of community

470
00:46:40,260 --> 00:46:47,140
social aspects and knowledge but also surrounded by all this different algorithms in a seamless way

471
00:46:47,140 --> 00:46:52,420
I think that's super exciting and it's something that uh you need to uh strike the right balance

472
00:46:52,420 --> 00:46:57,940
but uh it's something that just a few years ago we wouldn't thought about because you know again

473
00:46:57,940 --> 00:47:04,900
algorithms uh were the the the school evil thing that you'd kind of like wanted to stay away from

474
00:47:04,900 --> 00:47:08,740
uh so I think that's that's a very interesting trend and uh something that I'm excited about

475
00:47:10,500 --> 00:47:15,300
we're coming to the end of our time but I've got a couple more quick questions for you the first is

476
00:47:16,100 --> 00:47:19,300
you go to a lot of conferences what are your favorite conferences in the space?

477
00:47:20,500 --> 00:47:27,700
um I wouldn't say I go to a lot of conferences unfortunately uh especially now since my

478
00:47:27,700 --> 00:47:34,420
time as a VP of engineering is pretty precious and I don't get that much time there's some conferences

479
00:47:34,420 --> 00:47:40,100
that I have ties for a very long time and I keep going to them because I I'm very interested in

480
00:47:40,100 --> 00:47:45,700
the content but also I'm interested in the community one of them is uh is a small conference actually

481
00:47:45,700 --> 00:47:51,380
the it's the ACM recommender systems conference that's a conference that is purely focused on

482
00:47:51,380 --> 00:47:58,260
personalization and recommendations and I helped start the whole thing I was uh the general chair

483
00:47:58,260 --> 00:48:05,780
for that in 2010 uh back in Barcelona and I kept kind of keep in touch it's in one of the

484
00:48:05,780 --> 00:48:10,580
interesting things about this community which uh I think it's a little bit similar to for example

485
00:48:10,580 --> 00:48:18,900
KDD is that it's a very diverse kind of uh audience and you don't get the pure machine learning

486
00:48:18,900 --> 00:48:28,740
nips audience everyone focused on the algorithm and uh you know squeezing uh one percent more or

487
00:48:28,740 --> 00:48:37,300
less uh RMSC or MAE out of their algorithm there's a combination of algorithms but also application

488
00:48:37,300 --> 00:48:42,980
and then user oriented research which uh I think connects to the vision that I was saying right

489
00:48:42,980 --> 00:48:49,700
this connection between uh user orientation and algorithms uh it's very interesting so yeah uh

490
00:48:49,700 --> 00:48:54,740
the ACM recommender systems conference which by the way is happening in Boston if anyone is

491
00:48:55,540 --> 00:49:01,700
listening from Boston or wants to travel there uh this year is in the US and it's going to be

492
00:49:01,700 --> 00:49:10,180
super interesting uh when is it coming up right yeah it's in September 15 so yeah in a few weeks

493
00:49:10,180 --> 00:49:18,180
uh we're going to be there um and I'm just to give an example I'm giving a tutorial uh with

494
00:49:18,180 --> 00:49:27,860
together with Deepak Aval from LinkedIn on uh all the latest research and um all the evolution of

495
00:49:27,860 --> 00:49:33,060
recommendation systems in industry and we're going to be giving a holistic perspective of

496
00:49:33,860 --> 00:49:39,540
me coming from Netflix and now Kora and him having been at Yahoo and now meeting machine learning

497
00:49:39,540 --> 00:49:46,340
at uh LinkedIn uh so it's going to be sort of like uh an overview of all this kind of uh

498
00:49:46,340 --> 00:49:52,740
machine learning techniques for recommendations uh so that's that's an example of a small

499
00:49:52,740 --> 00:49:59,060
focus conference but also with a very broad audience which I kind of enjoy uh KDD which

500
00:49:59,060 --> 00:50:05,780
just happened to be in San Francisco recently uh I like the community a lot and uh I think I

501
00:50:05,780 --> 00:50:14,900
can find all of uh very interesting uh uh approaches and applications um I usually um yeah I'm very

502
00:50:14,900 --> 00:50:21,060
application driven in my uh approach to machine learning so although I will I will read all the papers

503
00:50:21,060 --> 00:50:30,180
or not not all sorry some papers from MIPS and ICML I I tend to uh go to more sort of like

504
00:50:30,180 --> 00:50:36,420
application driven conferences and and then there's also a lot of uh small conferences that are

505
00:50:36,420 --> 00:50:43,780
organized now uh there are kind of local and focused uh on the industry side of machine learning

506
00:50:44,900 --> 00:50:50,900
ML conference one that comes to mind that I attend regularly because I find the audience to be

507
00:50:50,900 --> 00:50:57,620
very uh interesting and very engaging and uh it's a lot of uh practitioners from industry

508
00:50:57,620 --> 00:51:04,420
mix together with uh a bunch of researchers and that intersection I think it's uh it's really

509
00:51:04,420 --> 00:51:12,420
interesting um great great and then uh one more question that you're in a particularly good

510
00:51:12,420 --> 00:51:18,340
place to answer for us and that is who are the people to follow uh the machine learning folks to

511
00:51:18,340 --> 00:51:27,140
follow on Quora? Oh that's a great question but we have a lot of them so we've been doing actually

512
00:51:27,140 --> 00:51:36,020
uh uh very strong push for this product feature that we have which is sessions which is similar to

513
00:51:36,020 --> 00:51:43,860
an AMA AMA and we brought in uh I would say like all the top machine learning researchers

514
00:51:44,740 --> 00:51:53,220
to do some uh session in the past uh we've had people like uh I mean most of the deep learning folks

515
00:51:53,220 --> 00:52:01,380
like Jan Lecun and uh Joshua Benjo and we've had Andrew Eng we've had Peter Norbeck

516
00:52:02,500 --> 00:52:12,020
we've had um a lot of different researchers and I would say most of the authors of the famous

517
00:52:14,100 --> 00:52:21,140
machine learning books like Kevin Murphy from Google and so on uh or we we we had Ian

518
00:52:21,140 --> 00:52:28,180
Goodfellow the main author of the deep learning book also recently so there's like a good

519
00:52:29,300 --> 00:52:35,540
yeah I would say 50 people that you would follow we've also had uh people that leave machine learning

520
00:52:36,180 --> 00:52:44,340
in different companies like uh Amazon we have my friend Ralph Herbys from Amazon uh or Joaquin

521
00:52:44,340 --> 00:52:50,660
from Facebook uh so there's like a huge machine learning community in Quora that is very active

522
00:52:50,660 --> 00:52:56,340
and very uh strong so it's one of our strongest areas right now so I would recommend

523
00:52:57,060 --> 00:53:02,260
people who are interested in in machine learning there's like a ton of knowledge there

524
00:53:02,260 --> 00:53:10,740
and growing so uh yeah great great uh well Chaviet thank you so much for spending the time with us

525
00:53:10,740 --> 00:53:16,580
I learned a ton and I'm sure the folks that listen uh will as well uh anything you'd like to leave

526
00:53:16,580 --> 00:53:24,020
us with uh no I mean thanks for having me and uh it was great to share a little bit of that knowledge

527
00:53:25,220 --> 00:53:31,300
in this different format which it's also it's a way of spreading knowledge and I look forward

528
00:53:31,300 --> 00:53:37,860
to interacting with people uh especially on Quora I myself write a lot of different answers

529
00:53:37,860 --> 00:53:42,420
on different topics including machine learning uh that's a good point before we go where can folks

530
00:53:42,420 --> 00:53:49,940
find you how can folks engage with you uh I'm pretty public on Twitter as you mentioned you

531
00:53:49,940 --> 00:53:57,300
you had seen a bunch of my tweets so I'm uh they can find me on Twitter on uh Shamat X-A-M-A-T

532
00:53:57,300 --> 00:54:05,620
or on Quora I'm also very active uh so you can follow me on Quora and message me there

533
00:54:05,620 --> 00:54:14,420
um I usually keep a very active public profile so uh it's not hard to find me and I have a pretty

534
00:54:14,420 --> 00:54:20,900
weird name and last name so it's like it's hard to uh to go into the wrong direction if you

535
00:54:20,900 --> 00:54:25,460
if you google my name yeah all right great thanks so much Chaviet yeah thank you Sam

536
00:54:29,540 --> 00:54:34,500
all right everyone that's it for today's interview before we go a reminder that

537
00:54:34,500 --> 00:54:39,860
this week in machine learning and AI and O'Reilly have partnered to offer one lucky listener

538
00:54:39,860 --> 00:54:45,780
a free pass to the inaugural O'Reilly AI conference which will be held at the end of September

539
00:54:45,780 --> 00:54:51,540
in New York City you can enter via twitter or the twimmaleye.com website by doing one of the

540
00:54:51,540 --> 00:54:58,340
following three things the preferred way of entering is via twitter just follow at twimmaleye

541
00:54:58,340 --> 00:55:05,620
t-w-i-m-l-a-i and retweet the contest tweet that i'll pin to the account and post in the show notes

542
00:55:05,620 --> 00:55:10,900
do those two things and you'll be entered if you're not on twitter you can sign up for my newsletter

543
00:55:10,900 --> 00:55:16,980
at twimmaleye.com slash newsletter and add a note please enter me in the additional comments field

544
00:55:18,100 --> 00:55:23,300
finally if you're not on twitter and you aren't interested in the newsletter no problem

545
00:55:23,300 --> 00:55:29,620
just go to the contact form on twimmaleye.com and send me a message with that form using AI

546
00:55:29,620 --> 00:55:34,980
contest as the subject the drawing will be open to entries through September first and i'll

547
00:55:34,980 --> 00:55:40,660
announce the winner on the September second show good luck and hope to see you in New York thanks

548
00:55:40,660 --> 00:55:56,260
again for listening

