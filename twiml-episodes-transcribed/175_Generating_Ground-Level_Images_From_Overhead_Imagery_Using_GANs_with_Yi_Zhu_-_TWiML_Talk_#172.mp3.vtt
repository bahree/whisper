WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.400
I'm your host Sam Charrington, a quick update for our faithful listeners and fans.

00:34.400 --> 00:37.960
We recently asked you to help us in our bid to secure our nomination in the People's

00:37.960 --> 00:39.520
Choice Podcast Awards.

00:39.520 --> 00:44.960
Well, thanks to you, we're a finalist in the best technology podcast category, along

00:44.960 --> 00:49.440
with other noteworthy shows like Riko Diko and the Verge Cast.

00:49.440 --> 00:54.480
The award ceremony will be stream live on September 30th, which turns out to be international

00:54.480 --> 00:56.400
podcast day.

00:56.400 --> 01:00.400
Keep your fingers crossed for us and a huge thanks to everyone who voted.

01:00.400 --> 01:05.160
You have our utmost gratitude.

01:05.160 --> 01:12.360
Today we're joined by EZU, a PhD candidate at UC Merced focused on geospatial image analysis.

01:12.360 --> 01:17.360
In our conversation, E and I discussed his recent paper, what is it like down there?

01:17.360 --> 01:22.800
Using dense ground level views and image features, from overhead imagery, using conditional

01:22.800 --> 01:25.080
generative adversarial networks.

01:25.080 --> 01:30.120
E and I discussed the goals of this research, which is to train effective land use classifiers

01:30.120 --> 01:35.500
on proximate or ground level images, and how he uses conditional gans along with images

01:35.500 --> 01:40.960
sourced from social media to generate artificial ground level images for this task.

01:40.960 --> 01:46.320
We also explore future research directions, such as the use of reversible generative networks,

01:46.320 --> 01:51.800
as proposed in the recently released open AI glow paper to produce higher resolution

01:51.800 --> 01:53.320
images.

01:53.320 --> 01:54.320
Enjoy.

01:54.320 --> 01:59.560
All right, everyone, I am on the line with EZU.

01:59.560 --> 02:03.080
E is a PhD candidate at UC Merced.

02:03.080 --> 02:05.840
E, welcome to this week in machine learning and AI.

02:05.840 --> 02:06.840
Hi, Sam.

02:06.840 --> 02:07.840
Hi, everybody.

02:07.840 --> 02:08.840
So thank you for having me.

02:08.840 --> 02:10.800
It's really excited to be here.

02:10.800 --> 02:11.800
Absolutely.

02:11.800 --> 02:15.520
I'm really looking forward to digging into your work today to get us started.

02:15.520 --> 02:19.400
Why don't you tell us a little bit about your background and how you got started in machine

02:19.400 --> 02:20.400
learning.

02:20.400 --> 02:25.080
So actually, my major at first is wireless communication, because my undergraduate study

02:25.080 --> 02:28.120
was on like signal processing and information theory.

02:28.120 --> 02:32.960
But then at the year of 2012, when deep learning first the show is great potential on image

02:32.960 --> 02:38.080
net challenge, so I was fascinated by simplicity and surprisingly good results.

02:38.080 --> 02:43.480
So I started to attend some like cargo challenges using deep learning, including the famous dog

02:43.480 --> 02:45.920
and cat classification challenge.

02:45.920 --> 02:50.120
So although at that time, most of the challenge winners are still using random forest of XG

02:50.120 --> 02:51.120
boost.

02:51.120 --> 02:55.800
But later I found myself like attractive to this machine learning field and especially

02:55.800 --> 02:56.800
deep learning.

02:56.800 --> 02:58.720
So I'm thinking so why not change a major?

02:58.720 --> 03:04.040
So in my understanding, so images are still signals captured by optical sensors.

03:04.040 --> 03:06.920
So they are not that different from my previous study.

03:06.920 --> 03:12.920
So I switched my major to computer vision and start my PhD study at UC Merced in 2014.

03:12.920 --> 03:16.160
So basically, I have two lines of researcher directions.

03:16.160 --> 03:19.560
Right now, one is geospatial image analysis.

03:19.560 --> 03:24.400
The other is a video analysis, but I think today I would just focus first one.

03:24.400 --> 03:29.160
You mentioned that you started competing in cargo competitions.

03:29.160 --> 03:31.760
What were some of the competitions that you competed in?

03:31.760 --> 03:34.600
So the first one will be the dog and cat challenge.

03:34.600 --> 03:39.480
So that's the object, like a recognition problem, like a binary thing.

03:39.480 --> 03:47.080
And later, I also competing the challenge of the eye contact of the driver to try to

03:47.080 --> 03:51.640
understand if the driver is sleepiness or is it a safe driver or something.

03:51.640 --> 03:57.000
So there's also an autonomous driving challenge.

03:57.000 --> 04:01.080
And then also for the insurance to see a car's image.

04:01.080 --> 04:04.600
So whether it is like a damage or not, is it a new car?

04:04.600 --> 04:09.040
So how much price should it sell for a second hand car?

04:09.040 --> 04:11.400
So that's for the insurance company.

04:11.400 --> 04:14.760
So there are several more, but I cannot remember the details.

04:14.760 --> 04:15.760
Yeah.

04:15.760 --> 04:16.760
Okay.

04:16.760 --> 04:17.760
Awesome.

04:17.760 --> 04:22.160
I don't know the last two, the driver eye contact or the insurance one, but the dog and

04:22.160 --> 04:25.360
cat one is one that we have gone through.

04:25.360 --> 04:30.920
I'm working with a group of folks to kind of go through the fast AI course and they talk

04:30.920 --> 04:34.640
about that dog and cats one in there quite extensively.

04:34.640 --> 04:37.680
How do you do and the Kaggle challenges you did?

04:37.680 --> 04:41.560
So firstly, I just tried like a linear regression.

04:41.560 --> 04:43.760
So that's the most basic model.

04:43.760 --> 04:48.440
I think most people still use it today for normal questions.

04:48.440 --> 04:52.040
And then I also tried like a random forest and XG boost.

04:52.040 --> 04:58.040
So at that time, the most winners are using those two techniques and then model assembling

04:58.040 --> 05:00.840
to get to the best score they can.

05:00.840 --> 05:02.800
But then I also tried like deep learning, right?

05:02.800 --> 05:06.400
So convolutional neural networks for this image recognition task.

05:06.400 --> 05:11.240
And at first, they cannot compete with like random forest because the images are not that

05:11.240 --> 05:12.240
large.

05:12.240 --> 05:15.120
So you know, like deep learning is a data hangry model, right?

05:15.120 --> 05:19.880
So if you have more data and if you have like a clean data, so you can get a very good

05:19.880 --> 05:20.880
result.

05:20.880 --> 05:25.680
But if you don't have enough data, so maybe sometimes it doesn't compete with those traditional

05:25.680 --> 05:26.680
algorithms.

05:26.680 --> 05:27.680
Yeah.

05:27.680 --> 05:30.880
That basically the four algorithms I'm using for Kaggle challenges.

05:30.880 --> 05:33.400
How did you rank in the competitions?

05:33.400 --> 05:35.760
Did you rank highly in any of them?

05:35.760 --> 05:40.960
I didn't rank like a top one or top 10 thumbs in, but I'm usually in the top 10%.

05:40.960 --> 05:41.960
So.

05:41.960 --> 05:42.960
Okay.

05:42.960 --> 05:43.960
Nice.

05:43.960 --> 05:48.600
And so your research now is on understanding images using deep learning.

05:48.600 --> 05:51.560
And you recently published a paper.

05:51.560 --> 05:56.360
The paper is called what is it like down there generating dense ground level views and

05:56.360 --> 06:01.960
image features from overhead imagery using conditional Gans.

06:01.960 --> 06:03.760
That there's a ton in there.

06:03.760 --> 06:07.880
Yeah, yeah, the title is quite long.

06:07.880 --> 06:09.680
The title is quite long.

06:09.680 --> 06:11.480
I mean, maybe let's get started.

06:11.480 --> 06:15.720
I think, you know, we've talked about Gans quite a bit on the podcast, but why don't you

06:15.720 --> 06:19.200
start us out by talking about conditional Gans?

06:19.200 --> 06:20.200
Okay.

06:20.200 --> 06:21.200
Yeah.

06:21.200 --> 06:24.520
So yeah, I have seen those like a Gans episode here and there.

06:24.520 --> 06:29.680
So it's very hot topic like last year and there's a trend is still going on.

06:29.680 --> 06:33.400
So as we know, so Gans consists of two components.

06:33.400 --> 06:36.160
So one generator, one discriminator.

06:36.160 --> 06:40.640
But the thing is when you generate something, so you're generating from a random noise.

06:40.640 --> 06:46.160
So each time you will get different results, different output, but sometimes you really

06:46.160 --> 06:49.840
want to have some like fixed output, right?

06:49.840 --> 06:53.400
So that is what conditional Gans for.

06:53.400 --> 06:59.520
And for like, for example, sometimes a like a fashion, like a shoe designer, they want

06:59.520 --> 07:03.840
to design using again to design a new shoe.

07:03.840 --> 07:09.520
But if you didn't control the output, sometimes it will begin something really random.

07:09.520 --> 07:15.240
So if we can give the model some information like prior information, like some texturing

07:15.240 --> 07:17.400
bad things like I want a shoe.

07:17.400 --> 07:20.840
So then the output will definitely be a shoe image.

07:20.840 --> 07:22.840
So that's what Gans for.

07:22.840 --> 07:23.840
So see again.

07:23.840 --> 07:28.680
So the conditional again, learn the distribution condition on some auxiliary information.

07:28.680 --> 07:35.600
So those auxiliary information can be class labels, or which is the most standard form,

07:35.600 --> 07:41.240
or texting bad things for generating images, or like image to image translation.

07:41.240 --> 07:46.800
So here in our work, so we use conditional again because we know what we want, right?

07:46.800 --> 07:51.120
We want ground level images corresponding to the overhead image.

07:51.120 --> 07:56.640
So given me overhead image, I want to generate a similar looking ground level images.

07:56.640 --> 08:01.560
So the overhead image will be considered as a prior information.

08:01.560 --> 08:02.560
Yeah.

08:02.560 --> 08:04.800
So that's why we choose a conditional again.

08:04.800 --> 08:13.240
And what you're going for kind of visually is if you send in to the system, the image

08:13.240 --> 08:18.960
of, you know, what an area that looks like farmland, you want to generate ground level

08:18.960 --> 08:21.080
pictures that look like farmland.

08:21.080 --> 08:26.080
Similarly, if you're sending in overhead images of an urban setting, you want it to generate

08:26.080 --> 08:27.080
urban images.

08:27.080 --> 08:28.080
Yes.

08:28.080 --> 08:29.080
Exactly.

08:29.080 --> 08:34.360
Tell us about the different data sources that you use to make all this happen.

08:34.360 --> 08:35.360
Okay.

08:35.360 --> 08:38.560
So the ground level images, the problem there is their spars.

08:38.560 --> 08:42.760
So what we can have is a satellite images, right?

08:42.760 --> 08:44.120
Satellite images is dense.

08:44.120 --> 08:45.120
So in everywhere.

08:45.120 --> 08:49.800
So it has all the satellite images at every spatial location.

08:49.800 --> 08:53.480
So that is our image input source.

08:53.480 --> 08:58.600
So the ground chooses and cover map is from the LCM data set.

08:58.600 --> 09:06.000
So our study region is a 77 by 71 kilometer region containing that in London.

09:06.000 --> 09:10.680
So the geographic images are labeled as urban or rural based.

09:10.680 --> 09:15.200
So then we can do a very simple like a binary classification problems.

09:15.200 --> 09:18.600
So our input sources are like a lot.

09:18.600 --> 09:25.240
So from like Google maps, statistics and a geographic API and also the satellite images.

09:25.240 --> 09:28.680
So those are the three image sources we're using.

09:28.680 --> 09:35.240
The LCM data set is that your overhead images or those your ground images.

09:35.240 --> 09:36.880
Those are the overhead images.

09:36.880 --> 09:37.880
Okay.

09:37.880 --> 09:41.680
And the ground level images is from the geographic.

09:41.680 --> 09:48.640
And that is a website and by some London researchers, so those are for the whole United Kingdom.

09:48.640 --> 09:53.680
So the length of our classes are provided on one kilometer grade.

09:53.680 --> 09:57.800
So for every grade, they have the user provided ground level images.

09:57.800 --> 09:59.840
So that's pretty accurate.

09:59.840 --> 10:02.240
So that is our ground level images.

10:02.240 --> 10:05.160
And the LCM is the overhead images.

10:05.160 --> 10:10.840
So we have the corresponding relationship to train our models to do some experiments.

10:10.840 --> 10:17.760
And so I realized that what you're doing here with Gantt is fundamentally generative.

10:17.760 --> 10:21.880
But part of the way the problem is set up sounds like an information retrieval problem,

10:21.880 --> 10:26.960
like you have this overhead image and you have this corpus of ground level images kind

10:26.960 --> 10:28.200
of find the best one.

10:28.200 --> 10:29.880
Are those related in any way?

10:29.880 --> 10:32.440
Oh, so there are there is difference there.

10:32.440 --> 10:38.080
So for like a image retrieval problem is so given me overhead images, I want to find

10:38.080 --> 10:40.720
the best matching like ground level images, right?

10:40.720 --> 10:44.160
So there is like a there's a database of the ground level images.

10:44.160 --> 10:47.080
So we try to find the like most matching one.

10:47.080 --> 10:52.360
So but for generate models, it's not where we want some specific images.

10:52.360 --> 10:57.400
So we don't want some specific ground level images to corresponding to the overhead image.

10:57.400 --> 11:00.320
We want the whole data distribution to fit.

11:00.320 --> 11:06.160
So the we want the generator to generate some real looking images to fit the data distribution.

11:06.160 --> 11:11.560
We don't care about so which images we're generating, we just want it to look realistic

11:11.560 --> 11:12.840
for that category.

11:12.840 --> 11:14.960
So that's the only difference.

11:14.960 --> 11:20.320
And do you do any kind of information retrieval as part of the solution?

11:20.320 --> 11:27.560
In other words, is your GAN conditioned only on the overhead image or is it conditioned

11:27.560 --> 11:33.560
on the subset of images that you retrieve from a database based on the overhead image?

11:33.560 --> 11:34.960
That's a good question.

11:34.960 --> 11:42.120
So actually right now we only like based our model on GAN, so we're not we're not using

11:42.120 --> 11:44.280
any information retrieval thing.

11:44.280 --> 11:49.680
So but in the future, we want to use that because the land use classification problem is

11:49.680 --> 11:50.680
really challenging.

11:50.680 --> 11:52.160
It's really, really hard.

11:52.160 --> 11:54.960
So maybe later I will introduce my another work.

11:54.960 --> 12:00.160
So it's also about large-scale land use classification, but the classification accuracy

12:00.160 --> 12:03.480
is only like 20%, 30%, so it's pretty challenging.

12:03.480 --> 12:08.360
So we need to have some like a human prior knowledge inside it.

12:08.360 --> 12:12.680
So like the information retrieval, like exciting information you mentioned about.

12:12.680 --> 12:15.120
So we will do that in the future work.

12:15.120 --> 12:21.160
So in this work, what are some of the big challenges that you had to overcome in the

12:21.160 --> 12:22.920
spaper and this research?

12:22.920 --> 12:23.920
Okay.

12:23.920 --> 12:27.720
So can I like a backup a little bit to talk about the motivation of this work?

12:27.720 --> 12:30.040
So that would be more clear.

12:30.040 --> 12:31.040
Okay.

12:31.040 --> 12:37.120
So the motivation to like all of our work is because although we have like enormous

12:37.120 --> 12:42.440
amount of online images like from Flaker, Instagram, so all those images, all those videos,

12:42.440 --> 12:46.200
we could use it to do some geospatial analysis.

12:46.200 --> 12:52.520
But if we want to do a detailed land use classification map, so land use I mean is, for

12:52.520 --> 12:57.480
example, so whether a building is a hospital or whether it's a shopping mall or something.

12:57.480 --> 12:59.160
So how do we use the land?

12:59.160 --> 13:02.560
So whether it's residential or for office use.

13:02.560 --> 13:08.840
So but overhead images cannot handle such case because overhead images is from like above,

13:08.840 --> 13:09.840
right?

13:09.840 --> 13:11.640
So from above, you can only see a building.

13:11.640 --> 13:13.680
So you cannot see inside of the building.

13:13.680 --> 13:15.960
So you don't know what that design for.

13:15.960 --> 13:18.240
So that's what we come up with.

13:18.240 --> 13:21.800
So we want to use social multimedia to do this kind of like a land use classification

13:21.800 --> 13:25.680
because social media is captured by phones or cameras.

13:25.680 --> 13:30.440
So they can see inside the building to easily infer what it is of this building.

13:30.440 --> 13:31.760
So where it is.

13:31.760 --> 13:38.840
But to the challenge, the most big challenge of using social media is it's uneven spatial

13:38.840 --> 13:39.840
distribution.

13:39.840 --> 13:44.280
So simple speaking because most images are coming from the famous landmarks, right?

13:44.280 --> 13:48.920
So not to the general spatial locations, like there were tons of people having the golden

13:48.920 --> 13:52.480
gate bridge in San Francisco, like after towering Paris.

13:52.480 --> 13:57.280
So it's very easy to tell these landmarks from the photographs, but for like residential

13:57.280 --> 14:02.240
areas or privacy, preserve the regions, which so we don't have enough images.

14:02.240 --> 14:05.480
So that's a very serious problem if we don't have images.

14:05.480 --> 14:10.520
So how can we infer the location and all those land use classification information?

14:10.520 --> 14:13.600
So that's the challenge we really face about.

14:13.600 --> 14:16.720
So traditional zero several ways to do that.

14:16.720 --> 14:20.040
So one simple way is to interpolate the features.

14:20.040 --> 14:26.360
So if we have, for example, we have several images at location A and we have several images

14:26.360 --> 14:30.760
at the land location B and these two locations are close to each other.

14:30.760 --> 14:34.320
But between these two locations, there is no images at all.

14:34.320 --> 14:38.720
So in this case, we just computed the image features of these all the images at those

14:38.720 --> 14:43.200
two locations and interpolate them along this line.

14:43.200 --> 14:49.040
But here we made assumption that the assumption is we hope the image features will change

14:49.040 --> 14:53.960
smoothly in the spatial domain, but actually in most cases, it's not a case.

14:53.960 --> 15:00.280
So if we have several images of like a residential and at the location A and a residential and

15:00.280 --> 15:05.520
location B, then if you do the interpolation things, it will interpolate.

15:05.520 --> 15:09.520
So between these two locations, all the areas are residential based.

15:09.520 --> 15:16.280
But actually in most cases, the space in between is like forest or park or just a river

15:16.280 --> 15:17.280
or something.

15:17.280 --> 15:19.320
It's like natural things.

15:19.320 --> 15:24.000
So the other kind of method is, so we try to use other information like remote sensing

15:24.000 --> 15:28.360
images or Google Street Wheels because those two information sources are like a dance

15:28.360 --> 15:31.080
so everywhere in the Earth.

15:31.080 --> 15:37.560
So there is a work from like Professor Nason Jacob's lab in ICCV 2017, so they just

15:37.560 --> 15:41.200
use Google Street Wheels to do these kind of things.

15:41.200 --> 15:47.200
But we find that so all those techniques are based on image features and are based on

15:47.200 --> 15:48.800
the assumption I just said.

15:48.800 --> 15:53.240
So the image features change smoothly in the spatial domain, but usually that's not

15:53.240 --> 15:54.240
the case.

15:54.240 --> 15:56.000
So that's why we do this work.

15:56.000 --> 16:00.440
So if we cannot use the image features, why not we just use images?

16:00.440 --> 16:02.960
But the images are missing at those locations.

16:02.960 --> 16:05.240
So how do we come up with new images?

16:05.240 --> 16:07.640
So fortunately, we have guns.

16:07.640 --> 16:10.800
So that's the motivation of the work.

16:10.800 --> 16:17.120
You mentioned that early in your research career, you spent some time looking at

16:17.120 --> 16:23.360
information theory and the like and it strikes me in that context that this is a pretty

16:23.360 --> 16:28.800
difficult problem and that there's just not enough information in these satellite images

16:28.800 --> 16:34.600
to do a very good job coming up with ground level images.

16:34.600 --> 16:36.160
How do you get around that?

16:36.160 --> 16:37.160
Yeah.

16:37.160 --> 16:43.080
So both like a overhead images and ground level images has this like advantages and drawbacks.

16:43.080 --> 16:48.880
So as I said, so the overhead images is very accurate and it stands in everywhere.

16:48.880 --> 16:52.600
So we can totally use this kind of information.

16:52.600 --> 16:56.960
But if you want to see inside the building, so overhead images cannot do that.

16:56.960 --> 17:01.840
But ground level images, we can see inside the building, but the biggest problem is the

17:01.840 --> 17:03.400
uneven distribution.

17:03.400 --> 17:09.560
So from the information theory side of like point of view, so we should use both information

17:09.560 --> 17:14.800
sources to how to combine them in the best way to get the best result.

17:14.800 --> 17:15.800
Yeah.

17:15.800 --> 17:16.800
This is a good point.

17:16.800 --> 17:23.320
Yeah, maybe to make my question more concrete, it has to do with what you considered your

17:23.320 --> 17:26.400
error function to be or something along those lines.

17:26.400 --> 17:32.600
And I guess the thing that I am trying to articulate here that strikes me as being particularly

17:32.600 --> 17:37.560
interesting and challenging here is with a ground level image, you know, beyond just

17:37.560 --> 17:44.560
trying to generate like high, you know, whether a particular image looks kind of urban or

17:44.560 --> 17:49.640
looks like, you know, greenery or forest or things like that, you know, there are things

17:49.640 --> 17:55.400
that you're trying to generate that aren't at all represented in your input.

17:55.400 --> 17:59.680
So for example, your satellite image, you know, has no building facades.

17:59.680 --> 18:05.000
But if you're trying to generate imagery around an urban area, those ground level images

18:05.000 --> 18:09.160
will have building facades, so it's just kind of making that stuff up.

18:09.160 --> 18:16.240
And I'm wondering, does that fact play into kind of how you build out the model and what

18:16.240 --> 18:18.720
the loss function is and stuff like that?

18:18.720 --> 18:19.720
Does that make any sense?

18:19.720 --> 18:20.720
Yeah.

18:20.720 --> 18:21.720
Yeah, it totally makes sense.

18:21.720 --> 18:22.720
Yeah.

18:22.720 --> 18:23.720
That's a good point.

18:23.720 --> 18:27.320
So, but actually our work is in the very like a initial stage.

18:27.320 --> 18:31.800
So currently we're just using a very traditional conditional again to do that.

18:31.800 --> 18:36.440
And that the loss function is just like generating realistic looking images.

18:36.440 --> 18:42.640
So we're not considering like whether there is a discrepancy between the ground level

18:42.640 --> 18:47.240
images or overhead images or any other like objective function.

18:47.240 --> 18:50.440
We're just using conditional again without other stuff.

18:50.440 --> 18:56.360
So the loss function only thinking about realism versus not realism.

18:56.360 --> 18:59.120
Is that kind of axiomatic for conditional gains?

18:59.120 --> 19:00.120
Yes, it is.

19:00.120 --> 19:06.160
So, yeah, for most of conditional gains, it's just a, so for the discriminator as job

19:06.160 --> 19:09.280
is just to say, so whether this image is fake or real.

19:09.280 --> 19:13.120
So it's a binary problem and the objective function is just that.

19:13.120 --> 19:14.120
So entropy loss.

19:14.120 --> 19:15.120
Okay.

19:15.120 --> 19:16.120
Got it.

19:16.120 --> 19:22.600
So you built this GAN based system to produce these images.

19:22.600 --> 19:27.800
We were just talking about loss functions like what did you find in terms of its performance?

19:27.800 --> 19:29.120
How did the system do?

19:29.120 --> 19:30.120
Yeah.

19:30.120 --> 19:36.520
So in terms of the image quality, so I don't like have a monitor to show the quality here,

19:36.520 --> 19:38.920
but basically it makes sense.

19:38.920 --> 19:45.160
So we can generate like some realistic images, like ground level images according to the

19:45.160 --> 19:46.760
overhead images.

19:46.760 --> 19:50.320
And but we don't have like a evaluation metric.

19:50.320 --> 19:53.480
So how real they are or how accurate they are.

19:53.480 --> 19:55.600
So we use another task.

19:55.600 --> 19:58.080
So that's a land use classification, right?

19:58.080 --> 20:05.680
Because if we can do a better land use classification, given these like fake images, so we can create

20:05.680 --> 20:07.800
fake images all over the ground, right?

20:07.800 --> 20:12.400
So if we can use these images to like to do better land use classification.

20:12.400 --> 20:17.480
So that of accuracy can be a good indicator of how our model works.

20:17.480 --> 20:22.120
So let me like share the performance of our land use classification problem.

20:22.120 --> 20:26.240
So if you if we are using the conditional GAN generated features to do the land use

20:26.240 --> 20:34.040
classification, we can achieve like a land cover classification accuracy with like 73% accuracy.

20:34.040 --> 20:40.760
So actually it's kind of like it's a reasonable, it's not high, but it's reasonable.

20:40.760 --> 20:47.120
The problems we're thinking here is because our generated images are not realistic enough.

20:47.120 --> 20:49.000
So that's some of our future work.

20:49.000 --> 20:51.520
So we have like a three future directions to go.

20:51.520 --> 20:56.200
So I can talk about later like if you like want.

20:56.200 --> 21:01.000
In the land use classification, how many classes are there?

21:01.000 --> 21:03.320
It's only binary classification.

21:03.320 --> 21:06.840
So like as I mentioned, it's a rural and urban areas.

21:06.840 --> 21:10.960
So because that's that's only the ground truth we have.

21:10.960 --> 21:11.960
Okay.

21:11.960 --> 21:14.600
So you've got a bunch of satellite data.

21:14.600 --> 21:18.640
You feed it into your conditional GAN.

21:18.640 --> 21:28.600
It generates an image that is meant to be representative of a ground level view of the

21:28.600 --> 21:32.520
area that you indicate.

21:32.520 --> 21:39.440
And then you are sending that into a classifier that is meant to determine whether it's

21:39.440 --> 21:41.520
urban or rural.

21:41.520 --> 21:53.880
And the is the 73% is that the accuracy of your classifier or the accuracy of the generator

21:53.880 --> 21:55.720
based on a trained classifier?

21:55.720 --> 21:58.120
Oh, it's a it's a the classifier.

21:58.120 --> 21:59.120
It's a classifier.

21:59.120 --> 22:00.120
Okay.

22:00.120 --> 22:01.120
The classifier itself.

22:01.120 --> 22:02.120
That's what I thought you were saying.

22:02.120 --> 22:10.800
And so, you know, how did the GAN itself perform relative to with that classifier?

22:10.800 --> 22:11.800
Okay.

22:11.800 --> 22:16.840
So the baseline of that is, so we compare to like a traditional approaches, right?

22:16.840 --> 22:20.720
So the first approaches I'm talking about is interpolated features.

22:20.720 --> 22:26.960
So if we have some images, we just do the feature extraction first and interpolate the features

22:26.960 --> 22:29.080
on those regions without images.

22:29.080 --> 22:32.840
So the baseline is like 65%, okay, so 65.

22:32.840 --> 22:38.360
So if we're using GANs without conditional, like we're getting like a two or three percent

22:38.360 --> 22:44.480
improvement, like to the almost the 68% and if we're using conditional again, we're

22:44.480 --> 22:50.080
having like 70% and if we're using conditional again, generated features, we're having the

22:50.080 --> 22:53.120
best accuracy like 73%.

22:53.120 --> 22:56.560
So that's the progress of our work.

22:56.560 --> 22:59.120
And I just want to make sure I understand this.

22:59.120 --> 23:05.000
I thought what you were saying was that the classifier itself, like totally separate

23:05.000 --> 23:10.680
from the GAN part of the system, had a 73% accuracy.

23:10.680 --> 23:16.800
Did you model the land use classifier separately and that was 73%.

23:16.800 --> 23:17.800
Yes.

23:17.800 --> 23:22.240
So the land use classifier is totally separate from the GAN, right?

23:22.240 --> 23:26.000
So the GAN, the GAN is about generated discriminator.

23:26.000 --> 23:30.480
The job is to like create, create these images, yes.

23:30.480 --> 23:37.560
And then we use the features from the discriminator as the input to the land use classifier,

23:37.560 --> 23:38.560
okay?

23:38.560 --> 23:39.560
Right.

23:39.560 --> 23:44.160
I guess what strikes me as odd is that, or at least curious, is that if I understood

23:44.160 --> 23:51.840
you correctly, your ultimate accuracy of your GAN turned out to be exactly the same as

23:51.840 --> 23:54.640
the accuracy of your classifier, 73%.

23:54.640 --> 23:57.640
No, maybe I'm saying wrong.

23:57.640 --> 24:00.680
So because for GAN, there's no accuracy, right?

24:00.680 --> 24:04.240
So GANs for the discriminator is just a real or fake.

24:04.240 --> 24:07.560
So we don't care about that accuracy or not.

24:07.560 --> 24:12.400
So it's usually very high, so like 80% or 90% is very high.

24:12.400 --> 24:16.240
But what we are concerned about is the land use classifier, yeah.

24:16.240 --> 24:20.840
So what I'm saying, the accuracy is all for land use classifier, it's not for GAN

24:20.840 --> 24:21.840
classifier.

24:21.840 --> 24:22.840
Okay.

24:22.840 --> 24:27.200
I think I'm confusing the issue here and not doing a good job explaining.

24:27.200 --> 24:33.960
So I guess I'm thinking that there are, as we established, there are two separate systems.

24:33.960 --> 24:36.560
There's the generating system.

24:36.560 --> 24:41.720
It's input is an overhead image and its output is a ground level image.

24:41.720 --> 24:47.880
And there's a classifying system and its input is a ground level image and its output

24:47.880 --> 24:52.400
is a binary land use classification.

24:52.400 --> 24:59.240
And so the performance of the GAN is a subset of that first system.

24:59.240 --> 25:03.360
And it's responsible for generating these images and it's kind of judged on whether

25:03.360 --> 25:06.200
the images are realistic or not.

25:06.200 --> 25:11.280
But that whole first system, the generator system as a whole, right?

25:11.280 --> 25:16.080
You're giving it a satellite image and it's spitting out a ground level image.

25:16.080 --> 25:22.640
You can measure that its accuracy with respect to producing kind of the correct land use,

25:22.640 --> 25:23.640
right?

25:23.640 --> 25:24.640
Yes.

25:24.640 --> 25:29.800
So that's one kind of accuracy measure and then there's another which is given any kind

25:29.800 --> 25:34.400
of image, whether it's generated from our generator or not, you know, is this land use

25:34.400 --> 25:40.280
classifier model accurate in classifying the input image correctly.

25:40.280 --> 25:45.680
And then there's like a third performance metric, which is the end to end, right, given

25:45.680 --> 25:50.240
the satellite image, can we identify the land use correctly?

25:50.240 --> 25:53.880
And so which of these is the 73 percent?

25:53.880 --> 25:54.880
It's the second one.

25:54.880 --> 25:58.320
So for the third one, we're not doing end to end right now.

25:58.320 --> 26:00.440
So we're just, we're doing two stage.

26:00.440 --> 26:04.560
So first stage is GAN, the second stage is then to use, we're not doing end to end at

26:04.560 --> 26:05.560
this point.

26:05.560 --> 26:09.480
Yeah, I guess if you're not doing end to end, you're not really looking at the first,

26:09.480 --> 26:13.160
the performance of the generator either because that really is the end to end.

26:13.160 --> 26:14.160
Uh-huh.

26:14.160 --> 26:15.160
Okay.

26:15.160 --> 26:23.520
So the main focus of the research around modeling and testing this, the classifier then

26:23.520 --> 26:29.520
or is it the generator system, uh, the classifier, right, because we're, um, so for geospatial

26:29.520 --> 26:32.800
analysis, we're more caring about the land use system.

26:32.800 --> 26:34.880
So basically it's a zoning system, right?

26:34.880 --> 26:39.920
So usually the government and city will make a zoning map so every year or so.

26:39.920 --> 26:42.240
So that is the most helpful things.

26:42.240 --> 26:45.120
So for the generator, it's just a technique we use.

26:45.120 --> 26:49.000
So if we don't use GAN, we can use other generator to generate images.

26:49.000 --> 26:54.600
Yeah, it's, it's kind of funny in that like, you know, GANs are so popular and quote unquote

26:54.600 --> 26:55.600
sexy term.

26:55.600 --> 26:59.640
It's almost like a head fake that kind of pulls your attention away from what you're actually

26:59.640 --> 27:01.000
trying to do in this paper.

27:01.000 --> 27:02.000
Yeah, yeah, yeah.

27:02.000 --> 27:03.000
Actually it is.

27:03.000 --> 27:05.720
So, um, okay.

27:05.720 --> 27:11.000
So I'm not sure you've even talked much about the classification model.

27:11.000 --> 27:15.640
So for land use, it's a pretty basic, like a, like a convolutional network.

27:15.640 --> 27:18.880
So like we use ResNet 101 to do that.

27:18.880 --> 27:24.080
Um, so actually, um, we have like a previous work like a three years ago.

27:24.080 --> 27:29.720
Um, so it, it's also doing land use classification with like a convolutional neural network.

27:29.720 --> 27:33.760
I think at that time where the first batch of work that used deep learning for this kind

27:33.760 --> 27:38.480
of work, we also received the best postal word in the SEM six spatial conference.

27:38.480 --> 27:39.480
Yeah.

27:39.480 --> 27:43.640
Hopefully it's you, um, at that time, it's also the similar problems we're using social

27:43.640 --> 27:44.640
media.

27:44.640 --> 27:45.640
We use deep learning.

27:45.640 --> 27:50.400
We do the land use classification, but the biggest problem is we don't have the ground

27:50.400 --> 27:51.400
juice, right?

27:51.400 --> 27:54.440
If we don't have ground juice, we cannot evaluate our model.

27:54.440 --> 27:58.800
So at that time, which is to check and like a university campus to do the land use classification

27:58.800 --> 27:59.800
task.

27:59.800 --> 28:00.800
It's a tribute example.

28:00.800 --> 28:03.720
It's a toy example, but we get like a very good accuracy.

28:03.720 --> 28:08.600
So since that time, we start to build a very large data set and actually we took like

28:08.600 --> 28:13.400
two years to build a data set and continue this line of land use classification system.

28:13.400 --> 28:14.400
Yeah.

28:14.400 --> 28:19.000
But then in this work in this game paper, we're just using a very standard resident one

28:19.000 --> 28:22.040
on one network to do the land use classification.

28:22.040 --> 28:25.480
Does the social media images come into play in this paper?

28:25.480 --> 28:26.480
Oh, yes.

28:26.480 --> 28:30.480
So for the ground level images from the geographic data set.

28:30.480 --> 28:33.280
So those are all like the ground level images.

28:33.280 --> 28:34.480
So social media, right?

28:34.480 --> 28:38.200
Those social media are contributed by just resident in United Kingdom.

28:38.200 --> 28:42.280
So anybody can submit an image to the website and the website will show it.

28:42.280 --> 28:46.120
So where the photo is taken and what it's about.

28:46.120 --> 28:50.720
So yeah, for that data set, it is totally like a social multimedia.

28:50.720 --> 28:54.560
And remind me how where that comes into play in the system?

28:54.560 --> 28:58.360
The system is when you do the discriminator, right?

28:58.360 --> 29:01.960
You need to tell this image is fake or real, right?

29:01.960 --> 29:05.080
But how do you determine whether it's real or fake?

29:05.080 --> 29:08.760
You have to look at some ground level images to know it is real or fake, right?

29:08.760 --> 29:09.760
Oh, right.

29:09.760 --> 29:11.640
So yeah, that's what they're coming to play.

29:11.640 --> 29:16.400
So although the input is overhead imagery, but the ground shows like something you compare

29:16.400 --> 29:18.640
to is the ground level images.

29:18.640 --> 29:22.040
So that's when the social multimedia coming to play.

29:22.040 --> 29:27.640
And those images are they're not labeled at all with regard to land use or anything

29:27.640 --> 29:28.640
like that.

29:28.640 --> 29:33.520
It's just that's the training the discriminator to understand real versus fake images.

29:33.520 --> 29:34.520
Yes.

29:34.520 --> 29:35.520
That's the beauty of gain, right?

29:35.520 --> 29:38.240
So we don't care about the labeling.

29:38.240 --> 29:40.560
We don't need to know the land use process.

29:40.560 --> 29:43.800
We just need to know this is a real image that is a fake one.

29:43.800 --> 29:44.800
So I.

29:44.800 --> 29:45.800
Okay.

29:45.800 --> 29:46.800
Cool.

29:46.800 --> 29:47.800
I think I've got it now.

29:47.800 --> 29:57.000
So the to sum it all up, right, you've got this challenge of being able to develop accurate

29:57.000 --> 30:02.800
land use classifiers, but you've got this problem of sparse data, right?

30:02.800 --> 30:07.680
So you've got all of this area that you might want to classify based on satellite images,

30:07.680 --> 30:12.320
but that you don't have specific ground level data for.

30:12.320 --> 30:18.640
So you generate some realistic looking ground level data using the conditional gain and

30:18.640 --> 30:22.280
then use that to train your classifier.

30:22.280 --> 30:28.280
And you've been able to kind of incrementally improve the performance of the classifier

30:28.280 --> 30:35.320
using this kind of data as opposed to previous data sources that are trying to approximate

30:35.320 --> 30:36.800
this data that you don't have.

30:36.800 --> 30:37.800
Yes.

30:37.800 --> 30:38.800
Yes.

30:38.800 --> 30:39.800
Correct.

30:39.800 --> 30:40.800
Yeah.

30:40.800 --> 30:41.800
Got it.

30:41.800 --> 30:42.800
Exactly.

30:42.800 --> 30:43.800
Got it.

30:43.800 --> 30:44.800
Okay.

30:44.800 --> 30:45.800
Awesome.

30:45.800 --> 30:46.800
Yeah.

30:46.800 --> 30:45.560
I don't know why this one was so difficult for me to wrap my head around, but I think a part of it

30:45.560 --> 30:48.760
had to do with this, this, this GAN head fake.

30:48.760 --> 30:49.760
Yes.

30:49.760 --> 30:51.400
So most of the people just focus on the GANs.

30:51.400 --> 30:54.520
Oh, what's GAN can structure you're using?

30:54.520 --> 30:57.320
Are you using the state of art again?

30:57.320 --> 30:58.320
So yeah.

30:58.320 --> 31:03.480
So actually, we're doing something like for geospatial analysis, so for the land use classification.

31:03.480 --> 31:05.120
So that's how we're finally aimed.

31:05.120 --> 31:06.120
Awesome.

31:06.120 --> 31:09.520
Are there other interesting aspects of this paper that we haven't touched on yet?

31:09.520 --> 31:10.520
Yeah.

31:10.520 --> 31:14.560
So for this paper, there's, no, not, but for the future work, right?

31:14.560 --> 31:16.720
I want to talk about the future work a little bit.

31:16.720 --> 31:18.040
So that's very interesting.

31:18.040 --> 31:22.520
So the first thing about the future work is, so right now, our generated ground level

31:22.520 --> 31:25.200
images, like, it's not real enough.

31:25.200 --> 31:27.560
So they lack the image details.

31:27.560 --> 31:32.600
So for some like houses or like animals, they don't, they don't look real enough.

31:32.600 --> 31:35.640
So there's like a plenty of room for improvement.

31:35.640 --> 31:41.080
So currently we're thinking so we want to use a technique called a progressive GAN.

31:41.080 --> 31:47.080
So from immediate, so that method is the key idea is to grow both the generator and

31:47.080 --> 31:48.880
discriminator progressively.

31:48.880 --> 31:53.840
So from a small network, from a small resolution, we add new layers to the model and

31:53.840 --> 31:57.600
increasing the model in a progressive manner.

31:57.600 --> 32:02.720
So this can both speed up the training procedure and stabilize the model.

32:02.720 --> 32:06.800
Because GAN is really hard to train, so sometimes it's just a model collapse.

32:06.800 --> 32:14.080
So this could eventually lead us to a very good image resolution, because currently our

32:14.080 --> 32:20.040
generated images only like 32 by 32 or 64 by 64, so it's very coarse.

32:20.040 --> 32:24.840
But eventually, we want to have some images like 1K by 1K or 2K or even 2K.

32:24.840 --> 32:29.600
So because most of the remote sensing images is like 2K by like 3K.

32:29.600 --> 32:31.960
So it's very large and it's very details.

32:31.960 --> 32:36.480
So we want our generated ground level images is also large and details.

32:36.480 --> 32:40.640
So I think that will bring up our performance by a large margin.

32:40.640 --> 32:41.640
Okay.

32:41.640 --> 32:43.440
So that's the first direction.

32:43.440 --> 32:48.840
And the second direction is, so there is also a recent work by OpenAI called GLOW.

32:48.840 --> 32:51.440
So it's there using reversible generated models.

32:51.440 --> 32:55.400
They're not using GANs, but reversible generated models.

32:55.400 --> 33:01.960
So that model, the most, the good thing about it is their latent space, I mean the features.

33:01.960 --> 33:05.440
So the features are useful for downstream tasks.

33:05.440 --> 33:11.280
Because in GANs, the data points can usually not be directly represented in a latent space.

33:11.280 --> 33:15.920
So because they have no encoder and they don't have all the data distribution.

33:15.920 --> 33:21.640
And for reversible generated models, they can, they can interpolate between these features

33:21.640 --> 33:22.640
space.

33:22.640 --> 33:23.640
So it's very smooth.

33:23.640 --> 33:26.760
So in that case, we can directly use the features.

33:26.760 --> 33:28.320
We don't need to use images.

33:28.320 --> 33:31.240
So I hope that can be a better solution.

33:31.240 --> 33:35.480
And then I think like a lot of people are interested in this paper as well.

33:35.480 --> 33:36.880
So the GLOW one.

33:36.880 --> 33:39.800
So it can generate very realistic images.

33:39.800 --> 33:44.200
And the third direction will be using more information as you talk about.

33:44.200 --> 33:49.080
So maybe using like a information retrieval thing, we're using some like a text.

33:49.080 --> 33:50.320
Text information, right?

33:50.320 --> 33:55.480
Because for example, if we want to generate like a forest like image.

33:55.480 --> 34:01.840
So if we just give him the overhead image, so maybe he cannot like give us a good image.

34:01.840 --> 34:08.200
But if we can say so I want the forest to wheel with like with one house inside it.

34:08.200 --> 34:14.120
So maybe in our generated GLOW images, we will see exactly one house in a forest.

34:14.120 --> 34:15.280
Like scenery.

34:15.280 --> 34:16.800
So that's very promising.

34:16.800 --> 34:19.160
So that's the third direction to go.

34:19.160 --> 34:27.640
So so this works, this can work is our initial attempt to do this dance dance interpolations

34:27.640 --> 34:28.640
in this direction.

34:28.640 --> 34:31.160
So we have a lot of work coming up.

34:31.160 --> 34:34.600
So hopefully we can get better results in the future.

34:34.600 --> 34:38.760
You mentioned with regard to the GLOW paper, I've seen it.

34:38.760 --> 34:43.440
But I haven't looked at it in any detail at all.

34:43.440 --> 34:49.520
You mentioned that part of what it allows you to do is to get these smooth representations

34:49.520 --> 34:52.320
in a feature space.

34:52.320 --> 34:57.760
And are you would you then be trying to use that feature space or is the only benefit

34:57.760 --> 35:01.480
to you of that that it produces better images?

35:01.480 --> 35:02.840
I think we will try both.

35:02.840 --> 35:04.840
So better images and the features.

35:04.840 --> 35:10.160
I think maybe the image maybe the features makes more sense because for GLOW work, they're

35:10.160 --> 35:11.160
reversible, right?

35:11.160 --> 35:16.920
But reversible means if you have some input and go into the output, it can also use output

35:16.920 --> 35:18.320
to get you the input.

35:18.320 --> 35:21.800
So the reversible makes the features more robust.

35:21.800 --> 35:22.840
So it makes sense.

35:22.840 --> 35:25.840
It is like explainable.

35:25.840 --> 35:30.880
So in that case, the features might be more powerful than the again features.

35:30.880 --> 35:33.280
Are the features in this feature space?

35:33.280 --> 35:34.880
Are they semantically related?

35:34.880 --> 35:40.480
Like is it kind of an embedding in the feature space where you can get these semantic relationships

35:40.480 --> 35:43.520
between these different types of generated images?

35:43.520 --> 35:44.520
Yeah, definitely.

35:44.520 --> 35:49.040
I think the features should be like semantic related.

35:49.040 --> 35:56.280
So if we do like a feature space visualization, we can see like a tree forest cluster together

35:56.280 --> 36:00.280
under the river water lake, those images are clustered together.

36:00.280 --> 36:03.040
So the features are definitely semantic related.

36:03.040 --> 36:07.560
And so do you think you'll get to a point where you can start with a ground truth image

36:07.560 --> 36:12.560
of a field and say like I want a little bit more, you know, rivers and then get a stream

36:12.560 --> 36:16.760
and a little bit more rivers and get like a bigger body of water or something like that?

36:16.760 --> 36:18.520
Well, you're really like smart.

36:18.520 --> 36:21.800
So that's something we're trying to do right now.

36:21.800 --> 36:25.960
So that's more like a manipulative of the image, right?

36:25.960 --> 36:32.000
So yeah, if you can do that, so that's very interesting to the geospatial communities.

36:32.000 --> 36:37.160
Just to kind of wrap things up, I'm curious you mentioned how difficult training the

36:37.160 --> 36:39.480
GANs has been.

36:39.480 --> 36:47.240
Can you maybe share with us some things that you've learned as you try to work with GANs

36:47.240 --> 36:48.480
and conditional GANs?

36:48.480 --> 36:49.480
Okay, sure.

36:49.480 --> 36:54.560
So for GANs, so because we're using a very standard conditional GANs, which is proposed

36:54.560 --> 36:58.840
like 2015 or 2016, so it's a very earlier stage of the GAN.

36:58.840 --> 37:03.400
So the training is like on stable, so I have a lot of problems.

37:03.400 --> 37:09.800
So as mentioning another paper, I forgot the exact name, but it should be like a good

37:09.800 --> 37:13.120
practice is during like training, implementing GANs.

37:13.120 --> 37:17.640
So they propose like we should use straight-ed convolution, not pooling because pooling can

37:17.640 --> 37:21.960
hurt the like image resolution thing during the down sampling.

37:21.960 --> 37:26.120
And we should like using smaller crop size and the smaller learning rates, something

37:26.120 --> 37:27.120
like that.

37:27.120 --> 37:33.280
But I don't think that's a major problem right now because most of GAN models is

37:33.280 --> 37:36.520
like easier to train at this moment.

37:36.520 --> 37:43.200
So because the loss matrix changes to the loss system loss, so that is a very stabilized

37:43.200 --> 37:48.080
loss function and also the network architecture change.

37:48.080 --> 37:51.520
So right now we can train very deep networks using GANs.

37:51.520 --> 37:56.640
So for example, like a ResNet 101 and the output can be several hundred resolutions

37:56.640 --> 37:58.280
or even one K resolutions.

37:58.280 --> 38:01.280
So the training of the GAN is not that hard right now.

38:01.280 --> 38:05.840
Awesome, well E, thank you so much for taking the time to share with us what you're working

38:05.840 --> 38:06.840
on.

38:06.840 --> 38:07.840
It's really interesting stuff.

38:07.840 --> 38:09.120
Yeah, no problem.

38:09.120 --> 38:14.160
Alright, everyone, that's our show for today.

38:14.160 --> 38:20.280
For more information on E or any of the topics covered in this episode, head over to twomolei.com

38:20.280 --> 38:23.320
slash talk slash 172.

38:23.320 --> 38:34.600
As always, thanks so much for listening and catch you next time.

