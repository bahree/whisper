1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:36,680
This week's shows are drawn from some of the great conversations I had at the recent Nvidia

5
00:00:36,680 --> 00:00:40,960
GPU technology conference and they're brought to you by Dell.

6
00:00:40,960 --> 00:00:44,840
If you caught my tweets from GTC, you may already know that one of the announcements this

7
00:00:44,840 --> 00:00:49,320
year was a new reference architecture for data science work sessions powered by high

8
00:00:49,320 --> 00:00:54,480
NGPUs and accelerated software such as Nvidia's Rabbids.

9
00:00:54,480 --> 00:00:58,960
Dell was among the key partners showcased during the launch and offers a line of workstations

10
00:00:58,960 --> 00:01:03,040
designed for modern machine learning and AI workloads.

11
00:01:03,040 --> 00:01:07,720
To learn more about Dell precision workstations and some of the ways they're being used by customers

12
00:01:07,720 --> 00:01:12,320
in industries like media and entertainment, engineering and manufacturing, healthcare

13
00:01:12,320 --> 00:01:23,040
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.

14
00:01:23,040 --> 00:01:25,840
Alright everyone, I am on the line with Trista Chen.

15
00:01:25,840 --> 00:01:30,000
Trista is the chief scientist of machine learning at Inventek.

16
00:01:30,000 --> 00:01:33,160
Trista, welcome to this week a machine learning and AI.

17
00:01:33,160 --> 00:01:35,640
Thank you Sam, thank you for inviting me.

18
00:01:35,640 --> 00:01:36,640
Absolutely, absolutely.

19
00:01:36,640 --> 00:01:39,720
I'm looking forward to our conversation.

20
00:01:39,720 --> 00:01:45,480
Before we jump into the heart of our talk, which would be focused on a presentation you

21
00:01:45,480 --> 00:01:52,960
did recently at the Nvidia GTC conference on edge AI and smart manufacturing, I'd love

22
00:01:52,960 --> 00:01:54,840
to learn a bit about your background.

23
00:01:54,840 --> 00:01:57,240
How did you get started working in machine learning?

24
00:01:57,240 --> 00:02:01,800
I'm always a visual person to start with as a child.

25
00:02:01,800 --> 00:02:09,200
So naturally I study computer vision and after I finished my PhD at Carnegie Mande University,

26
00:02:09,200 --> 00:02:15,360
I joined Nvidia to be the first video architect of their video processor.

27
00:02:15,360 --> 00:02:22,240
So video processor is their new line of processor in addition to the graphic processor, so that

28
00:02:22,240 --> 00:02:30,040
they can handle all the video encoding, decoding and preprocessing post-processing more smoothly.

29
00:02:30,040 --> 00:02:38,080
After Nvidia, I joined Intel to be their research scientist, I lead a team of researchers

30
00:02:38,080 --> 00:02:47,480
in Russia to do open CV development, as well as working on medical and multi-core processor

31
00:02:47,480 --> 00:02:48,480
research.

32
00:02:48,480 --> 00:02:57,280
So open CV to some of you who know the open source library is probably the biggest open source

33
00:02:57,280 --> 00:02:59,080
library on computer vision.

34
00:02:59,080 --> 00:03:07,160
You have 14 million dollars so far and I'm very happy to be proud of effort to contribute

35
00:03:07,160 --> 00:03:13,600
to the community to help everybody to have a good head start in all the computer vision

36
00:03:13,600 --> 00:03:15,720
applications.

37
00:03:15,720 --> 00:03:22,720
And after Intel, I think most people at Silicon Valley had this dream of studying their

38
00:03:22,720 --> 00:03:25,720
own company.

39
00:03:25,720 --> 00:03:33,240
So I started my own company doing computer vision for other companies.

40
00:03:33,240 --> 00:03:35,600
So basically a B2B effort.

41
00:03:35,600 --> 00:03:43,040
So I help company to annotate their image or video assets so that they can match related

42
00:03:43,040 --> 00:03:46,640
advertisement to their image or video.

43
00:03:46,640 --> 00:03:50,680
So example to that would be if you watch YouTube video array.

44
00:03:50,680 --> 00:03:56,600
So a lot of them are consumer generated and you want to monetize them.

45
00:03:56,600 --> 00:04:02,040
So our company will help them to look at what's inside a video and then what would be

46
00:04:02,040 --> 00:04:06,200
the appropriate related content.

47
00:04:06,200 --> 00:04:12,200
For example, if you are posted a video of writing a bicycle in the wild, then maybe we will

48
00:04:12,200 --> 00:04:17,760
match our AI advertisement for you and same thing with image.

49
00:04:17,760 --> 00:04:24,120
So if you write a blog about your dinner with friends and then you write a blog about

50
00:04:24,120 --> 00:04:30,320
going to somewhere for fun, then we can match that with travel advertisement and whatnot.

51
00:04:30,320 --> 00:04:35,000
And so we got accepted by a wide combinator in the last round.

52
00:04:35,000 --> 00:04:39,320
But eventually I didn't join the wide combinator effort.

53
00:04:39,320 --> 00:04:46,320
And then so because I was a first time entrepreneur, it wasn't successful.

54
00:04:46,320 --> 00:04:52,160
And after that, I get a taste of stuff and I decided to join another stuff which was

55
00:04:52,160 --> 00:04:55,320
called cognitive networks.

56
00:04:55,320 --> 00:04:58,760
So what they do is they do content recognition on TV.

57
00:04:58,760 --> 00:05:06,080
So they have this small device inside the TV that they partner with so that they can

58
00:05:06,080 --> 00:05:10,840
understand exactly what TV content you are watching.

59
00:05:10,840 --> 00:05:19,040
For example, you are watching Big Ben Theory episode A and two minutes of two and 30 seconds

60
00:05:19,040 --> 00:05:20,320
and whatnot.

61
00:05:20,320 --> 00:05:29,320
So by knowing what you are watching at the moment, they can do a lot of analytics, the video,

62
00:05:29,320 --> 00:05:35,760
they can know the advertisement or the campaign effectiveness and whatnot.

63
00:05:35,760 --> 00:05:42,200
So eventually the company was successful and got acquired by Vizio, one of the top TV

64
00:05:42,200 --> 00:05:51,200
manufacturer in the world, and so after a happy acquisition, I kind of semi-retire, per

65
00:05:51,200 --> 00:05:52,200
say.

66
00:05:52,200 --> 00:05:59,680
Yeah, after like 15 years of second value, like how working and craziness.

67
00:05:59,680 --> 00:06:05,600
So I actually moved across Pacific Ocean to Taiwan.

68
00:06:05,600 --> 00:06:10,880
So actually my career has been interesting in that it's been different countries.

69
00:06:10,880 --> 00:06:17,920
It's been different areas as a firm research to industry research from big companies and

70
00:06:17,920 --> 00:06:23,360
eventually start up and now I'm moving back to another big company, which is Inventake

71
00:06:23,360 --> 00:06:24,880
in Taiwan.

72
00:06:24,880 --> 00:06:31,960
So for a lot of you in the US, you might not be aware of the company Inventake.

73
00:06:31,960 --> 00:06:37,400
It's actually OEM or some of you might heard of FACSCOM, right?

74
00:06:37,400 --> 00:06:43,160
So FACSCOM make manufacturer all the Apple devices for Apple.

75
00:06:43,160 --> 00:06:47,560
So Inventake is a similar line of company.

76
00:06:47,560 --> 00:06:56,240
So they manufacture cloud servers, laptops and smart devices for all the famous computer

77
00:06:56,240 --> 00:07:03,440
they ever heard of, even though you never heard of Inventake itself because it's OEM.

78
00:07:03,440 --> 00:07:13,800
As a chief scientist in the company, I was actually I'm helping the company to optimize

79
00:07:13,800 --> 00:07:23,080
their industrial 4.0 effort in the factory as well as looking at future direction in products

80
00:07:23,080 --> 00:07:33,600
so that we can prepare ourselves to be ready to manufacture a new line of AI products for

81
00:07:33,600 --> 00:07:34,920
our customers.

82
00:07:34,920 --> 00:07:43,200
So the presentation that you did at GTC around the application of AI at the edge, was that

83
00:07:43,200 --> 00:07:49,160
for Inventake's own manufacturing facilities or for customers or partners?

84
00:07:49,160 --> 00:07:52,400
Yeah, the answer is yes and no.

85
00:07:52,400 --> 00:07:58,200
So to start the research, we actually focus mostly internally.

86
00:07:58,200 --> 00:08:03,400
So we go to the factory and talk to the people in the production line to understand what

87
00:08:03,400 --> 00:08:06,720
their needs and what their pain point is.

88
00:08:06,720 --> 00:08:09,400
And from there we develop the solutions.

89
00:08:09,400 --> 00:08:17,320
So what you see in our GTC presentation are the real application that we develop for

90
00:08:17,320 --> 00:08:19,360
the factories.

91
00:08:19,360 --> 00:08:22,160
And from there it becomes a solution.

92
00:08:22,160 --> 00:08:28,480
So it's possible that we provide the solution or we customize a solution for other customers

93
00:08:28,480 --> 00:08:30,480
other than Inventake.

94
00:08:30,480 --> 00:08:34,240
But that's another process.

95
00:08:34,240 --> 00:08:36,920
So now we are not talking about that in the presentation.

96
00:08:36,920 --> 00:08:40,520
So the presentation that you saw is for Inventake only.

97
00:08:40,520 --> 00:08:41,840
Okay, great.

98
00:08:41,840 --> 00:08:46,800
And so we're not going to assume that anyone listening has seen the presentation.

99
00:08:46,800 --> 00:08:53,200
So why don't you walk us through the main points of the case study?

100
00:08:53,200 --> 00:08:57,280
What were the main challenges that you are trying to solve?

101
00:08:57,280 --> 00:08:58,440
Let's start there.

102
00:08:58,440 --> 00:08:59,440
Okay.

103
00:08:59,440 --> 00:09:04,760
So manufacturing is actually a very interesting area.

104
00:09:04,760 --> 00:09:10,200
So most people, they had the whole their smartphones, they had their computers, but they

105
00:09:10,200 --> 00:09:13,200
had no knowledge about those factories.

106
00:09:13,200 --> 00:09:19,280
So they imagined something like, oh, there's this big huge factory in China somewhere.

107
00:09:19,280 --> 00:09:26,440
And some like huge robots or many robots working there along with maybe a lot of cheap labor

108
00:09:26,440 --> 00:09:27,440
and all that.

109
00:09:27,440 --> 00:09:31,160
Honestly, it's more advanced than that.

110
00:09:31,160 --> 00:09:40,280
So industry 4.0 is actually an effort to digitize a lot of the physical part of the factory

111
00:09:40,280 --> 00:09:48,200
so that we can seamlessly integrate what we sense from the physical, for example, the

112
00:09:48,200 --> 00:09:55,760
defects or what we see from the production line together with a lot of the knowledge that

113
00:09:55,760 --> 00:10:00,160
we learned from previously for that to optimize the process.

114
00:10:00,160 --> 00:10:01,160
Okay.

115
00:10:01,160 --> 00:10:03,160
So let me give you an example.

116
00:10:03,160 --> 00:10:11,200
So traditionally in like industry of 3.0, so we need a lot of humans to look at, for

117
00:10:11,200 --> 00:10:16,120
example, the PCB board or the motherboard of a computer, one by one, right?

118
00:10:16,120 --> 00:10:21,800
To make sure that the components in the PCB board are not misplaced, they are not having

119
00:10:21,800 --> 00:10:26,080
any defects, they are not missing and whatnot.

120
00:10:26,080 --> 00:10:30,040
But then you can imagine that it's pretty cost ineffective, right?

121
00:10:30,040 --> 00:10:36,200
And then a human can get tired and they can get inconsistent in their result.

122
00:10:36,200 --> 00:10:42,840
So by using a computer region to do this, we can ensure that accuracy of its action is

123
00:10:42,840 --> 00:10:44,160
much higher.

124
00:10:44,160 --> 00:10:46,720
And more importantly, you'll be more consistent, right?

125
00:10:46,720 --> 00:10:51,480
So a lot of people or a lot of labor before a lunchtime, they get so tired, they want

126
00:10:51,480 --> 00:10:58,600
to go to lunch and they will simply just press OK or back to the product, right?

127
00:10:58,600 --> 00:11:02,400
And then they just go to and they finish their job and they can go, right?

128
00:11:02,400 --> 00:11:07,920
But then for computer, they won't have such a problem, they'll just keep working tirelessly.

129
00:11:07,920 --> 00:11:15,160
So this is the very obvious example of application of AI in manufacturing.

130
00:11:15,160 --> 00:11:20,200
But honestly, industry application of AI is actually a very broad area.

131
00:11:20,200 --> 00:11:28,280
So according to McKinsey, a report of McKinsey in 2013, they actually more manufacturing

132
00:11:28,280 --> 00:11:33,760
data than any other sector combined in a year.

133
00:11:33,760 --> 00:11:40,080
So that would be two extra bytes or two and then 10 to the power of 18 bytes of data per

134
00:11:40,080 --> 00:11:43,040
year, according to the report.

135
00:11:43,040 --> 00:11:49,920
And that amount of data can be used to do a lot of interesting things.

136
00:11:49,920 --> 00:11:56,960
In addition to defect detection, so it could be used to do like pretty few maintenance.

137
00:11:56,960 --> 00:12:04,280
It can be used to do operational optimization, supply chain optimization and all that.

138
00:12:04,280 --> 00:12:12,360
So what we talk about in GTC is just a very small portion of how AI can help manufacturing

139
00:12:12,360 --> 00:12:13,360
or factory.

140
00:12:13,360 --> 00:12:20,000
Now, one of the things that I've heard in talking to folks about this space and I'm curious

141
00:12:20,000 --> 00:12:26,680
if you've seen similar is that one of the big challenges here is that while there

142
00:12:26,680 --> 00:12:35,400
is a ton of data, often it's locked in proprietary systems like robotics provider might provide

143
00:12:35,400 --> 00:12:41,760
the robot, but not open up the data to the customer to integrate it in with other things

144
00:12:41,760 --> 00:12:49,360
or they may have data in some SCADA system, but they aren't easy interfaces to accessing

145
00:12:49,360 --> 00:12:51,000
or using that data.

146
00:12:51,000 --> 00:12:54,440
Do you find that in your world as well?

147
00:12:54,440 --> 00:12:57,560
Yeah, that's an excellent question, actually.

148
00:12:57,560 --> 00:12:59,920
So there are two aspects to that.

149
00:12:59,920 --> 00:13:07,000
So one is the device, with the device actually output the data you need in order to do your

150
00:13:07,000 --> 00:13:08,000
AI thing.

151
00:13:08,000 --> 00:13:11,920
So first, for example, in the case of defect detection, right?

152
00:13:11,920 --> 00:13:17,640
So AOI machine stands for automatic optical inspection.

153
00:13:17,640 --> 00:13:23,240
So what they do is they use a camera, press some algorithm to detect defect, but a lot

154
00:13:23,240 --> 00:13:30,360
of times those AOI vendor, they won't output the raw image to the factory people.

155
00:13:30,360 --> 00:13:36,280
So what they're going to output is just a result of a pass or fail of this piece before.

156
00:13:36,280 --> 00:13:43,480
And there's a good reason that they don't want to release their precious data to the customer.

157
00:13:43,480 --> 00:13:48,160
So oftentimes you have to pay extra fee or maintenance fee or service fee in order

158
00:13:48,160 --> 00:13:56,240
to get those data to do further optimization as a factory customer to those device manufacturers.

159
00:13:56,240 --> 00:13:58,160
So that's one aspect to that.

160
00:13:58,160 --> 00:14:05,400
And the other aspect is even the factory has access to all of this confidential or secret

161
00:14:05,400 --> 00:14:06,400
data.

162
00:14:06,400 --> 00:14:14,200
It's usually hard or impossible for the outsider to touch those sensitive manufacturing

163
00:14:14,200 --> 00:14:16,960
data to do further things.

164
00:14:16,960 --> 00:14:18,960
So I will use the example, right?

165
00:14:18,960 --> 00:14:21,040
For example, like landing AI.

166
00:14:21,040 --> 00:14:27,160
So a company that is certified into one of the AI pioneers, right?

167
00:14:27,160 --> 00:14:37,080
So landing AI, they claim to work on this field of industry or AI, but honestly, any outsider

168
00:14:37,080 --> 00:14:43,360
is close to impossible to touch data like what is the EORA of the process.

169
00:14:43,360 --> 00:14:49,400
And then, for example, what is the inventory level and what is the order level or the

170
00:14:49,400 --> 00:14:55,120
older number, the sales order number of this factory or for this side.

171
00:14:55,120 --> 00:14:59,800
So those are so sensitive that it's impossible to reduce to outsiders.

172
00:14:59,800 --> 00:15:05,600
So to answer your question, I think a lot of the thing is happening inside the factory,

173
00:15:05,600 --> 00:15:07,480
inside the wall of the factory.

174
00:15:07,480 --> 00:15:14,040
So the wall could be physical, it could be digital, but whichever, everything is contained.

175
00:15:14,040 --> 00:15:20,560
So I would say it's not a sexy field to work with by design.

176
00:15:20,560 --> 00:15:29,520
So that they can keep all the secret data that can affect their like stock price and

177
00:15:29,520 --> 00:15:37,720
then that can affect a lot of things, transactions that keep everything inside and they work on

178
00:15:37,720 --> 00:15:40,600
those data inside the wall.

179
00:15:40,600 --> 00:15:44,960
And so you mentioned these ALI systems.

180
00:15:44,960 --> 00:15:54,080
When folks are deploying AI to do visual inspection, that's one of the big use case areas for

181
00:15:54,080 --> 00:16:03,400
AI in the industrial context is defect reduction and visual inspection is a goal to replace

182
00:16:03,400 --> 00:16:12,760
the existing ALI systems which tend to be based on more traditional computer vision algorithms

183
00:16:12,760 --> 00:16:18,240
as opposed to machine learning or is it to as you're suggesting you take their data

184
00:16:18,240 --> 00:16:22,360
and optimize but still use those systems.

185
00:16:22,360 --> 00:16:27,840
And there are lots of different types of computer vision based systems on the in a manufacturing

186
00:16:27,840 --> 00:16:28,840
environment.

187
00:16:28,840 --> 00:16:32,960
There are these defect detection systems, there are the sorting types of systems, but they're

188
00:16:32,960 --> 00:16:39,840
all kind of characterized as being these closed systems where you'd want to have the

189
00:16:39,840 --> 00:16:42,440
data to optimize.

190
00:16:42,440 --> 00:16:47,280
Are folks trying to replace these with more open types of systems or just get access to

191
00:16:47,280 --> 00:16:50,400
the data and feed it back to what those systems are doing?

192
00:16:50,400 --> 00:16:58,080
Yeah, I think it depends on the stage of your development in using AI in factory.

193
00:16:58,080 --> 00:17:04,880
So ALI is actually a pretty mature field so you can go out and talk to at least a hundred

194
00:17:04,880 --> 00:17:11,040
different ALI vendors in the market and buy machines that do ALI for you.

195
00:17:11,040 --> 00:17:16,880
So like you said, so currently most of the ALI machine in the market are using the so-called

196
00:17:16,880 --> 00:17:21,560
traditional computer vision methods to find out the defects.

197
00:17:21,560 --> 00:17:28,680
For example, it can detect the lines, it can detect a pattern and then the ALI machine

198
00:17:28,680 --> 00:17:38,320
will realize that this pattern is like 10 degrees different from the position or the

199
00:17:38,320 --> 00:17:43,480
angle that it should be, or the part is missing because the pattern doesn't match.

200
00:17:43,480 --> 00:17:51,440
So those are pretty typical computer vision thing that, for example, OpenCV can do.

201
00:17:51,440 --> 00:17:59,280
But for some of the defects like solder, that is kind of fuzzy or ambiguous.

202
00:17:59,280 --> 00:18:00,800
Human can do pretty good on that.

203
00:18:00,800 --> 00:18:05,480
They can look at the image and say, hey, this is too much solder and this is like too

204
00:18:05,480 --> 00:18:06,480
little solder.

205
00:18:06,480 --> 00:18:13,880
But for computer or for traditional computer vision, what they see is just a bunch of stuff,

206
00:18:13,880 --> 00:18:14,880
right?

207
00:18:14,880 --> 00:18:18,560
They don't know the angle, they don't, it's probably hard for them to calculate the area

208
00:18:18,560 --> 00:18:25,320
and that would be the thing that is most suitable for the did learning base AI agreement to

209
00:18:25,320 --> 00:18:29,960
attack or to solve and so like I said, right?

210
00:18:29,960 --> 00:18:35,240
So ALI machine is capable if you can get the image out of the machine, then you can have

211
00:18:35,240 --> 00:18:44,120
a second stage re-inspection engine to the image so that you can fix the defect detection

212
00:18:44,120 --> 00:18:49,080
after you have fixed already the first level of defect detection.

213
00:18:49,080 --> 00:18:53,880
The first level will be the easier cases that you can detect with traditional computer

214
00:18:53,880 --> 00:18:54,880
vision.

215
00:18:54,880 --> 00:19:01,240
And in the second stage, you can use AI based method to solve the harder to detect kind

216
00:19:01,240 --> 00:19:04,680
of fuzzy and ambiguous case.

217
00:19:04,680 --> 00:19:12,920
And eventually, if the system is smart enough and maybe eventually some of the AI machines

218
00:19:12,920 --> 00:19:19,920
smart enough, it can combine both of the traditional and the new deep learning base method together

219
00:19:19,920 --> 00:19:27,560
so it can either switch between or fuse them together so that you have one super powerful

220
00:19:27,560 --> 00:19:32,360
AI machine that does everything and that can look at different kind of defects that had

221
00:19:32,360 --> 00:19:34,160
different characteristics.

222
00:19:34,160 --> 00:19:39,280
So the way I see that is like it does not one answer to that.

223
00:19:39,280 --> 00:19:44,760
It depends on the stage of your development and sometimes it's two stages and it's like

224
00:19:44,760 --> 00:19:48,160
one big stage and that.

225
00:19:48,160 --> 00:19:52,560
So we've talked about ALI and defect detection quite a bit.

226
00:19:52,560 --> 00:19:56,000
Were there other use cases that you mentioned in your presentation?

227
00:19:56,000 --> 00:19:57,000
Yes.

228
00:19:57,000 --> 00:20:04,560
So this presentation is mostly focused on defect detection even though I talked earlier,

229
00:20:04,560 --> 00:20:08,120
like there's other application of AI in Infectory.

230
00:20:08,120 --> 00:20:15,160
For example, like older prediction, like predictive maintenance, like safety improvement using

231
00:20:15,160 --> 00:20:20,560
computer vision or environmental aspect of the AI.

232
00:20:20,560 --> 00:20:26,200
So there are actually a lot of more different applications but it's not mentioned in my presentation.

233
00:20:26,200 --> 00:20:30,920
Based on the title of your presentation, you also were incorporating the idea of edge

234
00:20:30,920 --> 00:20:31,920
in Twitter.

235
00:20:31,920 --> 00:20:33,800
Where does that fit in?

236
00:20:33,800 --> 00:20:38,000
So there are two kind of scenarios.

237
00:20:38,000 --> 00:20:40,960
One is edge, the other one is cloud.

238
00:20:40,960 --> 00:20:47,000
So cloud in general is the mainstream of most of the AI application right now.

239
00:20:47,000 --> 00:20:52,480
So in terms of image, you capture image or video, you send it to the cloud and the cloud

240
00:20:52,480 --> 00:20:58,960
had usually super powerful machine with graphic processing there and they do all the inference

241
00:20:58,960 --> 00:21:01,880
and the speedouts and results.

242
00:21:01,880 --> 00:21:08,560
For example, pass or fail or they tell you the object type does contain in this image

243
00:21:08,560 --> 00:21:11,960
and does them send it back to the user which is at edge.

244
00:21:11,960 --> 00:21:13,880
So that's one scenario.

245
00:21:13,880 --> 00:21:19,960
The other scenario is edge meaning you don't send your image or your data to the cloud.

246
00:21:19,960 --> 00:21:26,120
You compute everything on site and when I say on site, there are a few benefits to that.

247
00:21:26,120 --> 00:21:28,920
So one obvious benefit is privacy.

248
00:21:28,920 --> 00:21:33,080
So there's no intermediate point that you can hijack the data.

249
00:21:33,080 --> 00:21:38,080
You can keep everything within the locality of this edge device.

250
00:21:38,080 --> 00:21:42,000
So privacy is number one and number two is reliability.

251
00:21:42,000 --> 00:21:49,000
So reliability, what I mean by that is sometimes the network or the communication become reliable.

252
00:21:49,000 --> 00:21:54,880
You can drop off or the bandwidth can be reduced and there could be error in your transmission.

253
00:21:54,880 --> 00:22:01,680
And as I said earlier, they could be interference or somebody can hijack your data or your package.

254
00:22:01,680 --> 00:22:04,800
And that causes a huge issue in the result.

255
00:22:04,800 --> 00:22:09,160
So by having edge AI, then you avoid that problem.

256
00:22:09,160 --> 00:22:14,440
And the last point of edge AI benefit would be network efficiency.

257
00:22:14,440 --> 00:22:17,400
So in terms of a defect detection.

258
00:22:17,400 --> 00:22:22,560
So image data is an input for your AI engine.

259
00:22:22,560 --> 00:22:27,840
And image data are usually quite big, especially if you had like hundreds, thousands, if not

260
00:22:27,840 --> 00:22:32,360
millions of images sending to the cloud, every hour, right?

261
00:22:32,360 --> 00:22:37,040
That would pretty much jam all your infrastructure, the network infrastructure.

262
00:22:37,040 --> 00:22:40,840
So by computing on the edge, then you avoid that problem altogether.

263
00:22:40,840 --> 00:22:43,360
So you don't have that problem.

264
00:22:43,360 --> 00:22:50,680
And so most of the conversation around edge today is focused on edge inference.

265
00:22:50,680 --> 00:22:57,440
And we're seeing a lot of new hardware, actually almost every week, there's folks proposing

266
00:22:57,440 --> 00:23:04,240
new hardware approaches for edge-based inference, low-power inference, et cetera.

267
00:23:04,240 --> 00:23:10,640
But there's also work happening around more advanced scenarios.

268
00:23:10,640 --> 00:23:17,760
So just starting to see like federated learning and ways to combine the edge with the cloud

269
00:23:17,760 --> 00:23:22,720
and to have learning that incorporates the edge.

270
00:23:22,720 --> 00:23:27,840
Are you doing any work in that area or seeing anything interesting in some of these more

271
00:23:27,840 --> 00:23:29,840
advanced scenarios?

272
00:23:29,840 --> 00:23:31,160
Yes.

273
00:23:31,160 --> 00:23:37,120
So when you just mentioned, I believe you something called hybrid AI.

274
00:23:37,120 --> 00:23:44,440
So basically AI computation or inference can happen, like I said, in both cloud and edge.

275
00:23:44,440 --> 00:23:49,360
And edge device traditionally are pretty, they don't have that much power.

276
00:23:49,360 --> 00:23:53,520
So they can emote in the case of image, they can emote capture image.

277
00:23:53,520 --> 00:24:00,720
And it tracks some features like edge, like regional interests, and send the data back

278
00:24:00,720 --> 00:24:08,320
to the cloud to do most of the computation like detection and classification and all that.

279
00:24:08,320 --> 00:24:17,160
But now they more and more AI device like you said is popping up from the market pretty

280
00:24:17,160 --> 00:24:18,480
much every day.

281
00:24:18,480 --> 00:24:25,440
I think in addition to those AI chips, per se, a lot of existing device like cameras,

282
00:24:25,440 --> 00:24:31,360
they start to have more and more terror frops inside their module already.

283
00:24:31,360 --> 00:24:36,720
So I think it's right now it's kind of like a messy situation that everybody is trying

284
00:24:36,720 --> 00:24:42,600
to capture some piece of this AI shape or edge AI market.

285
00:24:42,600 --> 00:24:50,120
So what I observed right now may not be conclusive, but I think this trend is going to continue.

286
00:24:50,120 --> 00:24:57,080
And then everybody is going to expand their territory, so Amazon is going to claim that

287
00:24:57,080 --> 00:25:03,080
a cloud service is more powerful, more accurate, and then it's easier to deploy and all that.

288
00:25:03,080 --> 00:25:07,360
And then I think the edge AI people or the chip people, they're going to say, hey, it's

289
00:25:07,360 --> 00:25:11,920
more private, it's more reliable and what now I think this was going to continue.

290
00:25:11,920 --> 00:25:15,440
And then eventually nobody is going to win or lose.

291
00:25:15,440 --> 00:25:16,840
I think it's going to be win-win.

292
00:25:16,840 --> 00:25:22,520
And then it's depending on your usage scenario and then you will choose the best solution.

293
00:25:22,520 --> 00:25:23,920
I'm sure.

294
00:25:23,920 --> 00:25:32,960
Going back to this idea of hybrid AI, have you seen any use cases for doing some learning

295
00:25:32,960 --> 00:25:37,120
at the edge and some learning in the cloud?

296
00:25:37,120 --> 00:25:44,200
So in particular to learning, so most of the edge devices are not, they only do inference.

297
00:25:44,200 --> 00:25:50,320
So they don't have the capability of adapting or training on the go.

298
00:25:50,320 --> 00:25:55,640
So what usually happened is you had you download a model.

299
00:25:55,640 --> 00:25:59,120
So they call it model, a deep learning model from the cloud.

300
00:25:59,120 --> 00:26:02,000
So those are trained in the cloud.

301
00:26:02,000 --> 00:26:05,800
And in the edge, they simply do the inference or testing.

302
00:26:05,800 --> 00:26:14,120
So in the case of human detection and at edge, they deploy, like say smart camera.

303
00:26:14,120 --> 00:26:15,920
Smart camera with the AI chip inside.

304
00:26:15,920 --> 00:26:21,560
For example, the smart camera will do the detection and happy, right?

305
00:26:21,560 --> 00:26:30,480
But if somehow the population that this camera see starts to change or migrate over time,

306
00:26:30,480 --> 00:26:35,960
so you might need to update your model about how people around this area look like, right?

307
00:26:35,960 --> 00:26:38,640
So in this case, you have to update your model.

308
00:26:38,640 --> 00:26:44,160
And currently all the training is still in the cloud and updating of the model is still

309
00:26:44,160 --> 00:26:45,480
happening in the cloud.

310
00:26:45,480 --> 00:26:52,480
And you still have to periodically or depends on the event that you download the new model

311
00:26:52,480 --> 00:26:54,960
or updating model to the edge device.

312
00:26:54,960 --> 00:27:00,240
So there's no device training currently as far as I can see.

313
00:27:00,240 --> 00:27:07,160
So the hybrid AI that I see is mostly training in the cloud inference in the edge.

314
00:27:07,160 --> 00:27:13,480
Yeah, that's a little bit different from this thing that is you're still a bit undefined

315
00:27:13,480 --> 00:27:20,120
and maybe even still a bit mythical of federated AI where you're trying to do distributed training

316
00:27:20,120 --> 00:27:22,160
across multiple devices.

317
00:27:22,160 --> 00:27:26,840
But I just, I keep asking everyone who might know a little bit about, might have seen

318
00:27:26,840 --> 00:27:31,600
it to get a sense for if anyone's doing it out in the wild yet.

319
00:27:31,600 --> 00:27:32,600
Yeah.

320
00:27:32,600 --> 00:27:39,160
In terms of distributed AI in training, I do see a quite a few applications in there.

321
00:27:39,160 --> 00:27:43,360
So in coming back to the example of human recognition.

322
00:27:43,360 --> 00:27:48,640
So for example, you can place your camera in different locations, different cities,

323
00:27:48,640 --> 00:27:51,160
and different city would have different people looking differently, right?

324
00:27:51,160 --> 00:27:57,080
In New York, people look like they're dressing suits and like very serious and look serious.

325
00:27:57,080 --> 00:28:02,440
So and then you have some camera that's in urban area, suburban area that people dress at

326
00:28:02,440 --> 00:28:05,040
t-shirt and sandals and whatnot.

327
00:28:05,040 --> 00:28:11,920
So collection of data, they happen, distributionally, that's obvious, right?

328
00:28:11,920 --> 00:28:16,760
And in terms of training, I think distributed training, they might have a local training

329
00:28:16,760 --> 00:28:24,160
server that is in the proximity of those sensors and do their local training on how the

330
00:28:24,160 --> 00:28:29,840
human look like in that area and eventually they'll combine them out in the cloud so

331
00:28:29,840 --> 00:28:36,960
that you would have this like very small argument that can understand how human look like

332
00:28:36,960 --> 00:28:38,360
in different areas.

333
00:28:38,360 --> 00:28:44,680
So that's what I see in distributed AI training, but I don't see distributed AI inference

334
00:28:44,680 --> 00:28:50,480
because there's no such need to do inference in multiple locations as far as I see because

335
00:28:50,480 --> 00:28:56,000
inference usually is not as heavy as training, so you don't need distributed inference.

336
00:28:56,000 --> 00:29:03,880
And so this distributed training that you're referring to when the model is combined, is

337
00:29:03,880 --> 00:29:10,760
that a fairly simple combination like some kind of ensembling or where you're running

338
00:29:10,760 --> 00:29:16,560
multiple models and picking the one that's most confident or is it something more elaborate

339
00:29:16,560 --> 00:29:20,880
where you're kind of merging the models in some kind of way?

340
00:29:20,880 --> 00:29:28,040
Yeah, that's a different level of integration or a different level of merging of the model.

341
00:29:28,040 --> 00:29:31,480
So the simplest way is like winner winner take all right.

342
00:29:31,480 --> 00:29:37,960
So whoever whose model performs the best in that case then wins, right?

343
00:29:37,960 --> 00:29:39,960
So the other case is simple waiting.

344
00:29:39,960 --> 00:29:46,320
So you weigh the score from different model in different locations and combined results.

345
00:29:46,320 --> 00:29:49,840
So that's a second approach, still pretty simple.

346
00:29:49,840 --> 00:29:54,760
And eventually you can go through this like super complicated case that you have another

347
00:29:54,760 --> 00:29:56,800
neural network, right?

348
00:29:56,800 --> 00:30:03,080
That neural network is trying to combine all this like child model and combine it together.

349
00:30:03,080 --> 00:30:08,760
And in that you will need training data from more training data so that you can train

350
00:30:08,760 --> 00:30:17,160
the parameters of this like super neural network on top of all these child models.

351
00:30:17,160 --> 00:30:21,080
So there are different complexity in terms of how you combine it together.

352
00:30:21,080 --> 00:30:28,320
Going back to your presentation, one of the things that you mentioned is the need to

353
00:30:28,320 --> 00:30:38,480
redefine the problems in order to get the kind of results in terms of ROI and solution

354
00:30:38,480 --> 00:30:41,960
capability that you're ultimately looking for.

355
00:30:41,960 --> 00:30:45,320
Can you elaborate a bit on that and how that comes up?

356
00:30:45,320 --> 00:30:52,400
Yeah, so when we start talking to the factory people, right, there are there are how working

357
00:30:52,400 --> 00:30:57,520
people they they they focus on one specific area every single day.

358
00:30:57,520 --> 00:31:02,880
So one of them talk to us about the problem that they are facing, they're like, oh, our

359
00:31:02,880 --> 00:31:11,000
inventory is so big that we have to keep building like a warehouse to store all the parts that

360
00:31:11,000 --> 00:31:12,000
we need.

361
00:31:12,000 --> 00:31:16,320
And then the warehouse is getting bigger and bigger is getting harder and harder to manage.

362
00:31:16,320 --> 00:31:21,080
And then so they're like, oh, can AI help us to manage the inventory and also to help

363
00:31:21,080 --> 00:31:23,600
us to build a bigger warehouse?

364
00:31:23,600 --> 00:31:29,160
And we're like, hmm, that's an interesting question to have, let's figure out like how

365
00:31:29,160 --> 00:31:30,160
we do that.

366
00:31:30,160 --> 00:31:32,640
Well, now architect, we don't build warehouses.

367
00:31:32,640 --> 00:31:34,720
So how can we help them?

368
00:31:34,720 --> 00:31:42,640
So in so after digging deeper and talk to them more, they actually had different problem

369
00:31:42,640 --> 00:31:46,160
that caused this warehouse problem, right?

370
00:31:46,160 --> 00:31:52,840
So what they have is they actually don't have accurate forecast or accurate prediction

371
00:31:52,840 --> 00:31:58,320
of their order so that in order to meet or in order to fulfill the requirement of the

372
00:31:58,320 --> 00:31:59,320
customer.

373
00:31:59,320 --> 00:32:04,440
They can prepare a lot of parts in advance so that if the customer wants something, they

374
00:32:04,440 --> 00:32:09,800
can quickly pull out the parts and assemble the computer for the customer.

375
00:32:09,800 --> 00:32:14,880
So eventually this is not a warehouse problem or how to manage a warehouse problem.

376
00:32:14,880 --> 00:32:18,600
This is actually an older position problem.

377
00:32:18,600 --> 00:32:27,480
So this is what I mean by one of the key examples redefine the problem and using AI.

378
00:32:27,480 --> 00:32:32,040
And another thing that you mentioned is this AI thing.

379
00:32:32,040 --> 00:32:36,400
So our AI stands for return on investment.

380
00:32:36,400 --> 00:32:44,360
So this is a typical practice of everybody working in manufacturing or industry.

381
00:32:44,360 --> 00:32:51,480
So in order to buy something or in order to adopt a solution, you have to actually calculate.

382
00:32:51,480 --> 00:32:58,600
So this much cost will take you two years, three years, 80 months, whatever long time that

383
00:32:58,600 --> 00:33:02,560
to recapture the cost that you invest.

384
00:33:02,560 --> 00:33:09,520
And so for a lot of the thing, our AI is really hard to calculate because this is not a typical

385
00:33:09,520 --> 00:33:13,960
stationary or static optimization problem that you had all the parameters laying out in

386
00:33:13,960 --> 00:33:15,560
front of you.

387
00:33:15,560 --> 00:33:19,640
And then you can do your optimization and do your trick.

388
00:33:19,640 --> 00:33:23,640
And then you come up with some savings or you come up with some improvement.

389
00:33:23,640 --> 00:33:26,480
This is actually a dynamic process.

390
00:33:26,480 --> 00:33:34,520
So when you are optimizing the system, you're actually changing the nature of the factory.

391
00:33:34,520 --> 00:33:37,480
And it's similar to start market, right?

392
00:33:37,480 --> 00:33:43,800
So you don't, it's like Warren Buffet, like praying in the start market when he decided

393
00:33:43,800 --> 00:33:51,240
to buy something, actually he changed the S&P index already because the volume is so big.

394
00:33:51,240 --> 00:33:52,360
So similar to that.

395
00:33:52,360 --> 00:33:59,000
So when you actually change something or invest to something into the factory, you actually

396
00:33:59,000 --> 00:34:01,840
change the nature of the problem.

397
00:34:01,840 --> 00:34:04,120
So it's on the gold target.

398
00:34:04,120 --> 00:34:09,240
So our AI is consequently very difficult to calculate.

399
00:34:09,240 --> 00:34:17,240
Can you give a specific example of a change that AI causes in the factory that makes it

400
00:34:17,240 --> 00:34:20,320
difficult to estimate ROI?

401
00:34:20,320 --> 00:34:27,280
And one point of clarification before you do that is the challenge, specifically with

402
00:34:27,280 --> 00:34:32,160
forecasting ROI or measuring it after the fact or both.

403
00:34:32,160 --> 00:34:33,400
I think it's both.

404
00:34:33,400 --> 00:34:35,480
So forecasting is actually harder.

405
00:34:35,480 --> 00:34:38,200
So let's say I decided to buy robots.

406
00:34:38,200 --> 00:34:39,200
Okay.

407
00:34:39,200 --> 00:34:46,080
So buy robots to move all the parts in different size of my factory.

408
00:34:46,080 --> 00:34:50,880
And then you can say, hey, the robot cost me like a million dollars and blah, blah, blah.

409
00:34:50,880 --> 00:34:53,560
And then so the AI is like 18 months, right?

410
00:34:53,560 --> 00:34:57,680
But then this is actually eventually is going to be wrong.

411
00:34:57,680 --> 00:35:07,200
Reason for that is by adopting robots, you actually improve the accuracy of or the time that

412
00:35:07,200 --> 00:35:10,800
arrive at different manufacturing lines.

413
00:35:10,800 --> 00:35:14,080
So the time that you arrive at the line is more precise.

414
00:35:14,080 --> 00:35:19,240
So you can produce your product in a more timing manner, manner.

415
00:35:19,240 --> 00:35:21,400
So your euro will be higher and all that.

416
00:35:21,400 --> 00:35:30,840
So when your euro is higher, actually the overall profit or revenue of the companies is increased.

417
00:35:30,840 --> 00:35:35,240
And how do you factor that into your calculation of our AI, right?

418
00:35:35,240 --> 00:35:41,880
And that's why in prediction or imagery, both the our calculations is often wrong.

419
00:35:41,880 --> 00:35:42,880
Okay.

420
00:35:42,880 --> 00:35:46,160
It sounds like if I could maybe try to restate that.

421
00:35:46,160 --> 00:35:54,400
It sounds like when you automate in the manufacturing facility, you know, there's a specific

422
00:35:54,400 --> 00:35:57,120
thing that you're trying to improve.

423
00:35:57,120 --> 00:36:02,440
Maybe it's reducing defect and then you automate, but the introduction of automation has a

424
00:36:02,440 --> 00:36:09,160
bunch of downstream effects like increasing consistency and reducing wait times and things

425
00:36:09,160 --> 00:36:10,240
like that.

426
00:36:10,240 --> 00:36:16,920
And it's hard to produce an all encompassing ROI study that takes into account all these

427
00:36:16,920 --> 00:36:17,920
different factors.

428
00:36:17,920 --> 00:36:23,560
Trista, were there any other thoughts that you included in your presentation as key takeaways

429
00:36:23,560 --> 00:36:29,520
or any parting thoughts that you'd like to leave with the audience that, you know, might

430
00:36:29,520 --> 00:36:33,160
help them in their path towards industry for data?

431
00:36:33,160 --> 00:36:34,160
Yes.

432
00:36:34,160 --> 00:36:40,720
And industry for that is a seamless integration of physical and digital world.

433
00:36:40,720 --> 00:36:45,160
So the obvious benefit will be accuracy, consistency, and ROI.

434
00:36:45,160 --> 00:36:49,560
And then when I want to stress, it's actually go higher than that.

435
00:36:49,560 --> 00:36:57,080
So if you can actually get all those digital and physical data, IoT data, with you, you

436
00:36:57,080 --> 00:36:59,400
can do much, much more optimization.

437
00:36:59,400 --> 00:37:04,440
You can actually change the whole scope of how you design the manufacturing line.

438
00:37:04,440 --> 00:37:08,160
You can change how you take the order.

439
00:37:08,160 --> 00:37:09,600
You can change how you ship.

440
00:37:09,600 --> 00:37:13,200
You can change the location where you start your inventory.

441
00:37:13,200 --> 00:37:14,840
You can change the whole thing.

442
00:37:14,840 --> 00:37:23,640
I think without the digital and physical integration or people call it digital twin, without

443
00:37:23,640 --> 00:37:24,640
that.

444
00:37:24,640 --> 00:37:31,920
I know where you can do such a ground up change of how the whole manufacturing process.

445
00:37:31,920 --> 00:37:39,200
So in addition to focusing on the obvious benefit, I think everybody working in manufacturing

446
00:37:39,200 --> 00:37:48,280
should look one step up and looking at what you can do to change the whole process.

447
00:37:48,280 --> 00:37:54,240
And then this, again, it's not just 10%, 20%, it could be like exponential, like two times

448
00:37:54,240 --> 00:37:57,680
and it can be 10 times because the whole thing is changed.

449
00:37:57,680 --> 00:37:58,680
Awesome.

450
00:37:58,680 --> 00:38:01,280
Well, Trista, thank you so much for chatting with us.

451
00:38:01,280 --> 00:38:02,280
Thank you.

452
00:38:02,280 --> 00:38:08,920
All right, everyone, that's our show for today.

453
00:38:08,920 --> 00:38:15,440
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com

454
00:38:15,440 --> 00:38:18,440
slash GTC19.

455
00:38:18,440 --> 00:38:21,160
Thanks again to Dell for sponsoring this series.

456
00:38:21,160 --> 00:38:25,920
Be sure to check them out at dellemc.com slash precision.

457
00:38:25,920 --> 00:38:52,680
As always, thanks so much for listening and catch you next time.

