1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:36,520
Before we jump into today's episode, a big thanks to everyone who listened to shared

5
00:00:36,520 --> 00:00:40,560
and commented on our AI platform series.

6
00:00:40,560 --> 00:00:45,840
As always, we love hearing your feedback and we've received a ton of it on these shows.

7
00:00:45,840 --> 00:00:51,160
Stay tuned for part two of the series, early next year, and if you haven't yet, be sure

8
00:00:51,160 --> 00:00:56,600
to sign up for my upcoming series of e-books on the topic, which we'll be releasing soon.

9
00:00:56,600 --> 00:01:02,160
Finally, over the next few weeks, I'll be bringing you great interviews from the AWS

10
00:01:02,160 --> 00:01:07,400
Reinvent Conference, which I'm at right now, NURRIPS and CUBE CON.

11
00:01:07,400 --> 00:01:11,720
And I'd love to connect with any listeners and attendants, so please shoot me a message

12
00:01:11,720 --> 00:01:19,440
via at Sam Charrington on Twitter, email or thetwimlai.com website if you'll be around.

13
00:01:19,440 --> 00:01:24,320
Today we're joined by Stuart Reed, Chief Scientist at Numerical Research.

14
00:01:24,320 --> 00:01:29,800
Numerical, based in Stellanbaugh, South Africa, is an investment management firm that uses

15
00:01:29,800 --> 00:01:34,760
machine learning algorithms to make adaptive, unbiased, scalable, and testable training

16
00:01:34,760 --> 00:01:37,480
decisions for its funds.

17
00:01:37,480 --> 00:01:41,960
In our conversation, Stuart and I dig into the way Numerical uses machine learning and

18
00:01:41,960 --> 00:01:45,680
deep learning models to support the firm's investment decisions.

19
00:01:45,680 --> 00:01:50,960
In particular, we focus on techniques for modeling non-stationary time series, of which

20
00:01:50,960 --> 00:01:54,000
financial markets are just one example.

21
00:01:54,000 --> 00:01:58,920
We start from first principles and look at stationary versus non-stationary time series,

22
00:01:58,920 --> 00:02:03,680
discuss some of the challenges of building models using financial data, explore issues

23
00:02:03,680 --> 00:02:06,640
like model interpretability and much more.

24
00:02:06,640 --> 00:02:11,680
This was a very insightful conversation, which I expect will be useful not just for those

25
00:02:11,680 --> 00:02:13,480
in the Fintechs base.

26
00:02:13,480 --> 00:02:16,240
Enjoy.

27
00:02:16,240 --> 00:02:19,840
All right, everyone.

28
00:02:19,840 --> 00:02:22,240
I am on the line with Stuart Reed.

29
00:02:22,240 --> 00:02:25,440
Stuart is the Chief Scientist at Numerical Research.

30
00:02:25,440 --> 00:02:28,720
Stuart, welcome to this week in machine learning and AI.

31
00:02:28,720 --> 00:02:33,120
Yeah, thanks for having me, Sam, it's really great to be speaking to you.

32
00:02:33,120 --> 00:02:37,920
I've been listening to the podcast for a long time now, so it's great.

33
00:02:37,920 --> 00:02:40,280
And I'm really glad we were able to connect.

34
00:02:40,280 --> 00:02:44,520
This one has taken a while to put together for a variety of reasons.

35
00:02:44,520 --> 00:02:51,360
We initially connected around the time, kind of in the run-up to the deep learning and

36
00:02:51,360 --> 00:02:53,840
dava, which you participated in.

37
00:02:53,840 --> 00:02:59,640
And for whatever reason, it's taken us a bit to connect, but welcome once again.

38
00:02:59,640 --> 00:03:01,280
Yeah, thank you very much.

39
00:03:01,280 --> 00:03:06,120
It was a pretty busy week during the deep learning and dava, so I'm not surprised that

40
00:03:06,120 --> 00:03:10,120
it took a while, but good to finally be speaking to you.

41
00:03:10,120 --> 00:03:14,640
So why don't we get started with a little bit of your background.

42
00:03:14,640 --> 00:03:18,840
You are currently focused on applying AI to finance.

43
00:03:18,840 --> 00:03:21,440
How did you get here?

44
00:03:21,440 --> 00:03:28,760
Yeah, so I think that my interest in technology itself predates my interest in finance.

45
00:03:28,760 --> 00:03:35,640
I was actually the youngest South African to get an amateur radio license when I was 11.

46
00:03:35,640 --> 00:03:38,960
And that's kind of when I got into technology.

47
00:03:38,960 --> 00:03:44,980
But soon afterwards, I discovered finance, started out with the traditional books by Warren

48
00:03:44,980 --> 00:03:50,760
Buffett and Benjamin Graham on security analysis and how do you actually value companies.

49
00:03:50,760 --> 00:03:56,560
And then that slowly shifted into some of the more modern approaches, taken by lacronase

50
00:03:56,560 --> 00:03:59,440
on technologies in D.E. Shaw and AQI.

51
00:03:59,440 --> 00:04:01,200
And that's really quantitative finance.

52
00:04:01,200 --> 00:04:07,400
So that interest is actually what propelled me to study computer science in the first place.

53
00:04:07,400 --> 00:04:12,760
And you're now, again, chief scientist at numerical research, what does numerical research

54
00:04:12,760 --> 00:04:13,760
do?

55
00:04:13,760 --> 00:04:18,000
Yes, a numerical research is a startup financial services provider based in still in

56
00:04:18,000 --> 00:04:20,240
Barcelona, Africa.

57
00:04:20,240 --> 00:04:25,760
We have two investment funds which are run entirely by machine learning algorithms.

58
00:04:25,760 --> 00:04:26,760
So that's what we do.

59
00:04:26,760 --> 00:04:31,840
We're using deep learning algorithms for the most part to predict what is going to happen

60
00:04:31,840 --> 00:04:38,200
in global financial markets, which sounds like the best idea ever, but it's incredibly

61
00:04:38,200 --> 00:04:42,040
challenging for a number of different reasons.

62
00:04:42,040 --> 00:04:44,240
And we will get into those.

63
00:04:44,240 --> 00:04:49,480
The kind of way you framed your presentation as I understand it at the end of was around

64
00:04:49,480 --> 00:04:54,360
the application of machine learning to kind of the broader class of problems that you see

65
00:04:54,360 --> 00:05:02,800
in financial markets, and that is a specific type of time series analysis.

66
00:05:02,800 --> 00:05:08,640
And you made a distinction when we were talking before we got started between stationary

67
00:05:08,640 --> 00:05:11,280
versus non-stationary time series.

68
00:05:11,280 --> 00:05:12,960
Can you elaborate on that?

69
00:05:12,960 --> 00:05:13,960
Yeah.

70
00:05:13,960 --> 00:05:19,200
So, you know, a stationary time series is really one which is sample from a distribution

71
00:05:19,200 --> 00:05:21,200
which doesn't change.

72
00:05:21,200 --> 00:05:25,720
So that fits quite nicely with your traditional machine learning paradigm, which is where

73
00:05:25,720 --> 00:05:32,240
you are fundamentally assuming that your data generating process is constant or very slowly

74
00:05:32,240 --> 00:05:33,800
time varying.

75
00:05:33,800 --> 00:05:39,760
But financial markets are anything but stationary that continuously changing.

76
00:05:39,760 --> 00:05:44,680
In fact, I would go as far as to say that the markets themselves are adversarial.

77
00:05:44,680 --> 00:05:47,520
They don't really want you to succeed.

78
00:05:47,520 --> 00:05:50,320
And this gets into the whole debate of like market efficiency.

79
00:05:50,320 --> 00:05:55,680
So my focus is very much on how can we get deep learning algorithms which were designed

80
00:05:55,680 --> 00:06:02,320
to work in a stationary environment under some reasonably strict assumptions?

81
00:06:02,320 --> 00:06:07,080
How can we get those algorithms in our work in an environment where you can not only experience

82
00:06:07,080 --> 00:06:12,880
a lot of non-stationary but experience extreme shifts in the distribution of the data that

83
00:06:12,880 --> 00:06:14,640
has been generated.

84
00:06:14,640 --> 00:06:19,440
So these kind of points are regime shifts, structural breaks, critical transitions, change

85
00:06:19,440 --> 00:06:24,480
points, as many names for them in the literature, but I prefer change points.

86
00:06:24,480 --> 00:06:32,200
And so what are the main challenges that you see in doing this, do you, are there, you

87
00:06:32,200 --> 00:06:37,200
know, is it just hard or are there distinct challenges that you can point to in trying

88
00:06:37,200 --> 00:06:41,680
to apply machine learning and deep learning to these types of time series?

89
00:06:41,680 --> 00:06:42,680
Okay.

90
00:06:42,680 --> 00:06:43,680
Yeah.

91
00:06:43,680 --> 00:06:49,120
Now there are some very specific challenges, but let me first take a step back and say

92
00:06:49,120 --> 00:06:54,600
that if you are interested in applying deep learning algorithms to financial markets, there

93
00:06:54,600 --> 00:07:02,040
are more problems than just non-stationarity and kind of like this adversarial behavior.

94
00:07:02,040 --> 00:07:06,960
You also have a very challenging problem with signal to noise.

95
00:07:06,960 --> 00:07:09,200
So there's a lot of noise in financial markets.

96
00:07:09,200 --> 00:07:15,680
In fact, most derivatives that are priced these days are priced using random walk models.

97
00:07:15,680 --> 00:07:20,160
And that's kind of where my fascination with randomness comes in, but also markets are

98
00:07:20,160 --> 00:07:24,120
challenging because of this non-stationarity.

99
00:07:24,120 --> 00:07:29,880
But there are a number of different challenges associated with China predict time series

100
00:07:29,880 --> 00:07:34,960
which can abruptly change from one distribution to another one.

101
00:07:34,960 --> 00:07:40,960
And that's, you know, can we actually detect these change points in a timely manner?

102
00:07:40,960 --> 00:07:46,760
And how many of these change points are actually going to occur? Is it a one-soft kind of transition

103
00:07:46,760 --> 00:07:53,120
as you might see in some ecological systems or is it a multi-way kind of transition kind

104
00:07:53,120 --> 00:07:58,520
of like we have in finance where you cycle between maybe a low volatility and a high volatility

105
00:07:58,520 --> 00:08:02,200
regime or between a bear market and a bull market?

106
00:08:02,200 --> 00:08:06,760
Then the next challenge is the duration of that change point.

107
00:08:06,760 --> 00:08:13,840
So you can have change points which are extremely abrupt. Like for example, if we're using change

108
00:08:13,840 --> 00:08:19,520
point analysis or machine learning algorithms to identify the onset of an attack on a network

109
00:08:19,520 --> 00:08:25,000
like network intrusion detection, that's a very instantaneous kind of change in the distribution

110
00:08:25,000 --> 00:08:29,640
in that network activity. Whereas in financial markets, it's actually generally a slightly

111
00:08:29,640 --> 00:08:36,120
slower transition. And that can actually make it harder to detect because it's lots of small

112
00:08:36,120 --> 00:08:42,200
changes which kind of add up and eventually become a very large regime shift.

113
00:08:42,200 --> 00:08:49,320
Also another challenge is the extensiveness of the regime shift. Is this regime shift

114
00:08:49,320 --> 00:08:54,440
or is this change point affecting the entire model which we've trained or is it just affecting

115
00:08:54,440 --> 00:09:00,040
a subset of the model? Is it just affecting the part of the model which is looking at interest

116
00:09:00,040 --> 00:09:05,160
rates or the part of the model which is looking at currencies? And then the other two challenges

117
00:09:05,160 --> 00:09:11,240
is the magnitude. So very large or very small shifts and where those shifts are happening.

118
00:09:11,240 --> 00:09:16,360
And then the certainty around this. So how confident are we that we've identified a real

119
00:09:16,360 --> 00:09:23,080
change in the distribution as opposed to just an outlier or an anomaly in the data?

120
00:09:23,080 --> 00:09:28,880
So those are kind of like the challenges that you're presented with when you're trying

121
00:09:28,880 --> 00:09:36,880
to use deep learning algorithms to predict time series which are non-stationary and can

122
00:09:36,880 --> 00:09:44,560
have these abrupt transitions. And when you're faced with those very distinct challenges,

123
00:09:44,560 --> 00:09:51,360
do you attempt to kind of pick them off one by one or do they kind of together lead you to

124
00:09:52,000 --> 00:09:58,320
a class of solutions to this general problem that has good properties for some subset of the

125
00:09:58,320 --> 00:10:05,040
challenges? That's a very good question. I have definitely taken a very non-linear approach to

126
00:10:06,720 --> 00:10:12,640
this set of problems. In fact, I've only realized that some of them are problems quite recently

127
00:10:14,000 --> 00:10:21,120
and it's kind of like an ongoing area of research for me. But generally what kind of spurred

128
00:10:21,120 --> 00:10:28,000
the interest was that we developed this framework at an numerical and we were training all of these

129
00:10:28,000 --> 00:10:34,800
different models. And one of the early things I noticed is that the best models were generally the

130
00:10:34,800 --> 00:10:41,920
ones which got the financial crisis right, like in terms of their predictions. So that motivated me

131
00:10:42,560 --> 00:10:49,120
to take a look at those models and try and work out what made them get it right. And I think for

132
00:10:49,120 --> 00:10:54,160
the most part it was luck for those models. But then I got really interested in these kind of

133
00:10:54,160 --> 00:11:00,160
like change points because the reality is that significant changes in the distribution of

134
00:11:00,160 --> 00:11:05,840
financial markets, not only affects how you should make your investment decisions, which affects

135
00:11:05,840 --> 00:11:13,440
everybody's retirement savings and their ability to do things, but also policy changes. When we're

136
00:11:13,440 --> 00:11:20,800
talking about how the government should decide to set interest rates or how it should behave in

137
00:11:20,800 --> 00:11:28,160
situations like trade wars, you know, these kind of things. So my approach to it was very much

138
00:11:28,160 --> 00:11:34,480
from an applied perspective. It was like, this is an interesting problem. And then I tried a whole

139
00:11:34,480 --> 00:11:40,320
bunch of different things to try and solve that problem. And then slowly began to see that

140
00:11:40,320 --> 00:11:47,120
it was a very common problem, which actually arises in many different areas, not just finance and

141
00:11:47,120 --> 00:11:53,360
economics, but also in statistical quality control and manufacturing, speech recognition, medical

142
00:11:53,360 --> 00:12:00,240
condition monitoring, you know, disaster prediction. If we're talking about like earthquakes,

143
00:12:02,080 --> 00:12:06,400
network intrusion detection, there's a whole bunch of really interesting ideas. And then through

144
00:12:06,400 --> 00:12:11,040
looking at all of the different applications, kind of piece together the theory and what the

145
00:12:11,040 --> 00:12:15,680
challenges are when you're looking at these kinds of time series. And what's interesting is that

146
00:12:15,680 --> 00:12:20,720
different parts of the literature from different domains are focusing on different subproblems.

147
00:12:20,720 --> 00:12:28,720
Like duration of the change point is a much bigger issue in ecology, for example, than in

148
00:12:28,720 --> 00:12:33,920
network intrusion detection. So I hope that kind of answers the question. It's a very good question.

149
00:12:33,920 --> 00:12:38,560
I have no idea. I'm just kind of going at it and seeing what happens.

150
00:12:38,560 --> 00:12:47,200
You mentioned that one of the ways you or one of the hallmarks of a good model is the ability

151
00:12:47,200 --> 00:12:54,240
to predict the financial crisis, which kind of suggests to me that the main thing you're doing

152
00:12:54,240 --> 00:13:03,440
to test here is kind of back testing against historical financial data. And I guess I'm wondering

153
00:13:03,440 --> 00:13:10,160
is what exactly when we say a model or these models, what exactly are we talking about? Is it one

154
00:13:10,160 --> 00:13:15,680
model that predicts the price of the S&P or some portfolio or do you have models for individual

155
00:13:15,680 --> 00:13:21,040
securities or are you modeling some of the sub components that you mentioned like interest rates

156
00:13:21,040 --> 00:13:26,400
and bond prices and things like that? Like how granular are the models that you're developing?

157
00:13:26,400 --> 00:13:33,680
Yeah, that's a great question. So I think back testing is a little bit of a swear word in finance.

158
00:13:33,680 --> 00:13:36,720
Is it? What did I just walk into?

159
00:13:38,240 --> 00:13:44,000
So no, not at all. I mean, most people call it back testing. I'm not a huge fan of the term

160
00:13:44,000 --> 00:13:48,320
because back testing kind of implies that we have some model. It has some parameters.

161
00:13:48,320 --> 00:13:54,160
And what we do is we pick a whole bunch of different parameters. We run a simulation on

162
00:13:54,160 --> 00:14:00,080
historical data and we see which one did the best. And then generally that's the one that we pick

163
00:14:00,080 --> 00:14:06,720
going forward. And that's a very, very bad way to actually go about finding a good investment

164
00:14:06,720 --> 00:14:10,960
process because inevitably what you're going to do is you're going to curve, but you're going to

165
00:14:10,960 --> 00:14:15,680
you're going to memorize the historical data and you're going to pick very sub optimal parameters

166
00:14:15,680 --> 00:14:22,480
going into the future. So what we prefer to do is really like a walk forward simulation three

167
00:14:22,480 --> 00:14:30,640
time, which arises a whole bunch of additional challenges. One of the challenges that we

168
00:14:30,640 --> 00:14:38,320
have in finance, which doesn't exist in in some areas is this issue of survivorship bias.

169
00:14:38,320 --> 00:14:44,800
So for example, the S&P 500 as you mentioned today, like the constituents of that are not the same

170
00:14:44,800 --> 00:14:51,760
as the S&P 500 from 1980 or 1995. You actually have stocks coming in and coming out and more

171
00:14:51,760 --> 00:14:58,720
recently you have a lot of of stocks going out and and fewer like staying in that S&P for a long

172
00:14:58,720 --> 00:15:05,440
period of time. So we have this kind of like non stationarity, not only in the time series

173
00:15:05,440 --> 00:15:10,640
dimension, but also in the cross section. So like what stocks are we actually looking at at any

174
00:15:10,640 --> 00:15:16,240
particular point in time? And we can't just pick the S&P 500 today and run a simulation on that

175
00:15:16,240 --> 00:15:21,760
because then we've introduced a massive bias. None of the stocks in our simulation can fail,

176
00:15:22,800 --> 00:15:28,240
which is obviously not at all reality and reality. You have, you know, enrons which

177
00:15:28,240 --> 00:15:34,240
which completely fail and world comms which also completely fail. So the process that we've

178
00:15:34,240 --> 00:15:39,200
taken has been quite systematic. It's like how can we construct data sets which are truly

179
00:15:39,200 --> 00:15:46,080
representative of what the market looked like at that point in time? And and as far as what

180
00:15:46,080 --> 00:15:53,600
models we're using go, we started off quite simple. Then we very much focused on the recurrent

181
00:15:53,600 --> 00:15:58,800
models, recurrent deep neural networks. And lately we've actually had a lot of success with

182
00:15:58,800 --> 00:16:03,200
convolutional neural networks for time series analysis which also seems to be picking up in the

183
00:16:03,200 --> 00:16:09,120
literature. And generally we're looking at multi-variant time series prediction. And I think

184
00:16:09,120 --> 00:16:13,120
that that's interesting because there is this whole covariance matrix that you're trying to

185
00:16:13,120 --> 00:16:18,320
model. It's not just about, you know, if we can predict what the stock is going to do into the

186
00:16:18,320 --> 00:16:22,880
future, it's how the stock is going to influence, you know, these other stocks in the universe and

187
00:16:22,880 --> 00:16:31,840
how they evolve through time together as a as a collective. So as far as the number of models go,

188
00:16:31,840 --> 00:16:37,680
we actually want to have many many different models and then we kind of stack them together to

189
00:16:37,680 --> 00:16:42,480
generate better and better predictions. I think in our funds at the moment we've got about

190
00:16:42,480 --> 00:16:48,560
2000 neural networks in production, which is a challenge in and of itself, the engineering

191
00:16:48,560 --> 00:16:54,320
challenges is, you know, distinctly different to the theoretical challenge of how do you actually

192
00:16:54,320 --> 00:16:59,040
get the model to work in the first place. And those are continuously generating a lot of

193
00:16:59,040 --> 00:17:03,920
information, not just predictions, bad measures of their confidences in those predictions,

194
00:17:04,480 --> 00:17:09,520
measures of their errors and the predictions. Yeah, so there's a lot going on.

195
00:17:09,520 --> 00:17:16,480
Yeah. And can you speak it all to the granularity of those individual models or those 2000 models,

196
00:17:16,480 --> 00:17:22,880
all targeting trying to predict the performance of some basket of securities or you,

197
00:17:22,880 --> 00:17:28,240
I'm imagining you modeling different kind of underlying fundamentals. Is that the case?

198
00:17:29,360 --> 00:17:34,800
Yeah, so definitely looking at different things in different models, but also using different

199
00:17:34,800 --> 00:17:39,760
inputs into different models to generate those predictions. We're trying to come up with a very

200
00:17:39,760 --> 00:17:45,680
diverse set of predictions, which we can then ensemble over. As far as the granularity of the data

201
00:17:45,680 --> 00:17:50,880
itself goes, this is another challenge that you get in finance, which maybe you don't get in other

202
00:17:50,880 --> 00:17:57,120
spaces, is that you have some really, really important data, which only comes out quarterly.

203
00:17:58,320 --> 00:18:03,200
You know, if we're talking about like unemployment numbers or we're talking about GDP and, you know,

204
00:18:03,200 --> 00:18:08,000
all of these things obviously have an impact on the markets, but there's people are looking at

205
00:18:08,000 --> 00:18:12,800
them and they're using that to make investment decisions on like which countries they should be

206
00:18:12,800 --> 00:18:18,000
allocating capital to and which sectors in those countries they should be allocating to and then

207
00:18:18,000 --> 00:18:24,240
which stocks. But then you have this very high frequency data as well. So you've got price data,

208
00:18:24,240 --> 00:18:30,400
which is continuously coming at you. You've got volatility data. And what you want is a model

209
00:18:30,400 --> 00:18:35,840
which can actually weigh all of these time series, which may be occurring at very different time

210
00:18:35,840 --> 00:18:42,800
scales together in an unbiased way. Yeah, it's a very hard problem. I'm not sure that we've got a

211
00:18:42,800 --> 00:18:48,480
perfect solution to it yet, but we've tried a lot of different things. Ensembling is the most

212
00:18:48,480 --> 00:18:54,400
obvious approach and it's something that's worked quite well for us. Are you also incorporating in

213
00:18:54,400 --> 00:19:01,280
like natural language processing types of models or are trying to get at information that's

214
00:19:01,280 --> 00:19:06,960
embedded in unstructured text and documents as well or social media things like that?

215
00:19:07,760 --> 00:19:13,120
Yeah, this is a this is an interesting question because I'm actually a little bit of a sentiment

216
00:19:13,120 --> 00:19:19,280
skeptic. That's not to say that, you know, the sentiments that are extracted are wrong. But just

217
00:19:19,280 --> 00:19:26,880
consider the statement sales are down $40 million for the quarter. Now that is clearly a negative

218
00:19:26,880 --> 00:19:34,480
sentiment, right? I mean sales they're down for the quarter. But the reality is that if the

219
00:19:34,480 --> 00:19:41,680
analysts had expected sales to be down 60 million and then they were only down 14, the market would

220
00:19:41,680 --> 00:19:47,520
actually probably rally in that situation. So I think that the challenge with sentiment analysis

221
00:19:47,520 --> 00:19:54,320
is really that there is a lot of context which is very hard to capture. So we've spent quite a

222
00:19:54,320 --> 00:20:00,240
bit of time working on natural language processing and building these kind of like sentiment scores

223
00:20:00,240 --> 00:20:06,400
and including that in our models. And one of the other challenges with using deep neural networks

224
00:20:06,400 --> 00:20:12,720
in finances is obviously the problem of black box, right, is that it's very hard to interpret the

225
00:20:12,720 --> 00:20:18,560
models. And you recently had Sarah on your show. And I really enjoyed the the discussion that she

226
00:20:18,560 --> 00:20:25,120
had about interpretability because some people in machine learning seem to think that interpretability

227
00:20:25,120 --> 00:20:31,760
is a non-issue. But having spoken to many investors and many people who are interested in using

228
00:20:31,760 --> 00:20:38,080
this technology, but from outside of machine learning and and computer science interpretability

229
00:20:38,080 --> 00:20:43,040
is a big problem. So even if we did include these sentiment scores, it's actually quite

230
00:20:43,040 --> 00:20:49,680
challenging to work out if the model is is using that data and if it's using it in an optimal way.

231
00:20:49,680 --> 00:20:55,280
That's kind of like a segue into another like sub conversation. I mean the main technique we're

232
00:20:55,280 --> 00:20:59,920
using there is really ablation studies and sensitivity analysis. But to answer the question,

233
00:20:59,920 --> 00:21:05,200
yeah, we've looked at sentiment. I personally am a little bit of a skeptic, but that's maybe because

234
00:21:05,200 --> 00:21:12,720
I can't tell what my models are doing. So the general approach that you're taking is one of

235
00:21:12,720 --> 00:21:19,120
unsombling lots of models. Some might argue that you know what you're doing is kind of feature

236
00:21:19,120 --> 00:21:25,760
engineering and with the sophisticated and deep enough neural network and enough of the right

237
00:21:25,760 --> 00:21:30,800
data, the network could figure all that stuff out on its own and you should be looking at that.

238
00:21:30,800 --> 00:21:36,160
Have you have you looked at that approach and like how do you respond to to that kind of

239
00:21:37,040 --> 00:21:45,520
approach? I would say that that person is wrong. That's that's how I would respond. Okay, so

240
00:21:45,520 --> 00:21:51,840
this is another interesting discussion. You are very good at asking questions. I'm very impressed.

241
00:21:52,640 --> 00:21:57,520
You know, in the machine learning community, you know, data is considered to be like

242
00:21:57,520 --> 00:22:04,080
unreasonably effective, right? It's like the more data you have, the better your model is going to be.

243
00:22:05,520 --> 00:22:12,560
But my experience in trying to use deep learning and finance has been quite the opposite.

244
00:22:12,560 --> 00:22:18,000
And I think that the analogy that fits at best is really that the markets are kind of like a

245
00:22:18,000 --> 00:22:24,240
haystack. And there are a few needles in there which correspond to signals which you can extract

246
00:22:24,240 --> 00:22:30,240
and actually profit from. And when I get more data, so I go wider, I'm looking at more time

247
00:22:30,240 --> 00:22:35,920
series for more locations and whatnot. Generally, what I'm doing is I'm just throwing more hay

248
00:22:35,920 --> 00:22:42,960
on top of that haystack. So one of the challenges that we have is that if I and we're looking at

249
00:22:42,960 --> 00:22:48,960
about 400,000 independent time series at this stage, if I had to take all 400,000 of those time

250
00:22:48,960 --> 00:22:55,040
series and throw it into one model, it would almost certainly fail. And the main reason why is because

251
00:22:56,080 --> 00:23:02,000
because of two things. One is it's an incredibly wide data set. So you've got a lot of columns,

252
00:23:02,000 --> 00:23:08,000
right? But the other problem is that because this data is inherently non-stationary and now we're

253
00:23:08,000 --> 00:23:14,880
getting back to kind of like my main focus area, you know, a very small subset of that data if we're

254
00:23:14,880 --> 00:23:19,920
looking at, you know, a time horizon is actually relevant for predicting what's going to happen next.

255
00:23:20,480 --> 00:23:26,080
So your data kind of gets wider and wider and wider, but it's not necessarily getting deeper

256
00:23:26,080 --> 00:23:32,720
because, you know, the market fundamentally changed two years ago and, you know, including data from,

257
00:23:32,720 --> 00:23:38,560
you know, 2014 is not actually helping. It actually makes my models worse because it's learning

258
00:23:38,560 --> 00:23:44,400
from a regime which is no longer representative of the data which has been generated now in the

259
00:23:44,400 --> 00:23:52,080
process. So one of the reasons why ensembling is a good approach is because we can actually take

260
00:23:52,080 --> 00:23:57,920
that very wide data set and carve it up into different subsets and give it to smaller models and

261
00:23:57,920 --> 00:24:03,840
then kind of aggregate them and stack them in that way. It's very difficult to do feature engineering

262
00:24:03,840 --> 00:24:08,960
with a very, very big model when your data is more wide than it is deep. I guess that's kind of my

263
00:24:08,960 --> 00:24:15,200
answer to that question, but I accept the criticism. Maybe I just don't have enough data.

264
00:24:18,000 --> 00:24:24,800
One of the things that you talked about in your in Daba presentation and if you're making the

265
00:24:24,800 --> 00:24:29,680
slides available to anyone, they're really, really interesting and I'd encourage folks to

266
00:24:31,200 --> 00:24:34,640
take a look at them and we'd be happy to link to them or post them someplace.

267
00:24:34,640 --> 00:24:40,720
Yeah, I will definitely do that. They're just hyper animated in PowerPoint, so I've just got

268
00:24:40,720 --> 00:24:47,760
to turn them into videos and then I'll do that. Okay. So one of the things that you talked about in

269
00:24:47,760 --> 00:24:55,120
that deck was online learning which makes sense as a way to address this non-stationary

270
00:24:55,920 --> 00:25:00,320
nature of the signals that you're looking at. Can you talk a little bit about the way that you use

271
00:25:00,320 --> 00:25:06,160
online learning and what some of the challenges and discoveries you've made there are?

272
00:25:07,120 --> 00:25:14,240
Yeah. Okay. So there are a lot of intuitions and what I'm working on with my supervisor,

273
00:25:14,240 --> 00:25:23,440
I've actually decided to try postgraduate studies once again is to try and codify these ideas

274
00:25:23,440 --> 00:25:29,360
and publish some of them. But online learning is for anybody who's listening is not familiar.

275
00:25:29,360 --> 00:25:33,520
Really, when you have a machine learning model and it's trained on some data up to a current

276
00:25:33,520 --> 00:25:39,680
point in time and then what we do is we walk forward through time and we're using that model and

277
00:25:39,680 --> 00:25:44,720
we're just updating it on the most recent data which has happened. So there are a number of unique

278
00:25:44,720 --> 00:25:51,120
challenges that you face when you're doing online learning which is when you do kind of one pattern

279
00:25:51,120 --> 00:25:56,720
at a time or incremental learning which is when you have a model you move forward in steps and then

280
00:25:56,720 --> 00:26:01,920
you update on the most recent end patterns or n plus some window size. It's an incremental

281
00:26:01,920 --> 00:26:11,200
kind of batch learning and that's that by the time you have moved from let's say 2002 to 2007

282
00:26:11,200 --> 00:26:16,000
you might have actually forgotten a lot of the stuff that you knew in 2002. The model is actually

283
00:26:16,000 --> 00:26:22,560
forgotten a lot of information which means that when it enters into a regime such as the 2008

284
00:26:22,560 --> 00:26:30,000
financial crisis it actually doesn't know how to deal with that. Online learning is on the other

285
00:26:30,000 --> 00:26:36,800
hand a very good approach because we have these different regimes and if I had to just continuously

286
00:26:36,800 --> 00:26:43,760
include all of the available data and kind of like an expanding dataset fashion then my model would

287
00:26:43,760 --> 00:26:48,560
actually struggle to distinguish between the different regimes unless I had some sort of indicator

288
00:26:48,560 --> 00:26:54,480
of what regime the data was was being stumbled from. That's one of the challenges one of the other

289
00:26:54,480 --> 00:27:01,200
challenges which we were talking about in the office today actually is is for example regularization

290
00:27:01,840 --> 00:27:07,120
and this is I don't know if it's published in literature and I'm not sure if I'm 100% right in

291
00:27:07,120 --> 00:27:13,200
what I'm saying but different parts of the information which we're training our models on

292
00:27:13,200 --> 00:27:18,240
matter at different points in time. So if we have to take a neural network and we have to train

293
00:27:18,240 --> 00:27:24,880
it on let's say you know 2000 time series and the first 500 time series were not relevant from

294
00:27:24,880 --> 00:27:31,680
the period 2002 to 2004. Slowly but surely the regularization term in that neural network would

295
00:27:31,680 --> 00:27:39,440
push those weights very close to zero right but let's say from the period from 2004 to 2006

296
00:27:39,440 --> 00:27:45,040
those first 500 time series which weren't relevant in the past now all of a sudden become

297
00:27:45,040 --> 00:27:51,520
relevant in in the future then we have a challenge which is that you know we've actually pushed all

298
00:27:51,520 --> 00:27:58,320
of these weights very very close to zero and now we actually want to grow those weights again which

299
00:27:58,320 --> 00:28:03,600
which is just something which the models seem to struggle with through time and the other

300
00:28:03,600 --> 00:28:12,960
challenges that maybe the last 1500 time series are now not relevant from 2004 to 2006 and it pushes

301
00:28:12,960 --> 00:28:18,880
all of those weights down to zero. So a lot of the assumptions a lot of the ideas that we can apply

302
00:28:18,880 --> 00:28:24,400
successfully at a particular point in time. So if we just have to train a model on a data set

303
00:28:24,400 --> 00:28:30,320
deployed into production can cause problems when we're doing an online or incremental learning

304
00:28:31,040 --> 00:28:38,000
approach. One of them is regularization so you know that we tend to to bias ourselves towards

305
00:28:38,000 --> 00:28:45,040
you know dropout kind of approaches as opposed to L1 or L2 regularization but there are a lot of

306
00:28:45,040 --> 00:28:50,400
small issues like that. One of the topics that you mentioned early on but we haven't really

307
00:28:51,040 --> 00:28:58,000
died into yet is the types of models that you're using and that seems particularly relevant to

308
00:28:58,000 --> 00:29:05,760
this discussion. You know we think of kind of time series and models with memory and some of the

309
00:29:05,760 --> 00:29:10,560
comments you're making about you know models remembering and forgetting things I tend to think of

310
00:29:10,560 --> 00:29:17,760
recurrent networks but it sounds like you're shifting increasingly to using CNNs. Where does that

311
00:29:18,320 --> 00:29:27,440
memorization element come from in a CNN? Yeah so none of them are particularly good at remembering

312
00:29:28,320 --> 00:29:35,440
data which is very far in the past. So let's say we we have our data and it goes from 2002

313
00:29:35,440 --> 00:29:42,560
all the way through to 2008 and we're training on two years worth of data or four years worth

314
00:29:42,560 --> 00:29:49,440
of data at a time. By the time we reach 2008 we're only really updating our model on data from

315
00:29:49,440 --> 00:29:56,320
2004 you know we've forgotten 2001 2002 we've forgotten what a financial crisis looks like

316
00:29:56,320 --> 00:30:02,160
and whether you're using a convolutional neural network or an LSTM or a grue or you know just a

317
00:30:02,160 --> 00:30:07,200
feed for neural network you're going to run into similar problems because that regime is no longer

318
00:30:07,200 --> 00:30:14,000
in the data that you're training on. So you will forget it. So one of the techniques that I'm busy

319
00:30:14,000 --> 00:30:21,840
developing with my supervisor is kind of this idea of storing historical versions of your models

320
00:30:22,480 --> 00:30:29,440
in some sort of like explicit memory bank and then like let's say you find 2008 and you're like

321
00:30:29,440 --> 00:30:36,560
oh my model is struggling and we can identify that is struggling by looking at you know change point

322
00:30:36,560 --> 00:30:42,400
analysis and we can say all right well there's been a significant change in the data our historical

323
00:30:42,400 --> 00:30:50,160
data from 2007 and 2006 is no longer relevant what do we do either we re-initialize the model

324
00:30:50,160 --> 00:30:56,240
completely at that point in time and we just hope to hold that it learns or what we could do is we

325
00:30:56,240 --> 00:31:01,040
could actually go backwards in time and we could say well is there a point in time historically like

326
00:31:01,040 --> 00:31:08,160
from a long time ago where we learned something we learned to representation which is relevant for

327
00:31:08,160 --> 00:31:13,360
the regime that we're in now I hope that makes sense. No it makes a lot of sense and I've heard of folks

328
00:31:13,360 --> 00:31:22,480
doing similar things from kind of at a model management perspective so you've got some model

329
00:31:22,480 --> 00:31:28,720
in production and you've got kind of a constant evaluation system and it determines that

330
00:31:29,760 --> 00:31:36,240
the models performance has degraded as opposed to you know just immediately triggering a

331
00:31:36,240 --> 00:31:43,440
retrain you know what some folks have done at least is to look at the historical models that have

332
00:31:43,440 --> 00:31:49,520
been in production and kind of test the current regime against those models and see if you know

333
00:31:49,520 --> 00:31:54,880
as opposed to just us moving into new territory we've reverted into territory you know for which we

334
00:31:54,880 --> 00:32:01,680
previously had a model and you know switched to that one precisely so that's exactly kind of the

335
00:32:01,680 --> 00:32:08,080
approach that I've been working on lately although it sounds like in this case at somewhat a lower

336
00:32:08,080 --> 00:32:14,080
level in a sense of at the granularity of kind of the weights of the model or model you know sub

337
00:32:14,080 --> 00:32:19,760
components as opposed to kind of fully rolled models in production or something like that.

338
00:32:20,720 --> 00:32:25,200
Yeah so we're talking about like the weights right so what we would do is we would walk forward

339
00:32:25,200 --> 00:32:30,400
through time and we would actually drop these weights onto some sort of explicit memory so that we

340
00:32:30,400 --> 00:32:36,000
can load them at a later date. Now what I'm thinking and I haven't built anything like this yet but

341
00:32:36,000 --> 00:32:41,040
it's just like an idea that's in my head is really about whether we can model the transitions

342
00:32:41,040 --> 00:32:45,600
between those because you can kind of think of the states that we're dropping onto this memory

343
00:32:46,240 --> 00:32:52,960
as as being really states in like some sort of like Markov process and kind of like where I'm

344
00:32:52,960 --> 00:32:57,360
heading with my researchers whether we can model the transitions between these states and actually

345
00:32:57,360 --> 00:33:02,720
use it for simulation as well but that's just an idea. Interesting interesting.

346
00:33:02,720 --> 00:33:11,520
And so kind of going back to the application of convolutional networks in this case.

347
00:33:12,640 --> 00:33:23,200
So you're you're feeding the the CNN yeah let's call it a frame. Is that frame kind of a

348
00:33:23,200 --> 00:33:31,280
single point in time across a single time series or a single point of time across multiple time

349
00:33:31,280 --> 00:33:39,840
series or a historical frame that contains some time segments across you know one or more

350
00:33:39,840 --> 00:33:45,920
more time series like what what goes into the CNN? Oh yeah so we're we're generally using like

351
00:33:45,920 --> 00:33:52,800
dilated convolutional neural networks which means what? So ones which are preserving the

352
00:33:52,800 --> 00:33:59,040
the temporal structure of the data so we're never feeding in you know historical or future data

353
00:33:59,040 --> 00:34:03,600
into historical notes it's it's always kind of like wave net other than if you've seen the diagrams

354
00:34:03,600 --> 00:34:12,240
on on Google's block. But what's going in is really an image of where you've got your stocks

355
00:34:12,960 --> 00:34:19,120
on you know the on the columns and you've got a number of days at the bottom so we're feeding in

356
00:34:19,120 --> 00:34:27,840
kind of like a picture which is a whole bunch of time series put together you know column by the

357
00:34:27,840 --> 00:34:33,760
amount of time yeah I hope that makes sense. So if it's like 500 stocks and we're looking at you

358
00:34:33,760 --> 00:34:40,080
know 40 days worth of data at a frequency of one data point per day then it would be a 500 by 40

359
00:34:40,080 --> 00:34:49,200
image which is basically going into that convolutional neural network. And so the the difference between

360
00:34:49,200 --> 00:34:58,320
that kind of situation and something like an LSTM is that your memory if you will is kind of limited

361
00:34:58,320 --> 00:35:04,400
by this fixed window as opposed to some some thing that kind of sticks around a varying degrees

362
00:35:04,400 --> 00:35:08,480
indefinitely. Is that right? And what are the implications of that in the way you you model?

363
00:35:09,840 --> 00:35:16,160
Yeah so that is definitely true and I must say that I was surprised myself that the convolutional

364
00:35:16,160 --> 00:35:23,040
neural networks generally perform quite well. I wouldn't have expected that but we don't find a

365
00:35:23,040 --> 00:35:27,520
huge difference in the performance between our best convolutional neural networks and our best

366
00:35:27,520 --> 00:35:34,480
LSTM neural networks or between the best like grooves. Generally they're all performing quite

367
00:35:34,480 --> 00:35:39,440
well what makes a much bigger difference than architecture choices the choice of other hyperparameters

368
00:35:39,440 --> 00:35:45,920
like how much regularization are we are we adding what kind of dropout are we using how many layers

369
00:35:45,920 --> 00:35:51,040
are we using because the challenge with the data that we're looking at is because it's non-stationary

370
00:35:51,040 --> 00:35:56,160
we can't look at all of the historical data we can only look at the historical data that is relevant

371
00:35:56,160 --> 00:36:03,200
so sample from the same regime that we're in which means that if we have a very very over

372
00:36:03,200 --> 00:36:09,040
parameterized network like extremely large or extremely deep and then we shift into a new regime

373
00:36:09,040 --> 00:36:14,400
and we don't have that much data that model actually can't converge with the amount of data that we

374
00:36:14,400 --> 00:36:20,560
have so what matters far more than the choice of architecture in our situation has really been

375
00:36:20,560 --> 00:36:26,480
the hyperparameters that we're choosing for this I mean I'm pretty sure I could tune an LSTM

376
00:36:26,480 --> 00:36:33,760
to beat our best you know convolutional neural network or vice versa I'm not sure if that comes

377
00:36:33,760 --> 00:36:41,440
back to the free lunch theorem or what they generally work differently and because they work

378
00:36:41,440 --> 00:36:46,720
differently they can really come up with predictions which are hopefully uncarlated and then when

379
00:36:46,720 --> 00:36:52,720
combined in an ensemble or in another neural network downstream actually generate better and

380
00:36:52,720 --> 00:37:01,920
better predictions so beyond no free lunch implications would you say that it also has something to

381
00:37:01,920 --> 00:37:10,000
do with the fact that your regime durations are short enough to be kind of captured in your

382
00:37:10,000 --> 00:37:15,280
CNN window as opposed to you know something that might have longer maybe a longer tail.

383
00:37:16,240 --> 00:37:22,480
Yeah perhaps it's it's hard to say this gets back to the interpretability discussion

384
00:37:23,920 --> 00:37:33,760
maybe we should ask Sarah I'm not sure yeah so we we started down this path in talking about

385
00:37:33,760 --> 00:37:42,880
online learning and one of the the challenges that you raised in your slides was this issue of

386
00:37:42,880 --> 00:37:48,800
kind of weight transfer and the ability to kind of capture knowledge and project it forward to

387
00:37:50,080 --> 00:37:57,280
to the next time step how have you kind of dealt with that am I am I capturing that that issue

388
00:37:57,280 --> 00:38:03,120
correctly the weight transfer yeah so I think you know when we're doing online learning we don't

389
00:38:03,120 --> 00:38:08,080
want to continuously reset the model so we don't want to reinitialize it from scratch at every

390
00:38:08,080 --> 00:38:14,400
point in time especially if the model that we had at the previous point in time with the previous

391
00:38:14,400 --> 00:38:21,280
batch is actually relevant for for where we are now in time so in that situation we would like to

392
00:38:21,280 --> 00:38:26,720
transfer the weights from the previous model to the next model but that doesn't hold when we're

393
00:38:26,720 --> 00:38:33,360
talking about a change point so let's say you know something happens and there's a massive

394
00:38:33,360 --> 00:38:38,560
structural break in financial markets right so now the previous model we have is really learned

395
00:38:38,560 --> 00:38:43,440
to representation of the world which is which no longer exists we've we've moved on from that

396
00:38:44,240 --> 00:38:48,080
when we transfer our weights from the previous point in time to the next point in time

397
00:38:49,120 --> 00:38:54,080
we have a few options right so the first option is something we spoke about earlier which is

398
00:38:54,080 --> 00:38:59,680
really going backwards in time and trying to find some optimal weights which work and then

399
00:38:59,680 --> 00:39:05,040
transferring those into the model but another approach would simply be to reinitialize the weights

400
00:39:05,040 --> 00:39:15,040
completely which is extremely detrimental in the situation where you had a false positive so you

401
00:39:15,040 --> 00:39:20,560
made a prediction that there has been a change point that you've shifted from one regime into

402
00:39:20,560 --> 00:39:25,920
the next but you didn't now all of a sudden you've reinitialized your model you've forgotten

403
00:39:25,920 --> 00:39:33,440
everything that you learned previously and all for nothing so one of the things that we've we've

404
00:39:33,440 --> 00:39:38,080
also been playing around with this kind of like partial reinitialization which is this idea that

405
00:39:38,080 --> 00:39:43,760
we take our weights and we actually pass them through some sort of noising function so we add a

406
00:39:43,760 --> 00:39:48,160
little bit of of randomness to those weights at every single point in time to kind of keep them

407
00:39:48,160 --> 00:39:54,960
fresh keep them alive and also give the model the ability to remember some of what it's learned

408
00:39:54,960 --> 00:39:59,600
in the past but not all of it I like to call it kind of like optimal brain damage I know that

409
00:39:59,600 --> 00:40:04,240
there is actually something in machine learning called optimal brain damage and this is not it

410
00:40:05,440 --> 00:40:09,680
but I just I just love the name it's kind of cool it's like basically have a model you hit it

411
00:40:09,680 --> 00:40:13,840
on the head and and you hope that it learned something a little bit better than what I knew in the

412
00:40:13,840 --> 00:40:20,800
past all of these ideas are just different things that we've tried because I think that you know

413
00:40:20,800 --> 00:40:26,640
to understand where we're coming from is you know we you know deep learning is like the solution

414
00:40:26,640 --> 00:40:32,800
right so we we took these models and we applied it and we realized quite quickly that financial

415
00:40:32,800 --> 00:40:39,600
markets don't care how smart to model is they don't care how you know deep the maths is or or how

416
00:40:39,600 --> 00:40:46,720
you know optimal they work on on image net or on speech recognition problems the market is

417
00:40:46,720 --> 00:40:51,600
is this adversarial very complex system that's gotten on stationarity and the cross section in

418
00:40:51,600 --> 00:40:57,600
the time series it experiences these change points which can be partial affecting part of the

419
00:40:57,600 --> 00:41:03,760
model or full affecting you know the full model itself and they occur at kind of like a regular

420
00:41:03,760 --> 00:41:10,240
frequency you would be surprised at how often they occur so all of the ideas that we've tried and all

421
00:41:10,240 --> 00:41:15,280
my whole talk at the deep learning and Darva was really just a presentation of a whole bunch of

422
00:41:15,280 --> 00:41:19,680
tricks that we've tried and some of them have worked particularly well and some of them have not

423
00:41:19,680 --> 00:41:25,280
worked at all I tried to focus on the tricks that have worked as opposed to the tricks that happens

424
00:41:25,280 --> 00:41:29,840
but yeah that's really where all of this is coming from and I think like going forward I'd like

425
00:41:29,840 --> 00:41:34,560
to formalize you know some of those ideas and publish them yeah well one of those tricks that caught

426
00:41:34,560 --> 00:41:43,840
my eye was using reinforcement learning as a way to I guess control the way you ensemble these

427
00:41:43,840 --> 00:41:50,400
models or control the can you talk a little bit about that yeah sure so as I mentioned before what

428
00:41:50,400 --> 00:41:58,400
matters more than the choice of model is the choice of hyper parameters so for example if we're

429
00:41:58,400 --> 00:42:03,440
using early stopping which you know some people don't like I'm quite a fan of it and the main

430
00:42:03,440 --> 00:42:07,440
reason why is because I'm doing continuous learning through time I'm not training my model one

431
00:42:07,440 --> 00:42:13,120
some training at you know maybe three thousand times you know the there's a parameter there which

432
00:42:13,120 --> 00:42:17,680
is the patient so how many epochs are you willing to see without improvement before you just stop

433
00:42:17,680 --> 00:42:24,960
training and there is other parameters like your learning rate and both of these parameters your

434
00:42:24,960 --> 00:42:31,360
patients and your learning rate should probably change if you experience a different regime or if

435
00:42:31,360 --> 00:42:38,080
you enter into a different state if you think that you've moved from you know you know one regime

436
00:42:38,080 --> 00:42:43,920
into another regime it might actually make sense to increase your learning rate because you want to

437
00:42:43,920 --> 00:42:51,520
get into you want to push your weights to a place which matches the new regime quicker or maybe

438
00:42:51,520 --> 00:42:56,000
what you want to do is you want to train for longer because you want to give your model more of an

439
00:42:56,000 --> 00:43:03,280
opportunity to actually fit the new regime I hope that makes sense so you can kind of think of

440
00:43:03,280 --> 00:43:10,160
these as the actions that a reinforcement learning agent can take so it can change these hyperparameters

441
00:43:11,040 --> 00:43:18,080
in the models as we walk forward through time depending on the state which it observes so we're

442
00:43:18,080 --> 00:43:24,480
really using it as kind of like a meta optimization framework around each one of the individual agents

443
00:43:24,480 --> 00:43:31,360
in this in this massive ensemble and that's it's been something that's been quite fun and it works

444
00:43:31,360 --> 00:43:36,640
quite well it's not nearly as sophisticated as some of the reinforcement learning that's coming

445
00:43:36,640 --> 00:43:44,160
out these days but yeah it's it's worked quite well and that's also a better a cheaper approach

446
00:43:44,160 --> 00:43:48,960
than just creating a bigger and bigger ensemble right because what I could you know what you could

447
00:43:48,960 --> 00:43:55,920
argue is well why don't you just create more agents with the you know higher and lower learning rates

448
00:43:55,920 --> 00:44:01,520
or different patients and just grow that ensemble and make it bigger and bigger and the main reason

449
00:44:02,160 --> 00:44:06,560
why I would want to use reinforcement learning instead of just making this ensemble bigger and

450
00:44:06,560 --> 00:44:14,320
bigger is because I only have so many computers right and I have this this thing called a hard drive

451
00:44:14,320 --> 00:44:19,600
pulls up very very quickly when I'm training these models I mean we're producing about 200 gigs

452
00:44:19,600 --> 00:44:26,000
of data a week luckily we can delete a lot of it the next week and we can compress a lot of it

453
00:44:26,000 --> 00:44:31,680
but it's a it's a challenge to actually train massive ensembles like this what are the other

454
00:44:31,680 --> 00:44:39,760
interesting tricks that I came across in your in your slides relates to kind of the challenge of

455
00:44:39,760 --> 00:44:47,200
identifying fundamental patterns in time series data when you can have differences in frequency

456
00:44:47,200 --> 00:44:52,320
and magnitude but kind of the same underlying shape and it can be difficult for networks to

457
00:44:53,680 --> 00:44:58,000
to figure that out and I hadn't come across this notion of dynamic time working before

458
00:44:58,000 --> 00:45:05,520
yeah it's a very traditional technique I think it's been used since the 70s okay there was a paper

459
00:45:05,520 --> 00:45:14,400
which came out I think in 2017 actually proposing and a loss function for neural networks which

460
00:45:14,400 --> 00:45:20,720
incorporates the idea of dynamic time warping but essentially that I mean just to quickly explain

461
00:45:20,720 --> 00:45:27,520
the idea is that if we have two time series which are exactly identical in the sense that they have

462
00:45:27,520 --> 00:45:35,680
the same waveform and they just occur over different intervals like let's say we're looking at audio

463
00:45:35,680 --> 00:45:41,280
because it's mostly been used for speech recognition and I say the word apple and then I say it really

464
00:45:41,280 --> 00:45:50,960
slow apple the waveform of me saying apple is the same but the duration over which it occurred

465
00:45:50,960 --> 00:45:55,760
is different so it's a different time scale and one of the challenges with that is that if I

466
00:45:55,760 --> 00:46:02,080
to do a Euclidean distance between those two waveforms I would say that those two things were very

467
00:46:02,080 --> 00:46:08,240
different when in fact they're actually very similar they're the same thing they just were said

468
00:46:08,240 --> 00:46:14,720
in a different way so the idea behind dynamic time warping is really that you that you drop the one

469
00:46:15,600 --> 00:46:21,040
time series onto the one axis of a matrix and you drop the other time series onto the other

470
00:46:21,040 --> 00:46:29,360
axis of a matrix and then you use a procedure to actually draw a connection between between the

471
00:46:29,360 --> 00:46:34,080
top right hand corner and the bottom left hand corner now I can't remember you know for the life

472
00:46:34,080 --> 00:46:41,040
of me the exact details of the procedure but the idea is that it's a it's a more optimal measure

473
00:46:41,040 --> 00:46:46,480
of similarity between different time series and one of the things that we've been looking at for

474
00:46:46,480 --> 00:46:52,000
that is really in time series clustering so this is this idea of if we have a time series and

475
00:46:52,000 --> 00:46:57,920
we chunk it up into different sub sequences you know can we measure the similarity between those

476
00:46:57,920 --> 00:47:03,520
things and kind of group them together in times time series clustering is another approach to change

477
00:47:03,520 --> 00:47:10,080
point analysis that's one of the tricks which has not been very successful it's very computationally

478
00:47:10,080 --> 00:47:14,880
expensive and so I think that some of the papers which have come out on how to incorporate that

479
00:47:14,880 --> 00:47:19,680
into a loss function is maybe a more interesting approach have you implemented any of those

480
00:47:19,680 --> 00:47:26,240
papers is that what you're doing or are you doing it more procedurally like you mentioned yeah now

481
00:47:26,240 --> 00:47:31,040
we're getting into like some of the more secret source kind of stuff that that we have it with

482
00:47:31,040 --> 00:47:37,680
but yeah loss function engineering is important um I'll just leave it at that okay interesting

483
00:47:37,680 --> 00:47:43,040
interesting yeah when you when I saw that in the slide I envisioned something that you do kind

484
00:47:43,040 --> 00:47:48,080
of as a you know maybe even pre-processing or kind of windowing or something like that but

485
00:47:48,800 --> 00:47:52,720
the idea of doing this building this right into a loss function is kind of interesting

486
00:47:53,600 --> 00:47:59,520
yeah I mean there there are papers out there about it you know I recommend people going Google it

487
00:48:00,480 --> 00:48:06,080
we implemented that a while ago that was probably almost 18 months ago that we looked at that

488
00:48:06,080 --> 00:48:14,800
so we have touched on just a few of the tricks in this set of slides and we're starting to run out

489
00:48:14,800 --> 00:48:23,120
of time you also mention and discuss in the slides time series embeddings embeddings have come a

490
00:48:23,120 --> 00:48:30,560
very hot topic of late can you talk a little bit about how they apply to time series yeah that is

491
00:48:30,560 --> 00:48:40,080
a great question I'm a huge fan of of of order encoder is variational order encoders and also

492
00:48:40,080 --> 00:48:45,120
you know your your traditional kind of dimensionality reduction techniques multi-dimensional scaling

493
00:48:45,120 --> 00:48:54,960
PCA ICA FCA all of these techniques are quite useful at taking time series which have a lot of

494
00:48:54,960 --> 00:48:59,680
redundancy so there's a lot of of correlation and actually reducing it down into a lower

495
00:48:59,680 --> 00:49:06,640
dimensional space which really captures the statistical properties that are relevant so the

496
00:49:06,640 --> 00:49:12,800
reason why we do that is because there is a lot of correlation in financial markets and I think

497
00:49:12,800 --> 00:49:18,400
that I don't want to get into the debate of causation and the best way to to go about that because

498
00:49:18,400 --> 00:49:26,240
I'm not sure but I think that is difficult for the models to really assign importance to the

499
00:49:26,240 --> 00:49:32,960
the different inputs when all of them look very very similar so let's say you had stock A B C

500
00:49:32,960 --> 00:49:40,560
and D and we know for a fact that you know through domain knowledge or whatever that A is influencing

501
00:49:40,560 --> 00:49:51,120
D but maybe A is is also kind of influencing B C as well so maybe the model would look at the

502
00:49:51,120 --> 00:49:57,280
correlations and say well B is maybe the influencer of DNA to apply you know the weights in that way

503
00:49:58,320 --> 00:50:04,080
it's a bad explanation but it's very hard to tease out what is causation and what is correlation

504
00:50:04,080 --> 00:50:09,920
in the data and and financial market data is incredibly correlated so what we've been using the

505
00:50:09,920 --> 00:50:20,880
time series embeddings for is really to reduce this highly correlated space down into another

506
00:50:20,880 --> 00:50:27,440
subspace which has nicest statistical properties to predict and the nice thing about decoding or

507
00:50:27,440 --> 00:50:32,560
auto encoding is that you then have a network that if I had to generate a prediction from this

508
00:50:32,560 --> 00:50:38,880
latent representation I can actually then get it back into the original space so maybe I have like

509
00:50:38,880 --> 00:50:44,880
2000 time series which are all very similar maybe they're the Russell 2000 at this point in time

510
00:50:44,880 --> 00:50:49,440
they're all driven by similar market forces you know of Trump decides to tweet about something

511
00:50:49,440 --> 00:50:55,040
you know they're all going to move in in similar ways and what we can do is we can really squeeze

512
00:50:55,040 --> 00:51:01,280
that down into a latent representation which really captures the salient features which are useful

513
00:51:01,280 --> 00:51:05,920
for prediction but the other nice thing about those features is that they have good statistical

514
00:51:05,920 --> 00:51:11,440
properties especially if we're talking about a variational autoencoder and then what we do is we

515
00:51:11,440 --> 00:51:17,840
decode back into the original space after we've generated our predictions in the latent space

516
00:51:17,840 --> 00:51:22,000
I hope that that makes sense you can do it much more cheaply using principal components

517
00:51:22,560 --> 00:51:28,960
and those kind of more linear techniques but I'm just very partial towards

518
00:51:28,960 --> 00:51:35,200
ordering encoders and variational ordering encoders and so do you end up with essentially a time

519
00:51:35,200 --> 00:51:43,120
series of these or time series in this embedding space or are you is the embedding somehow

520
00:51:43,120 --> 00:51:49,520
are you using that more statically that question makes sense yeah so what you end up with

521
00:51:49,520 --> 00:51:56,640
is another time series but just of much lower dimension and preferably of better statistical

522
00:51:56,640 --> 00:52:02,880
quality which we can then generate our predictions in but it's it's still a time series

523
00:52:04,000 --> 00:52:09,120
and it kind of like preserves the the temporal ordering the ordering of the data

524
00:52:09,120 --> 00:52:15,840
so there are a few architectures that you can use for that and then you would you would use

525
00:52:15,840 --> 00:52:22,080
these embeddings as inputs to your other neural networks the the way you might with other

526
00:52:22,080 --> 00:52:28,880
kinds of embeddings yeah precisely and then it generates a prediction in that space and then

527
00:52:28,880 --> 00:52:34,400
we can actually decode back into the original space because there's no point in predicting let's

528
00:52:34,400 --> 00:52:41,520
say you know the first two principal components or predicting you know this five dimensional

529
00:52:41,520 --> 00:52:46,640
latent representation learned by an autoencoder because I don't know how to make decisions off of

530
00:52:46,640 --> 00:52:53,520
that so we need to be able to actually get it back into the original space so that we can make

531
00:52:53,520 --> 00:52:59,040
decisions about that because it's it's no good knowing you know what's going to happen to the

532
00:52:59,040 --> 00:53:05,280
first principal component or to the first dimension in this latent representation

533
00:53:06,080 --> 00:53:11,120
how does that help me make a constructed portfolio that somebody can invest in

534
00:53:12,480 --> 00:53:17,360
yeah but the main reason for doing that is really just that the statistical properties

535
00:53:17,360 --> 00:53:22,640
that you get out are better than the statistical properties you put in you don't have as many

536
00:53:22,640 --> 00:53:29,600
issues with cointegration and correlation between the time series it doesn't help with regime

537
00:53:29,600 --> 00:53:36,480
shifts which is the main focus of my research but it it has helped in improving the accuracy

538
00:53:36,480 --> 00:53:44,000
of the models very early on in our chat you mentioned that one of the techniques you use to

539
00:53:44,800 --> 00:53:49,600
kind of optimize these models and and kind of explore these tricks is ablation studies

540
00:53:49,600 --> 00:53:56,880
uh is that worth a quick comment before we wrap up? I'm a huge fan of ablation studies yeah but

541
00:53:56,880 --> 00:54:02,000
I mean basically the idea is that if you've architected your neural network in a particular way

542
00:54:02,000 --> 00:54:07,760
you can actually switch off parts of that neural network and measure the deterioration in your

543
00:54:07,760 --> 00:54:14,640
models performance in the same languages whatever loss function you specified and that's particularly

544
00:54:14,640 --> 00:54:20,560
useful when we go to investors or or somebody who would like to understand or have some confidence

545
00:54:20,560 --> 00:54:25,200
in the models that we're using we can say well listen we can't tell you exactly what the functional

546
00:54:25,200 --> 00:54:32,480
form or or the exact decision boundaries look like but what we can tell you is that you know these

547
00:54:32,480 --> 00:54:38,880
are the inputs which are contributing most time model at this point in time and uh and I stress

548
00:54:38,880 --> 00:54:44,640
that at this point in time because the ablation studies again are applied in this kind of like online

549
00:54:44,640 --> 00:54:52,080
learning setting and what's really interesting for me is just how how much they change I mean

550
00:54:52,080 --> 00:54:58,160
you have at certain points in time some variables are just absolutely you know if you took it out of

551
00:54:58,160 --> 00:55:04,480
your model your model would be useless um but then fast forward a year or two into the future I mean

552
00:55:04,480 --> 00:55:11,280
that variable is is it might as well be white noise uh going into the model and some other variable

553
00:55:11,280 --> 00:55:16,880
is now driving you know the performance of the model so what's really interesting to me is really

554
00:55:16,880 --> 00:55:23,440
the dynamics of the neural networks what inputs are mattering at what point in time and uh and then

555
00:55:23,440 --> 00:55:27,840
what I'm particularly interested in and something I haven't spent a lot of time on unfortunately

556
00:55:27,840 --> 00:55:35,600
is then seeing how those kind of measures of are variable importance uh match up to things like

557
00:55:35,600 --> 00:55:42,000
your your business cycle um you know if we're at the end of the bull run you know do we see that

558
00:55:42,000 --> 00:55:47,520
certain variables which we would expect to matter mattering more uh and and really seeing with

559
00:55:47,520 --> 00:55:53,760
or not we can actually test economic theory using neural networks um and and go beyond just

560
00:55:53,760 --> 00:55:59,920
fitting a function uh but actually trying to understand what that function does uh I hope

561
00:55:59,920 --> 00:56:04,800
that makes some sense but I'm a huge fan of that particular approach and I think you know anybody's

562
00:56:04,800 --> 00:56:10,880
interested in in interpretability should check out some of your previous podcasts but that's

563
00:56:10,880 --> 00:56:18,000
the approach that that we found the easiest to implement uh and the most useful from um from an

564
00:56:18,000 --> 00:56:23,920
insights perspective we're not necessarily using that uh for any decision making or to improve the

565
00:56:23,920 --> 00:56:31,360
model but simply to understand what is going on in that model um yeah awesome awesome well Stuart

566
00:56:31,360 --> 00:56:39,920
yeah any uh words of uh advice or pointers or kind of recommended resources for folks that are

567
00:56:39,920 --> 00:56:45,520
interested in the application of uh machine learning and deep learning to these types of time

568
00:56:45,520 --> 00:56:51,280
series whether in finance or any of the other uh domains that you rattled off earlier yeah I

569
00:56:51,280 --> 00:56:56,480
think that um you know there are a lot of really really cool applications in time series analysis

570
00:56:56,480 --> 00:57:03,040
and I think that there are very strong statistical motivations to spend some time looking at change

571
00:57:03,040 --> 00:57:08,400
point analysis and regimeships but also many applied motivations so I'd recommend students who

572
00:57:08,400 --> 00:57:13,840
are listening to this to really you know you know pick up the calls and and and do some research in

573
00:57:13,840 --> 00:57:21,440
it but as far as advice goes I'd say uh you know expect to be unexpected uh you know we expect to be

574
00:57:21,440 --> 00:57:27,840
surprised um because financial markets like I mentioned earlier are almost adversarial in the way

575
00:57:27,840 --> 00:57:35,120
that they behave uh and many of the things which we believe work uh in machine learning I'm not sure

576
00:57:35,120 --> 00:57:42,080
that they do work uh and you start to identify the cracks in the arguments when you apply these

577
00:57:42,080 --> 00:57:48,240
techniques to problems which they were not originally intended to be used for um so yeah I would

578
00:57:48,240 --> 00:57:54,720
just say you're going to be uh surprised at what you see and um and also always be prudent

579
00:57:55,760 --> 00:58:03,280
don't train a model and then see that it's getting 75% accuracy is predicting the S&P 500 and

580
00:58:03,280 --> 00:58:07,040
take all of your money and put it into that model because I guarantee you it is wrong

581
00:58:07,040 --> 00:58:14,720
uh with that level of accuracy uh if you're getting anything above you know 55% accuracy you probably

582
00:58:14,720 --> 00:58:22,880
have a bug um yeah that's just the reality of of the game um awesome yeah we'll start it's a

583
00:58:22,880 --> 00:58:30,640
sombring end sorry to not being like yeah it's great you should go and apply machine learning

584
00:58:30,640 --> 00:58:37,520
to finance is hard but it may save a listen or a ton of money all right I'll I'll be very happy

585
00:58:37,520 --> 00:58:44,080
if that is the case also interesting people who do this full time that's what I'm that's what I'm

586
00:58:44,080 --> 00:58:49,600
suggesting Stewart thanks so much for taking the time it was great chat yeah thank you very much

587
00:58:49,600 --> 00:59:00,400
and it was great to chat to you all right everyone that's our show for today for more information on

588
00:59:00,400 --> 00:59:08,720
Stewart or any of the topics covered in this show visit twimmel ai.com slash talk slash 203 if you're

589
00:59:08,720 --> 00:59:14,080
a fan of the show and you haven't already done so or you're a new listener and you like what you

590
00:59:14,080 --> 00:59:21,040
hear please visit your apple or google podcast app and leave us a five star rating in review your

591
00:59:21,040 --> 00:59:26,480
reviews help inspire us to create more and better content and they help new listeners find the

592
00:59:26,480 --> 00:59:45,680
show as always thanks so much for listening and catch you next time

