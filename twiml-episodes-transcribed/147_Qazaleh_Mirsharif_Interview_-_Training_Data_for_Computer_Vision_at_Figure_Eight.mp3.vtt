WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.760
I'm your host Sam Charrington.

00:31.760 --> 00:38.360
For today's show, the last in our train AI series, I'm joined by Kazale Mir Sharif, a machine

00:38.360 --> 00:42.400
learning scientist working on computer vision at Figure 8.

00:42.400 --> 00:47.040
Kazale and I caught up at the train AI conference to discuss a couple of the projects she's worked

00:47.040 --> 00:52.400
on in that field, namely her research into the classification of retinal images and her

00:52.400 --> 00:57.080
work on parking sign detection from Google Street View images.

00:57.080 --> 01:02.080
The former, which attempted to diagnose diseases like diabetic retinopathy using retinal

01:02.080 --> 01:08.200
scans, is similar to the work I spoke with Ryan Poplin about on Twimble Talk 122.

01:08.200 --> 01:13.120
In my conversation with Kazale, we focus on how she built her datasets for each of these

01:13.120 --> 01:18.320
projects and some of the key lessons she's learned along the way.

01:18.320 --> 01:22.160
If you've been following this series, you've heard me thank Figure 8 for their support

01:22.160 --> 01:23.320
and sponsorship.

01:23.320 --> 01:28.880
But have you taken a moment to check them out at figure-8.com yet?

01:28.880 --> 01:30.680
If not, you should.

01:30.680 --> 01:35.880
They provide the essential human in the loop AI platform for data science and machine learning

01:35.880 --> 01:41.440
teams, which trains tests and tunes machine learning models to make AI work in the real

01:41.440 --> 01:43.440
world.

01:43.440 --> 01:48.400
Finally, we're celebrating the second anniversary of the podcast this week.

01:48.400 --> 01:50.480
Have you found this podcast useful?

01:50.480 --> 01:53.640
If so, we want to hear how.

01:53.640 --> 02:00.680
Submit your written comments via our anniversary page at twimble.ai-2-A-V or leave us a

02:00.680 --> 02:12.600
voicemail at 636-735-3658.

02:12.600 --> 02:17.400
A quick note before we jump in, this episode was recorded live on-site at the Train AI

02:17.400 --> 02:21.400
conference, so there is some unavoidable background noise.

02:21.400 --> 02:32.480
All right, everyone, I'm here with Kazale Mir Sharif, Kazale is a machine learning scientist

02:32.480 --> 02:37.880
and computer vision at Figure 8, and we are here, as you can tell, perhaps by the background

02:37.880 --> 02:41.480
noise, at the Figure 8 Train AI conference in San Francisco.

02:41.480 --> 02:44.320
Kazale, welcome to this weekend machine learning and AI.

02:44.320 --> 02:45.320
Thank you.

02:45.320 --> 02:46.640
I'm excited about being here.

02:46.640 --> 02:52.160
I am as well, you know, it's the tradition on the podcast to have guests start out by introducing

02:52.160 --> 02:57.000
themselves and talking about how they got interested and started and machine learning

02:57.000 --> 02:58.000
in AI.

02:58.000 --> 02:59.000
What's your story?

02:59.000 --> 03:00.000
Sure.

03:00.000 --> 03:07.480
So, I'm currently a machine learning scientist with a focus in computer vision, Figure 8.

03:07.480 --> 03:15.640
So I completed my PhD in computer science last summer, and before that, I did my master

03:15.640 --> 03:19.040
in Iran in artificial intelligence.

03:19.040 --> 03:24.760
Then I applied for the PhD program in United States and came here for a school.

03:24.760 --> 03:30.040
So my focus was mainly computer vision, and I have work on different topics and application

03:30.040 --> 03:35.640
of computer vision in different areas, which I really enjoyed so far, and I decided that

03:35.640 --> 03:42.360
I want to do the same with joining the industry, so that's where I'm here now.

03:42.360 --> 03:44.760
What initially sparked your interest in computer vision?

03:44.760 --> 03:48.840
How did you choose that as a field to study in grad school?

03:48.840 --> 03:49.840
Yeah.

03:49.840 --> 03:57.520
When I was an undergrad student, I met this professor in one of the master program at University

03:57.520 --> 03:58.920
in my hometown.

03:58.920 --> 04:08.160
So he was doing research on a medical project and like a detection of diseases using the

04:08.160 --> 04:16.680
retinal images, I was really interested in that research and I talked to him and started

04:16.680 --> 04:18.680
to learn about that project.

04:18.680 --> 04:24.200
Then I applied for the master program and got into the artificial intelligence program

04:24.200 --> 04:29.560
in Iran, and I decided that after taking courses and image processing and computer vision,

04:29.560 --> 04:37.400
I said that this is an area that I really like to be in, and I like to learn how I can

04:37.400 --> 04:44.880
apply AI and computer vision to help in different topics, like medical project, which is really

04:44.880 --> 04:52.280
my interest, and I have experience in medical image processing.

04:52.280 --> 04:57.440
So that's where I decided to, I wanted to work on my thesis on this topic, and I got

04:57.440 --> 05:05.240
back to that professor, and I collaborated with him on this particular project and did

05:05.240 --> 05:10.000
my thesis on that, so that's how it all started.

05:10.000 --> 05:11.000
Okay.

05:11.000 --> 05:13.080
And so what was your thesis on?

05:13.080 --> 05:19.880
My thesis was on classification of retinal images, retinal vessels, and retinal images

05:19.880 --> 05:22.040
into arteries and veins.

05:22.040 --> 05:31.520
So this is still one of the latest topics in medical image processing that because retinal

05:31.520 --> 05:39.520
vessels, they can be indicative of eye diseases and prevent eye disease like diabetic retinopathy,

05:39.520 --> 05:42.440
which is the leading cause of blindness.

05:42.440 --> 05:50.200
So any alternation changes in the vessels could be early indicative of these such diseases.

05:50.200 --> 05:57.320
So we have this data collected from patients, these retinal images, and we wanted to process

05:57.320 --> 06:04.320
the data and classify the vessels into arteries and veins, and then measure the width of vessels

06:04.320 --> 06:11.160
and the vessels like arteries and veins and based on the ratio, and comparing it to a normal

06:11.160 --> 06:17.760
like this normal ratio in patients, to see if we can detect these such diseases that

06:17.760 --> 06:19.960
are earlier stages.

06:19.960 --> 06:23.640
So this was one of the topics that I work on, so I basically worked on classification

06:23.640 --> 06:30.160
of vessels into arteries and veins to help with doing some measurements, I can help predict

06:30.160 --> 06:37.600
such diseases at earlier stages and hopefully prevent blindness by detecting this at early

06:37.600 --> 06:38.600
stages.

06:38.600 --> 06:39.600
Interesting.

06:39.600 --> 06:44.520
So not too long ago I did an interview with Ryan Poplin, who's a research scientist at

06:44.520 --> 06:50.840
Google, who recently had some work published around starting from a assuming similar types

06:50.840 --> 07:00.560
of retinal scans, retinal funda scans, and from that determining all kinds of demographic

07:00.560 --> 07:08.840
types of information or indicators like whether it was a male or a female or their age,

07:08.840 --> 07:12.400
things like that from these retinal images, apparently we can learn a lot from the eyes

07:12.400 --> 07:14.840
in addition to just kind of preventing eye health.

07:14.840 --> 07:22.320
Yeah, I heard about that, actually, yeah, I saw that and I know like they're looking at

07:22.320 --> 07:29.520
the Western networks in images for females and for males and there are some differences

07:29.520 --> 07:37.640
that they can use in computer vision or artificial intelligence, they are now able to extract

07:37.640 --> 07:47.560
those patterns from the images and understand the small changes and find this network

07:47.560 --> 07:56.040
with a female or male, which is really hard to do by a human eye.

07:56.040 --> 08:01.200
So for your research, how did you get started?

08:01.200 --> 08:07.120
Did you inherit some training data set that was already labeled that you could just start

08:07.120 --> 08:09.360
working on or did you have to build that up from scratch?

08:09.360 --> 08:15.840
Yeah, that's a good question, without training data really, you kind of go anywhere, that's

08:15.840 --> 08:24.680
the first step to do a research is collecting data and ensuring that the data is clean

08:24.680 --> 08:31.440
and has the labels and then once you have enough data, you kind of start training the models

08:31.440 --> 08:34.680
and then improving the model by collecting more data.

08:34.680 --> 08:42.080
So a lot of time research is stuck at the first stage because that is like time consuming

08:42.080 --> 08:45.600
and hard process, especially for medical image annotations.

08:45.600 --> 08:53.440
It needs some expertise to label and annotate medical images, so we collect those images

08:53.440 --> 09:01.920
from a hospital and eye hospital in my hometown, we collaborated with a doctor and a team

09:01.920 --> 09:06.880
of doctors that were monitoring the research and then we used some online data sets that

09:06.880 --> 09:15.400
were publicly available with the labels, which have helped me to do the research, but

09:15.400 --> 09:20.840
not currently I'm a figure eight and providing training data is much easier and I think the

09:20.840 --> 09:24.640
research goes much faster from here.

09:24.640 --> 09:31.520
So when you had the collaborations with the hospital to provide access to training data,

09:31.520 --> 09:36.560
how did you, how did the project evolve, what were the key steps for you?

09:36.560 --> 09:44.280
The key step, like collecting the images or hard process, like medical images are collected

09:44.280 --> 09:52.680
in hospitals and using devices that can be expensive and like we learned from the doctors

09:52.680 --> 10:00.560
how to label that data and then they monitor the research, so make sure that we have the

10:00.560 --> 10:05.520
ground truth data that we can develop our model and validate our model.

10:05.520 --> 10:12.640
And then once we build that data, we started by looking through the computer vision methods

10:12.640 --> 10:19.200
that could be exploited to further this research.

10:19.200 --> 10:23.680
So I started reading papers, literature, people have done, the work that people already

10:23.680 --> 10:30.920
have done and decided that this, after like validating and testing different models, I decided

10:30.920 --> 10:37.080
that these particular kind of features are giving me good accuracy and this images for

10:37.080 --> 10:41.920
this type of task and then I write a paper on the list.

10:41.920 --> 10:46.960
What were some of the models that you explored?

10:46.960 --> 10:52.560
The model I used was some traditional model, at that time I was doing that research, it

10:52.560 --> 10:56.320
was like maybe six or seven years ago.

10:56.320 --> 11:03.880
So the advancement in deep learning, the super end, like at this stage.

11:03.880 --> 11:10.600
So I used some traditional models, computer models, some classifiers, like LDA, method

11:10.600 --> 11:17.320
to, like we did the feature extraction from the vessels, arteries and veins are different

11:17.320 --> 11:19.040
in color.

11:19.040 --> 11:28.040
And because like arteries carry the blood with oxygen, so they are brighter than veins,

11:28.040 --> 11:29.040
veins are darker.

11:29.040 --> 11:35.040
So from the pixel intensity, we could extract some features from the profile of vessels that

11:35.040 --> 11:38.920
could be differentiated vessels from each other.

11:38.920 --> 11:45.840
And then there is this specific structure in the vessel network that is like a tree that

11:45.840 --> 11:49.840
every vessel branches into two other vessels.

11:49.840 --> 11:55.760
And then arteries and veins never cross the same type of vessels, but arteries can cross

11:55.760 --> 11:56.760
veins.

11:56.760 --> 12:01.080
So if you have an intersection of arteries and veins, you know that one of them is artery

12:01.080 --> 12:03.040
and the other one should be vein.

12:03.040 --> 12:09.560
So using such information, we have improved the method and correct the vessels that were

12:09.560 --> 12:16.320
not returned as the correct labors by the model and we improved the model.

12:16.320 --> 12:17.320
Oh, interesting.

12:17.320 --> 12:23.240
And what were the key lessons learned that you discovered through this process that you

12:23.240 --> 12:26.600
have continued to use in your work today?

12:26.600 --> 12:33.320
Yeah, at that time, like I said, there were in that much training data, so we used some

12:33.320 --> 12:35.560
traditional models.

12:35.560 --> 12:42.400
And once you expand your research to like more images, you edit on more images, then you

12:42.400 --> 12:47.960
see that like you can explore, for example, at this time, there are deep learning models

12:47.960 --> 12:56.840
that they use a lot of training data and can do such, more accurately, classifying the

12:56.840 --> 12:57.840
vessels.

12:57.840 --> 13:02.920
And then if we can classify the vessels correctly, more accurately, then we can do this

13:02.920 --> 13:10.920
measurements, small measurements in the veins and arteries separately and help with, like

13:10.920 --> 13:14.400
help with the early detection of the diseases.

13:14.400 --> 13:22.360
So that I learned that at this time, like I really like to go back to that research again

13:22.360 --> 13:30.720
and provide more training data and like improve my model, use more advanced learning models

13:30.720 --> 13:36.320
to that, I know that deep mind, a Google deep mind are already doing a lot on this.

13:36.320 --> 13:44.760
And this is still an ongoing research because the diabetic retinopathy is prevalent.

13:44.760 --> 13:50.120
This is causing the leading cause of blindness in the world.

13:50.120 --> 13:54.880
And this research could really help to prevent that.

13:54.880 --> 14:01.720
I learned from the conversation with Ryan that there are a number of public datasets of

14:01.720 --> 14:02.720
retinal images.

14:02.720 --> 14:04.320
Did you use any of those?

14:04.320 --> 14:09.920
Yeah, there are coming like more and more data sets now people, I think it's really good

14:09.920 --> 14:17.520
to share data that research can, we can further research, but a lot of time we cannot really

14:17.520 --> 14:24.800
share medical data because it contains personal data and it's hard to call it that data.

14:24.800 --> 14:31.520
But yeah, that's good to know, like it's a research that I have done like a few years ago,

14:31.520 --> 14:36.880
but I really like to get back on that and see more of the data sets that are available

14:36.880 --> 14:37.880
now.

14:37.880 --> 14:46.160
And we can use our current AI human and the loop platform to get more labels and further

14:46.160 --> 14:47.960
the research.

14:47.960 --> 14:50.360
What are some of the things you're working on nowadays?

14:50.360 --> 14:56.160
Yeah, I'm currently working on this project, Parking Sign Detection, which I've heard

14:56.160 --> 14:57.160
about.

14:57.160 --> 15:00.160
I'm really excited about it.

15:00.160 --> 15:04.960
So I'm working on this with my co-workers, Dr. Huayner Schott.

15:04.960 --> 15:14.120
And it happens like when I came to San Francisco for work during a summer, I shipped my car

15:14.120 --> 15:20.520
from Houston, it was during the hurricane Harvey in Houston, so I really didn't have that

15:20.520 --> 15:27.080
time to explore how tough driving can be here in San Francisco.

15:27.080 --> 15:32.360
So I started driving and in the first couple of months I got a lot of parking tickets and

15:32.360 --> 15:40.120
I started using this app called the Spot Angel, Spot Angel, yes, like the app helps you

15:40.120 --> 15:47.680
based on the GPS information on your car, like you can't say what is your current location

15:47.680 --> 15:53.320
and then based on the parking regulation applied to curves, can send you a notification

15:53.320 --> 15:58.960
on when you need to move your car, for example, not to get a ticket for as retaining because

15:58.960 --> 16:01.400
that's something I forget a lot.

16:01.400 --> 16:07.480
And I remember about that app and I saw the app and that really saved me many times from

16:07.480 --> 16:08.480
getting tickets.

16:08.480 --> 16:14.320
So I said that I want to work on this research because this is something we started last

16:14.320 --> 16:20.720
year when he was doing an internship in Spot Angels by detecting parking signs, finding

16:20.720 --> 16:28.760
the location of parking signs, the actual location on the map, and like extracting a parking

16:28.760 --> 16:36.080
regulation that is assigned to each car, digitizing such information can help then with

16:36.080 --> 16:43.880
helping with drivers to have a less stressful experience driving as it is like San Francisco,

16:43.880 --> 16:44.880
including myself.

16:44.880 --> 16:49.680
So I'm kind of looking at myself with this research too.

16:49.680 --> 16:54.720
So if Spot Angel exists and does this, what are you trying to accomplish with the research

16:54.720 --> 16:56.120
that's different?

16:56.120 --> 17:02.840
Yeah, Spot Angels have been going through the street level images, Google street view

17:02.840 --> 17:09.840
or images that their user can collect, and then they have been going through these images

17:09.840 --> 17:13.360
manually and extracting the parking regulation.

17:13.360 --> 17:20.600
This is a really time-consuming process and it's costly.

17:20.600 --> 17:26.080
So computer vision models can help to extract a lot of those information in an automated

17:26.080 --> 17:27.400
way.

17:27.400 --> 17:35.160
So that's what I was doing there and now we are extending the research by using the

17:35.160 --> 17:41.720
planning models, by providing more training data on parking signs in San Francisco and

17:41.720 --> 17:49.720
then we're hoping to build more accurate models to then read the text on the parking

17:49.720 --> 17:54.040
sign and extract the parking rules.

17:54.040 --> 17:59.880
Have you found getting access to the Google Street View Data Difficult or is that freely

17:59.880 --> 18:01.880
available?

18:01.880 --> 18:11.080
Google Street View is a public data system that you can go and see the street view images

18:11.080 --> 18:20.560
but to download them and publicly release that then you need to talk with the providers

18:20.560 --> 18:21.800
of the data.

18:21.800 --> 18:33.000
So since we are doing this research and we contacted them and they are allowed us to release

18:33.000 --> 18:37.640
this data and use for this research because this would be a research that I'm sure a lot

18:37.640 --> 18:44.840
of people would benefit from because especially people who are driving in San Francisco or

18:44.840 --> 18:51.760
New York they know how hard it could be like we can have a safer environment by helping

18:51.760 --> 19:01.120
the drivers find parking spots or like understanding the regulations faster.

19:01.120 --> 19:06.640
So yeah we have that data now and then we are providing a training data on top of that

19:06.640 --> 19:10.480
building more accurate model.

19:10.480 --> 19:15.240
What kinds of techniques have you had to use to make this research work?

19:15.240 --> 19:20.080
Yeah like every other research the first step was training data.

19:20.080 --> 19:26.680
We actually announced that that data set that we used for the initial training of the

19:26.680 --> 19:36.200
model is released now publicly so everybody can run and build their models on that and

19:36.200 --> 19:38.160
test it.

19:38.160 --> 19:47.160
So we used our figurative AI human loop platform to label the data that we collected from Google

19:47.160 --> 19:54.720
Street View imagery and then we tested different learning models such as SSD and YOLO model

19:54.720 --> 20:01.040
and Sutter results and then using different round of active learnings we trained them

20:01.040 --> 20:08.000
retrained the model several times and fit new data to the model and improve the model.

20:08.000 --> 20:15.160
So currently like we can detect parking sign in the images with the like accuracy more

20:15.160 --> 20:20.720
than 90% and with very low false positive rate and we're hoping that we can extend this

20:20.720 --> 20:28.520
to other cities other dense areas such as Los Angeles, New York, San Diego to using transfer

20:28.520 --> 20:33.640
learning models to extend this model to those cities.

20:33.640 --> 20:40.640
And to wrap things up what advice would you have for someone who wants to take on a project

20:40.640 --> 20:42.160
like this?

20:42.160 --> 20:51.080
What I was, whenever I start a project like this like for me like when I was doing research

20:51.080 --> 20:57.120
in university a hard part was that like a lot of time you're spending a lot of time

20:57.120 --> 21:06.520
on creating and labeling the data like having access to data is not that easy and a lot

21:06.520 --> 21:11.000
of researchers are stuck in that step to build their models.

21:11.000 --> 21:17.120
I found that in the story like by providing more training data I can be able to like

21:17.120 --> 21:21.320
accurate more accurate models much faster.

21:21.320 --> 21:27.520
So when I learned about this like I could build models faster.

21:27.520 --> 21:35.120
So it's good that when people start a research they explore the resources that could be

21:35.120 --> 21:41.480
able to them to like the models that are already available like what data are publicly

21:41.480 --> 21:51.480
out of the bed or how they can access more data to like further research and that's good

21:51.480 --> 21:52.480
to know.

21:52.480 --> 21:53.480
Awesome, awesome.

21:53.480 --> 21:56.280
Well Casala thank you so much for taking the time to chat with us.

21:56.280 --> 21:57.800
Thank you for having me here.

21:57.800 --> 21:59.800
Thank you.

21:59.800 --> 22:05.480
Alright everyone that's our show for today.

22:05.480 --> 22:10.760
For more information on Casale or any of the topics covered in this episode head on over

22:10.760 --> 22:15.640
to twimlai.com slash talk slash 144.

22:15.640 --> 22:21.200
Thanks again to figure out for their sponsorship of this show and the train AI series.

22:21.200 --> 22:28.560
To follow along with the entire series visit twimlai.com slash train AI 2018.

22:28.560 --> 22:33.560
And show some love for this podcast second anniversary and share how it's been helpful

22:33.560 --> 22:39.040
to you over at twimlai.com slash 2av.

22:39.040 --> 22:41.760
Thanks so much for listening and catch you next time.

