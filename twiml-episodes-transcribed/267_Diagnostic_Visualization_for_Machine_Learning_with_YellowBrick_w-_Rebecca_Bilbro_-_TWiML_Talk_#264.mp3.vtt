WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.160
I'm your host Sam Charrington.

00:31.160 --> 00:35.280
This week on the podcast we're featuring a series of shows that highlight just a few of

00:35.280 --> 00:39.800
the great innovations and innovators at the intersection of three very important and

00:39.800 --> 00:46.280
familiar topics, data science, the Python programming language and open source software.

00:46.280 --> 00:49.960
To better understand our listeners' views on the importance of open source and the projects

00:49.960 --> 00:54.520
and players in this space, I'm conducting a survey, which I'd be very grateful if you

00:54.520 --> 00:57.040
took a moment to complete.

00:57.040 --> 01:01.760
To access the survey, visit Twimbleai.com slash Python survey.

01:01.760 --> 01:09.800
Please hit pause now and we'll wait for you to get back.

01:09.800 --> 01:15.880
That's twimbleai.com slash Python survey.

01:15.880 --> 01:19.840
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this

01:19.840 --> 01:22.200
series IBM.

01:22.200 --> 01:26.720
Speaking of open source, IBM has a long history of engaging in and supporting open source

01:26.720 --> 01:32.160
projects that are important to enterprise data science, projects like Hadoop, Spark,

01:32.160 --> 01:35.680
Jupiter and Cubeflow to name just a few.

01:35.680 --> 01:41.640
IBM also hosts the IBM data science community, which is a place for enterprise data scientists

01:41.640 --> 01:47.320
looking to learn, share and engage with their peers and industry renowned practitioners.

01:47.320 --> 01:52.440
Here you'll find informative tutorials and case studies, Q&As with leaders in the field

01:52.440 --> 01:57.800
and a lively forum covering a variety of topics of interest to both beginning and experience

01:57.800 --> 01:59.480
data scientists.

01:59.480 --> 02:05.160
Check out and join the IBM data science community by visiting IBM.com slash community slash

02:05.160 --> 02:06.160
data science.

02:06.160 --> 02:11.800
All right, everyone, I am on the line with Rebecca Billbro.

02:11.800 --> 02:17.120
Rebecca is the head of data science at ICX Media and co-creator of Yellowbrick.

02:17.120 --> 02:20.400
Rebecca, welcome to this weekend machine learning and AI.

02:20.400 --> 02:21.400
Thanks for having me, Sam.

02:21.400 --> 02:23.400
I'm excited to be here.

02:23.400 --> 02:26.960
I am excited to dive into our conversation as well.

02:26.960 --> 02:29.240
This should be an interesting one.

02:29.240 --> 02:36.760
Yellowbrick is a library focused on visualization and podcasts are not the best medium for conversations

02:36.760 --> 02:41.600
that rely heavily on visualization, but I think we will do okay.

02:41.600 --> 02:44.920
Why don't we get started by having you tell us a little bit about your background and

02:44.920 --> 02:51.000
how you got involved in data science and what led you to create Yellowbrick?

02:51.000 --> 02:54.200
Well, I am a practicing data scientist.

02:54.200 --> 03:00.520
I specialize in applied machine learning applications and in natural language processing

03:00.520 --> 03:02.960
usually specifically.

03:02.960 --> 03:08.480
Like you said, I currently lead the data science team at a company called ICX Media.

03:08.480 --> 03:13.960
So ICX Media is a social video intelligence company here in Washington, DC, which is

03:13.960 --> 03:16.000
where I live.

03:16.000 --> 03:21.760
And our focus is on using machine learning and natural language processing and data science

03:21.760 --> 03:26.240
to understand what different audiences are currently watching and what they're talking

03:26.240 --> 03:27.240
about.

03:27.240 --> 03:32.840
And we use that to sort of spot trends and let our clients know how basically how to make

03:32.840 --> 03:36.440
more of the kind of stuff that people are going to be excited to watch.

03:36.440 --> 03:41.000
Well, we don't need natural language processing and machine learning to know what people

03:41.000 --> 03:43.240
are watching and talking about today.

03:43.240 --> 03:45.480
Well, that's right.

03:45.480 --> 03:51.360
So yes, currently there's a lot of dialogue about end game and game of thrones, but we're

03:51.360 --> 03:56.520
not going to talk about it because some people maybe are not up to speed and we don't want

03:56.520 --> 04:01.000
any spoilers.

04:01.000 --> 04:03.880
But you asked sort of how I got into data science.

04:03.880 --> 04:07.560
I always love, you know, I love this part of the podcast.

04:07.560 --> 04:11.120
It's so fun to hear kind of how everybody got here.

04:11.120 --> 04:17.280
There's a million different stories of how people landed in this field.

04:17.280 --> 04:24.120
And so for me, I've always worked in the space between I would say like natural languages

04:24.120 --> 04:26.720
and formal languages.

04:26.720 --> 04:29.680
So as an undergrad, I was a double major.

04:29.680 --> 04:33.240
I'm double majored in mathematics and English.

04:33.240 --> 04:38.800
And then when I went to graduate school, I decided to study communication and visualization

04:38.800 --> 04:41.480
practices in engineering.

04:41.480 --> 04:48.600
And so after I finished my PhD, I came to DC and that was in 2011.

04:48.600 --> 04:54.720
And so this was, I joined the federal service as a presidential management fellow.

04:54.720 --> 05:02.080
And so this was during the Obama administration, they were putting a lot of kind of focus on,

05:02.080 --> 05:08.320
you know, analytics and kind of more data driven strategies inside the government.

05:08.320 --> 05:13.920
And this was just around the time that data science had started really happening.

05:13.920 --> 05:18.960
And so my job was basically analyzing like very, very messy data.

05:18.960 --> 05:21.720
But I didn't have access to a lot of tools.

05:21.720 --> 05:26.600
I mean, I think a lot of public servants can appreciate that, you know, the access to

05:26.600 --> 05:27.920
tools is somewhat limited.

05:27.920 --> 05:34.080
But in particular, I had a real hard time accessing tools to allow for like wrangling and modeling

05:34.080 --> 05:35.320
unstructured data.

05:35.320 --> 05:40.400
Okay. So I was working with a lot of like logs and reports and manifests and news stories

05:40.400 --> 05:42.800
and speeches.

05:42.800 --> 05:45.080
And there really just wasn't a lot of tooling out there.

05:45.080 --> 05:47.960
And so I basically, I taught myself Python.

05:47.960 --> 05:50.160
And I started building my own tools.

05:50.160 --> 05:53.120
And eventually that was my full time job.

05:53.120 --> 05:57.280
And I realized I could kind of take that and make that my full time job.

05:57.280 --> 06:01.440
And I started teaching data science and teaching machine learning because I realized there were

06:01.440 --> 06:05.640
a lot of other people who were kind of in the same boat and had a lot of the same kind

06:05.640 --> 06:08.200
of needs that I that I had encountered.

06:08.200 --> 06:15.280
Are you able to isolate what it is that prompted you to take the additional step to start building

06:15.280 --> 06:16.760
your own tools?

06:16.760 --> 06:20.720
Like I talked to a lot of people on the podcast that learn a programming language because

06:20.720 --> 06:22.680
they have some problem.

06:22.680 --> 06:26.600
But often, you know, you learn a programming language and the tool set that it offers

06:26.600 --> 06:31.840
you, but then building your own tools and then further kind of publishing those and supporting

06:31.840 --> 06:32.840
those for others.

06:32.840 --> 06:35.080
Like what is it that even told you that you could do that?

06:35.080 --> 06:37.680
Like what got you down that path?

06:37.680 --> 06:41.120
Oh, that's that's a great question.

06:41.120 --> 06:46.040
So I guess it would be, you know, one, one, one answer that I could give is that, you

06:46.040 --> 06:50.760
know, I was already in public service, you know, when I started getting started doing

06:50.760 --> 06:51.760
this.

06:51.760 --> 06:58.960
I already kind of had the sense that what I was doing was for more than just me, that

06:58.960 --> 07:04.200
it was for some greater good, but I think that really the longer that I've been working

07:04.200 --> 07:09.200
in open source software and people who are, like you said, kind of building these tools

07:09.200 --> 07:13.800
and putting them out there into the world, I would say that open source is a lot less like

07:13.800 --> 07:20.680
public service and more like activism or maybe for a lot of people, it's more like art,

07:20.680 --> 07:25.960
you know, that people are making the change that they want to see in the world.

07:25.960 --> 07:37.800
And it's the sort of wild and organic space and it's just fun, you know, it's sort of,

07:37.800 --> 07:38.800
it's fun.

07:38.800 --> 07:44.280
And once you sort of start doing it, it feels like, you know, it feels like the right

07:44.280 --> 07:46.400
way to do software.

07:46.400 --> 07:47.400
Awesome.

07:47.400 --> 07:49.440
So what is yellow brick?

07:49.440 --> 07:54.400
Maybe it might help if I sort of give like a little bit of a history of like why we made

07:54.400 --> 07:55.400
yellow brick.

07:55.400 --> 08:01.840
So it was, it really has to do with actually moving to trying to do data science and moving

08:01.840 --> 08:04.880
to try to teach data science.

08:04.880 --> 08:10.280
And when I started teaching, I realized like how poorly I was able to describe why I was

08:10.280 --> 08:14.160
making certain decisions, particularly around the machine learning workflow.

08:14.160 --> 08:18.160
And you know, it's like there's that, you know, Richard Feynman quote, like if you, if

08:18.160 --> 08:21.600
you can't explain it to your students or to your kid, you don't really understand it

08:21.600 --> 08:22.600
yet.

08:22.600 --> 08:24.120
So that was sort of like a moment for me.

08:24.120 --> 08:28.960
I realized I was, I was thinking about this, let I was talking about this problem a lot

08:28.960 --> 08:34.360
with another one of the machine learning faculty that I teach with, who's now my creative

08:34.360 --> 08:37.400
partner, Benjamin Bangfort.

08:37.400 --> 08:42.440
And essentially what we realized is like what we needed was a way to answer our students'

08:42.440 --> 08:45.840
questions, the way we would answer them for ourselves.

08:45.840 --> 08:50.600
And the way that we were answering them for ourselves was with plots, with plotting routines.

08:50.600 --> 08:55.200
And so, and when I say the questions, I mean like the questions the students ask are things

08:55.200 --> 08:58.520
like, you know, which features do I need to build this model?

08:58.520 --> 09:01.080
Which ones should I remove?

09:01.080 --> 09:03.080
You know, how do I know which model to pick?

09:03.080 --> 09:05.120
Why is that the right model?

09:05.120 --> 09:09.840
You know, why isn't my model doing a good job at modeling this data?

09:09.840 --> 09:13.920
And so that's really why we decided to start working on yellow brick and so Ben and

09:13.920 --> 09:20.720
I essentially we started this, this project, sort of in the beginning for our students.

09:20.720 --> 09:25.240
But now it's sort of expanded out into being this tool that we see as being useful, not

09:25.240 --> 09:30.840
just for students, but for engineers and for professional data scientists, who need

09:30.840 --> 09:38.000
a way to diagnose and visualize how data is being fit and transformed throughout the entire

09:38.000 --> 09:41.160
machine learning process and to end.

09:41.160 --> 09:45.800
So it's a pure Python project, it's fully open source, it's licensed under the Apache

09:45.800 --> 09:50.000
two license, you can find all of the source on GitHub.

09:50.000 --> 09:51.960
But yeah, it's a, I can say more about it.

09:51.960 --> 09:56.200
So it's, you know, it has an object oriented API.

09:56.200 --> 10:02.400
If you are familiar with the scikit learn API, so scikit learn is a very popular Python

10:02.400 --> 10:05.800
machine learning library that's open source.

10:05.800 --> 10:12.120
So the scikit learn API has this notion of estimators and transformers that can either

10:12.120 --> 10:15.520
change data or model data.

10:15.520 --> 10:20.480
And so our API sort of wraps that API with common visual diagnostic plotting routines

10:20.480 --> 10:24.000
that leverage pure Matplotlib.

10:24.000 --> 10:29.040
So that is kind of basically in a nutshell what yellow brick is.

10:29.040 --> 10:35.800
And so you reference that you you're using Matplotlib under the covers, what is yellow brick

10:35.800 --> 10:41.840
offering that you can't do with Matplotlib is it kind of like pre configured templates

10:41.840 --> 10:46.960
or way, you know, visualizations, you also mentioned in their diagnostic visualizations

10:46.960 --> 10:50.040
as opposed to other types of visualizations.

10:50.040 --> 10:52.360
Can you maybe elaborate on on all that?

10:52.360 --> 10:53.360
Sure.

10:53.360 --> 10:59.000
So to the question about what we offer on top of Matplotlib, it really is.

10:59.000 --> 11:08.480
About capturing those routines that are kind of become best practices, I think, for people

11:08.480 --> 11:13.160
who do machine learning kind of professionally or for their research.

11:13.160 --> 11:18.920
So you know that you like to use residuals plots to diagnose certain kinds of error when

11:18.920 --> 11:19.920
you're doing regressions.

11:19.920 --> 11:26.880
Or you know that you like to use confusion matrices to know which of your classes you're

11:26.880 --> 11:30.040
kind of failing to classify.

11:30.040 --> 11:37.600
And so really all yellow brick does is it sort of says, okay, well, in order to do a you

11:37.600 --> 11:43.720
know residuals plot, you need like these 50 or 60 lines of custom Matplotlib code or in

11:43.720 --> 11:47.760
order to do a confusion matrix, you need these 50 or 60 custom lines of Matplotlib code.

11:47.760 --> 11:52.440
And what we do is we wrap those into like an object.

11:52.440 --> 11:59.560
And so instead of doing, you know, 50 lines of custom code each time, you just import

11:59.560 --> 12:05.080
the object from, you know, you import the class from yellow brick and you create, you

12:05.080 --> 12:10.480
instantiate an object that does, you know, it executes that work for you inside an object

12:10.480 --> 12:11.480
oriented interface.

12:11.480 --> 12:16.960
And so it learns from the data and then it tells you kind of what it learned using a plot.

12:16.960 --> 12:18.760
Nice.

12:18.760 --> 12:24.800
And I would really encourage folks to take a look at the gallery that you've got in the

12:24.800 --> 12:25.800
documentation.

12:25.800 --> 12:32.320
There are tons of different types of charts and graphs here, everything from feature

12:32.320 --> 12:39.200
analysis to regression visual visualizers, classification visualizers, clustering, model

12:39.200 --> 12:45.960
selection, text modeling, decision boundaries, and target visualizers.

12:45.960 --> 12:53.680
Are there any particular of these tools that get the most use or kind of the bread and

12:53.680 --> 12:56.320
butter for yellow brick users?

12:56.320 --> 12:59.080
Oh, that's a fascinating question.

12:59.080 --> 13:03.280
Part of me kind of wishes I knew more about how people are using it.

13:03.280 --> 13:08.120
You know, one of the ways that we kind of try to keep a beat on this is just by like looking

13:08.120 --> 13:12.040
at the blog posts that people are publishing.

13:12.040 --> 13:14.640
You know, what people are using in those posts.

13:14.640 --> 13:21.160
It does seem like the regression and classification visualizers are incredibly popular.

13:21.160 --> 13:26.200
I think it's because, you know, those are two very common forms of, you know, supervised

13:26.200 --> 13:28.120
learning is very common.

13:28.120 --> 13:35.680
And so using things like classification heat maps or, you know, prediction error plots

13:35.680 --> 13:38.840
or residuals plots are very, very popular.

13:38.840 --> 13:41.080
We see those a lot in blog posts.

13:41.080 --> 13:46.440
There was just a blog post that went out where somebody was using our stochastic neighbor

13:46.440 --> 13:53.440
embedding plot, which is, which I really like to use when I'm doing a document modeling,

13:53.440 --> 13:54.440
right?

13:54.440 --> 13:58.440
So when I'm trying to understand a corpus and look at the different kinds of documents

13:58.440 --> 14:00.240
in that corpus.

14:00.240 --> 14:05.600
And so there's, you know, there seems like people are using kind of the whole gamut.

14:05.600 --> 14:09.720
And then the other way that we sort of listen is just by looking in the issues, you know,

14:09.720 --> 14:16.760
for future requests and bug reports to know where people are using different things and

14:16.760 --> 14:21.760
how they're using them and what they're encountering as they're trying to use these different

14:21.760 --> 14:24.040
kinds of visualizers.

14:24.040 --> 14:26.560
Are there any particular ones?

14:26.560 --> 14:33.000
Is there like a kind of lesser known hero among the different plots that you can do

14:33.000 --> 14:34.000
that?

14:34.000 --> 14:39.840
Yeah, I would have, you know, so I mentioned that I do a lot of NLP at my work and just kind

14:39.840 --> 14:43.800
of in general is sort of, that's my sweet spot in terms of what I like to do.

14:43.800 --> 14:50.400
And so as you might expect, there are, you know, a range of interesting text visualizers

14:50.400 --> 14:58.000
that you might be surprised to see just because they sort of, you know, they happen because

14:58.000 --> 15:04.440
Benjamin and I, my, at the other core maintainer, the other co-creator and I both do a lot

15:04.440 --> 15:06.040
of natural language processing.

15:06.040 --> 15:10.760
And so we've kind of evolved a lot of these sort of interesting different ways of thinking

15:10.760 --> 15:16.520
about text and how what might help people to model text more easily.

15:16.520 --> 15:20.320
And so stochastic neighbor embedding is a common one.

15:20.320 --> 15:24.360
So that's, you know, it's like TSNE, TSNE plots.

15:24.360 --> 15:29.240
So that's one that we have, we also have ones that people will probably recognize from

15:29.240 --> 15:31.840
another library called NLTK.

15:31.840 --> 15:37.800
So the natural language toolkit is kind of one of the first really robust Python libraries

15:37.800 --> 15:39.440
for doing natural language processing.

15:39.440 --> 15:46.480
So we have a visualizer that does like frequency distributions of words or tokens.

15:46.480 --> 15:50.400
But we also have a few things that you might not see in other libraries.

15:50.400 --> 15:54.840
So we have something called a dispersion plot.

15:54.840 --> 15:59.600
And so what a dispersion plot lets you do is see the incidence of a certain word across

15:59.600 --> 16:05.400
a corpus, which can be really useful for knowing it's sort of like a way of doing feature

16:05.400 --> 16:11.760
analysis for when your features are words and tokens.

16:11.760 --> 16:15.560
There's another one called a part of speech tag visualizer.

16:15.560 --> 16:19.160
This is when, so I'm sort of selfishly maybe talking about this one because it's one

16:19.160 --> 16:22.080
that I worked on recently.

16:22.080 --> 16:27.120
But I think a lot about, you know, what kind of document is this and how can I guess

16:27.120 --> 16:31.680
what kind of document this is without having to read every single document in my corpus.

16:31.680 --> 16:36.040
And it turns out that one really kind of easy way to guess what kind of kind of document

16:36.040 --> 16:39.920
it is is to just count the number of certain parts of speech.

16:39.920 --> 16:43.720
So like how many adjectives are there, how many nouns are there, you know, how many

16:43.720 --> 16:44.800
verbs are there.

16:44.800 --> 16:51.440
And so our part of speech tag visualizer is a way that you can like scan over a corpus

16:51.440 --> 16:55.080
and look at the proportion of different types of parts of speech.

16:55.080 --> 16:59.880
And then you can compare different slices of the corpus or different classes from the

16:59.880 --> 17:05.440
corpus and look at how much it varies, right, because our practices for using certain

17:05.440 --> 17:09.320
kinds of words are really, really different when we're talking about blogging and when

17:09.320 --> 17:13.120
we're talking about writing news articles when we're giving speeches.

17:13.120 --> 17:17.440
And so you can see really distinct variations in the ratios of different parts of speech.

17:17.440 --> 17:24.360
Can you give some examples of, you know, different ratios and what they signal in terms of

17:24.360 --> 17:25.680
the type of document?

17:25.680 --> 17:27.760
Oh, that's a good question.

17:27.760 --> 17:34.440
Yeah, I mean, I guess there's that kind of like, you know, when you're writing the news,

17:34.440 --> 17:38.160
you're supposed to not use any adjectives.

17:38.160 --> 17:40.440
And so that's kind of an easy example, right?

17:40.440 --> 17:45.960
So if you're scanning over the text and you do a part of speech tag visualization, let's

17:45.960 --> 17:54.840
see, let's say, and your proportion of adjectives is extremely low.

17:54.840 --> 18:00.960
There's a actually a pretty good chance that it's a news article.

18:00.960 --> 18:06.640
Whereas for instance, if you have, you know, a lot of adjectives, you know, you might

18:06.640 --> 18:14.200
be looking at something that's more akin to like poetry or music, like a song lyric,

18:14.200 --> 18:17.720
because adjectives are a lot more common.

18:17.720 --> 18:21.760
You know, so things kind of like that, I don't know if that helps to paint the picture

18:21.760 --> 18:22.760
a little bit better.

18:22.760 --> 18:23.760
It does help.

18:23.760 --> 18:28.160
I'm not able to easily come up with a lot of different classes that this would easily

18:28.160 --> 18:29.160
allow me.

18:29.160 --> 18:33.880
It seems like the kind of thing that, and maybe this is, this is like everything else,

18:33.880 --> 18:39.440
but you'd have to see a lot of examples to get an intuition for kind of how this might

18:39.440 --> 18:47.560
translate to a particular class of documents, and maybe it doesn't generalize all that

18:47.560 --> 18:48.560
well.

18:48.560 --> 18:49.560
That's fair.

18:49.560 --> 18:50.560
I think, you know, so you did.

18:50.560 --> 18:51.560
And that's not a critique.

18:51.560 --> 18:59.960
I'm just, it's very interesting that I'm intrigued that you can learn anything from

18:59.960 --> 19:01.760
looking at this consistently.

19:01.760 --> 19:02.760
Yeah.

19:02.760 --> 19:04.040
An example is a great one.

19:04.040 --> 19:09.240
I can definitely see that, and I'm just curious about others.

19:09.240 --> 19:13.760
It's a very interesting analysis to apply to a corpus.

19:13.760 --> 19:15.080
So here's another example.

19:15.080 --> 19:20.280
So I sort of think of another example, which is that a lot of times with students, one

19:20.280 --> 19:24.080
of the things that they'll try to do is they'll try to do a regression.

19:24.080 --> 19:28.880
You know, so let's say, you know, they have a bunch of data where each sample is a sample

19:28.880 --> 19:34.440
of concrete, and what we're trying to predict is how long the concrete is going to last

19:34.440 --> 19:39.960
given, you know, its components, you know, how much of cement isn't it, how much ash,

19:39.960 --> 19:41.680
how much water, et cetera, isn't it?

19:41.680 --> 19:47.680
So let's say they're trying to regress this data.

19:47.680 --> 19:52.560
And what they find is that their regression scores are really, really bad.

19:52.560 --> 19:58.400
And so one of the strategies that we suggest is like, well, why don't you bend the values

19:58.400 --> 20:01.960
into categories and then try to do classification?

20:01.960 --> 20:05.440
So let's say we'll, you know, we'll look at all of the samples that, you know, lasted

20:05.440 --> 20:10.200
between zero and five years, all the samples, the lasted between five and ten, but a lot

20:10.200 --> 20:16.720
of times when you pick, when you sort of manually pick those bins, you end up with really

20:16.720 --> 20:18.840
bad class and balance problems.

20:18.840 --> 20:23.640
Class and balance is like the surefire way to end up with, you know, terrible classifiers.

20:23.640 --> 20:28.320
And so we actually have a visualizer called balance binning.

20:28.320 --> 20:34.640
And what it does is it recommends bins when you are kind of trying to turn a regression

20:34.640 --> 20:39.400
problem that's, that's not successful into a successful classification problem.

20:39.400 --> 20:44.880
Is the idea that you would use the parts of speech, visualizer in conjunction with the

20:44.880 --> 20:50.160
binner to allow you to classify documents based on their parts of speech distribution?

20:50.160 --> 20:51.160
Yeah.

20:51.160 --> 20:54.160
So I guess I was, so for balance binning, I was just thinking about any regression

20:54.160 --> 20:58.800
problem that you would want to convert into a classification problem, but I can't imagine

20:58.800 --> 21:06.440
like a case where let's say we're looking at Yelp reviews.

21:06.440 --> 21:15.560
And let's say, you know, we were, you know, trying to regress a score, like an exact score

21:15.560 --> 21:17.520
for review is really hard.

21:17.520 --> 21:25.360
So trying to predict like this is exactly a 4.32 or this is like a 2.17 restaurant is

21:25.360 --> 21:26.560
really hard.

21:26.560 --> 21:32.080
But when you bin things by stars, like, you know, this is a one star, this is a two star,

21:32.080 --> 21:35.480
this is a three star, you can sort of get there a lot faster.

21:35.480 --> 21:40.600
So if you don't already have those bins, you could kind of use balance binning to help

21:40.600 --> 21:41.920
you decide on the bins.

21:41.920 --> 21:46.880
And then you could use something like part of speech tags to see if the parts of speech

21:46.880 --> 21:54.120
are a good way to differentiate positive reviews from negative reviews, let's say.

21:54.120 --> 22:03.480
Yeah, I can envision something like fake reviews, use more adjectives, for example, than

22:03.480 --> 22:04.960
real reviews or something like that.

22:04.960 --> 22:09.320
I think that's a very good hypothesis.

22:09.320 --> 22:10.720
Or more exclamation marks.

22:10.720 --> 22:12.760
Is exclamation mark a part of speech?

22:12.760 --> 22:13.760
Yes.

22:13.760 --> 22:17.640
Yeah, punctuation is included in that.

22:17.640 --> 22:21.800
This was originally built as a teaching tool.

22:21.800 --> 22:29.800
Can you maybe talk through some of the ways that you use it or have seen it use in real

22:29.800 --> 22:30.800
world?

22:30.800 --> 22:32.680
And we've kind of talked through a bunch of those, but I'm wondering if you can walk

22:32.680 --> 22:40.680
through and maybe more depth, kind of a workflow or use case that you're familiar with.

22:40.680 --> 22:41.680
Sure.

22:41.680 --> 22:48.440
I think that first, I would sort of say, I think a big part of how I think about the machine

22:48.440 --> 22:54.040
learning process is by using this expression, the model selection triple, which comes

22:54.040 --> 23:02.840
from a paper by Arun Kumar, which is actually a paper on sort of next generation databases

23:02.840 --> 23:05.440
that anticipate machine learning from the start.

23:05.440 --> 23:10.720
But he talks about the model selection triple as basically, this is what the machine learning

23:10.720 --> 23:12.520
workflow sort of boils down to.

23:12.520 --> 23:18.000
You do feature analysis, you do algorithm selection, and then you do hyperparameter tuning.

23:18.000 --> 23:22.320
And you might have to do a triple, like many triples, to get to the optimal model, but

23:22.320 --> 23:25.160
it generally follows that flow.

23:25.160 --> 23:28.400
And so what you've got then is a search problem, right?

23:28.400 --> 23:33.760
So you're searching, you've got this big grid of all of the possible features, all of

23:33.760 --> 23:39.000
the possible algorithms, all of the possible hyperparameters, and you're trying to find

23:39.000 --> 23:43.040
the best place in that search.

23:43.040 --> 23:50.080
And so what students generally do is they just like Google examples, and they kind of copy

23:50.080 --> 23:51.080
and paste.

23:51.080 --> 23:57.240
But frequently, I think people in a production context, right?

23:57.240 --> 24:04.200
So if you're in a company, you maybe are just buying machine learning as a service from

24:04.200 --> 24:05.200
somebody else, right?

24:05.200 --> 24:11.080
So you're paying some company to a black box kind of company, you give them your data,

24:11.080 --> 24:16.360
they kind of do something magical, they pass you back a model, and maybe you try to deploy

24:16.360 --> 24:17.360
that.

24:17.360 --> 24:23.480
But I think that what I am seeing is that people are starting to become dissatisfied with

24:23.480 --> 24:25.400
those black box models.

24:25.400 --> 24:29.520
They can't tune them, they don't understand them, they don't have a lot of insight into

24:29.520 --> 24:31.520
how they work.

24:31.520 --> 24:36.120
And there's ethical concerns with how is this made?

24:36.120 --> 24:41.040
How can we answer these questions about how this was made from our users, let's say?

24:41.040 --> 24:46.080
And so what I'm seeing is that people are starting to adopt yellow brick to do steering

24:46.080 --> 24:52.080
of the model selection triple, and kind of like taking that process back in-house so that

24:52.080 --> 25:00.800
they can do it in-house rather than kind of delegating that or paying for that service

25:00.800 --> 25:01.800
from somebody else.

25:01.800 --> 25:09.200
I'm curious, are you seeing folks that are adopting yellow brick as a path towards

25:09.200 --> 25:15.280
more explainable models or understanding the models better, you were just saying that?

25:15.280 --> 25:25.480
Yes, yes, so I think that it is a very powerful tool for model interpretation and explainability.

25:25.480 --> 25:32.840
It is not really designed for non-technical consumption, right?

25:32.840 --> 25:38.920
So it's not really designed to kind of produce plots that you could just drop into a business

25:38.920 --> 25:44.840
presentation and give to people who aren't already familiar with the process, but they

25:44.840 --> 25:54.840
are a very good way to simulate kind of more directed decision making for your data scientists

25:54.840 --> 26:00.920
so you can, you know, if your data scientists are using yellow brick to do feature analysis

26:00.920 --> 26:06.840
and feature engineering model selection and hyperparameter tuning, you know that they are

26:06.840 --> 26:09.840
doing this, you know, it's not a random walk, right?

26:09.840 --> 26:16.640
They're not just picking things randomly, they're doing a directed, focused search, they're

26:16.640 --> 26:18.480
not just using grid search, right?

26:18.480 --> 26:22.800
So they're thinking about the choices they're making and because they're thinking about

26:22.800 --> 26:28.440
the choices they're making and they have these artifacts of those choices, you can then

26:28.440 --> 26:33.240
trace back through those choices and say, this is why we ended up with these features

26:33.240 --> 26:37.880
and this is why we ended up with these models and you can kind of, you can tell that story

26:37.880 --> 26:43.400
with these plots that have been generated in the course of modeling, you know, these visual

26:43.400 --> 26:49.640
artifacts can be used, they can be used to explain how a model is working to the internal

26:49.640 --> 26:55.920
engineering team and so that level of explainability is oftentimes a blocker to deploying models,

26:55.920 --> 26:56.920
right?

26:56.920 --> 27:01.640
Because the data scientists know how to make models but they don't understand maybe very

27:01.640 --> 27:06.760
much about software engineering or deployment and the software engineers understand deployment

27:06.760 --> 27:14.360
but they don't understand kind of, you know, how to, for instance, how to test non-deterministic

27:14.360 --> 27:21.320
functions, they don't understand, you know, how to parameterize things so that they will

27:21.320 --> 27:28.040
work in a production context but yeah, you can use yellow brick for logging, for visual

27:28.040 --> 27:35.080
logging, once you've deployed a model, you can use yellow brick to generate plots on

27:35.080 --> 27:41.080
a regular basis to look for things like model decay, you know, if all of a sudden the

27:41.080 --> 27:46.760
classifier starts to perform poorly, that will show up in the plots and the engineers

27:46.760 --> 27:54.000
can know what to look for and that can be very powerful for explainability and interpretability.

27:54.000 --> 28:00.040
And so as visual logging in this use case where you're tracking model decay, is that are

28:00.040 --> 28:07.160
there specific yellow brick plots designed for that or are you just using yellow brick

28:07.160 --> 28:15.400
to display time series data? So I, that's a good question. I don't know of other people who

28:15.400 --> 28:20.280
are using this in context other than Benjamin and I both use this in, you know, in our jobs.

28:21.320 --> 28:25.960
And so essentially, you know, what you're just doing is you're, you're outputting, you know,

28:25.960 --> 28:32.120
just, just the way that you would output like an an F1 score on a regular basis, let's say,

28:32.120 --> 28:38.120
of your, your classifier and maybe you compute an F1 score on some regular, you know,

28:38.120 --> 28:44.840
routine basis and you look for a change. Well, instead of outputting just the F1 score,

28:44.840 --> 28:51.400
you could output a confusion matrix or a prediction error plot as another one of our classifier

28:51.400 --> 28:57.720
evaluations. You could output a prediction error plot, which would allow you to see not just

28:57.720 --> 29:04.120
the F1 score, which gives you the overall, you know, the harmonic mean of precision and recall,

29:04.120 --> 29:10.200
but actually be able to see specifically which classes where the decay is happening. And that

29:10.200 --> 29:16.120
is actually really helpful because you know, oh, it's this, you know, this specific class is where

29:16.120 --> 29:21.960
I'm struggling to classify, you know, it's not just kind of overall performance decay, it's decay

29:21.960 --> 29:28.120
in a specific way that I can use that information to make decisions about how to fix things. You

29:28.120 --> 29:36.760
mentioned in discussing that the third element of that model selection trip, triple the hyperparameter

29:36.760 --> 29:44.440
tuning that one of the advantages of using something like, well, this approach in general really

29:44.440 --> 29:52.440
is that you're taking a more principled approach to optimizing your hyperparameter is what are some

29:52.440 --> 29:59.800
of the plots that yellow brick offers that are there specific plots focused on hyperparameter

30:00.680 --> 30:06.120
tuning or is it just, you know, tracking model performance as you're working your way through

30:06.120 --> 30:11.320
the hyperparameter space? We do have a couple that are specifically for hyperparameter tuning,

30:11.320 --> 30:18.120
and so one that is very popular is the alpha selection plot. And so, you know, I'm sure you know

30:18.120 --> 30:26.360
this, but when you're doing regression, generally the problem is that you need to figure out how to

30:26.360 --> 30:33.880
execute enough smoothing so that your model will generalize to unseen data. But there is this

30:33.880 --> 30:40.360
sort of question about how much smoothing do I need? And generally the strategy is to grid search

30:40.360 --> 30:48.520
that, right? So you say, okay, well, smooth between alpha equals, you know, 0.001 all the way to alpha

30:48.520 --> 30:55.320
equals 10. That's a pretty big grid search, especially if you have a lot of data. That grid search

30:55.320 --> 31:02.440
could be running for quite a long time. And the worst part is that you don't know maybe if you've even

31:02.440 --> 31:09.160
picked the right range. And if you pick the wrong range to look inside, then you never get to the

31:09.160 --> 31:15.720
optimal answer. And so one of the things you can do with yellow brick is use the alpha selection,

31:16.600 --> 31:23.240
which allows you to visualize, you know, what the, let's say, regression score is at, you know,

31:23.800 --> 31:30.280
some number of cuts of alpha. And what you're hoping to see is some point where your score, you know,

31:30.280 --> 31:37.640
goes up, but starts to plateau. And if you don't see that, if you see kind of things hopping around,

31:37.640 --> 31:43.720
moving around a lot, where you can, you can sometimes even see if you've picked the wrong range

31:44.760 --> 31:50.600
to look inside. Another example that's sort of similar, but outside of regression is for when

31:50.600 --> 31:56.920
you're doing a centroidal clustering. So how do you pick the right K when you're doing K means?

31:57.800 --> 32:07.160
Well, you guess. There are, there are heuristics, let's say, for picking the best K. But

32:07.160 --> 32:13.720
generally, I mean, people are mostly just guessing, right? And so in the same way as with the

32:13.720 --> 32:21.560
smoothing coefficient, like you don't know how many K to pick. And so you can, you can grid search that,

32:21.560 --> 32:29.960
you can, you know, pick all of the K from four to 400. That is a very long running grid search.

32:29.960 --> 32:40.600
And so instead of kind of just trying to throw grid search at it, you can use the elbow plots.

32:40.600 --> 32:46.680
So we have silhouette scores and elbow plots, which are two visualizers in yellow brick that are

32:46.680 --> 32:55.000
very useful for centroidal clustering. And what a silhouette plot does is it computes the

32:55.000 --> 33:00.600
silhouette score for each of the clusters. So you know, you pick, okay, K will seven, let's say,

33:00.600 --> 33:06.680
will cluster to seven clusters. And then it will compute for you the silhouette score for

33:06.680 --> 33:11.400
each of the clusters, which essentially is just a way of saying, like, how dense is this cluster?

33:11.400 --> 33:18.200
And how far away is it from other clusters? And you want them to be as dense and as far away

33:18.200 --> 33:24.040
from others as possible. Because that's what, you know, that's what centroidal clustering is sort of

33:24.040 --> 33:32.280
for. And so you can use that to look for a single K. But because you can use that for a single K,

33:32.280 --> 33:41.160
you can actually scale that up so that when you are, when you place like a range of K to look in,

33:41.160 --> 33:47.080
you can compute the average silhouette score across all of the clusters at each number of clusters.

33:47.080 --> 33:54.760
And what you hope to see in your plot is an elbow. So some point where you reach a peak, a peak

33:54.760 --> 34:01.720
score. And if you get to like a peak score for a certain number of clusters where your average

34:01.720 --> 34:08.840
silhouette score for all the clusters is really high, you know, you've got to the best K. But

34:08.840 --> 34:14.360
you might also see something where there's no elbow and it's kind of jumping all around. And

34:14.360 --> 34:19.240
usually when I see that, it tells me that centroidal clustering is not the best algorithm. It's

34:19.240 --> 34:24.200
not the best model family. And I need to try to pick a different clustering methods, like maybe

34:24.200 --> 34:30.680
something that's hierarchical, let's say. So those are two hyperparameter tuning visualizers

34:30.680 --> 34:39.080
that I use regularly that I think are very popular. I'm curious about the community that uses yellow

34:39.080 --> 34:44.600
brick. First of all, how long has yellow brick been out and what have you seen over time?

34:45.960 --> 34:53.480
Well, so it's been open source for I think it's it's going to be three years this month or maybe

34:53.480 --> 35:03.080
next month. So in that time, we have accumulated almost, I think we have 75 contributors now.

35:03.080 --> 35:13.080
We have about 2,000 stars on GitHub. And, you know, it looks like we have about 5,000 downloads

35:13.080 --> 35:21.800
a month, unique downloads a month via the Python package index pi pi. So it looks like we are kind

35:21.800 --> 35:26.760
of we have reached a point where it is starting to become, you know, part of just the routine

35:26.760 --> 35:34.520
way that people do machine learning diagnostics. You know, it's hard to tell what the breakdown is

35:34.520 --> 35:41.160
between, you know, professional data scientists and students. But based on kind of what we're,

35:41.160 --> 35:45.400
you know, the feedback that we're getting and kind of what we're seeing, it looks like this is

35:45.400 --> 35:52.040
something that is being used across a lot of different communities. And we recently became a

35:52.040 --> 35:57.560
non-focus affiliated project. So I'm not sure if you're familiar with non-focus, it's, you know,

35:57.560 --> 36:04.120
it's a kind of open source umbrella organization that that helps promote open source projects. And

36:04.840 --> 36:11.000
particularly a lot of the ones that are very popular like, SciPy and Matplot Live and Scikit

36:11.000 --> 36:16.920
Learn. And so we recently became affiliated with non-focus, which is really nice. It helps us sort of

36:16.920 --> 36:22.600
be kind of more hooked into the conversation and they help us sort of spread the word.

36:23.720 --> 36:29.080
We work really hard to, you know, reach out to people. We give a lot of talks. You know,

36:29.080 --> 36:35.160
we go to PyCon and you know, PyData events all over the country. Even, you know, we've been to

36:35.160 --> 36:41.960
London. We've given talks in Argentina. So we really are doing our best to try to get this out

36:41.960 --> 36:48.680
to as broad an audience as we can. Because I think that that like the diversity of usage

36:48.680 --> 36:53.800
has really made yellow brick very interesting. So, you know, it's not just things for regression.

36:54.920 --> 36:59.720
It's not just things for classifications, not just things for text, you know, not really because

36:59.720 --> 37:05.160
we have users who are engaging in this conversation, who are doing all different kinds of things.

37:05.160 --> 37:08.120
And it's really made the library much richer over time.

37:08.120 --> 37:14.600
Oh, that's awesome. That's awesome. You know, one was the first feature that, you know,

37:14.600 --> 37:20.360
major feature or different type of graph that went into yellow brick that you or Benjamin

37:20.360 --> 37:25.480
didn't personally build. Oh, wow. That's been a long time.

37:26.680 --> 37:37.480
It has been a long time. You know, one of, I mentioned the dispersion plots that was added

37:37.480 --> 37:46.120
by one of our contributors, Larry. So Larry Gray is one of our core contributors and he does a

37:46.120 --> 37:52.600
lot of text analysis too. And so he added the dispersion plot. We have a joint plot visualizer,

37:52.600 --> 38:00.280
which is for doing pairwise feature analysis that was added by Prima, who is another one of our

38:00.280 --> 38:09.640
contributors. We have a very kind of special approach to doing unit tests. You can kind of imagine

38:09.640 --> 38:15.880
that when you're trying to do unit tests for visualizations, it gets a little bit hairy because

38:15.880 --> 38:20.600
you have to compare, you know, different plots and see if they're close enough.

38:21.640 --> 38:28.280
But one of our core contributors, Nathan, you know, essentially he found kind of a clever way

38:28.280 --> 38:34.840
of doing this that we've adopted. That was a really important contribution. We have two other

38:34.840 --> 38:41.640
contributors, Kristen and Adam, who have helped a lot with our documentation and getting it

38:41.640 --> 38:47.560
basically to a forum where, you know, people really enjoy reading the documentation and know where

38:47.560 --> 38:52.840
to find things because things are kind of easy, easily searchable and make sense.

38:52.840 --> 39:02.680
Can you give a quick summary of the approach to unit testing? Yes. So the trick is that you have

39:02.680 --> 39:12.920
to find a way to do visual comparisons. And so what we do is the library inside the test directory,

39:12.920 --> 39:20.040
we have baseline images. And every time we add a new visualizer or a change of visualizer,

39:20.040 --> 39:27.800
we update those baseline images. And those baseline images tell you, you know, the tests create

39:27.800 --> 39:36.040
images. And the baseline images are what we compare them to. And so when the tests run either on

39:36.040 --> 39:43.800
your machine or in CI on GitHub, they are building these plots. They're comparing them to the

39:43.800 --> 39:49.880
baseline images that are part of the repository and they're looking at the diffs between

39:49.880 --> 39:54.600
the two. So, you know, it's really just like black and white. Like if you actually look at the

39:54.600 --> 39:58.760
diffs, it's just black and white and you're hoping that, you know, you won't see any diff at all.

40:00.040 --> 40:06.440
But what actually happens in practice is that different operating systems have slight variations

40:06.440 --> 40:15.160
in how they execute fonts or colors, let's say, or, you know, translucency. And so you have to

40:15.160 --> 40:21.880
engineer, you have to use like Pytest parameterize or, you know, other tricks with Pytest

40:22.680 --> 40:27.480
to say, okay, well, if it's within this range of similarity, we'll call it the same close enough.

40:28.680 --> 40:34.920
Interesting. So yeah, it's a little bit tricky, but it's a pretty cool trick. Actually, if anybody out

40:34.920 --> 40:42.280
there is doing any kind of visual library stuff and having trouble with tests, I would strongly

40:42.280 --> 40:45.720
suggest looking at yellow brick. I don't really think that there's anything else out there.

40:47.560 --> 40:51.880
Like that. So it can be a really good model for people who are having similar kinds of challenges.

40:52.680 --> 40:58.920
Awesome. Awesome. Is that, have you put that visual compare into yellow brick or is it

41:00.360 --> 41:06.440
separate thing? So it's in there in the sense that it's part of our test suite. So like if you go

41:06.440 --> 41:12.120
into the test directory, yeah, it's so it's all in there. And we have actually a read me inside

41:12.120 --> 41:17.960
the test directory that even just kind of goes pretty in depth into like what visual comparison

41:17.960 --> 41:23.800
means and how it works. So if you're curious about that, you can dig in there. But yeah, so that is

41:23.800 --> 41:30.280
part of the source code. Well, that's awesome, Rebecca. Thank you so much for taking the time to

41:30.280 --> 41:35.800
chat with us about what you are up to and about yellow brick. Sounds like a really awesome

41:35.800 --> 41:43.000
library. Thanks so much. Thanks. And yeah, everybody keep an eye out version 1.0 is going to be coming

41:43.000 --> 41:48.920
out probably in the next month. So there's a lot of new stuff coming out with version 1.0. I think

41:48.920 --> 41:53.480
people are going to be pretty excited to see it. Oh, that's a huge milestone. Congratulations.

41:54.120 --> 41:57.480
Thank you very much. We are very excited. Awesome. Thank you.

41:57.480 --> 42:07.000
All right, everyone. That's our show for today. If you like what you've heard here, please do us a

42:07.000 --> 42:11.960
huge favor and tell your friends about the show. And if you haven't already hit that subscribe

42:11.960 --> 42:16.920
button yourself, make sure you do so you don't miss any of the great episodes we've got in store for

42:16.920 --> 42:30.360
you. As always, thanks so much for listening and catch you next time.

