Welcome to the Tumul AI Podcast.
Alright everyone, I am on the line with Jenshu Chen.
Jenshu is a scientist in the assay development group at the Allen Institute for Cell Science.
Jenshu, welcome to the Tumul AI Podcast.
Hi Sam.
Hey, it's great to speak to you and meet you and I'm looking forward to our conversation
to get started.
Why don't you tell us a little bit about your background and how you came to work at
the intersection of machine learning and biology?
Okay, I got my PhD in computer science in 2017 from University of Notre Dame and during
my PhD, I mainly conducted research in machine learning and image analysis and more importantly
and their applications in biology.
So before I came to Notre Dame, I have no clue what I'm going to do in the future and
later I just, all of a sudden, like we're randomly came into a very fascinating problem
in biology and trying to use computer science algorithm to solve it and that is the moment
I say, okay, oh my god, this is so fun and I'm very impactful.
So that's the place where I decide, okay, this is my career.
And then after I graduate in 2017, I came to Allen Institute for Cell Science where we
have very heavy like multi-team science where we have lots of biologists, computer scientists,
computational biologists and software engineer and everyone worked together to solve very
big fundamental cell biology problems and machine learning is playing a critical
role that attracts me here.
When you first got started, were you more coming from a biology perspective or more from
a computer science perspective?
I will say I came from more computer science perspective and I always use a very interesting
example.
I would say like before 2017, I know nothing about mitochondria.
I don't even know that particular English word and I don't even know what does it mean.
So that's really funny.
Right now I'm doing lots of segmentation analysis interpretation on mitochondria.
So that's really something really funny to me actually.
How did you approach that?
A lot of folks that are doing machine learning don't have deep experience in a domain
and have to get up to speed in the domain and the domain that you're getting up to speed
in is one that is particularly complex which has its own many open questions around the
biology.
How did you approach coming up to speed in it?
Yes, that's a fantastic question.
So actually I will probably cover more.
I cover a lot in that particular question in my GTC talk.
So the basic idea is that in computer science, we try to make design abstract problems.
Say we have a very practical problem like in biology, in medical domain or in whatever,
the problem in our real life is more complicated.
But computer scientists sometimes may want to start with a simplified version or abstract
version.
And starting from there, we develop methods, we develop all the kind of deep learning models
and artificial intelligence techniques put in there.
But later, I realized that how much we want to do this simplification or abstract, abstract
is actually a key when we want to put artificial intelligence into biology.
So sometimes the problem is oversimplified.
That's the problem.
That's the place I find myself really struggling with and having lost, I'm struggling a lot
when I do the transition from a computer scientist to like half and half.
So the strategy I took to do this transition is going back and forth between the knowledge
that the biologists and come back to my problem and come back for many times.
When I show my result to biologists, I try to understand how they see my result.
And sometimes, or most of the time, they see the result or they see the image or they
see the figures, the numbers completely different from what I saw it.
So they are viewing it from a very different aspect.
And by analyzing or by learning all such discrepancy between how I view the problem and
how they view the problem, that's the strategy I took to transition myself from computer
scientists to like half computer scientists, half biologists.
That's awesome.
Does any particular example of that kind of interaction come to mind where, you know,
you presented some results and they really saw them very differently from how you did
and you had to kind of close that gap?
Yes, absolutely.
There are tons of cool examples there.
A very simple example is when we know lots of like segmentation, say the basic idea
of segmentation is the computer raised in an image.
We want to find or we want to outline the structure in this image.
And then we can quantify the size of it or how long it is, something like that, right?
So the problem is pretty straightforward.
So for computer scientists, I will just say, hmm, I will do a like this and that method
and extract object.
I will, how good is my output or how good is my algorithm?
I will do that visually by a video assessment or some kind of comparison between my result
and what I see in the image.
So this is my understanding as a computer scientist.
But when I go to the biologist, they are seeing that in a different way.
So a key message is what you see in the image may not be the truth.
You have take the microscope effect into account when you shoot a light to light up a small
ball under the microscope, we are talking about spinning this confocal microscope, where
some particular cellular structure is labeled by some kind of fluorescent protein and then
it will be light up under the microscope and by the laser power.
But what you see in the image is actually you have to take into account the blurry
effect of the microscope because of all those optical details that I have no idea to understand
at the beginning.
So you see a ball with, for example, like 10 pixel wide, the actual object may be only
5 pixel wide, something like that.
What you see is not the truth.
That's what drives me crazy.
So I have to understand what the optical details of that particular microscope and all those
fundamental biology, say, for example, the things cannot be like having two branches.
It has to be a single filament, things like that.
I have to combine my computer knowledge and computer science, also my, the new things
I'm learning about optics and also the new things I'm learning about biology, I glued
these three piece together and reach a new, more accurate solution.
That's great.
And I think that example is one that you're going to come back to when I look through your
slides from GTC, a lot of the work that you're doing in the, the cell explorer toolkit,
which we'll be talking about, look like trying to kind of close this gap computationally
between what you might see in one image and what's really, you know, there, but you're
not seeing it due to some of these artifacts and effects that you're describing.
Exactly.
Yes.
Is that right?
Yes.
Right.
So maybe let's then use that as an opportunity to transition and talk a little bit about the
cell explorer toolkit and, you know, what is it really trying to do?
Oh, yeah, sure.
So at Alan Institute for Cell Science, we are trying to build this concept called Alan
Cell Explore Toolkit, which is a combination of cell creator, cell image generator, cell
image analyzer, cell image view analyzer and cell image simulator.
And in my GTC talk, I focused mostly on three parts, the generator analyzer and view
analyzer, where the computation or the GPU computing is heavily used there.
So the generator, if I want to describe it with one sentence is like how we get a image
to be analyzed.
So apparently we need to get image from microscope, but with the help of GPU computing, we can
generate more image from one single experiment.
So that's the key idea of generator.
The analyzer is, as the name indicates, it's just given this image how we analyze them.
And finally, realization is a big part of cell biology.
So and also for GPU computing is certainly a big, a play a critical role in the modern
realization tools and taking a step back, what's the goal for the Cell Explore Toolkit?
Is this an internal tool that's used at the Alan Center or is it designed to be used
by external folks and who are the main users, presumably biologists or is this something
different?
Yeah, great.
That's a perfect question.
So actually, at Alan Institute for Cell Science, we are doing open science and we want to
make the tool that we develop here to be usable for everyone outside this institute.
And the reason we believe our tool will be useful is that we are doing academic research
at the industrial scale.
By doing research at this level or at this scale, we may realize something we hope to build
some tools that will be more general or more stable or more user friendly for a broader
audience so that everyone can use it.
So that's the basic idea of why we want to build this toolkit.
And so you mentioned that you focused on these three areas, starting with the cell image
generator.
Tell us a little bit more about that and where machine learning comes into play.
Okay.
Cell image generator.
Of course, again, as I just mentioned, we need the microscope to get some image.
However, with the help of GPU computing, we can break some limit where the traditional
microscope has.
For example, in my talk, I showed an example where I took the image as a very low resolution,
which can help me get overview of the whole colony, which is like maybe 500 of cells in
the same image, but each of them has relatively not that detailed because of the resolution.
However, if we change some settings in the microscope, we switch to a different objective
or using a different modality, the same colony, we can get it in a very, very high resolution.
Say, for example, but we may sacrifice something because for example, we can only see maybe
tens cells in that particular image, but each cell has lots of details, which contains
lots of meaningful biological information in there.
But there's a trade-off.
So you cannot get both things at the same time.
So you want either get more cells, but less details, or get less cells, but more details.
There's always a trade-off.
So we are thinking that whether a computational model can help.
So that's what motivates us to design this, what we call the transfer function.
The basic idea is we build a deep learning model, train on some low-res image and high-res
image pair.
So whenever in the real image acquisition, we got a sequence of low quality or low-res
image, which give us like 500 cells in each time step.
And even though there's not that much detail.
We're after applying this fully trained transfer function model on this low resolution images.
We can, like, make it up and we can improve the quality or improve the resolution of
each image so that every single cell, we can put back all the details of each cell into
that low-res image.
In other words, we are seeing high resolution of each cell at a very large colony, or
it's a combination of larger field of view, or maybe seeing more cells in the image and
also seeing more details in the image.
So traditionally, yeah, in the microscope, we cannot do that.
But combining that with the computational model, we can achieve that to a very, very
high accuracy.
I'm envisioning something along the lines of applying style transfer or deoldify kind
of one of these image coloring techniques to the low-resolution images based on the
data from the high-resolution portion of those images.
Yes.
So the underlying techniques is pretty much the same as style transfer was the common models
people used in deep learning and computer science for style transfer for super resolution
and things like that.
But later, at the beginning, I thought, yeah, I have the exact same feeling.
I think if people can do that on natural scene image, I think it's just a piece of cake
and just put it on the microscope image.
There's no big deal.
But later, after doing more and more validation, I realized one important piece.
In the natural scene image, say you are trying to supervise your image of a cat, right?
So in your output, the supervised image, if the cat, the eyes of the cat, looks a little
bit larger or smaller, it doesn't really matter.
Maybe one pixel larger or two pixels smaller, you may not even notice that, right?
That's still a cat.
But for sale, that's different, that will be a totally different story.
I was going to ask that, I mean, a lot of, you know, when I think about those kind of
techniques, I think of generative models that, you know, in a lot of ways, they're just
making stuff up.
But if you're trying to use these images for scientific purposes, you actually want them
to reflect the reality of what's happening happening in that cell population, not just,
you know, a visual, high resolution thing that looks nice.
Yes.
Let's give us two things, first of all, it gives us some direction about how we can improve
our model on top of existing style transform models.
On the other hand, it gives us, it gives us some hint on how we should validate our result
and how much we should trust our generated image.
So why don't you go into those two in more detail?
Yes, for sure.
So for the first part, so after seeing this particular
thing about whether the prediction is to be larger or smaller and whether how much it
affects, it drives us going back to the model.
See, what can we do to, in the model, to make this prediction more accurate?
What's wrong with the model?
So then we realized that in our training data or the way we collect the pair of low-res
image and high-res image, it's very, very hard to get a fully aligned, low-res image
and low-res and high-res pair.
Think about how we capture the image in the real life.
Say in microscope, when we capture a 3D image, you can think of like, if you have a piece
of meat, you want to slice it, right?
So in the low-res image, the same piece of meat, you may slice it like at three different
positions.
But in a high-res image, you may slice that piece of meat like at ten different positions,
much denser, much smaller gap between each step.
So in that case, and also the position used to the slicing, or you take the slice, they
may not at the same place.
So anyway, anyhow, the low-res image and high-res image, they may miss a line in Z.
That is a fundamental issue.
Right.
So you can't necessarily directly match your target image or label image in the high-res
space to the low-res space because they're misaligned in terms of these slices.
Correct.
So we are trying two different ways.
First of all, we modified a model, which we embedded a spatial transformer network into
the model to learn such misalignment and try to correct it.
That's one thing we tried.
The other thing we tried is we tried to develop registration algorithm, which can make
this alignment to its best.
Maybe they are not perfect, but as much as we can using a traditional image registration
algorithm to the line image.
And we tried both, and each has pros and cons, but the good news is, both methods are
improving the quality of the prediction by a lot.
I'm not sure if the image that I'm thinking of is one that corresponds to the point you're
making.
There's an initial reaction I had to that, that you've got these additional slices, and
you're not able to align your slices, and so the approach is maybe one of interpolation,
but there's this one image in there that's showing that actually these different slices
have totally different things in them.
And so you can't necessarily statistical interpolation of some sort probably isn't going to work
very well.
So in terms of the slice, so I'm not sure whether I...
I explained that clearly, but if I would try to put it in a different way.
So if you think about the low-rise image, have three different slices on the same mid.
On the high-rise image, you have ten different slices on the same piece of mid.
So the first step, we need to absent or do some interpolation on low-rise image so that
it will have ten slices.
So now, on both images, we have ten slices, right?
So now, the number of slices match, but the first slice in the up-sample or the interpolated
version, the first slice of that may correspond to the second slice of the true, like, high-rise
slice.
And that's where you're doing your registration.
Yes, that's what we are doing either we want to shift them a little bit and at the beginning
or we can also, we also develop a model that can handle that inside the model.
Okay.
I think the image that I was thinking of is referring to something else.
It was talking about predicted channels, and it has, like, each of the channels has very
different things on it.
And I was interpreting that as slices, but that's probably something different.
Oh, by slice, I mean, the slice along C direction.
Yeah, so you're doing the registration, which allows you to then use your up-sampled image
as a label, is that essentially right?
So use my up-sampled image as the input, use my tool, yeah, use the true, high-rise
image as my target.
Cool.
So that was the first part, correct?
And then the second piece is what?
The second part of the, actually, there's a second part of the image generator, which
we call the label-free method.
So what we have been talking about is from low-rise to high-rise, but there's another type
of generation.
So think about, now we come to multi-channel, is what you are teaching a moment ago.
So yeah, so usually when we do, we collect cell images, we have multiple channels, say,
we should light at different, the light spectrum, we have different types of light.
And then we can have different channels, as they have different dye in there.
So usually we will have an image with some dye to light up the nucleus and some kind of
protein to light up the one of the intercellular structure and another type of dye to light
up the cell boundary.
And there's another type, another special channel, which is from the transmitted light, which
gives us what we call the bright field image.
The bright field image is different from what we talk about in the other three, which
requires some fluorescent tagging, some fluorescent imaging.
The bright field image is just transmitted light, you just shoot the light there, you see
the image without any harmful or photo, any dye that may damage your cell.
So the light will not damage your cell.
So in that sense, this is a general description of what we have in general.
So I mentioned that there's a specific channel tagging only one intercellular structures,
because that's a special cell line, which we did lots of hope, bunch of gene editing,
so that that particular cell can have that property where that particular structure
can be light up with the type of experiment.
Each experiment can only light up one intercellular structures.
And sometimes with very, very special gene editing techniques, we may light up two or
at most three, going beyond three will be super, super hard.
So basically, we can see three structures at the time, at most, in the real experiment.
However, we always have the bright field as a reference, the bright field image, I mean
the image we collect by shooting the transmitted light.
So that is something harmless and can be cheaply acquired.
So what we are thinking is, whether we can predict the special intercellular structure from
the bright field image, right?
So if we can do that, but given any single bright field image, we can predict 20 different
structures at the same time for the same style.
And an non-destructive way?
Yes, that is the most important part, or it's the brand new way of designing your essay.
Or your experimental essay.
So before, we can only tack one structure at a time.
And you cannot see how these 20 structure leads together in the same style.
You will never see that, because every time you can only see one structure or two, right?
So now with this particular technique, with any single bright field image, we can predict
a lot of different structures for the same style, that is a fundamental improvement in imaging
or how we generate images.
So that will allow us to study the cell as a whole, as like with all the different parts
playing together, not individually.
And so in both of these cases, kind of going back to the earlier comments around the
kind of the generative nature of some of these techniques, how do you measure performance
and compare these cells that you're generating to actual cells in a way that ensures that
they are high fidelity to what's actually there?
Yeah, that's a good question.
So the strategy we are taking is applications for the scientific validation.
Say you are carrying, if you are, for example, when we talk about the label free prediction,
say we are predicting 10 different structures from the same bright field image.
And how good they are, how much can we trust them?
And if we are talking about some application, we require the absolute accuracy.
Say whether the pixel-wise accuracy is exactly the same matching the choose, then probably
in the 10 structure, nine of them is not that trustable in that sense.
Because the pixel-wise accuracy may not be that high for some structures.
So maybe one structure will have very high pixel-wise accuracy.
In other applications, where we care about how different parts correlate it to each other
like in the space.
We don't actually need that much pixel-wise accuracy.
What we care more about is whether they are predicting the overall position of that particular
structure in the correct place.
So with that in place, we can study how different structures functioning together or whether
there's any correlation with the structures.
So anyway, that is what the strategy we call, like we care more is whether this result
is suitable for that application.
So this is our strategy.
So kind of taking an application-by-application approach to evaluating performance, some
of which are concerned about absolute characteristics like size and others more concerned with relative
characteristics like position.
Exactly.
That is the image generator component.
The next one you mentioned is the cell image analyzer.
What is that piece trying to do and where, you know, how have you used ML in that component?
Yeah.
For image analyzer, in lots of cases, when we've got an image, where we want to analyze
it, the very first step is to do a segmentation.
We just like binarize the image.
Generate a binary mask indicates where are the structures?
Where are the interstellar structures?
So that's the very first step we call a segmentation.
So doing segmentation in microscopy image or microscopy image of cells, it's actually
different from what we are doing like the semantic segmentation or instant segmentation
or all sorts of segmentation in computer vision in natural scene image.
That could be very different.
So that difference comes from a couple of different sources.
Let's start with a very simple example.
Most of people may know that in deep learning, people can just manually draw the boundary
of that object, so no matter if it's a people or cat or dog, we just manually draw the
boundary of that and we draw that boundary for, say, 500 images or 5,000 images and there
may not be that hard.
It may be take some time, but it's not that hard.
That's possible.
Right?
So we have to have this 5,000 image or even more, we just throw it into a unit or some
kind of rest net or some deep neural network and it will predict the mask for us.
That's what a lot of existing work are doing.
But in microscopy images, especially for cellular structures, if you think about how complicated
the topology or morphology of the structure could be, then you immediately appreciate that
this is not possible to do this kind of manual annotation.
So if you think about, I think when you think about, you know, bounding boxes and how tightly
packed these cells are in some cases, you know, it's clear that that won't work.
But then you have pixel masks that are manually applied and other techniques that could conceivably
be used.
For the pixel mask, even that, that could be hard if you think about it 3D.
By the way, I forgot to mention that everything we are doing is 3D.
Okay.
Yeah.
If you are thinking about a ball in 3D and if you ask me to draw the mask pixel wise, for
that particular ball, yes, 3D, I think I can do a pretty good job.
But if you give me a tree or a more complicated structure where different parts interacting
with different other parts, something like that, or a ball of yarn, you think about how
they tangle up, right?
And I want to you to draw the outline of each single piece of yarn, go through the ball.
Yeah.
It gets a lot harder in 3D.
Yeah.
It's not possible.
Right?
So the strategy we are approaching that is what we called, which we implemented in what
we call the segmenter is we use some classic method there, like more than 20 years of study
of classic image processing techniques that are still very useful and they can give us
a very, very good result to start.
So we make a collection of a classic algorithm to give us some quick start.
So based on that, for example, if you think about the ball of yarn, we design some classic
algorithm can give us a rough segmentation of each piece of yarn, mostly like not very
accurate, but give us a lot of good segmentation here and there.
So instead of manually draw where each piece of yarn is, we use a different way to do the
annotation.
We draw a bounding box here, this area, what the classic algorithm is doing is good.
Let's confirm that.
And then for that particular area, what the classic algorithm is doing is bad.
So throw that away.
So by doing this kind of curation, we say here are good, here are bad, here are good,
and there are bad, when we may have a small amount of good segmentation and use that as
our initial training data for our, to train our deep learning model.
And then you have this model one, you apply it on your data, and then the result will
be better.
Now let's do again, here is doing good job, here is doing not that good, throw it away,
here we are doing a good job, and there we are doing a good job.
Okay, now we have our second round of iteration, we collect a larger set of good segmentation.
Then we use that to train our second model.
Now we keep this iteration going, going like iteration by iteration.
So every iteration, you are model is improving a little bit, more or less.
So we hope to improve our model throughout these different iterations.
And at the end, finally, we will achieve a model that we can never achieve by collecting
a manual annotation.
What I'm hearing is that I know it's not quite the same, but it's making me think a little
bit of like an active learning type of a scenario where you can't be.
You're trying to feed back to the system, the training data that is struggled with the
most.
And in this case, you're using human annotators, but not to identify the actual features
in your image, but rather to identify where your segmentation algorithm didn't do a good
job.
Correct.
So that is a big part of the segmenter, there's another complementary part where you think
about if you have two types of cells in your image, say, mytotic cell and interface
cell.
They are in different cell cycle and they may show up as different morphology.
So one single classic method will not be able to do a good job there, right?
So then we can use two.
We use two methods, each will give us a different segmentation version.
Now we have the same image, we have segmentation version one, we have segmentation version
two.
Each version has its own advantage and disadvantage advantage.
Okay.
Now we just use our human annotator as the follows, say we circle out in cell one, in
this cell, we make a circle.
Okay.
In this cell, we use segmentation version one.
In that cell, we circle out, we use segmentation version two.
In that cell, we use segmentation version one.
In that cell, we use segmentation version two.
So by doing this kind of circling, we are creating emerged ground shoes that can help
us in the model.
At the end, the model is able to segment both type of cells correctly, you know, very single
model or multiple models, single model to deal with two different type of cells.
And so this is again resting on the segment or the part of the image analyzer.
So a little bit like one minute or two minutes down the segment or direction is sometimes
is, we may think of segment segmentation is most of the analysis is about segmentation.
Sometimes it's not.
Sometimes we can do something on the original image.
So that's another part of the analyzer, this is what we call the integrity cell, where
we build a auto encoder on the original image, where we try to learn the underlying correlation
between each part of the cells and how they function as the whole.
And that's another part of the analyzer.
You mentioned the auto encoder part, elaborate on what exactly that's doing.
Okay, sure.
So I showed an example in my slides where I see cell boundaries like this, I see DNA patterns
like this, and then can we use these two pieces of information, just purely taking an image
to predict where the mitochondria should live.
So there might be a correlation between the position of the mitochondria condition on
the shape and shape of the cell and DNA.
So this is how the auto encoder is trying to learn.
So basically I take this, I'm trying to reconstruct it, and if I'm able to reconstruct it, and
I learn throughout this process, I learn some correlation between the mitochondria and
the shape of the cell and the position of the DNA, things like that.
So by doing this, we are trying to model the relationship between the different structures.
I'm not sure if I make it clear, but once you've done that and you've got this hidden
representation in your auto encoder, how are you using that?
Yeah, that is actually, we'll give us lots of, that's something we want to try to make
some biological interpretation, say, what are the correlations, and how should we interpret
from a biological respective?
So that's something we are doing right now to give it a biological interpolation.
Got it, got it, so that part is ongoing.
Yes.
Okay, cool, and then just quickly the third part of the toolkit that you mentioned is
the visualizer.
How to folks use the visualizer is that, you know, just kind of a GUI that, you know, sits
on some data structures and allows you to look at these images that you've generated
or what else going on in there?
Yes, so we build this, I mean, I already instilled, we build this software called Agave, and
that used the GPU technology to do the photorealistic lightening and shading on our 3D
microscopy and data.
So the reason we care about making the realization more realistic and also including something
like dApps of the image and things like that to make it looking amazing, it's not more
than looking amazing, actually by looking this amazing image, it's give us some feedback
from biological perspective.
So when you look at a particular colony, the traditional rendering method will give you
kind of messy realization in 3D, especially in 3D, right?
So you don't have a good first impression about the biological property of the colony,
but with proper shading and the lightening and all these, the retree tracing techniques
that we are using, we make the image looking very clean and very easy to digest so that
biologists have a better understanding what's going on inside the cell and inside this
whole colony.
And we will include a link to your slides from GTC on the show notes page, and I encourage
folks to check it out for this part in particular, if only because the image of the, you've got
one mode in a gavi that's called cinematographic path tracing and the images do look spectacular.
Yes, this is from our animated cell team, so I love their image, they're very beautiful.
Cool, and is there a ML component to this work as well, or is it just kind of raw number
crunching using the GPU?
So far, there's no ML yet, but we are thinking about further integration.
Well, very interesting stuff.
Jen, she thanks so much for taking the time to chat with us about what you're working
on.
Very cool.
Any parting thoughts or words for folks that might be interested in exploring this area
further?
I would say we find that all these machine learning stuff and the GPU computing solutions
sounds like it's much more than faster computation, it's much more than better realization.
It's actually making us thinking about or doing cell biology research in a completely
new way, and we are also exploring different possibilities down this road, and that's
something we are doing now and keep pushing forward into different directions.
Well, once again, thanks so much.
Thank you.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening, and catch you next time.
