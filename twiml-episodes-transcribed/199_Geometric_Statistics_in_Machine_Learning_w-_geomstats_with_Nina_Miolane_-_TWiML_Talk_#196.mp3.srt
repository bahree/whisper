1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,160
I'm your host Sam Charrington.

4
00:00:32,160 --> 00:00:38,840
In this episode, we're joined by Nina Mielehn, researcher and lecturer at Stanford University.

5
00:00:38,840 --> 00:00:43,080
Nina and I recently spoke about her work in the field of geometric statistics and machine

6
00:00:43,080 --> 00:00:44,400
learning.

7
00:00:44,400 --> 00:00:49,240
Specifically, we discussed the application of reminion geometry, which is the study of

8
00:00:49,240 --> 00:00:52,360
curve surfaces to ML.

9
00:00:52,360 --> 00:00:57,120
Reminion geometry can be helpful in building machine learning models in a number of situations,

10
00:00:57,120 --> 00:01:01,840
including in computational anatomy and medicine, where it helps Nina create models of organs

11
00:01:01,840 --> 00:01:03,680
like the brain and heart.

12
00:01:03,680 --> 00:01:08,160
In our discussion, we review the differences between reminion and euclidean geometry and

13
00:01:08,160 --> 00:01:13,320
theory and practice and discuss several examples from Nina's research, which is a Python

14
00:01:13,320 --> 00:01:21,760
package that simplifies computations and statistics on manifolds with geometric structures.

15
00:01:21,760 --> 00:01:25,720
Before we dive into the conversation, I'd like to send a huge thanks to our friends at

16
00:01:25,720 --> 00:01:29,160
IBM for their sponsorship of this episode.

17
00:01:29,160 --> 00:01:35,160
Interested in exploring code patterns, leveraging multiple technologies, including ML and AI,

18
00:01:35,160 --> 00:01:37,280
then check out IBM Developer.

19
00:01:37,280 --> 00:01:43,200
With more than 100 open source programs, a library of knowledge resources, developer advocates

20
00:01:43,200 --> 00:01:49,560
ready to help and a global community of developers, what in the world will you create?

21
00:01:49,560 --> 00:01:55,960
Dive in at IBM.biz slash ML AI podcast, and be sure to let them know that Swim will

22
00:01:55,960 --> 00:01:57,360
send you.

23
00:01:57,360 --> 00:02:01,960
And now on to the show.

24
00:02:01,960 --> 00:02:04,760
Alright everyone, I am on the line with Nina Mielehn.

25
00:02:04,760 --> 00:02:09,840
Nina is a researcher and lecturer in machine learning for computational anatomy at Stanford

26
00:02:09,840 --> 00:02:10,840
University.

27
00:02:10,840 --> 00:02:13,600
Nina, welcome to this week in machine learning and AI.

28
00:02:13,600 --> 00:02:17,840
Hello, hello Sam, thanks for calling me today and hello everyone.

29
00:02:17,840 --> 00:02:19,600
Absolutely, absolutely.

30
00:02:19,600 --> 00:02:23,840
So, let's just start with a little bit of background.

31
00:02:23,840 --> 00:02:28,360
And in particular, you started out in theoretical physics.

32
00:02:28,360 --> 00:02:32,920
How did you make your way from general relativity to machine learning?

33
00:02:32,920 --> 00:02:39,240
Yes, that's correct, I started with theoretical physics, I did my master in theoretical physics

34
00:02:39,240 --> 00:02:45,680
and mathematical physics, which I think it might be an original background for someone

35
00:02:45,680 --> 00:02:48,000
in machine learning these days.

36
00:02:48,000 --> 00:02:54,680
And so yeah, initially I was looking at different mathematical models of reality, specifically

37
00:02:54,680 --> 00:03:02,360
at the infinitesimally small and the very big, so I was looking at models of particles

38
00:03:02,360 --> 00:03:08,520
elementary particles and their collisions at very high energies and also at models of

39
00:03:08,520 --> 00:03:13,800
the very big, like the stars, the galaxies, space, time and all these things.

40
00:03:13,800 --> 00:03:19,760
And so yes, I had courses like general relativity, black holes, and this was very interesting

41
00:03:19,760 --> 00:03:26,520
because after my master, in fact, I moved to machine learning for the medical imaging field.

42
00:03:26,520 --> 00:03:31,040
And at that time, a lot of people asked me the question like, what does this have to

43
00:03:31,040 --> 00:03:32,760
do with machine learning now?

44
00:03:32,760 --> 00:03:33,760
Why, why are you switching?

45
00:03:33,760 --> 00:03:34,760
What's the link?

46
00:03:34,760 --> 00:03:39,720
I started a PhD in machine learning and how could I do that after a matter in mathematical

47
00:03:39,720 --> 00:03:40,720
physics.

48
00:03:40,720 --> 00:03:48,960
And in fact, both fields are really related and the main link they have is geometry.

49
00:03:48,960 --> 00:03:53,720
So in theoretical physics, when I was looking at space time, in fact, I was looking at

50
00:03:53,720 --> 00:03:55,960
the geometry of space time.

51
00:03:55,960 --> 00:04:01,680
So you might know that Einstein in the early 90s used geometry, actually a particular kind

52
00:04:01,680 --> 00:04:04,840
of geometry, which is called remanion geometry.

53
00:04:04,840 --> 00:04:10,160
And so Einstein used geometry to describe space time and how the geometry of space time

54
00:04:10,160 --> 00:04:14,720
was changing depending on the massive objects that are in space time.

55
00:04:14,720 --> 00:04:20,240
So for example, if you go very close to a black hole, then time will slow down.

56
00:04:20,240 --> 00:04:24,400
And so the geometry of time near the black hole is changed.

57
00:04:24,400 --> 00:04:30,720
So geometry has a lot to do with the mathematical model that you put in theoretical physics.

58
00:04:30,720 --> 00:04:36,080
And then between my master and my PhD, I went to computational anatomy.

59
00:04:36,080 --> 00:04:40,040
So medical imaging, I was looking at organ shapes.

60
00:04:40,040 --> 00:04:45,800
And here again, I got to do a lot of geometry and remanion geometry.

61
00:04:45,800 --> 00:04:51,440
Because in fact, the data, the data in machine learning that I was looking at are geometrical

62
00:04:51,440 --> 00:04:52,440
data.

63
00:04:52,440 --> 00:04:59,400
So they live in a space, in a data space that also has a particular geometry.

64
00:04:59,400 --> 00:05:06,640
And so I was studying the geometry of this data space just in the same way that I was studying

65
00:05:06,640 --> 00:05:10,080
the geometry of space time during my master.

66
00:05:10,080 --> 00:05:15,760
So that's how I got from theoretical physics to machine learning for data belonging to

67
00:05:15,760 --> 00:05:18,520
these geometric spaces.

68
00:05:18,520 --> 00:05:21,160
And then after that, I kind of stayed in machine learning.

69
00:05:21,160 --> 00:05:26,720
So I was a software engineer for a little bit in a machine learning startup.

70
00:05:26,720 --> 00:05:30,200
And now I've come back to academia.

71
00:05:30,200 --> 00:05:35,000
And I'm now a postdoc at Stanford University, as you said it.

72
00:05:35,000 --> 00:05:42,640
And from my PhD to my work in software engineering and my postdoc through these three stages,

73
00:05:42,640 --> 00:05:47,520
three in these three steps, I was looking at data belonging to geometric spaces.

74
00:05:47,520 --> 00:05:51,440
And these data have been mostly organ shapes.

75
00:05:51,440 --> 00:05:54,200
So you said it, I'm in computational anatomy.

76
00:05:54,200 --> 00:05:58,400
So I built computational models of the human body.

77
00:05:58,400 --> 00:06:01,600
And so the data, I have are organ shapes.

78
00:06:01,600 --> 00:06:07,800
During my PhD, I looked at brain shapes, how brain shapes vary when you have a disease.

79
00:06:07,800 --> 00:06:11,920
For example, isomers disease or other forms of dementia.

80
00:06:11,920 --> 00:06:15,480
When I was a software engineer, I looked at hot shapes.

81
00:06:15,480 --> 00:06:21,240
So how the shape of your heart may vary if you have different cardiovascular diseases.

82
00:06:21,240 --> 00:06:24,360
And now back as a postdoc, I do two things.

83
00:06:24,360 --> 00:06:32,840
I look at brain shapes again, but I also look at the shapes of abstract data spaces.

84
00:06:32,840 --> 00:06:38,320
So that's how yeah, I'm kind of merging all the geometric background that I have gathered

85
00:06:38,320 --> 00:06:40,720
to come to the field of machine learning.

86
00:06:40,720 --> 00:06:46,240
Maybe we should start from the beginning and have you explain what rhymanian geometry

87
00:06:46,240 --> 00:06:52,800
is and how is it distinguished from the Euclidean geometry that we tend to think of?

88
00:06:52,800 --> 00:06:54,920
Yes, sure.

89
00:06:54,920 --> 00:07:01,960
So remanian geometry is this theory of mathematics that allowed Einstein to describe the geometry

90
00:07:01,960 --> 00:07:03,520
of space time.

91
00:07:03,520 --> 00:07:08,080
And actually, it's a very powerful mathematical theory, which is why I think Einstein has

92
00:07:08,080 --> 00:07:11,600
used it to describe this very complicated space.

93
00:07:11,600 --> 00:07:14,560
And it's a generalization of Euclidean geometry.

94
00:07:14,560 --> 00:07:18,440
Actually, rhymanian geometry is locally Euclidean.

95
00:07:18,440 --> 00:07:20,160
So let's take an example.

96
00:07:20,160 --> 00:07:26,840
If you consider the earth as the planet, you look at the surface of the earth locally.

97
00:07:26,840 --> 00:07:32,400
So for you and me in a given room, in a building, locally, the geometry is flat.

98
00:07:32,400 --> 00:07:37,920
It looks like we live on a flat to dimensional plane.

99
00:07:37,920 --> 00:07:42,360
But if you look at the global geometry of the earth's surface, it's a sphere.

100
00:07:42,360 --> 00:07:43,600
It's a 2-d sphere.

101
00:07:43,600 --> 00:07:46,240
So the geometry is in fact curved.

102
00:07:46,240 --> 00:07:47,240
No way.

103
00:07:47,240 --> 00:07:48,240
No way.

104
00:07:48,240 --> 00:07:49,240
I know.

105
00:07:49,240 --> 00:07:51,880
I'm just trolling the flat earthers out there.

106
00:07:51,880 --> 00:07:52,880
Sorry.

107
00:07:52,880 --> 00:07:53,880
No, no, no.

108
00:07:53,880 --> 00:08:00,760
But that's a simple example to explain the generalization of Euclidean geometry to

109
00:08:00,760 --> 00:08:07,840
rhymanian geometry, that's one example that everybody uses, because it's more intuitive for

110
00:08:07,840 --> 00:08:08,840
everybody.

111
00:08:08,840 --> 00:08:11,680
But it describes very well, actually, what it is.

112
00:08:11,680 --> 00:08:15,000
So rhymanian geometry is locally Euclidean.

113
00:08:15,000 --> 00:08:20,360
But if you look at the global shape of your space, then it's going to be curved.

114
00:08:20,360 --> 00:08:25,280
And when you start to do statistics on the curved spaces, then actually, everything

115
00:08:25,280 --> 00:08:29,360
that we know about usual statistics breaks down.

116
00:08:29,360 --> 00:08:32,240
So let me come back to the example of the earth for a little bit.

117
00:08:32,240 --> 00:08:35,080
And then we'll go into other examples if you prefer.

118
00:08:35,080 --> 00:08:40,400
But for the earth, imagine you take two cities on the surface of the earth.

119
00:08:40,400 --> 00:08:44,480
And you want to do statistics on the position of these two cities.

120
00:08:44,480 --> 00:08:45,720
So very simple example.

121
00:08:45,720 --> 00:08:48,600
We have only two data points.

122
00:08:48,600 --> 00:08:53,680
And an example of statistics that you may want to compute if, for example, an average

123
00:08:53,680 --> 00:08:56,200
in position of these two cities.

124
00:08:56,200 --> 00:09:03,640
If you use usual statistics in Euclidean space, you're going to compute a city that will

125
00:09:03,640 --> 00:09:08,480
be in between the two cities that you have on the surface of the earth.

126
00:09:08,480 --> 00:09:13,600
But this city, if it's the middle of the chord joining the two cities that you have on the

127
00:09:13,600 --> 00:09:19,560
surface of the earth, the average city will be inside the earth, will be on the surface

128
00:09:19,560 --> 00:09:20,560
of the earth.

129
00:09:20,560 --> 00:09:25,560
And so you are in a situation where you want to compute an average of cities that cannot

130
00:09:25,560 --> 00:09:30,000
possibly be a city, because it's not even on the earth.

131
00:09:30,000 --> 00:09:34,560
And that's a very simple example again, but that's a very, very important example because

132
00:09:34,560 --> 00:09:40,960
it shows that if you do statistics for data that belongs to curved spaces, even the very

133
00:09:40,960 --> 00:09:47,040
definition of mean cannot apply, because the very definition of mean is a linear definition.

134
00:09:47,040 --> 00:09:53,000
The mean is a weighted sum of the elements, so it's linear applying this linear definition

135
00:09:53,000 --> 00:09:56,960
to a nonlinear space, this completely breaks down.

136
00:09:56,960 --> 00:10:03,560
And that's why we got into developing this new field that we call geometric statistics.

137
00:10:03,560 --> 00:10:08,520
And we are asking the question, how should we generalize everything that we know about

138
00:10:08,520 --> 00:10:11,880
statistics to these curved spaces?

139
00:10:11,880 --> 00:10:18,560
When we're thinking about geometric statistics and Romanian geometries, are we exclusively

140
00:10:18,560 --> 00:10:28,120
looking at statistics as it relates to the points on these curves, as opposed to the relationships

141
00:10:28,120 --> 00:10:32,000
between different curves, for example?

142
00:10:32,000 --> 00:10:38,720
Yeah, so when we look at geometric statistics, we mean statistics for data that belong to

143
00:10:38,720 --> 00:10:41,200
these curved spaces.

144
00:10:41,200 --> 00:10:48,600
But actually, you can look at curves on these curved spaces, and the space of curves on curved

145
00:10:48,600 --> 00:10:53,960
spaces is also itself a space that is nonlinear.

146
00:10:53,960 --> 00:10:59,320
So you can look at different kind of data space, and as soon as you have something curved

147
00:10:59,320 --> 00:11:03,400
somewhere, it's going to propagate, and you're going to have some geometric somewhere.

148
00:11:03,400 --> 00:11:11,760
And I think the thought that was the origin of that question was trying to apply this to

149
00:11:11,760 --> 00:11:15,800
your computational anatomy types of problems.

150
00:11:15,800 --> 00:11:23,560
I'm imagining in those fields, yes, I'm interested in describing statistically the surface

151
00:11:23,560 --> 00:11:24,560
of the heart.

152
00:11:24,560 --> 00:11:31,640
But I'm also interested in things that are happening within the heart, and maybe there

153
00:11:31,640 --> 00:11:39,200
are complex surfaces that are composed of different things that are easier described with

154
00:11:39,200 --> 00:11:42,400
multiple curves as opposed to a single curve.

155
00:11:42,400 --> 00:11:48,600
So maybe you can use that as a segue to explain how this is applied in that field.

156
00:11:48,600 --> 00:11:54,560
Yes, so for sure, there are two types of geometry that you can consider when you talk about

157
00:11:54,560 --> 00:11:58,480
these geometric objects, like the surface of the heart.

158
00:11:58,480 --> 00:12:03,280
First of all, you can take one heart and look at its surface, and this surface of this

159
00:12:03,280 --> 00:12:09,240
one heart will have a particular geometry, but I'm not looking at one heart and the surface

160
00:12:09,240 --> 00:12:10,240
of one heart.

161
00:12:10,240 --> 00:12:15,720
Actually, I'm saying, in the way we model things currently, I'm saying that this heart

162
00:12:15,720 --> 00:12:23,360
is just a point in very high dimensional space, abstract space, that represents its shape.

163
00:12:23,360 --> 00:12:29,200
So the shape of the space I'm looking at is the shape of the data space.

164
00:12:29,200 --> 00:12:34,360
So actually, for example, if you go back to the example of the heart, imagine you have

165
00:12:34,360 --> 00:12:39,320
a heart and you want to study its shape, what we usually do is that we say, okay, the

166
00:12:39,320 --> 00:12:44,960
shape of this heart is actually going to be the deformation of a template heart.

167
00:12:44,960 --> 00:12:50,280
So we assume that there is a template shape, that is, for example, the healthy heart

168
00:12:50,280 --> 00:12:51,280
shape.

169
00:12:51,280 --> 00:12:57,680
So we fix that and then we represent each patient heart by the deformation of the template

170
00:12:57,680 --> 00:13:00,440
shape to the patient shape.

171
00:13:00,440 --> 00:13:06,240
And so now each heart shape is represented by a deformation.

172
00:13:06,240 --> 00:13:08,920
So one data is a deformation.

173
00:13:08,920 --> 00:13:13,040
And so if you have different patients, each of them is represented by the deformation

174
00:13:13,040 --> 00:13:16,200
from the template heart to the heart.

175
00:13:16,200 --> 00:13:22,600
And so your data are these deformations that are a point in a high dimensional space,

176
00:13:22,600 --> 00:13:24,960
space, which is the space of deformation.

177
00:13:24,960 --> 00:13:29,720
So it's a bit more abstract that looking at the surface of one heart is actually one

178
00:13:29,720 --> 00:13:34,040
data is a deformation from a template to the patient space.

179
00:13:34,040 --> 00:13:38,680
And the space of data is the space of deformations, which is curved.

180
00:13:38,680 --> 00:13:39,680
Okay, okay.

181
00:13:39,680 --> 00:13:41,240
I think I'm following here.

182
00:13:41,240 --> 00:13:50,640
So a question that pops up for me is I can imagine representing these, you know, a particular

183
00:13:50,640 --> 00:13:57,680
heart, a particular deformation as, you know, some set of parameters that kind of describe

184
00:13:57,680 --> 00:14:04,040
in the physical world, you know, how the heart is deformed from the template.

185
00:14:04,040 --> 00:14:10,000
But I can also imagine something more abstract, kind of like an embedding applied to, you

186
00:14:10,000 --> 00:14:12,320
know, this heart space.

187
00:14:12,320 --> 00:14:14,320
Does that concept apply here?

188
00:14:14,320 --> 00:14:16,040
Yeah, the completely makes sense.

189
00:14:16,040 --> 00:14:23,080
So I think initially people while looking in physical models, like mechanical models and

190
00:14:23,080 --> 00:14:29,720
imagining which clinical parameters may control the deformation from template heart shape

191
00:14:29,720 --> 00:14:32,280
to a patient heart shape.

192
00:14:32,280 --> 00:14:36,920
But then there are so many parameters that you can learn and maybe you're not going to

193
00:14:36,920 --> 00:14:42,960
find all of them and then even if you find this clinical parameters that govern the transformation

194
00:14:42,960 --> 00:14:48,520
from a heart shape to another, then how do you translate that to the next organ shape?

195
00:14:48,520 --> 00:14:53,520
You will need to do that all over again to do a deformation from a healthy brain shape

196
00:14:53,520 --> 00:14:55,720
to another brain shape.

197
00:14:55,720 --> 00:15:00,200
And so in our case, we are mourning the second scenario that you've described.

198
00:15:00,200 --> 00:15:06,520
We just look at the abstract deformation and we parameterize it just in terms of geometry.

199
00:15:06,520 --> 00:15:13,200
So this deformation, this point in our curved space, we represent it mathematically as a

200
00:15:13,200 --> 00:15:14,800
deformorphism.

201
00:15:14,800 --> 00:15:21,160
And then we look at how many parameters we need to describe this deformorphic transformation.

202
00:15:21,160 --> 00:15:25,120
So we look at that in the abstract way.

203
00:15:25,120 --> 00:15:30,800
And also because, you know, I don't have a medical background, so I come from the mathematical,

204
00:15:30,800 --> 00:15:32,040
from a mathematical background.

205
00:15:32,040 --> 00:15:38,680
So I think that's also a reason why we use our mathematical tools to describe transformation.

206
00:15:38,680 --> 00:15:44,120
Now the hope is that eventually both fields will merge together.

207
00:15:44,120 --> 00:15:50,480
So by describing these transformations, these deformations of organ shapes of heart shape

208
00:15:50,480 --> 00:15:57,440
by mathematical or geometric parameters, maybe by doing so we will discover new clinical

209
00:15:57,440 --> 00:15:59,040
parameters.

210
00:15:59,040 --> 00:16:06,800
And actually we started to see a hint of that when we did a study on heart shapes.

211
00:16:06,800 --> 00:16:12,880
So we had many hearts, together many heart shapes, that are set of many heart shapes.

212
00:16:12,880 --> 00:16:18,840
And we performed a principal component analysis on these heart shapes by using deformations.

213
00:16:18,840 --> 00:16:25,520
So we were looking at what were the main variations in terms of geometric shapes within these

214
00:16:25,520 --> 00:16:26,880
heart shapes.

215
00:16:26,880 --> 00:16:34,280
And the main variation geometric deformation that we found was the size of the heart, which

216
00:16:34,280 --> 00:16:41,360
is a clinical irrelevant parameter because it's very correlated with the body mass index.

217
00:16:41,360 --> 00:16:46,400
So basically just by looking at which general direction we were seeing in this data set

218
00:16:46,400 --> 00:16:53,960
of heart shapes, we found one principal component, which was linked to the body mass index.

219
00:16:53,960 --> 00:17:01,320
So yes, we have an abstract mathematical geometric approach to describing this transformation,

220
00:17:01,320 --> 00:17:06,400
but the hope is that we will recover the clinical, the usual clinical parameters and maybe

221
00:17:06,400 --> 00:17:09,480
even discover more clinical parameters.

222
00:17:09,480 --> 00:17:15,320
And so I think this is a very good instance of a pretty disciplinary approach because

223
00:17:15,320 --> 00:17:21,120
historically medicine and computer science have not been that linked.

224
00:17:21,120 --> 00:17:26,920
And now that we have more medical images being produced everyday in hospitals and also

225
00:17:26,920 --> 00:17:31,960
more computing power to actually analyze these images and do machine learning on these

226
00:17:31,960 --> 00:17:32,960
images.

227
00:17:32,960 --> 00:17:38,560
Now is a very good time to have both fields of mathematics and medicine or of computer

228
00:17:38,560 --> 00:17:44,880
science and medicine merging to see if we can establish a dialogue between the geometric

229
00:17:44,880 --> 00:17:47,680
parameters and the clinical parameters.

230
00:17:47,680 --> 00:17:50,440
Can you elaborate a bit more on that example?

231
00:17:50,440 --> 00:17:53,720
What did the data set look like?

232
00:17:53,720 --> 00:17:56,720
Was it an image-based data set?

233
00:17:56,720 --> 00:18:03,920
How did you go from presumably again some set of images in two or three-dimensional Euclidean

234
00:18:03,920 --> 00:18:11,120
space, again, another assumption to geometric representations and then how exactly did you

235
00:18:11,120 --> 00:18:23,480
apply PCA to that to determine these deformations, right, the deformations and derive this size

236
00:18:23,480 --> 00:18:24,480
factor?

237
00:18:24,480 --> 00:18:25,480
Yes.

238
00:18:25,480 --> 00:18:31,600
So originally it was an image data set, but we extracted, so image of the heart, but we

239
00:18:31,600 --> 00:18:36,160
extracted heart surfaces in terms of measures.

240
00:18:36,160 --> 00:18:41,640
So in order to do that, you have in medical imaging community what we call segmentation algorithm,

241
00:18:41,640 --> 00:18:45,760
so there are many data available and they basically do that.

242
00:18:45,760 --> 00:18:50,200
They are able to take an image and extract the surface of the heart.

243
00:18:50,200 --> 00:18:56,280
So for us, the starting point was this data set of meshed surfaces of the heart.

244
00:18:56,280 --> 00:19:02,000
In order to go from the surface, one data point to the deformation, one data point, we

245
00:19:02,000 --> 00:19:05,160
use this template modelization.

246
00:19:05,160 --> 00:19:11,160
In the sense that we took one heart shape and we named it the template.

247
00:19:11,160 --> 00:19:18,640
And so each of the other heart surfaces was a deformation from this template to the

248
00:19:18,640 --> 00:19:19,880
data point.

249
00:19:19,880 --> 00:19:24,720
You arbitrarily picked a heart shape to what you call the template.

250
00:19:24,720 --> 00:19:32,600
I envisioned earlier that the template was derived in some way, akin to an average

251
00:19:32,600 --> 00:19:35,520
or some statistical template.

252
00:19:35,520 --> 00:19:36,520
Exactly.

253
00:19:36,520 --> 00:19:43,360
So yeah, and actually template shape estimation is the main point of my PhD thesis on his

254
00:19:43,360 --> 00:19:44,360
whole.

255
00:19:44,360 --> 00:19:47,520
I wanted to simplify a little bit, but you're completely right.

256
00:19:47,520 --> 00:19:53,720
So in fact, the template shape and the deformations are jointly computed.

257
00:19:53,720 --> 00:19:59,440
So you have an iterative algorithm that is called template shape estimation that works

258
00:19:59,440 --> 00:20:06,680
as follows first, you take all the heart surfaces and you register them.

259
00:20:06,680 --> 00:20:08,880
So you align them in order to correct them.

260
00:20:08,880 --> 00:20:13,640
Find a centroid and try to make them overlap as much as possible.

261
00:20:13,640 --> 00:20:14,640
Exactly.

262
00:20:14,640 --> 00:20:19,120
So that the surface themselves, the meshes overlap as much as possible.

263
00:20:19,120 --> 00:20:21,400
This is called registration.

264
00:20:21,400 --> 00:20:24,960
After this registration step, you compute an average.

265
00:20:24,960 --> 00:20:29,800
If you're in images, you do that in terms of pixels, if you're with surfaces, you find

266
00:20:29,800 --> 00:20:32,440
a middle for each corresponding meshes.

267
00:20:32,440 --> 00:20:36,640
And so after the registration step, you have another edging step.

268
00:20:36,640 --> 00:20:41,000
This averaging step gives you an initial estimate of the template.

269
00:20:41,000 --> 00:20:43,200
And then you iterate the algorithm.

270
00:20:43,200 --> 00:20:47,880
Now that you have an initial estimate of the template, you register again, everybody,

271
00:20:47,880 --> 00:20:50,600
not among themselves, but to the template.

272
00:20:50,600 --> 00:20:52,040
And you have reg them again.

273
00:20:52,040 --> 00:20:54,760
You have a second estimate of the template.

274
00:20:54,760 --> 00:20:59,200
And you do that over and over again until you converge to a template.

275
00:20:59,200 --> 00:21:01,920
And so in the end, you have a template shape.

276
00:21:01,920 --> 00:21:08,480
And each single image or each single surface being described as a deformation from this

277
00:21:08,480 --> 00:21:09,480
template.

278
00:21:09,480 --> 00:21:15,400
And yeah, actually, as I said, my PhD thesis was exactly on this algorithm because this

279
00:21:15,400 --> 00:21:20,040
is an algorithm that is very well known both in medical imaging community, but also in

280
00:21:20,040 --> 00:21:27,880
signal processing community, where you want to align signals and then compute an average

281
00:21:27,880 --> 00:21:28,880
of signals.

282
00:21:28,880 --> 00:21:34,760
So in terms of signals, you might imagine spikes on neurons or stuff like that, temporal signals.

283
00:21:34,760 --> 00:21:40,040
And so they also have, if they want to see the shape of a signal, they also often compute

284
00:21:40,040 --> 00:21:42,040
a template shape and do registration.

285
00:21:42,040 --> 00:21:46,000
So it's a very well known algorithm in the field.

286
00:21:46,000 --> 00:21:52,160
And the goal of my PhD study was to analyze the statistical properties of this algorithm,

287
00:21:52,160 --> 00:21:57,560
which can be done using geometry, and which hadn't been done before.

288
00:21:57,560 --> 00:22:04,440
And so I'm imagining if you've got this template heart and you've got a corpus of other heart

289
00:22:04,440 --> 00:22:10,800
meshes or representations, that one of the things that you might want to do is like

290
00:22:10,800 --> 00:22:19,320
find a distribution of the way that a heart or heart's in general differ from the template

291
00:22:19,320 --> 00:22:26,880
and that part of what makes that difficult in trying to do that in Euclidean geometry

292
00:22:26,880 --> 00:22:32,200
is that you have to do it in any given point on the surface, a bunch of different directions.

293
00:22:32,200 --> 00:22:41,360
And is that part of what makes geometric models easier to apply here?

294
00:22:41,360 --> 00:22:43,720
Yeah, so that's exactly what we want to do.

295
00:22:43,720 --> 00:22:48,600
We want to describe the viability, for example, in a hard shape.

296
00:22:48,600 --> 00:22:52,280
And we do that on a geometric space.

297
00:22:52,280 --> 00:22:58,960
Not necessarily because it's going to be easier, but it's because it is what it makes sense.

298
00:22:58,960 --> 00:23:04,040
So if we go back, you know, to the example, when we were averaging two CDs on the surface

299
00:23:04,040 --> 00:23:09,760
of the earth, if we are computing Euclidean average, then we get something that is not a

300
00:23:09,760 --> 00:23:10,760
city.

301
00:23:10,760 --> 00:23:16,760
Going to the heart now, if we want to compute the average of all the healthy heart, for

302
00:23:16,760 --> 00:23:23,000
example, to get a template healthy heart, if we were to do that using Euclidean geometry,

303
00:23:23,000 --> 00:23:26,400
we will get something that doesn't look like a heart at all.

304
00:23:26,400 --> 00:23:31,280
So it's more in order to be able to describe the variability of heart by summary statistics

305
00:23:31,280 --> 00:23:36,640
like a mean, and then a standard deviation, we want this summary statistics to make sense.

306
00:23:36,640 --> 00:23:40,920
So we want to the mean of hard surfaces to be a hard surface.

307
00:23:40,920 --> 00:23:44,240
And that's why we use geometric statistics.

308
00:23:44,240 --> 00:23:45,960
Makes sense, makes sense.

309
00:23:45,960 --> 00:23:50,080
And then to describe the geometry of the hard shapes, we can do many things.

310
00:23:50,080 --> 00:23:56,760
So I'm taking the example of the mean heart shape, but then we can look at variations.

311
00:23:56,760 --> 00:23:59,480
So that's where PCA comes in.

312
00:23:59,480 --> 00:24:03,240
What are the healthy variations of a heart shape?

313
00:24:03,240 --> 00:24:07,920
For example, size can be a healthy variation of a heart shape.

314
00:24:07,920 --> 00:24:10,880
But then we can also look at clusters.

315
00:24:10,880 --> 00:24:17,200
And the intuition is that the different clusters that you might see will correspond.

316
00:24:17,200 --> 00:24:23,400
One will be for healthy heart shapes, and another one may be for a certain pathology that

317
00:24:23,400 --> 00:24:25,640
can be seen in a heart shape.

318
00:24:25,640 --> 00:24:31,280
And I'll show you what this is what we do for brain shapes, where you can, if you do that,

319
00:24:31,280 --> 00:24:37,680
a clustering on brain shapes, and you want to separate healthy brain shapes versus brain

320
00:24:37,680 --> 00:24:43,320
shapes with Alzheimer's disease, then you can easily separate that because Alzheimer's

321
00:24:43,320 --> 00:24:48,120
disease is a disease that you can see on the brain shapes.

322
00:24:48,120 --> 00:24:52,600
So there is an atrophy of the cerebral cortex that changes the brain shapes.

323
00:24:52,600 --> 00:25:00,520
How do you describe these types of geometric statistics in terms of, are there different

324
00:25:00,520 --> 00:25:07,920
sets of distributions or different fundamental laws, what's different about the way things

325
00:25:07,920 --> 00:25:13,240
work in the geometric world compared to what we're used to?

326
00:25:13,240 --> 00:25:14,240
Yes.

327
00:25:14,240 --> 00:25:21,240
So basically every time that you had something that was linear in the Euclidean word, then

328
00:25:21,240 --> 00:25:26,400
you need to say that into something that is not linear in the Riemannian word.

329
00:25:26,400 --> 00:25:30,080
And even the very basic operations don't work anymore.

330
00:25:30,080 --> 00:25:35,760
So if you're in a Euclidean space and you have two points on this Euclidean space, you

331
00:25:35,760 --> 00:25:40,360
can do a subtraction of a point to another and you get a vector.

332
00:25:40,360 --> 00:25:46,080
So if you have point a and point b, if you do b minus a, you get vector a b.

333
00:25:46,080 --> 00:25:51,920
Now even this very simple operation, the subtraction, you cannot do it in Riemannian geometry.

334
00:25:51,920 --> 00:26:00,040
So this variation has been translated to the Riemannian geometric framework and we call

335
00:26:00,040 --> 00:26:01,720
that the logarithm.

336
00:26:01,720 --> 00:26:06,520
It's not linked to the logarithm as a function, it's just called like that.

337
00:26:06,520 --> 00:26:12,640
Same thing in the Euclidean word, if you have a point a, you can add it a vector u a plus

338
00:26:12,640 --> 00:26:14,720
u and you get another point.

339
00:26:14,720 --> 00:26:20,120
You use that vector to shoot from point a and you arrive at another point.

340
00:26:20,120 --> 00:26:25,200
So the addition, this addition doesn't even exist in Riemannian geometry neither because

341
00:26:25,200 --> 00:26:30,080
if you imagine that you're in the surface of the earth, if you shoot a long attention

342
00:26:30,080 --> 00:26:33,880
vector and you see what you arrive, you're not going to be on the earth.

343
00:26:33,880 --> 00:26:39,720
So you want to generalize this operation in order to do very basic computation in Riemannian

344
00:26:39,720 --> 00:26:40,720
geometry.

345
00:26:40,720 --> 00:26:45,200
And so I took the example of the subtraction of the addition, basic operation that don't

346
00:26:45,200 --> 00:26:50,480
exist as is in Riemannian geometry and it we needed to generalize.

347
00:26:50,480 --> 00:26:55,600
And in fact, the way that we need to generalize these very basics of operations, links to the

348
00:26:55,600 --> 00:27:00,120
fact that we have generalize optimization algorithm, for example.

349
00:27:00,120 --> 00:27:05,280
So if you think about an optimization algorithm, you're in a machine learning framework and

350
00:27:05,280 --> 00:27:10,800
you want to find the parameter theta that optimizes your criterion.

351
00:27:10,800 --> 00:27:14,000
Then when you do gradient descent, that's what you're doing.

352
00:27:14,000 --> 00:27:18,640
You have your current estimate of theta and you shoot along the gradient.

353
00:27:18,640 --> 00:27:24,160
So in Euclidean geometry, you add you have a point which is theta and you use a vector

354
00:27:24,160 --> 00:27:27,320
which is the gradient to shoot along it.

355
00:27:27,320 --> 00:27:32,880
So now this operation, which is addition of a vector to a point, we cannot do it in

356
00:27:32,880 --> 00:27:33,880
Riemannian geometry.

357
00:27:33,880 --> 00:27:40,600
I mean, now we can, because we have generalized the addition, but we couldn't do it as is

358
00:27:40,600 --> 00:27:45,960
in Riemannian geometry, otherwise we would have gotten out of the space of parameters.

359
00:27:45,960 --> 00:27:50,200
Just the same way that we would have gotten out of the surface of the earth.

360
00:27:50,200 --> 00:27:57,080
So we've generalized the addition, but we've also made an analogous construct to a

361
00:27:57,080 --> 00:28:03,560
vector that's, you know, a curved vector on the surface, correct?

362
00:28:03,560 --> 00:28:12,720
Yes, yes, but in fact, actually in Riemannian geometry, we use the concept of tangent vectors.

363
00:28:12,720 --> 00:28:17,680
So if you take your curved space, if you take the surface of the earth, remember I said

364
00:28:17,680 --> 00:28:22,640
that Riemannian geometry was locally Euclidean, so on the surface of the earth, if you are

365
00:28:22,640 --> 00:28:26,560
in your office on the earth, then the earth looks flat.

366
00:28:26,560 --> 00:28:32,120
So Riemannian geometry is locally Euclidean, and in fact, it means that at each point of

367
00:28:32,120 --> 00:28:39,240
a Riemannian manifold, so a curved space, there is a tangent space that locally approximates

368
00:28:39,240 --> 00:28:41,800
very well your curved space.

369
00:28:41,800 --> 00:28:48,000
And so actually we use vectors all as well, because we use at the point of a curved space,

370
00:28:48,000 --> 00:28:52,640
we consider the tangent vectors at this point.

371
00:28:52,640 --> 00:28:58,640
And then we transform them into curved vector, if you want, so we add a tangent vector to

372
00:28:58,640 --> 00:28:59,640
a point.

373
00:28:59,640 --> 00:29:04,720
This brings us to another point on the curved space through a curved vector, you can imagine

374
00:29:04,720 --> 00:29:05,720
it like that.

375
00:29:05,720 --> 00:29:11,840
But since we have so many tools in Euclidean geometry, we wanted also to bring these tools

376
00:29:11,840 --> 00:29:17,040
back on Riemannian geometry, so we bring them back locally at each tangent space of the

377
00:29:17,040 --> 00:29:18,360
curved space.

378
00:29:18,360 --> 00:29:23,720
There's some vector at this tangent space that gets projected onto the surface, and that's

379
00:29:23,720 --> 00:29:27,200
your Riemannian vector, so to speak.

380
00:29:27,200 --> 00:29:30,080
Yes, yes, that's the way it works.

381
00:29:30,080 --> 00:29:34,720
Butchering terminology of course.

382
00:29:34,720 --> 00:29:36,800
That gives us a little bit of a foundation.

383
00:29:36,800 --> 00:29:43,160
Part of what you've been up to recently is building out a set of tools to make this

384
00:29:43,160 --> 00:29:45,600
a little bit more accessible.

385
00:29:45,600 --> 00:29:47,640
Can you talk a little bit about that work?

386
00:29:47,640 --> 00:29:54,720
Yes, so yeah, as I said, I'm very passionate about geometry, and even though I learned geometry

387
00:29:54,720 --> 00:29:59,720
in theoretical physics, when I saw how it could be applied to computational anatomy and

388
00:29:59,720 --> 00:30:05,600
computational medicine, I really wanted to democratize the use of geometry.

389
00:30:05,600 --> 00:30:12,640
And so that's why starting last year, I started to build a Python package to be able to

390
00:30:12,640 --> 00:30:14,240
democratize geometry.

391
00:30:14,240 --> 00:30:20,360
So it's a package that is called GeomStats, it stands for geometric statistics.

392
00:30:20,360 --> 00:30:26,480
And basically, it allows you to do this addition and subtraction, so generalization of addition

393
00:30:26,480 --> 00:30:30,720
and subtraction on Riemannian spaces, easily.

394
00:30:30,720 --> 00:30:35,720
It encapsulates the geometry so that you just have to think about, okay, I'm going to use

395
00:30:35,720 --> 00:30:37,120
the generalization of the addition.

396
00:30:37,120 --> 00:30:39,960
I'm going to use the generalization of the subtraction.

397
00:30:39,960 --> 00:30:45,880
And I wanted to do that because we have seen that, actually, in machine learning, a lot

398
00:30:45,880 --> 00:30:50,120
of spaces, data spaces have geometric properties.

399
00:30:50,120 --> 00:30:55,440
For example, if you're looking at protein shapes and you want to describe the shape of a protein

400
00:30:55,440 --> 00:31:01,840
by the list of angles from one carbon of the carbon bone to another, then you have a

401
00:31:01,840 --> 00:31:05,720
list of angles that belong to a set of spheres.

402
00:31:05,720 --> 00:31:11,240
How many ways, many examples in machine learning where data belong to geometric spaces?

403
00:31:11,240 --> 00:31:16,440
And so we wanted to give a package that allows people to do the computations on these geometric

404
00:31:16,440 --> 00:31:17,440
spaces.

405
00:31:17,440 --> 00:31:24,920
So we have implemented various spaces like hypersphere, hyperbolic spaces, also space of matrices.

406
00:31:24,920 --> 00:31:31,280
For example, symmetric positive definite matrices or rotation matrices, et cetera, so that

407
00:31:31,280 --> 00:31:35,840
people could do proper averaging in these geometric spaces.

408
00:31:35,840 --> 00:31:39,360
So this is a Python package that is available on GitHub.

409
00:31:39,360 --> 00:31:44,520
And we're also currently writing a journal paper that explains the need and the use of this

410
00:31:44,520 --> 00:31:48,360
package for the machine learning community.

411
00:31:48,360 --> 00:31:49,360
Interesting.

412
00:31:49,360 --> 00:31:55,240
So does Python support something like polymorphism that would allow you to just kind of drop

413
00:31:55,240 --> 00:32:00,200
in and replace the operations provided by this package?

414
00:32:00,200 --> 00:32:01,200
By plus and minus.

415
00:32:01,200 --> 00:32:02,200
By plus and minus.

416
00:32:02,200 --> 00:32:03,200
Yeah, exactly.

417
00:32:03,200 --> 00:32:07,640
Or do you have to kind of update your code to use the package?

418
00:32:07,640 --> 00:32:08,640
Yeah.

419
00:32:08,640 --> 00:32:09,640
No.

420
00:32:09,640 --> 00:32:10,640
Right now you have to update your code.

421
00:32:10,640 --> 00:32:15,680
It's true that it will be a very interesting use of polymorphism.

422
00:32:15,680 --> 00:32:16,680
No.

423
00:32:16,680 --> 00:32:20,400
Right now we've given these operations their names, so exponential and logarithm.

424
00:32:20,400 --> 00:32:22,920
The name they have in mathematics.

425
00:32:22,920 --> 00:32:25,400
And it's up to the user to do the translation.

426
00:32:25,400 --> 00:32:29,920
But that's actually an amazing idea we could use polymorphism.

427
00:32:29,920 --> 00:32:35,840
But I guess we also don't want to completely hide the fact that it's geometric, otherwise

428
00:32:35,840 --> 00:32:37,720
people might forget about it.

429
00:32:37,720 --> 00:32:41,920
And then when people run into bugs, then they won't understand what's going on.

430
00:32:41,920 --> 00:32:47,960
So for example, if you remember the example of averaging two hard shapes, if you do that

431
00:32:47,960 --> 00:32:52,080
in the Euclidean way, you're going to get something that is not a hard shape.

432
00:32:52,080 --> 00:32:53,800
And this is not a bug of your code.

433
00:32:53,800 --> 00:32:55,640
This is not a precision problem.

434
00:32:55,640 --> 00:32:59,320
This is a mathematical, this error as a mathematical foundation.

435
00:32:59,320 --> 00:33:03,280
Because the space of hard shapes is curved.

436
00:33:03,280 --> 00:33:08,760
So we don't want to completely hide the fact that there is geometry, but more encapsulate

437
00:33:08,760 --> 00:33:14,080
some subtle concept into a user-friendly environment.

438
00:33:14,080 --> 00:33:15,080
Yeah.

439
00:33:15,080 --> 00:33:22,320
And so using this library to implement something like gradient descent, is it, besides some

440
00:33:22,320 --> 00:33:30,120
of the fact that you're not using polymorphism, is it essentially substituting your pluses

441
00:33:30,120 --> 00:33:33,520
and minuses with your exponents and logarithms?

442
00:33:33,520 --> 00:33:34,520
Yes.

443
00:33:34,520 --> 00:33:35,520
Yes.

444
00:33:35,520 --> 00:33:36,520
So that's a way of thinking.

445
00:33:36,520 --> 00:33:43,000
So the library, we've used it at three points, at three levels in machine learning pipelines.

446
00:33:43,000 --> 00:33:49,120
So if you imagine a supervised learning, a typical supervised learning pipeline where you

447
00:33:49,120 --> 00:33:55,200
have an input, it's x, and then you want to predict an output y from this input x.

448
00:33:55,200 --> 00:34:00,040
And you do that by learning a function that can have a given parameter theta.

449
00:34:00,040 --> 00:34:02,600
Now we can put geometry at theta.

450
00:34:02,600 --> 00:34:06,520
So you say, oh, the space of parameter is actually curved.

451
00:34:06,520 --> 00:34:13,400
And the gradient descent in that curve space will need to use our exponential and logarithm,

452
00:34:13,400 --> 00:34:16,600
which are the generalization of the addition and the subtraction.

453
00:34:16,600 --> 00:34:21,720
So that's one thing that we've put into the package, actually, by creating our own version

454
00:34:21,720 --> 00:34:23,120
of keras.

455
00:34:23,120 --> 00:34:30,840
So now when you, when you, we have created all of keras, no, no, no, no, so we have re-implemented

456
00:34:30,840 --> 00:34:31,840
keras.

457
00:34:31,840 --> 00:34:32,840
No, no, no, no.

458
00:34:32,840 --> 00:34:37,040
We have forked the keras repository and just modified a little bit so that now you can

459
00:34:37,040 --> 00:34:42,520
add a parameter in some keras function, this parameter is called manifold.

460
00:34:42,520 --> 00:34:48,480
And basically, if you say manifold is hypersphere, then it knows that instead of using the addition

461
00:34:48,480 --> 00:34:54,520
and subtraction, they will need to use, the code will need to use exponential and logarithm.

462
00:34:54,520 --> 00:34:59,840
So that's the first way of putting geometry in this supervised learning pipeline is putting

463
00:34:59,840 --> 00:35:02,480
it for the space of the parameter theta.

464
00:35:02,480 --> 00:35:07,280
But now the way we've been using geometric statistics as well is by putting geometry

465
00:35:07,280 --> 00:35:14,400
on the space of the input x, but also on the space of the output y.

466
00:35:14,400 --> 00:35:20,440
And we've done an interesting example, putting geometry on the space of the output y.

467
00:35:20,440 --> 00:35:23,880
So we did a regression on the groups.

468
00:35:23,880 --> 00:35:31,840
So the situation was as follows, it was again, medical images and we had slices of brain

469
00:35:31,840 --> 00:35:34,680
volumes, actually, a fetal brain volume.

470
00:35:34,680 --> 00:35:40,080
With fetal brain MRI, actually, you have a lot of motion, so the images are motion

471
00:35:40,080 --> 00:35:45,680
corrected because the fetal moves in the belly of the mother, the mother may move as well.

472
00:35:45,680 --> 00:35:51,000
And so this is a case of medical images, when if you take slices, then the slices are

473
00:35:51,000 --> 00:35:53,680
not going to be aligned within another.

474
00:35:53,680 --> 00:35:58,200
And when you want to reconstruct the 3D volume of the fetal brain, then you're running

475
00:35:58,200 --> 00:35:59,200
to problem.

476
00:35:59,200 --> 00:36:05,520
And so what we've done using GM stats is that for each slice of the fetal brain, we

477
00:36:05,520 --> 00:36:13,800
were predicting the optimal pose of this slice in the reconstructed brain volume.

478
00:36:13,800 --> 00:36:19,440
And we did that using computational neural network, predicting a pose where a pose is

479
00:36:19,440 --> 00:36:20,440
a translation.

480
00:36:20,440 --> 00:36:26,880
So it's the position of the slice in the 3D brain volume and the orientation, a rotation.

481
00:36:26,880 --> 00:36:32,720
The rotation of the slice in the 3D brain volume, so we were able to take which slice was supposed

482
00:36:32,720 --> 00:36:37,600
to be where in the future reconstructed 3D brain volume.

483
00:36:37,600 --> 00:36:44,000
And in order to do that, we put some geometry on the space of these positions and orientations,

484
00:36:44,000 --> 00:36:48,680
which was the space of the output for this supervised learning algorithm.

485
00:36:48,680 --> 00:36:55,560
Input is slice of brain and output is position and orientation of this slice of brain.

486
00:36:55,560 --> 00:37:01,080
And by considering the fact that positions and orientations, they don't belong to your

487
00:37:01,080 --> 00:37:02,080
rotating space.

488
00:37:02,080 --> 00:37:07,960
They naturally belong to a curved space, which is a lead group, is the space of translations

489
00:37:07,960 --> 00:37:09,160
and rotations.

490
00:37:09,160 --> 00:37:14,920
And by taking into account the geometry of the space of the output of the supervised learning

491
00:37:14,920 --> 00:37:21,680
algorithm, we were able to get better brain volume reconstruction.

492
00:37:21,680 --> 00:37:27,400
So you can put geometry on the space of parameters, you can put geometry on the space of the input,

493
00:37:27,400 --> 00:37:34,040
you can put geometry on the space of the outputs, and you might increase the accuracy of your

494
00:37:34,040 --> 00:37:35,120
results.

495
00:37:35,120 --> 00:37:43,600
In general, are these fundamental operations in the remanion space of the same order of

496
00:37:43,600 --> 00:37:50,600
complexity as the Euclidean analogs?

497
00:37:50,600 --> 00:37:56,800
Which will depend for which space, but what's interesting is that they're actually exactly

498
00:37:56,800 --> 00:37:59,240
the same on the infinitesimal level.

499
00:37:59,240 --> 00:38:04,920
So if you really close, if you imagine your curved space and you close and you imagine

500
00:38:04,920 --> 00:38:09,520
your curved space, which has a tangent space at a given point, if you're really close to

501
00:38:09,520 --> 00:38:15,960
this given point, then using your curved vector or your tangent vector is the same.

502
00:38:15,960 --> 00:38:19,880
So at the infinitesimal scale, yeah, they're exactly the same.

503
00:38:19,880 --> 00:38:25,200
I'm wondering if you've ever tried to apply this someplace where just to try it and were

504
00:38:25,200 --> 00:38:29,920
surprised that it actually works, or are you usually applying this because you have a strong

505
00:38:29,920 --> 00:38:34,440
intuition based on the problem that you should apply it?

506
00:38:34,440 --> 00:38:37,840
No, usually it's based on a strong intuition.

507
00:38:37,840 --> 00:38:46,080
I'm trying to think, yeah, I think it's more the second scenario because I was just wondering

508
00:38:46,080 --> 00:38:53,680
if there are problems that might have some latent geometricness to them that we don't

509
00:38:53,680 --> 00:38:54,680
know.

510
00:38:54,680 --> 00:39:00,400
Now that you've made this easy with a library, if we could just apply it willy-nilly to

511
00:39:00,400 --> 00:39:03,920
see if that has some value.

512
00:39:03,920 --> 00:39:05,400
Yeah.

513
00:39:05,400 --> 00:39:12,600
So I think one set of problems is every time you're dealing with rotations and translation,

514
00:39:12,600 --> 00:39:14,480
but especially rotations.

515
00:39:14,480 --> 00:39:20,840
So rotation is a very intuitive geometric example because we have an intuition of how it works.

516
00:39:20,840 --> 00:39:23,400
You can rotate an object.

517
00:39:23,400 --> 00:39:27,240
And even if it's simple, it appears in so many fields.

518
00:39:27,240 --> 00:39:33,720
One example is in medical imaging when you want to register or align two shapes.

519
00:39:33,720 --> 00:39:38,640
You're rotating one shape to match as closely as possible the other shape.

520
00:39:38,640 --> 00:39:43,520
But also in robotics, when you want to control a robotic arm to do such and such, you want

521
00:39:43,520 --> 00:39:49,240
to move it in space, but you may also want to rotate the robotic hand, that is at the

522
00:39:49,240 --> 00:39:50,920
end of the arm.

523
00:39:50,920 --> 00:39:57,000
So rotations is, even if it's simple, it's a very good example of a geometric space.

524
00:39:57,000 --> 00:40:03,400
And a lot of people haven't been considering the geometry of this rotation space.

525
00:40:03,400 --> 00:40:07,240
And so this is an example where it actually makes a real difference.

526
00:40:07,240 --> 00:40:12,920
And we've shown it with this supervised learning approach where we are able to reconstruct

527
00:40:12,920 --> 00:40:17,880
better brain volumes using, taking into account the geometry of that space.

528
00:40:17,880 --> 00:40:25,160
OK, what I'm hearing is that as opposed to just kind of throwing this approach at a

529
00:40:25,160 --> 00:40:31,720
given problem from a modeling perspective, because it's easier now that we've got gyms

530
00:40:31,720 --> 00:40:38,560
that's rather use it as an opportunity to think about where geometric statistics might

531
00:40:38,560 --> 00:40:44,680
apply or if it might apply to the problem more deeply than you might otherwise, because

532
00:40:44,680 --> 00:40:48,760
now if it does, you've got an easier way to use that information.

533
00:40:48,760 --> 00:40:49,760
Yeah, exactly.

534
00:40:49,760 --> 00:40:53,960
And that's the all point of the paper that we are writing right now.

535
00:40:53,960 --> 00:41:00,560
So we do a review of geometry in machine learning because our feeling is that a lot of people

536
00:41:00,560 --> 00:41:03,280
are using geometric data spaces.

537
00:41:03,280 --> 00:41:08,520
It might be an intuitive space like the space of rotations, but sometimes it's a more abstract

538
00:41:08,520 --> 00:41:14,000
space like a hyperbolic space or the space for symmetric positive definite matrices,

539
00:41:14,000 --> 00:41:18,760
which has a geometric touch, but people don't often use that.

540
00:41:18,760 --> 00:41:23,080
So the first goal of the paper is to do a review and be like, oh, in that field, in fact,

541
00:41:23,080 --> 00:41:25,280
you have some geometry with this data.

542
00:41:25,280 --> 00:41:27,320
In fact, you have some geometry.

543
00:41:27,320 --> 00:41:32,720
And then once we've convinced people that deep down their data space is curved, then

544
00:41:32,720 --> 00:41:37,680
we tell them with gyms that which tools they can use.

545
00:41:37,680 --> 00:41:45,760
And a very important case, a set of tools that they can use is the metrics, the set of

546
00:41:45,760 --> 00:41:46,760
metrics.

547
00:41:46,760 --> 00:41:52,760
So if you have a curved space and you want to measure the distance between two elements

548
00:41:52,760 --> 00:41:58,440
of this curved space, you need to follow the curvature of the space and take the length

549
00:41:58,440 --> 00:42:02,040
of the shortest curve that links these two points.

550
00:42:02,040 --> 00:42:04,320
So we call that a geodesk.

551
00:42:04,320 --> 00:42:09,800
It means that the distance between two points in a curved space is defined as the distance

552
00:42:09,800 --> 00:42:12,440
as the length of the geodesk.

553
00:42:12,440 --> 00:42:17,480
And this means that if you are losing loss functions, if you are learning on a geometric

554
00:42:17,480 --> 00:42:22,200
space, the loss functions that you should use, which is the distance between the ground

555
00:42:22,200 --> 00:42:25,120
roofs and the prediction is the geodesk.

556
00:42:25,120 --> 00:42:26,120
Exactly.

557
00:42:26,120 --> 00:42:30,720
You should use a geodesk distance because you will encompass, you will take into account

558
00:42:30,720 --> 00:42:32,800
the geometry of your space.

559
00:42:32,800 --> 00:42:39,520
So that's what gyms that is about is first telling people, oh, your space is geometric.

560
00:42:39,520 --> 00:42:45,160
You should have a look at this geodesk distance because it will be more suited for your problem.

561
00:42:45,160 --> 00:42:47,640
What's next in this line of research?

562
00:42:47,640 --> 00:42:48,640
Oh.

563
00:42:48,640 --> 00:42:58,080
So in terms of implementation, so gyms that has been around for a year now and we are

564
00:42:58,080 --> 00:42:59,080
in development.

565
00:42:59,080 --> 00:43:02,560
So we are currently developing at different backends.

566
00:43:02,560 --> 00:43:06,960
So we want gyms that since we want to apply it to machine learning, we want gyms that

567
00:43:06,960 --> 00:43:10,240
to be able to be used on GPU.

568
00:43:10,240 --> 00:43:15,160
So we are currently implementing TensorFlow backend and also PyTouch backend.

569
00:43:15,160 --> 00:43:18,280
So this is from an implementation point of view.

570
00:43:18,280 --> 00:43:24,560
And now the goal will be to use gyms that so for me to study organ shapes, but also other

571
00:43:24,560 --> 00:43:25,880
kind of shapes.

572
00:43:25,880 --> 00:43:31,320
So initially I was looking at brain shapes or hard shapes, now I'd like to go into small

573
00:43:31,320 --> 00:43:35,120
the scales and maybe look at cell shapes and protein shapes.

574
00:43:35,120 --> 00:43:40,440
And actually use the tool that we've developed to look at these statistics on these new kinds

575
00:43:40,440 --> 00:43:41,440
of shapes.

576
00:43:41,440 --> 00:43:42,440
Sounds fantastic.

577
00:43:42,440 --> 00:43:47,040
Well, Nina, thanks so much for taking the time to chat with us about this really interesting

578
00:43:47,040 --> 00:43:48,040
work.

579
00:43:48,040 --> 00:43:49,520
Thank you very much.

580
00:43:49,520 --> 00:43:57,600
All right, everyone, that's our show for today for more information on Nina or any of

581
00:43:57,600 --> 00:44:05,040
the topics covered in this episode, visit twimmelai.com slash talk slash 196.

582
00:44:05,040 --> 00:44:09,200
If you're a fan of the podcast and you haven't already done so or you're a new listener

583
00:44:09,200 --> 00:44:14,960
and you like what you hear, visit your Apple or Google podcast app and leave us a five-star

584
00:44:14,960 --> 00:44:16,680
rating and review.

585
00:44:16,680 --> 00:44:22,600
Your reviews are super helpful and they inspire us to create more and better content as well

586
00:44:22,600 --> 00:44:26,080
as help new listeners find the show.

587
00:44:26,080 --> 00:44:29,600
Thanks again to our friends at IBM for their sponsorship of this episode.

588
00:44:29,600 --> 00:44:36,320
Be sure to check out the IBM Developer Portal at IBM.biz slash MLAI podcast.

589
00:44:36,320 --> 00:45:04,440
As always, thanks so much for listening and catch you next time.

