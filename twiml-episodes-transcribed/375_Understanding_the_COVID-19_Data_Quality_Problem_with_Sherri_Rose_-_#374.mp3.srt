1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:16,240
I'm your host Sam Charrington.

3
00:00:16,240 --> 00:00:24,440
Hey, what's up everyone?

4
00:00:24,440 --> 00:00:28,920
Before we get to today's show, I want to send a huge shout out to our friends at Waitin

5
00:00:28,920 --> 00:00:30,880
Biasis.

6
00:00:30,880 --> 00:00:35,920
Last week, we premiered my conversation with WNB Founder Lucas B.Wald on our YouTube

7
00:00:35,920 --> 00:00:36,920
channel.

8
00:00:36,920 --> 00:00:41,680
It was a great conversation about managing artifacts in the machine learning life cycle,

9
00:00:41,680 --> 00:00:46,680
and you can find it at www.tumelai.com slash artifacts.

10
00:00:46,680 --> 00:00:51,080
Waitin Biasis is a lightweight toolkit for machine learning practitioners.

11
00:00:51,080 --> 00:00:55,680
Reviews their original experiment tracking tool for quite a while in the Tumel community

12
00:00:55,680 --> 00:01:00,600
as part of various study groups, and folks have found it to be super useful.

13
00:01:00,600 --> 00:01:04,440
Now that they've added data set versioning and model management capabilities, you've

14
00:01:04,440 --> 00:01:10,240
got a one-stop shop for managing and visualizing your complete machine learning pipeline.

15
00:01:10,240 --> 00:01:15,400
If you'd like to learn more, Waitin Biasis is extending a special offer to Tumel listeners

16
00:01:15,400 --> 00:01:20,920
and viewers, including unlimited private projects and priority support.

17
00:01:20,920 --> 00:01:28,440
For more details or to start tracking your models today, visit WNB.com slash Tumel.

18
00:01:28,440 --> 00:01:31,120
And now on to the show.

19
00:01:31,120 --> 00:01:32,120
Enjoy.

20
00:01:32,120 --> 00:01:34,280
All right, everyone.

21
00:01:34,280 --> 00:01:36,240
I am here with Sherry Rose.

22
00:01:36,240 --> 00:01:40,080
Sherry is an associate professor at Harvard Medical School.

23
00:01:40,080 --> 00:01:42,440
Sherry, welcome to the Tumel AI podcast.

24
00:01:42,440 --> 00:01:43,720
Thank you for having me.

25
00:01:43,720 --> 00:01:46,160
It is great to have a chance to chat with you.

26
00:01:46,160 --> 00:01:53,320
I'm looking forward to digging into your background and your research and the things you're

27
00:01:53,320 --> 00:01:57,280
doing related to COVID to help out there.

28
00:01:57,280 --> 00:01:58,880
Let's start at the beginning.

29
00:01:58,880 --> 00:02:05,800
How did you become interested in machine learning and the intersection of that and healthcare?

30
00:02:05,800 --> 00:02:11,240
I always was very interested in science and mathematics and physics, and I didn't really

31
00:02:11,240 --> 00:02:18,360
have a good sense of how you could use that to solve problems when I was going to college.

32
00:02:18,360 --> 00:02:24,880
And it was during college that I was exposed to this summer program called the Summer Institute

33
00:02:24,880 --> 00:02:26,720
for Training and Biostatistics.

34
00:02:26,720 --> 00:02:31,920
And it really sounded like what I was interested in, which was bringing quantitative reasoning

35
00:02:31,920 --> 00:02:35,840
and thinking to problems in health and public health.

36
00:02:35,840 --> 00:02:42,880
And I realized very quickly that I needed more than my bachelor's degree in statistics

37
00:02:42,880 --> 00:02:46,280
in order to really solve a lot of those problems.

38
00:02:46,280 --> 00:02:50,360
And I didn't actually get any training in machine learning in my bachelor's degree.

39
00:02:50,360 --> 00:02:55,160
I graduated in 2005, and the curriculum definitely did not include it at that point.

40
00:02:55,160 --> 00:03:00,560
And so when I went to graduate school at UC Berkeley in biostatistics, that's where I saw

41
00:03:00,560 --> 00:03:07,000
the benefit of having really general frameworks in which to solve problems.

42
00:03:07,000 --> 00:03:11,840
And that's when I started working on non-parametric machine learning and having these kind of

43
00:03:11,840 --> 00:03:17,320
big picture ways to attack big problems in population health.

44
00:03:17,320 --> 00:03:23,040
And that was for me, that's been both machine learning in non-parametric models for prediction,

45
00:03:23,040 --> 00:03:25,600
but also causal inference.

46
00:03:25,600 --> 00:03:31,960
And the driver for me was really the ability to use these flexible tools to solve hard

47
00:03:31,960 --> 00:03:35,520
problems in health care and medicine.

48
00:03:35,520 --> 00:03:39,520
It must have been helpful having that undergrad in stats.

49
00:03:39,520 --> 00:03:40,520
It's been very helpful.

50
00:03:40,520 --> 00:03:45,240
I actually started as a mechanical and aerospace engineering major.

51
00:03:45,240 --> 00:03:51,600
And I did not feel very invigorated by the coursework there.

52
00:03:51,600 --> 00:04:00,480
And I also was a little frustrated that I was often the only woman in the classes.

53
00:04:00,480 --> 00:04:04,160
And there was a lot of reasons why I didn't feel like the right fit for me.

54
00:04:04,160 --> 00:04:09,880
I ended up taking my second semester in college statistics course, and I immediately saw

55
00:04:09,880 --> 00:04:14,080
how statistics could be used for solving lots of different problems.

56
00:04:14,080 --> 00:04:15,880
And engineering can as well.

57
00:04:15,880 --> 00:04:20,920
But for me, the statistics was really how I saw bringing all of my interests together.

58
00:04:20,920 --> 00:04:24,040
You mentioned non-parametric machine learning.

59
00:04:24,040 --> 00:04:29,000
What is that and how does that relate to both the broader field as well as the health

60
00:04:29,000 --> 00:04:30,000
care field?

61
00:04:30,000 --> 00:04:31,000
Yeah.

62
00:04:31,000 --> 00:04:35,640
So when I talk about non-parametrics, I mean it in the very broad statistical sense.

63
00:04:35,640 --> 00:04:40,880
A non-parametric model is a larger model space where we're making many fewer assumptions.

64
00:04:40,880 --> 00:04:46,600
And whereas with parametric models, more standard parametric models, we might be making really

65
00:04:46,600 --> 00:04:50,840
strict assumptions about the functional form, the underlying unknown functional form of the

66
00:04:50,840 --> 00:04:51,840
data.

67
00:04:51,840 --> 00:04:57,560
With non-parametrics, I want to really have a large model space, so I have a much better

68
00:04:57,560 --> 00:05:02,840
opportunity to uncover the truth with my machine learning estimator.

69
00:05:02,840 --> 00:05:07,080
So meaning like you're not assuming a normal distribution which has a couple of parameters

70
00:05:07,080 --> 00:05:10,080
and meaning and a standard deviation, it can be anything.

71
00:05:10,080 --> 00:05:11,080
Definitely not.

72
00:05:11,080 --> 00:05:12,080
Definitely not.

73
00:05:12,080 --> 00:05:14,760
That would be a limiting assumption in your work?

74
00:05:14,760 --> 00:05:16,760
Absolutely.

75
00:05:16,760 --> 00:05:22,080
And most of the data that I work with does not conform to those types of strict assumptions.

76
00:05:22,080 --> 00:05:28,200
Talk a little bit more about the scope of your research interests and where you apply

77
00:05:28,200 --> 00:05:29,200
machine learning.

78
00:05:29,200 --> 00:05:35,280
It sounds like you are interested both in the kind of the systematic issues, the health

79
00:05:35,280 --> 00:05:42,360
care system with the relationships between the providers and the payers, as well as clinical

80
00:05:42,360 --> 00:05:43,360
issues.

81
00:05:43,360 --> 00:05:44,360
Absolutely.

82
00:05:44,360 --> 00:05:49,320
Our services research were really interested in the whole broad scope of the health care

83
00:05:49,320 --> 00:05:54,360
system that includes costs, quality, access to providers and services, and also a health

84
00:05:54,360 --> 00:05:56,000
outcomes following care.

85
00:05:56,000 --> 00:06:00,960
So that clinical piece often comes into the health outcomes following care.

86
00:06:00,960 --> 00:06:06,480
And some of the major areas that I've worked in intersect with the health spending aspects,

87
00:06:06,480 --> 00:06:13,280
the financing aspects like mental health and telemedicine and cardiovascular treatments,

88
00:06:13,280 --> 00:06:20,640
all of these things intersect within this system that relies on the cost, the quality and

89
00:06:20,640 --> 00:06:21,840
the access to providers.

90
00:06:21,840 --> 00:06:27,920
So it's a really having a research program that encompasses both pieces of that can allow

91
00:06:27,920 --> 00:06:32,040
you to ask and answer questions in more integrated ways.

92
00:06:32,040 --> 00:06:38,840
It's difficult, but I find that if you understand those underlying systems and try and bring

93
00:06:38,840 --> 00:06:43,240
them into your work when you're looking at clinical work, it can really help you.

94
00:06:43,240 --> 00:06:45,920
You inform better answers.

95
00:06:45,920 --> 00:06:55,520
And when you are looking at those kinds of questions, are you primarily trying to understand

96
00:06:55,520 --> 00:06:57,000
or influence?

97
00:06:57,000 --> 00:06:58,000
Great question.

98
00:06:58,000 --> 00:07:02,360
So a lot of the work that I do, we are trying to understand some kind of phenomena in the

99
00:07:02,360 --> 00:07:07,760
system, but influence, yes, in a sense, that we're trying to inform policy.

100
00:07:07,760 --> 00:07:13,760
So understanding the comparative effectiveness of multiple different types of treatments,

101
00:07:13,760 --> 00:07:18,760
I would like to understand which treatments have better health outcomes.

102
00:07:18,760 --> 00:07:25,480
But if we find a particular treatment has very bad outcomes, we want to inform policy

103
00:07:25,480 --> 00:07:31,800
to the FDA or to the relevant stakeholder in order to potentially have that treatment removed

104
00:07:31,800 --> 00:07:33,080
from market.

105
00:07:33,080 --> 00:07:40,080
And we're talking towards the end of April, many of us have been in some form of another

106
00:07:40,080 --> 00:07:43,160
of lockdown due to COVID.

107
00:07:43,160 --> 00:07:47,640
You mentioned that your dog may start barking in time.

108
00:07:47,640 --> 00:07:49,640
He may, he may.

109
00:07:49,640 --> 00:07:54,600
My neighbor just, I think my neighbor is finished cutting the grass now.

110
00:07:54,600 --> 00:08:01,000
This is just the times, but it sounds like your work intersects with COVID as well.

111
00:08:01,000 --> 00:08:03,360
Can you talk about that intersection a little bit?

112
00:08:03,360 --> 00:08:04,360
Absolutely.

113
00:08:04,360 --> 00:08:11,080
A large focus of my work, because I'm so integrated in starting with the substantive problem

114
00:08:11,080 --> 00:08:15,320
and bringing either existing machine learning tools or developing new machine learning tools

115
00:08:15,320 --> 00:08:20,720
to answer those questions, it really, there has to be the strong grounding in data.

116
00:08:20,720 --> 00:08:27,040
And the coronavirus pandemic has really illuminated for a lot of people how much we need to care

117
00:08:27,040 --> 00:08:28,440
about data.

118
00:08:28,440 --> 00:08:36,000
And I, I mean, we have misclassification, we have missingness in the types of data that

119
00:08:36,000 --> 00:08:41,040
we're collecting for coronavirus, both for cases and mortality counts.

120
00:08:41,040 --> 00:08:46,160
And these are things that are very, very common in most of the electronic health data that

121
00:08:46,160 --> 00:08:50,560
we use in the healthcare system, where a lot of my work has focused on dealing with some

122
00:08:50,560 --> 00:08:51,760
of these types of issues.

123
00:08:51,760 --> 00:08:58,760
I mean, we use billing claims, we use clinical records, registry data, and on and on.

124
00:08:58,760 --> 00:09:01,160
And these data types were not designed for research.

125
00:09:01,160 --> 00:09:07,000
And so we need to be really aware of the issues in these types of data.

126
00:09:07,000 --> 00:09:11,840
And some of the newer forms of data, like wearable and implantable technology that people have

127
00:09:11,840 --> 00:09:14,160
been very excited about, measuring physical activity.

128
00:09:14,160 --> 00:09:19,280
We're now using in the coronavirus pandemic, you know, smartphone location data to try

129
00:09:19,280 --> 00:09:26,080
and understand how people are social distancing or with potentially with contact tracing.

130
00:09:26,080 --> 00:09:31,720
And then digital types of data like Google search trends and Twitter data, which has been

131
00:09:31,720 --> 00:09:34,280
used for different types of research questions in the past.

132
00:09:34,280 --> 00:09:39,520
Now Google is developing and has released this location history website where they're

133
00:09:39,520 --> 00:09:42,800
showing about, you know, how we can understand social distancing.

134
00:09:42,800 --> 00:09:49,160
And so a lot of the data related work that I've been focused on is very relevant to the

135
00:09:49,160 --> 00:09:53,760
pandemic and understanding our data sources and trying to bring rigorous flexible methods

136
00:09:53,760 --> 00:09:54,760
to them.

137
00:09:54,760 --> 00:10:00,320
Specifically, I had been working the last two years with my now former postdoctoral fellow

138
00:10:00,320 --> 00:10:02,600
who's an infectious disease expert.

139
00:10:02,600 --> 00:10:07,880
My am a jummer who's now faculty at Boston Children's Hospital and Harvard Medical School.

140
00:10:07,880 --> 00:10:13,640
We had been looking at news media data, CDC data and electronic health data to understand

141
00:10:13,640 --> 00:10:19,080
the generalizability of these data sources for both infectious disease and chronic disease.

142
00:10:19,080 --> 00:10:22,560
And now this has become very, very relevant to the coronavirus pandemic.

143
00:10:22,560 --> 00:10:27,320
We had been one of the conditions we've been studying was, was flu-like illnesses and

144
00:10:27,320 --> 00:10:33,520
understanding, you know, what electronic health data sources like billing claims and electronic

145
00:10:33,520 --> 00:10:37,560
health records, what we can really understand from these data sources.

146
00:10:37,560 --> 00:10:44,560
And we've seen people, many people now start modeling and making projections about cases

147
00:10:44,560 --> 00:10:47,320
and death counts.

148
00:10:47,320 --> 00:10:52,480
What we're going to start seeing next, once people start having access to different types

149
00:10:52,480 --> 00:10:59,080
of electronic health resources is trying to use this data to understand, you know, to

150
00:10:59,080 --> 00:11:04,320
predict outcomes, maybe to predict clinical courses or to try and do causal imprage,

151
00:11:04,320 --> 00:11:06,840
which is even more difficult.

152
00:11:06,840 --> 00:11:11,760
And it's very, very important that people understand the limitations of these data sources.

153
00:11:11,760 --> 00:11:17,840
And so that's one of the things that we're working on and hopefully the first paper from

154
00:11:17,840 --> 00:11:21,360
that work will be able to release in the next coming weeks.

155
00:11:21,360 --> 00:11:27,200
But this is something that's relevant for the coronavirus pandemic, but has been, you

156
00:11:27,200 --> 00:11:32,360
know, a problem going back, you know, decades is using data that people don't understand.

157
00:11:32,360 --> 00:11:38,040
And that's been at the forefront of my work is really making sure, especially, you know,

158
00:11:38,040 --> 00:11:42,040
with the theme of one of the themes of this podcast machine learning, a lot of people

159
00:11:42,040 --> 00:11:45,880
get very excited about machine learning and they throw a tool at data without understanding

160
00:11:45,880 --> 00:11:46,880
the data.

161
00:11:46,880 --> 00:11:50,720
And we're now in the midst of something where it's really crucial that people do not do

162
00:11:50,720 --> 00:11:51,720
that.

163
00:11:51,720 --> 00:11:52,720
Yeah.

164
00:11:52,720 --> 00:12:00,640
We just had a panel discussion, a tum live discussion earlier this week on responsible

165
00:12:00,640 --> 00:12:08,240
data science in the fight against COVID-19 and talked quite extensively about this issue.

166
00:12:08,240 --> 00:12:14,080
And, you know, a lot of the panel initially grew out of the reactions I was seeing to,

167
00:12:14,080 --> 00:12:19,120
you know, folks jumping in, wanting to, you know, help out, produce dashboards and models.

168
00:12:19,120 --> 00:12:22,600
And then you'd have this kind of counter reaction of folks saying, hey, you know, you're not

169
00:12:22,600 --> 00:12:28,000
an epidemiologist, stay in your lane kind of thing, which I kind of object to, to a large

170
00:12:28,000 --> 00:12:32,680
degree because, you know, a, people want to help and be, you know, people, you know, want

171
00:12:32,680 --> 00:12:37,120
to learn and, and, you know, everyone's bringing something.

172
00:12:37,120 --> 00:12:40,560
But at the same time, you know, the stakes are high.

173
00:12:40,560 --> 00:12:47,280
And even if you're an expert, it's easy to get things wrong because the data is, you

174
00:12:47,280 --> 00:12:51,360
know, as you mentioned, everyone's reporting different things.

175
00:12:51,360 --> 00:12:52,360
The data is messy.

176
00:12:52,360 --> 00:12:58,320
You know, talk a little bit more about the kinds of things you're, you're seeing in the data.

177
00:12:58,320 --> 00:13:03,800
And you mentioned you have a paper coming out is the, the objective of this paper to try

178
00:13:03,800 --> 00:13:11,560
to kind of quantitatively, qualitatively provide measures for data quality as applied to

179
00:13:11,560 --> 00:13:15,760
some of these use cases or what exactly are you trying to do with this paper?

180
00:13:15,760 --> 00:13:20,320
Yeah, this, this first paper is one of the main things that, that people will see.

181
00:13:20,320 --> 00:13:25,240
And this is true of most health conditions, but particularly among infectious diseases

182
00:13:25,240 --> 00:13:32,200
if that is that in billing claims in electronic health records, you will see under counts

183
00:13:32,200 --> 00:13:34,320
of infectious disease conditions.

184
00:13:34,320 --> 00:13:39,200
And so using this data and not understanding all of the different reasons why we might

185
00:13:39,200 --> 00:13:44,760
under count a particular health condition is, it would be very problematic.

186
00:13:44,760 --> 00:13:51,320
So one of the things that we will do in this work is try to quantify and we've got multiple

187
00:13:51,320 --> 00:13:56,880
years of data and so we can show trends over time, quantifying this under counting in electronic

188
00:13:56,880 --> 00:14:01,560
health data for influenza-like illnesses.

189
00:14:01,560 --> 00:14:04,560
And but this is even true of chronic diseases.

190
00:14:04,560 --> 00:14:10,880
We see with chronic diseases that one of the, in order to be counted in an electronic

191
00:14:10,880 --> 00:14:15,360
health database, you have to have an encounter with the health care system.

192
00:14:15,360 --> 00:14:18,720
And we know that there's many reasons why people may not have an encounter with the health

193
00:14:18,720 --> 00:14:19,720
care system.

194
00:14:19,720 --> 00:14:24,240
We know that people in rural communities whose hospitals have closed may not have an encounter.

195
00:14:24,240 --> 00:14:29,760
We know that people who do not have insurance or are low income may have additional barriers

196
00:14:29,760 --> 00:14:32,520
to getting to, to having care.

197
00:14:32,520 --> 00:14:39,800
Is the issue that the records that we do have under count because folks, there are folks

198
00:14:39,800 --> 00:14:44,360
out there that, you know, contract COVID and don't interface with the health care system

199
00:14:44,360 --> 00:14:49,840
or is it that even of those folks that are interfacing with the health care system,

200
00:14:49,840 --> 00:14:53,400
they're systematic under counting for some reasons.

201
00:14:53,400 --> 00:14:54,400
It's both.

202
00:14:54,400 --> 00:14:55,400
It's both.

203
00:14:55,400 --> 00:14:58,240
So not ever, so not, we won't see people who don't have an encounter with the health

204
00:14:58,240 --> 00:14:59,240
care system.

205
00:14:59,240 --> 00:15:02,800
And even people who do have an encounter with the health care system, they may not be coded.

206
00:15:02,800 --> 00:15:08,160
So there's now an ICD-10 code, which is a billing code for health conditions.

207
00:15:08,160 --> 00:15:12,440
There will be people who have coronavirus who will not be coded as having coronavirus,

208
00:15:12,440 --> 00:15:14,200
even though they have an encounter with the health care system.

209
00:15:14,200 --> 00:15:18,440
And there will be many reasons for this, including the fact that we don't have enough testing.

210
00:15:18,440 --> 00:15:21,880
But there's lots of reasons why somebody might not have a code.

211
00:15:21,880 --> 00:15:23,520
They might get coded for something else.

212
00:15:23,520 --> 00:15:26,120
They might get coded for flu instead of coronavirus.

213
00:15:26,120 --> 00:15:28,760
They might get coded for just a higher level.

214
00:15:28,760 --> 00:15:32,120
And then we're going to have people who are coded for coronavirus who don't actually have

215
00:15:32,120 --> 00:15:33,120
it.

216
00:15:33,120 --> 00:15:36,360
So there's going to be in this classification in both directions.

217
00:15:36,360 --> 00:15:39,160
So you're supposed to wait for a positive test.

218
00:15:39,160 --> 00:15:44,560
But again, because they're so little testing, a physician might be inclined to code somebody

219
00:15:44,560 --> 00:15:48,680
for coronavirus if they're a suspected case without a confirmed test, or because they're

220
00:15:48,680 --> 00:15:52,560
such a delay in getting the test results back.

221
00:15:52,560 --> 00:15:58,280
And so when you're trying to characterize this type of undercounting, does machine learning

222
00:15:58,280 --> 00:15:59,680
come into play there?

223
00:15:59,680 --> 00:16:02,840
And if so, what are the tools that you're using?

224
00:16:02,840 --> 00:16:09,640
So right now, this is not using, this is not a prediction question, it calls a inference

225
00:16:09,640 --> 00:16:10,640
question yet.

226
00:16:10,640 --> 00:16:12,840
This is a data quality question.

227
00:16:12,840 --> 00:16:14,640
This is a data quality question.

228
00:16:14,640 --> 00:16:18,200
And so there's data science techniques that go into this, for example, when we need to

229
00:16:18,200 --> 00:16:26,440
use different types of data aggregation methods in order to extract data from PDFs when

230
00:16:26,440 --> 00:16:29,080
we're comparing to maybe CDC reviews.

231
00:16:29,080 --> 00:16:34,440
One of the things that has been part of this project for the last two years has been,

232
00:16:34,440 --> 00:16:41,640
we wanted to understand the impact of news deserts on infectious disease outbreaks.

233
00:16:41,640 --> 00:16:45,920
And we have a lot of different types of data in order to understand communities where their

234
00:16:45,920 --> 00:16:48,880
local newspapers have closed.

235
00:16:48,880 --> 00:16:53,880
And it will be interesting over the long term to see how that, whether that will even

236
00:16:53,880 --> 00:16:57,080
matter for the coronavirus given that it's a global pandemic.

237
00:16:57,080 --> 00:17:03,240
And that's not necessarily how people are becoming aware of the pandemic.

238
00:17:03,240 --> 00:17:07,400
And that's not necessarily the way that we're counting cases anymore, with much smaller

239
00:17:07,400 --> 00:17:14,080
outbreaks, local news media is really vital in informing the community and also for researchers

240
00:17:14,080 --> 00:17:20,160
to use those local news reports to get another source of case counts.

241
00:17:20,160 --> 00:17:26,040
So not just informing the community of citizens, but informing the medical community, there's

242
00:17:26,040 --> 00:17:31,560
not a back channel of, hey, be on the lookout for this disease.

243
00:17:31,560 --> 00:17:34,880
Or if there is, it's inefficient, is that what you're saying?

244
00:17:34,880 --> 00:17:40,080
Well, I guess I'm highlighting that the local news media data is another resource.

245
00:17:40,080 --> 00:17:44,720
It's another way that we find out about outbreaks when they're smaller outbreaks.

246
00:17:44,720 --> 00:17:50,160
And it's another way for researchers to, again, there's no gold standard.

247
00:17:50,160 --> 00:17:55,480
There's limitations in infectious disease outbreaks in the data from the CDC, from an

248
00:17:55,480 --> 00:18:00,760
electronic, you know, billing claims resource from local news media data.

249
00:18:00,760 --> 00:18:04,880
And so one of the overarching goals of this, this project that had been ongoing was really

250
00:18:04,880 --> 00:18:09,400
to understand the generalizability of all these data sources and try to quantify how we

251
00:18:09,400 --> 00:18:13,960
could leverage multiple data sources to get at accurate case counts.

252
00:18:13,960 --> 00:18:19,600
And then more broadly, when you're applying machine learning to these types of problems,

253
00:18:19,600 --> 00:18:26,240
I'm curious about the tools that you end up using. You mentioned causality, causal

254
00:18:26,240 --> 00:18:27,240
inference.

255
00:18:27,240 --> 00:18:32,960
I saw in maybe the publications page on your site, which of course we'll link to in the

256
00:18:32,960 --> 00:18:43,120
show notes picture where you have your, it looks like you are trying to relate the, you

257
00:18:43,120 --> 00:18:45,240
know, costs of different interventions.

258
00:18:45,240 --> 00:18:49,240
So you've got some baseline formula and then, you know, mental health as a broad category

259
00:18:49,240 --> 00:18:51,720
of intervention, maybe, and then substance use.

260
00:18:51,720 --> 00:18:55,600
You can tell me what you're actually saying in this diagram.

261
00:18:55,600 --> 00:19:01,560
And then you've got increases and decreases of compensation, actually, so that may not

262
00:19:01,560 --> 00:19:02,560
be cost.

263
00:19:02,560 --> 00:19:06,960
But it kind of strikes me that you're looking at, it's your applying causal inference

264
00:19:06,960 --> 00:19:08,920
and you're looking at interventions.

265
00:19:08,920 --> 00:19:16,120
Yeah, so I'd have to confirm which diagram that was, but I believe it's probably from

266
00:19:16,120 --> 00:19:20,480
one of the studies where we're examining the impact of different types of payment models

267
00:19:20,480 --> 00:19:21,720
in the healthcare system.

268
00:19:21,720 --> 00:19:22,720
Okay.

269
00:19:22,720 --> 00:19:28,160
And to understand the impact of, you know, changing how spending occurs and the different

270
00:19:28,160 --> 00:19:33,360
types of what we call spending models, you know, we have payment systems in the U.S.,

271
00:19:33,360 --> 00:19:39,120
where sometimes we have sort of what we call a bundle payment, a certain specific amount

272
00:19:39,120 --> 00:19:44,560
for a particular type of procedure and other types of spending models where we have, you

273
00:19:44,560 --> 00:19:52,400
know, fee for service, every single thing that you do, whether it's a lab or a procedure,

274
00:19:52,400 --> 00:19:55,840
it has a particular dollar amount attached to it.

275
00:19:55,840 --> 00:19:59,840
And so then the incentive, of course, is if you're getting a fee for service is to have

276
00:19:59,840 --> 00:20:01,600
many, many services.

277
00:20:01,600 --> 00:20:08,360
And so a number of the studies that I've worked on is trying to understand the impact

278
00:20:08,360 --> 00:20:13,120
of these types of spending models, but one thing that I will highlight that I think

279
00:20:13,120 --> 00:20:20,560
a lot of people who don't work in health economics may not realize is that the impact of changing

280
00:20:20,560 --> 00:20:26,360
either, you know, improving how we allocate funding in the healthcare system or the

281
00:20:26,360 --> 00:20:31,200
impact of different funding models has the potential to improve human health.

282
00:20:31,200 --> 00:20:36,000
So it's not an exercise in trying to save money at the cost of human health.

283
00:20:36,000 --> 00:20:41,600
It's really about more efficiently serving people such that we can improve human health

284
00:20:41,600 --> 00:20:47,200
in one of the areas where we've seen this is in mental health care, where over, you

285
00:20:47,200 --> 00:20:56,360
know, the second half of the decade, sorry, the second half of, you know, 1950 to 2000,

286
00:20:56,360 --> 00:21:03,520
we saw that the vast majority of improvements in mental health treatment really came from

287
00:21:03,520 --> 00:21:07,920
changes in the financing of health care and improvements, things that led to improvements

288
00:21:07,920 --> 00:21:09,240
in access.

289
00:21:09,240 --> 00:21:11,560
And so it wasn't necessarily new treatments.

290
00:21:11,560 --> 00:21:16,440
But as far as the types of methods that I work on, so a lot of the work that I do in causal

291
00:21:16,440 --> 00:21:19,680
inference is focused on, on sampling methods.

292
00:21:19,680 --> 00:21:24,560
So bringing together multiple algorithms so we don't have to rely on a single algorithm.

293
00:21:24,560 --> 00:21:29,880
So especially when different types of algorithms become the new flashy tool.

294
00:21:29,880 --> 00:21:33,480
So when I was in grad school, it was random forest and now it's, you know, deep learning

295
00:21:33,480 --> 00:21:36,680
and neural networks and everyone's like, should I do a regression or should I do a random

296
00:21:36,680 --> 00:21:42,800
forest and I say you can do both and more by incorporating, you know, rigorous ensemble

297
00:21:42,800 --> 00:21:47,000
techniques by using multiple algorithms in a priori specified metrics.

298
00:21:47,000 --> 00:21:52,520
And then within causal inference, you know, we bring these ensembles into so-called double

299
00:21:52,520 --> 00:21:54,000
robust estimation.

300
00:21:54,000 --> 00:21:56,720
So we don't just use information from an outcome regression.

301
00:21:56,720 --> 00:22:02,560
We also use information for estimating what the, you know, the probability that you

302
00:22:02,560 --> 00:22:06,680
would have been treated and so a functional form for that, a flexible functional form

303
00:22:06,680 --> 00:22:07,680
for that.

304
00:22:07,680 --> 00:22:13,680
And so I've co-authored two books on this topic and really a lot of my research has focused

305
00:22:13,680 --> 00:22:19,160
on bringing double robust methods and machine learning together and again integrating that

306
00:22:19,160 --> 00:22:24,920
into health services research and really trying to develop tools to specifically answer

307
00:22:24,920 --> 00:22:27,840
questions in health services research.

308
00:22:27,840 --> 00:22:31,800
You mentioned rigorous ensemble methods.

309
00:22:31,800 --> 00:22:38,120
What does that mean for an ensemble method to be rigorous and how does one achieve the

310
00:22:38,120 --> 00:22:39,640
requisite level of rigor?

311
00:22:39,640 --> 00:22:40,640
Yes.

312
00:22:40,640 --> 00:22:45,120
So I threw in rigorous because anytime I mention ensemble techniques, I'm always a little

313
00:22:45,120 --> 00:22:50,040
bit concerned that for people who might be unfamiliar and I mentioned in a priori metrics

314
00:22:50,040 --> 00:22:53,800
that's being rigorous, but we really have to decide upfront.

315
00:22:53,800 --> 00:22:57,280
If we're going to run multiple algorithms, we really need to specify upfront what they

316
00:22:57,280 --> 00:22:59,400
will be, how they will be evaluated.

317
00:22:59,400 --> 00:23:01,880
We need to incorporate cross validation.

318
00:23:01,880 --> 00:23:06,640
Meaning as opposed to I'm working on a Kaggle competition, I'm going to throw every model

319
00:23:06,640 --> 00:23:12,280
I can think of against this dataset and see which produces the best accuracy on my test

320
00:23:12,280 --> 00:23:13,280
set.

321
00:23:13,280 --> 00:23:17,080
Well, you could do that if you make sure to choose beforehand that accuracy is going to

322
00:23:17,080 --> 00:23:19,120
be your metric and you incorporate cross validation.

323
00:23:19,120 --> 00:23:24,040
We can come back to why leaderboard accuracy is bad.

324
00:23:24,040 --> 00:23:28,200
Single metric, single metrics is another one of the thing, you know, I get data quality

325
00:23:28,200 --> 00:23:34,360
is a soapbox for mine, single metrics is another, but I threw in the word rigorous because

326
00:23:34,360 --> 00:23:38,800
I, a problem that you see a lot is people run an algorithm and then they tweak it and

327
00:23:38,800 --> 00:23:42,960
they run it again or they run multiple algorithms, but they run them in sequence and then they

328
00:23:42,960 --> 00:23:43,960
try another thing.

329
00:23:43,960 --> 00:23:49,160
And so, yeah, it's the machine learning version of P hacking.

330
00:23:49,160 --> 00:23:52,960
And once you start touching the data that's your estimator and I really feel like you

331
00:23:52,960 --> 00:23:59,400
need to, you need to be really upfront about what your estimator is going to be, what

332
00:23:59,400 --> 00:24:04,120
your metrics are going to be and, you know, a single metric is often not going to be sufficient.

333
00:24:04,120 --> 00:24:09,920
You can get a really high accuracy and have incredibly poor true positive rate.

334
00:24:09,920 --> 00:24:14,920
And I mean, and a lot of our, a lot of the problems that we're dealing with, you know,

335
00:24:14,920 --> 00:24:19,880
and a lot of the, unfortunately, a lot of the papers that we see in, you know, clinical

336
00:24:19,880 --> 00:24:24,280
medicine and health services research and health outcomes, now that people are starting

337
00:24:24,280 --> 00:24:29,160
to bring machine learning to this space, a lot of the papers are very much that.

338
00:24:29,160 --> 00:24:38,280
Oh, we got a, we got an accuracy of 0.97 versus the, the parametric regression was 0.95.

339
00:24:38,280 --> 00:24:42,840
And that, that type of a difference, you haven't explained whether it's meaningful, you haven't

340
00:24:42,840 --> 00:24:47,840
looked at any other metrics, you've usually only used a single data set and a single medical

341
00:24:47,840 --> 00:24:52,400
center, and suddenly they make these big broad claims that it can be useful in clinical

342
00:24:52,400 --> 00:24:55,680
practice, which is, is dangerous, is, is frankly dangerous.

343
00:24:55,680 --> 00:25:00,880
And when you put it in a clinical journal, the standards need to be really, really high

344
00:25:00,880 --> 00:25:05,680
for clinical research and, and making claims with machine learning.

345
00:25:05,680 --> 00:25:09,400
And that's, a lot of the work is not what I would call rigorous.

346
00:25:09,400 --> 00:25:14,080
And this would be also be very, very true of the coronavirus when we're putting these

347
00:25:14,080 --> 00:25:20,000
papers out there, they, you know, this, the speed cannot, cannot be an excuse for lack

348
00:25:20,000 --> 00:25:21,500
of rigor.

349
00:25:21,500 --> 00:25:29,920
One of the things you said was that when you apply a model to your data, that's your estimator.

350
00:25:29,920 --> 00:25:32,320
Oh, something along, like, along those lines.

351
00:25:32,320 --> 00:25:34,560
I mean, on that, what does that mean?

352
00:25:34,560 --> 00:25:35,560
Yes.

353
00:25:35,560 --> 00:25:38,840
So start touching your data, that's, that's, that's an estimator.

354
00:25:38,840 --> 00:25:44,160
So you need to incorporate all of that uncertainty into.

355
00:25:44,160 --> 00:25:47,680
What does it mean for that to be an estimator?

356
00:25:47,680 --> 00:25:54,400
So for example, in the, the P hacking scenario, if you run a regression and then you, you

357
00:25:54,400 --> 00:25:57,080
know, change it, run it again, change it, run it again.

358
00:25:57,080 --> 00:25:58,760
So now you've run it three times.

359
00:25:58,760 --> 00:26:02,800
Well, that, those three time, that whole thing is your estimator.

360
00:26:02,800 --> 00:26:06,640
It's not the last regression that you ran, but normally what people would do is they

361
00:26:06,640 --> 00:26:11,320
would publish that last regression they ran with the standard errors based on having only

362
00:26:11,320 --> 00:26:16,080
run that single regression, which means the standard errors aren't correct.

363
00:26:16,080 --> 00:26:20,720
And so the second you start touching your data, the second you run any kind of algorithm

364
00:26:20,720 --> 00:26:23,280
that you need to build that into your estimator.

365
00:26:23,280 --> 00:26:28,120
So if you do a multiple stages of estimation, you need to account for that.

366
00:26:28,120 --> 00:26:31,400
And that's why, you know, for a prediction problem, that's why you should just state

367
00:26:31,400 --> 00:26:37,560
all of your estimators up front, run them all together and, you know, we have both finite

368
00:26:37,560 --> 00:26:41,560
sample and asymptotic properties that allow us to, you know, know that this will have good

369
00:26:41,560 --> 00:26:43,400
statistical properties.

370
00:26:43,400 --> 00:26:51,080
But this, this iterative cherry picking of running algorithms can really disperius results.

371
00:26:51,080 --> 00:26:57,800
In the machine learning community, we often describe the fundamental process as one that

372
00:26:57,800 --> 00:27:04,000
is inherently iterative, where engineering features and trying out models and, you know,

373
00:27:04,000 --> 00:27:06,720
that's, that's just the job.

374
00:27:06,720 --> 00:27:11,040
And how do you reconcile that with what you're describing here?

375
00:27:11,040 --> 00:27:15,720
I think you need to be really clear when your analysis is hypothesis generating.

376
00:27:15,720 --> 00:27:20,000
So if you really are doing exploratory data analysis, be very clear about that.

377
00:27:20,000 --> 00:27:24,840
If you're doing feature selection, if you're trying to discover relationships, if you're

378
00:27:24,840 --> 00:27:28,600
doing something that's unsupervised learning and you're trying to discover groups, we'll

379
00:27:28,600 --> 00:27:29,600
be very clear about that.

380
00:27:29,600 --> 00:27:35,400
If you're then going to use those groups to define some causal intervention, again, you

381
00:27:35,400 --> 00:27:39,080
need to incorporate the uncertainty in how those groups were defined.

382
00:27:39,080 --> 00:27:45,080
So I think the transparency of what you're doing and what the goal is is, is absolutely

383
00:27:45,080 --> 00:27:46,400
paramount.

384
00:27:46,400 --> 00:27:51,240
And if you get into a place where you're then trying to do causal inference, then again,

385
00:27:51,240 --> 00:27:58,560
you can't have this sort of cherry picking iterative style because then our, the reliability

386
00:27:58,560 --> 00:28:02,080
of the results are going to be very flawed.

387
00:28:02,080 --> 00:28:09,280
Causal inferences sometimes presented as a, a tool that solves the kind of problems

388
00:28:09,280 --> 00:28:12,000
you're describing kind of by its very nature.

389
00:28:12,000 --> 00:28:18,360
It's more rigorous in, in some way than kind of the general, the other stuff that people

390
00:28:18,360 --> 00:28:24,800
do, but it sounds like there are lots of pitfalls and opportunities to do it wrong.

391
00:28:24,800 --> 00:28:30,480
Yeah, I think that the same way that there are some people who think that, you know, electronic

392
00:28:30,480 --> 00:28:36,480
health record data is the gold standard data and has no issues, which is false.

393
00:28:36,480 --> 00:28:41,400
There may be some people who think causal inferences, this, this, this, this area that is,

394
00:28:41,400 --> 00:28:48,040
does not have magic and pixie dust and it really, it really isn't.

395
00:28:48,040 --> 00:28:55,040
So again, you need the causal inferences difficult, the underlying research question should

396
00:28:55,040 --> 00:28:57,280
be driving whether it's causal inferences or not.

397
00:28:57,280 --> 00:29:01,200
If you're interested in a prediction question or an effect question, that should really

398
00:29:01,200 --> 00:29:08,200
be guiding what types of techniques you need to use, but that you can have really terrible

399
00:29:08,200 --> 00:29:13,360
causal analyses, the same way you can have really terrible prediction analyses or clustering

400
00:29:13,360 --> 00:29:14,960
analyses.

401
00:29:14,960 --> 00:29:20,040
And the transparency about what your assumptions are, what the limitation of the, of your

402
00:29:20,040 --> 00:29:24,960
data are, all of that is just something that needs to be up front.

403
00:29:24,960 --> 00:29:32,160
And it's something that I, I mean, that we recently had a, a workshop in the fall back

404
00:29:32,160 --> 00:29:38,160
when we, we gathered in groups of people at the, at the national academies.

405
00:29:38,160 --> 00:29:45,520
And I was really advocating for us, we need to have as a research community, you know,

406
00:29:45,520 --> 00:29:52,280
a baseline of Stanford machine learning research, especially in, in, in the clinical medicine.

407
00:29:52,280 --> 00:29:56,200
And when we, when we have prediction research, when you causal inference research in these

408
00:29:56,200 --> 00:30:01,000
clinical journals, the standards need to be incredibly high and at a minimum, we need

409
00:30:01,000 --> 00:30:02,840
to be very, very transparent.

410
00:30:02,840 --> 00:30:05,240
We need to share code.

411
00:30:05,240 --> 00:30:10,760
We need to be very explicit about what the causal assumptions are and when they might not hold.

412
00:30:10,760 --> 00:30:15,920
And creating a culture around this that's much less about having magic flashy results

413
00:30:15,920 --> 00:30:25,120
and much more about genuine discovery and having incredibly clear, probably appendices about

414
00:30:25,120 --> 00:30:26,520
these issues.

415
00:30:26,520 --> 00:30:29,720
But having that be the standard.

416
00:30:29,720 --> 00:30:39,160
Code sharing is, I think just starting to catch on and become a, I don't know if it's even

417
00:30:39,160 --> 00:30:45,200
one would say an accepted practice at the NURPS conference for the past couple of years.

418
00:30:45,200 --> 00:30:51,640
They've, there's been a repeatability effort that has encouraged researchers to submit

419
00:30:51,640 --> 00:30:53,160
papers with code.

420
00:30:53,160 --> 00:31:01,120
I've got to imagine, perhaps less so in the, you know, more traditional sciences, statistics,

421
00:31:01,120 --> 00:31:05,000
medicine, or I don't speak out of turn.

422
00:31:05,000 --> 00:31:06,520
So I'll just say this is my impression.

423
00:31:06,520 --> 00:31:12,000
I am a journal co-editor for one of the journals and statistics that the journal biostatistics.

424
00:31:12,000 --> 00:31:16,160
We are standard is that you have to share code when you publish.

425
00:31:16,160 --> 00:31:19,360
And a number of statistics journals have this now.

426
00:31:19,360 --> 00:31:23,280
So I think we might be a little bit ahead of the machine learning community.

427
00:31:23,280 --> 00:31:28,440
There might be a little bit more buy-in, but that's just my perception.

428
00:31:28,440 --> 00:31:36,800
I would say in the clinical journals, it's very early stages to have that be something

429
00:31:36,800 --> 00:31:40,240
that people think is reasonable and expected to do.

430
00:31:40,240 --> 00:31:46,760
And some of this has been helpfully driven by funders where they require that every project

431
00:31:46,760 --> 00:31:51,320
you might do in the clinical space that you have to share code.

432
00:31:51,320 --> 00:31:55,280
And I think that this is something else that I brought up at that National Academy's

433
00:31:55,280 --> 00:32:01,160
workshop was we need funders, we need journals, the researchers will do it if they're forced

434
00:32:01,160 --> 00:32:02,160
to do it.

435
00:32:02,160 --> 00:32:10,640
And we need to get allies on our side to make that happen because it's not necessarily

436
00:32:10,640 --> 00:32:16,720
going to keep erroneous results from being published, but again, it's that transparency.

437
00:32:16,720 --> 00:32:22,960
And with high impact work, with all of the work we were doing before the coronavirus and

438
00:32:22,960 --> 00:32:27,560
now that we have the coronavirus, when data can be reasonably shared and not all data

439
00:32:27,560 --> 00:32:31,960
can be shared, so a lot of electronic health data cannot be shared because of privacy

440
00:32:31,960 --> 00:32:32,960
considerations.

441
00:32:32,960 --> 00:32:38,480
But many of the data sources with coronavirus can be shared and if it can be shared, it

442
00:32:38,480 --> 00:32:39,480
should be shared.

443
00:32:39,480 --> 00:32:44,760
The same way your code should be shared and that should be the standard.

444
00:32:44,760 --> 00:32:50,840
We started talking about single metrics and leaderboard efforts.

445
00:32:50,840 --> 00:32:55,600
I felt like you wanted to jump in there, we didn't like jump in there.

446
00:32:55,600 --> 00:33:03,840
We did touch on it a bit in my frustration of what seems to be published in not just

447
00:33:03,840 --> 00:33:05,480
clinical journals, but other journals.

448
00:33:05,480 --> 00:33:08,840
And I say this in a critical sense, I mean, if you go back five years and look at some

449
00:33:08,840 --> 00:33:12,920
of my papers, I was publishing a single metric as well.

450
00:33:12,920 --> 00:33:17,280
I know you recently had a podcast about algorithmic fairness.

451
00:33:17,280 --> 00:33:21,120
And so one of the other areas that I work in is algorithmic fairness.

452
00:33:21,120 --> 00:33:26,960
And so not only do we need multiple metrics of global fit, like your accuracy or your

453
00:33:26,960 --> 00:33:32,720
AUCs and your R-squareds, they tell us different things, but we also really need to center

454
00:33:32,720 --> 00:33:38,320
group fit and understanding how particular algorithm might further marginalize already

455
00:33:38,320 --> 00:33:39,800
marginalized groups.

456
00:33:39,800 --> 00:33:42,520
And in the healthcare system, we have many different groups that we already know are

457
00:33:42,520 --> 00:33:46,920
marginalized, individuals with mental health and substance use disorders, individuals in

458
00:33:46,920 --> 00:33:51,040
rural communities, black and African-American individuals, there's a lot of groups that

459
00:33:51,040 --> 00:33:54,880
we need to make sure that if we're saying in a clinical paper, this algorithm should

460
00:33:54,880 --> 00:33:55,880
be deployed.

461
00:33:55,880 --> 00:33:59,760
And again, I already mentioned the fact that there's lots of other issues in that paper,

462
00:33:59,760 --> 00:34:04,960
a single medical center, not using cross validation, et cetera, et cetera.

463
00:34:04,960 --> 00:34:09,040
But they also haven't looked, they're saying use this when they haven't studied the harms

464
00:34:09,040 --> 00:34:10,040
of that algorithm.

465
00:34:10,040 --> 00:34:11,240
What are the potential harms?

466
00:34:11,240 --> 00:34:15,000
And one of the ways that we can assess that is with metrics of group fit.

467
00:34:15,000 --> 00:34:16,000
So-

468
00:34:16,000 --> 00:34:18,680
The current state of that in clinical practice?

469
00:34:18,680 --> 00:34:19,680
Almost nonexistent.

470
00:34:19,680 --> 00:34:28,320
I mean, the concept of studying algorithms for issues of fairness is starting to make

471
00:34:28,320 --> 00:34:33,360
a dent, but when you look at published papers, the vast, vast, vast majority of published

472
00:34:33,360 --> 00:34:36,880
papers in clinical journals do not even consider it.

473
00:34:36,880 --> 00:34:41,520
What's your sense for what it will take to resolve that?

474
00:34:41,520 --> 00:34:46,760
Is it, you know, just time or what are the things that you're doing in the worlds that

475
00:34:46,760 --> 00:34:53,320
you influence and the journals that you mentioned to try to drive the community towards considering

476
00:34:53,320 --> 00:34:55,360
those kinds of metrics?

477
00:34:55,360 --> 00:35:02,800
I'm excited to see more people working on algorithmic fairness in the health space.

478
00:35:02,800 --> 00:35:07,680
This is something that, historically, a number of the conferences in fairness have not had

479
00:35:07,680 --> 00:35:10,200
any papers on health.

480
00:35:10,200 --> 00:35:12,200
And so I had previously submitted some-

481
00:35:12,200 --> 00:35:13,200
Yes.

482
00:35:13,200 --> 00:35:15,720
And so it was only maybe two years ago that I saw the first paper.

483
00:35:15,720 --> 00:35:18,440
It's like, oh, they took a paper in health.

484
00:35:18,440 --> 00:35:24,120
And one of my papers that was recently published in a statistics journal in biometrics on

485
00:35:24,120 --> 00:35:30,000
fair regression for health care spending, that paper was rejected from one of the top

486
00:35:30,000 --> 00:35:32,360
fairness conferences.

487
00:35:32,360 --> 00:35:36,960
And so I had been trying to kind of bring these issues to some of the existing fairness conferences.

488
00:35:36,960 --> 00:35:42,080
I'm glad to see that the last two years, there's more health work at these conferences.

489
00:35:42,080 --> 00:35:48,480
There's also some newer conferences incorporating both people working in health and people working

490
00:35:48,480 --> 00:35:49,680
in fairness.

491
00:35:49,680 --> 00:35:56,520
So I do think that there are a number of people now in the fair, there's a growing consortium

492
00:35:56,520 --> 00:36:01,480
of people who care about fairness specifically in the health space.

493
00:36:01,480 --> 00:36:07,080
And I'm excited to see everyone in that community who's really trying and actively working to

494
00:36:07,080 --> 00:36:15,400
make sure that it gets more attention, especially given the fact that the health care system

495
00:36:15,400 --> 00:36:21,320
is just one of the biggest levers in the country that can have an impact on social policy.

496
00:36:21,320 --> 00:36:28,200
And so if we bring algorithms into this huge system without really vetting them for

497
00:36:28,200 --> 00:36:33,920
these issues, we can make people who are already incredibly marginalized, much less healthy.

498
00:36:33,920 --> 00:36:38,560
And so I'm speaking at a conference this summer, which I'm assuming will be virtual.

499
00:36:38,560 --> 00:36:41,160
One of these conferences, a chill conference.

500
00:36:41,160 --> 00:36:47,760
And I'm really thankful to the organizers who are really pushing forward this endeavor.

501
00:36:47,760 --> 00:36:53,760
And so I'm hopeful, but I'm also concerned.

502
00:36:53,760 --> 00:36:58,840
The paper that you mentioned, fair regression for health care spending, let's talk a little

503
00:36:58,840 --> 00:36:59,840
bit about that.

504
00:36:59,840 --> 00:37:04,840
What are the goals and what is fair regression?

505
00:37:04,840 --> 00:37:07,320
How does one achieve fair regression?

506
00:37:07,320 --> 00:37:09,880
So there were two main goals of this paper.

507
00:37:09,880 --> 00:37:16,640
So the methodological goal was that a lot of the work in algorithmic fairness and both

508
00:37:16,640 --> 00:37:20,520
from methods and definitions really focused on binary outcomes.

509
00:37:20,520 --> 00:37:24,800
And there was very little work in continuous outcomes.

510
00:37:24,800 --> 00:37:29,160
And so a large portion of my work is in health care spending, which is a bounded continuous

511
00:37:29,160 --> 00:37:30,320
outcome.

512
00:37:30,320 --> 00:37:36,400
And so we developed some new fair regression methods for continuous outcomes and also

513
00:37:36,400 --> 00:37:40,120
compared them to the few methods that are existed.

514
00:37:40,120 --> 00:37:44,320
But what are the two groups, fairness or some other type of?

515
00:37:44,320 --> 00:37:45,320
Yes.

516
00:37:45,320 --> 00:37:50,920
Relative to various measures of group fairness and also global fit.

517
00:37:50,920 --> 00:37:58,000
And so what fairness means for health care spending is that the formulas that we use

518
00:37:58,000 --> 00:38:03,080
to pay health plans are called risk adjustment formulas.

519
00:38:03,080 --> 00:38:06,920
And these aim to distribute health care funds based on health.

520
00:38:06,920 --> 00:38:12,840
But unfortunately, health plans can discriminate against groups, including those that are defined

521
00:38:12,840 --> 00:38:15,960
by certain health conditions, I mentioned one group earlier that I've worked on a

522
00:38:15,960 --> 00:38:20,160
lot, individuals with mental health and substance use disorder.

523
00:38:20,160 --> 00:38:26,040
So if these groups are currently costly to the insurer.

524
00:38:26,040 --> 00:38:33,440
So any group that is costly incentivizes the health insurer to discriminate against

525
00:38:33,440 --> 00:38:38,080
them through various mechanisms, like changing which providers are available to enrollees

526
00:38:38,080 --> 00:38:41,280
or the cost of the copay of certain prescription drugs.

527
00:38:41,280 --> 00:38:50,200
So our goal was to try and create regressions formulas that were fairer in the sense that

528
00:38:50,200 --> 00:38:54,160
under compensated groups would be less likely to be discriminated against.

529
00:38:54,160 --> 00:38:59,320
So if we could redistribute the funds within the formulas such that individuals with mental

530
00:38:59,320 --> 00:39:04,960
health and substance use disorders were not massively under compensated, then the insurers

531
00:39:04,960 --> 00:39:11,480
would have less of an incentive to try and change their plans to harm those enrollees.

532
00:39:11,480 --> 00:39:16,480
And we found these regressions performed incredibly well.

533
00:39:16,480 --> 00:39:24,160
We could improve fairness for groups by over something like 98% while global fit was reduced

534
00:39:24,160 --> 00:39:25,480
by maybe 4%.

535
00:39:25,480 --> 00:39:33,760
Just a very small loss of global fit for incredible improvements in group fairness.

536
00:39:33,760 --> 00:39:39,440
As some of my continued work in that space, that paper was written with a PhD student in

537
00:39:39,440 --> 00:39:40,680
a zinc.

538
00:39:40,680 --> 00:39:46,520
We then after that paper collaborated with our economist colleague, Tom McGuire, where

539
00:39:46,520 --> 00:39:51,160
we tried to look at not just a single group of multiple groups and bringing together fair

540
00:39:51,160 --> 00:39:55,760
regression with other types of interventions in the healthcare system.

541
00:39:55,760 --> 00:40:01,520
And because we were concerned, okay, if we help, if we improve under compensation for

542
00:40:01,520 --> 00:40:04,160
one group, what happens to other groups?

543
00:40:04,160 --> 00:40:09,800
And we found that by improving fairness for multiple groups, we picked four groups that

544
00:40:09,800 --> 00:40:15,840
we knew were under compensated and we knew that insurers might have an incentive to discriminate

545
00:40:15,840 --> 00:40:16,840
against them.

546
00:40:16,840 --> 00:40:22,200
By improving fairness for those four groups, we improved fairness for 88% of additional

547
00:40:22,200 --> 00:40:26,160
groups that we didn't even bring into the loss function.

548
00:40:26,160 --> 00:40:30,600
And we were also able to reduce the number of variables that were needed in the formula

549
00:40:30,600 --> 00:40:32,840
by something like 60%.

550
00:40:32,840 --> 00:40:35,560
What's your intuition for why that works?

551
00:40:35,560 --> 00:40:36,960
Why does that happen?

552
00:40:36,960 --> 00:40:41,360
Well, so what it ends up doing is it takes funds away from, so there are people in the

553
00:40:41,360 --> 00:40:45,680
healthcare system that are over compensated because the formula expects them to have more

554
00:40:45,680 --> 00:40:48,280
expenses that they don't have.

555
00:40:48,280 --> 00:40:51,840
So it actually redistributes funds in a very smart way.

556
00:40:51,840 --> 00:40:55,840
So people who are healthy and are currently over compensated, it moves that money to

557
00:40:55,840 --> 00:40:58,320
people who are being under compensated.

558
00:40:58,320 --> 00:41:00,400
And so we're really excited about this.

559
00:41:00,400 --> 00:41:09,000
You mentioned at the beginning of our discussion about am I trying to, is it about understanding

560
00:41:09,000 --> 00:41:10,560
for the influence?

561
00:41:10,560 --> 00:41:18,280
And I've been working on fairness methods and risk adjustment methods for a long time

562
00:41:18,280 --> 00:41:19,280
now.

563
00:41:19,280 --> 00:41:22,800
And we're really getting into a place where we're able to bring all of these advances together

564
00:41:22,800 --> 00:41:29,960
and make recommendations at a certain point to not just in the US, there's many different

565
00:41:29,960 --> 00:41:35,040
health systems in the world that use risk adjustment formulas.

566
00:41:35,040 --> 00:41:42,120
And I'm excited about the potential, again, to have such a potential big impact on a risk

567
00:41:42,120 --> 00:41:48,640
adjustment system that can have a tremendous influence on improving health and social

568
00:41:48,640 --> 00:41:49,640
policy.

569
00:41:49,640 --> 00:41:51,840
That's great.

570
00:41:51,840 --> 00:41:58,160
Before we wrap up any other, any parting thoughts or things that other things that you're excited

571
00:41:58,160 --> 00:42:01,560
about in the space that you're working?

572
00:42:01,560 --> 00:42:04,040
Oh, great question.

573
00:42:04,040 --> 00:42:10,320
I think my parting thoughts would be about reading research in general, but especially

574
00:42:10,320 --> 00:42:16,440
during a pandemic, read it with a critical eye, read things before you share them.

575
00:42:16,440 --> 00:42:20,920
I don't know if I'm naive or it's because I'm an elder millennial, but I think read things

576
00:42:20,920 --> 00:42:25,640
before you share them should really be the baseline.

577
00:42:25,640 --> 00:42:32,600
But please read things before you share them and be careful about, ask questions when

578
00:42:32,600 --> 00:42:37,800
you're reading whether it's a coronavirus paper or it's a flashy machine learning paper

579
00:42:37,800 --> 00:42:39,960
in a clinical journal.

580
00:42:39,960 --> 00:42:41,640
What data source did they use?

581
00:42:41,640 --> 00:42:44,760
Did they talk about any limitations of the data source?

582
00:42:44,760 --> 00:42:46,640
What kinds of metrics did they use?

583
00:42:46,640 --> 00:42:51,840
If they only used accuracy or AUC or R-squared, that's a red flag.

584
00:42:51,840 --> 00:42:53,160
What's the true positive rate?

585
00:42:53,160 --> 00:42:55,160
What's the false positive rate?

586
00:42:55,160 --> 00:42:56,160
What are their conclusions?

587
00:42:56,160 --> 00:42:57,160
Are they overstated?

588
00:42:57,160 --> 00:43:00,160
How many, what populations did they study?

589
00:43:00,160 --> 00:43:01,920
What does it mean for generalizability?

590
00:43:01,920 --> 00:43:05,680
Ask all of these questions, whether it's machine learning research or not.

591
00:43:05,680 --> 00:43:12,440
Just be interested, be excited, but be skeptical and thoughtful.

592
00:43:12,440 --> 00:43:18,960
What I love about your comment is that it doesn't require some kind of secret decoder

593
00:43:18,960 --> 00:43:25,480
ring, years of statistical stats courses or what have you.

594
00:43:25,480 --> 00:43:33,760
It's just asking, being a critical reader and consumer of all the news and articles

595
00:43:33,760 --> 00:43:40,360
and journal papers that you're reading and just thinking broadly about the stuff that

596
00:43:40,360 --> 00:43:42,000
the claims that they're making.

597
00:43:42,000 --> 00:43:43,000
Absolutely.

598
00:43:43,000 --> 00:43:44,000
Awesome.

599
00:43:44,000 --> 00:43:48,720
Well, Sherry, thanks so much for taking the time to share a bit about what you're up

600
00:43:48,720 --> 00:43:49,720
to.

601
00:43:49,720 --> 00:43:51,200
Well, thank you so much for chatting.

602
00:43:51,200 --> 00:43:52,200
Great.

603
00:43:52,200 --> 00:43:53,200
Thank you.

604
00:43:53,200 --> 00:43:59,280
All right, everyone, that's our show for today.

605
00:43:59,280 --> 00:44:05,080
For more information on today's show, visit twomolai.com slash shows.

606
00:44:05,080 --> 00:44:15,080
As always, thanks so much for listening and catch you next time.

