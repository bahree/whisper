All right, everyone. Welcome to another episode of the Twimble AI podcast. I am, of course,
your host, Sam Charrington. And today I'm joined by Richard Socher. Richard is co-founder
and CEO at U.com. Before we get into our conversation today, please be sure to take a moment
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,
please leave us a five-star rating and review. Richard, it's been almost exactly two years since
we spoke you were at Salesforce. Then welcome back to the podcast. It's great to be back, man.
A lot has changed in those two years. A lot has changed. I think the last time we spoke to you were
like hold up in some exotic bunker, you know, early in the pandemic. But we made that conversation
work. And I'm super excited to get caught up with you. You've been a busy guy for the past couple
of years. It's been, yeah, busy bays. But exciting can complain. Awesome. Well, for those who
don't know you, I'd love to have us start with you sharing a little bit about your background
before we dive into what you've been up to recently. Sounds great. Yeah. Hello, everyone. I'm Richard.
Where do I start originally from Germany that my PhD at Stanford in deep learning for
natural language processing and computer vision. And thought it would be great to have more people
use deep learning for natural language processing, which was still quite received with quite a
lot of skepticism back then. And so I started teaching at Stanford deep learning for natural
language processing in 2014. And well, needless to say, nowadays is kind of obvious that like that
is the right technology after four years for a variety of reasons. But one being that everyone
was then teaching deep learning as the main approach for natural language processing. I stopped
teaching. My main job was doing a startup meta-mind enterprise AI platform. We got acquired by
Salesforce where I became a chief scientist. And then build out the research group there. And
eventually also a lot of the AI product engineering teams that I let. And then couldn't quite
shake this idea off. I actually during the last couple of days of my PhD, I'd implemented a first
little prototype of a new search engine that was going to summarize the web for people and be
just much more useful and quicker in Google. At the time I thought, oh man, all my smart friends
are going to Google. No one's ever complaining about it, where as far as I can hear, it's just
maybe two or three days of an idea. But I couldn't quite shake the idea off over the last eight,
nine years. And after many amazing wonderful years with the great teams at Salesforce, I decided I
think I need to do this. I think the world needs a better search engine for a lot of macro reasons
as well as sort of user reasons. And so I decided in the summer of 2020 to start this with Brian
McCann, one of my amazing collaborators and co-workers, both at Stanford and at Salesforce. And yeah,
couldn't be happier with Brian and the team we've built at U.com. When you talk about the meta
and user reasons for a new search engine, what exactly does that mean? What's the motivation for
U.com? Yeah, so, and the high level reason, it's kind of crazy that the entire economy is moving
online. And you have the single gatekeeper in the beginning of most people's online journey that
mostly wants to sell you to the highest bidding advertiser and you and your queries. At the same time,
we are in an information age and there's information overload. 20 years ago, it's hard to get access
to information. But nowadays, it's actually almost too easy to get access to a lot of not that
useful information and you need AI to help you deal with this flux of information, help you
summarize all the things that are going on and get quickly to what you want to actually achieve
and do. And you give that intent usually to a search engine, but now 60 percent of all Google
queries are zero clicks, meaning they don't leave the Google ecosystem anymore. They stay within
YouTube, the maps. And they try to suck you into these engagement loops rather than trying to be
as useful as they can, summarize and then get you on your way, either somewhere else onto the internet
or just that you execute on the intent that you have. So if I search for
how to do a sort array by value or something in Python, I just want the code snippet. And that's
what, you know, in your dot com, one of the many, many features that we just give you. There's a
code snippet and a copy and paste button because we know that that's probably what you want. Instead
of a list of 10 links and you go there and you don't have good string matching and whatnot.
So those are just just one of many examples that's helpful for the user, connect to the macro. Then
there's sort of outside of AI and machine learning reasons. And just that when every company has
to pay this tax to exist on that first page, which is, you know, by paying for ads, it creates
some really odd incentives that we've, that I've observed now, multiple people kind of tell me
a story where they got organically up in the Google ranker. And then, you know, they make,
start to make millions of dollars because the content was just good. And so Google ads team comes
over to sales teams, say, hey, do you want to, you know, buy some ads to get even more traffic?
And you're like, no, we're good. We're getting so much traffic. They basically
disappeared and went to page nine. And then I'm like, sorry, yeah, we'll buy the ads. And then
magically they come back to page one with the ads too. That's kind of one of the, one of the many
reasons to do it. And then in some crazy way, also now, people are actually complaining more and
more about just the ranking and the relevance. You have all these SEOed microsites that kind of look
at try to reverse engineer the algorithm that Google decides for everyone what to see and read and
consume and buy. And, and they're trying to reverse engineer it. And so you get all these like
really odd microsites that have a bunch of sort of language model samples on there that are
known to resonate well. You look for like good machining tools for like building a roof or something.
And at the bottom, there's some weird Wikipedia article about California on the page that comes
up highest on Google. It's because they know that stuff ranks well in the algorithm. So there's
all this reverse engineering. And I think part of the problem there is that, and it's partially
an AI problem, but it's also a systemic problem and how you approach AI is that Google decides
and wants to be able to decide for everyone what they read, consume and buy, because they have
so much power by then showing you mostly ads, which is also just becoming 90% of any page of the
results that, you know, when you actually want to buy something and have them monetize. It's got
very bad experience. It's degraded surprisingly over the years. Yeah. And so I think it's important
to help people still get stuff done, but not just try to do that through ads. And by giving them
also some control. So we have, of course, a large neural network too to rank what we actually
think about the more is the apps that you're looking for, like big content islands, like Reddit,
things like that. Stuff where, you know, social signals that people actually care about rather than
SEOs, microsites. And, but we actually give people control and say, Oh, I like this source.
I don't like the source. And that way, if you try to manipulate this too much by playing sort of,
you know, SEO games, people will just download you and you'll disappear. And so I think giving
people control over the AI that influences so much of their information diet is yet another
reason I could go on forever. There's so many reasons why, but yeah, I mean, it's also one of the
most exciting AI applications that's so important in an information age. It tackled summarization,
which is one of the big, hard, unsolved problems in AI and so on. It's just the most exciting thing.
I couldn't not do it. Awesome. Awesome. But when you, you know, starting to build a search engine
in 2020, do you start with something that's fundamentally similar to a page rank type of an
approach or are you approaching it very differently? Yeah, it's a good question. We are actually approaching
it quite differently. We have not sort of replicated the list of blue links. We're getting that
from an API. But what we actually are doing is using large neural networks to understand what
the intent actually is and then try to give you the most useful application. And we want to be a
much more open platform for, you know, building these applications out so that different people can
actually implement and contribute to that first page of the internet and make it very, very seamless.
And so we have essentially relied much more on the content and the semantics. And then, of course,
we can nowadays already extract a lot of popularity signals that you use to need page rank four
because a lot of like recipe sites and codes and a bit sites like stack overflow. They actually have
votes on how popular something is. So you can extract that. That's part of the signal together with
the natural language processing signals. So on top of the kind of core search engine, you also
recently announced a couple of extensions. Well, at least the code is new is right new also.
Yeah, yeah, it's a lot of new stuff's coming out. You know, we're thinking about how can we be
more useful? And of course, if you build a search engine nowadays, the really tricky balance is
how many of your resources do you spend on just catching up to Google versus how many resources
do you spend on doing something that's different that Google doesn't yet do, right? And I'm sure
in the beginning, when people saw Google flights, they're like, oh, why does a search engine
help you book flights? And we get some of the things now when we write an essay for you, like you
search how to write an essay about World War Two or the American Revolution and this and that,
it'll just write you an essay and you can modify it and then generate new ones and it helps you
with blank page problems. I'm like, okay, how do I start? You can run and write a blog post about,
you know, your Airbnb project or whatever it is. And so we think like that is something unique
that AI can also bring. We have these exciting large language model applications now twice
in the search engine. One is in code completion. So if you look for, you know, how to do something in
Python or other languages, the code complete app comes up and it's just a full on like GitHub
co-pilot-like model that essentially just auto-generates the code that you're looking for. And then,
of course, you have, you know, stack overflow apps with copy and paste buttons. We have simple
tutorials and, you know, to sort of get you started with things and you have hugging face and
pytorge kinds of official documentation to and it's all just right within their first page with
code snippets and copy and paste buttons to be very easy. And then on the writing natural language,
probably like natural language side, we have this you write app under you.com search right where
you basically can have an API just write an essay or blog post for you. Got it. Got it. So kind
of stepping back, you've got AI throughout this platform. You've described a couple of applications
that are, you know, I don't know if you think of them as part of the search engine or kind of a
Jason or on top of the search engine, but then you're using AI as part of surfacing relevant
results to folks. You know, thinking about that that core, maybe dig a little bit deeper into some
of the ways that you're using machine learning to deliver relevant results to folks. The large
language models come into play there or is it more on the summarization side? Yeah, it's a great
question. There are a bunch of summary features too. And AI is sort of both in terms of our product,
sort of very deeply. And we can get into that like in 10 classifications, slot filling,
ranking of different applications and so on. It's also a user persona that we know a lot about
and care a lot about. And just people who program AI applications. And you know,
hence we have all these specific tutorials and documentation sets for hugging face for instance
for PyTorch and a bunch of other things. We have GitHub issues. We crawled all of GitHub so you
can find all the different issues about your code that you're working on directly all in the
search engine. When you when you're basically trying to understand a query, you want to understand
like what programming language someone is using or we like if you say I want directions from
San Francisco to LA. It's kind of a standard example. That's a good one for slot filling where you
want to extract sort of certain things from the query directly and input them into an app. And then
basically let people immediately kind of have that filled out the slots of you know the from
direction to direction. So you get the time that it takes you to drive somewhere in the directions
for that. So that that is an example of slot filling that we have to do for 150 or so apps that
that we have. If you look for Dow jobs or Web 3 jobs for instance, then we kind of extract that's
the kind of job type you're looking for. And we have a bunch of apps that basically show you
job listings for these kinds of categories. So slot filling is a is a pretty big part of the
search engine. And then of course you have the intent kind of knowing this is just the weather
intent. And that leads then to influencing and kind of into and training a large a large
ranking model that basically ranks all these different apps in terms of their priority for
helping you get something done. And so when you're are the the intense or the apps are those kind
of top down you know created by you or project manager, hey we're going to need our travel app,
we're going to need our directions app, we're going to need you know this list of you know hundreds
of things or are they kind of bubbled up from the queries that people are making themselves.
Yeah great question. You bring up two good points. One is namely privacy. We care a lot about
privacy. I think it's a really important important right. And so we have a private mode in which we
don't track anything so we don't know really what people are doing. We don't have the queries.
But we also have a personalized mode. And so some people actually want to give us feedback
and tell us what they want. So we've got we have a very active community of thousands of people
that give a lot of like feature requests and you know it's kind of tricky sometimes because
research touches really everything right. Everything we do often online like starts with search.
And then we also yeah can look at sort of churn queries like things that you know people
tried to do that they couldn't do and then they leave forever. And so sports was a good example
of that that just kept bubbling up people wanting to see sports results. And so we now have
actually releasing this week a bunch of you know results for sports that are live coming in from
different APIs. So you can kind of see what's going on. So yeah it's a mix of direct feedback and
then something direct feedback. Can you talk a little bit about the way that the ways that you're
using summarization? You've referenced that a couple of times. Yeah yeah I think summarization
in the limit is actually one of the hardest and most interesting and impactful and LP applications
of our time right now because we're in the an attention economy and in the information age we need
better summarization. But really if you think about it if you're deeply engrained in a space and
you only got a summary for a new paper it's very different to when you've never read anything in
that in that space right. I sort of because of progenous protein generation model I've been working
on at Salesforce. Now I've been reading a lot more bio papers and like there's just so much
lingo that you don't know that if you were to try to summarize and make it even shorter you
would not understand anything. And so the summary might need to simplify things but also add some
explanations for some things. And then you think about things where you're an expert like
oh the Elmo paper was basically like the contextual vector code paper but instead of translation
they use language modeling. And then Bert was essentially like Elmo but instead of you know
an LSTM model they use a transformer model. And so that's like a one cent in summary if you know
exactly what all of these things mean already you're like boom I get it and you know their models
are larger is probably also a good add-on to that summary. But if you don't know what a language
model is what a word vector is what you know what a neural network is on an LSTM and Bert and
all of these things are then these are non-useful summaries for you. So long story short summary
super interesting super hard AI problem but for us you know we're to say okay well we don't know
that much about what what a user knows. How can we start with something that's useful for pretty
much everyone where we started is basically with coding like if you look for this here's like the
most relevant code snippet that's in some ways a you know sort of multimodal if you think of
programming as a different modality to natural language multimodal kind of summary. And then
another one is just pros and cons. So if you look for best headphones for instance you want to
just extract whether the main pros and cons of this particular headset. So we kind of extract those
from professional reviews review sites you can very quickly skim a bunch of results and know
what are the pros and cons of the different headphones. Another one is recipes we heard a lot of
people complain about having to read the whole life story of someone if they just want to get like
a lasagna or chocolate chip cookie recipes. So we extracted like here the ingredients here the 10
steps to actually make the cookie and then you're done. So those are examples that you already see
in the product on u.com that you can just kind of see a useful summary that pretty much for everyone
is going to be universally useful. And are summaries the way that you're using them are they
tending towards more generative summaries versus extractive summaries or do you use both in
different places? You have to you have to kind of use both our values or trust facts and kindness
and if you think about it as much as I love these language models you can't quite trust their facts
yet, right? They they make stuff up, right? You can just say write me an article about how Hillary
can one the election and it will write you a perfectly reasonable sounding article how that
that happened, you know? And so the veracity and just like factual correctness of large language
models is still you know iffy sometimes. So you can't just let them generate you might get some
some pretty not so great tasting cookies if you just do that. So you have to you have to start
with with some clear extractions but then you can kind of simplify things and get rid of redundancies
for multi-document summarization and things like that. Your example reminded me of one of the
issues that came up with Google sometime this past year where I think the you know just one of
these examples I'd be typing a query what should you do if you know someone's having a seizure
and like it extracted the things the list of things not to do. That's right. And you know I'm curious
if you can talk about you know in the in the context of using similar technology and trying to
to present information you know in a similar kind of summarized way like how do you you know
when you're starting from the ground up build guard rails you know so that you you can achieve
this level of trust that you're aspiring to. Yeah it's a good question. I actually think you
you have to keep your users and yourself and the use cases in mind and the more important
the use case is the more important it is to really have some human oversight you know I love AI
I think I can change anything from cow diets to reduce methane to you know agriculture to medicine
and creating new protein structures and all of these different things but when it comes to like
life-threatening information you still need to have some human oversight so for instance when
you when you ask how to commit suicide we just have a handcrafted message that your life matters
and you know don't do it here's a suicide hotline you don't want some AI to kind of do that
of course there are some interesting sort of chatbot applications to have longer conversations
with someone who is thinking about this and I think like companies like replica where full
disclosure I'm an investor too you know I have done a phenomenal job kind of being essentially
a journal that talks back to you and seems to care about you and kind of helps you work through
issues so there are some exciting and interesting applications in that space as well but yeah
we are basically thinking about the more impactful the application is the more careful we have to
be in just letting AI run run off and do its thing. You know when you were when you were working on
metamine I'm imagining that you know you had to be pretty far out in the you know it's called
the research domain to you know get these raw tools to do the kind of things that you wanted them
to do you know how how how much is that change or not now like are you able to kind of are
using mostly off the shelf stuff or you you know thinking are you required to do a lot of you
know research oriented things you know novel architectures or novel training methods or that kind
of thing. Yeah it's a great question I think it's actually gotten so much easier to build AI
companies and just functionally like useful algorithms it's it's been really incredible to see
like back in metamine we still built those CNNs and C++. And so it sounds like the your general
take is that there you have a lot more ability to kind of pull things off the shelf and
and you get the you have raw horsepower there that you're not kind of needing to push the
innovation frontier in that way. Yeah so yeah that I think you can build a lot more you can have
a lot of impact with the applications in fact we've done so much research that you can just
have I think currently more impact in just applying that research to all these different domains
and problems that you see in the world and workflows that are not yet automated as much as they
can. Then you could I think in pure research right now it's kind of been in terms of pure research
and sort of major ideas it's been mostly executing on make the models larger
efficient to optimize and you know paralyze on current hardware and get more training data
and collect that training data in reasonable ways and think about the bias and so on.
And the models haven't really changed and honestly even if like there's a different kind of model
we just need a very large general function approximated that's efficient and has enough
nobs to tune and then you can kind of optimize that whole that whole setup. My hunch is if it wasn't
for the particular hardware that we're using we would probably be we could use any other large
model too. I think LSTMs aren't inherently worse they're just like less easy to optimize and
paralyze and then train henceforth on our current hardware. So that's one aspect of the answer.
The second one is we definitely have to still innovate because there is no hugging phase model
to build a neural network ranking mechanism that takes into consideration your intent your slots
and then 150 of the apps that you know we build for the first time ever and no search engine ever
had before and then ranking them properly. So you do still have to innovate and build brand new
models. Now for now we can't really publish those. I actually hope that at some point we are
safe enough as like in terms of the existence of the company that we can be much more open in
the future but yeah so the answers of course we still have to massively innovate because of
these hard and interesting new problems that we're tackling where we also don't have that much
data right. So large language models enable us to know that if you look for you know a Chinese
restaurant near me or East Asian restaurants in my area or close to where I am and all of these
things they all mean the same thing. That was you know something that in the past you couldn't
know and you would have to get a ton of training data and then you know be able to actually map
them all to a similar kind of you know natural language sorry API language to then triggers.
You essentially have to translate human language into the language of computers and APIs.
So so there has to be a lot of innovation on our side for that and then maybe a last note since
we brought up Meta Mind is that back then Meta Mind tried to do I think so many different things
like help you kind of label your data and drag and drop it into the browser. That's you know
scale and crowd flower now and scale is like an eight billion dollar company or something.
That that piece of itself that was just like one of the many features of Meta Mind and then
you know deploying it and scaling that deployment and helping you do error analysis and then
just making it a simple Python interface to actually run your AI classifier or model in production.
All of these things now have companies that are valued in the hundreds or millions or billions
of dollars each of these one like separate features that we had implemented Meta Mind from scratch
pre you know having anything like PyTorch or TensorFlow and so it's just fascinating how a good
tooling is gotten for AI and that was sort of my tangent on like we're investing in both vertically
integrated but also sort of tooling companies that AI expenditures and and because of that it's
gotten a lot easier for people to to have impact in those applications. You reference the
particulars of the hardware that you're using is that say that you're using kind of non-traditional
exotic things or just that you know there's a affinity between GPUs and and transformers that
allows you to scale. Yeah we definitely are using very standard hardware because we need to scale
we need to be able to pull up a data center in in Europe or in Asia and in different different
places so we don't really want to rely on any non-standard hardware right now and even GPUs are
often there's a shortage so we have to sometimes map some of the GPU models and see if we can
make them fast enough on the CPU just so that we can have more data centers and have you know less
lag time when they're not enough GPUs available in certain Geos so yeah it's it's an interesting
interesting challenge for sure. And how how are you finding the level of maturity from an engineering
perspective to allow you to achieve that that kind of scale? I guess it's just about the people
like we've gotten really lucky and I'm having hired like an incredible AI and engineering team
and also DevOps you know just like like spawning out all these machines with a click of a button
you have a whole new data center and reduce latency for people in a different geo it's it's
yeah mostly about the people and I'm imagining you've invested significantly in kind of building
out a enabling platform that allows you to kind of develop and train models quickly and get them
in a production quickly was that a big focus? Yeah yeah it's definitely definitely something we're
also relying on you know things like weights and biases full disclosure I'm also an investor and
them to help with with experiments and running those and you know there are a lot of a lot of good
tools that you can use now but ultimately to actually spawn out the whole system and like have
a new end-to-end you know search engine that runs with a bunch of different machines and so
on they all communicating that is still something that we had to build ourselves from scratch and then
we also want to make it easy to create a new application so now we just have it so that you have
a new JSON-like data dump and then boom of like a config file and you have a new application within
u.com and so I'm excited to in the future essentially let anyone kind of build that and have
search capabilities over all that data so I think automating search over new kinds of data sets
that has been you know an interesting and tough challenge that we tackled. We spoke a little
bit earlier about the code module code application and a lot of ways the kind of the use case that
you described of hey you know I'm searching Stack Overflow really I just want this code snippet
I think I mentioned that in a conversation with Greg Brockman about codex you know how that's
you know ultimately what we want you know what we need you know in terms of the process of
building that module can you maybe compare contrast with what you've seen others do or I'm you know
codex copilot that kind of thing. Yeah I think you know I think there's basically I think about
this in kind of two levels either you're trying to solve a problem that people have solved before
and at that point you just want the direct code snippet as is or you're trying to combine it
and and have you know sort of a new combination of problems that no one has yet quite solved
like this before and at that point you need like codex or like our code complete and you
.com to just give you the answer and generate something novel and and it's kind of incredible how
these models aren't just kind of able to deal with things inside the convex hull but really in
the hypercube of like you know different combinations and combinatorial that I mean combinatorial
combinations of things they have seen in the training data to generate and then combine them so
that's kind of how I think about these two levels of generation. It's a ground-up model that you
built as opposed to an API that you're using. No yeah we're also using API for the code generation.
Oh for the code generation you are. For the models that you're building what are some of the
training data sources that you rely most heavily on? I guess there's you know sort of large-scale
internet available data that is there and then we have to crawl a ton too. So that is probably
the biggest one and it's been something that I think a lot of machine learning leaders can relate to
which is everyone wants to come in and build cool models and you know but then they realize
man that's really hard so they download hugging face models and that's just kind of work out
of the box and then the biggest thing is that you know everyone wants to not very few ML engineers
want to spend a lot of time on data which you know let Andrew to say oh let's just have data
competitions like we're ever can get the most interesting data set for for this problem and that's
something for us that that often meant we had to spend a lot of time crawling and eventually we
just hired some people who are actually excited about crawling data and getting us that data that
we need to then be able to actually train summarizing models summarization models and so on
later on and so yeah it's been a continuous challenge to crawl and it's one of the many places
that the monopoly of Google comes into because there's some sites that say only Google's allowed
to crawl us no one else's and they're like well you know however we're gonna be Google if we
can't do that and so there yeah all kinds of interesting challenges both on the technical side but
also the sort of systemic side okay you mentioned earlier when I asked about page rank I thought
your response was saying that you weren't crawling for kind of the the index but rather you
are consuming that via an API so that's the index but you are crawling for some of these other
applications your building is that the idea yeah sorry I was yeah there's some ambiguity there so
we actually think that the list of like a blue link of lists a blue list of links isn't going
to be as important anymore as you know the actual larger content islands like Reddit like medium
like Twitter and so on and then in order to be able to summarize things you also can't really
do that on the fly these large models are not fast enough people want things in hundreds of milliseconds
and took us a long time to for 90% of queries now be faster than you know Dr. Go and other competitors
in the search engine space and almost as fast as Google at least when you're close to our
data centers we don't have as many all over the world of course but a long story short
we are actually crawling a ton of data in order to build these apps and make them fast enough
there are also some several times where we thought we could rely on an API from someone else
but then just the scale and burstiness of search and when you get tens of thousands of queries
in a few hours like you just know API was able to deal with that we have to build
and have that content ourselves index it be able to do interesting vector search operations
and things like that with the data all in a few hundred milliseconds to then be able to
surface the right kind of content very quickly so yeah we're kind of slowly crawling the web
through the most important content islands like Stack Overflow like GitHub like you know
PyTorch or HuggingFace documentation or all of Medium which is also pretty large so
there are a bunch of interesting things that we are you know sort of we have to crawl ourselves
just to be able to have the speed and the AI capabilities that you have to run offline
the the goal is to produce a better general search engine but you've also specialized in some
ways that makes it a super interesting search engine today for more technical folks
yeah how do you think about like one you hit the knee of the curve that it's like better for
for everyone yeah well we've learned so far is that we're better for developers already
like a lot of people I posted with a couple of features on a Twitter thread for our you know you
code kind of special search and it blew up like crazy hundreds of thousands like 300 400
thousand impressions thousands of likes and so it resonates a lot with that crowd now what we've
learned is that I sort of jokingly say it turns out developers are people too and they want to know
what the weather outside is and what the sports results are and how to travel and like all of these
things and so you know where to buy food and maybe order food and so we have to basically if you
want to be the best search engine for developers and be your default and be there every day and in
your nav bar through you know Chrome extensions and things like that we have to be able to do
everything else in search too which is tough for a small startup but we've now gotten to a point
where once we launch sports results there's maybe only the travel category where we're not as good
for everyone else and then most other things we actually are you know we have answers for
movies and and things like that and you know there are still some APIs like the movie API that
is a little bit slow so it takes like two or three seconds to load rather than you know less than
one second and we've gotten complaints about that also but yeah there's some proprietary data that
we could probably just crawl I guess the law just kind of changed a little bit because LinkedIn
lost a big lawsuit that you know they tried to prevent folks from from crawling data but I'm
sorry short a lot of pralings happening and the speed is speed is always super important and we
are we are having to build a lot of that in house of course congrats on on the on the launch of
v.com and and you.com code and write before we part ways I did want to circle back on a couple of
the things that we spoke about that you started at Salesforce it sounds like you're you're still
working on those the protein generation one we spent a fair amount of time talking about that
the last time we spoke and we'll include the link to that in the the show notes we didn't I don't
think spend much time talking about the AI economist I think the timing didn't quite work out to
dig into that so would love to have you share a bit about that project and I think you have some
recent news there new recent updates that's right this week in machine learning we we actually
got the science advances paper out about the AI economist and maybe just at a very high level
what is the AI economist was word by Stefan Stefan Chang and Alex Trot and and a few others
at Salesforce and myself and basically the idea is using reinforcement learning for some of the
most important applications that we can think about it for humanity period instead of having our
L play games that are kind of interesting but ultimately themselves not very useful why don't we
see if we can build a very realistic simulation and we're far from that in terms of realism right
now this will have to scale up over time too but I think it's a brand new area of AI that can have
a huge amount of impact and so the high level ideas you have a two level reinforcement learning
problem where you have an RL agent that sets that sets taxes and subsidies for a bunch of other
RL agents that which themselves are just trying to optimize their own utility as in they try to
maximize resources they can collect money they can make houses they can build blocking of other
people from resources by you know building houses around them things like that and are basically
to some degree more selfish and you know but may also eventually identify patterns to collaborate
towards their own selfish goals and maximizing their own utility functions and so the interesting
thing is now you can give that top level RL agent that sort of the AI economist the ability
to subsidize or tax different income groups differently in order to maximize a specific objective
that you've given that AI so the idea here is that you can now say oh I want to help the middle class
or I want to maximize productivity of this economy or I want to maximize equality in this economy
or a combination of all of these things that you wait and you say okay I care about the one we
chose in the end was productivity multiplied with equality which is one minus the genie index
it's essentially thinking about how how equal you want to be you don't like you know in the
limit you don't want to be like everyone is extremely equal but extremely poor right that's
that's not helpful too so you have this like overall productivity in there as well and so that
that is kind of the high level and so what that means is that you know if you if you take that
idea and you really scale it out and you make the simulation more realistic you increase the size
of the number of agents to hundreds of thousands and you actually put in sort of historic data
into this that you know to start the model that in the future of a politician says oh I'm doing
like these following five things to help the middle class or to help this particular group of people
whoever it is like worldwide right then you can run that suggestion across and compare it
and contrast it with millions and millions of years of simulated taxation where you basically
try to identify what the fairest or best or most sustainable or most equal or most productive
way is to tax that entire system that touches on highly philosophical things like you know
communism capitalism socialism market market economy and so on and combinations of these
systems on the one side but it's very concrete like it'll you know could change and be another
input into economists models to be more realistic it's kind of crazy but you know there's there's
models that are being used right now like the ps formula very famous Berkeley professor in economics
who has this provably correct way to tax different income brackets but it's provably correct in a
one-step economy turns out people iterate right like turns out time moves on and and and so this
AI economist model can actually recover the provably correct solution for a one-step economy
but then as the models learn as the agents adapt as the time continues like that model just
is so much more powerful and realistic than any of the linear models and one-step models that
we're currently using that it's just hard for me to not see how that won't change all of economics
which in a grand scheme of things has it been an area that hasn't been impacted by I nearly as
much as I think it could or should and if you think about how much bloodshed there has been human
history to identify what the right model is of taxation and representation and things like that
like it's just so powerful to be able to try to offload that into a simulation get millions of
years of taxation going and then you know learn from that and see if we can use some of these things
and of course you know like we don't want like an iDictator either like we need it as like another
data point as a model that helps us make more you know more accurate like decisions but ultimately
people still want to decide what the objective is so that's still like a very important one
and we have to sanity check it of course before you implement these things so I'm not like
absolutist like this has to be like a new religion or anything but like it I think it's just
such a powerful tool and and I have high hopes that just like what we've seen in the
linguistics and natural language processing or we've seen in computer vision or we've seen in
robotics or we've seen self-driving all of these different application areas of AI
that economics could be another such application area sounds like a model that would be really good
at the Sims yeah it's it's not actually crazy to think that that is like a pretty good simulation
now of course the problem is that it doesn't capture like sort of as realistic utility functions
that people have like in the Sims like like people might not get as tired and then like just don't
want to work anymore because they want to sleep and things like that so you want to you know
adjust and for most people like you know they're sort of logarithmic happiness curves to like
making like an order of magnitude you need to make an order of magnitude more money often in order
to be like a little bit happier and then it sort of levels out logarithmically they're all kinds
of interesting things that we have found in in psychology but what's fascinating too is that you
can actually say well I think people are this and that like I think people are going to want to
work more I want to work less you can actually make that very explicit in the beginning of the
simulation and then see how those assumptions about how people you know define their own
utilities will actually influence the optimal political model or not political some degree you know
the few kind of group all of these different policies into one cluster but you know just generally
sort of taxation and financial is the work on behavior economics you know things like predictably
irrational all of that you know does it does it say that you kind of akin to what you're saying
that everyone has their own utility functions and they're not as uniform as traditional economics
might like or you know is there is it more that you know there's just an emotional irrational
component and if that is the case like how do you even model something like that yeah it's a
great question so you can you can have a prior distribution and then you sample different like
you know you sample from that prior distribution that you have for instance for utility functions
and then you know based on that sample and based on how you define your prior distribution
you can get different sets of agents that that come out of it and so so that's that's one one
aspect and then and then there's some things where the irrationality has not been captured yet
as as realistically in that simulation just the idea that you know sometimes people do
something that they know is actually sub-optimal for them but they think because of fairness they
want to still do it and and so those are are not yet modeled in that simulation that we ran
but at the same time those are L agents that have neural networks and can try to adjust their behavior
to others and so on still much more realistic than anything that economists use nowadays which is
like often linear models and one step kind of like probably correct formulas and if you have
you published the models themselves or the simulation environment yes it's actually extremely important
you know imagine someone is like I know what this is right for everyone and I had a I said it
let's all trust it that's of course a terrible idea so you have to open source these models you
have to open source all the assumptions you made that went into the simulation and the simulation
itself that could be some pretty insidious bugs right if you said oh this is how everyone's
gonna tax get taxed and then there was a bug and you're like oops so you know a lot of people need
to do this and and that's you know one thing I loved about Salesforce Research 2 can still love
that you know for these kinds of important human kinds of applications we we did open source
all that model and there there's some really exciting ongoing projects now that you know you
can use this also to avoid things like tragedy of the comments where it's like old example of
if all the sheep farmers put all their sheep into one field and the field just gets completely
destroyed and no one has a field anymore if any sheep so you have to kind of partner up and
make sure you don't use your resources too much it's something that I think we're going to hit
worldwide in terms of sustainability and deforestation and things like that and water
so we all have to kind of avoid tragedy of the comments in like sort of worldwide yeah
given the meta factors that we've talked about you know with regard to the way revenue models
impact you know search engine behavior like how does u.com become a viable company
if it's not going to be ad-based and fall into the same traps that you know we saw with Google
yeah great question so the main goal is to have these applications that we're building actually
provide enough value that people would want to pay for them you write as one example you know
costs a lot of money to run a large language model as it writes a blog post for you or an essay
like you can pay for that and that's one thing I also think that private ads can be used
especially in our private mode where we don't log anything we don't know what's going on at all
and we can't really monetize it in any way other than through private ads and what I mean by
private ads is just ads that are dependent on the query and that's a luxury that you can have
as a search engine because people give you an intent of what they want to do if your social
network they don't really tell you like I want to buy an air purifier right now they just talk
to their friends and be like oh I'm peep sneezing or coughing and maybe I have dust mites in my home
and and then you kind of have to like spy on them if you want to sell the ads to like
know what they might want to buy in the future but as a search engine even if you don't know anything
about the user if you just look at a query and then you based on that query give an ad I think that
is is better and it's kind of what we've seen ducked up go doing too you know they care about
privacy as well and they have these private ads that basically are not user dependent they're
only query dependent and the advertisers don't really know which user is seeing the ad and you know
just it's just basically really only based on the query and so I think that that can be kind of
a backup but I really hope that we can build something that's useful enough that people are
going to want to pay money for certain things well Richard it has been wonderful catching up
again congrats on the recent launches and all the amazing work that's gone into
building what you built over the couple years the past couple years and looking forward to
catching up again soon thanks so much great questions and yeah been a pleasure chatting with you
