WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.120
I'm your host Sam Charrington.

00:34.120 --> 00:39.920
As we approach Twimblecon AI platforms, I'd like to let you all in on our first major announcement

00:39.920 --> 00:41.760
from the conference.

00:41.760 --> 00:46.600
Now you all love this podcast for great guests and interviews and we're bringing that concept

00:46.600 --> 00:48.760
right to the Twimblecon stage.

00:48.760 --> 00:54.720
I am super excited to announce that Andrew Eng will be joining me on stage at Twimblecon

00:54.720 --> 00:57.720
for a live keynote interview.

00:57.720 --> 01:02.320
Many of you know Andrew from his work at Stanford, Coursera or his many other efforts in the

01:02.320 --> 01:06.800
industry including recently founding deeplearning.ai.

01:06.800 --> 01:11.480
Andrew and his work have been super impactful on my life and career and I know that's the

01:11.480 --> 01:13.920
case for many of you as well.

01:13.920 --> 01:18.880
In our conversation we'll be discussing the state of AI in the enterprise, the barriers

01:18.880 --> 01:24.000
to using deep learning and production and how to overcome them, his views on tooling and

01:24.000 --> 01:30.640
platforms for efficient AI delivery and other topics from his recently published AI Transformation

01:30.640 --> 01:32.440
Playbook.

01:32.440 --> 01:36.920
Be on the lookout for more great speaker announcements rolling out over the course of the next few

01:36.920 --> 01:37.920
weeks.

01:37.920 --> 01:39.920
You don't want to miss this event.

01:39.920 --> 01:47.120
Get your tickets now at twimblecon.com slash register.

01:47.120 --> 01:50.280
Alright everyone, I am on the line with Stephen O'Diibo.

01:50.280 --> 01:55.640
Stephen is the founder, CEO and chief software architect at Retina AI.

01:55.640 --> 01:58.840
Stephen, welcome to this week in machine learning and AI.

01:58.840 --> 01:59.840
Thanks so much Sam.

01:59.840 --> 02:02.680
It's such an honor to be on the show here with you.

02:02.680 --> 02:04.760
It's great to have you on the show.

02:04.760 --> 02:13.200
So you are an MD, a medical doctor, in addition to having a background in math and science

02:13.200 --> 02:15.160
or math and computer science.

02:15.160 --> 02:20.440
Can you elaborate a bit on your background and how you kind of made your way into working

02:20.440 --> 02:22.000
on artificial intelligence?

02:22.000 --> 02:23.000
Yeah, most certainly.

02:23.000 --> 02:28.520
You know, as you said, I'm a physician, I'm an ophthalmologist, a Retina specialist,

02:28.520 --> 02:33.040
computer scientist, mathematician and a full stack AI engineer.

02:33.040 --> 02:39.160
And you know, it all, it's sort of been a journey that I've really enjoyed along the

02:39.160 --> 02:40.160
way.

02:40.160 --> 02:43.720
There wasn't sort of a grand plan at the beginning to sort of do all of this.

02:43.720 --> 02:48.680
But yeah, I started out at University of Alabama in undergrad and I was a math major, I really

02:48.680 --> 02:49.880
enjoyed that.

02:49.880 --> 02:55.120
But the entire way I was a pre-med student, so I was med school bound.

02:55.120 --> 02:58.640
And the program I was in was an NSF funded program called the Fast Track Program.

02:58.640 --> 03:01.720
So I got a master's degree in the course of that.

03:01.720 --> 03:08.120
And it was, it was a phenomenal place, you know, great mentorship.

03:08.120 --> 03:15.520
And it was, and I would say a little bit of a, however, it was a pure math, which it's

03:15.520 --> 03:17.120
a, however, it's also a great advantage.

03:17.120 --> 03:20.480
But I found that I had some work to do when I graduated.

03:20.480 --> 03:26.880
So we were, in the course of the program, focusing on abstract algebra, topology and things

03:26.880 --> 03:27.880
like that.

03:27.880 --> 03:32.880
And I remember probably halfway through grad school, through the master's part of it,

03:32.880 --> 03:36.320
looking at a friend of mine and saying, you know, where, where the heck did three go and

03:36.320 --> 03:37.320
where's 15?

03:37.320 --> 03:38.320
You know, there were no more numbers.

03:38.320 --> 03:43.920
It was all, whatever happened to 71, you know, I haven't seen that number in so long.

03:43.920 --> 03:48.360
You know, it was all symbols, you know, you know, the wheels and the complex and so on.

03:48.360 --> 03:54.400
But from there, I went to medical school, went to Duke and was in a MDPHD program, started

03:54.400 --> 03:55.400
out.

03:55.400 --> 03:58.880
And I went to the biochemistry department, worked with Bob Lefkowitz, who ended up with

03:58.880 --> 04:02.120
the Nobel Prize in chemistry in 2012.

04:02.120 --> 04:04.640
And you know, I enjoyed the, by the time of the lab, it was phenomenal.

04:04.640 --> 04:06.160
I was two years there.

04:06.160 --> 04:11.240
But somewhere in that course, I had sort of a, an awakening during the grad school part

04:11.240 --> 04:13.320
of my med school journey.

04:13.320 --> 04:20.520
And, and it, I, I, it became clear that I sort of wanted to continue what had started

04:20.520 --> 04:22.920
at in Alabama with the math.

04:22.920 --> 04:27.840
And it, it was clear to me that it wasn't going to happen with the traditional pathway

04:27.840 --> 04:29.880
in the way biology is done.

04:29.880 --> 04:33.960
Biology is done sort of like looking for a needle in a haystack, you know what I mean?

04:33.960 --> 04:35.440
In what sense?

04:35.440 --> 04:37.440
It's less systematic.

04:37.440 --> 04:39.880
I think some of that is going to change.

04:39.880 --> 04:49.240
It's the notion of there are certain people who, by virtue of their position or insights

04:49.240 --> 04:54.280
and prior success, judgment, whatever you want to say are, are able to pick the problems

04:54.280 --> 04:56.120
that are interested.

04:56.120 --> 05:02.840
And there's a lot of arbitrariness to that process and, and craft the story around it.

05:02.840 --> 05:09.600
And something, something about that felt in, in complete, you know, to me, I, I, I wanted

05:09.600 --> 05:15.920
a, I wanted, I, now looking back, I think I wanted the era of big data where we could

05:15.920 --> 05:20.320
more systematically search for things, and we could more systematically draw conclusions

05:20.320 --> 05:24.520
and have more faith in the conclusions that we draw in science.

05:24.520 --> 05:30.520
Does that speak broadly to kind of the difference between the way we approach science generally

05:30.520 --> 05:33.560
and more of an engineering type of an approach?

05:33.560 --> 05:38.920
It's, it's not even science, old science procedure because the engineering thing has changed

05:38.920 --> 05:40.120
too, right?

05:40.120 --> 05:46.360
In the last five years, for example, in, in machine learning, there was the era of feature

05:46.360 --> 05:53.200
engineering, which was more along the lines of the way biology was done is you manually

05:53.200 --> 06:01.400
look for features and manually craft them even, even design them explicitly yourself versus

06:01.400 --> 06:07.160
using an optimization type approach where you're letting the data tell its own story and

06:07.160 --> 06:12.520
you're sort of sitting back as a judge, as opposed to you're the script writer.

06:12.520 --> 06:14.560
Oh, got it.

06:14.560 --> 06:16.120
Interesting analogy there.

06:16.120 --> 06:20.160
So anyway, I, you know, I had, I had that, which coincided with the spiritual awakening

06:20.160 --> 06:25.000
as well, and I decided, you know, I have to go back to, I, so I decided to go to the

06:25.000 --> 06:29.680
computer science department, and you're not at that point, and I switched and, um, and

06:29.680 --> 06:35.960
I started to re ignite the math there with numerical analysis, and I, I had worked to

06:35.960 --> 06:40.480
do because my flavor of math was pure and abstract.

06:40.480 --> 06:44.720
And here I was realizing that, um, if I was going to impact the world, you have to sort

06:44.720 --> 06:47.160
of go through the computer.

06:47.160 --> 06:48.800
And that means hard numbers.

06:48.800 --> 06:56.720
That means matrices, you know, that means matrix matrix multiply, right?

06:56.720 --> 07:00.520
So, you know, that was a blast, you know, I would say that was probably the hardest part

07:00.520 --> 07:06.360
of my journey was, was getting, getting into, back into the game in terms of math computer,

07:06.360 --> 07:09.840
because I was having to really pick up new skills as well, you know, and that was a

07:09.840 --> 07:14.320
Duke also, you know, the computer science department, I was going to get a PhD there in computer

07:14.320 --> 07:15.320
science.

07:15.320 --> 07:18.760
I, I, um, passed the qualifying exams, et cetera, et cetera.

07:18.760 --> 07:23.960
And I acquired all my skills, had to fall in out with my advisor, um, and, and then

07:23.960 --> 07:24.960
I moved on.

07:24.960 --> 07:27.400
At that point, I got the master's, no problem.

07:27.400 --> 07:31.720
And, uh, and went back to medical school, uh, and I, but then I had just one more year

07:31.720 --> 07:37.920
to finish med school, uh, finish that and, and then stayed at Duke for my intranier, uh,

07:37.920 --> 07:42.200
and, uh, and by then I was back in the game, you know, clinically went back, went to Howard

07:42.200 --> 07:45.720
for my ophthalmology residency, uh, which was a lot of fun.

07:45.720 --> 07:47.240
At that point, I was already committed, right?

07:47.240 --> 07:50.680
I was already, I already knew that I have to keep doing this math thing.

07:50.680 --> 07:52.480
You know, I've paid enough of a price.

07:52.480 --> 08:00.280
It was, it was never gonna, I was never gonna let it go, uh, and so even during the course

08:00.280 --> 08:04.920
of my residency, I wrote a book on quantum mechanics, for instance, uh, and then, uh, went

08:04.920 --> 08:08.040
to the University of Michigan for a fellowship in retina.

08:08.040 --> 08:12.560
And I, I loved Michigan, you know, I visited an arbor, uh, it was a really intellectual

08:12.560 --> 08:13.960
mecca was the way that I saw it.

08:13.960 --> 08:17.160
And there was something really pure about the place in terms of what people at least

08:17.160 --> 08:24.040
aspired to, um, sort of a, they, they really, they really took, I felt that they really

08:24.040 --> 08:31.680
took, um, um, academic pursuit as a primary gain, as opposed to something for a secondary

08:31.680 --> 08:36.080
gain, for recognition or money or whatever, um, at the end of the day, we're all human,

08:36.080 --> 08:38.600
you know, and there's always that going on, right?

08:38.600 --> 08:42.000
There's always the politics and there's always stuff, you know, but I felt Michigan stood

08:42.000 --> 08:47.040
out in terms of its, uh, it's, uh, the genuineness of its commitment.

08:47.040 --> 08:50.320
And so I, it was my first choice, you know, and I could, coming down to Howard, I could

08:50.320 --> 08:51.640
have gone anywhere in the country.

08:51.640 --> 08:56.120
Um, so I went to Michigan as my first choice for fellowship, um, and it was really good.

08:56.120 --> 09:02.880
I wanted to stay in the academy at that point, you know, but, um, the irony was during

09:02.880 --> 09:06.600
the course of my time there, it started to dawn on me that if I'm really gonna be able

09:06.600 --> 09:11.360
to make a difference from really gonna be able to use computer, science, math, I didn't

09:11.360 --> 09:14.680
know exactly what I wanted to do at that point, but it dawned on me that if I really was

09:14.680 --> 09:20.440
going to do that, it was sort of a real epiphany, a surprise that I, I needed to sort of craft

09:20.440 --> 09:21.440
my own career.

09:21.440 --> 09:25.320
I really had to go into private practice to become a true academic.

09:25.320 --> 09:29.120
I couldn't do it at the university because there were these programs that were set up,

09:29.120 --> 09:33.800
you know, people said you have to fit into this box, right?

09:33.800 --> 09:38.400
And there was not a, a sense of, uh, and I, you know, that's just the way it is, you

09:38.400 --> 09:41.480
know, there was not a sense of, you know, go do whatever you want.

09:41.480 --> 09:42.480
Nobody says that.

09:42.480 --> 09:43.480
Right.

09:43.480 --> 09:44.480
Nobody wants to do that.

09:44.480 --> 09:46.760
And I, I get it, you know, so I picked the job.

09:46.760 --> 09:47.760
I went to Iowa.

09:47.760 --> 09:52.560
I worked three days out of the week, um, and it being private practice, you pay your, you

09:52.560 --> 09:53.560
pay your own salary.

09:53.560 --> 09:56.320
You wouldn't, you, and so it was no problem.

09:56.320 --> 09:59.800
And the rest of the time I did academics on my own.

09:59.800 --> 10:03.320
And there I wrote a book on a finite group theory, started getting more into the peer

10:03.320 --> 10:09.640
and theoretical part, I've always sort of oscillated between, um, the, uh, applied concrete

10:09.640 --> 10:19.800
real and the celestial ecclesiastic, the clouds, you know, I've always had a gun between

10:19.800 --> 10:20.800
the two.

10:20.800 --> 10:22.120
And so I was all the way in the clouds.

10:22.120 --> 10:26.400
You know, when I was in Iowa and I wrote that book on finite group theory, um, and then

10:26.400 --> 10:31.420
at that point, my brother, uh, David, David O'Dybo, who is co-founder of, uh, and a

10:31.420 --> 10:35.920
little AI, you know, he was ranked 70th in the world, uh, in Kaggle.

10:35.920 --> 10:40.160
Um, and so he started, he started telling me because I had left grad school at that point,

10:40.160 --> 10:43.600
you know, and he started telling me that there's this thing called deep learning.

10:43.600 --> 10:44.600
Hmm.

10:44.600 --> 10:52.800
I, I hear I was who I was basically a guy, a PhD holder in effect in computer science,

10:52.800 --> 10:58.880
who finished and left the computer world in 2010, and I had never heard the word deep

10:58.880 --> 10:59.880
learning.

10:59.880 --> 11:00.880
Right.

11:00.880 --> 11:01.880
That's a duke for that matter.

11:01.880 --> 11:02.880
Right.

11:02.880 --> 11:06.440
Um, there was no such thing, you know, the whole thing about AI, so on and so forth.

11:06.440 --> 11:11.880
There was nothing like that machine learning was the most boring class you can think about.

11:11.880 --> 11:13.880
It was horrible.

11:13.880 --> 11:20.440
I made it with like, uh, you know, Mark of chains and so on and things that are now so exciting,

11:20.440 --> 11:26.240
the most exciting things in science were really, you know, we're really in the cover.

11:26.240 --> 11:31.200
I mean, it's very deep inside of textbooks, they were gathering dust.

11:31.200 --> 11:35.360
And so anyway, my brother told me that this thing is so exciting what he's doing because

11:35.360 --> 11:38.880
he was in grad school at the time, you know, at the University of Alabama, I had left and

11:38.880 --> 11:42.880
then he had, he had been working, uh, he's a year older than I am and he had gone back

11:42.880 --> 11:44.720
to grad school and he said he's doing deep learning.

11:44.720 --> 11:45.720
It's so exciting.

11:45.720 --> 11:48.720
He said he's doing these things and he kept talking about it and you know, eventually

11:48.720 --> 11:51.800
one day at, uh, Christmas, uh, I was over it.

11:51.800 --> 11:57.360
We were spending the Christmas at their place, um, in, in Birmingham and, um, you kept

11:57.360 --> 11:58.560
talking, I said, okay, let me take a look.

11:58.560 --> 11:59.560
What is this?

11:59.560 --> 12:01.840
And he said something to do with images and image classification.

12:01.840 --> 12:04.480
And I said, you know, that's what I do in my work.

12:04.480 --> 12:10.000
Like I basically look at a picture, depending on what the picture tells me I go, uh, and

12:10.000 --> 12:15.440
I decide whether I'm going to stick a needle in somebody's eyeball, um, and administer

12:15.440 --> 12:20.960
injections, uh, or I determined whether I'm going to do laser surgery on that person,

12:20.960 --> 12:22.840
all based off of the picture.

12:22.840 --> 12:23.840
And so he said, really?

12:23.840 --> 12:28.240
I said, yeah, I said, it's all the picture, the picture guides the treatment.

12:28.240 --> 12:30.520
And, um, and so he said, no kidding.

12:30.520 --> 12:37.080
So, so you have a much more, uh, the, the, the stakes involved in image classification

12:37.080 --> 12:40.440
are perhaps much more tangible for you.

12:40.440 --> 12:41.440
Absolutely.

12:41.440 --> 12:42.440
Absolutely.

12:42.440 --> 12:46.520
And none of this was abstract at all because I'll do 40 injections in one day.

12:46.520 --> 12:50.920
You know, sometimes see 50 patients a day and I'm literally running from room to room.

12:50.920 --> 12:56.880
Um, um, and so I, I, so at that point, I'll study for my boards, the, uh, the biology

12:56.880 --> 12:58.440
boards, which that's a whole different story.

12:58.440 --> 13:01.680
We could do a whole another, uh, say a session on that, Sam.

13:01.680 --> 13:06.240
You know, it's, you know, they, you know, they, they put you in a hotel room and they

13:06.240 --> 13:07.240
ask you these questions.

13:07.240 --> 13:10.520
Literally, there's, there's like a bed there at somebody's hotel.

13:10.520 --> 13:14.960
You walk in and, uh, they're asking you questions about on the 10 different areas of the

13:14.960 --> 13:15.960
eye.

13:15.960 --> 13:19.000
And, and so off the body is, is, is it's own thing in that way.

13:19.000 --> 13:20.840
Um, so anyway, I'll study for this exam.

13:20.840 --> 13:27.280
That everybody dreads, um, and I, I, I, so I said, okay, let me finish, let me knock

13:27.280 --> 13:28.880
the boards out of the way.

13:28.880 --> 13:30.560
And then I'll take a look at this thing.

13:30.560 --> 13:36.680
So, uh, finished my boards in 2016, November, uh, and I started studying, I started looking,

13:36.680 --> 13:41.640
I said, okay, I'm going to look at deep learning, uh, and took a trip to Nigeria, um, which

13:41.640 --> 13:42.640
was already planned.

13:42.640 --> 13:46.760
You know, it was Christmas and, um, and that was when I first opened the book, I looked

13:46.760 --> 13:47.760
at it.

13:47.760 --> 13:48.960
So let me look at this deep learning thing.

13:48.960 --> 13:51.400
And then I thought, okay, very interesting.

13:51.400 --> 13:52.640
Do you remember the book?

13:52.640 --> 13:53.640
Well, yeah.

13:53.640 --> 13:54.640
Okay.

13:54.640 --> 13:55.640
Well, you know, it's actually not a book.

13:55.640 --> 13:57.800
It was, uh, um, nobody reads books anymore.

13:57.800 --> 14:02.600
I was, I was curious if it was one of the kind of the classic books, but, um, okay.

14:02.600 --> 14:03.600
Got it.

14:03.600 --> 14:04.600
Got it.

14:04.600 --> 14:05.600
We're speaking metaphorically.

14:05.600 --> 14:06.600
What's the fine metaphorically?

14:06.600 --> 14:07.600
Got it.

14:07.600 --> 14:08.600
Yeah.

14:08.600 --> 14:14.440
It was, it was, the book was, so the metaphoric book was the CS 231 course by, uh, Carpathy.

14:14.440 --> 14:15.440
Ah, yeah.

14:15.440 --> 14:16.440
That was the book.

14:16.440 --> 14:17.440
Got it.

14:17.440 --> 14:20.200
So I looked at the book and I said, holy smokes, this is good stuff.

14:20.200 --> 14:25.440
Uh, and then next thing I, I pulled up a media article with, um, uh, some Keras code in

14:25.440 --> 14:32.120
it, and the Python, uh, they actually had a tutorial on Python on the 231 course.

14:32.120 --> 14:37.520
And so I pulled out that 231, that tutorial, you know, I have a, I have a grad degree in

14:37.520 --> 14:38.520
computer science.

14:38.520 --> 14:40.200
It took me a week to pick up Python and so on.

14:40.200 --> 14:45.000
Um, and then I, I ran the M-ness training in the Keras.

14:45.000 --> 14:49.800
It was basically like copy pasted it into, uh, into the Jupiter notebook.

14:49.800 --> 14:54.200
And I saw the thing, you know, numbers running and, you know, I was like, wow, this is

14:54.200 --> 14:55.200
amazing.

14:55.200 --> 14:57.080
You know, this is fascinating.

14:57.080 --> 15:01.160
And so I wired up actual data, actual image and data to it.

15:01.160 --> 15:05.200
And when I saw the accuracy that was coming out, meaning clinical imagery data, clinical

15:05.200 --> 15:07.080
or like data and M-ness data.

15:07.080 --> 15:12.000
No, after I did the M-ness thing, okay, then I actually took clinical data, actual, you

15:12.000 --> 15:18.720
know, actual patients, you know, data, uh, and I ran that, uh, an image classifier on

15:18.720 --> 15:19.800
that.

15:19.800 --> 15:24.480
And based on the accuracy that was coming out, I was, my jaw dropped, you know, Sam,

15:24.480 --> 15:27.960
I was like, wow, okay, this is a game changer.

15:27.960 --> 15:32.320
And I'll never forget that night in, in that point, it dawned on me that, uh, there's

15:32.320 --> 15:35.240
about to be a revolution that really everything is going to change.

15:35.240 --> 15:41.000
And so, um, my brother and I got to talk in, I launched out the company, retina AI, you

15:41.000 --> 15:45.440
know, he helped me with it, um, and, uh, he, he turned out he was involved with so much

15:45.440 --> 15:47.760
at the time, you know, he was doing his Kaggle thing.

15:47.760 --> 15:49.480
He was ranked 70th in the world on Kaggle.

15:49.480 --> 15:51.040
He was a part of two other startups.

15:51.040 --> 15:54.040
Uh, he was completely his PhD and so he had a lot going on.

15:54.040 --> 15:57.280
So he said, I should carry on on my own, you know, with it.

15:57.280 --> 16:01.000
And you know, that, that turned out to be a good fit, you know, it was, it was challenging

16:01.000 --> 16:05.400
for me, but I had no backup, Sam, um, so you can imagine that's really where I became

16:05.400 --> 16:10.960
a full stack AI engineer, um, cause I had, I had left my job, you know, where, where

16:10.960 --> 16:16.560
you know, in full disclosure, where I was probably making five or 10 times what, what I,

16:16.560 --> 16:25.680
I thought I would ever make in my life, um, and I left that job and, uh, I, I, I left

16:25.680 --> 16:27.080
it because I thought I had a backup.

16:27.080 --> 16:30.440
I thought I had, there was a person who's going to be the CTO who's going to do all the

16:30.440 --> 16:39.600
actual hard work, you know, and I'm just going to be like, you know, a talk ahead, you

16:39.600 --> 16:40.600
know, no problem.

16:40.600 --> 16:43.920
I enjoy that, you know, running around, you know, talk about what we're doing, you know,

16:43.920 --> 16:47.880
connect with folks and, and kind of get things moving along and, you know, talk to investors

16:47.880 --> 16:51.480
and that type of thing and go give some clinical talks.

16:51.480 --> 16:56.840
Um, but here I was, you know, and I had to build a product and I had to build, I put out

16:56.840 --> 16:59.840
an MVP, you know, and I had to, to do all of that.

16:59.840 --> 17:03.440
And so I, I had to pick up a bunch of things along the way.

17:03.440 --> 17:09.640
I, so we built a mobile app and I had to, I had to pick up a Swift and build the iOS and

17:09.640 --> 17:15.240
then, you know, pick up Kotlin and, you know, and Java, uh, script and, um, you know, put

17:15.240 --> 17:19.960
out the Android version of this and I had to learn how to host models in the web, um,

17:19.960 --> 17:24.440
in the cloud, uh, how to serve and, you know, how I had to get into some details with

17:24.440 --> 17:29.280
serverless stuff as well as restful APIs and the like it and, uh, dock, uh, containerization

17:29.280 --> 17:34.240
and all the, all the needy greedy stuff that I quite honestly didn't think it was even,

17:34.240 --> 17:39.040
I didn't even conceive that it was even possible for a physician to ever be doing anything

17:39.040 --> 17:40.040
like that.

17:40.040 --> 17:45.080
Um, but I was, I was sort of forced into it and, you know, I'm really thankful for that.

17:45.080 --> 17:49.280
Um, but anyway, so our company is retina AI Health Incorporated.

17:49.280 --> 17:55.040
Um, we, I, I, whether that storm last year and at the end of the year started, uh, talking

17:55.040 --> 18:00.080
to investors, got a few angels together, all physicians who, you know, they wrote checks

18:00.080 --> 18:04.840
enough that we're going to be, we're going to be around this year for sure.

18:04.840 --> 18:06.400
And, and, you know, next year as well.

18:06.400 --> 18:12.080
And, you know, maybe forever, maybe, you know, sounds like you've definitely made the transition

18:12.080 --> 18:13.280
to startup life.

18:13.280 --> 18:14.280
Yes.

18:14.280 --> 18:15.280
Yeah.

18:15.280 --> 18:17.080
So that's sort of the short, that's a short story.

18:17.080 --> 18:24.160
And just in quick summary of what the company is, um, we are, we are using machine learning

18:24.160 --> 18:31.000
to build autonomous systems that will, that diagnose retinal diseases as well as diagnosed

18:31.000 --> 18:33.400
systemic diseases from pictures of the retina.

18:33.400 --> 18:38.560
For example, cardiovascular risk, uh, somebody who's that guy who's playing golf and looks

18:38.560 --> 18:40.400
really healthy and is 55.

18:40.400 --> 18:43.640
And suddenly you find out that this person dropped dead of a heart attack, a massive heart

18:43.640 --> 18:45.800
attack and has a young family.

18:45.800 --> 18:51.560
Um, it turns out that we, we are coming closer to be able to accurately find out who that

18:51.560 --> 18:54.000
person is by just looking in their eyes.

18:54.000 --> 18:55.680
And saying, you really need to get to a heart doctor.

18:55.680 --> 18:57.040
You really need to change your diet.

18:57.040 --> 19:01.560
You really need to be on anti cholesterol medicines, beta blockers and the like, um, to,

19:01.560 --> 19:03.000
to prevent something like that.

19:03.000 --> 19:04.120
So it's a big market.

19:04.120 --> 19:06.000
It's a big space and it's really exciting.

19:06.000 --> 19:13.320
Can you maybe contextualize what retina AI is doing relative to some of the other activities

19:13.320 --> 19:14.320
in the space?

19:14.320 --> 19:19.600
Uh, as you can imagine, you are paying even more attention to it than I am.

19:19.600 --> 19:25.640
And there are announcements happening constantly about folks using these particular types

19:25.640 --> 19:33.560
of images to make various predictions, uh, deep mind in UK had a tie up with the, uh,

19:33.560 --> 19:37.760
the NHS there around using retinal images.

19:37.760 --> 19:45.160
I did an interview with, uh, a Google developer who was doing some work around some of what

19:45.160 --> 19:49.520
you were just talking about predicting cardiovascular risk factors based on these retinal

19:49.520 --> 19:51.880
funness images.

19:51.880 --> 19:56.400
Can you maybe contextualize, uh, maybe talk a little bit about kind of the broad landscape

19:56.400 --> 20:02.360
of, you know, what's happening in this space and, and how, uh, what you're doing fits into

20:02.360 --> 20:03.360
that.

20:03.360 --> 20:04.360
Yeah.

20:04.360 --> 20:05.360
Great question, Sam.

20:05.360 --> 20:11.800
Um, you know, it's, uh, it's early days and, um, there's, uh, the problem is so big

20:11.800 --> 20:18.920
and the potential for impact, um, is so compelling that, um, both large companies as well as

20:18.920 --> 20:24.440
small startups are interested, uh, Google has, uh, interest in this area, you know, as

20:24.440 --> 20:30.880
you said, you know, a deep mind, uh, whereas working with the, is working with the NHS and,

20:30.880 --> 20:37.320
um, they, you know, recently put out an image classifier with, uh, I think they said 50

20:37.320 --> 20:42.320
different retinal diseases and that, um, sort of thin, you know, that's, uh, it's for

20:42.320 --> 20:47.040
us a standard problem, you know, we have a similar classifier with even more diseases,

20:47.040 --> 20:50.880
um, that, that, uh, we're about to roll out here.

20:50.880 --> 20:56.560
Microsoft isn't interested in this as well, uh, and, you know, has a platform that they're

20:56.560 --> 21:01.800
working on and, and Google is also working with Arvin and I Hospital in India, you know,

21:01.800 --> 21:08.120
uh, looking at the issue of diabetic retinopathy screening, uh, there are, um, startups, some

21:08.120 --> 21:12.880
that are large and have raised significant amount of capital, such as IDX, uh, which is

21:12.880 --> 21:19.960
based out of Iowa City, uh, Michael Abramoff's company that received FDA approval for a

21:19.960 --> 21:24.760
device to do autonomous screening for diabetes, um, and diabetic retinopathy.

21:24.760 --> 21:30.040
So there are a number of people involved interested because it's such a large space in terms

21:30.040 --> 21:36.520
of, um, both the market as well as the humanitarian potential for, for a real positive impact.

21:36.520 --> 21:42.040
The diabetes is a big focus for a lot of these companies as well as for us.

21:42.040 --> 21:49.160
It, uh, for instance, the US has about nine or 10% of our population, uh, is diabetic.

21:49.160 --> 21:52.560
And you know, that, that number is, uh, over 35 million people.

21:52.560 --> 21:56.120
And when you talk about prediabetes, there's over 86 million people.

21:56.120 --> 22:01.280
That's people who, if not diagnosed and treated, um, will develop diabetes within five years.

22:01.280 --> 22:03.280
That's 86 million people.

22:03.280 --> 22:06.160
And on a worldwide scale, it's a lot larger.

22:06.160 --> 22:11.560
It's half a billion people, you know, with diabetes, um, and about one and a half million

22:11.560 --> 22:15.320
people die every year, you know, from the disease.

22:15.320 --> 22:21.040
So the real issue here is that in terms of human labor physicians, it takes so long to train

22:21.040 --> 22:27.280
a doctor, uh, and that's, there's no hope there for us to use human power to train physicians

22:27.280 --> 22:31.280
to be able to diagnose this and to be able to, the world's population is growing much faster

22:31.280 --> 22:35.040
than we're turning out physicians, um, and so the problem is worsening.

22:35.040 --> 22:42.000
So AI is compelling and is actually necessary, um, to be able to address the, the problem.

22:42.000 --> 22:46.000
And so a lot of people are working on in this area.

22:46.000 --> 22:48.320
And we're, we, there, there's competition.

22:48.320 --> 22:50.080
There's also collaboration.

22:50.080 --> 22:56.120
It's all very good, um, for instance, at the National Medical, um, associations, uh, annual

22:56.120 --> 22:58.920
meeting, which is going to be in Hawaii this year.

22:58.920 --> 23:05.000
I'm going to be chairing a panel, um, on, on this very, on, on more broadly, innovations

23:05.000 --> 23:09.160
in ophthalmology, but I've geared it and focused it towards diabetic retinopathy.

23:09.160 --> 23:14.360
And so we have the, uh, technical lead from Google brain team, uh, Dale Webster working

23:14.360 --> 23:15.520
on this diabetic retinopathy.

23:15.520 --> 23:20.120
He's going to be there, um, on the panel and, uh, and that's, uh, Dale Webster.

23:20.120 --> 23:25.240
And then Anushra Trivedi is, she works in Brad Smith, the president of Microsoft's office

23:25.240 --> 23:27.360
in healthcare AI.

23:27.360 --> 23:28.440
And she's the lead on that.

23:28.440 --> 23:29.640
And so I invited her.

23:29.640 --> 23:31.120
She's going to be there as well.

23:31.120 --> 23:33.800
Uh, she's also working on diabetic retinopathy.

23:33.800 --> 23:37.080
And then Michael Abramoff, who's the founder of IDX, he's going to be on the panel as

23:37.080 --> 23:38.080
well.

23:38.080 --> 23:42.200
Um, and so where we collaborate, we're, we're looking at it, we're thinking about it.

23:42.200 --> 23:47.400
The truth of the matter is it's such an enormous problem, um, that, uh, you know, it's great

23:47.400 --> 23:52.360
to share ideas and see how everybody can move forward together.

23:52.360 --> 23:57.720
And so can you maybe talking a little bit more detail about your approach and some of

23:57.720 --> 24:03.760
the research you've published in this space and how it, uh, you know, is, is everyone

24:03.760 --> 24:10.400
kind of doing the same thing, applied to different data sets or folks taking, you know, either

24:10.400 --> 24:15.400
dramatically different or, you know, different in novel ways, uh, types of approaches, like

24:15.400 --> 24:18.520
what distinguishes the different things that are happening in the space.

24:18.520 --> 24:19.520
Right.

24:19.520 --> 24:21.560
It's great, great question, Sam.

24:21.560 --> 24:24.840
So yeah, there's a lot of commonality, a lot of similarity.

24:24.840 --> 24:31.040
Um, whenever one goes to these, um, machine learning conferences, you know, there's no, in

24:31.040 --> 24:32.960
terms of the actual techniques that are being used.

24:32.960 --> 24:35.520
There's nothing that's enormously novel, right?

24:35.520 --> 24:38.120
Everybody, you know, there's only, there's only Python in there.

24:38.120 --> 24:43.360
There's only Keras and, you know, PyTorch and, uh, um, there's, you know, and there's

24:43.360 --> 24:47.920
not that many tools, um, that we're using to execute the job.

24:47.920 --> 24:54.880
Approaches do differ, uh, and, uh, access to data, you know, uh, differs, uh, and that's,

24:54.880 --> 24:59.640
sometimes it's not always, more data is not always necessarily a good thing, which is

24:59.640 --> 25:04.520
a separate topic, um, it depends on the type of data that one has access to.

25:04.520 --> 25:08.800
And then one's understanding and knowledge of the specific domain.

25:08.800 --> 25:13.560
And so I think that's where we stand out, um, because of my experience as, uh, a physician

25:13.560 --> 25:17.440
and a thomadist, I'm a retina specialist, also a full stackier engineer.

25:17.440 --> 25:22.200
And I built the entire prototype of our first prototype on my own and to end, um, I think

25:22.200 --> 25:25.200
what that does for us is it gives us a very unique perspective.

25:25.200 --> 25:27.080
We can innovate very quickly.

25:27.080 --> 25:32.320
For example, one of that, the papers that I just published two weeks ago was, uh, using

25:32.320 --> 25:39.680
a generative adversarial network, um, to generate, uh, artificial data for data augmentation.

25:39.680 --> 25:44.720
And, um, I generated a certain type of scan called an OCT of the retina.

25:44.720 --> 25:48.000
And I passed it along to a few of my colleagues who are also retina specialists.

25:48.000 --> 25:54.200
And, uh, I was impressed to see that, um, at least half of people, uh, didn't get all

25:54.200 --> 25:55.200
of them right.

25:55.200 --> 26:01.440
These are experts, you know, who look at these, these images all day, um, so there are certain

26:01.440 --> 26:02.440
differences.

26:02.440 --> 26:07.760
It's the smaller startups who have more integrated domain knowledge can move quicker and can

26:07.760 --> 26:11.640
innovate, you know, uh, quicker, uh, in that space.

26:11.640 --> 26:14.560
And so everybody's going to have to work together and that's what we're doing.

26:14.560 --> 26:19.680
We currently have, um, we're currently in the Google Cloud, uh, platform, you know, startup

26:19.680 --> 26:25.440
program, so we, we get some, some, some benefits from, you know, having access to, to cloud

26:25.440 --> 26:26.440
infrastructure.

26:26.440 --> 26:31.320
And, you know, that helps us, uh, and we're currently also looking to, to further some

26:31.320 --> 26:35.800
ties with some of the other larger companies to see how everybody can move forward together.

26:35.800 --> 26:43.000
So that's, that's a really interesting perspective on the way these markets, uh, will evolve.

26:43.000 --> 26:47.680
And I want to dive into, uh, the, the GANs, uh, work that you've done.

26:47.680 --> 26:52.200
But, but before we do that, let's linger here for a second, uh, what strikes me as interesting

26:52.200 --> 26:58.560
is there is a period of time, maybe, you know, three, four years ago when everyone kind

26:58.560 --> 27:05.960
of believed that, you know, the, that, that AI was going to be totally driven by access

27:05.960 --> 27:10.520
to data and it was going to be a handful of large companies that would kind of lock up

27:10.520 --> 27:16.000
all of the data and, you know, thus build all of the best models and everyone else would

27:16.000 --> 27:17.640
be frozen out.

27:17.640 --> 27:20.240
And I, I think that, that's changing, right?

27:20.240 --> 27:24.200
That perception is changing and you touched on some of the reasons why.

27:24.200 --> 27:27.440
Can you elaborate on, you know, what you see there?

27:27.440 --> 27:28.440
Yeah.

27:28.440 --> 27:32.680
Um, exactly, you know, people, it was just, it was just 24 months ago.

27:32.680 --> 27:35.880
People were saying, oh, wow, you know, throwing the towel, don't worry about it.

27:35.880 --> 27:39.080
You know, Google's interested, they've, they've looked at it and so it's over.

27:39.080 --> 27:42.880
Um, and, you know, I, we always said, you know, it was, as people were saying that,

27:42.880 --> 27:50.360
that I was leaving my job, you know, because I, I absolutely didn't believe it.

27:50.360 --> 27:56.320
You know, I, and so one thing that I tell people is data is everywhere.

27:56.320 --> 27:59.320
And data has always been everywhere.

27:59.320 --> 28:04.400
And it's, it's kind of like oil, like natural gas, it's, it really is everywhere.

28:04.400 --> 28:08.600
You know, you dig deep enough anywhere, you'll, you'll strike oil somewhere, um, but

28:08.600 --> 28:15.400
it's really difficult to set up the rigs to know how to, to find the oil and how to, you

28:15.400 --> 28:22.040
know, extract it and, and, uh, convert it into something useful into gas and to fuel

28:22.040 --> 28:26.920
into natural gas for, for one's vehicle, for instance, just to jump in there.

28:26.920 --> 28:34.760
I'm not sure that's the best analogy for your position in the sense that actual exploration

28:34.760 --> 28:39.320
and extraction of oil is hugely resource intensive.

28:39.320 --> 28:43.440
And it's kind of the domain of the big oil companies as opposed to, well, I don't know

28:43.440 --> 28:44.440
this market.

28:44.440 --> 28:49.760
So I may be totally wrong, but I'm assuming that it's, you know, that is kind of one of

28:49.760 --> 28:54.840
these things where it's kind of dominated by, you know, huge energy companies and state

28:54.840 --> 28:56.800
on players and things like that.

28:56.800 --> 28:57.800
Right, right.

28:57.800 --> 28:58.800
Yeah.

28:58.800 --> 28:59.800
Yeah.

28:59.800 --> 29:03.400
That's, that's one part that the analogy doesn't work because it's actually the opposite

29:03.400 --> 29:11.400
in the case of, you know, in this era where, you know, a kid in Nigeria or Ghana who has

29:11.400 --> 29:16.640
access to the internet, you know, and has a laptop, you know, can train a machine learning

29:16.640 --> 29:17.640
algorithm.

29:17.640 --> 29:18.640
Right.

29:18.640 --> 29:26.000
Um, so it's other than that part, you know, it's, um, so yeah, the point there is that

29:26.000 --> 29:32.120
no end to data, having domain expertise, which is what these oil companies have, um, having

29:32.120 --> 29:37.400
domain expertise and knowing what to do with data, that's actually what's very expensive.

29:37.400 --> 29:43.000
And in, in, in this case, it's, uh, the analogy does work where expensive in a more general

29:43.000 --> 29:47.160
sense of the word, uh, it's hard to come by.

29:47.160 --> 29:52.760
It's so expensive that Google can't afford it, right, um, in the sense that, and that's

29:52.760 --> 29:57.000
part of the, the strategy they recognize that and so a lot of these big companies have

29:57.000 --> 30:04.280
venture arms that look to partner or acquire, uh, smaller startups, um, because they, that's,

30:04.280 --> 30:10.880
that's where the rubber hits the road is in the domain expertise coupled with the engineering

30:10.880 --> 30:17.840
know how, uh, allows people to find meaningful insights inside of data.

30:17.840 --> 30:23.920
And so another comment that you made was that more data is not necessarily a good thing

30:23.920 --> 30:29.400
and it goes against kind of the common wisdom as well in this space.

30:29.400 --> 30:30.920
Can you elaborate on that?

30:30.920 --> 30:32.280
It totally does.

30:32.280 --> 30:33.880
Yes, yes.

30:33.880 --> 30:37.120
So more data is not always a good thing.

30:37.120 --> 30:42.760
And for example, one of the reasons why, you know, uh, generative adversarial networks

30:42.760 --> 30:48.360
have been thought of as, you know, more powerful than preceding, uh, generative models is because

30:48.360 --> 30:56.880
they can somehow find somehow find, um, different pockets where there's, uh, an up variation inside

30:56.880 --> 31:01.200
of, uh, a probability distribution of a certain data type.

31:01.200 --> 31:06.280
Um, but, but they did that's, that's, that's only so true to a certain extent.

31:06.280 --> 31:12.200
Otherwise, what, what one might be getting is sort of an averaging or blurring of, of

31:12.200 --> 31:15.200
a certain part of the data.

31:15.200 --> 31:22.680
Um, that's the one example where knowing how to, uh, pick the data, understanding what

31:22.680 --> 31:29.760
the implications in the various pockets of variation within a data sample mean in the

31:29.760 --> 31:33.400
real world, um, is priceless.

31:33.400 --> 31:39.480
One example of where this played out is with the RSNA Kaggle competition in which, um,

31:39.480 --> 31:45.400
the top 10, uh, finalists winners were all radiologists who have somehow along the way acquired

31:45.400 --> 31:50.800
ML skills, uh, and there's, there's no substitute, substitute for that.

31:50.800 --> 31:53.440
Um, and that's sort of always going to be the case.

31:53.440 --> 32:00.680
Another example is no one went to this halt training of an algorithm, um, with your standard

32:00.680 --> 32:05.440
supervised learning type problem, uh, you can sort of go off of your loss function.

32:05.440 --> 32:09.040
You can say that the losses dropped, the looser and threshold we're going to stop, we're

32:09.040 --> 32:10.040
doing great.

32:10.040 --> 32:13.720
Um, that process is governed by the data, of course.

32:13.720 --> 32:19.160
So assuming that you had good data, you, you can safely trust your halt point.

32:19.160 --> 32:26.200
But in the case of, say, again, the ultimate arbiter is still the human visual cortex.

32:26.200 --> 32:33.960
It's still a human being who understands what, um, a dog or a cat should look like because

32:33.960 --> 32:37.880
you've got these two competing algorithms that are fighting with each other and the loss

32:37.880 --> 32:40.760
itself is clearly now insufficient.

32:40.760 --> 32:44.840
And so you can't, one can't stop training again when the loss drops below a certain point

32:44.840 --> 32:50.720
because the adversariality, the push and pull that the discriminator and generator are

32:50.720 --> 32:53.720
doing to each other continues past a certain point.

32:53.720 --> 32:59.480
So you don't really know when to stop, uh, and there's no real quantitative approach

32:59.480 --> 33:05.640
that we currently have for knowing when to stop training, uh, these gain algorithms.

33:05.640 --> 33:10.320
And so because adversarial examples are another way that, that make that very apparent,

33:10.320 --> 33:13.520
that you really ultimately still need the domain expertise.

33:13.520 --> 33:18.720
It's an enormous value, um, in today in ML and I think we're all going to, I think people

33:18.720 --> 33:24.440
are starting to get that, um, people, the, the initial big hype and excitement that

33:24.440 --> 33:28.400
computers are going to take off or start into, to dampen, uh, and people are starting

33:28.400 --> 33:34.800
to realize that, um, the only real way forward is going to be, uh, in any, is going to be

33:34.800 --> 33:39.400
domain specific and it's going to be with really integrated interdisciplinary teams and

33:39.400 --> 33:47.400
ideally, people who have, um, uh, detailed, uh, expert level knowledge of both the ML side

33:47.400 --> 33:53.200
as well as whatever other field that they're looking to apply ML to be at agriculture,

33:53.200 --> 33:56.200
you know, be a transportation, be it healthcare.

33:56.200 --> 34:02.240
Tell me a little bit about your experience getting up to speed with, with GANs ultimately

34:02.240 --> 34:05.160
resulting in, uh, this paper you published.

34:05.160 --> 34:06.160
Oh, yeah.

34:06.160 --> 34:11.760
It's, uh, yeah, you know, I basically, I, um, I, I, I thought, you know, well, this, this

34:11.760 --> 34:12.760
could be interesting.

34:12.760 --> 34:14.600
It started with a theoretical question.

34:14.600 --> 34:18.840
I was, you know, I was given a talk and somebody asked me in the audience and I was actually

34:18.840 --> 34:23.640
given talking Nigeria where I went for a data science, uh, Nigeria they invited me in

34:23.640 --> 34:24.840
January just last month.

34:24.840 --> 34:27.560
Oh, two months ago, we're already in March here.

34:27.560 --> 34:33.280
Um, and, uh, and somebody, uh, somebody brought up, uh, whether GANs could be used for

34:33.280 --> 34:40.240
augmentation of data and, you know, and my initial inkling, I hadn't really worked with

34:40.240 --> 34:44.880
GANs at that point, um, only really a month ago.

34:44.880 --> 34:49.760
And, uh, my inkling was to say, no, I don't see it.

34:49.760 --> 34:56.920
I, I don't see a person pulling themselves up by their own bootstraps that it's your,

34:56.920 --> 35:05.320
and your GAN model, your generative model, what it's able to generate is necessarily constrained

35:05.320 --> 35:11.160
or bounded in terms of its accuracy in terms of its fidelity to the actual native data set

35:11.160 --> 35:18.600
is bounded by these initial sample from that data distribution, which is your training

35:18.600 --> 35:20.920
data for the discriminator.

35:20.920 --> 35:26.040
That was my response, you know, and, you know, and, uh, but I, I kept thinking about it

35:26.040 --> 35:30.160
though, you know, that, was that, was that correct?

35:30.160 --> 35:36.480
You know, is that true, um, and, and, uh, it's, it is, you know, I, I'm still there.

35:36.480 --> 35:38.800
And so I said, okay, let me, let me play with this thing.

35:38.800 --> 35:43.080
Let me take a look at it and actually look at the details of how it works and what it

35:43.080 --> 35:44.080
does.

35:44.080 --> 35:50.320
Uh, and so I, I, I read up on it and, you know, trained up again model, uh, and, you

35:50.320 --> 35:54.600
know, generating some data and I'm getting some really insights from, from that, I've

35:54.600 --> 35:57.480
only been doing that over the last couple of weeks here.

35:57.480 --> 36:01.720
And uh, been learning a number of things about how data works and what data is, what the

36:01.720 --> 36:04.120
distributions are and so on and so forth.

36:04.120 --> 36:11.800
And that only, um, strengthens my conviction that for us to go forward in ML, um, we're

36:11.800 --> 36:18.800
going to have to understand the domain very well because they're, it's necessarily a heuristic

36:18.800 --> 36:20.400
field.

36:20.400 --> 36:27.880
And so, um, yes, we can come up with theories that are true to some extent.

36:27.880 --> 36:32.960
But ultimately, only, only so much as they can somehow encapsulate the general properties

36:32.960 --> 36:33.960
of data.

36:33.960 --> 36:39.440
For instance, um, there's the, uh, in signals processing world, right?

36:39.440 --> 36:41.800
Where one is looking at doing a sampling.

36:41.800 --> 36:49.640
If your signal has a certain character to it, you know, if it's band limited, um, then

36:49.640 --> 36:55.640
you know that if you sample a certain way with a certain frequency, you can perfectly

36:55.640 --> 36:59.720
reconstruct that data set, you know, that type of idea, you know, the Shannon Nyquist

36:59.720 --> 37:05.960
sampling theorem, you know, that type of idea, um, can provide some guidance about what

37:05.960 --> 37:08.520
we can do within ML.

37:08.520 --> 37:14.360
But it's completely clear that, you know, Google, the Googles of this world would be completely

37:14.360 --> 37:19.840
outmatched by small teams of people, could be even two or three people who have expertise

37:19.840 --> 37:23.040
in their area and are working on those problems.

37:23.040 --> 37:25.440
And that's a very exciting time in history.

37:25.440 --> 37:33.800
You mentioned this process of working with gans, you know, beyond reinforcing the, the

37:33.800 --> 37:41.360
value of domain expertise kind of led to some insights around some of the core machine

37:41.360 --> 37:45.320
learning problems that you're working on, can you elaborate on those a bit?

37:45.320 --> 37:46.640
Right, right.

37:46.640 --> 37:52.040
So I'm actually, I'm trying to, I want to write it up, you know, put it on the archive.

37:52.040 --> 37:59.560
Some of these same type of ideas are, some of the paradoxes are that more data is not

37:59.560 --> 38:00.560
necessarily better.

38:00.560 --> 38:03.000
You know, that's kind of a big one.

38:03.000 --> 38:05.400
It's what type of data, right?

38:05.400 --> 38:07.800
And what problem are you trying to solve?

38:07.800 --> 38:10.320
Sometimes you actually need less data of a certain type.

38:10.320 --> 38:14.840
Sometimes when you have more data, you're diluted something, depending on what you're trying

38:14.840 --> 38:15.840
to accomplish.

38:15.840 --> 38:22.160
And how do you know what's the, like, this is probably what the thing that you're, that

38:22.160 --> 38:28.280
you need to write up, or that you're kind of moving towards writing up, but like, my

38:28.280 --> 38:33.880
sense is that there's, you know, a set of kind of disciplines behind what you're saying

38:33.880 --> 38:36.280
that aren't really talked about a lot.

38:36.280 --> 38:37.280
Right, right.

38:37.280 --> 38:43.560
And this field is so nascent, you know, this field of ML and data is wide open.

38:43.560 --> 38:44.560
Yeah.

38:44.560 --> 38:45.560
Yeah.

38:45.560 --> 38:46.360
It's totally wide open.

38:46.360 --> 38:51.440
And I think there have been certain mantras that have been just, you know, just regurgitated

38:51.440 --> 38:58.360
verbatim, but I think that, you know, the more we look at it, the more we honestly look

38:58.360 --> 39:04.360
at it, right, as opposed to trying to force machine learning to fit a certain model, to

39:04.360 --> 39:08.480
fit a certain way of thinking.

39:08.480 --> 39:13.200
I think the more we honestly look at it, the more we'll be able to get deeper into what,

39:13.200 --> 39:15.360
what the character of this thing is.

39:15.360 --> 39:20.000
It comes down to specificity, you know, what are we trying to do, you know, and how we

39:20.000 --> 39:24.960
trying to get there in a data set, you know, in a data distribution, for instance, there

39:24.960 --> 39:32.200
would be certain features, certain characteristics within certain neighborhoods of the distribution,

39:32.200 --> 39:34.680
which would be very different from other neighborhoods.

39:34.680 --> 39:40.520
And so if you're trying to capture the essence of a certain neighborhood of the data distribution,

39:40.520 --> 39:45.920
you're better off sampling from that area than coming up with, you know, saying I have

39:45.920 --> 39:52.440
10 million images, you know, of this broad, ill-defined, you know, data class.

39:52.440 --> 39:58.080
When in reality, they're truly subfamilies, multiple subfamilies within that whole area.

39:58.080 --> 40:00.200
So one has to really understand what they're doing.

40:00.200 --> 40:05.640
One has to really understand what the landscape of that data type is, be it language, be it

40:05.640 --> 40:11.000
image and be it a particular type of image and then guide that.

40:11.000 --> 40:15.080
So the human visual cortex and the human experience will remain the eye.

40:15.080 --> 40:21.120
In my view, will remain the ultimate arbiter for at least, you know, for the next decade.

40:21.120 --> 40:22.800
Who knows what's going to happen after that?

40:22.800 --> 40:26.600
Well, Stephen, it was great chatting with you about all this stuff.

40:26.600 --> 40:32.480
I'm looking forward to following along as you start to publish some of your insights

40:32.480 --> 40:41.040
into, you know, GANS and this whole dynamic around data and domain expertise and machine

40:41.040 --> 40:42.040
learning.

40:42.040 --> 40:43.440
It's all really interesting stuff.

40:43.440 --> 40:44.840
Well, thank you so much, Sam.

40:44.840 --> 40:48.360
It's really been an honor to be on your excellent show.

40:48.360 --> 40:52.920
And I'm looking forward to going through your entire archive.

40:52.920 --> 40:57.160
Yeah, I am. It's really good quality, high quality stuff.

40:57.160 --> 41:00.040
Thank you for your contribution to the community.

41:00.040 --> 41:01.040
Thank you.

41:01.040 --> 41:02.040
Thanks, Stephen.

41:02.040 --> 41:03.440
You're most welcome.

41:03.440 --> 41:07.520
All right, everyone.

41:07.520 --> 41:09.400
That's our show for today.

41:09.400 --> 41:15.440
For more information on today's show, visit twomolai.com slash shows.

41:15.440 --> 41:21.360
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platform's

41:21.360 --> 41:22.720
conference.

41:22.720 --> 41:26.400
As always, thanks so much for listening and catch you next time.

