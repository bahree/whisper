Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Thank you very much for having me, this is a real pleasure to be here.
Awesome.
Thanks so much.
I am really excited to jump into this conversation.
You are someone that I follow on Twitter and we've had these kind of back and occasional
back and forths over time and it's great to finally meet you in person.
You've got some pretty varied interests.
You spend a lot of time, your research focus on reinforcement learning.
You also tweet a lot about music and the arts.
Looking at your background, you've done applied ML stuff at Google, add from and other things.
Tell us the story, how do all these threads come together.
Well originally I'm from Ecuador and I moved to Canada after high school to come study
at McGill.
So I did my undergrad and then I eventually did my master's in PhD at McGill with
Dwayna Prikup and Prakash Panagaden and so part of the reason why I stayed in Montreal
and McGill was for personal reasons, I was dating someone who's now my wife and I also
had a band and I also had a band so I've always been heavily involved with music.
I grew up with music, learning music, playing music so that was very important to me and
I didn't want to leave that so I decided to make that choice.
I know it's not the typical thing that suggested to do all your degrees in the same university
but for me it was more important to keep playing music.
So I graduated, I finished my PhD at around 2011 and then I moved to Paris for a post-doc
and this was at a time where AI isn't what we see here with 12,000 people in this conference
I mean NURPS didn't have, back then it was called NIPS, maybe 4,000 people.
So I wanted to stay in academia and I was working at this intersection, that was very theoretical
sort of between Markov decision processes and formal verification.
So I was finding it really hard to find a job because I wasn't formal verification enough
for the formal verification community and I wasn't reinforcement learning enough for
the reinforcement learning community.
And so after my post-doc I just feared, I already had two young kids and I feared that I would
just be going post-doc to post-doc for too long.
So I luckily got a job from Google doing applied machine learning and ads and I actually
said goodbye to academia at that point, I stopped reading papers and then I did a little
quick stint in Chrome doing building machine learning infrastructure, so back end infrastructure
and brain opened up in Montreal and Mark Belmar who I had done my masters with, he kept
in research, he was in deep mind for a while and he was one of the first people to join
brain in Montreal and he put in a good word for me and so then they offered me to join
them and I jumped at that possibility and I hadn't been following the research that also
was a huge shock to come back, I mean when I was doing my research we were all working
on grid worlds and very simple environments because a lot of it was theoretical, we didn't
really use deep networks at all for reinforcement learning.
So there was a lot of catch up trying to familiarize myself with the literature and how the
whole landscape had changed.
So throughout all this time I always kept with music, I had a few different bands, I've
always been performing live and writing music and the other thing is when I started my
PhD I was actually considering doing a PhD with Douglas Eck as well as with Doiner Precup
in something with machine learning and music but at the time what was available for music
generation didn't really excite me very much because it was still in the early days and
I feared that it would taint my love of music and I just wanted to keep my music aside
separate.
But when I rejoined the research world and I saw what the magenta team was doing I was
kind of blown away by the quality of things so then I decided to also start going along
that pathway and I think the day after I joined Brain this artist from Canada, he's called
David Usher, he's pretty well known in Canada.
He approached us wanting to, he approached us, he was actually first had a band in the
90s called Moist and it was really popular and he approached us, he wanted to do an album
using like AI techniques and so we just met and kind of brainstormed and the thing he
gravitated towards the most was lyrics and so Hugo La Hoshel who was my manager at the
time was very generous because I had just joined Brain he's like do you want to take this
project because I like music I said sure it sounds fun.
I had never trained a language model, I was still trying to figure out all this deep
network stuff because I hadn't looked at that but yeah, Hugo gave me that opportunity
and I learned a ton in that project and so still it's still an ongoing project so relative
to the first model that I trained with David which we actually made a video out of that
like he rewrote one of his songs with this first prototype and it worked okay but the
model we have now is so much better and I understand all of this language modeling so
much better than when I did before and that's just that experience kind of showed me to
not be afraid of sort of stepping out of because I'm very familiar in reinforcement
learning which is background to step out of that comfort zone and go into other areas
that I'm not as familiar with because they're all interesting problems and sort of really
trying to dig into the details and for me the way I learned the most is actually trying
to implement some of these models and architectures and play around with them because you read
about them in papers and you kind of get it that's fine but until you're actually trying
to get it to work for yourself it's that's a whole different experience and I've learned
so much just from doing this like jumping from one one problem to to the next and in
a separate kind of field and learning about those architectures but while still maintaining
my research and reinforcement learning well it sounds like you've landed in an incredible
place to do that not just kind of the resources of Google and the people that you're surrounded
with and have an opportunity to interact with but your role seems to be defined as like
advancing research you know via implementation absolutely yeah so I'm a software developer
like that's my official title there's also research scientists at Google and until recently
there were still like most people that are in research want to be research scientists
because that's like then you're officially doing science right right so I my like if
I had graduated say four years after when I graduated yeah likely I would have been applying
for research scientist role back when I applied at Google that wasn't really a maybe
Sammy Benjo was a research scientist but probably that about it and so I entered Google
as a software engineer and sort of advance my career and that and that track and when
I joined Google it was as a software engineer or developed we call it developer in Quebec
because engineer you get an iron ring and I don't have that okay initially I was a little
skeptical because the official description is you're there more supporting research scientists
and so I was worried that I wouldn't have a flexibility to sort of pursue my own research
interest but it's been not at all like that so I leave my own research projects and I
still support a lot of people with the engineering aspects of it because I'm I've been working
on this a lot so I'm more familiar with like Google infrastructure and just coding in general
and it's been I mean a lot of the major advances that we see in machine learning and AI nowadays
is a lot of it is engineering right so there is of course there's still math and there's
still a lot of theory behind it but a lot of it is engineering and I don't think it I think
more and more it is but a few years ago I don't feel like got the credit it really deserved
and so living in this sort of intersection of pure engineering and pure research is for
me super exciting because I kind of get to play around in both worlds and learn from both
world.
I've got a long list of things that I want to talk to you about but you mentioned something
that's got me really curious the you know what it means to evolve a language model you
started this prop project with David and came out with this you know early crappy language
model and have evolved it over some number of years it's been like a yeah no it's been
like a year and a half it's been actually it's been almost like two years I think since
we started it but two years calendar calendar wise but it's not it's not one of my main
projects so yeah exactly so it's when I get a chance that I work on that yeah so as I
said when I started this project I had never trained a language model I like I
knew what LSTM's were because I studied it in school but so the first thing I did was
I actually Andre Carpati has the yeah this famous blog post the surprising reliability
of of recurring neural networks something like that anyway so I brought that blog post
and I got his code and sort of played around with it and that was the the V0 model just
over characters and then I started tweaking that a bit and finding new data sets for lyrics
and that initial model that was basically a variant of Andre Carpati's model was the initial
model that I had and so that was okay like just a milestone like okay I was able to train
this and actually get it to do what I wanted it to do but it obviously was it has all the
shortcomings that these types of models do at this round I mean the attention is all you need
paper had come out not not too much before then and so then I started looking into these
attention models and and so it seemed like the right thing to do so I switched over to
to the transformer model and started playing around with that and so the V2 model was using
an attention model and so I had various versions of a V2 part of the difficulty that I had
with the language with training these language models on lyrics data set is that the lyrics
data set is is not the best what sense so the tricky thing about these language models
is that and and for lyrics in particular is that you're trying to get this model to learn
English kind of so how how to structure English phrases together but in a quote unquote poetic
way and to not be boring right because you're trying to use it for creative purposes and
so you don't want it to be boring so we trained this model and if you look at like perplexity
scores and things like that it was doing pretty well on this lyrics data set but then when
you actually look at the output it was extremely boring so because in pop songs you have lines
that repeat often I mean that's just how songs are written so the model would tend to just
repeat the same thing over and over and over again it also had certain phrases that it would
keep on coming back to like they just had very high likelihood so one of you talked about this
it I say like it's hand wavy but the the average pop line over the last six decades is you know
that I'm the one and so that one came up a lot and you can also add sometimes you get you know
that I'm the one come a baby so that's the average pop line it was boring and so the interesting
thing about working with with David is that I build like variants of these models and I
show him and one of the things he remarked on is that it was very non-specific in the sense
that the nouns that it was using it wouldn't use proper nouns so it would use like me you he she
they so it's very I'm kind of ambiguous if you think of like the Beatles I mean there's me
Mr. Mustard Paul Avine Pam Jude you know there's all these I mean the fictional characters but
they're characters yeah and so then you can sort of the ground the the song and something kind
of real where's if you're just talking about him like hey you so let me down like that's not
even though Pink Floyd has a hey you somewhere so anyway then from that feedback I started looking
at other datasets so for instance the fiction books dataset these are available online so we
traded with that and we got much more diverse vocabulary much more diverse themes than we would
get with with lyrics but the structure of the what was coming out no longer looked like a song
line like a song lyric it looked more just like a sentence that you'd sort of cut in half and
it just looked like a sentence with new line characters thrown in random places so then I thought
why not rather than trying to tackle this problem with a single language model where I'm trying
to get all these constraints of like making being coherent from the English perspective being
kind of creative and also following the structure of lyrics why try to do this all with one model
like there for me there's no where you go to multiple models with the you know the language models
you're working at working with at the time is there a way are you trying to express those constraints
explicitly or just based on the data that you're feeding into it was I was more playing with the
data so with the with the training data that I was using to try to enforce these these constraints
so the the lyrics I mean the idea was well if we want to write lyrics then we should train
on the lyrics dataset to try to replicate that distribution but there's as I'm saying it a lot
of the interesting lyrics are in the long tail of the distribution and the long tail is terrible
for a model because it's going to have low likelihood by definition but that's actually what
we're interested in as like it so it has to be coherent but unlikely and so so that's part of
the reason why I I was considering just going to why why try to do it with one single model
as my ultimate my ultimate goal was to have something that artists could actually use and David
could use for for writing a whole new song from scratch and so for him it doesn't like he
won't care and most artists won't care whether it's a single model or multiple models as long as
it is interesting and makes sense and just to understand the the goal is it start with some prompt
and the thing spits out an entire song or kind of is the artists or is the model one in which the
artist is kind of incrementally promptly in the model and kind of refining the output yeah it's
more of the latter so the way we're starting to build out a website that I'm hoping to release
early next year so it's it's almost ready but it's just as I said it's one of one project that I
work on when I have time and with nirips and all that it's been kind of tight but the ideas yeah you
have kind of like an almost like a notepad where you where you're writing your song and this is
coming a lot from feedback from david with how about how he writes songs and so you have you write
your part of your line or you can even start without a prompt and then just ask the the model for
suggestions and so then the model will give you suggestions for the next line you can specify
rhyming schemes so whether like if you want a bba or if you just want it to rhyme with your
current line and then the model will give you some suggestions and then you can like drag those
suggestions over to your your worksheet or inspire yourself from those the suggestions that the
model gave and and then continue working with with your song this way so it's more
what I what I'm not as interested in is having something where you press a button and it produces
your next top 40 hit it's really like a tool the way of view it is as a tool so one analogy I like
to use is if you think of recording music in the 60s or 70s if you wanted to record a good album
you had to go to a professional recording studio and hire a professional recording engineer
because otherwise it wouldn't sound very good now we have things like Pro Tools and Ableton
that you can do it in your basement and like a lot of people have become very famous from
recording albums completely by themselves in their basement so it's not like we got rid of
recording engineers or recording studios they're they're still around and they're doing the
same thing and you still require them for for many for many purposes but these tools like Pro Tools
and Ableton that kind of democratize the recording industry and so a lot of people can start playing
around with recording and maybe pursue music as a career by leveraging what they can do on their
own and so I view these tools as sort of in the same vein where the idea is not to sort of replace
musicians or songwriters but to enhance them so basically give them something that they can play
with the other thing that I find interesting that that could be quite useful for this is if
your first language is in English and but you want to write a song in English because you're
living in English, the language in country this tool might be helpful for because it does provide
well well structured sentences it might help you write your song because you can provide the
context and sort of the themes and the model would help you write in a way that that's more natural
sounding English than than than what you would be able to come up with on your own.
Okay and you mentioned that you're the one of the things the artist is able to do is put in kind
of the rhyming scheme that you're interested in is this kind of a filter after the model is generating
possibilities or is it some kind of constraint that's introduced into model itself?
It's not even no constraint or filter the way it's happening is by nature of how we're training
the model so I have as I mentioned we move to this this system where we have two models so one is
what I call the structure model so they're both transformer models but the first one is trained
on the lyrics data sets but rather than using the English words I convert them to other parts
of speech so like now an advert that type of thing and it also includes like the number of syllables
in the line and then the final phoneme syllable so like basically this is for rhyming
and so this model if you give it the structure of your current line it will the output of it
what it generates will be the structure of the next line and so then you can feed that
next line structure with the actual English words of your first line into the second model
and this is another transformer model which I call the vocabulary transformer and so this
one is trained on on books on the books data set because we want the diverse vocabulary from that
and so what this model is trained to do is to fill in these blanks so these
cat parts of speech and the condition on the condition on the so yeah so the first half gives it
the context right because it's using English words the second half gives it the structure so
the number of syllables what parts of speech and the last phoneme so we trained it with the books
data set by essentially because we know the ground truth we can specify when you have this context
and this structure this is the last phoneme and so the the the target that we're training to emulate
matches that so it rhymes really well the one thing that we're working on right now is that
the easiest way to rhyme is to just use the same word right so red always rhymes with red right
so that's something that that we're exploring a few different approaches on how to overcome that
so that we've tried a few things playing like the first things we tried just again playing
with the data to see if we could discourage it from from rhyming with the same word so far none of
those things have worked very well so like an easy thing you can do is basically when you have the
the weights for the the possible tokens that the model kind of met just set the weight to zero
for the words that you don't want to rhyme and then the model is forced to pick a different word
when when decoding so we're trying a few other things that are a bit more exciting for us
but yeah that's essentially how we get the rhyming for the model is this project is this has
this been published and are you working on the website but like yeah we had a paper in the workshop
the creativity workshop two years ago okay or last no it was last year sorry in the creativity
workshop nerfs in Montreal last year but it's a two-page viewer I think it's on our archive but it's
very like summarized to remember somebody asked me the one to collaborate on the project I sent
them the link and he's like this looks like a proposal it's not a paper so yeah that paper is
still very preliminary we have a more extended version but we are thinking of submitting it to
somewhere we're just trying to refine the model so the paper itself is not published but we're
working on on basically getting more human feedback because we can we have some quantitative
measures but because it's the purpose is creative it's kind of hard to measure quantitatively so
we want to get more humans to evaluate and measure it compared to other models or even true lyrics
so we're working on that and also we're working on making the code open source so you can take it
and train with whatever dataset you want so training with Wikipedia or something and you get
what's the training time or kind of resource for the training well so we train we've been
training on TPUs and that's really fast so the longest distributed like lots of TPUs
no just yes okay and it's been it's been it's pretty fast on the order of like we were training
less than in less than an hour oh wow so yeah it's great fast the the longest part is the pre-processing
of the of the dataset if your dataset is very large because we do this decomposition into parts
of speech and that type of thing that's typically what what takes the longest but once you do it once
and then you can reuse it for whenever interesting I can I ask the question earlier about
applying external constraints into the modeling process is there does that work with
transformer models or their folks that are doing that kind of thing like almost like a model-based
kind of there are yeah there's some work with that I've seen so there's there's some people
in the Toronto office from brain that have been doing some there's one paper called the insertion
transformer so rather than be coding left to right as you typically do with with language models
I don't know all the details but essentially the the way it works is now you can emit tokens that
say where to insert so not to go from left to right but actually inserting into different parts so
this allows it to potentially start from like a higher level of the structure of the phrase that
it's going to generate and then sort of start filling in more of the details once the structure
is is there and so it's actually something I've been meaning to look at because this might be
kind of useful for lyrics where you're trying to to satisfy some structure so maybe we can
generate a full verse rather than line by line if we have if we start doing in this kind of
incremental incremental way there's also like from the magenta team the the the music transformer
that they've been doing and that I don't know I don't think they explicitly encode constraints
but for instance the way they they encode time is is a bit different than than what we use for
for language modeling and so that just the result of that is is for music at least is a more coherent
and more pleasing we have started looking a little bit in reinforcement learning and seeing how
we could potentially use that I'm just going to ask is there an overlap or interplay with our
there is so we are looking into it a little bit it's tricky it's tricky to get right and uh yeah my
thinking is always I don't want to over complexify it if I don't have to so that's why I always start
with the simplest idea and if that isn't working then okay we start considering a bit more
sophisticated things for many reasons I mean when something breaks it's easier to fix if it's
yeah if it's simpler than if it's if you have a lot a lot of moving parts um and I also feel like
with the reinforcement learning aspect of it um the little I've been playing with it with for
this particular project it's a little harder to make it stable in the sense that you can kind of
reproduce the same type of quality that from run to run so it might be interesting to an instructive
to kind of explore like what's your first step how do you think about okay you've got this one tool
that you know well or al you've got this use case area that you want to apply it to like how do
you start to even formulate the problem um it's uh this is the way I work it's more trying to solve
very specific problems that I have with so this is what I'm saying where I like to start with
just a simple thing um because it probably won't work the way you want it and and there will be
very concrete aspects that or problems that you have with it at the at the moment and so then
you I like to focus on those particular problems and see what techniques I can use so um first
is for for this rhyming constraint uh there is some prior work that that has done similar type
things not with transformers but with with R&N so um Natasha Jacques when when she did a uh uh
an internship with the magenta team she had a paper where she was using RL to
for like an R&N that produces melodies um she was trying to use RL to make it encourage it
to to respect the rules of counterpoint so counterpoint is this set of rules from music theory that
um basically specifies how to write music for polyphonic voices so that it sounds better
and so she was trying to use RL for this because the the the R&N for for melody generation on
its own wasn't respecting this necessarily at all and sometimes it was producing like just repeating
the same note over and over which is a violation of one of the rules of counterpoint so that uh she
got some interesting results but I think part of the the limitation was also just the R&N
and encoding it into the training process of the transformers so we're how are you training
these attention models I think that might be a little challenging and it would likely slow down
our training because uh the technical aspects of it of getting these RL things working on first
is TPUs um is not trivial um just because TPUs work well on on very large batches but not as well
with with smaller batches and with RL when you're dealing with this kind of online yeah so then it
becomes tricky to to make it work well in a way that you because you still want to be able to train
these things quickly um so it sounds like it's much less of a hey I've got you know chocolate
peanut butter let's get them together and see what happens oh yeah no no the specific thing uh
you know what you know start at the simplest possible way to solve it yeah and kind of work your
way yeah up in complexity exactly exactly so I don't I don't want to throw RL at just because I know
well I don't necessarily want to use it for everything because sometimes you don't need RL like
sometimes just I don't know an SVM would work just fine and then you should just use that
and it might be pointed and now you've got a couple of papers I think a poster in a paper maybe
here at NERP so that are using RL yes yeah so we had a poster this morning from our team um so
Mark Belmara was the first author and he was the one manning the poster and and he he so I'm
afterwards he his voice was very tired uh so that one is called geometric perspective on optimal
representations and reinforcement learning and so we've been thinking a lot uh our
nerd team in Montreal about representations for reinforcement learning and myself in particular
I'm quite interested in this topic and I'm doing a lot of work right now in this space um but
this this uh paper is sort of a partner paper to this other work that came out in ICML
um I wasn't on the other paper but um I was along for the voyage um the value value function
polytope so this is a paper that basically shows that the the value functions that you get
in mark of decision processes form this this polytope in this high dimensional space
and so it has this these interesting characteristics so for instance the the vertices of this
polytope correspond to deterministic policies and so the path you can look at the path that's
are the different algorithms will take along this polytope as they try to get to the optimal
policy and there's some really interesting visualizations of when you compare like valley-based
methods versus policy-grading methods and some of them actually leave the polytope in their
trajectory so they're essentially the policies that are in a space where they're not valid policies
in the sense that um they're not consistent with with with the with the system but they eventually
end up coming back and and reaching some near optimal policy so the work we have here is basically
leveraging this polytope and trying to see how we can um use it for for optimal representations and
what I mean by optimal representations is that it's not just useful for the optimal policy but you
can actually use this representation theoretically for for many different policies so there's a whole
theory behind it but essentially you're trying to find a representation that will minimize
the error that you get when you use that representation to express a particular value function
for a policy that doesn't need to be optimal so it ends up being akin to having auxiliary tasks in
a sense where uh we there's a lot of work in in the reinforcement learning field where adding auxiliary
tasks to your to your learning process is helpful and that it serves almost like a regularizer for
for your representations a multi-task yeah exactly and so these these different um policies that
you're optimizing for so you're not just trying to get this optimal policy but sort of build your
representation in a way that you can express just express these other policies quite well
they end up serving sort of as auxiliary tasks and um they can help make the representation a bit
more interpretable but also more expressive so there's some visualizations in grid worlds where you
can see if you do the regular learning process if you look at the representations you you get
certain dimensions of the representation are almost useless in the sense that they're not
very expressive in terms of the state space of what they can represent in the state space but using
these uh these they're we call them adversarial value functions um so these are the value functions
for the other policies as these auxiliary tasks you get representations that are much more expressive
and that they cover the state space a lot more um and so they're able to have a richer expressive
power for representing multiple value functions you mentioned expressiveness are you trying to
have minimal representations in a sense that they're not not necessarily with this but so part of
what we're trying to achieve is that whatever representation you end up learning isn't own
overfit to the optimal policy so let's say you tweak your your reward function a little bit and
the policy you have is no longer um optimal under this new reward scheme um but let's say for
whatever reason you maintain your your representation fixed because you just want to do linear
approximation um if you don't have a good representation then you're not going to be able to express
the new the new policy um properly and so this is what these representations are trying to do
not necessarily reduce the mentionality but just increase the expressive power so that they're
able to pretty well express the your current optimal value function but if you were to want to express
the the value function under a different policy or an under a different reward function perhaps
that it would still have a pretty good expressive power to be able to do that with low
approximation error now the concept of generalizability is applicable to both the policy itself and
the representation and so is there a relationship between the two whereas more generalizable or better
generalized policies have better generalized representations or not necessarily unless you apply
this approach that you've described so by generalizable policies do you mean like policies that are
like if you've learned a suite of policies or um I guess I'm trying uh I don't know if policy
is the right place to apply this is the question I'm asking like if I'm thinking of uh thinking
of it from the perspective of I've trained an agent that uh can maximize some uh reward in some
environment you know and once in generalization is I want to be able to put it in a slightly
different environment and have it be able to perform well right and so that being the case where
where does generalization live in that world is it in the policy that it you know or as well you
you probably end up learning a new policy um but this tells you very closely to representation
because if you're representation it's almost like you're doing fine tuning at that point so you've
trained your agent under a particular reward function and now you have your trained agent and now
you say okay well now I want this new task or this new reward function um but I don't want to
retrain from scratch I want to start from where I started so it's kind of like fine tuning uh and
so there if if your representation is highly overfit to the the first policy that you ended up
learning um it might be very difficult to to sort of switch over to to this new policy if you're
doing something even more drastic where you're saying I I already trained this agent so I'm going
to fix the representation no longer backprop um through through the rest of the network and I'm
only going to be learning learning the the last linear layer if your representation is poor you're
not going to be able to learn the new task and so there uh is where generalization comes comes
into play if you have a representation that that is expressive then when you switch the reward
function or try to learn a new policy um if your representation is expressive you you should be
able to do that reasonably well at least better than than um with a lot of the existing methods
where there is evidence that they do tend to overfit to the current policy okay and so what was the
inspiration for this paper you know coming from the original polytope paper was it driven by a
particular use case or uh so uh Mark uh he's uh introduced the distributional approach to
reinforcement learning so rather than backing up values you're back backing up distributions when
when you're when you're doing learning and so this seems to give um a lot of advantages for
for learning uh in terms of performance and there's a lot of follow-up work I've seen a bunch of
papers today particularly there were a lot of distributional papers um but it's still not well
understood how why they're they're giving that advantage so we had a paper uh triple AI this
year um with Claire Lyle uh where we were investigating where this difference comes from comes from
so the traditional way of of doing the rl backup is we call it expectational because you're taking
expectations and then you get a single number versus the distributional approach where you're
backing up uh distributions and so Claire did a lot of work and and she essentially proved that
the there is no difference under certain mild conditions there is no difference between
expectational distribution neither intitributional neither in the tabular setting nor the linear
function approximator setting so they're essentially identical distributional doesn't give you an
advantage then it's really when you go into deep networks that the the advantage of distributional
comes in and it's not always guaranteed to give you an advantage so it's just going to be different
and so sometimes it might actually hurt you um and um there's a like a counter example in the paper
where it shows that uh distributional can actually provide worse performance than expectational
so we in our team we're also very interested in in trying to understand distributional methods
more and so Mark was looking into this quite a bit and uh he came up with this idea of
basically looking at distributional at the distributional perspective almost as a auxiliary
task and through that um he he had a remember a good lunch meeting with Dale Schoenman's and I
think that's where they came up with this idea of the adversarial value functions um but it came
from this initial idea of let's try to understand distributional methods a bit okay and you've got
another uh paper that is being presented in the financial yeah yeah so that's so this is a
collaboration with the Bank of Canada I mean they they've done most of the work I'm more on an
advisory well on an advisory role so um they a lot of them their team their economists so they're
not as familiar with reinforcement learning so that's where I come in and so I just basically
they show me what they're thinking and and we we had a lot of brainstorms at the beginning to
try to frame the the the problem properly so that it sort of satisfies their does it route up for
for their economic theory but also that it makes sense from a reinforcement learning perspective
and so what they're looking at they're part of the Bank of Canada which is part of the government
and uh one of the tasks the main tasks of of the Bank of Canada is to make sure that the economy
is stable and so one of the things they they look at is is the interbank payments that happen
on a daily basis between different banks in Canada so um Bank A owes Bank B some money and so it
may send it throughout the day and so if Bank B has has that extra equity then Bank B can pay
other banks and so how they how they manage these payments affects how much how many of the payments
they can make and how many payments they receive and so not making payments in time can give them
interest penalties and these types of things and so the Bank of Canada plays a role as an
intermediary to try to regulate these things so that you don't have um problems where where there's
a bank that's not making any of the payments and all of the other banks are stuck and not making
able to make any other payments as you're stuck in the stalemate so it's a very complex problem
it's a very dynamic problem and they're interested in in looking at this obviously from the
economics theory perspective but also they're interested in seeing if you can simulate some of these
uh dynamics via reinforcement learning so we've been looking at um framing it as a multi-agent
reinforcement learning problem where each of the agents are the different banks in question
and uh seeing trying to train them to to learn optimal policies and their co-learning so all
of the agents are sort of learning independently and you get some interesting dynamics as as this
is happening so the workshop the paper that we have here is still very preliminary work um where
we're essentially trying to demonstrate that this is even feasible um so as I said I like to take
things from the simple uh angle and then grow from there so we're we're decomposing the the big
problem of solving this interbank payment system um with decomposing it into smaller sub-problems
that we can analytically find the solution for um that's what the economists know how to do well
um and we can sort of validate that uh reinforcement learning is able to to simulate that faithfully
and so far we've we've had a pretty good success with that and so now we're starting to combine some
of these sub-problems and go into the more challenging tasks what are some examples of the kind of
granularity of the sub-problems so the two sub-problems we're considering in this paper the
the basically the simplest that you can consider one is um each bank at the uh start of the day
can choose how much liquidity it's going to bring into start making payments so it's a prediction
problem because you have to this one is almost like a bandit problem because you choose initial
liquidity so like basically pulling one of the the bandit arms and then based on that initial
liquidity that um determines how much you'll be able to pay so obviously if there were no cost
to that you would just pull all of your liquidity and then you'd have as much money as you need to
to make all the payments but there's a cost to to pulling um initial liquidity because this is
coming from the bank of Canada it's almost like you're borrowing money from the bank of Canada
to be able to make payments and then at the end of the day you return it um to the bank so this is
one of the sub-problems that this was more like a bandit problem so they've run some simulations
with with uh log data historical log data that they have um and then the other problem is the
intraday payment so they the if you think of the day divided into hours at each hour you can
make a decision of paying a particular bank that you owe money to um or not um so you may owe
money to to a bunch of different banks if you don't pay them back then that bank might not have
enough liquidity to pay you back um and so there's is when you start adding adding more agents then
this becomes more and more complex and so this one is is less of a bandit problem it's more of a
sequential decision-making problem and for in order to decompose them for the first bandit problem
we're essentially keeping the intraday payment fixed in the sense that the the the choice that
you made at the beginning doesn't really affect the dynamics of what happens later it's a fixed
policy and for the second problem we keep essentially assuming giving the agents as much liquidity as
they want so the the problem is very simple there all they have to do is make all the payments
that that they need to make but because there's this multi-agent interaction um they don't
necessarily always will find that optimal policy when you first started describing the problem
one of the thoughts that came to mind was kind of a graph of the individual banks I don't know
that I've heard much conversation about kind of the intersection of graph and reinforcement learning
is there a war happening there um there's a little bit there's a little bit especially in the
deterministic I believe I saw a paper come out recently where they're decomposed they're
basically using graph algorithms for solving uh um certain reinforcement learning tasks so you
can do um value function approximation by by using different types of of graph learning algorithms
in this particular case I mean they are basically representing a graph the the connections between
these these uh kind of the actual transactions are not necessarily yeah an actual thing between
the right and and so most of the work I've seen with at the intersection of of graph whether
be with their own networks or not and reinforcement learning is in the single agent setting where the
the graph is more representing the environment where's in this case the graph is is representing
the connection between the agents so you have multiple agents that they're not sharing parameters
so they're kind of independent agents and the multi-agent setting um it's not something I have
done a lot of work on so it's also been kind of interesting for me to learn more about the literature
it's a really challenging problem um you have a lot of game theoretic aspects uh to it um and it's
not a clear for for many problems there's no clear solution you have like Nash equilibria but
it's it's that's as good as you can get for for many problems and uh so I haven't in that space
I haven't seen much with with graphs okay with graph theory cool well uh Pablo thanks for taking
a time to chat with us and share a bit about what you're up to thanks so much for chatting with me
yeah absolutely thank you all right everyone that's our show for today for more information on
today's guests visit twomla.com slash shows thanks so much for listening and catch you next time
