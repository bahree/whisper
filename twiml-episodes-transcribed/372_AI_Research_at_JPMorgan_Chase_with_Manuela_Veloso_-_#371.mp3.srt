1
00:00:00,000 --> 00:00:15,680
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:15,680 --> 00:00:27,880
Alright everyone, I am on the line with Manuela Veloso. Manuela is the head of AI Research

3
00:00:27,880 --> 00:00:34,060
at JP Morgan Chase and a professor at Carnegie Mellon University. Manuela, welcome to the

4
00:00:34,060 --> 00:00:38,800
Tumel AI Podcast. Yeah, it's great to be here. Thanks for having me.

5
00:00:38,800 --> 00:00:45,880
Absolutely. It is wonderful to get a chance to speak with you. You are currently in New

6
00:00:45,880 --> 00:00:52,680
York City where you are recording this from your home on lockdown. How are things there?

7
00:00:52,680 --> 00:01:02,040
So again, we are on lockdown. I'm in the apartment. I've been here for three weeks but surprisingly

8
00:01:02,040 --> 00:01:10,080
I continue to be very productive in terms of work because with all of our virtual meetings

9
00:01:10,080 --> 00:01:16,640
everything seems to be going quite well. What is challenging is to exercise and to make

10
00:01:16,640 --> 00:01:23,100
sure that you get some fresh air and some sun. But from the work point of view, I miss my

11
00:01:23,100 --> 00:01:28,720
team but we have a lot of Zoom meetings. It seems to be very successful.

12
00:01:28,720 --> 00:01:33,760
Did you work from home quite a bit beforehand or is this a transition for you?

13
00:01:33,760 --> 00:01:39,560
So Sam, I came to the United States in 1984 and I've never worked from home ever.

14
00:01:39,560 --> 00:01:48,600
Wow. I know. I know. I know. I mean, it was like these like, you know, getting up in

15
00:01:48,600 --> 00:01:54,040
the morning, making your bed, getting ready and just leave every morning and then come

16
00:01:54,040 --> 00:02:00,440
back in the evening. So this is quite new to me to be here at home every day but somehow

17
00:02:00,440 --> 00:02:05,200
it says it's been working. Are you finding yourself more or less productive

18
00:02:05,200 --> 00:02:10,720
than like, how do you compare your level of productivity with this new way of working?

19
00:02:10,720 --> 00:02:17,920
I think that with the setup we created at JP Morgan in which we try to have a lot of

20
00:02:17,920 --> 00:02:23,600
meetings so that eventually we see each other as often as possible. I think it's working

21
00:02:23,600 --> 00:02:32,280
fine. I'm not 100% sure in three weeks if we are still in the same productive chain.

22
00:02:32,280 --> 00:02:35,560
But I think we have been quite productive. I mean, more than what I thought the first

23
00:02:35,560 --> 00:02:41,960
couple of days, I thought why that I would like be eating all day in the kitchen or doing

24
00:02:41,960 --> 00:02:48,720
my laundry or something. What is this of being home? But no, I mean, my office, I have the

25
00:02:48,720 --> 00:02:55,600
computer and I'm literally in my office from, you know, 18 in the morning to 16 in the afternoon.

26
00:02:55,600 --> 00:03:04,360
So I'm glad to hear that everything is going well on your end. Thank you. Why don't we jump

27
00:03:04,360 --> 00:03:12,040
into your background? You are currently on leave from current email and CMU where you

28
00:03:12,040 --> 00:03:18,120
were faculty in the Department of Computer Science. Tell us a little bit about your background,

29
00:03:18,120 --> 00:03:23,480
how you got involved and interested and involved in machine learning research and your

30
00:03:23,480 --> 00:03:31,200
kind of broad research interests. Yeah. Yeah. So this is like a very nice to explain. So somehow

31
00:03:31,200 --> 00:03:38,360
as a kid, I got very much interested in math. Math was really what I like. arithmetic

32
00:03:38,360 --> 00:03:42,840
from arithmetic all the way to calculus or everything. Math is what I wanted, what I

33
00:03:42,840 --> 00:03:49,080
like to do. In fact, I thought I would become a math teacher when I was like 14, 13, 15

34
00:03:49,080 --> 00:03:57,280
years old. And my father is a mechanical engineer and my mom also kept saying, no, you should

35
00:03:57,280 --> 00:04:03,960
be an engineer. So I became an electrical engineer and I was also very happy with the amount

36
00:04:03,960 --> 00:04:09,640
of math I had there. And it was very interesting. And okay, that's how I became an electrical

37
00:04:09,640 --> 00:04:15,680
engineer. And in fact, I am one of the people that I never read a single science fiction

38
00:04:15,680 --> 00:04:24,160
book. I didn't like science fiction movie. Zero, zero. So I'm not at all this typical

39
00:04:24,160 --> 00:04:32,560
care that loves computers and robots and aliens and start tracks and companies zero. It's

40
00:04:32,560 --> 00:04:38,560
true. I only cared about this math and what it could do for you in your daily life. And

41
00:04:38,560 --> 00:04:44,560
a lot of like interested on teaching math to people, very, very kind of stick. And then

42
00:04:44,560 --> 00:04:49,760
when I finished this, I'm just going to tell you what made me like a change in some sense.

43
00:04:49,760 --> 00:04:56,760
So when I finished my undergrad degree, I was still in Lisbon. So I was a, I am from Portugal

44
00:04:56,760 --> 00:05:04,760
from Lisbon. And I was involved in a master thesis in electrical and computer engineering

45
00:05:04,760 --> 00:05:12,760
in a project that was in the early 80s, very, very innovative in which we had to buy computers

46
00:05:12,760 --> 00:05:20,200
and automate the production of a company of a line. So the actual, there was a company

47
00:05:20,200 --> 00:05:25,440
that produced it freezers and refrigerators. It's just a manufacturing company. And all

48
00:05:25,440 --> 00:05:32,040
the orders in Venturi tracking, everything was then manually. And so we were supposed

49
00:05:32,040 --> 00:05:36,760
to work on how do we computerize, digitalize all that. That's very early.

50
00:05:36,760 --> 00:05:43,480
That sounds like a great project. I know. I mean, I still, I mean, I, this was fantastic.

51
00:05:43,480 --> 00:05:49,000
And then through this, through talking with people, so how do you represent these? How

52
00:05:49,000 --> 00:05:52,960
do you write that? How do you generate the list of parts? How do you do that? How do you

53
00:05:52,960 --> 00:05:59,520
do that? I became fascinated by what computers could actually do in terms of representing

54
00:05:59,520 --> 00:06:04,280
knowledge, in terms of search, in terms of learning from feedback, in terms of accumulating

55
00:06:04,280 --> 00:06:10,160
data in a digital way. I mean, so that was the beginning of AI for me was basically this

56
00:06:10,160 --> 00:06:17,720
engineering of getting like the functioning that we used to be done manually done by machines.

57
00:06:17,720 --> 00:06:24,240
So that's basically no really wanting to understand how does the brain work or how do

58
00:06:24,240 --> 00:06:31,400
I mean, it was this functional engineering goal of getting these computers to do more

59
00:06:31,400 --> 00:06:38,680
than just numerical computations, but to capture behavior, choices, learning, data, fascinating

60
00:06:38,680 --> 00:06:46,520
kind of like two years of my life in which I became in love with AI. Wow. Yeah.

61
00:06:46,520 --> 00:06:52,320
And so that was after undergrad. How did you then develop that interest?

62
00:06:52,320 --> 00:07:00,800
So then I was finished my masters and for family reasons, we came to the United States

63
00:07:00,800 --> 00:07:09,640
and I did another masters in computer science now at Boston University. And then I applied

64
00:07:09,640 --> 00:07:15,520
for a PhD in computer science and I went to Carnegie Mellon. And at Carnegie Mellon,

65
00:07:15,520 --> 00:07:21,160
if you think about Carnegie Mellon, that's like the, I mean, without offending anybody,

66
00:07:21,160 --> 00:07:26,760
but in those days, that's the mecca of AI. You know, so it was herb Simon there, Ellen

67
00:07:26,760 --> 00:07:32,480
Newell, Raj Reddy, Jaime Carbonell, Tom Mitchell, Jeff Finton in those days was there, Mark

68
00:07:32,480 --> 00:07:39,760
Raybird. I mean, it was all these people that thought nothing else than AI, literally.

69
00:07:39,760 --> 00:07:50,000
I remember when I came in 86, there was this meeting of the incoming students and the faculty

70
00:07:50,000 --> 00:07:54,560
would present themselves. And there was Raj Reddy that walked into the room and said,

71
00:07:54,560 --> 00:08:02,080
hey kids, imagine if all these walls, these cement walls would be displaying live the

72
00:08:02,080 --> 00:08:09,560
best masterpieces of the Louvre. This was like before we even had warloads. I'm telling

73
00:08:09,560 --> 00:08:15,840
you, this was all about taking big an Allen, you will say in 1987 or 86, what if all the

74
00:08:15,840 --> 00:08:22,120
people in their dorms could log into their computers and be able to know whatever is

75
00:08:22,120 --> 00:08:28,520
happening everywhere. And we were like, oh my god, you know, it's true. So I came here

76
00:08:28,520 --> 00:08:33,680
to the United States first in Boston and then I landed at Carnegie Mellon and I thought,

77
00:08:33,680 --> 00:08:39,400
where am I? I mean, this world of computers doing it all was fascinating at the same time

78
00:08:39,400 --> 00:08:45,080
that we were like trying to be chess for them, the Kasparov and all these chess playing

79
00:08:45,080 --> 00:08:54,000
and all sorts of, you know, anyway, Carnegie Mellon was for me and throughout the time

80
00:08:54,000 --> 00:08:59,280
was there, like this place where we dream big and we get amazing things accomplished. And

81
00:08:59,280 --> 00:09:05,720
just so you know, actually also in 1986 when I came to CMU, there were already autonomous

82
00:09:05,720 --> 00:09:13,200
cars there. And in fact, there was this car that was coming down the campus and in the

83
00:09:13,200 --> 00:09:17,320
morning, we would see by the library and then in the evening, when we would leave, it

84
00:09:17,320 --> 00:09:23,120
had moved like, I don't know, maybe like a hundred feet, I don't know. And it was supposed

85
00:09:23,120 --> 00:09:28,840
to be a big accomplishment that the thing had moved that far by itself. And this is

86
00:09:28,840 --> 00:09:35,000
the late 80s. So that's how I grew this, I mean, grew this passion for AI was a product

87
00:09:35,000 --> 00:09:40,440
of Carnegie Mellon and this dreaming big of all the things that machines could actually

88
00:09:40,440 --> 00:09:45,840
do. And it was fascinating thing. Well, it sounds like an amazing environment and time

89
00:09:45,840 --> 00:09:54,640
to be there. So fast forward to your faculty career. What, you know, it's also a little

90
00:09:54,640 --> 00:10:02,040
bit about your research interests. So in some sense, I build an interest always on building

91
00:10:02,040 --> 00:10:10,680
complete AI systems. A lot of my research has been on trying to connect the perception part

92
00:10:10,680 --> 00:10:16,520
with the cognition part and with the action, perception, cognition and action. I think

93
00:10:16,520 --> 00:10:23,840
that that first slide has been in my talk since for always. Because in some sense, also

94
00:10:23,840 --> 00:10:30,440
falling up on things that Alan Newell was telling us, AI has been a science of components,

95
00:10:30,440 --> 00:10:37,040
natural language processing, vision processing, planning, search, machine learning, speech

96
00:10:37,040 --> 00:10:44,920
recognition. So it has been a science of lots of interest, lots of dimensions. And I always

97
00:10:44,920 --> 00:10:49,320
thought that it would be good to put it all together as an agent that could do like the

98
00:10:49,320 --> 00:10:54,440
vision and not just the vision to classify this as a cup, but then to pick it up and not

99
00:10:54,440 --> 00:11:00,200
just to pick it up, but then to actually take it somewhere and not to take it somewhere,

100
00:11:00,200 --> 00:11:04,840
but actually to execute not just plan, but execution and all the problems of perception,

101
00:11:04,840 --> 00:11:09,240
cognition and action. And then that led me to spend all my life, all my life, building

102
00:11:09,240 --> 00:11:15,440
autonomous robots. So I've built a lot of autonomous robots, not necessarily autonomous cars.

103
00:11:15,440 --> 00:11:22,600
I never worked that much in outdoors robotics. I worked in indoor robotics. And my passion

104
00:11:22,600 --> 00:11:30,800
was not necessarily on manipulation, but on mobility. So from robot soccer to mobile robots

105
00:11:30,800 --> 00:11:38,600
in the environment, I've built tons of different robots that moved, interacted with people,

106
00:11:38,600 --> 00:11:44,840
could see the image, could see actually could do perception in real time, detect an orange

107
00:11:44,840 --> 00:11:52,520
ball in Robocup or other things. So I've been, I spent all my life building autonomous robots,

108
00:11:52,520 --> 00:12:00,520
capable of this perception, cognition, deciding where to go, and actually going. And that's

109
00:12:00,520 --> 00:12:08,680
what I've done in the late 90s. We started mid-90s, we started, I co-founded this Robocup initiative

110
00:12:08,680 --> 00:12:15,880
in which robots would play soccer in different leagues and all sorts of like different conditions,

111
00:12:15,880 --> 00:12:25,160
but basically the fascination was the autonomy. Even today, when I look at videos from robot soccer,

112
00:12:25,160 --> 00:12:32,520
the only thing I appreciate is thinking there was all done, it was all done by an algorithm.

113
00:12:32,520 --> 00:12:40,120
It was all automated. It's not exactly just the learning part, but it's actually the

114
00:12:41,160 --> 00:12:47,880
ability to frame a whole problem into something that the computer can actually do from beginning

115
00:12:47,880 --> 00:12:55,880
to end. I don't think we've talked about the Robocup challenge on the podcast previously. Can

116
00:12:55,880 --> 00:13:03,000
you frame the problem for us? I think my exposure to it has primarily been through watching

117
00:13:03,000 --> 00:13:08,680
blooper reels, which probably isn't representative. You know, the research accomplishment.

118
00:13:08,680 --> 00:13:16,760
Yeah, so Robocup started around like 94, 93, 92, and then the first competition,

119
00:13:16,760 --> 00:13:27,720
actual competition was in 1997 in Japan. And basically, so think about those days. In the 90s,

120
00:13:27,720 --> 00:13:35,080
there was like the blooming of the internet, a lot of like, you know, competitions on speech

121
00:13:35,080 --> 00:13:42,280
recognition, text understanding, and the physical world was not as much the focus in those days

122
00:13:42,280 --> 00:13:49,320
of what AI was doing. And the robot soccer came about to address this problem of

123
00:13:50,200 --> 00:13:57,000
of processing information, but at the physical world level. So robot soccer came about, in fact,

124
00:13:57,000 --> 00:14:04,120
as a challenge also for multi robot reasoning, the robots had to work in a team, they had to be able

125
00:14:04,120 --> 00:14:10,760
to have an algorithm that looks at the field, at the playing field, and decides where to pass the

126
00:14:10,760 --> 00:14:17,240
ball, and eventually had a very concrete goal, which was winning. So you had to score these goals.

127
00:14:17,240 --> 00:14:24,120
So that has motivated thousands and thousands of people. Robocup is even as if today

128
00:14:25,080 --> 00:14:30,360
happens every year, and it brings people like, it's limited. The number of teams that can come,

129
00:14:30,360 --> 00:14:35,320
but it's more than 3,000 people every year that come from all over the world. And the beautiful

130
00:14:35,320 --> 00:14:40,920
thing about Robocup is these autonomous. They are all autonomous. There has nobody joysticking

131
00:14:40,920 --> 00:14:47,560
these robots. There is nobody like telling anything the robots, the complete system has to be

132
00:14:47,560 --> 00:14:55,720
autonomous. Anyway, at the beginning, they barely moved. Yeah, yeah, it's true. They barely moved.

133
00:14:55,720 --> 00:15:00,520
We had a big problem with this connection of the perception and the reasoning. By the time we

134
00:15:00,520 --> 00:15:06,520
had the robot decide to go to the ball, the ball wasn't there anymore. So you had the camera that

135
00:15:06,520 --> 00:15:13,560
would process the image of the ball, that it was too slow, it was too noisy, it was everything was

136
00:15:13,560 --> 00:15:22,760
very hard to make the robot really act by processing its own images. Anyway, so that's a long story,

137
00:15:22,760 --> 00:15:28,680
and but only I had it put me always on to this mode of getting things that work.

138
00:15:28,680 --> 00:15:34,840
Anyway, so that's went from there to having like co-bots, which are these robots that move down,

139
00:15:34,840 --> 00:15:43,560
the corridors that Carnegie Mellon, they have moved thousands of kilometers in our environments,

140
00:15:43,560 --> 00:15:50,520
by themselves, not thousands, thousands and many, so close to 2000 maybe now. But again,

141
00:15:50,520 --> 00:15:56,440
like this goal of trying to put it all together, perception, cognition and action, data, reasoning

142
00:15:56,440 --> 00:16:03,000
and action. So the processing of the data combined with what we need these data for, and then eventually

143
00:16:03,880 --> 00:16:09,480
deciding and executing. So that was my life. There has been always my life. A lot of learning,

144
00:16:09,480 --> 00:16:15,320
but at the reinforcement learning level, learning at the action selection level, learning at the

145
00:16:16,120 --> 00:16:22,680
execution level, learning to improve perception with experience. As I keep calling these learning

146
00:16:22,680 --> 00:16:29,800
from experience, not necessarily the technique itself, whether it was neural nets or decision trees

147
00:16:29,800 --> 00:16:36,120
or SVNs or whatever method we use was more of these concepts of learning from experience.

148
00:16:36,680 --> 00:16:42,120
And so with this, with the kind of background and passion in robotics, how did you end up at

149
00:16:42,120 --> 00:16:49,560
JP Morgan Chase? I don't see many robots, you know, when I go to the bank. Wait, wait, wait,

150
00:16:49,560 --> 00:16:53,560
wait, wait, wait, wait, but I'm just saying. So that's very interesting. So it's true. I mean,

151
00:16:53,560 --> 00:17:00,600
I was quietly at Carnegie Mellon doing my research with a large group, many students, beautiful.

152
00:17:00,600 --> 00:17:07,160
And I was actually at the time in 2016, since 2017, I was the head of the machine learning department.

153
00:17:07,880 --> 00:17:16,520
And I got a call from JP Morgan asking, so telling us, telling me about these

154
00:17:16,520 --> 00:17:24,520
efforts, they were trying to start bringing basically AI reasoning, AI research, really, not

155
00:17:24,520 --> 00:17:35,880
just applied AI, but AI research to the firm. And what actually made me come to JP Morgan,

156
00:17:35,880 --> 00:17:42,360
which I'm very happy to be here at JP Morgan Chase, was in fact, the thinking like this,

157
00:17:42,360 --> 00:17:49,720
oh my god, this is a new area that I never thought about. And it was this fascination of thinking,

158
00:17:49,720 --> 00:17:54,600
oh my god, I have a chance of doing something in another area. It's almost as if I,

159
00:17:55,240 --> 00:18:00,840
robotics was beautiful, there was a lot to do, but I was at home in robotics. I mean, what,

160
00:18:01,720 --> 00:18:07,400
more image less image, more object less object, more command was kind of like, but then it was

161
00:18:07,400 --> 00:18:13,480
this chance. And then there was this major comment from the leadership at JP Morgan Chase that

162
00:18:13,480 --> 00:18:19,800
made the difference. They said, many of the companies, Google's, Amazon's, Facebook, Twitter,

163
00:18:19,800 --> 00:18:25,160
they were all born digital. They were all born, thinking about machine learning, thinking about

164
00:18:25,160 --> 00:18:30,200
data, thinking about computing, thinking about algorithms, everything was born digital. But all these

165
00:18:30,200 --> 00:18:37,640
other industries, like financial health, the construction, the cities, you name it existed well

166
00:18:37,640 --> 00:18:45,240
before the computers existed. So that was fascinating to me to think that I could come and I'm here

167
00:18:45,240 --> 00:18:52,840
to try to see how to bring more AI, my experience of AI, building these systems from beginning to

168
00:18:52,840 --> 00:18:59,640
end, can make a difference in terms of like the financial domain. So that's basically what brought

169
00:18:59,640 --> 00:19:06,920
me here. And you know, it was basically, it was somehow a mixture of being afraid, you know,

170
00:19:06,920 --> 00:19:14,840
being like, you know, apprehensive and all, an enormous excitement to be in a different thinking

171
00:19:14,840 --> 00:19:22,120
mode. So these are the two things. And the group that you run there, it's a research organization

172
00:19:22,120 --> 00:19:27,960
as opposed to, you know, building churn models for credit cards or something like that.

173
00:19:27,960 --> 00:19:35,800
Exactly. So we try to, we are a research group and we try to focus on, how can I say,

174
00:19:35,800 --> 00:19:43,880
aspirational goals? We try to focus on things that would make an impact if they were

175
00:19:43,880 --> 00:19:51,480
solved one day. So for example, I can, if this is okay, I can just share a few of the goals.

176
00:19:51,480 --> 00:19:58,120
We'll just focus on a few, but we kind of structured this AI research after a one year of being

177
00:19:58,120 --> 00:20:05,640
there absorbing what this financial domain was all about into seven big research goals,

178
00:20:06,440 --> 00:20:10,600
which we call these aspirational goals, these big research goals. And they are like this.

179
00:20:10,600 --> 00:20:17,720
Three of them have to do a lot with the actual domain itself and then use of the domain. And so

180
00:20:17,720 --> 00:20:25,320
they are. So how do you use a AI to really eradicate financial crime? Not to decrease the number of

181
00:20:25,320 --> 00:20:30,360
positive false positives or false negatives, the alerts, the frauds, the many laundry, but really,

182
00:20:30,360 --> 00:20:36,200
do we eradicate financial crime? It's like the cure cancer kind of goal. How do you do that? So

183
00:20:36,200 --> 00:20:41,320
that's why it's part of the research. It's like, how do you eradicate this financial crime and

184
00:20:41,320 --> 00:20:46,600
do a use a guy for that? Then another goal that is very important is this goal of like, how do we

185
00:20:46,600 --> 00:20:54,200
actually, we call it liberate data safely? How do we go about engaging with universities,

186
00:20:54,200 --> 00:21:01,240
making sure that people can help us and even in the firm, we can all share data and can make secure

187
00:21:01,240 --> 00:21:07,560
computations in a way that's safe private. I mean, another big goal. How do you go about doing

188
00:21:07,560 --> 00:21:14,120
this in a world full of like privacy issues and encryption issues and messiness of the data and

189
00:21:14,120 --> 00:21:21,720
ownership of data? How do we go about liberating these data safely? The third goal is the very

190
00:21:21,720 --> 00:21:26,920
interesting goal that has a lot to do with by Robotsocker when we're another, which is looking at

191
00:21:26,920 --> 00:21:33,800
this problem of the financial domain as a large, large multi-agent system. There are many

192
00:21:34,920 --> 00:21:42,520
interacting pieces. So we want to predict and affect these large complex economic systems

193
00:21:42,520 --> 00:21:50,040
in a way that we can actually make a difference in terms of like understanding what makes them,

194
00:21:50,040 --> 00:21:57,160
their dynamics and what makes them actually, how do they function in such a complex way?

195
00:21:57,160 --> 00:22:03,960
And so we do a lot of simulations, very simulation, very multi-agent simulations, very much like trying

196
00:22:03,960 --> 00:22:13,080
to understand the components of this very complex global problem with all, and it's a, I mean,

197
00:22:13,080 --> 00:22:18,360
again, it's a huge goal if we could one day understand how the multiple countries, the multiple

198
00:22:18,360 --> 00:22:26,040
parties, the modern times, virus, no virus, how can these, all these things affect the financial

199
00:22:26,040 --> 00:22:32,680
health of a society of individuals and what is this all about? So these three goals are very much

200
00:22:32,680 --> 00:22:38,440
like finance-oriented goals. And then we also have three goals that are very beautiful too,

201
00:22:38,440 --> 00:22:44,440
because they capture the stakeholders of these financial domain. And basically, the stakeholders

202
00:22:44,440 --> 00:22:51,000
are your own employees, your clients, and the regulators. And so there are these three

203
00:22:51,000 --> 00:22:56,840
components that you have to care about and try to bring a guy to improve the life of your

204
00:22:56,840 --> 00:23:03,320
own employees to help them move up in the value chain. You want to perfect the experience of our

205
00:23:03,320 --> 00:23:10,920
clients, and you also want to be able to agentize or to be able to automate this policy understanding,

206
00:23:10,920 --> 00:23:17,880
this policy compliance, this policy compliance in some sense. So you have these three other goals

207
00:23:17,880 --> 00:23:23,400
that have to do with people, with rules, with handling like how do we do the employees,

208
00:23:23,400 --> 00:23:28,360
that clients, and the regulators as well. And the seventh goal is kind of an overarching goal,

209
00:23:28,360 --> 00:23:37,320
which is how do we make these AI be ethical and taking into account the social good of the whole

210
00:23:37,320 --> 00:23:44,760
story. So we have these establishing ethical and socially good AI as the seventh kind of overarching

211
00:23:44,760 --> 00:23:50,520
goal. So that's what we do at JP Morgan Chase in terms of AI research is worked towards these

212
00:23:50,520 --> 00:23:57,160
seven goals. You spent the first year kind of studying the domain and articulating

213
00:23:57,160 --> 00:24:01,720
these goals. And you've been there a year and a half or so. Have you made much progress in any of

214
00:24:01,720 --> 00:24:07,720
these areas? Or how are you, you know, where have you made the most progress? So very good question.

215
00:24:07,720 --> 00:24:15,480
So as you can imagine, I started one person myself alone. So yeah, but you know, I did very

216
00:24:15,480 --> 00:24:22,440
different from CMU exactly. However, I was like surrounded by 250,000 people at JP Morgan Chase.

217
00:24:22,440 --> 00:24:29,320
So it would kind of like overwhelming also the scale. But on the other hand, the JP Morgan Chase

218
00:24:29,320 --> 00:24:36,200
happened to have been very, it has been a delightful journey because of the support that the

219
00:24:36,200 --> 00:24:42,920
leadership and everybody has given to these kind of like AI thinking. So it has been very nice.

220
00:24:42,920 --> 00:24:52,360
So we are currently 30 people. We are 31 or 32 or 29 with the offers we have somewhere around 30.

221
00:24:52,360 --> 00:24:59,080
And we kind of have worked in kind of like how we think about things is like all the projects

222
00:24:59,080 --> 00:25:04,920
somehow that we work on be it like us thinking about things or like the business asking us to

223
00:25:04,920 --> 00:25:10,840
think about things are all towards one of these goals. One way or another. Now everything is in

224
00:25:10,840 --> 00:25:16,360
place. I'll give you an example of two because I think maybe three. But I'll tell you one thing that

225
00:25:16,360 --> 00:25:25,000
is one example that is very nice. So basically we knew I'll give you this example first.

226
00:25:25,000 --> 00:25:30,920
We knew that the actual success currently a lot in machine learning had been like this

227
00:25:30,920 --> 00:25:40,120
neural net deep learning. And and we also I also know that deep learning is very successful on images,

228
00:25:40,120 --> 00:25:48,120
on images in particular. So when I was brought to visit the traders floor. So I was in these

229
00:25:49,000 --> 00:25:54,760
in London actually I was in London at JP Morgan in London and they brought me to this floor which

230
00:25:54,760 --> 00:26:00,520
is like well what we see in the movies right. So everybody's surrounded by screens with all these

231
00:26:00,520 --> 00:26:06,920
little kind of like graphs on the screens and people are making choices of sell no sell by no buy

232
00:26:06,920 --> 00:26:14,040
windows. And literally they I had this tour of that floor and they were explaining to me all these

233
00:26:14,040 --> 00:26:21,000
things and I was fascinated that I couldn't see anything else but images. Those screens that people

234
00:26:21,000 --> 00:26:27,480
were looking at they were not apples and orange they were not cats and dogs but they were these

235
00:26:27,480 --> 00:26:34,280
images of these little kind of graphs. And a lot of research has been done on applying math to

236
00:26:34,280 --> 00:26:41,480
predict and to this time series data but I could only see images. So I went to my lab and worked

237
00:26:41,480 --> 00:26:47,240
with us few of my colleagues in those those days and we said can we use these with a neural net

238
00:26:47,240 --> 00:26:55,000
to try to classify the images on their screens as by no buy choices. And we built this system

239
00:26:55,000 --> 00:27:01,640
and fascinating system we called it Mondrian. And we built this system we were able to use those

240
00:27:01,640 --> 00:27:09,880
images and with historical data on S and P 500 decisions we were able to reproduce 95 percent

241
00:27:09,880 --> 00:27:16,200
accuracy by just using images images or the by no buy choices that humans had made.

242
00:27:16,680 --> 00:27:23,960
That is fascinating. Surprisingly it is counterintuitive like the images are you know kind of

243
00:27:23,960 --> 00:27:29,320
dumbing down the data so that we as humans can interpret it why not just use the data itself.

244
00:27:29,320 --> 00:27:35,000
Well we could use the data itself but it doesn't do as well the data needs to be normalized like

245
00:27:35,000 --> 00:27:40,440
you know it's more the shape that we are capturing because the data the value 5 6 7 doesn't

246
00:27:41,160 --> 00:27:46,520
it's going to confuse a little bit the learning thing while the shape was remarkable at

247
00:27:46,520 --> 00:27:53,160
capturing it. And but on the other hand if you use also a good representation of the actual numbers

248
00:27:53,160 --> 00:27:59,880
it also works except that because we looked at how humans made decisions based on those numbers

249
00:27:59,880 --> 00:28:06,280
on the optional images images we then could do images for many other things that we have been doing

250
00:28:06,280 --> 00:28:16,200
that are not objects like we have traditionally applied to at least in those days to deep deep

251
00:28:16,200 --> 00:28:23,960
learning but they were images of other things of other processes of diet of other images of

252
00:28:23,960 --> 00:28:29,640
other visualizations of other decision making environments. I think about the doctor also they

253
00:28:29,640 --> 00:28:35,560
basically came look at your EEG data and make a decision normal normal normal and that decision

254
00:28:35,560 --> 00:28:41,720
comes from looking at an image and and so it's fascinating this concept was fascinating and

255
00:28:41,720 --> 00:28:48,840
it's this concept of associating this time series data to an image that then you can actually

256
00:28:48,840 --> 00:28:56,040
classify for decision making and we we it's a that was a very nice project. Is your ground truth

257
00:28:56,040 --> 00:29:03,560
in that example is it the decisions that the trader made or the performance no no no no no no

258
00:29:03,560 --> 00:29:08,920
so the decisions that traders made we are not yet like beating the whole world by using these

259
00:29:08,920 --> 00:29:15,640
images no we are you know I'm thinking of like all of the you know the pictures of some of these

260
00:29:15,640 --> 00:29:22,360
financial you know stock charts and you know how many different kind of voodoo incantations

261
00:29:22,360 --> 00:29:27,000
there are about this curve crossing that curve and this shape and that shape and it sounds like

262
00:29:27,000 --> 00:29:34,840
you're yeah in fact capturing some of that exactly exactly and I tell you I'm telling you

263
00:29:34,840 --> 00:29:43,240
you cannot imagine how much information about that correlates with the decisions is visually

264
00:29:43,240 --> 00:29:49,480
visible and in fact you just said the crossing the the the shape going up going down the other

265
00:29:49,480 --> 00:29:56,600
one coming down and down is very visible in an image as a feature and to the I mean to extract

266
00:29:56,600 --> 00:30:03,480
that from data from the actual numbers is another representation that is more that's different

267
00:30:03,480 --> 00:30:09,960
so I basically build upon the success we knew that images have the with machine learning

268
00:30:09,960 --> 00:30:16,120
to transform this problem into an image classification problem an image understanding project

269
00:30:16,120 --> 00:30:21,400
problem and it was very successful and it has been very successful successful and I'm telling you

270
00:30:21,400 --> 00:30:27,400
we are now doing prediction basically by looking at what are the what we don't know but what are

271
00:30:27,400 --> 00:30:34,440
the images what would the future which image would be the be the future that less disturb I mean

272
00:30:34,440 --> 00:30:41,400
anyway we are able to predict the images of the future and so it's a you know it's it's really

273
00:30:41,400 --> 00:30:46,600
beautiful so that's like to show you that you know it's a transformational way of looking at

274
00:30:46,600 --> 00:30:52,920
something that is very classical eventually making decisions by no buy on time series data but I

275
00:30:52,920 --> 00:30:58,680
looked at it as an image so that was another one that's very nice and then I will stop is just

276
00:30:58,680 --> 00:31:07,800
that we also have been trying to understand this problem of a client experience or fraud or all

277
00:31:07,800 --> 00:31:14,760
sorts of problems that we need to classify into let's think about fraud from a behavior point

278
00:31:14,760 --> 00:31:19,880
of view and I give you an example that is very important for us to understand so if you and I

279
00:31:19,880 --> 00:31:27,720
would have been asked to classify in New York City which of the cars are taxes and which ones are

280
00:31:27,720 --> 00:31:34,680
personal cars you know 10 years ago you and I use only the color of the car right you and I

281
00:31:34,680 --> 00:31:41,480
would say yellow taxi non yellow private that's it right so there was this classification based on

282
00:31:41,480 --> 00:31:47,960
this feature if we are asked now which cars are service cars and which are private cars the color

283
00:31:47,960 --> 00:31:55,240
doesn't help anymore only because basically there are all these ubers lifts v as everything

284
00:31:55,240 --> 00:32:01,960
so now it's a question of classifying behavior rather than just these one shot features

285
00:32:02,680 --> 00:32:08,120
because the behavior now is like okay this car went from here stop there then went there stop

286
00:32:08,120 --> 00:32:15,880
again has been goes like that oh that looks like a service car not a personal car it's the behavior

287
00:32:15,880 --> 00:32:23,080
in some sense that helps us classify service non service and we are trying to apply the same thing

288
00:32:23,080 --> 00:32:27,880
to problems that have been classified a lot by just looking at this particular transaction

289
00:32:27,880 --> 00:32:33,000
and deciding oh this is an amount this goes to an account that we don't know and just looking at

290
00:32:33,000 --> 00:32:43,240
that by building the behavior of these you know these users to try to detect what can be eventually

291
00:32:43,240 --> 00:32:49,880
behavior that is not normal behavior that is this this that we can distinguish from other type of

292
00:32:49,880 --> 00:32:56,040
behavior and so it became not a classification only of the transaction or only of this snapshot

293
00:32:56,040 --> 00:33:02,440
but we now in terms of research are building these representation of all these time dependencies

294
00:33:02,440 --> 00:33:09,800
of all these connections in terms of knowledge of all these graph based representation to try to

295
00:33:09,800 --> 00:33:17,640
capture much higher level the to try to classify these fraud no fraud to capture these complicated

296
00:33:18,920 --> 00:33:25,160
classification problems the site very similar to the taxi versus no taxi or car service versus

297
00:33:25,160 --> 00:33:30,840
no car service in New York the same problem and so that's another example in which we have shown

298
00:33:31,400 --> 00:33:39,080
that indeed by capturing how much of these nodes these these users in our network of

299
00:33:39,080 --> 00:33:45,720
interactions communicate with each other and these accounts transfer from one another to another

300
00:33:45,720 --> 00:33:53,720
you are able to detect much better behavior that is not normal so this is another example

301
00:33:53,720 --> 00:33:58,600
and we have done also the third example I can give you just to finish this kind of like examples

302
00:33:58,600 --> 00:34:08,760
is that we have been also basically automating the generation of a lot of documents either

303
00:34:08,760 --> 00:34:15,080
for regulators or for our clients or internally PowerPoint presentations representation

304
00:34:15,080 --> 00:34:22,840
as a translation from data that is in some format like numbers lots and lots and lots of

305
00:34:22,840 --> 00:34:30,600
information about transactions into English into another representation charts insights comments

306
00:34:30,600 --> 00:34:39,080
and we have been automating that kind of like a description of the data in a representation that

307
00:34:39,080 --> 00:34:46,520
is readable or that is formatted according to what's required to show when you say translation

308
00:34:46,520 --> 00:34:51,880
are you meaning that literally like using a neural machine translation type of a tool and applying it

309
00:34:51,880 --> 00:34:58,600
to this particular scenario very good question in fact we have not yet learned this translation we

310
00:34:58,600 --> 00:35:07,160
are really just writing an algorithm to make the translation computes basically all these it's

311
00:35:07,160 --> 00:35:15,320
like a you could think about is as a template feeling kind of like a algorithm the template is

312
00:35:15,320 --> 00:35:21,640
complex where do we get it we have to represent it well and then we feel and then we are already

313
00:35:21,640 --> 00:35:28,200
eventually at some point to do the learning of what this parameter should be and how the template

314
00:35:28,200 --> 00:35:34,040
should move and the personalization and who goes where but it's another level of learning it's not

315
00:35:34,040 --> 00:35:40,280
really learning that you know how do you say this word in a different language but so we built the

316
00:35:40,280 --> 00:35:47,560
translation machinery and now we have to adjust all the parameters of this machinery through feedback

317
00:35:47,560 --> 00:35:54,440
and through data through the use of this but we built the translation machinery and that required

318
00:35:54,440 --> 00:35:58,760
so the beautiful thing about building this translation machinery is that we try to

319
00:35:58,760 --> 00:36:05,640
built in in a way that it doesn't apply only to a specific task so it is general in terms of

320
00:36:05,640 --> 00:36:11,560
like different types of reports so they all require eventually the data to come into this form so

321
00:36:11,560 --> 00:36:18,600
you change the data and you manipulate these knowledge in a way that you can then automate so in

322
00:36:18,600 --> 00:36:27,160
some sense I am as you can imagine from my first days I am always a how can I say an automation

323
00:36:27,160 --> 00:36:34,040
person I'm more a young and hearing person and then machine learning for me is like a tool to try

324
00:36:34,040 --> 00:36:46,040
to get these these these automation machinery adjusted and personalized and generalized and

325
00:36:46,040 --> 00:36:55,080
generalized and transferred and all sorts of things but so the data plays the role for me as okay

326
00:36:55,080 --> 00:37:01,400
now I adjust all of these and this goes to that side this goes to this side so yeah so that's

327
00:37:01,400 --> 00:37:07,160
what these are examples of three projects the Mondrian project this kind of like behavior-based

328
00:37:07,160 --> 00:37:15,480
fraud detection and these automation of reports out of several others that we have done yep

329
00:37:15,480 --> 00:37:25,000
and do you see your charter at the bank in terms of a time horizon like there are lots of embedded

330
00:37:25,000 --> 00:37:31,960
machine learning AI folks I imagine all over the bank and creating organizations and client

331
00:37:31,960 --> 00:37:38,200
service marketing etc you know that are working on immediate projects or is one of the ways that

332
00:37:38,200 --> 00:37:47,240
you see your charter as being you're in you know five to you know 20-year realization or whatever

333
00:37:47,240 --> 00:37:53,720
the numbers might be it's a good question so indeed JP Morgan Chase is amazing in terms of how

334
00:37:53,720 --> 00:38:01,800
much machine learning is being applied throughout throughout the firm and for classification of emails

335
00:38:01,800 --> 00:38:07,960
for marketing decisions for decisions on credit no credit learn all these are supported by

336
00:38:07,960 --> 00:38:13,640
data driven algorithms I mean one way or another they are present throughout it's 50,000

337
00:38:13,640 --> 00:38:22,360
technologists that JP Morgan Chase has and more out of those thousands now more hundreds or AI

338
00:38:22,360 --> 00:38:28,440
and machine learning people very close to the business somehow we are building this understanding

339
00:38:28,440 --> 00:38:38,360
that the we work close to them we and they work close to us when the problems are reach a level

340
00:38:38,360 --> 00:38:45,640
of complexity or of dreaming that cannot they don't they don't have the immediate way of addressing

341
00:38:45,640 --> 00:38:50,920
them we start a much better dialogue for example this Mondrian thing or even like the generation of

342
00:38:50,920 --> 00:38:57,640
these documents automated or generalized and learning from the use is like kind of something we

343
00:38:57,640 --> 00:39:04,280
are involved it's not yet deployed so it's a while the applied machine learning the applied AI

344
00:39:04,280 --> 00:39:10,040
they actually are very close to the business and making sure things get immediately deployed

345
00:39:10,040 --> 00:39:15,800
but out in Havier our research we do these steps I mean think about the cure cancer problem

346
00:39:15,800 --> 00:39:20,440
you know there might be steps in which you find better chemo there might be steps that you find

347
00:39:20,440 --> 00:39:27,560
better devices and they end up you know getting to be in the hands of the protectors and

348
00:39:27,560 --> 00:39:32,200
so it's the same for us same thing with us for example these things we are doing about behavior

349
00:39:32,200 --> 00:39:37,320
recognition and eventually understanding the network of interactions for fraud detection

350
00:39:38,120 --> 00:39:46,280
will eventually be is closer to deployment than other things we are working on so we are in

351
00:39:46,280 --> 00:39:52,280
that spectrum and you know and we we interact a lot with the business we like a lot with our

352
00:39:52,280 --> 00:39:59,800
colleagues in applied machine learning and but we still kind of live in this world of being free

353
00:39:59,800 --> 00:40:09,400
to how do you say to transform to reinvent to to just think a little bit more without the boundaries

354
00:40:09,400 --> 00:40:15,960
of having something that is needed today now five 20 years I don't know but I do expect to

355
00:40:15,960 --> 00:40:26,200
that we expect so just so we know I expect that we produce novelty and innovation every year

356
00:40:26,200 --> 00:40:31,640
every year we do something when do we reach the goals who knows I mean in Robocup we used to say

357
00:40:31,640 --> 00:40:39,640
by 2050 we will beat the World Cup human players robots will beat the World Cup human players and

358
00:40:39,640 --> 00:40:47,320
we made that 2050 we are getting too close to that but anyway but now exactly when we'll eradicate

359
00:40:47,320 --> 00:40:55,480
financial crime but we also but these goals we built them to be like for 2030 it's like any

360
00:40:55,480 --> 00:41:04,120
or goal in some sense but is your your novelty and innovation metric is what is it you know direct

361
00:41:04,120 --> 00:41:08,200
value to the business many you've come up with something and they've implemented it and it's

362
00:41:08,200 --> 00:41:14,440
solving a problem or is it you know is there a publishing model for example were you expect to be

363
00:41:14,440 --> 00:41:19,160
you know publishing an academic literature and that's a metric for a novelty and innovation

364
00:41:19,160 --> 00:41:23,800
definitely the latter too we have a lot of connections with universities we publish

365
00:41:24,440 --> 00:41:30,040
all our results so we want to be we are involved in all the research activities and

366
00:41:31,400 --> 00:41:36,200
I could like tell you Sam a whole other conversation about our connections with universities

367
00:41:36,200 --> 00:41:42,120
the fellowships we created the research awards we created I'm sorry I did not focus on that as

368
00:41:42,120 --> 00:41:52,520
much but yes we are literally a research group and as such we are a research group however I do

369
00:41:52,520 --> 00:41:59,480
believe that we will that we make a difference also at another level of value to the company

370
00:41:59,480 --> 00:42:08,360
oh we bring talent you know it's a very good like we a very good benefit for our firm a lot of

371
00:42:08,360 --> 00:42:17,160
talent in this computer science core and AI core and machine learning AI and computer science

372
00:42:17,160 --> 00:42:26,520
computer engineering awesome are there any particular places that we can look for publications

373
00:42:26,520 --> 00:42:33,320
or some of the work that you're doing yeah yeah there is a we have an external website with all

374
00:42:33,320 --> 00:42:41,640
our publications and I can text to you but I believe it's just JP Morgan slash AI or JP Morgan

375
00:42:41,640 --> 00:42:46,920
Chase I forget now I have it the bookmarked but I will chase it down and include it in the

376
00:42:46,920 --> 00:42:51,880
general page perfect and it has all this description of the goals the publications we have done

377
00:42:51,880 --> 00:43:00,520
these different pillars everything okay how how do you see the the role in the group evolving

378
00:43:01,320 --> 00:43:06,840
you know again you're kind of recently having established these principles and kind of getting

379
00:43:06,840 --> 00:43:12,520
underway you've grown pretty significantly you know what do you expect to see over the next few

380
00:43:12,520 --> 00:43:23,480
years so we are a group of basically me heading and then I have three research directors Tucker

381
00:43:23,480 --> 00:43:29,880
Balch who came from Georgia Tech Prashantredi who came from Google and then Samina Shah came from

382
00:43:29,880 --> 00:43:38,920
S&P 500 and basically the three of them and me are the directors of these group we have only

383
00:43:38,920 --> 00:43:46,040
masters and PhDs in my group masters level and PhDs and we expect to grow by next year to be a

384
00:43:46,040 --> 00:43:53,080
group of probably 50 by the end of this year and then probably grow up to be 100 within the next two

385
00:43:53,080 --> 00:44:03,240
three years and stay at 100 150 within JP Morgan doing research AI research in the financial domain

386
00:44:03,240 --> 00:44:13,080
so anyone who has an interest on doing AI research in not the not only I mean or wants to consider

387
00:44:13,080 --> 00:44:19,640
not only the technical the tech the the giant tech the Google's the Facebook and

388
00:44:19,640 --> 00:44:25,960
and so forth this is a great place to be to do AI research in this particular kind of like

389
00:44:25,960 --> 00:44:34,040
financial domain but in great so we publish in great through AI research and AI machine learning

390
00:44:34,040 --> 00:44:40,440
so that's what I'm trying to build is a little AI research group up to 200 people I would say

391
00:44:40,440 --> 00:44:46,600
within the next five years well Manuel thanks so much for taking the time to chat with us

392
00:44:46,600 --> 00:44:52,120
share a bit about what you're up to it was great great meeting you and great learning about what

393
00:44:52,120 --> 00:44:57,240
you're doing thank you very much Sam and thanks a lot for hosting me absolutely please be safe

394
00:45:00,040 --> 00:45:05,480
all right everyone that's our show for today to learn more about today's guest or the topics

395
00:45:05,480 --> 00:45:11,880
mentioned in this interview visit twimmel ai.com of course if you like what you hear on the podcast

396
00:45:11,880 --> 00:45:18,680
please subscribe rate and review the show on your favorite pod catcher thanks so much for listening

397
00:45:18,680 --> 00:45:24,680
and catch you next time

