Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Solan Barokas, Assistant Professor of Information Science at Cornell
University.
Solan is also the co-founder of the Fairness Accountability and Transparency in Machine Learning Workshop
that's hosted annually at conferences like ICML.
Solan and I caught up to discuss his work on model interpretability and the legal and
policy implications of the use of machine learning models.
In our conversation, we discussed the gap between law, policy, and ML and how to build
the bridge between them, including formalizing ethical frameworks for machines to look at
his paper, the intuitive appeal of explainable machines, which proposes that explainability
is really two problems, inscrutability and non-intuitiveness, and that disentangling
the two allows us to better reason about the kind of explainability that's really required
in any given situation.
And now on to the show.
All right, everyone.
I am on the line with Solan Barokas.
Solan is an Assistant Professor of Information Science at Cornell University.
Solan, welcome to this week in Machine Learning and AI.
Thanks so much for having me.
Why don't we get started by having you tell us a little bit about your background and
how you got involved in machine learning?
Yeah, it's a nice story, at least.
I have a think unusual story in the way that I found my way to machine learning.
So I got my PhD at NYU a couple years ago now, and I actually graduated from what is essentially
a media studies department.
I worked with a particular professor named Helen Nissenbaum, who for many years has been
one of the kind of leading people doing work on the ethics of information technology.
So I had spent time there specifically to work with her, but I knew early on when I was
a graduate student, that I was interested in what was more commonly called data mining
then, and decided to actually take some classes and got to know some of the faculty at NYU,
who then really kind of brought me up to speed in the fundamentals of machine learning.
And it became very clear to me that there were enormous number of interesting ethical and
policy questions that were raised by machine learning, and that those were quite different
than the kinds of topics that people were discussing quite speculatively without knowing
very much about how machine learning actually worked in practice.
And so I went off to do a dissertation that was really trying to translate some of these
interesting questions that grow out of the technical aspects of machine learning into
ethics and policy questions, and it has meant over a kind of a long period of time becoming
more and more familiar with machine learning and both the community of researchers and practitioners
in this area to the extent now that I feel pretty comfortable and at home in the broader
machine learning community in general.
And a lot of that work is also taking the form of trying to bring some of these normative
concerns around privacy or fairness or transparency further into the machine learning community.
So making these issues first order questions for the field and that has included staging
workshops for about five years now attached to the main machine learning conferences like
NIPS and ICML that were explicitly attempts to bring questions of fairness accountability
and transparency into the mainstream of the community.
In fact, the event we started was still called fairness accountability and transparency
and machine learning.
So that has been one of the main mechanisms by which I've kind of entered the community.
I think people, if they know me at all, probably know me through that work.
And it's been great actually because five years ago when we started this kind of conversation,
it was a pretty niche group, some great people from the start but still quite a small community.
And over the past two years in particular, these have just become kind of mainstream issues
for the field, such that papers are now being accepted at these conferences, a bunch
of best paper awards have gone to papers on fairness recently.
And if you follow the news at all, a lot of companies, a lot of the big tech companies
have also begun to take these issues really seriously and begin to work on them in practice.
And I've had opportunity to kind of see how that's progressed.
So that's really the role I've been filling in this community the past couple of years.
And while I really appreciate that aspect of kind of bridging these two communities, I find often
or found that a lot of people in machine learning care about these issues of ethics, but there
aren't a lot of people that have both the machine learning background and a background
in ethics.
And so you often observe, appears to be reinventing the wheel, coming up with ethical frameworks
from scratch.
In fact, there's clearly been a lot of work put into that kind of thing on the ethics side.
And it's just an issue of like connecting these dots or like connecting the two sides of
the bridge or something like, do you experience that as well?
It's a great question.
I think what's been exciting about this area of research and policy discussion is people
sort of recognized early on that at least when it comes to questions of fairness and discrimination,
there were some existing ways of thinking about the issue which lent themselves to formalization.
So just to give a kind of quick anecdote here, it's sort of a rule of thumb in discrimination
law that if there's a disparity in outcome greater than 20% between, let's say, men and
women who are applying for a job, that itself can be a trigger for a case to investigate
whether in fact there is discrimination.
It's called the four fifth rule, it comes from the, the regulator, the employment regulator.
And even though it's not enough to establish that some kind of decision making process, there's
discriminatory, it's often used as a heuristic to say like, well, we should try to minimize
the disparity so that it's never greater than 20%.
And because this is an existing way of thinking about this in quantitative terms, it really
lent itself to the kind of work that machine-linear people like to do, which is this kind of constrained
optimization problem.
So you know, how can we build classifiers or predictive models that aim to achieve, you
know, a maximum performance, subject to this additional constraints.
And some of the earliest work on fairness and machine learning grew directly out of some
of these principles from the law.
Since then, what's happened though is that there's been a bunch of new ways of trying to
formalize notions of fairness that are really different and pretty much detached from
the way that the law and policy community think about them.
And you could maybe think about that as being reinventing the wheel or not being sufficiently
sensitive to how much work has been done already.
But part of what's been kind of exciting and fascinating is that some of these new formalizations
are actually just sort of interesting new ways of thinking about the problem that haven't
come up in the law and policy discussion in part because perhaps people haven't had
reasons to think about them in the way that these new formalizations actually parse the
issue.
And so an example of this is, you know, there's a debate now whether differences, differences
not only in the accuracy of the model between different groups matter, but whether differences
in the error rates matter.
So, you know, it might matter very much in different settings that you're subject to a false
positive or false negative and perhaps you want to equalize across these rates, but which
one of those actually matters or do these matter at all.
And that's sort of the direction that the field is going, proposing these new ideas that
don't have a neat or obvious analog in law.
And I think there's a lot that can come out of that.
Part of it might be that maybe these aren't, in fact, good ways to think about it given
what people actually care about from a normative perspective, but it also can potentially
reveal that some of the ways that law and policy have thought about these problems are incomplete
or incoherent or we could actually do better.
So I think there's a lot of opportunity here.
That's a fantastic take on that.
What are, you kind of mentioned, you know, broadly different formalisms and ways to parse
this issue?
Are there some other examples that come to mind of the way folks have formalized ethical
and fairness frameworks applying them to machine learning?
Yeah, I think at a high level, there's a way to maybe categorize some of them.
So I think a set of concerns have to do with just straight up differences in the performance
of models for different groups, for men and women, for people of different races or what
have you.
And you could think about that using any number of different metrics, right?
So not just accuracy, not just these kind of false positive or false negative rates,
but a bunch of other things as well, including calibration and other terms that might be familiar
to you folks.
What's interesting about that work is that it sort of sets aside the question, whether
the data, the underlying training data itself is reliable.
It says, even if the data is perfect, there might be circumstances where we still observe
disparities in the performance of these models between groups.
And if that's the case, you know, what can we do about it?
How do we actually try to avoid those situations?
A separate bucket, though, actually starts from a different position.
It says, we should never actually believe that the training data we have is reliable.
And in fact, there's good reason to believe that it's systematically unreliable in a particular
way.
So an example of this is, you know, you might think about how does the accuracy of a predictive
policing model differ between groups?
That's the sort of question you get asked in the first camp.
And the second camp, which is concerned with the underlying quality of the data, they might
say, yeah, you actually don't have data that is a good representation of the true incidence
of crime in society.
So anything you might do around the edges concerning performance is really not addressing the
fundamental problem, which is that you're learning from highly biased data.
And a slightly more serious version of this concern might be that maybe you are even
using data that encodes some kind of prejudice.
So it's not just that the selection or sample of data is biased, but that the training
data is itself some past decision made by someone, which was made in a prejudicial way.
Though the example I tend to give for this is something like an employer trying to use
machine learning to help find good people to hire in the future.
And you might say, let's take a look at which employees that we've hired in the past
have gone on to be particularly successful.
The target variable we might select in this model is the annual review score that we've
given our past employees.
And so what you want the model to do is to predict what the likely annual review score
would be for any new job applicant given the training data from your past employees.
The problem, of course, is that even though one of these annual review scores is meant
to be carefully considered and people are asked to go through a pretty systematic process
to assign this score invariably, and there's even research empirical research to show
this, that score is going to be inflected either potentially by conscious prejudice or
implicit bias, where people don't even really realize that their assessment is somehow swayed
by these preexisting beliefs about gender or race or what have you.
And so what ends up happening is that the training data actually encodes that prior prejudice
or bias, and so the model is not learning to predict how people would actually do on
the job.
The model is learning to predict how human assessors who had previously been given, who had
been previously giving these annual review scores, would likely review this future person.
So these give you a taste for some of the subtle differences, and each of these problems
really require quite different responses, right?
The very first type of problem that's describing, you're trying to figure out how to deal with
the fact that there are situations where your model just won't perform as well, and these
other situations where the underlying data is actually unreliable, require more creative,
potentially kind of more ambitious solutions, which try to just compensate for what you
believe to be the underlying problem with the data, even though you might have no direct
way of measuring the thing you really care about.
Just following that thread of the job performance reviews, to what degree has that specific problem
been, explored, and what kinds of approaches or solutions have you seen folks taking
with that?
That's a great question.
So I don't think there's any concrete case yet where we've been able to establish that
the training data actually encodes the past discriminatory decisions of management.
But as a kind of thought experiment, and given other empirical work on how humans actually
do assign these kinds of scores to their employees, there's very good reason to believe
that that training data would, in fact, whatever training data people might put together
would suffer from that kind of problem.
There's a lot of people who are becoming more and more familiar with this as a potential
concern, and so that's resulted both in lawyers trying to find cases to potentially bring
against employees.
But separately, there's also a lot of companies who specialize in machine learning, who are
trying to integrate these concerns into the products they then sell to clients.
So these are sort of recruitment machine learning specialists to then develop tools that
they license or sell to employers.
And part of the solution to these problems can vary considerably.
So on the one hand, a lot of the simplest interventions involve reporting performance
in a way that just breaks apart overall performance into performance by group.
So can we just observe that this model does a much worse job for certain people than others?
And are there ways that we can try to compensate for that?
And my sense is that that's becoming an increase in the more common approach.
The problem with that approach, of course, is that you actually need to know those details
about the people you're trying to score.
So in order to separate out the performance for men and women or for people with different
of different races, you actually have to collect information about that.
And as a kind of standard practice, employers at least really have good incentive not to
collect that information because they don't want to even create the possibility that of
being accused of having considered it when making decisions.
So it's an interesting tension, right?
We might want the information to prevent discrimination, but historically, companies have been instructed
not to collect it to limit the likelihood that they could ever even consider it.
The problem with encoding or your training data is just past decisions which might have
been influenced by human bias.
That's much more difficult.
And it's not even obvious what a principled approach to that would be.
Some of the positions that people have put forward is to sort of just make certain assumptions
about what you think is a more reasonable distribution of attributes across the population so that
you sort of override what is the actual distribution and the training data you're dealing with.
In a way, you're basically saying, I suspect the data to be systematically flawed and I'm
going to kind of put my thumb on the scale to compensate for that.
And some people now have pretty rigorous mathematical methods for doing this without necessarily
sacrificing performance.
But I'm just not so familiar yet with what is happening in practice on that front.
And the final thing I would say is that, you know, in a situation where you have all
their mechanisms to potentially measure these dynamics, you might take a different approach.
You might say, I'm not going to treat this as a pure prediction problem.
I'm going to try to do some kind of empirical study to see if in fact, you know, there's
a problem in my workplace.
So rather than just relying exclusively on these annual review scores, maybe you go and
try to find some other thing to measure, which is not as vulnerable to human bias.
So an example of this might be something like, well, it's pretty hard to argue with the
fact that you've achieved some sales figure at the end of the year if you're a sales person.
Like that seems like a metric that's much less vulnerable to this kind of bias assessment.
You can challenge that, but the argument anyway is that maybe we can begin to measure
other things and then compare that to the kinds of assessment people get at their annual
review.
And that might reveal some kind of underlying disparity or sort of misalignment between
people's true performance and the score they're given.
And then given that finding, you can go about trying to correct things.
But there's, you know, even in this very long answer, I haven't even exhausted the list
of possibilities.
I mean, I guess it's obvious that a lot of these challenges are like fundamental human
organizational people issues and technology is but a small part of the overall picture.
That's right.
And I think what's interesting about the current state of the research is that a lot of it
is sort of head-to-head comparison between existing decision-making process and some
model under perfect conditions.
And I think a lot of progress will depend really ultimately, I think, on figuring out
how to think more kind of formally and carefully about these fairness concerns and integrate
them into the model development process.
But separately, also think about how to then put that model into practice and potentially
reform the institution itself, as you were saying, right?
Like really consider how this fits into the bureaucratic decision-making process, how it
figures into the dynamics of the workplace that would have you.
So even if we are successful in trying to deal with these concerns within the model itself,
that is certainly not sufficient to achieve these broader fairness or justice goals we might
have.
And so I asked about that, about an example in the context of performance reviews, but
are there more broadly any examples that you've come across of kind of this process taking
place full circle within an organization or some political structure where the algorithmic
or data biases were observed, some sets of adjustments were made to modeling as well
as the underlying organization, organizational practices, and that leading to a better
outcome?
Yeah, it's a great question.
And I don't think there's some shiny and example to point to, unfortunately, at least
not yet.
I mean, there's certainly some examples of changes that have been made that were a little
bit more straightforward.
So some scholars were able to show that there were disparities in the performance of
kind of off-the-shelf facial recognition software and that these disparities were along the
lines of both race and gender, such that these systems did a much, much worse job, for
instance, for black women than for white men.
And the result of this research, which I certainly encourage your listeners to take a look
at, was some pretty rapid changes on the part of the companies that provided these often
API facial recognition services, so they just basically went about trying to figure out
how to improve the performance for these populations.
You know, that's a different story because it doesn't involve this entire kind of workflow
and bureaucracy, but an example I could still point to, which I think is an interesting
one, is in Allegheny County, which houses Pittsburgh, the county was working with some academic
researchers to not only use machine learning, but just in general, you sort of more data-driven
approaches to the way that it handled its child protective welfare agency.
And this got written up as a kind of long future article in the New York Times magazine
a few months back.
It's also actually the focus and the chapter of an excellent book called Automate in
Equity both described this pretty careful process by which researchers engage with the city,
but also with agency workers, with advocates for children and families, for people affected
by these systems, and really try to take into consideration all the different interests
that this agency was charged with serving.
And the people developing the tool actually observed that there were, in fact, some of
these disparities and how well the model performed.
It was likely to produce these kinds of kind of disparate impact in the way that it would
suggest people for scrutiny when it came to potential child maltreatment or child abuse.
And one of the interesting things to think about when focusing on this story is that despite
the fact that this effort really involves a considerable amount of community engagement
and consultation, and even really explicitly took into consideration some of these questions
around fairness, that people will nevertheless still have, I think, some legitimate concerns
around the use of these tools.
So it's hard to say that this is like a clear model for what everyone should be doing
in similar situations, but it gives some sense of just how difficult it may be to kind
of more fundamentally address questions of fairness, even if you've gone to the effort
of integrating them into the model development process.
I think what it points to for me is the need for kind of lots of people and perspectives
to get involved in understanding this issue and how it applies to the problems that they
care about and taking on the little pieces of it that they can take on, even if they're
not exposed to the full cycle, if you will.
Yeah, you know, the way you think about this is heavily influenced by the fact that one
of the people who introduced me to machine learning was someone who had more than a decade
of experience and practice, this is Foster Provo who's a professor at NYU.
And his way of teaching machine learning really emphasized this kind of first step in the
process, very different than what you get in kind of traditional academic machine learning
education.
You know, he really emphasized that one of the main challenges is how do you translate
a business problem into a machine learning problem?
So, you know, how do you specify the target variable correctly or in a way that's reasonable?
And for people who I think have experience and practice, that is a familiar problem, you
know, it's like not that easy to know exactly how to solve some general business problem
by figuring out how to specify the target variable.
And my sense is that when people do think about that, they think about it in terms of, you
know, what is the appropriate thing to try to predict when you're in online marketing,
right?
Clearly, like, click through is not the best thing.
We want someone to convert and so you can have a pretty obvious debate around what is
the right thing that you should be predicting.
I think there's a similar thing that happens in these domains that involve much more high
stakes decisions like, you know, child welfare or employment or credit, right?
We can have, I think, a pretty straight forward conversation about what it is that we're
actually trying to achieve and does the way the problem has been specified actually correspond
to our normative goals.
And the advice that you get from some of the early data money, literature, I think is
really relevant here, it's sort of about, as you say, you really want to have a pretty
deep and thoughtful conversation with your stakeholders.
You want to understand the problem that you're being charged with solving and you want
to understand whether or not it really, the way you've kind of set up the problem really
reflects the concerns of the people it's supposed to serve.
So my sense is that, you know, a lot of existing ways of doing machine learning well in practice,
those insights, the kind of ideas people have from their experience on the ground, could
be super helpful to the conversation people are having now about ethics.
So you recently published a paper that looked at the applicability of some of the work that's
happening around transparency and explainability to various regulatory frameworks like GDPR and
others.
Can you give us an overview of that work?
Sure.
So this is a forthcoming paper and Ford and Law Review and my co-author Andrew Selbst and
I were trying to sort of bring together a couple different conversations that were happening.
So in the law and policy world, there's a lot of anxiety around the use of machine learning
for high-stakes decision making because the fear is that you won't be able to explain
the outcome of some decision-making process and for people who come from machine learning,
you'll know that there's a long history of working on interpretability in machine learning
and that this has become especially hot area as deep learning has become more successful
and more dominant and there's been some really interesting research breakthroughs in trying
to be able to explain what is happening with deep learning models.
So what we were trying to kind of show is that there's ways to sort of have these two
things speak to each other a little bit.
Part of it is about explaining exactly what it is that the existing laws and regulations
actually require when they require explanations and then part of it is also trying to show
that there are in many cases tools for satisfying those laws.
So although GDPR which is the European general data protection regulation is the thing that
has really generated a lot of attention around these issues recently, there are laws here
in the United States that also will require explanations for automated decisions and
the key example of that is the Equal Credit Opportunity Act and the Fair Credit Reporting
Act.
These are both laws that regulate credit scoring and credit decision-making and the acronyms
are ECOA and FICRA and these laws when they stipulate that when a person applies for
credit and the creditor denies that person, the creditor actually has to give reasons
for why they denied the loan.
And this law actually dates from the 1970s so this is not a new law, it's been around
for a very, very long time and it has actually really structured the credit industry.
If you speak to people in practice, you'll know that this really is the way that they've
had to orient all the work that to make sure that they could always give reasons for
their decisions regardless of the mechanism by which they got to their decision about
whether they issued the loan.
And the concern is that can you give reasons for a decision around credit if your model
is using something like deep learning?
Well, if you know the work in interpretability and machine learning at all, you'll know
that a lot of the recent proposals involve for going any attempt to actually provide global
transparency into the model, meaning forget any effort to describe the full relationship
that the model maps out.
And instead, let's use some other mechanisms to see if we can say what in any given decision
actually seem to be the most salient variable or set of variables, so which features in
the model really account for this particular classification or outcome.
And those methods have proven pretty powerful in general for purposes of kind of any deep
learning or machine learning model, but certainly it's not hard to imagine that they would
be well suited to satisfying this existing requirement in eco and Fickra, which literally
say you have to give specific and the actual reasons why someone was denied a loan.
So what's interesting about this is it feels sort of like a silver bullet.
It says like, oh, you can go off and build an arbitrarily complex model so long as you
can provide specific reasons for any particular decision.
And it feels that there might be a way to avoid the longstanding perceived tradeoff between
the performance of the models you build and the interpretability of those models.
So that seems like a good outcome.
It seems like progress in kind of the research domain of machine learning has really helped
solve a longstanding issue with regulation.
The paper with my co-author kind of goes into some detail about why this might not always
be so helpful in practice.
And it's a bit complicated.
So I'm not sure if I should carry on or I should let you ask another question.
Well, we definitely want to dig into what makes it complicated.
But I'm curious with what you stated, kind of the broader history of explainability or
transparency requirement by other regulations, did you generally feel like all of the
hullabaloo about GDPR and its implications for machine learning and deep learning and
innovation and all this stuff like do you feel it was overblown or based on some of the
challenges that you're aware of appropriate?
So I think the reactions to GDPR in general is appropriate in the sense that the law
is not radically different from the existing national laws that the regulation is meant
to replace.
So in the European Union, a regulation refers to a law that is standardized across all
member states.
There was something called the data protection directive for more than 15 years I think
that was a sort of earlier version of what is now GDPR.
The difference is that the directive was sort of guidance from member states and they
were expected to sort of follow similar rules, but it wasn't standardized across all
of Europe.
The regulation was updated to kind of deal with some new problems, but the main radical
change between existing laws and the regulation was financial penalty.
So the reason is appropriate, I think, to actually think through these things is whether
or not you care about these things from a kind of normative perspective or making sure
that you obey the law.
Companies actually now face genuinely significant financial penalty for failing to comply with
the law.
My sense is that the motivation for taking this serious, much more seriously anyway than
it had been in the past is less that the law has changed dramatically.
It's much more that the kind of consequences for failing to meet the law are now significantly
more severe.
So if you had been following the law already, which you should have been, this will not
actually require a radical change.
But the fact of the matter is that most people were not even really aware of the law, let
alone following it.
So to me, that really accounts for the difference.
Having said all that, I still think that the main thing people at least were concerned
about when it came to machine learning was this provision that required that you explain
the decisions.
So for certain types of decisions, automated decision making in general is forbidden
unless people give consent.
And even when they give consent, you still have to be able to explain the decision.
And as I mentioned, there's a sense in which perhaps there is a trade-off between performance
and explainability.
So even if this wouldn't necessarily prohibit machine learning, it might be a constraint
on the complexity of the model in order to make sure that it remains explainable.
And that that might mean a degradation in performance.
My sense is, though, that depending on how this law is interpreted, and this is also
something we go into in the paper, you could potentially satisfy this requirement without
necessarily building a model that is sufficiently simple that even a layperson could be able
to look at it and understand it.
There might be other ways of providing explanations, which still satisfy the law that don't require
this potential trade-off.
Can this bring us back to the point that you were about to make about looking at the details
of complying with these various laws?
Yeah, it's certainly in that direction.
So I think the hope is that by explaining how decisions are made, you will know whether
or not that is a good way of making decisions.
So explanations are sort of a mechanism to check for other things you care about.
Check to see that the decision making process, or in this case, the model, is taken to consideration
things that you think of as being legitimate and relevant.
So it should be considering these factors.
So I can check, is it considering these factors?
It could also be a way to check that it's not taken to consideration things that it shouldn't
be.
So it shouldn't be considering explicitly things like race, or it shouldn't be considering
things that are arbitrary or clearly irrelevant.
One of the challenges here is that sometimes you might be in a situation where even if
you explain how the model makes the decisions, you as a human may not have any good intuitions
for whether or not that is a reliable or sound basis for decision making.
So one of the reasons people are interested in machine learning is that they can uncover
patterns and data sets that would just escape humans' attention.
And one would really be able to figure out that there are some kind of subtle signal across
10,000 features that none of which on their face seem particularly relevant to the task
at hand.
So if it turns out, however, that the model has found such a signal, you could potentially
try to explain the ones that are relevant to any particular decision, but even if you
did, it would be potentially impossible for humans to know whether or not that's reasonable
or really appropriate.
So the example, I'll just come up with a kind of toy example here.
If it turns out that the way you tie your shoes is predictive of some kind of performance
on the job on its face that seems sort of laughable, right?
Like why should that matter to my job performance?
But let's just say for the sake of argument that it's a pretty robust finding that like
the data really supported and wants you to play the model in practice, it actually shows
that it does a pretty good job.
On what basis do you actually say this is a good or bad model?
It's good in the sense that it's potentially accurate, but it's not bad in the sense that
it's choosing to pay attention to something irrelevant or obviously unfair.
It's unsettling because it has discovered that there's something relevant in a feature
that we as humans just cannot see is possibly relevant.
And this is, I think, one of the real challenges here, right?
So even if we use some of these awesome machine learning techniques to give explanations of models,
they may not help us as humans to be able to assess whether those are even reasonable
things to rely on to make important decisions.
I think one of the interesting things here is some of the work that is going into kind
of as opposed to trying to make your models, your fundamental fairness technique, being
one of trying to make your models blind to factors like race or gender, actually taking
part of this fairness challenge to build into your models, trying to predict these things
like race and gender as part of the decision making and using whether the models can predict,
race, for example, in the features that you kind of build into your model as an indicator
that your model may be surreptitiously kind of making decisions based on race, right?
It's there in the signal, it's just not obvious to you.
Yeah, and so this is exactly the approach that some people are taking.
It's an interesting problem, right?
So you say, well, my model is non-discriminatory because it doesn't consider race or gender
explicitly, but it turns out that other features are highly correlated.
So maybe what you want to do is strip out those features that are highly correlated.
This becomes a kind of impossible exercise at some point because with sufficiently
rich data sets, it's almost certain that these kinds of details about you will be reflected
in the other features that you have.
And so it's not so obvious at what point you really are supposed to stop, right?
Like how many things you'd remove from your model until you feel comfortable.
And it's interesting because it actually relates to the other issue of explainability that
we were just discussing.
And the simple fact that some feature is correlated with race or gender on its own is not enough
to say that it's illegitimate or illegal to actually consider that feature, right?
And the reason for that is that there just happens to be inequality in society.
Some people possess features at a certain value, at a higher or lower rate than others.
That's a fact potentially of the world.
And saying that this kind of difference in the value of this feature means that we shouldn't
be actually considering that feature is not itself a sufficient argument, even when it
comes to like cases around discrimination.
And this is a complicated issue, right?
Because it may well be the reason that some feature actually is correlated with race,
for instance, is because of a long history of racial discrimination.
So for instance, zip codes, right, they can be very informative when trying to predict
the value of someone's home, but of course at the same time zip codes are highly correlated
with race because of a long-term race-based discrimination in housing, right?
So this is a tricky problem.
In other situations, what happens is that people might understand if I'm talking to people
in practice, is that they will want to find out which features are in fact correlated
with race or gender in their model.
And rather than just stripping them out because they happen to be highly correlated, they
actually will just go look at them.
They'll have a kind of rank-ordered list where the top is those features have been most
correlated with the sensitive feature race or gender would have you.
And then they look at it and they say, is it okay?
Is it like reasonable to consider this feature, even though it is highly correlated with
race and gender?
And in some cases, like the zip code example, you might say no.
You know, there's obvious historical reasons why this is not acceptable.
But in some cases, you might say, yes, you might say, well, this doesn't seem to be the
result of some kind of past injustice.
It may just reflect some true difference in the distribution of this feature in the population.
So we're going to stand by its relevance for the decision at hand.
What's an example that falls into that category?
A good example of this might be something like this.
What university do you go to?
I'm a person trying to figure out who to hire and my model, maybe unsurprisingly, assigns
a lot of significance to people who graduated from, I don't know, the Ivy League, right?
But it may well be that the actual people who graduate from the Ivy League tend to be
disproportionately white, right?
And you might say, well, let's actually not consider this because in a way, it's sort
of like saying, if you're white, you have a better chance of succeeding on this job, right?
If we wanted to tell some story yet, we could about like why it is that the population
in these schools looks the way it does.
And we could potentially get to a point where we feel like it's in fact reasonable to
say, don't consider the university someone want to, but I think a lot of employers would
make a reasonable and plausible case for the relevance of the university you graduated
from, right?
You would say, no, no, no, no, no, like these are actually like reasonable markers of
some lines, performance, like the performance and potential.
And so it's reasonable for us to consider it.
And so, you know, with all that, like where, where does that leave us in terms of, you
know, there's certainly a lot of kind of potential thick gray lines here in terms of the counterpoint
to, I think that example was an example where there is some historical, you know, evidence
of historical discrimination, and certainly that's the case in this university example
that you gave.
So it's not quite the opposite.
Are there concrete examples of how folks have parsed through all of this with some kind
of framework, or is it kind of everyone making a judgment call based on what they think
is right?
That's a great question.
It's a really good way of putting, I think, the current state of affairs.
So I think for some people, there should be some kind of bright line, and that the kind
of university using, you know, your alma mater as a way to determine whether to hire you
should be obviously reasonable, right?
Then there are other people who I think take the view that we, you know, there's lots of
reasons to be suspicious of the admissions policies of those places.
There's lots of reasons to be even more suspicious of the quality of the high school education
that people receive to prepare them to apply to college.
And you can go back even further, right?
The disadvantage you face as a person earlier in life that kind of sets you on this particular
course.
So you know, this may be frustrating, but I think ultimately there are going to be these
ongoing debates around how to even parse this issue, right?
For some people, it will be clear cut that there are certain factors that despite how
correlated they are, that they are legitimate to consider when building these models.
For other people, you know, they can make very strong arguments about the need to actually
use the model development process to compensate for the unfair disadvantage that people had
suffered earlier in their lives.
And ultimately, this is not a machine learning debate, right?
This is not something that is peculiar to building machine learning models.
This ultimately is just the kind of longstanding debate that people have had in general about
the fairness of decision making and certain settings about what is the appropriate role
of discrimination law in general.
So it's unsurprising ultimately that some of these things are not settled or are not
going to be settled in part because people have been arguing about this for at least 50
years when it comes to discrimination law and for millennia when it comes to questions
around fairness.
So did we cover all of the points that you wanted to cover with regards to your paper?
Yeah, I mean, I think the final thing I would just say about is there's going to ultimately
be, there's going to be situations in which the attempt to achieve fairness will require
explanations, right?
You actually know whether or not something is a reasonable thing to consider.
You need to be able to explain what the model is doing and you need to let humans actually
look at it and see if they can kind of weave some story that makes it feel like a reasonable
basis for making these important decisions.
In some cases, we might say the effort to get questions of fairness through explanations
is misguided.
Maybe what we should do instead is just abandon these requirements for explanations and
focus on providing kind of formal fairness guarantees.
Say, you know, we just want to ensure that whatever model we build, we can prove we'll
not have certain problems, right?
And we can do that more directly rather than relying on explanation.
And this just sort of summarizes, I think, the point I was making a moment ago, which
is that this, I think, just sort of ultimately depends on your perspective.
For some people, it will feel inadequate to just provide guarantees.
I can imagine myself actually feeling pretty dissatisfied with something that said, like,
this system is certified fair, but we're never going to tell you how it makes its decisions,
right?
At the same time, I think there's a lot more you can achieve with these kind of formal
approaches to fairness than what people expect they will get out of explanations.
And so I just think there's a role here for both things and an opportunity to spend a
lot more time figuring out when each of those is most appropriate.
Well, Salon, thank you so much for taking the time to chat with me about this service.
It's super interesting and super important as well.
Great.
Yeah, thank you.
I really enjoyed it.
All right, everyone.
That's our show for today.
For more information on Salon or any of the topics covered in this show, visit twimlai.com
slash talk slash 219.
As always, thanks so much for listening and catch you next time.
