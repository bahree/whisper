Hello and welcome to another episode of Twinmultaugh, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charington.
The show you're about to hear is part two of our O'Reilly AI New York series sponsored
by Intel Nirvana.
I'm super grateful to them for helping make this series possible.
And I'm excited about the cool stuff they launched at the O'Reilly AI New York Conference,
including version 2.0 of their neon framework and their new Nirvana graph project.
Be sure to check them out at intelnervana.com.
If you haven't already listened to the first show in this series, where I interview Naveen
Raugh, who leads Intel's newly formed AI products group, and Hanlon Tang and algorithms
engineer on that team, it's Twinmultaugh number 31, and you definitely want to start there.
My guess for this show is Doug Eck of Google Brain.
Doug did a keynote at the O'Reilly conference on magenta, Google's project for melding
machine learning and the arts.
Doug and I talk about the newly announced performance RNN project, which uses neural networks
to create expressive AI-generated music.
The demonstrations of this project are truly incredible and I encourage you to check them
out via the links we're placing in the show notes.
All right, on to the show.
All right, hey everyone, I am here with Doug Eck at the O'Reilly AI conference.
Doug is a research scientist on the Google Brain team, who's principally focused on the
magenta project.
Doug, welcome to the podcast.
Hey Sam, thanks for having me.
Excited to be here.
Absolutely, so this podcast, while I focus on interviews now, I originally focused on
covering news in the space and I covered the magenta project when it launched.
And I remember this vividly because my daughter was in a summer program or leadership program
or something like, I live in St. Louis and I had to drive her down state and I spent a
bunch of time on the trip back listening to something about, I might have been listening
to my notes or articles that I had doing like text-to-voice about the magenta project.
And so I'm excited to finally get a chance to talk to you about it and learn more about
it.
And yeah, so welcome once again.
I'm happy to be here a little over a year later from our initial launch and still
add it.
Absolutely, absolutely.
And I think this is the first time from an interview perspective that we're really
getting into the intersection of AI and art, which is an area that I've been wanting to
talk a little bit more about as well.
So very excited to have you on.
Earlier today, you delivered a keynote at the conference and we'll jump into that.
But first, why don't you walk us through your background and how you got into AI?
So how did I get into AI?
Good question.
My undergraduate was in English literature, creative writing, and I finished that, and
it turns out some of your listeners may not know this, but it's hard to get a job when
you're undergraduate.
It's in English literature.
You actually have to work at something.
So there's David Foster Wallace and then there's me.
I became a database programmer, which actually I love coding and worked for a while as just
a coder.
Coding databases in Albuquerque, New Mexico and playing music, the pinnacle of my music
career happened in Albuquerque as well, playing for dozens of fans in coffee houses all around
the street.
Is that nice?
I don't know.
I just was doing what I could.
I was passionate about it.
And I drifted back into grad school.
I think I was just, you know, for the intellectual challenge of it and it made more sense
to stay with computer science.
So actually, I think it was one of the better decisions that I made in my life.
I just not easily took the joint, you know, the overlap of two things I was passionate
about, which is computing and music.
And I just said, well, I like music and I like computers.
So what can you do with computers and music?
And you know, it's 24 and that was what I was thinking.
And I said, hey, let's do AI.
Let's do AI music.
So I wanted to work with a guy named Doug Hofstetter at Indiana University.
Okay.
I ended up actually not doing my PhD under his direction, but took courses with him and
ended up working with some other advisors there.
Just kind of dove into a PhD in music and music cognition and computer science and kind
of kept at it and eventually, you know, eventually ended up at Google, having been a faculty
member for a while at University of Montreal.
Okay.
But really, it was kind of following a passion, just doing what I thought I was good
at.
Oh, that's awesome.
That's awesome.
So maybe tell us a little bit about your keynote today and along the way, weave in magenta
or you can start with magenta and then go into the keynote, whatever kind of makes
the most sense.
Sure.
So today's keynote had a particular focus that I think is important, which is that we
can't do machine learning for music or art without the music in the art.
So machine learning is about learning how to solve a problem.
You know, you build an algorithm that itself can learn how to solve a problem.
So I guess it stands to reason that if you want to build machine learning algorithms
that can make music or make art or be tools for musicians and tools for artists, they have
to see the right data.
They have to see the world in the right way.
And so I talked about two projects.
One of them came out today on the magenta blog.
Please check it out, g.co slash magenta.
It's a recurrent neural network trained to make piano music perform a score that it writes.
And crucially, it's trained on real piano performances, captured and midi.
And there's captured and midi really isn't that important.
We still know where, you know, all of the keys were pressed and how long they were pressed
and how hard, you know, the velocity, et cetera.
For your listeners who aren't familiar with midi, think of that as just kind of a way
to store the events that happen when you play, you know, a synthesizer or electric keyboard
or an appropriate piano.
And it turns out that when you train on this data versus a bunch of musical scores that
is with no performance, timing, you know, just the score.
The resulting output of the models is dramatically different.
And to my ear, at least, much more human sounding, almost delicate sometimes in terms of how
the model figures out how to play the piano.
So I thought that was a really nice story.
And the scores that you played during the keynote were incredible.
I mean, the after.
Like there was the before and the after before is just the model train on the score.
But the after was incorporating in, I guess, the more subtle effects, like, I don't know,
attack and delay, I guess, of the ways I'm thinking of it.
Like, just the force with which they pressed the nodes and that kind of thing and timing,
subtle timing differences.
And yeah, I never heard anything like that from a computer-generated program.
Yeah, it's quite nice.
I mean, I haven't done a thorough enough literature search to know if, you know, I haven't
heard anything like it either.
Hopefully people will come up and say, hey, we did this cool work before and we'll say,
great, we'll credit it.
We'll learn from it.
You know, this is a research project.
We're trying to always give credit where it's due, but I haven't heard anything quite
like this.
And I think you're right, you know, the way to think about it is, you know, Chopin wrote
a piece of music, right?
Yeah, we still love listening to different pianists interpret that music.
And especially with that kind of music, the interpretation actually matters.
You know, if you don't believe it, it's really entertaining to listen to a truly robotic
performance of a piece of music that just doesn't work.
And I think, you know, in modern pop and rock and jazz, it's even more so the case.
Like think of Jimmy Hendrix playing the star-spangled banner.
Like we're not listening to that because of the score, right, because this melody is
so good, right?
Right.
It has cultural significance and it's this carrier for the sounds that Jimmy's making
with his guitar.
Yeah.
So like this idea that the expressive timing matters is, you know, if you kind of unpeel
the onion a little bit, it's pretty clear.
And it's really fun to see what a model can do when it finally has that data.
Well, you made a comment that I thought was really, really interesting and that was that,
you know, what this work gets you closer to is looking at the keyboard or other instruments
in other cases, but as, you know, sensors that are capturing sensory motor control from
humans all the way up, you know, from fingers to muscles to, you know, neural transmission
and that kind of stuff.
Can you elaborate on that a little bit?
Yeah.
And did I capture that?
You did.
I think you did, yeah.
So first in terms of just the model, yeah, what's being captured is how hard the finger
hit the key and there are lots of just kind of the data picks up on, you know, human constraints
that I think are important to music, right?
And I've always been fascinated by the connections between dance and music and between motor control
and general and music.
There's lots, you know, piles of books written about why is music here?
Is it maybe creating co-presence between people or it's about motor control synchronization
and it's about just having fun, but in any case, it's clear that there's the motoric aspect
is really important.
The same thing is true for drawing, right?
The constraints of the hand, what the hand can do, I think, is really important whether
it's holding a paintbrush, you know, or a pencil.
I guess arguably if you move to Photoshop here in a different world, I'm not sure that
it matters how your hand holds the mouse, but maybe there, too.
And it's really cool that we're able to train on the data that drives that.
I could say more about this if you want.
Sure.
The other thing that I would add is a lot of people work in machine learning right now
are working with images and with audio.
I mean, we are, too.
And I think it's really important.
But I'm always concerned by, or not concerned by, but intrigued by the idea that when we
as people, when we make new artifacts, whether it's like a new painting or a new piece of
music, we tend to actually not do it pixel by pixel or, you know, we don't have the control,
even our voices, we don't really have control over the waveform, right?
We have a buzzer in our throat called, you know, called vocal cords and then we're shaping
our vocal tract, right?
It's very, very in machine learning terms, it's, it's a pretty low dimensional control
surface, right?
So we're not dealing with like millions of parameters.
We're able to like move some muscles around and make something vibrate.
With playing a piano, we build this thing out of wood and metal and then fundamentally
we bang it with our fingers, right?
And so, you know, I think, I think trying to do machine learning for art and music, trying
to move into these spaces that are really low dimensional and by that, I mean, you've
only got 88 keys and that may seem like a lot until you think of the number of pixels
there are in an image or the number of moving numbers there are in a one second of CD quality
audio.
It's a relatively low number of parameters to work with.
And I think that that ties to these really beautiful ideas about, you know, what is meaning
but at some level, compression, like just pulling the important bits out of a hard problem
and it's easier to get there.
I'm getting too philosophical, but...
No, I mean, it's an interesting conversation and it reminds me a little bit of not much
of an audio file admittedly or not at all an audio file really admittedly.
But...
Big headphones, I mean, come on.
But, you know, you get people argue over, you know, CDs versus vinyl records and like
the, you know, the get into these debates about the richness of the vinyl records or transistors
versus tubes and that kind of thing and it does, you know, when you think about it in
this context, it does kind of think of, you know, the machine learning, the input that
we're giving to machine learning models in a lot of ways is kind of reductionists and
do we, you know, how do we make sure we don't reduce out the essence of the thing, right?
Does that make any sense?
Yeah, it does.
I mean, so first, I think that we are always at risk of doing that and what makes the
problem retain, you know, retain for me its beauty is that, you know, we're always there.
Like, I'm not interested in a machine learning algorithm where I can just push a button and
have it do its work.
I mean, I think it's cool, like the samples we posted today were basically pushing a
button, but that's more or less to understand how the model works.
I think these get interesting when you close the loop and you have musicians able to work
with them and use them as ways to kind of expand possibilities or create some, some
line and then you add another line, et cetera, like that.
I think that becomes, becomes less reductionist, but yeah, look, let's be real.
What are these bottles doing?
They're basically warping data.
They're doing some, they're taking some numbers in and they're transforming them and
they're pushing some numbers out.
Hopefully you've done the math right and you can, you can roll the dice and sample from
these and get lots of different really interesting instances.
But yeah, I mean, reductionist is a pretty fair term.
Were we close to being able to do like a music style transfer, like, you know, I've got
this rough idea of, you know, score or melody or, yeah, I don't know what the input would
be, but, you know, show pan eyes this for me.
The demo that you played was kind of a vocative of that kind of idea for me.
If we rely on the representation of MIDI, where what we're manipulating are the notes
and when they happen and how many of them there are and we're adding and subtracting notes
and we're performing them, yeah, I think we can, we can imagine with some work being able
to do a style transfer over something.
However, can we take an Adele tune and make it sound like it was done by, I mentioned
Jimmy Hendrix, right?
Right.
It's a very different kind of style transfer.
Or can we, can we take it and Adele tune and make it sound like bebop jazz from the
audio?
Right.
We're very, very far from that.
I think, to be honest, I think that's more getting at the flavor of what style transfer
for images does.
And the reason I bring it up is that there's a nice trick in style transfer for images.
You know, for example, taking a painting and making it look more like Picasso, which is
that it's, it's looking at little patches in the image and the patches are varying sizes,
but it's always local.
Right.
Literally, you can imagine just like moving a little spotlight over, over the image.
And then doing transforms at some layer of granularity.
So you can like get thicker brush strokes versus thinner ones, right?
But in music, you know, music unfolds in time.
And it's not always the case that it's the nearest sounds that are the most important.
There's not this locality, this spatial locality in music.
And I think, I think that would make like a style transfer so that something sounded more
like, you know, an electric guitar quite, quite a bit harder.
I don't, certainly the tools that are used for image style transfer don't quite make sense.
Which is why, by the way, we don't see lots of groups have tried this.
We don't see compelling audio based style transfers.
Okay.
We're getting there.
We'll get there.
I mean, someone will figure it out.
But it's just, it doesn't sort of fall out for free from the, from the image side.
Right.
Right.
So why don't you walk us through the, kind of the technical underpinnings, the architecture
of the, the project that we just talked about.
The project performance RNN, we give things names just to have something to talk about.
It's not an important name, but performance RNN is a recurrent neural network called
LSTM, long short-term memory, that is listening to, to lots and lots of performances, in this
case, panel performances.
What it's seeing is actually a very simple encoding of that performance, a note turned
on that had this velocity, a note turned off, let's advance time, let's advance the clock.
So it's almost like, imagine you've got a paper punch in your cutting holes, you punch,
punch, punch, punch, and then you move forward and you punch, punch, punches, a very, very,
very simple way to reduce to represent the data, but it's not losing any of the data.
You can reconstruct the entire performance from there, right?
The recurrent neural network is, is trying to solve an interesting problem.
It's listening, so to speak.
It's processing this information.
It's trying to predict what's coming next.
And in our case, it can predict, hey, generate another note, turn off a note, or move the
clock.
And every time it gets it right, every time it predicts correctly what it's being trained
on, you know, it gets a good job.
And every time it gets it wrong, then we adjust the weights of the network.
So these are weighted connections between computational units or nodes in the network, so that
it does better next time.
And so it's really playing kind of a funny telephone game, you know, where you, maybe that's not
the right word for it.
I had an analogy for this, but like, it's kind of a weird thought.
You know, you're listening.
You've heard six notes of a melody, and you're trying to guess what the seventh note is.
Even people aren't going to get it right all the time, because there's lots of possible
ways, which things can go.
So in the end, these models, they don't memorize specific tunes.
Instead, they kind of figure out patterns of what should come next given these previous
notes.
And in fact, they learn about chords.
They learn about arpeggiation.
They learn about scales, because those happen in the data.
Okay.
And when we want to make a piece of music, we do take a clever, very simple trick that's
been around for 20 years, you start with a note or two, and then you predict what should
come next.
And that's like a, not an exact answer.
The model says these are the possible things that can come next.
You choose one based upon those probabilities.
And then you feed it in, like you feed, it's, you feed the networks output back in as
input, and you kind of keep going called auto regression.
And in that way, you end up like composing a new score.
Okay.
How many order of magnitude musical samples are you training this on?
And are you training them on full scores or snippets or does that matter?
It does matter.
And the end, for technical reasons, you end up kind of chunking things, but we have ways
to do that automatically.
So in terms of when the model receives the input, it processes an entire score.
I think it's on the order of 20 or 30,000 pieces of music in this case, it's not huge.
They're all performances, and they're all from solo piano.
Okay.
So, and, and did, did you commission them or co, no, these are, so these are all pieces that
were, you know, like Chopin, you know, it's all public old, you know, old classical music
and their performances that come from a number of sources, all kind of freely out there
on the web.
And you can, you can track those down if you follow the magenta blog where we have it.
Okay.
It's probably not interesting to go and aware the data source has actually come from,
but they're, but behind the dollar, are people having performed these for sometimes
for competitions?
Okay.
One of the sources is that you can use is, is Yamaha has a particular kind of piano that's
like a player piano called the disclivier, and they've had competitions where very, you
know, top to your pianist come and play.
And at the same time, the piano records all of the movements of the hammers, so you have
the trace left behind of what they played.
Oh well.
So we're training, we can train that data like that.
Okay.
Oh, really interesting.
So you've got this, you know, 20, 30,000, and were they all specifically Chopin or
no matter how much variation, large variation across the classical tradition.
Okay.
And we have other, we can collect other data sets.
In fact, you know, it's perfectly reasonable for someone to actually just collect a few
hours of playing of their own, and that's going to be enough to train a model.
Not one hour, but if you, if like one of your listeners is say like a jazz pianist and
does jazz improv, you know, you could, you could train a model on just a few hours of
improv, and then they'd have their own, like the model would encode their own playing style,
which is kind of cool.
Right.
Right.
And then when you sample from the model, you can kind of hear some, some of your own aspects
of playing.
Wow.
So what degree was the network architecture for this particular neural network?
How is it unique from other network architectures that are used for, you know, other LSTM based network
architectures that are used for like, you know, the kind of thing you do to, you know, we've
seen the projects where we're getting scripts auto generated.
Right.
That's good.
Yeah.
Yeah.
Yeah.
The network is actually pretty much a generic LSTM.
What's somewhat novel is the representation of the data.
So what the model is trying to predict.
There have been other people who have predicted like the duration of, of a note in terms of
its relative duration in the score, like, I'm going to generate a middle C, and it's
going to be a 16th note.
And we've simply extended that to say, well, forget about whether it's a 16th note.
Instead, let's just move the clock forward some number of milliseconds.
And that way, we'll just learn about real time.
Right.
So if that's not clear, that means we don't need a metronome to figure out what a quarter
note is.
Right.
If that's not clear, like, what a quarter note is is that it's a quarter of a measure.
And what a measure is is defined by the beat.
And if the beat's fast, quarter note takes less time to be played, right?
Here we're just saying, we're just going to look at the time of the pieces at unfolds
in the performance.
And so the model doesn't know what a quarter note is at all.
It just generates notes and moves the clock.
And that, I think that's unique.
Again, we just put this blog posting out and we're in the middle writing a paper around
it.
And it's pretty hard and research to do something that's completely new.
You always find that someone did something similarly, right?
I mean, there's almost nothing new under the stars and sun.
Nice.
Well, we'll definitely include the audio that you shared in your keynote in the show notes
for folks to check out.
It was really, really compelling.
We also talked about a project based on the Google Draw experiment.
Yeah.
Tell us a little bit about that one.
That was an AI experiment called Quick Draw.
It was done by the Creative Lab folks in New York at Google.
And what they did was they, we already had some image classifiers that we can use that
can identify like tens of thousands of different kinds of images.
You see, you point your mobile device and it recognizes through the camera that that's
a lamp or that's a dog or that's a cell phone.
And so someone had a clever idea of saying, well, what if could it identify a sketch of
a dog or a sketch of a cell phone, it turns out, yeah, I can't.
So you get to play Pictionary against an image classifier.
Actually, you're playing Pictionary with an image classifier.
Remember Pictionary is collaborative, right?
And so you're given a prompt like, hey, you have 20 seconds to draw a dog and you try
to draw a dog.
And if the image classifier can guess it, then you get a point, right?
So at some point, we decided we wanted to use this data for machine learning.
So we changed the messaging on the site.
So it said, hey, we're going to, you know, if you want to play the game, we're just going
to keep anonymized drawings around, we're going to learn from them, right?
We're going to train.
So so we kept that data.
We gave it back to the community to use for other, for artistic purposes and people have
done tons of crazy things with this data.
What are some of those things?
So this, by the way, this is an art machine learning.
This is just the people's drawings, analyzing how different people in different cultures
draw circles, turns out like some cultures draw circles clockwise and other cultures
draw them counterclockwise and you kind of clustered things.
The way that people draw chairs, Asian culture is apparently draw chairs often, I'm going
to get these backwards because yeah, they usually draw chairs in, no, it's in three, whether
you do it in 3D or 2D, whether a chair is just like an H almost, right?
Which is what I would draw.
And I think that's actually an Asian pattern.
Okay.
Yeah, someone else outside of Google took the data, the circle stuff is like a blog posting
from a couple of weeks ago, encourage your listeners to find it.
Just a Google like quick draw circles and it has you draw a circle and tells you some
things about the circle you drew and analyzes the circles from around the world.
And so I don't know, you just kind of get this kind of fun, I mean, it's not world changing,
but it's interesting, right?
It's a kind of sociology behind it.
Anyway, I digress.
We had this data and David Ha, who is the primary person on this paper, decided to train
a recurrent neural network to try to reproduce the strokes.
And we have the strokes as they appeared in order.
So, you know, if you were trying to draw a garden, if you drew a flower first and then you drew
the grass, then it would be reproduced in that order.
This was a slightly different model.
This was two different recurrent neural networks and an intermediate representation.
But maybe that's too far a field too.
The upshot is you can now generate new instances of like dogs or cats or, you know, several
hundred classes and kind of try to get a better understanding of what people are doing
when they draw them and also have a way to explore the space.
Hmm.
So what exactly does that mean?
You've got a kind of encoder, decoder encoder set up where the decoder is learning, how
people are drawing these things and you've got an encoder that is trying to, you know,
give it a thing that it's trying to create that thing or...
I think you reverse it to reverse it to reverse it to reverse it.
Yeah, so the one way to think about it is that the work that I described with the performance
RNN, you could think of as a decoder.
It's not encoding into some different representation.
It's basically just taking the score and trying to predict the next note.
This model does something slightly different.
It takes the strokes and tries to encode them into a vector, into a sequence of numbers.
And then only from that sequence of numbers does it try to reproduce or decode the drawing.
So it's getting these strokes in.
It's pushing some information via recurrent neural network into this kind of intermediate
representation called our latent space if you want the technical term.
And then it's trying to then recreate the drawing by decoding it using another recurrent
neural network.
And the crucial, the reason that we use this intermediate step of this latent space is
that if we've constructed it correctly, we can do a really nice job of generating new
samples with lots of variants for the same problem.
I don't know.
A library on that.
Yeah.
No idea how technical they get.
So for example, we could take the drawing of a face that has like almost like a triangle
inverted triangle or like a pointy chin, right, and we could run that through the encoder.
And that would give us some vector.
And then we might take another face that's a big round face, we know with a big round
nose and run that through the encoder.
And that would give us another vector in this latent space, right.
But now we have these two vectors that are not very big, maybe a few dozen numbers or
maybe a hundred numbers.
And we could just take the average of those two.
And that should, if we've trained our model right, give us the face that is somewhere
towards, yeah, kind of between this triangular face and between this circular face, right.
And if you do that enough, you get to like the meaning of a face, right?
Is that the idea?
Yeah.
I mean, you capture, yeah, I mean, in a hand-woven philosophical way, you get the platonic
face.
Right.
In a less ridiculous way, at the very least you get like, what's happening is that latent
space is forced, is encoding those aspects of the face that are the most important to
remember if you want to be able to cover the variance of faces.
And so, yeah, it's captured something really important, some of the really important aspects
of faces.
So in that latent space, as you move around that space and maybe you generate some random
vector in that space, you should still be able to decode some kind of face, right.
Let's call it a variational model.
Okay.
So I don't know if that captures the gist of the tech.
I think that one of the take home messages is the goal, the reasons why we're caring
about having this embedding space, this latent space is that we can use it as a way to
give artists more control over a model like that.
So for example, you could take, you could draw a face and encode it through the encoder.
And then now you have this vector and you could perturb that vector, you could move that
vector around, you could use this as a starting point for some, you know, some other work
with the model.
You could just email that vector to someone else and they could reproduce it, they could
know what you think about.
Now granted, this is all in the context of drawings that took 20 seconds to make using
a mouse on a computer.
So our expectations are not that, you know, people will go, oh, I can make, you know, the
next shagall painting or whatever with this.
But that in principle, you have this space where, you know, we can encode some drawing
into some space that is kind of really good at storing drawings.
And then from there, we can carry that around and we can decode not just that original
drawing, but tons of drawings like it, and then maybe, you know, be able to move forward
with, you know, possibilities for moving around that space for reasons like for animation
and things like that.
Okay.
It's interesting.
One of the other, one of the interviews I did yesterday that'll be coming out at the
same time in this O'Reilly series is, we're talking about word to veck.
And it's interesting to kind of think about all of the various, we talked about several
applications of embeddings in the context of, you know, word to veck and related things.
But, you know, this is definitely drawing to veck.
Right.
Yeah.
So you encode your samples into this vector or vector space and then you use that to,
you can use that as, you know, almost like a filter for creating things through the decoding
process.
I think one of the examples you showed was, and contextualize this for us, but you had
folks draw like an eight-legged pig.
You encoded that and then decoded it using the kind of the pig vector.
Yeah.
Is that the way to think about that?
Yeah.
So you're trading this autoencoder model only on pigs.
Yep.
So it knows nothing but pigs.
Yeah.
Oh, it's pigs all the way down.
Right.
And, you know, we make these latent spaces pretty small and inject some noise.
We don't want them to overfit.
It's boring if they just memorize the data.
Mm-hmm.
Right.
So this little vector can barely, barely manage to generate a pig, but it does pretty
good job of generating pigs.
Mm-hmm.
Yeah, and you give it a pig with eight legs.
Well, it never sees pigs with eight legs.
So it encodes through the encoder, the recurrent norm that work.
It encodes some numbers that were driven by those, those strokes, including the eight
legs.
Mm-hmm.
But when it decodes it, what's remembered by that embedding is that there were four.
Mm-hmm.
Right.
I think it's kind of nice.
Also, the other example that I talked about in the keynote was if you have the same pig
class, the same pig auto encoder, and you take a nice drawing of a semi-truck, you know,
it decodes, guess what?
It's a pig truck, right?
Like, it kind of turns into a pig because that's what the pig model knows about is pigs.
And it's at one level, that's a weakness, right?
It's like, well, but there are so many other things in the world.
But of course, you can train more models and you can train models that are conditional
so they can represent more than one thing.
It's that this latent space is really capturing some important aspects of pigness, right?
And people have someone else in open source, not us, took those very embeddings and then
did something fun.
They started looking for examples of pigs in the quick draw dataset that were far away
from that embedding.
So you imagine you embed a bunch of pigs, so you get the average embedding, you know,
you don't have to embed all of them, but 10,000 of them get the average, right, to
take the average.
Even from there, you embed a pig and you take its distance, how far away is it from that
average, right?
And then, like, what's fun is they just, they just found really poorly drawn pigs, or
they occasionally someone did just written the word pig, you know, and it's like, that's
not a pig.
I mean, it says pig, but these models didn't learn to read.
And so, and so you get this weird kind of fun outlier detection.
And you know, you can cluster the space too.
That's often done with word-to-vec models where you say, okay, like for cats, there are,
it's not that there are just, you know, several million, completely unique cats.
People kind of either draw them in profile, you know, or they draw them just the cat's
head.
And sometimes the profile flips us where this way.
And if you, in this embedding space, if you do clustering in the embedding space, so
you look for, you know, it's not like given the number of possible, the possible values
in this embedding, everything is equally spread out.
It's like, there are mountains, right?
And here's the profile mountain, and here's the profile, the other way mountain, and here's
the face mountain.
And you can visualize that and kind of get an idea, you know, what
people are really doing with these drawings.
Can you then identify a distance from your average pig vector beyond which you won't be
able to recognize something as a pig?
Yeah, and that's the classifiers job.
In this case, it was an autoencoder, so it wasn't trying to decide whether something was
a pig or not.
It was trying to draw a pig.
So there's no, there's no magic value, but you could always calculate one, right?
You can kind of figure out where, where you're going.
It's important to keep track of the goals, right?
Right.
You know, this is, this is a kind of small, it's a large data set in terms of the number
of samples we have, but you know, it's a pretty simple drawing task.
I think it's, it's fun to think about where we can go if we have, if we have better data
and, and better models.
And where is that?
So I think there are a couple of possibilities.
One is that we just nail it, and we, we basically do something that happens one in a million
times, which is we invent a new art form, where we enable a new art form.
Now, it's been done before, technology has done this before, the film camera, enabled
a new art form, right?
The drum machine enabled a new art form.
And we could probably, together we could riff, and if we had a whiteboard, we'd come up
with 30 or 40 of them.
But we wouldn't want to forget that there were another, you know, thousands and thousands
of, did the theorem in invent a new art form, maybe, right?
The synthesizer, yes, right?
So maybe we'll invent a new art form, and it's really interesting to think like what
would, the core of that art form would be that the artist would have something smart
against which to, to project ideas.
I think that would be the core.
That, you know, you can try things out with, with your sequencer, you know, or you can
try things out with Photoshop, or you can try things out with, with a pen and paper.
But the idea that we would have machine intelligence models trained that are, by analogy,
as smart as translate, right?
Because they're trained right.
And you, like, that's really, really interesting, I mean.
There's an inherent limitation in that, in that it, we're training on, for the most
part, things that we've seen before, right?
And so it kind of cuts off, at least intuitively, seems to cut off this creative avenue for
innovating, for, you know, creating the holy new thing.
I agree.
And I think that, I think that if we create a new art form, it will be through gradual,
through many, many interaction loops of artists working with this technology.
So for, like, that's not that crazy to think about, if you watch what happened with the
808 drum machine and drum machines moving forward, people first use drum machines on, on
sort of, you know, Roland's terms.
This is what a drum machine is, you know, but gradually, they just are like, I'm going
to use it however I want, right?
And then the drum machines were manufactured to adapt to that, et cetera.
And I think one way to think about this is, if this were to actually generate a new
art form of interest, you train a model, it generates some things that are somewhat surprising
to you, but you like, you keep them.
You change your writing style somewhat or you change your performance style somewhat.
Maybe, you know, like, I can imagine living in a world where the data is so important,
like part of what you do as an artist is create data.
And so in some sense, you're like, you are helping create this model because you're the
one providing it with its lifeblood with data, or ultimately, you're hacking the model,
either because you're a coder, right?
So then you say, okay, this model changed slightly how I do music.
Maybe it just added some rhythms to my repertoire.
I don't usually use it.
It does some crazy harmonization that I wouldn't normally do.
And then you make some more music and then you retrain your model, but now there's new
stuff there, right?
And you've moved a little bit, right?
I don't think the movement has to be earthquake large.
You know, it doesn't have to be groundbreakingly large to be new, right?
I mean, imagine just like if we kind of work out a new way to harmonize music.
So all, you know, you have a melody and you have all these extra voices.
You know, just imagine via a model that's trained right, we suddenly get different kinds
of harmonizations that we didn't get before.
That would already be interesting or another example I like, which is crazy hard,
but I think it's very evocative.
I think like a long form, something long form like a novel, right?
And think about plot, right?
Like plot, plot is hard, right?
You can, like I, my hat's off to someone who can write like a long,
my hat's off to Game of Thrones and keeping all these characters straight, right?
George R. Martin is my hero, but you could imagine like,
imagine that some writers care less about plot and they really care about character,
they care about texture, imagine the right kind of machine learning model
that can generate intricate plots with really interesting relationships between characters
and movement of action, right?
Such that like maybe in a way that's even, you know, at some level of complexity
too hard for a human to do because it can search out more possibilities,
but yet maybe everything kind of clicks and lands and it feels good as the reader to be like,
oh wow, that character just came and did that, that's crazy, right?
Almost like writing jokes that have sort of three punchlines that all
in, you know, land at the same time.
That like because, you know, machine learning algorithms are really good at
high dimensional spaces with lots of possibilities.
You know, maybe we'd land at some new form of storytelling.
I don't think it would be one where we would care about reading the computer's story,
but that the computer might add something to the writer's world where like,
in some sense the writer might offload plot.
You're like, that's not, I don't do plot.
What I do with the plot is something even more interesting.
Like I shape it, I craft it, I make it beautiful to read, right?
So that's one way to think about it that like, you know,
I don't want this to be too long-winded, but it's really important to me like,
what the drum machine did was in one really important way,
it offloaded some of the percussion work.
But, you know, if you want to be cynical about it, then you're like,
oh yeah, and it just killed, it just killed percussion.
It didn't.
It offloaded one thing and that opened up a bunch of other opportunities, right?
And so then the idea is, well, what sort of things can you offload onto a super smart
machine learning model? And to be honest, I don't know, I don't know the answer,
but I mean, if you move through different kinds of media from painting to music,
to literature, you certainly can get some ideas.
Yeah, awesome, awesome.
And so magenta as a whole is kind of an umbrella for a bunch of these different
directions, right? Maybe you take a second to talk about magenta and how Google
thinks about that, why Google's even, you know,
bothering. Tell us about magenta.
Yeah, this isn't funny. This question usually comes early, right?
It's good to have this come after people have heard some specifics.
So the basic idea is of magenta is posing the question, you know,
what can we do with deep learning and reinforcement learning in this basic creativity?
And we've already fully realized that the meat of the problem lies in these algorithms
interacting with musicians and artists. And that's not because we're afraid of trying to get
these models to be interesting on their own, but that I think that's how art works.
It's collaborative, you know, other artists are working together and we're thinking,
you know, these things are going to work with artists.
And so we, in the last year, magenta has been around publicly for about a year and before
we launched, we were working on it for another six months.
We've done work in music sequence generation, including musical scores and also performances
like we did today. We've done work in generating new kinds of sounds and synth project, which is
basically building a synthesizer where all of the sounds are dreamt up by a neural network.
And we've done some unpublished work in joke telling.
And it's, it's unpublished because, well, it wasn't very funny.
Now we did a summer internship like looking at joke telling as exercise in generating
interesting surprises. I think especially punchline driven humor is like, oh, that was
surprising in a nice way. You know, yes, my kids like getting to the level of dad joke.
It should be hard. I know. I know. I know we're both dads here. We should be experts at this.
Like if we just team up. Yeah. And what my motivation actually is speaking of kids might,
okay, so let me finish the, and then thinking a little bit more about language, but most of what we've
had to do, oh, and then learning to sketch the sketch to sketch our NN stuff. And then some
implementations of style transfer for images. So, so we've got kind of a wide array of things
that we're trying. I think the glue of the project, what holds it together is a, we're limiting
ourselves to deep learning and reinforcement learning. We're looking at creative applications
of machine learning, but we're not going to try everything. Like there are lots of ways to solve
these problems. Lots of great ways. Right. We're not saying that machine learning is the only way
of the sort we're doing. It's just like, we got to limit ourselves to something. So we're part,
we know, we're really linked to TensorFlow. Also that really what we care about are like behind
these, this one year of trying some new things, I think the really core issues lift above any
specific medium. I think it's about, it's about storytelling and narrative, whether it's music or
paintings or literature. And it's about structure and narrative arc and about these ideas about
surprise and what, what makes something simple. So I think there's a, there's a whole area of like,
you know, generative models for media that, that, you know, can we generate interesting
bits of media for us to share that help us tell the story of our lives and, you know,
different modes of communication, possibly coming from this. Actually, almost certainly coming
from this. You know, those are the goals. Why is Google doing this? Well, like, one thing is,
we're publishing a lot of papers. So we're part of Google Brain and the framework of generative
models is important. It's important. It's an important research topic. And the idea that you,
you know, maybe you'd want to generate, you know, new candidate molecules, maybe you'd want to
look at healthcare, maybe you'd want to look at something for robotics and generating trajectories.
But I think, you know, when I look at what my kids are doing with their mobile devices,
I've got a 13 year old and an 18 year old, you know, they're, they're information seeking,
they're entertainment and they're communicating with their friends, right? So there's a huge chunk
of what we're doing with computation that has to do with entertainment. And I think, you know, it's
a really important area of research. I mean, just, you know, just point blank. I think music and
art are important. They are important for our lives and it's absolutely worth investing some time
into it. Awesome. That's what we're doing. Awesome. Anything else that you'd like to
mention or any other places that folks should look or are there, you know, three canonical
resources to, you know, for folks that really want to dig into this? Yeah. So there's two things
I would say. First, please visit g.co slash magenta. That's the shortest link I have. And we have
a blog, we have a blog there that's getting, if you want to geek out, would you call it nerd time?
What did you say? Uh, nerd alert. Nerd. Yeah. We nerds. Listen or nerds. If you like that,
come look at our blog. And the other thing is we're very, very actively trying to engage with
some particular types of folks in the community. There are three types. One type is pretty obvious.
Artists and musicians, fun. And then there's always the machine learning folks, nerd alert. But
there's this middle, middle ground where I think there's probably more there than any place else. And
it's, it's in this kind of world of creative coding. Do you know what I mean by that? So like,
I mean, what do you mean by that? Well, I mean, I'm sort of stealing that, you know, creative coding
is, is just that it's coding. I mean, the way I define it is it's the creative aspects of coding.
Right. So I don't want to add to it. So coding or coding applied to creative. It's that. Yeah.
Sorry. I said wrong. Yeah. And I think, you know, I was talking about how a musician might
play a few hours of music and then use that to drive a machine learning model. And in some sense,
they're hacking the model because they're providing the data. But also building these models
as a creative thing in and of itself. And we're trying to build some frameworks where like,
if there were an artist who knew how to code some, you don't have to be like the world's best
machine learning coder. But we have some models where, you know, you could change a few things.
You could, you could say, I want, if you had some way that you could, for example, let's say you
wanted the music that was generated by the model to be more shimmery. Whatever that means. Fine.
If you think you know what it means and you can define that in a way that if we get a piece of
music and you say, oh, that sounds more shimmery, then we can actually use that with the, we can train
the model to do a better job of generating that kind of music. So I don't know. I think there's
a whole, a whole direction of like having coding be part of artistic generation. And that machine
learning and a project like magenta is really a core place to try that. And so we're trying to get
more people through open source to work with us, to collaborate with us, to make art and make music
and hack stuff. Awesome. And so we'd love to see more people from, you know, from your listeners
join us. That's it. That's what I want to say. Awesome. Well, thanks so much. I really enjoyed
this conversation and I'm sure folks will enjoy listening to it. Thanks for all of your great
questions, Sam. Thank you. All right, everyone. That is our show. Thanks so much for listening and for
your continued support, comments and feedback. A special thanks goes out to our series sponsor,
Intel Nirvana. If you didn't catch the first show in this series where I talked to Naveen Rao,
the head up Intel's AI product group about how they plan to leverage their leading position and
proven history and Silicon innovation to transform the world of AI, you're going to want to check
that out next. For more information about Intel Nirvana's AI platform, visit intelnervana.com.
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference.
To enter, just let us know what you think about any of the podcasts in the series or post your
favorite quote from any of them on the show notes page on Twitter or via any of our social media
channels. Make sure to mention at Twomo AI, at Intel AI and at the AI Conf so that we know
you want to enter the contest. Full details can be found on the series page and of course,
all entrants get one of our slick Twomo laptop stickers. Speaking of the series page,
you can find links to all of the individual show notes pages by visiting Twomo AI.com slash
O'Reilly AINY. Thanks so much for listening and catch you next time.
