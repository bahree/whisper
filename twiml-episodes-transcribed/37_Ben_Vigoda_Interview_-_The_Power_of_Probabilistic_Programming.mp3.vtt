WEBVTT

00:00.000 --> 00:16.000
Hello and welcome to another episode of Twimultoth, the podcast why I interview interesting people

00:16.000 --> 00:20.960
doing interesting things in machine learning and artificial intelligence. I'm your host,

00:20.960 --> 00:26.880
Sam Charington. The show you're about to hear is part three of our five part O'Reilly AI New York

00:26.880 --> 00:32.640
series sponsored by Intel Nirvana. I'm very grateful to them for helping make this series possible

00:32.640 --> 00:36.880
and I'm excited about the cool stuff they launched at the O'Reilly AI conference including

00:36.880 --> 00:43.120
version 2.0 of their neon framework and their new Nirvana graph project. Be sure to check them out

00:43.120 --> 00:49.280
at intelnervana.com. And if you haven't already listened to the first show in this series where I

00:49.280 --> 00:56.000
interview Navine Rao who leads Intel's newly formed AI products group and Hanlon Tang and algorithms

00:56.000 --> 01:00.960
engineer on that team, it's Twimultoth number 31 and you definitely want to start there.

01:01.840 --> 01:08.800
My guess for this episode is Ben Vigota. Ben is the founder and CEO of Gamalon, a DARPA funded

01:08.800 --> 01:15.520
startup working on Bayesian program synthesis. We dive into what exactly this means and how it enables

01:15.520 --> 01:21.760
what Ben calls idea learning in this show. Note that Ben and I go pretty deep in this discussion,

01:21.760 --> 01:26.160
so I'm issuing the Nerd Alert. All right everyone onto the show.

01:33.440 --> 01:39.760
Hey everyone, I am here with Ben Vigota of Gamalon and I've been meaning to catch up with Ben

01:39.760 --> 01:45.920
for a long time and I was just telling him before we got started about how we do these Nerd Alerts

01:45.920 --> 01:52.400
now and he's like, oh, I'm going full Nerd Alert on this show. So if you like the Nerd Alert,

01:52.400 --> 01:57.040
you're probably going to enjoy this show. So Ben, why don't we start by talking a little bit about

01:57.040 --> 02:03.520
your background and what you're doing, the company, all that stuff. My background, well that

02:03.520 --> 02:10.800
that requires a full Nerd Alert, a super full Nerd Alert. Well, I was at MIT PhD and I was actually

02:10.800 --> 02:18.640
in a physics group second today. Oh, okay. Yeah, so physics is a good background, especially

02:18.640 --> 02:24.160
statistical physics and a lot actually at Gamalon, a lot of our, a lot of folks there are

02:25.120 --> 02:29.840
from physics background as well. I was kind of obsessed actually with machine learning and

02:29.840 --> 02:34.560
AI stuff back in high school and middle school and it was lucky enough to

02:34.560 --> 02:41.520
you get a few audiences with a few of my heroes and some of them recommended that I went

02:41.520 --> 02:46.800
going to physics. That's what I did. It's kind of, it's like if you want to learn how to play

02:46.800 --> 02:52.320
rock guitar where you go study jazz or classical, so you have good chops or something. So you know,

02:52.320 --> 02:57.280
that kind of thing. Yeah, so that's my background, did a postdoc and it was a visiting scientist at

02:57.280 --> 03:02.320
MIT for a little while and then I founded my, my first company was, we called them probability

03:02.320 --> 03:06.720
processors. So they're microchips. Okay. And it was really the first microchip architecture,

03:06.720 --> 03:12.640
first processor for machine learning, you know, before kind of GPUs and before the Google

03:12.640 --> 03:17.680
Tensor processor unit, we created a sparse dense tensor processing processor. Oh, wow.

03:17.680 --> 03:24.480
About five years actually before the TPU CPU. So that's in a bunch of stuff that's in like

03:25.520 --> 03:29.920
every cell phone call you place probably goes through one of those because it's in most like

03:29.920 --> 03:35.280
cell phone towers. Okay. Or a lot of them. And it's in cars and, oh, wow, might be going into

03:35.280 --> 03:40.880
the Amazon echo. So it's in a bunch of places. And is that company still going or was it like

03:40.880 --> 03:45.840
outside or acquired? Yeah. So that's kind of the basis of, you know, it's a big part of analog

03:45.840 --> 03:50.240
devices now, just the company that acquired us. Big Boston, Boston, based semiconductor, yeah,

03:50.240 --> 03:54.320
one of the best semiconductor companies and definitely the best one in Boston.

03:54.320 --> 04:01.520
Right. So, you know, but through that experience, you know, we always thought, you know,

04:02.160 --> 04:05.520
if we're going to have these processors, we got to make them easy to program.

04:05.520 --> 04:09.040
Right. And so we came up with what we call probabilistic programming.

04:09.040 --> 04:14.800
Okay. And, you know, back then you could kind of count on two hands, the other people who thought

04:14.800 --> 04:19.600
they were working on probabilistic programming. We were a small cult. We all knew each other.

04:19.600 --> 04:24.640
Yeah. Now it's just exploded. But, you know, we built one of the first industrial scale,

04:24.640 --> 04:28.560
probabilistic programming systems and compiler systems. Back around the same time,

04:28.560 --> 04:32.320
Microsoft was building in fur.net, okay, which they used. I don't know that one.

04:32.320 --> 04:37.200
Oh, you don't know that one? Yeah. So, you know, it was used to prototype some cool stuff for gamers.

04:37.760 --> 04:43.840
It matches like Xbox people, like if you're an Xbox player, and they used in fur.net to build

04:43.840 --> 04:47.600
a machine learning model that'll find you really good people to play against. Oh, wow.

04:47.600 --> 04:52.560
Okay. Kind of well matched your skill level. Yeah. And it's called true skill. And they also

04:53.120 --> 04:57.600
did some stuff for Bing that I heard about that. Okay. Obviously not an expert on all the things

04:57.600 --> 05:02.640
they didn't tell me. But yeah. So that's kind of where I came from in my background.

05:02.640 --> 05:05.840
And were you in the keynote this morning here at O'Reilly?

05:06.320 --> 05:11.040
I didn't get chance to catch it. It was, uh, I forget the name Jonathan, I think, from MIT.

05:11.760 --> 05:16.160
Professor there at CSAIL was talking about some of the work that they were doing was

05:16.160 --> 05:20.320
that involved probably ballistic programming. Oh, maybe Professor Josh Tannenbaum.

05:20.320 --> 05:26.320
Josh Tannenbaum, yeah. Yeah. Okay. Yeah. He came to my talk and then we were hoping to get to work

05:26.320 --> 05:30.720
together somewhere. Oh nice. Like we have great reverence for for what those folks do in their

05:30.720 --> 05:36.240
lab at MIT. And there's a lot of cross fertilization there. Also one of his former postdocs,

05:36.240 --> 05:40.480
Noah Goodman, who's not Professor at Stanford. Okay. He's a real leader in this field. There's

05:40.480 --> 05:45.840
a few others. David Blight at Columbia here in New York. Okay. So probabilistic programming seems

05:45.840 --> 05:49.360
like a good thing to dig into, but I don't think we got to what Gamalon is up to.

05:49.360 --> 05:53.920
Well, Gamalon started as the largest investment by DARPA in probabilistic programming.

05:53.920 --> 05:58.240
Okay. Over the last few years. And thus you can't really talk about any of the things you're doing.

05:58.240 --> 06:04.320
No, of course you can. Of course you can. What they call a six-to-one technology readiness level one,

06:04.320 --> 06:09.280
which is the academic and commercial research. Okay. You know, so we basically set out to make it

06:09.280 --> 06:16.720
really easy for data scientists or human statisticians or modelers to create large-scale complex

06:16.720 --> 06:23.280
Bayesian models and solve them with data. And then I had this breakthrough where we realized

06:23.280 --> 06:26.720
with the tools we had built, we could actually get a computer to write its own probabilistic

06:26.720 --> 06:32.560
programs. That's a new thing. That's not really an academia. That's just a Gamalon thing.

06:33.360 --> 06:37.520
We call the underlying technology Bayesian programs synthesis, but we branded it idea learning.

06:37.520 --> 06:42.320
Okay. Kind of compete with deep learning a little bit, like good branding, so we needed a good

06:42.320 --> 06:47.680
term. So idea learning. There's some wind behind these sales now, the deep learning. Yeah.

06:47.680 --> 06:53.520
It's interesting to see. So you have kind of a standard way. You walk folks through kind of

06:53.520 --> 06:59.600
Gaussian stuff, probabilistic programming, kind of how that all fits together. Yeah, I do.

07:00.320 --> 07:05.120
You know, you can also check out Gamalon.com. You can follow along on some of the examples that

07:05.120 --> 07:12.160
we have on the technology tab. Like one good example would be say I wanted to make a little drawing

07:12.160 --> 07:17.200
app, right? So we actually did this in one of our hackathons at Gamalon. It's pretty fun.

07:17.200 --> 07:23.040
The idea is basically like Microsoft Surface or a tablet, you know, you would want to

07:23.040 --> 07:26.800
with your finger or stylus, you know, straw triangle or square or whatever.

07:27.840 --> 07:33.440
If you're like me, you're not I'm not I'm a terrible artist. So my squares and rectangles

07:33.440 --> 07:38.240
and triangles come out super lumpy and messed up. But the system should still be able to recognize

07:38.240 --> 07:44.320
them. And then it replaces what you drew with what you meant to draw. Okay. So I've seen there,

07:44.320 --> 07:49.520
there are apps to do this and for that. Absolutely. They kind of I think ours is the coolest.

07:49.520 --> 07:54.640
Cause I mean, I was totally just a toy. Like it's not a product. It's just to kind of play with

07:54.640 --> 07:58.800
the technology. But one of the things that's really satisfying about it is most of those apps,

07:58.800 --> 08:03.440
you sort of draw a triangle and it's like, Oh, that's a triangle. Yeah. But then the triangle it

08:03.440 --> 08:09.040
puts there isn't the triangle you meant to draw. It just puts down like a 60, 60, 60, 60,

08:09.040 --> 08:16.880
60s on the bottom, you know, standard size. So it's not really your triangle. It's their triangle.

08:16.880 --> 08:24.320
Right. The thing, you know, so our app puts it's that map our experiment prototype hackathon thing.

08:24.320 --> 08:30.400
But it's a good pedagogical tool. It puts your triangle. Nice. Okay. So the way it does that.

08:30.400 --> 08:34.080
So I'm going to just take you through what's basically a cooking recipe. You could do a cooking

08:34.080 --> 08:38.320
recipe on a podcast, right? Like take two eggs or some flour. Yeah. So it's going to be like that.

08:38.320 --> 08:44.000
Nice. So what we're going to do is a cooking recipe to draw a triangle. Okay. All right. So

08:44.000 --> 08:49.440
triangle has three corners. You know, we're going to have to pick a x y coordinates for the first

08:49.440 --> 08:53.280
corner of the triangle. Cause this is going to be a random triangle. The corners could be all over

08:53.280 --> 08:56.480
the place. It's going to be anywhere. So we're going to pick those at random. So let's pick the

08:56.480 --> 09:01.760
first triangle. Maybe we're doing this. I'm going to date myself. But this is going to be a 1024

09:01.760 --> 09:07.040
by 768 JPEG image or something, right? I'm going to draw this triangle. So we're going to pick a

09:07.040 --> 09:11.680
number between one and 1024. And that's going to be the x coordinate. And I'm going to pick another

09:11.680 --> 09:20.880
number at random between one and seven 68. I guess zero and seven 67 on the vertical axis. So we

09:20.880 --> 09:25.920
picked that first coordinate. And then we're going to pick a second coordinate. We have two points

09:25.920 --> 09:31.040
and we draw a line between them. And then, okay. So now we got a line, right with two ends.

09:32.000 --> 09:37.120
And then pick a third point. And then we draw another line. Now we got almost a triangle.

09:37.120 --> 09:41.920
Then we just got to bring it on home, close the triangle, draw the last line. We didn't need any

09:41.920 --> 09:47.440
new points. We just need to connect to the opening. So that was a good probabilistic program.

09:47.440 --> 09:51.760
We had some randomness because we picked the corners at random. We also had some determinism

09:51.760 --> 09:56.880
because we drew the lines straight and just, you know, they're the same every time. If you tell

09:56.880 --> 10:01.920
the points, what points are at the ends, the line is the same every time. That's what probabilistic

10:01.920 --> 10:07.920
programs are good at, sort of combining randomness and determinism. And I could ask a question about

10:07.920 --> 10:13.520
this specific app. Sure. Yeah. And I'm totally biased by this thing that I've seen on the iPad.

10:13.520 --> 10:18.960
Okay. Why are you picking the random points if I'm drawing with my finger or triangle? Yeah.

10:18.960 --> 10:23.600
And I have seen the thing where it'll draw the isosceles triangle and what I drew is like nothing

10:23.600 --> 10:28.000
like that. Right. Right. But why not just figure out what the vertices were that I drew. The

10:28.000 --> 10:33.280
corners changes the direction and the lines. Yeah. You're asking the exact right question. Right.

10:33.280 --> 10:38.640
So we've got this cookbook recipe, but we want the real corners of the triangle you meant to draw.

10:38.640 --> 10:42.640
So here's what we're going to do. We're going to take this cookbook recipe and we're going to run it

10:42.640 --> 10:47.680
over and over and over again. And every time we run it, we're going to adjust those random three

10:47.680 --> 10:53.280
corners and move them a little. And then we're going to move them and then we're going to re-render

10:53.280 --> 10:57.920
the triangle and we're going to compare it to the triangle you drew. And we're going to score it.

10:57.920 --> 11:03.760
And if it got better, if it got closer to the triangle that you drew this random cookbook

11:04.720 --> 11:10.800
recipe output, then we're going to keep that new set of corners. And if the corners got,

11:10.800 --> 11:14.960
if the output got worse, like it doesn't, it's moved away from being like the triangle you drew,

11:15.600 --> 11:20.880
then a lot of the time where we're going to reject that and try again, try some new corners.

11:21.520 --> 11:26.720
Is this like some of the squares of the distances between the vertices or something like that?

11:26.720 --> 11:31.200
Yeah, any distance metric. This is the red light district, the CD underbelly

11:32.640 --> 11:37.040
that side at the railroad tracks of probabilistic programming is you always have to have a good

11:37.040 --> 11:41.920
way of scoring at the bottom there. Mean squared errors gives any. That's basically saying,

11:41.920 --> 11:46.240
hey, I think the triangle that the cookbook output, if I add some Gaussian noise to it,

11:46.240 --> 11:50.960
it would look like your triangle. That's really what mean squared distance means there in that context.

11:51.520 --> 11:55.520
So anyway, so yeah, so you could pick absolute value, L1 distance, you could, and that's

11:56.080 --> 12:00.880
it's another thing that's really saying there's pass on noise between your cookbook recipe and what

12:00.880 --> 12:06.720
I drew the real observed triangle. So anyway, so that's the south side of town. But once you get

12:06.720 --> 12:13.920
that figured out, then you can just keep adjusting those corners until it matches. And once it matches,

12:13.920 --> 12:20.560
as best as it's going to, then you could take, say, the most likely run you had of the cookbook

12:20.560 --> 12:25.040
recipe. And you say, what are the corners there? You're like, okay, I've got a set of corners.

12:25.040 --> 12:29.360
They've matched as, you know, the best ones we found. And you could draw a triangle with those

12:29.360 --> 12:34.240
corners. You take that triangle and you draw it right on the tablet. And the person goes, wow,

12:34.240 --> 12:39.920
that looks like the triangle I meant to draw. Now, what I just described to you is totally not all we do

12:41.520 --> 12:46.560
because it is insane idea to run a program over and over again and adjust the parameters

12:46.560 --> 12:51.040
or random and hope you hit the output. So that would be like, I don't know, throwing

12:51.760 --> 12:56.960
from our hotel room here at a top Manhattan, we would be throwing darts and trying to hit the

12:56.960 --> 13:01.600
statue of Liberty or something. So we don't actually do that. But I think that's a good way to think

13:01.600 --> 13:05.920
about it. And for simple programs, simple policy programs, you can actually do it that way.

13:06.560 --> 13:12.400
What it is, it's a good way to think about what's happening. And what this algorithm will do is

13:12.400 --> 13:17.760
it also those talked on the show recently in the context of industrial applications of AI.

13:17.760 --> 13:23.440
We've talked about simulation a lot. Simulation. This is a simulator, right? This is a simulator.

13:23.440 --> 13:29.840
So what did you just did? You just simulated where your data came from. This is a story or in fact,

13:29.840 --> 13:34.640
the way we usually teach us to new employees or new people that we're trying to teach it to is

13:34.640 --> 13:38.640
we say, oh, you have some data and you want to do machine learning on it. You want to do

13:38.640 --> 13:42.720
probabilistic programming on your data. Okay. Well, the first thing you should do is in English,

13:43.840 --> 13:50.800
get out your literary cap in your quill pen. And write down in English, the story of how you think

13:50.800 --> 13:55.680
your data came to be, right? Which is a simulation. It's a simulation of the system that made your

13:55.680 --> 14:02.240
data. Whether that's a biological thing, you know, this is heartbeat, data, or whether that's

14:02.240 --> 14:07.360
financial data or whatever. And the characters in your story, those are going to be the variables in

14:07.360 --> 14:12.480
your probabilistic program. And then the things they do, the actions they take are going to be some

14:12.480 --> 14:17.440
of the determinism like like the straight lines we drew. And if they have choices to make, those

14:17.440 --> 14:22.160
are going to be random coin flips or dice rolls in your probabilistic program. You know, if you

14:22.160 --> 14:26.800
knew exactly your system, if you could deterministically simulate it, it wouldn't be a probabilistic

14:26.800 --> 14:32.000
program, it would be a regular program. So the whole point of this is that there are certain things,

14:32.000 --> 14:35.440
you know, you have a pretty good idea of the story that created your data, but there's some stuff

14:35.440 --> 14:39.520
you're not sure about. Whatever that is, that's where you put the random variables in your

14:39.520 --> 14:44.400
probabilistic program. And then whatever those random variables are, the program's going to try

14:44.400 --> 14:49.760
as many possibilities through those random variables as a kin to try to match your data. And that's

14:49.760 --> 14:55.680
what gives you the fancy term, the posterior distribution. That's what tells you if I made a coin flip

14:56.320 --> 15:02.160
and, you know, if I flip the coin heads, I choose to draw a triangle. If I drop flip the coin tails,

15:02.160 --> 15:09.440
I choose to draw square. And I've got my data, you know, looks like a square. Then if I run this

15:09.440 --> 15:14.400
over and over again, that coin flip is going to tend toward the side where the, yeah, I'll put

15:14.400 --> 15:20.720
ahead and square. And so you'll, you'll figure out that's the kind of choice that this square-drawing

15:20.720 --> 15:26.640
system is doing is making a lot of the time. Probably probabilistic programming then is you just

15:26.640 --> 15:31.280
really summarize it for us. It's kind of a methodology for programming that allows us to introduce

15:31.280 --> 15:38.880
randomness, right? And, yeah, and for, and really for simulating systems, simulating systems that

15:38.880 --> 15:45.200
create data. And I think when people say probabilistic programming, they don't just mean the programming

15:45.200 --> 15:49.760
language with random variables, because like Python has random variables. Technically, Python's

15:49.760 --> 15:53.920
a probabilistic programming language, but that's not what we mean in the field. We also mean the

15:53.920 --> 15:59.360
solver, the thing that tried lots of different values of the random variables and tried to find

15:59.360 --> 16:05.360
good ones. And that was actually a segue to my next question, which is, you know, when I think about,

16:05.360 --> 16:11.600
you know, not knowing a whole lot about probabilistic programming applied, you know, and I think about

16:11.600 --> 16:17.600
how I might try to approach this. It's like, you know, Python either, you know, do, you know,

16:17.600 --> 16:22.640
get a loop and do a bunch of random stuff, you know, or get a loop and, you know,

16:22.640 --> 16:27.760
deterministically do like a brute force search of my space or something like that.

16:27.760 --> 16:33.760
Right. Is, you know, how are you doing this, you know, or what different techniques and

16:33.760 --> 16:38.560
the programming languages, platforms, frameworks, that kind of thing are you are being used for

16:38.560 --> 16:43.360
probabilistic programming? Yeah. Well, may I give you a little quick overview of, you know,

16:43.360 --> 16:48.880
three sentence overview of what people are doing in general on this? So I would say people are

16:48.880 --> 16:54.480
doing all kinds of different things. So some people, if they want to figure out, was it a square

16:54.480 --> 16:58.640
or triangle? And they write a program that flips a coin and if it's a heads, it draws it square,

16:58.640 --> 17:03.760
and if it's a tail draw triangle, there's a bunch of different ways you could try to evaluate the

17:03.760 --> 17:08.960
way to that coin. So you could run it over and over again, and that's called a sampling method.

17:09.600 --> 17:14.560
And you could do a Monte Carlo style thing there. And then within Monte Carlo methods, there's all

17:14.560 --> 17:20.000
the tons of those. There's metropolis Hastings and important sampling and Gibbs sampling and all

17:20.000 --> 17:27.440
those things. And then other people will use variational methods. And there's black box variational

17:27.440 --> 17:31.760
methods and there's mean field and there's, you know, all kinds of things there.

17:32.320 --> 17:37.440
And what are the principal differences between Monte Carlo, Sim, and variational methods?

17:38.160 --> 17:43.680
If I had to like summarize it well, just like in a sentence, I would say a sampling method picks

17:43.680 --> 17:50.880
a value over and over again, and a variational method fits a curve or a function. Okay. So like,

17:50.880 --> 17:56.160
it's basically think of a regular old Gaussian distribution, right? And how did it get made? Well,

17:56.160 --> 18:02.000
say there's some, your data actually has a real Gaussian distribution. And you want to fit that.

18:02.000 --> 18:06.640
So one thing you could do is you could pick a mu, pick a sigma, draw the curve,

18:07.760 --> 18:13.440
take the mean square to the actual data, didn't get it right. Okay, adjust a little,

18:13.440 --> 18:18.720
keep doing that, keep moving the mean, keep moving the sigma until it fits. That's a variational method.

18:18.720 --> 18:23.280
Okay. Because what you did was you changed some parameters of this curve and fit the curve model.

18:23.280 --> 18:27.760
Yeah. Fitting a curve is basically a variational method. And then the other way you could do it

18:27.760 --> 18:32.400
is Monte Carlo method, which is you could just sample a bunch of samples from a Gaussian with some

18:32.400 --> 18:39.520
mean invariance and then compare the histogram you got to the data and keep adjusting the parameters

18:39.520 --> 18:43.600
of where you're sitting. So one way to do it, you know, the science museum, they got those pegboards

18:44.160 --> 18:48.880
and the ping pong balls fall down and they make a Gaussian pile at the bottom, right? So you could,

18:48.880 --> 18:54.560
you know, you could change the spacings of the pegs in there until that would be a more like a

18:54.560 --> 19:00.160
sampling method. You know, change all the maybe there's biases of those pegs like they push the

19:00.160 --> 19:04.560
balls more to the right or the left, you get a skew. Okay, stuff like that. So, you know, so there's

19:04.560 --> 19:11.280
all different kinds of ways to sort of fit variables to data in a probabilistic program. And I would

19:11.280 --> 19:16.240
say the community is in this massive exploration of ways to do it. Another way is gradient-based

19:16.240 --> 19:19.920
methods, which is what's usually done in neural networks. You can do backpropagation through a

19:19.920 --> 19:24.560
probabilistic program using auto differentiation methods and there's a bunch of papers on that.

19:25.280 --> 19:32.400
What we do at Gamalon is a little different. We sort of look at each of these core methods

19:32.400 --> 19:39.040
as or even some of their sub components like taking a gradient as kind of like an atom

19:39.920 --> 19:45.760
or a subatomic particle or like an amino acid like a building block. And then what our system does

19:45.760 --> 19:52.160
is kind of evolves solvers on the fly to do the best job of solving the probabilistic program you

19:52.160 --> 19:57.840
throw at it. So, it's a pretty complex piece of engineering. Yeah, what does that mean? Does that

19:57.840 --> 20:06.480
mean that the system knows about some, you know, a handful of archetypal solvers and it kind of

20:06.480 --> 20:12.400
picks one and fits its parameters or is it it's kind of like more evolutionary than that? It's more

20:12.400 --> 20:18.960
evolutionary than that. It's kind of like doing some approximation of what maybe a human mathematician

20:18.960 --> 20:25.840
who want to design a solver for a model would do. What's the cookbook for that? There's no good

20:25.840 --> 20:29.680
cookbook, right? In fact, there's a theorem that says there's no good cookbook. It's called the

20:29.680 --> 20:35.360
no free lunch theorem, which basically says given a problem, there's no one solver that will solve

20:35.360 --> 20:40.720
all problems best. So, every problem, you know, they're definitely always going to be for any given

20:40.720 --> 20:45.360
problem. You know, you might there may be a general class of methods that work pretty well,

20:45.360 --> 20:50.000
but better than some other class of methods. But that class of methods would work that worked well

20:50.000 --> 20:54.720
on that problem could work terribly on some other problem. So, there's no guarantees, right?

20:54.720 --> 20:58.720
That a particular solver will be particularly good on a particular prompted program. So,

20:58.720 --> 21:04.000
you really just have to try stuff, which is what humans do. You know, if you look at like, you know,

21:04.000 --> 21:09.520
nips the conference or AI stats or any of these conferences, you kind of look through a lot of the

21:09.520 --> 21:15.600
papers historically, like somebody said, here's a model I invented. I want to solve, you know,

21:15.600 --> 21:20.720
this problem. I wanted to, I don't know, cluster documents by topic or something. And then,

21:20.720 --> 21:26.560
okay, this model was hard to solve. So, I had to spend a year of grad school or two years or five

21:26.560 --> 21:32.720
years figuring out a solver method that would work on it. Right. And sometimes for very complex

21:32.720 --> 21:36.080
models, people cobble together a couple of different solver methods for different parts of the

21:36.080 --> 21:40.080
model. And then, you know, at the end of grad school, they get it working and they publish their

21:40.080 --> 21:44.800
paper. Yeah. And, you know, if you're real good, you can do one of those a year. And so, most

21:44.800 --> 21:52.080
papers are are a choice of a model and a choice of a solver kind of made it. Uh-huh. And what we're

21:52.080 --> 21:57.520
trying to do is automate that, you know, grad, we're trying to, I guess, automate grad school. It's

21:57.520 --> 22:04.000
funny. It's funny that you describe it like that that I've spent a lot of time and have asked a

22:04.000 --> 22:09.920
bunch of folks on the podcast about, you know, trying to understand deep neural network architectures

22:09.920 --> 22:16.000
and how do folks get to Google Net or, you know, one of these kind of, how do you arrive at that?

22:16.000 --> 22:21.440
Yeah. And the best explanation, the best sarcastic explanation that I heard recently was

22:21.440 --> 22:27.520
gradient descent by graduate student. And there's an acronym for it totally. But here's a non-sarcastic

22:27.520 --> 22:33.600
because there's a theorem for that too, which is if a model is at its core, a simulation of where

22:33.600 --> 22:39.120
your data came from. Think about what that, what a synonym is for that. Another synonym for that

22:39.120 --> 22:45.600
is scientific theory. A scientific theory, a good one is a very clean, accurate simulation

22:45.600 --> 22:51.760
for your data came from. Whether it's a black hole or a mitochondria or something. And a good

22:51.760 --> 22:56.960
scientific theory will, you know, fit the data. It'll predict future events. It'll do all these

22:56.960 --> 23:02.720
things great because it's a great simulation. It really models the system well. Well, we know that

23:02.720 --> 23:09.200
the space of scientific theories is super exponential. There's a ton of them. There's way more than

23:09.200 --> 23:17.120
even NP complete. Right. So the search for scientific theories is by its definition, a computationally

23:17.120 --> 23:21.840
hard exercise. Like, you know, there's never going to be, well, we hope there's not going to be

23:21.840 --> 23:26.800
a polynomial time algorithm that just gives, well, there'd be some pros and cons. So if there was

23:26.800 --> 23:31.360
like a fast, you could run on your laptop and it could find a scientific theory to fit any data,

23:31.360 --> 23:37.200
if that algorithm existed. The good news is science, we'd know all science, right? Any data we

23:37.200 --> 23:41.760
got from anything we'd understand it immediately, science would be done. That'd be kind of cool in a

23:41.760 --> 23:48.400
way. Problem is if the aliens come, if the aliens come, they probably discovered this too. How

23:48.400 --> 23:52.560
they get here, they had like really good science. They've got a fascinating light speed travel.

23:52.560 --> 23:57.600
And they're really not interested in our culture, right? They don't want to hear about our science

23:57.600 --> 24:02.400
and our theorems and trade because we came up with different ones because it's hard. Like,

24:02.400 --> 24:05.760
they already have them all because they have this polynomial time algorithm. So the only reason

24:05.760 --> 24:10.000
they're here is for our water and our, you know, probably going to eat us or something.

24:11.040 --> 24:16.400
So that would be, so we most of us hope that there's no polynomial time algorithm for finding

24:16.400 --> 24:20.800
good models. You don't think there are benevolent aliens that just want to explore the universe and

24:20.800 --> 24:27.440
make friends? Well, if they have, if they have all theorems, why don't they stay home, right? Still play.

24:28.640 --> 24:31.120
Anything we would say would be boring to them. Yeah. Yeah.

24:33.360 --> 24:36.560
Because they could simulate, they could make a theory of everything about the Earth.

24:36.560 --> 24:40.480
They could just have us running in an aquarium and simulation. Yeah, exactly.

24:40.480 --> 24:44.640
Why come all the way here? My father. Yeah. Yeah. Interesting. I guess that's the other

24:44.640 --> 24:49.040
alternative is we would be, if they had this algorithm, then we would be that simulation.

24:49.040 --> 24:54.160
Right. Yeah. So I need to get you. If anyone has a connection with Elon Musk,

24:54.160 --> 24:58.800
so I can get him on to talk about this, you know, we are a simulation theorem, make it happen.

24:59.680 --> 25:06.320
So the triangle example, I think, is interesting and illustrative because it's of its simplicity,

25:06.320 --> 25:13.040
but it's not satisfying because it doesn't tell me where I would use this. And more specifically,

25:13.040 --> 25:19.840
I guess, what are the classes of problems for which this Gaussian approach is better than any of

25:19.840 --> 25:24.560
the other things that I would have otherwise tried to do? And by the way, it's much more than

25:24.560 --> 25:28.640
Gaussian. So we use all kinds of different random distributions and positive programs and

25:28.640 --> 25:33.920
also the determinism. So in fact, programming is more much more than Gaussian.

25:33.920 --> 25:37.680
Because in fact, we can express any probability distribution.

25:37.680 --> 25:41.040
Progressive programs are sort of turning complete, if you will, for describing probability

25:41.040 --> 25:48.240
distributions, they can make any probability distribution, any space. So here is where we should

25:48.240 --> 25:53.680
probably make a distinction between the model and the solver. So on the model side, a process

25:53.680 --> 26:00.560
program can express a model of any data. It's a great scientific tool. It's a great, you know,

26:00.560 --> 26:04.560
I mean, what if all scientific theories were written in a clear stochastic lambda calculus,

26:04.560 --> 26:09.360
so across all different branches of the sciences, we could all just have this clear

26:09.360 --> 26:16.480
language that we could trade our theories in. So someone said, you know, Bayesian modeling is,

26:17.120 --> 26:22.000
oh, you know, who was this Pedro Domingos said to me, you know, the Bayesian tribe think they're

26:22.000 --> 26:26.880
they're right, they're doing things the right way. He wrote the master algorithm, yeah.

26:26.880 --> 26:31.520
Yeah, because Bayesian is like clearly they're kind of the like correct way, you know,

26:31.520 --> 26:37.040
you need like a British accent. So like express, express a theory on anything, you know,

26:37.040 --> 26:41.120
a probability theory is the theory of uncertainty, like this is the right way to do. But of course,

26:41.680 --> 26:48.000
all but the simplest Bayesian models are computationally intractable to solve. And so, you know,

26:48.000 --> 26:53.200
I think people who are in other tribes, like neural networks would say, well, you know, we restricted

26:53.200 --> 26:59.040
ourselves to set of probabilistic programs that you can use gradient based methods on. And that's

26:59.040 --> 27:04.160
how we're getting all these great results. And so we would say, oh, great, gradient methods sound

27:04.160 --> 27:08.240
good to us. We're incorporating that into our probabilistic programming solver platform,

27:08.240 --> 27:13.440
along with all these other methods. And when they come in handy, we'll use them. And so,

27:14.080 --> 27:19.920
we do everything that TensorFlow does. In fact, our system, like David Blisystem in Columbia,

27:19.920 --> 27:26.000
Edward actually is built on top of TensorFlow. So there's probably a programming system on top of

27:26.000 --> 27:34.240
TensorFlow. Okay. Interesting. Yeah. So David BliB-L-E-I. Okay. David Bli. Brilliant. Faculty member at Columbia

27:34.240 --> 27:40.800
University and good colleague. Okay. So yeah, absolutely. And so, you know, there are some

27:40.800 --> 27:45.680
architectural limitations in using TensorFlow. But you get the benefit of, you know, this

27:45.680 --> 27:50.960
open source project that has a lot of good momentum and energy behind it. And so, you know,

27:50.960 --> 27:57.280
probably the takeaway is it's not an either or. But I like to start my design flow when I'm

27:57.280 --> 28:03.360
thinking about machine learning with a probabilistic program with an English story in English with

28:03.360 --> 28:09.840
my quill pen of where my data came from. Because I think it helps you do model discovery by

28:09.840 --> 28:14.800
graduate student creating descent to know what that generative story is of your data. That's a good

28:14.800 --> 28:22.480
starting point. So can you maybe walk us through that design thinking process for application that

28:22.480 --> 28:27.680
you've applied this to? Yeah. That's a good question. I hope to think of one. Yeah. I mean, so think

28:27.680 --> 28:33.840
about enterprise data dirt. You're trying to clean up enterprise data. So you can think about,

28:33.840 --> 28:39.360
well, what are all the ways that data gets corrupted? Oh, it gets abbreviated. So someone deleted

28:39.360 --> 28:45.040
some letters, gets permuted. There's a whole bunch of things you can you know, sat down for a half

28:45.040 --> 28:50.160
hour with your friend and brainstormed ways that data could get corrupted. You could write all

28:50.160 --> 28:55.360
those down and then you could make a story of data dirt. Okay. So that would be an example. There's

28:55.360 --> 29:02.400
a really beautiful example. My friend Julian at MIT who used, he didn't actually use a probabilistic

29:02.400 --> 29:07.680
programming system because they didn't exist at the time yet. But essentially did this for finding

29:07.680 --> 29:14.960
exoplanets. So exoplanet is a planet that is circling a star, you know, in a distant, not galaxy,

29:14.960 --> 29:22.720
but distant solar system. And he found clouds on a planet that was a thousand light years away.

29:24.160 --> 29:29.840
And he did that by writing a story. So he used Kepler space telescope to receive the data.

29:30.560 --> 29:35.920
And he wrote a story of where the data came from. You know, the light came from the star. I had to

29:35.920 --> 29:42.000
pass through the atmosphere of the solar of the planet. And then it had to disperse, you know,

29:42.000 --> 29:47.840
through the scatter through through molecules in its atmosphere. And then it had to then disperse

29:47.840 --> 29:53.840
through the traversal to Kepler. And then Kepler's optics, he had to model all that. And so

29:54.720 --> 30:01.440
he went from that to all the way back to what was the density of the atmosphere at different

30:01.440 --> 30:06.080
locations on the planet, just inverting that that model. And he had all kinds of different

30:06.080 --> 30:10.800
solver methods. So the orbital dynamics were continuous variables. He did gradient descent on

30:10.800 --> 30:18.080
those. But the, I think the distribution over the scattering, that stuff was deterministically

30:18.080 --> 30:23.440
done because they had put hot gases into a kiln and they knew their scattering properties

30:23.440 --> 30:27.840
at very high densities and temperatures. And so, you know, that was like a deterministic system

30:27.840 --> 30:32.160
with some parameters that needed to be Markov chain money, Carlos sampled. All these things kind

30:32.160 --> 30:37.120
of cobbled together. So doing that kind of science, I think astrophysics is a really great example.

30:37.120 --> 30:40.960
You know, that's the kind of thing you can do really beautifully with the probabilistic programming.

30:42.240 --> 30:47.920
And so in one of the cases that you see on the enterprise side, whether it's this dirty data,

30:47.920 --> 30:52.560
a problem or something else, like so you start out, you write out your problem in English,

30:52.560 --> 30:58.560
like what next, where how do you evolve that to a solution? We translated into Python. Okay.

30:58.560 --> 31:03.760
So just take your English and turn into Python simulation. And that, did you mention DSL earlier,

31:03.760 --> 31:09.760
you have a descriptive language for doing this or we just use Python. So straight up Python.

31:09.760 --> 31:13.920
Okay. You know, if you look at what we call particle is our probabilistic programming platform.

31:13.920 --> 31:17.840
Okay. An idea learning platform. Get an idea learning at the end here.

31:17.840 --> 31:22.560
The simulations are just straight up Python. And you can do anything you can, you can do

31:22.560 --> 31:28.560
in Python. If you have a science library or a business process thing that you wrote that

31:28.560 --> 31:31.840
simulates part of your business process or whatever, you can just, you know, patch that right in.

31:31.840 --> 31:36.000
Python's a great glue language. So it's a really great way of putting together simulation.

31:37.120 --> 31:41.680
And so that's the first step is you just build that Python model of your data. And it should

31:41.680 --> 31:47.680
spit out random data, kind of nonsense data, but it should look statistically like your real data.

31:49.440 --> 31:55.200
So you've got a problem. You model your problem in English. You create a Python

31:56.160 --> 32:02.800
version of your model. What we're calling a simulation here. And I think I guess what

32:02.800 --> 32:07.280
is interesting here is you're not, this Python isn't trying to solve your problem. It's just

32:07.280 --> 32:12.320
trying to model your problem. That's what's different than like traditional approaches. And then

32:12.320 --> 32:18.880
yes, kind of apply the solver to that exactly back. Yes. It works backwards to figure out what

32:18.880 --> 32:24.560
the actual model is to the actual parameters are right. Okay. Exactly. We call it posterior. Yeah.

32:24.560 --> 32:30.240
Okay. Yeah. Exactly. That's it. Yeah. Okay. Oh, very interesting. So it's a very rational design

32:30.240 --> 32:34.960
flow. Mm-hmm. You know exactly what you put into your model and you know exactly what you left

32:34.960 --> 32:41.040
free for the solver to noodle with. So on that last point, one of the big challenges

32:41.040 --> 32:48.480
in applying neural networks to, you know, problems that matter, if you will, is explainability.

32:48.480 --> 32:55.200
Yeah. Does the approach that we're talking about here, because you know your model,

32:55.200 --> 32:59.600
better it is at lead to better results from an explainability perspective, or there's still

32:59.600 --> 33:06.080
challenges there. I mean, it gives you a knob you can turn from perfect explainability to the

33:06.080 --> 33:10.880
same situation you're in with a neural network. Okay. So neural network is just a probabilistic

33:10.880 --> 33:14.880
program. You can write it. Right. A probabilistic program. The neural network written as a probabilistic

33:14.880 --> 33:22.480
program is flip these million coins. Uh-huh. Now multiply their heads and tails by some weight,

33:22.480 --> 33:28.400
which sampled from a Gaussian. Right. And then add them some number of times. Yeah. Do all that,

33:28.400 --> 33:33.040
you know, how that looks. And then add them up and do the sigmoids and then get to make that be

33:33.040 --> 33:37.760
the weight of another batch of coins. Okay. The weights of another batch of coins. And that's

33:37.760 --> 33:40.880
the next layer of the neural network. So there's your neural network as a probabilistic program.

33:40.880 --> 33:45.600
Okay. So you can do that. That's a pretty, you know, you can run that model. You can train it on

33:45.600 --> 33:50.480
data. You can use back prop in the probabilistic program as the solver. And now you're just doing

33:50.480 --> 33:55.040
tensorflow. You're doing neural network. Right. So the fact that it is a probabilistic programming

33:55.040 --> 33:59.760
language, it's just one that only has Bernoulli variables, Gaussian variables, and gradient descent

33:59.760 --> 34:04.960
solver. Okay. This is very limited probabilistic programming system. Okay. And it only can express

34:04.960 --> 34:09.600
models which tend to be not very explainable because that there's no deterministic code. You can't

34:09.600 --> 34:16.320
really put any ideas in there. But we can turn a knob all the way over to a super explainable model

34:16.320 --> 34:22.880
if you want to, like a very causal model, where if, you know, you flip a coin to decide whether

34:22.880 --> 34:27.040
the person was crossing the street and you flip a coin to decide whether the bus was coming at

34:27.040 --> 34:32.560
that same time. And then if they were crossing the street and the bus was coming at the same time,

34:32.560 --> 34:38.080
they get hit by the bus else they don't. And then you sample the statistics of people getting hit

34:38.080 --> 34:42.880
by buses and like someone gets hit by, gets hit by a bus. You go in the model and you say like,

34:42.880 --> 34:47.040
why do you think they got, you know, we're like good to get hit by a bus and say, oh, look,

34:47.040 --> 34:52.480
you know, it's very likely for them to cross the street when the bus was coming.

34:52.480 --> 34:58.240
Right, right. It's kind of a morbid example. But, but super explainable, super simple. You can

34:58.240 --> 35:03.280
make them. Did you happen to see the video on Twitter of the bus that hit the guy and the guy

35:03.280 --> 35:07.280
like bounce off the bus and got up. This happened today. And I think in the UK somewhere.

35:07.280 --> 35:12.000
I'm so glad that so that's what happens in this process. Every time someone does get hit by a bus,

35:12.000 --> 35:17.200
he just bounces off. Everybody's fine. That's great. And it's 100% probability of that happening.

35:17.200 --> 35:26.000
Just good. So yeah, so what you do is you get a knob. And the knob is what we call model capacity.

35:26.000 --> 35:31.280
Not so the neural network is of extreme. It's a universal function approximator. It's a super wide

35:31.280 --> 35:36.880
variance model. It can if with enough training data can be do me anybody be anybody it wants to be

35:36.880 --> 35:41.920
right. It can learn any to fit any data. And it's totally unexplainable and it takes a ton of data

35:41.920 --> 35:47.680
to or very unexplainable and it takes a ton of data to train it. And as you turn this knob toward

35:47.680 --> 35:53.360
narrower variance, you get models which are tighter, more deterministic, more understandable,

35:53.360 --> 35:59.040
more explainable. And if they're the right model, they also take a lot less data to fit and they

35:59.040 --> 36:03.920
also make much better predictions. If they're the wrong model of their biased, then you got a problem.

36:04.560 --> 36:09.120
And so that's the second thing. In addition to making this really fancy solver system for the

36:09.120 --> 36:15.280
second thing our system has is a bunch of it's basically the first IDE for probabilistic programming.

36:15.280 --> 36:20.240
Because so it lets our staff basically get a lot of the kinds of profiler and debugger

36:20.960 --> 36:25.200
feedback kind of the analogy to profiler and debugger feedback that you get from regular

36:25.200 --> 36:30.480
program. So it tells you, you know, is this line of code helping fit your data or hurting you when

36:30.480 --> 36:34.960
you try to fit your data? Is it helping your solver converge or is it hurting you with your

36:34.960 --> 36:40.880
solver conversions? Things like that. And that's super important. We need development tools for

36:40.880 --> 36:45.920
a machine learning next. We, you know, like take tensor flow, you know, it doesn't give you a ton

36:45.920 --> 36:51.760
of feedback when your model is the wrong model. You mentioned wanting to get to idea learning.

36:51.760 --> 36:59.440
Oh, idea learning. Yeah. So that's our kind of new aha at Gamlon. And I don't know if we

36:59.440 --> 37:06.640
have really time to get into it. But the simple idea is that instead of inputting the probabilistic

37:06.640 --> 37:12.080
programs by probabilistic programmers programming probabilistically, you just talk to the system.

37:12.960 --> 37:19.520
And tell it stuff. And it interprets what you're telling it as a probabilistic program. And so

37:19.520 --> 37:24.240
you can insert ideas into the middle of it. So be as if you could talk to tensor flow in

37:24.240 --> 37:30.800
English and have it adjust its weights. Not in terms of supervisor unsupervised data. But in

37:30.800 --> 37:37.120
terms of like, but almost be like saying, Hey, tensor flow, I want weight number 219 to be 1.3.

37:38.240 --> 37:44.000
You'd be adjusting the insides of it. Of course, that wouldn't make a lot of sense in the case

37:44.000 --> 37:48.320
of a neural network. But in the case of the kinds of models we build, you can actually have a

37:48.320 --> 37:53.440
pretty nice little conversation with your system. So the analogy here is, you know, training a dog.

37:53.440 --> 37:59.200
So Pavlovian conditioning, right? If you want a normal supervised neural network, whatever machine

37:59.200 --> 38:05.760
learning, you ring the bell. You show the food, the meat, delicious meat, the dog salivates.

38:05.760 --> 38:10.000
You do that over and over again. Now the dog salivates every time they hear the bell.

38:10.000 --> 38:14.560
Right? So what did you do? You basically give it, it's just like when we teach tensor flow,

38:14.560 --> 38:19.600
how to recognize a cat. You show a picture of a cat. You say, this is a cat. If it gets it right,

38:19.600 --> 38:24.560
then you reward it with that prop. If it's a wrong, you punish it. It's the same thing. Stimulus

38:24.560 --> 38:30.320
and a response. And then you course it to give you the right response for the stimulus, desired

38:30.320 --> 38:34.960
response for the stimulus. Do that over and over again. With a dog, it's like 12 repetitions.

38:34.960 --> 38:40.800
With a tensor flow, it tends to be, you know, 30,000 cat images or labeled data points for each

38:40.800 --> 38:46.480
category. And that's how we ordinarily, but when we were the human, you know, with my kids,

38:46.480 --> 38:51.200
you know, when I went on the comfort dinner, you know, and they was just a summer now. So we

38:51.200 --> 38:56.560
got it, they're out playing. So we got the dinner bell. And you just say, look, this is the dinner

38:56.560 --> 39:01.760
bell. And when I ring it, that means it's dinner's ready and you should come. And, you know, normal

39:01.760 --> 39:06.800
people would hear that once. And they would just come to the dinner. They don't need repetitions.

39:06.800 --> 39:12.080
They're not heavy in learning stimulus responses. We're not conditioning them. I mean, if you're my

39:12.080 --> 39:16.000
kids, you have to repeat yourself like three or four times. But, you know, I mean, what did you,

39:16.000 --> 39:20.480
you did something different? You didn't put an input output stimulus response and train now. What

39:20.480 --> 39:24.880
you did was you stuck an idea in their head in between the input and output right in there,

39:24.880 --> 39:30.720
like conception, by talking to them. So that's what we need for machine learning because there's no

39:30.720 --> 39:35.760
way we're going to just like, how do you build modern civilization if humans could only train

39:35.760 --> 39:40.000
each other through stimulus and response? It's nuts. That's crazy. This isn't going to work.

39:40.000 --> 39:46.000
Right. Interesting. Interesting. Great. Well, what's the, for folks that want to learn more about

39:46.000 --> 39:51.280
this, you've mentioned the Gamalon website is one place. There are other. If you want to play with

39:51.280 --> 39:57.760
models, you don't even have to install any software. There's probmods.org, P-R-O-B, M-O-D-S. I guess

39:57.760 --> 40:03.280
prob stands for probabilistic. I don't know what mod stands for. Models. Dot org. There's a lot of

40:03.280 --> 40:08.880
good examples. That's Josh Tenenbaum, no goodman. Fikashman, Shinga, some others collaborate on that.

40:08.880 --> 40:12.560
It's fun to play with because it's all in JavaScript. I think it's, it's all in the browser.

40:13.120 --> 40:16.960
You just literally go this web page and you can live play with probabilistic programs and edit them

40:16.960 --> 40:24.800
and run them. It's kind of nice start. Any papers that are seminal and this area?

40:24.800 --> 40:31.920
Yeah, there's the, there's a lot of seminal papers and I would say go play. The papers are really

40:31.920 --> 40:40.000
pretty tough to read. They tend to combine programming languages, semantics with compiler theory

40:40.000 --> 40:46.000
with heavy vision math and everything in between. It's going to be an expert in like three different

40:46.000 --> 40:51.760
terrible vocabularies, mathematical notation. So yeah, I think playing is maybe the, could be

40:51.760 --> 40:55.920
more fun. Awesome. Awesome. Well, it's so great to have you on the show. I really appreciate you

40:55.920 --> 41:05.680
taking the time out. Yeah, thanks, Robbie. All right, everyone. That is our show. Thanks so

41:05.680 --> 41:11.600
much for listening and for your continued support, comments and feedback. A special thanks goes

41:11.600 --> 41:17.040
out to our series sponsor, Intel Nirvana. If you didn't catch the first show in this series where

41:17.040 --> 41:22.480
I talked to Naveen Rao, the head of Intel's AI product group about how they plan to leverage

41:22.480 --> 41:27.920
their leading position and proven history and silicon innovation to transform the world of AI,

41:27.920 --> 41:33.440
you're going to want to check that out next. For more information about Intel Nirvana's AI

41:33.440 --> 41:40.960
platform, visit intelnervana.com. Remember that with this series, we've kicked off our giveaway

41:40.960 --> 41:47.520
for tickets to the AI conference. To enter, just let us know what you think about any of the podcasts

41:47.520 --> 41:53.200
in the series or post your favorite quote from any of them on the show notes page on Twitter

41:53.200 --> 41:59.680
or via any of our social media channels. Make sure to mention at Twomo AI, at Intel AI,

41:59.680 --> 42:08.000
and at the AI come so that we know you want to enter the contest. Full details can be found on

42:08.000 --> 42:13.680
the series page. And of course, all entrants get one of our slick Twomo laptop stickers.

42:13.680 --> 42:19.680
Speaking of the series page, you can find links to all of the individual show notes pages

42:19.680 --> 42:49.520
by visiting TwomoAI.com slash O'Reilly AINY. Thanks so much for listening and catch you next time.

