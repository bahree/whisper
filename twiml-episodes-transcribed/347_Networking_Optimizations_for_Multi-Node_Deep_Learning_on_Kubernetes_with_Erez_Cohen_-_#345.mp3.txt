Welcome to the Twomo AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
It's been nearly three years since we launched the Twomo Online Meetup, initially as a way
for listeners to connect with one another and study academic research papers together.
The group really took off in mid-2018 though when I mentioned offhandedly in my interview
with fast.ai's Rachel Thomas that I'd be taking their course and I invited the Twomo
listener community to join me.
That led to the very first Twomo study group and the folks who came together for that group
have become the backbone of a community that's now over 3,000 members strong.
Fast forward 18 months and we've supported each other on our personal learning journeys
and a wide variety of courses from stanfordfast.ai, deeplearning.ai, on Kaggle projects and much
more.
I say all that to say that I am humbled by the role that we've been able to play in helping
folks learn machine learning and I am excited about the new educational programs that we're
bringing to the Twomo community this year.
We recently launched a collaboration to bring you the causal modeling and machine learning
course and study group which I've mentioned here before and today I'd like to share some
new details on the AI enterprise workflow study group that we're launching in just a
few weeks.
If you've been listening for a while you know that I am very excited about the work going
on in ML and AI communities to make developing and deploying machine learning and deep learning
models in the enterprise more accessible and efficient.
In fact we hosted an entire conference TwomoCon AI platforms on this topic just last fall.
Until now there have been very few formal resources that folks could turn to to learn
real world machine learning workflows and deployment strategies.
Our folks at IBM are working to change this though with the AI enterprise workflow specialization
that they recently made available on Coursera.
And I am super excited to share that we've partnered with them to host a study group for
this six course specialization program which I will personally be hosting.
The courses in the sequence teach real world machine learning and an enterprise environment
including applying the scientific process to understanding business use cases and structuring
data collection visualizing and analyzing data and hypothesis testing feature engineering
and identifying and addressing data biases selecting the best models for machine learning
vision and NLP use cases and using ensembles deploying models using microservices and containers
Kubernetes and Apache spark and applying unit testing to ML models and monitoring model
performance in production.
I am really looking forward to working through these courses and I would love for anyone
interested in these topics to join me.
If this sounds interesting and you'd like to learn more, I invite you to join a webinar
that I'm hosting with Ray Lopez, the courses instructor on Saturday, February 15th at 9.30
a.m. Pacific time to register visit twimmelai.com slash AI workflow and now on to the show.
Everyone, I am here in San Diego at TubeCon and I am with Eris Cohen, Eris's Vice President
for Cloud and AI at Melanox.
Eris, welcome to the Twimmelai.com podcast.
Thank you very much.
I'm great to be here.
Awesome to be chatting with you.
Tell us a little bit about your background.
So I am leading the Cloud and AI at Melanox Technologies.
I'm actually with the company for some time now over 19 years.
So I've been with the company from the very, very early days.
I am coming from an engineering background, I'm a computer engineer based out of Israel.
And over the past five years or so, I'm focusing mostly on cloud and containerized infrastructure
for high speed networking and obviously machine learning is a big part of it.
Yeah, now I've been familiar with Melanox for quite a while primarily in the context
of high performance computing and infinite band and stuff like that, but not too long ago
the company was acquired by NVIDIA who obviously has a big stake in deep learning and AI.
What's the idea there?
Yeah.
So Melanox was, you know, when we started, it was a networking company and at the time
we focused on Infinibin, which was the new generation of networking at the time that
was supposed to be.
Did I date myself?
I'm sorry?
Did I date myself?
Yes, definitely.
Well, actually, to be honest, Infinibin is still very much a very live technology.
We just this week we have super computing conference, which is the large, you know, gathering
all the high performance components and then very exactly.
And if you look at the top 500 strongest computers in the world today, a lot of them still
run Infinibin, which is the most efficient, still interconnected, highest performance,
most of cost effective.
So it's 20 years old technology, but it's still the cutting edge, definitely running
in on rates of 200 gigabit per second, which is phenomenal if you think about it.
But Melanox wasn't designed to be a high performance computing company.
It was designed to be a networking company that provides the best interconnect for data
centers in the world.
And it doesn't matter if it is high performance computing, when you're running 10,000 servers
doing some crash simulation of the universe, or you're running your telecom infrastructure,
or you're running your machine learning and big data.
So if you look at the essence of networking in the end of the day, it is similar to many
of those applications.
You need a very low latency high bandwidth, very efficient and offloaded infrastructure.
This is exactly what Melanox does, we do it in a way that it can fit a lot of different
verticals.
And machine learning and big data, and I will bundle them in a way together, even though
they're not necessarily bundled together.
But if you look at data analytics, machine learning is part of the data pipeline, and there's
typically also big data because we usually work on large data sets.
If you look at big data and machine learning, they have many high performance networking
requirements that I will talk probably about in a few minutes.
And this is where a lot of the focus of Melanox is these days.
We're looking at acceleration, accelerating networking and providing more efficient networking.
And if you look at what Nvidia does, they do something very similar in a different domain
they're accelerating compute.
And so it was very natural for a company like Nvidia who is accelerating compute and providing
a much more efficient compute with GPUs to look at a company like Melanox which accelerate
networking.
And together we're forming the two or two and a half pillars of networks or of clouds
basically which is compute, networking, and also storage.
The reason I'm saying two and a half pillars is because there's a lot of networking in storage.
So we're touching, Melanox is touching mostly networking and storage in a sense, the networking
part of the storage.
And we're moving a lot of compute into the networking, we can talk about that.
And Nvidia is focusing a lot about the compute, the engines themselves.
Okay.
So if you've got a talk here at the conference tomorrow on networking optimizations for
multi-node deep learning on Kubernetes, when I think about deep learning and in particular
the training phase of deep learning, I think of that as a primarily a compute bound process,
compute bound exercise, you know, why do we care about networking in that?
Exactly.
This is exactly the point.
It is a compute bound process.
And one of the ways to solve compute bound processes is to add more compute.
What we call scale out.
Now when you add more compute to distribute the workloads so that it can run faster, you're
hitting a networking situation.
If you look at models today, models grew enormously over the past several years.
It's not in several percentages, you know, 100 or 200X, the size six years ago.
And the computation time and complexity is enormous.
We see some models training takes weeks on the state-of-the-art GPUs or multiple GPUs.
And the only way to shorten this time, and it has to be shortened because we need those
learning phases to happen much frequently, then, you know, once every three weeks, is
to scale out.
So basically we add multiple servers, each of them with multiple GPUs, and the expectation
or the plan is that the more servers you add, you reduce the time in a linear manner.
That's kind of our goal, right?
So if it takes me one hour with a single server, adding two servers, I would love to have
it in half an hour.
And if I have 60 servers, it should take a minute, and you can do the math from here, right?
Unfortunately, it's not that easy just adding servers and cutting the time.
What happens is that you have now a distributed problem, moving a problem that runs on a single
GPU to run on multiple GPUs and multiple nodes requires a lot of synchronization, requires
a lot of data that needs to go between the servers in a very efficient way.
And this is exactly where the networking is really making a big difference.
Yeah.
Our listeners probably have heard of things like distributed TensorFlow and Horavod and
other technologies that are more focused on kind of the software aspect of that and the
coordination, the state, the shared state that enables distributed training.
But you're saying that's not that part of it, but not all of it, right?
That's part of it.
Yeah.
So definitely.
I mean, if you look at distributed TensorFlow and things like Horavod, which all they do
is they try to improve the way the workers communicate with each other, you know, the
very kind of basic, most simple approach was to have a kind of a node that is doing all
the coordination.
So, you know, you have X amount of workers and they do training of mini batch, a small set
of the data and then they send the information, the gradient factor to the server that usually
we call it a parameter server.
He would gather all the gradients from all the nodes, we'll do some crashing of the data
and distribute new new weights for the different computes and they would do that over and over
again in iteration.
That is a very naive implementation, obviously, when you look at large scale models, every
node will send tens or more of gigabit per second to a single node that needs later on to,
you know, crash it locally and distribute that doesn't scale, that doesn't work, right?
So Horavod and other solutions try to look at different ways of doing this, what we call
collective operation, collective operation in a sense that you have a number of nodes that
are working in a collective way on a single problem and they need to synchronize each
other, right?
So sending to a single server and get it back is one very simple way and then there are
other ways like rings that each send to these neighbors and so on.
Those are all ways to drive the synchronization in a much more efficient way, but in the end
of the day you actually send and receive data and you send and receive data in large quantities
that needs to be very low latency, you don't want to burden your CPUs with all this data
sending and receiving and a lot of the data that you send is not necessarily originating
in the GPU, in the host memory, it is in the GPU memory, the question is how do you send
data from GPU through the network?
Again, the simple implementation is copy that data to the host memory and then copy it
again to the network buffers and then send it out.
That's a very, very inefficient way to do that.
So it's an efficient but again for a CPU bound, compute bound problem rather, we're not
as worried about latency, why do we?
So think about how does it work?
You do the training on a mini batch, everybody, you have X amount of servers, they all do
this training and then they do the communication phase.
Now if the communication phase is long then and you do those cycles, hundreds of thousands
or millions of times, this adds up significantly adds up.
I can tell you that when we use technologies like RDMA, which is an invest transport service
or GPU direct, which is a way to send and receive data from the GPU, we can improve the
total system efficiency by multiple axis and those systems are very expensive.
If you look at a 10 node GPU system with 8 GPUs each, that's a lot of money.
If you can do two X performance, you just save several million dollars or watch yourself
another system.
Exactly.
And this is only networking.
And this is what people don't realize.
Actually people do realize, if you look at all the big guys, all the hyperscalers, they
all run this technology already for many years.
They're running RDMA and they're running GPU direct and they are now looking at additional
technologies that we bring to move some of the computation into the network, actually,
called sharp.
We just announced it in super computing.
It would be part of nickel.
Nickel is the Nvidia communication library, so it will be available together with nickel.
So we kind of talked about the setup in a while, this is important.
And then we've been rattling off a bunch of different technologies that play a role
in here.
I imagine part of your presentation was kind of walking through that stack or the evolution
of technologies.
It's a good time to drill into what is GPU direct and RDMA and nickel and sharp and all
these things.
Yeah.
So the talk we'll be talking about the challenges that we just discussed.
And very briefly talk about RDMA and GPU direct, there's two key technologies that enable
better networking.
And then it will go and talk about the actual implementation in Kubernetes.
So we are in Cubcon after all.
And one of the things we want to make sure is we can enable advanced platforms like a
Kubernetes class to take advantage of these technologies.
It doesn't come up free.
You know, if you look at Kubernetes, networking in Kubernetes is very naive in a sense.
Still.
And evolving obviously it's a new platform or framework.
And it is evolving very fast now, you know, you could see at the amount of people here
in the conference.
It's amazing to see how much energy and how much enthusiasm there is here.
And we, you know, networking will evolve.
What we are trying to do, melanox together with the partners in the ecosystem and definitely
Nvidia is a big part, by the way, they're part of the talk, they're actually a joint
talk between Nvidia and melanox, is we are trying to make Kubernetes networking to be
able to consume advanced network solutions in a very natural, upstream, standard, transparent
way.
This is really, and the talk is talking about mostly that, obviously we need to go through
the concept of what are we trying to solve and what are the technologies, but we will
try mostly to focus about how do we integrate it into Kubernetes, and then talk a little
bit about that will be the Nvidia side, they're running this at scale in large clusters,
and they will talk about what do you need to do in your data center in order to properly
run this and what's the best practices.
Okay.
So that will be the talk most.
Okay.
So you were kind of running through the technologies and the acronyms that were thrown around
and then we'll dive into the Kubernetes therapies.
So I just throw three different technologies, RDMA and GPU directives that are the ones
that we will talk on the talk tomorrow, and I'll just mention that it will not be part
of the talk, but I'll be happy to explain.
RDMA is remote direct memory access, and this is a technology,
which it's a transport service, if you like, it's the same way or it is.
Yeah, it came from the infiniband days, it is a transport service, very much like TCP
or UDP, I think most of the listeners at least heard about TCP and UDP, but unlike those
TCP and UDP, it was designed in 2000 rather than the 70s of the last decade, and it is
much more advanced.
It has few very important capabilities, first is the notion of read and write versus
only send and receive in TCP or UDP or just send a packet, and somebody on the other side
needs to get it and decide what to do with that, it is a send, receive mechanism.
RDMA allows you not only to do send and receive, but actually write to a specific memory
address on the remote host or read from there, just like a local DMA.
And that's open up a lot of very interesting use cases for writing applications, truly
change how you write your application.
This is one thing, the other thing is something called kernel bypass, in TCP and UDP, an
application that wants to send and receive data will do a socket call, and then it will
go to the Linux kernel or Windows kernel or whatever kernel, and the kernel will do a bunch
of work preparing the packet and send it out, so there's a lot of dependencies on the kernel.
In RDMA, you just from the user space can send and receive data directly, and there's
no basically kernel involvement at all, so it allows you first low-verlo latency.
We're looking at micro-second latency sending and receiving packet versus at least older
of magnitude higher when you look at TCP, minimum.
And then the fact that you don't go to the kernel and the NIC itself, the network adapter,
implements all the transport and how there allows you to move enormous amount of data without
any CPU utilization, which means that if anybody been working on high performance networking,
you know that you need to tweak the kernel and the CPUs like crazy to get the juice out
of the network, but in Finland it's not with RDMA, it's not the case, you just tell it
to send and it will send 100 gigabit without CPU, 200 gig doesn't really matter.
RDMA is available on Finland initially, but now it's a part of Ethernet as well, it's
a standard, it's not a proprietary intercom.
And is available on any kind of enterprise Ethernet NIC?
Yeah, so in the Melanox is definitely one of the leaders in this space we've been doing
that for 20 years.
These days all the network adapters have Ethernet that would have RDMA, I would argue
that Melanox is a far more feature rich and stable in this space.
Nothing less.
Yeah, but you know there's the benefits of experience I guess.
And so my guess then is the GPU direct is RDMA for GPU memory?
Exactly, so exactly right.
So RDMA allows you to write and read directly to memory so the NIC can access the host memory,
there's a lot of relationship in RDMA between the NIC and the host memory and there's a lot
of mechanism that allows it to be very efficient, including like an IOMMU implemented on the
NIC and IOMMU.
IOMMU?
Yeah, I'm sorry for all the heckering, but this is basically a mechanism that makes sure that
you write the data to the right address in the memory and that you allow to do that.
So you don't expect input, output memory, something unit or something.
Yeah, this is typically an Intel component of the server, but the point is that if you think
about it when a remote host writes to your memory of local host, this memory may be in different
physical, there's virtual addresses and physical addresses so you need somebody to make sure
that first you're allowed and you're not bridging in their security breaches and then that
you're writing to the right place because the physical memory kept on shifting and changing.
So there is a mechanism to make sure that the memory is intact properly.
GPU direct is a co-development that NVIDIA and Melanox did together, actually it started
a while back, it started from the high performance computing days when NVIDIA started getting
into this space.
I think it was over 10 years ago, I don't remember exactly, but at least over 10 years ago
and basically what it allows is it allows the NIC and the GPU to be able to communicate
over PCI directly without going through the host memory and so there's no CPU intervention
and there's no copies, there's no other challenges, blockers and so on that we had before.
And that really opens up a huge button like that GPU networking had, all those copies
took a lot of time and they killed performance completely and they synced up a lot of CPU cycles
for that.
So that is GPU direct, it is almost transparent or some because the NIC and the GPU need
to talk there, there's some system requirements that needs to be met on how the PCI layout
is done and where's the NIC and where's the GPU.
It is especially important with the large GPU boxes and typically in large GPU boxes you
will see that there's a GPU in a box like the DGX one, for example and there's a very
interesting PCI structure, it is not a single PCI switch that connects all the GPUs and
the NICs there, there's a topology that and then you need to be very careful on which
NIC talks to with GPU, to enable GPU direct, but other than that it is quite transparent
for the user if they use the right drivers and applications.
The last technology that I mentioned and this is not going to be on the talk is sharp, sharp
here.
I explained earlier about the need to get all the vectors from when you do the, the
needs to be the training, you get all the vector to a single location that does some crunching
and redistribute the new weight vector.
What if you could do that instead of a server or in a ring, do that on the network itself.
So this is what Malanox does, basically we are in our infinivance switches and this
is specifically for infinivance right now.
We added a capability to run compute inside the network.
So doing the, what we call reduction, the vector reduction, we do that actually on the switch
systems themselves.
And we can do that in line rate through a lot of many ports, you know, those switches
has 30, 30 ports or 40 ports depends on the generation.
Switch is not the adapters, the adapter sends the data to the switch, each server sends
the vector of the gradients to the switch and the switch think, let's say that it has,
let's say 30 ports, it gets 30 such streams, the switch will do the reduction, we'll get
all the vectors, we'll do some crunching and we will redistribute that to the servers
to continue the next phase of the training.
And that allows you to open up a lot of bottlenecks.
And is the, the crunching that the switch is doing, is it like how configurable is that,
is it, you know, there's the way that it does it today or the, kind of the way that, you
know, melanox says that it should do it or is it a software defined thing where I can
have a strategy and push it to the switch and it's going to do what I want, is it, to
what, how generalized is the compute available on the switch?
So I think it's a good question.
One of the things that I haven't mentioned is our strategy around software, melanox is
very open source, centric company, on, you know, we do, we believe the best hardware.
And we, the software piece is all upstream first, we, everything we do is upstream, not
private or no, no black box software, we try to open up everything and have standard
APIs as part of this for shop, for example, we have standard APIs that is developed as part
of the RDMA interface.
Okay.
And then from a software perspective, you know, it's just a standard API.
The hardware implementation, obviously there is some flexibility, but that's, I think,
less of the point, the point is that any application that uses a standard open source API,
like at an advantage of any hardware that implements this API capabilities, specifically
for our implementation, it is flexible, but the flexibility is not exposed outside,
it's exposed through those standard APIs.
Okay.
Okay.
So that the idea then would be that somewhere between melanox and NVIDIA, you would identify
what the strategies are of interest, whether it's Horavod or distributed TensorFlow or
something else, and make the module available for the switch.
Yeah.
So the way you consume this is through applications that do distributed training.
Now, obviously the market is quite active with a lot of different projects and so on.
We try to focus on the most dominant frameworks, TensorFlow is definitely one of the frameworks
we see for performance-oriented environments, which is mostly used, and we work mostly with
TensorFlow, not only not limited, we also work with PyTorch and Cafe and others, but definitely
TensorFlow is where we invest more of our time, we're part of the community, the open source
community of TensorFlow.
If you look at the distributed implementation, today the most common configuration, it's
like a Lego, you can put it in multiple ways, but if I try to look at what is common in
the industry today, people will typically use TensorFlow together with Horavod and Nikol,
Nikol is the distributed library of NVIDIA.
We usually look at how do you do the same challenge of distributed workload, but within a server,
right, in a server you have multiple GPUs, you also need to distribute the workload between
those GPUs.
That's kind of a very similar way, and then when they scale out multiple servers, they
use RDMA and GPU direct and all those technologies.
Okay.
We layer those together, Nikol, TensorFlow and then Nikol and then Horavod, and the library
that enables sharp NRDMA and GPU direct is Nikol, okay, and that is something that will
be available very soon.
It's not limited to Nikol, I mean, anybody can use that, it's an open source, anybody
if there's somebody here in the audience that listen and want to enable that for Python,
should we, we definitely can do that, there's no problem.
And so then real quick, the Kubernetes piece of this, like we're talking about hardware
and network adapters, you know, I must, it's part of me that says, you know, this is
of, is in the box, the kind of host operating system, you know, has drivers for all the
stuff, like why do we care about this at the Kubernetes level?
What are the challenges there?
Excellent question.
You know, somebody once said that advanced or very smart silicon is nothing but very
expensive send if you don't have the right software, right?
So everything I said here, if you remember, is talking about how do I connect the application
directly to the hardware almost, right, all the bypasses and all the acceleration.
You're looking at layers that provide direct one activity as much as possible from the
application all the way to the hardware.
And when you go to virtualization or containerization, you actually go the opposite direction, right?
You go and you add software layers, like service, and all the other things you hear as
the ends and so on, that all they want to do is virtualize the network and provide a
layer that completely disconnect the application from physical resources, like networked up.
So you're trying to couple the two to gain optimization, but these things are trying
to decouple the two.
So I think they're not exactly working against each other.
Each of them is looking for a different direction.
And what we try to do is marry all the direction to something which is cohesive and high performance.
So what all those SDN software-defined networks and service meshes and virtualization technologies
are trying to do is they're trying to provide you a layer of standardization so you don't
have to write your application multiple times for different hardware and automations and
all the other goodies, like provision network through code, software-defined, and so on.
This is very important, you do want to have a controller that you set security elements,
you set behavioral, you create networks and ports and so everything in software.
But at the same time, if you add multiple layers of software, you have the flexibility,
you have the features, you don't have the efficiency, you don't have the performance,
which you know, you're kind of shooting yourself in the leg.
And what we are trying to do is we're trying to provide a high performance together with
all the flexibility.
So we're trying to marry software-defined service meshes and all these technologies and still
allow in a transparent manner and without any compromises to run in full, bare metal kind
of performance.
You know, getting the same performance as you would get with a bare metal server in a very
highly virtualized, containerized environment.
This is the challenge that we're trying to address.
And so is there a solution?
Is there a project or something that people can go take a look at?
What is that?
Absolutely.
So it's called Kubernetes.
We're doing that as part of Kubernetes, right?
Okay.
So it's going to get vacant.
In Kubernetes, we are developing a CNI, CNI is a network plugin.
Kubernetes has this concept of CNI's.
And we are today using a CNI for technology called Desarai UV, which is a PCI technology.
I don't think we have a lot of time to go through that.
But that is the direction that we provide the direct connectivity.
It's still not fully complete.
There's more and more things that will go in like SDN offloads and other things that today
are available for, say, solution like VMware or OpenStack and other places.
Kubernetes is still young.
And but we are working.
We're working on this with a lot of partners in the open source community where basically
we want to provide the full flexible secure network with the highest performance possible.
Awesome.
Well, Edas, thanks so much for taking the time to walk through all this stuff.
Really good.
Sure.
My pleasure.
Thank you very much.
Thank you.
All right, everyone.
That's our show for today for more information about today's guests or any of the topics mentioned
in the interview, visit twomelai.com slash shows.
To learn more about the IBM AI Enterprise workflow study group, I'll be leading, visit
twomelai.com slash AI workflow.
Of course, if you like what you hear on the podcast, please subscribe, rate, and review
the show on your favorite pod catcher.
Thanks so much for listening and catch you next time.
