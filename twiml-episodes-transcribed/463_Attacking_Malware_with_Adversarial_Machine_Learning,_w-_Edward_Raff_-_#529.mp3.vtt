WEBVTT

00:00.000 --> 00:16.000
All right, everyone. I am here with Edward Wrath. Edward is a Chief Scientist at Boos

00:16.000 --> 00:22.160
Alan Hamilton. Edward, welcome to the Twimal AI podcast. Thank you so much for having me.

00:22.160 --> 00:28.320
Really excited to be here. Looking forward to jumping into our conversation. You lead a

00:28.320 --> 00:35.760
machine learning research group at BAH. What is a machine learning research group at BAH? Focus on.

00:36.640 --> 00:42.080
Yeah, so it's a little different for us compared to many other organizations that have research

00:42.080 --> 00:48.640
teams because we are consulting firms. So our business model is basically based on renting out

00:48.640 --> 00:55.680
people's brains. And you have some hard problem you want to solve. So we need to have smart people

00:55.680 --> 01:02.800
who can work on these hard problems. So we sort of view research both as a way to sort of let

01:02.800 --> 01:07.280
people know like, hey, we actually work on some really cool things that require this level of

01:07.280 --> 01:13.920
difficulty and thought and challenge. But also as a way to train staff like one of the things that

01:15.280 --> 01:22.000
we've been doing a lot of work around is adversarial machine learning where there's you have your

01:22.000 --> 01:27.120
model that you've developed and you put blood sweat and tears into it and it's your baby and

01:27.120 --> 01:34.240
you want it to work well and go into production. And there may be some nefarious actor out there

01:34.240 --> 01:42.880
who wants to subvert your model. So like a lot of my work is in malware analysis, malware detection

01:42.880 --> 01:47.360
where the malware author like actively wants to subvert the model. They don't want to be detected

01:47.360 --> 01:53.520
as malware. They want to be able to run uninhibited. So if there is someone who's going to try and

01:53.520 --> 01:59.600
sort of mess with your model and try to make it produce errors, how do you prevent that or how do

01:59.600 --> 02:07.280
you even accurately quantify what's happening. And there's no course that you can sign up for yet

02:07.280 --> 02:13.680
on adversarial machine learning and school that's not part of anyone's curriculum yet. So if we want

02:13.680 --> 02:18.640
people of that skill set, we sort of have to grow them organically and research is one of the ways

02:18.640 --> 02:24.320
to do that and to blow people up of these really deep technical skills that we need but are not

02:24.320 --> 02:31.280
sort of off the shelf yet. Yeah, yeah. I'm glad you brought up adversarial ML because I know

02:31.280 --> 02:36.880
that's an area of personal interest for you and that's one of the things that I wanted to dig

02:36.880 --> 02:44.240
into with you as a way to get that conversation going. It's actually been quite a while since I've

02:44.240 --> 02:52.400
had someone on the show focused on this intersection of AI and cyber security which has continued

02:52.400 --> 02:59.360
to be of interest particularly as we see more and more activity let's say in the cyber security

02:59.360 --> 03:10.320
realm. Can you give us an overview or what's your kind of temperature read your take on the space

03:10.320 --> 03:16.400
and where it is and how it's evolved over the past couple of years. Yeah, I mean it's interesting,

03:16.400 --> 03:22.880
it's if you go back to like the first like applications of machine learning for malware detection

03:22.880 --> 03:33.680
goes back to like 1985 people have been looking at this intersection for a long time and in many

03:33.680 --> 03:39.760
ways it's still nascent despite having been a problem and been something that like people were

03:39.760 --> 03:45.760
really looking at for a long time in part because it's so different from normal data like okay

03:45.760 --> 03:52.000
convolutional neural networks and deep learning have come in and eat and everyone's a lunch at

03:52.000 --> 03:58.400
image classification at natural language processing a signal analysis all these different problems

03:59.200 --> 04:06.400
but all these problems have some underlying similarity of like things near each other are

04:06.400 --> 04:11.280
related to each other the words that I'm saying right now like they have meaning based on their

04:11.280 --> 04:17.600
order and the things that I say tomorrow really have no relationship to what I'm saying today.

04:17.600 --> 04:23.760
There's this correlation in time pixels in an image if you look at this sort of pixel on my shirt

04:23.760 --> 04:28.240
and it's blue you look at the pixels around it they're probably going to be a very similar shade of

04:28.240 --> 04:36.960
blue there's this spatial correlation that's true for signals as well malware that does exist

04:36.960 --> 04:46.800
but it's so much more complex than that you've got this sort of arbitrary system designed by humans

04:46.800 --> 04:52.320
that is like instruction code and assembly and how that gets converted into like literal zeros and

04:52.320 --> 04:59.360
ones and how the compiler is optimizing the layout of the code and which functions get put together

04:59.360 --> 05:08.880
and not and inlining and just a huge amount of complexity that is very different from what people

05:08.880 --> 05:15.840
are working on it's a very different scale a single executable like if you go download a new browser

05:15.840 --> 05:21.760
that's like 30 megabytes would be a really small browser download but if you look at that as one

05:21.760 --> 05:27.440
data point that's a massive data point like that's huge like most people like data point you're

05:27.440 --> 05:36.480
working on is maybe like a kilobyte at like the most we're talking about 30 plus megabytes is

05:36.480 --> 05:43.840
like pretty like normal thing to occur anything if you're looking at your you've got some data set

05:43.840 --> 05:50.400
of software programs and you've trained malware detector on software programs your browser would

05:50.400 --> 05:55.920
be like a feature a 30 megabyte feature that you're trying to do inference or classification

05:55.920 --> 06:01.120
against is that what you're that what you're about that it's like a variable size feature and like

06:01.120 --> 06:05.360
maybe there's an image and maybe there's multiple images embedded inside of it maybe there's a

06:05.360 --> 06:10.480
word doc embedded inside of it there's like every other file format could be embedded inside of this

06:10.480 --> 06:18.560
one file format and it's describing like code this like thing that could do anything of arbitrary

06:18.560 --> 06:26.560
complexity like that's what turning completeness is and we have proofs on like how you can't know

06:26.560 --> 06:30.320
what any arbitrary code is going to do about running it well you don't want to run it because it

06:30.320 --> 06:38.160
might be malware so there's there's just sort of this huge explosion of complexity as you dig into

06:38.160 --> 06:46.720
all these details and machine learning in the broader communal sense has not really

06:47.440 --> 06:52.880
developed the tools to deal with data that's so weird and so different in all these unique ways

06:52.880 --> 06:59.120
so there's a lot of just sort of unsolved machine learning problems at this intersection

06:59.120 --> 07:05.680
and that also creates lots of avenues for attack at this intersection because the malware author

07:05.680 --> 07:09.440
they don't need to abide by their rules that's part of the whole point is they're trying to break

07:09.440 --> 07:16.160
the rules if there's a spec that says oh you don't set this flag and executable because it will

07:16.160 --> 07:20.240
behave poorly well if that helps the malware author they're going to do it they don't care

07:22.000 --> 07:30.640
yeah so it's it's it's a very rich and interesting area I think from both like a machine learning

07:30.640 --> 07:35.520
math side of like how do we get the math to work with these new kinds of complexities these new kinds

07:35.520 --> 07:43.840
of relationships but also just from like the low level really technical side of like I could use

07:43.840 --> 07:49.040
some undocumented instruction to try and make my malware work that like oh yeah this only runs

07:49.040 --> 07:53.680
on new CPUs that have this undocumented instruction because it causes some weird side effects

07:53.680 --> 08:02.160
wow and like I've seen things on like research that gets done on a like specter-based malware

08:02.160 --> 08:11.360
where like the malware only works in the like prefetching of the CPU trying to prefetch code and

08:11.360 --> 08:16.080
data so that if you run it on like the wrong type of CPU or like in a virtualized environment it

08:16.080 --> 08:21.280
won't prefetch the same way malware won't run so you sort of hide what's actually happening and

08:21.280 --> 08:25.840
if you just look at the code the code doesn't make it obvious what's going on because the malicious

08:25.840 --> 08:33.120
intent is hidden to the prefetching logic so just a huge richness of complexity that that makes

08:33.120 --> 08:42.400
it interesting and fun and malware is you know just one element or part of this broader kind of

08:42.400 --> 08:55.440
cyber security patchwork or you know set of problems problem domain is it representative do you

08:55.440 --> 09:03.600
think of the other aspects in that other aspects of cyber security kind of suffer from the same

09:03.600 --> 09:13.680
kinds of problems or are they all kind of unique in their own ways I'm definitely I'm a machine

09:13.680 --> 09:21.120
learning man first and learned about cyber as like an application area or malware specifically

09:21.120 --> 09:26.560
so I don't want to speak too authoritatively but I think a lot of these have their own sort of

09:26.560 --> 09:35.920
unique snowflake problems especially around data collection and building a data set that is

09:35.920 --> 09:42.080
again sort of like unique to this problem space in general where if you want to get your data labeled

09:44.400 --> 09:51.760
it's really easy to go like for images like you can a toddler can label images literally like

09:51.760 --> 09:56.800
you could you could set up that app and they put it on the iPad and they would do it for hours

09:57.920 --> 10:04.080
no special training required to be like this is a cat this is a dog like okay here's this

10:04.080 --> 10:08.560
this arbitrary executable tell me if it's malicious or not or like what kind of malware family it's

10:08.560 --> 10:16.480
from that's a huge amount of work or like if here's some keycap data here's some network traffic

10:16.480 --> 10:22.720
data like is anything weird going on on this network I would just stare at you I don't know maybe

10:24.160 --> 10:29.840
what do you what do you think the answer is like it's it's very technically deep and complex

10:30.720 --> 10:33.360
and each each avenue sort of has its own unique problems

10:36.400 --> 10:45.840
one of the ways that you've taken on this malware problem is a recent paper focus on

10:45.840 --> 10:53.200
adversarial transfer attacks can you talk a little bit about that paper and the problem that

10:53.200 --> 10:59.680
it's seeking to address yeah so this was a paper's adversarial transfer attacks with unknown

10:59.680 --> 11:07.120
data and class overlap that a couple of people on our team have worked on Luke Richards was the

11:07.120 --> 11:14.800
lead author on it some great work we also had one of our collaborators from Nvidia and UMBC this

11:14.800 --> 11:20.240
paper we were really looking at a core adversarial machine learning problem which is

11:21.280 --> 11:27.200
been motivated by a lot of work recently but you have some model that you want to defend

11:28.080 --> 11:33.520
and we'll call you the victim because someone's going to try and attack you and so there's some

11:33.520 --> 11:41.040
adversary that wants to attack the victim model and they know that the models there or they know

11:41.040 --> 11:49.360
you're going to do it but they don't have access to the model specifically so quote unquote black

11:49.360 --> 11:55.920
box attack black box it is black box and it's a specific type of black box attack that we

11:55.920 --> 12:00.000
sort of generally call transfer learning because what the adversary does is they build their own

12:00.000 --> 12:06.320
model that sort of does the same task and they attack their own model and the assumption is well

12:06.320 --> 12:11.280
I built a good model you're doing the same thing these attacks should probably work on your model

12:12.000 --> 12:17.040
and the research that's been done to date sort of says yeah that does work

12:19.360 --> 12:25.600
and we were thinking about this and thinking about it from like okay if we were doing this in

12:25.600 --> 12:37.200
really real life does this match reality and maybe maybe sometimes but oftentimes not because

12:37.200 --> 12:43.520
what a lot of this work has done has sort of said I know you're working on this problem I know

12:43.520 --> 12:49.920
you have a model to detect malware because you're the government and malware is coming at you or

12:49.920 --> 12:58.080
you're a large bank or everyone has malware detectors you're supposed to that's part of the game

13:00.000 --> 13:05.280
or you can you can imagine sort of like many different tasks you would expect like the

13:05.280 --> 13:11.120
government should be doing this like that would make sense and if you're an adversary you just

13:11.120 --> 13:16.000
want to like mess with people you're a bad actor and so you build your own model but you're not

13:16.000 --> 13:24.240
probably going to have the exact same data that the victim does and you might not know like you're

13:24.240 --> 13:29.200
know you know you you can guess what they're going to do but you don't know exactly how they've

13:29.200 --> 13:35.440
set it up like there could be reasonable design choices on like well what are my classes exactly

13:35.440 --> 13:43.120
and where is that line so if you're in that situation where you don't have access to their model

13:43.120 --> 13:48.400
you're probably not going to have a perfect match for their data or how they've designed the

13:48.400 --> 13:55.280
class structure and it sounds like a lot of the prior work assumed that the adversary was

13:55.280 --> 14:01.040
operating from the same playbook so to speak it was using the same data and or the same classes

14:01.760 --> 14:08.080
the exact same data and the exact same data and exact same classes guy which is like unrealistically

14:08.080 --> 14:16.160
optimistic which like if you can defend against like an omnipotent adversary good for you

14:16.160 --> 14:24.160
congrats but I want to know what's going to happen in real life so that's for the motivation

14:24.160 --> 14:30.320
and I'm not claiming ours is perfect to real life but I think it's closer so we started building

14:30.320 --> 14:38.080
some tests to sort of vary like from zero percent overlap in the data to like a hundred percent

14:38.080 --> 14:44.320
and how much how much is that data overlap really a factor in the transfer success rate

14:45.600 --> 14:51.440
and also overlapping the number of classes that are in common so you have like all the exact same

14:51.440 --> 14:56.880
classes as each other to you have like two classes in common so like a very small attack

14:56.880 --> 15:04.880
attack surface there we spoke a little earlier about the difficulty of kind of defining the data

15:04.880 --> 15:13.680
in this space as a feature an executable or is it something else in this particular case in

15:13.680 --> 15:20.400
the example that you used in this paper what was a piece of data feature and what did the labels

15:20.400 --> 15:26.720
look like and this we were focusing just on the the adversarial machine learning part so we picked

15:26.720 --> 15:32.320
easy data that everyone has access to so we just did image data like CIFAR 10 CIFAR 100

15:33.680 --> 15:40.560
mini image net and the the classes were the normal classes like cat dog car truck

15:41.040 --> 15:44.160
okay deer and frog I think are also classes in there

15:44.160 --> 15:51.840
I mean we're just focusing on the the adversarial part for for this one all the complexity of

15:51.840 --> 15:56.560
malware just like that's too much extra complexity right now right you're you're you're basically

15:58.320 --> 16:04.800
trying you're hey let's step back take a simple model and just test this fundamental assumption

16:04.800 --> 16:11.440
about transferability different data and classes yeah something that I often

16:11.440 --> 16:19.360
probably probably one of the best skills that I try to like pass on to like employees and students

16:19.360 --> 16:25.120
is like okay like you have this complex goal just figure out some way to cut this down into

16:25.120 --> 16:31.520
different chunks because you can't eat the whole sandwich it's too big forget about an elephant

16:32.160 --> 16:39.760
you can't even eat the sandwich it's too big you got to cut something off so we cut off this part

16:39.760 --> 16:45.440
so we're using just image data's convolutional neural networks that everyone sort of like feels

16:45.440 --> 16:51.440
comfortable with you're going to understand what the results mean and when we set this up

16:55.040 --> 17:04.720
we we saw that both less class overlap and less data overlap both would hurt the attackers

17:04.720 --> 17:12.560
success rate which makes sense but we also saw some odd behaviors that it wasn't sort of as

17:12.560 --> 17:17.520
consistent as you would expect it wasn't sort of like the smooth degradation down like it would

17:17.520 --> 17:27.200
get worse and it would like start to get better again but randomly and as we start like some kind

17:27.200 --> 17:32.560
of generalization property kicking into effect or something that's that's part of what I think

17:32.560 --> 17:39.920
it is because what really became very interesting is from just one of the base results of that

17:39.920 --> 17:44.960
behavior we think a lot of it is like it once you sort of reach a minimum sort of threshold of

17:44.960 --> 17:50.880
like lack of similarity there's a lot more randomness that comes into play that the model just

17:50.880 --> 17:58.800
is not trained to like expect in any possible way and now it's making errors maybe not because

17:58.800 --> 18:04.560
the adversary did such a good job crafting the example but the data is just so different from

18:04.560 --> 18:12.080
what the model itself understands because the amount of overlap has decreased so much and

18:13.280 --> 18:18.240
the the the really sort of scary part is when it comes to like okay you want to defend your model

18:18.240 --> 18:25.680
what defenses work currently the the overall best defense is adversarial training

18:25.680 --> 18:34.880
it's been the best defense basically since it was introduced in like 2017-2018 and it's pretty

18:34.880 --> 18:41.120
simple strategy of like okay you have your model you train it and now as you're training attack

18:41.120 --> 18:46.960
your own model then feed the attacked inputs back into the training and just sort of be doing that

18:46.960 --> 18:56.800
continuously so you're constantly training the model to be better at correctly classifying attacked

18:56.800 --> 19:05.280
data points and our result showed if you do this in this more realistic scenario adversarial training

19:05.280 --> 19:14.480
actually weakens the defender wait what yeah the attack success rate increased on models that

19:14.480 --> 19:22.000
had been adversarially trained I think what's happening there what we what we sort of believe is

19:22.000 --> 19:30.720
going on is when you're doing adversarial training you're in a way overfitting to a very specific

19:30.720 --> 19:39.200
adversary an adversary that has the exact same classes and data you do but when the adversary

19:39.200 --> 19:47.040
has different classes their attacks are going to naturally go in different directions that your

19:47.040 --> 19:55.520
model has just never optimized for because it sort of started from an initial condition and it

19:55.520 --> 20:03.040
got better that it's sort of just converging on whatever initial path worked best and so it's

20:03.040 --> 20:12.320
sort of overlearned to the sort of unrealistic scenario and these new attacks are tightly coupled

20:12.320 --> 20:17.120
like the the data is the same because it's it's baked into the process of creating the model

20:17.840 --> 20:26.400
exactly and so the attacks seem to transfer more successfully if the victim has done adversarial

20:26.400 --> 20:32.880
training in this sort of imperfect knowledge scenario which if you're actually trying to build a

20:32.880 --> 20:38.640
robust model for real life production like that's actually a huge concern now because you say okay

20:38.640 --> 20:44.560
I'm going to keep my model private I'm going to try and sort of mitigate as much information that

20:44.560 --> 20:53.680
the adversary could acquire by sort of keeping this close hold and you want to do all the best things

20:53.680 --> 20:57.360
okay I'm going to do adversarial training like oh actually if you believe that's the correct

20:57.360 --> 21:02.400
threat model you might not want to do that it might actually make you more susceptible

21:02.400 --> 21:13.360
rather than less interesting can we maybe take a second to kind of punch into the degree to which

21:13.360 --> 21:19.520
adversarial training and a lot of these you know techniques around adversarial ML generally

21:19.520 --> 21:26.400
the way the degree to which those are you know practical concerns and implemented by people

21:26.400 --> 21:33.680
that are building actual models versus academic thought exercises that people really aren't

21:33.680 --> 21:37.360
thinking about when they're putting models in a production at this point but what's your read

21:37.360 --> 21:47.200
on that from where you sit I think on average it's more academic for most real most most real world

21:47.200 --> 21:52.240
like usage of it is more academic like people don't necessarily a reason to really believe

21:52.240 --> 21:57.040
they're under attack so a lot of people motivate this with like self-driving cars that

21:58.240 --> 22:02.480
someone's going to trick the self-driving car into plowing through a stop sign

22:03.760 --> 22:10.720
which like I don't want that to be a thing that can happen to my future self-driving car

22:10.720 --> 22:19.920
match true I'm sure maybe some horrible person out there like tries to screw of them but like in

22:19.920 --> 22:23.440
general it's not like a very well-motivated threat like who's going around just trying to

22:23.440 --> 22:33.840
destroy every self-driving car it's like it's important I think it's more important in like

22:33.840 --> 22:38.640
the academic sense of like trying to figure out how to make models robust to just like errors

22:38.640 --> 22:49.600
in general then it is there's an actual adversary trying to trick you and a lot of times a lot of

22:49.600 --> 22:58.000
the research that gets done becomes sort of somewhat cartoonish and like the amount of information

22:58.000 --> 23:02.560
that they give the adversary makes it like why would they do it this way if they were that

23:02.560 --> 23:10.400
powerful like I remember seeing some work on like all adversarial attacks on medical imaging

23:10.400 --> 23:16.400
and they'll they'll adjust the medical image and the to make the AI models would give you

23:16.400 --> 23:20.400
their long prescriptions and the wrong drugs and the wrong medication to like hurt your health

23:20.400 --> 23:25.680
and if they have like that much control to like you get all this data and like access model

23:25.680 --> 23:34.560
it just flip a bit in the database like it seems I'm like if they're that powerful there's

23:34.560 --> 23:41.280
so much easier things they could have done right so sort of the goal of like how we came up with

23:41.280 --> 23:46.240
this work to begin with was like well in a realistic scenario what's it actually going to look like

23:46.240 --> 23:53.840
and for malware it is a really like true true to life realistic problem that people do deal with

23:53.840 --> 24:00.960
like day-to-day which is part of why I like doing work in this space is it's it's not academic

24:00.960 --> 24:10.560
it is like this is really happening meaning not the the broad existence of motivated adversaries

24:10.560 --> 24:20.320
but the specific application of adversarial attacks to in kind of the malware world

24:20.320 --> 24:26.960
I mean a malware you know there are existing malware detectors that are based on machine learning

24:26.960 --> 24:33.200
models and there are people out there that are trying to deploy adversarial attacks against those

24:33.200 --> 24:39.280
models and there are people in that world that are building adversarial robustness into those

24:39.280 --> 24:47.840
models like that's all real and extent today yeah that that that is all real to varying degrees

24:47.840 --> 24:52.720
today it's more complex in the malware space because an adversarial attack in the malware space

24:54.400 --> 25:02.400
can happen earlier in the process so because again like these they're so executables are so complex

25:02.400 --> 25:11.440
you can sort of mess with the executable itself to maybe screw up the way that the antivirus

25:11.440 --> 25:18.640
process is the features to begin with so you can change like the whole floor out from under the

25:18.640 --> 25:28.720
the model that you're trying to fool uh and meaning like like obfuscation of the the malware code

25:28.720 --> 25:34.320
within the broader executable or something different that that's one of many many possible ways so

25:34.320 --> 25:39.520
you could uh there's a thing called packing which is like let me put my executable inside of

25:39.520 --> 25:44.960
another executable so you're sort of you have to figure out how to peel this onion to figure out

25:44.960 --> 25:53.360
what's going on underneath or like a dynamic analysis which is when you do try to run the malware

25:53.360 --> 26:00.880
or to get features a lot of malware will just like initial like step of the malware is just like

26:00.880 --> 26:06.240
wait 24 hours because you're probably not going to run this dynamic analysis for 24 hours you're

26:06.240 --> 26:10.400
going to run it for like three minutes tops so they're just going to try and like outweigh you

26:12.160 --> 26:18.000
and so like you might have features that like oh does it call this api does it call the crypto

26:18.000 --> 26:23.120
functions that's that might be a good sign of ransomware does it call the file delete functions like

26:23.120 --> 26:27.760
okay that's a really good sign of ransomware it has both of those oh they just wait until the clock

26:27.760 --> 26:35.440
runs out then you never see the features to begin with um so there's more complex and interesting

26:35.440 --> 26:43.920
ways that the malware adversary can sort of mess with the model uh and uh I I can go on like lots

26:43.920 --> 26:50.400
of fun tangents on my favorite one that I've seen which is not I've never seen it actually used

26:50.400 --> 26:57.680
by malware but it's more it's just fun that like this is possible uh there is this project that

26:57.680 --> 27:06.880
people released called the mob fiscator uh fiscator mob like MOV until you've instructed yeah because

27:06.880 --> 27:12.160
it's an instruction for people who are to wear it's like this is the assembly code that moves

27:12.160 --> 27:18.960
data from one location in memory to another location in memory okay and so you might say like

27:18.960 --> 27:24.000
please move like please move this memory from like please move this data from like memory into this

27:24.000 --> 27:29.360
register because I'm going to do some work on it or from register like back to disk to save it or

27:30.000 --> 27:38.800
whatever uh this one instruction has so many side effects that is actually turn turn complete

27:38.800 --> 27:45.360
so you can compile any program into one that contains only the move instruction

27:46.560 --> 27:51.120
ostensibly it looks like this program only moves data and never actually does anything with it

27:51.120 --> 27:56.800
mm-hmm but it has so many weird side effects that you can actually and it works you can recompile

27:56.800 --> 28:03.280
any program to contain only one single instruction wow because the program itself is just

28:04.000 --> 28:09.520
putting bits into memory locations and executing them yep right and there's there's

28:09.520 --> 28:15.040
enough special like side effect cases on there's one instruction that that's the only instruction

28:15.040 --> 28:25.120
you technically need to be able to do anything on an x86 computer wow you see weird stuff like this

28:25.120 --> 28:32.000
with malware so often it's just like yeah at anything you can think of like I'm sure there's some

28:32.000 --> 28:36.400
way someone could get around this and then it becomes a numbers game I'm like how many more

28:36.400 --> 28:44.240
machines am I protecting how many more cases am I covering and am I sort of making any new like

28:44.240 --> 28:48.960
gaping holes that I need to address because you're not going to get it all um but you got to

28:48.960 --> 28:52.400
that doesn't mean you sit around and go like well I can't solve it perfectly so I might as well

28:52.400 --> 28:58.480
not do it no you build the best solution you can you sort of get it out there you try and fix

28:58.480 --> 29:06.400
what you can and see how do they adapt what what happens next and sort of this continual back and

29:06.400 --> 29:16.480
forth of you taking a step in the adversary taking a step so malware you know is a place where

29:16.480 --> 29:25.120
these kinds of things are happening today and we kind of got to that from talking about the

29:27.680 --> 29:34.160
adversarial or adversarial training adversarial robustness kind of incorporating into

29:34.160 --> 29:40.960
training and that having an adverse effect on robustness um is there a solution to that

29:42.240 --> 29:47.440
working on it I'm not out of a job yet though thank you

29:50.800 --> 30:00.720
it's it's sort of a double edge sword if we can I mean if like the the gut reaction of everyone is

30:00.720 --> 30:05.280
like these adversarial attacks are definitely a bad thing we don't want them to exist we've got to

30:05.280 --> 30:13.680
get rid of them uh which yes it is definitely a bad thing uh but it's in the context of how things

30:13.680 --> 30:21.600
are being used if someone's using a machine learning model in a bad way then being able to subvert

30:21.600 --> 30:28.400
that model now actually becomes a defense a good thing so if someone is using the machine learning

30:28.400 --> 30:34.160
model to sort of like create like a surveillance like state or something uh like okay that we're

30:34.160 --> 30:39.680
not comfortable with that we don't like that um there's actually there was a recent paper I had

30:39.680 --> 30:49.760
this idea years ago but didn't do it um I get no credit for just having a nice idea but uh this

30:49.760 --> 30:56.960
is a really interesting paper that got published uh or put online at least on uh using like adversarial

30:56.960 --> 31:03.040
attacks to figure out how to put on makeup in such a way that you look normal but the machine learning

31:03.040 --> 31:09.040
model will just be like yeah there's no one there so sort of giving you a way to sort of get some

31:09.040 --> 31:16.800
of your privacy back you don't want to be tracked or looked at so these are things that um they're

31:16.800 --> 31:20.960
complex from many perspectives they're not just from a technical perspective but from like an

31:20.960 --> 31:24.560
ethics perspective I'm like what is and is not an okay use of machine learning

31:24.560 --> 31:28.880
how do we want to deploy these things these are the things that we often have to work about and

31:28.880 --> 31:36.400
think about and be conscious of uh but it also does mean that adversarial attacks and defenses

31:37.120 --> 31:43.760
are like whichever way it goes we we ultimately can or cannot defend against these attacks

31:44.320 --> 31:49.200
also means we ultimately can or cannot subvert people using machine learning for

31:49.200 --> 31:58.960
or inappropriate unethical use cases uh yeah so that's something that I uh think about a lot

31:59.760 --> 32:05.840
the main idea is that hey we don't uh you know the research that's happening here isn't really

32:05.840 --> 32:11.200
considering our real world scenario your paper considered a real world scenario and you found that

32:11.200 --> 32:26.480
um in general the less overlap um the more difficult an attack is but in a kind of weird way where

32:26.480 --> 32:35.360
a weird non-linear way and in addition we could from from the attackers perspective we could

32:35.360 --> 32:44.800
restore some more predictable behavior to the attacker uh so if we sort of simulated uh

32:46.240 --> 32:52.240
attacking a model with unknown classes or if sort of imperfect classes by sort of like

32:52.240 --> 32:57.280
randomly masking them out and we generate the attacks so we're like each time we attack it we're

32:57.280 --> 33:03.680
going to randomly pretend some of these classes don't exist and don't count um so maybe you're trying

33:03.680 --> 33:13.200
to uh maybe the model's first gut instinct is to convert the uh model from predicting truck

33:13.200 --> 33:19.040
to car you say well we've masked out car that's not an option so you're not getting credit for that

33:19.040 --> 33:27.920
you have to do something else uh if we do that we restore a lot of behavior that makes more sense

33:27.920 --> 33:38.400
so it it sort of removes the variance of uh class overlap from the attacker's accessory uh so

33:38.400 --> 33:46.400
rewind this for me this is when you're training your target model you are doing what

33:47.200 --> 33:51.760
so when we're when we're generating the attacks uh generating the attacks okay

33:51.760 --> 33:58.400
okay to sort of transfer to the victim uh so the adversary has their own what we call a surrogate

33:58.400 --> 34:02.320
model they built their own model that they think hopefully matches what you've done

34:03.120 --> 34:08.560
and they're basically going to perturb their own surrogate model in order to simulate that

34:08.560 --> 34:17.280
they don't know exactly what you're doing and that uh the attacker pays a penalty in terms of

34:17.280 --> 34:22.320
attack success rate in order to do that but what they gain is sort of certainty about their

34:22.320 --> 34:32.160
attacks success rate if that makes sense so in the normal situation uh maybe the average

34:32.160 --> 34:38.480
success rate of their attack would have been 40% but they're not sure that it's actually 40%

34:38.480 --> 34:44.400
they're well maybe it's between like 15 and 60 and I can't really tell what exactly it is

34:44.400 --> 34:54.240
but then if you do this modified attack they're pretty sure that it's somewhere between like 30

34:54.240 --> 35:01.280
and 35 so it's lower but they they actually know what they're going to get so kind of greater bias

35:01.280 --> 35:15.600
less variance yeah exactly and on the the adversarial training um the adversarial robustness

35:15.600 --> 35:25.120
training um are you or are you aware of folks that are like how would you go about trying to

35:25.120 --> 35:35.760
um like reformulate that problem in light of kind of the real world thing like you know I'm thinking

35:35.760 --> 35:41.440
of hey is there an analogy to dropout that's like class dropout where you kind of forget about

35:41.440 --> 35:47.360
classes in the adversarial loop and like does that help like are folks working on that specific

35:47.360 --> 35:53.440
problem we're not not the class dropout thing that the broader problem we actually tried the

35:53.440 --> 36:03.120
class dropout thing and unfortunately did you we tried that one I didn't work we were very sad about

36:03.120 --> 36:12.160
that but uh no I I don't know what the answer is yet uh something that I want to look at and

36:12.160 --> 36:17.760
part of the problem with these experiments where they were hugely expensive to run because

36:17.760 --> 36:24.880
instead of sort of attacking one model for one data set we have to attack like 50 models

36:26.000 --> 36:31.760
for one data set because we have to vary the class overlap and the data overlap every single time

36:32.640 --> 36:36.800
and then we want to run it multiple times for each combination because

36:37.840 --> 36:42.720
you can pick different classes each time that might bias the results so it turns into this

36:42.720 --> 36:50.640
computational explosion of model training and attacking so something that we want I want to look at

36:50.640 --> 36:59.040
but I think we might need to figure out something more intelligent first is these approaches that

36:59.040 --> 37:06.640
try to sort of build provably robust models from the onset where you can sort of think of it like

37:06.640 --> 37:10.720
your model makes a prediction and it's sort of making a prediction about like a single data point

37:10.720 --> 37:16.640
like one point comes in and you get one answer and what they try to do is they try to build models

37:17.840 --> 37:23.680
this is one approach anyway there's multiple but they try to basically classify instead of one

37:23.680 --> 37:28.640
data point like a region so you sort of have like the data point is the center and there's like a

37:28.640 --> 37:36.160
sphere around it and you're saying everything in this sphere gets the same answer and if you can

37:36.160 --> 37:42.000
do that then you've sort of you're provably robust to attacking that data point for a certain sized

37:42.720 --> 37:49.760
radius and then as you train you try to sort of increase the size of that sphere that you can do so

37:49.760 --> 37:55.040
that spheres start out sort of infinitely small well you just the data point and you try and push

37:55.040 --> 38:03.200
them wider and wider as you go and that means you're becoming more robust theoretically that would

38:03.200 --> 38:10.400
work better but I'm not sure in theory there's no difference between theory and practice but in practice

38:10.400 --> 38:20.720
there is it sounds like that has strong implications on the the type of model you're using like

38:23.600 --> 38:29.920
you know maybe even as far as not a neural network model or can you incorporate that kind of

38:29.920 --> 38:36.720
technique into a neural network formulation you you can for a neural network it's it's really

38:36.720 --> 38:42.560
expensive so it's sort of like if I wanted to do that for these experiments I'm going to increase

38:42.560 --> 38:49.200
the compute time by like at least another factor of 10 or it took us like four or five months to run

38:49.200 --> 39:04.400
all these experiments I was like all right 40 50 months okay maybe I think in terms of

39:06.240 --> 39:11.600
real world like when you're really thinking through do I need to be concerned about an adversarial

39:11.600 --> 39:18.240
attack am I an entity that is likely to be attacked if I'm a government if I'm a bank if I'm a

39:18.240 --> 39:25.520
large enough corporation then yeah you you are likely to be targeted and attacked then okay what

39:25.520 --> 39:31.440
what are the models that I train that are sort of the most likely to be attacked or that people

39:31.440 --> 39:35.920
are going to try and subvert so like if you're a credit card company like you have fraud detection

39:35.920 --> 39:44.240
models yeah that people are trying to subvert all the time so okay I have this model like I have

39:44.240 --> 39:50.480
these models that I have then identify these are the models at risk and now let's really think

39:50.480 --> 39:56.800
through like the specifics of the threat model for this particular thing not this sort of academic

39:56.800 --> 40:04.080
abstract I can apply this to any problem kind of thing but what do we know about the domain

40:04.080 --> 40:10.800
that we can use to build a robust model for this specific problem that's what I've had the most

40:10.800 --> 40:15.680
success doing anyway that's the way that I generally try to help like our clients approach these

40:15.680 --> 40:22.560
kind of problems is to really focus in on that scope and narrow down like where do we actually

40:22.560 --> 40:26.560
need to do this let's not panic and think that everything's under attack all the time because

40:26.560 --> 40:33.760
that's not realistic and you're going to give yourself a heart attack and we've done that we've

40:33.760 --> 40:38.160
done some of that for malware for specific kinds of malware models and we've done some of that

40:38.160 --> 40:45.360
for a computer vision before and gotten a lot of success much more quickly with that approach

40:45.360 --> 40:49.600
and as you say like part of that can be like well maybe you shouldn't use a neural network maybe

40:49.600 --> 40:54.640
you should be using a linear model and sort of carefully crafting your your features to make a

40:54.640 --> 41:01.840
linear model work well or maybe a shallow decision tree or a random forest or something would be

41:01.840 --> 41:11.520
better because you can sort of understand to what degree can this really be attacked and what's

41:11.520 --> 41:18.960
that envelope and and design around it so like part of how we did that for some of our malware work

41:18.960 --> 41:25.840
was if we if we go back to like the example of dynamic analysis where if you're actually going to

41:25.840 --> 41:32.000
see the thing run let's just assume it's not perfect I don't claim perfection

41:34.480 --> 41:38.000
but if you say okay I'm actually going to see the thing run I'm going to see it

41:39.680 --> 41:45.040
delete the freed in the files encrypt them write them out delete the original files

41:45.840 --> 41:52.800
I know this is malware and for it to be ransomware it has to like actually have those steps

41:52.800 --> 42:05.120
if you build a model that can only sort of normal models look for things that both indicate like one

42:05.120 --> 42:09.200
or the other so I look for things that tell me it's ransomware I look for things that tell me it's

42:09.200 --> 42:17.760
not ransomware that's a design flaw in this scenario I shouldn't be looking for things that tell me

42:17.760 --> 42:23.920
it's not ransomware because if it does those sort of finite things that qualify it's ransomware

42:25.680 --> 42:31.200
and what malware will do is what what they can do is just insert lots of random other benign

42:31.200 --> 42:36.240
activity like oh look at all these benign things I do that way that outweighs all these malicious

42:36.240 --> 42:41.600
things I did and a normal machine learning model go yeah that's right you did do more benign

42:41.600 --> 42:50.160
things than malicious things you are benign that doesn't make sense at all if you do anything

42:50.160 --> 42:58.560
malicious it's malicious and by default everything is benign it's benign until you do something bad

42:58.560 --> 43:04.400
so we can incorporate that into the model that there are no features that contribute to a score of benign

43:05.120 --> 43:09.920
that that's just sort of the default and then you have to do enough sort of malicious indicators

43:09.920 --> 43:14.800
for the model to change its decision to be like no no no you're actually malicious you don't get

43:14.800 --> 43:21.840
to run anymore and is the the implication then that the models that tend to be used in the space are

43:22.640 --> 43:29.280
kind of very heavily hierarchical or ensemble or something like that where you're

43:30.160 --> 43:35.280
identifying you've got modules that are identifying specific features or characteristics and then

43:35.280 --> 43:42.480
kind of bubbling that up like Microsoft has published some of their like strategy for how they

43:43.040 --> 43:47.120
handle Windows Defender and trying to protect computers and they they've published they have

43:47.120 --> 43:52.640
this whole hierarchical strategy of like okay here's the super fast model and sort of

43:53.840 --> 43:58.240
they can handle a lot of things and for the things that can't like bumping that up a stage

43:58.240 --> 44:03.040
to something more complex it's going to do something more sophisticated and I forget how many

44:03.040 --> 44:07.680
stages it had in it but like the end stage was sort of like okay we're actually going to like run

44:07.680 --> 44:11.680
this and like a dynamic environment in the cloud to try and figure out what this is doing

44:13.600 --> 44:18.480
so you yeah people definitely do that of this sort of building this hierarchy of

44:20.720 --> 44:26.080
speed and complexity trade-offs because you can't afford to run everything through dynamic

44:26.080 --> 44:33.920
analysis all the time like that would just be running all computers but again like there's

44:33.920 --> 44:39.760
not a lot of computers be running all the programs that we want to run to make sure the computers

44:39.760 --> 44:45.280
can run them it doesn't make sense right right right yeah when you were talking about dynamic

44:45.280 --> 44:54.640
analysis I imagined it to be uh well maybe not dynamic I imagined it to be like this design time

44:54.640 --> 45:00.320
tool like hey we collect all these samples of things in the wild and like we run them through this

45:00.320 --> 45:05.680
dynamic analysis thing and like maybe we're creating labels or something but it sounds like

45:05.680 --> 45:10.400
from this Microsoft example it's like no you try to run this app and it says hey hold on I'm

45:10.400 --> 45:15.120
going to you know put this in the malware detector in the cloud and I'll be back to you shortly

45:15.760 --> 45:22.640
yep I remember one of my colleagues who's uh he's a malware analyst uh a lot of experience

45:22.640 --> 45:26.800
you're telling me about how like for the for the analysts who are just doing their job like as part

45:26.800 --> 45:32.560
of a manual process to figure out what things are they would have a big sort of a virtual machine

45:32.560 --> 45:40.720
cluster to run things and try and observe them and see what they're doing and uh sometimes there

45:40.720 --> 45:46.480
there is malware that has what's called a VM escape where there's a bug in the VM and it is

45:46.480 --> 45:53.120
possible for the the code to recognize it's being run inside a virtual machine and leave the

45:53.120 --> 45:58.240
virtual machine and infects the host that it's on and occasionally that would happen and they

45:58.240 --> 46:03.920
would just be like all right burn everything down like yeah this is you're complex now just

46:04.960 --> 46:09.680
light everything on fire we build again from scratch this is the only way to make sure it's clean

46:10.400 --> 46:16.320
right right uh that that might be a slightly hyperbolic description but it's the

46:16.320 --> 46:22.880
the complexity is just so great it's never ending yeah awesome awesome um

46:24.560 --> 46:30.240
what what kind of future directions you know as if we haven't already talked about enough of them

46:30.240 --> 46:39.360
are you excited about in this video uh I'm excited about uh graph neural networks

46:39.360 --> 46:47.840
they've been getting more traction the past two years uh that I think is good uh some of it's

46:47.840 --> 46:55.200
been a little contentious on like relating to a theme of uh reproducibility that's also been

46:55.200 --> 47:01.600
picking up on like okay graph neural networks were very nascent a few years ago and then there was

47:01.600 --> 47:06.640
sort of like a small like kickoff of people iterating and publishing oh here's my new fancy this

47:06.640 --> 47:11.120
and then a lot of people were saying like oh actually all this is wrong and really you just

47:11.120 --> 47:18.000
didn't tune things correctly but so there there's still a lot of sort of working things out there

47:18.000 --> 47:24.080
I think uh and are because like the complexity of dealing with graphs like it's now much more arbitrary

47:24.080 --> 47:28.480
connectivity and how much this depends on the specific kinds of graphs you were looking

47:28.480 --> 47:36.480
what looking at versus someone else's graphs uh but I think a graph structure really is going to be

47:36.480 --> 47:43.440
the most at least from like a machine learning modeling perspective the most ideal way to model

47:43.440 --> 47:48.480
a lot of malware you can sort of create this graph of like which code parts connect to which other

47:48.480 --> 47:53.840
code parts you can have features on the nodes of the graph but also features on the edges of the

47:53.840 --> 48:01.600
graph so you can also have uh if there's a chunk of the executable that you couldn't disassemble you

48:01.600 --> 48:06.800
couldn't figure out what's there you could still have that as part of the model with some features

48:06.800 --> 48:12.240
sort of indicating that like this is connected here somehow but we're not sure what's actually

48:12.240 --> 48:18.960
happening at the node um like bailed to parse correctly uh so I think that's something that

48:18.960 --> 48:28.240
is probably the right direction to be headed in but is not yet fast enough and scalable enough

48:29.120 --> 48:34.720
for the malware sort of use case where like our data points are like the size of other people's

48:34.720 --> 48:42.880
datasets um like I wanted one of the datasets that uh I work with regularly the largest file in

48:42.880 --> 48:49.600
the dataset has like 200 megabytes which is like all as if our 100 is 200 megabytes like literally

48:49.600 --> 48:55.520
there's one data point is the size of this dataset everyone's using uh it's interesting that you

48:56.240 --> 49:03.840
provided that context about graph uh graphical models graph uh neural nets has applied to this

49:03.840 --> 49:12.640
space I'm pretty sure one of my first interviews around cyber was on graph stuff and I did not

49:12.640 --> 49:18.800
realize that there was a bit of a graph neural networks and security winter that happened

49:20.800 --> 49:27.200
I'm not sure it's so much as a winter is uh the the graph neural networks for malware just like

49:27.200 --> 49:31.840
hasn't germinated yet in the first place okay so the graph neural networks in the machine

49:31.840 --> 49:38.320
or in the machine learning land people have done graph based things for malware a lot before but

49:38.320 --> 49:46.320
just like normal graphs not neural network based uh got it uh so I I can see like the room for

49:46.320 --> 49:56.000
rules to collide but they haven't collided yet got it very cool very cool well Edward it was

49:56.000 --> 50:01.600
wonderful chatting and learning a bit about what you're up to in this space thanks so much for

50:01.600 --> 50:07.040
joining us yeah thank you again for having me it was a really fun conversation and I really enjoyed

