1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,360
I'm your host Sam Charrington.

4
00:00:23,360 --> 00:00:28,160
This past week, I spent some time in San Francisco at the Artificial Intelligence Conference

5
00:00:28,160 --> 00:00:30,800
by O'Reilly and Intel Nirvana.

6
00:00:30,800 --> 00:00:36,560
I had a ton of fun and got a bunch of great interviews from some amazing people doing

7
00:00:36,560 --> 00:00:39,360
awesome work in ML and AI.

8
00:00:39,360 --> 00:00:45,440
I got to talk to folks like Gunner Carlson of IAZD and Stanford who's applying topological

9
00:00:45,440 --> 00:00:51,440
models to machine learning like Ian Stoica of UC Berkeley whose RISE lab is building

10
00:00:51,440 --> 00:00:57,280
Ray, a distributed computing platform for reinforcement learning, and like Mo Patel and

11
00:00:57,280 --> 00:01:02,640
Laura Furlick of Think Big Analytics who shared a bunch of great use case stories with me.

12
00:01:02,640 --> 00:01:06,800
I'm super excited about my interviews from the conference and I'm looking forward to

13
00:01:06,800 --> 00:01:08,240
sharing them with you.

14
00:01:08,240 --> 00:01:13,520
Make sure you check back with us on October 9th to catch the full series.

15
00:01:13,520 --> 00:01:17,600
In the meantime, I've got a great interview for this week.

16
00:01:17,600 --> 00:01:21,840
Like last week's interview with Bruno Gunn's office on Word to Vic and Friends,

17
00:01:21,840 --> 00:01:27,520
this week's interview was also recorded at the last O'Reilly AI conference back in New

18
00:01:27,520 --> 00:01:29,880
York in June.

19
00:01:29,880 --> 00:01:34,640
Also like last week's show, this week's is focused on natural language processing and

20
00:01:34,640 --> 00:01:36,400
I think you'll enjoy it.

21
00:01:36,400 --> 00:01:41,920
I'm joined by Jonathan Mugen, co-founder and CEO of Deep Grammar, a company that's building

22
00:01:41,920 --> 00:01:47,320
a grammar checker using deep learning and what they call deep symbolic processing.

23
00:01:47,320 --> 00:01:51,600
This interview is a great compliment to my conversation with Bruno and we cover a variety

24
00:01:51,600 --> 00:01:57,400
of topics from both subsimbolic and symbolic schools of NLP, such as attention mechanisms

25
00:01:57,400 --> 00:02:04,040
like sequence to sequence and ontological approaches like wordnet, syn sets, frame net and sumo.

26
00:02:04,040 --> 00:02:06,720
I'm looking forward to your feedback on this show.

27
00:02:06,720 --> 00:02:12,880
Jump over to the show notes at twimmalei.com slash talk slash 49 to let me know what you liked

28
00:02:12,880 --> 00:02:13,880
and learned.

29
00:02:13,880 --> 00:02:20,160
Finally, before we dive into the show, the details for the upcoming Twimo Online Meetup

30
00:02:20,160 --> 00:02:22,240
have been set.

31
00:02:22,240 --> 00:02:29,480
On October 18th at 3pm, Pacific Time, we'll discuss the paper visual attribute transfer

32
00:02:29,480 --> 00:02:34,880
using deep image analogy by Jing Liao and others from Microsoft Research.

33
00:02:34,880 --> 00:02:40,400
The discussion will be led by Duncan Stothers, for anyone who's missed the last two meetups

34
00:02:40,400 --> 00:02:46,160
or for those who haven't yet joined the group, please visit twimmalei.com slash meetup for

35
00:02:46,160 --> 00:02:47,440
more information.

36
00:02:47,440 --> 00:02:52,920
There, you'll find video recaps of the last two meetups along with the link to the paper

37
00:02:52,920 --> 00:02:55,400
we'll be reviewing next month.

38
00:02:55,400 --> 00:02:59,840
If you'd like to present your favorite paper, we'd love to have you do it.

39
00:02:59,840 --> 00:03:05,920
Just shoot us an email at teamattwimmalei.com to get the ball rolling.

40
00:03:05,920 --> 00:03:08,800
And now on to the show.

41
00:03:08,800 --> 00:03:19,680
All right, everyone, I am here at the O'Reilly AI conference.

42
00:03:19,680 --> 00:03:25,680
And I am with Jonathan Mugan, who's the founder and CEO of Deep Grammar, Jonathan, welcome

43
00:03:25,680 --> 00:03:26,680
to the podcast.

44
00:03:26,680 --> 00:03:28,080
Oh, thanks for having me.

45
00:03:28,080 --> 00:03:30,400
I'm excited to get into this conversation.

46
00:03:30,400 --> 00:03:34,720
So you are speaking later today at the conference.

47
00:03:34,720 --> 00:03:37,720
And I'm looking forward to having you walk us through your presentation.

48
00:03:37,720 --> 00:03:41,200
But why don't we start by having you tell us a little bit about your background and

49
00:03:41,200 --> 00:03:42,480
how you got into AI?

50
00:03:42,480 --> 00:03:43,800
Yeah, sure.

51
00:03:43,800 --> 00:03:47,800
So I started off in psychology.

52
00:03:47,800 --> 00:03:53,120
Went and got my undergraduate in psychology and I wanted to understand the human mind.

53
00:03:53,120 --> 00:03:57,800
And as my advisor used to say, the interesting parts weren't scientific and the scientific

54
00:03:57,800 --> 00:04:00,200
parts weren't quite interesting.

55
00:04:00,200 --> 00:04:02,400
And I love that.

56
00:04:02,400 --> 00:04:07,640
Yeah, we didn't quite have a firm grasp on concrete principles that we could

57
00:04:07,640 --> 00:04:10,560
that we could use to really understand what was going on.

58
00:04:10,560 --> 00:04:15,920
So I became a little disillusioned and got my MBA and company called Pricewaterhouse Coopers

59
00:04:15,920 --> 00:04:20,360
trained me up in computer programming and set me off in a consulting world.

60
00:04:20,360 --> 00:04:21,360
Join the ranks.

61
00:04:21,360 --> 00:04:22,360
I did.

62
00:04:22,360 --> 00:04:23,360
Yes.

63
00:04:23,360 --> 00:04:26,720
And then as I was, I was programmed and I was like, you know, you have to tell a computer

64
00:04:26,720 --> 00:04:28,160
exactly what to do.

65
00:04:28,160 --> 00:04:33,360
This might be the kind of, the kind of rigor that we need if we're going to, we're going

66
00:04:33,360 --> 00:04:34,360
to do psychology.

67
00:04:34,360 --> 00:04:35,360
Uh-huh.

68
00:04:35,360 --> 00:04:39,520
And then of course, AI is a mix of psychology and computer science.

69
00:04:39,520 --> 00:04:40,720
So it seemed natural.

70
00:04:40,720 --> 00:04:45,000
So I decided to go back and get my PhD, but my undergraduate was a bachelor.

71
00:04:45,000 --> 00:04:46,480
And when did you do that?

72
00:04:46,480 --> 00:04:50,680
Well, I went back in 2003.

73
00:04:50,680 --> 00:04:55,120
I went back to get my masters because my undergraduate was in psychology.

74
00:04:55,120 --> 00:04:58,320
So I couldn't get into a PhD program straight away.

75
00:04:58,320 --> 00:05:01,360
So I had to take calculus and all that kind of stuff and all the kids.

76
00:05:01,360 --> 00:05:02,360
Yeah.

77
00:05:02,360 --> 00:05:07,120
The masters at UT Dallas and then got into the PhD program at UT Austin and started

78
00:05:07,120 --> 00:05:08,320
working with Ben Kipers.

79
00:05:08,320 --> 00:05:09,320
Okay.

80
00:05:09,320 --> 00:05:10,920
And so what was the focus of your research there?

81
00:05:10,920 --> 00:05:11,920
Yeah.

82
00:05:11,920 --> 00:05:13,160
My focus was on developmental robotics.

83
00:05:13,160 --> 00:05:17,760
So how can you get a robot to learn about the world in the same way a child does?

84
00:05:17,760 --> 00:05:24,280
And so the idea is the robot just wakes up or is born and has some knowledge built in,

85
00:05:24,280 --> 00:05:25,280
but not much.

86
00:05:25,280 --> 00:05:29,960
And it wants to build it all up from the beginning and the robot pushes objects around

87
00:05:29,960 --> 00:05:37,560
and learns relationships between its hand and the object and it learns how to form actions

88
00:05:37,560 --> 00:05:42,400
and how to build up perceptions like, oh, my hand is to the left of the block.

89
00:05:42,400 --> 00:05:46,880
That's actually significant because that determines when I can hit it to the right.

90
00:05:46,880 --> 00:05:49,120
And so it built it up that way.

91
00:05:49,120 --> 00:05:55,080
And then I finished that up and graduated in 2010.

92
00:05:55,080 --> 00:06:02,040
And at that time, AI was not hot like it is now and it's amazing to change.

93
00:06:02,040 --> 00:06:08,920
And so I got a postdoc at Carnegie Mellon and I studied under Norman today and at the

94
00:06:08,920 --> 00:06:14,120
intersection between kind of computer science and human computer interaction.

95
00:06:14,120 --> 00:06:19,480
So we studied if you have this location device that gives you your location or excuse me

96
00:06:19,480 --> 00:06:25,240
that broadcast your location to family and friends, when exactly do you want to share your

97
00:06:25,240 --> 00:06:26,240
location?

98
00:06:26,240 --> 00:06:30,080
So at the time, this was somewhat novel being able to share your location.

99
00:06:30,080 --> 00:06:31,080
Right.

100
00:06:31,080 --> 00:06:34,400
And there were a lot of privacy issues around it then, like, of course, they're still

101
00:06:34,400 --> 00:06:35,400
hard now.

102
00:06:35,400 --> 00:06:40,840
And so the device would learn through interaction with you when you want to share based on who's

103
00:06:40,840 --> 00:06:44,320
asking and where you are in time of day.

104
00:06:44,320 --> 00:06:45,320
Interesting.

105
00:06:45,320 --> 00:06:46,320
Interesting.

106
00:06:46,320 --> 00:06:50,640
So my wife actually used a Texas gal and when I went to Pittsburgh, she didn't come with

107
00:06:50,640 --> 00:06:51,640
me.

108
00:06:51,640 --> 00:06:54,520
I flew back home every weekend.

109
00:06:54,520 --> 00:06:55,520
Oh, wow.

110
00:06:55,520 --> 00:06:56,520
Yeah.

111
00:06:56,520 --> 00:06:57,520
It was quite a deal.

112
00:06:57,520 --> 00:07:00,480
And so I flew back home every weekend and eventually she says, you know, we're not going

113
00:07:00,480 --> 00:07:01,480
to Pittsburgh.

114
00:07:01,480 --> 00:07:11,120
So I got a job in Austin at a small company called 21CT where we do defense contracting

115
00:07:11,120 --> 00:07:13,080
work for the Department of Defense.

116
00:07:13,080 --> 00:07:19,600
So data mining and at that job, I pushed into natural language processing because one

117
00:07:19,600 --> 00:07:26,800
problem I found with the development of robotics is it was really hard to get funding

118
00:07:26,800 --> 00:07:30,120
unless you're at a university because it's so far off.

119
00:07:30,120 --> 00:07:35,120
You know, we have robots and factories all over the place, but we don't want them staring

120
00:07:35,120 --> 00:07:37,640
at their navel and wondering about life.

121
00:07:37,640 --> 00:07:43,860
And so most of the funding, most of the push is towards robots that can do actual concrete

122
00:07:43,860 --> 00:07:46,040
things right now, right?

123
00:07:46,040 --> 00:07:49,800
And I'm more interested in the fundamental concepts below that.

124
00:07:49,800 --> 00:07:54,640
The fundamental concepts that enable a child to just grab the world and understand it.

125
00:07:54,640 --> 00:08:00,400
And so I saw that language might be a good kind of in between.

126
00:08:00,400 --> 00:08:02,480
So language is very important right now.

127
00:08:02,480 --> 00:08:05,320
It's very useful chat bots are a thing.

128
00:08:05,320 --> 00:08:09,000
Language interfaces are important, computers have to read tons of documents.

129
00:08:09,000 --> 00:08:15,560
I thought, okay, well, language might be a way that I can both feed my family and study

130
00:08:15,560 --> 00:08:17,200
the stuff I care about.

131
00:08:17,200 --> 00:08:22,080
And then of course, along the way, I found a deep grammar co-founded with Christian Storm.

132
00:08:22,080 --> 00:08:26,160
But that ties into my talk today because my talk today is about how can we go from natural

133
00:08:26,160 --> 00:08:32,120
language processing down to these fundamental concepts into real understanding.

134
00:08:32,120 --> 00:08:40,560
Because right now, natural language processing is kind of sad because we're just at the surface.

135
00:08:40,560 --> 00:08:41,560
We treat these tokens.

136
00:08:41,560 --> 00:08:43,320
I was amazed when I first got in the field.

137
00:08:43,320 --> 00:08:47,640
We treat these words, we tokenize in the words and maybe we parse, but we just have

138
00:08:47,640 --> 00:08:49,120
this string of tokens.

139
00:08:49,120 --> 00:08:51,920
And then we do stuff with the tokens.

140
00:08:51,920 --> 00:08:53,800
The computer has no idea what these tokens mean.

141
00:08:53,800 --> 00:08:56,360
We just look for patterns in the tokens.

142
00:08:56,360 --> 00:09:00,960
And so in my talk, I start off with a TF IDF, which you take a document and convert it

143
00:09:00,960 --> 00:09:02,400
into a vector.

144
00:09:02,400 --> 00:09:06,840
If you're vocabulary is 50,000 words, it's a 50,000 long vector.

145
00:09:06,840 --> 00:09:09,920
And you lose, you lose the word ordering.

146
00:09:09,920 --> 00:09:12,920
So man by its dog is the same as dog by its man.

147
00:09:12,920 --> 00:09:16,520
And TF IDF is term-frequency inverse document.

148
00:09:16,520 --> 00:09:17,520
That's right.

149
00:09:17,520 --> 00:09:21,680
So the term frequency is, you know, if ardvark shows up twice, then you get a two in the

150
00:09:21,680 --> 00:09:23,040
ardvark's one.

151
00:09:23,040 --> 00:09:27,520
And then you scale that with the inverse document frequency by how often ardvark shows

152
00:09:27,520 --> 00:09:29,040
up in your corpus.

153
00:09:29,040 --> 00:09:33,920
So the less frequently it shows up, the more important it is in the context of the document

154
00:09:33,920 --> 00:09:34,920
is that's the idea.

155
00:09:34,920 --> 00:09:35,920
That's right, that's right.

156
00:09:35,920 --> 00:09:41,720
And then that way, that vector helps discriminate that document better because it has that

157
00:09:41,720 --> 00:09:42,720
scaling.

158
00:09:42,720 --> 00:09:49,040
And so it's interesting that you start off by talking about, you know, the fact that

159
00:09:49,040 --> 00:09:55,960
NLP is not, you know, it's not based on a lot of inherent structure because previous

160
00:09:55,960 --> 00:09:59,400
conversations I've had with folks, I might, the general kind of understanding I've come

161
00:09:59,400 --> 00:10:04,880
into is that that's where, you know, that lack of structure, meaning taking a statistical

162
00:10:04,880 --> 00:10:10,280
approach as opposed to a linguistic approach, is been the source of all of the advancements

163
00:10:10,280 --> 00:10:15,040
in, or much of the advancements in NLP over the last few years.

164
00:10:15,040 --> 00:10:16,800
Do you disagree with that generally or?

165
00:10:16,800 --> 00:10:20,040
Well, it's definitely true that we've been able to do a lot of cool stuff.

166
00:10:20,040 --> 00:10:25,000
And so in my talk, I talk about two paths, the symbolic path, and the sub symbolic path,

167
00:10:25,000 --> 00:10:27,400
which is the deep learning stuff that everybody's doing now.

168
00:10:27,400 --> 00:10:28,400
Okay.

169
00:10:28,400 --> 00:10:33,240
And yeah, with deep learning, we're able to generalize across tokens.

170
00:10:33,240 --> 00:10:38,760
So one problem we had before, if you said, I got into my car and went to the store versus

171
00:10:38,760 --> 00:10:43,560
I got into my truck and went to the supermarket, those looked like very different sentences

172
00:10:43,560 --> 00:10:45,080
in TF IDF.

173
00:10:45,080 --> 00:10:49,720
And you had to manually go in and say truck and car are pretty similar.

174
00:10:49,720 --> 00:10:50,720
Yeah.

175
00:10:50,720 --> 00:10:53,200
And store and supermarket are pretty similar.

176
00:10:53,200 --> 00:10:58,360
And you can do that for a few things, but you just can't think of all of these possibilities.

177
00:10:58,360 --> 00:11:01,400
And deep learning is really great from that, the word to VEC.

178
00:11:01,400 --> 00:11:05,920
So everything's a vector, and it turns out that, of course, car and vehicle are going

179
00:11:05,920 --> 00:11:08,880
to be very similar in car and truck and supermarket and store.

180
00:11:08,880 --> 00:11:13,960
And so if you, instead of due to TF IDF, you do like a, you can just even average the

181
00:11:13,960 --> 00:11:18,520
word vectors, or you can do an RNN where the last state is the meaning of sentence, you're

182
00:11:18,520 --> 00:11:24,440
able to really capture similarity across sentences in a way you can't do as well with symbolic

183
00:11:24,440 --> 00:11:25,760
methods.

184
00:11:25,760 --> 00:11:30,360
But you still don't have any understanding there.

185
00:11:30,360 --> 00:11:36,200
So when you do word to VEC, what you're doing is you're learning a vector for a word based

186
00:11:36,200 --> 00:11:40,280
on the words that typically go around it.

187
00:11:40,280 --> 00:11:45,000
And so the algorithm is, is you go through your whole corpus and for every word in the

188
00:11:45,000 --> 00:11:49,200
corpus, you go through one by one, you take the vector for that word, and you push the

189
00:11:49,200 --> 00:11:53,000
vectors for the other words closer to it, and you push all the vectors for the other words

190
00:11:53,000 --> 00:11:56,440
that aren't close to it away, and then you move to the next one, and you keep doing

191
00:11:56,440 --> 00:11:59,800
that over and over again until you've converged.

192
00:11:59,800 --> 00:12:04,440
And that's great, but it only captures what people say.

193
00:12:04,440 --> 00:12:08,560
So most of the knowledge that's needed to understand language is so obvious that we

194
00:12:08,560 --> 00:12:10,320
never mentioned it.

195
00:12:10,320 --> 00:12:13,800
And so that kind of stuff just doesn't show up in word vectors.

196
00:12:13,800 --> 00:12:19,040
And so even when you get this vector at the end, you still not clear what to do with it.

197
00:12:19,040 --> 00:12:23,040
And so you think about some of the biggest advances have been, or most exciting ones

198
00:12:23,040 --> 00:12:29,840
have been in machine translation, the machine still has no idea what it's just spitting

199
00:12:29,840 --> 00:12:30,840
out tokens.

200
00:12:30,840 --> 00:12:37,000
It encodes it with the encoding RNN, and then the decoder, it spits out the next word

201
00:12:37,000 --> 00:12:40,840
based on the previous state, the previous word, and then if it has attention, all of the

202
00:12:40,840 --> 00:12:46,240
previous encodings in the encoder, but it's just a softmax putting out tokens, it doesn't

203
00:12:46,240 --> 00:12:53,200
have any understanding of what it's doing, which is in some degree why it's so applicable

204
00:12:53,200 --> 00:12:57,400
in so many different domains, you can create a parse tree with it, you can even encode

205
00:12:57,400 --> 00:13:03,280
a picture into a vector using using a CNN, and then run the decoder, and that's how you

206
00:13:03,280 --> 00:13:04,280
get this captioning work.

207
00:13:04,280 --> 00:13:09,160
That's really exciting, but there's still no understanding there.

208
00:13:09,160 --> 00:13:14,840
And so you end up with this vector, so now we're on the sub symbolic path, but what can

209
00:13:14,840 --> 00:13:15,840
you do?

210
00:13:15,840 --> 00:13:19,560
And so the next thing that people started doing was, well, so attention, what you're doing

211
00:13:19,560 --> 00:13:24,400
when they added attention to the encoder decoder method, you're, when you're about to generate

212
00:13:24,400 --> 00:13:29,440
a word in your translation, you're looking at all of the previous words in the sentence,

213
00:13:29,440 --> 00:13:35,840
or they're encoded representation, the hidden state representation of the sequence.

214
00:13:35,840 --> 00:13:39,600
And so what it's doing is it's looking at facts about the world to figure out which ones

215
00:13:39,600 --> 00:13:42,160
are relevant for generating the next word.

216
00:13:42,160 --> 00:13:47,040
And so what people started doing was they said, well, what if I just feed it in a story?

217
00:13:47,040 --> 00:13:52,080
And so I can feed it in a story where like Tom went to the store, Tom came home, Tom

218
00:13:52,080 --> 00:13:58,800
picked up a jar, Tom went to the airport, and now the question is, where is the jar?

219
00:13:58,800 --> 00:14:02,800
And if you feed it enough of these stories, it's pretty amazing.

220
00:14:02,800 --> 00:14:08,280
The computer can answer, it's at the airport, presuming that you never put down this jar

221
00:14:08,280 --> 00:14:11,760
that's just carry with you for your life.

222
00:14:11,760 --> 00:14:16,760
And that's really cool, but you have to generate these stories automatically.

223
00:14:16,760 --> 00:14:20,800
And the reason you have to generate them automatically is because you need so many stories

224
00:14:20,800 --> 00:14:24,560
that it needs to be able to find these statistical patterns underneath.

225
00:14:24,560 --> 00:14:25,560
Okay.

226
00:14:25,560 --> 00:14:31,640
And this mechanism that's enabling this as attention, can we maybe double click on that

227
00:14:31,640 --> 00:14:37,040
to talk about how that's implemented to maybe get a...

228
00:14:37,040 --> 00:14:41,360
I've heard attention come up a bunch of times, but I haven't dug into it in any level

229
00:14:41,360 --> 00:14:46,680
of detail, and I'm wondering how that manifests itself in some of these deep networks and stuff

230
00:14:46,680 --> 00:14:47,680
like that.

231
00:14:47,680 --> 00:14:48,680
Yeah.

232
00:14:48,680 --> 00:14:54,200
So I'm thinking that Google just came out with this new tensor to tensor thing, which is,

233
00:14:54,200 --> 00:14:55,680
and I'm thinking of how they do attention.

234
00:14:55,680 --> 00:15:00,880
So you have like a set of keys and a query and keys and values.

235
00:15:00,880 --> 00:15:06,720
And what you're doing is for some query, you're looking at all of the keys to find the

236
00:15:06,720 --> 00:15:10,400
most similar key, and then you take that value.

237
00:15:10,400 --> 00:15:16,200
And the similarity between the query and the key is the weight that you use for the value.

238
00:15:16,200 --> 00:15:19,200
So you end up doing a weighted average of the values.

239
00:15:19,200 --> 00:15:21,200
Is that implemented in a neural network?

240
00:15:21,200 --> 00:15:22,200
Are we talking about...

241
00:15:22,200 --> 00:15:26,440
There's an external structure like a database or a key value store or something there.

242
00:15:26,440 --> 00:15:27,680
No, it's all neural network.

243
00:15:27,680 --> 00:15:32,000
And so when I say key and query and value, these are all vectors.

244
00:15:32,000 --> 00:15:33,000
Okay.

245
00:15:33,000 --> 00:15:34,000
Yeah.

246
00:15:34,000 --> 00:15:35,000
Got it.

247
00:15:35,000 --> 00:15:40,680
And so in the sequence-to-sequence model, what you're looking at is these, and maybe

248
00:15:40,680 --> 00:15:45,480
that the keys and values are one and the same, but you're looking at where your query is

249
00:15:45,480 --> 00:15:47,600
my current state when I'm trying to generate.

250
00:15:47,600 --> 00:15:52,760
So, you know, you could have, I went to the store, and then you're translating to Spanish

251
00:15:52,760 --> 00:15:57,080
and Yofuya Super Mercado, and when you're trying to, my great accent, when you're trying

252
00:15:57,080 --> 00:16:03,360
to generate a Super Mercado, you look back at, I went to the, and you went to, you look

253
00:16:03,360 --> 00:16:10,200
at those encoded representations along the way, and you take your state at Super Mercado

254
00:16:10,200 --> 00:16:15,600
or at the state before you generate Super Mercado, and you compare how similar those are.

255
00:16:15,600 --> 00:16:21,080
And then you take the weighted average of those values, and then that value comes in to

256
00:16:21,080 --> 00:16:26,320
where you would normally generate Super Mercado, and that value is taken into account.

257
00:16:26,320 --> 00:16:30,920
This is just another vector along with the vector for the previous state in your decoder

258
00:16:30,920 --> 00:16:34,320
and the vector for the previous word you generated.

259
00:16:34,320 --> 00:16:39,400
And then for each vector, you have another matrix, which you multiply it by, and then you

260
00:16:39,400 --> 00:16:42,640
add those things up, and you throw that in the softmax, and then that's your output.

261
00:16:42,640 --> 00:16:43,640
Okay.

262
00:16:43,640 --> 00:16:49,560
Yeah, and so neural network at each point is generally a, or very often, a multiplication

263
00:16:49,560 --> 00:16:55,720
of a matrix by a vector, and then you put some nonlinearity on the result.

264
00:16:55,720 --> 00:16:56,720
So.

265
00:16:56,720 --> 00:16:57,720
Okay.

266
00:16:57,720 --> 00:17:03,760
So attention basically is, is your storing kind of, you're kind of storing up these vectors

267
00:17:03,760 --> 00:17:10,280
and referencing them from the past, essentially, to be able, and including those in your,

268
00:17:10,280 --> 00:17:11,760
your end calculation.

269
00:17:11,760 --> 00:17:12,760
That's right.

270
00:17:12,760 --> 00:17:17,000
And so what they're doing in the story generation, or excuse me, in a story question answering,

271
00:17:17,000 --> 00:17:21,360
is they're encoding the parts of the story as vectors.

272
00:17:21,360 --> 00:17:25,320
And then when they want to ask, answer a question, they go back and look at the parts of

273
00:17:25,320 --> 00:17:29,280
the story and figure out which parts of the story are most relevant to answering that

274
00:17:29,280 --> 00:17:30,280
question.

275
00:17:30,280 --> 00:17:33,120
And then they do a little computation on top of that, and that's, that's where your answer

276
00:17:33,120 --> 00:17:34,120
comes from.

277
00:17:34,120 --> 00:17:35,120
Interesting.

278
00:17:35,120 --> 00:17:40,640
And are there limitations to the amount of memory that you're able to refer back to?

279
00:17:40,640 --> 00:17:45,200
Well, so generally, there's not limitations in the amount of memory, but you're generally

280
00:17:45,200 --> 00:17:46,760
taking a weighted average.

281
00:17:46,760 --> 00:17:47,760
Mm-hmm.

282
00:17:47,760 --> 00:17:52,120
And you do that because if you just take kind of a hard attention, then you can't do the

283
00:17:52,120 --> 00:17:53,920
back propagation as well.

284
00:17:53,920 --> 00:17:58,520
And so you take a weighted average, so it, things kind of get watered down a little bit.

285
00:17:58,520 --> 00:18:02,480
But I don't think that's a huge problem.

286
00:18:02,480 --> 00:18:07,520
More of the problem is it's just a very simple mechanism, and it can only do so much.

287
00:18:07,520 --> 00:18:11,400
And I think that's where you were going before I kind of interrupted you to push into this

288
00:18:11,400 --> 00:18:17,400
attention, is starting to approximate things that look and feel like meaning, but not,

289
00:18:17,400 --> 00:18:18,400
it's still not quite there.

290
00:18:18,400 --> 00:18:22,560
Yeah, you're going back over your previous experiences and say, oh, this one's relevant.

291
00:18:22,560 --> 00:18:23,560
Right.

292
00:18:23,560 --> 00:18:26,480
Yeah, which is cool.

293
00:18:26,480 --> 00:18:29,000
But the robot doesn't have any previous experiences.

294
00:18:29,000 --> 00:18:34,480
So these story generating, or these story question answering systems are really cool,

295
00:18:34,480 --> 00:18:37,200
but there's no built-in knowledge.

296
00:18:37,200 --> 00:18:38,200
Right.

297
00:18:38,200 --> 00:18:42,880
So when we answer questions about stories, we bring a whole lifetime of knowledge.

298
00:18:42,880 --> 00:18:44,600
And these all start from scratch.

299
00:18:44,600 --> 00:18:51,440
And what we need to do, I think the next step on the sub symbolic path is we need to have

300
00:18:51,440 --> 00:18:56,920
systems that interact in our world with the objects and relationships in our world.

301
00:18:56,920 --> 00:19:02,440
And so you can imagine like a little robot that can pick up objects and move them around.

302
00:19:02,440 --> 00:19:08,120
And then it knows what a bottle is, because a bottle is partially the hand fixture that

303
00:19:08,120 --> 00:19:09,360
it needs in order to pick it up.

304
00:19:09,360 --> 00:19:13,120
A bottle is partially that, if it knocks it off the table of bricks, a bottle is partially

305
00:19:13,120 --> 00:19:15,720
that that turns it to the side.

306
00:19:15,720 --> 00:19:16,720
Water comes out.

307
00:19:16,720 --> 00:19:19,080
All these things are part of the definition of a bottle.

308
00:19:19,080 --> 00:19:24,760
And so when it's pulling up memory, it's not just pulling up parts of a story.

309
00:19:24,760 --> 00:19:30,520
It's pulling up huge banks of things that it is experienced before.

310
00:19:30,520 --> 00:19:34,400
And then you can make inference from that that you wouldn't be able to otherwise.

311
00:19:34,400 --> 00:19:40,240
Now we don't quite know how to do these advanced inferences based on experience other than

312
00:19:40,240 --> 00:19:45,360
the kind of basic models we have now, which is like sequels of sequence and CNN and some

313
00:19:45,360 --> 00:19:46,360
other ones.

314
00:19:46,360 --> 00:19:50,280
And it's going to be exciting to see one of my, one of the things I really enjoy about

315
00:19:50,280 --> 00:19:55,120
the deep learning is every time a new configuration comes out or you know, a new one that goes

316
00:19:55,120 --> 00:19:56,120
into zoo.

317
00:19:56,120 --> 00:19:57,680
I'm like, oh cool, we're getting a little closer.

318
00:19:57,680 --> 00:19:58,680
Right.

319
00:19:58,680 --> 00:20:02,680
I envision in the brain, you know, there's just thousands upon thousands of different kinds

320
00:20:02,680 --> 00:20:08,200
of configurations and neurons and at least to some approximation, one of them might be

321
00:20:08,200 --> 00:20:09,200
a sequence of sequence.

322
00:20:09,200 --> 00:20:12,880
And another one might be a CNN, but there's, you know, hundreds more that we haven't discovered.

323
00:20:12,880 --> 00:20:13,880
Right.

324
00:20:13,880 --> 00:20:14,880
Right.

325
00:20:14,880 --> 00:20:17,760
It would be cool as we get better and better with each new one.

326
00:20:17,760 --> 00:20:23,840
So I think most of what we covered now, I mean, it sounds like a lead up to, you know,

327
00:20:23,840 --> 00:20:30,880
a specific area of research or interest that you have that kind of promises to help address

328
00:20:30,880 --> 00:20:31,880
this issue.

329
00:20:31,880 --> 00:20:37,040
Like where does it, where do we go from the sub symbolic, maybe another way to ask this

330
00:20:37,040 --> 00:20:42,920
is, is it your observation that a more symbolic approach is kind of the answer to the ills

331
00:20:42,920 --> 00:20:46,960
of the sub symbolic approach, or do you think the path forward is still sub symbolic,

332
00:20:46,960 --> 00:20:51,760
but extending it to incorporate more understanding?

333
00:20:51,760 --> 00:20:54,120
I don't know.

334
00:20:54,120 --> 00:20:55,120
Yeah.

335
00:20:55,120 --> 00:20:59,760
So in my talk, I cover both approaches as if they're separate approaches.

336
00:20:59,760 --> 00:21:05,520
And there's been surprisingly little overlap in the approaches.

337
00:21:05,520 --> 00:21:10,320
And what we've talked about the sub symbolic mostly, have we talked, have you, should we

338
00:21:10,320 --> 00:21:13,080
take a few minutes talking about the symbolic stuff and what's happening there?

339
00:21:13,080 --> 00:21:14,080
Sure.

340
00:21:14,080 --> 00:21:15,080
Sure.

341
00:21:15,080 --> 00:21:16,080
We can do that.

342
00:21:16,080 --> 00:21:19,560
So we can, yes, we were talking about a TIF IDF factor and that is, it throws out word

343
00:21:19,560 --> 00:21:21,400
order.

344
00:21:21,400 --> 00:21:22,440
But you can do a lot of stuff with it.

345
00:21:22,440 --> 00:21:25,080
You can say this document is similar to this other document.

346
00:21:25,080 --> 00:21:29,440
You can even throw it into machine learning classifier and do sentiment analysis or document

347
00:21:29,440 --> 00:21:31,360
classification.

348
00:21:31,360 --> 00:21:32,760
And that's pretty neat.

349
00:21:32,760 --> 00:21:34,760
And I mentioned sentiment analysis.

350
00:21:34,760 --> 00:21:41,800
So the next step in sentiment analysis, getting a little closer to actual meaning is a sentiment

351
00:21:41,800 --> 00:21:42,800
dictionary.

352
00:21:42,800 --> 00:21:43,800
Okay.

353
00:21:43,800 --> 00:21:48,720
So often you'll have a dictionary that says, okay, the word terrible has a negative sentiment.

354
00:21:48,720 --> 00:21:51,800
And the word good has a positive sentiment.

355
00:21:51,800 --> 00:21:55,680
And you have some simple mechanism that says, well, not terrible.

356
00:21:55,680 --> 00:21:58,400
You have the inverse, reverse to word.

357
00:21:58,400 --> 00:22:00,400
And that can get you pretty far.

358
00:22:00,400 --> 00:22:04,080
And but you're, for each symbol now, you're going into your dictionary and you're assigning

359
00:22:04,080 --> 00:22:05,680
some very simple meaning.

360
00:22:05,680 --> 00:22:10,320
So that's kind of the first step to assigning meaning or one, you could consider it one

361
00:22:10,320 --> 00:22:11,640
first step.

362
00:22:11,640 --> 00:22:15,160
But there's also a whole set of representations that people have built.

363
00:22:15,160 --> 00:22:19,520
And so when you build a representation, what you're doing is you're taking symbols and

364
00:22:19,520 --> 00:22:22,200
you're creating relationships just between symbols.

365
00:22:22,200 --> 00:22:26,640
And then presumably if you set up the symbol system, you can map what people say to the

366
00:22:26,640 --> 00:22:32,240
symbol and then you can map what the computer should do based on whatever symbol got lit

367
00:22:32,240 --> 00:22:33,240
up.

368
00:22:33,240 --> 00:22:37,440
So if you had like, let's say you're a tire company and you want to watch Twitter to

369
00:22:37,440 --> 00:22:41,680
see who you should try to sell tires to, you could mention anybody, you could have set

370
00:22:41,680 --> 00:22:49,000
up a symbol system where it says, okay, a car has tires, a truck has tires, you know, Toyota

371
00:22:49,000 --> 00:22:50,600
is a kind of car.

372
00:22:50,600 --> 00:22:54,160
And therefore anybody mentions a Toyota, it links in the tires.

373
00:22:54,160 --> 00:22:55,160
Okay.

374
00:22:55,160 --> 00:22:58,880
And then you can just, that helps you when you take a sentence, you say, given the sentence,

375
00:22:58,880 --> 00:23:02,280
can I find tires linked anywhere in there?

376
00:23:02,280 --> 00:23:03,920
And I don't mention tires explicitly.

377
00:23:03,920 --> 00:23:07,800
But of course, this is kind of brittle and there's been a lot of work in setting up these

378
00:23:07,800 --> 00:23:08,800
symbol systems.

379
00:23:08,800 --> 00:23:14,240
The most famous is probably wordnet, okay, where you have these set of synths, which is

380
00:23:14,240 --> 00:23:17,520
a synth set is a group of words that all mean the same thing.

381
00:23:17,520 --> 00:23:18,920
It's like a meaning.

382
00:23:18,920 --> 00:23:24,400
And so vehicle might be one synth set in that vehicle, you'd have, well, maybe car is

383
00:23:24,400 --> 00:23:25,400
one synth set.

384
00:23:25,400 --> 00:23:30,600
And then car, you'd have motor car, car, and you could have, you know, a car in all different

385
00:23:30,600 --> 00:23:33,160
languages, but it means car.

386
00:23:33,160 --> 00:23:37,880
And then you set up relationships between these things like car is a kind of vehicle.

387
00:23:37,880 --> 00:23:40,840
And then you would have sports car as a subset of that.

388
00:23:40,840 --> 00:23:42,760
And so wordnet is really popular and it's really good.

389
00:23:42,760 --> 00:23:47,200
It kind of gives a, it kind of gives a sense of a definition for most words.

390
00:23:47,200 --> 00:23:50,120
And you can also have a word being two different synths set.

391
00:23:50,120 --> 00:23:56,680
So bank would be in the synth set for river bank, but also a bank where you deposit money.

392
00:23:56,680 --> 00:23:57,680
Okay.

393
00:23:57,680 --> 00:24:00,960
That's one, another one is frame net.

394
00:24:00,960 --> 00:24:04,560
And so frame net builds up a little bigger situation.

395
00:24:04,560 --> 00:24:07,840
So wordnet is about individual words.

396
00:24:07,840 --> 00:24:09,160
Frame net is about situation.

397
00:24:09,160 --> 00:24:16,320
So one example is the frame commerce by, which means somebody buys something from someone

398
00:24:16,320 --> 00:24:17,320
else.

399
00:24:17,320 --> 00:24:23,840
And so that frame is triggered by some set of keywords, like bought, purchased, sold.

400
00:24:23,840 --> 00:24:27,360
And when that frame is triggered, what frame net does, or at least an implementation that

401
00:24:27,360 --> 00:24:30,160
uses frame net, goes into tries to find the roles.

402
00:24:30,160 --> 00:24:31,160
Who was the buyer?

403
00:24:31,160 --> 00:24:32,160
Could be Bob.

404
00:24:32,160 --> 00:24:33,560
Bob bought a car from Tom.

405
00:24:33,560 --> 00:24:34,560
Okay.

406
00:24:34,560 --> 00:24:39,640
Buyers, Bob, the seller is Tom and the thing purchases car.

407
00:24:39,640 --> 00:24:46,600
And so you've converted this sentence into a frame with roles and now that's machine understandable.

408
00:24:46,600 --> 00:24:47,600
Okay.

409
00:24:47,600 --> 00:24:51,520
And it's kind of nice because you move up from individual words into kind of meaning

410
00:24:51,520 --> 00:24:52,520
of situations.

411
00:24:52,520 --> 00:24:53,520
Yes.

412
00:24:53,520 --> 00:24:54,520
Yeah.

413
00:24:54,520 --> 00:24:56,400
But frame net doesn't go very deep.

414
00:24:56,400 --> 00:25:01,520
You know, if frame net doesn't say, you don't have like the fundamental things going on,

415
00:25:01,520 --> 00:25:05,000
like forces and, and well, there's a little bit of that.

416
00:25:05,000 --> 00:25:07,800
But you don't have the things that a child knows, a very young child.

417
00:25:07,800 --> 00:25:10,840
And it turns out in, in AI, that's the hardest part.

418
00:25:10,840 --> 00:25:15,080
You know, we started off thinking that chess was the pinnacle of intelligence.

419
00:25:15,080 --> 00:25:21,840
And now it turns out that picking up a bottle of water is really hard.

420
00:25:21,840 --> 00:25:23,680
And who would have thought?

421
00:25:23,680 --> 00:25:29,000
So all the things, you know, they say, all of the things a kid knows by three or four,

422
00:25:29,000 --> 00:25:32,560
if we could get those in their computer, that would just be amazing.

423
00:25:32,560 --> 00:25:35,640
And that's what I would really love to try to do.

424
00:25:35,640 --> 00:25:39,840
And so if we're going to build that with a symbol system, we have to go deeper.

425
00:25:39,840 --> 00:25:43,400
And so one symbol system that does go deeper is Sumo.

426
00:25:43,400 --> 00:25:49,120
And so what Sumo is is, is a full ontology, meaning it goes all the way down.

427
00:25:49,120 --> 00:25:52,560
And so you look at like cooking, what does the word cooking mean?

428
00:25:52,560 --> 00:25:58,320
Well cooking is, I don't remember the exact, the exact thing, but cooking is a process,

429
00:25:58,320 --> 00:26:03,400
which is a thing, which is an entity, it goes all the way, it takes it all the way down.

430
00:26:03,400 --> 00:26:10,560
And so, so that's really useful, but what we need to do now is figure out how we can get,

431
00:26:10,560 --> 00:26:13,080
how we can get things like Sumo tied in the word net.

432
00:26:13,080 --> 00:26:16,800
And there has been some, some linkages, Sumo already does tie in word net.

433
00:26:16,800 --> 00:26:21,120
But how we can get all these different representations together, because what we want to do next

434
00:26:21,120 --> 00:26:26,600
is build, if we're staying in a symbolic land, build a causal model of how the world works.

435
00:26:26,600 --> 00:26:30,440
And here at this conference, Josh Chenabong yesterday was talking about that.

436
00:26:30,440 --> 00:26:36,840
And so we need, you know, an entity needs to understand that when it pushes a table, all

437
00:26:36,840 --> 00:26:39,800
the things on top of the table are going to move.

438
00:26:39,800 --> 00:26:43,440
And if you try to put that in logic, it's hard.

439
00:26:43,440 --> 00:26:47,040
And you need like a model where you can just read it off the model.

440
00:26:47,040 --> 00:26:49,960
So in some sense, frame net is kind of like that.

441
00:26:49,960 --> 00:26:57,800
So if, if there's a frame where Bob sold a car to Tom, and then you ask, well, who has

442
00:26:57,800 --> 00:26:59,760
the car afterwards?

443
00:26:59,760 --> 00:27:00,760
It's Tom.

444
00:27:00,760 --> 00:27:04,200
You can just read it right off the frame, or you can have that associated with the frame.

445
00:27:04,200 --> 00:27:06,760
You can put that in with the frame.

446
00:27:06,760 --> 00:27:10,720
And so what we need to do is build deep causal models that go all the way down to these things

447
00:27:10,720 --> 00:27:16,280
called image schemas, or image schemas are the language independent concepts that we

448
00:27:16,280 --> 00:27:20,960
use to understand everything in our world. So like Lake often Johnson and, and these kind

449
00:27:20,960 --> 00:27:22,400
of guys, Manler.

450
00:27:22,400 --> 00:27:26,760
And so you put a, she's a psychologist and you put a developmental psychologist.

451
00:27:26,760 --> 00:27:30,040
And so like one is containment.

452
00:27:30,040 --> 00:27:34,480
So when you have a bottle of water, the water is contained in the bottle, which means

453
00:27:34,480 --> 00:27:37,400
that if you move the bottle, the water goes along with it.

454
00:27:37,400 --> 00:27:38,400
Another support.

455
00:27:38,400 --> 00:27:40,320
So the bottle is on the table.

456
00:27:40,320 --> 00:27:45,040
And so you need these concepts before you can understand language, because the language

457
00:27:45,040 --> 00:27:47,560
understanding is built on all this stuff.

458
00:27:47,560 --> 00:27:51,000
When we talk to each other, we never, we never say these things.

459
00:27:51,000 --> 00:27:52,000
Sure.

460
00:27:52,000 --> 00:27:57,040
So yeah, one of my, one joke I like to say is you can imagine a romance novel where there's

461
00:27:57,040 --> 00:28:01,360
a table in between two lovers, and the man pushes the table aside, and then the novel

462
00:28:01,360 --> 00:28:05,000
they would never say, and as he pushes the table aside, all the objects on the table mood

463
00:28:05,000 --> 00:28:06,000
because they were spushing down.

464
00:28:06,000 --> 00:28:07,000
Right.

465
00:28:07,000 --> 00:28:08,000
Like that's just not in there.

466
00:28:08,000 --> 00:28:11,680
And it was a sound of the light scraping across the floor, right?

467
00:28:11,680 --> 00:28:19,320
That produces gashes in the floor, and suddenly the objects were at a new location.

468
00:28:19,320 --> 00:28:21,320
We make a lot of assumptions when we talk.

469
00:28:21,320 --> 00:28:22,320
We do.

470
00:28:22,320 --> 00:28:24,440
I mean, if we didn't, we'd never get anything done.

471
00:28:24,440 --> 00:28:25,440
Right.

472
00:28:25,440 --> 00:28:26,440
Yeah.

473
00:28:26,440 --> 00:28:30,360
And so, so what we need to do is build up cause and models of the world onto which we

474
00:28:30,360 --> 00:28:32,400
can put these symbols that we define.

475
00:28:32,400 --> 00:28:38,720
I can't decide if it would be fun or extremely boring and tedious to run these models in

476
00:28:38,720 --> 00:28:41,720
reverse and generate that romance novel.

477
00:28:41,720 --> 00:28:45,720
That'd be the worst novel ever.

478
00:28:45,720 --> 00:28:48,520
That'd be awesome.

479
00:28:48,520 --> 00:28:52,880
It's like, you know, you have the, you have different editions of books, the big print,

480
00:28:52,880 --> 00:28:56,120
and then this is the computer edition.

481
00:28:56,120 --> 00:28:57,120
Yeah.

482
00:28:57,120 --> 00:28:58,120
Yeah.

483
00:28:58,120 --> 00:29:03,520
And so those are, those are basically the two paths and to get from where we are now

484
00:29:03,520 --> 00:29:07,400
in natural language processing, which is just working at the symbols either with the

485
00:29:07,400 --> 00:29:11,520
sum symbolic, where we're able to learn vectors for these individual things or the symbolic

486
00:29:11,520 --> 00:29:15,680
where we just write down what they are in the computer that can really understand because

487
00:29:15,680 --> 00:29:17,560
it understands the fundamentals.

488
00:29:17,560 --> 00:29:25,520
So it's hard to say where to go from here because one problem is you need to find a commercially

489
00:29:25,520 --> 00:29:31,520
viable need for the simplest possible common sense knowledge.

490
00:29:31,520 --> 00:29:40,840
So we all have chatbots everywhere now, but they require already way too much knowledge

491
00:29:40,840 --> 00:29:41,840
to be good.

492
00:29:41,840 --> 00:29:45,520
If you go out out of, if you stay within a particular domain and you basically just

493
00:29:45,520 --> 00:29:50,320
hard code everything, and you can, and you can have chatbots where it's learned using

494
00:29:50,320 --> 00:29:53,480
sequence sequence models, but that's just gibberish back and forth.

495
00:29:53,480 --> 00:29:54,840
It's no different from Eliza, really.

496
00:29:54,840 --> 00:29:59,520
If we have a chatbot that actually is general, if you ask it things off script and can answer

497
00:29:59,520 --> 00:30:02,880
your questions, we're going to need these fundamental concepts.

498
00:30:02,880 --> 00:30:10,560
In fact, one of my dreams is to build a chatbot for children that you get at age three or

499
00:30:10,560 --> 00:30:13,640
four, and it lives in your mom's phone.

500
00:30:13,640 --> 00:30:18,480
And it teaches you concepts about the world, and it's also your friend, and it learns

501
00:30:18,480 --> 00:30:19,800
about you.

502
00:30:19,800 --> 00:30:25,440
And the cool thing about being a teacher is that if it teaches you, then it knows what

503
00:30:25,440 --> 00:30:26,440
you know.

504
00:30:26,440 --> 00:30:29,600
So then it explains other things to you.

505
00:30:29,600 --> 00:30:34,040
It can explain it to you and things you understand in terms you already understand.

506
00:30:34,040 --> 00:30:37,640
And it can also make things interesting, because let's say it knows that your favorite animals

507
00:30:37,640 --> 00:30:38,640
are draft.

508
00:30:38,640 --> 00:30:42,240
And I can say, when it teaches you math, I can say, if you have six drafts, and you buy

509
00:30:42,240 --> 00:30:44,760
two more, how many do you have?

510
00:30:44,760 --> 00:30:48,680
And that's the kind of thing that engaged parents do.

511
00:30:48,680 --> 00:30:51,800
And it would be cool to do that in an app, and I have this dream that you put that in

512
00:30:51,800 --> 00:30:55,520
for children, and you have your developers feverishly working behind the scenes, making better

513
00:30:55,520 --> 00:31:00,160
and better and better technology, so that when a child gets older, the app just turns

514
00:31:00,160 --> 00:31:02,640
into the operating system for the child.

515
00:31:02,640 --> 00:31:07,400
And so the child now uses this app to interface with its whole world.

516
00:31:07,400 --> 00:31:11,520
And since the app has been with the child since the beginning, the app really knows the

517
00:31:11,520 --> 00:31:15,240
child, and so it can be the ultimate and customized.

518
00:31:15,240 --> 00:31:19,280
And when it's, you know, and then as an adult, when it's guiding me through how to fix

519
00:31:19,280 --> 00:31:23,800
my dishwasher, it knows that I know nothing, and it literally has to tell me, lefty Lucy

520
00:31:23,800 --> 00:31:29,720
Rady Tidy, remember, as opposed to going Wikipedia or on the web, you just have no idea.

521
00:31:29,720 --> 00:31:34,400
Yeah, and then even when you're old, you know, if you become, your faculty start to go,

522
00:31:34,400 --> 00:31:37,800
and if you're standing in the kitchen, you can't remember how to make coffee, you know,

523
00:31:37,800 --> 00:31:42,760
the app can then be in the cameras in the room and say, hey, you make coffee this time

524
00:31:42,760 --> 00:31:43,760
of day.

525
00:31:43,760 --> 00:31:44,760
The filters are in the cupboard over there.

526
00:31:44,760 --> 00:31:45,760
That's the first step.

527
00:31:45,760 --> 00:31:49,280
And it guides you through, and maybe we could stay independent longer.

528
00:31:49,280 --> 00:31:50,280
That's...

529
00:31:50,280 --> 00:31:51,280
I would have great vision for an app.

530
00:31:51,280 --> 00:31:52,880
That would be awesome, yeah.

531
00:31:52,880 --> 00:31:56,280
Now, how much of that is deep grammar trying to take on?

532
00:31:56,280 --> 00:31:57,280
None.

533
00:31:57,280 --> 00:31:59,120
So, yes.

534
00:31:59,120 --> 00:32:01,440
Deep grammar is trying to take on.

535
00:32:01,440 --> 00:32:03,880
When I write, I make a lot of really dumb mistakes.

536
00:32:03,880 --> 00:32:04,880
Okay.

537
00:32:04,880 --> 00:32:06,760
It's just the human in me, right?

538
00:32:06,760 --> 00:32:10,400
I think one thing in my hands just outputs something different.

539
00:32:10,400 --> 00:32:15,760
And I've always been amazed that grammar checkers couldn't capture that.

540
00:32:15,760 --> 00:32:18,760
And you know, spell checkers came along and they were amazing.

541
00:32:18,760 --> 00:32:19,760
They really...

542
00:32:19,760 --> 00:32:22,800
I don't know how many people around remember days with four spell checkers, but it was

543
00:32:22,800 --> 00:32:23,800
a huge advance.

544
00:32:23,800 --> 00:32:24,800
Yeah.

545
00:32:24,800 --> 00:32:27,120
And, you know, I always told my teachers, I'm not going to have to know how to spell.

546
00:32:27,120 --> 00:32:32,800
And it turns out, I was right about one thing, very few times was I right, but that

547
00:32:32,800 --> 00:32:33,800
I was right.

548
00:32:33,800 --> 00:32:37,680
So, the grammar checkers always, you know, they were in word for a long time and they

549
00:32:37,680 --> 00:32:42,080
just would miss obvious wrong stuff, and it really bugged me.

550
00:32:42,080 --> 00:32:44,560
And I always thought machine learning would be the way to go.

551
00:32:44,560 --> 00:32:49,880
And so, I started working with endgrams, which is sequences of tokens, this is a few years

552
00:32:49,880 --> 00:32:50,880
ago.

553
00:32:50,880 --> 00:32:55,720
And it turns out, if you think about it for five minutes, it turns out that, you know,

554
00:32:55,720 --> 00:33:00,240
for endgrams of like sequences of three or four, if you take the, you know, I went to

555
00:33:00,240 --> 00:33:03,360
the, that's like four words.

556
00:33:03,360 --> 00:33:05,880
And then you have a distribution over the next word.

557
00:33:05,880 --> 00:33:12,120
And so, if you write a word that is not in that distribution, or not, you know, doesn't

558
00:33:12,120 --> 00:33:17,440
have a lot of weight in that distribution like donkey, but it's similar to a word that

559
00:33:17,440 --> 00:33:21,440
should have high weight like store, although donkey and store are similar, then you've

560
00:33:21,440 --> 00:33:23,120
probably made a mistake.

561
00:33:23,120 --> 00:33:27,880
So, if it's a, I went to the store, right, that's a mistake.

562
00:33:27,880 --> 00:33:28,880
It should be obvious.

563
00:33:28,880 --> 00:33:32,160
Stored is very different, very similar to store, and store is going to have a very

564
00:33:32,160 --> 00:33:33,760
low probability.

565
00:33:33,760 --> 00:33:39,360
The problem with endgrams is you can't, it's that similarity problem talked about before,

566
00:33:39,360 --> 00:33:45,680
because I went to this, I went to the store is, you know, I went, I drove, I meandered,

567
00:33:45,680 --> 00:33:50,480
I walked, all those are very different to an endgram probability thing.

568
00:33:50,480 --> 00:33:56,280
And so, in order to train such a thing, you would have to have seen all these things.

569
00:33:56,280 --> 00:34:03,000
And you'd have to store the vocabulary size to the fourth to store all this probability.

570
00:34:03,000 --> 00:34:07,760
And so, when I started working in deep learning, and I said, oh, sequence of sequences, the

571
00:34:07,760 --> 00:34:08,760
way to go for this.

572
00:34:08,760 --> 00:34:13,600
You're in code this thing, and then you decode it, and you get the power of deep learning

573
00:34:13,600 --> 00:34:17,920
that power we talked about before, that similar words are going to have similar vectors.

574
00:34:17,920 --> 00:34:21,640
And similar sentences are going to have similar vectors to other similar sentences.

575
00:34:21,640 --> 00:34:24,560
You know, the first thing is, oh, okay, I got to write a patent on this, this is, this

576
00:34:24,560 --> 00:34:27,920
is going to be how we're going to do grammar checking, and that, that's how we got started.

577
00:34:27,920 --> 00:34:28,920
Yeah.

578
00:34:28,920 --> 00:34:33,040
And so, what we do is we encode it, and then we decode, and if the thing we decode is different

579
00:34:33,040 --> 00:34:37,520
from what you wrote, then there's a problem, especially if what you wrote is different

580
00:34:37,520 --> 00:34:41,320
than it's similar to something that would have high probability.

581
00:34:41,320 --> 00:34:42,320
Okay.

582
00:34:42,320 --> 00:34:45,800
And then we also sort of, how do you capture that similarity?

583
00:34:45,800 --> 00:34:50,280
We do, it's just typical, typical, you know, the easiest thing you can do is like, what

584
00:34:50,280 --> 00:34:54,720
Levinstein distance, which is edit distance on the letters, you can do that with similarity,

585
00:34:54,720 --> 00:34:58,560
but there's also a bunch of other little similarity things we do, okay, that we take advantage

586
00:34:58,560 --> 00:35:02,000
of a lot of the acquired knowledge over the years and grammar.

587
00:35:02,000 --> 00:35:06,040
So we, yeah, we have a kind of sophisticated similarity measure, and then in addition

588
00:35:06,040 --> 00:35:11,120
to sequence sequence, we throw the kitchen sink of deep learning at it, a bunch of different

589
00:35:11,120 --> 00:35:15,600
CNNs and stuff, and so we've got it pretty good now.

590
00:35:15,600 --> 00:35:21,320
So sometimes it still fails in a way that's disturbing, but if you make a mistake like

591
00:35:21,320 --> 00:35:27,400
the wrong version of there or 222 is really good at that, it can catch it better than anything.

592
00:35:27,400 --> 00:35:28,400
Oh, wow.

593
00:35:28,400 --> 00:35:31,640
So, there's a lot of different markets, so the biggest market, as you might imagine, is

594
00:35:31,640 --> 00:35:36,520
English as a second language, but, and we get people all the time emailing me, please

595
00:35:36,520 --> 00:35:39,240
get to sing going, please, please, please, you know.

596
00:35:39,240 --> 00:35:44,800
The English as a second language is particularly challenging, because sometimes when you're

597
00:35:44,800 --> 00:35:50,240
not familiar with language, which you write is so far from correct, that the machine just

598
00:35:50,240 --> 00:35:54,640
can't, it just doesn't know where to start, and so that's a particular challenge.

599
00:35:54,640 --> 00:35:59,040
And then sometimes there's even a bigger fundamental challenge that the whole sentence has

600
00:35:59,040 --> 00:36:01,040
to be rewritten.

601
00:36:01,040 --> 00:36:05,440
And the only way to do that is to understand what the person said, and we've been talking

602
00:36:05,440 --> 00:36:09,200
this whole whole interview about how computers just can't do that, right?

603
00:36:09,200 --> 00:36:15,560
So that's going to be a problem for a lot of, a lot of time to come, but we can finally,

604
00:36:15,560 --> 00:36:19,160
these dumb mistakes I look at them, I can't believe that the grammar tracker didn't catch

605
00:36:19,160 --> 00:36:20,160
that.

606
00:36:20,160 --> 00:36:21,160
Now it can catch those.

607
00:36:21,160 --> 00:36:22,840
So, so that's really exciting.

608
00:36:22,840 --> 00:36:31,160
Do you offer this as a service for folks, like I use a service for writing called Grammarly,

609
00:36:31,160 --> 00:36:37,400
which you may be familiar with, that does a decent job for some things, some aspects of

610
00:36:37,400 --> 00:36:43,360
their implementation are kind of bad, I don't, just they use it, the UI, user experience

611
00:36:43,360 --> 00:36:45,040
is kind of wonky.

612
00:36:45,040 --> 00:36:51,040
But I can imagine that as a go-to-market model, I can imagine more of a platform-ish approach

613
00:36:51,040 --> 00:36:55,960
where you're offering APIs to developers to build things around, how are you guys going

614
00:36:55,960 --> 00:36:56,960
at it?

615
00:36:56,960 --> 00:37:01,960
We are in the process of trying to decide where exactly we're going to focus because Grammarly

616
00:37:01,960 --> 00:37:07,040
is now really big, they've got a lot of smart people working, and it's going to be hard

617
00:37:07,040 --> 00:37:08,480
to go head-to-head with them.

618
00:37:08,480 --> 00:37:13,560
I think we've got some ideas that are really good, and I think we do some things better,

619
00:37:13,560 --> 00:37:16,960
but they're just hiring like crazy.

620
00:37:16,960 --> 00:37:22,640
You can start by plugging into any of the editing apps on the Mac, which they don't support,

621
00:37:22,640 --> 00:37:26,920
or I don't think that they plug into Google Docs or anything like that.

622
00:37:26,920 --> 00:37:31,000
So, there are some holes there, now, what kind of a note that gives you, that's another

623
00:37:31,000 --> 00:37:32,000
issue.

624
00:37:32,000 --> 00:37:33,000
And also, Grammarly is pretty expensive.

625
00:37:33,000 --> 00:37:36,960
It thinks like $10 a month, and there's a lot of people around the world who really need

626
00:37:36,960 --> 00:37:39,040
this, but $10 a month is a lot of money.

627
00:37:39,040 --> 00:37:43,240
So we can, if we have a service that's really good, better than things that used to be

628
00:37:43,240 --> 00:37:47,560
around before, but maybe we don't have all the bells and whistles at Grammarly, especially

629
00:37:47,560 --> 00:37:52,520
if we can fix those things that are really hard for a computer to catch.

630
00:37:52,520 --> 00:37:56,440
So Grammarly, looking at what they've done, it looks like they spend a lot of time implementing

631
00:37:56,440 --> 00:38:02,280
a lot of rules, like calmer rules, but we can catch the subtle things that just pure learning

632
00:38:02,280 --> 00:38:03,680
can find.

633
00:38:03,680 --> 00:38:06,680
And that's what a lot of people need, because when you're English as a second language,

634
00:38:06,680 --> 00:38:10,160
you don't have the ear that we do, the ear for the language.

635
00:38:10,160 --> 00:38:13,680
And the ESL market is really interesting.

636
00:38:13,680 --> 00:38:18,120
Language is a hobby of mine, and one of the apps that I've been using recently that I really

637
00:38:18,120 --> 00:38:20,320
enjoy is this app called Tandem.

638
00:38:20,320 --> 00:38:25,920
That basically allows you to, it's kind of a global language learning community, and there

639
00:38:25,920 --> 00:38:28,800
have been a bunch of these, but it's the best implemented by far.

640
00:38:28,800 --> 00:38:33,200
You basically go on this app, you tell it what language is your learning, and it'll match

641
00:38:33,200 --> 00:38:38,320
you with people, the people that speak those languages natively that are trying to learn

642
00:38:38,320 --> 00:38:40,480
your languages that you know.

643
00:38:40,480 --> 00:38:47,880
But you'll get in these conversations with folks, and depending on their level, I think

644
00:38:47,880 --> 00:38:52,600
the interesting conversations are when folks are beyond the, hey, I'm going to Google

645
00:38:52,600 --> 00:38:57,040
translate everything I want to say, because you know the failure mode, like you can spot

646
00:38:57,040 --> 00:38:59,680
those, you know, really quickly.

647
00:38:59,680 --> 00:39:04,200
But then there are folks that know enough English that they're just typing what they

648
00:39:04,200 --> 00:39:10,000
think is right, and sometimes it's a little hard to decipher, but most of the time you

649
00:39:10,000 --> 00:39:14,160
can kind of get what they're trying to say, they're just not saying it wrong.

650
00:39:14,160 --> 00:39:23,040
And if your stuff was plugged into this process, as like a kind of a side channel trainer

651
00:39:23,040 --> 00:39:28,800
or coach or something like that, I think it would, you know, the big challenge for language

652
00:39:28,800 --> 00:39:34,480
learners is like decreasing the cycle time of, you know, learning and iteration and accelerating

653
00:39:34,480 --> 00:39:35,480
the process.

654
00:39:35,480 --> 00:39:37,360
Something like that could be really interesting.

655
00:39:37,360 --> 00:39:38,360
Yeah.

656
00:39:38,360 --> 00:39:43,840
No, I hadn't thought of that as like a coach, you know, that you said this and maybe change

657
00:39:43,840 --> 00:39:44,840
it to this other thing.

658
00:39:44,840 --> 00:39:46,400
No, that's a good idea.

659
00:39:46,400 --> 00:39:50,920
Another place that's like video transcription is big now, and a lot of times you have to

660
00:39:50,920 --> 00:39:52,440
pay a human to do it.

661
00:39:52,440 --> 00:39:55,920
Well, it's done automatically, but then you need to pay a human to make sure it's done

662
00:39:55,920 --> 00:39:56,920
right.

663
00:39:56,920 --> 00:39:57,920
Right?

664
00:39:57,920 --> 00:40:00,800
Because you get this text out and sometimes it doesn't quite hear.

665
00:40:00,800 --> 00:40:03,600
And so that's very much like a grammar correction problem.

666
00:40:03,600 --> 00:40:04,600
So we could do that.

667
00:40:04,600 --> 00:40:08,960
Yeah, we're trying to say what exactly, what niche, you know, we should go make it cheap

668
00:40:08,960 --> 00:40:15,040
or make it an API, do transcription, maybe there's some publishers that have reached out

669
00:40:15,040 --> 00:40:19,200
to us, they say, look, we write, we send out all these books and we have to pay people

670
00:40:19,200 --> 00:40:21,280
to go in and read each one.

671
00:40:21,280 --> 00:40:26,440
And so if we use you guys, then we have to pay them, you know, we can have them do more

672
00:40:26,440 --> 00:40:31,240
books per person because they would have less, you know, less tedious stuff to do.

673
00:40:31,240 --> 00:40:32,760
You'd catch the idea of stuff.

674
00:40:32,760 --> 00:40:33,760
And so that's another option.

675
00:40:33,760 --> 00:40:37,880
So we're kind of standing at the crossroads right now, trying to figure out what we're

676
00:40:37,880 --> 00:40:39,040
going to do with it.

677
00:40:39,040 --> 00:40:44,920
Does the technology get into or give you the ability to address stylistic issues as

678
00:40:44,920 --> 00:40:47,960
opposed to correctness?

679
00:40:47,960 --> 00:40:55,400
It kind of does both at the same time, but it doesn't help you rewrite things.

680
00:40:55,400 --> 00:41:00,520
So it's basically going to help you write the way it was trained.

681
00:41:00,520 --> 00:41:06,280
So we started out training on Wikipedia, but then there was everything that it wanted

682
00:41:06,280 --> 00:41:08,760
to fix everything to be very Wikipedia.

683
00:41:08,760 --> 00:41:13,840
Well, you know, the thought that came to me was, you know, the artistic style transfer

684
00:41:13,840 --> 00:41:17,000
stuff where, you know, take this picture and make it Picasso-esque.

685
00:41:17,000 --> 00:41:18,000
Yeah.

686
00:41:18,000 --> 00:41:22,560
Like, you know, I'd love to take my writing and, you know, make it, you know, the form

687
00:41:22,560 --> 00:41:23,560
of some other author.

688
00:41:23,560 --> 00:41:24,560
Right?

689
00:41:24,560 --> 00:41:25,560
That would be cool.

690
00:41:25,560 --> 00:41:29,960
Now, so it doesn't work as well in languages, it doesn't vision because in language,

691
00:41:29,960 --> 00:41:33,320
you're making a set of discrete decisions.

692
00:41:33,320 --> 00:41:39,920
And so in vision, you have pixels which are much more amenable to small gradients.

693
00:41:39,920 --> 00:41:42,320
And that's why they've had such huge success with vision.

694
00:41:42,320 --> 00:41:43,320
And language is harder.

695
00:41:43,320 --> 00:41:45,880
They're starting to get some work in that area.

696
00:41:45,880 --> 00:41:49,520
So someone a new stuff is applying GANs to sequence of sequence models.

697
00:41:49,520 --> 00:41:54,680
And so what you do is instead of using the cost of generating each token while you're

698
00:41:54,680 --> 00:41:59,760
training in the decoder, you use some other measure of the sentence.

699
00:41:59,760 --> 00:42:04,480
And so, and again, it would be the probability based on some discriminator function, the

700
00:42:04,480 --> 00:42:08,240
probability that this is generated by the computer or by a human.

701
00:42:08,240 --> 00:42:14,320
And then you have to back prop, or well, you have to get that answer back into the system

702
00:42:14,320 --> 00:42:15,320
so it can learn.

703
00:42:15,320 --> 00:42:17,920
And that's usually done like with reinforcement learning.

704
00:42:17,920 --> 00:42:22,760
And that's not very efficient right now for language and it kind of works and there's

705
00:42:22,760 --> 00:42:26,360
a lot of advancements, but still got a ways to go.

706
00:42:26,360 --> 00:42:27,360
Okay.

707
00:42:27,360 --> 00:42:35,240
I just finished a report on industrial applications of AI, and ended up being like 30 pages.

708
00:42:35,240 --> 00:42:45,600
And I'd love to put that through like the Hemingway translator, or that will be awesome.

709
00:42:45,600 --> 00:42:46,600
Yeah.

710
00:42:46,600 --> 00:42:47,600
I think we'll get there.

711
00:42:47,600 --> 00:42:51,600
I can assure you the sentences, I think for Hemingway will be a lot shorter, I think

712
00:42:51,600 --> 00:42:52,600
shorter.

713
00:42:52,600 --> 00:42:53,600
Yeah.

714
00:42:53,600 --> 00:42:55,400
Or run on sentence kind of guy.

715
00:42:55,400 --> 00:42:58,400
Yeah, that would be great.

716
00:42:58,400 --> 00:42:59,800
And there is some of that.

717
00:42:59,800 --> 00:43:05,680
So if you, you know, you train the system on Hemingway, it's going to want to generate

718
00:43:05,680 --> 00:43:07,800
tokens that are Hemingwayish.

719
00:43:07,800 --> 00:43:12,240
So you feed your sentence in and it's going to translate it into be shorter and something

720
00:43:12,240 --> 00:43:13,640
about a fish probably.

721
00:43:13,640 --> 00:43:14,640
Mm-hmm.

722
00:43:14,640 --> 00:43:15,640
Nice.

723
00:43:15,640 --> 00:43:16,640
Awesome.

724
00:43:16,640 --> 00:43:21,120
Well, what's the best way for folks to kind of keep tabs on what you're up to and you

725
00:43:21,120 --> 00:43:25,360
know, follow along as you guys iterate on this model and figure stuff out.

726
00:43:25,360 --> 00:43:26,360
Yeah.

727
00:43:26,360 --> 00:43:28,360
So we have a website, dogrammer.com.

728
00:43:28,360 --> 00:43:30,760
On that website, you can try it out, type in a sentence.

729
00:43:30,760 --> 00:43:34,600
Only does one sentence at a time right now, just because we have a cheap server up on

730
00:43:34,600 --> 00:43:35,600
Amazon.

731
00:43:35,600 --> 00:43:37,720
And then you can join our mailing list.

732
00:43:37,720 --> 00:43:41,600
And I tweet my life out at Twitter, at J-M-U-G-A-N.

733
00:43:41,600 --> 00:43:42,600
Okay.

734
00:43:42,600 --> 00:43:43,600
Awesome.

735
00:43:43,600 --> 00:43:44,600
Well, thanks so much.

736
00:43:44,600 --> 00:43:45,600
It was great chatting with you.

737
00:43:45,600 --> 00:43:46,600
Oh, thanks.

738
00:43:46,600 --> 00:43:47,600
It's been fun.

739
00:43:47,600 --> 00:43:48,600
Awesome.

740
00:43:48,600 --> 00:43:54,360
All right, everyone, that's our show for today.

741
00:43:54,360 --> 00:43:56,280
Thank you so much for listening.

742
00:43:56,280 --> 00:44:00,840
And of course, for your ongoing feedback and support.

743
00:44:00,840 --> 00:44:05,840
For more information on Jonathan and the topics we covered in this episode, head on over

744
00:44:05,840 --> 00:44:10,520
to twimmolai.com slash talk slash 49.

745
00:44:10,520 --> 00:44:15,240
If you liked this episode or you've been a listener for a while and haven't yet done

746
00:44:15,240 --> 00:44:20,080
so, please take a moment to jump on over to Apple Podcasts or your favorite podcast

747
00:44:20,080 --> 00:44:23,680
app and leave us that five star review.

748
00:44:23,680 --> 00:44:30,480
We love to read these and it lets others know that the podcast is worth tuning into.

749
00:44:30,480 --> 00:44:33,800
If you've already done this, then thank you so much.

750
00:44:33,800 --> 00:44:35,800
We greatly appreciate it.

751
00:44:35,800 --> 00:44:41,160
One last note, you've probably heard me mention Strange Loop, a great technical conference

752
00:44:41,160 --> 00:44:43,800
held each year right here in St. Louis.

753
00:44:43,800 --> 00:44:48,400
I'll be attending later this week and I encourage you to check it out.

754
00:44:48,400 --> 00:44:54,560
Also, the following week, on October 3rd and 4th, I'll be at the Gartner Symposium IT

755
00:44:54,560 --> 00:44:59,800
Expo in Orlando, where I'll be on a panel on how to get started with AI.

756
00:44:59,800 --> 00:45:03,040
If you plan on being there, send me a shout.

757
00:45:03,040 --> 00:45:14,320
Thanks once again for listening and catch you next time.

