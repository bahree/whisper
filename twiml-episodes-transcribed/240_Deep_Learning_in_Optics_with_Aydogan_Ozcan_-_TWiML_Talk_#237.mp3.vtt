WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.120
I'm your host, Sam Charrington.

00:32.120 --> 00:37.480
Today we're joined by Ida Wan-Euzjin, professor of electrical and computer engineering at UCLA,

00:37.480 --> 00:43.600
where his research group focuses on photonics and its applications to nano and biotechnology.

00:43.600 --> 00:48.000
In our conversation, we explore his group's research into the intersection of deep learning

00:48.000 --> 00:51.640
and optics, holography, and computational imaging.

00:51.640 --> 00:56.640
We specifically look at a really interesting project to create all optical neural networks,

00:56.640 --> 01:00.640
which work based on diffraction, where the printed pixels of the network are analogous

01:00.640 --> 01:02.320
to neurons.

01:02.320 --> 01:07.080
We also explore some of the practical applications for their research and other areas of interest

01:07.080 --> 01:10.240
for their group, and now on to the show.

01:10.240 --> 01:13.520
Alright everyone, I am on the line with Ida Wan-Euzjin.

01:13.520 --> 01:18.960
Ida Wan is a professor in electrical and computer engineering department at UCLA.

01:18.960 --> 01:22.240
Ida Wan, welcome to this week in machine learning and AI.

01:22.240 --> 01:24.600
Well thanks for having me, Sam.

01:24.600 --> 01:31.440
So you are currently working at the intersection point between optics and deep learning, and

01:31.440 --> 01:35.520
I'm really interested in jumping into this and learning more about the work that's coming

01:35.520 --> 01:40.640
out of your research group, but before we do, tell us a little bit about your background

01:40.640 --> 01:44.320
and how you started working with deep learning.

01:44.320 --> 01:46.320
Sure, sure.

01:46.320 --> 01:53.000
Yes, I work in optics, applications of optics, specifically computational imaging and sensing

01:53.000 --> 01:58.680
techniques, where we're creating new types of imaging systems, new types of microscopes

01:58.680 --> 02:07.040
and sensors, with heavily focusing on applications in biomedical space.

02:07.040 --> 02:11.640
For example, pathology, creating new types of microscopes in pathology, at the same time

02:11.640 --> 02:17.440
different types of sensors for telemedicine, mobile health related applications, and more

02:17.440 --> 02:22.120
and more recently we're interested in the environmental monitoring looking into water

02:22.120 --> 02:23.800
and air quality.

02:23.800 --> 02:30.080
And there's a huge opportunity ahead of us at the intersection of optics and machine

02:30.080 --> 02:36.240
learning that we've been exploiting for the last several years.

02:36.240 --> 02:44.440
Let's first start with essentially using machine learning to look at the images that we

02:44.440 --> 02:48.240
reconstruct and finding specific targets of interest.

02:48.240 --> 02:54.240
Let's say you're looking at certain parasites in drinking water and you create a mobile

02:54.240 --> 03:02.040
microscope and create images of particles inside your observation volume chamber, disposable

03:02.040 --> 03:03.440
cartridge.

03:03.440 --> 03:09.320
And then of course you need some sort of machine learning tool to look at what's captured

03:09.320 --> 03:14.000
there and specifically label, let's say, certain parasites of interest.

03:14.000 --> 03:20.120
Say it starts from that direction of annotating images and looking for specific signatures

03:20.120 --> 03:22.520
that you're looking that you're after.

03:22.520 --> 03:29.320
But more and more recently we're realizing the power of specially deep learning to design

03:29.320 --> 03:35.240
instruments, the design sensors from image labeling, image classification, we're moving

03:35.240 --> 03:42.120
toward data driven designs for optical instrumentation for optical sensors.

03:42.120 --> 03:49.960
In a sense, I've spent easily the last 10 years creating new types of computational reconstruction

03:49.960 --> 03:55.480
methods for different kinds of microscopes, holographic microscopes, for example.

03:55.480 --> 03:59.880
And in the last few years, we're realizing that we're moving from physics driven solutions

03:59.880 --> 04:07.320
to data driven solutions, how data and the framework around deep learning, holistically they're

04:07.320 --> 04:13.160
helping us to essentially better reconstruct images, especially for holography, we've

04:13.160 --> 04:20.800
seen some great examples of that where without any iterations, training purely a neural net

04:20.800 --> 04:27.560
based on input output labels, you can actually reconstruct holograms much better than traditional

04:27.560 --> 04:32.240
methods of solving an inverse problem and that's very, very exciting for actually optics

04:32.240 --> 04:35.480
community at large as a whole.

04:35.480 --> 04:37.560
What does it mean to reconstruct the hologram?

04:37.560 --> 04:40.120
What specifically are we trying to do there?

04:40.120 --> 04:47.160
So one class of mobile microscopes that we've created is actually using diffraction of light.

04:47.160 --> 04:52.200
In fact, holographic shadows of specimen to reconstruct images.

04:52.200 --> 04:58.000
So in this line of work, you're taking, for example, a CMOS image or like the SCCD or

04:58.000 --> 05:01.920
a CMOS image or the same thing that you have at the back of your cell phone and place

05:01.920 --> 05:08.520
specimen right in front of it without any lenses where the specimen can be, let's say, pathologist

05:08.520 --> 05:16.200
sample or a smear and you shine light through it and you capture with that simple configuration

05:16.200 --> 05:21.920
with a simple 510 megapixel imager, the diffraction hologram of a specimen.

05:21.920 --> 05:28.000
So that contains all the three-dimensional information of the specimen, but it needs

05:28.000 --> 05:35.880
to be reconstructed because it doesn't immediately give you the micro scale information of the sample.

05:35.880 --> 05:39.280
It gives you a hologram which is light interference.

05:39.280 --> 05:44.800
The light scattered by the object interferes with the background light giving you some fringes

05:44.800 --> 05:47.280
and a hologram.

05:47.280 --> 05:49.360
It needs to be reconstructed.

05:49.360 --> 05:57.040
So physics is great at providing us tools to do this reconstruction.

05:57.040 --> 06:04.880
But nowadays, one alternative method that we've been exploring and being very successful

06:04.880 --> 06:11.280
at is to use actually holograms as inputs to a neural net that has been trained with

06:11.280 --> 06:17.200
reconstructions, gold standard labels coming from physical reconstruction methods which

06:17.200 --> 06:23.280
are relatively speaking cumbersome in the sense that they're typically using some iterations

06:23.280 --> 06:31.120
to clean up the image and give you something that with reasonable resolution shows you

06:31.120 --> 06:33.840
what's going on in your sample.

06:33.840 --> 06:38.080
Now we're seeing that through data you can actually teach a neural net to reconstruct

06:38.080 --> 06:45.080
the hologram from those interference patterns, from those fringes, to go back to the sample

06:45.080 --> 06:51.920
plane, sample volume to give you all these signatures of the samples.

06:51.920 --> 06:57.080
Better than before, in the sense that this reconstruction is now extremely fast, and

06:57.080 --> 07:02.800
it can also provide some better rejection of some of these interference artifacts that

07:02.800 --> 07:04.720
are coming to holography.

07:04.720 --> 07:12.720
It's just one example where data-driven approaches are providing alternatives to intuition driven

07:12.720 --> 07:16.280
by physics to do every construction.

07:16.280 --> 07:25.160
And so in this case is the your ground truth and image that is image like traditionally

07:25.160 --> 07:31.120
like with a lens in front of the same type of CCD, and then you take an image of the

07:31.120 --> 07:38.000
same subjects without the lens just using the diffractive patterns, and that's how you

07:38.000 --> 07:43.400
produce your labels or is it more involved in that?

07:43.400 --> 07:50.720
You can definitely take that approach and use standard microscopes to create labels.

07:50.720 --> 07:57.320
A better approach is actually to use traditional physical reconstruction methods that take

07:57.320 --> 08:03.400
maybe a bunch of holograms to reconstruct the sample's information.

08:03.400 --> 08:07.840
Different than a traditional lens-based bright field microscope, holography has two different

08:07.840 --> 08:09.480
channels of information.

08:09.480 --> 08:16.000
One of them relates to the amplitude of the sample, amplitude of how the light is scattered,

08:16.000 --> 08:19.000
and the other one is the phase information of the sample.

08:19.000 --> 08:24.480
And that's where holography is very powerful as a coherent imaging modality.

08:24.480 --> 08:30.080
And that's why a better approach to use as gold standard ground truth images is actually

08:30.080 --> 08:35.040
coming from traditional holographic reconstruction solutions.

08:35.040 --> 08:39.000
They provide essentially the physics enabled ground truth.

08:39.000 --> 08:44.560
And after we've done that, we see some very interesting results where it solves the

08:44.560 --> 08:51.320
inverse problem in a different path, but very robust in terms of what you need to see,

08:51.320 --> 08:55.560
what you need to reconstruct at the microscope scale.

08:55.560 --> 08:58.800
For both of these channels that I mentioned, phase and amplitude.

08:58.800 --> 09:03.720
The phase channel is extremely important, especially for transparent samples like jellyfish,

09:03.720 --> 09:08.920
for example, thin samples that do not scatter light as much.

09:08.920 --> 09:13.000
And is the reconstruction that you're targeting?

09:13.000 --> 09:20.800
Is it a visual reconstruction that you might get if you had a traditional microscope

09:20.800 --> 09:27.520
or is it something different that better captures the two types of information that you

09:27.520 --> 09:30.520
have access to with holographic images?

09:30.520 --> 09:36.280
It bears the advantages of holography in the sense that it can look at these two different

09:36.280 --> 09:42.160
information channels that I mentioned, amplitude and phase, and at the same time over a larger

09:42.160 --> 09:43.320
volume.

09:43.320 --> 09:50.200
One advantage of holography is it can see objects at different depths without a physical

09:50.200 --> 09:51.200
focusing mechanism.

09:51.200 --> 09:57.400
You can time reverse the optical field and go to different depths within your sample.

09:57.400 --> 10:03.200
And that gives an advantage to a holographic reconstruction, and it essentially gives

10:03.200 --> 10:06.920
you microscopic features at different planes of your sample.

10:06.920 --> 10:10.200
And that's what the network output gives you.

10:10.200 --> 10:16.160
And we've shown that actually if you trade the neural net with different holograms acquired

10:16.160 --> 10:22.440
at different sample to sensor distances, you can extend a depth of field.

10:22.440 --> 10:28.480
In other words, the neural net can be taught to not only holographically reconstruct objects,

10:28.480 --> 10:30.480
but autofocus them as well.

10:30.480 --> 10:36.440
Which is being very interesting, imagine for example, as a volume of a sample, which contains

10:36.440 --> 10:41.960
several different objects, cells scattered for example in a volume, you can actually

10:41.960 --> 10:46.440
break them all into focus within the reconstruction.

10:46.440 --> 10:52.600
And that's something that we recently shown as a holographic reconstruction doing both standard

10:52.600 --> 10:59.040
hologram to image transformation that physics has been very powerful doing for decades.

10:59.040 --> 11:04.280
But at the same time, merging it with autofocusing so that different parts of the sample come

11:04.280 --> 11:07.360
into focus digitally at the output of the network.

11:07.360 --> 11:14.160
And what do these networks tend to look like? Are they convolutional neural nets with

11:14.160 --> 11:15.760
traditional architectures?

11:15.760 --> 11:22.520
Or are you doing very domain specific things in the various layers of the network?

11:22.520 --> 11:26.280
Well, these are powered by convolutional neural nets.

11:26.280 --> 11:32.360
And we're taking essentially standard architectures, but of course fine tuning them without making

11:32.360 --> 11:35.760
them unnecessarily complicated at the same time.

11:35.760 --> 11:38.960
This is a microscopic imaging modality.

11:38.960 --> 11:47.240
We're also fine tuning some of the parameters in this space to make it learn this transformation

11:47.240 --> 11:53.440
across different spatial features so that it can actually reconstruct larger features at

11:53.440 --> 11:58.360
the same time subcellular features, for example, if you're living into a cell.

11:58.360 --> 12:07.280
And when you're building these networks that essentially map from this kind of raw data

12:07.280 --> 12:16.960
from the imager to the result of something that's trained on the physics-based reconstructions,

12:16.960 --> 12:23.080
is there anything about the networks that you've seen produced and their function that

12:23.080 --> 12:31.760
suggests that the network is learning structure that's analogous to the rules of a physics

12:31.760 --> 12:37.920
that a, you know, the govern the system or is it taking shortcuts that don't necessarily

12:37.920 --> 12:41.520
relate to the physical representation?

12:41.520 --> 12:43.440
Yeah, a fantastic question.

12:43.440 --> 12:50.200
So it learns whatever you teach it to do and it rejects anything else.

12:50.200 --> 12:54.840
Even though that something is physical, I'll give you an example.

12:54.840 --> 13:00.640
We've taught a neural net to do reconstructions of holograms, but these holograms were

13:00.640 --> 13:06.000
planar objects like we were interested in tissues used in pathology, right?

13:06.000 --> 13:11.240
There are thin sections of tissue and our goal was to reconstruct those tissue sections

13:11.240 --> 13:13.320
from their holograms.

13:13.320 --> 13:18.160
This was a plane to plane transformation in a sense that the sample is planar, its hologram

13:18.160 --> 13:24.880
is planar, and we were taking the raw holograms and transforming them back to how the sample

13:24.880 --> 13:27.920
should look like in its phase and amplitude.

13:27.920 --> 13:32.440
And the network was very successful learning kind of like, you know, without understanding

13:32.440 --> 13:37.760
physics, learning the physical principles of this transformation.

13:37.760 --> 13:40.720
However, it's rejecting anything else.

13:40.720 --> 13:47.120
For example, in our experiments, we usually have some dust particles that are seated somewhere

13:47.120 --> 13:52.760
in our optical path above or below the sample, but not on the sample plane.

13:52.760 --> 13:58.000
These dust particles are unavoidable, especially, you know, if you're not having a vacuum type

13:58.000 --> 13:59.520
of an environment.

13:59.520 --> 14:06.520
So in a physical reconstruction, those dust particles appear as some sort of artifact

14:06.520 --> 14:07.520
in the image.

14:07.520 --> 14:08.520
And we understand it.

14:08.520 --> 14:09.520
That's a physical solution.

14:09.520 --> 14:13.680
That's a physical particle creating some sort of an interference pattern, superimpose

14:13.680 --> 14:16.480
on the image of the sample, right?

14:16.480 --> 14:17.480
It's not part of the sample.

14:17.480 --> 14:20.480
It's somewhere else, but it's a physical particle.

14:20.480 --> 14:25.520
The network rejects that, although it appears in the physical reconstruction.

14:25.520 --> 14:29.800
The reason is, the network thinks it's an artifact of holography.

14:29.800 --> 14:33.480
It doesn't seem like it's in focus.

14:33.480 --> 14:38.800
It's an out of focus, you know, dust particle or artifact related to that.

14:38.800 --> 14:44.600
And actually, it's rejecting those kinds of physical, but outside the solution domain

14:44.600 --> 14:47.000
type of pixels.

14:47.000 --> 14:50.760
And that's very powerful in the sense that you see in the physical reconstruction some

14:50.760 --> 14:55.880
artifacts that must be there, but they're actually attacked by the network.

14:55.880 --> 15:00.800
Similar things that you can mention for particles or objects at different depths.

15:00.800 --> 15:07.040
Physics give you a certain solution, and the network sometimes violates that.

15:07.040 --> 15:11.160
But if only it is part of what you've trained it for.

15:11.160 --> 15:20.120
So you said that the physical reconstructions do contain these artifacts from the dust

15:20.120 --> 15:25.360
particles, and these are your ground truth, how then is the network learning to reject

15:25.360 --> 15:26.360
them?

15:26.360 --> 15:28.320
Well, they're rare, right?

15:28.320 --> 15:29.720
So that's the good thing.

15:29.720 --> 15:34.040
The physical reconstruct that, the physics, I mean, the dust particles are obviously happening

15:34.040 --> 15:35.040
rarely.

15:35.040 --> 15:40.080
Like over a large field of view, maybe you have a few places, and by and large, everything

15:40.080 --> 15:42.160
else is plain to plain.

15:42.160 --> 15:48.440
That's why it generalizes to that transformation and attacks anything of that nature.

15:48.440 --> 15:52.560
You're creating these new types of reconstruction methods.

15:52.560 --> 15:54.920
How also you applying deep learning?

15:54.920 --> 16:01.360
Well, I mean, a recent work that we've done on optics deep learning intersection is

16:01.360 --> 16:09.320
actually creation of an optical network, optical neural net that's based on diffraction.

16:09.320 --> 16:16.840
So it's not a traditional deep neural net in the sense of standard convolutional neural

16:16.840 --> 16:22.280
nets with non-denarities like rectified linear units, et cetera, that you find in electronic

16:22.280 --> 16:23.280
neural nets.

16:23.280 --> 16:25.080
This is actually an analogous to that.

16:25.080 --> 16:34.680
It's an optical analog of it where, imagine the input is an object where you shine light,

16:34.680 --> 16:38.240
and behind the input, there is an optical construct.

16:38.240 --> 16:45.400
It's a passive system, meaning you design this optical hardware using a computer, using

16:45.400 --> 16:47.960
deep learning principles.

16:47.960 --> 16:54.560
And once you optimize it, you fabricate it using a 3D printer, if your wavelength is

16:54.560 --> 16:57.640
of interest to that, or using lithography.

16:57.640 --> 17:02.800
You fabricate features to craft essentially a volume.

17:02.800 --> 17:09.000
And that volume as light is penetrating from the input plane into the diffractive layers

17:09.000 --> 17:14.880
is solving essentially a problem, let's say a classification problem.

17:14.880 --> 17:21.520
And that volume is composed of different layers, where every layer is composed of several

17:21.520 --> 17:24.160
pixels, thousands of pixels.

17:24.160 --> 17:28.480
And these pixels are representing what we call as neurons.

17:28.480 --> 17:34.800
The transmittance phase and amplitude of the transmittance of each neuron is a trainable

17:34.800 --> 17:36.480
learnable parameter.

17:36.480 --> 17:39.680
And at the end of the network, there's a light pattern.

17:39.680 --> 17:46.280
And that light pattern is the output of your network that you're trying to solve a problem.

17:46.280 --> 17:48.680
Let's look at, for example, a classification problem.

17:48.680 --> 17:53.280
A simple one, let's look at classification of handwritten digits.

17:53.280 --> 18:01.720
If I input to this network, zeros, like handwritten zero digits, the light that is transmitted

18:01.720 --> 18:07.520
from that object is diffracting through these different layers that are all optimized to

18:07.520 --> 18:15.080
guide the light to a certain detector at the output plane that is assigned to zero.

18:15.080 --> 18:20.920
If I bring a new zero handwritten, it's, if it's correctly built, it's going to actually

18:20.920 --> 18:25.400
channel most of the photons at the output to the correct detector, saying that it's all

18:25.400 --> 18:29.000
optically in fearing that it's a zero.

18:29.000 --> 18:32.000
Same idea applies for other classes.

18:32.000 --> 18:38.240
And it's all optically, essentially, using diffraction, solving this problem that you

18:38.240 --> 18:40.040
asked the network to solve.

18:40.040 --> 18:47.040
This is really fascinating to think of that you can implement a deep neuron that all

18:47.040 --> 18:48.960
in optics.

18:48.960 --> 19:00.200
You mentioned that the pixels are analogous to neurons in a traditional deep neuron network

19:00.200 --> 19:03.560
and that the parameters can be learned.

19:03.560 --> 19:08.240
Are you learning those parameters offline, so to speak, meaning before you're printing

19:08.240 --> 19:16.240
these, this network, or is there somehow, these networks are not, they're mutable,

19:16.240 --> 19:17.240
posts printing?

19:17.240 --> 19:18.240
Great question.

19:18.240 --> 19:26.320
So yes, let's think first, an entirely passive system where it's fabricated and fixed.

19:26.320 --> 19:32.000
Before that fabrication process, you use a computer to finalize the, through deep learning,

19:32.000 --> 19:36.880
through backpropagation, error backpropagation, you finalize the transmittance values of

19:36.880 --> 19:38.800
each one of these neurons.

19:38.800 --> 19:43.560
And then you fabricate it, so it's essentially fixed, but you can also create a hybrid system

19:43.560 --> 19:51.600
where some of these layers are composed of dynamic light modulators, like spatial light

19:51.600 --> 19:57.000
modulators where you can actually change them and make them as part of a learning scheme

19:57.000 --> 20:01.040
or kind of like mend the network's function.

20:01.040 --> 20:06.920
But in the implementation that we had, experimentally, these were trained using a computer in

20:06.920 --> 20:14.520
TensorFlow and then fabricated and then all optically tested with light as input passing

20:14.520 --> 20:16.440
through some input.

20:16.440 --> 20:23.440
One thing that's very important here is that this framework is, it has two branches to

20:23.440 --> 20:24.440
it.

20:24.440 --> 20:31.160
One, you can create something with this kind of layer-to-layer design using linear materials,

20:31.160 --> 20:38.720
linear optical materials, meaning that they are essentially not including any non-linearities.

20:38.720 --> 20:46.560
As a result, if you use no non-linearity in your diffractive layers, this becomes a linear

20:46.560 --> 20:53.720
classifier, meaning that you can have all these different layers of free space propagation

20:53.720 --> 21:00.760
of light and a diffractive layer, some more free space diffraction, some more layer coming

21:00.760 --> 21:03.920
after it and repeating the same process.

21:03.920 --> 21:11.240
All of this can be mathematically squeezed into a single matrix operation, but even using

21:11.240 --> 21:16.960
these linear materials, the network has depth to it and that's where the diffractive

21:16.960 --> 21:20.880
deep network, optical network analogy comes from.

21:20.880 --> 21:25.040
The depth of the network is because of this.

21:25.040 --> 21:31.240
You cannot take a single layer between the input and output planes and generalize to any

21:31.240 --> 21:36.480
function that multiple diffractive layers collectively can produce.

21:36.480 --> 21:41.920
In fact, we've shown that as you add more layers to this system, one after another, all

21:41.920 --> 21:49.600
trainable, your network gets better in its classification and its power efficiency, its

21:49.600 --> 21:56.600
output contrast, per detector, per class improves as the number of layers increases.

21:56.600 --> 21:58.680
That's one aspect that I would like to emphasize.

21:58.680 --> 22:03.800
The other aspect is, before you move on, just to make sure I understand this, are you saying

22:03.800 --> 22:12.000
that adding additional layers inherently introduces non-linearity or that you're using non-linear

22:12.000 --> 22:18.600
materials with non-linear reflective and refractive characteristics to introduce the non-linearity?

22:18.600 --> 22:26.000
Certainly, we can use non-linear materials as part of the diffractive layers to introduce

22:26.000 --> 22:35.120
non-linearity in the system and then it becomes a more sophisticated tool in terms of how

22:35.120 --> 22:38.600
different kinds of functions it can generalize.

22:38.600 --> 22:43.640
You can include non-linearities like non-linear materials, crystals, polymers, semiconductors,

22:43.640 --> 22:49.160
as you fabricate these things as part of the network to introduce non-linearity.

22:49.160 --> 22:55.280
What I was referring to is the fact that even though you don't have any non-linear material

22:55.280 --> 23:03.080
in this system, there is depth to it in the sense that multiple layers perform or have

23:03.080 --> 23:09.880
more degrees of freedom to perform a more general function better than a single diffractive

23:09.880 --> 23:16.960
layer, and that's where, even with the linearity of the material, there is depth and deepness

23:16.960 --> 23:19.640
of the network in its performance.

23:19.640 --> 23:27.440
You've got this essentially a classifier that you've printed just to make this very concrete.

23:27.440 --> 23:32.280
What type of scale are we talking about physically?

23:32.280 --> 23:33.280
Great question.

23:33.280 --> 23:40.360
We tested these experimentally using Terahertz wavelengths. The weigh length of which in

23:40.360 --> 23:46.800
air was about 0.75 millimeters, so it's a big weight length, it's Terahertz.

23:46.800 --> 23:53.280
That's why we were able to use luckily a standard 3D printer, which is transparent at

23:53.280 --> 23:54.280
those weigh length.

23:54.280 --> 24:00.360
We could put many layers, like five layers, one after another, forming a diffractive optical

24:00.360 --> 24:05.760
network made out of, literally, plastic coming from a 3D printer.

24:05.760 --> 24:10.560
The size of this network was on the order of 8 to 9 centimeters.

24:10.560 --> 24:15.880
Obviously, a divisible weight length at shorter weight lengths.

24:15.880 --> 24:22.040
What you're looking at is maybe half a centimeter by half a centimeter type of a network in terms

24:22.040 --> 24:26.320
of width, which would be sufficient in terms of number of neurons, et cetera, that you

24:26.320 --> 24:28.840
can fabricate there with lithography.

24:28.840 --> 24:38.880
So 8 to 9 centimeters sounds very large relative to what I envisioned for this.

24:38.880 --> 24:42.520
That was the weigh length is very large.

24:42.520 --> 24:49.280
The weigh length, as I said, is about 0.7 to 0.8 millimeters.

24:49.280 --> 24:53.600
That's almost about 1,800 microns.

24:53.600 --> 25:00.960
If you go to visible weigh lengths, sub-micron weigh lengths, you're looking at a drastic reduction

25:00.960 --> 25:01.960
in the size.

25:01.960 --> 25:08.840
That's why a few millimeters by a few millimeters would be the size of this kind of a network working

25:08.840 --> 25:15.040
in visible weigh length, let's say in green weigh length, like half a micron weigh length.

25:15.040 --> 25:18.080
That's why everything will be scalable to the weigh length.

25:18.080 --> 25:28.840
In this Terahertz weigh length that you fabricated, the 8 to 9 centimeters is that 8 to 9 centimeters

25:28.840 --> 25:37.720
square for the plane that the light initially hits, and then is there a separate depth for

25:37.720 --> 25:40.720
this or is it 8 to 9?

25:40.720 --> 25:43.720
What dimension is that 8 to 9 millimeters referred to?

25:43.720 --> 25:44.720
Great.

25:44.720 --> 25:46.640
The 8 to 9 centimeters.

25:46.640 --> 25:49.360
Right, right, sorry.

25:49.360 --> 25:59.480
Is the width and the depth of this system is actually layered to layer spacing is only

25:59.480 --> 26:00.480
3 centimeters.

26:00.480 --> 26:12.280
Essentially, you're looking at maybe a cube of 8 to 8 centimeters by maybe about 15 centimeters

26:12.280 --> 26:14.080
in total depth.

26:14.080 --> 26:21.200
The spacing between the layers being 3 centimeters, we also had another network which was much

26:21.200 --> 26:27.520
more compact with a 4 millimeter gap, but you should all convert these to weigh lengths

26:27.520 --> 26:34.200
because that's what is more relevant to discuss because here we use a specific weigh length.

26:34.200 --> 26:40.480
In terms of weigh length, we're looking at layer to layer spacing of less than 50 weigh

26:40.480 --> 26:46.320
lengths, so if your weigh length is let's say a micron, you're talking about 50 micron

26:46.320 --> 26:50.240
at the most layer to layer spacing, sub-micron.

26:50.240 --> 26:57.160
In terms of the width, we're talking about maybe 100 weigh lengths in terms of width and

26:57.160 --> 27:03.520
height of the network, so in that regard, these are really very compact networks that will

27:03.520 --> 27:11.320
scale down with weigh length, obviously, in this example, we've used off the shelf 3D printers

27:11.320 --> 27:17.120
and that's why Terahertz was used to show the fidelity of this thing, this thing with relatively

27:17.120 --> 27:19.920
very inexpensive fabrication methods.

27:19.920 --> 27:24.840
It costs us only a few dollars to print one of these things with 3D printers.

27:24.840 --> 27:29.680
The target of he would be more expensive, but with economies of scale, obviously, the

27:29.680 --> 27:36.160
fabrication of something like this using gloss and using photolithography would be no

27:36.160 --> 27:42.880
different than fabricating, for example, your CMOS imager at the back of your cell phone.

27:42.880 --> 27:47.760
In that regard, it will be also pennies at large scale fabrication.

27:47.760 --> 27:55.440
In terms of visualizing what the layers might look like, should we or could we think of

27:55.440 --> 28:01.800
them kind of as like Fresno lenses or maybe more digital patterns, like QR codes or

28:01.800 --> 28:05.720
is there a way that you can articulate what these things might look like if we were looking

28:05.720 --> 28:08.560
at a layer individually?

28:08.560 --> 28:13.000
Each layer would look like speckle pattern to you.

28:13.000 --> 28:18.920
It wouldn't mean much, but as you come toward the output layer, you will see emergence

28:18.920 --> 28:22.640
of some patterns depending on where you put your detectors.

28:22.640 --> 28:29.400
So essentially, it's a gradual shaping of statistical waveforms coming from different

28:29.400 --> 28:33.720
objects that you've learned to classify.

28:33.720 --> 28:40.880
So, it's very difficult to exactly understand how they work together because there are

28:40.880 --> 28:48.680
multiple layers and your signal is essentially a stochastic signal in the sense that it

28:48.680 --> 28:52.040
can come in different forms.

28:52.040 --> 28:57.840
But what you can see from each layer is that there is a face pattern and it's slowly

28:57.840 --> 29:06.320
emerging at the output to shape itself as if it's facing a bunch of detectors that each

29:06.320 --> 29:09.680
of which is assigned to a class.

29:09.680 --> 29:13.960
If you have 10 classes, then you're looking at 10 detectors at the output so that the

29:13.960 --> 29:19.800
inferences is all optical except the final detection part per detector.

29:19.800 --> 29:25.120
So obviously, you can have more detectors to be on the class.

29:25.120 --> 29:31.800
In fact, in our recent work as a follow-up on this, we've shown the merge of these diffractive

29:31.800 --> 29:37.800
optical layers forming the all optical network with electronic neural nets.

29:37.800 --> 29:43.880
So formed kind of like a hybrid system where the front end was all optical and the backhand

29:43.880 --> 29:49.720
was standard neural nets with the standard non-linearities, et cetera.

29:49.720 --> 29:54.200
So that obviously has some very interesting and appealing features.

29:54.200 --> 30:01.520
And one thing that we've shown is that the input pixels to the electronic neural net can

30:01.520 --> 30:07.360
be compressed by the all optical network if they're both optimized at the same time.

30:07.360 --> 30:14.160
The optical neural net and the electronic neural net, if they are optimized jointly, we

30:14.160 --> 30:17.000
see some very interesting advantages.

30:17.000 --> 30:19.680
But this is something that's unpublished.

30:19.680 --> 30:22.880
We just put it into archives.

30:22.880 --> 30:28.800
And we're seeing some very interesting hybrid systems that can emerge from the same diffractive

30:28.800 --> 30:30.440
neural net concept.

30:30.440 --> 30:35.880
And how is that physically implemented, this mixed mode system?

30:35.880 --> 30:44.720
So this was an analysis, but in an experiment, imagine you take an optical electronic sensor,

30:44.720 --> 30:50.920
let's say 100 pixel by 100 pixel or a few hundred pixel by a few hundred pixel type of design.

30:50.920 --> 30:55.840
In front of it, as if you're putting a lens, you're going to be putting a bunch of diffractive

30:55.840 --> 31:01.400
layers that have been optimized for a certain inference task.

31:01.400 --> 31:09.960
Assuming that there is a very simple neural net that is running beyond the CMOS.

31:09.960 --> 31:16.600
And that's the CMOS or the CCD, the low pixel count CCD or CMOS imager is kind of like

31:16.600 --> 31:22.160
the layer between the optics and the electronic neural net.

31:22.160 --> 31:30.040
And are you using it as an electronic or a dynamic detector?

31:30.040 --> 31:31.040
Is that its role?

31:31.040 --> 31:39.440
Or are you also implementing layers beyond that in the analysis that you did?

31:39.440 --> 31:47.760
We've shown it works as a classifier, just like the all optical network, but a better

31:47.760 --> 31:49.080
classifier.

31:49.080 --> 31:56.360
And it also takes a very primitive neural net that the inference performance, not very deep,

31:56.360 --> 32:02.560
not a lot of trainable parameters, very low power requirement type of a neural net.

32:02.560 --> 32:06.080
You can take something like that and make it work very good.

32:06.080 --> 32:14.040
So in a sense, it will be very useful, especially for low power and mobile systems that need

32:14.040 --> 32:20.560
to frequently, with a very high frame rate, look at some scene or some data.

32:20.560 --> 32:28.280
That's where you don't have the luxury of working with very sophisticated neural nets.

32:28.280 --> 32:31.440
For power reasons, for frame rate reasons, et cetera.

32:31.440 --> 32:38.560
That's where optics can help, because of its speed and its front end, making the rest

32:38.560 --> 32:43.680
after the digitization step to be compact in terms of number of pixels, in terms of

32:43.680 --> 32:48.960
frame rate being fast, and also the depth of the electronic neural net or the complexity

32:48.960 --> 32:56.880
of the electronic neural net becoming kind of much less advanced compared to purely electronic

32:56.880 --> 32:58.120
neural net?

32:58.120 --> 33:06.800
Other experiments were based on an M-ness style database, handwritten digits.

33:06.800 --> 33:17.120
Do you have a sense for the way the physical characteristics of the layers change with the

33:17.120 --> 33:22.000
complexity of the underlying data set, the input data?

33:22.000 --> 33:26.440
We've also tried another data set, which is going to be more complicated.

33:26.440 --> 33:28.440
It's called the fashion M-ness.

33:28.440 --> 33:36.440
We had 10 different classes of fashion products, like sneakers, bags, t-shirts, trousers, et cetera,

33:36.440 --> 33:37.440
et cetera.

33:37.440 --> 33:47.160
It's not as challenging as C-Far-10, for example, but it's more complicated than a fashion

33:47.160 --> 33:48.680
M-ness.

33:48.680 --> 33:55.920
We've also tried that and shown that another network trained for that data set 3D printed

33:55.920 --> 34:03.160
and tested experimentally works nicely to match our predictions or numerical analysis.

34:03.160 --> 34:12.240
In terms of complexity, hard for us to understand what changed from M-ness layers to the fashion

34:12.240 --> 34:14.240
M-ness layers.

34:14.240 --> 34:22.400
As I look at them in front of me, they look similar to me, but of course, all the details

34:22.400 --> 34:28.400
are hidden there in terms of the face profiles of each layer, so it's very difficult to obviously

34:28.400 --> 34:29.400
understand.

34:29.400 --> 34:30.400
Sure.

34:30.400 --> 34:37.400
But there wasn't anything obvious like you needed to go wider or deeper to achieve reasonable

34:37.400 --> 34:38.400
performance.

34:38.400 --> 34:39.400
No.

34:39.400 --> 34:43.720
Of course, there's a huge parameter space that we need to optimize here, and maybe there's

34:43.720 --> 34:49.640
more room to optimize to improve the inference performance for more complicated data sets.

34:49.640 --> 34:56.320
It's still research ongoing in that regard, but what we've shown is that the same 5 layers

34:56.320 --> 35:01.560
with the same number of learnable, trainable parameters worked.

35:01.560 --> 35:06.600
Of course, lower accuracy compared to M-ness because fashion is more difficult.

35:06.600 --> 35:12.320
But essentially, it's the same framework that nicely generalizes for another task.

35:12.320 --> 35:14.640
But how well you can push it further?

35:14.640 --> 35:16.160
It's still a research question.

35:16.160 --> 35:19.760
In fact, in one of our recent work, we have improved.

35:19.760 --> 35:28.720
For example, the paper that the science paper report something like about 92% for M-ness.

35:28.720 --> 35:37.800
We've pushed it to now 98% without using any nonlinear optical materials with some changes

35:37.800 --> 35:42.920
in the way that we optimize the neural net, optical neural net.

35:42.920 --> 35:47.280
So essentially, we've already optimized and improved some of the parameters that we've

35:47.280 --> 35:53.760
been using in designing this diffractive layer set to push it by a good margin from, as

35:53.760 --> 35:58.800
I said, for M-ness, for example, from about 92% with 5 layers now.

35:58.800 --> 36:05.560
We're approaching 98% with also 5 layers, same number of trainable parameters.

36:05.560 --> 36:10.280
It's a lot of different things that you can do to improve the performance of something

36:10.280 --> 36:11.280
like this.

36:11.280 --> 36:15.120
One unique aspect of diffractive layers is it's passive, right?

36:15.120 --> 36:19.560
I mean, once you've fabricated, it doesn't consume any power except the illumination

36:19.560 --> 36:23.600
power and maybe the detectors that you have at the output plane.

36:23.600 --> 36:26.120
Everything else in thin is just material.

36:26.120 --> 36:28.480
It's just you fabricated and it stays.

36:28.480 --> 36:29.480
It's good.

36:29.480 --> 36:30.720
It doesn't consume any power.

36:30.720 --> 36:31.720
But it's bad.

36:31.720 --> 36:32.720
It's static.

36:32.720 --> 36:36.920
If your data changes, you need to reprint it.

36:36.920 --> 36:43.160
The one thing that we've shown is actually you can take a neural net with several layers

36:43.160 --> 36:49.280
and peel off some layers and add new layers, trainable, with respect to the rest of the

36:49.280 --> 36:55.960
static ones and kind of mend and improve the performance of the entire system.

36:55.960 --> 36:58.920
So it's in that sense, it's like a Lego piece.

36:58.920 --> 37:02.160
Kind of like fine tuning applied to a physical network.

37:02.160 --> 37:03.160
Right.

37:03.160 --> 37:04.160
Exactly.

37:04.160 --> 37:09.680
Different layers, you take out some of them or you patch additional layers onto an existing

37:09.680 --> 37:10.680
one.

37:10.680 --> 37:15.560
That's one way of bringing some reconfigurability to the system.

37:15.560 --> 37:20.960
Another way of bringing reconfigurability to the system is replacing some of these layers

37:20.960 --> 37:24.800
with dynamic, electrooptic modulators.

37:24.800 --> 37:31.120
So rather than printing and having the code as part of gloss, permanently, you have a

37:31.120 --> 37:33.640
layer that you can actually change the pixels.

37:33.640 --> 37:35.840
Specialized modulator type of systems.

37:35.840 --> 37:41.320
Of course, you can create a hybrid system where some layers are static, some layers are

37:41.320 --> 37:48.280
dynamic and create some sort of a trade-off between complexity and reconfigurability of

37:48.280 --> 37:49.800
active layers.

37:49.800 --> 37:51.720
This is really fascinating.

37:51.720 --> 37:54.920
How do you see this playing out?

37:54.920 --> 37:58.760
Do you see practical applications for this?

37:58.760 --> 38:06.080
You know, some arbitrary time frame and what might those be or is this kind of a research

38:06.080 --> 38:13.480
direction that's driving you on a broader path that you're not necessarily trying to see

38:13.480 --> 38:15.760
this use practically?

38:15.760 --> 38:19.440
First of all, we're enjoying playing with it.

38:19.440 --> 38:21.200
It's a toy.

38:21.200 --> 38:24.440
And it's making us happy.

38:24.440 --> 38:28.080
So it sounds really fun.

38:28.080 --> 38:30.280
I can definitely see that part of it.

38:30.280 --> 38:34.440
Yeah, it's certainly a good toy that keeps you awake.

38:34.440 --> 38:35.640
It's like a good puzzle, right?

38:35.640 --> 38:40.640
I mean, so many, so many things that you can do with it and try different things.

38:40.640 --> 38:42.560
And it's just at its infancy.

38:42.560 --> 38:46.720
So it's our baby and we're trying to play with it and be happy.

38:46.720 --> 38:50.760
But at the same time, there are so many interesting applications that we foresee.

38:50.760 --> 38:57.880
I believe for defense and security, it has tremendous applications, especially at

38:57.880 --> 39:04.960
longer wavelengths, infrared wavelengths, mid-inferred wavelengths, where some of these

39:04.960 --> 39:11.640
thermal cameras or other types of focal plane arrays, I think those will benefit tremendously

39:11.640 --> 39:18.980
from an optical front end because omega pixel at those wavelengths, omega pixel image

39:18.980 --> 39:25.600
or at those wavelengths is very expensive and not every country has the no-have to fabricate

39:25.600 --> 39:27.000
something like that.

39:27.000 --> 39:31.080
So I think for defense, the applications are enormous.

39:31.080 --> 39:35.920
Overall, I'm very excited about one direction that I've been constantly thinking and that

39:35.920 --> 39:43.680
is to create low power and mobile hybrid systems that are powered at their front end with

39:43.680 --> 39:53.080
some all optical machine learning front end at the backside and inexpensive CMOS or CCD

39:53.080 --> 39:57.360
followed by a very modest low power neural net implementation.

39:57.360 --> 40:02.160
It can go very low power this way, it can be extremely fast this way, frame rates can

40:02.160 --> 40:07.240
be very high and at the same time we're talking about very modest form factor.

40:07.240 --> 40:13.160
So that obviously has to me some very interesting security applications at different parts

40:13.160 --> 40:15.480
of the electromagnetic spectrum.

40:15.480 --> 40:21.160
And I can obviously, because of my background, think enormous amount of biomedical problems

40:21.160 --> 40:27.240
that one can tackle with this kind of front end integrated with optoelectronic sensors

40:27.240 --> 40:28.840
and electronic neural nets.

40:28.840 --> 40:32.800
That's one area and the other area that I'm very excited about is actually nonlinear

40:32.800 --> 40:38.960
materials, meta materials, plasmonics, exotic material systems.

40:38.960 --> 40:44.880
As we discussed this in our paper that we published in science, the nonlinearity aspect

40:44.880 --> 40:48.400
of material science, optical nonlinearity is in material science.

40:48.400 --> 40:57.280
So I think opens up a huge platter of opportunities for enhancing the function of something like

40:57.280 --> 40:58.280
this.

40:58.280 --> 41:03.720
And that's where I think some of these metamaterial basic, exotic structures, plane by plane by

41:03.720 --> 41:09.960
plane, following each other, would really generalize some very sophisticated functions,

41:09.960 --> 41:17.200
maybe coming close to electronic neural nets with favorite, favorite choices of nonlinearities.

41:17.200 --> 41:20.560
We'll see how it goes, but this is another area where I'm very excited about it.

41:20.560 --> 41:24.680
Well, I don't want to thank so much for taking the time to walk us through what you're

41:24.680 --> 41:25.680
up to.

41:25.680 --> 41:29.040
This is really fascinating and it sounds very fun.

41:29.040 --> 41:32.480
So your group must have a good time playing with the stuff.

41:32.480 --> 41:33.480
Thank you.

41:33.480 --> 41:34.480
Really?

41:34.480 --> 41:35.480
Yeah.

41:35.480 --> 41:36.480
We're enjoying ourselves.

41:36.480 --> 41:37.480
I'm enjoying the time.

41:37.480 --> 41:38.480
Yeah.

41:38.480 --> 41:42.680
Thank you so much for having me.

41:42.680 --> 41:47.520
All right, everyone, that's our show for today for more information on I'dawan or any of

41:47.520 --> 41:53.920
the topics covered in this show, visit twomalai.com slash talk slash 237.

41:53.920 --> 42:21.360
As always, thanks so much for listening and catch you next time.

