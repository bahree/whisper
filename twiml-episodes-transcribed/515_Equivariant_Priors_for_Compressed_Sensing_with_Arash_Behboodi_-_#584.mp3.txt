All right, everyone. Welcome to another episode of the Twomel AI podcast. I am your host,
Sam Charington. And today I'm joined by Arash Bebudi. Arash is a machine learning researcher
at Qualcomm Technologies. Before we get going, be sure to take a moment to hit that subscribe
button wherever you're listening to today's show. Arash, welcome to the podcast.
Yeah, hi, Sam. Thanks for having me. It's great to have you on the show and I'm looking forward
to the chat. I'd love to have you start out by sharing a little bit about your background and
what led you into the field of machine learning. Well, my background is information theory,
a mathematical signal processing on the one hand, that my PhD was about basically deriving
Shannon theoretic bounds on information capacity of certain cooperative networks on their
channel on certain T. So kind of mathematical PhD work. And after that, kind of transition into
in the field of compress sensing, it's quite fascinating to me because the core problem
in compress sensing was you have set an observations of an unknown signal, but you don't have enough
observation to recover it fully. So you have to rely on some prior assumption about the signal.
And the question is, if you know that prior, what is a good, you know, how you can recover your
signal and what are theoretical bounds on, basically, sample complexity, how many observations you
need and all that. So kind of dealing with that notion of a structure, signal structure,
and how this can be used in different kind of inverse problems. That was very, very interesting
for me and kind of I was working on that afterward. But then of course deep learning comes with a lot
of a lot of interesting promises and challenges. So immediately I said, you know, maybe you know,
with deep learning can even do better with, you know, to do a better job of working with structures
and learning better recovery algorithms. And even from theoretical side, I was very
interested in understanding like mysteries of deep learning. So I think kind of that dragged
me into deep learning. But funny enough, you know, on the side note, when I was undergrad,
I could one of my main interests was philosophy. And kind of I did that in parallel doing whole,
in my whole life. And when I was doing my PhD in parallel, I also did master degree on that. And
kind of a lot of topics that kind of studied there, philosophy of mind, language and all that.
Question of meaning, intelligence, consciousness, all that's kind of re-emerged in this field
of machine learning again. And kind of it's it's it became a very exciting area to work on.
And kind of I'm happy that all my interests are coming together within within this field.
Oh, that's fantastic, fantastic. What, you're at obviously Qualcomm AI research now. What are your
primary research interests on that team? Yeah. So, you know, information, I talked about
information theory and compress and seeing a mathematical signal processing. And
wireless communication, of course, you know, Shannon's theory is mathematical theory of communication.
So kind of communication and wireless communication in particular has always been very also
interesting to me. And if I worked on that also as kind of a motivation for a lot of problems,
I was I was formulating. And so what we're trying to do here, we're basically looking at wireless
communication from machine learning perspective, put it that way. We're trying to understand how
we can design new machine learning architectures useful for different wireless tasks and how we
can improve different parts of wireless system design using machine learning. And this kind of
relates to the current technologies that we have and the next technologies that are going to
come. And so the research that we have, I like to call it wireless AI research. So it's a kind of
doing machine learning research for wireless communication. And one of the papers that we're
going to talk about that you have had accepted at ICML this year is in the same field of
compress sensing that has come up a few times. It sounds like it's an area that you're continuing
to explore pretty deeply. Yeah, absolutely. I think, you know, the inverse problems there
everywhere. We kind of from medical imaging to all sorts of kind of you can talk about drug discovery
problems by, you know, you know, protein unfolding is another example. So we can find a lot of
a lot of areas where inverse problems are important. And wireless communication is actually
full of those problems like from child estimation to other topics. So you can find a lot of inverse
problems. And yeah, I mean, the compress sensing is definitely like a core core interest for me there.
Now, I've talked to some of your colleagues about some of the work that other teams are doing
in compression. How does compression and the setting there compare with compress sensing?
Is it the same? Is it different? It is different. So in compress sensing, the idea is that you want
to efficiently sense the environment in order to infer something. In compression, the idea is you
have a source, you have information source, and you want to efficiently compress it. So there is no
sensing medium there. The only thing that's important is how, how will you compress and reconstruct
your information source? In compress sensing, the idea is how you sense the medium given the
constraint that is given to you. You do not choose that. And then how well you can reconstruct
based on based on the observation you have. So they they're conceptually they might be connected
but if they're kind of a parallel field, I would say. And so with that said, what is the motivation
for your ICML paper, which is called equivalent priors for compress sensing with unknown orientation?
Great. So let me let me start with the first talking about generative priors and its relation
to compress sensing. This is probably a good start. So when I talked about the structure,
so assuming some some prior about the structure of the signal, for example, the typical example
of a structure in classical compress sensing was sparsity. You assume that your signal is sparse
in a given basis, for example. And then you use that to some sort of a one minimization to basically
find your signal. Now with deep learning, with generative models specifically, we notice that
actually generative models provide a way of parametrizing space of signals of interest using
basically the latent space of the generative model. So and specifically, if you cannot
tractably represent a signal, in this case, let's say images, if you cannot tractably represent
them, represent the prior generative models provide a way of parametrizing. And there was a work by
Bora and co-authors around 2015 that showed actually you can use generative priors,
generative models, and do gradient descent on the latent space of that generative model
in order to estimate the signal. So generative models in that sense provide a structural
structure prior on your signal. And the paper showed quite interesting results and it was quite
quite an encouraging work with theoretical guarantees on top, which is always good to have.
So that was the original work. Now a lot of work came afterward basically using flows,
using different techniques for doing a better job in this thing. We're extending it into
different type of non-linear problems and values extensions. Some challenges that these models have
are, let's say, convergence and latency, meaning that sometimes when you do gradient descent
on the latent space, you might need to restart the whole process because it's not converging.
So the convergence and the way you optimize your problem can become an issue,
especially if you're interested in low latency, let's say solutions, that can become a bottleneck.
So there were actually some works about how you can effectively invert a generative model.
And actually Alex Demarkis from the TU Austin had some follow-up works. Actually it was a quarter,
a quarter of the original papers, some follow-up works on kind of a layer by layer inversion and
some other tricks following that as well. So that's one part, like how why generative models are
relevant for compressency. The other part was, okay, in many applications, we might not have,
when we want to measure the signal, the signal might go under some transformation. Let's say
some rotation before the measurement. This can happen. A typical example is a cryo-electron
microscopy. So cryo-electron microscopy is a way of basically taking a picture of a biomolecule.
You can look at it like that. And the problem is that when you take this picture, of course,
the picture is very noisy and all that, but the molecule that you have is in an unknown orientation.
And if you have multiple pictures, these orientations are not the same, so are not aligned.
Also, you can find in different, so you can, in general, you can think of a signal that you
actually can turn a generative model on, but the orientation of that signal might change
before the measurement. So you might say, okay, let's, you know, what I can do probably is I try
to first figure out the orientation and then reconstruct the signal and or doing it iteratively
basically finding solving the problem like that. What we thought was, oh, why do we need to
kind of separate orientation discovery and the signal recovery? So we can do this jointly.
And the way we can do it is we can train a generative model that has already disinformation,
has already disinformation embedded in it. Of course, you can say, oh, you know, what we can do,
we can get a generative model, use data augmentation and basically, now we have it already there somehow
because we have seen all these different orientations. But when we know that we want to have this
this steerable basically transformable latent space, equivalence models are excellent candidates.
So what are equivalent equivalence models? So the equivalence models are models that when you
transform the inputs of the network according to certain group transformations, let's say rotation.
When you rotate the input, then the features that are outputs of that network also rotate.
So therefore, a rotation, a change of orientation at the output correspond to the change of orientation
as the input. This also imposes some sort of a structure on the latent space of your generative model.
So the main idea was, let's try to use equivalence priors so that we can just solve this whole problem
altogether. Awesome. Now I've talked with colleagues in the past of yours about Equavareans as well.
I think in the context of those previous conversations, we're primarily talking about supervised
types of problems, has Equavareans been applied to generative types of problems previously?
Yeah, this is a good question. Actually, the answer is yes. The answer is in context of
generative model, there have been some works that try to incorporate this equivalence into the
model architecture. The way we kind of did that in our work, they're kind of equivalent VAE
that we built, as far as Vino, as far as I know, this is the first time that we have such an
equivalent VAE constructed. But at the end of the day, I think another example that comes to my
mind is the kind of equivalence normalizing flows, that's something that has been
published I think last year. So one challenge of those works are very nice and elegant,
but kind of scaling them up to kind of a problem that wants to have low complexity, low latency,
which is actually the main motivation at the end of the day. Scaling them or bringing them to that
to that type of problem, it becomes a bit challenging. For example, for normalizing,
Equavare normalizing flows, you know, you had to work with this type of a kind of continuous
time and neural networks, like ODE type models. And training those models are difficult,
scaling them up are difficult. And what we're targeting here is just having a nice small
equivalent models that can already, can perform already quite well compared to the counter-counter parts.
How did you, assuming you started with a traditional VAE type of an architecture, how did you
evolve it to be equivariant based or to understand equivariants?
Equivariants, you know, the variational auto encoders, you have an encoder network and decoder
network. The decoder network is the network that is going to be used for our recovery task,
because that is the generator part, and we're going to use the latent space. So we want the
output orientation change. So basically transformation of our signal translates into the transformation
of our input. So what we want is that the generator of the network should be equivariant,
so that the decoder network will pick an equivariant network off the shelf from the existing models.
Now, what happens to the encoder network? The encoder network basically in VAE gives
parameters of your approximate posterior. So if you're assuming that we have a Gaussian type
of posterior, you have a mean value, and you have a Koreans matrix basically. Now, when you rotate
the image, you, since this, you're going to, your encoder network is going to give you a
distribution basically approximate posterior, you will get a new distribution when the input
is rotated. What should be this distribution? So the random, consider random variable corresponding
to the untransformed image, right? Then you transform that image, you want to get a new random
variable, a random variable, that is the transformed version of that. So basically, if the
distribution that you want, you want it to correspond to the transformed random variable.
So, okay, so that is, that would, let me know. Now, what happens to the parameters of this
distribution? Let's start with mean value. The mean value, as we wanted, is when we transform
a random vector with the rotation matrix, for example, then the mean value of that random
vector is also transformed according to a, to rotation matrix. So this basically means that
the, the part of the network giving us mean value should be equivalent. So that is also
a figure out. The challenge is for the covariance matrix. Then you transform your random vector
according to a matrix, let's say, A, you have A times your random variable, random vector.
The covariance matrix changes according to A times the previous covariance matrix times
a hermitian. So we do not have like the classical equivalence anymore because if you transform
your input according to A, the covariance part should be transformed according to A, A hermitian.
First conclusion out of that is that we cannot use the typical diagonal covariance matrix assumption
used in VAs. So we need to consider like a full covariance matrix. Another thing is how we're going
to build this kind of, this transformation, this, this type of network. So the way we did that,
we said, okay, with covariance matrix is a positive definite matrix. So we can already
transform, you write it down as a V, V transpose. We considered other ways of parameterizing that
as well. So we write the, the covariance matrix, the positive definite covariance matrix as VV
transpose. And then if that V matrix is equivalent. So it's just transformed. Then the whole
covariance matrix satisfies the condition that we want. So we built the, the, the, the, the
decoder network giving us the encoder network, giving us the covariance matrix in a way that,
that part of the network is, is equivalent and it's just parametrizes basically the V presentation.
There are other ways of doing that, but we tried that also in the paper more or less they give
same, same performance, all of them. And does the way that you approach that, does that apply
constraints to your inputs? There's no, there's no constraints necessary on the input.
So the only thing you need to know is how the transformation that you're interested in,
that's a rotation, is acts on the input space. That's what you need to do in order to build your
equivalent networks. You need to know how the input is transformed so that the equivalence is
built into the architecture. That's the only thing you need to know. Otherwise, you don't need to
put any constraint on it. And so how did you go about assessing the performance of the
architecture? So, yeah. So what we did is said, okay, let's, of course, we are interested in
recovering signals with unknown orientation, right? So we said, let's try to pick like a powerful
flow networks with, you know, the most powerful flow network that, the normalizing flow network
that we can have, like a typical real MVP setup. And then let's try to iteratively find
the orientation and kind of a latent space latent code by just the gradient descent on the
latent. This is not, this is the so-called vanilla or like a baseline that we had.
And then we compared that with our VAE, equivalent VAE method. We had other baselines as well,
but I think the normalizing flow is the most interesting part because it usually gives the best
result in terms of reconstruction. So we noticed that our equivalent VAE is much simpler,
it's much smaller, and it already gives same or better result than flows in many cases, which
is more complicated and then you have to do all those iterative assumptions and all that.
And then something interesting happened as well that even if the orientation is known,
so or you don't have any random orientation, actually equivalent VAE turned out to perform
quite well in this task. And our conjecture is that actually the equivalence
adds certain additional structure on the latent space that makes the life of optimization
algorithm much easier. And therefore, you know, we can do, we can do a lot, a lot with it.
So that was an interesting thing. So the gain was not only in terms of reconstruction quality,
not only in terms of finding unknown orientation, but also in terms of latency, convergence,
which are kind of important metrics for these generative priors. I really think like
lattice in convergence or do metrics that are quite important for applicability of these
methods in practice, especially if we could target something that requires, you know,
implementation on edge device, low latency constraint, that's quite crucial.
You talked about wireless and cryo electron microscopy as the use cases in the microscopy case,
there's kind of this obvious image that you're trying to work with. And wireless are you
applying, are we talking about applying this to images that happen to be transmitted wirelessly,
or is there some application of the same idea to broader wireless problems?
That's an excellent question. Actually, so let's start with the new kind of frequency band that is
being introduced in 5G. It's kind of a millimeter wave frequency bands. So when we go higher in
frequency for wireless communication, what happens is that the beams become very, very, basically,
you need to use directional beams to get the performance that you want. These are some artifacts
of like having a smaller antenna aperture and all that. So to to summarize the whole thing is that
you you just cannot rely on kind of a rich scattering environment that you have in order to get
signals that you want. You need to really be able to steer your beams, your antenna beams,
in a direction that kind of gets the most energy and also direct your beams and design your beams
accordingly. Now, if you want to do that on your device and kind of I leave it at a high level
like that, if you want to do that on device, you're usually kind of keeping your device and you're
talking and then you put it on the table or walk around, your device is permanently rotating.
But the environment is the same environment. It's just the way you view it is changing. So it's
kind of undergoes some transformation. So that is the main main idea in wireless communication.
So that's why kind of having incorporating those geometric priors in our design, wireless design
becomes really, really crucial, especially when we talk about these high frequencies.
Speaking of wireless, what are some other research areas that you're pursuing in that domain?
I think, you know, look at wireless communication. At the end of the day, we are working with
with lots of physics there. You have Maxwell equations and at the end of the day, there's an
electromagnetic wave moving from one point to another undergoing a lot of effects in an environment,
reflected from different objects scattered, diffracted and all that. That's the underlying
nature of wireless communication is kind of physics, right? So modeling these complicated
complicated propagation effects precisely, real precisely, it's a challenging task.
And the way people traditionally build models, at least so far, so you have two class of models
for wireless. One is kind of a ray tracing type models, which is based on just, you know,
kind of deterministically following paths of the ray. And you have what I can call statistical
channel models, that kind of build an average case statistical model that you put a distribution
on the channel gains that you have on the delays that you get and you work with those hard-coded
assumptions. Now, with machine learning, we know that using data, we can build much better
models if we can just use those models to build either better statistical channel models or
better kind of a more especially consistent channel models. So one of the research areas that are,
you know, or is quite important for us is exactly this channel modeling, which basically is about
kind of learning physics. This is kind of a topic. It's not limited to wireless. You know that
there are other folks working on neural networks, a physics-inspired neural network or
neural networks that can learn physics. So that's a topic that is quite interesting. And one
thing that like we did was, you know, let's try to gather very simple field data, not requiring
really kind of hard really like special devices. And let's build a generative model on that that
generates the underlying channel. So implicitly learns the underlying channel. So that's kind of one
of the one of the works we had and we continue to work on on those topics as well.
The other thing that is quite interesting is wireless perception. So again, our visual perception
is it's still based on electromagnetic waves. It's just a different frequency. Of course,
it's much higher frequency than what we're working right now in wireless. But on the other hand,
in wireless we might have access to much more details, much more refined concepts than what we
probably have visually. So the question of wireless perception and sensing is quite important as well.
Like how we can build right now, let's say, machine learning models that can perceive the
environment through the learns of wireless signal. And also that was one of the topics that we're
working on basically trying to solve a simple slam problem by incorporating physics of
propagation into the machine learning model and try to learn that from a wireless signal.
Slam being the same kind of slam that comes up in robotics, simultaneous location and mapping,
I think. Precises, yeah, some sense of localization mapping. So that's exactly the same thing.
But this time you want to do it with wireless signal. Awesome, awesome. So kind of returning to
ICML, what are some other papers that you and your team or other teams that Qualcomm are presenting
there? Yeah, I mean, there were a couple of very interesting works from my colleagues at Qualcomm.
And the first paper that I want to mention is a paper that has an oral presentation paper this
year at Qualcomm. And it is about basically oscillation of, you know, a quantization of a training.
So basically, and dealing with that, those problems. So to give a little bit of background on that,
you know, again, you you probably talked with some other colleagues before that they work on
quantization and compression of neural networks. So if you know that,
yeah. So if you know that kind of with quantization, let's say with post training,
a quantization techniques, you train a network, you go and quantize it, you can actually get
very good performance decent performance with 8-bit quantization out of these networks, right?
So this is quite quite quite interesting. However, if you want to push it down, push this
quantization down to four bits, then just post quantization post training quantization techniques
are not sufficient for pushing the performance. So we need to basically do what people call
quantization of their training. This basically means that the quantization noise that kind of
is added because of quantization operation, you include that in the training process. So what you
are doing is that you quantize and then you backprop through the quantization operation.
Now, back and learn to minimize those errors as it's training. Exactly. Exactly. Now,
then you backprop through the through the quantization operation, basically the technique that's
used is straight to estimator, one of the techniques. So the straight to estimator, what it does is that
you basically have your shadow weights, basically weights that they live in real valued numbers.
And they get quantized by quantization operation and gives you your quantized value.
And now, when you backprop at inference, you work with only quantization quantized value.
So that's the ultimate network. However, when you're training it and you backprop through that,
you straight through estimator is roughly speaking means, okay, forget about the quantization
operation that you have here, just directly backprop through the shadow weight. That was the real number.
So that is the rough idea. Okay, that already helps kind of overcoming some of the some of the
difficulties. However, it's not it is not sufficient. And one interesting phenomena that can happen
is that then shadow weights are close to quantization threshold. So basically threshold between two
different quantization beans, they start to actually oscillate around that. And if you look at the
quantized value now, they oscillate between different quantization. And this oscillator behavior has
been reported before and it's kind of a peculiar phenomena. The interesting thing about this
phenomena is that, first of all, the learning grade, but by choosing different learning grade,
you cannot control this oscillation. The oscillation, the amplitude might change, but the frequency
of oscillation remains. And what my colleagues noticed that is actually this frequency is dependent
on the gap between the best, the ground truce value of the optimal value and the quantized value.
If the gap is larger, that oscillation is going to be bigger. If the gap is smaller,
the solution can be smaller. So something something like that, the more details are in the paper.
But they observed such such a behavior. And then what are some drawbacks of this oscillatory behavior?
The first drawback is the discrepancy in bachnormous statistics, meaning that the bachnormous statistics
that you compute during training is going to be different from what you will get at test because
of this oscillation. So what you need to do, you need to recompute the bachnormous statistics,
which manages to solve this discrepancy. However, the problem is not only that. Actually, it has
negative impact on the optimization process as well. This basically means that
if you look at the optimization process, this oscillation actually prevents network to
converge to a better local minimum and gets better performance. So it is important to kind of
overcome this oscillatory behavior. And on that last point, the idea is that beyond just
resulting in a larger error, it will prevent converges altogether.
Or converges to a good good point. So you might converge to something that is not a good place.
So that is the main issue. So the other thing is that, okay, what are some solutions? How
you can actually deal with that issue? So my colleagues actually proposed two methods. The first one
is a dampening regularizer. The idea is, when you're close to quantization threshold, you're going
to see this oscillatory behavior. So what if you put a regularizer on training that pushes the
weights to be close to the center of beings instead of quantization thresholds? And that already
is quite helpful. The other approach that they have is about freezing the weights. What does that
mean? This means if you see that the weights are oscillating, then you freeze them. You just
don't update them because you know that this is going to have a negative impact. But there are
subtleties in the way you do that. First of all, how do you decide if you freeze a weight or not?
You have to monitor the oscillatory behavior and oscillation frequency. And you basically have to
compute that oscillation frequency, which they do by using some kind of exponential moving average.
And then put a threshold on that. So this is, are you tracking this and managing this on a weight
by weight basis? Or is it, are you looking at some notion of oscillation that's kind of vector
based? So as far as I know, this is weight by weight basis. Again, I encourage everyone reading
the papers in all details to make sure I'm not destroying my colleagues' work. But I think it's
a weight by weight because you compute it in a weight by weight basis. So at the end of the day,
what happens there is that, yeah, you monitor that after a certain threshold, you just freeze it,
and then you freeze it to a value that kind of occurred most. So there's actually like a mechanism
for selecting that. As a matter of fact, it manages with this technique, you can actually bring the
quantization with down to, let's say, four bits, three bits, for, let's say, mobile net efficient
net type architectures on image net data, which is quite remarkable result. Wow, wow.
And is oscillation, you know, to your knowledge, is oscillation been the primary impediment to
reducing the level that we're able to quantize to? Or are there other key challenges there?
Yeah, I mean, again, I have to give a caveat that my colleagues are definitely more knowledgeable
in that area. But that definitely was one of the main factors, for sure, because after fixing
that oscillation problem, you could do a great job that you couldn't do before. So this is
definitely one important factor. And there are another couple of papers that your colleagues
are presenting. Exactly. We can rapidly cover some of those as well. So on, on personalization,
there's this paper of my colleagues on variational, on the fly personalization paper. So
when you deploy a model on edge device, sometimes you would need kind of personalization of that
model specifically to to the user that is using it. Let's say, for example, let's talk about
the speech verification, right? You want to be able to personalize the machine learning model
that you have for a specific user that is using that service, right? Now, there are some challenges
for personalization. You really don't want to do on device training because of the kind of the
you don't know what's happening. You have to monitor everything and it's kind of challenging. You
want to do it in an unsupervised way, right? You want to do it few shots. So you really don't want to
get that much label from the user and you want to do it with few samples. And yeah, I mean,
you don't want to basically send data back to a source. So you really want to be able to do it
on device and kind of without requiring great training and all that. So then this personalization
problem was formulated using a variational principle. The idea is that you actually train so-called
variational hyper-personalizer, so we can call it that way. So there's this hyper-personalization
basically network that gets those few samples and basically based on that updates
weights of the model according to those samples. So it's just a way of adjusting the distribution of
those weights to the model that you're getting. So of course, there are more details in the paper,
but the core idea that we have is we have there is that, you know, this is the first time that you
can do on the fly personalization. And yeah, this is I think it's very, very important for
age device machine learning model deployment. Awesome. Awesome. And the last one that we wanted to
cover was on kind of causal identifiability for or from temporal intervened sequences or citrus.
What's that one about? Sounds juicy. Indeed, indeed. That's a that's a that's a 40 page paper. So
also encourage everyone to to go and read all the details. It's a very exciting topic. I mean,
we all know that causality and identifying causal factors are quite quite important. So
so the core idea of the paper is that you so first you want you represent your causal factors
as a multinational vector. So that's the that's the first thing. This is representation of causal factors.
The second thing is that the data that you have and based on which you try to solve this problem,
this causal identification is basically what it works based on access to a temporal sequence.
In which there are some intervention is happening. So basically there are some interaction with
with the scene. So through that you can actually figure out some causal factors out of those
those interactions and out of the temporality that you have. So then there is a V.A. model
is proposed where the the causal fact that the latent space of that are basically in the latent
space of that you can disentangle causal factors represented as this multidimensional factors.
So basically you can in the latent space of that V.A. you can actually disentangle causal factors
using having access to those things. And the other one interesting thing about the paper is that
this is just not like a you can use that V.A.E. but as a matter of fact if I give you an arbitrary
autoencoder, pre-trained autoencoder without knowledge of this, then it is possible to train
a normalizing flow that manages to disentangle causal factors from the latent space of that autoencoder,
which is actually quite an interesting thing. So definitely encourage everyone to go and read
the details of the of the paper. Awesome. And before we finish up we also wanted to touch
on a workshop that you're participating in your colleagues. This one on Bayesian optimization.
Yes exactly. So that's also an interesting interesting paper by my colleagues on micro based
on optimization for macro placement. So what is macro placement? So if you look at kind of a
cheap placement, cheap placement problem, you you basically have a macro unit. Still though those
are memory blocks that you have and then you have standard cells that you can put on on the chip.
So usually you design the you know you place all those objects in chip under some constraints.
These constraints basically are basically you know you can say power performance area. Those are
very important constraints, but to evaluate these for each placement that you choose to evaluate
those that the objecting function that you have is very costly because it has to go like a lengthy
simulation and kind of compute everything. So you really cannot integrate that in the optimization.
So you have to use some sort of a black box optimization for that. And people kind of
traditionally use simulated annealing type methods, but Bayesian optimization actually
provides an elegant way and alternative way of solving this problem. It's a combinatorial
optimization problem. So it's a very tough problem. And Bayesian optimization roughly like you build
a surrogate model and you have access to use a Gaussian process to build a surrogate model and
basically integrate that into use that for your optimization. So the contribution of the paper
is basically solving this macro placement problem using Bayesian optimization. Also very,
very interesting work and yeah of course quite important for what we took. Got it.
And that one's at the real ML workshop. Do you know what the real ML acronym is?
I cannot remember. But it was I think it has to something to do with active learning for real
world systems. Real world active learning and machine learning or something like that.
Exactly, exactly. But it cannot remember. Those are complicated names sometimes.
Awesome. Well, Arash, in terms of your research on kind of machine learning for wireless, what are
some of the future directions that you're excited about? Yeah, I would say I think I touched a
little bit on this modeling part. The fact that the wireless channel modeling at the end of
today is about learning physics. Neural networks that are integrated with this bias coming from
physics or they can learn part of physics. That's a very, very interesting and exciting
the research direction for me and kind of investing a lot of time on that. I hope that in
a couple of months I can be back and talk about that. But that's a very interesting research
topic and I think it can be quite game changer. Well, thanks so much for joining and sharing
a bit about what you're up to and your work, your presentations at ICMA. Yeah, absolutely.
Thanks for having me. It's a lot of fun.
