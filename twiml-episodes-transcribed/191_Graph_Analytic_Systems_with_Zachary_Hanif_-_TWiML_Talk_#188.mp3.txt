Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
In this, the final episode of our Strata Data Conference series, we're joined by Zachary
Hanif, director of machine learning at Capital One.
Zach led a session at Strata called Network Effects, working with modern graph analytical
systems, which we had a great chat about back in New York.
We start our discussion with a look at the role of graph analytics in the machine learning
toolkit, including some important application areas for graph based systems.
We continue with an overview of the different ways to implement graph analytics with a
particular emphasis on the emerging role of what he calls graphical processing engines,
which are systems that excel at handling large data sets.
We also discussed the relationship between graph analytics and probabilistic graphical
models, graphical embedding models, and graph convolutional neural networks in deep learning.
Capital One is a long time supporter of my work in this podcast, and I'd like to spend
a big shout out to them for sponsoring this series.
At the NIPPS conference in Montreal this December, Capital One will be co-hosting a workshop
focused on challenges and opportunities for AI and financial services and the impact
of fairness, explainability, accuracy, and privacy.
A call for papers is open now through October 25th.
For more information on submissions, visit twimmolai.com slash C1 NIPPS.
The letter C, the number one NIPPS.
Also a huge thanks to our friends at Cloudera, who also sponsor this series.
If you didn't catch my interview with Cloudera's Justin Norman earlier in this series, you'll
definitely want to check it out.
Cloudera's modern platform for machine learning and analytics, optimized for the cloud,
that you build and deploy AI solutions at scale, efficiently and securely, anywhere you want.
In addition, Cloudera Fast Forward Labs' expert guidance helps you realize your AI future
faster.
To learn more, visit Cloudera's machine learning resource center at Cloudera.com slash
ML.
If you're a fan of this show, please show some love to our sponsors and make sure to
tell them that Twimmol sent you and now on to the show.
All right, everyone.
I am at Strata in New York City and I am here with Zachary Hanif.
Zach is the director of machine learning at C4ML.
That's the center for machine learning at Capital One.
Zach, welcome to this week in machine learning and AI.
Thank you for having me.
It's pleasure to be here.
Absolutely.
Absolutely.
I'm going to go back a little bit.
He was a guest's presenter at the AI summit that I did in actually not even the AI summit.
It was the future of data summit.
It was a year and a half ago or something at this point.
Why don't we start out by having you introduce yourself to the audience?
Sure.
No problem.
As I've said, my name is Zach, I've spent probably about the last decade working inside
of machine learning and large scale data analytics.
The specific domain that I traditionally worked in has been in the area of security and
kind of adversarial domains, so paying attention to cyber security, fraud, money laundering,
all sorts of things like that.
You kind of play a cat and mouse game with the entity behind your data to try and keep
something safe and to keep it protected.
I've been doing that for about 10 years in a variety of different contexts and for the
last two, two and a half years now at Capital One, I've been a leader in Capital One Center
for Machine Learning, helping Capital One kind of bring machine learning into every single
corner, every little area of the business as a whole to kind of spread out the future,
so to speak.
Awesome.
Awesome.
And I interviewed Adam Wenzhel, who heads up that center not too long ago on the podcast,
so for folks that want to learn more about the center and what the center is up to.
We had a really great conversation about how an organization like Capital One particularly
with all of the constraints that a financial services organization has, how an organization
like that builds out a capability for data science and machine learning, and we'll link
to that show in the show notes, but that you've got a presentation later today actually.
What's the title of your presentation?
So my presentation is about doing graph analytics over large data sets, and it's a survey at
the end of the day of modern open source technologies and techniques for doing graph analytics
and machine learning over graphs.
I'll be perfectly honest, the title suddenly escapes me, Sam knows, I just got off the
train coming up from the DC region, which is where I'm based out of, so my head's a little
all over the place.
But the talk's a whole is kind of an examination of how we use graph analytics and how I've
used graph analytics in the past to solve a lot of very hairy problems, especially inside
of security and cyber defense domains.
Why don't we start by having you give us some examples to contextualize what motivated
this look into graph analytics.
What are some of the use cases where it's been helpful for you?
Absolutely.
So graphs are helpful really anywhere where the connectivity of your nodes, the connectivity
of your data is relevant, the topology of the data is relevant.
Many fields have data sets where individual records really aren't related to each other.
There's a very well known trial data set, which pretty much any machine learning engineer
probably deals with at least once when they're getting an understanding of how to do machine
learning in a practical sense.
And it's this data set of flowers, some researchers somewhere went out and collected the pedal lengths
and widths and other as physical dimensions of flowers in a study in an attempt to determine
whether or not machine learning could differentiate different types of flowers from each other.
In data sets like that, those data sets don't really have, each element of data doesn't
have a relationship with any other element outside of the class that that particular row
represents.
Graphs are useful in kind of the opposite area where every single, every single element that
you have has a relationship or some kind of interconnectivity with other other elements
in your data set.
So where measurements of flowers aren't really interrelated in and of themselves, the
relationships between people or the relationships between entities definitely have a great deal
of contextual information embedded inside of their, the relationship itself, right?
So to kind of put a fine point on it, you and I have a relationship, obviously as a result
of our previous collaborations and as a result of this particular discussion here, now
you and I both have individual elements of data associated just with ourselves, right?
We're different people, obviously, but we are interconnected and that interconnection
in and of itself contains valuable information that we can learn from, which we wouldn't
be able to learn if we just simply looked at yourself or myself kind of in isolation.
Does that make sense?
It does.
Do you make it more concrete by talking about specific business applications that you've
looked at?
Absolutely.
Right now, my group is using graph analytics to gain a better understanding of money laundering
and other financial crimes that people attempt to commit using the modern financial system.
And as Capital One is very much a part of the modern financial system, we will occasionally
have individuals attempt to perform criminal activity, your laundrom money or something like
that, you know, inappropriately using Capital One's capabilities, right?
And so our job is to utilize graph analytics to detect this kind of activity occurring and
make sure that it is dealt with in an appropriate fashion so that it doesn't affect either
any of Capital One's customers, anyone else in the United States financial system or doesn't
pose the kind of moral and legalistic threat that the activities that generate this kind,
these kind of funds can represent.
Yeah, I imagine folks that are outside of the financial services industry might not have
an appreciation for the amount of regulatory pressure that banks are under with regards
to anti-money laundering AML, but also the amount of investment that banks make in fighting
money laundering.
It comes up in a lot of different areas.
Yeah, it's definitely very important.
Capital One's regulators have a very vested and very good interest in making sure that
the financial security of the United States remains strong.
And so they've definitely taken an appropriate stance in making sure that we are vigilant
about making sure the activity that goes on through our bank is appropriate.
And we take that responsibility very, very seriously, investing both internally and working
with, you know, working collaboratively with academics and other researchers and other
organizations who specialize in this to make sure that we're as correct as we can possibly
be.
Because I mean, there's a lot of, it's a very significant responsibility on our part.
And so you found graph analytics to be useful here.
When I hear graph come up, I tend to immediately think of a handful of different things on the
one end of the spectrum, I think of graphical models like from a machine learning perspective,
but I also think of graphical databases.
It sounds like you're incorporating graphical databases for sure.
Are you incorporating graphical models as well?
We're actually incorporating a lot of things across that overall spectrum, right?
Everything from graphical models, which would be things like obviously graphical,
basic networks.
There are some more modern techniques being used inside of the deep learning space for
generating graphical embeddings.
And of course, the traditional algorithms in the area of performing label and belief propagation,
right?
These are all different areas that you can apply inside of the probabilistic graph modeling
space.
We absolutely do do that.
In addition to that, we're working not just with graph databases in the vein of say something
like Neo4j or something like Titan or Janus graph, for example, we're also working with
graph processing engines, such as the material inside of Giraffe, Apache Giraffe, I should
say, Apache Spark, a GraphX, GraphFrames.
I believe Flink even has a graph API behind it, and even some more esoteric systems as
well.
So we're really trying to cover the entirety of that spectrum because having a diversity
in our modeling approaches really allows us to kind of explore the entire space, and
it gives us lots of options to try and find the best way to solve the various problems
that we're attempting to address.
So what's the best way to walk through that?
How do you cover any of your talk?
Do you kind of go bottom up or top down or use case out?
I'm actually going in my talk, starting with kind of going in the direction of starting
with graph processing engines, comparing and contrasting them with graph databases, and
then finally talking about what is kind of a pet project that we've got going on right
now, which is exploring neural network applications inside of graphs.
I chose that flow because of a conversation that I actually had about four or five years
ago.
Four or five years ago, I was working with a couple of my colleagues at another organization
and preparing a talk on using graphs inside of malware networks.
So getting an understanding of how malware is interrelated.
It was a different role that I was in, a different company at the time, and I was talking
with some of my co-workers at this organization.
And one of them mentioned to me that they were curious to know how we were going to make
this kind of graphical model scale because he said that in his experience, there were
a number of graph databases and they didn't scale very well at a certain level of nodes
and edges, and he said, well, how are we going to get around this, right?
And over the course of this conversation, it kind of became, I think it became clear
that both of us, there's a much wider variety of different things inside of the graph
analysis space that not everyone is completely aware of.
Certainly because of events like strata and the larger big data movement that came through
several years ago and is still very much ongoing, we've got a really great understanding
of batch processing and stream processing systems.
But graphs are still kind of a niche application inside of that space.
And so kind of as a direct result, when people think graphs, they think graph databases,
and they naturally think of some of the scalability concerns of graph databases in that space.
And they're not always aware of kind of the related material inside of graph processing
engines and some of the probabilistic graphical models and other ways to kind of solve and
tackle this problem while dealing with a model in a graph space.
So let's walk through this, the way that you do in your presentation, graphical processing
engine.
What are they doing for us?
How do they help you solve these problems?
So any kind of graph computation engine, graph processing engine is going to be a system
that's similar to the way we think about the relationship between Spark and Hadoop or
HDFS, right?
You have some system that works on some series of data either in memory or with disk flushes,
cashing, and it allows you to model your data as a graph and operate it, operate on it
with a graph DSL or using graph formalisms, right?
And so a couple of clear examples in the software space are graph X, which is attached to the
Spark project, graph frames, which is a library that's being built into and around graph X,
a Pachi giraffe, which is a BSP style graph computation engine that's modeled after Google's
prequel.
There's a now defunct project, I believe, called a Pachi Hama, or if it's still ongoing,
it's definitely died down in its activity.
You said BSP, BSP bulk synchronous parallel.
It's a model of computation, which is very extensible and applies well to working with
graphs in a distributed environment.
The works primarily off of the concept of performing message passing between individual
nodes or vertices in the graph and passing messages along the individual connective
edges.
It allows you to take a vertex centric view of your graph, which allows you to calculate
aspects of each individual vertex based on the connectivity of that vertex to its neighbors.
A good example of this would be something like belief propagation or reputation propagation,
where individual entities may or may not have some known level of trust inside of the
graph.
They propagate that trust to everyone else around them for multiple humps with some kind
of decay value attached.
What this functionally allows people to do is allows them to label a set of nodes, propagate
those labels and say, based on the things I know about and based on what they're connected
to, how do we start adding labels to those other nodes?
It allows us to, specifically inside of the work that we're doing right now, it allows
us to say, hey, we suspect that this particular entity of performing some kind of money laundering
or fraudulent activity, and I've seen that this person has transacted with 10 people
around them.
I don't have any view of the 10 people's reputation around this individual.
What should my view be?
So it's kind of a suspicion by association model.
The way you describe the BSP, it makes me think of, as opposed to a top-down analytical
operation on this graph, you're almost treating the graph as a distributed system.
And each of the vertices in the graph is transacting with its neighbors and accumulating,
for example, certain properties, and then you allow this to happen over time, and then
you can take a look at the graph top-down, I guess, and learn about these relationships.
That's absolutely the case.
I think that's a really great way of expressing how BSP works in practice.
It's a very interesting distributed systems architecture because it allows the graph itself
to compute in its own manner, and then there's a concept called a superstep where every single
vertex turns around and says, OK, I've done all the things I need to do.
I'm confident that there's no more work for me to do here when your graph says, hey,
each individual node is complete, then at that point you say, OK, we take a look at the
entirety of the graph, we see if it's reached convergence or some other stopping function,
and then do any kind of work that we have to do at that level, and then we allow the next
stage of computation to continue again.
Before you comparing these different graphical processing engines in your talk, or really
just sharing that they exist and what some of the major capabilities are, so it's really
more of an overview.
So sharing their major capabilities and kind of what makes them special when you should
be using them, right?
I think one of the big takeaways that I'd like to communicate to the audience is that we
should be using these graph processing engines when we want to work with very large graphs
that don't fit well into memory, and we're attempting to perform some kind of computation
on those graphs to calculate a value for each vertex or a large subset of the vertices
in the graph that isn't easily expressed in a traversal-based system, right?
Or maybe complicated enough, so that way each of the individual nodes kind of affects
all the other ones around it, right?
By doing this, it kind of allows us to do the thing that Spark and other distributed processing
engines are really great at.
It allows us to take a large amount of data, iterate it on it very quickly in memory,
right, and then present a result or more likely a long series of results, right, for the
user then to interact with in a more interactive method elsewhere.
And I think that one of the things that I communicate in the talk, or I try to communicate
in the talk, is that we want to use this in the same way that you would use Spark.
We don't use Spark generally speaking for truly interactive analytics.
You could build probably a text search engine and Spark or something like that, but it's
probably better to do those computations in Spark and then load that into something
like a elastic search.
And that kind of model, as applied to graphs, is one of the things we're talking about.
That's my transition point from talking about graph processing engines and graph computation
engines to talking about graph databases themselves.
Before we jump into graph databases, the systems that you mentioned, you'll certainly Spark
exists in the Java ecosystem. A lot of machine learning is happening in the Python ecosystem.
And I actually asked the previous interview guest this question earlier.
You seeing any activity in this graphical processing realm in that Python ecosystem?
There's a lot of graphical interest going on inside of Python.
It's mostly expressed in the area of probabilistic graphical models because there's so much
such a strong ecosystem around scientific computing inside of Python.
I think the JVM ecosystem is seen a little bit more in the distributed systems space for
similar reasons. There's a lot of material that already exists for building distributed
systems, especially when we're talking about graph processing engines that are built on
top or with a lot of the same fundamental concepts as some of the more traditional processing
systems. Spark, for example, it makes plenty of sense to allow
these developers to kind of have that historical basis and use those historical tools.
One of the things we're seeing a lot of, however, is that in the same way that Spark
has PySpark, we're also starting to see Python implementations of GraphX and Python implementations
of graph frames that call out to JVM-based systems on the back end or performing similar
computations themselves in a Python manner.
I think one of the areas that we're seeing a lot of excitement inside of Python space
for graph processing is not in the distributed system space, but in the single node space,
which is actually really exciting because there's a lot of stuff that you can do that is
arguably even more efficient in terms of certain metrics on a single node as opposed to multiple
nodes. This has been discussed in academic literature over the last couple of years from
about 2015 on, there's been an ongoing discussion in distributed systems, the academic literature
for distributed systems, talking about kind of what is the configuration that outperforms
a single thread and things like that. It's kind of interesting to look at the two different
areas where we're seeing a lot of this development. There's a great deal of sophisticated
graph processing libraries meant for single node work in the Python ecosystem, running
from network X to other more sophisticated tools that are very relevant.
Each of these camps is building on their respective strengths job on the distributed
computing side, Python on the scientific computing side and data science side and building extensions
and bridges toward the other side, but those historical strengths still remain.
Still remain, right? I think that's something that we want to encourage inside of computer
science as a whole, right? Identifying those foundational elements that have strengths
and allowing them to be good at them while opening up APIs and bridges, as you said, to
allow people who don't have necessarily that specific underlying knowledge of that foundation
or that language element to be able to use those systems effectively. This is a little
bit of a fringe area, but there's a fair amount of graph processing work going on right
now, even in the compiled language space. Right now, there's a researcher by the name
of Frank McSherry, who is doing a spiritual successor to the Niant system that was hosted
and published and worked on at Microsoft Research. It's called Timely Data Flow, and there's
a lot of work going on there. He is an incredible researcher.
That's heard his name before. Yes. He's involved in a lot of things. He was one of the authors
of the cost paper I referenced earlier. One of his major research areas is understanding
distributed systems. When do we need a distributed system? When do we need a single node system
and doing high-performance computing? He's been using Rust recently to do a lot of very
interesting things, exploring what the data flow model of computation is, and exploring
how that can be applied to more traditional data processing problems, as well as graph
processing problems. I mentioned this simply because I think that the area that he's working
is just fascinating. While that work, I don't think is entirely production ready yet.
By his own admission, the work that he's doing is he's exploring the space and doing
a lot of research. I think that there's a lot of stuff that's going on that's bearing
some very meaningful fruit, and I think people inside of the distributed system space
should definitely be aware of those discussions that are going on. You also mentioned a project
out of Google. What was that one? Pregel. P-R-E-G-E-L. Pregel? Yes. Much like the
MapReduce paper, which was launched some time ago, and then kicked off a whole new
revolution in open source computing. Google published another paper slightly there a couple
years after which described a system called Pregel, in which they describe building a system
internally that they built out. In some cases, if I remember the paper correctly, they
said that it had turned into one of their primary processing engines, competing with or
supplementing or assisting MapReduce, their MapReduce implementation internally at
the time, at least, and they called it Pregel, which relied on the BSP model of computation
to perform these large distributed graph computations. If you take a look, you can imagine
that they have tons of to do. Absolutely. To be completely clear, I'm fairly
sure that they published this paper and I have no idea how it's used internally or if
it's still being used today. I don't know anything about that, but it was a fascinating
paper when they published it at the time because it kind of exposed the concept of doing
distributed graph computations, how they solved particular, hairy problems in the space,
and they also exposed kind of an API, the internal API that they're utilizing, and saying,
if you define a system that has the following properties, it would be similar to the Pregel
system we have internally, and this is how we've modeled that style of computation. Again,
very similar to their original MapReduce and HDFS papers, right? Or GFS for them, I suppose.
That's an interesting system. I'd love to know what's happened to it since the publication
of that paper, but unfortunately, I don't have that inside our knowledge.
What's the open source project that's implemented that?
Giraffe. G-I-R-A-P-H. Apache Giraffe. Yeah, I may have misspelled that. Apache Giraffe
is the project, I believe, that's the closest BSP style implementation since then. I
think that while both Jelly, which is the graph processing system attached to Flink and
GraphX, which is the graph processing system, again, attached to Spark, both have Pregel
operators, so you can use the Pregel API. My understanding is that the backend is informed
by the BSP style model of computation, but has been continually developed and represents
kind of a bit of a step in a slightly different direction.
Meaning the backend of the Giraffe, as opposed to the Flink and the...
Yeah, the backend of Giraffe, I believe, is much more similar to the original publication
of the BSP model of computation, whereas my understanding is that both Spark, the GraphX
system and Flink system were informed by it, but are not as tightly tied to the BSP style.
And so, you transitioned from talking about graphical processing engines to graph databases.
Absolutely. How do you see kind of the state of the art there, or maybe let's go back
to the original flow, like, what's the role of the graphical database in helping you
solve the kinds of problems you're solving with graphical networks?
So in the architecture that I propose, right, I propose that users are probably best suited
to use a graphical processing engine to compute most of the values that they need, right,
to modify all of the properties in their graph, right, and then to output those results
into a graph database, which is much easier and much more native for humans to actually
interact with.
I think in my talk, I actually go out and say, you know, someone does a whole bunch of
math on things, and that's great, but at some point humans need to actually use the
resultant data.
And so one of the things I state is that because graphical processing engines are capable
of doing large-scale computations, but are not designed for interactive analytics, for
example, saying, show me all the friends of this particular individual filter that list
by the following criteria, so on and so forth.
One of the things that I suggest is performing as many of these computations as possible
beforehand, so if you want to do a label propagation or something like that, you do that in
your graph computation engine, and then you load the graph with all of those computed results
into a graph database, which is designed for people to interact with and allows you
often comes with dedicated domain-specific languages for doing traversals very efficiently
and very easily and very natively, right?
So in the same way that we probably wouldn't do dynamic astyle analytics using Spark, usually,
we would want to provide an analyst with a SQL front-end to be able to do that sort
of thing.
That's kind of the same model that we're applying here.
You compute as much as you can inside of your processing engine, and then you pass that
material over to a graph database, which actually allows someone to interact with it.
Historically, graph databases have had some degree or another of this graphical processing
capability built in, and if only because the graphical processing engines weren't standalone,
they weren't popular as standalone components, so for example, Neo4j has long had a fairly
well-developed graphical programming model that you can use for data that's in the database.
Is the idea behind separating that outstrictly scalability notion, or are there other reasons
why you'd want to do that?
Well, I think there's two main ones.
Scalability is absolutely one of the main cases.
I think the second one ultimately comes down to how you're comfortable expressing certain
concepts inside of that graph to kind of relate it back to a topic that I think probably
all of your listeners have worked within the past sequel, right?
Sequel is a touring complete language, but there's plenty of programs I wouldn't want
to write in sequel, right?
Just because it's touring complete doesn't mean that it's necessarily easy, or that it's
really the tool that's designed for this particular purpose.
And so while Neo4j, as you referenced, historically had the Gremlin interface, the Gremlin query
language attached to it, and I believe now supports the cipher query language, I wouldn't
suggest that either of those query languages are always the best choice for all of the
graph computations you may want to compute, right?
So even ignoring possibilities of benefiting from a distributed systems environment, right,
for scalability.
The way you are able to express your computations may be benefited by working in one environment
or the other.
Possibly due to my background, possibly due to my own biases, I generally have an easier
way of thinking about graphs from a vertex or a graph-based centric model of computation,
and so I tend to favor that.
That combined with the scalability metric is kind of where my recommendation comes from.
And is there an emerging, you know, gql kind of analogy to SQL for the interface between
the graphical processing engines and the graphical databases?
That is a super hot topic.
Okay.
So, I think that's probably the easiest way of handling that is doing your computation
first, emitting that in, let's just say, for the purposes of discussion, CSV or JSON
or something like that, and then loading that into your graphical database, right?
That may be a little ineligant at some point, but it definitely works.
And actually, there's a couple of new services coming out that make doing that particular path
a little bit easier.
Let's put that to the side for just a second, right?
One of the things, one of the projects that's definitely being worked on right now is an
interface to GraphX utilizing the Grumlin query language.
And I think that that's one very interesting way that you would be able to go about doing
this.
And I would be reasonably certain that that would allow you to kind of dynamically load
some data into various Graph databases that that may exist out there.
Or at the very least, it's an area that I'd like to see some development kind of go into.
There are new Graph Processing systems getting, not just Graph Processes, but Graph Data
Bases getting published on a regular basis. One of them is AWS's Neptune, which I should
at this point, insert my common disclaimer, I'm a gentleman at Capital One, but nothing
in here when I talk about any technologies is Capital One telling you to use any of these
tools.
I'm merely expressing the fact that I've used them before and I've had various results,
but I'm not a mouthpiece for my company when I'm talking to you.
Sorry, got to do that.
Neptune is a new graphical offering from AWS, which has a lot of interesting properties,
including saving snapshots of your data into S3, reloading snapshots, reloading data dynamically,
which might actually take the somewhat in-allegate method that I described kind of at the beginning
of this area of the discussion and make it a little bit more workable depending on the
situation.
Things to think about, taking that from kind of, I think I talk about mostly just dynamically
just loading your data and taking a snapshot, loading it into a graph database in my talk.
I talk about that specifically because that's something that's relatively easy to understand
and discuss and doing something in a more dynamic fashion or a less elegant fashion is a
matter of engineering and not always a matter of capability.
As a result, there's a lot of area that you can play in there and that's really judged
by what's your use case, what kind of reliability and speed guarantees you need to fulfill,
and that differs on every single area.
So I'm trying to describe more of a model of architecture, more so than these are the
specific links that you definitely should be using because all of these graph systems
at the end of the day, they're really specialist tools, even though some of them are becoming
more, I don't want to say generic, but more generalized frameworks or being built on
generalized frameworks.
I think graph computation is something that is very, very tied to the kind of work that
you're attempting to do and the kind of space you're operating in and as a result, you
want to be very sensitive to your use case at every single stage of the operation.
So maybe taking a further philosophical detour, do you think that in the space we need
something analogous to a standard query language or conversely do we not need it because
of what you just said that the architectures and the way we build these applications is
very use case dependent or is it the case that kind of times of change and if the relational
database was being invented now as opposed to you have for many years ago, maybe we wouldn't
have a sequel because we're so comfortable kind of throwing around Jason and manipulating
it on the fly, that kind of thing.
So it's funny, it's funny you bring that up.
I happen to be a proponent in the general case, certainly for graph databases and other
traversal based systems to say that a common query language is likely to increase adoption.
I think having the sequel standard is what allowed SQL databases to become as large as
they effectively have, just simply because of the fact that you knew that you could train
a series of people in a particular, in a particular style of SQL and it would probably
transfer in 90% of its volume to any other implementation of the SQL standard assets
pressed by my sequel or by Postgres or by Oracle or so on and so forth, there's obviously
some caveats there, but in general I think that led to many, many good things.
I would suggest that standardization around a query language or a standard for query languages
is probably something that will have similar beneficial effects as well.
I've got my own thoughts on which ones I think would be interesting to have, but I don't
know. I gave you an opportunity.
I actually favor Gremlin quite a lot, but that may be because that was one of the first
graph DSLs that I really worked with or spent any kind of time with and so that's probably
a bias that I have there, but I like the standard as a whole and I think that that organization
has done a good job of stewardship, the TinkerPop organization used to be a standalone company
which before that it was a research project from one of the key developers and they've
transitioned it into kind of a public Apache project since then and so I'd like seeing
that chain of stewardship moving forward and that's part of that key part of creating
an eventual standard.
I think there's a lot of good things about it, so that's one of the areas that I would
go in, but one way or the other I generally believe that standardization for query languages
definitely can lead to good things and at a bare minimum definitely leads to kind of
a greater ease of acceptance, certainly less shock when you deal with a new system for
the first time.
It's funny you bring up the statement of chucking around JSON and some of the more modern
database families moving away from SQL because I think we've actually seen, I'm remembering
back to when kind of the big wave of distributed key value stores started pouring out into
the Apache projects and the JVM ecosystem again systems like Cassandra for example which
originally was a pure key value store around because I think it was Cassandra 2.0 when
they started integrating the literal CQL query language into it which looks very similar
to SQL and I think we've seen that with a couple of other databases which kind of just
talks about SQL's overall kind of cultural dominance, the hegemony that it represents
is kind of very interesting.
The databases role is almost like if you're using a database to serve up a model like
you're building the essentially building the model, the graphical model using the processing
engine and then you're taking that and sticking into the database for more interactive use.
I think you're right, I think at the end of the day you're definitely serving the results.
I don't know if I want to say you're serving the model because it's almost a little bit
more restrictive right because your model in a standard machine learning environment if
you're just serving a model, you're definitely just serving one particular thing that takes
a defined input and exports some kind of defined output whereas for a graph database you've
got a lot more flexibility you can actually write your own queries and at some level you
can write secondary stage analytics on top of that if you if that suits your use case.
But you're definitely right it would be serving kind of a pre-digested at some level some
kind of digested result in an interface that's maybe a little bit more native to your
use and utilization.
Yeah, yeah, absolutely, okay.
And then so the next part of your talk is then starting to look at graphical models
from machine learning perspective and potentially even you mentioned graphical embeddings which
is I'm very intrigued as to the direction that that goes.
So graph convolutional neural networks are a very active research field currently and
I caveat any discussion about them by saying that like from an academic perspective the academic
community still isn't completely decided on whether or not they are a good approach,
how powerful they actually are and what's going on.
There's a lot of there's a lot of interest and there's a lot of papers that have been
published that speak to kind of different techniques and different approaches and different
ways of thinking about it working with this which have published positive results and
material like that.
But as anything inside of academia, two years, three years is not really enough time to
kind of come to a global consensus on things, right?
Even in the larger neural networks space that we're seeing right now, there's still discussion
on like what are we actually learning and how are we actually doing it, right?
And so in a more specialized area of that larger discussion, something that's even at some
level newer and has fewer researches kind of staring at it, of course that's a new
discussion as I'm going.
One immediate question I have is, has a standard data set, a training data set emerged for
graphical operations, like is there an image net for graph stuff?
You know that's interesting, there's actually a lot of different data sets that are frequently
cited in literature in general, right?
And they range from the very small to the very large, right?
Stanford, the SNAP data sets, Stanford publishes, I believe it's Stanford, but it's the SNAP
data sets, publishes this large list of publicly available graph data sets, and they include
things like the trust relationship of the website slash dot.
I don't know if I'm not sure what the demographic age range of your audience usually tends towards,
but if you're an individual of a certain age, you probably remember slash dot.
It's old, old kind of things before bread and dig and all that and stuff.
So the trust graph of slash dot is one of those data sets.
Live journal had a friendship listing, there's Twitter follower graphs, going all the way
down to the very small, which are things like kind of a famous graph testing data set called
Zachary's Karate Club, which is, I kid you not, a collection of I think just a couple
handfuls of individuals inside of a youth Karate Club associating who identified as friends
of whom, right?
So there's less than you publish this data set.
This isn't me.
That data set predates me.
It just happens actually at the different exactly entirely, but so there's a number of these
data sets that have been published, which are very well studied.
So there's a couple of them that get get changed about and because of the fact that image
that image that has a lot of benefits for it in that it's a very large, a very general
law, all sorts of different images are contained there in graphs, which model kind of the interconnectivity
of individual nodes mean that attempts at getting one standard kind of data set are going
to look very different.
The PGP Web of Trust looks very different than your Facebook association, then your LinkedIn
association patterns, everything looks a little different.
Looks very different in what sense?
The number of things that you connect to, how willing you are to connect to certain things,
and kind of the general level of interconnectivity of individual nodes, right?
I would say that probably on social media sites, Facebook and Twitter and things like
that, you're highly incentivized to connect with a large number of people.
People that are your close, close friends and people who are acquaintances that you've
interacted with a few times before, or even in the case of, I guess, both of those websites,
people who you don't know personally, but they produce content or have opinions that
you find interesting.
And so you want to subscribe to them to get their material sent to you, right?
As opposed to say something that's much more intentional, the PGP Web of Trust, for example,
for people who use encrypted communications for signing emails and things like that, those
are intended to be very intentional communications, right?
You wouldn't state in that kind of Web of Trust that you trust an individual you've never
met before.
You probably only trust individuals that you've deliberately sent mail to who you know
are the people you want to talk to, things like that, right?
So is the idea then that the popping it up a level, the density or sparsity of the networks
is dramatically different in, and maybe just not the level of density, but some quality
of the density is different in that because the methods that we use to process these graphs
are very sensitive to that, it doesn't make sense to have some general data set.
Yeah, I would say that's a good way of describing it, right?
It's the way the graph was formed, the way the graph evolves over time, you know, kind
of all these things are embedded in the actual structure of the network itself.
And so as kind of a result, I would suggest that when looking at, you know, papers discussing
graph computation, you'll usually see, especially looking at systems papers discussing graph
computation, you will see individuals publishing these papers study the way their algorithm
or their system performs over multiple different graphs, simply to be aware of that.
But there are definitely a series of these well-published, well-understood, well-studied
graphs out there.
That's not quite like ImageNet, but not that different.
So then these graphical neural networks, what's, can I give us a snap?
Parts out of the state of the state there.
Sure, sure, absolutely.
So the intuition behind it is, can we use a neural network to gain an understanding of
individual vertices in a graph, right?
And over the course of a series of papers, the concept of using neural networks effectively
modified convolutional neural networks to get an understanding of an embedding of individual
nodes inside of a graph has come about.
So in the same way that we kind of look at data that is traditionally, maybe a little
difficult to vectorize or very, very sparse, and we want to build a denser representation
of that data, word to vac being kind of the most famous example, but there's all sorts
of, you know, star to vac and all sorts of things running around right now.
We would want to do that for graphs as well, because for graphs, right, if you look at
it in terms of like adjacency matrix, adjacency matrices in large graphs are almost necessarily
very sparse.
There's only so many people that you can possibly know.
There are so many people in the world.
And so no matter how well traveled or socially, maybe you'll never really make a dent, a
meaningful dent in the overall population of the planet, right?
So for a theoretically maximal social graph, right, it's going to be a very sparse matrix.
Working with sparse matrices is very difficult.
There's all sorts of mathematical properties you run into to say nothing of like space
requirements and things like that.
And so one of the things that people would want it to do is, you know, to come up with
an embedding matrix to take kind of each of those individual things and bring it to an
a easily or more easily described point in space that you can perform distance measurements
on, right?
And graphical convolutional networks are a means to do such a thing.
And it's an active area of research right now and there's all sorts of different levels
of debate going on about it, which I personally find fascinating.
And one of the things I talk about in my talk is kind of what are they and a couple of
different ways that you can, you can work with them to some degree.
One of the things that I'm currently experimenting with now is getting an idea of whether or
not we can use graphical, the results of these embeddings, I should say, if we can use
these embeddings to perform more like this queries.
So let's say that we identify that there is a fraudster somewhere.
We know that this person has committed fraud and we know that the pattern of which this
person was committing fraud is somehow distinct or unique.
One of an investigator asks themself, how do I find more people like this?
Can you show me some system out there?
Can I be shown a listing of individuals who have similar patterns of their transactions
or of the way they behave inside of the financial network that may lead me to believe that
there is more people like this, this is a newer emerging trend, this is something we can
look at.
And getting an idea of what that looks like, we're currently exploring the use of embeddings
to be able to kind of see what that gives us and how we work with that.
So in the talk, I kind of expressed that idea, you know, introduce the concept of embeddings
and talk a little bit about how you would serve embeddings in a manner that would allow
you to do more like this queries in something significantly more efficient than, you know,
O to the N squared time, which is always fun.
You mentioned that it's very early in this space.
Are there specific areas where it's been or what are the characteristics of areas that
has been demonstrated to be a useful technique?
So there's a couple of areas, most of them in the areas of kind of clustering, graph
coloring, and association into groups, defining community detection.
That's probably the one that I've spent most of my time looking at.
There are a handful of others, of course, in the spaces of whole, but I would suggest
that that's probably the most fruitful and initial area, identifying communities inside
of a graph, which is a very difficult problem.
What is the pre-processing or training that's required to train up one of these graphical
neural networks?
Is it, are you basically kind of vectorizing your graph in some kind of way as some huge
sparse vector and just feeding that into a standard CNN or have we changed the...
So it's a more...
I mean, architecture, like...
As everything, it's a little bit more complicated than that, but that's a good kind of
imagined stuff, that's a good high level representation.
At the end of the day, you're still working with that adjacency graph, right?
The adjacency matrix.
And you're still passing kind of the window that you're working on of the neural network
over that adjacency matrix.
Oh, well that's a thing right there.
It's like, hey, we've got this huge connectivity, or the connectivity of our graph can be expressed
as this huge sparse matrix, but there's no way we're turning that into a vector and
feeding it to a network where we're windowing around within this environment.
Kind of like...
I guess you can think of it as a convolution.
I guess that's the whole point of this.
That's the whole thing.
And feeding the data into that convolution for the particular techniques that I'm working
with right now are very similar to performing kind of a random walk through that graph, right?
So you're following this random walk, you run a series of those random walks, and then
that gets you kind of what you're looking for based on a particular starting point.
And you feed that, which is effectively a subgraph into your neural network.
There's a couple of papers.
I'll send you a few after after we talk a little bit today, because right now the majority
of the work that I'm doing in that particular space is working with techniques that have
been kind of published and discussed and trying to see whether or not they give reasonable
results so that I'm trying to effectively, I'm trying to find kind of where, what the
best area to dive in is based on some experimental trials in the data sets that we're working
with now.
So for a kind of a deeper mathematical based understanding, I'll send you a paper or two,
and I think you'll be able to put it up for your listeners or something like that.
Do you know off the top of your head, some of the key researchers who are working in this
area?
I'm terrible with names, so I wouldn't be able to tell you off the top of head.
Well, we'll include it in the shout outs for sure.
Absolutely.
Oh, and fortunately, many of those researchers are really good about doing kind of reproducible
research.
So many of them have published the implementations they had for their individual papers on GitHub,
and you know, it's always really great to see it when researchers do that kind of work.
Absolutely.
That's always amazing.
Absolutely.
So we've walked through these kind of three elements of your presentation.
How did you some things up for folks?
So effectively, I tried some things up by drawing an architectural diagram saying like,
you know, if you're working with large scale graphs, if you have problems similar to kind
of this set of things, you may want to consider an
architecture that looks a little bit similar to this, right?
Where your underlying layer is some kind of large block data store, you have a graphical
processing engine directly over that, reading from that and performing its computations,
feeding those results through some means and mechanism into a graphical database, and
then serving the results of that graph database, either directly through interfacing directly
with the graph DB itself or through a series of APIs, a restful API or even a graph
of results or something.
Exactly.
Something along those lines, right?
Because at the end of the day, the goal of all of this material, right, is to enable
some kind of analysis, to enable some individual to perform useful work over that data, right?
And so providing that in different avenues and different ways is definitely relevant because
you'll have extremely technical and sophisticated investigators or analysts doing direct line
queries into the database.
You'll have others that are consuming that material through reporting or through kind
of a guided interface in some form or fashion, depending on what your use case is.
You will probably be in some spectrum there.
And so as a direct result, we want to make sure that you have the ability to communicate
those results to kind of all those different use cases.
Awesome.
Awesome.
Well, sounds like an awesome talk.
I'm glad to say much for taking some time to share with us.
Been a pleasure being here.
All right, everyone, that's our show for today.
For more information on Zach or any of the topics we covered in this show, visit twomolai.com
slash talk slash 188.
For more information on the entire Strata Data Conference podcast series, visit twomolai.com
slash strata and Y 2018.
Thanks again to our sponsors, Capital One and Clara for their support of this show in
this series.
As always, thanks so much for listening and catch you next time.
