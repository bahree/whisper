WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:36.480
A couple of weeks ago I spent some time at the Pegaworld conference in Las Vegas.

00:36.480 --> 00:41.920
The theme of the conference was automation, particularly in service of the customer experience.

00:41.920 --> 00:47.200
And I had a great time seeing all the advancements coming into this field by way of machine learning

00:47.200 --> 00:49.080
and AI.

00:49.080 --> 00:55.160
In this show, the first of our Pegaworld 2018 series, I'm joined by Kirk Born, principal

00:55.160 --> 01:01.080
data scientist at management consulting firm Booz Allen Hamilton.

01:01.080 --> 01:06.280
In our conversation, Kirk shares his views on automation as it applies to enterprises

01:06.280 --> 01:08.080
and their customers.

01:08.080 --> 01:14.240
We discuss his experiences evangelizing data science within the context of a large organization

01:14.240 --> 01:19.600
and the role of AI in helping organizations achieve automation.

01:19.600 --> 01:24.760
Along the way, Kirk shares a great analogy for intelligent automation comparing it to

01:24.760 --> 01:27.120
an autonomous vehicle.

01:27.120 --> 01:31.560
We cover a ton of ground in this chat, which I think you'll get a kick out of.

01:31.560 --> 01:36.640
Before we jump into the interview, I'd like to send a huge thanks to our friends at Pegasystems

01:36.640 --> 01:40.400
for hosting me at Pegaworld and sponsoring this series.

01:40.400 --> 01:45.440
One of the great announcements coming out of the conference was Pegasystems' new self-optimizing

01:45.440 --> 01:48.720
AI-powered marketing capabilities.

01:48.720 --> 01:53.800
This is a really interesting offering designed to reduce marketers' dependence on traditional

01:53.800 --> 02:01.200
segment-based campaigns and transition them towards real-time one-to-one customer engagement.

02:01.200 --> 02:05.920
These new capabilities will be available as part of their new Pegga Infinity platform,

02:05.920 --> 02:08.680
which was also announced at the event.

02:08.680 --> 02:13.840
For more info on Pegga Infinity, head to pegga.com slash infinity.

02:13.840 --> 02:17.440
All right, let's do it.

02:17.440 --> 02:20.200
All right, everyone.

02:20.200 --> 02:26.520
I am here at Pegaworld in Las Vegas, and I have the distinct pleasure of being seated across

02:26.520 --> 02:28.280
from Kirk Bourne.

02:28.280 --> 02:32.680
Kirk is a principal data scientist at Booz Allen Hamilton.

02:32.680 --> 02:35.480
Kirk, welcome to this week in Machine Learning and AI.

02:35.480 --> 02:36.480
Fantastic.

02:36.480 --> 02:37.480
Thank you, Sam.

02:37.480 --> 02:42.120
We're here with you in Vegas, or I should say hot Vegas.

02:42.120 --> 02:43.120
Hot Vegas.

02:43.120 --> 02:44.720
Not hot, Lanna hot Vegas.

02:44.720 --> 02:45.720
Exactly.

02:45.720 --> 02:49.600
The great time to talk about AI, machine learning, and other cool things in a hot environment.

02:49.600 --> 02:50.600
Absolutely.

02:50.600 --> 02:51.600
Absolutely.

02:51.600 --> 02:55.520
Why don't we jump right in and have you tell us a little bit about your background?

02:55.520 --> 02:56.520
Wow.

02:56.520 --> 03:01.480
So, I've been around the block a few times, so my background is astrophysics.

03:01.480 --> 03:05.960
I fell in love with trying to understand the universe years ago, which meant I fell

03:05.960 --> 03:10.920
in love with data years ago, because data tells us about things, how things work, and

03:10.920 --> 03:15.160
the universe is my system that I studied for decades.

03:15.160 --> 03:21.640
I worked at NASA for nearly 20 years on data systems to help other astronomers access data,

03:21.640 --> 03:25.080
as well as myself accessing data to study our universe.

03:25.080 --> 03:30.720
And during that period, I fell in love with concepts like machine learning, data mining,

03:30.720 --> 03:34.880
and things that we call data science nowadays, and even AI.

03:34.880 --> 03:39.200
And how can we do greater discovery from data?

03:39.200 --> 03:46.160
And so during those years, I eventually decided that I wanted to move out of the NASA world

03:46.160 --> 03:52.320
into education, to teach the next generation students, next generation workforce, about

03:52.320 --> 03:53.640
data science and data.

03:53.640 --> 04:01.440
So I left NASA that was 15 years ago, and spent 12 years at George Mason University in Virginia,

04:01.440 --> 04:03.640
teaching data science to students.

04:03.640 --> 04:08.680
And that's what I love doing, teaching, educating, informing people about this.

04:08.680 --> 04:13.680
And then this company, Booz Allen Hamilton, called me up a few years ago, and said, hey, how

04:13.680 --> 04:19.080
do you like to do that for our clients and for a bigger world than just the science world?

04:19.080 --> 04:21.160
And I said, yes.

04:21.160 --> 04:25.000
So I've been doing this principle data science thing for Booz Allen Hamilton for three

04:25.000 --> 04:26.000
years now.

04:26.000 --> 04:27.000
Awesome.

04:27.000 --> 04:28.000
Awesome.

04:28.000 --> 04:32.920
And you're also quite prolific on Twitter, and speaking at conferences, and kind of

04:32.920 --> 04:37.760
the whole gamut, I don't know that there's a point to that other than, I'm one of your

04:37.760 --> 04:41.840
Twitter admirers, and we go back and forth quite a bit on there.

04:41.840 --> 04:42.840
Yeah.

04:42.840 --> 04:43.840
Well, thank you.

04:43.840 --> 04:48.680
Well, one thing I don't do, I admire that you're doing is the podcast series.

04:48.680 --> 04:54.760
My son-in-law gave me a bunch of equipment a few months ago for my birthday and said, hey,

04:54.760 --> 04:55.760
here's your chance.

04:55.760 --> 04:56.760
Nice.

04:56.760 --> 05:00.800
I haven't quite put it to use yet, but no, it's, I really love sharing knowledge about this

05:00.800 --> 05:03.720
field. I mean, it's exciting what's happening.

05:03.720 --> 05:08.680
I don't, and I think we're part of a sharing economy in the world in general, but I think

05:08.680 --> 05:13.440
in the data science community, especially data scientists love to share their knowledge.

05:13.440 --> 05:17.480
If you look at all the hackathons that take place, people love to use their knowledge for

05:17.480 --> 05:18.480
social good.

05:18.480 --> 05:23.280
If you see all the data for good hackathons and competitions and activities that people

05:23.280 --> 05:25.400
participate in.

05:25.400 --> 05:29.800
And so it's just part of our community to share knowledge, to share what we know, to

05:29.800 --> 05:30.800
help things.

05:30.800 --> 05:33.360
And if we can earn a living doing it also, that's fantastic.

05:33.360 --> 05:34.360
Nice.

05:34.360 --> 05:35.360
Nice.

05:35.360 --> 05:38.440
Yeah, that is one of the great things about the community, both from a research perspective

05:38.440 --> 05:42.680
as well as the commercial side of it, the willingness to publish, for example, you don't

05:42.680 --> 05:44.520
see that in a lot of other areas.

05:44.520 --> 05:52.360
Yeah, I think the open source community, which includes Python and R, is sort of ingrained

05:52.360 --> 05:54.360
in this community.

05:54.360 --> 05:57.120
And as such, people are willing to share lots of things.

05:57.120 --> 06:00.960
You know, not just their code, but knowledge that they've learned and ideas that they've

06:00.960 --> 06:01.960
had.

06:01.960 --> 06:06.040
And I mean, I grew up in an era where people were very protective, you know, peer-reviewed

06:06.040 --> 06:11.480
research was published journals that you had to pay subscriptions to receive, and otherwise

06:11.480 --> 06:14.360
you wouldn't have access to the knowledge.

06:14.360 --> 06:18.040
And you know, I think the world has changed a lot that we believe more in sort of open

06:18.040 --> 06:23.600
science, open data, open knowledge, because it's really the benefit to society that this

06:23.600 --> 06:29.960
stuff brings that should take precedence over me getting some kind of special accolades

06:29.960 --> 06:33.320
or attention because I came up with a thought.

06:33.320 --> 06:34.320
Right.

06:34.320 --> 06:38.000
I've learned, if I'm old enough to know that if I came up with a thought, someone else

06:38.000 --> 06:43.960
probably has also thought, so I don't think it's worth arguing over who came up with

06:43.960 --> 06:44.960
the idea first.

06:44.960 --> 06:47.240
The fact is we have great ideas in this community.

06:47.240 --> 06:52.040
We love to share them with each other and to benefit our clients and ourselves and our

06:52.040 --> 06:54.120
society from it.

06:54.120 --> 06:57.000
So what is your day job at Booz Allen Hamilton?

06:57.000 --> 07:01.240
Are you primarily evangelizing and educating or are you doing?

07:01.240 --> 07:03.720
Are you involved in projects as well?

07:03.720 --> 07:09.000
It's mostly the former, but some of the latter, so primarily what they call the horizontal

07:09.000 --> 07:11.080
matrix guy, okay?

07:11.080 --> 07:15.040
So I like to tell people we have a thousand data scientists, the world's best kept secret

07:15.040 --> 07:20.280
and data science, because our data scientists are working primarily on client data, on client

07:20.280 --> 07:25.480
projects, on client site, and we're not allowed to talk about it.

07:25.480 --> 07:28.240
And so people pretty much don't know what we're doing.

07:28.240 --> 07:32.760
So all these different projects are in different vertical markets, mostly in the federal government

07:32.760 --> 07:37.960
space, but things like healthcare or military or intelligence or transportation or energy

07:37.960 --> 07:42.920
or treasury or Homeland Security, we got something going on.

07:42.920 --> 07:48.200
And there's chief data scientists who basically manage, not so much manage, but they basically

07:48.200 --> 07:54.520
oversee the talent acquisition and the work acquisition in those markets.

07:54.520 --> 07:58.800
So we have many chief data scientists, but the firm has one principal data scientist and

07:58.800 --> 07:59.800
that's me.

07:59.800 --> 08:03.600
So one of the ways they were able to extract me at the university made me an offer I couldn't

08:03.600 --> 08:07.960
refuse and he actually created a job title just for me.

08:07.960 --> 08:11.920
So my day job doesn't include interacting with some of these projects and occasionally

08:11.920 --> 08:16.920
doing some advising and mentoring of people who are working on those projects, but more

08:16.920 --> 08:22.520
often than not, it's the evangelization thought leadership is a phrase they like to throw

08:22.520 --> 08:23.520
around.

08:23.520 --> 08:28.320
It means lots of writing and public speaking, even executive advising.

08:28.320 --> 08:34.480
So I say I mentioned the newest members on the block and doing this stuff and the more

08:34.480 --> 08:39.800
senior executives who maybe have heard of this, but don't know quite what it is.

08:39.800 --> 08:46.800
And so I have this desire to share knowledge, and I sort of fulfilled that desire by

08:46.800 --> 08:50.400
being a university professor for 12 years.

08:50.400 --> 08:55.200
And I haven't surrendered that passion of teaching and mentoring and training by going

08:55.200 --> 08:56.200
to Booz Allen.

08:56.200 --> 09:01.000
I'm just doing it in more interesting and diverse environments.

09:01.000 --> 09:04.800
Much more interesting use cases than maybe the galaxies I used to work on, even though

09:04.800 --> 09:09.400
I used to love those galaxies, there's probably a handful of people in the world who cared

09:09.400 --> 09:11.400
about those galaxies.

09:11.400 --> 09:16.600
But now when I tweet something on Twitter, there's 200,000 people who are reading

09:16.600 --> 09:17.600
it.

09:17.600 --> 09:18.600
So that's interesting.

09:18.600 --> 09:20.600
That impact it now has.

09:20.600 --> 09:21.600
Right.

09:21.600 --> 09:22.600
Right.

09:22.600 --> 09:28.480
So we've been here at Peggo World for the past couple of days, and a lot of the conversation

09:28.480 --> 09:34.400
has been around automation, automation and service of digital transformation and service

09:34.400 --> 09:44.160
of customer experiences, and increasingly adding elements of intelligence to that process

09:44.160 --> 09:45.160
of automation.

09:45.160 --> 09:47.880
And what's your take on that?

09:47.880 --> 09:51.720
Well, first of all, I just love this.

09:51.720 --> 09:58.760
And one of the more trivial reasons why I love this is it gives me the opportunity to

09:58.760 --> 10:05.240
use one of my most favorite words in a sentence, and that word is confluence.

10:05.240 --> 10:08.840
So if you understand what confluence means and the confluence of rivers and if you ever

10:08.840 --> 10:11.760
visit such things, that's really interesting.

10:11.760 --> 10:13.080
But that's another story.

10:13.080 --> 10:16.160
But we're living in an age of the confluence of many technologies.

10:16.160 --> 10:23.160
Many technologies converging, merging, working together to a greater outcome in the same

10:23.160 --> 10:30.160
way that many rivers, tributaries, can merge together and create a mighty force.

10:30.160 --> 10:36.840
And so we have automation, we have AI, and when you think about customer experience, things

10:36.840 --> 10:39.960
like that, for me, it's like everyone has a customer.

10:39.960 --> 10:44.800
I get a little bit of friction with some of my astrophysics colleagues from days of

10:44.800 --> 10:48.120
long ago saying, Kirk, what are you doing now?

10:48.120 --> 10:49.920
What is the stuff you're doing now?

10:49.920 --> 10:53.800
And I said, well, that's not really all that different.

10:53.800 --> 10:59.240
Because back then, the people who we needed to impress in order to get grant money might

10:59.240 --> 11:05.960
have been the proposal reviewers or the agencies who fund our research or the paper reviewers

11:05.960 --> 11:09.280
to get our papers published in journals.

11:09.280 --> 11:13.200
And so we're selling our ideas, we're selling our thoughts, we're selling our work in some

11:13.200 --> 11:14.200
sense.

11:14.200 --> 11:18.800
Okay, maybe not in the strict sense of selling, but we're making a good case for what we've

11:18.800 --> 11:20.800
done as a value to someone.

11:20.800 --> 11:27.160
And so customers need to have those value propositions from companies, and they just don't

11:27.160 --> 11:28.160
want to hear the words.

11:28.160 --> 11:32.760
They want to sort of, I say, sort of the three questions have to be answered for that

11:32.760 --> 11:33.760
customer.

11:33.760 --> 11:36.160
What, so what, and now what?

11:36.160 --> 11:37.160
What is it you did for me?

11:37.160 --> 11:38.160
Why should I care?

11:38.160 --> 11:40.680
And what does that mean for me now?

11:40.680 --> 11:45.760
And so being able to answer those questions improves both customer relationship and customer

11:45.760 --> 11:49.080
experience, customer journey, whatever you want to call it.

11:49.080 --> 11:51.000
And every, like I said, everyone has a customer, right?

11:51.000 --> 11:55.040
So whether you're in a government sector and you have stakeholders or publicly traded

11:55.040 --> 11:56.960
company, you have shareholders.

11:56.960 --> 12:02.440
If you're actually customer facing, obviously you have customers, whatever it is, someone

12:02.440 --> 12:07.760
is, will buy your product, will buy your ideas, will listen to you, will pay attention

12:07.760 --> 12:08.760
to you.

12:08.760 --> 12:13.240
You know, if all you're selling are your words and your thoughts and your ideas to someone,

12:13.240 --> 12:18.840
you have to be able to say it in ways that are empathetic, that is puts the words in ways

12:18.840 --> 12:22.600
that they can understand it, that they see the value for them.

12:22.600 --> 12:27.560
And so it's putting yourself in their shoes and allowing them to sort of see it from their

12:27.560 --> 12:28.560
perspective.

12:28.560 --> 12:35.480
And as a data scientist, I love this way of describing what I do, one of the ways I describe

12:35.480 --> 12:38.600
my job to people is I talk the walk, all right?

12:38.600 --> 12:40.960
So there's a lot of companies out there that can talk the talk, right?

12:40.960 --> 12:46.560
There's AI, lots of hype, data science, machine learning, blockchain, I mean, you name it,

12:46.560 --> 12:48.840
there's hype cycles all over the place.

12:48.840 --> 12:51.200
People love to talk the talk, right?

12:51.200 --> 12:55.960
And there's a famous quote from years ago, when big data was at the peak of its hype cycle,

12:55.960 --> 12:59.760
a person said that, you know, big data is like teenage sex.

12:59.760 --> 13:00.960
Everyone is talking about it.

13:00.960 --> 13:04.960
No one really knows how to do it, but everyone thinks everyone else is doing it.

13:04.960 --> 13:07.640
So you claim you're doing it even though you don't know what you're doing, okay?

13:07.640 --> 13:12.680
So I think a lot of hype cycles are like that, people do a lot of talking the talk.

13:12.680 --> 13:16.600
And so we talked about moving beyond that to being able to walk the talk, being able

13:16.600 --> 13:20.360
to actually do the thing you're talking about, and a lot of companies are now reaching

13:20.360 --> 13:21.360
that pace.

13:21.360 --> 13:25.800
So when we see data analytics, I think we're past the hype cycle, AI is sort of peaking

13:25.800 --> 13:26.800
out.

13:26.800 --> 13:31.720
And I think we see tons of implementations, amazing processes being automated and all

13:31.720 --> 13:36.960
kinds of things being implemented, AI showing up and I'll crawl across enterprises everywhere.

13:36.960 --> 13:40.400
And so people are really, you know, walking the talk.

13:40.400 --> 13:44.640
And so I say, I see my role now as to being able to now to describe that to someone.

13:44.640 --> 13:47.800
You know, I don't want to sit there and describe something in very mathematical language

13:47.800 --> 13:51.800
and all about deep learning and neural networks and words that people have no idea what I'm

13:51.800 --> 13:52.800
talking about.

13:52.800 --> 13:57.200
I had to be able to explain it in a way that's empathetic, that is they can see why, you

13:57.200 --> 14:01.360
know, the what, the so what, and the know what, and this thing that we're doing.

14:01.360 --> 14:07.000
And so I take that as my personal role at Booz Allen Hamilton and in my life in general

14:07.000 --> 14:12.400
to be able to explain to people in those terms, what is this stuff we're doing?

14:12.400 --> 14:17.040
And I think this is a very interesting time to do that because of this confluence of

14:17.040 --> 14:18.040
so many technologies.

14:18.040 --> 14:21.160
And to be able to explain what we're doing to people, we have to be able to explain what

14:21.160 --> 14:26.800
machine learning is, what AI is, what big data is, what data, you know, what data privacy

14:26.800 --> 14:28.040
has to do with it.

14:28.040 --> 14:30.160
And we're not trying to steal people's identity.

14:30.160 --> 14:32.120
We're not trying to destroy the world with robots.

14:32.120 --> 14:35.520
I mean, so you got to have all kinds of different kinds of conversations with people, but you

14:35.520 --> 14:40.560
got to put yourself in their shoes at the same time not being untrue to yourself and

14:40.560 --> 14:42.800
untrue to the technology.

14:42.800 --> 14:47.840
And that's, you know, that's a fine line and I sort of enjoy walking that tight rope.

14:47.840 --> 14:51.000
What do you mean by untrue to the technology?

14:51.000 --> 14:53.800
That is slipping into talking to talk.

14:53.800 --> 14:54.800
All right.

14:54.800 --> 14:58.520
It's really easy when you talk to people about the AI machine learning and stuff like this

14:58.520 --> 15:02.360
and say, oh, it's going to cure cancer, it's going to change the world, it's going to

15:02.360 --> 15:08.640
reduce poverty, it's going to remove gender inequalities, it's going to fix our environment.

15:08.640 --> 15:12.440
And all of a sudden, you're just sort of like mouthing all these platitudes.

15:12.440 --> 15:15.840
And it's not being true to the technology because some of this stuff is just plain hard.

15:15.840 --> 15:21.800
And some of the AI's we see in businesses are pretty small and minor and that's okay.

15:21.800 --> 15:23.040
That's really okay.

15:23.040 --> 15:26.920
Like if you use a cell phone, right, use a smartphone and you're using text messaging.

15:26.920 --> 15:32.040
There's an autocomplete, right, a spell check, an autocorrect on your cell phone, right,

15:32.040 --> 15:35.800
as you're typing the word, it sort of completes the word for you, all right, so that's a type

15:35.800 --> 15:36.800
of head feature.

15:36.800 --> 15:39.640
It sees what you're typing and it sort of guesses what the word is going to be, even the

15:39.640 --> 15:40.960
next word.

15:40.960 --> 15:47.320
That's an AI, all right, that I don't think any autocomplete or autocorrect has ever come

15:47.320 --> 15:51.720
out of your phone and taken over the world.

15:51.720 --> 15:55.480
So we, so we can be true to the technology, say this, this is an AI, what you have in

15:55.480 --> 15:57.880
your hand is an AI.

15:57.880 --> 16:04.200
But it's not the kind of AI you see in the movies, right, all right, so I can, so I don't

16:04.200 --> 16:10.880
want to say that AI is this pure force that's going to cure all of the world's illnesses

16:10.880 --> 16:14.000
and problems and will live happily ever after.

16:14.000 --> 16:16.640
I mean, that's not being true to the technology.

16:16.640 --> 16:20.840
It's hard work and a lot of that hard work, it has to deal with the ethical questions

16:20.840 --> 16:24.400
and all the bias questions and data privacy questions.

16:24.400 --> 16:29.240
There's lots of really hard problems to solve there and it's not being true to the technology

16:29.240 --> 16:32.360
or to yourself to ignore those.

16:32.360 --> 16:40.800
I imagine that you have had encounters with executives at Booz Allen where, you know,

16:40.800 --> 16:43.800
you hear, well, why can't we just?

16:43.800 --> 16:48.320
And it's, you know, some, you know, then a miracle occurs type of statement or, you

16:48.320 --> 16:52.600
know, something that is more like, you know, maybe something out of the movies or something,

16:52.600 --> 16:53.600
you know.

16:53.600 --> 16:57.640
How do you deal with that beyond just saying it's hard?

16:57.640 --> 17:02.600
Well, to tell you the truth, it's usually the other way around, really, at least the

17:02.600 --> 17:03.600
bike experience.

17:03.600 --> 17:08.280
I find a lot of the, of course, we're a management consulting firm, right, so we go

17:08.280 --> 17:14.280
to clients and we're pitching our consulting services, whether it's analytics or digital

17:14.280 --> 17:16.720
or cyber security or something.

17:16.720 --> 17:20.520
And so we're selling to a customer, right, and this customer could be a federal director

17:20.520 --> 17:22.520
of some agency or something like this.

17:22.520 --> 17:25.760
And so they're a very critical person, right, they're not going to spend their money any

17:25.760 --> 17:28.640
more than when you would go buy a product, you're going to spend your money just because

17:28.640 --> 17:31.920
some salesperson says so, right, you're going to be critical.

17:31.920 --> 17:36.840
So I see a lot of good critical thinking and critical question asking from that side of

17:36.840 --> 17:42.080
the table, but occasionally what happens is on the data science side of the table, which

17:42.080 --> 17:43.080
is where I sat.

17:43.080 --> 17:49.360
And I see myself as a younger person doing this years ago when I was really getting excited

17:49.360 --> 17:53.320
about this automation and AI and machine learning stuff, and I first just sort of discovered

17:53.320 --> 17:58.040
it, if you will, 20 years ago, I was, I was that, I was that sort of, you know, pying

17:58.040 --> 18:02.120
this guy guy and the other side of the table, talking to my NASA clients, oh, we can be

18:02.120 --> 18:04.000
able to do all this space exploration.

18:04.000 --> 18:08.920
We can autonomously drive spacecraft around Mars and fight all kinds of interesting discoveries

18:08.920 --> 18:14.880
and new, you know, new water deposits and new titanium deposits, and we can build colonies

18:14.880 --> 18:19.320
on Mars and we're going to have an Amazon type service, automatic supply chain of delivery

18:19.320 --> 18:24.480
of the supplies that astronauts need just in time from a supply ship or in orbit that's

18:24.480 --> 18:29.080
delivering packages to their door by satellite deployment.

18:29.080 --> 18:31.720
And so I was going on and on with all this pie in this guy stuff.

18:31.720 --> 18:36.240
So I was, I was the guy who was sort of needed to be pulled back, you know, and of course,

18:36.240 --> 18:43.200
I learned a bit in my old age to rein in some of that sort of unrealism in the stories.

18:43.200 --> 18:48.520
And I think it works better because again, the person who's, who's buying, if you will,

18:48.520 --> 18:52.600
the client, who you're trying to sell a product or a service to, you know, they're not

18:52.600 --> 18:56.360
going to buy that bloney and it's, it's snake oil, right?

18:56.360 --> 18:59.800
And so I remember a friend of mine years ago in astronomy wrote a book called, with the

18:59.800 --> 19:00.800
internet first started.

19:00.800 --> 19:03.720
I mean, not the internet per se, but the web first started.

19:03.720 --> 19:07.560
He wrote this book about the, the internet being silicon snake oil, right?

19:07.560 --> 19:11.440
And he, he predicted the demise of the internet.

19:11.440 --> 19:13.000
He said, this will last a couple of years.

19:13.000 --> 19:14.000
It's just a fad.

19:14.000 --> 19:15.000
This is stupid.

19:15.000 --> 19:16.000
It's just snake oil.

19:16.000 --> 19:21.080
Smoking mirrors, and yeah, completely wrong, right?

19:21.080 --> 19:25.640
And so I sort of forgot about that, but a few years ago, I sort of remembered that he wrote

19:25.640 --> 19:26.640
this book, right?

19:26.640 --> 19:32.520
And so I went back to Wikipedia where, and looked up in his page there, and he actually

19:32.520 --> 19:38.720
has a pretty deep and long apology about how stupid and arrogant and naive he was, even

19:38.720 --> 19:44.720
though he was a PhD astrophysicist, he was, I'm like, a lot of PhD folks who don't normally

19:44.720 --> 19:50.200
admit when they're wrong, he was very humble about the fact that he really screwed up

19:50.200 --> 19:52.960
on that prediction.

19:52.960 --> 20:00.760
It's a tough line to walk, seeing, you know, as we do the promise of technologies like

20:00.760 --> 20:05.600
AI and others, but knowing the limitations or at least as you put it, the hard work that

20:05.600 --> 20:11.720
has to go to get there and knowing, you know, being able to kind of project, you know,

20:11.720 --> 20:17.400
how long it's going to take to get to some vision state X, and it's really hard.

20:17.400 --> 20:18.400
Yeah, exactly.

20:18.400 --> 20:22.480
I mean, if you think about like self-driving cars, there's an example where so many things

20:22.480 --> 20:27.160
have to work together, and autonomous vehicle in order for it to not only do what it does,

20:27.160 --> 20:33.000
but do it safely, and not only safely one time, but safely all the time in very different

20:33.000 --> 20:40.960
environments, and so step one, I mean, just get a car to steer straight, okay, get a

20:40.960 --> 20:45.840
car to recognize a stop sign, get a car to turn a corner, get a car to recognize the

20:45.840 --> 20:46.840
speed limit.

20:46.840 --> 20:52.560
I mean, just one little step at a time, and there's like so many steps.

20:52.560 --> 20:57.520
And so I think a lot of implementations and enterprises using AI, machine learning,

20:57.520 --> 21:02.320
automation, whatever you want to call that stuff, intelligent automation, again, requires

21:02.320 --> 21:08.720
a lot of moving parts to get right, and so we need to be more humble, so to speak, in

21:08.720 --> 21:15.760
our way of believing our own story, believing our own promises, that how far can we get

21:15.760 --> 21:21.960
an X amount of time, and I frequently say to people, and I still believe this, that one

21:21.960 --> 21:27.800
should think big, but start small, that is don't think small and start small, because that's

21:27.800 --> 21:35.640
not very useful, but think big, but no way of saying it is think strategically, but

21:35.640 --> 21:40.480
tactically, and my father was Air Force, so I learned some of that language when I was

21:40.480 --> 21:46.600
younger, that strategy is about winning the war, and tactics have to do about theater or

21:46.600 --> 21:50.640
our battle specific, so sometimes you have to lose the battle to win the war, sometimes

21:50.640 --> 21:56.280
you have to give up the hill in order to win the battle, okay, so it's not always about

21:56.280 --> 22:04.040
win, win, win, win, win, it's about the long-term goal, and that long-term goal is what keeps

22:04.040 --> 22:08.680
sure, certainly your North Star keeps your focus going, so you need to have that, but

22:08.680 --> 22:14.040
recognize that there's a lot of steps on the way, those sort of tactical steps, and tactical

22:14.040 --> 22:19.480
failure is, people say failure is not an option, I say strategic failure is not an option,

22:19.480 --> 22:25.800
but tactical failure is how you learn, so sometimes that's called fast fail, I get a lot

22:25.800 --> 22:30.800
of sort of knee jerk negative reactions from my clients when I talk about fast fail,

22:30.800 --> 22:35.440
they say we don't want to fail here, so now I call it just fast learn, okay, so we

22:35.440 --> 22:38.880
want to have a fast learn environment, and the implication is how do you learn, but you

22:38.880 --> 22:44.480
learn from mistakes, from failures, you learn from edge cases that didn't work out, and

22:44.480 --> 22:50.480
so you want to be in a fast learn environment, so that you can do these smaller incremental

22:50.480 --> 22:54.960
steps, and I learned a new expression this week, we used to call those sort of incremental

22:54.960 --> 22:59.640
steps minimal viable products, the MVP minimal viable product, now you're going to talk

22:59.640 --> 23:04.280
about the MLP, yeah, so this week, that was awesome, so this week I learned about the

23:04.280 --> 23:09.440
minimal lovable product, and I said I like that minimal lovable product, and one of the

23:09.440 --> 23:18.400
keynotes here in the world, so I think that's going to be my new thing, MLPs, I didn't

23:18.400 --> 23:22.200
say NLP for people who are listening out there, that's natural language processing,

23:22.200 --> 23:28.560
I said MLP, minimal lovable product, and remember, I mean data like products are like

23:28.560 --> 23:34.240
your children, you love all of them, because even when something goes wrong, you learn

23:34.240 --> 23:38.520
from that, and that's what this is all about, it's about learning.

23:38.520 --> 23:42.760
You mentioned just a moment ago self-driving cars, and I've heard you use a really interesting

23:42.760 --> 23:49.720
analogy between applying the idea of self-driving cars to the enterprise to explain intelligent

23:49.720 --> 23:55.560
automation, the theme of this event, what does the self-driving enterprise mean to you?

23:55.560 --> 24:00.360
To me, it means basically doing what the self-driving car does.

24:00.360 --> 24:06.400
It senses its environment, it sort of sees what's coming, takes an action that's going

24:06.400 --> 24:12.320
to optimize the outcome, and it uses all the contextual data, so what I just described

24:12.320 --> 24:18.360
is descriptive analytics, predictive analytics, prescriptive analytics, and cognitive analytics.

24:18.360 --> 24:22.680
So a self-driving car, it's collecting data, what's going on now, so it's diagnosing

24:22.680 --> 24:25.760
its environment from sensors in the car.

24:25.760 --> 24:29.240
An enterprise, no matter what it is, whether you're customer engagement, your sales, your

24:29.240 --> 24:35.640
marketing campaign, your employee activities, your human resources, anything in your business,

24:35.640 --> 24:40.520
and across your enterprise, whether it's an enterprise specific or a customer-facing

24:40.520 --> 24:44.400
thing, you're collecting data.

24:44.400 --> 24:46.520
Just like a car, you're collecting data.

24:46.520 --> 24:51.680
You got to do more than just collect data from your sensors, you got to do something.

24:51.680 --> 24:53.840
You want to take some action.

24:53.840 --> 24:58.120
So part of your action is to sort of look ahead and see, well, where is the road going?

24:58.120 --> 25:03.080
So let me make sure I stay on the road as I move forward.

25:03.080 --> 25:07.760
So that's the predictive model, so you see what's ahead, so you can move forward in that

25:07.760 --> 25:08.760
direction.

25:08.760 --> 25:14.240
So if you're trying to increase sales and prove customer interaction or whatever, you

25:14.240 --> 25:18.240
sort of see what kind of steps you can take that will move you in that direction.

25:18.240 --> 25:23.200
That's pretty predictive model, but more so, what's prescriptive that is, you can say,

25:23.200 --> 25:25.520
what can I do to optimize the outcome?

25:25.520 --> 25:32.520
So just like a car driving, it says, if I go down this street, my app tells me less traffic,

25:32.520 --> 25:37.000
even though there's more stoplights, normally I wouldn't go that way because it's a longer

25:37.000 --> 25:41.800
drive and a normal low traffic day, but a high traffic day is going to be faster to go

25:41.800 --> 25:43.320
this other way.

25:43.320 --> 25:48.200
So in the same way with customer engagement, you might have a more optimal outcome.

25:48.200 --> 25:53.720
If you take a particular path or a particular road, so to speak, but it's, again, it's

25:53.720 --> 25:58.800
even more than that, and that's the cognitive analytics, the cognitive phase of driving,

25:58.800 --> 26:01.200
which is now you take in all contextual data.

26:01.200 --> 26:05.840
So using the car analogy again, you look at all the data, what's the weather condition?

26:05.840 --> 26:06.840
What's the road condition?

26:06.840 --> 26:09.480
Are there pedestrians on the road?

26:09.480 --> 26:11.480
Are there children playing down the street?

26:11.480 --> 26:13.280
You know, am I in a school zone?

26:13.280 --> 26:14.280
Is it whatever?

26:14.280 --> 26:19.720
And so all this contextual information now informs you how to take next best action.

26:19.720 --> 26:25.320
So cognitive analytics is about next best action, or I like to say next best question.

26:25.320 --> 26:28.200
What is the thing I should be asking of my data?

26:28.200 --> 26:32.360
What kind of things should I be informed about from my data?

26:32.360 --> 26:37.600
And so that's the cognitive phase of the self-driving enterprise is not just doing the

26:37.600 --> 26:43.400
thing that you always do, collecting data, selling products, serving customers.

26:43.400 --> 26:45.000
What is the thing you ought to be doing?

26:45.000 --> 26:49.920
What is the more contextually based thing you should be doing?

26:49.920 --> 26:52.320
That context could be time of day, right?

26:52.320 --> 26:57.160
So let's just say you're building something as simple as a recommender engine to a customer.

26:57.160 --> 27:01.520
What you recommend a particular customer is not always the same, even for the same customer

27:01.520 --> 27:06.080
could depend upon time of day, or day of week, for example, I might be looking at completely

27:06.080 --> 27:10.080
different products online if I'm at home on a weekend, then if I'm at work, or if I'm

27:10.080 --> 27:14.160
on vacation in Vegas, or if I'm at work.

27:14.160 --> 27:21.760
And so context, location, time, those kind of contextual data points sort of change your

27:21.760 --> 27:25.720
action even for the exact same customer.

27:25.720 --> 27:30.520
And so being able to bring in the contextual data makes you more cognitively aware and

27:30.520 --> 27:35.520
able to take those best, next best actions and ask the next best questions.

27:35.520 --> 27:40.280
And so that enables your enterprise, your business to be that self-driving enterprise in

27:40.280 --> 27:46.360
the sense you can start automating more of the processes, automating more of the activities,

27:46.360 --> 27:52.960
not taking human out of the loop, but augmenting the human with the right information and knowledge

27:52.960 --> 27:56.480
and inputs and insights that they need to do their work.

27:56.480 --> 27:59.840
So I like to say AI is no longer about artificial intelligence.

27:59.840 --> 28:03.120
In fact, there's nothing artificial at all about it in my mind.

28:03.120 --> 28:10.240
It's about other types of AI, amplified intelligence, assisted intelligence, accelerated, augmented,

28:10.240 --> 28:15.000
adaptable, and just go on and on and on.

28:15.000 --> 28:20.640
I mean, there's all these interesting ways of thinking about AI, besides artificial.

28:20.640 --> 28:25.880
And so your self-driving enterprise is that one that, in a sense, it's doing for your

28:25.880 --> 28:29.600
enterprise what an autonomous car is doing for a driver.

28:29.600 --> 28:34.520
You don't want to have the driver completely disengaged from the driving experience, I

28:34.520 --> 28:35.520
think.

28:35.520 --> 28:41.960
I think we've learned that from recent incidents that you still need to have a person there.

28:41.960 --> 28:46.640
And so you still need to have a person there in the enterprise, obviously.

28:46.640 --> 28:51.840
So future of work is another completely different dialogue we could be having in this area.

28:51.840 --> 28:54.920
But you did mention a phrase earlier called digital transformation, which is one of the

28:54.920 --> 29:00.040
big themes here at Peggo World this week, and digital transformation includes two words,

29:00.040 --> 29:05.640
right, digital, which means we're looking at digital information, digital data, digital

29:05.640 --> 29:12.200
signals to inform us and to do our self-driving car thing, self-driving enterprise thing.

29:12.200 --> 29:17.440
But there's the other word, their transformation, and transformation means change, and change

29:17.440 --> 29:18.440
means change.

29:18.440 --> 29:19.440
Love.

29:19.440 --> 29:24.840
So jobs will change, career paths will change, what people do will change in the same

29:24.840 --> 29:27.680
way with every industrial revolution.

29:27.680 --> 29:35.760
And so it is happening, and you can't stop it from happening, it would be ridiculous

29:35.760 --> 29:40.720
to stop it from happening, any more than stopping the invention of the printing press or something

29:40.720 --> 29:41.720
like that.

29:41.720 --> 29:42.720
Oh, my gosh.

29:42.720 --> 29:47.120
What are we going to do with all those monks and monasteries whose job it is to transcribe

29:47.120 --> 29:56.880
our copy to write copies, endless copies, beautifully artistic copies of the Bible or

29:56.880 --> 29:57.880
whatever.

29:57.880 --> 30:04.960
So they found some other things to do, obviously, so work changes, the tasks we do change.

30:04.960 --> 30:08.200
And it means transformation.

30:08.200 --> 30:13.960
So it's okay that we are going through a change now because change is good, and it's

30:13.960 --> 30:19.040
growing pains, you might say we're going through the adolescence phase of digital transformation,

30:19.040 --> 30:22.640
so there's a lot of growing pain right there.

30:22.640 --> 30:31.120
Earlier you mentioned you were discussing tactics versus strategy, tactical, the Hill versus

30:31.120 --> 30:33.600
the battle, the battle versus the war.

30:33.600 --> 30:43.480
A lot of what we talk about in kind of applied AI, automated decision making is very

30:43.480 --> 30:49.760
tactical decisions, what's the next offer, what's the next step.

30:49.760 --> 30:56.560
Are you seeing AI, machine learning applied to helping businesses get a handle on the

30:56.560 --> 30:59.320
strategy, the bigger picture?

30:59.320 --> 31:00.520
That's a good question.

31:00.520 --> 31:04.800
I'm not sure where we are in that state.

31:04.800 --> 31:10.480
I see a lot of a lot more discussion on topics that you might label data strategy or analytic

31:10.480 --> 31:11.480
strategy.

31:11.480 --> 31:16.400
And I imagine also AI strategy.

31:16.400 --> 31:24.080
So I think that the idea would be, let's stop and think what are our business goals,

31:24.080 --> 31:26.880
what are our outcomes we're trying to achieve.

31:26.880 --> 31:29.280
So be outcomes driven and not technology driven.

31:29.280 --> 31:34.520
I mean, I sort of cringe when I see people say that their business is data driven, and

31:34.520 --> 31:39.520
I've used that phrase myself, so I've all pointed myself to be guilty there.

31:39.520 --> 31:43.960
They say they're data driven or technology driven, and I'm starting to catch myself

31:43.960 --> 31:44.960
before I say that.

31:44.960 --> 31:50.840
We're data empowered, we're data fueled, data informed, and we're technology powered.

31:50.840 --> 31:52.640
I mean, that's a better way to say.

31:52.640 --> 31:58.120
We're data informed and technology powered, but we need to be product or outcome or

31:58.120 --> 32:00.440
customer driven, driven, driven.

32:00.440 --> 32:01.440
Right.

32:01.440 --> 32:06.720
Well, outcome driven, because a customer's success might be your desired outcome, customer,

32:06.720 --> 32:08.920
and you might say customer sales might be an outcome.

32:08.920 --> 32:11.560
I mean, that's a metric, right?

32:11.560 --> 32:15.160
So when you think about outcomes, you need to think about the metrics to measure whether

32:15.160 --> 32:16.640
you've achieved your outcomes, right?

32:16.640 --> 32:20.360
So that's traditional KPI, that's traditional six sigma, right?

32:20.360 --> 32:23.240
You say what you're going to do, you do it, and then you prove it, right?

32:23.240 --> 32:25.040
That's how I learned six sigma.

32:25.040 --> 32:26.040
And so how do you prove it?

32:26.040 --> 32:30.240
Well, you have some kind of measurement that you've agreed to, that this is the thing we're

32:30.240 --> 32:35.640
going to capture and measure to demonstrate whether or not we've achieved the outcome.

32:35.640 --> 32:43.320
And so the outcome is customer sale is a metric, I would say customer success, customer

32:43.320 --> 32:48.360
loyalty might be outcomes that you're, that's the big goal, right?

32:48.360 --> 32:50.920
And so you sell a product to a customer, that's a tactic, right?

32:50.920 --> 32:56.200
And so I make a recommendation, the person bought the product, yay, hooray for us, but

32:56.200 --> 32:57.200
is that a loyal customer?

32:57.200 --> 32:59.840
Are they going to come back and buy more from us?

32:59.840 --> 33:03.440
And so sometimes, like I said, you've got to sort of lose the battle to win the war.

33:03.440 --> 33:09.360
So this is why companies offer things like discount coupons and premium type products where

33:09.360 --> 33:10.960
they give you something for free.

33:10.960 --> 33:16.120
Hopefully later you'll buy the more premium plan when you have the sort of the low, the

33:16.120 --> 33:21.480
zero cost plan, which has a fewer bells and whistles, fewer services than the full premium

33:21.480 --> 33:22.800
plan.

33:22.800 --> 33:26.440
But if you really like what you get to see there, you're willing to pay more.

33:26.440 --> 33:31.880
And so the company is willing to take that loss in order to win the bigger, the bigger

33:31.880 --> 33:35.640
war that is going to gain a loyal customer in the end.

33:35.640 --> 33:39.840
So yes, so if you looked at the bottom line, you say, well, we lost money today because

33:39.840 --> 33:44.840
we gave away all this stuff at 20% off and, you know, we have a 15% margin in our company,

33:44.840 --> 33:46.600
so we're doing some money today.

33:46.600 --> 33:50.200
But in the long run, you've gained lifetime customers, loyal customers.

33:50.200 --> 33:54.680
And that, that's really the bigger picture, the bigger, the bigger goal.

33:54.680 --> 33:55.680
And so strategy intact.

33:55.680 --> 34:00.040
I mean, some people interchange those words and not here to argue semantics, but I'm really

34:00.040 --> 34:06.960
arguing about, think about sort of long-term goals versus short-term metrics and accomplishments.

34:06.960 --> 34:13.040
And sometimes that, the negative step backward leads to a bigger step forward later, and

34:13.040 --> 34:14.560
that's okay.

34:14.560 --> 34:20.200
That brings to mind for me, and in fact, this came up in an interview earlier today,

34:20.200 --> 34:26.840
the notion of architecting the, you know, our optimization functions, our reward functions

34:26.840 --> 34:33.320
to, as you put it here, better reflect the outcomes as opposed to the individual transactions.

34:33.320 --> 34:39.000
What kind of progress are you seeing towards data scientists being able to capture a more

34:39.000 --> 34:43.280
holistic view of the, the business strategy and the business outcome in the way that they

34:43.280 --> 34:48.200
optimize, in the way that they build, you know, machine learning algorithms and AI systems

34:48.200 --> 34:55.640
to optimize towards those, and where do you, what's your sense for where we are in the

34:55.640 --> 34:59.960
maturity curve and how we get to, you know, the next best thing?

34:59.960 --> 35:06.080
Well, I think we're in a much better place than just a few years ago, in what sense?

35:06.080 --> 35:14.440
And I mean that primarily in the access to more data, but there's really sort of three

35:14.440 --> 35:18.240
sort of, again, using the word confluence in a sentence, there's sort of three things

35:18.240 --> 35:24.560
that are, technologies that are merging, large amounts of data, that is, sensor technologies

35:24.560 --> 35:30.440
for collecting data on just about every process, person, thing, and enterprises and homes

35:30.440 --> 35:33.200
and cars in the universe.

35:33.200 --> 35:38.600
There's also faster, better, more powerful algorithms, so lots of development and algorithms

35:38.600 --> 35:42.080
for detecting patterns and trends and behaviors and data.

35:42.080 --> 35:46.040
And then the third one is access to high performance computing.

35:46.040 --> 35:50.160
So yes, we've had HPC high performance computing for many years, but you have to buy a super

35:50.160 --> 35:53.040
computer to have access to a super computer.

35:53.040 --> 35:55.400
Now you can rent one on the cloud, right?

35:55.400 --> 35:59.960
So you basically can rent as many CPUs as you need from a cloud service provider for

35:59.960 --> 36:02.880
the two minutes or five minutes that you need it and then give it back.

36:02.880 --> 36:05.800
So all you've paid for is those couple of minutes.

36:05.800 --> 36:10.600
And it's the cloud services provider's job to buy the hardware, to maintain the hardware,

36:10.600 --> 36:16.880
to upgrade the hardware and all these things, which most, and past days, I remember working

36:16.880 --> 36:20.760
in institutions where they didn't want to make that expense because they knew that within

36:20.760 --> 36:24.560
five years it would be obsolete and what a huge capital investment that would be and wouldn't

36:24.560 --> 36:29.480
it be better if we invested our of money and XYZ, other direction, like hiring more staff

36:29.480 --> 36:35.120
or more funding more students or doing whatever, yeah, we all agree, ultimately, no, don't

36:35.120 --> 36:40.680
go buy the big super computer because it'll be buddy down the drain five years or no.

36:40.680 --> 36:45.240
And so we got powerful computing, we got powerful algorithms, we got lots of data.

36:45.240 --> 36:46.240
So what does that buy us?

36:46.240 --> 36:51.560
It buys us insights and the ability to derive insights from data.

36:51.560 --> 36:55.520
And so when you're talking about optimizing a function, if a function is multi-dimensional,

36:55.520 --> 37:00.120
which in this case, I would say it is, there are many, many factors that determine the

37:00.120 --> 37:05.760
optimal, something, right, optimal customer experience, optimal sales, optimal, whatever.

37:05.760 --> 37:10.000
I mean, no matter what it is, optimal performance in a manufacturing plant, optimal supply chain,

37:10.000 --> 37:11.760
think about the traveling salesman problem.

37:11.760 --> 37:17.040
I mean, it's like in factorial, which is a very large number for a traveling salesman

37:17.040 --> 37:19.440
who's going to end different stops, right?

37:19.440 --> 37:24.880
And so this is a challenge problem that's quantum machine learning is addressing.

37:24.880 --> 37:31.120
How can you do a faster, much faster solution to the traveling salesman problem, which

37:31.120 --> 37:37.480
is basically optimal routing, whether it's for any kind of routing, whether it's shipping,

37:37.480 --> 37:44.040
industry, logistics in the military, or whatever, a network traffic, always looking for optimal

37:44.040 --> 37:45.040
routing.

37:45.040 --> 37:51.120
Okay, so how do you solve an in-factoid problem, one that basically is grow so fast, there's

37:51.120 --> 37:55.280
enough computing power on the planet and the universe to solve the problem.

37:55.280 --> 37:57.240
And that is you have more data.

37:57.240 --> 38:03.320
So the data from all these sensors gives us essentially a map of our, if you will, an

38:03.320 --> 38:10.400
indimensional map of the output variable, and then I'll just pick a number, let's just

38:10.400 --> 38:11.400
say revenue.

38:11.400 --> 38:15.840
Okay, let's just say that's our, if revenue is our thing we're trying to maximize, there's

38:15.840 --> 38:16.840
all kinds of factors.

38:16.840 --> 38:22.920
So we can look at all these different conditions and factors and activities and see which

38:22.920 --> 38:26.200
ones lead to the maximum of that function.

38:26.200 --> 38:33.040
And so you no longer have to do complex modeling per se, you can, but the data becomes

38:33.040 --> 38:36.320
the model, because you know, have enough data that the data now tells you, this is how

38:36.320 --> 38:41.760
the system responds, and in the case of say marketing campaigns, right, I heard a story

38:41.760 --> 38:46.880
once years ago that eBay had, they did, like, A.B. testing on their website, you know,

38:46.880 --> 38:50.320
like changing the fonts and changing the colors and changing the locations of things on

38:50.320 --> 38:56.560
the page, they did 10 million A.B. tests every single day.

38:56.560 --> 38:59.760
Every single day, they've moved things around and changed things, changed colors, changed

38:59.760 --> 39:02.960
fonts, changed locations, changed the sizes of the pictures, et cetera.

39:02.960 --> 39:08.400
And at that point, you don't need any kind of model of customer behavior.

39:08.400 --> 39:12.320
You just look at the data, say, this is what works, let's go.

39:12.320 --> 39:19.920
And so I think the, the ability to just use all the different data sources we now have,

39:19.920 --> 39:26.840
and again, if we're going back to the customer story, we got sales data, we got customer

39:26.840 --> 39:31.760
call center data from that customer, we have return data, you know, we have all kinds

39:31.760 --> 39:38.320
of data, customer care data, customer interest data, you know, product history purchased

39:38.320 --> 39:41.000
data, all kinds of information about that customer.

39:41.000 --> 39:45.760
So we can now figure out how to optimize our interaction with that customer based upon

39:45.760 --> 39:50.360
the data as opposed to what we would do in the past with some kind of modeling, right?

39:50.360 --> 39:54.120
So okay, I'm a white male or 50 and might live in a certain zip code, therefore every

39:54.120 --> 39:58.160
white male who lives in my zip code over the 50 must like the same things.

39:58.160 --> 40:01.240
And every morning I walk up my front door on my house, I know that isn't true because

40:01.240 --> 40:05.160
the guy next door to me has a Pittsburgh dealer banner hanging out front of his house,

40:05.160 --> 40:06.840
and I'm a Baltimore Raven thing.

40:06.840 --> 40:12.480
Okay, and so I mean, I'm immediately informed that no, it is not true that every single

40:12.480 --> 40:16.920
person in my demographic likes the same thing, I mean, and of course we know that now,

40:16.920 --> 40:17.920
right?

40:17.920 --> 40:21.480
So I say the big data era represents the end of demographics, we're no longer like

40:21.480 --> 40:29.440
using these limited biased variables to determine outcomes or marketing campaigns or offers

40:29.440 --> 40:35.120
or whatever, we just look at the data and say, what does this individual prefer like desire

40:35.120 --> 40:39.160
and serve them for who they are and what they like and desire?

40:39.160 --> 40:44.240
And again, that's our optimal optimization of customer experience, which leads to optimization

40:44.240 --> 40:48.040
hopefully a revenues is driven by data.

40:48.040 --> 40:53.480
And at the end of the day, that's what I say, it's all about data, therefore digital transformation

40:53.480 --> 40:54.480
is happening.

40:54.480 --> 40:59.720
It's not the gutted instinct anymore that drives your decisions as a business, it's the

40:59.720 --> 41:00.720
data.

41:00.720 --> 41:01.720
Awesome.

41:01.720 --> 41:07.240
Well, that sounds like a great note to close on any parting thoughts before we push the

41:07.240 --> 41:08.240
button.

41:08.240 --> 41:09.240
No, I think this is great.

41:09.240 --> 41:10.240
I thank you so much.

41:10.240 --> 41:11.760
Sam really enjoyed the conversation today.

41:11.760 --> 41:12.760
Same here.

41:12.760 --> 41:13.760
Thanks for.

41:13.760 --> 41:20.080
All right, everyone, that's our show for today.

41:20.080 --> 41:25.520
For more information on Kirk or any of the topics covered in this episode, head on over

41:25.520 --> 41:30.520
to twimmelai.com slash talk slash 151.

41:30.520 --> 41:38.520
To follow along with the Pegaworld series, visit twimmelai.com slash Pegaworld 2018.

41:38.520 --> 41:44.600
For more information on Pegasystems or their new Pegat Infinity offering, visit pegat.com

41:44.600 --> 41:46.840
slash infinity.

41:46.840 --> 41:50.360
As always, thanks so much for listening and catch you next time.

