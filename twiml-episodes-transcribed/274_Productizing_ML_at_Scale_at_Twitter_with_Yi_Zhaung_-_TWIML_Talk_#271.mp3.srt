1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:37,840
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.

5
00:00:37,840 --> 00:00:42,840
And last week we kicked off the second volume of our listener favorite AI platform series,

6
00:00:42,840 --> 00:00:47,200
sharing more stories of teams working to scale and industrialize data science and machine

7
00:00:47,200 --> 00:00:49,800
learning at their companies.

8
00:00:49,800 --> 00:00:54,160
We've been teasing that there's more to come and today I am super excited to announce

9
00:00:54,160 --> 00:00:59,360
the launch of our inaugural conference, Twimblecon AI platforms.

10
00:00:59,360 --> 00:01:04,600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

11
00:01:04,600 --> 00:01:09,400
necessary to scale the delivery of machine learning and AI in the enterprise.

12
00:01:09,400 --> 00:01:14,840
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

13
00:01:14,840 --> 00:01:18,640
we're creating our Twimblecon events to build on that tradition.

14
00:01:18,640 --> 00:01:24,240
The event will feature two full days of community oriented discussions, live podcast interviews

15
00:01:24,240 --> 00:01:30,440
and practical presentations by great presenters sharing concrete examples from their own experiences.

16
00:01:30,440 --> 00:01:34,640
By creating a space where data science, machine learning, platform engineering and ML ops

17
00:01:34,640 --> 00:01:39,640
practitioners and leaders can share, learn and connect, the event aspires to help see

18
00:01:39,640 --> 00:01:44,800
the development of an informed and sustainable community of technologists that is well equipped

19
00:01:44,800 --> 00:01:48,560
to meet the current and future needs of their organizations.

20
00:01:48,560 --> 00:01:52,880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

21
00:01:52,880 --> 00:01:58,120
learning and deep learning models into production, how to apply ML ops and DevOps to your machine

22
00:01:58,120 --> 00:02:03,040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

23
00:02:03,040 --> 00:02:07,520
support for data management, experiment management and model deployment.

24
00:02:07,520 --> 00:02:11,680
The latest approaches, platforms and tools for accelerating and scaling the delivery of

25
00:02:11,680 --> 00:02:17,280
ML and DL and the enterprise, platform deployment stories from leading companies like Google,

26
00:02:17,280 --> 00:02:23,480
Facebook, Airbnb, as well as traditional enterprises like Comcast and Shell and organizational

27
00:02:23,480 --> 00:02:27,000
and cultural best practices for success.

28
00:02:27,000 --> 00:02:31,600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

29
00:02:31,600 --> 00:02:33,600
love to meet you there.

30
00:02:33,600 --> 00:02:39,520
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first 10 listeners

31
00:02:39,520 --> 00:02:45,720
who register the amazing opportunity to get their ticket for 75% off using the discount

32
00:02:45,720 --> 00:02:48,160
code TwimbleFirst.

33
00:02:48,160 --> 00:02:54,920
Again, the conference site is Twimblecon.com and the code is TwimbleFirst.

34
00:02:54,920 --> 00:03:00,080
I am really grateful to our friends over at Sigopt who stepped up to support this project

35
00:03:00,080 --> 00:03:01,880
in a big way.

36
00:03:01,880 --> 00:03:07,000
In addition to supporting our AI Platforms podcast series and next ebook, they've made

37
00:03:07,000 --> 00:03:12,960
a huge commitment to this community by signing on as the first founding sponsor for the event.

38
00:03:12,960 --> 00:03:17,760
App Software is used by enterprise teams to standardize and scale machine learning experimentation

39
00:03:17,760 --> 00:03:23,200
and optimization across any combination of modeling frameworks, libraries, computing

40
00:03:23,200 --> 00:03:25,760
infrastructure and environment.

41
00:03:25,760 --> 00:03:31,480
Teams like Two Sigma, who will hear from later in this podcast series, rely on Sigopt Software

42
00:03:31,480 --> 00:03:36,520
to realize better modeling results much faster than previously possible.

43
00:03:36,520 --> 00:03:41,440
Of course, to fully grasp its potential, it's best to try it yourself and this is why

44
00:03:41,440 --> 00:03:46,760
Sigopt is offering you an exclusive opportunity to try their product on some of your toughest

45
00:03:46,760 --> 00:03:49,600
modeling problems for free.

46
00:03:49,600 --> 00:03:55,760
To learn about and take advantage of this offer, visit Twimbleai.com slash Sigopt.

47
00:03:55,760 --> 00:04:00,920
And now on to the show.

48
00:04:00,920 --> 00:04:03,600
All right, everyone.

49
00:04:03,600 --> 00:04:09,400
I am on the line with Yizhuang, E is a senior staff engineer at Twitter and tech lead

50
00:04:09,400 --> 00:04:13,320
for the machine learning core environment on the core techs team.

51
00:04:13,320 --> 00:04:16,520
E, welcome to this week a machine learning and AI.

52
00:04:16,520 --> 00:04:17,520
Thank you.

53
00:04:17,520 --> 00:04:19,320
My pleasure to be here.

54
00:04:19,320 --> 00:04:20,320
It's great to chat with you.

55
00:04:20,320 --> 00:04:26,640
I'm really looking forward to digging into what you are working on on the platform side

56
00:04:26,640 --> 00:04:28,720
of things.

57
00:04:28,720 --> 00:04:32,600
Before we do that, I'd love to start out with a little bit of your background and how

58
00:04:32,600 --> 00:04:36,720
you started working in machine learning platforms and infrastructure.

59
00:04:36,720 --> 00:04:41,440
Sure, actually, I would say there are two parts to this question.

60
00:04:41,440 --> 00:04:46,600
There's how I, I guess, how I started working on machine learning and also how I started

61
00:04:46,600 --> 00:04:48,960
working on platform and infra.

62
00:04:48,960 --> 00:04:50,960
So I can dive into both.

63
00:04:50,960 --> 00:04:51,960
Does that sound good?

64
00:04:51,960 --> 00:04:52,960
Absolutely.

65
00:04:52,960 --> 00:04:53,960
Yeah, let's do it.

66
00:04:53,960 --> 00:04:54,960
Okay.

67
00:04:54,960 --> 00:05:02,280
So I think ever since I was a kid, I always had an affinity to both math and computer

68
00:05:02,280 --> 00:05:08,880
science, so that's why I got attracted to discipline like machine learning where I get

69
00:05:08,880 --> 00:05:11,600
to practice both.

70
00:05:11,600 --> 00:05:16,600
So I would say the first time I touched machine learning was actually in college.

71
00:05:16,600 --> 00:05:23,680
I was on my college's robotics team and we programmed these robot dogs to play soccer.

72
00:05:23,680 --> 00:05:29,400
Those are Sony eyeballs and the competition was called Robocop and we participate in

73
00:05:29,400 --> 00:05:35,680
this competition and program robot dogs to play soccer against each other.

74
00:05:35,680 --> 00:05:41,080
Unlike what you would have guessed today, machine learning actually wasn't used in, wasn't

75
00:05:41,080 --> 00:05:43,800
used very much in our vision system.

76
00:05:43,800 --> 00:05:46,520
It was actually used for a different case.

77
00:05:46,520 --> 00:05:51,360
We used machine learning to tune the gates of the robots by gate.

78
00:05:51,360 --> 00:05:56,200
I mean, the walking posture, there is a set of parameters that needs to be optimized

79
00:05:56,200 --> 00:05:58,880
and the search space is pretty large.

80
00:05:58,880 --> 00:06:05,480
So what we did was we essentially made the robot dogs walk back and forth on the playing

81
00:06:05,480 --> 00:06:13,200
field and recorded the running speed and then we tune each parameter by a little bit and

82
00:06:13,200 --> 00:06:15,800
then make them walk the playing field again.

83
00:06:15,800 --> 00:06:22,240
Then we can compute the gradient of the walking speed with respect to that parameter we

84
00:06:22,240 --> 00:06:28,000
nudged by a little bit and we do that to all the parameters and then we perform one round

85
00:06:28,000 --> 00:06:35,640
of gradient descent that allowed us to optimize the running speed of our robot dogs.

86
00:06:35,640 --> 00:06:37,720
It's a very tedious process actually.

87
00:06:37,720 --> 00:06:39,320
That sounds super tedious.

88
00:06:39,320 --> 00:06:41,160
Yeah, exactly.

89
00:06:41,160 --> 00:06:48,400
I remember these Sony Ibo dogs from many years ago, I didn't realize they were quite

90
00:06:48,400 --> 00:06:52,160
that programmable.

91
00:06:52,160 --> 00:07:01,080
They were programmable, but yeah, the experience was pretty, it was not as good as today.

92
00:07:01,080 --> 00:07:06,720
Essentially, for example, if we get a segmentation fault, the whole robot turns off, basically

93
00:07:06,720 --> 00:07:08,840
the operating system shuts down.

94
00:07:08,840 --> 00:07:14,200
But one thing that was pretty amazing was our college's robotics team.

95
00:07:14,200 --> 00:07:22,840
We started as the last place in the U.S. open competition in around 2015.

96
00:07:22,840 --> 00:07:28,880
The same team actually won World Championship in 2007 and 2007.

97
00:07:28,880 --> 00:07:29,880
Wow.

98
00:07:29,880 --> 00:07:36,360
We actually beat very reputable teams like Carnegie Mellon 8-1 in terms of score.

99
00:07:36,360 --> 00:07:42,760
I would say it was the definitely hard working team and the techniques we were using, using

100
00:07:42,760 --> 00:07:48,880
machine learning to tune the gate of the robots, they worked out well.

101
00:07:48,880 --> 00:07:49,880
Wow.

102
00:07:49,880 --> 00:07:51,400
And where did you go to school?

103
00:07:51,400 --> 00:07:53,760
I actually went to Carnegie Mellon right after.

104
00:07:53,760 --> 00:07:59,160
I think for basically beating their robotics team, definitely helped me get that offer

105
00:07:59,160 --> 00:08:05,240
from both Carnegie Mellon's robotics institute and computer science department.

106
00:08:05,240 --> 00:08:11,480
What I ended up choosing, though, was I was both a math major and a CS major in college.

107
00:08:11,480 --> 00:08:17,320
I ended up choosing a very mathematical field of computer science at Carnegie Mellon where

108
00:08:17,320 --> 00:08:26,480
I model system performance by building mathematical models, essentially building a mark of models

109
00:08:26,480 --> 00:08:33,000
and analyze queuing, et cetera, to basically predict the performance of computer systems.

110
00:08:33,000 --> 00:08:35,160
That's what I do in grad school.

111
00:08:35,160 --> 00:08:42,080
And I got into platform around, like I would say, system engineering around roughly two

112
00:08:42,080 --> 00:08:47,920
years into my PhD program at Carnegie Mellon, I interned at Google.

113
00:08:47,920 --> 00:08:54,600
So I worked on the basically performance modeling for the next generation storage system

114
00:08:54,600 --> 00:08:55,800
at Google.

115
00:08:55,800 --> 00:09:00,480
And I offered to, for example, build math models predict performance there.

116
00:09:00,480 --> 00:09:02,320
And they were less excited.

117
00:09:02,320 --> 00:09:08,040
They were more, yeah, they were like, we don't want you to build math models.

118
00:09:08,040 --> 00:09:12,160
We want you to actually measure and tune the parameters.

119
00:09:12,160 --> 00:09:15,880
So this is, again, very similar to the robotics case, right?

120
00:09:15,880 --> 00:09:21,320
It's, essentially, we have like a black box storage system, but we have many knobs we can

121
00:09:21,320 --> 00:09:22,320
tune.

122
00:09:22,320 --> 00:09:29,720
Essentially, we're looking for the best set of parameters that can actually perform best.

123
00:09:29,720 --> 00:09:33,520
So that was my Google internship and afterwards.

124
00:09:33,520 --> 00:09:39,120
I started realizing that my PhD program, I do a lot of math, but I don't do a lot of

125
00:09:39,120 --> 00:09:41,120
hands-on engineering.

126
00:09:41,120 --> 00:09:45,840
And I actually found myself to like building stuff.

127
00:09:45,840 --> 00:09:51,280
And that's when I dropped out of my PhD program and came to Twitter.

128
00:09:51,280 --> 00:09:56,280
My first projects were actually on Twitter search.

129
00:09:56,280 --> 00:10:03,720
And this is where I learned to be an actual engineer and learned about system and platform

130
00:10:03,720 --> 00:10:05,200
engineering.

131
00:10:05,200 --> 00:10:09,760
And I took like around 2014 or so.

132
00:10:09,760 --> 00:10:16,880
I led a group of people and built this trillion documents scale search engine at Twitter.

133
00:10:16,880 --> 00:10:21,200
This allowed Twitter to index every single tweet ever published.

134
00:10:21,200 --> 00:10:27,640
And now you can search for Jack's first tweet setting up my Twitter and that tweet would

135
00:10:27,640 --> 00:10:28,640
show up.

136
00:10:28,640 --> 00:10:29,640
Oh, wow.

137
00:10:29,640 --> 00:10:33,080
That's briefly how I got into platform engineering.

138
00:10:33,080 --> 00:10:38,440
And afterwards, I came to cortex building machine learning platform for Twitter.

139
00:10:38,440 --> 00:10:44,640
And so cortex, it was, is cortex a team that grew organically in Twitter or there was an

140
00:10:44,640 --> 00:10:49,360
acquisition that was part of that as well, right?

141
00:10:49,360 --> 00:10:54,000
Yes, actually, there were multiple acquisitions.

142
00:10:54,000 --> 00:10:59,200
There was a tweet from Jack around Jack was our CEO.

143
00:10:59,200 --> 00:11:07,000
He tweeted about this around 2016 and Twitter acquired three companies.

144
00:11:07,000 --> 00:11:09,040
One of them is called Mad Bits.

145
00:11:09,040 --> 00:11:13,080
One of them is called Magic Pony and one of them is called WetLab.

146
00:11:13,080 --> 00:11:19,720
These three companies, the acquisition formed the original cortex org.

147
00:11:19,720 --> 00:11:28,080
And the original focus was deep learning research, essentially, we inherited the DNA from these

148
00:11:28,080 --> 00:11:30,080
three acquisitions.

149
00:11:30,080 --> 00:11:31,080
Got it.

150
00:11:31,080 --> 00:11:32,080
Got it.

151
00:11:32,080 --> 00:11:40,160
And so to what degree was machine learning really heavily used at Twitter or maybe the

152
00:11:40,160 --> 00:11:47,040
right way to ask this is broadly used, it sounds like you, there were some activity around

153
00:11:47,040 --> 00:11:51,080
search and I imagine there were some other kind of point use cases, but was it in broad

154
00:11:51,080 --> 00:11:54,040
use at Twitter before cortex?

155
00:11:54,040 --> 00:12:01,640
So machine learning before cortex was used, but it wasn't practiced in a consistent way.

156
00:12:01,640 --> 00:12:07,080
It was definitely used for us to do advertisements.

157
00:12:07,080 --> 00:12:16,720
For example, CTR prediction to fight spam and adversarial users, bad accounts on our platform

158
00:12:16,720 --> 00:12:20,120
and also used to rank search.

159
00:12:20,120 --> 00:12:27,480
Cortex, essentially, in the past two years or so, transitioned from a deep learning research

160
00:12:27,480 --> 00:12:30,960
org to a machine learning platform org.

161
00:12:30,960 --> 00:12:37,880
And the cortex is basically bringing consistency to how machine learning is used across all

162
00:12:37,880 --> 00:12:41,400
sorts of product teams at Twitter.

163
00:12:41,400 --> 00:12:46,280
So I wouldn't say machine learning wasn't used at Twitter widely before cortex, machine

164
00:12:46,280 --> 00:12:53,320
learning was widely used, but exactly because it was widely used and it was practice, like

165
00:12:53,320 --> 00:12:56,760
there are practitioners across many different teams.

166
00:12:56,760 --> 00:13:02,480
It was a very fragmented landscape and different teams did things differently.

167
00:13:02,480 --> 00:13:10,240
So cortex around 2017, our CTO product took over the org and started focusing the org

168
00:13:10,240 --> 00:13:14,360
around serving customers and being a platform team.

169
00:13:14,360 --> 00:13:20,760
And later our current director sent deep to cover and continued to sharpen our customer

170
00:13:20,760 --> 00:13:27,720
focus, now we are a platform team building machine learning platforms to serve various

171
00:13:27,720 --> 00:13:29,320
product teams.

172
00:13:29,320 --> 00:13:37,000
And as we currently stand, most Twitter product teams are using cortex machine learning platforms

173
00:13:37,000 --> 00:13:40,080
to practice and know at Twitter now.

174
00:13:40,080 --> 00:13:48,040
Is the goal of cortex and building platforms, would you say, it's to drive more consistency

175
00:13:48,040 --> 00:13:56,800
and efficiencies in the way folks use machine learning or is it to broaden the kind of the

176
00:13:56,800 --> 00:14:01,480
base of people that can use machine learning, not that those are strictly a dichotomy,

177
00:14:01,480 --> 00:14:08,080
but I'm wondering if one or the other really drove the establishment of the organization.

178
00:14:08,080 --> 00:14:15,600
I think it's both actually ultimately our goal is to empower practitioners at Twitter

179
00:14:15,600 --> 00:14:22,160
to use and know both more efficiently and empower more people to be able to leverage machine

180
00:14:22,160 --> 00:14:23,160
learning.

181
00:14:23,160 --> 00:14:28,960
So it's, I would say the emphasis is more on the latter.

182
00:14:28,960 --> 00:14:33,840
Bringing consistency itself is a intermediate goal in my mind.

183
00:14:33,840 --> 00:14:40,040
We're hoping to bring efficiency to the company by bringing consistency.

184
00:14:40,040 --> 00:14:47,480
And once everybody is practicing ML in a consistent way using our offerings, it makes our job

185
00:14:47,480 --> 00:14:50,560
easier to bring productivity to them.

186
00:14:50,560 --> 00:14:52,760
I don't know if this answer makes sense.

187
00:14:52,760 --> 00:14:54,760
I can further elaborate.

188
00:14:54,760 --> 00:14:56,480
Yeah, it absolutely does.

189
00:14:56,480 --> 00:15:01,520
And I've got one more question on kind of these meta organizational questions before

190
00:15:01,520 --> 00:15:07,000
we dig into some more detail about the platform there and it, and that is around this transition

191
00:15:07,000 --> 00:15:13,200
from deep learning research focused organization to a platform organization.

192
00:15:13,200 --> 00:15:18,280
It strikes me that those are very different missions, perhaps calling for very different

193
00:15:18,280 --> 00:15:19,960
skill sets.

194
00:15:19,960 --> 00:15:24,040
What was the experience of going through that transition like?

195
00:15:24,040 --> 00:15:34,160
Yeah, it was definitely a difficult transition, I would say, and the most, the biggest

196
00:15:34,160 --> 00:15:41,880
shift is the mindset from doing cutting edge research to serving internal customers.

197
00:15:41,880 --> 00:15:49,840
And the customer focus mindset is the biggest change in the org and it took us, many took

198
00:15:49,840 --> 00:15:57,040
us a lot of effort to get our, for example, engineers and researchers to be aligned on

199
00:15:57,040 --> 00:15:58,560
that.

200
00:15:58,560 --> 00:16:03,760
Part of it comes from, did most people buy into the idea of doing that shift or did the

201
00:16:03,760 --> 00:16:07,320
org turn over quite a bit in order to get there?

202
00:16:07,320 --> 00:16:09,120
Both, both.

203
00:16:09,120 --> 00:16:14,960
We had some turnovers as the transition, but most people actually stayed and bought into

204
00:16:14,960 --> 00:16:19,640
this new vision or focus.

205
00:16:19,640 --> 00:16:25,280
And we are now very customer oriented and we do what our customer asked for.

206
00:16:25,280 --> 00:16:32,120
And we still have a research org, but even that research org is focused on, for example,

207
00:16:32,120 --> 00:16:36,320
improving the production model performance of our customer teams.

208
00:16:36,320 --> 00:16:40,640
So for people who wants to do like machine learning, deep learning research, we still

209
00:16:40,640 --> 00:16:46,600
have a place, but we definitely repurposed the goal of the research.

210
00:16:46,600 --> 00:16:57,400
Yeah, the idea of kind of a customer, a customer centric view in providing platforms, I think

211
00:16:57,400 --> 00:17:05,960
is one that makes a lot of sense is kind of, it's a very kind of straightforward approach.

212
00:17:05,960 --> 00:17:10,760
And I'm thinking specifically about this conversation that I recently had and was published

213
00:17:10,760 --> 00:17:16,480
on the podcast just a couple of shows ago with Eric Colson at StitchFix.

214
00:17:16,480 --> 00:17:25,080
And when he talked about the role of the platform's team in his group, it was the way that

215
00:17:25,080 --> 00:17:31,880
they figured out the features that the platform needed, it was, it was really focused on things

216
00:17:31,880 --> 00:17:33,840
that their customers weren't asking for.

217
00:17:33,840 --> 00:17:40,720
It's like how could they add value that the user doesn't even know about versus the way

218
00:17:40,720 --> 00:17:45,520
I think of traditional kind of product management, your understanding requirements and kind

219
00:17:45,520 --> 00:17:51,760
of organizing those requirements and figuring out how to get there with a platform is either

220
00:17:51,760 --> 00:17:59,120
one of those approaches resonate with the way things tend to happen at Cortex?

221
00:17:59,120 --> 00:18:01,320
I think actually both.

222
00:18:01,320 --> 00:18:06,440
So we can talk about our strategy for last year and this year.

223
00:18:06,440 --> 00:18:12,480
Last year is about adoption and consistency, we need to get our users to use our platform.

224
00:18:12,480 --> 00:18:18,680
And this year we started to look into, for example, ease of use and iteration speed.

225
00:18:18,680 --> 00:18:24,320
This is when we think about what kind of features make our users' lives easier.

226
00:18:24,320 --> 00:18:31,520
So I would say for 2018, getting customers to adopt our product and essentially switching

227
00:18:31,520 --> 00:18:39,720
from their current machine learning tool kits to our platform, a customer-focused mindset,

228
00:18:39,720 --> 00:18:43,400
customer-driven feature development is more valuable.

229
00:18:43,400 --> 00:18:48,960
We really need to listen to our customers to understand what makes them to change from

230
00:18:48,960 --> 00:18:58,760
their current ML tool kits and ML, for example, frameworks that switch to our offering.

231
00:18:58,760 --> 00:19:04,720
And afterwards, when we think about what features makes their life easier, we need to adopt

232
00:19:04,720 --> 00:19:11,480
a strategic thinking and think about what our customers are not asking for.

233
00:19:11,480 --> 00:19:13,040
Does this answer make sense?

234
00:19:13,040 --> 00:19:18,000
So I think it's a gradual shift in the beginning of creating this platform.

235
00:19:18,000 --> 00:19:26,080
We definitely need to put more focus on serving customers and catering to their asks.

236
00:19:26,080 --> 00:19:32,960
And as the platform matures, we gradually increase the strategic bets in our portfolio

237
00:19:32,960 --> 00:19:40,320
and do more work that's not necessarily being asked by customers, but we're anticipating.

238
00:19:40,320 --> 00:19:44,200
So we've talked very abstractly about the platform.

239
00:19:44,200 --> 00:19:47,040
Maybe walk us through the platform now.

240
00:19:47,040 --> 00:19:48,840
What are the major components?

241
00:19:48,840 --> 00:19:54,440
How do you think about it architecturally or how would you describe it to someone?

242
00:19:54,440 --> 00:19:55,440
Sure.

243
00:19:55,440 --> 00:20:03,520
So Quartex right now, we offer multiple solutions in our ML platform.

244
00:20:03,520 --> 00:20:12,520
We offer the core model training and evaluation part, which is based on TensorFlow.

245
00:20:12,520 --> 00:20:20,200
And we offer data preprocessing and featureization, something we call feature store.

246
00:20:20,200 --> 00:20:27,640
It allows users to consistently prepare features for machine learning models, both at training

247
00:20:27,640 --> 00:20:34,760
time and prediction time, which is offline in offline and online context.

248
00:20:34,760 --> 00:20:40,760
And we offer production model serving based on JVMs.

249
00:20:40,760 --> 00:20:46,760
This is like a TensorFlow serving equivalent, except it's specialized and custom-bued for

250
00:20:46,760 --> 00:20:47,920
Twitter.

251
00:20:47,920 --> 00:20:56,920
And we have pipeline orchestration, which is a automation solution that allows people to

252
00:20:56,920 --> 00:21:04,760
run dependency graphs of machine learning workloads and basically chain a dependency

253
00:21:04,760 --> 00:21:09,960
of tasks into a single graph and run them in an automated manner.

254
00:21:09,960 --> 00:21:17,360
And we have also added more efforts in our platform, including embeddings, nearest neighbor

255
00:21:17,360 --> 00:21:20,160
search for candidate generation.

256
00:21:20,160 --> 00:21:26,760
We have added machine learning observability, which allows us to observe feature distributions

257
00:21:26,760 --> 00:21:29,480
and also analyze models.

258
00:21:29,480 --> 00:21:35,400
And in the end, we also started a new team inside Quartex, which is called meta, which

259
00:21:35,400 --> 00:21:47,280
is studying the bias and accountability, fairness, those concerns inside algorithmic decisions.

260
00:21:47,280 --> 00:21:51,560
That's a high level overview of what Quartex comprises of today.

261
00:21:51,560 --> 00:21:53,920
Lots of interesting stuff to dive into.

262
00:21:53,920 --> 00:21:58,240
The first of the things that you mentioned was at the framework level?

263
00:21:58,240 --> 00:22:02,520
Yes, the first one is what I mentioned is called the DeepBird.

264
00:22:02,520 --> 00:22:06,400
It's our model training and evaluation solution.

265
00:22:06,400 --> 00:22:08,280
So maybe let's start there.

266
00:22:08,280 --> 00:22:13,880
What is the goal of DeepBird and how do users use it?

267
00:22:13,880 --> 00:22:14,880
Yes.

268
00:22:14,880 --> 00:22:23,400
So DeepBird started as, so historically, Twitter Quartex uses Torch, which is Lua Torch,

269
00:22:23,400 --> 00:22:24,760
not PyTorch.

270
00:22:24,760 --> 00:22:32,720
It's the initial, I would say it's one of the older established deep learning frameworks.

271
00:22:32,720 --> 00:22:39,600
What we noticed around 2017 was that the Torch community started to lose steam and the

272
00:22:39,600 --> 00:22:45,880
tensor flow and PyTorch started gaining popularity in the community.

273
00:22:45,880 --> 00:22:55,040
And DeepBird and actually DeepBird version 2 is our effort in partnership with Google

274
00:22:55,040 --> 00:22:58,440
to bring tensor flow to Twitter.

275
00:22:58,440 --> 00:23:09,000
So the goal of this component DeepBird V2 is to unlock latest technology backed by Google

276
00:23:09,000 --> 00:23:16,560
mostly tensor flow and its ecosystem and tensor flow extended for use at Twitter.

277
00:23:16,560 --> 00:23:26,600
And specifically to be more specific, Twitter is more of a Java shop like most of our code

278
00:23:26,600 --> 00:23:33,480
are either in Java or Scala, basically JVM languages, whereas tensor flow is mostly Python

279
00:23:33,480 --> 00:23:35,200
and C++.

280
00:23:35,200 --> 00:23:43,320
So we had to build quite a lot of production, gluing logic to actually make tensor flow

281
00:23:43,320 --> 00:23:45,160
work at Twitter.

282
00:23:45,160 --> 00:23:52,840
And also DeepBird provides an additional abstraction layer between tensor flow and our machine learning

283
00:23:52,840 --> 00:23:55,680
practitioners at Twitter.

284
00:23:55,680 --> 00:23:58,640
We do this for multiple goals.

285
00:23:58,640 --> 00:24:02,240
One is to reduce the complexity of using tensor flow.

286
00:24:02,240 --> 00:24:08,840
This tensor flow was actually released, production released was in February 2018.

287
00:24:08,840 --> 00:24:14,560
It's a relatively new thing and we would like to hide complexity whenever possible.

288
00:24:14,560 --> 00:24:20,440
And inside the abstraction layer, we prescribe default values for different knobs and we

289
00:24:20,440 --> 00:24:28,880
also include optimizations that are specialized and customized for the Twitter data centers

290
00:24:28,880 --> 00:24:30,480
and Twitter workload.

291
00:24:30,480 --> 00:24:38,000
A ton and there to dig into, you mentioned needing to build a lot of glue code to bring

292
00:24:38,000 --> 00:24:44,360
this Python oriented system into your primarily JVM based environment.

293
00:24:44,360 --> 00:24:48,520
Can you give some examples of specific things that you had to do?

294
00:24:48,520 --> 00:24:54,440
Yeah, the main example I can give is that tensor flow is serving, right?

295
00:24:54,440 --> 00:25:03,920
So tensor flow serving is actually a C++ app speaking GRPC at Twitter, with GRPC is not

296
00:25:03,920 --> 00:25:10,360
our standard RPC protocol and also C++ is not the main language.

297
00:25:10,360 --> 00:25:14,240
Our engineers don't know how to maintain C++ apps.

298
00:25:14,240 --> 00:25:21,080
So we essentially built an equivalent of tensor flow serving, but inside using Java

299
00:25:21,080 --> 00:25:25,400
and Scala and we ship that app to our customers.

300
00:25:25,400 --> 00:25:32,320
This is mainly for internal maintenance so that our teams knows how to be on call and

301
00:25:32,320 --> 00:25:36,120
fix issues for the serving solution.

302
00:25:36,120 --> 00:25:41,120
And also it's for tighter integration with our internal observability stack.

303
00:25:41,120 --> 00:25:49,360
This allows us to integrate with our monitoring and alerting solution seamlessly because tensor

304
00:25:49,360 --> 00:25:53,080
flow serving doesn't have that integration.

305
00:25:53,080 --> 00:25:54,440
Does that make sense?

306
00:25:54,440 --> 00:25:55,440
Yes.

307
00:25:55,440 --> 00:26:00,320
So deeper V2 is very focused on bringing tensor flow to Twitter.

308
00:26:00,320 --> 00:26:08,920
Does that mean it's a highly opinionated system and user that is interested in using PyTorch

309
00:26:08,920 --> 00:26:13,920
for example, isn't supported with deep bird?

310
00:26:13,920 --> 00:26:15,840
That's currently the case.

311
00:26:15,840 --> 00:26:22,680
So remember what we started with, the goal was we were trying to defragment the machine

312
00:26:22,680 --> 00:26:24,920
learning practices at Twitter.

313
00:26:24,920 --> 00:26:32,720
What we noticed was in 2017, we had users of LuaTorch and we had tensor flow users,

314
00:26:32,720 --> 00:26:37,840
we had scikit learn users, we had XG boost users.

315
00:26:37,840 --> 00:26:43,160
So this fragmentation caused several issues.

316
00:26:43,160 --> 00:26:47,160
First of all, it causes difficulty sharing.

317
00:26:47,160 --> 00:26:53,200
Different teams can't share their machine learning models, they're tooling and resources.

318
00:26:53,200 --> 00:26:58,600
And sometimes it prevents expertise sharing as well, like if an engineer wants to move

319
00:26:58,600 --> 00:27:05,560
from one team to another, he has to learn a new set of expertise in order to be effective.

320
00:27:05,560 --> 00:27:10,400
So that's when we noticed the fragmentation is a problem and then we also noticed work

321
00:27:10,400 --> 00:27:12,000
duplication, right?

322
00:27:12,000 --> 00:27:21,320
So Twitter is a very large scale company and I can introduce our scale maybe separately,

323
00:27:21,320 --> 00:27:26,800
but essentially we have to invest in a lot of resource to duplicate serving solutions.

324
00:27:26,800 --> 00:27:33,000
For example, let's say to serve a scikit learn model versus serving a PyTorch model versus

325
00:27:33,000 --> 00:27:40,120
serving a tensor flow model, if we end up building like three different set of serving solutions,

326
00:27:40,120 --> 00:27:44,920
which is what happened before, it wastes a lot of engineering resource.

327
00:27:44,920 --> 00:27:52,280
So that's why deep bird is an opinionated, like you said, prescribed opinionated way of

328
00:27:52,280 --> 00:27:59,400
doing machine learning based on top of tensor flow that tries to get our machine learning

329
00:27:59,400 --> 00:28:05,240
practitioners to do things in a consistent way that allows different teams to share machine

330
00:28:05,240 --> 00:28:12,000
learning models, share their tools and resources and even knowledge across teams.

331
00:28:12,000 --> 00:28:18,960
You mentioned scikit learn is deep bird also an abstraction for traditional machine learning

332
00:28:18,960 --> 00:28:23,640
workloads and beyond the deep learning workloads.

333
00:28:23,640 --> 00:28:29,760
So there's really not a clear line between deep learning and traditional machine learning.

334
00:28:29,760 --> 00:28:36,360
So yes, deep bird can support traditional machine learning, tensor flow supports, for

335
00:28:36,360 --> 00:28:41,680
example, traditional machine learning methods as well.

336
00:28:41,680 --> 00:28:46,040
One of the why it is to use the traditional machine learning method is actually logistic

337
00:28:46,040 --> 00:28:47,440
regression.

338
00:28:47,440 --> 00:28:55,120
It's very widely used inside Twitter and in fact, there's really not a clear line between

339
00:28:55,120 --> 00:29:00,560
logistic regression and deep learning because we can actually think of logistic regression

340
00:29:00,560 --> 00:29:04,560
as a one layer neural network with a single output.

341
00:29:04,560 --> 00:29:12,640
So sure, but often folks find the overhead of deep learning frameworks relative to the

342
00:29:12,640 --> 00:29:18,800
tools that they might use for traditional ML to be a pretty heavy weight.

343
00:29:18,800 --> 00:29:19,800
Yes.

344
00:29:19,800 --> 00:29:27,120
So for example, tensor flow itself, the estimator API is the main API tensor flow recommends

345
00:29:27,120 --> 00:29:30,480
for productionization in tensor flow one.

346
00:29:30,480 --> 00:29:35,720
And that API we acknowledge that it's very clunky and many of our users don't really like

347
00:29:35,720 --> 00:29:36,720
it.

348
00:29:36,720 --> 00:29:38,080
They think it's too heavy weight.

349
00:29:38,080 --> 00:29:43,000
That's exactly what we provide in our deep bird abstraction.

350
00:29:43,000 --> 00:29:49,640
We're trying to hide the complexity whenever possible and prescribe good defaults.

351
00:29:49,640 --> 00:29:54,080
So I usually use a camera analogy.

352
00:29:54,080 --> 00:29:59,760
Think of tensor flow as a very powerful DSLR camera.

353
00:29:59,760 --> 00:30:04,960
Many users actually prefer like a mobile phone, one button point and shoot camera.

354
00:30:04,960 --> 00:30:12,000
So what we're trying to do is we're trying to wrap tensor flow inside our layer of abstraction.

355
00:30:12,000 --> 00:30:17,680
We try to encapsulate all the knobs and buttons on the digital SR camera.

356
00:30:17,680 --> 00:30:21,680
And we expose a single button point and shoot solution.

357
00:30:21,680 --> 00:30:24,160
So this is a double edge sword, right?

358
00:30:24,160 --> 00:30:28,920
It's most users like it, especially the production ML engineers.

359
00:30:28,920 --> 00:30:31,840
They really like solutions where they type a single command.

360
00:30:31,840 --> 00:30:34,200
It trains a machine learning model for them.

361
00:30:34,200 --> 00:30:40,040
But some of the more advanced users and deep learning researchers actually don't necessarily

362
00:30:40,040 --> 00:30:42,480
like the point and shoot solution.

363
00:30:42,480 --> 00:30:46,800
It's just like how a professional photographer might feel like it's an insult.

364
00:30:46,800 --> 00:30:49,160
If you're giving a point shoot camera, right?

365
00:30:49,160 --> 00:30:56,880
So I would say to answer your question, we added this abstraction layer to hide complexity

366
00:30:56,880 --> 00:31:01,840
of deep learning frameworks such that if you want to do logistic regression using our

367
00:31:01,840 --> 00:31:04,680
ML platform, it's very simple.

368
00:31:04,680 --> 00:31:08,920
And we aim for like a simple solutions where you type a command.

369
00:31:08,920 --> 00:31:13,200
We can launch training and save models on HDFS and you type another command.

370
00:31:13,200 --> 00:31:19,560
We launch hundreds of prediction servers serving the model saved on HDFS.

371
00:31:19,560 --> 00:31:23,920
But we do need to think about how to cater to the more powerful users as well.

372
00:31:23,920 --> 00:31:30,000
To what degree does DeepBird replicate a lot of the work that has been done with Keras

373
00:31:30,000 --> 00:31:37,200
as a front end to TensorFlow, it seems like there are similar goals in terms of increasing

374
00:31:37,200 --> 00:31:42,480
usability, although Keras is certainly not a one button type of solution.

375
00:31:42,480 --> 00:31:43,720
Correct.

376
00:31:43,720 --> 00:31:49,000
So I would say there's definitely some overlap.

377
00:31:49,000 --> 00:31:56,800
Essentially TensorFlow 1.0 made this, basically, in TensorFlow 1.0, there are these two ways of

378
00:31:56,800 --> 00:32:03,280
practicing ML estimators and Keras estimator was more targeting production use, scaling

379
00:32:03,280 --> 00:32:07,600
to large data set, while Keras was targeting ease of use.

380
00:32:07,600 --> 00:32:14,160
So DeepBird is basically building on top of estimator and hoping to improve ease of

381
00:32:14,160 --> 00:32:15,160
use.

382
00:32:15,160 --> 00:32:23,120
Upcoming in TensorFlow 2.0, TensorFlow is consolidating estimator and Keras into a single Keras API.

383
00:32:23,120 --> 00:32:30,320
So users no longer need to choose between scalability and usability.

384
00:32:30,320 --> 00:32:37,200
We also envision that going forward once TensorFlow 2.0 is released, DeepBird will also

385
00:32:37,200 --> 00:32:41,520
most likely adopt the Keras based API.

386
00:32:41,520 --> 00:32:45,520
So that is the training elements.

387
00:32:45,520 --> 00:32:52,520
Do you also, does DeepBird also offer features focused on experiment management, tracking

388
00:32:52,520 --> 00:32:57,160
model parameters, hyperparameter tuning kind of that whole space?

389
00:32:57,160 --> 00:32:58,760
Yes, we do.

390
00:32:58,760 --> 00:33:05,520
So before our ML platform, most of our customers track to these in spreadsheets and we

391
00:33:05,520 --> 00:33:12,560
notice that, so we build a repository where our DeepBird training jobs can automatically

392
00:33:12,560 --> 00:33:21,240
push the hyperparameters used and the experiment name and the resulting metrics like a PRAOC,

393
00:33:21,240 --> 00:33:24,720
accuracy, et cetera, into this model repository.

394
00:33:24,720 --> 00:33:31,400
And then you can query the repository for the experiments you have run and examine their

395
00:33:31,400 --> 00:33:34,320
hyperparameters and metrics.

396
00:33:34,320 --> 00:33:43,040
And do you support visualization with TensorBoard or do you have your own solution or an alternate

397
00:33:43,040 --> 00:33:45,960
solution for visualization?

398
00:33:45,960 --> 00:33:49,520
We mostly visualize using TensorBoard.

399
00:33:49,520 --> 00:33:56,440
So when you launch a DeepBird training job in our internal private cloud, we automatically

400
00:33:56,440 --> 00:34:04,200
start TensorBoard to watch the training process and render loss and other metrics.

401
00:34:04,200 --> 00:34:12,240
So this model repository we just talked about, once you query for experiment that finished,

402
00:34:12,240 --> 00:34:16,400
there's actually a button right in the UI that says launch TensorBoard.

403
00:34:16,400 --> 00:34:23,400
If you click on that button, it launches TensorBoard on an instance in our internal private

404
00:34:23,400 --> 00:34:29,160
cloud and it points the TensorBoard to that experiment that actually finished running

405
00:34:29,160 --> 00:34:36,360
and shows how the loss came down and how the resulting metrics look like.

406
00:34:36,360 --> 00:34:45,200
Is the platform also opinionated in terms of whether users use a notebook experience or

407
00:34:45,200 --> 00:34:49,240
traditional code files or ID?

408
00:34:49,240 --> 00:34:58,240
We're not opinionated on how the user developed code, but we do offer a notebook solution

409
00:34:58,240 --> 00:35:04,880
that's integrated with our internal clusters.

410
00:35:04,880 --> 00:35:11,400
We offer this thing called PyCX where our users can type a single command and it launches

411
00:35:11,400 --> 00:35:18,120
a notebook instance and it tunnels to the instance and gives back a URL that the user can

412
00:35:18,120 --> 00:35:19,120
use.

413
00:35:19,120 --> 00:35:25,360
It's a semi-hosted notebook solution, basically our users can type a single command and

414
00:35:25,360 --> 00:35:32,000
we launch a notebook server on our internal cloud and the user can use the notebook and

415
00:35:32,000 --> 00:35:36,840
it contains most of the dependencies our users would need.

416
00:35:36,840 --> 00:35:40,360
But we don't force our users to use notebooks for development.

417
00:35:40,360 --> 00:35:49,720
Are you doing anything to try to streamline the process of going from notebook to production,

418
00:35:49,720 --> 00:35:57,480
like some kind of automated code pulls or code extraction from the notebooks that kind

419
00:35:57,480 --> 00:35:58,480
of thing?

420
00:35:58,480 --> 00:36:00,000
I've seen that from time to time.

421
00:36:00,000 --> 00:36:01,480
Yeah, not yet.

422
00:36:01,480 --> 00:36:05,440
This is definitely an area that's worth considering.

423
00:36:05,440 --> 00:36:12,040
I've seen in the industry that there's paper mill that allows people to execute notebooks

424
00:36:12,040 --> 00:36:19,640
as a parameterized script and there's also git plugins that allows people to create

425
00:36:19,640 --> 00:36:22,520
very nice looking diffs of notebooks.

426
00:36:22,520 --> 00:36:30,560
Those are not yet explored in PyCortex, but it's a upcoming area that we plan to look

427
00:36:30,560 --> 00:36:31,560
into.

428
00:36:31,560 --> 00:36:33,680
Also, hyper parameter optimization.

429
00:36:33,680 --> 00:36:35,800
Did you mention anything in that regard?

430
00:36:35,800 --> 00:36:36,800
Are you doing that?

431
00:36:36,800 --> 00:36:39,080
Are you providing a solution to automate that?

432
00:36:39,080 --> 00:36:40,800
Yes, definitely.

433
00:36:40,800 --> 00:36:45,840
So this touches on the machine learning workflow component that I talked about.

434
00:36:45,840 --> 00:36:49,600
I briefly mentioned earlier to the pipeline orchestration piece.

435
00:36:49,600 --> 00:36:51,120
Yes, exactly.

436
00:36:51,120 --> 00:36:55,800
One thing we learned about machine learning is it's a very tedious process, and when we

437
00:36:55,800 --> 00:37:06,520
don't automate, our users don't necessarily do things in exhaustive or they don't, for

438
00:37:06,520 --> 00:37:07,520
example, typewriter.

439
00:37:07,520 --> 00:37:10,280
If you don't help them too much, they'll stay on their laptops and do it the way they

440
00:37:10,280 --> 00:37:11,280
used to.

441
00:37:11,280 --> 00:37:12,280
Exactly.

442
00:37:12,280 --> 00:37:13,280
Exactly.

443
00:37:13,280 --> 00:37:14,520
And hyper parameter search is one of them.

444
00:37:14,520 --> 00:37:23,360
We noticed that it often requires very repetitive and tedious repetitions of experiments.

445
00:37:23,360 --> 00:37:29,240
So our machine learning workflow solution allows our users to perform randomized search

446
00:37:29,240 --> 00:37:36,160
and grid search by launching a large array of experiments on our internal cloud and

447
00:37:36,160 --> 00:37:40,360
automatically recording hyper parameters and results.

448
00:37:40,360 --> 00:37:44,200
And they can essentially just pick the best solution.

449
00:37:44,200 --> 00:37:53,160
We also have a solution based on Bayesian optimization, where we have a service called wetlab,

450
00:37:53,160 --> 00:37:55,360
this is from the company we acquired.

451
00:37:55,360 --> 00:38:02,280
We give it the set of hyper parameters that we just tested and the results and the service

452
00:38:02,280 --> 00:38:06,160
tell us back what's the next set of hyper parameters to test.

453
00:38:06,160 --> 00:38:12,080
It automatically takes into account exploration and exploitation and recommends sets of hyper

454
00:38:12,080 --> 00:38:14,640
parameters to try next.

455
00:38:14,640 --> 00:38:24,080
And that's so far seems very effective and our ML workflow solution has completely automated

456
00:38:24,080 --> 00:38:26,520
solution for using wetlab.

457
00:38:26,520 --> 00:38:31,000
Essentially, you just need to say I want to do hyper parameter tuning and configure the

458
00:38:31,000 --> 00:38:36,840
hyper parameter tuner to be wetlab instead of grid search or randomized search.

459
00:38:36,840 --> 00:38:42,040
And the automated solution would take care of the hyper parameter tuning for you.

460
00:38:42,040 --> 00:38:46,360
I guess what's the kind of the fundamental currency in this system?

461
00:38:46,360 --> 00:38:52,600
Is it code that's checked into Git repository?

462
00:38:52,600 --> 00:38:54,360
Some people focus at the code level.

463
00:38:54,360 --> 00:39:00,000
I see others are dealing with containers and checking it and checking out containers.

464
00:39:00,000 --> 00:39:04,760
Is the core artifact for your system?

465
00:39:04,760 --> 00:39:10,200
For our system, the core artifact, I would say, are tensor flow graphs or actually the

466
00:39:10,200 --> 00:39:13,120
code that's defining the tensor flow graphs.

467
00:39:13,120 --> 00:39:19,600
The tensor flow graphs essentially defines the machine learning model.

468
00:39:19,600 --> 00:39:25,800
And as part of it, we also define which features are used.

469
00:39:25,800 --> 00:39:31,800
After that, we send this for training and training spits out another artifact called

470
00:39:31,800 --> 00:39:33,280
saved bundles.

471
00:39:33,280 --> 00:39:35,520
It's a tensor flow concept.

472
00:39:35,520 --> 00:39:40,320
And these saved bundles are stored on HDFS for serving.

473
00:39:40,320 --> 00:39:47,720
So I would say the core artifacts are both the tensor flow graph and the trained models.

474
00:39:47,720 --> 00:39:53,560
And presumably, you're versioning all of these and tracking the versions transparently

475
00:39:53,560 --> 00:40:00,160
to the end user, or is the end user thinking about development workflow explicitly?

476
00:40:00,160 --> 00:40:05,680
The end users do need to think about development workflow and how they version their code.

477
00:40:05,680 --> 00:40:10,320
Essentially, the tensor flow graph is code, so it's version by Git.

478
00:40:10,320 --> 00:40:16,440
But the models are end users need to version themselves.

479
00:40:16,440 --> 00:40:24,200
And have you seen within your user base that you've got ML engineers that are very comfortable

480
00:40:24,200 --> 00:40:28,160
with that kind of workflow, but data scientists that are less comfortable with that workflow

481
00:40:28,160 --> 00:40:32,480
and prefer it to happen transparently?

482
00:40:32,480 --> 00:40:40,280
So we have definitely seen that one of the things we learned was that there's a wide

483
00:40:40,280 --> 00:40:47,960
variety of machine learning practitioners ranging from machine learning engineers all the

484
00:40:47,960 --> 00:40:51,640
way to deep learning researchers and data scientists.

485
00:40:51,640 --> 00:41:00,040
And their use cases differ from production engineering versus more exploration and analysis.

486
00:41:00,040 --> 00:41:03,120
And they prefer quite different solutions.

487
00:41:03,120 --> 00:41:05,680
And what you mentioned is one.

488
00:41:05,680 --> 00:41:12,080
The scientists, they tend to prefer, for example, notebook, centric exploration and development

489
00:41:12,080 --> 00:41:19,000
and machine learning engineers prefer to write code and just git and check into Git.

490
00:41:19,000 --> 00:41:25,400
Right now, we don't have a very opinionated solution or we don't prescribe a development

491
00:41:25,400 --> 00:41:31,600
workflow across this different type of machine learning practitioners.

492
00:41:31,600 --> 00:41:40,360
We are, we start with catering to production ML engineers, we're in the process of starting

493
00:41:40,360 --> 00:41:46,280
to look at how deep learning researchers and data scientists test to use our platform

494
00:41:46,280 --> 00:41:48,480
and how we can make their lives easier.

495
00:41:48,480 --> 00:41:53,560
We jumped over to talking about that workflow or pipeline orchestration layer.

496
00:41:53,560 --> 00:42:00,480
Is that based on something open source like airflow or is it proprietary orchestration

497
00:42:00,480 --> 00:42:01,480
tool?

498
00:42:01,480 --> 00:42:04,600
It's based on top of Apache airflow.

499
00:42:04,600 --> 00:42:10,520
Are you doing both kind of online and offline workflows with that?

500
00:42:10,520 --> 00:42:17,800
Are you like doing offline scoring and or batch scoring using the workflow tool as well

501
00:42:17,800 --> 00:42:26,960
or is it primarily for the experimentation and kind of model development loop?

502
00:42:26,960 --> 00:42:31,960
It's primarily for offline training.

503
00:42:31,960 --> 00:42:35,880
For Twitter, the online part is actually real time, right?

504
00:42:35,880 --> 00:42:44,200
So we have prediction servers for that machine learning workflow is for training the models.

505
00:42:44,200 --> 00:42:49,920
I didn't fully understand what you mean by batch scoring like our predictions are like

506
00:42:49,920 --> 00:42:52,680
this like a user comes to twitter.com.

507
00:42:52,680 --> 00:42:57,680
For example, we need to present advertisement, we immediately need to respond.

508
00:42:57,680 --> 00:43:03,720
So this request hits our prediction servers and we generate scores in a real time manner.

509
00:43:03,720 --> 00:43:06,680
That's not using machine learning workflows.

510
00:43:06,680 --> 00:43:11,680
I think we thoroughly explored the first thing on your list of like seven things and we're

511
00:43:11,680 --> 00:43:15,320
40 minutes in.

512
00:43:15,320 --> 00:43:17,320
I see.

513
00:43:17,320 --> 00:43:23,160
We may need to to to be continued this.

514
00:43:23,160 --> 00:43:28,800
I am very curious about the meta team that you mentioned and maybe we can spend a few

515
00:43:28,800 --> 00:43:36,520
minutes talking about what you're doing there because it is an issue the issues around

516
00:43:36,520 --> 00:43:42,880
bias and accountability are and fairness are ones that are you know folks are starting

517
00:43:42,880 --> 00:43:50,200
to realize they need to pay more attention to and I'm curious how you've staffed up a

518
00:43:50,200 --> 00:43:55,240
team with the charter of that team is what the teams practices are and how they're tackling

519
00:43:55,240 --> 00:43:56,720
this issue.

520
00:43:56,720 --> 00:43:57,720
Sure.

521
00:43:57,720 --> 00:44:04,560
So this started from a year ago RCU Jack publicly tweeted that we're committing to

522
00:44:04,560 --> 00:44:10,200
increasing the collective health openness and civility of public conversations.

523
00:44:10,200 --> 00:44:15,560
And as part of that we're using machine learning to make algorithmic decisions to curate

524
00:44:15,560 --> 00:44:17,760
the public conversations right.

525
00:44:17,760 --> 00:44:25,480
So one thing we realize is we don't yet fully understand the impact of those algorithmic

526
00:44:25,480 --> 00:44:34,440
decisions and for example these algorithms decide what our users see right and what people

527
00:44:34,440 --> 00:44:41,160
see might actually change their behavior in response to the algorithmic recommendations

528
00:44:41,160 --> 00:44:47,440
and as a result their behavior shift and their behavior creates training data which feeds

529
00:44:47,440 --> 00:44:53,520
back to the algorithms which feeds back to the training data we use to train our machine

530
00:44:53,520 --> 00:45:00,760
learning models and this creates feedback loops and it's not yet super clear to us what

531
00:45:00,760 --> 00:45:09,480
exactly these feedback loops cause and for example there's right now there's research

532
00:45:09,480 --> 00:45:15,600
about how machine learning in social networks cause like polarization of opinions.

533
00:45:15,600 --> 00:45:25,880
So we started this team called meta to study like the bias for example and fairness and

534
00:45:25,880 --> 00:45:32,200
accountability and explainability of our machine learning models and we're staffing the

535
00:45:32,200 --> 00:45:37,440
team by partnering up with UC Berkeley professors.

536
00:45:37,440 --> 00:45:45,640
This is for two reasons for one this is the interdisciplinary effort it involves not

537
00:45:45,640 --> 00:45:53,880
just engineering there's also social science concerns there's legal concerns there's I

538
00:45:53,880 --> 00:45:58,680
don't know there's a lot of like essentially human concerns other than engineering concerns

539
00:45:58,680 --> 00:46:04,480
that's why we want diverse perspective and we're partnering with UC Berkeley professors

540
00:46:04,480 --> 00:46:11,640
and researchers the second thing they bring is they bring a like a because we're thinking

541
00:46:11,640 --> 00:46:18,880
about fairness and bias right the third party which is UC Berkeley which is not really a

542
00:46:18,880 --> 00:46:27,920
part of Twitter they bring a I think they bring exactly exactly that's why we staffed

543
00:46:27,920 --> 00:46:35,920
the team to be a partnership between Twitter engineers and UC Berkeley professors and researchers

544
00:46:35,920 --> 00:46:41,200
we want to make sure that we're not we're getting perspectives not just from engineers

545
00:46:41,200 --> 00:46:46,680
because this is not just an engineering problem it's also a social problem and when you when

546
00:46:46,680 --> 00:46:53,440
I think about a role like this or a team or a charter like this and what we want from

547
00:46:53,440 --> 00:46:59,560
them in today's environment it strikes me that a big part of this by necessity is kind

548
00:46:59,560 --> 00:47:06,120
of research and exploration and building understanding of these issues and how they play out in

549
00:47:06,120 --> 00:47:12,000
a network like Twitter's but you also want that to have engineering impact and you want

550
00:47:12,000 --> 00:47:16,440
to create a place where data scientists that are working on a problem or machine learning

551
00:47:16,440 --> 00:47:20,480
engineers that are working on a problem can take advantage of you know some degree of

552
00:47:20,480 --> 00:47:26,960
expertise in a very kind of practical tangible today kind of way how do you manage that

553
00:47:26,960 --> 00:47:36,000
that dichotomy with this team yes so this team is right now the main objective is to provide

554
00:47:36,000 --> 00:47:45,360
for example tooling and resource to increase explainability and transparency of our machine

555
00:47:45,360 --> 00:47:52,320
learning models for now we need to first understand what's going on right before we actually

556
00:47:52,320 --> 00:48:02,200
propose what to change so I think for now there really isn't a dichotomy yet between engineering

557
00:48:02,200 --> 00:48:10,520
impact and research essentially we're trying to measure for the first step and we are providing

558
00:48:10,520 --> 00:48:15,960
tools for different teams to be able to measure well it has been a great conversation

559
00:48:15,960 --> 00:48:23,680
still so much more for us to chat about but I think this was a very very interesting exploration

560
00:48:23,680 --> 00:48:28,640
of deep bird and how you're approaching training and I appreciate you taking the time to chat

561
00:48:28,640 --> 00:48:34,800
with us about it happy to continue the conversation next time if we we get a chance yeah absolutely

562
00:48:34,800 --> 00:48:43,560
thanks so much all right everyone that's our show for today for more information about

563
00:48:43,560 --> 00:48:49,520
today's guest or to follow along with AI platform volume 2 visit twimmelai.com slash

564
00:48:49,520 --> 00:48:57,480
AI platforms to make sure you visit twimmelcon.com for more information order register for twimmel

565
00:48:57,480 --> 00:49:03,280
con AI platforms thanks again to seek out for their sponsorship of this series to check

566
00:49:03,280 --> 00:49:07,760
out what they're up to and take advantage of their exclusive offer for twimmel listeners

567
00:49:07,760 --> 00:49:14,440
visit twimmelai.com slash sigopt as always thanks so much for listening and catch you next

568
00:49:14,440 --> 00:49:42,640
time.

