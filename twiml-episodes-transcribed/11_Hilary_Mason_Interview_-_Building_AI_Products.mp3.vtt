WEBVTT

00:00.000 --> 00:16.080
Hello and welcome to another episode of Twimmel Talk, the podcast where I interview

00:16.080 --> 00:21.600
interesting people, doing interesting things in machine learning and artificial intelligence.

00:21.600 --> 00:27.320
I'm your host Sam Charrington.

00:27.320 --> 00:32.200
I want to start out by wishing everyone a very happy and very belated new year.

00:32.200 --> 00:36.280
I'm finding it really hard to believe just how quickly the last few weeks of last year

00:36.280 --> 00:39.000
and the first few weeks of this year flew by.

00:39.000 --> 00:44.120
Needless to say, I'm super pumped to bring you this new episode of the show.

00:44.120 --> 00:47.920
Before we get going, I've got a bit of a holiday gift for some of you.

00:47.920 --> 00:52.400
That's right, over the last few weeks I've received a few requests from listeners who've

00:52.400 --> 00:56.400
wanted to listen to the podcast on their favorite home assistance.

00:56.400 --> 01:02.040
Well, it's taken a bit of doing, but I'm happy to report that the podcast is now available

01:02.040 --> 01:05.760
on both Amazon Alexa and Google Home.

01:05.760 --> 01:06.760
Check this out.

01:06.760 --> 01:12.680
Alexa, play the podcast this week in machine learning.

01:12.680 --> 01:16.840
You'd like to play the program called this week in machine learning, right?

01:16.840 --> 01:19.160
Yes.

01:19.160 --> 01:23.160
This week in machine learning in AI, getting the latest episode.

01:23.160 --> 01:36.400
Where it is from tune in, Hey, Google, play the podcast this week in machine learning.

01:36.400 --> 01:41.320
Learning in AI podcast, twimmel talk number 10 Francisco Weber statistics versus semantics

01:41.320 --> 01:47.240
for natural language processing.

01:47.240 --> 01:52.080
Note that for whatever reason, Alexa doesn't like when you ask for the podcast using its

01:52.080 --> 01:55.720
full name this week in machine learning and AI.

01:55.720 --> 02:00.680
But this week in machine learning works fine on Google either works.

02:00.680 --> 02:05.320
If you have any problems, just repeat the commands that I used in the demo.

02:05.320 --> 02:11.160
Now, I like to think that at least some of you are listening at home on your phone speakers

02:11.160 --> 02:15.400
and I've just commanded your device to play the podcast if that's the case.

02:15.400 --> 02:16.400
Enjoy it.

02:16.400 --> 02:17.400
Nice.

02:17.400 --> 02:20.040
All right, moving along to our program.

02:20.040 --> 02:25.160
This time around, our guest is Hillary Mason, who I interviewed last year at the O'Reilly

02:25.160 --> 02:28.280
AI and strata conference in New York City.

02:28.280 --> 02:32.160
I don't know that she'd refer to herself this way, but Hillary was really one of the

02:32.160 --> 02:35.280
first quote unquote famous data scientists.

02:35.280 --> 02:39.840
I remember the first opportunity I had to hear her speak was back in 2011 at the strange

02:39.840 --> 02:42.120
loop conference in St. Louis.

02:42.120 --> 02:46.600
At the time, she was chief scientist for Bittley, the company that popularized short links

02:46.600 --> 02:47.600
on the web.

02:47.600 --> 02:52.200
One of the days she's running fast forward labs, which helps organizations accelerate their

02:52.200 --> 02:58.920
data science and machine intelligence capabilities through a variety of research and consulting offerings.

02:58.920 --> 03:02.920
I tracked Hillary down at the AI conference after hearing from an attendee that her talk

03:02.920 --> 03:08.840
on practical AI product development was their absolute favorite session.

03:08.840 --> 03:13.040
Hillary and I had a wonderful, although somewhat brief chat, that I'm sure you're going

03:13.040 --> 03:15.960
to enjoy and learn a lot from.

03:15.960 --> 03:22.520
Of course, you can find this week's show notes at twimmelai.com slash talk slash 11.

03:22.520 --> 03:24.200
And now on to the show.

03:24.200 --> 03:25.200
All right.

03:25.200 --> 03:26.200
Hey, everyone.

03:26.200 --> 03:29.040
I'm here with Hillary Mason of fast forward labs.

03:29.040 --> 03:34.840
And we're at day two of the O'Reilly AI conference, the first actual O'Reilly AI conference

03:34.840 --> 03:36.200
as we were just discussing.

03:36.200 --> 03:37.200
That's right.

03:37.200 --> 03:41.360
And Hillary gave a talk yesterday that I didn't get a chance to see, but I heard great things

03:41.360 --> 03:42.360
about it.

03:42.360 --> 03:43.360
Oh, thank you.

03:43.360 --> 03:47.600
Now we start by having you introduce yourself and then you can tell us what your talk was

03:47.600 --> 03:48.600
about.

03:48.600 --> 03:49.600
Sure.

03:49.600 --> 03:50.600
So I'm Hillary Mason.

03:50.600 --> 03:56.320
I'm the founder of an independent machine intelligence research company called fast forward

03:56.320 --> 03:57.320
labs.

03:57.320 --> 04:03.600
And we look into approaches and algorithms that are emerging in the machine learning AI

04:03.600 --> 04:06.920
space, but that are not yet widely understood.

04:06.920 --> 04:11.800
And we do our own independent research to make them useful to people.

04:11.800 --> 04:16.800
So we write reports that are a survey of the techniques from a technical perspective

04:16.800 --> 04:20.880
at a conceptual level, talking about where we think it's going to go, any ethical issues

04:20.880 --> 04:25.000
that might come up, do a survey of the commercial landscape.

04:25.000 --> 04:29.320
So what vendors are out there, what we think the interesting application opportunities are.

04:29.320 --> 04:32.120
We also build working prototypes of these things.

04:32.120 --> 04:37.200
And finally, we act as technical advisors to our clients, like their nerd friend and

04:37.200 --> 04:42.720
help them actually build their AI machine learning products more effectively.

04:42.720 --> 04:44.920
So yeah, that's what we do.

04:44.920 --> 04:46.160
Everyone needs a nerd friend.

04:46.160 --> 04:48.520
Yeah, I mean, we all have that friend.

04:48.520 --> 04:53.600
Even if you are a nerd, you have your nerd friend on your music nerd friend and your

04:53.600 --> 04:57.920
friend who's most likely sitting in front of a computer at 9 p.m., we all have those

04:57.920 --> 04:58.920
people.

04:58.920 --> 04:59.920
Right.

04:59.920 --> 05:00.920
Right.

05:00.920 --> 05:02.160
So your talk, what was the title of your talk?

05:02.160 --> 05:06.120
So my talk was practical AI product development.

05:06.120 --> 05:11.000
And what I was trying to accomplish with this talk was coming into this AI conference.

05:11.000 --> 05:17.280
There's a lot of hype and a lot of lack of clarity around what it means to actually build

05:17.280 --> 05:18.760
an AI product.

05:18.760 --> 05:20.600
What an AI product is.

05:20.600 --> 05:24.840
So what I was trying to talk through are some of the challenges we've seen going from

05:24.840 --> 05:29.520
the idea and the algorithm, going from the press release, if you want to say it that way

05:29.520 --> 05:31.200
to the product.

05:31.200 --> 05:36.440
So being able to say, we have a data set, we have a business problem, we understand, and

05:36.440 --> 05:40.160
we have some, you know, we're willing to invest in trying to make something, how do

05:40.160 --> 05:41.160
we actually do that?

05:41.160 --> 05:45.880
And how does it differ from data analytics and how does it differ from software product

05:45.880 --> 05:46.880
development?

05:46.880 --> 05:51.840
There's a lot of people today are trying to take a machine learning product and sort

05:51.840 --> 05:56.080
of put it into the software development framework, and they tend to run into a few common

05:56.080 --> 05:58.040
friction points when they do that.

05:58.040 --> 06:02.280
Okay, and so I suppose those friction points were the body of your talk?

06:02.280 --> 06:08.840
Yes, so things like how agile software development is really optimized for building a product

06:08.840 --> 06:16.560
with commodity technology, but that isn't how you build a data product because you have

06:16.560 --> 06:24.040
to understand that maybe, even if it's a good idea, sometimes the algorithm you've chosen

06:24.040 --> 06:30.280
won't work, you have to make sure that the accuracy of the system is within sufficient

06:30.280 --> 06:31.280
bounds.

06:31.280 --> 06:36.200
There's a lot of work to do around how you productionize and operationalize these things,

06:36.200 --> 06:40.400
how you monitor, not just that the server is up, but that the model continues to return

06:40.400 --> 06:44.920
high quality results over time as the context and the data changes.

06:44.920 --> 06:50.120
And so all of these details are something that you really have to learn right now by doing

06:50.120 --> 06:55.440
it, and we have yet to really standardize on a common accepted practice.

06:55.440 --> 07:00.360
And so in my talk, I was sharing what we've learned and what we do, and then hoping to

07:00.360 --> 07:06.160
have conversations with people around what they do and what they're trying to do.

07:06.160 --> 07:08.640
And so yes, that's what the talk was.

07:08.640 --> 07:15.600
It seems like agile would be perfect for this environment where things like working closely

07:15.600 --> 07:23.800
with your customer, the end user of your product, failing fast, at least the things we commonly

07:23.800 --> 07:24.800
think about agile.

07:24.800 --> 07:28.800
There's also a whole software development lifecycle thing, which may be what you're referring

07:28.800 --> 07:29.800
to.

07:29.800 --> 07:30.800
Right.

07:30.800 --> 07:33.800
So on the surface, it's absolutely a compatible philosophy.

07:33.800 --> 07:35.800
Which is why everyone falls into the trap.

07:35.800 --> 07:36.800
Exactly.

07:36.800 --> 07:40.640
Because when you go to implement the details is when you run into the problem, when you

07:40.640 --> 07:46.000
have to say, you know, how long do I, how many points will do I think it's going to take

07:46.000 --> 07:50.280
me to find an algorithm that can produce a useful result?

07:50.280 --> 07:58.200
And it doesn't take into account the machine learning process of developing, really experimenting

07:58.200 --> 08:02.840
and saying that, you know, I might try to solve problem A, but it turns out problem A is

08:02.840 --> 08:06.520
really a lot harder than I thought it would be, but I can solve problem B that's also

08:06.520 --> 08:12.000
useful in this product context, and it also doesn't deal with the, once you have something

08:12.000 --> 08:18.760
that sort of works, doing the simplification and scalability work, which is just as hard

08:18.760 --> 08:24.960
as the initial algorithmic work, but often gets overlooked in a, you know, AI conference

08:24.960 --> 08:27.520
where everyone's excited about what's shining in me.

08:27.520 --> 08:28.520
Okay.

08:28.520 --> 08:35.520
So to me, that says that it's not that agile methodologies are fundamentally, you know,

08:35.520 --> 08:43.280
placed in these types of problems, it's that our sensibilities for estimating and, you

08:43.280 --> 08:49.320
know, understanding the development process that kind of feed into an agile methodology

08:49.320 --> 08:50.320
are off.

08:50.320 --> 08:51.320
Like we don't have.

08:51.320 --> 08:52.320
We don't have a philosophy.

08:52.320 --> 08:53.320
Right.

08:53.320 --> 08:54.320
They agree.

08:54.320 --> 08:58.400
But the, the mechanisms are second-class citizens.

08:58.400 --> 09:03.680
So you can allocate a spike time to go figure out an algorithm that that's a hack and there's

09:03.680 --> 09:07.920
no first-class mechanism for this sort of experimentation and iteration.

09:07.920 --> 09:08.920
Okay.

09:08.920 --> 09:09.920
Okay.

09:09.920 --> 09:13.520
And so how did you, with that being kind of the premise for your talk, what were some of

09:13.520 --> 09:16.720
the things that you, that you dove into?

09:16.720 --> 09:18.240
So I love to tell stories.

09:18.240 --> 09:22.040
So I talked about a couple of projects we've attempted that didn't work out.

09:22.040 --> 09:26.760
So one was using a deep learning image classifier to let you take a picture of your plate and

09:26.760 --> 09:31.600
get a calorie estimate, which, okay, that sounds, maybe that sounds like a good idea.

09:31.600 --> 09:34.480
My team, we thought it was a good idea, at least worth trying.

09:34.480 --> 09:36.040
We found a lot of data.

09:36.040 --> 09:39.000
There's a lot of food photography out there.

09:39.000 --> 09:43.400
And there's also a lot of data on, you know, a cheeseburger has this many calories.

09:43.400 --> 09:44.400
Right.

09:44.400 --> 09:50.600
It did not work because that data, a cheeseburger can have anywhere from 300 to 2,400 calories

09:50.600 --> 09:54.360
and these data sets just simply don't agree.

09:54.360 --> 09:58.240
And we did, you know, first we're like, okay, we want the actual calorie count from the

09:58.240 --> 10:02.000
plate and then we decided on a more modest problem, which was, can you tell us if it's

10:02.000 --> 10:05.480
very healthy, healthy or not at all healthy?

10:05.480 --> 10:11.480
And eventually we decided that it was no longer worth the time and investment because

10:11.480 --> 10:14.920
the quality of result we could get was not actually useful.

10:14.920 --> 10:18.760
And of course, this is a fun story to tell because a couple months after we did this whole

10:18.760 --> 10:23.520
process, Google announced that they had in fact solved this problem.

10:23.520 --> 10:27.920
And you know, to me, that sort of validates that it was a good idea, but we didn't have

10:27.920 --> 10:30.120
the resources to make it work.

10:30.120 --> 10:34.680
So I talked about that story also went into depth on a brief, which is an extractive

10:34.680 --> 10:40.160
summarization prototype we built using neural networks for articles.

10:40.160 --> 10:45.280
So being able to take an article and pull sentences out of that article that are an effective

10:45.280 --> 10:48.760
summary of the entire content of the article.

10:48.760 --> 10:52.640
And that's something where there's a product design piece and there's an algorithm design

10:52.640 --> 10:59.160
piece and they have to work together well in order to make a usable, useful, fun prototype.

10:59.160 --> 11:02.200
And so I went through that whole example in the talk.

11:02.200 --> 11:04.480
Can you talk about that one in a little bit more detail?

11:04.480 --> 11:07.480
How did you go about that and what was the process like?

11:07.480 --> 11:11.560
Yeah, so the work we do is always framed around an application.

11:11.560 --> 11:16.360
And so as much fun as it might be to say, like, okay, we want to spend four months using

11:16.360 --> 11:20.560
deep learning to analyze text, which is really what we did.

11:20.560 --> 11:25.720
We decided to focus first on summarization and then under summarization or sort of two

11:25.720 --> 11:28.920
major schools of system one is extractive.

11:28.920 --> 11:33.840
So pulling words and sentences out of the body of text.

11:33.840 --> 11:37.880
And there's abstractive, which is constructing a summary that may contain language that does

11:37.880 --> 11:41.680
not appear in the underlying text that's new language.

11:41.680 --> 11:46.120
We focused on extractive because again, in the product context, we could actually build

11:46.120 --> 11:49.120
something with a high enough quality result to be useful.

11:49.120 --> 11:53.640
Whereas on the abstractive side, we're still as a community very early.

11:53.640 --> 11:55.920
And so the results are kind of variable.

11:55.920 --> 11:57.320
So again, there was that focus.

11:57.320 --> 12:01.120
And then within that, we looked at a couple formulations of the problem.

12:01.120 --> 12:06.120
So one is, can I take any article and extract those sentences?

12:06.120 --> 12:08.680
And that's the system we ended up building.

12:08.680 --> 12:15.720
It's trained on about 18,000 human authored summaries with quotations of news articles.

12:15.720 --> 12:18.640
And it works very effectively on those.

12:18.640 --> 12:23.280
We also did a second formulation of the problem around multi-document summarizations.

12:23.280 --> 12:29.080
So if you have 5,000 documents on the same topic, can you cluster them effectively and

12:29.080 --> 12:30.560
then summarize each cluster?

12:30.560 --> 12:33.920
And for that, we used LDA for that first step.

12:33.920 --> 12:38.200
And actually, my colleague Mike Williams will be at strata tomorrow talking about all of

12:38.200 --> 12:41.360
the technical fun stuff underneath it.

12:41.360 --> 12:43.320
Yeah, if you're interested in that.

12:43.320 --> 12:50.640
OK, and so for that example, the data set that you use, was that a public data set?

12:50.640 --> 12:55.400
Yes, it's from a website called The Browser, which is the terrible website name.

12:55.400 --> 12:57.080
Because of the ambiguity there.

12:57.080 --> 13:00.240
But yeah, so it's a public data set.

13:00.240 --> 13:02.680
And one that turned out to be quite effective.

13:02.680 --> 13:03.440
Oh, interesting.

13:03.440 --> 13:06.680
And LDA, latent, dearest lay allocation.

13:06.680 --> 13:07.440
Absolutely.

13:07.440 --> 13:09.800
And how does that, I've heard that come up a few times.

13:09.800 --> 13:11.040
I don't really know how it works.

13:11.040 --> 13:12.520
What's the 30,000 foot on that?

13:12.520 --> 13:20.120
OK, so the quick conceptual overview is that it's a non-supervised or unsupervised algorithm,

13:20.120 --> 13:26.320
meaning you take the stream of text, and it is able to infer related clusters in the

13:26.320 --> 13:28.720
text fairly effectively.

13:28.720 --> 13:32.760
One of the limitations is that you have to tell it how many clusters to look for, which

13:32.760 --> 13:38.000
you may or may not have an intuition for going into an analysis.

13:38.000 --> 13:42.480
Which again, means that practically the way people handle that on any given body of text

13:42.480 --> 13:48.480
is to sort of try 10 clusters, 100 clusters, and then narrow their way intuitively.

13:48.480 --> 13:52.720
And by clusters, are we talking like N-grams, or are we talking conceptual clusters?

13:52.720 --> 13:56.120
We're talking groups of documents in this particular case.

13:56.120 --> 13:59.440
So we applied it to Amazon product reviews.

13:59.440 --> 14:04.640
And we found particularly great results in the pet product review category, because this

14:04.640 --> 14:13.240
is a section where people are quite passionate about the items they, it's no surprise, I guess.

14:13.240 --> 14:19.560
But we were, you know, a couple of examples we ran into were things like a dog toy that,

14:19.560 --> 14:24.360
you know, 90% of the reviews were five-star and 10% were one-star.

14:24.360 --> 14:28.680
And so when you look at the clusters of those reviews, you see that, you know, most of

14:28.680 --> 14:32.200
them are things like, this is cheap, I can buy it in Amazon, it's great.

14:32.200 --> 14:36.520
This is really good for my dog's emotional well-being, and yes, people are very concerned

14:36.520 --> 14:41.480
with their dog's emotional well-being, and then the 10% were sort of like, yeah, my dog

14:41.480 --> 14:46.240
ate part of this and had to have a $4,000 surgery.

14:46.240 --> 14:52.280
And so that's the kind of structure you're able to pull out with LDA.

14:52.280 --> 14:58.840
And the utility there, I think, is fairly obvious, or rather, one of the things I mentioned

14:58.840 --> 15:03.600
in the talk is that we tend to see these algorithms applied to making things we already do more

15:03.600 --> 15:04.880
efficient.

15:04.880 --> 15:10.560
So if you can make that 20-page article down to two pages, that's making me more efficient.

15:10.560 --> 15:15.880
But if you can make me able to read 5,000 documents, which I could not possibly, I could

15:15.880 --> 15:21.360
not possibly ever stand to read 5,000 reviews of the same dog toy, I can tell you that.

15:21.360 --> 15:24.480
But now I can get a similar amount of value.

15:24.480 --> 15:29.040
That's sort of a really useful AI product.

15:29.040 --> 15:34.400
When you say a similar amount of value, what was your optimization function?

15:34.400 --> 15:37.920
How did you measure whether the value was similar?

15:37.920 --> 15:39.880
So that's a really good question.

15:39.880 --> 15:47.080
And in the case of our brief prototype, we had some human curated test data.

15:47.080 --> 15:53.200
But to be honest, a lot of this is really intuition, which I know is a dirty word in this

15:53.200 --> 15:55.160
context, the world of AI.

15:55.160 --> 16:02.440
But I really do believe in the value of user testing feedback loops and human intuition

16:02.440 --> 16:07.800
and guiding the product aspects of this sort of work.

16:07.800 --> 16:14.320
So what were the, did you have kind of an enumerated list of takeaways from the talk?

16:14.320 --> 16:16.640
Was it prescriptive or was it?

16:16.640 --> 16:22.480
So it was more laying out a shared vocabulary and then sharing some experiences, but I'm

16:22.480 --> 16:29.640
not going to presume to tell you how it's done, because I think that where we are in the

16:29.640 --> 16:36.360
development of the practice of AI product building is still very early.

16:36.360 --> 16:42.480
And this is, you know, I've been a data scientist since the very beginning, and it's very similar

16:42.480 --> 16:47.320
to what happened with the evolution of the profession of data science, where a lot of

16:47.320 --> 16:52.920
people are doing a lot of different interesting things that are all related.

16:52.920 --> 16:58.160
But there's no one vocabulary, no one process that everyone has agreed on yet.

16:58.160 --> 16:59.960
And so I shared my point of view.

16:59.960 --> 17:05.080
I got to talk to people afterwards for an hour and a half out here hearing other people's

17:05.080 --> 17:06.080
point of view.

17:06.080 --> 17:10.520
And it's just, we're at one of those really exciting moments, I think.

17:10.520 --> 17:11.520
Yeah.

17:11.520 --> 17:12.520
Yeah.

17:12.520 --> 17:16.440
Have you done, have you set in on anything else at the event?

17:16.440 --> 17:20.680
Like what else do you think is cool and interesting kind of in this realm?

17:20.680 --> 17:26.080
So at this particular conference, one thing I'm really impressed by is the different perspectives

17:26.080 --> 17:27.080
in the room.

17:27.080 --> 17:32.680
So most of the conferences I've been to are either technical or sort of business or sort

17:32.680 --> 17:36.040
of product design.

17:36.040 --> 17:41.120
Here we have everyone in the same room, which is great, you know, VCs, business folks start

17:41.120 --> 17:47.640
up people, big company people, you know, software developers and machine learning professors

17:47.640 --> 17:48.640
all here.

17:48.640 --> 17:50.840
So that's really cool.

17:50.840 --> 17:55.520
I've heard a couple of, you know, I always love the opening keynotes.

17:55.520 --> 17:56.920
They were pretty great.

17:56.920 --> 18:01.240
And then there have been talks on everything from, you know, TensorFlow for mobile poets,

18:01.240 --> 18:02.840
which is Pete Wardens talk.

18:02.840 --> 18:07.520
And he is a great blog post if you haven't seen it all the way over to the future of natural

18:07.520 --> 18:11.200
language generation from the folks that automated insights.

18:11.200 --> 18:14.040
So it's, you know, just a few of the things I've been enjoying.

18:14.040 --> 18:15.040
Yeah.

18:15.040 --> 18:16.040
Nice.

18:16.040 --> 18:17.040
Nice.

18:17.040 --> 18:18.640
So how long have you been doing fast forward labs?

18:18.640 --> 18:22.840
So fast forward labs is going to be two and a half years old soon.

18:22.840 --> 18:24.160
Okay.

18:24.160 --> 18:28.080
And we are eight people plus two interns based in Brooklyn.

18:28.080 --> 18:29.080
Oh, nice.

18:29.080 --> 18:34.440
We are actually moving our office this week over to Atlantic Ave Barclays Center.

18:34.440 --> 18:35.440
Oh, cool.

18:35.440 --> 18:39.800
So yeah, you were any of your audience should let us know and come stop by if you're in

18:39.800 --> 18:40.800
the new your head.

18:40.800 --> 18:41.800
Nice.

18:41.800 --> 18:42.800
Nice.

18:42.800 --> 18:43.800
Awesome.

18:43.800 --> 18:44.800
Well, I appreciate you taking the time.

18:44.800 --> 18:45.800
I know you've got a meeting and run off too.

18:45.800 --> 18:46.800
Well, thank you so much.

18:46.800 --> 18:48.640
It's great to get an overview of your talk.

18:48.640 --> 18:49.640
Oh, yes.

18:49.640 --> 18:50.640
Great to have this conversation.

18:50.640 --> 18:51.640
Thank you.

18:51.640 --> 18:52.640
All right.

18:52.640 --> 18:53.640
Thanks a lot.

18:53.640 --> 18:54.640
All right, everyone.

18:54.640 --> 18:56.280
That's it for today's show.

18:56.280 --> 18:58.160
A quick note for you guys tomorrow.

18:58.160 --> 19:02.640
I'm off to reworks deep learning summit in San Francisco.

19:02.640 --> 19:08.040
If any twimmel listeners are attending or will be in the area, please reach out to me.

19:08.040 --> 19:11.240
I would love, love, love to connect up with you.

19:11.240 --> 19:17.720
Also, please do leave a comment on the show notes page at twimmelai.com slash talk slash

19:17.720 --> 19:25.560
11 or tweet to me at at Sam Charrington or at twimmelai to discuss this show and let

19:25.560 --> 19:27.160
me know how you liked it.

19:27.160 --> 19:28.920
Thanks so much for listening.

19:28.920 --> 19:35.920
Catch you next time.

