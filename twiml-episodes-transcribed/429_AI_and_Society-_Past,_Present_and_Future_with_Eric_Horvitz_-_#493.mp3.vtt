WEBVTT

00:00.000 --> 00:23.000
All right, everyone. I am here with Eric Corvitz. Eric is the chief scientific officer at Microsoft. Eric, welcome to the Twomal AI podcast. Great to be here. Thanks for having me.

00:23.000 --> 00:33.000
I am really looking forward to our conversation and I'd love to start it off by having you share a little bit about your background and how you came to work in AI.

00:33.000 --> 00:47.000
Thanks. Well, I had a long interest in how the brain works, how the mind works. I think this is some of these are some of the earliest questions we ask ourselves as kids.

00:47.000 --> 01:03.000
And going into as an undergraduate, I was interested in biology and the origins of life and basic questions. I did a degree in biophysics. I went off to do an MD PhD in neuro biology in neuroscience.

01:03.000 --> 01:23.000
Larger because as I got into my junior and senior years at college, I started working in a lab where I would a neuro biology lab where I would actually pull micro pipettes, the little electrodes that can get into the neuron of brains in this case of little animals.

01:23.000 --> 01:46.000
And listen in to the clicks and the clacks, actual potentials. I started getting more and more curious about brains. I wanted to go through the MD PhD path. I thought that would give me access to people someday and I was interested also with going interest in biomedicine.

01:46.000 --> 02:11.000
It's Stanford where I started my graduate work in the first six or seven months or so. I started making a commitment to moving more towards computation, because I thought more and more that we wouldn't get great insights through sticking little electrodes in brains for a thousand years that might go to the mathematics side of things.

02:11.000 --> 02:24.000
I remember thinking that what I would do if I was to stay on the MD PhD path in neuro biology would be kind of akin to taking a little copper wire and putting it into a chip.

02:24.000 --> 02:44.000
The motherboard of a most days in Apple to computer and try to figure out the operating system. I just didn't want to be going there. I thought I had better insights to jump ship to its computer science. And I just said, remember the moment where I was in CS and doing classes and I said, this is what I want to be doing.

02:44.000 --> 03:12.000
It's interesting that you describe that transition. I think some of the most interesting conversations I have on the podcast are ones that explore this interplay between the biology and what we're doing on the computational side and the two way intellectual street between the two fields of study.

03:12.000 --> 03:24.000
Absolutely. And you might say that in both cases, and of course, the related, there's this interesting magic. We don't understand the bio and how bio makes minds.

03:24.000 --> 03:40.000
And then we have this mechanism machine computing side that we actually can make progress on and understand and we're always facing off at this big mystery and getting we hope inching towards it.

03:40.000 --> 03:48.000
But sometimes the casm is even even dark matter. We don't know how far away we are from getting getting there.

03:48.000 --> 04:08.000
Among other things, you've been very involved in a number of industry organizations, including the association for the advancement of AI. I think you probably didn't go into all that because if you listed all of them, we'd be, you know, 40 minutes in.

04:08.000 --> 04:28.000
One of the, and I think you are president at one point. And at that in 2009, you as part of your involvement in that organization kicked off a study of AI ethics, which is going to be one of the themes that we dig into in this conversation.

04:28.000 --> 04:39.000
And if you can maybe tell us a little bit about that effort, how it got started. Yeah, context was for it. You know, and it's funny, because it seems so.

04:39.000 --> 04:46.000
Over the top back then when I proposed this, but when I, when I.

04:46.000 --> 05:00.000
I was president of triple AI, you basically, you know, it's kind of a process, your, your president elect, then your president, then your past president. So it's multi your service. But when I came in, it's interesting.

05:00.000 --> 05:23.000
And I was a election of where AI was in 2007, 2009 was my kind of term. And I decided to make the theme of my presidency, AI in the open world. And what I meant by that was, you know, let's take what AI is coming out of the laboratories and it's becoming, it's getting useful, it's getting real.

05:23.000 --> 05:36.000
And the open world as I, as I like to refer to it is pretty complicated, it's much bigger than our systems can represent, it's much more complicated than our laboratory environments.

05:36.000 --> 05:59.000
And the build systems on a technical side that have the ability to understand their limitations such that they're humble and know what they don't know. And might this might not this be one of several technologies, we need to master to get systems to be robust and reliable in the open world that they weren't so overconfident and brittle and fragile.

05:59.000 --> 06:09.000
And so I gave my presidential lecture on that topic of where we had to go as a field, president to triple, I give a lecture one of their years of their office.

06:09.000 --> 06:23.000
And then I also as a proposed that we needed to sort of come together as leading AI scientists and think about the implications of our technologies.

06:23.000 --> 06:33.000
On people on society, the influence is broadly about what it would mean for ethics and law and safety.

06:33.000 --> 06:42.000
And it was this was the time when people were reading books by, and then probably still are by Ray Kurzweil and singularity coming and.

06:42.000 --> 07:04.000
This kind of utopian view of what AI where it might go, as well as a distopian view kind of being bantered about. And I thought, you know, we haven't really weighed in as folks that are committed to the technology as the scientists who you think should be responsible for the technologies go.

07:04.000 --> 07:22.000
All together about 25 people that I chose, especially for diversity and also for knowing them quite well, I mean, these people would be committed to figure things out wouldn't take, wouldn't say I was crazy for asking questions about long term.

07:22.000 --> 07:39.000
AI advances as well as ethical and legal challenges as well short term disruptions. And so we had we booked the this group up into three different working groups, short term disruptions long term AI futures and ethics and law.

07:39.000 --> 07:56.000
And at, and for several months with me, we all came together in February of 2009 at a cinema that was symbolic, that's where the biologists had met a couple of decades before talk about recombinant DNA, what it would mean.

07:56.000 --> 08:11.000
Two days of two and a half days of fabulous meetings covering multiple topics and that was the first time I even heard working with, even heard this phrase, I remember from the third term group, criminal AI.

08:11.000 --> 08:28.000
What is that, you know, criminal uses malevolent uses by nation state and non state actors of how you could use AI now with these new kinds of information feeds coming together from consumer uses of AI, for example, right, and data sets and connectivity.

08:28.000 --> 08:46.000
We also looked at a long term AI futures addressing the concerns of dystopian futures losing control of AI we hear from in books like later that came out like from Nick Postram and others super.

08:46.000 --> 09:11.000
I asked somebody, one of the members was very good, a fabulous scholar, I said, hey, look at Asimov's laws of robotics, you know, and could you, could we implement those laws in constraints and preferences and really feel the system we could completely trust to do the right thing when it came to safety.

09:11.000 --> 09:28.000
And we had other issues as well and law and someone that became, you know, again, we're, I would say foreshadowing of what came of this area, by the way, speaking of disruptions and possibilities on the first day of that asylum are meeting.

09:28.000 --> 09:50.000
I invited two people who I thought would be the best to able to answer this question, it was, it was Andrew Ang at the time, Tom Dean that might then Google, I said, can you tell us, you know, what will, what surprises would, would were on our horizon in the near term, like what would surprise us as AI researchers.

09:50.000 --> 09:58.000
It doesn't, this is basically 2009 in February, and they both gave deep learning talks, which was kind of exotic at the time.

09:58.000 --> 10:13.000
It provides an unsupervised learning and that summer, you know, literally, it was like that summer in July and August deep neural networks were, were found to basically be.

10:13.000 --> 10:26.000
It's an interesting power that was not leveraged in the past because we just didn't have the data set to do it, and that's when we saw the explosion right after they're in a workshop in October and boom, so they predicted a surprise that was going to have big effects.

10:26.000 --> 10:34.000
That's still even three years before what we often think of as the real explosion with ImageNet in 2012 right.

10:34.000 --> 10:46.000
Well, I think what happened was a permit lens on the situation at the time was, this is a funny story behind this involving Jeff Hinton and me.

10:46.000 --> 10:54.000
But I'm not going to go there exactly unless you really problem that, but Jeff and join us at Microsoft, it sounds like a challenge.

10:54.000 --> 11:17.000
Jeff joined us at Microsoft research that summer, and our, we looked at our speech recognition and word error recognition rates, and we saw this jump on a data set called the switchboard data sets that had been flat in terms of the people hammering on it for, for a decade.

11:17.000 --> 11:28.000
We said, like, this was a big deal, and this happened at MSR, you know, building 99 here at Microsoft, and that led to a workshop in that in the fall.

11:28.000 --> 11:33.000
That was really what people together to say what was going on, like how did this happen.

11:33.000 --> 11:45.000
And that was the path up towards ImageNet and so on, but it came out of, and the discovery was, by the way, and we're off the track a little bit of the ethics and responsibility.

11:45.000 --> 11:50.000
You might just hear your listeners.

11:50.000 --> 12:00.000
The discovery was that, because the methods weren't that different from the neural nets of the late 80s and 90s, early 90s.

12:00.000 --> 12:09.000
But the discovery was these neural nets had been famished all these years for data, and we just didn't know it.

12:09.000 --> 12:17.000
And so the, so you kicked off this effort in 2009, and what was the scope of it?

12:17.000 --> 12:24.000
Was it limited to this meeting, or was it a long term study? How, how did you approach thinking about this issue?

12:24.000 --> 12:36.000
Yeah, we, I thought it was exotic enough just to have a multi month single study with a simple report that we wrote this web page on the, on the event.

12:36.000 --> 12:42.000
We did a report out at the Ijkai conference. It's another international meeting.

12:42.000 --> 12:45.000
The following summer.

12:45.000 --> 13:01.000
Got lots of interest in response that briefing led to an article that was surprisingly on the front page of the New York Times about our gathering above the fold as they say.

13:01.000 --> 13:09.000
And I created all this excitement and fear, as well as, you know, questions.

13:09.000 --> 13:14.000
I remember the phone was ringing off the hook when a headline appeared.

13:14.000 --> 13:19.000
Something along the lines. You still find this online as New York Times piece.

13:19.000 --> 13:29.000
Like AI scientists discuss how machines might outsmart man. That was kind of the headline.

13:29.000 --> 13:34.000
And that, you know, we get calls from all over the major news, you know, organizations and so on.

13:34.000 --> 13:39.000
Like, wait a minute. This wasn't like a crack pot. This was a group of scientists saying this.

13:39.000 --> 13:44.000
Should we take this seriously? That was the kind of questions we were getting at the time.

13:44.000 --> 13:54.000
So, you know, again, this meeting is, you know, predates the popularity of deep learning predates Alex net.

13:54.000 --> 14:03.000
And clearly predates kind of our contemporary conversations around trust, trustworthiness and responsibility and bias.

14:03.000 --> 14:12.000
How, how do you compare and contrast what you talked about then and what you talk about now and how do they relate to one another.

14:12.000 --> 14:17.000
You know, Sam, this is, this is like the reality of where we were with AI back then.

14:17.000 --> 14:24.000
So the basic assumption.

14:24.000 --> 14:29.000
My colleagues and I had as grad students and then young, you know, early early career scientists.

14:29.000 --> 14:37.000
We're along the lines of if we can just get this thing to work, you know, it's team coming out of it and huffing and puffing years rotating.

14:37.000 --> 14:39.000
It's a big success.

14:39.000 --> 14:48.000
And, and the things you would build would be like small numbers of things like a prototype or a prototype and its descendant.

14:48.000 --> 14:54.000
You build like a read missions prediction system for health care. And I was like, you just would say, yes, it works.

14:54.000 --> 15:02.000
I get well calibrated probabilities out of this thing that a patient will bounce back in 30 days, for example, the training off electronic health records.

15:02.000 --> 15:10.000
It was always a celebration of the goodness of getting anything to just darn work.

15:10.000 --> 15:26.000
And it was a stretch at the time, start thinking about stuff that not just works with it work poorly or not, but that would be distributed and have huge effects on the planet.

15:26.000 --> 15:39.000
And, and so it's just a different feel for the level of influence and responsibility that became much more obvious.

15:39.000 --> 15:51.000
A few years later, in fact, in 20, 13 or so, just four years later, I thought to myself.

15:51.000 --> 16:01.000
And this again, we should have another meeting like this and get this same group or a bigger group together because it's getting more important now to get the scientific point of view, the scientists who are really committed.

16:01.000 --> 16:06.000
They could understand some of the problems deeply and maybe address them.

16:06.000 --> 16:11.000
And I said, OK, it will be five years in 2014.

16:11.000 --> 16:26.000
And then I thought, you know, it's kind of like induction and gets in plus one. We should do this every five years for like the rest of the history of human civilization is this stuff is going to move fast as technology and it's going to have a huge influence.

16:26.000 --> 16:36.000
And we need to understand our relationship with the technology and that's when I had a discussion with my significant other.

16:36.000 --> 16:50.000
I wonder if we could make our first bit of major, you know, first philanthropy, significant philanthropy, the endowment of some sort of a center that would always do a report every five years and make sure it happened.

16:50.000 --> 16:59.000
That would be proactive that would look out that would have a sort of framing of a set of core topics that will be kind of timeless in terms of concerns.

16:59.000 --> 17:08.000
And this is what led to the 100 year study that's now at Stanford. It's called the 100 year study on artificial intelligence.

17:08.000 --> 17:16.000
But it really is a study that will happen every five years for as long as Stanford University exists.

17:16.000 --> 17:25.000
And John had to see at the time assured me he thought it would last longer than 100 years.

17:25.000 --> 17:38.000
So in thinking about we can maybe take a detour a little bit and talk about your role at Microsoft briefly.

17:38.000 --> 17:53.000
And because I want to explore, you know, the ethics of, you know, the theoretical ethics and what I imagine is much more real and tangible.

17:53.000 --> 18:06.000
Yeah, you know, issues that you must encounter when talking to customers that are actually trying to do things and assessing, you know, risk and responsibility and things like that.

18:06.000 --> 18:23.000
Yeah, so this we're got, yes, I just mentioned some of the framing, which was the Stanford study for me and thinking about this for other colleagues and then getting this, this Stanford University.

18:23.000 --> 18:27.000
Long term study set up over time.

18:27.000 --> 18:42.000
And at Microsoft, there was rising interest and concerns as well, given our role in the world and how central AI was two Microsoft and to our future.

18:42.000 --> 19:03.000
And as far as the, I say the, the modern history of work in this space, Sachin Adele kicked off a discussion in 2016, called together a small group and we asked ourselves and he asked you let the discussion, what are our AI principles.

19:03.000 --> 19:27.000
And it was a very interesting discussion and it converged on six principles that was 20 at one time and 14 but can't converge down to six, which we have, we call Microsoft AI principles, fairness, reliability and safety, privacy and security, inclusiveness, transparency and accountability.

19:27.000 --> 19:39.000
And we have a little internally at Microsoft, we have a little sentence and then click through for bigger paragraph and more detail on each one of those six principles that we've now.

19:39.000 --> 19:48.000
Socialized with the company, we have an annual training about what they mean, but that was the beginning and then around those principles.

19:48.000 --> 20:16.000
Working with Brad Smith, I set up what's called the ether committee, and ether A E T H E R stood for and stands for AI ethics and effects in engineering and research now is the start in 20 late 2016 or 2017 of a committee and then a set of working groups that would focus on specific topics largely aligned with those principles, a few more.

20:16.000 --> 20:40.000
One of them was called for your question, Sam, the sensitive uses panel and this panel became the body by which we would define and then late roll out a process for the company to send sensitive uses for review.

20:40.000 --> 20:57.000
And even defining for Microsoft for our processes for ether committee, what was what was a sensitive use of AI technology is kind of an interesting endeavor in itself early on.

20:57.000 --> 21:26.000
Keeping it simple, but powerful, for example, so we just give you a sense, we said, okay, a sensitive use of AI, one that should come to the sense of uses panel for review is if a product team is thinking about developing a product or has one out there already that can has the risk of denying consequential services to somebody or subgroup health care, financial education.

21:26.000 --> 21:36.000
Or is there a risk of physical or psychological harm or could the technology in fringe upon human rights.

21:36.000 --> 21:49.000
So a few years ago we developed technology and others have come from similar things that we call neural TTS text speech.

21:49.000 --> 22:17.000
That was fabulous in that you could speak to a system, give it a few snippets of how you speak and have the a deep neural net model learn to speak like you so well that it could, you know, it could, if you had ALS and losing your voice, you could rely on this instead of a machine like text of voice to speak to your family.

22:17.000 --> 22:27.000
Or have it be used to narrate in a very natural way broadcast news.

22:27.000 --> 22:38.000
It also can be used from a level in purposes, someone can steal your voice and I have you call your bank and write a check and send it somewhere.

22:38.000 --> 22:48.000
Or could fame to be a politician, a political leader making a statement that could be very dangerous and threatening fraudulence.

22:48.000 --> 23:00.000
So sensitive uses committee deliberated and send a recommendation to our SLT that this technology should be gated should not be freely available.

23:00.000 --> 23:09.000
It should be licensed on a certain restrictions such that like a like a news organization could license it for use.

23:09.000 --> 23:22.000
That's an example of the kind of technology that really it went through a process whereby a panel met with work with the local expert team in our cognitive services area.

23:22.000 --> 23:39.000
And the report which went to the ether board that report was refined typically what's done in those reports is that there's a recommendation that's made an alternate or one or more alternates each one will be a cost benefit analysis.

23:39.000 --> 23:55.000
That's why we're doing this if we don't do this here's some costs here's the upside the downside and then we if if if it's something that has not been precedent that really needs senior top leaders or the company to look at and make a call.

23:55.000 --> 23:59.000
We take it up to the senior leaders.

23:59.000 --> 24:12.000
I say that I don't remember a time where the you know where the top one or two recommendations wasn't the one that was selected from the ether committee for execution by the company.

24:12.000 --> 24:36.000
The responsibility of AI is to educate the folks that are going to use it. I imagine there's quite a lot that goes into internal communications around a effort like ether and you know how training engineers to know when they should raise the flag and start to ask questions about a technology.

24:36.000 --> 24:52.000
Are there a set of educational responsibilities or things that you think that we as an industry, you know need to be committed to from an education perspective around responsible use of AI.

24:52.000 --> 25:05.000
How do you think about that I think so and this you know Microsoft feels and has been running like an AI business school trying to share these principles with with partners.

25:05.000 --> 25:17.000
I I'd like to see more sharing going on at the level of details among all the partnership on AI organizations, including the nonprofits.

25:17.000 --> 25:23.000
To come up with best practices and to educate about them.

25:23.000 --> 25:32.000
Education and training came up as being critical topics and requirements for recommendations being made.

25:32.000 --> 25:51.000
When it came to this we haven't gotten to this this story yet, which is the story of the national security commission on AI, which I just came off of and we just fielded a report after two years, but it really called out the need.

25:51.000 --> 26:01.000
You know for training and best practices when it comes to these to the fielding of AI technologies when it comes to national security agencies of various kinds.

26:01.000 --> 26:14.000
Sorry to jump ahead if you would go there, but no such a central point of the training and sort of getting people on the same page when it comes to engineering, building, fielding, reviewing these systems.

26:14.000 --> 26:24.000
I hope to have you share a bit about that effort and your involvement in it is one of the things that caught my interest.

26:24.000 --> 26:33.000
Because I think there's you know we think you mentioned you didn't use the words, but you alluded to kind of smart cities as one of the ways that folks.

26:33.000 --> 26:45.000
The government wants to apply artificial intelligence and you know sometimes that comes off as a euphemism for surveillance and we.

26:45.000 --> 27:02.000
I think have a you know we're justified in being suspicious of security and security apparatus and their use of technologies like AI tell us about that report that effort, how it came about and some of the key findings.

27:02.000 --> 27:05.000
Yeah, so in.

27:05.000 --> 27:16.000
In 2018 Congress, US Congress, which commissioned this study.

27:16.000 --> 27:21.000
Called for a deep analysis of.

27:21.000 --> 27:44.000
AI and its relationship to where the United States is and could be when it came to applications of AI and this panel, we call the national security commission on AI or NSC AI was established in 2019.

27:44.000 --> 27:55.000
It's gone for two years and on March 1st, it fielded its report that's available at NSC AI dot gob if you want to read the report.

27:55.000 --> 27:59.000
It's a 750 plus page report.

27:59.000 --> 28:02.000
Don't forgive him for not having finished it.

28:02.000 --> 28:04.000
I think I think it's well done.

28:04.000 --> 28:19.000
I think we did sections you want to look at with care. I was mostly involved in terms of my detailed attention in on on for the chapters, as they're called one on the future of R and D, whereas AI heading.

28:19.000 --> 28:38.000
The second one on trustworthy AI and building trustworthy systems, the third on civil liberty civil rights and privacy of AI systems getting into our values.

28:38.000 --> 28:51.000
And the fourth on this really interesting and challenging area that there's lots of emotion about for good reason AI and lethal autonomous weapons.

28:51.000 --> 29:10.000
These four chapters that I think are and the others are really lay out the ideas, directions, challenges with clarity and put a lot of work into writing for both experts and non experts alike.

29:10.000 --> 29:28.000
I was chair of the area we called ethical and responsible AI which led to those two chapters chapter seven and eight seven on trustworthiness and eight on I would say values ethics civil liberties.

29:28.000 --> 29:49.000
Interesting, the original name for my line of effort that I chaired was AI ethics, but you think about what we mean by ethics ethics often brings to mind issues of values and rights and trade offs and

29:49.000 --> 30:04.000
deep questions about beliefs. And but there's a whole other area of responsible AI which is in which which gets into engineering issues like

30:04.000 --> 30:17.000
human AI collaboration methods, the role of the human being when it comes to working with AI systems issues around security and

30:17.000 --> 30:26.000
trustworthiness reliability, how well does an AI system face adversarial attacks.

30:26.000 --> 30:44.000
These new kinds of attacks called machine learning attacks and so on until you wouldn't necessarily call those kinds of issues ethics I would call them like you know engineering challenges with robustness and trustworthiness.

30:44.000 --> 31:11.000
So just splitting up the that are area into two pillars, one focusing on what I would say is value civil rights civil liberties, which I think is very important in a report on national security agencies using AI as you mentioned before, does this interesting tension and we want to be careful to to know who is that we are as the United States and our constitutional principles.

31:11.000 --> 31:32.000
Looking at issues of reliability, what I can say is and I'm very proud of about this report is the report really considered values civil liberties and civil rights and privacy as foundational to everything in the report.

31:32.000 --> 31:54.000
And I remember when somebody in one of the plenary we don't have these plenary sessions with like public and journalists, somebody asked a question at the zoom meeting wasn't this going to be costly all those values and all the reviews you're asking for and the oversight and the committees and the processes in place.

31:54.000 --> 32:15.000
That put us in a non competitive place in the world with AI my reaction was you know this is who we are and it means it's going to be costlier and it may slow us down but it's who we are.

32:15.000 --> 32:44.000
This is quiet at that point because you think about it, the technologies can be dangerous, they can be oppressive, they're already being used by authoritarian regimes to oppress and we need to be cautious and careful and provide an example to the world about a norm for how these technologies can and should be used when it comes to agencies like FBI, NSA, CIA,

32:44.000 --> 32:55.000
Homeland Security, health and human services and so on that we were kind of looking at for the agencies that will be using these technologies.

32:55.000 --> 33:05.000
Well, I want to dig into that report some more and if you don't mind I'll pick your brain as to some folks to talk to the dig deeper into some of the issues that.

33:05.000 --> 33:17.000
Really, by the way, I was curious, well, how did I get selected to do this to be a call commissioners and if you're looking in the way that Congress wrote the.

33:17.000 --> 33:20.000
The charge.

33:20.000 --> 33:43.000
But they said the committee will consider the methods and means necessary to advance the development of AI machine learning and associated technologies to comprehensively address the national security and defense needs of the US, well you look at why I selected they had a whole like a list of how commissioners will be selected in the actual legislation they had.

33:43.000 --> 33:59.000
You know, on record, you know, so the the chair of the House Armed Services Committee gets to vote for one person and that's happened to be Eric Horvitz I said well, that's really.

33:59.000 --> 34:11.000
Adams with, you know, the sent you know I was actually invited directly the Pentagon has and so each commissioner came was invited by a different agency or a different process.

34:11.000 --> 34:21.000
But it was such a wonderful group of people to work with these commissioners fabulous some of some long term colleagues some people had just met.

34:21.000 --> 34:32.000
All, all pretty, you know, you know, deeply passionate about doing a good job and thinking through these issues for the United States.

34:32.000 --> 34:43.000
I want to be sensitive to the time that you've been so gracious to share with us and maybe start to wrap up and have you share a bit about.

34:43.000 --> 34:53.000
Not just your vision for the future, but also how kind of dovetailing to our conversations about.

34:53.000 --> 35:13.000
You know citizenry how we as citizen scientists developers can engage in the process at all the various levels that it's taking place government company organizations any any thoughts on that it's a bit of a broad question.

35:13.000 --> 35:35.000
Yeah, I have lots of thoughts. I mean, look, I mean, I just was invited to give some opening remarks to computer science educators at the sexy technical symposium couple weeks ago, which just brought it's the largest gathering of educators.

35:35.000 --> 35:43.000
My comment was boy, it's it's great to see all this interest into your science also great to have all the influence.

35:43.000 --> 35:51.000
In the world, but with great power and influence now as computer scientists.

35:51.000 --> 35:55.000
Whether it be an AI or systems or networking.

35:55.000 --> 36:12.000
Or HCI, you know, comes incredible responsibility now. You never would think, you know, when I was doing computer science work, we were worried about, you know, algorithms again, again, getting things to work and understanding big O and so on.

36:12.000 --> 36:21.000
You would never would have thought that all of a sudden it's like we have to be teaching in classes now and we are it's been great to see the innovation going on.

36:21.000 --> 36:47.000
You know, you know, you know, even asking ourselves big questions like, is it okay to use machine learning to optimize clicking on an ad or how much time someone engages with a service with a timeline with an Xbox game.

36:47.000 --> 36:56.000
And so we're getting these interesting questions that we have to start thinking about and become very mindful of engineers as we go now.

36:56.000 --> 37:05.000
That we are talking about people's lives, how they spend their time with content they digest.

37:05.000 --> 37:32.000
But we're talking about security issues, we're talking about breaches of data potentially, we're talking about oppression versus freedom in the world, who would have thought that computer science would have become so influential as to require us all as scientists and as engineers and as students to be thinking deeply about

37:32.000 --> 37:48.000
issues that are now not foreign but that there's such that there are a deep part of computer science and computer engineering, whether it be an industry or in other areas.

37:48.000 --> 38:00.000
And that's just I have to take a deep breath sometimes just think of how far we've come and how important these technologies are and how important they will be going forward.

38:00.000 --> 38:17.000
Only more important over time in terms of thinking through how small biases, for example, let's say they're small, they can be quite big, but even a small bias in how a system is working based on small biases in data.

38:17.000 --> 38:29.000
And that come from the way the data was collected from a bias society as we all know historically could now amplify the biases unmass.

38:29.000 --> 38:41.000
And so we have those issues, the issues of privacy, you know data is the fuel of AI systems.

38:41.000 --> 38:53.000
We know and many of us we get our we get our applications on our phones and we see some sort of, you know, no matter how privacy oriented we are, we're kind of busy people, we say okay, okay, okay, okay, sure, sure location.

38:53.000 --> 38:55.000
Yeah, okay, I'll be useful.

38:55.000 --> 38:58.000
And we don't realize that enough.

38:58.000 --> 39:21.000
We see a story in the press that a government agency was able to purchase a large scale database of the location trails of people commercially, which is okay, concurrent laws that is a gray area in particular, I think, actually, and we see new questions coming up.

39:21.000 --> 39:35.000
Meaning we have large scale commercially available data about faces with names that are available in many applications that could be brought together and fusing to databases that are available commercially.

39:35.000 --> 40:01.000
So you can just walk through every one of, for example, Microsoft AI principles, I'll say that again fairness, reliability and safety, privacy and security, inclusiveness, transparency, accountability, you can walk through to those six principles, and you have rich hard questions, incredible opportunities for getting things right.

40:01.000 --> 40:13.000
Also, the ability to stumble. And so let's work to be done.

40:13.000 --> 40:41.000
So, pulling back from asking another question that, oh, for it, we will pull up a few minutes. So, you know, in there, you mentioned, you mentioned HCI in passing, and I know that one of your, one of your topics of interest is kind of complementarity and kind of the way humans in AI will interact.

40:41.000 --> 40:53.000
So, my sense is that that shapes a bit of your vision for how, you know, we should be developing AI, and I thought it would be great to hear you riff on that for a bit.

40:53.000 --> 41:13.000
So, I would say that my, besides the deep principles of how computation leads to minds of the form that we find ourselves thinking with right now, this might, my second most,

41:13.000 --> 41:24.000
the focus has been on human AI complementarity and collaboration and the future of our relationship with AI systems. I think that we can do a lot there.

41:24.000 --> 41:39.000
We're already making some technical advances where it's designed plus there's a science, for example, how machine learning can be harness to understand human blind spots and weaknesses and to complement people.

41:39.000 --> 42:08.000
We work together as a team for greater holes. I think by, we, we can design better futures for, for civilization, for humanity in terms of our well-being and our labor and our work by being deliberate about how we think about how we should be designing AI systems to work with people versus

42:08.000 --> 42:30.000
thinking about an automated agent or fully autonomous technologies. That will happen too, of course, but there's so much opportunity in this space of, for example, helping physicians to recognize when they may have missed something in a patient will crash.

42:30.000 --> 42:41.000
The phrase in medicine, I was just talking about at a Stanford symposium called failure to rescue, which means that you miss something and a patient died in the hospital.

42:41.000 --> 42:53.000
And we've built systems that can actually look out eight hours, 12 hours and say, this patient will need a life saving intervention within four hours, just to alert every staff and the team.

42:53.000 --> 43:02.000
For example, of, of the kinds of complimentary we can have between humans and machines that we have a great partnership.

43:02.000 --> 43:11.000
People are always going to be central in the world and to think through what that means in terms of having supportive nurturing technologies is going to be important.

43:11.000 --> 43:31.000
We have some really fun work on design for you many collaboration and exciting work on mathematical principles and optimizations will change the objective function of the machine learning, you know, the gradient descent such that now it's considering where people are strong and weak.

43:31.000 --> 43:41.000
We just published that piece at triple AI in January and then a piece on optimizing for teamwork in February edge, Kai, so it's fabulous directions.

43:41.000 --> 43:46.000
Maybe to poke in a little bit more on that and maybe be more concrete.

43:46.000 --> 44:12.000
You talked about kind of this human centrism to the way we think about developing AI, do you think as a thought experiment that we should have put the investment that we're applying into autonomous vehicles to more compliment, I don't know what we would call it driver assist technologies and should.

44:12.000 --> 44:41.000
If we, you know, is it the case that by investing in the moonshot of full autonomy, we support or get the technology advances to the driver assist as an interim state or would we have, for example, reduced the, you know, the number of deaths per mile or per thousand miles more quickly if we focused on the human computer centric model directly as opposed to as a.

44:41.000 --> 44:44.000
Byproduct of full autonomy.

44:44.000 --> 44:59.000
Well, no one, no one would argue that having a perfectly autonomous vehicle wouldn't be desirable, you can read a book on the way to Portland from Seattle, for example.

44:59.000 --> 45:21.000
But on the trajectory of where the technologies have gone and the pace, different companies have taken different approaches, so one major manufacturer has said we believe AI should be there as safety nets and to assist drivers to be to drive more safely and to reduce deaths that way.

45:21.000 --> 45:32.000
Other companies have said we're going for it and you'll you'll have to grab the wheel once in a while in the system shouts, but we're trying our best to go for it.

45:32.000 --> 45:43.000
It's, you know, it's hard to know what's the best path, but at any moment, I would think you want to take the technology that you have and think through its best use.

45:43.000 --> 46:12.000
The given the nearly 40,000 deaths in the US, by way, it's been going up, I think because of distraction devices, but the per year US and I think it's over over a million deaths in the internationally per year of driving that we need it's a it's a health health disasters, we need, you know, assistance and safety and nurturing as soon as possible.

46:12.000 --> 46:36.000
But you wouldn't necessarily think that the goals are disparate right you can get into the design and the details of what we were talking about before, how do you design for really great nurturing systems that can enhance safety and let humans remaining control and look at the best of both worlds at the current technical prowess capabilities that we have.

46:36.000 --> 46:51.000
Awesome, well, so much more there, as has been the case with all of the areas that we've explored, but I want to thank you so much for taking the time to chat with us and share a bit about what you're working on and thinking about.

46:51.000 --> 46:57.000
Yeah, thank you, it sounds like we need more time Sam, so fun talking.

46:57.000 --> 46:59.000
Absolutely, thanks Eric.

46:59.000 --> 47:06.000
Thank you very much.

