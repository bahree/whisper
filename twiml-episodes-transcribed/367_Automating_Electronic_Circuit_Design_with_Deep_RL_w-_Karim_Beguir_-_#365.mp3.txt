Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Hey, what's going on everyone?
I was just checking my calendar and noticed that by this time last year, I had already
been to about half a dozen conferences in three different cities.
While I don't miss planes, airports or hotels at all, I really do miss the opportunities
that conference visits created for me to really connect one-on-one with listeners and guests
of the podcast.
Of course, what we're all learning now is that physical separation does not need to
mean that we're disconnected.
So let's you and I use this as an opportunity to connect virtually.
I'd love to hear from you, yes, you, about how things are going for you and these unique
times we've found ourselves in.
What are you working on?
What new routines are you establishing?
What new breakthroughs or realizations might you be finding in your work or personal
life?
Please reach out and let me know what you're up to via text or voicemail to 314-582-5935.
via email to sam at twomolei.com or via message on our website or your favorite social media
platform.
I hope you're staying safe and healthy and helping others stay healthy by observing social
distancing.
And now on to the show.
Alright, everyone.
I am on the line with Karim Begear.
Karim is co-founder and CEO at InstaDeep.
Karim, welcome back to the Twomolei podcast.
Hi, Sam.
It's a pleasure to speak again.
Absolutely.
So if Karim's name sounds familiar, that's because we spoke, we were trying to figure
this out.
It was between a year and a half and a year ago.
The show actually was published in September.
It was number 302 and you should definitely check it out for Karim's full background.
But Karim, why don't you give us a brief overview of what you're up to as well as an update
from what we last spoke?
Absolutely.
So first, it's a pleasure to be back and continue our conversation.
On our side, it's been pretty inventful.
The lot has happened.
As you know, InstaDeep is a decision-making AI startup.
So we focus on problems related to making complex decisions.
We also do our own innovation in AI and try to be helpful to the community.
And we've made progress, basically, on these three areas.
We've been able to release innovative products in decision-making AI.
We've also been able to publish and innovate in research, publishing original pieces that
were actually welcome, that no ribs were we got as spotlight presentation, for example,
with Google DeepMind.
And we've also been very active on the community side, organizing major events in Africa
and helping basically lots of young talents find and save the opportunities in AI.
That's right.
And we most recently saw one another at Nurebs and had a chance to catch up briefly
at the Black and AI dinner, where you really pick my interest around one of the company's
new initiatives or products, which is called DeepPCB.
Tell us about what DeepPCB is.
Absolutely.
So DeepPCB actually started with a conversation.
Two years ago, I had a dinner with a good friend of mine, who is actually an expert in hardware
design, worked on chips for well-known phones, et cetera.
And we were speaking about what is AI doing in this particular sector.
And he was like, not that much, like in particular, like PCBs stand for printed circuit boards.
So basically, those are the chips that you will find with all sorts of consumer electronics
products, iPhones, speakers, Bluetooth, et cetera.
And you know, the situation in that market was that, you know, auto-rooters basically
automated systems to connect the different components, like build basically the electrical
circuitry have been going on for many years, but they were not that great.
And we were like, hey, that sounds like an interesting problem to look at.
We started looking to it, eventually, this good friend, Nabil Truba, who is now leading
our hardware team, joined in steady.
And we've worked very hard on this project, and we're very proud, you know, to have been
able to achieve our goals.
And in November last year, we've released it in beta form, and it is a world first.
For the first time, we have an AI system that is end-to-end, fully deployable and scalable
on the cloud, capable of understanding how to root chips, essentially.
And now last time we spoke, our conversation was focused on the work your company was
doing, applying deep reinforcement learning to logistics, is DPCB also based on deep reinforcement
learning?
Absolutely.
And there is a very strong commonality and like design philosophy between our products.
So in a sense, let me give you an example.
We've continued to do great work in logistics, and recently last September, we've won a major
contract, for example, with Dutchaban, the German railway company.
And to give you an idea, this is about routing trains on a large scale, talking about 10,000
trains a day, you know, and something on some 33,000 kilometers of railway.
But turns out, there are commonalities between routing trains and routing chips on a board.
And so we've realized that, you know, the projects and the type of research that InstaDep
is doing, is actually applicable to multiple fields.
And when it comes to in particular printed circuit boards, the opportunity was compelling.
So we went full speed ahead and this turned out to be our first product.
Alrighty.
So when you initially met with your friend, your friend mentioned that they're, you
know, while these auto routers have been in place or have been in use for many years,
they were not without their challenges and problems.
What were some of those challenges and problems and what was the opportunity to introduce AI?
I think auto routers have been, you know, they've been, there's been a lot of great work done
on auto routers, but in terms of like design philosophy, the design philosophy is all about
essentially using heuristics to solve problems.
And we spoke a little bit about this in our past conversation.
So it's very similar to, let's say, what was the status of software for chess before Alpha
Zero came out, those systems, which worked very well actually, but still are built on
heuristics for the hardest problems, you know, heuristics have limits.
And a system that can essentially mobilize learning and learning at scale can get better
results.
When it comes in particular for in the status of printed circuit boards, it is actually
incredible.
And we are in 2020 that actually complex circuits are still designed manually.
And the reason why people design those manually is because auto routers essentially fail
to deliver the goods to the degree of quality, which is expected by high quality customers.
So as a consequence, we see a really compelling opportunity with modern AI built on the latest
innovation.
Some of it actually developed in house.
We actually have patents on the work we've done for the PCB.
There is an opportunity to like accelerate the design cycle of products, because it's
not just about quality.
Human engineers do absolutely amazing work and have amazing intuition.
That's about the speed.
A human engineer could take in certain cases multiple weeks if not months to completely
route a complex board with modern AI.
We do believe that instead of that, this timing can be brought to 24 hours.
This, if done at scale, would be tremendous for the industry and it would accelerate the
product cycle.
We are very used in consumer electronics to have a cycle every six months or a year, there
is a new version coming.
We believe that AI could actually accelerate that cycle.
And as a consequence, also make it easier to design new products and experiment and ultimately
unleash more human creativity mobilizing AI.
You mentioned complexity of the boards as being one of the challenges.
What are we talking about when we talk about complexity?
I'm assuming we're measuring that in, for example, a number of components, but having
work with circuit boards before there are also issues like the number of layers and things
like that.
When you talk about a complex board, what exactly are you talking about?
So a board really consists in basically an AI is that we need to connect and as a consequence,
like those, they can be what needs to be connected.
So you have pairs of components essentially that need to be connected and there could
be thousands of those.
And as you mentioned rightly, they could be multiple layers.
Simple designs start with two or four layers, but you could have a lot more.
And the more you have layers, the more components you have to connect, the harder the problem.
It is an NP-hard problem.
And so this is where AI can help, particularly when you're looking at the most difficult
designs that we take human engineers' significant amount of time to solve.
So how do you frame this problem as one that reinforcement learning can be applied to solving?
So this is another case of needle and a haystack style problem.
So you have an extremely large solution space to give you an idea.
The game, of course, was 10 to the power 170, roughly.
We are talking here at significantly more, like this can be very, very large.
And so the question is, how can I build a system that's going to figure out what is a good
solution to this problem?
And of course, you have lots of rules that you need to abide by.
We call those DRCs, like basically design rules checks that need to be absolutely verified
for the circuit board to be valid and to be deployable in production.
So the question is, you have this massive optimization under constraints where essentially
you need to find a good solution.
And we know from recent progress and from also our own experimentation that AI systems
can be built using deeper reinforcement learning to build up intuition.
And so it's a little bit like humans will do it, except that you have the benefits of
doing it very consistently and importantly at a very large scale.
We're talking about potentially like hundreds of CPUs, if not thousands working together
with GPUs to accumulate experience and sort of build up the right knowledge base to crack
the problem.
And so when you talk about this knowledge base, can you go into a little bit more detail
into how you're building the agent and the learner and how you're kind of representing
this knowledge that's being accumulated?
Yeah, so this is very similar to, for example, what has been done by DeepMind for the
game of chess and Go.
You're going to have a learner that's essentially going to play games.
And based on the outcome of these games is going to start to learn what works and what
doesn't.
So you can see this problem as basically having a reward function, which is completing
the design.
If you've completed the design and it is DRC clean, so it satisfies to all the design rules,
then that's a good outcome.
And so you are effectively starting, let's say from a random point, but learning as you
go and sort of putting more probability on the paths that are yielding a good solution.
And of course, devil is in the detail on those systems.
But if you do this diligently enough, you're going to get a system that's going to start
to figure out what it should do, first on small boards and then on bigger ones, and ultimately
get you to competitive results.
Now, I would imagine when a human engineer is working on these kinds of problems, there
are certainly completion of the board is a primary goal, but I'm imagining they're also
worried about the total length of their traces, which probably corresponds to noise on the
board or latency between components, that kind of thing.
In other words, a lot more kind of nuance characteristics than just whether all of the
components are connected.
Are you able to take these kinds of things into account?
Absolutely.
So those actually participate into the design of the reward function.
So this is the way to look at it.
Of course, exactly like you said, it is desirable to have a secret length as small as possible.
It is desirable also to have as little as possible changes from one layer to another through
what we call VS.
So those can actually be expressed in the design of the reward function, such that you sort
of have everything you need and that the system has everything it needs to learn.
But absolutely, and one of the areas where we spent a lot of time was how can we incorporate
those into like a functional mechanism that triggers learning.
And we're very happy to say that actually this is positive, like this is possible.
And when we look at the first announcement we made in November last year, when we announced
the beta release, actually many engineers came to us and they just couldn't believe that
an AI system would go end-to-end and crack this problem.
But this is another proof of how powerful AI can be in 2020.
And in particular, deep reinforcement learning applied to decision-making problem is truly
disruptive.
So at InstaDip, we are focusing on those type of problems and we believe there is tremendous
value to be unlocked for customers in terms of improving efficiency, accelerating design
cycles and the like.
If you describe the process as one of starting with simple boards and working your way up
to more complex boards, can you elaborate that on that?
If you've got a particular problem with a particular set of circuits that need to be
connected to one another, is the learner that you're trying to create to route this particular
board?
Is it starting with the subset of components and then gradually increasing the complexity
of what it's doing or is it are you talking, are you referring to kind of building up a
knowledge base across multiple, maybe even theoretical, you know, board layouts that
you have created just to train the learner and then you can take this pre-trained learner
and apply it to new boards?
It's not necessarily the final algorithm that we designed, but it's more like the approach
we took, like to crank them problem like this that actually many people have tried and
failed is in and this is a good generic principle in different foster learning.
Start with a small problem that is almost too stupid, right?
But make sure that it is working, make sure that what you expect to see is indeed what
you see.
Devil is in the detail in those systems and deep aerial systems in particular are not
too usually tricky to train because, you know, a problem could come from multiple sources.
It could come from this like straight out bug into your system or it could be that you
have the wrong initialization parameters from an applied math point of view and so you're
doing everything right, the code is right, but the system still won't learn because
the parameters have been badly initialized, so I'm describing more philosophy.
We really start with baby cases and build up our skills, expertise and train progressively
the better agents, realize what work what doesn't.
So there is a lot of like in-depth analysis and details that you need to do to be able
to have systems that actually are ready for the real world and I think that's an interesting
problem like, you know, when we look at the work that the Institute teams do, I think
what is really excited about is how can we take those algorithms that have been tested
mostly on games and very well-defined environments if you want and sort of translate them into
the real world, you know, whether it's routine boards or trains or anything else.
And you know, there are lots of challenges that come with that, but also at the same time,
if you manage to overcome the challenges, it is truly an exciting time and you can see
actually customers come back and give you great feedback.
Customers say, please keep me in the loop for your next release and we've been constantly
improving our product and answering recently releases, for example, supporting Altram,
which is a key standard and so on and so forth.
So I think there is a true excitement in doing things in the real world, even though of course
it's much harder.
So can you maybe compare and contrast the RL applied to games in this particular scenario?
You know, I'm just thinking through some of the differences in the game scenario, you've
often got, you know, other agents or things in the game, in the environment kind of respond,
you know, randomly or probabilistically to the things you do or at least via some,
you know, some complex function.
In this world, you've got your set of components and unless you're doing kind of deep physics
based, you know, simulations of the interactions between the components, you maybe you're not
getting into kind of random responses to things like follow down that thread for me.
So one of the key differences is when it comes to gaming, the reward system is very clear.
You know, there is a score to maximize, there is an opponent to defeat.
So there is full visibility in a sense on what the reward is, which is quite important.
So you're already saving a lot of time.
You know what's the objective.
When it comes to the real world, if I tell you, for example, like, okay, I have this,
you know, and this is what we were talking about.
I have this complex board with, you know, you know, eight layers and a thousand pairs
to connect.
Well, what is the reward function is actually a very good question.
You could probably work for years on what is the best reward function you could come up
with.
So that is one first real challenge.
The second challenge is when you design the environment, making sure you incorporate all
the elements that are important in actually design, like coding and modeling the environment.
If I take the example in logistics that we did and the work we do with railway companies,
that is actually not a trivial thing to do in games.
The environment by construction is already given to you and is very clear, not just the reward,
but the environment itself.
If you're talking about, for example, a complex train network with dozens of thousands
of trains operating every day, you know, there could be problems on one railway.
What's the consequence of that problem?
Just modeling the environment is a challenge in itself.
Once in a good scenario, you've modeled the environment.
The next challenge is speed in games.
By construction, most games are quite fast because there are, for example, designed for
massively online multiplayer games, for example, things of that nature.
So the latencies are, in the order of milliseconds, sometimes, building a real-world environment
that represents properly the problem that you're looking at, and at the same time, ensuring
that it's quick enough so that you deploy deeper out is actually a non-trivial thing.
So as you can see, every step of, you know, this challenge, which is kind of already pre-built
in for games, becomes very tricky in the real world, but also very interesting.
This is also why it's not a surprise that all the deeper out breakthroughs tend to happen
on games, because that is sort of the ideal framework to get results.
And so going back to my earlier question, is what you've done here that you've trained
a model using reinforcement learning that you can then apply to kind of an arbitrary
new board that you're presented with, or is there an element of training that has to
happen when you see a new board, or fine-tuning, or something like that?
It's a bit of both.
So the right analogy is what would qualify a human expert, too, and he comes with a set
of knowledge, a knowledge base, an intuition about how to tackle those problems, but of course
he's going to spend time sort of fine-tuning his approach to the problem at hand.
So our design philosophy is pretty much the same.
There is pre-built-in experience and knowledge that comes when we tackle a new board, but
there is also an amount of fine-tuning, basically essentially.
This system will learn an experiment on every new board and get something out of it.
And when you think about it and you compound those effects, this is how you build a learning
system that can progressively tackle harder and harder problems, and together with compute,
with more customer-served, sort of potentially redefine the standards in the industry.
This is our goal with DPCB.
Today DPCB is in beta format, so it's sort of still learning every day and learning quickly,
but still learning a lot.
We would like to bring it to a point where the tool is so useful that it is widely adopted
by in the industry and helps the industry achieve its goals faster and more efficiently.
If we manage to crack boards at scale in less than 24 hours, this would change the life
of many companies operating in consumer electronics and for the better, because they could be able
to experiment with different designs faster, maybe bring products to market that would
have been impossible otherwise, because the cost of having a design team work for it for
two or three months is just an affordable.
How do you characterize where you are relative to that goal?
How complex are you able to get now and how long does it take to do a board?
I think it's pretty remarkable what we have achieved already, especially that we are
a small startup and do not have access to large compute.
The key thing for us now, if you want to take DPCB to its full potential, is really
unleash tremendous amount of compute and train on much larger boards, so our goal for this
year is really move from free beta, where we are effectively trying to help customers all
over the world and getting feedback in the process, getting them to a point where we can
compete with professional product offerings, and ultimately, if we do everything well,
we design the state of the art in the industry.
So I would say we are well on our way, probably more than 50 percent done, but still a lot
of work.
You are not going to answer my question.
We are in beta, we didn't tell you that.
I am trying to get a sense for how far along this is relative to both how you measure
it.
You talked about the number of components and layers and all that stuff, but also how
far along you are relative to that for solving real world problems, is this something that
is currently useful to someone that wants to route a PCB or not and how do you know
or how would they know if their problem is practical for what you've done so far?
Yes, so the great news is we are actually already solving real PCB boards for real customers
and we're very proud of that.
Those boards are still small, we're currently limiting in beta to 150 pairs and two layers
and we're going to progressively expand that.
So this is where we are at the moment, but the good news is that actually we've already
received very good feedback from customers from literally everywhere at the US Asia and
that the system works, so we're pretty excited about it.
We made a comment that one of the limitations is access to compute or at least you gave
the impression that this is constantly running and constantly improving, talk a little bit
about the relationship with compute and the compute requirement and how you're addressing
that.
Absolutely, so if you look at how the parallel systems work, there is always this concept
of accumulating experience through multiple simulations, gathering this experience to improve
your model through gradient descent and iterating again.
And if you look at the kind of amount, the kinds of amount of compute that you need to really
have, for example, a breakthrough in DPRL, it's quite significant.
If you look at StarCraft, for example, like DeepMind actually did spend millions of
dollars to crack that problem, today, instead, it cannot spend millions of dollars to define
the StarCraft.
So what we're doing actually, we're raising funds and one of the reasons we're raising
a series B, essentially, one of the reasons actually to have enough compute capabilities
to push our systems to redefine the state of the art.
So there is a real opportunity out there, but a compute is a necessary equation when it
comes to DPRL, of course, you can focus on sample efficiency and the like, but you will
not be able to crack those problems with, let's say, a limited amount of GPUs and CPUs
available.
You will need to have essentially a cloud partner to be able to progress this to the next
level.
And we actually don't more than that, we actually design DPCB to be fully on the cloud
from day one.
So every time a customer actually uploads a board, it is solved in real time on the cloud.
And so scalability and scalability of learning is essential if you want to have systems that
build up new state of the art per like results.
And we're pretty excited about that.
And we think this is actually achievable in the near term for instead, meaning something
like a year or less.
We talked a little bit about this kind of notion of transfer learning to use that term
very broadly, that you're training a model and agent and you can, when you're faced with
a new board, you can leverage the experience that you've, you know, you slash this agent
has experienced this models experience with previous boards.
We've also talked about, you know, in the real world, like different designs have different
requirements, maybe, you know, one board has specific requirements around noise or, you
know, the number of layers relative to others, you know, maybe, I guess what I'm, I'm wondering
is if, you know, as you evolve your reward function when you're faced with new scenarios,
does that interfere with transferability or, you know, can you transfer from one reward
function to another?
It really depends of like how far other reward functions are from respective to each other.
But in general, transfer learning is pretty robust.
So, you know, there is a remarkable ability to transfer knowledge from one problem to
another.
And this is something we see across the board, across the board in AI and it's the same
in printed circuit boards.
So if your reward function is not very dissimilar, you will be able to transfer knowledge.
If your board is relatively similar to previously seen and solved boards, transfer learning
will apply as well.
And this is actually a key point, Sam, because this is what makes those systems so interesting.
The fact that they learn, but then when faced with a very complex problem, you know, they
won't have to necessarily burn all that compute to redo everything again.
If you look at how optimization works in many cases today, there is no learning, no
memory of what happened.
And so, you know, if you have, for example, people, you know, doing optimization on boards
or on root problem, or no matter which routing problems these are, well, essentially, you're
burning compute overnight to solve something.
And maybe you kind of come in the next night and do the same thing all over again.
So the transfer learning part is actually critical.
And this is what allows you to keep progressing, you know, as you deploy compute, sort of
this compute is actually more wisely used, if you get two results that can improve what
you're going to do tomorrow.
You mentioned that you had a spotlight presentation at NURBS.
Was that related to this work or is that something separate?
So yeah, we were very surprised this year, actually, to have a spotlight paper at NURBS.
And this was work done with DeepMind, with Nando Freitas and his team.
And interestingly, we looked at, you know, deeper questions, but around compositionality.
How can I, in a bit in the same spirit, but how can I look at certain problems?
And while I saw simple tasks, use those as sort of like, you know, basic tasks to compose,
more complex tasks and so on.
So I think that we're very proud, despite our humble origins, to be able to partner with
the world's best in AI when it comes to research and innovation.
And one of the things, which is unusual about the company, is our ability to, on one side,
innovate in pure research and in particular, deeper, but then productize and experiment
with this innovation in the real world.
And I think that's kind of something pretty exciting, because you get exposed to things
that are both intellectually challenging, but also you have a chance to crack real world
problems.
And so this, you know, it was a big surprise for us, you know, first collaboration with
DeepMind, for us in itself was a milestone, to get that distinction, which roughly is
equivalent to being ranked in the top 2% of all worldwide papers submitted at NURBS.
So that was one of the milestones of our, of our year.
So can you share a little bit more detail about the, the paper and the results?
Yeah, so the paper is called Alphine PI, you can find it on our website, and study.com.
And what we did is we looked at, in this particular case, toy problems.
If you remember, we were speaking about starting with relatively simple things and building
up from there, we looked at, for example, the Hanoi Towers problem.
So Hanoi Towers is classically, you have those disks, you need to move them from one port
to another, and you need to respect a certain order in which you do this.
You know, this is very difficult to solve the Hanoi Towers problem, let's say, a classic
D-barrel album, like the DQN of PPO.
So Alphine PI was able to crack that problem using recursion and also using planning.
So if you look at, for example, the insights from Alphago and Alphazero planning is key.
So the ability to look ahead and sort of build the tree of possibilities and then decide
I'm going to act according to, you know, this path, because I really thought about it
or another way to, to say it is like system one and system two in Daniel Kahneman's classification.
Well, we've actually applied this.
But rather than have just a simple planning algo, like a tree, that is, like a Monte Carlo
tree search that is guiding the neural net, we've done multiple layers of trees.
So as we have compositionality across multiple levels, we actually are calling trees and
doing planning and search across multiple levels, which had never been done before.
So if you're interested, feel free to have a look at the website, the paper is there,
and also the code is available, we try to be very open even though we're a small startup.
So everything is out there available on GitHub and accessible through our website.
Well, Karim, thanks so much for taking some time to catch up.
It's been great chatting with you as always and super excited about what you're up to.
Thanks a lot Sam and talk to you soon.
All right, thank you.
All right everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
