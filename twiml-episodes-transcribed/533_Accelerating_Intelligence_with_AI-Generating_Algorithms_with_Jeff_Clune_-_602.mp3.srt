1
00:00:00,000 --> 00:00:04,880
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host,

2
00:00:04,880 --> 00:00:10,560
Sam Charrington, and today I am joined by Jeff Kloon. Jeff is an associate professor at the

3
00:00:10,560 --> 00:00:16,960
University of British Columbia and a faculty member at the Vector Institute. Before we get into

4
00:00:16,960 --> 00:00:21,280
today's conversation, please be sure to take a moment to head over to Apple Podcasts or your

5
00:00:21,280 --> 00:00:26,400
listening platform of choice. And if you enjoyed the show, please leave us a five-star rating and

6
00:00:26,400 --> 00:00:31,680
review. Jeff, welcome to the podcast. Thank you. It's a pleasure to be here. I'm looking forward

7
00:00:31,680 --> 00:00:39,200
to our conversation. We will be talking about some of your work relating to the path towards AGI.

8
00:00:39,200 --> 00:00:45,600
In particular, this idea of AI generating algorithms. But before we dive into that topic,

9
00:00:45,600 --> 00:00:50,080
I'd love to have you share a little bit about your background and what brought you to the field.

10
00:00:50,080 --> 00:00:55,520
Yeah. Well, thanks again for inviting me. So my background actually going way back is philosophy.

11
00:00:55,520 --> 00:00:58,720
So when I got through, you know, as an undergrad at University of Michigan,

12
00:00:58,720 --> 00:01:02,800
I was trying to figure out kind of how does thinking work, you know, and how do we think about

13
00:01:02,800 --> 00:01:07,360
thinking and how maybe could we be recreated in a machine. And I thought, you know, who's got

14
00:01:07,360 --> 00:01:11,760
the market cornered on thinking about thinking, and I thought that was philosophy. So it was a

15
00:01:11,760 --> 00:01:16,000
fascinating, you know, a few years. But ultimately, I started to get frustrated because I couldn't

16
00:01:16,000 --> 00:01:22,240
test my hypotheses and try to learn by building. And so over time, I came to realize that the best

17
00:01:22,240 --> 00:01:26,480
place for me would be in machine learning and trying to build intelligence to learn more about it.

18
00:01:27,040 --> 00:01:31,600
And another quest that I've been on my whole life, which I was also fascinated about then,

19
00:01:31,600 --> 00:01:36,320
still to this day, is kind of the explosion of complexity we see in the natural world.

20
00:01:36,320 --> 00:01:41,280
You know, how does such an unintelligent algorithm like Darwinian evolution produce jaguars at hawks

21
00:01:41,280 --> 00:01:46,560
and human brain and three-toed sloths? This amazing explosion of marvels in the natural world.

22
00:01:47,680 --> 00:01:52,000
And I am super excited to think about could we create computer algorithms that can

23
00:01:52,000 --> 00:02:00,080
replicate that creativity and endless innovation. So I went back to graduate school and started

24
00:02:00,080 --> 00:02:03,440
studying, you know, machine learning, got a PhD in computer science, actually got a master's

25
00:02:03,440 --> 00:02:09,200
degree in philosophy. That's a little bit of a different story. I went, that one did a postdoc

26
00:02:09,200 --> 00:02:14,160
at Cornell and became a professor at the University of Wyoming. And then my friends and I had a

27
00:02:14,160 --> 00:02:20,640
startup called geometric intelligence. Uber acquired that to create, you know, Uber artificial

28
00:02:20,640 --> 00:02:26,160
intelligence lab, Uber AI labs. And so they asked us to move out to San Francisco to stand that up.

29
00:02:26,160 --> 00:02:30,320
So we built up Uber AI labs and it was a wonderful place, a great intellectual

30
00:02:31,200 --> 00:02:34,880
environment. And then I went over to open the eye and let a research team there.

31
00:02:35,840 --> 00:02:39,440
And now I am a professor at the University of British Columbia.

32
00:02:39,440 --> 00:02:42,720
Awesome. Awesome. Now geometric intelligence, was that with Ken Stanley?

33
00:02:42,720 --> 00:02:48,000
It was. Ken Stanley is a long-standing friend and collaborator of mine. We still talk every week.

34
00:02:48,000 --> 00:02:52,960
And he is, you know, part and parcel all the ideas that I've been working on throughout my career.

35
00:02:52,960 --> 00:02:57,200
So yeah, he is the person who brought me, one of the people that brought me geometric and

36
00:02:57,200 --> 00:03:00,560
we get to work alongside each other standing up Uber AI labs.

37
00:03:00,560 --> 00:03:09,200
Awesome. Awesome. You mentioned that you went to philosophy to kind of learn about thinking.

38
00:03:09,200 --> 00:03:13,440
You know, what's your top list of, hey, if you want to think about thinking, these are the things

39
00:03:13,440 --> 00:03:20,080
you need to go read from philosophy. I think one of the most fascinating questions in all of science

40
00:03:20,960 --> 00:03:25,360
is this idea that shows up in philosophy and we're starting to wrestle with it and think about

41
00:03:25,360 --> 00:03:31,440
it and machine learning as well. And that is, when do you go from rocks to feeling? You know,

42
00:03:31,440 --> 00:03:37,360
originally this planet was rocks and weather. And now we have beings that feel pain and fall in love

43
00:03:37,360 --> 00:03:43,520
and love the taste of chocolate and they below the taste of something else. And we, you know,

44
00:03:44,080 --> 00:03:48,800
philosophers call this qualia. And it's kind of this extra thing that gets layered onto the world

45
00:03:48,800 --> 00:03:53,600
at some point. And we have very little understanding of how that might happen physically,

46
00:03:55,040 --> 00:03:59,920
despite, you know, talking about it for millennia. And a central question that we're starting to

47
00:03:59,920 --> 00:04:07,280
grapple with in machine learning is when might such feelings show up in AI systems? When might we

48
00:04:07,280 --> 00:04:12,720
start to have to recognize the pain and suffering of AI agents? And when might we have to start

49
00:04:12,720 --> 00:04:18,480
treating them as beings that have ethical worth that we can't say enslave or torture or make suffer

50
00:04:19,360 --> 00:04:24,640
for our own ends? And this is the kind of stuff that I used to be afraid to even say out loud,

51
00:04:24,640 --> 00:04:28,640
on a public podcast like this, because I thought I was career suicide to let people know that I was

52
00:04:28,640 --> 00:04:33,120
thinking about these things, even though I deeply think that they're true. But increasingly,

53
00:04:33,120 --> 00:04:37,040
you see more and more mainstream figures in the field worried about this. You know, I actually

54
00:04:37,040 --> 00:04:41,120
recently saw a tweet from Richard Sutton that brought this idea up just like last week.

55
00:04:42,080 --> 00:04:46,080
And there are many other people in the field that are becoming concerned. I think nobody really

56
00:04:46,080 --> 00:04:50,320
thinks we're there yet. But also, I think nobody knows when we're going to cross that threshold.

57
00:04:50,320 --> 00:04:54,080
And so we probably want to get ready so we don't accidentally cause tremendous amount of

58
00:04:54,080 --> 00:05:00,000
suffering before we realize it and then have to feel really bad about it. So that's one example

59
00:05:00,000 --> 00:05:05,280
of thinking by thinking. I think that, you know, I think I could probably make up some other

60
00:05:05,280 --> 00:05:10,480
examples. But for the most part, it's probably why I went over to machine learning is that I wasn't

61
00:05:10,480 --> 00:05:14,480
finding a lot of, I mean, there's a lot of fascinating topics for sure, but I wasn't finding a lot

62
00:05:14,480 --> 00:05:20,000
of the answers to how do we really build the thinking machine and how does thinking really happen

63
00:05:20,000 --> 00:05:24,800
in our head that I couldn't, that I wasn't seeing better address in the sciences than in philosophy.

64
00:05:24,800 --> 00:05:31,680
Coming back to machine learning, you've talked about this idea of AI generating algorithms as,

65
00:05:31,680 --> 00:05:40,240
you know, one of the possible paths to getting us to to generalize artificial intelligence.

66
00:05:40,240 --> 00:05:46,000
Why don't you talk about that and introduce that idea in the context of the other paths and how

67
00:05:46,000 --> 00:05:50,800
you think about the role each of them will play? Sure. So I think that if you look out into

68
00:05:50,800 --> 00:05:57,360
machine learning, it's pretty clear that there is kind of a dominant paradigm. And this was

69
00:05:57,360 --> 00:06:00,800
certainly true a couple of years back. It's increasing a little less tree, but for the most part,

70
00:06:00,800 --> 00:06:07,440
look at any Neurops ICML-ICLEAR conference and what are most of the papers doing? They're saying,

71
00:06:07,440 --> 00:06:13,040
hey, I think that there's a building block that is important, you know, and I'm going to try to

72
00:06:13,040 --> 00:06:18,320
either introduce a new building block that we may not have had in our system before or create an

73
00:06:18,320 --> 00:06:23,360
improved version of an existing building block. You know, different types of neural activation

74
00:06:23,360 --> 00:06:28,960
functions are ways to normalize weights or layers, you know, better optimizers, maybe better

75
00:06:28,960 --> 00:06:33,120
recurrent cells of memory and writable memory and, you know, all these different things.

76
00:06:33,120 --> 00:06:37,680
And the thing that I find interesting about that is what we don't usually do is take a step back

77
00:06:37,680 --> 00:06:41,360
and think about the really grand emissions of the field and think about how are we going to get

78
00:06:41,360 --> 00:06:47,360
all the way to our grandest ambitions, you know, which is maybe making AGI. There's kind of this

79
00:06:47,360 --> 00:06:52,320
assumption of one that in this first phase, we'll be able to manually identify all of these building

80
00:06:52,320 --> 00:06:57,120
blocks and the right version of them. And then two, at some point, there's got to be some phase two

81
00:06:57,120 --> 00:07:02,480
where we put all these pieces together. And that is just a herculean challenge if you really

82
00:07:02,480 --> 00:07:06,320
think about it. And I think we should be clearied about how hard that might be to take all of the

83
00:07:06,320 --> 00:07:10,000
different things that all of the community is building and stick them all together and tune all

84
00:07:10,000 --> 00:07:14,800
their hyper parameters and make them interoperate perfectly. I mean, just think about debugging that thing,

85
00:07:14,800 --> 00:07:19,680
let alone building it in the first place. It would require an Apollo scale or Manhattan project

86
00:07:19,680 --> 00:07:26,320
scale effort, in my opinion, to pull that off. However, I think that if you look in the history of

87
00:07:26,320 --> 00:07:32,320
the last 10, 20 years of machine learning, there is a undeniable trend. And that is that hand

88
00:07:32,320 --> 00:07:38,960
design pipelines give way to learn pipelines as you have more compute and data. So let's take some

89
00:07:38,960 --> 00:07:46,080
examples. The classic and first one, the shot herred on the world, with features inside of pipelines,

90
00:07:46,080 --> 00:07:51,840
like vision, language, speech detects, et cetera. You know, we used to hand design all of our

91
00:07:51,840 --> 00:07:55,840
features and then learn a little bit on top of those features. People like Jan LeCune and Yasha

92
00:07:55,840 --> 00:07:59,840
Benjio and Jeff Hinton were saying, let's just learn the whole thing. They were right and everybody

93
00:07:59,840 --> 00:08:04,240
knows that now. But that same exact trend has applied over and over again on other topics. So

94
00:08:04,240 --> 00:08:08,400
look at architectures. We used to design them by hand, we still mostly do. But increasingly,

95
00:08:08,400 --> 00:08:13,520
the best architectures are learned or at least they're competitive. RL algorithms themselves,

96
00:08:13,520 --> 00:08:19,120
like learning to reinforcement learn, hyper parameter tuning, even data augmentation pipelines,

97
00:08:19,120 --> 00:08:22,640
et cetera, et cetera, et cetera. The writing is on the wall. Once we get a lot of compute and

98
00:08:22,640 --> 00:08:27,280
data, we let machine learning do the heavy lifting, we should just learn the whole thing. So what I

99
00:08:27,280 --> 00:08:32,160
argue is that we should apply that thinking to the process of our grandest ambitions of creating

100
00:08:32,160 --> 00:08:36,400
AGI itself and say, instead of trying to manually design all of the pieces of some

101
00:08:36,400 --> 00:08:41,840
Rube Goldbergian thinking machine, let's just try to get out of the way, set up the system such

102
00:08:41,840 --> 00:08:46,400
that compute and data and machine learning can do the heavy lifting for us. And let's try to

103
00:08:46,400 --> 00:08:50,800
learn the whole thing. And so if you want to make progress on that, that would be this thing

104
00:08:50,800 --> 00:08:54,720
that I call an AI generating algorithm. It's basically AI that makes better versions of itself.

105
00:08:54,720 --> 00:08:59,840
It starts simple and it bootstraps itself up from simple origins all the way potentially to

106
00:08:59,840 --> 00:09:04,320
AGI. And if we want to make progress, I think we have to push on three pillars. One, we have to

107
00:09:04,320 --> 00:09:08,720
meta-learn the architectures. Two, we have to meta-learn the learning algorithms themselves.

108
00:09:08,720 --> 00:09:13,680
And three, we have to automatically generate the learning environments so that the system can

109
00:09:13,680 --> 00:09:18,080
learn forever and it's not trapped within one specific domain. And so that's the idea of an AI

110
00:09:18,080 --> 00:09:23,920
generating algorithm. And I think it might be the fastest path to produce AGI. And even if it's

111
00:09:23,920 --> 00:09:28,960
not, it's still scientifically interesting because it teaches us how can a system like Darwinian

112
00:09:28,960 --> 00:09:33,920
evolution have produced us. It strikes me that the way AI generating algorithm, the way you kind of

113
00:09:33,920 --> 00:09:42,800
pose that problem, it is maybe there's a pushback a little bit. It's kind of an accelerator to building

114
00:09:42,800 --> 00:09:52,320
the building blocks, but to kind of switch rails from kind of this broader, more holistic approach,

115
00:09:52,320 --> 00:09:59,760
we need to, we kind of have to figure out what's the objective function on general intelligence.

116
00:09:59,760 --> 00:10:05,200
I'm like, we have no idea how to do that. Completely agree. I'm not saying it's going to be easy.

117
00:10:06,320 --> 00:10:13,440
But and I think that you hit upon one of the central grand challenges in an effort like this.

118
00:10:13,440 --> 00:10:17,840
Like, if we want to make such a system work, we have to figure out, yeah, what is the reward

119
00:10:17,840 --> 00:10:22,720
function or the loss function for the overall system? What is it trying to optimize? And how,

120
00:10:23,680 --> 00:10:29,600
how can we create what's called an open-ended algorithm that will keep going forever

121
00:10:30,320 --> 00:10:35,280
and not stagnate? But I don't think that we lack ideas on this front. I think this is a

122
00:10:35,280 --> 00:10:38,800
place where we can do a lot of research. And I think there's probably a touring award out there

123
00:10:38,800 --> 00:10:44,240
for somebody who can figure out the right answer to that question. So I think it's a fascinating

124
00:10:44,240 --> 00:10:48,800
kind of grand challenge of science to figure out what kind of a loss function could you put in

125
00:10:48,800 --> 00:10:53,360
such a system such that it would bootstrap itself all the way up from, you know, almost being

126
00:10:53,360 --> 00:10:58,480
completely unintelligent all the way past human intelligence. Yeah, and, you know, I kind of pose

127
00:10:58,480 --> 00:11:05,200
that as a way to kind of fine tune my understanding of what you're trying to accomplish with your

128
00:11:05,200 --> 00:11:12,160
work around AI-generating algorithms. Do you see that work as being applied to kind of evolving,

129
00:11:12,160 --> 00:11:18,080
you know, that reward function in such a way that, you know, you get us beyond building the

130
00:11:18,080 --> 00:11:24,160
individual components or, you know, are you focused on kind of evolving those individual components

131
00:11:24,160 --> 00:11:29,120
more quickly without really understanding how they get us to, you know, this broader way of thinking

132
00:11:29,120 --> 00:11:38,240
about, you know, getting to AGI. Yeah, so that very interesting question inspires me to make

133
00:11:38,240 --> 00:11:43,760
three different comments because there's three things that are related there. So one thing is that

134
00:11:43,760 --> 00:11:47,680
I think that one of the hardest things is trying to figure out how do you take all these different

135
00:11:47,680 --> 00:11:52,640
pieces that like the community might be building and put them all together in the way that really

136
00:11:52,640 --> 00:11:57,760
works with the kind of elegance and beauty and effectiveness of, say, the human mind.

137
00:11:59,360 --> 00:12:04,000
And so I think rather than trying to separately create components even with a system that was

138
00:12:04,000 --> 00:12:09,520
learning the components and then later like figure out how to put them together, the algorithm should

139
00:12:09,520 --> 00:12:15,120
do all of that work for us, you know, it should say, you know, you've got system one type thinking

140
00:12:15,120 --> 00:12:19,280
fast and slow, you know, system one, system two, fast and so you've got hierarchical RL with

141
00:12:19,280 --> 00:12:23,520
different levels of abstraction, you know, that you have long term planning and short term planning

142
00:12:23,520 --> 00:12:28,160
and continual learning and all these things have to work well together and we should just basically let

143
00:12:28,160 --> 00:12:34,800
the an AIGA figure out how to create those pieces and make them work well together. So this path

144
00:12:34,800 --> 00:12:40,080
would not be trying to separately create these components onto the reward function issue. There's

145
00:12:40,080 --> 00:12:44,480
actually two fascinating questions there. One is kind of what would the reward function be for an

146
00:12:44,480 --> 00:12:52,240
individual, individual agent in the system? And one thing that I think is fascinating is the system

147
00:12:52,240 --> 00:12:59,680
itself would probably end up creating at least all of the intrinsic rewards for the agent. So evolution,

148
00:12:59,680 --> 00:13:04,080
there's debate about exactly what it's what it's reward function is, but let's assume itself

149
00:13:04,080 --> 00:13:09,520
replication, for example, with that simple kind of higher level reward function, look what it did

150
00:13:09,520 --> 00:13:15,360
in your brain, you know, if you're like me, you love ice cream and you love guy, you know, seeing

151
00:13:15,360 --> 00:13:21,600
beautiful views and you love running up mountains and you might be interested in romantic partners

152
00:13:21,600 --> 00:13:26,480
and you probably really don't like putting your hand on a hot stove. You're probably also curious

153
00:13:26,480 --> 00:13:31,680
and you like to play and learn and you probably care about what your peers think of you. You've got

154
00:13:31,680 --> 00:13:37,360
all of these different rewards kind of baked into that motivates you to do the things that you do

155
00:13:37,360 --> 00:13:42,800
every day, every week, every year, right? Where did those come from? Well, basically evolution figured

156
00:13:42,800 --> 00:13:50,320
out that if it makes beings that have those kind of internal rewards, they are more effective at

157
00:13:50,320 --> 00:13:55,440
this other thing which is self replication. And so I also don't think we should be trying to hand

158
00:13:55,440 --> 00:13:59,760
specify like we do often in machine learning, oh, it should be curious, it should try to go to new

159
00:13:59,760 --> 00:14:05,760
states, it should probably try to maximize its reward, it should probably try to interact with other

160
00:14:05,760 --> 00:14:09,280
people and maybe it should try to communicate with other people, we'd like manually design all

161
00:14:09,280 --> 00:14:14,320
these like hack down rewards that we inject in the system in the hopes to get to do what we want.

162
00:14:14,320 --> 00:14:19,440
I think instead we probably need to be looking for that outer loop meta reward, it's really kind of

163
00:14:19,440 --> 00:14:25,120
general and probably simple. And with enough compute, the internal stuff all kind of shows up in

164
00:14:25,120 --> 00:14:29,600
the system and it kind of knows, it kind of creates a ton of intrinsic rewards like paying in

165
00:14:29,600 --> 00:14:34,320
pleasure and curiosity, et cetera. So then the question is what is that outer loop reward function,

166
00:14:34,320 --> 00:14:39,440
which is this is the third thing I wanted to talk about. We don't know exactly what that is,

167
00:14:39,440 --> 00:14:43,280
but I will just give you a sketch and I'm sure this is wrong, it's not the right answer,

168
00:14:43,280 --> 00:14:47,040
but it feels like it's pointing in the right direction and suggests the research we could do.

169
00:14:47,040 --> 00:14:54,960
What if you had an agent that was motivated to learn somehow, it's like wants to continually

170
00:14:54,960 --> 00:15:01,760
learn new things and the things that it wants to learn, we find interesting or useful and by

171
00:15:01,760 --> 00:15:07,040
we, I mean humans. And so maybe somehow some way it's grounded to our world, maybe it has to

172
00:15:07,040 --> 00:15:12,320
get better and better at solving real world problems or making money on earth or just making humans

173
00:15:12,320 --> 00:15:18,720
think that what it's doing is worthwhile or imitating YouTube and things that we do.

174
00:15:19,360 --> 00:15:25,680
That basically allows broadly writ a system that can learn forever and not learn to memorize

175
00:15:25,680 --> 00:15:30,640
white noise patterns, but instead learn things that we find useful. Now that's overly simplistic,

176
00:15:30,640 --> 00:15:34,160
it won't work in practice, but that is I think where a lot of the research should be focused,

177
00:15:34,160 --> 00:15:38,960
which is can we figure out those kind of general reward functions plug it into an AIGA

178
00:15:38,960 --> 00:15:42,880
and good things continue to happen forever. And I think that's a really interesting thing to

179
00:15:42,880 --> 00:15:48,160
wait a frame it. If you look out into the natural world, we have seen two at least open-ended

180
00:15:48,160 --> 00:15:53,280
processes that innovate forever. And one of them is a natural evolution that's been going

181
00:15:53,280 --> 00:15:59,280
about 3.5 billion years, continues to surprise us with things like COVID. And human culture is the

182
00:15:59,280 --> 00:16:06,240
other one, right? We continuously, we solve problems and in the process we create new opportunities,

183
00:16:06,240 --> 00:16:09,600
new problems. You create one technology and suddenly it has a cascading effect that will

184
00:16:09,600 --> 00:16:13,840
open up new opportunities and doors. We solve those problems and it generates et cetera, et cetera.

185
00:16:13,840 --> 00:16:16,960
And so the question that I think is fascinating is one of my colleagues put it is,

186
00:16:17,520 --> 00:16:21,680
could we create the computer algorithm that was worth running for a billion years?

187
00:16:22,240 --> 00:16:28,720
That continues to innovate and delight, surprise, and be creative for a billion years. Right now,

188
00:16:28,720 --> 00:16:32,880
when I started my career, the best algorithms weren't worth running for more than a few hours.

189
00:16:32,880 --> 00:16:37,360
I'd say we're now at a point where we have some algorithms that are worth running for about

190
00:16:37,360 --> 00:16:43,600
a month to three, and not much beyond that. But could we create something that would truly innovate

191
00:16:43,600 --> 00:16:47,840
forever? And if we can, then we've made a lot of progress, I think, towards some really

192
00:16:47,840 --> 00:16:53,280
fascinating scientific questions. So you've referenced a couple of times, you've got

193
00:16:53,280 --> 00:17:00,720
a reward function, AIX or scenarioX, you stick it into AIGA and AI Generating Algorithm.

194
00:17:02,640 --> 00:17:08,160
Let's talk about the state of AI Generating Algorithms. Let's make that a little bit more

195
00:17:08,160 --> 00:17:16,400
concrete. What AI Generating Algorithms are out there, how far along are we? How do you think

196
00:17:16,400 --> 00:17:22,800
about the state of that line of work? Yeah, great question. So as I mentioned, there are three main

197
00:17:22,800 --> 00:17:28,960
pillars to an AIGA. You have to learn the architectures, you have to metal learn the learning algorithms

198
00:17:28,960 --> 00:17:33,280
themselves, you have to automatically generate the environments. So we could separately say,

199
00:17:33,280 --> 00:17:36,640
what kind of major work has been done in each of those pillars? And then we could talk about what's

200
00:17:36,640 --> 00:17:40,800
even more fun as like has anything been trying to put those pieces together? Yeah, right.

201
00:17:43,360 --> 00:17:48,720
So first in the automatically learning the architectures has been an explosion of work in

202
00:17:48,720 --> 00:17:52,960
neural architecture search. It's kind of a thriving field and it's doing really well. I don't think

203
00:17:52,960 --> 00:17:55,600
I need to mention the work that's done there. Your readers are probably familiar with it,

204
00:17:55,600 --> 00:18:00,400
but suffice to say that many of the best architectures now are being learned, not hand design.

205
00:18:01,040 --> 00:18:07,120
And I expect that soon will be most. In the second pillar, there is a lot of work that's

206
00:18:07,120 --> 00:18:10,800
been happening over the last couple of years in this area called, often it just goes by metal

207
00:18:10,800 --> 00:18:15,440
learning, but it's basically learning to learn. I was kicked off by these two amazing papers,

208
00:18:15,440 --> 00:18:19,920
RL squared, and learning to reinforcement learning by Jane Wang. You've got the work by Chelsea

209
00:18:19,920 --> 00:18:23,680
Finn and Sergey Levin on mammal. And you have some of my favorite work is the work I'd

210
00:18:23,680 --> 00:18:28,720
open the eye, which happened before I was there on Rubik's Cube, but they basically just take

211
00:18:28,720 --> 00:18:33,360
a big neural net. They asked it to solve Rubik's Cube in a variety of different situations with

212
00:18:33,360 --> 00:18:38,560
different like friction and weight of the cube and the size of the hand, et cetera. And what they

213
00:18:38,560 --> 00:18:43,760
showed, which is just mind blowing, is that it inside of a giant recurrent neural network

214
00:18:43,760 --> 00:18:48,640
figures out a learning algorithm such that when you want to use it at the end of the day,

215
00:18:48,640 --> 00:18:52,560
you've turned off your own SGD. You're not doing a hand design in the algorithm anymore.

216
00:18:52,560 --> 00:18:57,040
You're completely handing it off to the recurrent neural net, which is invented its own way

217
00:18:57,040 --> 00:19:01,840
to conduct experiments in the world, figure out kind of what type of world I'm in,

218
00:19:02,400 --> 00:19:06,560
take the information it has learned from the world, and use it to then get very efficient

219
00:19:06,560 --> 00:19:11,440
in solving the task. And we don't know how it did that. And that's an example of where it figured

220
00:19:11,440 --> 00:19:15,520
out how to like do the thing we wanted to do without us having to hand design it and probably

221
00:19:15,520 --> 00:19:21,920
a very complicated way that we wouldn't have been able to hand engineer. And so that is fascinating

222
00:19:21,920 --> 00:19:27,360
work. And then the final pillar is automatically generating environments. So one of the first

223
00:19:27,360 --> 00:19:32,320
big works in this area was Poet. This worked that I did with Ken Stanley and Joel Layman

224
00:19:32,320 --> 00:19:39,040
and many others at Uber. AI Labs and Ray Rang was the lead on this. And the here the idea is

225
00:19:39,040 --> 00:19:42,880
typically a machine learning. We pick the problem and then we try to like solve it for a

226
00:19:42,880 --> 00:19:46,800
long time. Then we move on to the next problem. So we work on chess for a while, then we go to

227
00:19:46,800 --> 00:19:52,240
go and then we go to starcraft and then we go to dodo. The problem there is that no matter how much

228
00:19:52,240 --> 00:19:57,280
time you run that algorithm on go or starcraft, all you're going to get is a go playing agent

229
00:19:57,280 --> 00:20:01,120
for a starcrafted playing agent. That's it. It's not going to do anything else more interesting

230
00:20:01,120 --> 00:20:05,520
than that. And so the idea behind code is we don't want that. We want the system to produce

231
00:20:05,520 --> 00:20:10,240
the learning challenges forever. And so the way the Poet works is it basically starts out

232
00:20:10,240 --> 00:20:13,520
with an environment. And then once the agent is pretty good at solving that environment,

233
00:20:13,520 --> 00:20:18,400
it creates an entirely new environment that it thinks is close enough that the skills learned

234
00:20:18,400 --> 00:20:22,480
in the first environment will help in the second environment. And it basically keeps going forever

235
00:20:22,480 --> 00:20:26,800
and kind of adding more and more learning challenges to the agent. And as the agent masters them,

236
00:20:26,800 --> 00:20:31,120
it basically has it switched to harder and harder stuff. But not in a linear chain. It's not

237
00:20:31,120 --> 00:20:36,480
headed in any direction, trying to solve one particular final target challenge. It's more like

238
00:20:36,480 --> 00:20:40,320
what you see on earth or in human culture. It's just fanning out and getting better and better

239
00:20:40,320 --> 00:20:44,080
and better and more not more knowledgeable at a variety of different things.

240
00:20:44,080 --> 00:20:47,840
Do you think of that as kind of deriving from some of the work that's been done around

241
00:20:47,840 --> 00:20:53,520
curriculum learning where you're kind of staging out a set of learning objectives to

242
00:20:53,520 --> 00:21:02,160
accelerate an agent's ability to zero in on the specific things and then build on those things?

243
00:21:02,160 --> 00:21:06,240
I do. Yeah, of course, there has been, obviously, decades of work in curriculum learning.

244
00:21:06,240 --> 00:21:10,960
I would say that in my opinion, the history of most curriculum learning work however is,

245
00:21:10,960 --> 00:21:15,280
I have this thing that I want to solve. What is the right curriculum that will get me there?

246
00:21:15,280 --> 00:21:20,640
Yeah, there's a pre-determined target. Whereas Poet is saying, I want to learn everything.

247
00:21:20,640 --> 00:21:29,200
How do I learn forever? There's a wonderful body of work and principles and kind of almost

248
00:21:29,200 --> 00:21:34,720
like a philosophical paradigm that dates back to Ken Stanley and Joel Lehman. I'm this idea that

249
00:21:34,720 --> 00:21:38,560
oftentimes when we pick an objective ahead of time and we try to get there, if it's a simple

250
00:21:38,560 --> 00:21:42,960
objective, we can do it. If it's a really hard objective, we'll probably fail. Usually,

251
00:21:42,960 --> 00:21:48,320
the best way to solve a really, really hard challenge is not to try to get anywhere in particular,

252
00:21:48,320 --> 00:21:52,800
but to try to just learn and go everywhere. Go in any direction, follow any stepping stones,

253
00:21:52,800 --> 00:21:58,880
kind of have serendipity inside of your algorithm. Then, eventually, you will learn the skills and

254
00:21:58,880 --> 00:22:03,920
the knowledge to solve this task that maybe you originally did want to solve, but you shouldn't

255
00:22:03,920 --> 00:22:08,400
try to solve it. There are a lot of examples from the history of technology on this. For example,

256
00:22:08,400 --> 00:22:14,800
if you went back to Melania and you said, I have cooking over a fire and you said, all right,

257
00:22:14,800 --> 00:22:19,120
I'm the king of the universe and I'm the king of earth and I'm only going to found scientists

258
00:22:19,120 --> 00:22:23,120
in my kingdom that will give me better and better cooking technology that will cook things

259
00:22:23,120 --> 00:22:29,040
faster and with no smoke. Well, you will never invent the microwave because to invent the microwave,

260
00:22:29,040 --> 00:22:33,200
you had to have been working on radar technology and notice that a chocolate bar melted in your pocket.

261
00:22:34,800 --> 00:22:38,240
Similarly, if you wanted to invent, you know, the modern computer, go back to the abacus.

262
00:22:38,240 --> 00:22:41,920
Really good device. It's pretty good at computing at the time and you're like, I will fund anybody

263
00:22:41,920 --> 00:22:45,920
in my kingdom and only people who will give me more computing and all of your grant proposals

264
00:22:45,920 --> 00:22:52,000
would be like longer rods, more beads, you know, maybe a 3D abacus, but you would never invent

265
00:22:52,000 --> 00:22:56,880
the modern computer because to do that, you had to have been working on electricity and vacuum

266
00:22:56,880 --> 00:23:02,560
tubes and those are technologies that were not invented because they help with computation.

267
00:23:03,920 --> 00:23:09,040
And so, we're trying to capture that kind of serendipity inside of these AI-generating algorithms

268
00:23:09,040 --> 00:23:13,200
and open-ended algorithms. This is building on a subfield that's now thriving called

269
00:23:13,200 --> 00:23:18,640
quality-diversity algorithms of which poet is one. And the idea is that you basically want

270
00:23:18,640 --> 00:23:26,800
as many high-quality, yet diverse things in a growing archive or library of skill sets or

271
00:23:26,800 --> 00:23:30,720
innovations, whatever it is you're trying to do. And so, poet is like that. It basically says,

272
00:23:30,720 --> 00:23:33,840
I want to create creating environments, totally different environments, and agents that know how to

273
00:23:33,840 --> 00:23:38,000
solve those environments. Overall, the system is getting smarter, the skill level is going up,

274
00:23:38,000 --> 00:23:41,520
the amount of things we know how to do is going up, and that will continuously unlock

275
00:23:41,520 --> 00:23:45,120
new stepping stones and new things that we can do. And eventually, you bubble out,

276
00:23:45,120 --> 00:23:49,440
and in theory, if you did that in the right way, you might get all the way to human-level intelligence.

277
00:23:51,200 --> 00:23:54,400
Now, if you don't mind, I want to go back to finishing my thought on the question of putting

278
00:23:54,400 --> 00:23:59,040
pillars together. I think probably the most exciting work I know of that is put two of the pillars

279
00:23:59,040 --> 00:24:06,960
together, not three, is X-land by Max Yeterbury out of DeepMine. They have this wonderful paper

280
00:24:06,960 --> 00:24:11,440
where they build on the ideas of open-endedness and AI-generating algorithms. And even Max said

281
00:24:11,440 --> 00:24:16,080
on Twitter in exchange, I had that this is an AI-generating algorithm, or I think that's in the paper as

282
00:24:16,080 --> 00:24:20,240
well. And I think it's one of the best first examples of somebody going really big toward this

283
00:24:20,240 --> 00:24:25,600
idea. And what they do there is they basically describe a whole, they say, sample from this huge

284
00:24:25,600 --> 00:24:31,680
space of possible tasks that agents have to play against each other. And so, they sample a task,

285
00:24:31,680 --> 00:24:35,920
they train an agent to solve the task, as the agent levels up, they give it new tasks and some kind

286
00:24:35,920 --> 00:24:40,640
of fanning out curriculum, and they basically try to make it as good as possible as in as many as

287
00:24:40,640 --> 00:24:45,920
possible. And in the end, the system itself trains agents that go from initially not knowing how to

288
00:24:45,920 --> 00:24:53,120
do anything to ultimately being able to zero-shot solve tasks like hide and seek and capture the flag,

289
00:24:53,120 --> 00:24:59,120
and all other kind of games that you might think would be good for agents to just know how to

290
00:24:59,120 --> 00:25:03,760
to solve if they were generally intelligent. So, I think that was great work in that direction.

291
00:25:03,760 --> 00:25:12,240
And in that particular case, how are they demonstrating that generalizability? Are they doing

292
00:25:13,040 --> 00:25:16,160
something analogous to pull it where they're creating new environments, or are they

293
00:25:17,280 --> 00:25:23,280
more hand-picking new environments, and showing that this agent that they've created is generalizable?

294
00:25:23,280 --> 00:25:28,320
Yeah, good question. So, what they did is, like Poet, they basically said, we're going to be

295
00:25:28,320 --> 00:25:32,960
able to describe an environment in a parameter vector. So, normally, we're used to think

296
00:25:32,960 --> 00:25:36,640
of a parameter vector describing the weights of a neural net. Now, the environment is being

297
00:25:36,640 --> 00:25:43,280
described by some descriptor, and they have a search space. So, basically, the huge space of

298
00:25:43,280 --> 00:25:48,720
possible parameter vectors is specify all these environments, and they can sample an environment

299
00:25:48,720 --> 00:25:53,120
and train an agent on it. One thing that they did, which I really like, is they also show that

300
00:25:53,120 --> 00:25:58,960
vector to the agent. So, the agent knows what task it's trying to solve, which is a really good

301
00:25:58,960 --> 00:26:04,640
idea, because it can then learn to generalize and even zero-shot new tasks. So, for example,

302
00:26:04,640 --> 00:26:09,200
if it has learned over time the language of how the worlds are being specified. So, in there,

303
00:26:09,200 --> 00:26:14,400
what it's like, your job is to get the blue ball and take it to the green area,

304
00:26:16,080 --> 00:26:20,240
and not let your enemy get the green ball and take it to the red area. Well, that basically

305
00:26:20,240 --> 00:26:24,640
describes capture the flag. I have to go get the thing, and I have to take it to my area,

306
00:26:24,640 --> 00:26:29,360
and you know, prevent you from doing likewise. And so, they train on enough of these tasks, where

307
00:26:29,360 --> 00:26:33,520
maybe some of the simpler tasks are get the green ball, and then some of the harder tasks are get

308
00:26:33,520 --> 00:26:39,440
the green ball and move it to, you know, move it anywhere. And eventually, it's like this more

309
00:26:39,440 --> 00:26:45,200
complicated thing. And so, in the end, they can basically sample a new task from a hell.task set,

310
00:26:45,200 --> 00:26:51,040
like hide and seek or capture the flag. Give it to the agent. It has learned how to read the

311
00:26:51,040 --> 00:26:56,640
description of the world, and immediately go and solve that relatively complicated game that's

312
00:26:56,640 --> 00:27:02,560
never seen before. What you're describing here, and to some extent, what you're describing with

313
00:27:02,560 --> 00:27:08,240
Poet reminds me of the work that's been done around NetHack, which is kind of specifying,

314
00:27:08,960 --> 00:27:13,040
you know, trying to, and it's been a while since I had that conversation, but I'm thinking about

315
00:27:13,040 --> 00:27:18,960
like, you know, creating this one vector that kind of specifies an environment. Are you familiar

316
00:27:18,960 --> 00:27:24,320
with that work? And if so, how do you think about them relative to, how do you think about NetHack

317
00:27:24,320 --> 00:27:30,800
relative to X-Land and Poet? Yeah, so I did hear about the competition, and I remember that

318
00:27:30,800 --> 00:27:35,280
it's like a symbolic hybrid B.D. learning, which caused quite a splash. That's about the extent of

319
00:27:35,280 --> 00:27:39,840
what I know about it. I will, I'll be able to just say just a few things that one is that I think,

320
00:27:39,840 --> 00:27:46,000
in general, having agents be told the task that trying to solve is very, very helpful. It

321
00:27:46,000 --> 00:27:50,640
shaves off probably orders of magnitude in terms of exploration difficulty. The example I love to

322
00:27:50,640 --> 00:27:56,640
give is like, my dog is very capable, athletic, right? And if I wanted my dog to go around my

323
00:27:56,640 --> 00:28:01,520
apartment and like, pick up all the green things and put them in a bin, my dog could do it if they

324
00:28:01,520 --> 00:28:05,120
knew that was the task that would give them the treat, but I can't just tell that to my dog,

325
00:28:05,120 --> 00:28:09,040
because we don't have the ability to communicate that kind of high level task descriptor. So,

326
00:28:09,040 --> 00:28:12,880
in our old agent, it's in the same boat. If you just put it in a room and say, go, it's never

327
00:28:12,880 --> 00:28:16,320
going to figure out to pick up all the green things and put them in a bin. But if you could tell

328
00:28:16,320 --> 00:28:21,120
the agent that and knew what you meant, and it had some basic athletic skills, that's probably

329
00:28:21,120 --> 00:28:25,760
no longer that hard of a task. So, I like that thing. And also, I think, you know, just the fact

330
00:28:25,760 --> 00:28:30,240
that NutHack is being done in text is a great accelerant, because, you know, as we've seen with

331
00:28:30,240 --> 00:28:34,080
so many things in the last few years, operating in text is cheap and quite powerful because you can

332
00:28:34,080 --> 00:28:42,560
bring, you know, things like GPT to bear on. In the case of X on this vector,

333
00:28:42,560 --> 00:28:49,760
is it specifying the task in specifying the environment or? I think the right, the easiest way

334
00:28:49,760 --> 00:28:54,160
to think about it is that they do create a world, and then they create the description of what you're

335
00:28:54,160 --> 00:28:58,960
supposed to do in that world. And so, the agent is basically told, oh, in this world, you're supposed

336
00:28:58,960 --> 00:29:05,840
to get the green things and take them over to this bin. And it learns over time that, like, basically,

337
00:29:05,840 --> 00:29:12,320
what those words mean, and that learns the skills to do that. Now, one thing you might be

338
00:29:12,320 --> 00:29:17,440
thinking, and I think a lot of people ask me about in, when we talk about the agias, is, you know,

339
00:29:17,440 --> 00:29:22,560
this is what Josh said in one one asked, once asked me, it's just like, this sounds super great,

340
00:29:22,560 --> 00:29:26,240
but how are you going to do this without a planet-sized computer? Because that's what Earth had.

341
00:29:28,000 --> 00:29:35,760
Fair point, Josh. I think that what we have to look at is, where are we going to get abstractions

342
00:29:35,760 --> 00:29:40,960
or innovations that shave many orders of magnitude off of what was required to make Earth work?

343
00:29:40,960 --> 00:29:44,960
Darwinian evolution is very, very unintelligent, and I'm not advocating we even necessarily need

344
00:29:44,960 --> 00:29:49,600
evolutionary algorithms, just to be clear. I'm inspired by evolution, but not advocating that

345
00:29:49,600 --> 00:29:54,320
that technology into the hood. But I do think there's probably many places where we could look

346
00:29:54,320 --> 00:29:59,280
to things that could, you know, save orders of magnitude. So, if you have the right abstraction,

347
00:29:59,280 --> 00:30:03,040
if you have the right domain, where you don't have to simulate chemistry and physical,

348
00:30:03,040 --> 00:30:06,720
like low-level physics, you can simulate things higher level, or even be in an environment,

349
00:30:06,720 --> 00:30:11,360
maybe like net hack, a text only environment. Those things can really help. And another thing I

350
00:30:11,360 --> 00:30:19,040
think that it can help is to, you know, steal a quote from Newton and riff on it, that I think AI

351
00:30:19,040 --> 00:30:24,240
will go farther if it stands on the shoulders of giant human data sets. And so we've seen that

352
00:30:24,240 --> 00:30:30,080
with GPT and all the unsupervised pre-training right now is that we can basically take a huge step

353
00:30:30,080 --> 00:30:34,240
forward just by, you know, starting from where humans are at. Now that we know how to learn from

354
00:30:34,240 --> 00:30:38,880
human data. And so my team at OpenAI recently had a paper that was accepted at NURPS this year

355
00:30:38,880 --> 00:30:44,400
called Video Pre-Training, which is a nod to GPT. And the idea there is that we have a very

356
00:30:44,400 --> 00:30:49,440
simple way that you can go on to the internet and have AI learn by watching YouTube,

357
00:30:50,400 --> 00:30:55,440
and learn how to act by watching YouTube. So mostly in games like using your computer or,

358
00:30:56,240 --> 00:31:00,720
you know, maybe like playing Minecraft or whatever, there's all sorts of video tutorials online

359
00:31:00,720 --> 00:31:05,040
trying to do that. The trick was that we didn't know how to get the labels. So if you're trying to

360
00:31:05,040 --> 00:31:11,040
generate text or generate pixels or images or generate music, all the labels are out there, right?

361
00:31:11,040 --> 00:31:14,240
From the previous text, you just have to predict the next word. The next word is right there on

362
00:31:14,240 --> 00:31:19,200
whatever web page you're scraping. The problem in videos of people acting, whether they're humans

363
00:31:19,200 --> 00:31:24,080
or robots or video game characters, is you see the agent doing its thing, but you don't know what

364
00:31:24,080 --> 00:31:28,800
buttons it's pressing, like how it's moving its mouse or what keyboard, what keys it's pressing

365
00:31:28,800 --> 00:31:34,560
as keyboard. So the simple idea behind VPT is that we just train a simple model that basically learns

366
00:31:34,560 --> 00:31:39,920
to predict, oh, in Minecraft, if all of a sudden the character like, you know, jumped, you must have

367
00:31:39,920 --> 00:31:44,560
hit the jump button. Or if it placed a block, you hit the place block button. That's a pretty simple

368
00:31:44,560 --> 00:31:49,440
task. We train a little model to look at, to like learn how to do that. Then we run it across,

369
00:31:49,440 --> 00:31:55,920
you know, years and years of video of people playing Minecraft on YouTube. We then get label data

370
00:31:55,920 --> 00:32:00,320
for all of these videos on YouTube of how to play the game. We then pre-trained a model just like

371
00:32:00,320 --> 00:32:06,960
GPT to go from past to what's the next action. And now we can zero shot have the agent out of

372
00:32:06,960 --> 00:32:12,640
pre-training do very complicated behaviors. And with that baseline, we can then fine tune the

373
00:32:12,640 --> 00:32:17,280
agent to solve any task we want it to do in Minecraft or, you know, very hard tasks. And what we

374
00:32:17,280 --> 00:32:21,760
show in the paper is that, you know, zero shot, the agent's able to do really complicated things

375
00:32:21,760 --> 00:32:27,360
to take humans like two to five minutes. But once you give it a challenge, if you don't pre-trained,

376
00:32:27,360 --> 00:32:31,520
you have no hope of solving the challenge. But with this pre-training, you can end up learning

377
00:32:31,520 --> 00:32:35,920
things like getting diamond tools in the game that take humans more than 20 minutes. And I think

378
00:32:35,920 --> 00:32:41,120
it's like 24,000 sequential actions to achieve. And you can learn it relatively easily with

379
00:32:41,120 --> 00:32:46,480
reinforcement because you started off by standing on a shoulders of giant human datasets. So to

380
00:32:46,480 --> 00:32:52,080
me, that is kind of like how the era of pre-training fits into AAGAs is that human datasets can be

381
00:32:52,080 --> 00:32:57,760
this huge catalyst or this huge level up that allows us to skip all the bootstrapping. Maybe the

382
00:32:57,760 --> 00:33:01,680
first couple billion years of evolution get right to the good stuff where we have an agent that

383
00:33:01,680 --> 00:33:06,320
understands language, understands our world, maybe knows how to act in that world. And then now we're

384
00:33:06,320 --> 00:33:10,400
challenging it to go off and like learn new types of science and math and learn new skills and

385
00:33:10,400 --> 00:33:14,880
like solve cancer and do things that we aren't even able to do yet. And so I think that's kind of

386
00:33:14,880 --> 00:33:19,200
one of these exciting accelerants that might mean we don't need a platform as computer to pull

387
00:33:19,200 --> 00:33:23,760
us off. In what ways do you see the VBT work as being generally applicable, meaning

388
00:33:24,400 --> 00:33:29,680
are is it a set of techniques for teaching an agent to get really good at playing Minecraft or

389
00:33:31,440 --> 00:33:39,920
is it something that we can generally apply to video understanding or video, you know, inferring

390
00:33:39,920 --> 00:33:44,160
you know actions from videos? Yeah, I think it's very general. I think that you could use this

391
00:33:44,160 --> 00:33:52,720
technique to learn how to do whatever it is that you see people on YouTube doing. Now there's

392
00:33:52,720 --> 00:33:57,440
some major caveats. First of all, it's much easier, you know, if you're talking about a computer

393
00:33:57,440 --> 00:34:02,480
usage. One of the things that we did in the Minecraft work is very intentionally we said we're

394
00:34:02,480 --> 00:34:07,600
not going to go for a handcrafted Minecraft action, you know, a space where you have like a macro

395
00:34:07,600 --> 00:34:13,200
where I hit a button and it like does this big complicated thing like chop sound a tree or places

396
00:34:13,200 --> 00:34:19,600
of lock or crafts a crafting table or whatever. We basically within the game of Minecraft most humans

397
00:34:19,600 --> 00:34:24,960
play it by using a mouse and keyboard. And so we said our agent has to learn the same thing, it has

398
00:34:24,960 --> 00:34:29,680
to learn how to use a mouse and use a keyboard. And it has to just remind craft figure out how to

399
00:34:29,680 --> 00:34:34,160
not only play the game but also within the game there's all these little tiny graphic user interfaces

400
00:34:34,160 --> 00:34:38,400
where you have to drag and drop little icons and click on buttons and browse through like recipe

401
00:34:38,400 --> 00:34:42,240
books and all sorts of stuff. It looks a lot like just learning how to use a computer like changing

402
00:34:42,240 --> 00:34:46,960
your preferences and like dealing with menus and Microsoft Word and things like that. So I think

403
00:34:46,960 --> 00:34:52,960
this technique is very general and basically could for example have an agent learn how to generally

404
00:34:52,960 --> 00:34:58,480
use a computer if we got enough tutorial videos of how to use a computer. There's some unsolved

405
00:34:58,480 --> 00:35:03,120
issues like different keyboard shortcuts and different programs do different things. So the

406
00:35:03,120 --> 00:35:06,400
challenge is a little harder for that thing that has to predict what action must have been taken

407
00:35:06,400 --> 00:35:12,080
given to video I've seen. But I think that's like a research problem that is solvable. You could even go more

408
00:35:12,080 --> 00:35:16,400
crazy to something like robotics and say I can maybe learn by watching robotics videos but now you

409
00:35:16,400 --> 00:35:20,800
have to solve a couple of extra problems like how do I infer what action must have been taken for whatever

410
00:35:20,800 --> 00:35:24,240
rub but I end up wanting to use it with other actions in that rub that are a little bit different.

411
00:35:24,800 --> 00:35:29,120
So there are some things to solve but I think it's um directionally right that you know this is

412
00:35:29,120 --> 00:35:33,440
probably an easy path to extract a lot of the knowledge of learning how to act by watching

413
00:35:33,440 --> 00:35:38,800
video demonstrations. And at a minimum anything that's in the computer states mouse and keyboard

414
00:35:38,800 --> 00:35:43,920
that covers that covers so much. Think of all the things you can do on a computer with you know

415
00:35:43,920 --> 00:35:48,000
just a mouse and keyboard and this system you know in theory should be able to learn how to do all of

416
00:35:48,000 --> 00:35:53,920
that as well. And so how do you say a tying into the idea of AI generating algorithms?

417
00:35:53,920 --> 00:35:58,880
Yeah well I you know so imagine for example that we like right when we did poet or when they did

418
00:35:58,880 --> 00:36:06,320
X land the agent basically starts out knowing almost nothing and it has to learn how to pick up

419
00:36:06,320 --> 00:36:11,360
the green ball and go you know get you to take it to the to the blue area or in poet it has to

420
00:36:11,360 --> 00:36:15,520
learn how to walk or if it was Minecraft it has to learn from you know if we didn't pre-trained

421
00:36:15,520 --> 00:36:19,840
to learn from scratch how to do all this stuff that's very computational inefficient.

422
00:36:19,840 --> 00:36:26,400
So if you had this pre-trained model that you can use to seed the agents you know give the

423
00:36:26,400 --> 00:36:31,680
agent some kind of common sense understanding we might refer to it as about its environment and

424
00:36:31,680 --> 00:36:37,920
how actions correspond to outcomes then it could stand on the shoulders of giants as you put it

425
00:36:37,920 --> 00:36:43,760
paraphrasing Newton. Exactly so imagine if it comes with GPTA skills so it knows all of human

426
00:36:43,760 --> 00:36:47,920
language it knows how to talk to you it knows how to get instructions on what it should do it

427
00:36:47,920 --> 00:36:52,080
also knows how to move around the world it knows how to do things on a computer or walk around

428
00:36:52,080 --> 00:36:58,400
a 3D video game it knows how to play Starcraft, Dota, Doom, Chess go it knows all that stuff and

429
00:36:58,400 --> 00:37:04,400
then you kick off this process. Now you're basically you're so far down the road that you probably

430
00:37:04,400 --> 00:37:08,640
don't need that many more pieces to create a system that's truly kind of can auto-catalycally

431
00:37:08,640 --> 00:37:12,960
learn tremendously interesting things and so you're skipping over that really inefficient thing

432
00:37:12,960 --> 00:37:16,720
where it's learning to see and it's learning to understand language and it's learning how to

433
00:37:16,720 --> 00:37:22,720
move its body so just like GPT went from you know think about the world like the pre-NLP world

434
00:37:22,720 --> 00:37:27,360
before GPT you know we had hacked together all this manual stuff and it kind of worked but not

435
00:37:27,360 --> 00:37:32,560
really and all of a sudden the GPT paradigm just like wow AI is exciting and it can do amazing

436
00:37:32,560 --> 00:37:37,040
things that it's metal-learning and it can almost be human level but now apply that to learning to

437
00:37:37,040 --> 00:37:43,840
like act in three dimensional worlds on computers you know I think that that is such a huge

438
00:37:43,840 --> 00:37:49,120
place you know such a huge leapfrog forward as a place to start from when you're trying to do

439
00:37:49,120 --> 00:37:57,840
something that create an open-ended system in an AI generator. Going back to Poet, a key element

440
00:37:57,840 --> 00:38:08,960
of what you're trying to do there is you're creating new environments that serve the purpose of

441
00:38:08,960 --> 00:38:21,440
teaching the agent new things and I want to try to to kind of dive into how direct it is

442
00:38:21,440 --> 00:38:30,080
is that the you know the n plus 1th environment is that you know designed again some you know

443
00:38:30,080 --> 00:38:36,000
reward function that is optimizing for learning or tries to predict the amount the agent will

444
00:38:36,000 --> 00:38:40,320
learn or is it more of our random we're going to create a bunch of environments and hope the

445
00:38:40,320 --> 00:38:47,840
agent learns something like tell me more about the algorithmic aspect of that. Yeah great question

446
00:38:47,840 --> 00:38:52,400
so in Poet we were in trying to produce an initial proof of concept with a lot of these ideas

447
00:38:52,400 --> 00:38:57,200
a lot of the individual pieces are pretty simple so in that work it was a simple evolutionary

448
00:38:57,200 --> 00:39:02,240
based system you take a description of an environment you mutate it which means you just change

449
00:39:02,240 --> 00:39:05,520
some of the numbers in the parameter vector that describes the environment that produces a

450
00:39:05,520 --> 00:39:09,840
slightly different environment and then you basically keep it if it's not too hard and not too

451
00:39:09,840 --> 00:39:15,440
easy for the agent which is basically a proxy for will it learn on it or not. Now you definitely

452
00:39:15,440 --> 00:39:19,280
could get more intelligent and I think this is a fantastic direction to go where you train a

453
00:39:19,280 --> 00:39:25,440
neural network model whose job it is to look at the history of what this agent has learned so far

454
00:39:25,440 --> 00:39:30,080
and intelligently give it the next learning challenge that it thinks is right at the cusp of

455
00:39:30,800 --> 00:39:34,320
I think you could like you're ready for this you know and this is where you're going to have

456
00:39:34,320 --> 00:39:39,200
maximal learning progress but also that the task itself is worth learning is interesting.

457
00:39:39,200 --> 00:39:45,280
Well even even in the case of Poet are you are you predicting whether it's too hard or too

458
00:39:45,280 --> 00:39:51,760
easy or are you letting the agent try and kind of measuring progress and you know doing things

459
00:39:51,760 --> 00:39:57,600
like early stopping or does the agent actually have to do it and that's when you know if it's too

460
00:39:57,600 --> 00:40:01,760
hard or too easy. Yeah so in that work we took a pretty easy approach which is we created the

461
00:40:01,760 --> 00:40:08,720
environment we tested the current agents that we had so far and we had hand to find for this

462
00:40:08,720 --> 00:40:13,760
particular type of environment if you're basically have a score that's not too low and not too high

463
00:40:13,760 --> 00:40:18,080
we'll say that's like that's in the middle and then after a while if you haven't been learning on it

464
00:40:18,080 --> 00:40:23,120
I think we can get out but that's you know that's kind of in in retrospect since then there have

465
00:40:23,120 --> 00:40:27,760
been people that have been working on versions that don't have these kind of hand defined limits

466
00:40:27,760 --> 00:40:32,160
they're automatically basically trying to estimate learning progress and saying if I'm seeing

467
00:40:32,160 --> 00:40:37,360
learning progress then keep going. For example so work we had out of OpenAI before VPT

468
00:40:38,160 --> 00:40:43,440
it came out of Ingmar was the Lolita author came out of our team at OpenAI basically was

469
00:40:43,440 --> 00:40:47,120
doing a different technique that was trying to measure learning progress on a task so this was

470
00:40:47,120 --> 00:40:52,560
also in Minecraft we give you a challenge and we say you know initially we'll give you a challenge

471
00:40:52,560 --> 00:40:56,720
and we'll search your records statistics on it and if it looks like after a few trials

472
00:40:56,720 --> 00:41:01,360
that you are in fact in the sweet spot of learning then we'll keep it and if not we'll basically

473
00:41:01,360 --> 00:41:04,880
stop sampling that as much so you're trying to basically it's almost like a bandit problem you're

474
00:41:04,880 --> 00:41:08,640
pulling all these arms and if you start to get learning you keep pulling that until you run

475
00:41:08,640 --> 00:41:12,480
you stop getting learning and then you go on to find something else and there's lots of different

476
00:41:12,480 --> 00:41:16,720
ways that you could approach this probably this problem but I want to point out what I think is

477
00:41:16,720 --> 00:41:20,960
probably the most interesting challenge which is I mean learning progress is hard to get right no

478
00:41:20,960 --> 00:41:27,600
question but then the other question is is it interesting is the challenge worthwhile because

479
00:41:27,600 --> 00:41:32,720
almost all of the learning progress based systems have pathologies that when you start to optimize

480
00:41:32,720 --> 00:41:38,400
for learning progress you start to get environmental challenges the agent shows huge learning progress

481
00:41:38,400 --> 00:41:45,680
on that are totally worthless imagine if I told you that your job was to I don't know like you

482
00:41:45,680 --> 00:41:50,800
let's say you're you're running an obstacle course your job is to get I invented a whole obstacle course

483
00:41:50,800 --> 00:41:55,440
and it had like you know balance beam and like a wall at a rope you know ladder you had to climb and

484
00:41:55,440 --> 00:42:00,240
you did really well okay so you're pretty good on that task and then I just tack on at the end

485
00:42:00,240 --> 00:42:05,840
like wiggle your arms in exactly this particular way but it's just a little bit of that at the end

486
00:42:05,840 --> 00:42:10,320
well okay so you first did the obstacle course maybe we considered that interesting and at the end

487
00:42:10,320 --> 00:42:13,920
you can get some learning if you happen to luckily figure out how to do wiggle your arm in

488
00:42:13,920 --> 00:42:17,760
exactly the right way okay and now you're no longer after you learned that you no longer

489
00:42:17,760 --> 00:42:21,440
learning anything because you figured out the right wiggling I just tack on a little bit more

490
00:42:21,440 --> 00:42:25,680
different random wiggling this is kind of the equivalent of of memorizing white noise but in

491
00:42:25,680 --> 00:42:31,120
motor space and I could probably endlessly add just a little bit more stuff for you to memorize

492
00:42:31,120 --> 00:42:35,200
that is not going to generalize to our world or to even other environments it's just totally

493
00:42:35,200 --> 00:42:40,720
uninteresting now that's super that's super interesting how do you characterize that interestingness

494
00:42:40,720 --> 00:42:47,520
so that you can optimize for it yes welcome like decades of thinking in like open-endedness trying

495
00:42:47,520 --> 00:42:51,360
to figure out the answer to this problem the short answer is we don't know yet but I have some

496
00:42:51,360 --> 00:42:56,720
ideas we're currently working on in my lab now so stay tuned but I'll give you a flavor of some ideas

497
00:42:56,720 --> 00:43:01,840
that are not exactly our current best ideas but you know in the spirit of them imagine for example

498
00:43:01,840 --> 00:43:06,960
if you had to learn some skills in one environment that give you learning progress and they also

499
00:43:06,960 --> 00:43:11,600
help you generalize to other environments that's already a little bit better because you're not

500
00:43:11,600 --> 00:43:17,760
going to memorize random motor twitches another idea you could do is you could grind into the real world

501
00:43:17,760 --> 00:43:23,840
so you have say a held up set of tasks from real games or the need to make money in the world or

502
00:43:23,840 --> 00:43:29,520
to imitate you know animals on YouTube or something and you have to like practice if they the

503
00:43:29,520 --> 00:43:33,920
the environment has obstacle courses maybe you have to learn to break your client if you never see

504
00:43:33,920 --> 00:43:40,240
a human wiggling their arms in in the wild then it's probably not useful is that kind of the idea

505
00:43:40,240 --> 00:43:45,840
or if learning that skill practicing on that wiggling task doesn't make you better at like being

506
00:43:45,840 --> 00:43:53,120
American ninja or imitating you know American like humans in the American ninja obstacle course game

507
00:43:53,120 --> 00:43:57,440
or imitating orangutan breaky eating or something like that if you don't get better at doing

508
00:43:57,440 --> 00:44:01,120
something we care about by training on these tasks then maybe we consider those tasks to be

509
00:44:01,120 --> 00:44:05,520
uninteresting so the system that's generating those tasks then has an extra challenge it's not

510
00:44:05,520 --> 00:44:10,640
just trying to get learning progress because that's not sufficient it's necessary but not sufficient

511
00:44:10,640 --> 00:44:15,760
it also has to generate tasks that are learning useful skills that transfer and help us solve

512
00:44:15,760 --> 00:44:20,160
all the problems that we do care about and that all of a sudden starts to feel like okay maybe I

513
00:44:20,160 --> 00:44:25,280
can see how that wouldn't have massive pathologies it might actually get us to worry why yeah it it

514
00:44:25,280 --> 00:44:31,040
uh to your point it strikes me that this is a super interesting problem because

515
00:44:33,440 --> 00:44:40,880
there's a big risk in like a big part of what we're trying to do is use algorithms and the

516
00:44:40,880 --> 00:44:48,320
computer you know compute and data to allow the system to you know create and learn from you know

517
00:44:48,320 --> 00:44:58,560
discontinuous patterns right that you know that we're not going to produce um but part of what

518
00:44:58,560 --> 00:45:05,920
you're trying to do with this interesting the interestingness you know regularization let's call

519
00:45:05,920 --> 00:45:15,760
it is like um it is to kind of dampen just kind of random things so it's like you you you want to

520
00:45:15,760 --> 00:45:24,720
capture and you know amplify randomness that's useful but uh but kind of suppress you know

521
00:45:24,720 --> 00:45:30,400
randomness it's not useful it sounds really hard without some kind of signal or ground truth or

522
00:45:30,400 --> 00:45:36,320
something that's going to tell us and part of the point is like you don't if you think about it

523
00:45:36,320 --> 00:45:42,400
from an evolutionary perspective you don't know what kind of step evolutions are going to

524
00:45:42,400 --> 00:45:48,400
ultimately produce the success right yeah I it's super hard I think that's why it's fascinating

525
00:45:48,400 --> 00:45:53,120
I consider this to be one of many grand challenges it's kind of laying over here in this general

526
00:45:53,120 --> 00:45:58,480
area of research and AI generating algorithms um but it also doesn't feel hopeless um I feel like

527
00:45:58,480 --> 00:46:03,040
maybe take your net hack domain or a text domain if you could if the system could generate the right

528
00:46:03,040 --> 00:46:08,640
kind of problems basically like a teacher generating like lesson plans like work on this math problem

529
00:46:08,640 --> 00:46:12,960
work on write this essay etc and periodically you were testing how they're doing out there in

530
00:46:12,960 --> 00:46:17,920
the world at like helping us with our text-based problems or is it a better thing for an API of

531
00:46:17,920 --> 00:46:23,920
people who want to use a language model um maybe there's some maybe maybe there's enough learning

532
00:46:23,920 --> 00:46:27,840
signal there that could figure out the right sort of challenges to generate especially that again

533
00:46:27,840 --> 00:46:31,520
if it's pre-training on what human teachers provide to students right and that's where you get

534
00:46:31,520 --> 00:46:36,240
that big catalyst where you're not just trying to like do it from some from like from just randomly

535
00:46:36,240 --> 00:46:40,400
generating problems and hoping that one of them happens to help you but you can take advantage

536
00:46:40,400 --> 00:46:45,520
what we know about teaching maybe just get better at it yeah maybe switching gears a little bit um

537
00:46:46,320 --> 00:46:54,480
but continuing in the broader theme of AGI how do you think about the the safety challenges of AGI

538
00:46:54,480 --> 00:47:02,880
and the context of AI generating algorithms great question so I am in general very

539
00:47:02,880 --> 00:47:09,760
concerned with the safety challenges regarding AGI so putting aside the specific concerns that

540
00:47:09,760 --> 00:47:15,120
might come up with AI generating algorithms I think we as a community are playing with fire and

541
00:47:15,120 --> 00:47:21,200
we're doing we're creating a very powerful technology and we have to be very careful that we do

542
00:47:21,200 --> 00:47:27,520
it in the right way um and I'm not even honestly sure that you know if you gave me a button and

543
00:47:27,520 --> 00:47:31,840
said could you pause a AGI development so we can figure out the right way to make it benefit

544
00:47:31,840 --> 00:47:36,560
all humanity would you hit that button I probably would but I think that humanity doesn't have a

545
00:47:36,560 --> 00:47:44,240
good track record I would because I'm so concerned about like the potential negative impacts that we

546
00:47:44,240 --> 00:47:50,400
need time to get this right however I think that a humanity basically doesn't have a very good

547
00:47:50,400 --> 00:47:55,600
track record of not inventing things when they can and like you know it's going to be the case

548
00:47:55,600 --> 00:48:00,800
as somebody makes progress so condition on that I think that we need to make sure that we do the

549
00:48:00,800 --> 00:48:07,760
best job that's possible maximize the potential upside and minimize the downside that's just my

550
00:48:07,760 --> 00:48:14,240
comments on AGI in general now there are specific ethical concerns that come up with AI GA's that I

551
00:48:14,240 --> 00:48:21,360
think are interesting one of them we get to that if I can okay I'm sure okay is in what ways is

552
00:48:22,480 --> 00:48:29,600
is your working on AI not the exist you know proof that you wouldn't actually hit that button

553
00:48:29,600 --> 00:48:38,880
well it's because of that very important caveat that I don't think that humanity will stop working

554
00:48:38,880 --> 00:48:43,840
on it and so therefore I think that we mean it doesn't really matter if you individually hit the

555
00:48:43,840 --> 00:48:48,480
button unless everybody hit the button hits the button so it's like a game theory kind of problem

556
00:48:48,480 --> 00:48:55,840
in a sense yeah yeah yeah I don't know that I like the words it doesn't matter because I think

557
00:48:55,840 --> 00:49:00,240
another way to say it is it's inevitable and so we might as well I might as well do my best to

558
00:49:00,240 --> 00:49:04,240
try to make sure that it goes as positively as possible that's kind of how I think about

559
00:49:06,000 --> 00:49:09,280
that's why if you gave me this button that I could stop everybody from working on it so we could

560
00:49:09,280 --> 00:49:14,480
take 20 years and discuss the right way to do it I'd probably hit that button so if you're

561
00:49:15,440 --> 00:49:19,360
I could tell you about what I think are some AI GA specific ethical concerns but also this is

562
00:49:19,360 --> 00:49:25,360
an important topic so I'm happy just just to talk about AI safety as well and now we can you know

563
00:49:25,360 --> 00:49:31,760
we'll we'll continue to talk about it but we can jump into the AIGA part okay so you might say

564
00:49:31,760 --> 00:49:37,600
all right well is something there's something more risky about AIGAs than the manual path to AI

565
00:49:37,600 --> 00:49:44,080
for example I think there are some things that might be more risky one of them is that it's almost

566
00:49:44,080 --> 00:49:49,840
by its nature trying to create kind of a auto catalytic process that's getting bootstrapped thing

567
00:49:49,840 --> 00:49:54,960
itself up and so and any given instance of the algorithm you might finally stumble upon the right

568
00:49:54,960 --> 00:49:59,600
ingredients and boom you create this kind of lightweight process so you're kind of looking in

569
00:49:59,600 --> 00:50:06,400
some sets for something that is like a fast takeoff or at least a medium takeoff in a way that

570
00:50:06,400 --> 00:50:12,000
maybe the manual path won't have I think increasingly that's less less obvious that the manual path

571
00:50:12,000 --> 00:50:17,440
also might not have that you know maybe GPT-6 just suddenly has the power to do amazing you know

572
00:50:17,440 --> 00:50:24,160
like this AGI and can control the world's digital systems so who knows but at least in AIGA

573
00:50:24,160 --> 00:50:28,000
as you're looking for that I think there's another one going back to this issue of qualia

574
00:50:28,000 --> 00:50:34,320
and philosophy and that is you know what if I what if we could create a system that had like a

575
00:50:34,320 --> 00:50:38,080
whole bunch of agents learning a whole bunch of different tasks interacting with each other

576
00:50:38,080 --> 00:50:43,680
and we're basically simulating an entire human civilization or at least maybe sorry it's

577
00:50:43,680 --> 00:50:48,000
simulating an earth up to the dawn of female civilization and then AGI is like once you get

578
00:50:48,000 --> 00:50:53,040
humans in this simulation well how much untold suffering what you have caused in all of these

579
00:50:53,040 --> 00:50:58,240
digital beings you know like like who is responsible for all the pain and suffering of dinosaurs

580
00:50:58,240 --> 00:51:02,960
and hyenas and lions and their ancestors and is that a price we're willing to pay I think it's

581
00:51:02,960 --> 00:51:08,320
interesting if we knew that these creatures were actually suffering is it worth it to get to AGI

582
00:51:09,120 --> 00:51:13,680
that's something we should consider and a final thing that I think is really interestingly unique

583
00:51:13,680 --> 00:51:21,360
to AGI is more so than many other attempts is that if and this is a big if but if you tried to create

584
00:51:21,360 --> 00:51:27,360
your outer loop meta reward function that we talked about to be similar to the one on earth which

585
00:51:27,360 --> 00:51:33,280
might be just like kill and don't be killed survival then maybe you recreate the red and tooth and

586
00:51:33,280 --> 00:51:40,080
claw situation that happened on earth and so the values and the instincts of the entities that

587
00:51:40,080 --> 00:51:45,120
come out of it are also potentially like us maybe they're selfish and they're violent and they

588
00:51:45,120 --> 00:51:50,800
are deceptive etc you I think it'd be really awesome and interesting if we could create

589
00:51:50,800 --> 00:51:54,800
versions of AGI's where the outer loop thing is very cooperative and you don't end up with those

590
00:51:54,800 --> 00:52:00,240
kind of vices and you get more virtues and I also don't know to what extent the manual path

591
00:52:00,240 --> 00:52:05,840
is going to create vices instead of virtues but thinking about kind of the values of the the

592
00:52:05,840 --> 00:52:12,160
AGI that we create is absolutely essential like we need its values to be aligned with ours and

593
00:52:12,160 --> 00:52:17,200
to the extent that it doesn't have our vices that dramatically increases the chances that things

594
00:52:17,200 --> 00:52:25,680
go well once we make AGI. Yeah I think there's maybe an argument that of all of the ways the folks

595
00:52:25,680 --> 00:52:33,760
are trying to get to AGI the AIGA is probably the most dangerous and that you are specifically

596
00:52:34,720 --> 00:52:42,080
trying to enable the machine to you know create its own environments and as we discussed

597
00:52:42,080 --> 00:52:47,120
earlier you know that may lead to creating its own reward functions right because that's going

598
00:52:47,120 --> 00:52:55,040
to be the big challenge and that seems specifically like a point where you lose control.

599
00:52:55,040 --> 00:52:58,880
Totally agree and I make that exact point in the paper on AGI generating algorithms

600
00:53:00,800 --> 00:53:05,360
however if that is a con an important con I think we should need to do research to try to

601
00:53:05,360 --> 00:53:10,080
like minimize the possible downsides of that con but let me also express what the pro is that

602
00:53:10,080 --> 00:53:16,720
comes along with that and that is that if we believe that the manual path is kind of designing

603
00:53:17,840 --> 00:53:22,240
AI it's probably we're probably going to be designed in our image and it's similar to us and

604
00:53:22,240 --> 00:53:26,320
it's probably at least in the the the current path is going to be consuming human data so it's

605
00:53:26,320 --> 00:53:32,480
going to look a lot like us. AIGA is because they could go off in so many different directions

606
00:53:32,480 --> 00:53:40,880
is all gives us the amazing possibility of effectively doing alien cultural travel we get to see

607
00:53:40,880 --> 00:53:46,960
all these totally different types of intelligences and societies and cultures that might pop out

608
00:53:46,960 --> 00:53:51,440
of the system because it's so like possibly going in all these different directions that don't

609
00:53:51,440 --> 00:53:55,680
have a lot to do with the biases that were making either the system and so yes some of them might

610
00:53:55,680 --> 00:54:00,960
be unsavory and we need to make sure we have like the safety in place to like not like that be a

611
00:54:00,960 --> 00:54:07,360
problem or try to minimize that for being a problem but at the same time like coming to understand

612
00:54:07,360 --> 00:54:12,000
intelligence in general the space of possible culture and the space of intelligence like what is

613
00:54:12,000 --> 00:54:17,200
the music and the math of an alien culture look like well here is a system that might actually show

614
00:54:17,200 --> 00:54:22,240
it to us with greater probability than anything we're doing in the manual path AI and you know I

615
00:54:22,240 --> 00:54:25,680
don't think we're going to invent interstellar travel for quite some time so this is kind of our

616
00:54:25,680 --> 00:54:31,920
best shot to you to explore the space of possible intelligences in the universe and that

617
00:54:32,800 --> 00:54:37,600
sounds like the science it's the stuff of science fiction but it's probably not as far off as

618
00:54:37,600 --> 00:54:47,920
many people would imagine yeah it it does seem that even even kind of optimizing or or kind of

619
00:54:47,920 --> 00:54:57,200
pursuing this AIGA oriented path it's still in the context of you know all of the data that we have

620
00:54:58,960 --> 00:55:05,120
you know which has its buy I guess I'm I'm pushing back in that I'm not sure the AIGA to the

621
00:55:05,120 --> 00:55:10,880
extent that it's built on you know access to all of our data and and that's going to be an

622
00:55:10,880 --> 00:55:18,080
accelerant that it necessarily divorces us from the biases that the manual path will have totally agree

623
00:55:18,080 --> 00:55:25,440
so the first AIGA is probably consume human data and look a lot like us and in that sense I

624
00:55:25,440 --> 00:55:30,320
totally agree you don't get this this cultural travel you know seeing this massive diversity of

625
00:55:30,320 --> 00:55:35,200
possible cultures but I think once we figured out how to make that system we probably then can go

626
00:55:35,200 --> 00:55:39,440
back and try to create the system without consuming human data without that big step forward

627
00:55:39,440 --> 00:55:45,760
um standing on the shoulders of human data sets giant human data sets and also maybe not as

628
00:55:45,760 --> 00:55:50,960
much stuff built in our image and that's when you can start to kind of explore the space of

629
00:55:50,960 --> 00:55:56,640
possibilities much more and so there's kind of a spectrum even within AIGA between like to what

630
00:55:56,640 --> 00:56:00,160
extent they're going to look at us and it's not just consuming human data if we use the tricks that

631
00:56:00,160 --> 00:56:05,360
I described for grounding what counts as interesting as solving problems in our world especially on

632
00:56:05,360 --> 00:56:10,080
earth you know making money in our economy or whatever then that also is grounding it too

633
00:56:10,080 --> 00:56:15,520
basically it would look like a lot like something that is quite human but maybe we figure out a way

634
00:56:15,520 --> 00:56:20,080
to figure out tasks that are intrinsically interesting that are just internal to that world

635
00:56:20,080 --> 00:56:23,840
like I said you have to learn tasks that we're here that generalize to other tasks within that

636
00:56:23,840 --> 00:56:29,120
system within that kind of bubble maybe that still gives you intelligence but now it's totally

637
00:56:29,120 --> 00:56:38,800
unhinged from earth so we've we've covered a lot of ground I think if we it's easy for me

638
00:56:38,800 --> 00:56:43,440
you know relatively easy for me to wrap my head around kind of the you know the near term at

639
00:56:43,440 --> 00:56:49,440
least the the kind of work that you're doing now and generating environments and training agents

640
00:56:49,440 --> 00:57:00,880
on those environments and the the kind of the impulse that that you're using and creating

641
00:57:00,880 --> 00:57:06,640
these environments to maximize learning is something that's going to you know accelerate

642
00:57:08,080 --> 00:57:14,560
the evolution of intelligence let's say and then the long term is you know if you know if that

643
00:57:14,560 --> 00:57:21,440
continues and we throw lots more compute at it lots more data at it you can see how there's an

644
00:57:21,440 --> 00:57:32,000
argument that that's going to going to create an intelligence that is far beyond what the manual

645
00:57:32,000 --> 00:57:38,000
path might do or there's a lot there in that statement that I don't like but at least that you

646
00:57:38,000 --> 00:57:44,720
know there's some there's the opportunity to kind of to serendipitously stumble onto some

647
00:57:44,720 --> 00:57:53,520
discontinuous innovation that gets us there that the manual thing might not at least what's in

648
00:57:53,520 --> 00:57:59,440
that middle what do you see in that kind of middle term you know beyond the stuff that we're doing

649
00:57:59,440 --> 00:58:06,720
now you know not quiet at the you know then the miracle occurs like what what what what what's the I

650
00:58:06,720 --> 00:58:14,480
don't know three to five year kind of path for AIGA yeah good question so I think one thing that

651
00:58:14,480 --> 00:58:18,400
we haven't seen people do a lot of some people have but not a lot of is putting all the pillows

652
00:58:18,400 --> 00:58:23,360
together so for example we haven't seen a lot of architecture search kind of the learning

653
00:58:23,360 --> 00:58:28,080
in conjunction with the learning to learning algorithm stuff in conjunction with environment

654
00:58:28,080 --> 00:58:32,880
generation so I think we're going to see I think we're going to see more of that I also think

655
00:58:32,880 --> 00:58:37,600
you're probably going to see more stuff like poet and like excellent you have these agents

656
00:58:37,600 --> 00:58:42,400
who are just increasingly training on increasingly diverse challenges and becoming very general

657
00:58:42,400 --> 00:58:48,720
intelligent and so what you're starting going to start to see I think is more and more sample

658
00:58:48,720 --> 00:58:54,480
efficient reinforcement learning for two reasons one is that the algorithms the architectures

659
00:58:54,480 --> 00:59:00,960
etc are better optimized to learn quickly but also that you're not always starting from scratch

660
00:59:01,680 --> 00:59:06,800
you know people complain about the sample efficiency on like Dota or Atari or chess or whatever

661
00:59:06,800 --> 00:59:12,160
compared to a human but like a human doesn't learn to play chess waking up in the middle of a

662
00:59:12,160 --> 00:59:15,760
chess plane universe they start learning planning learning to play chess when they're six or seven

663
00:59:15,760 --> 00:59:20,000
they've had years of data coming in so I think increasingly what you're going to see are

664
00:59:20,000 --> 00:59:26,560
internal to these systems agents that basically already know a lot as they approach the next problem

665
00:59:26,560 --> 00:59:31,040
and maybe you via pre-training maybe even the system itself being pre-trained and maybe you

666
00:59:31,040 --> 00:59:36,000
can also models that are come from somewhere else that are dropped into these systems that are

667
00:59:36,000 --> 00:59:39,840
trained and also models that are exported from these systems to become pre-training that are used

668
00:59:39,840 --> 00:59:44,720
for other things so increasingly I think the era of starting from random weight initializations

669
00:59:44,720 --> 00:59:49,760
will go away we're starting to see that now almost all many many many AI papers now are starting

670
00:59:49,760 --> 00:59:56,880
with pre-trained components right and so this is kind of a quiet revolution that is basically AI

671
00:59:56,880 --> 01:00:03,200
is no longer being as sample inefficient because we are starting not from scratch starting from

672
01:00:03,200 --> 01:00:09,600
from a good foundation so I think that's probably what the next you know three to five years look

673
01:00:09,600 --> 01:00:15,040
like increasingly capable sample efficient learning agents that can generalize and another

674
01:00:15,040 --> 01:00:20,240
one that I would mention I think text in a conditioning on the task knowing what I'm supposed to do

675
01:00:20,240 --> 01:00:24,960
is another way that they get much more sample efficient so I just think you'll see more and more

676
01:00:24,960 --> 01:00:29,680
things like excellent and Rubik's Cube where these systems can do quite impressive things in

677
01:00:29,680 --> 01:00:34,160
generalizing big distributions of environments instead of narrow agents trained from scratch for one

678
01:00:34,160 --> 01:00:39,280
environment awesome well Jeff it was wonderful wonderful speaking with you and getting to learn a

679
01:00:39,280 --> 01:00:45,360
little bit about your research and looking forward to keeping in touch likewise thank you very

680
01:00:45,360 --> 01:00:49,680
much for having me I really enjoyed the conversation and it's a great fantastic podcast so it's

681
01:00:49,680 --> 01:01:17,840
an honor to be on thanks so much Jeff

