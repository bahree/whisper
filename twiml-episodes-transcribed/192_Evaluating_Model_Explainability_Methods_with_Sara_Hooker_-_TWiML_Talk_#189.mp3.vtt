WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.760
I'm your host Sam Charrington.

00:31.760 --> 00:37.560
A few weeks ago, machine learning and AI researchers, practitioners and students from Africa and around

00:37.560 --> 00:42.720
the world met in Cape Town for the second annual deep learning in Daba, an event that aims

00:42.720 --> 00:47.160
to expand African participation and contribution in the field.

00:47.160 --> 00:50.960
While I wasn't able to make it to Cape Town myself, I did have a chance to speak with some

00:50.960 --> 00:55.080
of the event's awesome speakers and I'm excited to present to you our deep learning in Daba

00:55.080 --> 00:56.880
series.

00:56.880 --> 01:02.200
In this, the first episode of the series were joined by Sarah Hooker, AI resident at Google

01:02.200 --> 01:03.200
Brain.

01:03.200 --> 01:07.040
I had the pleasure of speaking with Sarah in the run up to the endaba about her work on

01:07.040 --> 01:09.960
interpretability and deep neural networks.

01:09.960 --> 01:14.360
We discussed what interpretability means and when it's important and explore some nuances

01:14.360 --> 01:19.080
like the distinction between interpreting model decisions versus model function.

01:19.080 --> 01:24.400
We also dig into her paper evaluating feature importance estimates and look at the relationship

01:24.400 --> 01:29.040
between this work and interpretability approaches like Lime.

01:29.040 --> 01:33.240
We also talk a bit about Google, in particular the relationship between Brain and the rest

01:33.240 --> 01:38.120
of the Google AI landscape and the significance of the recently announced Google AI Lab in

01:38.120 --> 01:42.240
a Cragana, being led by friend of the show Mustafa Sisay.

01:42.240 --> 01:46.480
And of course, we chat a bit about the endaba as well.

01:46.480 --> 01:50.280
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their

01:50.280 --> 01:54.240
support of the podcast and their sponsorship of this series.

01:54.240 --> 01:58.360
In this podcast, you heard Sarah talk about the AI residency program she's in at Google.

01:58.360 --> 02:03.840
Well, just yesterday, they opened up applications for the 2019 program.

02:03.840 --> 02:07.960
The Google AI residency is a one year machine learning research training program with the

02:07.960 --> 02:13.080
goal of helping individuals become successful machine learning researchers.

02:13.080 --> 02:17.640
The program seeks residents from a very diverse set of educational and professional backgrounds

02:17.640 --> 02:19.480
from all over the world.

02:19.480 --> 02:23.960
So if you think this is something that sounds interesting, you should definitely apply.

02:23.960 --> 02:29.280
Find out more about the program at g.co slash AI residency.

02:29.280 --> 02:31.920
And now on to the show.

02:31.920 --> 02:34.560
All right, everyone.

02:34.560 --> 02:36.520
I am on the line with Sarah Hooker.

02:36.520 --> 02:39.560
Sarah is an AI resident at Google Brain.

02:39.560 --> 02:42.360
Sarah, welcome to this week in machine learning and AI.

02:42.360 --> 02:43.360
Hi, Sam.

02:43.360 --> 02:46.440
I am so excited for this conversation.

02:46.440 --> 02:54.280
Let's start with a little bit of background and you've got a somewhat non traditional

02:54.280 --> 02:57.760
background for a Google brain researcher.

02:57.760 --> 03:00.000
You taught yourself machine learning.

03:00.000 --> 03:02.000
Tell me a little bit about that.

03:02.000 --> 03:07.840
Yeah, it's interesting with these with these type of questions of how did you get into

03:07.840 --> 03:08.840
research.

03:08.840 --> 03:12.960
And you have these very common reference points that I shared.

03:12.960 --> 03:16.040
I went to this school and then I did this PhD.

03:16.040 --> 03:22.720
And a lot of my career has been driven by things which are less compactly described,

03:22.720 --> 03:31.400
which is they've been much driven by curiosity and like moderate obsession with certain questions.

03:31.400 --> 03:38.800
But I think probably one of the pivotal moments was I started a nonprofit four years ago.

03:38.800 --> 03:44.400
And it was cool delta at what is cool delta analytics and it works with other nonprofits

03:44.400 --> 03:49.480
all over the world and also teaches machine learning.

03:49.480 --> 03:54.600
But at the time when I started it, it was my background originally academically as an

03:54.600 --> 03:55.600
economics.

03:55.600 --> 04:00.200
And I was very excited to just do economic modeling for nonprofits.

04:00.200 --> 04:05.840
And then the composition of volunteers that we had because we were in the Bay Area was

04:05.840 --> 04:13.600
kind of eclectic mix of economists and engineers and machine learning researchers.

04:13.600 --> 04:17.000
And that was kind of a turning point for me because in some ways the tasks that we were

04:17.000 --> 04:18.440
looking at were really exciting.

04:18.440 --> 04:25.440
And we were working with nonprofits that were doing education programs using pre-smart

04:25.440 --> 04:27.960
technology in Nairobi.

04:27.960 --> 04:34.840
We also worked with fascinating problems where it was detecting illegal chainsaw activity

04:34.840 --> 04:37.920
and rainforests using audio.

04:37.920 --> 04:42.840
And it both gave me this idea that machine learning is just so exciting and so interesting.

04:42.840 --> 04:47.760
But also I was working with people who were a lot better than me.

04:47.760 --> 04:56.320
And so I think that's also was fairly critical in just my own sense of measuring my technical

04:56.320 --> 05:01.960
progress and feeling like, wow, I really want to learn both the tools and the framework

05:01.960 --> 05:05.320
to be able to solve some of these problems.

05:05.320 --> 05:12.960
The second part really was I've spent two years working deploying algorithms.

05:12.960 --> 05:18.040
So I joined an online education company called Udemy.

05:18.040 --> 05:23.280
And there I was working on recommendation and spam detection.

05:23.280 --> 05:29.760
And that was fascinating because a lot of it was how do we deploy models which is very

05:29.760 --> 05:35.360
different from only focusing on developing models mainly because you're willing to give

05:35.360 --> 05:41.880
up quite a bit of accuracy in order to deploy successfully and have a robust deployment pipeline.

05:41.880 --> 05:47.600
So for me, kind of at the end of those two experiences and Delta has really been in parallel

05:47.600 --> 05:53.680
this whole time, for me brain was really a question of, do I enjoy research?

05:53.680 --> 05:58.480
Because I really enjoyed all these very domain specific questions.

05:58.480 --> 06:05.000
And I want to see, firstly, does that curiosity translate to a research framework and is

06:05.000 --> 06:08.520
it satisfying in the same way?

06:08.520 --> 06:14.200
And the residency has been, in particular, doing research at brain has been a very good

06:14.200 --> 06:18.040
way to answer that question.

06:18.040 --> 06:23.120
So that kind of brings me to today a lot of what I've done over the last years being

06:23.120 --> 06:28.080
doing research on interpretability and now model compression of brain.

06:28.080 --> 06:32.640
Have you arrived at an answer in terms of how you feel about research?

06:32.640 --> 06:41.720
And I'm curious from someone who, the perspective of someone who is new to research from an applied

06:41.720 --> 06:47.440
background and not to mention a self-taught background like, how has that transition been

06:47.440 --> 06:48.440
for you?

06:48.440 --> 06:50.520
How has that experience been for you?

06:50.520 --> 06:52.200
What do you like about research?

06:52.200 --> 06:54.360
What do you prefer about application?

06:54.360 --> 06:56.960
Yeah, that's an excellent question.

06:56.960 --> 07:04.320
And it's important because I think that there's few of my colleagues who have perhaps experienced

07:04.320 --> 07:05.320
both.

07:05.320 --> 07:12.640
And so, although it's a sample size of one, maybe useful in some ways for other people

07:12.640 --> 07:17.080
thinking about whether they want to do research or stay in applied, I think there's different

07:17.080 --> 07:18.680
pros and cons.

07:18.680 --> 07:28.280
So with my applied work, a lot of what is important about the skills and the thought process

07:28.280 --> 07:33.160
that applied work in parts is that you can't really abstract any part of the pipeline.

07:33.160 --> 07:40.680
So you have data preparation and your data is normally very specific to your use case.

07:40.680 --> 07:45.320
And so often you spend a lot of time getting usable data.

07:45.320 --> 07:50.920
And many times, for example, in the Delta projects, the cost of incremental data collection

07:50.920 --> 07:55.720
is fairly high unless you're using a mobile app or something that automatically generates

07:55.720 --> 07:57.200
new data.

07:57.200 --> 08:02.640
So understanding the distribution of your data is much, much more important.

08:02.640 --> 08:08.040
And then you also have the deployment phase and applied, which again, imposes very severe

08:08.040 --> 08:13.480
constraints on what you can actually do because you need to justify every additional step

08:13.480 --> 08:16.320
of complexity that you add to a model.

08:16.320 --> 08:22.120
On the other hand, I think research is this very kind of particular way of having a discourse

08:22.120 --> 08:23.120
by the problem.

08:23.120 --> 08:25.320
And it's often prickly.

08:25.320 --> 08:33.400
And I mean that in the sense that you're often using a very clearly stated set of assumptions

08:33.400 --> 08:40.360
about how you want to navigate a given task.

08:40.360 --> 08:48.280
But you are advancing solutions that should have a contribution, even if a time is marginal

08:48.280 --> 08:53.040
to a large set of problems, and not just a specific problem.

08:53.040 --> 08:57.080
But why I raised the difference earlier, you have to think about your data pipeline,

08:57.080 --> 09:01.200
you have to think about your deployments, what applied is that research or at least the

09:01.200 --> 09:05.040
current body of machine learning research largely abstracts those two.

09:05.040 --> 09:08.880
So we work with three datasets.

09:08.880 --> 09:14.800
And I'm sure most researchers listening know which ones, but evidence, sci-fi, and

09:14.800 --> 09:15.800
image net.

09:15.800 --> 09:17.760
And we take that as given.

09:17.760 --> 09:21.000
And we're not too concerned about deployment.

09:21.000 --> 09:25.840
We don't really think about this very interesting new research that is focusing on the interaction

09:25.840 --> 09:27.840
between the model and the hardware.

09:27.840 --> 09:31.320
But for the large part, we focus on the representation.

09:31.320 --> 09:37.560
And so large part of our time is very much thinking about how can we learn better representations

09:37.560 --> 09:44.200
given this data, and not really fusing too much about the details of how it translates.

09:44.200 --> 09:50.560
This is good and bad, it really, and maybe here's, and probably, I sense in plighting

09:50.560 --> 09:55.240
your questions also for me how abrupt the transition was.

09:55.240 --> 10:00.200
And like, what were challenges, personally, and what did I enjoy?

10:00.200 --> 10:05.640
I think that doing this and doing the residency is a very particular form of entering research

10:05.640 --> 10:13.320
because you have a lot of contact with very senior established researchers in the field,

10:13.320 --> 10:16.240
probably even more so than you would have in a PhD program.

10:16.240 --> 10:22.120
So to a large degree, if anything, I would say the fact that I really enjoy research

10:22.120 --> 10:29.560
and the fact that I think this way of discourse is valuable is probably also because I've

10:29.560 --> 10:33.520
had a very exciting set of collaborations here.

10:33.520 --> 10:37.280
And a lot of what determines how you think about a problem is also who you work with.

10:37.280 --> 10:40.800
And so that has been very charged and very exciting.

10:40.800 --> 10:48.920
I will say I do miss the feedback loop that comes from deploying a model because a lot

10:48.920 --> 10:57.480
of where how we measure progress in a task and research are perhaps metrics that might

10:57.480 --> 11:04.040
not actually might, and I say this with a note of hesitation in my voice because I think

11:04.040 --> 11:10.640
it's an active debate, but we use metrics like accuracy or for interpretability, it's

11:10.640 --> 11:13.920
even more complex, what metrics do we even use.

11:13.920 --> 11:21.200
But these are metrics where you may actually have a very high, may have a successful representation

11:21.200 --> 11:25.720
that inches up interpretable accuracy for these given data sets, but it's unclear

11:25.720 --> 11:31.280
like if you were to actually translate that to real world tasks, whether that would be

11:31.280 --> 11:32.800
a successful or not.

11:32.800 --> 11:39.040
And I think I'll point there because I'm curious like whether anything I said I should elaborate

11:39.040 --> 11:40.040
on.

11:40.040 --> 11:41.240
And I do a few things out there.

11:41.240 --> 11:46.080
Well, what's interesting about what you said was that you're probably at, you're doing

11:46.080 --> 11:52.720
research at one of the places that, you know, along the spectrum of pure and applied

11:52.720 --> 11:59.520
research is probably a lot closer to the applied side than most.

11:59.520 --> 12:05.280
And in fact, we see all the time how research that's happening at Google Brain ends up

12:05.280 --> 12:13.760
in products like Duplex and the Google Cloud products, I'm wondering how that fits into

12:13.760 --> 12:14.760
the equation.

12:14.760 --> 12:23.880
And I guess what's interesting is that you still characterize it as fairly disconnected

12:23.880 --> 12:28.360
from application, whereas I tend to think of it as fairly close.

12:28.360 --> 12:32.200
And I'm wondering if you can provide some context for that.

12:32.200 --> 12:33.200
Yeah.

12:33.200 --> 12:37.040
So I think it's much to do with incentive structures.

12:37.040 --> 12:44.840
So brain is exciting for researchers, because there's no incentive in how we, in our iteration

12:44.840 --> 12:48.720
cycle of ideas, that obliges us to think about product at all.

12:48.720 --> 12:56.360
In fact, like that's what's so charged about the atmosphere is that we don't have to,

12:56.360 --> 13:02.000
at any time, even if we don't deploy to a Google product or to any product, our research

13:02.000 --> 13:06.320
can still be considered successful and a contribution.

13:06.320 --> 13:12.560
The larger part of how brain, specifically, is orientated as around contributing to

13:12.560 --> 13:15.080
the wider research discourse.

13:15.080 --> 13:22.520
So that's what I meant by it's removed is that largely our framework is the same as academia.

13:22.520 --> 13:30.120
We start with an idea, and then a lot of times the combination of ideas is a successful

13:30.120 --> 13:36.280
contribution to a conference or it's open-sourcing code, so things that enrich the larger community.

13:36.280 --> 13:40.160
That being said, I will say one way in which perhaps this is a little bit different from

13:40.160 --> 13:49.360
what we imagine academia to be, is that because of, because it's an industry lab, there's

13:49.360 --> 13:55.400
much more room to do empirical experiments, which I think is quite exciting, and when

13:55.400 --> 14:02.160
I say empirical experiments, what I mean is that oftentimes that, previously, I would

14:02.160 --> 14:10.000
say, you can see the evolution in what data sets are considered the benchmark for just

14:10.000 --> 14:14.640
to find a certain hypothesis, or just to find a certain theoretical approach.

14:14.640 --> 14:21.360
We kind of moved from amnesty and sufficient to now, probably most people would not consider

14:21.360 --> 14:26.280
amnesty sufficient as evidence for your hypothesis is correct.

14:26.280 --> 14:32.880
And now you have cypher, and I would say at places like Google, these research labs,

14:32.880 --> 14:37.720
it is possible to do large scale experiments for data sets like ImageNet and do a lot

14:37.720 --> 14:45.800
of empirical and important empirical work to corroborate hypothesis, or perhaps to say

14:45.800 --> 14:51.160
that certain hypotheses that were previously held may not be correct given the data.

14:51.160 --> 14:56.720
So that is exciting, and that's one way in which the conversation is different.

14:56.720 --> 15:01.440
That being said, I'm sure there's aspects to a culture of academic lab that foster new

15:01.440 --> 15:07.560
directions of thinking, that perhaps because we can iterate so quickly with experiments

15:07.560 --> 15:13.360
here, we may not inform that in the way that we talk about these things.

15:13.360 --> 15:17.840
That I'm not sure about, because this is the only lab I've known, but I can imagine that

15:17.840 --> 15:20.600
would be a counterpoint in someone would say.

15:20.600 --> 15:27.120
It does surprise me a bit to hear that you are working day-to-day with the same data sets

15:27.120 --> 15:28.520
that everyone else is.

15:28.520 --> 15:36.240
I would have imagined that you had some super ImageNet plus plus or cypher plus plus that

15:36.240 --> 15:43.280
you're using, and when you're ready to publish, you dump things down for the outside world.

15:43.280 --> 15:49.640
I can only speak for my experience, my immediate colleagues, but there's no ImageNet plus

15:49.640 --> 15:55.720
that we just scurry around with behind the scenes.

15:55.720 --> 16:03.080
And perhaps that's because of essentially the cost of switching in between data sets.

16:03.080 --> 16:04.760
So this is another interesting insight.

16:04.760 --> 16:11.840
This is a lot of the reason why we, this is my opinion, but I sense a lot of the reason

16:11.840 --> 16:17.040
why we're using these same data sets is that often, during an idea and implementing

16:17.040 --> 16:24.200
these data sets, when you try and translate it to perhaps a more realistic data set or

16:24.200 --> 16:29.080
more complex, you may, it's not that you would get different results, it's just that like

16:29.080 --> 16:33.240
the implementation and how you go about it may be fairly different.

16:33.240 --> 16:38.960
And so there's given limited time resources and given that everyone else is articulating

16:38.960 --> 16:42.040
progress in terms of these three data sets.

16:42.040 --> 16:47.480
If you are a researcher, you also want to be talking in the framework of these publicly

16:47.480 --> 16:52.520
available data sets, because research is a lot about how do you measure progress in

16:52.520 --> 16:58.680
a task, and how do you articulate convincingly that you've made a contribution, and that

16:58.680 --> 17:04.640
largely involves referencing prior work in the same area that has used these same data

17:04.640 --> 17:05.640
sets.

17:05.640 --> 17:10.640
And then you say, given that this is a commonly understood reference point, this is what

17:10.640 --> 17:15.440
I advance is for the progress, that kind of restricts how much you can jump back and

17:15.440 --> 17:16.440
forth.

17:16.440 --> 17:21.040
But maybe, maybe what, maybe some researchers are using ImageNet for suspects, but I just

17:21.040 --> 17:25.200
haven't been kept in the loop.

17:25.200 --> 17:33.680
So in some ways, I'm thinking it's a bit of a negative critique on the state of our methods

17:33.680 --> 17:41.640
and research in general, and perhaps even application, that we're so tied to specific

17:41.640 --> 17:42.640
data sets.

17:42.640 --> 17:49.480
You know, certainly having a data set as kind of a lingua franca for comparing results

17:49.480 --> 17:51.320
and all that makes sense.

17:51.320 --> 17:58.480
But there's also a degree to which our methods and our research results are tied to these

17:58.480 --> 18:02.080
specific data sets, kind of the classic overfit on ImageNet.

18:02.080 --> 18:04.640
Do you see that as well in your work?

18:04.640 --> 18:05.640
Yes.

18:05.640 --> 18:09.400
Sam, yeah, you're bringing up an important point.

18:09.400 --> 18:16.680
And this is one that I often, I think, is a very active thread of discussion at brain.

18:16.680 --> 18:23.320
I sense people, well, I sense I've heard two different perspectives on this.

18:23.320 --> 18:29.480
There's a general consensus that, as you described, overfitting the data sets that we have.

18:29.480 --> 18:35.000
And I would say that a few researchers would dispute that entirely or would at least acknowledge

18:35.000 --> 18:41.640
that we've really centered a lot of attention on these three data sets.

18:41.640 --> 18:42.640
You can even go further.

18:42.640 --> 18:47.440
Like, both the fields that I've worked in interpretability and model compression, a lot

18:47.440 --> 18:49.920
of the focus has been on computer vision.

18:49.920 --> 18:57.800
In fact, I would venture that very few papers have talked about different tasks or different

18:57.800 --> 18:58.800
architectures.

18:58.800 --> 19:04.480
Even though there's an urgent need for interpretability beyond just computer vision models.

19:04.480 --> 19:07.840
And I kind of said at the beginning, there's two perspectives to this.

19:07.840 --> 19:14.920
The other perspective is that research is a very particular way of talking about a problem.

19:14.920 --> 19:25.040
And the framework of how discussion occurs in research community has to be, is by design

19:25.040 --> 19:35.560
fairly narrow, mainly because it's a very precise way of advancing a scientific hypothesis

19:35.560 --> 19:37.160
and a contribution.

19:37.160 --> 19:44.080
So it's unclear to me that if someone did deviate from this norm and showed up with a new

19:44.080 --> 19:48.520
paper on a new data set and said, this is a huge improvement.

19:48.520 --> 19:52.920
I think either they would have to benchmark previous methods on a new data set, which

19:52.920 --> 19:59.040
is a large technical contribution, depending on the field.

19:59.040 --> 20:02.640
All I would be left with doubt is to what their contribution actually is.

20:02.640 --> 20:07.280
And so that kind of captures the dilemma that a lot of researchers feel is that they acknowledge

20:07.280 --> 20:10.920
it, but it's unclear how to proceed.

20:10.920 --> 20:18.080
So your particular research, or at least the research that you've spent the most time

20:18.080 --> 20:21.680
on thus far, is around interpretability.

20:21.680 --> 20:25.760
And you're starting to do some model compression work now, but tell us specifically about the

20:25.760 --> 20:29.840
interpretability work that you've been publishing on.

20:29.840 --> 20:36.680
Maybe I'll start by just introducing interpretability and how it's commonly thought of within

20:36.680 --> 20:38.760
research.

20:38.760 --> 20:44.080
And then that will provide some context for the work that I've been doing.

20:44.080 --> 20:46.480
Interpreterability is a very interesting problem.

20:46.480 --> 20:53.040
interpretability broadly, when I ask, for example, Sam, if I asked you, do we want models

20:53.040 --> 20:54.040
to be interpretable?

20:54.040 --> 20:57.160
What would your answer be?

20:57.160 --> 20:59.920
Do we want models to be interpretable?

20:59.920 --> 21:03.440
I think sometimes if it doesn't cost too much.

21:03.440 --> 21:04.440
Excellent.

21:04.440 --> 21:05.440
Interesting.

21:05.440 --> 21:06.440
Yeah.

21:06.440 --> 21:13.200
You captured one of the key misconceptions that exists in the field, which is that all models

21:13.200 --> 21:14.200
must be interpretable.

21:14.200 --> 21:18.280
In fact, there's like new answers there, which is what you're describing.

21:18.280 --> 21:19.280
Yeah.

21:19.280 --> 21:23.680
The starting point when I ask people those questions is that it's a firm yes.

21:23.680 --> 21:28.360
So I like that you've hedged.

21:28.360 --> 21:32.800
And I think that's because we instinctively think of interpretability as this desirable

21:32.800 --> 21:37.240
property, kind of like fairness or bias.

21:37.240 --> 21:42.680
And it's interesting because then the next question is, well, what do you think is an

21:42.680 --> 21:44.000
interpretable model?

21:44.000 --> 21:50.800
And that's generally where there's uncertainty on the part of the person who's answering.

21:50.800 --> 21:57.040
And maybe here I'll say is that interpretability within research has really focused on this

21:57.040 --> 22:03.360
idea for these deep neural networks, where we can't articulate in a compact way what the

22:03.360 --> 22:10.280
function that the model learns is, can we arrive at a methodology to try and explain

22:10.280 --> 22:17.040
the model prediction or perhaps even the model function, which means I can the model function,

22:17.040 --> 22:23.320
like can we understand how the model maps every input to every output.

22:23.320 --> 22:28.440
And you illustrated one of the key misconceptions is that the degree to which we want to do this

22:28.440 --> 22:35.040
and where we really want to invest effort may of fact depend depend upon the task.

22:35.040 --> 22:37.760
So you talked about the burden, well, the burden can be thought of in a few different

22:37.760 --> 22:45.040
ways, but one way is this idea that for some tasks, like if we can't incorrectly explain

22:45.040 --> 22:51.320
how the model rises the decision, the cost on human welfare may be intolerable.

22:51.320 --> 22:57.640
Like examples of this could be, for example, healthcare, where we're using a deep neural

22:57.640 --> 23:00.160
network to revert diagnosis.

23:00.160 --> 23:04.720
And then the doctor has to try and explain that diagnosis to a patient.

23:04.720 --> 23:09.240
If there's an incorrect explanation that's given to the doctor, you can understand it

23:09.240 --> 23:15.000
has perhaps, like we do, as a society, we wouldn't be willing to tolerate that.

23:15.000 --> 23:21.480
However, there may be other tasks where either because the actual cost to humans is seen

23:21.480 --> 23:28.160
to be, I hate to say the words low impact, but at least there's not a significant notions

23:28.160 --> 23:34.320
of decreasing welfare involved, or the task has enough empirical evidence where we're

23:34.320 --> 23:37.960
just more confident in like the overall behavior of the model.

23:37.960 --> 23:41.920
Those are tasks which are currently said we don't need to focus as much as interpretability

23:41.920 --> 23:47.160
because we have a, we have reassurance in different ways that this model is working

23:47.160 --> 23:48.880
the way it is.

23:48.880 --> 23:56.280
The other key challenge about interpretability is that it's very unclear when we've actually

23:56.280 --> 23:59.520
arrived.

23:59.520 --> 24:05.560
And this is really the tricky part is that when do we say this model is interpretable?

24:05.560 --> 24:08.440
We're happy, we signed off, job done.

24:08.440 --> 24:13.600
In fact, it's both hard to measure progress on the task, but interesting enough, the

24:13.600 --> 24:17.200
finish line may look very different for different people.

24:17.200 --> 24:22.680
The burden we carry of delivering a satisfying interpretable explanation may be different

24:22.680 --> 24:26.360
depending on even what our downstream task is.

24:26.360 --> 24:34.520
Like a good example of this is if you're sitting on a plane, and your plane is just delayed,

24:34.520 --> 24:42.480
and the pilot comes on in and says, oh, we have an arrow with one of the engines on the

24:42.480 --> 24:48.360
plane, and then they say that, we're fixing it, we have ground staff.

24:48.360 --> 24:53.360
So as a passenger, you're just sitting in the cheap seats, and like you actually don't

24:53.360 --> 24:58.880
know much beyond the technical error that's being fixed.

24:58.880 --> 25:00.560
The pilot probably knows a lot more.

25:00.560 --> 25:04.040
They've probably been talking to the ground staff, and they have an understanding of at least

25:04.040 --> 25:06.720
like what part of the plane and what would it influence.

25:06.720 --> 25:10.920
And then the ground staff probably has the most technical level of conversation because

25:10.920 --> 25:14.400
they're precisely locating a certain sensor.

25:14.400 --> 25:19.920
And so when the ground staff tells the pilot that it's fixed, that probably involves some

25:19.920 --> 25:23.240
level of technical detail, so the pilot's confident.

25:23.240 --> 25:29.240
When the pilot tells the passengers, it's normally, okay, sold, we're taking off.

25:29.240 --> 25:35.400
And so I think that in a compact way kind of describes that we may need different levels

25:35.400 --> 25:38.400
of explanations, and that is okay.

25:38.400 --> 25:43.360
In fact, like if the pilot were to tell you every single technical detail where the ground

25:43.360 --> 25:49.800
staff told them, we may feel slightly overwhelmed, or at least we would feel like,

25:49.800 --> 25:53.480
oh wow, this is really serious, because they're telling a lot of different job game that

25:53.480 --> 25:58.640
I don't know, and I'm feeling very, very disorientated.

25:58.640 --> 26:06.080
And a lot of interpretability is trying to understand how do we both create a meaningful

26:06.080 --> 26:11.880
explanation, and meaningful here really means given the domain, and given who the person

26:11.880 --> 26:15.120
is, and what they have to do with that information.

26:15.120 --> 26:21.600
But the topic of my research has been that it also should be reliable, and by reliable,

26:21.600 --> 26:31.920
I mean that in creating a meaningful explanation, we shouldn't communicate information that

26:31.920 --> 26:35.600
is not an accurate reflection of what the model has learned.

26:35.600 --> 26:42.000
And this is a delicate area because in some ways the key problem is that with deep neural

26:42.000 --> 26:44.920
networks, we don't know the ground truth to begin with.

26:44.920 --> 26:49.440
So it's hard to say this explanation is better than this one, because we don't know the

26:49.440 --> 26:51.480
true explanation.

26:51.480 --> 26:57.440
And so a lot of what I've worked on for the last year is how do we create frameworks to

26:57.440 --> 27:03.360
measure progress on this task, even in spite of the fact that there's no ground truth?

27:03.360 --> 27:10.920
And setting the stage for us, you pointed out the distinction between, I think you refer

27:10.920 --> 27:17.560
to it as interpreting model function versus model decision.

27:17.560 --> 27:23.880
But you also use the term interpretability and explainability interchangeably, or at

27:23.880 --> 27:25.440
least that was the impression that I got.

27:25.440 --> 27:30.400
I tend to think of like interpretability as that functional, you know, what is really

27:30.400 --> 27:36.000
happening in the model and explainability is, you know, given the model is more or less

27:36.000 --> 27:39.520
black boxy, how do we make some sense of what it's telling us?

27:39.520 --> 27:44.200
Is there a distinction there for you at all, or do you use them interchangeably, but,

27:44.200 --> 27:48.040
you know, in light of acknowledging the two different tasks?

27:48.040 --> 27:56.760
Yeah, really what I meant when I talked about explanations is this idea that a lot of interpretability

27:56.760 --> 28:02.960
so in interpretive research is often explanations is considered a subset.

28:02.960 --> 28:06.280
Explanations are just trying to explain a single example.

28:06.280 --> 28:11.920
So that's how I use that word there is that for an explanation, you have a given input

28:11.920 --> 28:17.360
and you're trying to say for this input, why did the model arrive at this prediction?

28:17.360 --> 28:21.800
Whereas when you're trying to understand the function as a whole, you're trying to understand

28:21.800 --> 28:29.600
perhaps what was most important to the function of the model learned over at least a larger

28:29.600 --> 28:34.560
subset of examples, if not the entire data set.

28:34.560 --> 28:41.800
Most interpretability work for deep neural networks is focused on the single example explanation.

28:41.800 --> 28:49.000
So there often it's an image and you're trying to say, why did the model arrive at this

28:49.000 --> 28:50.880
prediction for this image?

28:50.880 --> 28:56.040
That is being the largest part of the discourse so far within deep neural networks.

28:56.040 --> 29:04.360
And so your work is almost at a meta level, it's, you know, there's a bunch of research

29:04.360 --> 29:11.240
that is happening and needs to happen in interpretability, but how do we compare these

29:11.240 --> 29:17.560
different results without really knowing what the model is doing in the first place?

29:17.560 --> 29:22.200
Yeah, I love how you capture that very succinctly.

29:22.200 --> 29:25.840
Yeah, that's exactly the challenge.

29:25.840 --> 29:29.840
We now have a rich set of methods.

29:29.840 --> 29:37.880
And the question that has to be answered is, okay, I have all these different methodologies

29:37.880 --> 29:41.640
for arriving an explanation, which one do I use?

29:41.640 --> 29:48.200
And so my work is focused on methods that estimate feature importance in a network, and

29:48.200 --> 29:50.440
they estimate imperfection importance.

29:50.440 --> 30:00.160
So the estimators that I have evaluated with my co-authors in both pieces of work that

30:00.160 --> 30:07.400
I did over the last year have asked in this given input image, these estimators will say

30:07.400 --> 30:13.200
this pixel will essentially arrive at a ranking of these and the most important pixels for

30:13.200 --> 30:15.120
the model prediction.

30:15.120 --> 30:23.240
So imagine an image of an ostrich and these estimators will arrive at a different estimate

30:23.240 --> 30:29.760
for how important the pixel of the ostrich knows is to the prediction of the model at the

30:29.760 --> 30:32.160
other end.

30:32.160 --> 30:40.520
And the first piece of work that I did was it essentially started with a premise that

30:40.520 --> 30:46.920
one definition of reliability is that if the model is not affected by a transformation

30:46.920 --> 30:54.320
of the input, then these estimators should not be affected because if the model is unchanged,

30:54.320 --> 31:00.400
we want these estimators to reflect the model.

31:00.400 --> 31:10.200
And to carry out this test, we establish a very narrow ground truth where we created a

31:10.200 --> 31:12.960
transformation to the input, which was a mean shift.

31:12.960 --> 31:17.640
And then by construction, we ensure that the model was not affected.

31:17.640 --> 31:23.000
And so the gradients of the model did not change, the weights of the model will unchanged,

31:23.000 --> 31:27.200
the model was impervious to this change in the input.

31:27.200 --> 31:30.520
But then we showed that many of these estimators changed.

31:30.520 --> 31:33.320
What specifically do we mean by estimator here?

31:33.320 --> 31:37.480
So estimators, I actually use them to change the way with methods.

31:37.480 --> 31:40.400
So think of it as just these set of methods.

31:40.400 --> 31:45.880
And I use them as estimators because I think it's useful to think of some of these tasks

31:45.880 --> 31:49.760
as trying to, because there's no ground truth, really that model is trying to estimate

31:49.760 --> 31:51.640
what's important.

31:51.640 --> 31:55.600
And we're just trying to evaluate whether that estimate was good or not, whether it was

31:55.600 --> 31:57.920
accurate or reliable.

31:57.920 --> 32:06.240
When I think of the description you gave of the method, it calls to mind the Lyme method

32:06.240 --> 32:13.920
of explainability, which also seeks to perturb or inject noise into the inputs and use that

32:13.920 --> 32:20.160
to determine what parts of the input are most relevant.

32:20.160 --> 32:26.720
Can you maybe, for those that are familiar with Lyme, talk about how the two ideas relate?

32:26.720 --> 32:29.720
They relate, it's the same idea.

32:29.720 --> 32:30.720
Yeah.

32:30.720 --> 32:31.720
Okay.

32:31.720 --> 32:34.000
That was an excellent way to anchor the conversation.

32:34.000 --> 32:41.040
Yeah, so when I talk about methods or estimators, Lyme is a key example of an estimate that many

32:41.040 --> 32:47.440
people are familiar with, and other examples of methods or estimators can be things like

32:47.440 --> 32:48.800
just taking the gradient.

32:48.800 --> 32:55.880
So you end up with a gradient heat map or it could be a guide of backdrop or this integrated

32:55.880 --> 33:03.560
gradients, which is another fantastic example of an estimator that is trying to weigh importance

33:03.560 --> 33:05.800
of these input pixels.

33:05.800 --> 33:11.320
But yes, I'm really glad that you interjected there because it's one in the same.

33:11.320 --> 33:16.600
But so Lyme is one of these estimators and it's doing noise injection, but it sounds

33:16.600 --> 33:22.120
like you're essentially taking that same approach, but applying it again at kind of this meta

33:22.120 --> 33:26.120
level to evaluate the estimators.

33:26.120 --> 33:30.280
Well, maybe not quite.

33:30.280 --> 33:37.960
So, Lyme is one of the estimators that could be evaluated, for example.

33:37.960 --> 33:44.000
And I think perhaps if you think about it, so we have something like Lyme, we have these

33:44.000 --> 33:47.960
other methods that are estimating importance.

33:47.960 --> 33:57.760
And in both, in my line of research, essentially what I'm asking is all of the estimates correct

33:57.760 --> 34:09.520
or not, Lyme is very intuitive because I think it's exciting because it gives a clear explanation

34:09.520 --> 34:13.360
of what the model arrived at for that prediction.

34:13.360 --> 34:17.760
But the question I ask is that how do we actually know that what is exciting to us as humans

34:17.760 --> 34:21.160
is also a reliable reflection of the model?

34:21.160 --> 34:27.440
And to do that, the first piece of research I described essentially takes these methods

34:27.440 --> 34:33.880
and says, we've applied a transformation to the input images, which is a means shift.

34:33.880 --> 34:35.640
And the model has not changed.

34:35.640 --> 34:37.240
Do the rankings change?

34:37.240 --> 34:45.720
And if they change, then that is considered a failure in this test case because they

34:45.720 --> 34:49.120
should simply reflect what the model thinks is important.

34:49.120 --> 34:54.520
And if they diverge from that, then it's unreliable.

34:54.520 --> 35:00.400
The second research that I did was one step further, and again, this idea of how do we

35:00.400 --> 35:06.840
measure progress on this task of reliably estimating importance?

35:06.840 --> 35:15.600
And what we did there was we said, OK, these different methods all rank the pixels.

35:15.600 --> 35:20.240
Let's remove the pixels that they rank as most important a fraction.

35:20.240 --> 35:27.440
And then give those examples of these modified inputs back to the model.

35:27.440 --> 35:32.920
And the presumption is that if the methods actually identified what was important, if they

35:32.920 --> 35:38.200
actually rank these pixels, then it should really damage the model if you're training

35:38.200 --> 35:40.120
it again.

35:40.120 --> 35:44.680
And we compared it to a random removal of pixels.

35:44.680 --> 35:50.520
So really what they were asking, if you randomly remove pixels, essentially you're randomly

35:50.520 --> 35:53.120
assigning importance to the inputs.

35:53.120 --> 35:59.000
And if these methods don't damage the model more than just a random removal, then in some

35:59.000 --> 36:03.320
ways, they're less able to accurately rank than just random removal.

36:03.320 --> 36:05.600
So that was a very interesting piece of research.

36:05.600 --> 36:11.320
And it's kind of again, again, this idea that in the absence of any ground truth, what

36:11.320 --> 36:17.520
we need to do in this field is kind of stay precisely these desirable properties and

36:17.520 --> 36:22.880
come articulate frameworks in which to measure these, because then at least we haven't

36:22.880 --> 36:31.760
established, we've articulated a way of saying we are measuring it in this sense and given

36:31.760 --> 36:37.320
this framework, this method appears to be more accurate or reliable.

36:37.320 --> 36:46.360
And so was the end result of these two research efforts, a kind of a ranking of these different

36:46.360 --> 36:47.920
estimators.

36:47.920 --> 36:52.120
And if so, what did you tend to find?

36:52.120 --> 36:59.720
But also is ranking these estimators a one-dimensional thing or are there multiple ways that we should

36:59.720 --> 37:02.000
be comparing these different methods?

37:02.000 --> 37:03.000
Yeah.

37:03.000 --> 37:11.360
And you raise an important point, so to kind of share the results, on both we found very

37:11.360 --> 37:12.360
interesting results.

37:12.360 --> 37:20.040
And in fact, kind of an advance, at least for me, I felt taught me new things about how

37:20.040 --> 37:24.600
the model actually arrives at feature importance.

37:24.600 --> 37:32.840
So I'm both, it was a little bit of a grumpy picture.

37:32.840 --> 37:41.920
We found that in the most recent work, which is a paper called remove and retrain where

37:41.920 --> 37:47.320
you essentially are moving the most important and retraining the model, we found that the

37:47.320 --> 37:51.120
estimates that we consider, and we only consider a small subset.

37:51.120 --> 37:56.720
So I'll caveat with that, they didn't perform better than a random assignment of importance.

37:56.720 --> 38:06.600
But we did find this, which is fascinating, which is that if you ensemble, so if you take

38:06.600 --> 38:14.120
for the same image, you take various noisy estimates, which means that you add noise to

38:14.120 --> 38:18.360
the estimate and then do that repeatedly.

38:18.360 --> 38:20.920
And you combine with squaring.

38:20.920 --> 38:25.040
So these are two separate transformations applied to the estimate.

38:25.040 --> 38:32.800
So imagine this for Lyme, like essentially you'd be taking noisy Lyme estimates, squaring.

38:32.800 --> 38:39.040
We found that that far outperformed a random estimate of importance.

38:39.040 --> 38:44.880
And so that's kind of interesting, aha moment, because it's not quite aha moment, because

38:44.880 --> 38:49.040
it's not the theoretical justification yet for why that is the case.

38:49.040 --> 38:54.480
But it suggests firstly, the importance of empirical research, which is an option you

38:54.480 --> 39:01.800
discover on interesting things that motivate research in a new direction, but also tells

39:01.800 --> 39:09.480
us that we, in some ways, how we're considering these estimates now, there's very exciting

39:09.480 --> 39:15.120
things that we can do that will improve progress on a task.

39:15.120 --> 39:20.800
And then you ask, like your second question is critical, because you say, is this one

39:20.800 --> 39:21.800
dimension?

39:21.800 --> 39:22.800
Absolutely.

39:22.800 --> 39:28.960
Like, this work should be considered one dimension, and in fact, like, there's almost,

39:28.960 --> 39:34.560
even if we were to narrow it to reliability and accuracy, which is really what this

39:34.560 --> 39:38.880
work is trying to get at, there could be other dimensions for measuring this.

39:38.880 --> 39:44.280
Because essentially what we've done is we've just stated, this is our way of technically

39:44.280 --> 39:51.600
defining these properties, and given our definition, this is what we think are the best methods.

39:51.600 --> 39:58.400
Someone else would come along and say, there's another way to define technical reliability,

39:58.400 --> 40:04.040
and given this, this is what we find, that's the nature of this field.

40:04.040 --> 40:05.440
And I would encourage that.

40:05.440 --> 40:09.960
I think it's important for researchers and for the community as a whole to think about,

40:09.960 --> 40:20.040
how do we measure, how do we ensure that we are actually delivering an explanation that

40:20.040 --> 40:24.600
people can trust that is a reflection of the model?

40:24.600 --> 40:30.320
I will say one more thought on that, and then I'll pause, which is that you can also

40:30.320 --> 40:36.040
think about a whole other set of soluble properties that are on the being meaningful

40:36.040 --> 40:37.640
to human side.

40:37.640 --> 40:44.640
So often how that's measured now in the body of research is that user studies are conducted,

40:44.640 --> 40:49.720
and things like, do you trust this, are asked?

40:49.720 --> 41:00.320
And that in itself, I sense, could also benefit from a more, well, both larger scale treatments,

41:00.320 --> 41:05.760
but also much more nuance, like, is for this task, for this person, does this make sense?

41:05.760 --> 41:10.600
Because, inevitably, interpretability is going to evolve on a few different levels.

41:10.600 --> 41:17.080
Like, based on our perception as a society of what do we need for us to trust, technical

41:17.080 --> 41:23.800
implementations of deep neural networks, as well as in a job-specific way, which is that,

41:23.800 --> 41:27.840
you know, a doctor is probably going to always want an explanation of every example.

41:27.840 --> 41:32.280
It's always going to be at the single example level.

41:32.280 --> 41:37.680
But as a researcher, for example, what's more important to me, I would suggest as an interpretability

41:37.680 --> 41:42.520
tool, is that I want to get a sense of the distribution, and I want to understand, out

41:42.520 --> 41:51.000
of the distribution of data, what data points are perhaps more, either, problematic, and

41:51.000 --> 41:52.920
you can define problematic in a few ways.

41:52.920 --> 41:58.360
But if I have a better way to understand what is different in a data set, that's helpful

41:58.360 --> 42:02.080
for me in understanding, am I comfortable to deploy this model?

42:02.080 --> 42:06.640
So all these vantage points will require different desirable properties, but I'll stop

42:06.640 --> 42:07.640
them.

42:07.640 --> 42:15.240
One question that I did have was in the evaluating feature importance estimates paper,

42:15.240 --> 42:21.680
you look at, as you mentioned, a subset of these different estimator types, gradient

42:21.680 --> 42:28.160
heat map, integrated gradients and some others, is lime a subset of one of these?

42:28.160 --> 42:34.040
Like are you using a generic term of these that you include that lime falls under, or

42:34.040 --> 42:37.960
did you just not look at lime particularly?

42:37.960 --> 42:44.480
We did not look at lime, and I would be really excited to see the results online.

42:44.480 --> 42:51.640
The constraint on these papers is always this tension between, we have limited time and

42:51.640 --> 42:56.800
certain amount of computational resources, but partly, I think what this motivates more

42:56.800 --> 43:02.960
than anything is open sourcing code, which is something we're doing for this paper.

43:02.960 --> 43:08.440
And that's important because really if this is going to be a sustainable benchmark, researchers

43:08.440 --> 43:15.160
have to be able to self-serve and also be able to take what we did and apply to that method.

43:15.160 --> 43:20.480
But I would be really interested to see the results of this for lime, mainly because lime

43:20.480 --> 43:27.920
imposes contiguous feature importance by that. I mean that let me frame this using a

43:27.920 --> 43:29.040
counting example.

43:29.040 --> 43:34.200
So gradient heat maps, for example, don't require that feature importance are connected

43:34.200 --> 43:35.200
set of pixels.

43:35.200 --> 43:44.680
You just take the gradient of the pre-soft mix activation for the model prediction with

43:44.680 --> 43:47.320
respect to the input and then you rank.

43:47.320 --> 43:54.400
But that tends to result in very diffuse attribution, meaning that importance when you rank it

43:54.400 --> 44:02.640
doesn't concentrate exclusively on a set of connected pixels, whereas lime imposes this

44:02.640 --> 44:11.640
constraint by its methodology that importance is restricted to the connected pixels.

44:11.640 --> 44:18.600
Humans tend to, and I say this hesitantly because this is not a research conclusion, this

44:18.600 --> 44:25.640
is hypothesis, but I sense humans tend to like connected pixels as more what they perceive

44:25.640 --> 44:27.520
to be interpretable.

44:27.520 --> 44:33.760
And so I would be excited to at least benchmark lime on another example of a method that

44:33.760 --> 44:39.360
does assist feature importance in this way using remove and retrain, because it may have

44:39.360 --> 44:42.240
very different results.

44:42.240 --> 44:48.600
Or at least tell us something about how connected importance versus this very few importance

44:48.600 --> 44:55.000
actually is related to the reliability of the method.

44:55.000 --> 44:56.000
Interesting.

44:56.000 --> 45:04.480
It strikes me that from that perspective, the idea that humans prefer these connected

45:04.480 --> 45:13.280
areas versus diffuse areas, almost says that humans want explainability that makes sense

45:13.280 --> 45:18.800
to them more than interpretability that might be more functionally accurate.

45:18.800 --> 45:29.160
Yeah, I hesitate to say, we're just talking here, we're just throwing stuff out there.

45:29.160 --> 45:37.160
But I will say the most, how researchers articulate this in papers is quite funny.

45:37.160 --> 45:41.400
So this comes up, no one quite gets at this explicitly, because again, it's not something

45:41.400 --> 45:46.760
that's exactly testable, and even in my current research, I don't benchmark any of these

45:46.760 --> 45:53.640
connected methods, so I really hesitate to venture, but what I will talk about this frustration

45:53.640 --> 45:58.280
with diffuse methods, because that's kind of a coherent pattern in these papers.

45:58.280 --> 46:07.800
Researches will say that the method is perceived to be visually noisy, and that's interesting,

46:07.800 --> 46:08.800
right?

46:08.800 --> 46:14.160
Because that actually says nothing about whether the method is accurate or not for gradients.

46:14.160 --> 46:19.360
It just speaks to our reluctance to, either it's hard to discern what the explanation

46:19.360 --> 46:24.640
is saying, what's actually important, this image of going to gradients, and that might

46:24.640 --> 46:35.720
be because as humans, we may not know what to do with the complexity, again, here I'm

46:35.720 --> 46:42.160
using another word that is very subjective, like visual noise, the complexity of the explanation.

46:42.160 --> 46:49.680
Whereas if something is connected and placed in a specific area, at least I sense it may

46:49.680 --> 46:53.800
be more reassuring, but again, I think we're both throwing things out there, and that's

46:53.800 --> 46:59.920
what makes this domain so challenging, is that a lot of it is subjective, a lot of it

46:59.920 --> 47:08.280
is inherently like, my preference may differ in fact from your preference, and that's

47:08.280 --> 47:09.800
why there's no clear finish line.

47:09.800 --> 47:16.760
And did you find that the specific approach to ensembling, you mentioned squaring the estimate,

47:16.760 --> 47:22.680
I'm not sure, well, you can elaborate on that, but the question is, did it apply equally

47:22.680 --> 47:28.280
to all of the methods you looked at, gradient, heat map, integrated gradients, smooth gradients,

47:28.280 --> 47:37.240
grid gradient, etc., or are there specific formulas or formulations that apply to individual

47:37.240 --> 47:38.240
methods?

47:38.240 --> 47:42.800
Well, this is the cool thing, is that it benefited all the estimators, all the methods

47:42.800 --> 47:47.680
that we considered, and I apologize for, I keep on interchangeably using those words

47:47.680 --> 47:57.640
so yeah, it really dramatically, I would say, improve the accuracy of the three estimators

47:57.640 --> 48:06.480
that we considered, and what, what it, when I say it, like it was firstly adding noise

48:06.480 --> 48:13.720
to the images, so for a single image, you would create a set of 15 noisy images, and then

48:13.720 --> 48:20.040
you would arrive at 15 different predictions, and given those 15 different predictions,

48:20.040 --> 48:25.280
you would take the estimate for the methods you're considering, and you would square it,

48:25.280 --> 48:29.240
and then you would average those, so you left with one estimate, and again, from these

48:29.240 --> 48:35.560
15 different images, and again, an estimate, in this case, is,

48:35.560 --> 48:46.120
lie, example, but the actual estimate is a, like a per pixel probability, for example,

48:46.120 --> 48:49.880
or something on a per pixel basis, right?

48:49.880 --> 48:55.400
It's on a per pixel basis, it's often, in the case of gradients, it's not capped at a

48:55.400 --> 49:00.760
certain, it's just the magnitude can be as important, so it's not probability, but other

49:00.760 --> 49:06.920
methods will cap it, so it's cumulative, so the sum of all the estimates and importance

49:06.920 --> 49:13.560
should sum to one, that's a property called completeness, but yes, so you're correct,

49:13.560 --> 49:20.080
it's on a per pixel level, so you're averaging across, however many noisy estimates you

49:20.080 --> 49:24.760
have to arrive at a single estimate for a given picture.

49:24.760 --> 49:30.440
So that's your interpretability research, if you have a few more minutes, I'm curious

49:30.440 --> 49:38.280
about some of the model compression work that you're doing, and that ties into the event

49:38.280 --> 49:42.560
that's kind of brought us together, which is the deep learning in Dava, which is coming

49:42.560 --> 49:50.760
up in South Africa, and you, you know, we're well beyond our typical background segment

49:50.760 --> 49:53.760
here, but you grew up in...

49:53.760 --> 50:01.240
I'm at a similar reaction when you said that, I'm like, oh wow, this is introducing

50:01.240 --> 50:13.920
some context later in the game, but it's, yeah, I grew up in Africa, and mostly in Southern

50:13.920 --> 50:21.840
Africa, so I grew up in Mozambique, South Africa, in the Soutu, Swaziland, Kenya, my family

50:21.840 --> 50:30.160
just moved to West Africa, they just moved to Monrovia, Liberia, so I, then Dava, so

50:30.160 --> 50:38.520
then Dava's coming up, you mentioned, but also Google is opening this AI lab in Acra,

50:38.520 --> 50:44.560
and both are very exciting, because so much of it, I think, is quite exciting personally

50:44.560 --> 50:52.240
for me, as a way to, well, in Dava, I would say, is very much directly doing this, and

50:52.240 --> 50:57.960
Dava, the motivation is, let's build technical capacity, and that's the most important

50:57.960 --> 51:03.320
determinant, I kind of mentioned this when I talked about what made this year possible,

51:03.320 --> 51:09.080
it's contact with very experienced researchers, and just the ability to collaborate, and I

51:09.080 --> 51:13.840
think that Dava is really in the same vein, like you're bringing these researchers from

51:13.840 --> 51:19.240
all over the world, and all these students, and also practitioners from all over Africa

51:19.240 --> 51:23.920
to the same place, and wow, is that exciting?

51:23.920 --> 51:29.120
I don't need to, I'll be too exuberant about it, but I think, I recently went to Data

51:29.120 --> 51:42.760
Science Africa in Kenya, and the energy level is insane, mainly because we, right now,

51:42.760 --> 51:49.480
a technical talent, I would say, is very correlated with geographic location, so places like San

51:49.480 --> 51:57.240
Francisco, New York, Paris, a handful of other geographies have a lot of, just, I'd

51:57.240 --> 52:03.080
say, experience, and as soon as you leave one of these cities, you realize it's a clip

52:03.080 --> 52:09.520
hanger, like the ability to have a community to learn and grow in the places where I grew

52:09.520 --> 52:16.520
up is extremely limited, and so for students who attend these gatherings, it's just really,

52:16.520 --> 52:21.960
they're so excited because this is a way for them to connect, many times what they've

52:21.960 --> 52:28.920
been studying by themselves online, with people who are actually doing this and practicing

52:28.920 --> 52:34.400
in the field or doing research, so that's why it's so exciting, I forget the second part

52:34.400 --> 52:40.880
of your question, but maybe, yeah, maybe on points there.

52:40.880 --> 52:49.680
Well, that provides some interesting context for your move to the Accra Office and your

52:49.680 --> 52:54.600
work in model compression, I think that was the tie-in that I wanted you to elaborate

52:54.600 --> 52:55.600
on.

52:55.600 --> 53:06.920
Well, the Accra Office, so most of us, these days, is leading the Google AI Accra Office,

53:06.920 --> 53:07.920
and he's amazing.

53:07.920 --> 53:15.920
He's one of my, both mentors and collaborators here at Brain, and the Google Accra Office

53:15.920 --> 53:22.160
is like any other Brain Office, so the goal is to go and have, attract the best researchers

53:22.160 --> 53:28.480
and do research, so very much in the same vein of a lot of the things I've described today.

53:28.480 --> 53:33.880
I think what most of us, I sense would agree with, is that this is other ideas, that by

53:33.880 --> 53:43.680
placing in researchers in different geographies and in different environments, you have often

53:43.680 --> 53:50.800
very novel approaches to ideas, and that is both because the resource constraints that

53:50.800 --> 53:58.240
you find may be different, as well as because simply a lot of researchers who you work with

53:58.240 --> 54:04.240
and your day-to-day conversations, and you may find novel directions because of that.

54:04.240 --> 54:10.520
The other hope for this Accra Office is that having researchers there may provide important

54:10.520 --> 54:17.120
externalities for the ecosystem as a whole, and Mr. Fah has also really championed this

54:17.120 --> 54:22.840
master's program, which will, is also starting this year, and that's supported by Facebook

54:22.840 --> 54:29.520
and Google, I believe, but how that relates to monocompression, it doesn't necessarily

54:29.520 --> 54:35.800
have to, but it happens to, for my current research, which is also with Mr. Fah and Eric

54:35.800 --> 54:44.920
and another Collaborate Trevor at Brain, and there, what I'm interested in, is this idea

54:44.920 --> 54:51.440
that really, the starting point for this research is, why do we need such large models?

54:51.440 --> 54:59.000
Do you know on networks that are notorious for the amount of parameters that need it,

54:59.000 --> 55:05.960
and also the tendency for the number of parameters to grow year-over-year in the body of research?

55:05.960 --> 55:10.680
But I think it's interesting if we impose different constraints, like, for example, if we

55:10.680 --> 55:16.560
actually think about deployment in very resource-constrained environments, such as mobile phones, which

55:16.560 --> 55:24.000
a lot of parts of Africa have jumped directly from neither having a laptop or mobile phone

55:24.000 --> 55:26.240
to just jumping to a mobile phone.

55:26.240 --> 55:34.320
So if we think about that, that imposes very different resource boundaries than what

55:34.320 --> 55:41.480
we're currently used to, and often, like, thinking about that drives interesting ways of attacking

55:41.480 --> 55:42.480
the same problem.

55:42.480 --> 55:48.080
There's a great example of this, and how engineers at Google, and I believe what's that Facebook

55:48.080 --> 55:55.320
have tackled, how do they make engineering products, which are better suited to low bandwidth

55:55.320 --> 55:57.120
or limited connectivity environments?

55:57.120 --> 56:02.400
And the solution was one of the solutions that we experienced and is highly visible as

56:02.400 --> 56:07.640
this idea that there's an entirely separate internet connection that you can connect to

56:07.640 --> 56:13.800
that will give you the experience of being in an impaired bandwidth environment.

56:13.800 --> 56:19.960
And this was actually like a starling catalyst for a lot of engineering innovation around

56:19.960 --> 56:22.080
these problems.

56:22.080 --> 56:30.600
Again, like, this is my interest in model compression, I sense could have been equally

56:30.600 --> 56:38.480
pursued in Manavue at Google Brain headquarters or in the new office in Accra.

56:38.480 --> 56:46.000
But it's exciting to sense, like, energize, like, what I'm thinking about by also connecting

56:46.000 --> 56:52.800
with people who are experiencing the pains of trying to deploy models.

56:52.800 --> 56:57.240
And that's one of the most frequent questions that you get from students when you go

56:57.240 --> 57:02.480
to teach in a place like Nairobi, is that a lot of people have an idea, have tried to

57:02.480 --> 57:08.320
implement it, and now trying to deploy it using something like T.F. Light and now experiencing

57:08.320 --> 57:10.520
severe pain points.

57:10.520 --> 57:13.840
That's exciting for me at least.

57:13.840 --> 57:19.440
My research direction is specifically on this question of how do we effectively remove

57:19.440 --> 57:23.560
weights from a model while preserving accuracy?

57:23.560 --> 57:28.720
There's a few different threads to how people tackle this problem.

57:28.720 --> 57:33.880
You may have to rein me in again if we get started, so I'll pause.

57:33.880 --> 57:37.400
I don't know if there were any immediate questions.

57:37.400 --> 57:44.040
It sounds like from our earlier chat that you can't go into a ton of detail about your research

57:44.040 --> 57:48.560
because it hasn't been published yet, but if you could give us an overview of kind of

57:48.560 --> 57:55.880
the landscape that you're playing in as we wrap up, that would be great.

57:55.880 --> 58:03.040
I'll mention probably four key directions that this problem has been thought about, and

58:03.040 --> 58:07.960
the other defining characteristic of this field of research is that I sense it's been

58:07.960 --> 58:08.960
underserved.

58:08.960 --> 58:17.760
There's being very little, it's a rather new field of research.

58:17.760 --> 58:23.120
But some of those solutions are actually very old.

58:23.120 --> 58:31.960
I know that was an odd caveat, but I'll give you more context for what I mean.

58:31.960 --> 58:36.760
The key approaches to this problem are that you have things like pruning, and pruning

58:36.760 --> 58:42.000
says, let's reduce the number of weights in a network.

58:42.000 --> 58:47.200
You either do that over the course of training by setting certain weights to zero or regularizing

58:47.200 --> 58:52.080
the certain weights become very close to zero, or at the end of training by saying, this

58:52.080 --> 58:58.760
is our model, and now we're going to try and arrive at a much smaller model.

58:58.760 --> 59:06.960
The metric that's been optimized for is a normally level of sparsity given a certain degree

59:06.960 --> 59:08.480
of accuracy.

59:08.480 --> 59:13.600
The second approach, which has been enormously successful, and in some ways has taught

59:13.600 --> 59:18.720
us a lot about deep neural networks in general, is quantization.

59:18.720 --> 59:24.080
Quantization refers to this idea of you have a certain level of precision and the weight

59:24.080 --> 59:32.400
of themselves, and you can essentially take a floating point weight, and you can reduce

59:32.400 --> 59:38.400
the number of bits, and then you can still have a remarkable level of accuracy.

59:38.400 --> 59:46.960
So this takes, like most often, a trained model, and then changes the representation of

59:46.960 --> 59:48.760
the weights.

59:48.760 --> 59:56.040
And this has huge implications for memory, so often you're able to really improve memory

59:56.040 --> 01:00:02.280
of the models, the memory needed to store the models by simply changing how the weights

01:00:02.280 --> 01:00:03.280
represented.

01:00:03.280 --> 01:00:13.000
This is third direction, which is model distillation, so this is also interesting enough in present

01:00:13.000 --> 01:00:19.400
as an interpretability direction, but you have this teacher model, which is your massive

01:00:19.400 --> 01:00:25.800
deep neural network, and then you're trying to train a student network to have the same

01:00:25.800 --> 01:00:34.240
accuracy as a teacher network with a fewer amount of parameters, and all these are quite

01:00:34.240 --> 01:00:35.240
exciting.

01:00:35.240 --> 01:00:41.480
The fourth, which I think is probably the most underserved, is this idea of trying to

01:00:41.480 --> 01:00:48.240
do things that are optimized for the actual hardware.

01:00:48.240 --> 01:00:53.760
And this is tricky, because this is a nice loop back to how we began this conversation,

01:00:53.760 --> 01:01:01.520
but it's hard to pursue a research that is optimized to hardware, because hardware tends

01:01:01.520 --> 01:01:03.680
to be non-standard.

01:01:03.680 --> 01:01:12.360
And so think about a TPU, which is made by Google, is one of the first hardware that's directly

01:01:12.360 --> 01:01:16.040
has in mind deep learning, and then a GPU made by Nvidia.

01:01:16.040 --> 01:01:18.640
But even when you think about cell phones, it's non-standard.

01:01:18.640 --> 01:01:23.560
And so this research, in some ways, the constraint is that you want to make it generalizable enough

01:01:23.560 --> 01:01:29.760
but you still want to make significant inroads into how it's being deployed.

01:01:29.760 --> 01:01:38.240
But I started this kind of framework by saying, this odd disconnect, I said, oh, in some

01:01:38.240 --> 01:01:42.760
ways this is a very new field of research, and other ways it's not, I'm thinking specifically

01:01:42.760 --> 01:01:43.760
of pruning.

01:01:43.760 --> 01:01:47.520
So pruning has actually been around since the 1990s.

01:01:47.520 --> 01:01:52.680
So pruning was the first field I mentioned where you're trying to remove weights and arrive

01:01:52.680 --> 01:01:54.560
in a smaller model.

01:01:54.560 --> 01:01:57.640
And there was the first paper came out in the 1990s.

01:01:57.640 --> 01:02:04.640
It was initially called double back prop, but the current proposal in the form of optimal

01:02:04.640 --> 01:02:08.000
brain damage, which I think is a great name for a paper.

01:02:08.000 --> 01:02:14.240
It certainly has a ring to it, but he proposed one of the first methods.

01:02:14.240 --> 01:02:18.400
And so some of these approaches have been around for a while.

01:02:18.400 --> 01:02:23.440
It's just that there's a new level of attention, both because there's a sense that now the

01:02:23.440 --> 01:02:26.520
resource constraint is firm.

01:02:26.520 --> 01:02:31.720
There's a lot of discussion around Moore's Law and how we can't quite get the hardware

01:02:31.720 --> 01:02:34.760
to just catch up with whatever researchers want to do.

01:02:34.760 --> 01:02:41.720
And then, instead, we must think of interesting ways for researchers to meet the hardware.

01:02:41.720 --> 01:02:49.560
And that's not an easy feat, because hardware and research has tended to be very siloed

01:02:49.560 --> 01:02:51.400
in both directions.

01:02:51.400 --> 01:02:56.040
So that's what makes this particularly, like, perhaps like a very interesting time for

01:02:56.040 --> 01:02:59.440
a lot of the enthusiasm around the subfield.

01:02:59.440 --> 01:03:05.640
I think we could launch into another hour long conversation about this topic.

01:03:05.640 --> 01:03:11.680
I didn't want you to have to ring me in, so.

01:03:11.680 --> 01:03:16.960
But we will have to find a time and place to reconvene on that one.

01:03:16.960 --> 01:03:21.920
But Sarah, thank you so much for taking the time to chat with us.

01:03:21.920 --> 01:03:28.520
It's been a pleasure to have you on the show and to learn about what you're doing across

01:03:28.520 --> 01:03:30.760
all the various things that you're working on.

01:03:30.760 --> 01:03:31.920
Thank you so much.

01:03:31.920 --> 01:03:35.200
It was really fun chatting with you, Sam.

01:03:35.200 --> 01:03:42.000
And oh, I think Sam had mentioned offline, which I'll repeat here for social peer pressure,

01:03:42.000 --> 01:03:46.600
but he mentioned that he would come visit the Ghana Acro office.

01:03:46.600 --> 01:03:49.160
So I plan to hold you to that.

01:03:49.160 --> 01:03:53.880
Let's do it, for sure.

01:03:53.880 --> 01:04:02.760
And you mentioned earlier, Mustafa Sisay, who is heading up that office for folks who didn't

01:04:02.760 --> 01:04:03.760
catch it.

01:04:03.760 --> 01:04:10.760
I interviewed him when he was at Facebook AI Research, and we will drop a link to that

01:04:10.760 --> 01:04:12.560
show in the show notes.

01:04:12.560 --> 01:04:14.920
But once again, Sarah, thanks so much.

01:04:14.920 --> 01:04:19.680
Yeah, thank you.

01:04:19.680 --> 01:04:20.680
All right, everyone.

01:04:20.680 --> 01:04:22.360
That's our show for today.

01:04:22.360 --> 01:04:28.240
For more information on Sarah or any of the topics covered in this show, visit twimlai.com

01:04:28.240 --> 01:04:31.360
slash talk slash 189.

01:04:31.360 --> 01:04:36.920
To follow the entire deep learning and double podcast series, visit twimlai.com slash

01:04:36.920 --> 01:04:39.320
endaba 2018.

01:04:39.320 --> 01:04:42.400
Thanks again to Google for their sponsorship of this series.

01:04:42.400 --> 01:04:49.520
Be sure to check out the 2019 AI residency program at g.co slash AI residency.

01:04:49.520 --> 01:05:01.280
As always, thanks so much for listening, and catch you next time.

