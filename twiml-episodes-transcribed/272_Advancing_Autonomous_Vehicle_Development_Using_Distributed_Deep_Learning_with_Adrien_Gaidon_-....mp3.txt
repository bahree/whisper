Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
If you missed our last show and if you did you definitely want to go check it out because
it was a great conversation.
But if you missed that show you missed the first of the many exciting updates we have for
you this summer.
Last time we announced Twimble's third birthday and our 5 millionth download which happened
right around the same time.
To help us celebrate this occasion and to request your commemorative Twimble birthday sticker
visit TwimbleAI.com slash birthday 3.
This week we're continuing the action by kicking off volume 2 of our AI platform series.
You recall that last fall we brought you AI platform's volume 1 featuring conversations
with platform builders from Facebook, Airbnb, LinkedIn, OpenAI, Shell and Comcast.
This series turned out to be one of our most popular series of shows ever and over 1000
of you downloaded our first ebook on machine learning platforms, Kubernetes for machine
learning, deep learning and AI.
Well we'll be back at it over the next few weeks sharing more experiences from teams
working to scale and industrialize data science and machine learning at their companies.
And we've got even more in store on this topic so if it's an area you're interested
in be sure to stay tuned.
You can follow along with the series at twimbleai.com slash AI platforms 2 and by following us on
Twitter at at Sam Charrington and at Twimble AI.
Before we dive in, I'd like to send a giant thanks to our friends over at Sigopt.
They've been huge supporters of my work in this area and I'm really excited to have them
as a sponsor of this series of shows on machine learning and AI platforms.
If you don't know Sigopt, I spoke with their CEO Scott Clark back on show number 50.
Their software is used by enterprise teams to standardize and scale machine learning
experimentation and optimization across any combination of modeling frameworks, libraries,
computing infrastructure and environment.
Teams like 2 Sigma who will hear from later in this series rely on Sigopt software to
realize better modeling results much faster than previously possible.
Of course to fully grasp the potential of a tool like Sigopt is best to try it yourself.
That's why Sigopt is offering you the twimble community an exclusive opportunity to try
their product on some of your toughest modeling problems for free.
To take advantage of this offer, visit twimbleai.com slash Sigopt.
All right everyone, I am here with Adrian Gaden.
Adrian is a machine learning lead at Toyota Research Institute.
Adrian, welcome to this week in machine learning and AI.
I'm super happy to be here.
Thank you for inviting time.
So we are here in Las Vegas at the AWS re-invent conference where you gave a talk and we
will dig into the topic of your talk, which was about advancing autonomous vehicle development
using distributed deep learning.
But before we do that, I'd like to hear a little bit about your background.
How do you get into machine learning?
Yeah, absolutely.
So I've been doing deep learning and machine learning for more than 10 years now.
I was really interested initially in human learning, human psychology, but I also really
like computers and building stuff, so machine learning and AI kind of was like a natural
match made in heaven.
And so I started doing a double major in computer science and math and at the same time looking
into AI more.
And then I did an internship at Inria in the very one on group from a Codidia Schmidt
and Computer Vision, where I participated to some competitions like the ancestors of
ImageNet, so Pascal VOC, Visual Object Challenges, which I won in 2008.
And then I continued the PhD that was with Microsoft Research in Inria, I joined Center in
Paris where I was working on video understanding, more specifically human action recognition.
And after that, I joined XRC, XRC Center Europe, and you actually interviewed a friend of mine
in my former boss, Nile Murray, and computer vision team there, great episode, by the way.
And so I joined them as a research scientist and worked in video analysis in general, so
I started that research effort there.
At the same time, deep learning emerged, so that's when I really transitioned from principled
convex optimization and kernel methods into the alchemy of deep learning and never looked
back since.
And are we saying alchemy just in celebration of the fact that Neurops is next week?
Yeah, that's a good one.
And yeah, and from then on, I did a lot of work on tracking and especially domain adaptation.
And because we didn't have a lot of data, I had to make my own, so I started looking
into simulation a lot.
Game engines to generate data.
And I did a couple of CUPR papers on the topic that were noticed by the industry at large
nut and mistriving, which at the time was like really getting into simulation.
And that's how I joined TRI, because I really dedicated to simulation and very, very large
scale.
These Brahms only happened at a large scale.
If you have just small needs, like Robotaxi, et cetera, you can just label data, but at
a very, very large scale and like Toyota's number one car maker in the world, 100 million
cars on the road today, you need to think about these problems.
And that's what gets me excited as a machinery in person, because it's all about generalization.
And when you think about worldwide, like Japan, Australia, US everywhere, it has to work.
And that's what's really cool, because you both have to invent new things in the research,
but you also have to make it work.
And you get to touch on all these things.
So that's how I got into it.
And I got really hooked into Robotax, based in general, and at the time it was driving in
particular, because it's such a great application for machine learning.
Okay.
Before we get too deep into what you spoke about here, what's the focus that TRI in general,
and then your focus there?
Yeah, absolutely.
So TRI was created almost like three years ago now.
It's basically a separate company that was created by Toyota with $1 billion funding initially,
and we got $2.8 billion more, and it's been up a new company called TRI AD Advanced
Development recently.
Our focus is really, like we're a Robotax company.
And our focus is really about autonomous driving, home robots, and we also do some material
science research for designing better batteries and things like this.
But most of our efforts is really in driving.
And the team I lead in machine learning is really about research for autonomous driving.
We do also things for Robotax a little bit, because from our perspective, a car is a robot.
As a sensory motor loop, essentially, you have perception, prediction, planning, decision
action, and these feedback loops from the real world, which is what exciting is a physical
system.
And TRI really has a mission to improve quality of life in general.
I know it sounds very Silicon Valley, but in that case, it's actually true, because we
have already hundreds of millions of users.
And so the goal is one is the project called Guardian, which is to make a car that can
crash.
So it's the ultimate driver assistance system.
Another one is Schofr, which is the real autonomous car, like not the ones we're talking
about today, but the long-term, the long game, which is real autonomy, like these cars
that can drive themselves completely, autonomously, everywhere, all the time, which obviously is
not going to happen tomorrow.
We talk a lot about that one today.
We talk, yeah.
So this is, but that's, it depends on the product, right?
What people think and hear to your eyes thinking really about the long-term thing.
The cool thing is that these two, Guardian and Schofr, in terms of the machine learning
side of things, they have a huge intersection.
You still need a semantics segmentation, object detection, tracking, a lot of the algorithms
that we're talking about in computer vision are actually completely in common, almost completely
in common.
So from the perspective of my research, I don't make a difference necessarily between
these products, because most of the research I do is very well aligned with those purposes.
And then we also do home robotics, so we have really, really good teams there, XNAS, JPL,
et cetera, where they work on mobile manipulation platforms, so that to assist the elderly for
home care and these kind of things.
And does Toyota have products in market in these, in the home robotics space?
So actually, Toyota manufactures the robot that's called the HSR, the human support robot.
That was, I think, the official platform for the Robocop recently.
So Toyota is really big in robotics.
Robocop.
I was thinking Robocop.
Robocop with a U, pardon my French.
No.
So, yeah, so the goal is basically, how do we transform Toyota into a robotics company?
Yeah.
They have this amazing industrial robotics side, of course, but really, what is the future
of cars?
It's going to be Robocars, but it's also going to be robots beyond cars.
And also how they become a software company and actually a machine learning company.
That's really what's exciting.
Because at this scale of a company that they want to change and see your Toyota, I was
really talking about like, you know, the song that Andy Jassy is talking about is key
notes, the clash song.
We don't do it.
It's not good.
We do it.
We have to do it, right?
Right.
So that's what's really exciting.
Tell me a little bit about the key message of your talk here at MianBet.
Yeah.
So here, what we wanted to talk about was how we, you can do a distributed deep learning
infrastructure in the cloud that actually scales really well and is highly performance.
So when we started this thing, when I took over the team a bit more than a year and a half
ago, I, like, Jerry, we're really well funded, as I mentioned.
So I had dollar signs in my eyes and I was like, all right, I'm going to buy so many GPUs.
I'm going to splurge.
I'm going to, and we had a server room.
We had everything there.
And, and then actually was still, even if you have the money, even if you have the
means that your disposal, it's still fairly slow to ramp up.
And we had Mike Garrison, which was doing a talk with me, which is our lead of infrastructure
engineering, was telling me, hey, what about Amazon?
I was like, they have K80s, you know, they have old GPUs, it's slow, et cetera, but keeping
an open mind, we tried a couple of things.
And we got in touch with the AWS folks and, and we did a lot of infrastructure work to
really, like, make it work, first single node, then multi nodes.
And using PyTorch, we're a PyTorch shop, we used to be in anything shop, and then a
tens of those shop.
And we really like switched to PyTorch full time a year ago or something.
And, and the talk was really about this kind of journey through which we went from like,
yeah, you have a on-prem compute and you can do stuff to really, really large scale distributed
deep learning in the cloud that's efficient.
And efficiencies is really the key here.
And driving in particular, there's one thing that is very different from, let's say,
normal machine learning that you would see at NIPs or CPR, which is we care about small
networks that operate at a high resolution.
And there's two reasons for that.
One is that they need to be small because even if you can compress them, quantize them,
and all these kind of things that we know, we can make them more efficient.
Still, you need a smaller model initially to fit in like computational budget that we
have in the car because safety critical, so you have to have like, really efficient models.
The second thing is you need very high resolution because time is equal space, so I was talking
in my talk about like these weird equations, talking about lean deep learning, so you want
like faster and around time and these kind of stuff.
We want to create some kind of Toyota production system of deep learning and stuff.
So that we can iterate really quickly from idea to model, to validation, and go back
to the drawing board because it's research.
And this idea of very high resolution is part of one of our constraints that we have to
deal with because we want to predict things from far.
And so seeing far is like when you read the California Handbook of Drivers, it tells you
you have to look far in the distance to look far into the future.
And so resolution is kind of a key thing.
It's actually talking literally camera resolution.
Camera resolution.
Camera resolution.
And specifically for the computer vision models that we're using.
And so that means that the compute workload is kind of different because you have small
models and very high resolution. So in terms of data flow operations, time you spend in
this metrics multiplies and all these kind of things, it's very different.
So we had can't down sample or crop everything to 224.
Not.
No.
Or site far resolution doesn't cut it for us, sadly, no, it doesn't.
And so we had to, if you use the standard tools like the data parallel or distributed data
parallel from PyTorch, which are amazing at the image net and these kind of stuff, they
didn't scale for us.
And so we had to rewrite a couple of things and that's what we talked about.
Okay.
So let's walk through that journey.
So you mentioned that one of the first steps was you kind of had to build up the infrastructure
like at a node level from scratch, was that where it started or was there where there's
steps before?
Yeah.
No, no.
So we started.
So that's the cool thing about TRI is that we're fairly young and we're small.
And so there is no, no technical debt because there's nothing when I started, right?
And that was super cool because I'm, as a research scientist, I was mostly, you know, use
this, use that.
All right.
It's there.
You know, use Slurm because it's this way.
I use that file system.
It's there.
Okay.
And here was really just sky's the limit.
What you should do.
And so we really got the opportunity to use the best partner with the best so we worked
directly with a lot of different partners.
And then we really created the thing from scratch and first single node because it was
really easy and ended all kinds of tricks.
Now you have some machines that are monster machines, you know, 700 gigs of RAM.
And so you can scale quite well, but up to a point.
And so that's when we started to switch to using distributed file systems.
So we did a BGFS base, distributed file system.
Before we leave that initial node, I thought I heard you say earlier that it was difficult
and you had to go through a lot of steps to get on that first, they get that first
node up and running.
But you just said it was really easy, assuming that means relative to a full distributed
kind of.
I'm kind of curious about the, you know, the pain points that you had to go through just
to get this up and running.
And also the extent to which there's still pain points are there other things that have
kind of wiped that all the way.
So yeah, okay, the first one is this base, right?
So the first one is because of the scale of the data we have, you cannot.
So for a lot of like, that's a debug experience or research experiments on small data sets,
you can fit them on the RAM and you should do that because that's just like the best
thing for your bug.
But when you have a large data, a large data sets, then that becomes much more complicated.
And so we, we first switch from the RAM disks to EBS volumes or more EFS or we tried everything.
But for like, these kind of like, this high resolution, small networks to not be network
bound, right?
To not have this GPU starvation problem where your average utilization of the GPU is like
15% or something ridiculous.
And these machines are expensive.
So you want to bump that to 90% or above.
That's what we had to actually, even before we started really doing distributed computations,
using a distributed file system enabled us to really download the data once and not every
time you set up a machine because if you auto provision machines and you have to download
data from a three, every time you start a machine, then you're saying like, oh, I have
this idea.
That's way two hours before I can just like press play, right?
So that was a big pain points for research to have this faster and around time.
So the distributed file system was something that was very useful at a single node level
and of course, scaled to the multi node.
So we did it two birds with one stone.
So where did you end up with that?
We used the BGFS as file system.
And we're going to look at Luster, like these announcements that were made recently.
That's very interesting.
Another pain point that we had was the BGFS you're managing yourself.
We just deploying it on the node in your amy or whatever.
Yeah.
So we have like, instead of instances that serve that file system that is then mounted
on these instances.
And we have some infrastructure as code to just like spin this off, like all configured
and ready.
There's something around containers.
So we were baking stuff a lot into the amy, into the machines themselves that were when
they started, you're just there directly because not everybody was familiar with a Docker.
But we picked up Docker too because there's obvious reproducibility benefits.
And when you hack a lot of things quickly at the beginning of a research project, having
this kind of Docker file where people can reproduce your environment and not just, you know,
your experiments.
That's actually extremely helpful for collaboration in the team.
So we used to tie us back to that agility and being able to move quickly.
Exactly.
Close to booting up a whole machine.
Yes.
Yes.
And our IT folks were so happy because it's not like this.
This doesn't work.
Yeah.
But because you happy to get installed something that wrecked the system and that's of course.
So DevOps.
Tracing DevOps, even for researchers actually was quite powerful because you can only do
the research that, you know, the mastery of the tools is really important to empower you
to do research beyond, you know, just pipe Jupyter notebook, let's say.
It's an awesome tool.
But if you want to go beyond, you need to master other tools.
And that's what we've been doing.
It's a journey through engineering, craftsmanship as much as deep learning research.
Is the, you know, when you talk about kind of applying DevOps in this world to what degree
in your experience does it apply directly or are there, you know, gaps or it only takes
you so far.
You have to modify the way you think about it.
And I realized that I'm saying that as if DevOps is this well-defined thing.
Yeah.
But I think it's a good question.
I think there's like two ways to, like let's say there's two extremes, right?
There's the extreme of you do everything yourself.
And there's the extreme of you just use blindly something that someone does for you.
And in that space of, you know, all the grad students in the world in machine learning,
they spend considerable amount of time configuring their environment.
That's a skill we developed during our PhDs.
And Docker and these kind of things, if you don't become an IT guy or DevOps guy, but just
learn from the best there.
And they do some of the things that around security and that's really important for
data that we have that I don't know.
I don't have an inkling, but they expose us to AWS services, they expose us to some
Docker stuff.
So I'm not an AWS expert.
I'm not Docker expert.
I'm not a Kubernetes expert.
But knowing a little bit of that enables empowers you to try more bold research ideas and
actually debug.
And when you care about the performance of your model, not just in terms of its accuracy,
but its speed, having these knowledge enables you to do research much faster actually, which
is counterintuitive a little bit.
But again, when you're beyond MNIST, that's what it takes.
Right, right.
You started out doing a lot of this yourself, yourself, meaning like within, you know, as
research, a community of research scientists, it sounds like you're presenting with an infrastructure
person.
So now you've got kind of, you know, professional support.
Yeah, we do, we do work really tightly with them.
I also, my team is like probably like 30% engineers.
Okay.
And it's really, I think it's really good for research teams to have this mix of really
scientists and engineers.
And because again, as I said, the lines are blurred at large scale research, and you need
these two skills.
And obviously, also like all the DevOps and infrastructure engineering teams.
So the collaborative spirit of terror is really, really good.
Like because we're small, we're very tightly in it.
And because there was no technical debt, we're building everything together.
And really nothing that the infrastructure engineering built was done in isolation without
consulting us.
So that's why we have a system that works really smoothly because all the concerns were
shared and addressed at the same time from all the pieces of the puzzle.
So it's really nice to have that like kick-ass modern infrastructure built around, around
you somehow and with you.
Yeah.
And so did that infrastructure engineering team and support?
Was that always there or did that come at a certain point after you'd built some things?
Yeah.
It's a fairly recent addition.
Okay.
So it started kind of organically and then you had some people that were there.
And it started to be formalized only recently as we scaled up and where that need became
much more obvious.
So yeah.
And is that infrastructure team primarily responsible for like where's kind of the line
that they how far up the stack did they go?
Are they worrying about like tools and frameworks and software platforms or is it primarily infrastructure
and network and disk and file systems and connections to the cloud and all of that stuff?
So I would say the latter.
So I think the lines are blurry.
But you need this single responsibility principle that applies well for software.
It also applies for organization.
There's this conways law that says that software organization writes software that is architected
in a way that reflects the organization.
And so I think it's really good if you have like clear responsibilities but also the lines
are a bit blurred because that means that you get a system that is flexible.
But you need these kind of responsibilities too.
So there's some separation.
And in my team in machine learning research and we are the ones that made the decision
to switch to PyTorch for instance and the way we did that is that for instance I re-implemented
YOLO myself a year and a half ago in all the different deep learning frameworks.
And it was after doing that like object detection is really nice because it's a structured
prediction problem that's shoehorned into a classification one.
And so it breaks the APIs that most frameworks support like from the get go.
And so if you use that you know you're stretching a little bit the capabilities of the network
in terms of the framework in terms of their APIs.
And so re-implementing YOLO in all these different frameworks made it clear that as a research
scientist I value flexibility and PyTorch had the flexibility.
Chainer is also very good.
There is other alternatives but debugging and extra.
So at certain levels like that's why I said like research scientists were making engineering
decisions because choosing PyTorch is something that we wanted to make as a research scientist
group.
And for the reason of also the particular research we're doing.
So for instance one of the things we're doing is with the paper recently called SuperDepth
which is a paper about predicting the depth of a scene from a single image.
And so we self-supervised method where is geometry a supervision instead of using labels
because for that you can't label.
And this is again another example where you super-resolution so this idea of high resolution
is actually important also for accuracy.
If you super-resolve the images this helps you predict better depth maps.
It was one of the key findings that we made in the paper.
And so all that is also enabled because of the choices we made on the software sites
and PyTorch and all these kind of things.
And also around the community that there's around it so that enables us to really move
fast and set on the shoulder of giants.
So I talked to different organizations that have differing opinions on how opinionated
to be for their organizations.
It sounds like you're of the mind to kind of stand it as in this case on PyTorch at TRI
as opposed to other places.
We're going to build a framework of platform and it's going to be able to support whatever
the research scientists or engineer wants to use.
Talk me through a little bit of the way you think about that.
Yeah.
I think about it in almost mathematical terms.
That's the bias of our interest trade-off.
If you have small bias and if you have high variance and you're really favoring exploration
for these kinds of stuff, you need a lot of people that are willing to support you.
So if you say, oh yeah, Slurm and Kubernetes and PyTorch and TensorFlow and everything and
the little framework that that random guy made on his own free time.
So first of all, what is actually your business?
Is it making those that infrastructure and no, for us, it's not for us.
It's making awesome robots, awesome machine learning.
So I clearly err more in the bias area, but it's just a little bit of map-produced exploration
and exploitation trade-off.
When you first have high variance and for a little while, you go wild, you explore and
you're maybe not bound by...
You implement yellow and every framework?
Exactly.
Something like this.
But then, at some point, you need to make a decision, because that's not sustainable.
And you want to move fast in a clearly identified direction.
Once you have identified that direction and you never have enough data to prove that
you're right.
So at some point, you have to have expressed leadership and just go with it.
And then you go for it.
And of course, you keep an open mind because then there's the next phase of exploration
because you're right for only a short amount of time in this field of deep learning.
So we take a diversion on kind of the path that you laid out in the present.
Oh, yeah.
We take a turn at step one.
We got beautifully sidetracked, but in a wonderful direction.
So yeah, so we were single nodes, everything in the RAM, and then moved to try the existing
storage solutions, then moved to more distributed file system.
And once we had this, because it's an in-memory distributed file system, we didn't have GPU
starvation anymore.
But then our training was slow because we were limited to a single machine.
And then, Peter instances happened, so we started to use the 100 GPUs much better that
required also tuning the storage again to avoid GPU starvation.
And then we again, augmented to go into multi-node.
And with the distributed file system, that at least the data was easily accessible from
all the different nodes.
And then that's when we started to hit the limitations of distributed PyTorch, which
was very recent at the time.
Before we jump to distributed, I'm curious about the, you know, you've got some, I guess,
quote unquote hyperparameters like virtual CPUs, or you know, the machine configuration
parameters.
Like, you know, they're kind of universal rules of thumb for that kind of thing that
you figured out, or do you experiment with it a lot?
Is it job-dependent?
A lot?
Are you overly focused on economic optimization?
Like how do you work through all that stuff?
So we optimized for time.
We don't optimize for cost yet.
That one was easy.
Yeah.
That was easy.
We haven't.
So that's more again, the job of the infrastructure engineering people.
So does that mean you just get the biggest one with the best GPU and?
You got it.
Exactly.
That's exactly it.
And also because our workloads, it was obvious that that was the only thing to do.
So go big or go home.
That's basically what we did.
Yeah.
Yeah.
So for a single machine, we just like try to scale as much as possible on a single machine.
And that meant these big, big instances, we're psyched to use soon the new ones that
were announced or even bigger.
So actually, that's feedback that we directly gave AWS.
It's quite cool to see that we give them feedback a year ago.
And then, like, keynotes was, oh, and we heard you.
We did this.
Yeah.
And so the biggest instances that they made, that's something that we had asked for and
a couple of other cool stuff.
So.
But you're still limited on a single machine.
And so when you were kind of topping out at a single machine, how long were your jobs
running for?
So at this stage, it was more in the order of weeks.
But that's what kind of job is this?
So the main one in terms of like computational, the most computational expensive one is
semantics segmentation.
Okay.
Because again, it's like high resolution.
It's very dense.
It's dense prediction.
And so that was the most computational expensive job.
Another type of job that we do that is also very expensive is imitation learning.
So we do a lot of research on end-to-end driving.
The main reason is not so much that we believe that it's all you need to driving, obviously
not.
But we get a lot of data from actual cars.
And so we get a lot of demonstrations.
And so there's this really interesting research question that we're working on, which
is how much value can you derive from these demonstrations?
This is a form of supervision on driving that you want to distill down into your models.
And so we do a lot of research there.
And that's, you know, use all the data is really the question that animates us.
How can we use all the data?
And because we can't label everything, we're not going to active learning routes and
the same thing that everybody else is doing because obviously we're doing that.
And that's not the open research challenge.
Everybody knows active learning is a good thing to do when you labels things.
We're really interested in self-supervised learning.
How can we really use all the data by leveraging geometry?
Right.
For instance, how do we use demonstrations at scale?
And so those are the workflows because motivated by the research direction we're going
in, those were the most intensive ones.
And a single machine, these are things that easily take weeks.
Okay.
So then that necessitated jumping over to distributed training?
Yes, absolutely.
Did you do that after the decision to go with PyTorch or did you have to figure that out
twice?
No, we had made so because also we have a lot of like we're in Silicon Valley.
So it's really nice that there's a lot of dense communication between people are not
afraid to share their plans or are going.
So we know to some extent where things were going.
And we know where we wanted to go.
So we also were open about this with different partners.
And so we knew that when we were going to hit the distributed wall, we would be ready
for it.
So we had all those factors were factored in at the decision time at the first one.
So we didn't have to revisit it later.
Okay.
Thankfully.
But you did have to, it sounds like weighed on some PyTorch features to support doing distributed
the way you wanted.
Absolutely.
So initially we were starting to be a little bit afraid that we would have to either
fork or do some like really big upstream contribution to PyTorch too.
And as again, as I was mentioning, it's kind of like a niche application from the deep learning
era.
Like a high resolution semantic segmentation, for instance, it's not something that a
lot of people are pursuing.
So we were starting to wonder if there was another way than to hit like low in the stack.
And we did like fairly intense debugging performance profiling, which is not easy in the cloud because
everything is like in the ether.
And what we found actually, and that's kind of like was an interesting end of the debugging
journey for the performance optimization, was that in the distributed setting when we
had many machines and a very efficient distributed file system, our epochs, right, our passes
over the entire training data became really fast because we had this huge batch sizes.
And everything was flowing really well to the GPUs, GPUs were crunching really quickly.
And what happened is that there was like huge down times, like wait, like there was a bottleneck
somewhere.
And it turns out that bottleneck, which was hard to find, was in the data loaders when
you do, you know, you how you do multiple workers that pre-fetch the data for you in parallel
to feed the GPUs, like the super hungry GPUs, like really quickly.
And in PyTorch, because you have this global interpreter lock, you have to use processes
and not threads to do that.
And so it's stuck PyTorch data loaders, it starts workers, which starts their multiple
processes.
And forking, like creating a process is much more heavy than creating a thread.
And when you do this very quickly in a distributed setting, that actually became the bottleneck.
So we had to change the data flow and the way we were doing this pre-fetching and those
queues by having some kind of like always warm queues that were kind of like infinitely
producing and then infinitely consuming on the other hand.
And we're playing with fire a little bit there because we're creating racing conditions.
And so that locks can happen.
But because this doesn't sound like a plug-in or something that's like a funnel plug-in.
So this was on top, right?
This was something that we were using in stock PyTorch except for the data loaders.
Except for the data loaders, where we changed the data loaders to something else.
And that's what I mentioned by this, warm producers, infinite and this racing conditions.
And recently we've been playing more and more with Horovod.
This is also an open-source library made by Uber.
And it's with PyTorch?
It works with PyTorch.
I didn't realize that.
It's TensorFlow and PyTorch.
It's starting with TensorFlow and now it's PyTorch.
And actually, this provides this great MPI interface.
And that enables, so it's a little bit less efficient for our niche application.
But we have other applications.
And so the flexibility that you get with Horovod might be worth the price in performance.
So we're considering moving more and more stuff to Horovod.
It sounds like you were able to, you invested a little bit in kind of tweaking PyTorch to make
it work.
But it kind of caught up.
And now you've got some solutions that work for you.
And so you are able to do distributed training like were you done, like pop the champagne.
So it's interesting.
And one way, yes, because there was a lot of internal questions.
So like I said, TRIRI is a robotics company.
And one thing you have to understand is that in autonomous driving, roboticists, they do
very things very differently than the really hardcore deep learning crowd, which is they
used to light our sensors, clustering methods, like DARPA challenge stuff.
They work awesomely well and have like much stronger safety guarantees than what we
do in deep learning.
And so they're not necessarily very experienced in the deep learning way.
And so doing these kind of things also means that like training for weeks to develop an
algorithm, that sounds insane.
And so here doing this distributed training and showing them internally that, hey, you can
do things really quickly in the cloud at scale.
And you can tweak your models and do your develop your algorithms almost as quickly as if
you were not doing deep learning.
That was kind of like a champagne popping bottle popping moments.
Where is it?
Oh, that's super cool.
Now we actually are going to run with it.
Of course, we're not done on the research side.
Now we can basically study what happens when you do self-supervised learning on a lot
of videos, what happens if you do imitation learning on really a lot of demonstrations.
And actually we have a paper that we're going to push on archive soon, where we really
push the boundaries of imitation learning and show that you can go quite far with like
deeper models and more data.
It's kind of like a prototypical deep learning story, more data, deeper models, that works
really well.
That's only thanks to the infrastructure that we had that we had an awesome intern, Felipe,
that could do these experiments, thanks to that.
So we're not there, but we're definitely enjoying the fruit of our labor.
Nice.
So there's a semantic segmentation that before you made it over to distributed was taking
weeks, what does it take now typically?
So we can do things in like under two hours now.
Oh, wow.
Wow.
That's really fast, yeah.
What does that require in terms of a cluster size?
So we typically run jobs at, I think right now, beyond eight machines, so beyond 64 GPUs
for single network, right?
We find that we don't need to go beyond that at this stage, so we don't do like a single
network on 256 GPUs or something like this, which is the most people that do that at least
publicly do that is just to beat speed records on ImageNet, which you know, it's nice.
That's not really what we're going for.
So for the jobs we do, let's say between four and eight machines, so 32 and 64 GPUs,
provides us with like a small turnaround time and good deterioration speed for our research.
Is it that, you know, the complexity involved in going from eight to, you know, some multiple
of eight isn't, you know, is overburdened some?
Is it that the value of going from two hours to, you know, 30 minutes isn't there?
So there's, there's some like more infrastructure problems around like limitation of supply,
you know, like we often joke at TRI, we have infinite GPUs because we're in the cloud.
But in reality, it's not necessarily there because availability zones, et cetera, so some
things that I don't fully understand.
The other thing is also at some point, you start to hit algorithmic difficulties.
So like for instance a year ago, people were convinced you couldn't do large batch SGD
because you would have generalization performance issues.
And that's when Facebook made their, you know, oh, actually no, it's just a numerical optimization
problem.
You just got to do the linear scaling rule, this warm up, you have to twit a little bit.
And then yes, it generalizes the same way.
And that's when you have this explosion of large batch training methods, but still there's
a limit to that, right?
And depending on your data sets, depending on your learning algorithm, depending on also
the data at hand, right?
So the particular generalization gap that you have to overcome large, like there's a good
size of batch size.
So beyond like very, like there's a limit to how big your batch can be.
Okay.
So if you have a single cluster running at a time, or do you, you know, spin up multiple
clusters and run multiple training jobs kind of constantly all the time, and does that,
you know, if that's the case, or even if not really, does that level of change drive
you to use something like Kubernetes or some kind of infrastructure, may you've mentioned
Kubernetes and Slurm and some other things.
Yeah.
So the way we do it is we provision clusters on demand by the researchers.
So we tend to have a couple of like clusters per researcher, per project.
So that's really nice also because it helps a lot with experimental management, you know,
like babysitting experiments.
It's this full time job when you get closer to the deadline and having like these separate
clusters for the separate workflows for the separate people.
That helps with just the cognitive load of where you were, et cetera.
And so we didn't feel like, and again, my team is like fairly small, we're like 12, 13
people.
Okay.
So we don't need necessarily, we do very large experiments, but we don't necessarily
do many, many different experiments, we like probably have four or five projects at the
same time.
Okay.
So no need for like complex scheduling or monitoring or queuing or these kind of things.
It's going to get there and we know it.
So that's why we're preparing for that.
And I have like more than HPC experience.
So that's why I favor a bit Slurm also because when we started having this discussion,
Kubernetes was not supporting GPUs now they do.
And the only thing I'm a little bit afraid of is adding interaction levels because again,
we care about speed and performance.
So the story, the stock that I was talking mentioning before, we could do that because
we were a working tightly with the infrastructure engineering team or AWS or Nvidia.
We were actually talking to them directly and we actually knew what was going on under
the hood.
So we could pop up the look under the hood and say, oh, yeah, this is wrong or this is
wrong or this smells funny.
Can you check this?
And so if we do too much, add too many layers of interaction like Kubernetes might be
that.
I don't know.
I'm not sure.
I'm a little bit afraid that we lose control and we lose interpretability in a sense.
And our models are already hard to interpret.
You mentioned in passing the managing experiments, experiment management.
Have you built any higher level tooling or infrastructure to help research scientists
do that?
Or is there something that you're using off the shelf or is it, you know, post-it notes
and Excel spreadsheet something?
Yeah.
So we did our fair share of Excel scheduling.
Of course, do that a little bit.
But we had an interesting journey where we initially used TensorBoard, but then TensorBoard
didn't scale for us because that's on disk.
And so it just didn't work.
We switched to VISDOM, but VISDOM is a little bit too bare bones.
It's very flexible, but there was also other issues there.
So we're really starting to think about this.
And at the same time, we got in touch with a company startup called WNB, WNB, WNB, WNB.
And they basically, because we're just creating this company and basically talk to us and
open AI and looking for like, what do you guys need?
And we really worked really tightly with them.
We have the customers now.
And we have this really cool, like, experiment dashboard, experiment management system, where
we can do a lot of visualization of experiments, multi-user, multi-project.
It scales really well.
And yeah, so that's what we used today.
And so we're really, again, not optimizing for cost.
We're optimizing for time.
And because there's a lot of excitement around machine learning, there's a lot of opportunities
to work with great partners.
So that's our approach.
When you first mentioned scale there, you were talking about on disk performance of
TensorBoard, but then later when you're talking about its scaling, like, how is it doing
in terms of, I mean, you're not a huge organization, but is it scaling in terms of the number
or experiments that you do?
So right now, we're, like, probably less than 20 users, so that's service.
So I can't say about the scaling in the user, but we do a lot of experiments.
Like, I mean, as you know, researchers, we, we, a lot of research is the speculative
late strategy, which is you throw it at the wall and you see what sticks.
So you have in a high-merch optimization, all these kind of things means that the single
researcher, especially when you have this nice infrastructure in terms of machines and
experiments you can run, you're going to, like, have a fire hose of metrics you want
to visualize.
And so that scales really well for that.
And again, we're not in the business of making dashboards or these things, so we're really
happy to partner or buy whatever is not our core business, which is really about this deep
learning models for driving.
And does WNB do the hyper-performer optimization for you?
Or do you do service?
Yeah, so we do our own stuff there.
They have some services there, which we don't use, but we, I think hyper-optimization
is, like, I'm still on the fence whether this is something internal or something that
we can partner with, because there is, like, typical patterns and typical, like, algorithms
and I've been a big user of hyper-opt, so you can do biogen hyper-optimization and these
kind of things.
And sure, this is, like, almost standardized, so you could imagine having a service for
that instead.
But some of these things actually have to hook up really deep into, so it depends on the
model and the research project you're doing.
So there's a blurred line there, and there's awesome, like, general purpose algorithms,
I think.
The one that I use the most recently that I really like is hyper-band, which is kind
of, like, a band-it approach that's using the fact that it's our optimization is sequential,
so you can restart from a checkpoint and continue any of these kind of things.
So yeah, on the fence, some of the things in-house, some of the things, black box, it's
not, I'm on the fence for this.
And so I'm making an assumption that you don't have to worry about, besides from the kind
of distributive file system issues that we've talked about, a lot of the kind of traditional
enterprise data management, you know, data lakes, data warehouse, hooking into data stores,
that stuff.
You just need big, dist store stuff.
Yeah.
So we have, like, we use S3 a lot, and we use to use S3 directly.
And so basically, what we've been doing is just, like, what happened with processors.
We add layers of cache, you know, and hotter and hotter caches, which are getting smaller
as they are getting hotter.
And we have to, like, asynchronously, preemptively fill those caches and these kind of things.
So it's always the same thing.
Is that these caches, like, the, you know, the various S3 features, like, glacier and all
these?
Yeah, so you can see basically all these different types of storages, right, as these
cache, like, like, these are, I call them caches, but it's a metaphorical cache, right?
Right.
S3, we used to use S3 directly as, like, S3 to GPUs, and that obviously didn't scale.
And so we added this, like, you know, different storages and distributed files, yeah, EBS,
etc.
Yeah.
Yeah.
And then you have these prefetch and queues that are, literally, feeling the RAM with,
next to your GPUs, and so you have this ultimate layer of cache, right?
Okay.
And then on the back end, are you, do you have to worry about inference and model serving?
So at this moment, the inference is the models we serve are in our robots, right?
So that's a big thing, right?
Right.
Is that the models we serve are the ones that are going to be in the path to actuation
of the car.
So there we have, like, amazing driving technology teams, like, that own parts of the
stack, like, we have an object perception team, we have a slam team, we have a planning
and controls team.
And these guys, they basically take our models or make their own.
And they make them more efficient, fit them in the computational budget that they have.
And that's how we serve models.
So that's a fairly different model than, let's say, web-based application.
Yeah.
Is that the process of getting the models to fit compression or pruning or what have
you, is that there's still a manual process to large degrees, right?
So to some extent, it's kind of a little bit weird because there are some ways that are
productized, a little bit like there's sensor RT and these kind of things.
But it's still more an art than a science, so it doesn't always work.
It works for certain types of networks like out of the box with some it doesn't.
And so there you have some tools or bag of tricks that are general purpose that you can
throw at this and that you should throw at this, definitely, and that our teams are doing.
So upstream or upstream on the research side and my team what we're doing, you can learn
models that are compressible or that are amenable to compression by having some compressibility
factor built in.
You can have also small models, like as I mentioned, right?
And there's more and more research results that show that small models can generalize as
well as big models.
They just have to train for longer or you have to change the learning algorithm.
And another thing is one of the big things we're trying to do is how far can we do multi-test
learning, right?
Because if you can have a shared backbone and squeeze many different things, that's awesome.
And the one recent project that we did around panoptic segmentation is basically this.
It basically takes a mental segmentation and takes instant segmentation, so mask our
CNN and these kind of things and that works really well, but that's extremely slow.
I think like mask rescinds me like 150 milliseconds per image or something and you have to basically
reduce those models and merge them together with maybe different heads to make it efficient.
And we made a recent paper, it's going to be an archive soon called tasknet for things
and stuff consistency network where we basically merge them together and have a task consistency,
a cross task consistency because the main problem with multi-test learning is if you just
sum the losses, it doesn't necessarily, they maybe contradict each other.
So when you're arriving at an intersection, it says turn left or turn right, you don't
know you go in the middle, there's a lot of work, not a good idea.
So like imagine the gradients might be pushing in orthogonal directions.
So one of the key things we did is we actually augmented the objective to have a consistency
encouraging objective and between the stuff classes, like so road, sky, et cetera, on
the semantic segmentation side and the thing classes on the instant segmentation side.
So merging these networks is one way to be more efficient and that's some of the very
recent work that we've been doing.
How have we done in terms of kind of getting a lay of the land of your presentation?
We went way beyond.
Yes, so excellent, nice, awesome.
Any kind of parting thoughts or words?
No, I think we're really excited to continue this direction of large-scale deep learning
in the cloud and tackling this really, really challenging open research questions, yeah.
So we're continuing to grow very, very fast and excited to be in that space of self-driving
robots and with deep learning.
So very happy to have been able to talk about it.
Awesome.
Well, thanks so much, Adrian.
It was awesome to have you on the show.
It was a pleasure.
All right, everyone, that's our show for today.
For more information about today's guests or to follow along with our AI platform's volume
2 series, visit twimmelai.com slash AI Platforms 2.
Thanks once again to Sigop for their sponsorship of this series and support of the show.
To check out what they're up to and take advantage of their exclusive offer for Twimmel listeners,
visit twimmelai.com slash Sigop.
As always, thanks so much for listening and catch you next time.
