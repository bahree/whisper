Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Rob Walker, Vice President of Decision Management at Pegasystems.
Rob joined us back in episode 127 to discuss hyper-personalizing the customer experience
and today he's back for a discussion about the role of empathy in AI systems.
In our conversation we dig into the role that empathy plays in consumer facing human
AI interactions, the differences between empathy and ethics and a few examples of the ways
that empathy should be considered when building enterprise AI systems.
So what do you think?
Should empathy be a consideration in AI systems?
And if so, do any examples jump out for you of where and how it should be applied?
We'd love to hear your thoughts on the topic.
Feel free to reach out with a tweet at Sam Charrington or leave a comment on the show notes
paid with your thoughts.
Before we jump in, I'd like to thank Rob and the folks at Pegas for sponsoring this
episode.
If the topic of AI empathy peaks your interest, I'd encourage you to join Rob and I at Pegasworld
June 2nd through 5th in Las Vegas, where he'll be delivering a keynote on this very topic.
At the event you'll also hear great stories of AI applied to the customer experience at
real Pegasystems customers, and of course, I'll be there and speaking as well.
To register, visit pegaworld.com and use the promo code Twimal19 when you sign up for
$200 off.
Again, that's Twimal19, it's as easy as that.
Hope to see you there.
And now on to the show.
Alright everyone, I am on the line with Rob Walker.
Rob is the Vice President of Decision Management and Analytics with Pegasystems.
You may remember Rob's name from TwimalTalk number 127 on hyper-personalizing the customer
experience with AI.
Rob, welcome back to this week in Machine Learning and AI.
Yeah, thanks Sam, glad to be back.
Absolutely, glad to have you back.
Looking forward to what I believe will be a really interesting conversation on the role
of empathy in AI.
For folks that want to learn more about you, I'll refer them back to the previous episode,
but give us a quick overview of your focus at Pegasystems.
Yes, happy to do that.
So with the impact, I'm responsible for our AI space, but we really try, I mean, there's
so much hype around AI and we don't do AI just for AI sake.
We really try to focus on making AI work for typically pretty large enterprises and
typically in the area of customer engagement.
Right?
So in the previous episode, we talked about hyper-personalization and hyper-personalization
is really trying to be one-to-one conversations with customers for companies to do that.
And that requires a lot of AI.
It also requires lots of other things, but AI is an important aspect of that.
And that's what I mostly worry about, and also sort of areas around AI.
It's not just, hey, AI is cool, let's use that in customer engagement to make customer
engagement better, but it's stuff like, can we trust it?
Who in your organization should be responsible for it if it makes weird decisions, if there
is a bias, those kind of things?
So that's typically what I worry about.
The concept that kept coming up in our last conversation, I think this is pretty central
to the way you think about applying AI to optimizing customer experiences, this idea of helping
your customers figure out the next best action to take with their customers, is that right?
That is correct.
Yes.
So the companies we work with typically implement like this customer decision hub, right?
And it's a centralized decision authority that across all the different channels that
companies may have, figures out the next best action during conversations, right?
So what should we say?
What should we not say?
What price should we mention if it's commercial, basically trying to have very reasonable
conversations, but at the same time, because most of the companies we work with are not
just charities, right?
So at the same time, you need to sort of improve customer value, right?
So the next best action, the best in next best action, is typically some metric about customer
value and trying to improve that over time by doing the best thing possible to optimize
that.
So this concept of empathy in AI is something that you'll be speaking about at the next
Pegaworld, an event that your company hosts annually.
I attended the last one and I'll be attending the next one.
How did this idea of empathy, introducing empathy into these kinds of transactions or customer
experiences?
Where did that come from?
Well, so I've always been interested.
So before I joined Pegaw at some point before that I was a scientist, right, in AI, I did
my PhD in that area and I've always been interested in not just all the cool things AI can do
around predicting customer behavior and things like that, but also potentially the not so
cool things, right?
So can you trust it?
Is it transparent?
Is it opaque?
Is there a bias?
Can it go rogue?
Those kind of things.
So over the last years, we've really tried to sort of guard the moral high ground, if you
will, around AI and now just look at what it can, the value it can bring, but also
mitigate the risk that you can have with AI.
And following sort of that path, empathy was a very natural thing.
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,
which is, you know, that's a big beast.
And sort of more ethical behavior and empathy seemed to be something that was just about
tangible enough to try to really put it into the, into the product and into the vision
of best practice around using AI.
So we've been spending quite some time thinking about that and how you can operationalize that
kind of thing.
Now, when you start talking about the morality of AI, certainly, and even to large degree
empathy, start to, you know, the picture in my head starts to form around, you know, what
some of us will call AGI, artificial general intelligence, you know, what we talk about
more commonly is like sci-fi AI, right?
Is that what we're talking about here?
Or are we talking about something that, you know, how can you make this more concrete
for us?
Yeah, because, because I'm definitely not talking about that kind of thing.
I mean, I mean, that's really, well, it's, it's, it's interesting.
Right?
I mean, I think everybody in AI is thinking about how that, how that, how that works.
But I think just as a human species, if I just, you know, just even reading the news, you
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,
very clear for, for, for everyone, at least there is a lot of discussion about what would
be a moral judgment and not a moral judgment to even expect that of AI.
I think it's already a tough act.
But I'm certainly not talking about it in that kind of realm quite yet, although it's a
very interesting topic, right?
I was just thinking about, you know, self-driving cars, right?
And, and, and sort of the moral judgments, they may, one day, have to make, right?
In extreme circumstances.
But this is very much sort of a smaller subset of those challenges where we're talking about
customer engagement and, and those kinds of things.
And I think in that area, empathy really shows in, in, in, in, in, in, in, in pretty clear
dimensions, like stuff like, is this, if we are talking to me as a company, and it's an
AI driven conversation, is that, is that a relevant, right?
Or are you wasting my time stuff like, is it, is it appropriate?
Right?
So you're talking to me about something.
And it might be interesting, it may be interesting to me,
but it may not be still, it may not be appropriate, right?
Maybe you shouldn't be selling me a gun or a car or a credit
card that I actually can't pay back when I'm in debt, right?
So it's those kind of things.
And is there mutual value?
Are you talking to me for something that, you know,
can we have a transaction that has a mutual value?
Or is it just about the company?
And I think if companies implement those kind of
considerations well, I think that will do pretty well
on an empathy scorecard for starters.
Now that's interesting.
And actually somewhat different from the,
I don't know if it's different from the direction
that I thought we were going to go here or that, you know,
the picture that formed in my mind when we were talking
earlier about empathy or if one is part of the other.
But I'll tell you, I'll kind of recount the picture
that I have in my money.
Let me know where it fits into this world.
You know, I was envisioning primarily the kinds of interactions
that you might have via a chat bot or, you know,
chat kind of interface.
And you often have, or even, you know,
the extent to which AI is driving a call,
a call center agent and their responses
because that's some of that, or an IVR.
You know, some of that is starting to happen.
But I was envisioning kind of this set of capability
where, you know, maybe the, you know,
whether the chat bot or the IVR, you know,
IVR is a great example, right?
It's like, IVR should be able to tell from my voice,
or could tell from my voice that I'm getting frustrated
navigating the 50 million, you know, menus
and maybe escalate me, you know,
a little bit more quickly to someone.
Or, you know, you can imagine the same kind of thing
happening in a chat interaction where, you know,
I'm interacting with this virtual agent.
It's not getting what I need to do
or it's, you know, needing asking me to repeat myself,
you know, multiple times.
You know, there's a degree of empathy and all that
where it's understanding my,
I guess I'm kind of simplifying that
as understanding my emotional state
and using that as part of the decisioning
around what the next best action to take is.
Yeah.
But it sounds like maybe that's a piece of what you're saying,
but you're also talking about, you know,
maybe about the broader AI ethics conversation,
your example around, you know,
should we offer the credit card to the person
who can't afford it is one that kind of,
you know, resonates and kind of drives me in that direction.
Yeah. Now, I think, so I think the example that you gave,
right, like is somebody getting frustrated
and those kind of things I think are a very important part
of empathy, but I think it's part
of the delivery mechanism.
So I think what we're trying to do is sort of take that
in two different layers, if you will, right?
One is when you're talking to someone
and somebody is getting frustrated
or if you do voice detection and or inflection
and you sort of notices that somebody is getting upset,
you may want to change that may influence the next best action
as part of the context.
So I would call that, although it's cool,
more of the sort of the superficial way of empathy, right?
It's trying to feel somebody's mood
and use that as a context,
but that can become from a lot of different senses.
It could be, you know, as I said,
the inflection of a voice,
it could be when this is face to face
or you're in front of a camera,
it can be that, you know, people can sort of read their face
or the system can read the face of the customer
and see that's not going well.
But that's not the same as the underlying level
of making sure that the next best action
that you are contemplating is one that is empathetic
or even moral, right?
So I see that as two different, different thing.
I think people think about empathy a lot
like you were just describing it like,
hey, I see this is making you upset.
So I need to, you know,
hurry this along or ask you what's wrong
and that's all cool, very human stuff.
But it's on the delivery of a particular action.
But determining what you're going to do,
that also requires empathy
and that's more along the line of is this,
is this relevant, is this appropriate,
is this suitable, does kind of things.
And do you specifically use the word empathy
to distinguish it from ethics
or are those ideas different in your mind
or are they, you know, just a different word in this case?
Yeah, now I think, I think the,
if I think about ethics, that sort of, you know,
ethical behavior, I think empathy is basically
the step where humans, for now, maybe AI at some point,
will basically be able to, you know,
please themself in someone else's place
and say, hey, if this is happening to me, you know,
is this going to make me, you know, happier?
And things like that.
So I think they are, in that sense, very, very related.
So in that sense, ethics is kind of relevant
to some broader set of, you know, societal norms,
whereas empathy, we don't have to figure all that out.
It's just about, you know, this particular customer,
am I doing the right thing by this customer?
Yes, am I doing the right thing?
And I think there are a few dimensions to that, right?
Am I doing the right thing in terms of,
am I forcing you to take like a risk
that I actually think, you know, is too high?
I already know that you won't be able to pay back
that mortgage or that credit card or that thing,
or you don't really need this kind of thing, right?
So that's part of that decision,
but also is it relevant in the first place, right?
It's an empathetic thing to not try and waste somebody's time,
right? I mean, if you don't, I don't know about you, Sam,
but if you like all these ads, all the stuff that you get,
if you, you know, browse the internet
and look at all of these pages, it's, we're used to it.
It doesn't show a lot of empathy, right?
Everybody's trying to get your attention to do things
that, you know, maybe 1% of the time,
you're vaguely interested in, right?
So that's part of it.
Is it relevant?
Is it not wasting my time?
Is it, do you remember the context?
Do you remember that I just spoke to you in another channel, right?
I just walked into the branch, I just visited your website
and now I'm going into the IPR.
Do you even remember that or are you forcing me
to basically repeat everything I did?
I did, you know, in the previous channel.
That's empathy, right?
Empathy with customers that are trying to solve a problem
or that want to get value out of their interaction
with the company.
This is clearly an issue that is much bigger than AI, right?
We don't have to, you know, look very far to recognize
that in many ways, the previous financial crisis
with the mortgage bubble grew out of giving loans
to people that, you know, weren't qualified for them.
And there are many, many more examples
where organizations, you know, fail to exhibit
the kind of empathy that you are describing
that have nothing to do with artificial intelligence
or machine learning, you know, why take this on
from an AI perspective?
Well, I think that's a good question.
And I think the answer to that is
that the way we look at customer interaction in general
is to always do this next best action kind of thing.
And the next best action is actually collaboration
between humans, you know, inside the company,
deciding on, you know, rules or thresholds or policies
working together with AI, where AI is maybe determining
the risk, it's determining the level of likely interest
from the customer.
And it's that combination that creates the metric
for this is the best thing to do right now, right?
So you're quite right.
It's actually that mortgage example or the bubble
that you just described is a great combination, right?
There are analytic models that should have said,
listen, for this group of people,
the risk is not really acceptable.
And you shouldn't be pushing them
on this level of mortgage, right?
Suitability, for instance, is not taking into account, right?
So it's that combination of AI and rules that I, you know,
we call that decisioning or decision management
that basically needs to represent empathetic behavior, right?
So it's not just the AI, it's also the rules.
But one of the reasons I think the AI aspect is so important
is because the AI is learning, right?
So it can, you know, have evolved a particular bias, right?
It may be a very opaque algorithm
that may have evolved at bias and you don't even know, right?
So there are a lot of aspects of AI
that I think really touch on ethics and empathy as well.
When you're talking about this at Pegaworld,
are you, you know, you raising this as an issue
that customers should start thinking about this?
Are you talking about new capabilities
that you're unveiling at Pegas with, you know,
with the product that will help them address these issues?
Is it, you know, or is it something else?
Well, I don't want to feel all my own thunder,
but we'll definitely, so two years ago,
one of the things that I was talking about in the keynote
was about this thing called the T switch
and the T was, you know, stands for transparency,
but also for trust.
And it's basically the ability that once you have
this centralized next best action capability
inside of your company, that you can have full control
over where you allow sort of opaque algorithms
like deep learning or genetic algorithms
or that's kind of fancy stuff
or where you insist on more transparent algorithms
that you can actually explain to a customer
and that you really explain or that you really understand yourself.
So that was one aspect.
Next up is the thing around empathy.
I want, I think it's a really good thing if companies
are aware at all times how empathetic their behavior is.
So think about sort of a dashboard
where you would see of all the actions my company is taking
and these are, you know, the side of companies we work with,
these are hundreds of millions a day, right?
Hundreds of millions of interactions a day
and then being able to see, okay, these are actions
that we are taken automatically
is a combination of AI and rules
that are not empathetic
and that means that they are going against the relevance
or they are not appropriate, like not suitable.
So we're talking about this credit card
but it's not really suitable or it doesn't really create value
for the customer, right?
So imagine that while all of this is running,
this combination of AI and rules
and it's making hundreds of millions of decisions
and having all of these conversations with customers
that you can just see that as a real-time thing
and say, hey, really, we're getting less empathetic.
Let's see in our strategies, in our customers' strategies
that we have in the algorithms that we use,
where we're losing that, right?
Are we pushing products that we shouldn't be pushing?
Don't we have the rules that are determining suitability?
It's those kind of things and then in addition to that,
I think we can also determine sort of the cost
of not being empathetic, right?
So if you, for instance, if you are going against
somebody's interests
and I don't mean interest in terms of relevance, right?
So what a lot of marketing is currently doing, right?
They're spamming you with stuff,
they're wasting your time on things
that are actually not that relevant to you.
It would be good to not only know the percentage
of events where that happens,
it would also be really good if you had a sense
of the money you're actually losing
and you can calculate that mathematically
by, for instance, saying, hey,
this is what we actually talked about to this customer.
We talked about this mortgage and the customer said,
no to it because it wasn't relevant.
What we could have been talking about
is this particular issue that we spotted in a different channel
or last week or an hour ago,
or maybe a much more relevant offer
that we didn't think we wanted to do
because the margin was not as big as on the mortgage, right?
Having, making that a transparent thing,
having people own the kind of empathy level of the brand
I think is a really important thing going forward.
It strikes me that that latter point
around quantifying the cost of these non-emphathetic
non-emphathetic actions is really a big part
of where the problem lies.
It's easy to know the cost of the expected revenue
or profit from offering something,
but a lot harder to know the cost
of just wasting the customer's time
or reducing the brand goodwill
because of some series of less relevant
or poor experiences.
How do you overcome that gap?
Yeah, well, I think some of the math
actually works out quite nicely, right?
So remember that when we do this next best action,
and I said before, the best is a function of the value
that is created in the relationship.
So the math works out that you actually can know
that if you go against the propensity,
because for instance, you're selling,
let's use this mortgage example that works really well,
if you are thinking this is not particularly relevant,
but if the customer says yes, this is the margin,
this is the money I as a bank in this case will make, right?
If you calculate all the times a customer said no,
because you're basically offering stuff
that's not particularly relevant,
just you know, you're hoping that the customer will say yes.
What, how could we have used that moment?
How could we have used that interaction
with the customer in a better way
that would have created more value?
If you just multiply that with a hundred of millions
of decisions you make, you get to a monetary value.
And you find in examples like this
that there's some explicit decision
that where the customer is saying,
hey, let's offer this more profitable thing,
even though we know it's not as relevant,
or does that happen in more subtle ways?
No, I don't think these ways are particularly subtle, right?
So the way, because the way we work is like,
so to do this next best action, right?
We would calculate every single thing,
this is also part of this hyper-personalization vision,
to be completely one to one,
it means that of all the possible conversations
you could have, right, you're going to rate them
in real time based on the context,
and you're going to say,
this is the thing we are going to talk about
as a combination of what the AI thinks
is particularly appropriate and relevant,
as well as my rules that I have around profitability
and inventory and all of those kind of things, right?
So it's that combination,
but we calculate them all in parallel.
So it's relatively easy to see, okay,
this is what we chose to actually talk about,
but this is what we could have talked about
if we had weighted suitability higher,
or if we didn't overrule this very low,
or this very high propensity and said,
well, even though that's relevant,
it's not what we want to talk about, right?
So you can see how you get a drop off of typical,
or of specific conversation topics
that you decide not to pursue for other reasons,
and that's what you can calculate.
Is the task then starting to build awareness
on the part of customers or users
or kind of the industry as a whole
to incorporate these types of empathy metrics,
if we call them that, into their systems,
their rules, their algorithms,
and start to, is it as, I don't know if simple is the right word,
but as simple as starting to try to put numbers
around these suitability context
to where relevance risk and then feeding them
into your automation tooling with kind of appropriate weights
or does it go beyond that?
I think the way you describe it,
so what we're trying to do is, first of all,
we want to do that like an easy task.
No, no, no.
We're not conclusion or anything like that.
Yeah, yeah.
No, that's not easy in itself,
but what we're trying to do is, first of all,
make it very explicit.
So when we talk about next best action,
we have like, there are patterns that we've seen
that are repeatedly successful,
and they include things like relevance,
suitability, mutual value, risk mitigation, right?
So the tooling and the methodology already encourage
companies to at least think about it, right?
They may think, okay, well, for suitability,
we really don't care about it, right?
Or not as much, or we let profitability,
Trump suitability anytime, right?
But at least the product,
well, if you follow the product guidance,
you will have to take all of these considerations
into account, right?
So there is an ethical framework built into the software,
into the strategies that it will generate.
So that's one aspect of it,
and then the other aspect of it are like these,
the dashboard that you will show, right?
So it's basically shaming companies a little bit,
if they won't, right?
Having them self-shame them into appropriate behavior,
right, where they would say, hey, listen,
we cranked up profitability,
but it's at the expense of suitability or customer interest.
And at least, I think the awareness
and the transparency around these things
will be leading to better behavior.
How does the company begin to put tangible numbers
and costs around things like mutual value
and suitability and contexts awareness and relevance?
And relevance is maybe easier
because it impacts like propensity to buy.
Risk is something that's kind of fundamentally numerical,
but like some of these others are a little bit squishier, maybe?
Yeah, well, but I think, you're right.
And I think the, from what we've seen,
is that sort of the less squishy things, right?
Are once you are aware that you need to put them in,
and at the AI or the decisioning in general,
touching 100 million, making 100 million decisions,
and all the different general with all of your customers,
that that is part of your brand, right?
And you need to protect that.
I think that's a very important thing.
I think for the squishier things,
I think what we also encourage and also make possible
is continuous experimentation, right?
So there's always, there's control groups,
there is all sorts of things where you can, you know,
for a small percentage, a small sample of the customers,
you can actually measure if you're having an effect, right?
If they have a positive response to the brand,
and you can see if, you know,
what kind of strategy changes would improve that?
And that is a best practice kind of thing to do.
Do you have any examples of folks that you've worked with
that you can kind of walk us through
how this all plays out,
and how they want about making, you know,
kind of incorporating these ideas
into the way they make decisions?
Yeah, so I think even before we sort of, you know,
invested in all this around, you know, empathy
and also before the transparency and opacity,
it's not like, you know, these big brands
are not aware of these issues, right?
I mean, if I think 10 years ago, maybe longer ago,
I had long conversation with banks
that were sued for, like, misscelling, right?
That I think, you know, we've seen more recent examples
where obviously these companies
want to sort of, you know, control that,
even if just for their own sake, right?
To not be part of some class action.
And in the next best action methodology,
stuff like relevance, appropriateness, failure and risk,
have always been sort of first class citizens, right?
What we're now trying to do is to make sure
that it's much harder to break those patterns,
or if you are, you know, don't want to be compliant
with these kind of ethics practices,
you at least have to make the effort.
I think to your question, I think especially the banks,
I think we've seen it in other industries as well,
are getting very worried about their brand image
in that regard, and they are putting, you know,
suitability criteria, for instance,
is a pretty hot topic right now.
They're putting that as part of their next best action
strategy.
We just want to help them by showing the cost of that
and the benefits of that.
Okay, you mentioned compliance in there.
Do you envision a time where an enterprise
will have a formal empathy compliance regime?
Will it be called that?
Will it be called something else?
Does it already exist under some other guys?
I think that definitely will happen
in some cases already happens.
I don't think it's called empathy
is probably more on the ethics board,
where they, and again, it's not only about the company
itself, it's also about basically a compliance issue
out of self-interest, especially now,
and this is what where the AI is so important,
where there's so much self-learning going on
on this incredible scale that there could be a bias,
and there could be all sorts of things happening
that may not be so easy to control or even spot.
So I know that some of the companies I talk to,
and these are larger companies,
but they have a board already for all the algorithms
that are involved in customer interaction.
And I think that's a sensible thing to do.
It's part of what inspired this transparency
or trust switch in the software to make sure
that all AI is at least you can control
the level of transparency that you require
in certain circumstances, talking to customers.
Do you envision like a chief empathy officer?
It sounds like no, it's probably going to be
if anything, it'll be a chief ethics officer
or some other role.
Where do you see this all sitting?
Yeah, well, I think this is a very interesting topic
because I think this will become very, very important
if it isn't already.
And I think you will get into a situation
where you have at the first level AI trying to check
other AI for biases or an ethical behavior,
because it's just a lot,
and it would only escalate if such a bias
or other irregularities is detected.
But it's certainly, and again, I'm talking about
the larger companies with tens to hundreds
of millions of customers that are very worried about,
especially with the level of automation
that's now available, and then AI
that is dynamically learning new things
or evolving new things to have an ethics board like that.
And we try from a product perspective,
we try to make sure that like you have QA tests
on quality assurance tests, on performance
and other things that as a matter of course,
you would do the same thing around bias detection
or other irregularities before you release
the next version of your corporate brain,
so to speak, to make that easier.
I think the ideas of kind of making these
more empathetic types of qualities of your various offers,
you know, as you've suggested throughout this conversation,
it's very much kind of connected to this idea
of transparency, right?
There are, you know, these dollars and cents things
that we kind of build into decision-making algorithms
all the time, but there's all this other stuff
that goes into the customer experience
and what we're doing here, you know,
we're calling empathy is really the idea
of making a lot of those non-premafacy financial aspects
you know, A, more transparent and then B,
like putting, trying to make them more financial
or putting numbers against them and then, you know,
incorporating them into decision-making,
dashboarding them so that there's some awareness of them
and so that the organization can manage against them.
Really interesting set of ideas around how to make,
how to make this idea of A, I think it's a lot more tangible.
Yeah, I think, yeah, yeah, tangible, I think it's the right word.
So can we, are there straightforward ways to, you know,
make sure that in our customer's strategies, you know,
empathy is well represented and we can choose to ignore it,
but then there are these, as you say, these,
these desporting, these gauges, these dials
that show you, that shame you into some compliance.
And also, let's not forget that, like,
I think the reason we as humans have empathy,
you know, there can be lots of different theories around that,
but personally, I think that evolved, right?
It evolved out of a desire to collaborate, right?
So it's empathy is not like a cost to the company.
Empathy is actually establishing your, you know,
better relationship and a longer term relationship
with your, with your customers.
Well, it would be interesting to kind of follow along
with this work and see, you know, I'd love to hear a case study
of, you know, how a customer kind of implements this end
and once you've got this out in the market
and have folks working with it.
Yeah, I mentioned PegaWorld earlier,
any besides from your own keynote,
other things that you're looking forward to at the conference?
Yeah, well, I mean, this will be the biggest effort.
So it's always just very exciting about, you know,
to show people, you know, where we are at,
and it's not just about empathy, right?
It's about, it's about also making decisions in general
at a huge scale, you know, with this real-time AI
on the one hand, and then on the other hand,
and that's also part of empathy,
although we didn't talk about it right now,
but then following up on it, right, with the processes, right?
So we are really, and you will hear a lot about that at PegaWorld,
we're trying to, you know, have that,
that combination very strongly.
So it's, we have the AI and a decisioning
to decide what to do, right?
And then we have sort of the end-to-end automation
that will tell the company how to do it,
and to do it fast and efficiently,
which also plays into empathy.
So I really lost that interplay
between sort of decisions and processes.
So I'm expecting a lot of really good inter-discussions
and presentations from my customers.
Awesome, awesome.
Well, Rob, I'm looking forward to seeing you once again
at the event.
Thanks so much for taking the time to jump on
and talk this through with us.
Okay, well, you're very welcome.
Thank you.
Thanks, Rob.
All right, everyone, that's our show for today
for more information on Rob or any of the topics
covered in this episode.
Visit twimmolai.com slash talk slash 248.
Be sure to leave your thoughts on AI empathy
while you're there.
As always, thanks so much for listening and catch you next time.
