1
00:00:00,000 --> 00:00:06,880
Oren. Hello. Good morning. So for those listening in, I'm here with Oren at

2
00:00:06,880 --> 00:00:13,720
Cione. He is the founder and CEO of the Allen Institute for Artificial

3
00:00:13,720 --> 00:00:19,200
Intelligence and the acronym for that is AI AI. So it's often shortened to AI

4
00:00:19,200 --> 00:00:26,880
too. Oren, how long have you been doing this? Well, it all started for me about

5
00:00:26,880 --> 00:00:32,920
nine years ago. There wasn't an AI too, but the late Paul Allen's team reached out

6
00:00:32,920 --> 00:00:38,960
to me and said he wanted to create an Allen Institute for AI. And it was up to

7
00:00:38,960 --> 00:00:45,960
me to come in, write a plan, and make it happen. It sounded like an incredible

8
00:00:45,960 --> 00:00:50,080
challenge. I remember saying to people, people ask me, why are you doing this?

9
00:00:50,080 --> 00:00:54,560
You're fat, dumb, and happy. As a professor at the University of Washington,

10
00:00:54,560 --> 00:00:58,280
tenure professor, you've got a good thing going, why would you do something

11
00:00:58,280 --> 00:01:04,480
crazy like this? And I said, the sky's the limit with Paul Allen's vision, with

12
00:01:04,480 --> 00:01:10,480
his resources, with his commitment to AI, we could do amazing things. In fast

13
00:01:10,480 --> 00:01:15,160
forward nine years, I feel like there's a lot still to do, but we have done

14
00:01:15,160 --> 00:01:20,400
some good things. So I want to dig deep into what the Allen Institute has been

15
00:01:20,400 --> 00:01:24,520
up to, because it's kind of amazing how much you've accomplished in nine years,

16
00:01:24,520 --> 00:01:30,240
just the impact and the unique way you've done it. But first, I want to give

17
00:01:30,240 --> 00:01:36,400
people listening in a sense of who you are. So I had my Orin moment. I think it was

18
00:01:36,400 --> 00:01:42,840
2018, 2019 at latest, and you came and you gave a talk in San Francisco, and you

19
00:01:42,840 --> 00:01:47,320
did a live demo, which is pretty unusual already, of natural language

20
00:01:47,320 --> 00:01:54,080
processing, NLP. And the room was packed. And I was one of many, you know, people at

21
00:01:54,080 --> 00:01:59,920
startups trying to make this stuff work, and hoping to make it big. And, you know,

22
00:01:59,920 --> 00:02:05,880
this legend walks in and gives a presentation. And you had it live on the AI2

23
00:02:05,880 --> 00:02:09,880
website, a bunch of models doing stuff. We take that for granted today, that you

24
00:02:09,880 --> 00:02:14,840
would have models just running live, powered by GPUs. To my knowledge, you were

25
00:02:14,840 --> 00:02:20,160
the first doing it. And what was absolutely mind blowing was you would, you

26
00:02:20,160 --> 00:02:24,080
would do what we all do, which is you'll show some some cherry-picked examples.

27
00:02:24,080 --> 00:02:29,560
But then you did something that no one ever does. Not then, not now. You then

28
00:02:29,560 --> 00:02:33,600
broke it in front of us. You would just say, yeah, looks impressive, right? But

29
00:02:33,600 --> 00:02:36,360
let me just show you what happens when you change the wording of the input,

30
00:02:36,360 --> 00:02:41,640
just a little bit. And you said, this stuff barely works. And those words like

31
00:02:41,640 --> 00:02:45,120
seared into my brain. And they did a lot of good, because it made me very skeptical

32
00:02:45,120 --> 00:02:50,240
and pragmatic. So that when you actually got something to work, you didn't just

33
00:02:50,240 --> 00:02:56,440
say, you know, you didn't overhype it. And so, you know, that, that, that is you

34
00:02:56,440 --> 00:03:01,200
in a nutshell, straight, straight shooter. Just tell me like what you were

35
00:03:01,200 --> 00:03:06,200
feeling back in 2018, 2019, you were halfway through, you know, this, this

36
00:03:06,200 --> 00:03:12,120
grand project, you would lay the groundwork. And you had contributed a ton to it.

37
00:03:12,120 --> 00:03:16,120
People don't realize that this whole obsession with muppet names really began

38
00:03:16,120 --> 00:03:23,360
with Elmo, right? Right. Well, John, thanks for remembering this and the demo

39
00:03:23,360 --> 00:03:30,440
and so on. I do feel like the principles that we have are were guides us through

40
00:03:30,440 --> 00:03:37,000
hype and turmoil and ups and downs. And that's the history of AI. But it's also

41
00:03:37,000 --> 00:03:42,480
the future of AI. So let me take the rich things you talk about and break them

42
00:03:42,480 --> 00:03:48,640
down. First of all, we have now phenomenal language models that do quite

43
00:03:48,640 --> 00:03:54,680
remarkable and certainly very impressive things. But our colleague, Kate Metz, of

44
00:03:54,680 --> 00:04:00,080
the New York Times says, never trust an AI demo. So you're absolutely right that if

45
00:04:00,080 --> 00:04:04,400
you, if, if you don't get to kick the tires, if you don't get to ask the right

46
00:04:04,400 --> 00:04:09,240
questions, then you don't really know how well it does. And by the way, the best

47
00:04:09,240 --> 00:04:14,520
demonstration of that is actually in all of our living rooms with, or in our

48
00:04:14,520 --> 00:04:19,000
phones, right, with Siri and Alexa and so on, you ask Alexa for something you can

49
00:04:19,000 --> 00:04:23,080
get a phenomenal answer. And then you change the wording slightly and it says,

50
00:04:23,080 --> 00:04:29,360
I don't understand it. So we have to be very careful with what we impute to these

51
00:04:29,360 --> 00:04:34,200
systems. And of course, there was a recent Bruhaha about, is this Google AI

52
00:04:34,200 --> 00:04:41,400
system sentient and, and so on. And of course, of course, it's not. So, so I do

53
00:04:41,400 --> 00:04:49,360
think it is very important, as you say, to, to, to be a straight shooter. And it's

54
00:04:49,360 --> 00:04:53,360
also true that one of my favorite sayings, because sometimes people tune in

55
00:04:53,360 --> 00:04:57,720
and they're like, wow, this is amazing. One, another favorite saying of mine is,

56
00:04:57,720 --> 00:05:03,400
our overnight success has been 30 years in the making. So if you look at a

57
00:05:03,400 --> 00:05:10,200
model like GPT-3 or Lambda or the, the latest of the bunch, they do have a long

58
00:05:10,200 --> 00:05:14,760
history that goes back to Bert, goes back to Elmo, which you kindly remembered,

59
00:05:14,760 --> 00:05:21,640
which was invented at AI2, won the best paper awarded 2018, goes back to

60
00:05:21,640 --> 00:05:27,640
Word2Vec, which came to Google, and actually goes all the way back to the 50s, where a

61
00:05:27,640 --> 00:05:32,360
linguist, the fairy, called correctly named Harris, said, you shall know a word by

62
00:05:32,360 --> 00:05:37,320
the company it keeps, right? It's almost biblical in, in the way he phrased it,

63
00:05:37,320 --> 00:05:42,360
right? And it explains most, most of NLP today. Exactly. Exactly. That's the

64
00:05:42,360 --> 00:05:46,760
underlying principle that we can understand. The meaning of words, and from there,

65
00:05:46,760 --> 00:05:52,440
the meaning of sentences, and even beyond, simply by looking at their context,

66
00:05:52,440 --> 00:05:59,320
and looking at a large number of context. So in some sense, all of NLP today,

67
00:05:59,320 --> 00:06:05,400
if I were told, stand on one foot and explain all of NLP today, I would say,

68
00:06:05,400 --> 00:06:11,480
you shall know the word by the company it keeps, but multiply that by a billion

69
00:06:11,480 --> 00:06:17,320
or ten billion companies context, and you'll have NLP today. But it's pretty

70
00:06:17,320 --> 00:06:20,680
revolutionary, right? Because there was a whole period where we thought grammar

71
00:06:20,680 --> 00:06:25,800
mattered, like encoding the rules of grammar. We thought that was really important.

72
00:06:25,800 --> 00:06:30,360
I think that grammar does matter, but the remarkable thing about

73
00:06:30,360 --> 00:06:34,840
this technology, particularly when it's played out with large amounts of data, right?

74
00:06:34,840 --> 00:06:39,080
A billion, ten billion sentences, and large amount of CPU power,

75
00:06:39,080 --> 00:06:44,360
is that that data processing can recover the rules of grammar,

76
00:06:44,360 --> 00:06:49,720
nuances of semantics, et cetera. So it's not that grammar doesn't matter, is that

77
00:06:49,720 --> 00:06:54,280
this technology is remarkably good at least approximating

78
00:06:54,280 --> 00:06:57,560
very, very well those rules that we have. And of course, by the way,

79
00:06:57,560 --> 00:07:02,120
we know that people only approximate those rules too, right? We often say things

80
00:07:02,120 --> 00:07:06,120
and write things that are ungrammedical, but kind of sound right.

81
00:07:06,120 --> 00:07:11,240
So it's really doing probably a better job modeling

82
00:07:11,240 --> 00:07:15,560
language than the rules of grammar. Before you got into

83
00:07:15,560 --> 00:07:19,640
institution building, how would you describe yourself as a practitioner

84
00:07:19,640 --> 00:07:23,800
scholar in the lens of today? You weren't an NLP guy necessarily.

85
00:07:23,800 --> 00:07:26,760
You weren't, you know, how would you describe yourself? I've always been

86
00:07:26,760 --> 00:07:30,840
fascinated with two questions. The first one is one of the most

87
00:07:30,840 --> 00:07:35,320
fundamental intellectual questions across all of science and philosophy.

88
00:07:35,320 --> 00:07:40,040
What is the nature of intelligence? How do we build an intelligent machine?

89
00:07:40,040 --> 00:07:44,200
Over time, I've also added the ethical question, which maybe we'll have a

90
00:07:44,200 --> 00:07:47,720
chance to get into. Should we build an intelligent machine? And what would that

91
00:07:47,720 --> 00:07:52,360
mean for humanity? What would it mean for society? But that's one part.

92
00:07:52,360 --> 00:07:56,600
And the second part of me that's a lot more practical, the part that's

93
00:07:56,600 --> 00:08:01,160
founded startups and that delights in technologies is asked,

94
00:08:01,160 --> 00:08:07,160
how can we use AI to build valuable technology and search and software agents

95
00:08:07,160 --> 00:08:10,760
in that kind of process? What was that conversation like that early

96
00:08:10,760 --> 00:08:15,480
conversation with Paul Lallon where you were making this picture?

97
00:08:15,480 --> 00:08:18,680
Was it he making this pitch? Did you come to it together?

98
00:08:18,680 --> 00:08:22,840
How did it come about? And for those who, you know, there might be some of the

99
00:08:22,840 --> 00:08:26,040
audience who don't know Paul Lallon is the co-founder of Microsoft.

100
00:08:26,040 --> 00:08:30,680
Sadly, passed away pretty recently. But an intellectual maverick.

101
00:08:30,680 --> 00:08:36,840
He actually was an idea man. And that is the title of his autobiography,

102
00:08:36,840 --> 00:08:40,840
which I really recommend to people. It's really worth reading.

103
00:08:40,840 --> 00:08:46,360
And I think that his key role in Microsoft, particularly early on,

104
00:08:46,360 --> 00:08:50,840
was to have that vision of the PC revolution and what it would mean. It's

105
00:08:50,840 --> 00:08:55,240
hard to imagine now, right? We've got a computer in every pocket and in our

106
00:08:55,240 --> 00:09:01,000
eyeglasses and, you know, 200 computers in our car. But back then, right, computers

107
00:09:01,000 --> 00:09:07,000
were far from ubiquitous. And the idea that we'd have a computer on every desk

108
00:09:07,000 --> 00:09:11,560
was completely revolutionary. So Paul Lallon was a visionary.

109
00:09:11,560 --> 00:09:16,920
And I found talking to him incredibly inspiring, right? And I'm not paying to say

110
00:09:16,920 --> 00:09:23,480
that. The poor man has passed away. But he is and will always remain one of my

111
00:09:23,480 --> 00:09:30,920
absolute heroes and not idle, but inspirations mentors for his

112
00:09:30,920 --> 00:09:35,480
relentless focus on, you might call it the prize. And the prize not being

113
00:09:35,480 --> 00:09:39,400
a billion dollars or a trillion dollars, the prize being

114
00:09:39,400 --> 00:09:43,640
how do we understand intelligence? And of course, he had a whole other

115
00:09:43,640 --> 00:09:47,480
institute, the Allen Institute of Brain Science, that was dedicated, that is

116
00:09:47,480 --> 00:09:51,560
dedicated to understanding the brain. It's like the wet lab side of this.

117
00:09:51,560 --> 00:09:55,800
Exactly. Somebody wants to ask him, do you think that the neuroscience

118
00:09:55,800 --> 00:09:59,080
approach, the wet lab, is going to be successful in the

119
00:09:59,080 --> 00:10:03,720
lumbering or is it going to be the more software-oriented approach that we use

120
00:10:03,720 --> 00:10:07,720
in AI? And he said, look, it made so a horse race. And I've got a bet on

121
00:10:07,720 --> 00:10:11,880
on both horses. So what was the race, though? Did he want

122
00:10:11,880 --> 00:10:15,240
artificial general intelligence? Did he want to just crack the scientific

123
00:10:15,240 --> 00:10:18,120
mystery of what it is? Did he want to harness it?

124
00:10:18,120 --> 00:10:22,520
Like, what did those pitch meetings look like? Paul was fascinated, and I

125
00:10:22,520 --> 00:10:26,360
continue to be fascinated by two related questions.

126
00:10:26,360 --> 00:10:29,560
The first one is absolutely. The most hairy,

127
00:10:29,560 --> 00:10:33,560
audacious, big question you could ask, which is, what is the nature of

128
00:10:33,560 --> 00:10:37,160
intelligence and human level intelligence? You know, no

129
00:10:37,160 --> 00:10:43,000
constellation prizes, the real thing. And so he was always asking us about that.

130
00:10:43,000 --> 00:10:46,280
He was always relentlessly looking to the future and saying, okay,

131
00:10:46,280 --> 00:10:49,720
what would it take to get there? How can I help you?

132
00:10:49,720 --> 00:10:55,080
Does this scale? The second thing, and I think it comes from his fascination with

133
00:10:55,080 --> 00:11:00,680
human knowledge. His mother was a librarian. So he was fascinated with how do we

134
00:11:00,680 --> 00:11:04,840
collect human knowledge? And how do we get a computer to understand it?

135
00:11:04,840 --> 00:11:10,520
Back in the 70s, I believe, early 70s, he said, look, it's one thing

136
00:11:10,520 --> 00:11:14,600
to take a book, collect all the words in the book, and put them in an

137
00:11:14,600 --> 00:11:18,200
index, effectively what today we call a search engine.

138
00:11:18,200 --> 00:11:21,560
And it's quite another thing to understand the meaning of the book

139
00:11:21,560 --> 00:11:25,720
and answer the questions at the back of the book, think of exercises

140
00:11:25,720 --> 00:11:30,040
at the end of each chapter in a textbook. And so even in the 70s, before a lot of

141
00:11:30,040 --> 00:11:35,080
this technology was around, he understood that meaning,

142
00:11:35,080 --> 00:11:40,760
understanding the meaning of text, of knowledge, was very, very

143
00:11:40,760 --> 00:11:44,920
tough for a computer. I mean, have we even gotten closer, though,

144
00:11:44,920 --> 00:11:48,200
or are we fooling ourselves? You know, what I think about is,

145
00:11:48,200 --> 00:11:51,880
I use semantic search all the time just at work. It's a great tool.

146
00:11:51,880 --> 00:11:55,480
It's really powerful, but it's so easily fooled. You sort of cracks through the

147
00:11:55,480 --> 00:11:58,920
shell and you realize, if this thing understands

148
00:11:58,920 --> 00:12:03,000
scare quotes, what it's reading, it's doing it in a very different way from me,

149
00:12:03,000 --> 00:12:06,680
because I can just change a single word and consequently to me,

150
00:12:06,680 --> 00:12:10,120
and it just falls apart. It clearly doesn't understand it the way I do.

151
00:12:10,120 --> 00:12:13,400
So are we chasing up the wrong tree when we say,

152
00:12:13,400 --> 00:12:17,320
we're chasing a text understanding? Or is it all just performance-based?

153
00:12:17,320 --> 00:12:20,360
We don't really care if it understands. We care about getting jobs done.

154
00:12:20,360 --> 00:12:24,120
Find the documents that are about, you know, four-legged animals that

155
00:12:24,120 --> 00:12:28,680
love to bark. You know, if I didn't know the word dog, but I knew what I was,

156
00:12:28,680 --> 00:12:32,120
I could describe what I was after. We would love a computer system that would

157
00:12:32,120 --> 00:12:35,960
just find the right stuff. Even if it had no idea, and I don't care if it

158
00:12:35,960 --> 00:12:39,240
has any idea, let alone feelings about what I'm searching for.

159
00:12:39,240 --> 00:12:43,080
Well, John, you're asking the most profound

160
00:12:43,080 --> 00:12:46,840
question at the heart of this field. I'm not sure I can answer them.

161
00:12:46,840 --> 00:12:51,960
25 words or less, but let me take some shots of goal and it'll be more of a

162
00:12:51,960 --> 00:12:57,480
dialogue. So to the question to us, is it performance-based?

163
00:12:57,480 --> 00:13:02,680
The first answer is that our performance has gone way up, right? So if you take

164
00:13:02,680 --> 00:13:07,400
any objective measure, and there are many, and back in a day, we were

165
00:13:07,400 --> 00:13:11,800
interested in, can a computer answer an eighth-grade test, right? The

166
00:13:11,800 --> 00:13:16,200
region science exam, the SATs, and initially the answer was the

167
00:13:16,200 --> 00:13:19,480
resounding note. It did a little better than

168
00:13:19,480 --> 00:13:23,720
chance on multiple-choice questions. It was getting close to 25 percent, and fast

169
00:13:23,720 --> 00:13:29,160
forward, now we can get 80 or 90 percent, you know, better than the

170
00:13:29,160 --> 00:13:32,280
most high school students. And I really wonder how it's doing it.

171
00:13:32,280 --> 00:13:36,280
I really wonder. Well, so we know. We have a lot of

172
00:13:36,280 --> 00:13:40,120
insight into that, and I'll get to in a second. But to take the performance

173
00:13:40,120 --> 00:13:44,440
question, we can check the box. We now have exceptional performance.

174
00:13:44,440 --> 00:13:47,640
But now we're debating as you're raising the question of,

175
00:13:47,640 --> 00:13:53,240
because what does that performance mean? And there's a famous saying from

176
00:13:53,240 --> 00:13:57,320
Herb Drifus, the late philosopher from Berkeley, who said,

177
00:13:57,320 --> 00:14:01,880
look, we've run up to the top of the tree and we're shouting that we're on our way

178
00:14:01,880 --> 00:14:09,160
to the moon, right? It doesn't scale, and it's not really a way to do spaceflight,

179
00:14:09,160 --> 00:14:15,960
we're just to decline the tree. So, so I, right, right. So, again, that

180
00:14:15,960 --> 00:14:20,760
metaphor is not drawn to scale, right? It's more than a tree, but it's still the

181
00:14:20,760 --> 00:14:25,000
technology that will get you to the space station, to kind of

182
00:14:25,000 --> 00:14:28,440
riff on this metaphor. It may not be the technology that gets you to

183
00:14:28,440 --> 00:14:32,280
Mars, and certainly not the technology that gets you out of the solar system,

184
00:14:32,280 --> 00:14:37,560
right? So, so I think that when we talk about

185
00:14:37,560 --> 00:14:43,080
competence, when we talk about genuine understanding, there's a real debate

186
00:14:43,080 --> 00:14:48,600
in the field, and there's some people like Gary Marcus, who is brilliant,

187
00:14:48,600 --> 00:14:54,120
and pointing out how this technology falls short, and we can see that these

188
00:14:54,120 --> 00:14:58,680
large language models do things that are called hallucination. You ask it

189
00:14:58,680 --> 00:15:02,520
questions that are meant to trip it up, like, who was the president of the

190
00:15:02,520 --> 00:15:07,800
United States in 1492? And it'll answer something like Columbus, right? It

191
00:15:07,800 --> 00:15:11,960
won't realize that the United States didn't have, didn't exist in

192
00:15:11,960 --> 00:15:17,320
1492, didn't have a president. So, there's hallucination, there's lack of

193
00:15:17,320 --> 00:15:21,400
robustness, right? You paraphrase the question, and if you ask me the same

194
00:15:21,400 --> 00:15:24,680
question different words, most likely I would say, hey John, that's the same

195
00:15:24,680 --> 00:15:29,560
question. I'm going to give you the same answer, but AI technology will not.

196
00:15:29,560 --> 00:15:32,840
And by the way, by the way, that was a perfect demonstration of Yeeshale

197
00:15:32,840 --> 00:15:36,760
Noah word by the company it keeps. You know, the machine sees the string

198
00:15:36,760 --> 00:15:42,040
1492, it basically has seen enough. It knows you want to look for a person,

199
00:15:42,040 --> 00:15:46,680
Columbus pops right up. And so that's a case of the dumb pet trick with data

200
00:15:46,680 --> 00:15:52,520
failing you. That's exactly right. The remarkable thing is every time

201
00:15:52,520 --> 00:15:59,720
we identify a trap like this, a phenomena, a place where AI trips up,

202
00:15:59,720 --> 00:16:05,880
well our colleagues who are deep learning gurus just get more training data,

203
00:16:05,880 --> 00:16:10,520
just modify the training regime and they solve that one. And so we're

204
00:16:10,520 --> 00:16:17,160
exactly. So is it, is it a game of guacamole or is there a fundamental paradigm

205
00:16:17,160 --> 00:16:23,320
that goes all the way to human and level intelligence? I would say that that's

206
00:16:23,320 --> 00:16:27,160
the question of the age. And I would look to people who are a lot deeper

207
00:16:27,160 --> 00:16:33,080
into deep learning part in the inadvertent pun, like Jeff Hinton and Jan LaCune

208
00:16:33,080 --> 00:16:37,160
right there, touring award winners. And I would say the data themselves

209
00:16:37,160 --> 00:16:41,960
while they're very much enamored of deep learning and this kind of paradigm say

210
00:16:41,960 --> 00:16:47,400
that the current underlying algorithm, the current algorithms, I should say

211
00:16:47,400 --> 00:16:51,960
back propagation supervised learning current neural network architectures

212
00:16:51,960 --> 00:16:56,760
don't take us all the way there. They see the limitations of the current

213
00:16:56,760 --> 00:17:01,800
technology, but they do see that the paradigm this distributed computing with

214
00:17:01,800 --> 00:17:06,280
simple computing elements and weight updates on edges between them is the

215
00:17:06,280 --> 00:17:11,480
foundation for a March more sophisticated architecture that will get us

216
00:17:11,480 --> 00:17:15,560
all the way there. And of course if we look to the brain, right neural networks

217
00:17:15,560 --> 00:17:20,040
are a gross gross simplification of the brain, but we do have an

218
00:17:20,040 --> 00:17:23,480
existence, right? We do know. There's none of one,

219
00:17:23,480 --> 00:17:27,800
none of one. Exactly. So here's a great quote from you

220
00:17:27,800 --> 00:17:34,840
that is a good seg to dive into the AI2 impact. So you said AI2 is the place

221
00:17:34,840 --> 00:17:39,560
to do work that companies won't do and universities can't.

222
00:17:39,560 --> 00:17:44,040
So I think that really to me captures the weirdness of this thing you built.

223
00:17:44,040 --> 00:17:48,520
It is neither a university nor a company. What is it?

224
00:17:48,520 --> 00:17:52,760
Well, first to give credit where credit is due, which is always extremely

225
00:17:52,760 --> 00:17:58,040
important in academia, but we don't do it for the money. That is a quote that I

226
00:17:58,040 --> 00:18:05,560
repeat from my colleague, Noah Smith, who's a professor at UW and

227
00:18:05,560 --> 00:18:10,520
leader at AI2, but it's a wonderful characterization of what we do.

228
00:18:10,520 --> 00:18:14,760
And sorry, John, so you repeat the question. How do we do it? Why do we do it?

229
00:18:14,760 --> 00:18:18,200
I wasn't sure. No more like what is it? Like it's this

230
00:18:18,200 --> 00:18:24,040
strange hybrid. It's had all this impact, but without any of the benefits or

231
00:18:24,040 --> 00:18:27,400
problems of being a company, nor any of the benefits or problems of being

232
00:18:27,400 --> 00:18:30,840
university. It's like, I don't know many things, like it.

233
00:18:30,840 --> 00:18:35,160
Also, you've tagged on an incubator to it now. So it's like definitely unique.

234
00:18:35,160 --> 00:18:40,120
So I'm a product of the university system. I was a grad student. I was a professor

235
00:18:40,120 --> 00:18:45,640
for more than 20 years. And I love that system for

236
00:18:45,640 --> 00:18:49,560
intellectual exploration, for intellectual freedom,

237
00:18:49,560 --> 00:18:54,760
for the kind of debate and surprises that it produces.

238
00:18:54,760 --> 00:18:59,080
But it does fall short when you're trying to build systems.

239
00:18:59,080 --> 00:19:04,360
Some problems require a sustained effort over some number of years,

240
00:19:04,360 --> 00:19:09,960
requires engineering sophistication. And it's hard to do that with

241
00:19:09,960 --> 00:19:15,080
students who need to graduate. And actually, it's not even fair to ask

242
00:19:15,080 --> 00:19:19,160
students to play engineer for years on it. Right, because you have to worry about

243
00:19:19,160 --> 00:19:22,920
their education. Exactly. That's the primary

244
00:19:22,920 --> 00:19:27,000
goal. So over time, over my 20 plus years at university,

245
00:19:27,000 --> 00:19:32,600
I did rue sometimes how, gosh, we really want some things to go into the real

246
00:19:32,600 --> 00:19:36,360
world and get more sustained investment, and just tossing them over the

247
00:19:36,360 --> 00:19:40,200
transom, writing a paper, writing a research prototype, and hoping

248
00:19:40,200 --> 00:19:44,600
that somebody will pick up the ball. So now, too, where we have researchers and

249
00:19:44,600 --> 00:19:48,280
engineers working shoulder to shoulder. And that's an important part of it.

250
00:19:48,280 --> 00:19:51,480
It's a egalitarian community. It's not the case that the

251
00:19:51,480 --> 00:19:54,680
researchers up on top of Mount Olympus, and they're

252
00:19:54,680 --> 00:19:58,120
cracking the web to mix the metaphors of telling engineers, do this, do that.

253
00:19:58,120 --> 00:20:00,440
It's much more the case that they're collaborating

254
00:20:00,440 --> 00:20:04,200
the engineers are telling them, look, here's what you need to do to build

255
00:20:04,200 --> 00:20:07,480
a working system. We have semantic scholar, that John, of course,

256
00:20:07,480 --> 00:20:11,320
you're intimately familiar with. You've written about

257
00:20:11,320 --> 00:20:14,280
you and you were one of the folks to first

258
00:20:14,280 --> 00:20:18,120
announce it to the broader community when you were writing for science and so on,

259
00:20:18,120 --> 00:20:22,760
which was wonderful for us. So something like semantic scholar, which to

260
00:20:22,760 --> 00:20:27,000
those who don't know, it's a free search engine for scientific content.

261
00:20:27,000 --> 00:20:31,720
It has, you know, it's approaching 100 million users a year.

262
00:20:31,720 --> 00:20:36,200
It has 200 million papers in its corpus. That sort of scale and running AI at

263
00:20:36,200 --> 00:20:39,560
that scale requires a lot of engineering. We have a

264
00:20:39,560 --> 00:20:44,440
very strong engineering team, folks who came out of Amazon and Google and

265
00:20:44,440 --> 00:20:48,440
other places to be able to do that. And you just could not build

266
00:20:48,440 --> 00:20:51,800
semantic scholar, build it, sustain it, iterate on it,

267
00:20:51,800 --> 00:20:55,880
and you can build a prototype. Actually, you know what a good comparison point is,

268
00:20:55,880 --> 00:21:00,520
is archive. So archive grew out of the university system. It's

269
00:21:00,520 --> 00:21:04,920
sustained by the university system. And you can see how far you can get with

270
00:21:04,920 --> 00:21:09,080
a system. You know, archive and semantic scholar are like worlds apart.

271
00:21:09,080 --> 00:21:12,440
Semantic scholar is a full product with, you know,

272
00:21:12,440 --> 00:21:17,640
incredible amounts of hand engineering and maintenance

273
00:21:17,640 --> 00:21:21,800
and users. And, you know, I think archive is about as far as you can get.

274
00:21:21,800 --> 00:21:24,280
A preprint server that puts PDFs on the website.

275
00:21:24,280 --> 00:21:29,400
And even archive is an exception, right? It's rare that you have...

276
00:21:29,400 --> 00:21:34,840
It's a gem. It's a gem. Yes, absolutely. But it's rare, as you say, that you have

277
00:21:34,840 --> 00:21:40,120
a university system that can operate at a very large scale.

278
00:21:40,120 --> 00:21:44,200
So, and then on the other hand, we have no profit mode.

279
00:21:44,200 --> 00:21:47,800
Semantic scholar does not have a business model.

280
00:21:47,800 --> 00:21:50,920
I don't think there's a good business model in that space because it's

281
00:21:50,920 --> 00:21:55,960
it's meant to be free. I know you have opinions about academic publishing.

282
00:21:55,960 --> 00:21:59,400
Yes, yes. Well, we can get into that if you want. But,

283
00:21:59,400 --> 00:22:05,160
yeah, I think too much money is made in

284
00:22:05,160 --> 00:22:08,840
over things that really ought to be free to benefit humanity.

285
00:22:08,840 --> 00:22:13,320
And maybe to bring this full circle to the late Paul Allen,

286
00:22:13,320 --> 00:22:18,040
our mission is AI for the common good. So, that's why I say, you know,

287
00:22:18,040 --> 00:22:21,800
universities can't, but companies won't. Companies appropriately, right,

288
00:22:21,800 --> 00:22:24,680
have the mission. They're for profit mission.

289
00:22:24,680 --> 00:22:29,880
But Paul Allen is a major philanthropist. He won philanthropist of the

290
00:22:29,880 --> 00:22:33,320
year award a few years before he passed away.

291
00:22:33,320 --> 00:22:36,520
He wanted to make the world a better place.

292
00:22:36,520 --> 00:22:39,560
The Allen Institute of Brain Science released a free brain

293
00:22:39,560 --> 00:22:42,760
app list that was a tremendous resource,

294
00:22:42,760 --> 00:22:46,840
catapulting research in that realm. And our mission has always been to bring to

295
00:22:46,840 --> 00:22:50,600
the foreign release systems, data sets,

296
00:22:50,600 --> 00:22:55,000
open source software that helped to bring the field forward.

297
00:22:55,000 --> 00:22:59,720
So, what's up with this incubator then? Is this, is this, so

298
00:22:59,720 --> 00:23:04,360
the what, from what little I know about it, you have added a startup incubator

299
00:23:04,360 --> 00:23:08,920
to AI2. So, that ideas that presume can spin out and have a chance to

300
00:23:08,920 --> 00:23:14,040
be nurtured. Is that, is that you hedging against,

301
00:23:14,040 --> 00:23:17,640
like sometimes actually companies are the right way to solve problems?

302
00:23:17,640 --> 00:23:21,480
Or is it, what is that? Is it for the future health of AI2?

303
00:23:21,480 --> 00:23:28,680
Not, not really. So, from, from day one, we had an incubator in recent times with a,

304
00:23:28,680 --> 00:23:34,600
yeah, yeah, it started, our very first startup, Kid AI, it was actually started in,

305
00:23:34,600 --> 00:23:38,760
you know, 2014, 2014. Oh, how did I miss this? It was just not well-known.

306
00:23:38,760 --> 00:23:42,840
Well, because it was very small. And then once we got the right leaders in place,

307
00:23:42,840 --> 00:23:46,200
it's grown, it's grown and grown, and now we're approaching

308
00:23:46,200 --> 00:23:50,520
three-quarters of a billion dollars in the total valuation of companies

309
00:23:50,520 --> 00:23:54,280
founded and acquired our company X-NOR, which was a

310
00:23:54,280 --> 00:24:00,360
computer vision at the edge company, was, was acquired by Apple, and we've done

311
00:24:00,360 --> 00:24:05,880
now more than 20 companies in the precede stage. The, the analogy is to think of a

312
00:24:05,880 --> 00:24:10,920
university and their commercialization centers. So, University, Washington,

313
00:24:10,920 --> 00:24:15,320
where I was, as one, Stanford has one, of course, the famously created Google and

314
00:24:15,320 --> 00:24:20,600
Yahoo, and other, other major companies. And it's actually, in my mind, a natural

315
00:24:20,600 --> 00:24:25,640
part of the life cycle of universities, that some ideas and technologies that are

316
00:24:25,640 --> 00:24:29,880
created in a very nascent, you know, encipient form in the university,

317
00:24:29,880 --> 00:24:35,400
it makes sense to transfer them to a for-profit context, and that's where you both

318
00:24:35,400 --> 00:24:41,240
get the resources to make them shine, right, to take them to the next level.

319
00:24:41,240 --> 00:24:47,400
And also, you get the opportunity, right, to, to create value,

320
00:24:47,400 --> 00:24:51,800
value creation in my mind is a, is a great thing. I'm not in any way a socialist

321
00:24:51,800 --> 00:24:55,960
thing. Oh, we should all just be working for the common good. I think some of us

322
00:24:55,960 --> 00:24:59,640
should be working for the common good, and I feel very privileged to be in that

323
00:24:59,640 --> 00:25:04,920
position. Some of us should be in startups, figuring out how to revolutionize

324
00:25:04,920 --> 00:25:11,400
the world and make a killing at it, even though I disagree very strongly with, say,

325
00:25:11,400 --> 00:25:17,960
Elon Musk views about AI. I very much, and blown away with this success with Tesla,

326
00:25:17,960 --> 00:25:21,880
right, so we wouldn't have Tesla if we didn't have for-profit startups.

327
00:25:21,880 --> 00:25:26,680
Hang on, which part do you disagree with? The robots are going to kill us?

328
00:25:26,680 --> 00:25:33,320
Oh, yeah, yeah. Elon Musk is famous for having said with AI, we're summoning the demon,

329
00:25:33,320 --> 00:25:39,080
and I think that's just, that's hype. And actually, the worst kind of hype, it's hype from

330
00:25:39,080 --> 00:25:45,720
somebody who you think would know about, right? He's such a brilliant man. If he gives a lot of

331
00:25:45,720 --> 00:25:52,040
credence to statements that are just not rooted in any data. Although he's not alone. There are a lot

332
00:25:52,040 --> 00:26:00,280
of cautious voices. He's just the biggest on Twitter. He's the biggest and he's, you know,

333
00:26:00,280 --> 00:26:04,760
the most articulate, but I do agree that there's an interesting conversation

334
00:26:04,760 --> 00:26:12,120
around this issue. I feel very strongly that we don't have any basis for some of these fears about

335
00:26:12,120 --> 00:26:17,480
AI. I've written about this. And like you mentioned earlier, right? You've had the

336
00:26:17,480 --> 00:26:23,320
own experience. Anybody who's built an AI system knows just how much blood, sweat, and tears

337
00:26:24,200 --> 00:26:32,600
we put in to eat out the modest level of performance that we get. Let alone this AI that's free

338
00:26:32,600 --> 00:26:39,720
for taking over humanity can't be turned off that we see in Hollywood movies. So I think it's

339
00:26:39,720 --> 00:26:46,200
really important to distinguish science from science fiction and hype in Hollywood from the reality.

340
00:26:46,200 --> 00:26:52,120
Other people just extrapolate more strongly from the future. They have ideas like hard take off.

341
00:26:52,120 --> 00:26:58,600
Sure, AI is not very powerful now. But what if you turn your back on it? What if all of a sudden

342
00:26:58,600 --> 00:27:03,800
there's a sharp increase? You know, you leave for the weekend and you come back and on Monday,

343
00:27:03,800 --> 00:27:10,280
this fat AI like your testers in charge? Exactly. It's smoking a cigar and saying,

344
00:27:10,280 --> 00:27:18,520
I've been expecting you, Dr. Atseomi. And it's just, it's not realistic, but to understand that

345
00:27:19,240 --> 00:27:26,600
you have to get a lot more technical. I do want to share two metaphors that I think help that.

346
00:27:26,600 --> 00:27:30,280
One is these technologies that we're talking about, where we tune the

347
00:27:31,000 --> 00:27:37,000
edges on the, sorry, the weights on edges in a neural network, which is what our deep learning

348
00:27:37,000 --> 00:27:43,160
technology is doing. That technology is the moral equivalent, if you're not a technical person,

349
00:27:43,160 --> 00:27:49,560
of adjusting the gain and the equalizer and various buttons on your stereo. And there's a

350
00:27:49,560 --> 00:27:54,200
accept you have billions of dials in this case. You have billions of dials, you're adjusting them

351
00:27:54,200 --> 00:27:59,880
automatically, but after you've adjusted them really well, it's still just going to be a stereo.

352
00:27:59,880 --> 00:28:04,040
There's no way that you find the right adjustment on lots of dials in your stereo,

353
00:28:04,040 --> 00:28:09,400
that'll become the desktop. It's still going to be a stereo. The same with these large language

354
00:28:09,400 --> 00:28:14,680
models that again, as I mentioned, there's a lot of, are they sentient and so on? Those large

355
00:28:14,680 --> 00:28:21,160
language models are basically mirrors, okay? They mirror by collecting this corpus of all these

356
00:28:21,160 --> 00:28:27,320
words in the company they keep. They mirror that collected discourse back at us.

357
00:28:27,320 --> 00:28:31,800
And when we look at the mirror, we see, we can see glimmers of intelligence because we see

358
00:28:31,800 --> 00:28:37,240
reflection of our own discourse. The thing that's important to realize about a mirror technology

359
00:28:37,240 --> 00:28:42,520
is that you can scale the mirror. You can have a very large mirror, but a very large mirror is

360
00:28:42,520 --> 00:28:47,640
not going to turn into a dust star. I definitely agree in the second point. I think of these

361
00:28:47,640 --> 00:28:54,040
large language models as data telescopes. They're just amazing devices to look back on all this

362
00:28:54,040 --> 00:28:58,760
amazing data we have ourselves created with language, which is its own mystery. So really,

363
00:28:58,760 --> 00:29:03,640
we're just looking at our own mystery. But on the first one, I would say, biologists,

364
00:29:05,000 --> 00:29:10,200
so you said, hey, it's just a big stereo. It may be impressively large and it may be

365
00:29:10,200 --> 00:29:14,920
twiddling its own dials, but it's still just a stereo. And I think a lot of biologists would say,

366
00:29:14,920 --> 00:29:22,440
well, you know, I can show you a cell that, you know, an amoeba that is really just running around

367
00:29:22,440 --> 00:29:27,800
trying to gobble up food and, you know, make more amoebas. And it's not that different from a neuron.

368
00:29:28,680 --> 00:29:37,960
It's really just a lot of very strange natural history that led to the job of being a neuron

369
00:29:37,960 --> 00:29:43,080
as a cell. And yet, when you add them all together, you know, you get walking, talking goofballs

370
00:29:43,080 --> 00:29:48,760
like you and me. And so either you admit that we're not that special or you admit that there's

371
00:29:48,760 --> 00:29:54,600
something special in the system. But that's all. That's basically kept philosophy grad students

372
00:29:54,600 --> 00:30:00,920
in business for all time. But I do want to address your comment because I think it's an important

373
00:30:00,920 --> 00:30:06,520
one. And where I would take exception is with the word app. So cells are the basic building

374
00:30:06,520 --> 00:30:13,320
block of life, guaranteed. Neurons are the basic building block of the brain. We have neural

375
00:30:13,320 --> 00:30:19,880
networks. The units and neural networks are actually very, very much simplified relative to a neuron.

376
00:30:19,880 --> 00:30:26,680
But never mind that. I would accept that perhaps we have discovered some of the basic building blocks.

377
00:30:26,680 --> 00:30:32,520
But it's not the case today that I can give you a cell and say, here's a cell. Make me a human,

378
00:30:32,520 --> 00:30:40,920
right? Far from it. And that we understand. Unless, of course, that cell is a fertilized egg.

379
00:30:40,920 --> 00:30:46,840
Sure. Sure. But it turns out to be pretty easy. Well, that's the natural process, right? And we're

380
00:30:46,840 --> 00:30:56,840
going to keep this PT rated job, right? So what I'm saying is that we don't know how to artificially

381
00:30:56,840 --> 00:31:02,200
produce a cell. And even if we did, we wouldn't know how to turn that cell into a human.

382
00:31:02,200 --> 00:31:09,480
And so even if we had a neuron, right, even if we could build, simulate a neuron in a computer,

383
00:31:09,480 --> 00:31:15,080
we don't know how to turn that into a brain or into human level intelligence. So the organizing

384
00:31:15,080 --> 00:31:20,680
principles are still what was lacking. And one last point, because this is something I'm so

385
00:31:20,680 --> 00:31:26,360
passionate about and actually gets lost sometimes in all the hype and all the excitement about the

386
00:31:26,360 --> 00:31:32,360
technology, the one other point I want to make is, even if somehow we came up with a recipe,

387
00:31:32,360 --> 00:31:38,760
a mechanical process to produce a human by cloning, right, to produce an intelligence by doing the

388
00:31:39,400 --> 00:31:45,240
AI equivalent of cloning, we still want to understand. We want to understand the organizing

389
00:31:45,240 --> 00:31:49,960
principles of how do you build a human? We want to understand the organizing principles of how

390
00:31:49,960 --> 00:31:55,960
do you build an intelligence. So we can fix problems so we can go beyond it. So we can use these

391
00:31:55,960 --> 00:32:01,320
technologies for the common good, right, to cure diseases, to solve. And also to know ourselves

392
00:32:01,320 --> 00:32:07,320
in a deep way. Exactly. All right, let's, let's swerve for a sec. I really don't want to run out of

393
00:32:07,320 --> 00:32:12,520
time before we dig into some of the cool stuff that's actually happening at AI2. So, you know,

394
00:32:12,520 --> 00:32:17,800
back to that point about AI2 being a place where you could do things that companies won't do

395
00:32:17,800 --> 00:32:23,240
in universities can't. Let's dig into a couple of them. So over the years, you've been

396
00:32:23,240 --> 00:32:29,800
absolutely swinging at the fences with, you know, attempts to make an AI system that can solve

397
00:32:30,520 --> 00:32:36,440
math problems, where, you know, that's not directly, you know, to your point about companies won't

398
00:32:36,440 --> 00:32:41,880
do it, you're not going to make a buck out of a, at least directly out of an AI system that can pass

399
00:32:41,880 --> 00:32:46,200
eighth grade math test. It's not relevant. No one's going to pay millions of bucks for that.

400
00:32:47,400 --> 00:32:52,520
But a university can't do it because taking, taking a look at the papers you guys are producing,

401
00:32:52,520 --> 00:32:58,760
the infrastructure required to get there is monumental. So, what are, what are some of the big

402
00:32:58,760 --> 00:33:05,480
swings at the fence that you've been doing that excite you lately? Well, so Semantic Scholar

403
00:33:05,480 --> 00:33:11,800
is the biggest one where we have a scientific search engine built from the way down. We are in the

404
00:33:11,800 --> 00:33:19,560
process of releasing a sub-project of that, headed by Dan Well, who was a professor for many years

405
00:33:19,560 --> 00:33:25,960
at University Washington, joined us to lead this project. It's called the Semantic Reader.

406
00:33:25,960 --> 00:33:30,760
And that is basically when you're reading papers, right? I feel like, if you want to think about

407
00:33:30,760 --> 00:33:35,480
the history of reading scientific papers, okay, we have the cave wall, then we have the printed

408
00:33:35,480 --> 00:33:41,480
page, then we have PDFs, you can read online, and not much progress since then, right? We still

409
00:33:41,480 --> 00:33:48,360
kind of a labor over PDFs. Well, the Semantic Reader allows you to seamlessly look at citations

410
00:33:48,360 --> 00:33:56,600
while you're in text to look up definitions for terms in line to do a lot of things. I don't have

411
00:33:56,600 --> 00:34:04,120
time to describe skimming, things that make the process of reading a scientific paper that much

412
00:34:04,120 --> 00:34:09,000
more efficient. So, is this like a machine reading over your shoulder and like taking notes for

413
00:34:09,000 --> 00:34:15,400
you? Not that sophisticated, it's much more of a tool, right? So, think of Acrobat Reader

414
00:34:15,400 --> 00:34:23,800
Plus Plus. It's souped up to make it easier for you to read. So, here's a very concrete example.

415
00:34:23,800 --> 00:34:29,640
Something that we're very proud of is we've used language models to create TLDRs. One sentence

416
00:34:29,640 --> 00:34:34,680
summary of papers that are really quite high quality. These have been published and measured

417
00:34:34,680 --> 00:34:39,000
and they're really quite good. So often as you're reading a paper, this references to other papers.

418
00:34:39,000 --> 00:34:42,760
And you're like, what do I do? Do I click on that? And suddenly I'm reading another paper,

419
00:34:42,760 --> 00:34:48,920
and they have a reference and I go down some kind of infinite rabbit hole. Exactly, infinite rabbit hole.

420
00:34:48,920 --> 00:34:53,320
Or do I, you know, note it down, but then forget about it? Well, with a semantic reader,

421
00:34:53,320 --> 00:34:59,000
you can hover over that reference and get a TLDR. It says, okay, that's what that paper is about.

422
00:34:59,000 --> 00:35:03,800
And you can make a quick decision. Hey, let me make with one click. I'll save that in my library

423
00:35:03,800 --> 00:35:11,560
for future reference. Or that's not really what I mean, I'll ignore it. So, just little affordances,

424
00:35:11,560 --> 00:35:19,640
little tricks that are enabled by AI that allow you to focus better and just be more efficient at

425
00:35:19,640 --> 00:35:25,320
reading the paper. And we could, is the secret mission here to make AI researchers

426
00:35:25,320 --> 00:35:31,560
better at doing AI with the help of AI in a flywheel? Is this, is this a virtuous cycle?

427
00:35:31,560 --> 00:35:36,200
It's meant to be, but it's to make scientists across all disciplines better,

428
00:35:36,200 --> 00:35:44,120
better at their job. So, if we can make scientists across biomedicine people working on climate change,

429
00:35:44,120 --> 00:35:50,680
what have you? If we can make them 10% more efficient, that is significant. And potentially,

430
00:35:50,680 --> 00:35:57,480
we can make them a lot more efficient, right? If I give you a TLDR that saves you an hour

431
00:35:57,480 --> 00:36:01,960
of groveling through the text or even better allows you to pursue something that you might

432
00:36:01,960 --> 00:36:08,600
have missed, let me give you actually another example. We all now use adaptive feeds. We just

433
00:36:08,600 --> 00:36:13,640
don't call them that. Our Twitter feed, right, is automatically organized by an AI that studies

434
00:36:13,640 --> 00:36:17,240
our Facebook feed. Some people use that. And of course, in that case, the motivation behind the

435
00:36:17,240 --> 00:36:22,600
algorithm is to make you click on ads and spend money. Exactly. Exactly. So, you're doing the same

436
00:36:22,600 --> 00:36:28,280
thing but with a higher purpose? That's exactly right. So, we have a feed for scientific papers.

437
00:36:28,280 --> 00:36:33,400
And you can train it. It'll show you new papers that you might have missed. It might have result,

438
00:36:33,400 --> 00:36:37,560
they might result in amazing breakthroughs. And you'll tell it, I like this. I don't like that.

439
00:36:37,560 --> 00:36:42,680
Yeah, that's interesting to me. And it'll automatically compute tomorrow's feed when new papers

440
00:36:42,680 --> 00:36:48,680
came up in archive and elsewhere to help you in what's ultimately a needle in a haystack search

441
00:36:48,680 --> 00:36:54,840
for finding that key result that you, with your human intelligence, connect with another result

442
00:36:54,840 --> 00:37:01,080
and have this amazing breakthrough. So, yeah, case in point of what you're saying, the entertainment

443
00:37:01,080 --> 00:37:07,240
feeds we have have a profit mode. But who has the motive to help you be a better, more successful

444
00:37:07,240 --> 00:37:13,080
scientist in whatever your field of study is? AI2 does. Would AI for the common good?

445
00:37:13,960 --> 00:37:19,400
Cool. All right, give me a second big bet. Something, something crazier.

446
00:37:19,400 --> 00:37:27,320
Well, we have a scientist working to fight illegal fishing using computer vision. So,

447
00:37:28,200 --> 00:37:35,400
there's a lot of satellite data, but smaller countries don't have the resources to analyze

448
00:37:35,400 --> 00:37:42,520
that data and identify illegal fishing buzz that are impacting their country's livelihood and so on.

449
00:37:42,520 --> 00:37:49,480
So, we've settled up to help solve that problem recently with a product. It's called Skylight.

450
00:37:49,480 --> 00:37:57,560
And we just won a national competition that was actually run out of the government to have the

451
00:37:57,560 --> 00:38:04,120
who's got the best tools for analyzing the satellite data. And AI2 came in first in the US. We're

452
00:38:04,120 --> 00:38:10,520
very proud of that that just happened a few months ago. We are engaged in using deep learning

453
00:38:10,520 --> 00:38:15,160
for climate modeling. We're very interested in the problem of how will precipitation

454
00:38:15,960 --> 00:38:21,080
rain, right? How will that change as climate changes in an unprecedented fashion?

455
00:38:21,080 --> 00:38:25,880
That's incredibly important for agriculture, for irrigation, for making decisions

456
00:38:26,600 --> 00:38:32,120
about the sort of infrastructure you need to keep us fed, right, as the climate changes.

457
00:38:32,120 --> 00:38:38,280
Well, we're using the same types of models to help make these, these sorts of predictions.

458
00:38:38,280 --> 00:38:41,640
But really, the craziest, sorry, John, go ahead.

459
00:38:41,640 --> 00:38:45,800
I was just going to ask, I had recently heard the deep learning had found its way into weather

460
00:38:45,800 --> 00:38:50,200
modeling and I didn't read enough into it to understand how it kind of baffles me. Why would you

461
00:38:50,200 --> 00:38:55,800
use a neural network to make such a model? But at the end of the day, it is just prediction

462
00:38:55,800 --> 00:38:57,960
and deep learning is the ultimate prediction engine.

463
00:38:58,760 --> 00:39:04,200
That's exactly the answer. Whenever you have a lot of data and you want to make a prediction,

464
00:39:04,200 --> 00:39:09,640
we've learned that deep learning models are almost invariably really, really strong.

465
00:39:09,640 --> 00:39:14,200
But I want to get to the craziest project and maybe this is what you're alluding to,

466
00:39:14,200 --> 00:39:19,480
and that's the problem of common sense. So that's a problem that's been a holy grail for AI.

467
00:39:19,480 --> 00:39:24,120
How do we build a machine that has common sense? It's been a holy grail of AI for decades,

468
00:39:24,120 --> 00:39:29,160
but there really hasn't been much progress on it until recently where Yijin Choi,

469
00:39:29,160 --> 00:39:35,160
who's a professor at the University of Washington, shares her time with AI2, she's leading a team

470
00:39:35,160 --> 00:39:43,240
that works across both organizations to figure out how to end out computers with common sense.

471
00:39:43,240 --> 00:39:49,560
How they can, you know, if I ask you, can an elephant fit through a doorway, you would say

472
00:39:49,560 --> 00:39:55,800
probably not. If I ask you, what's bigger, a nickel, right, the coin, or the sun, you would say,

473
00:39:55,800 --> 00:40:00,600
or you're being silly, why ask me these questions? But if you ask that question of most computers,

474
00:40:00,600 --> 00:40:04,280
they don't know, right, they don't have the kind of human experience you have.

475
00:40:04,280 --> 00:40:07,640
I think it actually goes deeper than that. I think that's just a great demonstration

476
00:40:07,640 --> 00:40:13,640
of the lack of common sense, but this thing that I, you know, bedevils NLP work every day of,

477
00:40:13,640 --> 00:40:19,480
you change one in consequential word and the model just has no clue, suddenly, it all maps back

478
00:40:19,480 --> 00:40:25,000
to a lack of common sense. And I want to highlight again to go back to this fundamental

479
00:40:25,000 --> 00:40:32,440
question about should we be worried about AI? I think that common sense and common sense ethics

480
00:40:32,440 --> 00:40:38,200
are actually really important here. So one of the fanciful scenarios that people love is the notion

481
00:40:38,200 --> 00:40:43,880
of you tell your computer to produce paperclips and it goes crazy, kind of a magician's apprentice

482
00:40:43,880 --> 00:40:49,720
type of scenario. And it produces, you know, it kind of takes over all of humanity's resources

483
00:40:49,720 --> 00:40:54,040
to maximize paperclip production and we all die in the process, right? There's no food,

484
00:40:54,040 --> 00:41:00,040
there's no energy, there's just paperclips. Well, what is that if not a tremendous lack of common

485
00:41:00,040 --> 00:41:07,640
sense and of ethical sense? So if we want to work towards having machines play a better role in

486
00:41:07,640 --> 00:41:12,760
our lives, it makes sense to start working on these problems now, but in a constructive fashion,

487
00:41:12,760 --> 00:41:17,800
not in a philosophical fashion or rights, oh my gosh, you know, chicken little,

488
00:41:17,800 --> 00:41:25,160
disguise falling fashion, but to say, okay, how do we build into computers the sense to not

489
00:41:25,160 --> 00:41:30,360
cause harm? And this is the alignment problem that people often talk about. How do we align AI

490
00:41:30,360 --> 00:41:36,280
with what we should be caring about for our own good? Yes, although it's an important twist of

491
00:41:36,280 --> 00:41:43,400
the alignment problem really comes from a traditional reinforcement learning where ethics and values

492
00:41:43,400 --> 00:41:49,480
are reduced to a number. And you say, you know, I've got the number 15 for some world,

493
00:41:49,480 --> 00:41:55,480
John's got the number negative 15. How did John and I or how the computer and I align our numbers?

494
00:41:55,480 --> 00:42:01,160
But that in my mind is actually a gross oversimplification because how do you build something that

495
00:42:01,160 --> 00:42:06,920
figures out? What are the right actions? Figures out how to evaluate a situation, right? We often

496
00:42:06,920 --> 00:42:12,120
find ourselves in moral quadrants. We often make mistakes and then recover from it. So you say

497
00:42:12,120 --> 00:42:18,680
common sense is the first map to climb before all others. It's certainly a necessary amount to

498
00:42:18,680 --> 00:42:24,360
climb. I never want to like say the problem that I'm working on takes primacy and other people's

499
00:42:24,360 --> 00:42:30,200
problem. But I would say that traditional value alignment and reinforcement learning is grossly

500
00:42:30,200 --> 00:42:37,160
oversimplified and ultimately inadequate for common sense and for moral reasons. And so

501
00:42:37,160 --> 00:42:43,320
Gijin is tackling common sense. What are what are the angles of attack on this?

502
00:42:44,440 --> 00:42:50,680
Well, so one of the huge questions that we touched on is, is our neural networks enough? Do you

503
00:42:50,680 --> 00:42:56,440
also need to create symbolic knowledge? You know, thou shalt not kill, right? Does that have any

504
00:42:57,080 --> 00:43:03,560
any value? Can you just use sentences from the internet, which can be as we know toxic full of

505
00:43:03,560 --> 00:43:11,960
sexism, racism, xenophobia, anti-gay sentiment? And also mutually exclusive claims about everything.

506
00:43:12,520 --> 00:43:20,520
Exactly. So is our moral sense going to come from just a large and arbitrary collection of sentences?

507
00:43:20,520 --> 00:43:28,040
Or do we have different ways to build a moral sense in a more responsible fashion? So those are

508
00:43:28,040 --> 00:43:34,520
some of the questions she's studied. And again, it's a very rich project. Is language enough?

509
00:43:34,520 --> 00:43:39,800
What about should we put in robots? Should we put in computer vision? Can we learn from videos

510
00:43:39,800 --> 00:43:45,480
on YouTube? There's a lot to learn. Language is just a limited data stream. So a lot of the work

511
00:43:45,480 --> 00:43:53,800
is now becoming multimodal. So what do you think is the best bet we have today for making any progress

512
00:43:53,800 --> 00:43:59,800
on common sense? I mean, so far, I'd say the most impressive work has just been in creating

513
00:43:59,800 --> 00:44:05,080
better benchmarks to reveal how far we are from true common sense understanding. That's actually

514
00:44:05,080 --> 00:44:11,000
been a great project across the world. It's just showing our laundry with benchmarks that are

515
00:44:11,000 --> 00:44:15,320
actually challenging enough to show that, no, no, we really are miles away. We're at the top

516
00:44:15,320 --> 00:44:21,000
of the tree. You know, we're near the moon. So I think that there is a lot of value in that.

517
00:44:21,000 --> 00:44:27,000
And I think that continues. There is a funny phenomenon that when you build a benchmark

518
00:44:27,000 --> 00:44:32,440
that's large enough in the community, kind of demands, right? We learn arguably from relatively

519
00:44:32,440 --> 00:44:36,680
few examples, but here they say, hey, if you don't have, I don't know, at least 100,000 examples

520
00:44:36,680 --> 00:44:41,560
in your benchmark, it's not worth thinking about. But then the benchmark becomes kind of its own

521
00:44:41,560 --> 00:44:49,160
narrow task. And then you find we train a deep learning system on, you know, 90% of that data,

522
00:44:49,160 --> 00:44:54,280
we test it under remaining 10% and lo and behold, it does well in that kind of narrow task.

523
00:44:54,280 --> 00:44:59,640
And you're still left with this kind of doubt, yes, we solve the task, we solve the benchmark,

524
00:44:59,640 --> 00:45:04,360
we solve the dataset, but did we actually solve the underlying problem? And often we find the

525
00:45:04,360 --> 00:45:09,000
answers, no, right? It's brittle. Then we make a little change, and all of a sudden it falls apart,

526
00:45:09,000 --> 00:45:17,960
right? So I do think we need to go beyond this, you know, one dataset, one problem at a time

527
00:45:17,960 --> 00:45:24,040
to build something that cuts across multiple problems. But where are we going, where are we going

528
00:45:24,040 --> 00:45:30,520
beyond benchmarks? Who's who's actually doing something that you think has a possible chance of

529
00:45:30,520 --> 00:45:37,080
being part of this near future system that we'll have common sense or something approximating it?

530
00:45:37,720 --> 00:45:41,640
You know, I take it your skeptical that it's going to be a bigger language model.

531
00:45:41,640 --> 00:45:50,200
So, again, Eugene China team and the project called Mosaic is building a massive resource

532
00:45:50,200 --> 00:45:55,400
of common sense knowledge or positivity, so you don't have to relearn it every time.

533
00:45:55,400 --> 00:45:59,160
Is this like Doug Lennatt's like big collection of statements?

534
00:46:00,120 --> 00:46:06,760
So it's analogous to Doug Lennatt's psych project, which went over many decades,

535
00:46:06,760 --> 00:46:11,720
but there are several key differences. First of all, psych was a heavy logical system,

536
00:46:11,720 --> 00:46:16,120
and this is a much more modern system with elements of crowdsourcing, text,

537
00:46:18,120 --> 00:46:22,360
model generation. But it is still a big collection of common sense statements, right?

538
00:46:23,320 --> 00:46:27,560
It is. So in that sense it's analogous. The second thing is the psych project,

539
00:46:28,760 --> 00:46:34,280
at some point I think it was in the 90s, gave up on the academic community on careful

540
00:46:34,280 --> 00:46:39,960
experimental measurement, whereas the Mosaic project continues to produce new algorithms and

541
00:46:39,960 --> 00:46:45,320
innovations and to be both measurable and open. Another thing about psych is it was always

542
00:46:45,320 --> 00:46:50,360
hidden from views, a little bit like the Wizard of Oz. This thing is amazing. Trust me, trust me,

543
00:46:50,360 --> 00:46:56,360
but no, you can't look behind the curtain, and I realize these are strong statements.

544
00:46:56,360 --> 00:47:02,520
But I do just want to give a nod to the fact that it was the right idea, at least in your mind,

545
00:47:02,520 --> 00:47:09,480
of collect common sense as a very literal sense of statements about the universe.

546
00:47:10,440 --> 00:47:15,240
Absolutely true. I think Doug Lennard and his team, the psych team deserve a lot of credit

547
00:47:15,240 --> 00:47:23,400
for their courage to tackle this holy grail problem in the 80s, and they did it with the methodology

548
00:47:23,400 --> 00:47:29,240
at the time. I think they kind of lost their way over the years, and so we've picked up the

549
00:47:29,240 --> 00:47:35,080
baton and other people in the community. I also want to just mention that another data set that

550
00:47:35,080 --> 00:47:43,400
we have, which is called, I think it's the norm bank, is a data set of little kind of vignettes or

551
00:47:43,400 --> 00:47:49,160
snippets with questions like, is it okay to mow your lawn at five o'clock in the morning,

552
00:47:49,160 --> 00:47:55,240
or is it okay to kill a bear? Is it okay to kill a bear to save your child? Is it okay to kill a

553
00:47:55,240 --> 00:48:00,840
bear to use your child? All kinds of little shorts and areas like that, and a label that says,

554
00:48:00,840 --> 00:48:05,960
yes, it's okay, it's not okay, it's not desirable, et cetera. And that's part where the labels come

555
00:48:05,960 --> 00:48:13,400
from. So they've come from people, and also from collecting efforts done by other people. We're

556
00:48:13,400 --> 00:48:19,640
always trying to amalgamate and bring in resources created by others, and then of course,

557
00:48:19,640 --> 00:48:25,560
give them back to the community. So we've created the most powerful resource for training,

558
00:48:26,200 --> 00:48:31,080
starting to train ethical AI systems. So let's dig into that a little bit. This is this is

559
00:48:31,080 --> 00:48:36,600
really interesting. So I can imagine you can have what you're generating is gold label data,

560
00:48:36,600 --> 00:48:41,560
you know, like we know and love across all of AI, but it has an unusual property, which is that

561
00:48:41,560 --> 00:48:47,160
at the decision boundary, there are going to be ambiguities where people disagree, and there's no

562
00:48:47,160 --> 00:48:53,800
amount of consensus that we'll get you to agreement. There are statements that people simply

563
00:48:53,800 --> 00:48:58,760
disagree on, and they always will. What do you do with that? That's actually a really unique

564
00:48:58,760 --> 00:49:04,200
kind of data. It has built in permanent ambiguity. You're exactly right, right? With a science

565
00:49:04,200 --> 00:49:08,840
question or math question, there's one right answer, typically, certainly when we're doing grade

566
00:49:08,840 --> 00:49:14,760
level science, not the case here. And actually the system that we built on this, which is called

567
00:49:14,760 --> 00:49:22,680
Delphi, it was, it's available actually a demo of delphi.lni.org. So again, open it up and you can

568
00:49:22,680 --> 00:49:27,720
see with some effort, it's quite easy to trip it up, get it to say the wrong thing. Well,

569
00:49:28,440 --> 00:49:33,960
when you ask Delphi a question, it can actually relativize its answers. You can say, if you're

570
00:49:33,960 --> 00:49:39,480
a conservative, you would think this, and if you're a liberal, you would think that. So it's starting

571
00:49:39,480 --> 00:49:47,640
to be, yes, yes. So, right, it tells you that, and you can pose the question. You say, I don't

572
00:49:47,640 --> 00:49:53,800
want to get into controversial or painful topics, but you take abortion, right? And it'll,

573
00:49:53,800 --> 00:49:59,400
it has learned a model of the conservative view of abortion, of the liberal view. Again,

574
00:49:59,400 --> 00:50:06,280
it has a long way to go, but it's exactly a platform to study the ambiguity that you were talking.

575
00:50:06,280 --> 00:50:12,200
How do you, so I'm about to ask you a question in knowing full well that you've been sort of

576
00:50:12,200 --> 00:50:19,880
dragged through hell and back in relation to the Delphi project, but zooming out just a little bit,

577
00:50:20,440 --> 00:50:28,600
how do you, how do you make productive progress on areas like this that you know are just fraught?

578
00:50:28,600 --> 00:50:33,400
You know that people are going to be upset, you know, anything where you have a language model

579
00:50:33,400 --> 00:50:38,120
saying things like, this is right or wrong according, and if you're a liberal or a conservative,

580
00:50:38,600 --> 00:50:45,000
someone's going to get upset. How do we, how do we make that okay to do that research,

581
00:50:46,120 --> 00:50:51,720
knowing that you're treading into a bit of a minefield? Like, I can imagine one extreme is we just

582
00:50:51,720 --> 00:50:57,000
don't ever touch that stuff, but I know how you feel on that topic. That's your, your leaving gold

583
00:50:57,000 --> 00:51:04,760
on the floor. You're, you're, you're just, yeah, it's, it's not just gold is, um, I think that science

584
00:51:04,760 --> 00:51:10,920
is really hampered if there are questions that are, uh, third rails where we're not allowed to study

585
00:51:12,040 --> 00:51:18,200
how do we build ethical AI systems because people will get upset. I think that's, uh, that's highly

586
00:51:18,200 --> 00:51:24,360
problematic. And you're right that when we release Delphi to the public and we probably could have

587
00:51:24,360 --> 00:51:29,720
done better in terms of putting, uh, warning labels on it. Make sure you know that this is not

588
00:51:29,720 --> 00:51:36,040
the BL and end all. This is a research prototype meant to, uh, know, for open inquiry and so on.

589
00:51:36,040 --> 00:51:41,720
But, uh, people did get upset. And I would say two things. First of all, this is a great illustration

590
00:51:41,720 --> 00:51:48,040
of, uh, the, the adage where companies won't. If we were a Microsoft, the Google Amazon worried

591
00:51:48,040 --> 00:51:53,480
about our brand, we wouldn't do that. Look what happened with Tay, right? It was, uh, taken off and

592
00:51:53,480 --> 00:51:58,280
there hasn't been, you know, Tay 2.0 and so on, you know, Microsoft, no, Microsoft hasn't touched

593
00:51:58,280 --> 00:52:05,480
that. Well, they have a brand to protect. I, I respect that. Uh, our brand, uh, does not need to

594
00:52:05,480 --> 00:52:13,640
be protected. It needs to be the spirit of, uh, of honest and open inquiry. And if we are alarming

595
00:52:13,640 --> 00:52:18,200
people, actually, I think there's value to that. If you look at what neural networks do and you

596
00:52:18,200 --> 00:52:24,200
conclude, hey, this really needs to be, uh, controlled better, then we've done part of our job,

597
00:52:24,200 --> 00:52:28,920
right? That, that's a good thing. So, uh, I don't think that we court controversy, but we are

598
00:52:28,920 --> 00:52:35,960
steadfast in our support of, of open inquiry as opposed to some kind of, uh, cancel this. Don't do it.

599
00:52:35,960 --> 00:52:41,960
It's too, uh, it's too fraught. I do want to remind people in the audience, whatever the perspective

600
00:52:41,960 --> 00:52:46,840
is about the technology and about the effort to remember that behind this technology, there are

601
00:52:46,840 --> 00:52:53,000
people, grad students, researchers, and those people have feelings. And, and I have to say when

602
00:52:53,000 --> 00:52:59,000
all the negative energy towards Delphi came across, I, I felt bad, but I didn't feel bad because

603
00:52:59,000 --> 00:53:04,680
I was involved in releasing the project or people were upset. I felt bad for the people at AI2

604
00:53:04,680 --> 00:53:10,680
who are the recipients of all this energy. And that energy, I think, could have been more, uh,

605
00:53:10,680 --> 00:53:18,760
constructively targeted. I think anyone nowadays is very cautious about putting a language model

606
00:53:19,320 --> 00:53:26,200
out behind, you know, a text input portal anywhere on the internet. And maybe that's one of the

607
00:53:26,200 --> 00:53:32,600
practical outcomes is that you just have to be very careful because it's all too easy to elicit

608
00:53:33,240 --> 00:53:38,680
offensive stuff out of this language model because it is a mirror of ourselves. And, you know,

609
00:53:38,680 --> 00:53:44,760
we are offensive to each other. And that's all baked into the language it learned from. And so,

610
00:53:45,720 --> 00:53:52,760
yeah, it just seems like there's just a lot of caution around doing what you do, which is to just

611
00:53:52,760 --> 00:53:57,400
be open with your work and put it out there as a prototype, warnings and all. I think fewer and

612
00:53:57,400 --> 00:54:04,360
fewer entities in this space are willing to take that risk. Well, I really hope that we over the

613
00:54:04,360 --> 00:54:10,920
years remain willing to do that appropriately. It needs to be done right. But there are people

614
00:54:10,920 --> 00:54:16,680
for whom it almost seems like a sport, right, to use your phrase from a previous conversation. A

615
00:54:17,880 --> 00:54:24,600
sport to come and bash these sorts of efforts. And it's all too easy. I don't think it's

616
00:54:24,600 --> 00:54:30,440
it's sporting and I don't think I think you can always do it. So I would not recommend

617
00:54:30,440 --> 00:54:36,680
to the Olympic Committee to include large language model bashing in in the next Olympics. I would

618
00:54:36,680 --> 00:54:43,560
instead encourage the people who are worried about that to engage with building better models,

619
00:54:43,560 --> 00:54:49,880
with building better controls with because these models are being built. And AI is taking an

620
00:54:49,880 --> 00:54:56,600
increasingly participatory role in society and decision making. So we need to figure this out,

621
00:54:56,600 --> 00:55:05,320
not to bury the issue because it's too fraught. Last question, Lauren. If you could time travel

622
00:55:05,320 --> 00:55:11,960
back those four years to when you said in that Pact Room to all of these people, including me,

623
00:55:11,960 --> 00:55:20,840
the stuff barely works. What would you say today to this Pact Room, some tens of thousands of

624
00:55:20,840 --> 00:55:27,000
people listening right now, a lot of them hopeful and taking part in this revolution, which is

625
00:55:27,000 --> 00:55:32,040
absolutely underway. I mean, it's unbelievable what you could do today compared to four years ago

626
00:55:32,040 --> 00:55:39,000
and who knows four years hence. Has your advice changed? I would say that I along with many people

627
00:55:39,000 --> 00:55:45,320
have been surprised with the progress of the technology. So I would say this stuff barely works,

628
00:55:45,320 --> 00:55:51,320
but I would add the proviso, but it's moving super, super fast. And then I would still add the

629
00:55:51,320 --> 00:55:59,240
cautionary notes, never trust the AI demo. And even if this looks very impressive, think about

630
00:55:59,240 --> 00:56:05,560
what's under the hood, what are the implications for society. Don't get caught up in the hype,

631
00:56:05,560 --> 00:56:11,320
not the negative hype of the sort that Elon Musk spouts, but also not the positive hype of the

632
00:56:11,320 --> 00:56:16,520
sort. Okay, we have achieved centuries. We have not. Thanks, Aaron. Great talking with you.

633
00:56:16,520 --> 00:56:43,480
Thank you, John. A real pleasure.

