WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.240
I'm your host Sam Charrington. Today we continue our exploration of trust in AI with this interview

00:35.240 --> 00:40.640
with Richard Zemel, professor in the Department of Computer Science at the University of Toronto

00:40.640 --> 00:45.840
and Research Director at the Vector Institute. In our conversation, Richard describes some

00:45.840 --> 00:50.320
of his work on fairness in machine learning algorithms, including how he defines both

00:50.320 --> 00:56.480
group and individual fairness, and his group's recent Norrop's poster predict responsibly,

00:56.480 --> 01:02.280
improving fairness and accuracy by learning to differ. Thanks once again to Georgian

01:02.280 --> 01:07.720
partners for their continued support of the podcast and for sponsoring this series. Georgian

01:07.720 --> 01:12.440
partners is a venture capital firm that invests in growth stage business software companies

01:12.440 --> 01:19.000
that use applied artificial intelligence, conversational AI and trust to differentiate

01:19.000 --> 01:24.720
and advance their business solutions. Post investment Georgian works closely with portfolio

01:24.720 --> 01:31.080
companies to accelerate the adoption of these key technologies for increased value.

01:31.080 --> 01:36.120
To help their portfolio companies hire the right technical talent Georgian recently published

01:36.120 --> 01:42.560
building conversational AI teams, a comprehensive guide to lead you through sourcing, acquiring

01:42.560 --> 01:51.680
and nurturing a successful conversational AI team. Check it out at twomlai.com slash Georgian.

01:51.680 --> 01:58.880
And now on to the show.

01:58.880 --> 02:04.200
Alright everyone, I am on the line with Rich Zemel. Rich is a professor in the Department

02:04.200 --> 02:09.000
of Computer Science at the University of Toronto as well as being Research Director at the

02:09.000 --> 02:14.280
Vector Institute in Toronto. Rich, welcome to this week in Machine Learning and AI.

02:14.280 --> 02:19.760
Thanks very much. It's great to have you on the show. I'd love to get started by hearing

02:19.760 --> 02:25.560
a bit about your background and your path to your work in machine learning.

02:25.560 --> 02:31.040
Sure, so I've been interested in machine learning or I've been interested in artificial

02:31.040 --> 02:36.560
intelligence for a long time. In fact, since high school, I got lucky and had a summer

02:36.560 --> 02:42.160
job and I was growing up in Pittsburgh and had a summer job at Carnegie Mellon working

02:42.160 --> 02:48.400
with my sister neighbor was named Hans Berliner, one of the original person who worked on

02:48.400 --> 02:56.680
road game playing programs for played games like Backam and then Checkers and had some of

02:56.680 --> 03:02.800
the original ideas that later led to some of the ideas in chess and go the famous game

03:02.800 --> 03:08.600
playing of these days. So yes, I've been interested in it since that time and worked for companies

03:08.600 --> 03:14.760
back in the 80s, artificial intelligence companies back in the 80s when the AI went

03:14.760 --> 03:22.440
to help first hit. So that was my first experience with AI. I went to graduate school after

03:22.440 --> 03:27.280
that and got my PhD in machine learning, finished in the early 90s and I've been working

03:27.280 --> 03:34.360
in it ever since. Oh, fantastic. And I mentioned your post with the Vector Institute in Toronto.

03:34.360 --> 03:38.400
Can you share a bit about what the Vector Institute is up to there?

03:38.400 --> 03:46.240
Happy to. So the Vector Institute is an independent non-profit institute that's a combination

03:46.240 --> 03:53.240
of academia and industry. So there's a lot of faculty members here who have their home

03:53.240 --> 04:00.520
in universities such as University of Toronto or Waterloo or Guelph and as well as more

04:00.520 --> 04:07.240
distant universities like Delhause and UBC in Canada. And those people are all doing machine

04:07.240 --> 04:12.120
learning research. We also have some research scientists who are with their primary employment

04:12.120 --> 04:18.440
is here at Vector. And there's a lot of graduate students and postdocs associated with Vector

04:18.440 --> 04:23.240
or affiliate with Vector all doing research in machine learning. And so that's my focus

04:23.240 --> 04:27.800
is the machine learning research side of it. There's also another very strong side of it

04:27.800 --> 04:34.160
which is industry and working with industry and trying to help industry grow in terms

04:34.160 --> 04:40.040
of their machine learning capabilities. So Vector has funding from the federal and provincial

04:40.040 --> 04:48.000
government in Canada as well as a lot of industrial sponsors to both to help the kind of industrial

04:48.000 --> 04:52.800
business side of machine learning as well as the research end of it. Now I've taken a look

04:52.800 --> 04:58.080
at some of your publications and you've got a pretty broad, what appears to be a pretty broad set

04:58.080 --> 05:04.160
of research interests. Can you talk about where you tend to focus and some of the things that you're

05:04.160 --> 05:11.840
working on? I've been interested in some of the kind of long-standing problems in machine learning

05:11.840 --> 05:17.600
and in AI in general. Seen understanding is an example. So how can the computer really understand

05:17.600 --> 05:22.960
what's in the scene? How can it pick out somebody saying something of interest to you across a

05:22.960 --> 05:30.720
crowded room or find your friend in a crowded bar? So it's like the kinds of things that were

05:30.720 --> 05:38.560
quite good at have been hard for computers for many years. So that involves good learnings,

05:38.560 --> 05:43.040
a lot's taking a lot of data and trying to find patterns in it. And one of the things we've

05:43.040 --> 05:47.360
learned over the years is that the important thing that machine learning can add to this is the idea

05:47.360 --> 05:54.480
of learning good representations for inputs based on the data. So any kind of learning of

05:54.480 --> 06:00.400
representations that involves what's known as unsupervised learning where you may not be told what

06:00.400 --> 06:04.320
to take out of the data. You just try to do it. The system tries to figure out itself what are

06:04.320 --> 06:10.160
good features for the learning. And so that's my research. And more recently I'm interested in

06:10.160 --> 06:15.200
things where the aim is to come up with what we would call a more structured representation.

06:15.200 --> 06:21.360
Something where the human has some input into this and can define what it wants. So it can say,

06:21.360 --> 06:26.560
for example, we know that there's going to be, if we want to recognize a face that there's

06:26.560 --> 06:31.120
going to be certain features of the face that we think are important. So the human is giving

06:31.120 --> 06:36.400
some hints that the computer is then going to use. So we've made a lot of good progress on these

06:36.400 --> 06:40.480
kinds of things, I think, much more so than I originally thought in some ways. I thought

06:40.480 --> 06:45.360
it was going to be a long way off, but we're making a lot of good progress in this area.

06:46.160 --> 06:55.440
That example you gave about the features in the face, it does resonate with this trend that I'm

06:55.440 --> 07:02.560
seeing in machine learning research where we've started with these models that, in many cases,

07:02.560 --> 07:09.920
at least, are very much grounded in the physical world, in the case of computer vision,

07:09.920 --> 07:15.360
kind of feature detectors and edge detectors and things like that. We kind of swung to the

07:15.360 --> 07:21.200
other end of the pendulum with deep learning where we don't explicitly do any of that stuff.

07:21.200 --> 07:25.520
And now, more and more, we're kind of trying to find a middle balance where we're incorporating

07:25.520 --> 07:30.880
in our knowledge of the physical world, but still trying to use the power of deep learning.

07:32.000 --> 07:34.240
Does this work fit into that mold?

07:34.240 --> 07:40.000
Exactly. So I think that, yeah, so for an example, something that we've recently made a lot of

07:40.000 --> 07:46.800
progress in my group and many other groups is where the input to the learning system isn't just

07:46.800 --> 07:54.240
like an image or something, or, you know, a sound bite. Instead, it's something that is represented

07:54.240 --> 07:58.800
as a graph. So you have nodes and relations between the nodes. And so then you know,

07:58.800 --> 08:04.560
so you have some sort of, that's what I meant by a structured representation. And the idea is that

08:04.560 --> 08:08.000
then you can learn from that. So you're kind of given something, some sort of information that's,

08:08.000 --> 08:13.200
you know, not starting from the basics, but given that, we're able to do a lot of interesting

08:13.200 --> 08:19.280
processing on top of it. So I think that's something where it's combining some kind of information

08:19.280 --> 08:23.280
that humans have in a lot of different forms, right? We have a lot know about objects and their

08:23.280 --> 08:30.800
relations could be in big databases or, you know, descriptions of the world. And then we can learn

08:30.800 --> 08:37.360
from those on top of it. And the biggest challenge is not to start with that kind of relational

08:37.360 --> 08:42.240
information, but try to learn it as you go along. So that's the kind of the frontier, I would say,

08:42.240 --> 08:50.240
in this area. And then you've also have an interest in fairness in machine learning and of

08:50.240 --> 08:55.760
public several papers in that area. Can you talk a little bit about your work there? For example,

08:55.760 --> 09:02.400
you've got, I noticed you've got a paper that has been accepted to the upcoming NURPS conference

09:02.400 --> 09:06.160
that will both be added in Montreal. Maybe we can start with that one.

09:11.520 --> 09:16.880
And I'll interrupt if you, if you would like to kind of broadly,

09:16.880 --> 09:22.960
you know, contextualize your interest in that space and your, the kinds of things you're working

09:22.960 --> 09:27.920
on, we can start. We can sit there. So I've done some other work with some colleagues on fairness

09:27.920 --> 09:34.000
and was invited to a workshop a few years ago in Washington, D.C., where the discussion was about

09:34.880 --> 09:39.840
computer algorithms and the courts. And so this was a workshop that had people studying

09:39.840 --> 09:50.160
civil rights and as well as people from the legal system, judges and lawyers as well as some more,

09:50.160 --> 09:54.480
people who are more on the activist side of things like the Million Hoodies group. And so I think

09:54.480 --> 09:58.800
it was quite an interest and then there were some computer science people. And so it was an

09:58.800 --> 10:03.360
interesting workshop. But one of the things I learned, and a lot of debates there, one of the things

10:03.360 --> 10:09.040
that about how much influence computers should have and those kinds of things. So I learned a lot

10:09.040 --> 10:14.640
of that meeting about how computers are being used to set bail and for sentencing or at least to give

10:14.640 --> 10:19.360
advice. It's the kind of thing like I learned about a system in Pennsylvania where there was

10:21.040 --> 10:26.000
the input to the system was a description of a defendant. And the system would then come up and say,

10:26.000 --> 10:30.720
what's the probability that that person is going to commit a violent crime in the next few years.

10:30.720 --> 10:35.040
Right. So these, these kinds of systems have become quite publicized in the popular press.

10:35.040 --> 10:41.520
But this one was one that was developed specifically for the state of Pennsylvania. And it was

10:41.520 --> 10:46.400
intriguing to me because it would came up with some probability, like I would say, probability of 0.7.

10:46.400 --> 10:50.560
This this defendants is going to commit a violent crime. And then the question is, what does that

10:50.560 --> 10:55.360
mean to the judge? How does the judge use it? And was that number really meaningful?

10:56.640 --> 11:02.480
So one question I asked the person who was what? A description of the defendant, something about

11:02.480 --> 11:08.640
the defendant's history. And it wasn't that clear. They said they weren't able to get access to

11:08.640 --> 11:13.280
too much information about the defendant for various either privacy reasons or just not having

11:13.280 --> 11:18.000
having enough information. So it'd be things like what kind of crimes the person has committed,

11:18.000 --> 11:22.480
what they were arrested for, what they've already been convicted for, that type of thing.

11:23.920 --> 11:30.880
And the shocking thing to me was just how little validation had gone into the system. So there

11:30.880 --> 11:36.640
wasn't a real sense that they had tested the system and said that 0.7 really meant what you'd

11:36.640 --> 11:44.080
hope it would, which would be that that in some unseen data, 70% of the people who were assigned

11:44.080 --> 11:49.840
of probably 0.7, 70% of those people actually went on committed to violent crime. So you'd like to

11:49.840 --> 11:56.800
have some, so we call it calibration, that's a well calibrated system. And in talking to the people

11:56.800 --> 12:01.840
who developed it, they said there really wasn't that much enough data. And the data was quite noisy

12:01.840 --> 12:08.080
that they couldn't do a very confident job of calibrating it. And so my, I was very surprised

12:08.080 --> 12:13.840
to find that this was actually in use in the court systems in Pennsylvania. So my reaction was,

12:13.840 --> 12:17.440
well, we got to do a better job before machine learning people. This is at its heart, a machine

12:17.440 --> 12:22.320
learning problem. So we should be working on this and trying to figure out how our systems can

12:22.320 --> 12:27.840
interact better and do a better job working in this. So that inspired the paper that we have in

12:27.840 --> 12:33.760
the upcoming NURPS conference. We call it predicting responsibly. Here, a machine learning system

12:33.760 --> 12:37.520
isn't making the ultimate decision. It's more of an assistive tool. It's something that's

12:37.520 --> 12:42.400
providing an input to someone like a judge or a doctor or whatever it might be. And so you want

12:42.400 --> 12:49.040
that assistive tool to produce information that's useful to the final decision maker. And so what

12:49.040 --> 12:54.160
we were doing in this case was saying, imagine that the tool comes out and says not yes or no,

12:54.160 --> 12:58.160
this person is going to commit a violent crime. But it could, you know, it's not going to even

12:58.160 --> 13:02.080
produce a probability because it's unclear what a judge or somebody might do with that probability.

13:02.080 --> 13:09.440
But a simpler problem is to say yes or no or kind of pass and just pass that information on and

13:09.440 --> 13:15.520
allow the judge in this case to make the decision more clearly so that on their own. So it's kind

13:15.520 --> 13:22.000
of like an initial system that's doing a call of kind of saying, you know, we're narrowing down

13:22.000 --> 13:27.040
with the next step that the decision maker would have to pay attention to. That could be useful

13:27.040 --> 13:31.520
in a lot of settings like, you know, and there's too many inputs and you want too many applicants

13:31.520 --> 13:36.400
to a job and you want to have some initial calling system. But the key is this calling system,

13:36.400 --> 13:42.560
the first system that we're building has to be fair in some sense. It has to not just try to

13:42.560 --> 13:51.440
be accurate and say yes or no or, you know, pass it on, but try to take into account information

13:52.000 --> 13:59.360
about the defendant in this case that would explicitly try to not discriminate against that

13:59.360 --> 14:04.480
defendant. And that could be based on society definition of what discrimination is, right? So

14:04.480 --> 14:10.960
discrimination could be based on, you know, race or ethnicity or gender or various things. So

14:10.960 --> 14:14.960
that's what, in general, that's what the area of fairness is about is saying that, you know,

14:14.960 --> 14:19.680
you're training up a machine learning system and you want it to make decisions that aren't biased

14:19.680 --> 14:25.680
against any particular, and most of these, most of the work in fairness has been done where that

14:25.680 --> 14:30.480
particular group is defined ahead of time, right? We like I said, it could be race or gender

14:30.480 --> 14:37.440
or, you know, or age or whatever the attribute may be that you want to be fair to prevent

14:37.440 --> 14:43.040
discrimination against. So that's what the aim is in this general and fairness. And that's what,

14:43.040 --> 14:48.080
that's what we hope in this particular system is that when it makes a decision to say yes or no

14:48.080 --> 14:55.200
or to pass it on, it's making those decisions in a way that is fair. That's taking into account

14:55.200 --> 15:01.440
and making decisions that are ensuring that there's not discrimination against the, what we call

15:01.440 --> 15:08.400
the sensitive attribute. We talk a lot in machine learning about how important problem definition

15:08.400 --> 15:15.920
is and just listening to the way you are describing this system and it's clear that a lot of

15:17.120 --> 15:24.240
the path that you take is set out by how you, you know, define the problem more so, you know,

15:24.240 --> 15:31.600
perhaps than even in some other areas. Have you learned to observe anything about that particular

15:32.880 --> 15:38.480
challenge as applied to this space in your work? Yeah, so you've, yeah, you've hit on the kind of

15:38.480 --> 15:44.640
key key question in this work, which is about how do we define it? And I agree that's a,

15:44.640 --> 15:50.560
it's in general a machine learning problem that's often not paid that much attention to in the

15:50.560 --> 15:55.840
sense that, you know, we were used to machine learning defining classification problems or,

15:55.840 --> 16:01.280
you know, something like an image classification is this, is this a hippopotamus in the image

16:01.280 --> 16:06.080
or is it a dog? And for that, you can say, well, there's a right answer and you can judge whether

16:06.080 --> 16:11.840
there's a right answer or not. And so that's just us, you know, not, but the answers you get depend,

16:12.720 --> 16:19.440
depend a lot on how exactly you score your answers, right? So is it important to, you know,

16:19.440 --> 16:25.280
is it better to say this is a hippopotamus if the right answer is a rhino than to say it's a dog,

16:25.280 --> 16:30.400
for example? So if you have a scoring function that's sensitive to how similar the classes are,

16:30.400 --> 16:35.840
then you'll get a very different learning system than if you use a definition of an error that isn't

16:35.840 --> 16:42.560
sensitive to that. And that applies in spades when we come to the problem, the area of fairness,

16:42.560 --> 16:50.480
because, you know, it's an area that, so I started working on it about six years ago. My wife,

16:50.480 --> 16:56.800
Tony Potassi and I spent the summer with Cynthia Duwork and her colleagues and Microsoft Research

16:56.800 --> 17:02.640
in Silicon Valley and we were working on exactly trying to come up with, we landed on this problem

17:02.640 --> 17:06.880
with fairness and we worked hard on trying to figure out what is a good definition of fairness.

17:06.880 --> 17:11.760
We spent the whole summer debating it. And we ultimately came, we wrote a paper where we came up

17:11.760 --> 17:17.280
with two different definitions. So this is with Omar Ryan Gold and Moritz Hart in addition to

17:17.280 --> 17:22.960
Cynthia and Tony myself. And we wrote a paper where we came up with a group definition of fairness

17:22.960 --> 17:27.280
or an individual fairness. So a group definition of fairness would be one that would say,

17:27.920 --> 17:36.880
you know, overall, for example, decision is fair if, let's say, the two groups have the same

17:36.880 --> 17:44.880
number of positive outcomes assigned to them. Okay, that's a very simple thing that's like a kind

17:44.880 --> 17:48.960
of thing like affirmative action gets at, right? So if you want to give admittance to a school,

17:49.520 --> 17:55.520
you know, we have to say that the same number of males and females should get in, right? So that's

17:55.520 --> 18:01.120
group fair. That's an example. And then there's kind of more, the individual level of fairness is

18:01.120 --> 18:05.680
saying, well, what you really want is for an individual that other individuals that are similar to

18:05.680 --> 18:10.640
that individual should get the same outcome, the same decision, right? So that's more in the individual

18:10.640 --> 18:16.640
level. And since, since, so that we wrote a paper where we had that and talked about various

18:17.360 --> 18:23.760
ways of carrying that out. And in the main idea was that when this decision made is made,

18:23.760 --> 18:27.840
it should be aware of whether the person belongs to that group. So we called this fair

18:27.840 --> 18:33.360
through awareness. And then in the years, since then, there's been a lot of debates about what

18:33.360 --> 18:37.520
the proper definitions are of fairness and like different kinds of group fairness and different

18:37.520 --> 18:43.520
types of, you know, individual fairness. Somebody wrote a paper about the, you know, 21 definitions

18:43.520 --> 18:49.040
of fairness. So exactly like you said, you know, defining it is the key problem and that's what,

18:49.600 --> 18:54.000
that's what makes it very interesting and challenging. It's a fun area to work in partly for this

18:54.000 --> 18:58.640
because, you know, you can come up with a definition to debate whether it's right or wrong. I would say

18:58.640 --> 19:03.120
in the end, none of them are actually right, but some of them are less wrong than the others.

19:03.120 --> 19:09.840
And so in the paper that you were describing, one of the things that I kind of zeroed in on

19:09.840 --> 19:19.120
quickly in your description was this system is designed to output, yes, no, it kind of almost

19:19.120 --> 19:26.080
sound like yes, no, maybe yes, no pass in particular where pass is deferring on making a decision.

19:26.080 --> 19:40.480
And the idea there is to with the pass presumably to kind of recognize the system's own uncertainty.

19:42.240 --> 19:47.360
Yes, so it's a combination of things. So, yes, so people have worked on the idea of what's

19:47.360 --> 19:51.920
called been rejection learning in the past. And that is, you know, saying, well, we can say,

19:51.920 --> 19:57.440
I don't know or pass when the system's not confident, right? So back to the idea of, you know,

19:57.440 --> 20:02.480
the defendant is defendant going to commit a violent crime. Well, we can say pretty confidently,

20:02.480 --> 20:06.880
you know, within some error bars, it's going to say yes, or pretty confidently, no,

20:07.600 --> 20:12.320
even though you can see in those cases that confidence is a little hard to assess, but

20:13.760 --> 20:18.880
or and there might be a big space in the middle of between those two where the system isn't very

20:18.880 --> 20:24.160
confident. And so the idea would be that if it's I don't know, it gets passed on to the next

20:24.160 --> 20:32.320
to the decision maker. But in our in our work, the idea was that we aren't going to just pass on

20:32.320 --> 20:38.160
when based on uncertainty, we're going to pass also taking into account what we know about the

20:38.160 --> 20:44.160
downstream decision maker. So imagine that, you know, the system is trained up on judges decisions,

20:44.160 --> 20:50.160
and it's also trained up on or in a doctor case rate, it knows a lot about the doctors. And so

20:50.160 --> 20:55.680
it's going to say taking into account the kinds of decisions that the judge or the doctor tends

20:55.680 --> 21:01.440
to make and how, where, what kinds of problems they're accurate on, what kinds of people they're

21:01.440 --> 21:08.400
fair or unfair to, that the that could influence our machine learning system when it wants to say

21:08.400 --> 21:14.960
pass versus yes or no. Okay, so that's the key idea. So it's going to defer when it it's it'll

21:14.960 --> 21:19.360
be smarter in terms of when it defers based on knowing something about what's going to happen

21:19.360 --> 21:24.640
downstream and not only smarter in terms of being more accurate, but also less discrimination.

21:25.520 --> 21:32.640
So specifically, the the system, if it knows that it doesn't have a high degree of certainty

21:32.640 --> 21:40.320
about a particular decision, but it knows that the the decision maker is worse in a particular

21:40.320 --> 21:46.640
category of decision making, it might make the decision anyway. That's right. And so that's on

21:46.640 --> 21:53.120
the accuracy side. And it could be that if it's not very confident, but this person happens to be,

21:53.120 --> 21:58.880
you know, in this protected set, right? So it could be in a particular gender or something that

21:58.880 --> 22:04.640
in it knows that this particular decision maker happens to be discriminatory against that group,

22:04.640 --> 22:10.400
it could decide to make a decision rather than sit that deferring to the downstream decision maker.

22:12.560 --> 22:18.800
Another thing that jumped out at me here is I think in the context of kind of these examples

22:18.800 --> 22:24.400
with the courts, you know, what, you know, if you think about this relative to thresholds,

22:24.400 --> 22:31.360
and you know, say that the yes represents some kind of, you know, guilty or something with

22:31.360 --> 22:36.480
negative implications, right? So guilty or higher bail or longer sentencing or what have you.

22:38.400 --> 22:45.440
You know, the threshold ban that I might want to attribute to a yes is going to be maybe different

22:45.440 --> 22:50.240
than, you know, what I would attribute to a no because it has a much greater human impact.

22:50.240 --> 22:56.080
Does the system come for that at all? Certainly. So that goes back to our definition in some sense

22:56.080 --> 23:01.760
or what we would call the loss function, right? So it could be that making certain kinds of

23:01.760 --> 23:09.200
errors are worse than others, right? So making a false positive, like, you know, some would say,

23:09.200 --> 23:14.720
like letting somebody out of jail when they are going to commit the violent crime, potentially is

23:14.720 --> 23:21.200
worse to society than, you know, then locking up an extra person who really shouldn't have been

23:21.200 --> 23:26.000
locked up. That's debatable, right? But I mean, there's, but again, so it's just a lot of these

23:26.000 --> 23:35.760
decisions are kind of these definitions have to come from society. And so, but I think, but certainly

23:35.760 --> 23:40.320
on the machine learning side, it has the flexibility to do that to say, you know, what certain kinds of

23:40.320 --> 23:46.240
errors are more costly than others and to set the thresholds appropriately. But I mean, but that

23:46.240 --> 23:52.160
does lead to these questions about, you know, how are we going to decide, you know, who does what

23:52.160 --> 23:57.520
kind of errors are more costly, right? And so that's a, I think something that society has to weigh

23:57.520 --> 24:02.560
on in on. Similarly, you know, what kinds of attributes do we want to not describe,

24:02.560 --> 24:06.880
what kinds of people do we do not want to be biased against? It's another kind of question.

24:06.880 --> 24:11.440
So part of the aim here goes back to what we were saying earlier is that we really want to

24:12.160 --> 24:17.440
add some, some, not my view is we're trying to add some knobs to a decision-making system.

24:17.440 --> 24:22.720
So a machine learning system, add some knobs to a black box so that society or other people can

24:22.720 --> 24:27.440
come in and add some control, has some insight into what's going on and add some control to it.

24:27.440 --> 24:31.840
And so it's control in this way by like you're saying, you know, it could be that certain kinds

24:31.840 --> 24:36.240
of errors are more important and it could be that we want to reduce the bias against this group or

24:36.240 --> 24:42.800
that group and what does that actually mean? I'm curious for the decision maker, the system has to

24:42.800 --> 24:49.040
have some, you've got to be able to represent the decision maker's decisions in some ways.

24:49.040 --> 24:53.360
Is it you providing it a distribution or a set of rules or something else?

24:56.000 --> 25:00.720
Well, the decision maker is going to make its decision, you know, based on whatever features

25:00.720 --> 25:06.960
it was originally going to make its decision. So the, at least the way we formulated the problem

25:06.960 --> 25:11.920
is that our system is either going to make a decision yes or no or pass it on. And when it gets

25:11.920 --> 25:16.560
passed on and that the downstream decision maker, you know, gets to look at the case and just

25:16.560 --> 25:21.440
review it on, on their own. So they get the same kind of input features. They would have

25:21.440 --> 25:27.120
anyways, you know, another work that we've done in the fairness approach is that we're actually

25:27.120 --> 25:32.960
taking in a little bit of a different attack on this. We're saying what we'd like to do is change

25:32.960 --> 25:39.760
the representations of come up with new features that could be used by a downstream decision maker.

25:39.760 --> 25:43.280
So that's not this particular paper you were talking about, but other work that we've done in

25:43.280 --> 25:48.480
this area is, is about that is about constructing a different representation of somebody so that

25:48.480 --> 25:55.200
the downstream decision has our own representation that we've constructed available to them rather

25:55.200 --> 26:02.240
than the original representation. But in this paper, the model is learning based on some observations

26:02.240 --> 26:08.640
from the decision maker. And so I guess my, my prior question was, are you, are you handing it

26:08.640 --> 26:14.240
some kind of representation of the decision maker in the form of a set of rules or a distribution

26:14.240 --> 26:19.760
or is it more like an active learning thing where it's actually observing, you know, independent,

26:19.760 --> 26:23.600
I don't know, coin flips or whatever. I guess it's still a distribution of some sort.

26:23.600 --> 26:27.360
I mean, yes, you're right. There's lots of different ways we can formulate what the downstream

26:27.360 --> 26:32.320
decision maker's doing. So the way we're thinking about it would be that the

26:33.040 --> 26:39.200
imagine that there's a database available of, you know, some dataset available of this downstream

26:39.200 --> 26:44.720
decision maker or decision makers and how they've responded to cases in the past. So that's kind

26:44.720 --> 26:50.480
of a separate training set that we're observing that. And we're taking that into account when we

26:50.480 --> 26:56.400
build a model of what's the decision maker likely to say for this new, new case. And then that's

26:57.120 --> 27:04.160
when our system is deciding yes or no or pass, it's, it's using it, the model we've developed

27:04.160 --> 27:12.720
separately of that downstream decision maker. And is this, is the system is it something that

27:12.720 --> 27:16.160
you've implemented? Is it something that you've modeled mathematically? Like how?

27:16.160 --> 27:21.840
Yeah, so we, you know, we don't have good data for this. It's very hard to get, you know,

27:21.840 --> 27:26.000
it's something that we've implemented and we, like it, like often happens, unfortunately,

27:26.000 --> 27:31.120
in this fairness area is that there aren't great datasets out there. There's very few. There's one

27:31.120 --> 27:36.480
that's well known as this compass dataset, which is a dataset like I was talking about that looks

27:36.480 --> 27:41.360
at this exact case of here's a defendant. And what's probably in the system's aim is to come up

27:41.360 --> 27:45.200
what's the probability that person's going to commit a violent crime in the next three years.

27:45.200 --> 27:51.520
So that's a very well well known dataset. Some of the other datasets that are in use are ones

27:51.520 --> 27:56.080
that we've actually constructed ourselves where we'll take a existing dataset and then we'll make

27:56.080 --> 28:02.000
it into one that's relevant for fairness by identifying some sensitive attribute. Like we'll say

28:02.000 --> 28:08.240
there's one known as the adult income dataset where you're trying to decide the decision to be made

28:08.240 --> 28:13.200
is this person going to, you know, is there income greater than 50,000 or not, right? So that

28:13.200 --> 28:19.680
could be useful for for loans or whatever, but the we made it into a fairness dataset by saying

28:19.680 --> 28:25.200
taking one of the attributes gender in the gender of the person and saying we want to make sure

28:25.200 --> 28:30.160
that the decisions are made are not only accurate about the income, but they're also fair with

28:30.160 --> 28:35.840
respect to gender. So that's so we take existing datasets and they can be relevant for fairness by

28:35.840 --> 28:42.960
identifying one or more attributes that are, you know, the relevant things for preventing discrimination.

28:43.840 --> 28:50.240
So that's what we did in this case is that we took some existing datasets and implemented it.

28:50.240 --> 28:54.160
And then what we had to do though, we had to simulate the downstream decision maker. We had to make

28:54.160 --> 28:58.400
up, you know, what is this downstream decision maker? And so we tried to think of three different

28:58.400 --> 29:05.600
cases. One case is a downstream decision maker that has more information available than the

29:05.600 --> 29:11.280
the system we're building, right? So I imagine it's a judge and the judge gets to not only get the

29:11.280 --> 29:16.640
same kind of information our system does about the defendant, but also gets to see the person face to

29:16.640 --> 29:21.200
face and ask the person questions and gets additional information about the defendant that way.

29:21.200 --> 29:27.280
So in a way, it's a more knowledgeable judge, but let's say that judge doesn't care about being

29:27.280 --> 29:32.720
fair, okay? It just wants to take make make his or her best decision. So that was our model of

29:32.720 --> 29:38.000
one version of a decision maker. We had other ones too, ones that were intentionally unfair,

29:38.000 --> 29:42.160
ones that were intentionally discriminating. And so what we did is we trained up our system based

29:42.160 --> 29:47.520
on these different simulated downstream decision makers and then observed what would happen.

29:48.160 --> 29:53.760
How do you characterize the results? So the results are character, I would characterize

29:53.760 --> 29:58.080
as saying that it depends what you want to compare to, right? So with it, so an interesting

29:58.080 --> 30:06.400
comparison is to say, how would we do if we A, we could force our system to make a decision.

30:06.400 --> 30:11.680
So there's that and not allow it to defer, right? So that's a kind of standard machine learning

30:11.680 --> 30:16.160
classification problem. We can take the decision maker out of the loop and just use our system.

30:16.160 --> 30:20.000
The other version of it is one where we take our system out of the loop and just use the ultimate

30:20.000 --> 30:24.160
decision maker, right? So those are the two extremes. And then we have different versions of our

30:24.160 --> 30:31.280
our system, one of which just says I don't know without paying attention to what it thinks about

30:31.280 --> 30:37.280
the downstream decision maker. And then the fourth is the kind of ultimate goal, which is what we

30:37.280 --> 30:41.440
wanted to really propose, which is this idea of learning about the downstream decision maker

30:41.440 --> 30:48.000
and taking that model into account in making our decisions. And in general, and we looked at

30:48.000 --> 30:54.640
these different scenarios, and in general, the effect was we wanted in the sense that we were able to

30:55.040 --> 31:02.560
achieve a pretty good tradeoff of fairness and accuracy using the system as we had formulated it.

31:02.560 --> 31:07.120
It's certainly better than either just relying on the downstream decision maker or relying on our

31:07.120 --> 31:12.960
system alone. And the interaction where we were able to take it, build the model of the downstream

31:12.960 --> 31:18.160
decision maker that was pretty good did improve overall both in terms of accuracy and fairness.

31:19.120 --> 31:26.320
I came across another paper that you worked on recently learning adversarially fair and transferable

31:26.320 --> 31:32.240
representations. Can you give me an overview of that one? So that one is along the lines of what I

31:32.240 --> 31:38.560
was mentioning earlier where the goal now of the system isn't necessarily to make a decision,

31:38.560 --> 31:43.200
but rather to come up with a representation. So this fits in with what we were talking about early

31:43.200 --> 31:49.280
on that machine learning systems. One of the big advances that we've had is in coming up with

31:49.280 --> 31:54.640
representations, right? So learning good features for visual recognition or for speech recognition.

31:56.160 --> 32:01.600
And so this is along those same lines. And so what we've worked on for several years is what we

32:01.600 --> 32:07.840
call fair representations. And that is coming up with representations for, let's say, individuals in

32:07.840 --> 32:16.160
this case rather than images or text bite sound bites that are fair in some sense. So and obviously

32:16.160 --> 32:21.120
again, it depends on what your definition of fair, but one intuition there would be what we'd like

32:21.120 --> 32:28.560
to do is have a representation of an individual that is clean of that doesn't have any information

32:28.560 --> 32:33.840
that kind of obfuscates any information about whatever the sensitive attribute is, right? So

32:33.840 --> 32:38.640
for instance, we'd like to come up with a representation of you where it's not clear if you're

32:38.640 --> 32:48.000
a male or female because if we want to ensure that that any classifier using that representation

32:48.000 --> 32:54.000
will not have be able to discriminate against you based on your gender, then we want to remove

32:54.000 --> 32:59.120
all information about gender in the representations. So then that will ensure that the downstream

32:59.120 --> 33:04.400
classifier won't be able to make a decision about you based on your gender. So that's the

33:04.400 --> 33:10.960
notion of fair representations. You kind of remove any information about gender before handing off

33:10.960 --> 33:16.880
that representation to a classifier. You mentioned an image previously. Is that typically the domain

33:16.880 --> 33:21.440
that you're working in for these fair representations like something that's characterizing, you know,

33:21.440 --> 33:29.360
that starts from an image of me and then generates a genderless or raceless image, or is it more abstract

33:29.360 --> 33:36.480
like you you're looking for some embedding space that isn't correlated with race or gender or

33:36.480 --> 33:41.360
things like that? Yeah, so it's more of the embedding space that isn't correlated with race or

33:41.360 --> 33:46.800
gender, and generally so far we aren't starting with though the original representation isn't an

33:46.800 --> 33:51.600
image. It's, you know, think of it as a database record of your demographics or something, right?

33:51.600 --> 33:56.800
It's about where you live and how old you are. This is something that you might be using

33:57.920 --> 34:01.600
you know, and another kind of setting that's not based on images or hearing your voice or anything.

34:01.600 --> 34:08.720
So it's, you know, a typical kind of let's say demographic record of an individual and that we

34:08.720 --> 34:13.760
want to do is take that demographic record record and construct a representation of it like an

34:13.760 --> 34:20.160
embedding of it as you describe where that embedding has lost information about your gender or your

34:20.160 --> 34:26.320
race or something like that. Okay, so that's where they identified attribute we've removed information

34:26.320 --> 34:31.360
from that. So that was, so that's the idea of the fair representation. So our paper now it's

34:32.240 --> 34:38.960
paper with David Madras and Elliot Krieger and as well as Tony and myself and that paper the idea

34:38.960 --> 34:44.080
was exactly, as you described, you want to cover it in embedding that doesn't have information about

34:44.080 --> 34:51.600
that attribute, let's say it's gender. And the way to do that is to construct an adversary where

34:51.600 --> 34:57.600
the adversary is going to take the representations or embeddings that we construct and try to

34:58.240 --> 35:05.280
pull out the information to figure out what is the gender of that individual. So we're trying to

35:05.280 --> 35:10.160
create a good embedding that will thwart this adversary, make it impossible for it to pull out that

35:10.160 --> 35:17.120
information. And the interesting thing about this is, so that idea is, you know, this adversarial

35:17.120 --> 35:23.600
approach, but we can then come up with a, there's been, as I said, there's been a bunch of

35:23.600 --> 35:28.640
definitions of fairness that have gone beyond the original kind of group fairness that we described.

35:28.640 --> 35:34.480
And so one of them is, it's called equalized odds. Another way of saying that is, you can think

35:34.480 --> 35:39.520
of it as a balanced errors. So rather than making decisions that are balanced between the two groups,

35:39.520 --> 35:45.360
you know, like 60% of the people of the males and females are going to get in. Instead, another

35:45.360 --> 35:52.000
criteria for fairness is that you want that when the system makes an error, those errors are balanced.

35:52.000 --> 35:56.720
So, you know, whatever the errors are, half of the errors are for males and half of the errors are

35:56.720 --> 36:01.840
for females. Or, you know, if they have the same kind of base rate or, you know, if they're 70%

36:01.840 --> 36:08.080
as many males in the databases as females and 30% females, then 70% of the errors are males

36:08.080 --> 36:13.920
and 30% of them are females. Okay, so that's a different fairness criteria. And we can,

36:13.920 --> 36:19.280
and we can adjust our adversary to reflect that fairness criteria. So it doesn't have to be that,

36:19.280 --> 36:25.840
so it could be that it's only going to try to extract the information about gender on the error

36:25.840 --> 36:30.320
cases. Okay, so that's the, and we want to thwart the adversary from doing that. So all I'm saying is

36:30.320 --> 36:36.160
that the, you know, we can kind of tailor our system, our adversary to different definitions

36:36.160 --> 36:42.480
of fairness and then train it up in that way. And so that's the main idea and what we call laughter

36:42.480 --> 36:48.160
learning adversarily, fair and transferable representations. And the key notion is that,

36:48.160 --> 36:54.720
so why do we want to come up with these representations that are fair is that this transfer idea.

36:54.720 --> 36:59.280
So it might be that we want to use that same representation in other settings. So we might want

36:59.280 --> 37:04.400
to say now I have this individual, I've taken your demographic information, I've taken out your

37:04.400 --> 37:12.320
gender information. And now I want to see if, and now maybe many different advertisers may want

37:12.320 --> 37:18.000
to decide whether they're going to advertise to you or not. And by making this new representation

37:18.000 --> 37:22.240
that doesn't have your gender information, we're ensuring that all of those different advertisers

37:22.240 --> 37:28.560
when they work on that on that new representation won't be able to discriminate against your,

37:28.560 --> 37:33.600
based on your gender. So that's the transfer ideas that we can take that same representation and

37:33.600 --> 37:39.200
use it in different settings for different kinds of classification problem. And if you come across

37:39.200 --> 37:47.440
anyone doing something like that in practice, you know, identifying some representation that

37:47.440 --> 37:53.920
is fair in this way and using that for downstream decisioning. I've talked to a number of people

37:53.920 --> 37:58.880
in companies where they're certainly concerned about fairness and trying to ensure that they have

37:58.880 --> 38:05.760
a classification system that is fair. And so they're interested in this idea of having a representation

38:05.760 --> 38:11.600
that would be kind of, you could give a stamp of, you know, a fairness to that representation that

38:11.600 --> 38:15.840
they could use multiple settings. I don't know anybody's actually using that idea in practice,

38:15.840 --> 38:20.800
but certainly there are more and more interest in companies where they want to be able to

38:20.800 --> 38:27.200
A, assess to what extent is their system fair that they're using and, you know, improve its

38:27.200 --> 38:31.520
fairness. And so they, I think this notion that you have a representation that could be used

38:31.520 --> 38:38.880
for multiple, multiple settings, multiple problems is of interest to people, but I don't know

38:38.880 --> 38:46.160
if anybody actually doing that yet. You mentioned the assessment piece. So this can be used,

38:46.160 --> 38:53.920
the, this method could be used for assessment independent of whether you're actually using

38:53.920 --> 39:01.040
these kind of representations downstream, right? That's right. I mean, you can evaluate the kind

39:01.040 --> 39:04.160
of, you know, based on whatever definition of fairness you might want to have, you could say,

39:04.160 --> 39:09.200
how well does this, you know, to what extent is this system violating that, right? So that's

39:09.200 --> 39:15.040
kind of like gives you an idea that says it, you know, are they errors? One definition I mentioned

39:15.040 --> 39:21.200
is balanced errors, definition, or equalized odds. You can say how much is our current system

39:21.200 --> 39:27.280
violating that? It's a little bit hard to quantify, you know, to the degree, but you can say

39:27.280 --> 39:33.440
on what percentage of the cases is it violating that or having out. So, you know, so there,

39:33.440 --> 39:38.480
so certainly people are interested in this notion that you can kind of audit an existing system

39:38.480 --> 39:44.080
and say how well is it doing based on various fairness metrics? So we're talking about a couple

39:44.080 --> 39:51.920
of your recent papers in this space. Can we take a couple of minutes and maybe get your perspective

39:51.920 --> 39:56.800
on the broader landscape around fairness and machine learning and what some of the big

39:56.800 --> 40:02.640
challenges and opportunity areas are? Yeah. So I think fairness is a kind of interesting

40:02.640 --> 40:07.600
microcosm of the machine learning in general, right? So some of the same ideas we've talked about

40:07.600 --> 40:13.360
about adversarial ideas is reflected in general and in machine learning, trying to come up with

40:13.360 --> 40:20.400
representations that are have identify either, you know, separate out or in our case, reduce the

40:21.840 --> 40:27.920
information in a particular case. And I think that's true of in general, but the rest of the

40:27.920 --> 40:33.520
fairness work these days. So one one interesting bit of work I would highlight is on fairness and

40:33.520 --> 40:40.160
causality. So in general, machine learning is very good at pulling out patterns and saying what

40:40.160 --> 40:45.360
which things are what aspects of data are correlated with some label movement want to give.

40:46.240 --> 40:50.560
But there's a big push now to try and come up with more causal reasoning. So it's not just

40:50.560 --> 40:54.640
that things are correlated, but they're actually causal and that would enable us to do

40:56.080 --> 40:59.760
counterfactual reasoning, right? So not only if you have something that's causal, you could say,

40:59.760 --> 41:06.880
well, what would hypothetically happen if we flip that that bit and one of our causes changed it.

41:06.880 --> 41:12.640
What do we expect to happen, right? It's a big challenge for the field because generally that

41:12.640 --> 41:17.200
means that kind of data isn't available in most cases. And so it's, you know, there aren't great

41:17.200 --> 41:23.520
data sets for it. And it's something that's, but it is an aim that's generally true in machine

41:23.520 --> 41:28.320
learning. And I think it's really important for the case of fairness because understanding the

41:28.320 --> 41:34.560
underlying causes for why some decision is made may be an important step to try to reduce

41:34.560 --> 41:39.680
discrimination. So that's a, I think a very, very interesting area of research. We actually have

41:39.680 --> 41:44.880
a paper in that direction coming out at the next fat star conference, but I think there's been

41:44.880 --> 41:50.320
great work in the field. Some number of papers on things called counterfactual fairness in other

41:50.320 --> 41:54.960
areas and other kinds of papers in that. That's one thing I would say is causality is another one

41:54.960 --> 41:59.920
which is there's a simplification I've mentioned, which is that you have a single sensitive attribute

41:59.920 --> 42:06.720
like its gender or race and in the real world, we know there's many sensitive attributes and

42:06.720 --> 42:13.200
we might want to be ensuring that the system is fair with respect to several at a time.

42:13.200 --> 42:19.840
And that in itself is not easy. So there's some nice work out of Stanford and some parallel

42:19.840 --> 42:25.440
work out of Penn research groups. Well, something they call fairness gerrymandering, which is,

42:25.440 --> 42:30.080
you know, just a cute name where it means like you can be fair with respect to one dimension.

42:30.080 --> 42:34.480
And that may make it less fair with respect to another. So how can you simultaneously be fair

42:34.480 --> 42:39.760
with respect to several attributes? So I think that's an important and interesting area in that.

42:39.760 --> 42:44.800
It also makes me wonder if some of the recent work happening around multitask learning could be

42:44.800 --> 42:51.120
applied in this space. Yeah. So I think the multitask learning was part of our inspiration for the

42:51.120 --> 42:55.520
the laughter thing, right? Which is your transfer ability, right? So you want to have a same representation

42:55.520 --> 43:02.160
that's useful for multiple tasks in that case. And I think the multitask, in this case, it's kind

43:02.160 --> 43:06.320
of harder than multitask. So I think when you have this multi-attributes, you want to be fair

43:06.320 --> 43:12.160
against all these different attributes. And the real challenge is that it's kind of like,

43:13.440 --> 43:16.480
it's also related to these other areas of machine learning. Like I said that, you know,

43:16.480 --> 43:20.480
I think it's a microcosm machine learning. It's a lot of work and few shot learning where you want

43:20.480 --> 43:26.400
to be able to learn with little data that's available, little label data. So it may be that you have

43:26.400 --> 43:31.120
some attribute that you want to learn, that you want to be fair to, but you have very little data

43:31.120 --> 43:35.680
available for that. So how can we, that really taxes a machine learning system, right? It's

43:35.680 --> 43:42.480
how it's very hard to do that. And then there's the kind of big open question. Maybe we don't really

43:42.480 --> 43:47.520
know what what attributes we want to be fair to. But you know, so rather than humans deciding

43:47.520 --> 43:51.040
that it's gender and race that are important, it could be some other group that's actually

43:51.040 --> 43:55.200
being discriminated against. And how do we, you know, so that's a bigger kind of open question of,

43:55.200 --> 44:01.680
you know, undefined attribute discrimination. And people are thinking about that these days as well.

44:01.680 --> 44:08.880
So I think those are some of the main areas I would say of in fairness that are currently

44:08.880 --> 44:13.760
of interest. And like I said, there's a lot of work that's paralleling what's going on and

44:13.760 --> 44:19.440
other parts of machine learning in the fairness literature. Any recommended starting places for

44:19.440 --> 44:26.560
folks that want to learn more or dive more deeply into this area? I know there's a book that's

44:26.560 --> 44:35.280
being put out by Solon Barakus and more its heart on fairness. They've developed it online.

44:35.280 --> 44:43.920
It's available. There's a number of very nice invited talks from people Kate Crawford gave

44:43.920 --> 44:50.320
one at NURPers last year. And in terms of papers, I think, you know, there's a fair number of

44:50.320 --> 44:56.800
papers that are looking at the definitions of fairness and interesting questions about

44:56.800 --> 45:02.880
incompatibility of fairness. So some some work there. John Kleinberg with some colleagues

45:02.880 --> 45:11.760
had some papers. So I would say that in these days, what's interesting is it used to be there

45:11.760 --> 45:15.840
were one or two papers on fairness a year and now if you go to machine learning conference

45:15.840 --> 45:20.880
like ICML or NURPers, you'll see there's five or six at least maybe eight or ten papers these

45:20.880 --> 45:25.520
days. So, you know, I think picking up some of the recent papers in any of these areas is a good

45:25.520 --> 45:30.400
good starting place. So that's my recommendation. Well, Rich, thanks so much for taking the time

45:30.400 --> 45:34.080
out to chat. I really enjoyed it. Sure, then fun. Thanks.

45:37.360 --> 45:43.120
All right, everyone, that's our show for today. For more information on Rich or any of the topics

45:43.120 --> 45:50.880
covered in this episode, visit twomlai.com slash talk slash 2009. Thanks once again to the great

45:50.880 --> 45:56.240
folks at Georgian Partners for their sponsorship of this series. Be sure to visit twomlai.com slash

45:56.240 --> 46:02.160
Georgian for more information on their building conversational AI teams guidebook. As always,

46:02.160 --> 46:32.000
thanks so much for listening and catch you next time.

