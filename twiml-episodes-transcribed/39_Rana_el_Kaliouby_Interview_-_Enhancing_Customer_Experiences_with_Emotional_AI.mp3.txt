Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
To show you about to hear as part five of our five part O'Reilly AI New York series, sponsored by Intel Nirvana.
Now if you've listened to the entire series, you've heard me say this again and again, but I am super grateful to them for helping make this series possible.
And I'm also excited about the products and projects that they launch at the O'Reilly AI conference, including version 2.0 of their Nirvana framework and their new Nirvana Graph project.
Be sure to check them out at intelnervana.com.
And if you haven't already listened to the first show in this series, where I interview Naveen Rao, the head of Intel's AI products group and Hanlon Tang and algorithms engineer on that team, it's Twimble Talk number 31 and you definitely want to check it out.
My guess for this show is Rana Alkayubi.
Rana is co-founder and CEO of Affectiva. If you remember my conversation about Emotional AI with Pascal Fung from last year's O'Reilly AI conference, you're going to love this one.
My conversation with Rana kind of picks up where that one left off with a focus on how her company is bringing Emotional AI services to market.
It's a great conversation and I know you'll love the show.
Let's get to it.
All right, everyone. I am on the line with Rana Alkayubi, who is the co-founder and CEO of Affectiva.
Rana was here at the O'Reilly AI conference and delivered a presentation on Emotional AI.
This topic may sound familiar to you because last year at the O'Reilly AI conference, I interviewed one of her colleagues in the field, Pascal Fung.
And so I'm looking forward to speaking with you, Rana, to learn a little bit about Affectiva and how you're applying Emotional AI and making it useful for enterprises.
Welcome to the show, Rana.
Thanks, Sam. Thanks for having me. So, you know, when we look at human intelligence, there's many aspects to that and one important aspect is Emotional Intelligence.
And that references the ability that we each have to read and understand other people's emotional and social and mental states and then adapt our behavior accordingly.
And so, based on years of research, we know that people who have higher emotional intelligence tend to do better in life. They're more likable, they're more persuasive, they are more successful in their professional lives, and they even live longer.
So, our thesis is that advanced AI systems and autonomous vehicles and, you know, all the technologies that are surrounding us today, they all have high cognitive intelligence,
but they're really lacking in this emotional intelligence area. And we are, you know, my team and I are on a mission to humanize technology by bringing in artificial emotional intelligence.
We see that there are two sides to this coin. One is to build technologies and digital experiences and even devices that can sense emotional, you know, consumers emotional states in real time and adapt to that accordingly.
So, that's kind of one stream of work that we do. The other is more around gathering data about how people feel towards things. It could be online video ads or product concepts or new experiences and we collect all that data and aggregate it.
And as it turns out, this information is very, very important and meaningful for organizations as well as consumers.
Interesting. How did you get started in this area? What led you to study emotional AI?
So, my own background as an undergraduate, I studied computer science. I was fascinated by the role technology plays and actually changing how we connect with one another as human beings.
And then I got my first, you know, living a broad opportunity. I got the opportunity to do my PhD at Cambridge University and at the time I used to live in Egypt, so I moved to Cambridge, UK.
And I realized I was spending more time with my laptop than I was with any other human being. Kind of sad.
And of course, it was frustrating because even though we were intimate, the laptop and I, it had absolutely no idea how I was feeling.
So, there were oftentimes when I would be stressed because I had, you know, a project deadline or a paper deadline and it was just completely oblivious to all of that.
But I also realized that it was my main portal of communication back to my family back home. And this was before we had smartphones.
And I think it's still true with smartphones, right? Like it's, you think of these devices and they are becoming the main portal of communication.
Absolutely.
Exactly. But yet I felt like even though, you know, this kind of, you know, social platforms have allowed us and messaging apps and all that have allowed us to connect with more people.
The quality of the connection is quite core because a lot of the nuances and the non-verbals are kind of lost in cyberspace. So I, you know, one example that really kind of stuck in my brain and has motivated a lot of my research.
I remember feeling very lonely in Cambridge and, you know, I'd be chatting with my family back home and I'd be sometimes in tears.
Like I'd be really upset and they would have absolutely no clue until I send them that little emoji with tears, you know, I would be like, oh, so you're kind of sad.
And it just, it was very explicit and it took away from the gen, you know, what do you call it, the sincerity of the emotion and that made me realize that we could do better.
And so I started imagining what would it take to build a computer that had some emotional intelligence skills the way my friend would or my mom does, you know, and just got very interested in the psychology aspect of human emotional intelligence and then how to replicate that in a machine.
It is really interesting the degree to which, you know, something as simple as an emoji expands the bandwidth we have for communicating emotion.
And so I'm curious how you, you know, with that as kind of a backdrop, tell us a little bit about affectiva and the specific things that you guys are offering to allow companies to better, you know, harness emotion.
Yeah, so like all great things I came to affectiva, you know, my accident. So my plan, my career plan was to do my PhD and then apply for a faculty position and take an academic and research career.
I ended up joining Professor Rosalind Picard's group at MIT Media Lab. So she wrote the bill, affective computing in the late 90s and that's kind of what inspired me to get into this research space.
Okay.
And I met with her and ended up joining her lab at MIT in 2006 and she and I collaborated on the application of emotion sensing to specifically mental health.
And being at MIT Media Lab twice a year, we would invite all these sponsor companies and they would say, you know what, we're very interested in this work you're doing.
Like we want to apply to advertising research or, you know, monitoring drivers in cars, there was just like a whole slew of applications.
And after about like four years of doing this at MIT, we decided that there was a commercial opportunity and that the market was ready for, you know,
was kind of technology. So we spun affective out of the media lab.
Okay.
With the vision of bringing emotion AI to the market in a commercializable way.
And it's been a few years, we found that, you know, we still have a very broad vision. We still see that this technology is a core AI capability that's going to play in a lot of areas.
The idea is to sequence the markets. And so the first use case, if you like, where we've applied this technology is an advertising research and market research.
And the problem we're solving there is, you know, like think about Coca-Cola or Kellogg's or Pepsi.
They want to understand how people are engaging emotionally with their content.
Interesting.
Yeah, because emotions drive behavior. So, right, if you feel very positive or inspired by an ad, you're going to share it with your network.
You're going to go out and, you know, you're going to build a positive brand affinity or loyalty.
You're going to buy the product.
So a lot of, you know, a lot of the marketers and advertisers work with want to elicit a very strong positive emotion.
Sometimes it's sentimental and it's not necessarily positive, but they want to drive an emotional journey, but they struggle to measure how effective they are doing it.
And before our technology, what they did is they just asked people, okay, how are you feeling about this?
And we know that this is very unreliable and susceptible to all sorts of bias.
Right. It's expensive as well.
So what we do instead is we use our technology, which is essentially a machine learning and a computer vision platform that uses a camera on any device.
And it reads your facial expressions in real time and it maps it to an emotional state.
Oh, interesting.
So do you position it as kind of a virtual focus group type of opportunity or is it more, I guess, passive observation of people engaging in, you know, real media experiences?
Does that question make sense?
Yes, absolutely.
It's both.
So most of the work we've done to date is the latter. So we send people a URL and we say, okay, if you click on this, do you have your permission to turn the camera on while you're watching this movie trailer?
And if they say yes, all they do is they, you know, they watch the movie trailer, the cameras on, it's streaming their responses.
We do that for a thousand people and then we aggregate their responses and we end that dashboard, that moment by moment journey is what we present to, you know, to the client or the TV studio.
Okay.
Interesting.
I can't help but think about a conversation that I had yesterday with Raza Zade.
Raza Zade, you know, Raza from matrioid.
What he and his company is doing is they are indexing TV streams and allowing companies to build detectors that can identify, you know, different brands and objects in the TV screen in the streams, not necessarily TV, but, you know, media streams.
And I wonder if the degree to which some of the brands as they have, you know, the product placements are huge in some of these TV shows and things like that.
You know, if they'd get some additional insight by having kind of real time, you know, detection of emotional response to product placement, kind of in situ, in context, in a, you know, media as media is being consumed random thought.
We often get asked, you know, we will often get approached by an advertiser who, you know, the ad, they are featuring two celebrities and they want to know which version resonates the most or, yeah, they often play with different product placements or different music in the background.
And the idea is to really detect, because it's often hard for people to articulate if they like something, it's very hard to articulate what is it that resonated and so we can detect your visceral reaction on a frame by frame basis so we can say, okay, when, you know, when the product revealed happened, this is what we saw.
And I've got to imagine that for, you know, obviously emotion is very subtle and the expression of emotion, particularly if you're where I'm assuming you're using laptop cameras and we're looking primarily looking at face, you know, we're talking about fine motor movements, things like that.
How do you convince the skeptic that, you know, this technology can really accurately pinpoint an emotional response.
So when we first started doing this work, the very first question we had to answer was, you know, when you're in front of a laptop, do you even emote, right?
Because there's no other human, right?
But I'm smiling and laughing in front of my laptop now as I'm engaging with you, but yes, I can, I definitely get the question.
If you're just watching an online video ad, do you even show any expressions? And so we collected, I mean, now, you know, today we've collected five and a half million face videos from 75 countries in the world of people interacting with their laptops, with their phones, playing games, driving and cars.
So we have a very substantive data set of people's emotions. But when we first started, we did not have all this data. And so we initially collected data of people watching Super Bowl ads.
And it was, you know, we knew that some of them were successful and went viral and garnered a lot of attention and some of them didn't.
And we found that, yes, people emote expressions, even if there is no other human being present, and we saw the entire gamut of expressions we saw laughter, we saw swerks, when people were kind of, you know, really skeptical.
We saw confusion, we saw boredom. And that data set became the data that we used to train our algorithms.
So we used, you know, deep learning to train models to detect very, very nuanced expressions. You know, the machines now able to or the algorithm is now able to identify 20 different facial expressions.
And it maps them to eight different emotional states. And just to give you an example, some of the expressions that we're able to identify now include a smile or a brow farrow, which are kind of obvious.
But then it also is able to read like a lip sock or an ice quint, you know, like very, very fleeting nuanced expressions. And that, that's all because we have the data to drive the machine learning.
And I recall from Pascal that there, there's a standardized chart for emotions like it's there are two axes and the emotions are mapped out on these axes.
Do you know what I'm referring to? And can you remind me what that's called?
Yes. So there's a certain complex model of emotions and basically that maps or it's the dimensional model of emotions. So it, it posits that you can think of emotions in terms of these main dimensions like valence.
Valence is how positive or negative your experience is. And that actually is really, you can get a very accurate measure of valence from the face.
And then the other axes is arousal, kind of the intensity of the emotion. And that is best quantified through your voice. And also your physiology like if you're measuring heart rate or heart rate variability or skin conductance.
Okay. And so is the implication then that your technology is primarily concerned with valence or do you attempt to measure both and map everything on to this two dimensional model?
So it's interesting because we have a valence measure and we have a proxy to arousal. We use like, you know, level of engagement, which is how much emotions are you showing?
Yes, fixation and stuff like that. Yeah, exactly. Are you paying attention or not? Are you fidgeting too much? So all of these measures could be kind of indirect proxies to arousal.
But we're also very interested in measuring discrete emotions. So that's an alternative model for thinking about emotional states. And that is triangulating exactly what the emotion is. So is it joy?
Is it surprise? Is it confusion? Is it contempt? And the face again is quite good at the specificity of the emotion. Whereas if you're looking at heart rate variability, it's often very hard to depict what kind of emotion is it?
Even though, you know, you know, it's a higher-ousal emotion, but it's hard to tell what exactly it is. We also provide our partners and our customers with, you know, a specific emotion measure. And we give you a probability scores because sometimes they co-occur.
Sure, sure. Have you applied this at all in the political sphere?
Not in the most recent political cycle. In fact, the most recent political cycle, we have a partner company called Higher View. They are in the recruitment space.
So they work with big companies that hire many, many people, but they have to screen, you know, thousands of people. And instead of requiring that a candidate sends in a word resume, you send in a short video interview.
Yep. And then they use our technology to rank order the video. So, you know, if you're a sales, if you're applying for a sales position or a flight attendant, having social skills is really a key component of that job. And so we're able to quantify that.
And so anyway, so their position was that, you know, the presidential debates are the ultimate job interview. So they, you know, they use our technology to analyze both Trump and Clinton's expressions. And we published that. So it's available online.
Oh, interesting. Interesting. I'll get the URL from you on that and we'll include it in the show notes.
Tell me a little bit about the underlying technology said it's based on deep learning. How is the technology evolved? And how would you characterize the approach?
Yeah, so it's so we use deep learning. The idea is that we collect, you know, hundreds of thousands of images of people and videos of people expressing emotion in the wild.
So it's not your selfies. It's not, right? It's not like your Facebook profile pictures. It's actually people, you know, sitting in front of a TV and just like chilling, right?
Or driving around Boston in their car in the typical Boston traffic. So it's very spontaneous data, which means that it's very hard data.
Like the background is messy. Sometimes it's quite dark. There's multiple people in the video. The expressions are not exaggerated. In fact, they're very nuanced and subtle.
So it's real data. It's what you'd see in the real world. And we take that data. We get a portion of it labeled for training, a portion of it labeled for validation.
And that becomes the data we inject into the, say, the convolutional neural nets. We also have temporal models because expressions are very dynamic and they unfold over time.
And over the years, we've continued to accumulate this data. So we now have about 2 billion facial frames. Not all of it is labeled.
And we've recently decided as a company that it's time to include other modalities. So we've started to research your emotions as they are represented in your voice as well, not just your face.
OK, OK, make sense. How did you collect all of this data? Do you have an app that provides some value to the consumer to incent them to just turn it on when they're driving or watching TV or is it all via clients?
I mean, at some point you had to kickstart this or cold start it and accumulate data before you had clients. How did you go about doing that?
We partnered with the very, very first project we did was with Forbes. And we put this thing on their website. And you can just go watch Super Bowl ads with the camera turned on. Then you would see your emotional profile.
But since then, we've done, you know, we've done this in 75 countries around the world through partners. So we have partners that are collecting this data as, you know, as part of, you know, testing ads.
OK, now it sounds a little bit like was it OK, keep it or the dating site that did the quizzes to get people to psychologically profile themselves.
One of the dating services was known for doing these small personality quizzes that were kind of entertaining for the people that were participating in them. But as a result, they were helping the company collect lots of profile data.
And you're doing something similar. It sounds like with your partners around ads and other media experiences.
So you're using convolutional neural nets and you mentioned some time domain neural nets as well, like LSTMs I'm imagining.
What is the, like, can you tell me a little bit about the data pipeline? How do you manage all of this video and get it ready to train the networks?
Yeah, so we have a cloud-based infrastructure that we've invested a lot into building that over the years. So all of our data goes into S3, then we use Amazon Web Services to do all the, so all the training is done in the cloud.
We also built our own cloud-based data labeling or video labeling infrastructure. So, and we have a team of certified, you know, face coders who, you know, basically their job is to watch these videos and identify when an emotional expression occurs.
And then we've built, again, in the cloud, we have this infrastructure that takes all this data and says, okay, there's agreement on the set of videos. There's no agreement on the set of videos. If there is agreement on a set of videos, that becomes, you know, split, it gets split into training and validation data.
And if there's no agreement, it goes back to the labeling tile. We also use active learning because we have a ton of videos. A lot of it has no emotions. Like if you're driving around Boston for the most part, you're just neutral.
We don't necessarily care about all these frames. So we use what we call active learning, which is we use the machine learning models to bootstrap the labeling.
So the machine takes a first stab at the labels and then based on what the machine says, we either fork it to actual human labelers for more labeling and more fine tuning or we kind of, you know, not throw the data away, but we park the data if it's not interesting.
So it's a human in the loop labeling infrastructure. And then once we have that, we say, for instance, you know, so let me take a step back. We do have a cloud based set of APIs where you would send a set of frames or videos and we would send you back the emotion probabilities in that video.
We have our partners use the technology that way. We call it emotion as a service, but we also have a number of partners that are developing real time experiences like think of a social robot that needs to respond to you in real time or Amazon Alexa kind of.
And so we can't be streaming, you know, tons of videos to the cloud and waiting for a response. And in that case, we have developed on device software development kits so that you could integrate the core engine into your device or your app and have run have everything run on the device.
You don't need to talk to the cloud at all. And so in that case, there's a trade off between accuracy and speed and computational weight, you know, how lightweight is it. And so we run experiments in the cloud, you know, where we say, okay, we're going to train with, I don't know, 200,000 samples.
And we are going to evaluate this model on how accurate it is across all these different emotions, but also how fast it is and how heavy it is. And that all happens in the cloud and our scientists, you know, will evaluate, you know, we come in the morning and there's a spreadsheet of all the different experiments that were run overnight.
And our team will kind of evaluate, okay, which ones, if we want one for the mobile SDK, then we're going to really care about how fast it is and how lightweight it is.
Does that make sense?
It does. It does. So there's mobile SDK. What are the platforms that that runs on?
We have it running on iOS, Android, Unity 3D Raspberry Pi, which was quite challenging windows, Linux, Mac OS X.
And was Raspberry Pi challenging because of RAM and CPU limitations?
Yeah, exactly.
Okay, okay. And is the idea with a Raspberry Pi type of implementation that it might be the back-and-compute module for a camera that's doing, you know, some kind of set top box or surveillance type of application?
We stay away from surveillance applications, but set top box or, you know, a connected home device that is, yeah, that is powering a social robot or, you know, a home monitor.
And why do you stay away from surveillance? Is that an ideological decision or technological decision?
It's ideological. So when we spun out my co-founder Rosalind Picard and I basically made a decision that we would, you know, in respecting how important and how personal this data is, that we would only work in use cases or industries where you knew that the camera was on.
And you knew that you were being analyzed in this way and we would veer away or steer away from applications where that was not the case.
We have stayed away from surveillance and security applications, yeah.
Well, I certainly respect and appreciate that.
I've seen some very scary surveillance demos recently and to imagine layering on this emotional context, which I'm, you know, it's one of those things.
It's always one of those things where like someone's going to do it at some point, but I can appreciate your decision to not be that someone that does it.
Right. Our thesis is if we're going to spend a lot of our mindset and our passion and our energy solving a problem.
You know, there's a lot of other problems we can solve.
Yeah.
Right. Right. Where do you see the space going?
Very exciting. So the ultimate vision is that our devices on our technologies are going to have emotion AI embedded in them.
We see a world where, you know, this is going to run passively in your phone. It's going to track your mood.
And on the path to that, we're seeing a lot more interest in the automotive space, for instance, where our kind of technology can help improve safety, especially in a world where we have semi autonomous vehicles.
So you can imagine, you know, your car is on autopilot, but it may need to relinquish control back to you as a driver.
And so the question becomes, you know, in a matter of seconds, it needs to know, are you awake or not? Are you paying attention or not?
And that's where our technology comes into play.
Yeah, I can also imagine a world where you've got digital billboards and you have some driver opt-in, some driver opt-in to this system.
Maybe they get, you know, some benefit or compensation, but you're reading their emotion as they're engaging, quote, unquote, with these billboards.
Lots of interesting applications. And I can, you know, certainly as, well, I was going to talk about, you know, being a New York City born in bread driver and road rage, but I probably won't go there and producers will cut this segment out of the conversation.
Road rage is definitely one of the emotions that we get asked about, but there's actually, oh yes, yes, road rage, drowsiness, distraction.
And you know what, too, like just general infotainment.
So a lot of these cars are now rethinking the brand experience, the in-cap brand experience, and they want to, you know, get a sense of the sentiment inside the vehicle, not just the driver or the cold pilot, but also the passengers,
and adapt the experience about the lighting, adapt the music, adapt the general environment and personalize it. So we get a lot of interest around that as well.
Okay, so that's where the business and use cases are going. Where do you see the technology going to, are there things the technology needs to go to enable these, you know, new and more ubiquitous use cases?
Yeah, definitely multimodal. So combining both, you know, computer vision with speech to get a better understanding of how you feel or, you know, your social and emotional signals, and that's especially important in conversational context.
So it's okay to just focus on your facial expression if you're just watching a movie, that's fine.
But if you are in a conversation with a chatbot or a personal assistant or, you know, social robot, or your car, then combining all these different modalities is going to be key.
That's definitely one class of product development and, you know, core science development that we're focused on.
But just more generally, there's a lot of applications in, say, healthcare, where there has been academic research that showed that, you know, your facial expressions and your voice can, can be biomarkers for pain, can be by markers for depression, even suicide assessment.
Oh, wow. So I would like to really see the technology move into that area and that would require, you know, substantial data collection of healthy population versus, you know, a depressed population.
But it's, yeah, I think that's where the technology can be really transformative.
Fantastic. Fantastic. Well, thank you so much for taking the time to speak with us.
These applications, this whole emotional space is one that I find really fascinating and I'll definitely be keeping my eye on what you guys are doing.
And, yeah, I wish you the best of luck. Thanks so much.
Thank you. Thanks for having me.
All right. Thanks, Ronald.
All right, everyone. That is our show. Thanks so much for listening and for your continued support, comments and feedback.
Special thanks goes out to our series sponsor, Intel Nirvana.
If you didn't catch the first show in the series where I talked to Naveen Rao, the head of Intel's AI product group about how they plan to leverage their leading position and proven history in Silicon innovation to transform the world of AI, you're going to want to check that out next.
For more information about Intel Nirvana's AI platform, visit intelnervana.com.
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference to enter.
Just let us know what you think about any of the podcasts in the series or post your favorite quote from any of them on the show notes page on Twitter or via any of our social media channels.
Make sure to mention at Twomo AI, at Intel AI, and at the AI come so that we know you want to enter the contest.
Full details can be found on the series page.
And of course, all entrants get one of our slick Twomo laptop stickers.
Speaking of the series page, you can find links to all of the individual show notes pages by visiting Twomo AI.com slash O'Reilly AINY.
Thanks so much for listening and catch you next time.
