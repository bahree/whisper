Welcome to the Twimal AI Podcast.
I'm your host Sam Charington.
Alright everyone, I am on the line with Mateo Kinazi.
Mateo is an associate research scientist at Northeastern University.
Mateo, welcome to the Twimal AI Podcast.
Hi Sam, thank you for having me.
It's great to be part of this show.
Thank you.
I'm looking forward to jumping into our conversation about some work that you've done recently
to apply machine learning in particular word embeddings and word-to-vec to the physics
research space.
In fact, we've covered similar applications.
First recently, the Twimal AI Podcast number 291 took a look at some applications of machine
learning to identify new materials by looking at materials research publications.
But very interested in learning a little bit more about your application.
But before we do that, tell us a little bit about your background and how you came to work
on kind of physics and machine learning.
And you're also doing some work in computational epidemiology as well, if I understand correctly.
Yeah, correct.
So actually, my background is a bit strange in a sense because my former background is
actually in economics.
So I hold a PhD in economics from Santana University in Italy.
But actually, during my career path, I've always worked, let's say, the intersection
between I would say economics, physics, and computer science in a sense because I'm
always being involved in this kind of field of research, which is called complex systems
and applied network science, typically.
And so what I've been working lately actually are essentially two main kind of broad area
of research.
One is, as you mentioned, now computational epidemiology, which means this idea of actually
building these large-scale simulators that can allow us to kind of model the spread of
a disease at the global scale.
So you can imagine, essentially, what we're doing at our lab is kind of have this essentially
complex piece of software that is able to kind of simulate in a realistic manner the evolution
of an epidemic at the global scale.
So imagine, let's say, I've been Zika starting a given point in Brazil and then spreading
it on the world or having, for example, to try to model, let's say, the patterns of
the seasonal flu, or the pandemic flu, let's say, in the United States.
So this is the kind of research we are doing.
And on the other hand, the other, let's say, focus of research was actually to work on
a discipline, which is called science of science.
And the idea there is essentially to kind of look under a microscope of how science works,
meaning, for example, how it evolves over time, how collaborations occurs between different
scientists and between different fields, how scientists speak their research problems,
how they, for example, move across different institutions, how nations develop expertise
in different fields of research and so on.
So those are kind of my main two topics of research at the moment.
How did you get started, in particular, on that science of science?
What led you to start exploring that area in your research?
Well, actually, that kind of comes in a sense from my economic background, because the
reason why I was attracted to economics in the first place was this idea of actually
trying to understand, as human behavior and all the interactions occurs between individuals.
And so translating that to science of science, the idea is that, okay, you have all these
many people working on producing new innovative research, you are trying to understand how
you can, for example, accelerate science and, let's say, redirect funding, so you can
get, let's say, the most out of it, and you can maybe, let's say, promote the specific
set of expertise in specific institutions, or pressing the word, so how will you do that?
And one way, which I approached the problem was actually to first try to understand how
you can actually map a research space in a given field.
In our case, we started actually looking at physics as a work.
Let's say, baseline, if you wish, because I mean, the people with whom I'm working most
of the time are actually people training physics or computer science, so that was kind
of the natural way to go in a sense.
Well, let's dig into this research.
So you start with the goal of mapping the physics community via the research.
Did you define it more specifically than that, or did you set out broadly to see what
you could see by applying machine learning to the space?
No, actually, we are using kind of a process definition given that this kind of, due to
the kind of data we are using.
So in our case, we are using publications from the American Physical Society.
So essentially, we have a data set with all the publications, all the papers that were
published in the journals of the American Physical Society, and we use that as our corpus
in a sense.
And the reason why we started looking at this is because there is, let's say, a very specific
classification that allows you to kind of look at each paper and know if that paper, which
kind of topics are covered in that specific paper, given that they have this classification
that we different essentially cause that tells you, okay, this paper is about nuclear physics
and specifically this subtopic of nuclear physics versus astrophysics or something different.
And so the fact that we were able to kind of map simultaneously publications, authors,
locations through the affiliations of the authors and topics, such topics, that's kind
of what we needed to be able to generate and then infer this knowledge and research space
in physics.
So the technique that you applied is word-to-vec specifically or word embeddings more generally?
The word embeddings, in a sense, that the model that we're actually using is star space,
which was developed by Facebook Research AI Group.
And but is one of, let's say, of the many, in a sense, variations of the word-to-vec approach.
So the idea, in our case, is that we are treating each author as a bag of topics.
So as a bag of research fields in which that author has worked.
And then we use this bag of topics to kind of infer the embeddings for each specific
research sub-area, in a sense, and create this mapping of the overall nor space by essentially
looking at the expertise of authors to guide us on what it means to be, for two topics
to be similar to each other.
So that's the general idea.
So for example, if we, let's say if we go back to the word-to-vec idea there, I mean,
the main assumption is to use this distribution hypothesis idea, then the fact that essentially
you can infer the meaning of a word by looking at the context of that word.
And in our case, let's say the analogy in our case is that we're saying, okay, we actually
observe the publication record of scientists.
So we know what each scientist is able to actually produce in terms of research.
And so we use that as our, let's say, way to infer what is the context of a specific
sub-topic.
So we're saying, okay, given that this author has published, for example, in nuclear physics,
we can look at what other topics that author has published on, and we use that as a context
to kind of predict the embedding for the former, for example.
Okay.
In that sense, does it mean that the approach that you've taken is more of a supervised type
of an approach using the labels that were available in the journals for the different
research than word-to-vec, or would you still consider it unsupervised or semi-supervised
or something like that?
So yeah, actually, I mean, it can be considered if you wish supervised learning in a sense
that we do have the code.
So we do have this Pax code that stands for Physics and Astronomy Classification Scheme,
the data attached to each paper.
Okay.
But actually, the way in which we use them, they are as if they were simple keywords.
In a sense that we are not trying to actually, for example, create an embedding of the author,
given the labels that we have access to the author, we are actually treating each bag of
labels in a sense as a standalone document.
So for example, the idea is that you can, I mean, our idea actually is that you then you
can apply this approach in general to any kind of keyword list that you have associated
to papers, which can be even inferred, for example, semi-automatically from titles or abstracts
of papers.
But the reason why we chose to use this specific data set with this specific taxonomy already
embedding into it was because this will give us a way to actually have a ground truth
baseline to compare our results with.
So the idea is that we know that this taxonomy exists, we know that we have these hierarchical
schemes that tell us that physics is divided, for example, in 10 big sections, and then
each section has many different sub-fields and so on, but we are not using that kind of
information at training time.
So the idea is that once we have our embedding, can we actually see that same, in a sense,
classification scheme exposes?
Can we use that to actually validate what we are going to observe?
And that's why we're using it in that sense.
Got it.
Are you also using the full texts of the research papers or are you only using these keywords
as kind of your document?
No, we are only using these keywords because really our idea was to kind of just, in a sense,
start with a minimal level of information we could get, in a sense, but yes, really
I mean, our goal actually is try to kind of extend it beyond physics, actually, where
we do not have any sense of classification scheme.
And then at that point, yes, we will go towards the direction of using, for example, text
of abstracts or titles to actually infer automatically, in a sense, these keywords.
These keywords.
Okay.
And so with that as the backdrop, what are some of the things that you've been able
to kind of identify within this embedding space that you were able to create?
Yeah.
So what we were able to do essentially was to kind of fingerprint the scientific production
of cities and recover what is, let's say, an effect that is already observed in literature
using different approaches, which is this idea of this so-called principle of relatedness.
Principle of relatedness?
Yes.
That's correct.
And so that's something that comes, let's say, from the economic geography literature
in the first place.
And that's the idea, for example, that if you look, let's say, the production and exports
of the nation, so the country, and you look, let's say, what kind of product category is
the export to their trade partners?
You can kind of predict what kind of, what new categories they are going to be able to
export next by looking at, by first, getting a measure of the relatedness between the
different product categories.
And then by essentially kind of checking where is the overall production, the overall level
of expertise of the country, and then use that to kind of predict what is the next item
you're going to export.
And the idea there is that if you are working in a part of, let's say, this product space
in which we have, let's say, you are able to, you have skills and expertise in different
product categories, and you have a way to measure relatedness within another product category
that will help you kind of predict.
If that's where you're going to go next or not.
And in our case, essentially, this kind of, the analogy in our case is that now at this
point, we're looking at cities.
So we kind of collect all the publications at the geographical level.
And then we use our knowledge space, our research space, build using your buildings to measure
this level of relatedness, and then kind of predict where each city would go next in
terms of their scientific expertise.
So for example, we might look at the publication records in a specific time window, let's say
from 2000 to 2003, they were going to kind of predict using this embedding space, whether
or not the specific city will have a so-called comparative advantage, a revealed comparative
advantage in specific subfielding physics in the next time window, for example.
And presumably, you were able to identify those kinds of trends with embedding space you
created?
Yes, that's correct.
So what are some examples of any examples of cities and research topics come to mind?
Yeah, me.
So what we do essentially is, on one hand, what we do is, for example, look at specific
cities and look at how their activities cluster in this embedded space.
And so, for example, as we show in the paper, we can, let's say, see that cities like
Brassos, for example, they have a clear level of expertise and expert knowledge in nuclear
physics, for example, while you might have, let's say, other cities like Grenoble to
instead specialize more on condensed matter physics.
So this is what you can actually really, really observe by just using this embedding space
and using an effort to kind of represent the interactions between different, between
these different vectors representing the topics.
And then what we show is that we can then use this model to kind of rank the probability
of a given city to enter in a specific field.
And they use that if you want as your classifier that will tell you whether yes or no whether
or not that activity is going to be developed at the next time step.
And for that, we kind of show that let's say, distribution of our predictions is actually
better than a simple random model where, let's say, all these expertise will evolve randomly.
And so is the model primarily good at identifying new research topics that are closely adjacent
to existing research topics as a direction for a given city?
Or is it, yeah, there's some ability to take more leaps if you all.
So that's actually one of the things that are studied in the literature.
And the idea is that when you build this space, you compute a so-called measure of knowledge
density, in a sense.
So imagine that now you have this very, in our case, very complex and dimensional space
where you have different research topics, so different points placed in this and dimensional
space.
And you want to look if there is any specific area of this space where specific entity
in our case, let's say, city has a high density of topics that are active.
So what you show is that actually, that city is more, let's say, likely to be actually
able to develop an expertise around that area.
So in areas in which it actually did the density of points is high.
And like, and that is also what is found in the literature using essentially also different
techniques.
And that is like a pretty robust result.
The only, let's say, exception to that is that, for example, in the case of thing that
was of, I think, economic activities, what they show is that if you are, for example, a
country that is in an intermediate level of development, your more chances of actually
jump to a different part of the space, in a sense.
And there might be ways for you to kind of optimize your trajectory in the space precisely
by kind of knowing what is them up around you and knowing where you have your level of expertise.
You mentioned earlier that you used an algorithm that was developed at Facebook, what was that
algorithm?
The name is star space.
Star space?
Yes.
And can you go into a little bit more detail?
How does it differ from word to vac?
Yeah, sure.
So the main difference that here we actually don't have a neural network.
So the embeddings are not trained as in word to vac as like an even layer in a very simple
neural network.
But in this case, instead, you kind of directly treat the problem as an optimization problem.
So you have these metrics that represent essentially your dictionary.
So in our case, let's say, all the different research topics that we have in our papers.
And then the other dimension will be the actually embedding dimension that you picked.
And then you have a loss function that kind of tries to optimize essentially these embeddings
and optimize these metrics, but essentially allow you to play with essentially what is
to be considered similar to each other.
So essentially, the way the loss function is written is that you have a generator of
positive and negative pairs, which will change depending on the kind of problem that you
are trying to address.
So for example, in our case, let's say we have a list of topics for a researcher.
So the positive pairs will be, for example, the fact that I leave one of these topics
out and that would be my target.
So what I actually want to kind of predict.
And then all the other topics within that pack will actually be something that I can use
to generate these positive pairs.
So these pairs that you have in our case, for example, are very high cosine similarity.
And then on the other side, you have a negative pair generator, which instead kind of throws
randomly other topics, for example, other bag of topics, or you already actually dictionary.
And those are the ones in which you kind of want to stay away from, in a sense.
And that is done in a similar way as it is done, or some implementation work to act using
negative sampling, meaning that you indeed sample randomly what are, let's say, the negative
cases, so that the points in the space from which you want to be far in a sense.
And so all of this essentially is treated as just an optimization problem, which tries
to directly learn these metrics.
So do you think that a neural network-based approach would get you different results
in any significant way?
Is that something that you're interested in or not so much?
Now, actually, I think they will probably give different results in a sense that what
we are not playing with right now is actually this idea of having, in a sense, a time-spend
list of topics, like, let's say, when you use, let's say, word-to-back in a word idea,
is that you have, for example, this kind of rolling window that you define your context,
right?
So you can say, OK, look at two words before or afterwards.
And then depending on whether you're using the continuous bag of words or the skip-gram
approach, one is the target or the other.
But essentially, the idea is that you have this kind of rolling window in your sentence
that speaks up what is the context.
Well, in our case, what we are doing is that we are treating everything as if all the
production of a given scientist is the context.
And there is no idea of, in a sense, temporal distance embedded yet.
So one idea would be to maybe try to apply this algorithm maybe using a word, even just
using a continuous bag of word implementation word-to-back, for example, to adapt this idea
of having an early window.
And the other thing is that, for example, if we're to use word-to-back in this skip-gram
approach, then actually we'll give us something probably maybe that might be different
than what we're observing right now.
So of course, let's say, in the paper, what I show is that actually this method seems
to have better results in the use cases they test.
Then, for example, fast text, word-to-back, and I think also gloves, as far as I remember.
But yes, it's definitely something we are willing to explore.
So our idea is in the action not to try to kind of come up with what would be the best
way of creating these embeddings, giving that we have a very specific problem in mind.
So we are not teaming at creating embeddings that should work for, let's say, whatever
task at hand, but actually the shoework for very specific, to test very specific ideas
in a sense.
Are you incorporating, in any way, some kind of waiting of a given researcher's volume
of work or significance of work in a given area, or is the, their kind of bag of topics
just based on anything that they've been active in?
No, actually, it's based on anything that they've been active in, and that's actually
by design, and that's something we wanted to do like that.
And the reason for that is that, let's say, the other approaches that do not use embeddings
to kind of address the same problem have that kind of filtering.
So have this idea of actually first filtering, for example, what are the topics in which
information or an institution or an entity have what is called a reveal comparative advantage?
So which putting in your words would be something like, okay, identify where that scientist
is most active on.
So remove, for example, all the site papers that have topics that come out maybe of site
projects or feeds the abandoned or something like that.
Well in our case, what I said, no, we actually don't want to embed any of that information
in our embeddings.
So we want to try to see, okay, what if we are applying, we just keep and get a big pile
of papers from an author, we just list all the topics in which has worked on, and train
on that.
So we don't for the filtering without trying to kind of pick the data to get the best
results possible.
Our idea was to show that you don't need to do that to have the same results that you
have with a lot of this manual, let's say, checks and optimizations.
Given that you're focused on a very specific task, what was your performance metric against
that task?
So okay, there's different in a sense checks and validations with it.
So the first validation, which was not in a sense quantified, but was just in a sense
of visual inspection, was what I was saying before, of actually comparing our results from
this box classification scheme.
So the idea is that what we did was that we obtained these embeddings, then we used
them to create actually a research network space.
In the idea, there is that we are borrowing, let's say, techniques and tools from network
science.
And at that point, what we're doing is that we are treating each topic as a node in a network
where the nodes are connected to each other through edges and each hedge as a way has
actually the amount of similarity relatedness to those two topics share, which in our case
would simply be because of similarity of two topics.
Then after building this network, we kind of use what are colleagues spring layouts
in this kind of literature, which essentially the idea, the basic idea is that imagine that
you have these balls into space that are representing your different topics, and you have these
different edges connecting them.
Now, imagine those edges are actually springs that have different force depending on the
level of similarities of those nodes, so those entities.
And so the strong of the similarity, the closer those nodes will be in your visual representation.
And so we use that to build essentially this research network space.
And in a sense, that is our way to kind of make the projection from the embedding space
to a 2D dimensional plot in a sense.
And by doing that, we can kind of clearly identify the 10 macro areas in which we know that
Pax classification divides the different branches of physics.
And that was kind of our first in a sense validation after running the embedding problem,
because we knew that we weren't providing that kind of information to the algorithm.
So we know that that kind of information of the existence of this hierarchical classification
of topics was not used at training time.
So that was our first way to benchmark ourselves and say, okay, look, actually our results
in the look promising.
And the second way in which we tested was actually to indeed measure the predictive power of
the algorithm.
And at that point, what we did was essentially to look at each city at a given point in time.
So in a given time window, list the topics in which that city had a comparative advantage.
So list the topics in which the research topics in which that city was particularly good at
producing new research.
And then try to predict what will happen next as if it was, let's say, a classifier problem
where you have that you have one, see if you're going to develop in that new field and
zero otherwise, and then use a standard metric like rock curve or something like that to
actually kind of get a sense if our model was better than a random model when instead
this evolution of expertise was occurring randomly.
So those are the two ways in which we tested our results.
And did you have any external comparisons available to you in the first of those two cases
you're kind of comparing against common sense, but as you've mentioned previous research
on the economic side, for example, are the results compatible such as you can kind of compare
what you're seeing to what previous papers have shown?
Yes.
So for example, in terms of the kind of, in a sense, so let's say the map of the knowledge
based with finding physics is actually very similar to the maps that are obtained.
For example, by other researchers using different techniques, for example, looking at citation
networks.
So how different fields, subfields cite each other.
And yes, actually, what we have found is that our results are well in line with what
has been found using other methodologies and other approaches and actually even other
data sets as far as I remember.
And so that that was, let's say, confirm in a sense.
So on the physics side, on the, let's say, more, let's say, theoretical side of whether
or not we could like reproduce this principle of relatedness effect, in the dose in that
case, we are getting results that are well in line with what has been found in the literature.
What do you say this going?
What's next in this line of inquiry for you and what do you think it opens up?
Well, first of all, our idea is to actually extend this and this and we were already doing
to other things rather than physics.
So for example, now there is, let's say, a lot of data available to researchers in this
field.
So Google Scholar, Microsoft Academic Graph and other big databases that could allow you
to actually extend this approach in a sense of scale, look at different disciplines.
And then one, and we're already working on that, we already have some preliminary results
on that.
And for example, what we have seen is that by using the same approach in the Microsoft
Academic Graph, which is essentially another of these bi-burematic data sets that kind
of included at that point all the, all the different fields in science.
And for example, using it there, we kind of can replicate fully that it's us we get in
physics to actually all the other fields.
But then actually our goal is to start using this embedding space as our, let's say, additional
tool in our tool kit to actually try to something new.
So for example, you know all these papers that, let's say, for example, crack the evolution
of the semantics of a term over time, for example.
So you can imagine that given that we have this space in which we can embed topics, but potentially
also authors, fields, and so we can kind of try to understand, for example, how authors
move in this space.
Of, for example, can we find the same kind of analogies that we have in the classical,
let's say, examples of King, Queen, man, woman, and so on.
Arigmatics can we do the same things in science, for example.
So can we kind of understand where they might be gaps into the, into science by actually
try to play with these analogies?
So try to see, for example, what happens if you use, or if you apply a specific, sub-fielding
computer science to maybe a field in which that was never considered as kind of technique.
So once that you get a sense of how to navigate this space, then you can try all these kind
of scenarios and thought experiments that maybe can give you more of an intuition of how
science really works underneath.
So what is the actual structure of the knowledge that is kind of keeping everything together?
It strikes me that it's possible that the, you know, the shifts that we see in research
have less to do with changes happening, you know, in, you know, new research in a city,
and more to do with changes in the way we describe our research and these tags that we applied
to them.
And I'm wondering if you've explored that at all.
Knowing a sense of that, I mean, the way I think I understand your point is, is not
a problem for us, so the way in which we define strength in a sense of a city in a specific
topic is by using this concept borrowed from economics of trivial comparative advantage,
which put in a very simple terms is like saying, okay, that your podcast, for example, if
you compare it to other shows, as a comparative advantage in machine learning simply because
they let's say the share of time that you devote to this topic compared to all the other
possible topics that you might consider is larger than the share than the average share
of podcasts that talk about machine learning.
So it's a relative measure.
So it's like saying, okay, imagine, for example, now from T2T plus 1, you have an explosion
of papers using Word2Vac, for example.
So let's say that your share of Word2Vac papers goes from 5% of the publications to 10%.
Okay.
That's not really going to affect the results because what we're going to look is what
is your own, as a country, as a city, share of publication in that topic compared to what
the Word is doing.
So for example, if a 10% of your publications that were in Word2Vac in the Word was 5%, then
you kind of have a value of this trivial comparative advantage that is true, which means that you
are above average on how much you're publishing in that topic.
If at the next time step, let's say that when, let's say, for example, Word2Vac publications
increase, your share of publication doesn't change.
So you still have that actual share of publications in Word2Vac papers is 10%, but now the Word
share is 10% as well, then your kind of strength have decreased.
So the effect of controlling for the actual absolute size of a different topic, sort of different
fieldings, embedding in our definition of how we define, in a sense, strength and weakness
or expertise in different fields of research.
So what we observe is actually indeed a relative measure of how good you are with respect
to all the other players in the arena in a sense.
I think the scenario is trying to point out, and this may be way in the weeds and not
at all irrelevant, but what I was trying to get at is, you know, what if we have this
machine learning podcast and we talk about embeddings all the time, and then the term
Word2Vac suddenly becomes more popular and so we start using that as opposed to another
way of describing the same thing, or the field matures and we get more and more specific.
Your approach, out of necessity, is really concerned with the way we're describing what
we're doing and what we can learn about the way we describe what we're doing as opposed
to what we're actually doing that your research isn't really involved in kind of mapping
that to the cities themselves and trying to project, for example, economic value that
comes out of any of these changes in research areas to validate the model in any way.
So in terms of, let's say we, as we said, maybe we get more specific in like talking
on, let's say, discussable specific topics, so we might kind of switch the way in which
discuss that.
That's actually the whole reason why we introduced the embedding idea, embedding model around
this problem precisely because if your embeddings work well, then that kind of transition
shouldn't really matter because, for example, if now you start from talking, I don't know,
from word-to-back tool, let's say, using only glove, in theory, your, let's say, system
so your, let's say, knowledge space should have already accounted for that.
So it should have already placed, for example, the two vectors very close to each other.
So I don't think that's actually a problem given how we are setting up the whole system,
which is kind of why we decide to go with the embedding approach rather than, for example,
do probability counts or quick current counts as others, what authors have been doing precisely
because of the problem you're saying.
And regarding the economic impact, actually, indeed, that's actually the next step in
the system.
That's actually why we want to kind of extend our approach to different, to different
areas.
For example, because another thing that we have been working on and we can actually,
you can imagine doing is that you don't only have academic publications in the world,
but for example, you also have patents.
So you actually also have recordings of what, for example, cities, if we want to stick
with the geographical dimension, what cities have been publishing both in terms, for example,
of technology, so patent applications and also publications.
And so you can imagine that if you actually are able to build this big knowledge space,
that includes not only knowledge as publications, but actually, for example, technology, then
maybe you're actually able to kind of throw a line between the expertise that you might
be developing in a specific field, research-wise, and the kind of maybe patenting activity might
open up for you in the future.
And then the actually link between the patent and the activity and the actual economic value,
I would say is pretty straightforward, and it's still out of work to do, but I mean,
it's something that any way can be done.
So that's actually the reason why we are kind of trying to refine this approach, because
once you have this space that can tell you essentially how science works and how technology
works as an extension depending if you put those patent applications in or not, then you
are actually able to answer those kind of questions.
Fantastic.
Mateo, thanks so much for taking the time to chat with us about what you're up to.
Thank you for having me.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening, and catch you next time.
