1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,940
I'm your host Sam Charrington.

4
00:00:31,940 --> 00:00:36,660
You are invited to join us for the very first Twimblecon conference which will focus on

5
00:00:36,660 --> 00:00:41,220
the tools, technologies and practices necessary to scale the delivery of machine learning

6
00:00:41,220 --> 00:00:43,740
and AI in the enterprise.

7
00:00:43,740 --> 00:00:48,600
The event will be held October 1st and 2nd in San Francisco and early bird registration

8
00:00:48,600 --> 00:01:01,720
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

9
00:01:01,720 --> 00:01:07,120
Before we continue, a big thanks to the folks at C3 for their sponsorship of today's show

10
00:01:07,120 --> 00:01:11,520
which was recorded at their C3 Transform Conference earlier this year.

11
00:01:11,520 --> 00:01:17,720
C3's AI suite helps enterprises rapidly develop, deploy and operate AI at scale and offers

12
00:01:17,720 --> 00:01:23,720
adaptable AI powered applications for predictive maintenance, fraud detection, customer engagement

13
00:01:23,720 --> 00:01:25,800
and many more use cases.

14
00:01:25,800 --> 00:01:30,840
Take a look at what they're up to at c3.ai and if you enjoyed this conversation be sure

15
00:01:30,840 --> 00:01:35,920
to let them know on Twitter at c3 underscore ai and thank them for their support of the

16
00:01:35,920 --> 00:01:36,920
show.

17
00:01:36,920 --> 00:01:39,200
Alright let's do it.

18
00:01:39,200 --> 00:01:47,440
Alright everyone I am here at the C3 Transform Conference in San Francisco and I've

19
00:01:47,440 --> 00:01:53,440
got the amazing pleasure to be seated with Dan Gevins, a veteran of the podcast and

20
00:01:53,440 --> 00:02:00,520
Adi Bashaam, Dan is the general manager of data science at Shell and Adi is the vice

21
00:02:00,520 --> 00:02:05,600
president of Ford Deployed Solutions, alliances and strategy at C3.

22
00:02:05,600 --> 00:02:13,520
So Dan was on the show twimblecon number 202 back in November which was rolled out as

23
00:02:13,520 --> 00:02:16,400
part of our AI platform series.

24
00:02:16,400 --> 00:02:21,840
So Dan I'm going to refer folks back to that show to get your full background but before

25
00:02:21,840 --> 00:02:28,640
we dive into the heart of the topic Adi tell us a little bit about your background.

26
00:02:28,640 --> 00:02:29,640
Sure happy to.

27
00:02:29,640 --> 00:02:36,280
So I have had a few roles at C3 but I've been at C3 now for about four years.

28
00:02:36,280 --> 00:02:41,640
I currently lead forward deployed solutions that think of that as sales consulting, sales

29
00:02:41,640 --> 00:02:42,640
engineering.

30
00:02:42,640 --> 00:02:45,400
I also lead other alliances and strategy arc.

31
00:02:45,400 --> 00:02:51,480
I joined C3 in the product organization and had to get my hands dirty both verifying

32
00:02:51,480 --> 00:02:55,640
machine learning algorithms deployed correctly and also writing product spec, actually build

33
00:02:55,640 --> 00:03:01,080
applications that use those algorithms to actually solve business problems.

34
00:03:01,080 --> 00:03:04,520
Prior to C3 I was at McKinsey and company.

35
00:03:04,520 --> 00:03:11,320
I was there for a total of about 12 years which in consulting years feels like 50.

36
00:03:11,320 --> 00:03:16,880
And in that time and most recently before I left I was largely leading our predictive

37
00:03:16,880 --> 00:03:22,360
analytics and big data solutions focused on B2C subscription businesses, things like

38
00:03:22,360 --> 00:03:25,880
telecom and other SaaS businesses.

39
00:03:25,880 --> 00:03:31,520
I have an MBA from the Kellogg school at Northwestern University just not the Chicago and I'm

40
00:03:31,520 --> 00:03:35,880
a computer science and I have a computer science degree from IIT Bombay.

41
00:03:35,880 --> 00:03:40,120
So I have my graduate work was an electrical engineering at Northwestern.

42
00:03:40,120 --> 00:03:45,680
I lived in the basement of tech for I live mostly in Jacob's center, but yes.

43
00:03:45,680 --> 00:03:46,680
Nice.

44
00:03:46,680 --> 00:03:50,800
Dan I just had an opportunity to catch part of your presentation here or actually all

45
00:03:50,800 --> 00:03:57,360
of your presentation here at the conference and one thing that really struck me was at

46
00:03:57,360 --> 00:04:02,560
least from my perspective based on our last conversation how much has matured around

47
00:04:02,560 --> 00:04:07,120
AI platforms at Shell in just four months.

48
00:04:07,120 --> 00:04:10,800
So you had a slide that says Shell.ai and like a platform or three different we didn't

49
00:04:10,800 --> 00:04:13,440
talk about any of that stuff.

50
00:04:13,440 --> 00:04:16,560
Talk about the kind of evolution over.

51
00:04:16,560 --> 00:04:20,960
So it's kind of funny because obviously you get to talk about more as it matures right

52
00:04:20,960 --> 00:04:22,960
so that's that's part of it.

53
00:04:22,960 --> 00:04:25,480
Obviously you didn't come up with it and do it in form.

54
00:04:25,480 --> 00:04:26,480
Exactly.

55
00:04:26,480 --> 00:04:27,480
Exactly.

56
00:04:27,480 --> 00:04:30,760
No I think we were looking for a banner to bring this all together.

57
00:04:30,760 --> 00:04:36,080
So I talked a lot about in the presentation the real importance that this is not just

58
00:04:36,080 --> 00:04:42,000
a platform play, it's also a play around how do you create a narrative and a culture

59
00:04:42,000 --> 00:04:46,720
and a way of working that starts to drive transformation right across the business.

60
00:04:46,720 --> 00:04:47,720
And that's the vision really.

61
00:04:47,720 --> 00:04:49,920
So we're doing that under this banner of Shell.ai.

62
00:04:49,920 --> 00:04:55,400
If you want to check it out we have a website now, Shell.ai is real, you can just google

63
00:04:55,400 --> 00:04:58,120
it and you'll see a lot of the things we're doing.

64
00:04:58,120 --> 00:05:01,200
We've got some little videos out there talking about some of the work that we're doing

65
00:05:01,200 --> 00:05:05,000
and we're going to put more and more out there over the coming months.

66
00:05:05,000 --> 00:05:10,560
And it's becoming a bit of a narrative within Shell and also something we're using quite

67
00:05:10,560 --> 00:05:13,760
prolifically now with the external world as well.

68
00:05:13,760 --> 00:05:16,800
And just out of curiosity why externalize that?

69
00:05:16,800 --> 00:05:21,200
I'm assuming you're not going to have external users of Shell.ai, I'm not going to go create

70
00:05:21,200 --> 00:05:24,440
an account and put in my credit card and run some TensorFlow.

71
00:05:24,440 --> 00:05:25,440
Yeah, that's right.

72
00:05:25,440 --> 00:05:26,440
Exactly right.

73
00:05:26,440 --> 00:05:28,080
It's not going to be that sort of thing.

74
00:05:28,080 --> 00:05:30,680
I think there will be, so there's two reasons.

75
00:05:30,680 --> 00:05:35,640
And increasingly we recognize within Shell that this whole journey is about partnership.

76
00:05:35,640 --> 00:05:39,720
It's about partnership with companies like C3.

77
00:05:39,720 --> 00:05:44,840
It's about partnership with the innovative tech startups right here in San Francisco

78
00:05:44,840 --> 00:05:47,360
of which I'll be meeting a few this week.

79
00:05:47,360 --> 00:05:53,000
It's about also trying to attract the top talent that want to work on really tough problems

80
00:05:53,000 --> 00:05:55,280
in the energy industry.

81
00:05:55,280 --> 00:05:59,200
And the final thing is it's also about partnership with some of the people we want to work with

82
00:05:59,200 --> 00:06:01,840
and because a lot of our assets are ventures there.

83
00:06:01,840 --> 00:06:03,840
It's not just Shell, it's other companies involved.

84
00:06:03,840 --> 00:06:06,480
So we need to be able to say to them, this is what we're doing.

85
00:06:06,480 --> 00:06:07,480
Now let's work together.

86
00:06:07,480 --> 00:06:12,640
So our spirit is not one of creating a product that we want to sell, but it's much more

87
00:06:12,640 --> 00:06:16,200
something which we say let's work together on this because that's the whole spirit of

88
00:06:16,200 --> 00:06:17,200
digital.

89
00:06:17,200 --> 00:06:22,880
So let's maybe dig in a little bit into this platform that you've created.

90
00:06:22,880 --> 00:06:26,680
You described kind of three specific elements.

91
00:06:26,680 --> 00:06:29,520
Do you walk us through the elements and the roles that they play?

92
00:06:29,520 --> 00:06:30,520
Yeah, absolutely.

93
00:06:30,520 --> 00:06:37,720
So if you think at the raw end, we've got a whole raft of advanced citizen data scientists.

94
00:06:37,720 --> 00:06:38,720
It's a horrible term.

95
00:06:38,720 --> 00:06:40,960
I don't like it, but it kind of characterizes what they are.

96
00:06:40,960 --> 00:06:46,120
These are people who are engineers there, subsurface specialists in Shell.

97
00:06:46,120 --> 00:06:50,520
They've typically got some sort of technical background and they're quite comfortable with

98
00:06:50,520 --> 00:06:51,520
algorithms.

99
00:06:51,520 --> 00:06:55,760
So the question is how can I make it really easy for those folks to use some standard

100
00:06:55,760 --> 00:07:00,680
tools that are available on the market to come in and rapidly prototype something.

101
00:07:00,680 --> 00:07:04,760
So we work with companies like Ultrix and Databricks to make it really easy for those people

102
00:07:04,760 --> 00:07:09,840
to come in and also other companies like MathWorks, for example, and the MATLAB product, which

103
00:07:09,840 --> 00:07:12,160
actually has come on quite a long way in the last few years.

104
00:07:12,160 --> 00:07:17,680
So we create that environment largely focused on R and Python to allow them to prototype

105
00:07:17,680 --> 00:07:19,400
these things quickly.

106
00:07:19,400 --> 00:07:22,800
We've then got kind of a group of people who also need a platform where they want to

107
00:07:22,800 --> 00:07:24,280
train deep learning.

108
00:07:24,280 --> 00:07:27,880
So we're applying deep learning in spaces like Autonomous Well Drilling.

109
00:07:27,880 --> 00:07:33,040
We talked about that the last time, deep learning on seismic fault detection.

110
00:07:33,040 --> 00:07:37,520
We're also looking at deep learning applications in a number of other areas, so things like machine

111
00:07:37,520 --> 00:07:40,080
vision or natural language processing.

112
00:07:40,080 --> 00:07:43,560
And those guys tend to want to work in the latest open source technologies.

113
00:07:43,560 --> 00:07:47,080
So they might be comfortable using a Databricks and a Spark, but they also need things like

114
00:07:47,080 --> 00:07:49,640
Cubeflow and Kubernetes.

115
00:07:49,640 --> 00:07:53,080
They want to be able to manage their data versionings.

116
00:07:53,080 --> 00:07:55,560
So we work with companies like Packidum for that.

117
00:07:55,560 --> 00:07:59,720
So we're looking at those sort of technologies and bringing those together into an integrated

118
00:07:59,720 --> 00:08:06,560
hole to allow our real specialist data scientists to train things at scale and to make sure that

119
00:08:06,560 --> 00:08:11,160
they're able to deal with the big data volumes in an predominantly cloud-based environment.

120
00:08:11,160 --> 00:08:13,520
But then you've got to take that to the masses somehow, right?

121
00:08:13,520 --> 00:08:17,880
So you've got to be able to deploy that easily and effectively.

122
00:08:17,880 --> 00:08:20,720
And so we're working with the cloud vendors, but we're also working with companies like

123
00:08:20,720 --> 00:08:26,400
C3 to help really take those to enterprise grade software solutions that you can then

124
00:08:26,400 --> 00:08:30,600
deploy out to your business users and that they can get the benefits from the applications

125
00:08:30,600 --> 00:08:33,960
that we're building in the context of their day-to-day work.

126
00:08:33,960 --> 00:08:39,240
Clearly, as an organization, you're not afraid of rolling up your sleeves and integrating

127
00:08:39,240 --> 00:08:41,280
pieces together.

128
00:08:41,280 --> 00:08:46,680
A lot of the other companies in fact that we featured in our platform series, the likes

129
00:08:46,680 --> 00:08:49,760
of Facebook and Airbnb, they're building platforms.

130
00:08:49,760 --> 00:08:54,520
They kind of rolled up their sleeves, glued a bunch of stuff together or welded, in

131
00:08:54,520 --> 00:08:58,080
the case maybe, you're not afraid to do that.

132
00:08:58,080 --> 00:09:06,200
Why rely on a partner like C3 to provide more of an off-the-shelf kind of solution as

133
00:09:06,200 --> 00:09:12,400
opposed to just going that extra step and doing the welding or gluing?

134
00:09:12,400 --> 00:09:13,400
It's a great question.

135
00:09:13,400 --> 00:09:18,200
I think we thought about it, so just being completely honest and Adi and I had some

136
00:09:18,200 --> 00:09:23,080
tough conversations around this, but I think the real thing for me was, if you look at

137
00:09:23,080 --> 00:09:27,480
my data science heritage as an organization, we trace our origins, my team right back

138
00:09:27,480 --> 00:09:32,320
to the 1970s, where Shell was the industry leader in things like scenario planning and

139
00:09:32,320 --> 00:09:37,720
we found it from the first statistics groups in the industry, the issue is that we don't

140
00:09:37,720 --> 00:09:40,440
have that same heritage in software engineering.

141
00:09:40,440 --> 00:09:44,280
We were really looking for a partner that's going to help us move forwards with not just

142
00:09:44,280 --> 00:09:48,680
the stitching together of being able to make it work, but actually being able to make

143
00:09:48,680 --> 00:09:53,480
it work at scale in a production-ready product that's going to allow us to deploy this right

144
00:09:53,480 --> 00:09:55,960
across our business very, very fast.

145
00:09:55,960 --> 00:10:00,240
That's something that we weren't, we didn't feel we were fully equipped to do and that's

146
00:10:00,240 --> 00:10:04,160
also where C3 have been helping us in that whole journey.

147
00:10:04,160 --> 00:10:09,200
And I think the other thing that's worth saying is we liked the reusability of that type

148
00:10:09,200 --> 00:10:10,200
system.

149
00:10:10,200 --> 00:10:17,080
We know that a lot of the setups that you have in the sort of, I guess the cloud space,

150
00:10:17,080 --> 00:10:21,000
they don't really think about a common data model to knit it all together.

151
00:10:21,000 --> 00:10:26,000
Having that ability to create reusable data assets that can then be built on for other

152
00:10:26,000 --> 00:10:31,040
software development is actually pretty powerful, particularly when you start thinking about

153
00:10:31,040 --> 00:10:35,240
a world in which you've got a lot of common data in common areas that is then reusable

154
00:10:35,240 --> 00:10:36,720
for multiple use cases.

155
00:10:36,720 --> 00:10:42,680
Now, I think we've talked about in my previous interviews here a bit about the data model

156
00:10:42,680 --> 00:10:48,360
and the notion of virtual data lakes and the like, but from your perspective as helping

157
00:10:48,360 --> 00:10:54,800
to support Shell, how have you been able to kind of bring these ideas to bear to support

158
00:10:54,800 --> 00:11:00,920
their use cases and what are some of the things that you've learned in that process?

159
00:11:00,920 --> 00:11:02,600
It's a great question.

160
00:11:02,600 --> 00:11:08,440
I think, let me answer that question to very distinct ways.

161
00:11:08,440 --> 00:11:16,640
In the work we are doing with the Shell COE team, the center of excellence team, the core

162
00:11:16,640 --> 00:11:26,680
value of the C3 AI suite or the platform is really around very high or rapid extensibility

163
00:11:26,680 --> 00:11:29,040
and reusability.

164
00:11:29,040 --> 00:11:33,960
And hopefully, and I think we're doing this, though there's no easy peer to peer or

165
00:11:33,960 --> 00:11:38,520
like to like comparison, significantly reducing the amount of lines of code you actually do

166
00:11:38,520 --> 00:11:42,000
write when you build an app.

167
00:11:42,000 --> 00:11:51,000
So to give you an illustration of that with Dan's team, what we were presented with was

168
00:11:51,000 --> 00:11:57,480
a fully working solution that they had prototyped that had scaled to about an order of 28 to 30

169
00:11:57,480 --> 00:12:01,360
units, in this case valves, right?

170
00:12:01,360 --> 00:12:07,920
All drawn on a historical data set with some data feeds from one location.

171
00:12:07,920 --> 00:12:16,120
What we did with it as part of a very intense month in the month of May last year was replicate

172
00:12:16,120 --> 00:12:28,240
that historic data set 2,000 times, thus creating something like 400,000 unique valves that

173
00:12:28,240 --> 00:12:32,720
we had to deal with, not 20, but 400,000.

174
00:12:32,720 --> 00:12:38,880
And then train automatically in the platform a unique machine learning model for each valve,

175
00:12:38,880 --> 00:12:44,800
discovering along the way the features that affected the performance of that valve under

176
00:12:44,800 --> 00:12:50,160
the normal operating conditions, as well as under anomalous operating conditions.

177
00:12:50,160 --> 00:12:55,600
The work I did to do that was entirely baked into the platform, really what I wrote was

178
00:12:55,600 --> 00:13:01,360
a parallelization script that went and trained 400,000 models, right?

179
00:13:01,360 --> 00:13:08,400
Turns out in doing so I spun up something like 150 workers, elastically in the cloud, used

180
00:13:08,400 --> 00:13:14,200
it to train these models, persisted those model weights back into the platform as objects

181
00:13:14,200 --> 00:13:18,920
with metadata, callable as an API, and callable on demand.

182
00:13:18,920 --> 00:13:26,000
And having seen that, I think the question naturally became why not use this now to scale,

183
00:13:26,000 --> 00:13:27,000
right?

184
00:13:27,000 --> 00:13:29,680
And that's kind of where we ended the proof of technology and said, okay, now let's put

185
00:13:29,680 --> 00:13:32,800
this into a production type application.

186
00:13:32,800 --> 00:13:38,160
Where we are in that journey is like Dan described a little earlier in our, in his keynote

187
00:13:38,160 --> 00:13:48,680
at C3 transform, we have fully scaled out from 20 valves to about 400 valves from one location.

188
00:13:48,680 --> 00:13:55,680
We have also built out the infrastructure to talk to three distinct data sources from

189
00:13:55,680 --> 00:14:02,600
Shell, along with what we call canonicals that talk to those systems seamlessly irrespective

190
00:14:02,600 --> 00:14:05,400
of what data is being sent to us.

191
00:14:05,400 --> 00:14:11,800
But similarly, I have trained not just the model itself, but I've written algorithms

192
00:14:11,800 --> 00:14:20,720
or programming logic that trains the model automatically for any one of those assets

193
00:14:20,720 --> 00:14:25,480
and persists both the model and all associated metadata with it and makes it available

194
00:14:25,480 --> 00:14:27,480
to the end user, right?

195
00:14:27,480 --> 00:14:31,880
When you start solving those problems, what happened naturally in the course of this

196
00:14:31,880 --> 00:14:35,800
is one project, the valves project got started earlier.

197
00:14:35,800 --> 00:14:40,400
They had more teething problems and they just kind of had to solve issues along the way.

198
00:14:40,400 --> 00:14:46,080
The second project, the compressor project started about order of two months after valves.

199
00:14:46,080 --> 00:14:51,680
And when they started, they realized from their teammate sitting kind of two desks down

200
00:14:51,680 --> 00:14:56,560
that that teammate had already solved the problem of ingesting all sensitive data.

201
00:14:56,560 --> 00:15:01,280
That teammate had already solved the problem of what a model looks like in Shell with associated

202
00:15:01,280 --> 00:15:03,440
metadata fields.

203
00:15:03,440 --> 00:15:09,520
Had already solved the problem of transforming data from the raw data feed into the variants

204
00:15:09,520 --> 00:15:14,760
that I need to use to support the asset hierarchy within Shell.

205
00:15:14,760 --> 00:15:15,760
Done problem.

206
00:15:15,760 --> 00:15:22,160
The other piece that got better in the second time we did it was the understanding of what

207
00:15:22,160 --> 00:15:24,000
the app needs to do, right?

208
00:15:24,000 --> 00:15:28,480
And this is to dance point around actually building cloud-based software applications, right?

209
00:15:28,480 --> 00:15:31,480
What's the specification of what problem we're solving?

210
00:15:31,480 --> 00:15:35,160
What should each screen look like?

211
00:15:35,160 --> 00:15:41,600
Having done that, invariably because everything in C3 is an object that we call a type, had

212
00:15:41,600 --> 00:15:47,000
digitized some parts of the UI and other elements in the middleware that actually transformed

213
00:15:47,000 --> 00:15:51,520
data and make it presentable, were instantly available downstream for other applications

214
00:15:51,520 --> 00:15:52,520
as well.

215
00:15:52,520 --> 00:15:54,760
So there is immense reusability.

216
00:15:54,760 --> 00:16:00,520
We solved data integration problems effectively once and then reuse it for all cases.

217
00:16:00,520 --> 00:16:06,320
And by the way, having done that, you then point your data scientists at actually solving

218
00:16:06,320 --> 00:16:12,200
the complex problems, which is how do I now detect a compressor anomaly every 10 minutes

219
00:16:12,200 --> 00:16:14,440
with very high degree of certainty?

220
00:16:14,440 --> 00:16:15,440
That's the hard problem.

221
00:16:15,440 --> 00:16:22,200
Not the problem of stitching data together and building data transforms and doing the plumbing.

222
00:16:22,200 --> 00:16:26,760
And to that point, as it relates to scaling things up, that's where I believe C3 really

223
00:16:26,760 --> 00:16:29,160
makes a difference.

224
00:16:29,160 --> 00:16:37,720
The other piece that I'm talking to Darbert is to also use C3 as a means of experimentation

225
00:16:37,720 --> 00:16:38,720
as well, right?

226
00:16:38,720 --> 00:16:46,240
So the idea is if a model is a type with associated metadata, a Jupyter notebook is also a type

227
00:16:46,240 --> 00:16:48,200
with associated metadata.

228
00:16:48,200 --> 00:16:51,560
And so we are about to launch essentially what we call the notebook service.

229
00:16:51,560 --> 00:16:56,200
So you spin up a notebook on demand, whether it's an R notebook or a Python notebook.

230
00:16:56,200 --> 00:17:01,720
It turns out once you do that, the platform, which doesn't care what it's asked to store

231
00:17:01,720 --> 00:17:07,600
as long as it can represent it somehow, can store notebooks, can store versions of notebooks,

232
00:17:07,600 --> 00:17:15,880
can store variants of notebooks, including ownership data, can allow identity-based access

233
00:17:15,880 --> 00:17:20,880
control into those notebooks so that you're seamlessly connected and can get going.

234
00:17:20,880 --> 00:17:27,360
With no requirement whatsoever to store data in C3 or use the C3 platform for anything

235
00:17:27,360 --> 00:17:32,320
more than that, turns out that when you're actually solving a problem that has some gold

236
00:17:32,320 --> 00:17:38,040
as you mind, it is then a trivial exercise to push it into C3 and then scale with it,

237
00:17:38,040 --> 00:17:39,040
right?

238
00:17:39,040 --> 00:17:40,640
And that's where we're going on that side of it.

239
00:17:40,640 --> 00:17:44,960
So the idea is, start with scaling, we got that.

240
00:17:44,960 --> 00:17:49,440
But then let's also extend to experimentation, not just for core machine learning, but also

241
00:17:49,440 --> 00:17:54,400
then deep learning frameworks, NLP, etc.

242
00:17:54,400 --> 00:17:58,520
And just building on that, it's quite appealing because just to quantify some of the scale

243
00:17:58,520 --> 00:18:04,120
we're talking about, if you look at just one of our assets, it's spitting off around

244
00:18:04,120 --> 00:18:06,920
100,000 measurements per minute.

245
00:18:06,920 --> 00:18:11,480
So if you think of that over five years, you're talking between 70 and 80 billion rows

246
00:18:11,480 --> 00:18:13,800
of data.

247
00:18:13,800 --> 00:18:17,680
And if you're a data scientist in that sort of domain, actually just getting hold of that

248
00:18:17,680 --> 00:18:22,320
data is problematic and being able to put it in a form where you can process it.

249
00:18:22,320 --> 00:18:25,680
If C3's already done that data munging for us and put that in a form with some of these

250
00:18:25,680 --> 00:18:30,800
production apps that we're already developing, and they can manage versioning on the experimentation,

251
00:18:30,800 --> 00:18:32,400
it's quite an appealing prospect.

252
00:18:32,400 --> 00:18:34,120
So I think there's a lot of merit to the platform.

253
00:18:34,120 --> 00:18:37,720
I think the challenge is obviously getting it to the point where all of this works together,

254
00:18:37,720 --> 00:18:39,400
but we like the vision.

255
00:18:39,400 --> 00:18:44,480
So I think one of the things that you mentioned that I'm curious about in the process of

256
00:18:44,480 --> 00:18:53,560
this Valve project, replicating the data, I forget how many times, 440,000 or 2,000 times.

257
00:18:53,560 --> 00:18:58,040
In some of the previous conversations I've had today, one of the themes was not needing

258
00:18:58,040 --> 00:18:59,280
to replicate the data.

259
00:18:59,280 --> 00:19:02,840
Can you just reconcile those ideas for me?

260
00:19:02,840 --> 00:19:03,840
Yeah, it's absolutely.

261
00:19:03,840 --> 00:19:07,520
I think the idea of not replicating the data, which you might have heard from some of

262
00:19:07,520 --> 00:19:14,400
the other conversations you had with C3 folks, was really around this idea of referring

263
00:19:14,400 --> 00:19:18,720
to the capability of the C3 platform to virtualize any data source.

264
00:19:18,720 --> 00:19:21,960
And so the idea is, if you've already stored some data in the cloud, or you've already

265
00:19:21,960 --> 00:19:27,720
built something where it sits in a data warehouse that you can access, you do not need to then

266
00:19:27,720 --> 00:19:33,440
copy that over and make a copy of that entire data set inside the C3 cluster.

267
00:19:33,440 --> 00:19:39,480
Because the C3 object model allows external virtual references to any system that it can

268
00:19:39,480 --> 00:19:45,480
access, right? So in that sense, there is no need to copy data again if you've already

269
00:19:45,480 --> 00:19:47,040
made it available.

270
00:19:47,040 --> 00:19:53,520
What we're talking about when I said we replicated data 2,000 times was more to just purely demonstrate

271
00:19:53,520 --> 00:19:55,360
scalability, right?

272
00:19:55,360 --> 00:19:58,480
So it's, I have one unit of data.

273
00:19:58,480 --> 00:20:04,360
We have not yet had done the work to extract all the data from all parts of Shell, but

274
00:20:04,360 --> 00:20:08,640
I know it's there and I know I need to work on all of that data set at some point.

275
00:20:08,640 --> 00:20:13,880
But because Dan wanted to see it work at that scale, we said, well, you don't have it.

276
00:20:13,880 --> 00:20:15,920
What's the best way I can mill it?

277
00:20:15,920 --> 00:20:20,840
And so I literally ran a job on inside of C3 to replicate that 2,000 times and port it.

278
00:20:20,840 --> 00:20:21,840
Got it.

279
00:20:21,840 --> 00:20:23,480
So that's all we did.

280
00:20:23,480 --> 00:20:27,000
One of the things I find so interesting about this conversation that ties to some work

281
00:20:27,000 --> 00:20:32,120
I'm doing around this platform Zbook is this idea of scale.

282
00:20:32,120 --> 00:20:38,600
And like we throw this word around, and it means more than one thing at least.

283
00:20:38,600 --> 00:20:45,600
One of these things is we've got 100,000 measurements for this one system per year, and

284
00:20:45,600 --> 00:20:49,960
if we have 10 of these systems, we multiply, right?

285
00:20:49,960 --> 00:20:59,920
But there's also a sense in which it means the ability to scale our ability to get models

286
00:20:59,920 --> 00:21:01,280
in a production, right?

287
00:21:01,280 --> 00:21:03,760
The ability to operationalize more quickly.

288
00:21:03,760 --> 00:21:07,920
Dan, I'm wondering, when you think about, do you have different words for that, or how

289
00:21:07,920 --> 00:21:09,840
do you think about that whole space?

290
00:21:09,840 --> 00:21:14,480
Well, I think my problem is a number of, it's scale at every level, right?

291
00:21:14,480 --> 00:21:16,120
So let me just talk about that.

292
00:21:16,120 --> 00:21:22,880
So if you talk about my business, so we have 43,000 retail sites.

293
00:21:22,880 --> 00:21:25,240
That's just one part of Shell.

294
00:21:25,240 --> 00:21:28,720
We're bigger than Starbucks, bigger than McDonald's, and the retail business, in terms of our

295
00:21:28,720 --> 00:21:30,800
global network.

296
00:21:30,800 --> 00:21:34,960
So I have a problem that I have lots of things across my business, and I do all of those

297
00:21:34,960 --> 00:21:35,960
things create data.

298
00:21:35,960 --> 00:21:40,360
So I have a data problem, if you will, that I've got vast scale in terms of the data that

299
00:21:40,360 --> 00:21:41,760
I need to deal with.

300
00:21:41,760 --> 00:21:42,760
That's one.

301
00:21:42,760 --> 00:21:46,120
But then if I move that up a level, I've then got a problem which is I need to aggregate

302
00:21:46,120 --> 00:21:47,520
that data at scale.

303
00:21:47,520 --> 00:21:51,960
So I've got to be able to bring that into an aggregate environment in the cloud, because

304
00:21:51,960 --> 00:21:53,680
a lot of these things aren't cloud native.

305
00:21:53,680 --> 00:21:55,320
They have legacy systems.

306
00:21:55,320 --> 00:21:57,680
We need to be able to bring all that together.

307
00:21:57,680 --> 00:22:01,920
Then I have a problem at scale in terms of machine learning, because if you think about

308
00:22:01,920 --> 00:22:06,800
the vows problem, it's not good enough to do 16 vows and solve the problem for one business

309
00:22:06,800 --> 00:22:09,080
unit in a very small area of the business.

310
00:22:09,080 --> 00:22:13,040
I need to be able to do this for half a million vows worldwide, and so I've got a machine

311
00:22:13,040 --> 00:22:16,920
learning at scale problem, which means I've got a model management problem, which is something

312
00:22:16,920 --> 00:22:19,080
we talked about a little bit the last time.

313
00:22:19,080 --> 00:22:22,320
And then I've got a problem with the fact that I've then got users at scale.

314
00:22:22,320 --> 00:22:28,080
So the people that want to need to consume that, I've got potentially tens, hundreds, thousands

315
00:22:28,080 --> 00:22:30,360
of users across the whole of Shell.

316
00:22:30,360 --> 00:22:35,360
We're 80 plus thousand person organization.

317
00:22:35,360 --> 00:22:41,560
We've also got huge numbers of contractors, partners, suppliers, all of them potentially

318
00:22:41,560 --> 00:22:45,440
need to get insight from some of the applications that I'm developing.

319
00:22:45,440 --> 00:22:51,160
And so at the end of the day, it's that scale at all of those levels that's so problematic.

320
00:22:51,160 --> 00:22:56,040
And you actually have to have something that can solve all of those things if you want

321
00:22:56,040 --> 00:22:58,120
to get anything into production.

322
00:22:58,120 --> 00:23:02,560
And so we talk a lot about the focus on scaling and replicating, so getting it to that scale

323
00:23:02,560 --> 00:23:09,560
and then replicating it to allow it to generate benefits because the problem with digital technology

324
00:23:09,560 --> 00:23:14,680
is it's very easy to solve the problem once, but it also tends to be very expensive.

325
00:23:14,680 --> 00:23:18,720
And the benefits in any business model in the digital space comes from that replication

326
00:23:18,720 --> 00:23:19,720
thrust.

327
00:23:19,720 --> 00:23:23,800
And so the key thing is we've got to be able to create a platform strategy that allows

328
00:23:23,800 --> 00:23:27,680
us to do that because otherwise we become a very expensive cost center.

329
00:23:27,680 --> 00:23:35,360
Your last comment reminds me a bit of the Nvidia founder and CEO, whenever he gets up on

330
00:23:35,360 --> 00:23:39,400
the stage and holds up a new GPU, he's like, this GPU costs a billion dollars.

331
00:23:39,400 --> 00:23:40,400
Exactly.

332
00:23:40,400 --> 00:23:41,400
Right?

333
00:23:41,400 --> 00:23:45,800
Because that first up front effort is so significant, but then as you're able to replicate

334
00:23:45,800 --> 00:23:47,840
it, the incremental cost goes down.

335
00:23:47,840 --> 00:23:48,840
Exactly.

336
00:23:48,840 --> 00:23:49,840
Right.

337
00:23:49,840 --> 00:23:54,400
You mentioned in your talk, I think a new use case that we hadn't talked about previously

338
00:23:54,400 --> 00:23:55,400
the reward engine.

339
00:23:55,400 --> 00:23:56,400
Yeah.

340
00:23:56,400 --> 00:23:58,120
Is that one new?

341
00:23:58,120 --> 00:24:00,400
Yeah, it is new.

342
00:24:00,400 --> 00:24:03,760
It's one that we've just gone live with pretty recently.

343
00:24:03,760 --> 00:24:06,000
It's now live in the UK market.

344
00:24:06,000 --> 00:24:11,240
What it's all about is we talk a lot in our retail business about treating customers as

345
00:24:11,240 --> 00:24:12,400
a guest.

346
00:24:12,400 --> 00:24:16,000
So when you come to a shell station, we want you to feel like you're the most special

347
00:24:16,000 --> 00:24:19,680
person in the world to us and we want you to have a great experience.

348
00:24:19,680 --> 00:24:23,960
And that means treating you really well with our service champions as we call them.

349
00:24:23,960 --> 00:24:28,040
And you actually arrive on the forecourt or in the store, but it also means digitally you

350
00:24:28,040 --> 00:24:30,800
need to have a fantastic customer experience.

351
00:24:30,800 --> 00:24:32,320
And so we did a lot of thinking about that.

352
00:24:32,320 --> 00:24:36,320
And we basically took the same principles that Amazon and Facebook apply and we said,

353
00:24:36,320 --> 00:24:40,320
how can we make the experience that you get whenever you turn up at a shell station

354
00:24:40,320 --> 00:24:41,640
extremely personal?

355
00:24:41,640 --> 00:24:45,800
So we know what you want from us and we want you to have that experience.

356
00:24:45,800 --> 00:24:50,240
We want you to have the sensitivity around how much data you share with us.

357
00:24:50,240 --> 00:24:53,520
But once you do share your data, we want to give you a great experience with great

358
00:24:53,520 --> 00:24:56,280
offers and great benefits of being part of shell.

359
00:24:56,280 --> 00:24:58,720
And so that's what the reward engine is all about.

360
00:24:58,720 --> 00:25:03,560
It's a capability that sits under the hood of our loyalty system and it effectively dictates

361
00:25:03,560 --> 00:25:07,560
what you would experience you get at the site when you turn up to shell.

362
00:25:07,560 --> 00:25:09,280
And we're really proud of it.

363
00:25:09,280 --> 00:25:13,360
It's just gone live in the UK, like as I said, we're planning to roll it out globally.

364
00:25:13,360 --> 00:25:17,800
Can you talk a little bit about it technically in terms of data sources, types of models,

365
00:25:17,800 --> 00:25:19,280
what the models are doing?

366
00:25:19,280 --> 00:25:23,560
To a lot of the data comes from a combination of things, it comes from transaction history,

367
00:25:23,560 --> 00:25:26,880
it comes from information we have about the store, where it is, what the traffic around

368
00:25:26,880 --> 00:25:28,400
that is, et cetera, et cetera.

369
00:25:28,400 --> 00:25:33,760
We also have a whole bunch of information around, for example, the loyalty scheme you've

370
00:25:33,760 --> 00:25:36,920
done with us previously, how many visits you've had, what sort of things you've bought

371
00:25:36,920 --> 00:25:40,480
on the loyalty scheme, how many points you've got, how many visits you've got, I should

372
00:25:40,480 --> 00:25:41,480
say.

373
00:25:41,480 --> 00:25:46,240
And so we take all of that information and we effectively, we take a Bayesian inference

374
00:25:46,240 --> 00:25:49,040
approach effectively and I'm horribly simplifying it.

375
00:25:49,040 --> 00:25:54,920
But what we do is we say a shell customer typically buys fuel and then we overlay a clustering

376
00:25:54,920 --> 00:26:00,480
approach and a series of rules as well to create a combination of things we want you to

377
00:26:00,480 --> 00:26:01,480
have.

378
00:26:01,480 --> 00:26:03,120
So we want you to get rewarded for frequency.

379
00:26:03,120 --> 00:26:06,000
So we apply visit logic over the top.

380
00:26:06,000 --> 00:26:11,400
But we also infer from history of customers who behave in a similar way to the way that

381
00:26:11,400 --> 00:26:12,400
you behave.

382
00:26:12,400 --> 00:26:14,920
So it's kind of like a camins, if you think about it that way.

383
00:26:14,920 --> 00:26:20,000
And so we're trying to cluster the behavior, infer your behavior, but ultimately try to

384
00:26:20,000 --> 00:26:23,840
then create offers that are relevant, but then continually iterate on that.

385
00:26:23,840 --> 00:26:25,160
So that's the inference aspect.

386
00:26:25,160 --> 00:26:31,160
So we constantly overlay behavioral observations in real time, back onto that customer and then

387
00:26:31,160 --> 00:26:33,280
use that to make the offers.

388
00:26:33,280 --> 00:26:38,800
One of the things I've learned over the past, over the course of the day in some of the

389
00:26:38,800 --> 00:26:44,440
sessions here at the conferences, kind of the evolution of C3, right, started it with

390
00:26:44,440 --> 00:26:48,840
the focus of this data layer that we've talked a bunch about.

391
00:26:48,840 --> 00:26:50,440
And actually you correct me if I'm wrong.

392
00:26:50,440 --> 00:26:54,680
I mean, the impression I have is that the company's kind of been pulled into machine learning

393
00:26:54,680 --> 00:27:01,200
in AI and working with companies like Shell and NL and others and is kind of building on

394
00:27:01,200 --> 00:27:03,520
that underlying data platform.

395
00:27:03,520 --> 00:27:09,280
Yeah, I think the question that I'm getting is like, there's so much, you know, a data

396
00:27:09,280 --> 00:27:11,920
platform is a huge challenge.

397
00:27:11,920 --> 00:27:16,360
And then we're talking about Bayesian inference and TensorFlow and machine learning models

398
00:27:16,360 --> 00:27:21,400
like the, I think one of the speakers earlier talked about the number, it was maybe Tom talked

399
00:27:21,400 --> 00:27:24,840
about the number of connections between all these things.

400
00:27:24,840 --> 00:27:27,040
How do you manage that?

401
00:27:27,040 --> 00:27:32,280
And I'm presumably part of the goal is to keep that simple for the people that are using

402
00:27:32,280 --> 00:27:33,280
the platform.

403
00:27:33,280 --> 00:27:39,000
I imagine it then wants to keep things, you know, the platform view simple to his consumers.

404
00:27:39,000 --> 00:27:42,000
So let me answer that question two ways, Sam.

405
00:27:42,000 --> 00:27:46,520
The first is it would be helpful to your listeners to understand how C3 got to where we are,

406
00:27:46,520 --> 00:27:47,520
right?

407
00:27:47,520 --> 00:27:52,200
And a little bit of just our evolution and and why it is the way we do certain things

408
00:27:52,200 --> 00:27:53,520
in certain way.

409
00:27:53,520 --> 00:27:58,320
The second is to give you a sense of how we think about platform users and frankly the answer

410
00:27:58,320 --> 00:28:02,400
is there are many platform users, they're all very different and you've got to expose

411
00:28:02,400 --> 00:28:06,280
different parts of the platform to them, but not everything to everyone.

412
00:28:06,280 --> 00:28:10,680
And in doing so, the right way is how we manage down the complexity.

413
00:28:10,680 --> 00:28:12,200
So let me answer the first thing first, right?

414
00:28:12,200 --> 00:28:16,280
So so C3 got started as a carbon trading company, okay?

415
00:28:16,280 --> 00:28:22,400
And the original C was represented carbon, right?

416
00:28:22,400 --> 00:28:27,520
And it came out around, I mean, the conversation at the time, and this is going back to 2009,

417
00:28:27,520 --> 00:28:33,840
2010, was around this thesis that because of the carbon trading act, everyone will need

418
00:28:33,840 --> 00:28:36,600
to measure their carbon footprint.

419
00:28:36,600 --> 00:28:40,040
And then because some have better and some have worse carbon footprint relative to each

420
00:28:40,040 --> 00:28:46,000
other, that is the opportunity to both measure it, monitor it, and then mitigate it by trading.

421
00:28:46,000 --> 00:28:47,000
Right?

422
00:28:47,000 --> 00:28:54,040
So there's therefore a natural need for companies like Cisco and Dell and GE to actually

423
00:28:54,040 --> 00:28:57,200
have software that measured their carbon footprint.

424
00:28:57,200 --> 00:29:04,160
And you would need to therefore measure facility use, person use, travel use, all of these

425
00:29:04,160 --> 00:29:05,880
different things.

426
00:29:05,880 --> 00:29:11,840
And so the company that is now C3, was also called C3 at the time, but the C stood for

427
00:29:11,840 --> 00:29:13,120
carbon.

428
00:29:13,120 --> 00:29:19,480
And we got going in the business of really being able to ingest a huge amount of data

429
00:29:19,480 --> 00:29:23,080
all related to measuring a company's carbon footprint, where did this data come from?

430
00:29:23,080 --> 00:29:25,600
It came from facility plans.

431
00:29:25,600 --> 00:29:28,160
It came from energy utility bills.

432
00:29:28,160 --> 00:29:30,720
It came from energy utility meters.

433
00:29:30,720 --> 00:29:37,240
It came from self-reported lead certification of buildings, things like that.

434
00:29:37,240 --> 00:29:45,920
Turns out we built that company or that piece of software in a way that said, and this

435
00:29:45,920 --> 00:29:52,360
was I think ahead of its time, that recognized that if we went to 10 enterprises, each of

436
00:29:52,360 --> 00:29:57,120
those 10 enterprises would have a different data model, different sources of data, but

437
00:29:57,120 --> 00:29:59,400
they were all trying to solve the same problem.

438
00:29:59,400 --> 00:30:06,640
And so we built that first piece of software to essentially allow a modular separation between

439
00:30:06,640 --> 00:30:12,040
the data we were getting and the use of that data going up.

440
00:30:12,040 --> 00:30:17,080
And because a lot of our hunters came from civil systems, that was the core of how it

441
00:30:17,080 --> 00:30:18,080
was built.

442
00:30:18,080 --> 00:30:21,160
And so a lot of that DNA kind of got transferred over.

443
00:30:21,160 --> 00:30:25,240
Turns out that market went to bust, carbon trading sort of evaporated.

444
00:30:25,240 --> 00:30:28,760
And we said, okay, we built this technology, what do we do with it?

445
00:30:28,760 --> 00:30:35,880
What we did with it was point it at a problem where there was a huge amount of data available

446
00:30:35,880 --> 00:30:41,320
that was the same carbon trading problem time, 10 or times 100, where we believed we could

447
00:30:41,320 --> 00:30:44,600
engineer the scalability required to solve the problem.

448
00:30:44,600 --> 00:30:51,720
And that was really building software solutions for the electric grid or the utilities, right?

449
00:30:51,720 --> 00:31:00,680
Which was at the time, and this is 2011, 2012, the most censored industry of them all, right?

450
00:31:00,680 --> 00:31:02,760
Sensored in the sense of having a lot of censors.

451
00:31:02,760 --> 00:31:07,320
Having a lot of censors as opposed to anything else, right?

452
00:31:07,320 --> 00:31:13,520
Turns out that these guys have been actually pretty good at installing censors, deploying

453
00:31:13,520 --> 00:31:19,960
censors, not just for smart meters, but also transformers, vibration sensors on grid

454
00:31:19,960 --> 00:31:23,840
equipment, re-closers, the works.

455
00:31:23,840 --> 00:31:29,280
And now one of our largest customers whom you may have heard on stage today was our customer

456
00:31:29,280 --> 00:31:33,200
when we were that company called C3 Energy.

457
00:31:33,200 --> 00:31:37,760
And the first problem was, can you deal with all this data, turns out we could.

458
00:31:37,760 --> 00:31:41,280
Because again, because we had modularized and abstracted the way we built the original

459
00:31:41,280 --> 00:31:47,200
piece of software, adding cloud services, elastic scalability, et cetera, came naturally,

460
00:31:47,200 --> 00:31:49,800
while retaining that abstraction.

461
00:31:49,800 --> 00:31:53,640
This is kind of the genesis of what we call the type system.

462
00:31:53,640 --> 00:31:59,320
And so the first problem to us was, can you handle 800 terabytes of data that I sent

463
00:31:59,320 --> 00:32:03,520
to you and then process it at a million transactions per second?

464
00:32:03,520 --> 00:32:06,000
No AI, no machine learning, that was it, right?

465
00:32:06,000 --> 00:32:07,400
Turns out we could.

466
00:32:07,400 --> 00:32:12,360
And turns out we proved it to them in order of a few cycles, just like dance, one million

467
00:32:12,360 --> 00:32:14,680
model problem.

468
00:32:14,680 --> 00:32:19,480
Having sought that, they said, okay, what do we do with this technology?

469
00:32:19,480 --> 00:32:23,640
And it turns out the most valuable problems to them, again, going back to like, where's

470
00:32:23,640 --> 00:32:28,760
the money, were in the application of these very large data sets, but also machine learning

471
00:32:28,760 --> 00:32:30,480
algorithms on that.

472
00:32:30,480 --> 00:32:36,440
And so we built in the same modular approach, the ability to build both temporal and spatial

473
00:32:36,440 --> 00:32:41,480
analytics, and then machine learning models using those temporal and spatial analytics,

474
00:32:41,480 --> 00:32:43,960
again, in this abstracted way.

475
00:32:43,960 --> 00:32:47,760
So that everyone's writing as little code as possible, but you're expressing logic very

476
00:32:47,760 --> 00:32:50,280
quickly.

477
00:32:50,280 --> 00:32:56,560
That's been now hardened to the point where it works seamlessly on all flavors and

478
00:32:56,560 --> 00:33:04,360
all types of data over the next, over the subsequent five issues, right?

479
00:33:04,360 --> 00:33:10,920
And in doing so, we've gone from offering SaaS solutions to the utilities to a platform

480
00:33:10,920 --> 00:33:15,040
that has these capabilities to all industries.

481
00:33:15,040 --> 00:33:22,320
And in doing so, we've now focused on building out these capabilities that look like better

482
00:33:22,320 --> 00:33:29,120
cloud services, better performance, better ML and AI and application development tooling,

483
00:33:29,120 --> 00:33:32,120
and more applications, all using the same logic.

484
00:33:32,120 --> 00:33:38,720
So you could call it chance, you could call it a natural necessity of the types of problems

485
00:33:38,720 --> 00:33:41,760
we solved, and therefore we're exposed to next.

486
00:33:41,760 --> 00:33:49,040
So as I like to joke, the price for the pie eating contest is more pie, and we just kept

487
00:33:49,040 --> 00:33:52,520
winning them every few weeks.

488
00:33:52,520 --> 00:33:53,960
And that's been our evaluation, right?

489
00:33:53,960 --> 00:33:59,800
A lot of the initial impetus came from customer conversations, right?

490
00:33:59,800 --> 00:34:05,560
It's like, hey, what problems do you need that are real actually need solving?

491
00:34:05,560 --> 00:34:10,040
A lot of it came from our roadmap, and we see this constant conversation going on, and

492
00:34:10,040 --> 00:34:16,120
it happened today, and it will happen tomorrow in the rest of our conference and so on.

493
00:34:16,120 --> 00:34:17,520
And that's how we think about this, right?

494
00:34:17,520 --> 00:34:22,840
So there's, and if you listen to Tom talking about the types of people we hire, we hire

495
00:34:22,840 --> 00:34:27,320
people that are restless, that are not willing to say the jobs done were done, right?

496
00:34:27,320 --> 00:34:31,160
There's always something new to solve, and it's constantly evolving.

497
00:34:31,160 --> 00:34:35,520
I suspect, if you look at our road, if you fast forward a year and look back at what

498
00:34:35,520 --> 00:34:40,160
we built, about half of the things we built would have come from our own product managers

499
00:34:40,160 --> 00:34:42,280
thinking through like, what do we need?

500
00:34:42,280 --> 00:34:46,400
And the remaining half would come from customer demands, and it's like, hey, I need this to solve

501
00:34:46,400 --> 00:34:47,640
this problem.

502
00:34:47,640 --> 00:34:48,640
You haven't solved it yet.

503
00:34:48,640 --> 00:34:49,640
Well, fine.

504
00:34:49,640 --> 00:34:50,640
We'll go solve.

505
00:34:50,640 --> 00:34:51,640
Right?

506
00:34:51,640 --> 00:34:52,640
And that's really how this will evolve.

507
00:34:52,640 --> 00:34:53,640
As we go.

508
00:34:53,640 --> 00:34:56,760
It's managing the complexity of all that.

509
00:34:56,760 --> 00:34:57,760
So now let's go to that.

510
00:34:57,760 --> 00:34:58,760
Right?

511
00:34:58,760 --> 00:35:05,280
So as we grow our capabilities in the platform, I think about how we organize ourselves, right?

512
00:35:05,280 --> 00:35:08,120
Which gives you a window as to how we think about the world.

513
00:35:08,120 --> 00:35:12,600
We have a core platform engineering team and the core platform engineering team deals with

514
00:35:12,600 --> 00:35:19,720
everything related to infrastructure services, whether they are cloud or on premise.

515
00:35:19,720 --> 00:35:24,600
They do everything related to security and authentication.

516
00:35:24,600 --> 00:35:31,320
Is it there's a pod that deals with data management and data source externalization?

517
00:35:31,320 --> 00:35:37,320
So how do I write connectors to MongoDB or Snowflake or whatever else?

518
00:35:37,320 --> 00:35:42,000
And there is a processing analytics engine team within the platform team that deals with

519
00:35:42,000 --> 00:35:47,880
what's the most efficient way to store a time series across archival storage and SSD

520
00:35:47,880 --> 00:35:51,680
storage so that I can actually access data in the fastest way possible.

521
00:35:51,680 --> 00:35:55,800
And how do I parametrize that so that it's easy to solve?

522
00:35:55,800 --> 00:36:00,240
All of these pods in core platform engineering are building with the same mindset, which

523
00:36:00,240 --> 00:36:07,600
is how do I abstract the services I provide to the layer above me from the raw code I

524
00:36:07,600 --> 00:36:09,880
write to actually manage the layer below me?

525
00:36:09,880 --> 00:36:10,880
Right?

526
00:36:10,880 --> 00:36:11,880
Everyone's doing the same thing.

527
00:36:11,880 --> 00:36:15,320
So at that level, you've got those pods.

528
00:36:15,320 --> 00:36:21,480
If you solve those correctly, which is kind of what they do, we then offer up to the next

529
00:36:21,480 --> 00:36:28,320
team within C3, which looks like application development, tool development and machine

530
00:36:28,320 --> 00:36:33,360
learning engineering, a series of native services that they can then use to build their own

531
00:36:33,360 --> 00:36:34,360
services.

532
00:36:34,360 --> 00:36:35,360
What are they doing?

533
00:36:35,360 --> 00:36:39,680
Machine learning engineering is literally building what we call an ML pipeline, right?

534
00:36:39,680 --> 00:36:46,520
An ML pipeline is essentially a sequence of steps that's executable in a runtime that

535
00:36:46,520 --> 00:36:52,920
is declared on compile or previously that can use any framework that you want to pull

536
00:36:52,920 --> 00:36:59,360
in, whether it's TensorFlow or Keras or anything else, along with any data processing and outputs

537
00:36:59,360 --> 00:37:01,560
that you need to do with it, right?

538
00:37:01,560 --> 00:37:07,080
And what they're doing is really writing bindings that can use a series of these frameworks

539
00:37:07,080 --> 00:37:11,560
and a series of external tools that can work with the platform.

540
00:37:11,560 --> 00:37:16,680
Application engineering is building applications, inventory optimization, predictive maintenance,

541
00:37:16,680 --> 00:37:22,160
et cetera, with features, functionality directed by the product managers, invariably

542
00:37:22,160 --> 00:37:24,800
in conversation with our customers.

543
00:37:24,800 --> 00:37:32,000
And the tools team is building via metadata APIs that the platform team exposes, the tooling

544
00:37:32,000 --> 00:37:39,840
to then change platform settings through very minimal, either no code or no code tools

545
00:37:39,840 --> 00:37:42,880
that they make available through browsers, right?

546
00:37:42,880 --> 00:37:48,920
And then we have customer service and sales engineering teams that use all of these capabilities

547
00:37:48,920 --> 00:37:53,480
to then go solve customer problems with a rich toolkit that sort of hangs on their

548
00:37:53,480 --> 00:37:57,640
ways like Batman's utility belt and they're pulling out what they need, right?

549
00:37:57,640 --> 00:38:02,840
But very rarely are they solving problems that go down to say, I need a new database

550
00:38:02,840 --> 00:38:04,200
connector.

551
00:38:04,200 --> 00:38:07,840
If that's the case, the platform team goes solves and it makes available a service above

552
00:38:07,840 --> 00:38:10,800
that they use and so on.

553
00:38:10,800 --> 00:38:17,960
Everything in C3 is a type and therefore the database connector to Impala is a type,

554
00:38:17,960 --> 00:38:18,960
right?

555
00:38:18,960 --> 00:38:22,880
The machine learning pipeline that uses Tesseract is a type.

556
00:38:22,880 --> 00:38:29,380
The application widget on the inventory optimization screen that shows the latest deviation from

557
00:38:29,380 --> 00:38:32,240
safety stock is a type, right?

558
00:38:32,240 --> 00:38:40,480
And the data integration tool logic that finds the best match for any ingested data terms

559
00:38:40,480 --> 00:38:45,920
of its target source in terms of its target field is a type or a functionality, right?

560
00:38:45,920 --> 00:38:50,680
And so everything's abstracted always and you're really dealing with internal APIs that

561
00:38:50,680 --> 00:38:53,000
work seamlessly across the platform.

562
00:38:53,000 --> 00:38:54,680
That's how we manage down the complexity.

563
00:38:54,680 --> 00:39:00,640
Now if you're a customer, right, then we are, then the question to ask is, what are you

564
00:39:00,640 --> 00:39:02,400
in that customer?

565
00:39:02,400 --> 00:39:07,080
If you're an end user of a customer, you don't care about the C3 platform at all.

566
00:39:07,080 --> 00:39:11,080
All you care about is what's the URL you forget to and what's the big red shiny button

567
00:39:11,080 --> 00:39:12,080
I need to press, right?

568
00:39:12,080 --> 00:39:13,440
That's all you care about.

569
00:39:13,440 --> 00:39:18,040
If you're an application developer in a customer, at a customer site, like a lot of Dan's team

570
00:39:18,040 --> 00:39:20,440
that are building applications with us.

571
00:39:20,440 --> 00:39:26,440
You care about the data ingestion APIs, the machine learning engineering APIs, maybe

572
00:39:26,440 --> 00:39:28,640
the application logic APIs.

573
00:39:28,640 --> 00:39:32,320
You don't care about the platform fundamentals at all because that's given to you.

574
00:39:32,320 --> 00:39:36,560
If you're a data scientist on the safety platform, you care about what pipelines the machine

575
00:39:36,560 --> 00:39:42,480
learning engine team has made available and how I can call the time series engine or the

576
00:39:42,480 --> 00:39:47,040
data wrangling engine in Jupyter Notebook to do what I need to do.

577
00:39:47,040 --> 00:39:48,520
That's really all you care about.

578
00:39:48,520 --> 00:39:53,360
So depending on who you are, it actually changes what we expose and that then also manages

579
00:39:53,360 --> 00:39:55,280
down the complexity.

580
00:39:55,280 --> 00:40:01,520
Frankly, there are about three people in the company that know it all and let's just

581
00:40:01,520 --> 00:40:06,280
say that they are incredibly valuable.

582
00:40:06,280 --> 00:40:12,840
Before we started rolling, I was mentioning that one of the first articles I wrote seven

583
00:40:12,840 --> 00:40:18,160
years ago, seven plus years ago when I started my company was about machine learning platforms.

584
00:40:18,160 --> 00:40:24,200
And at the time, I heard a lot of feedback that says that we're never going to be able

585
00:40:24,200 --> 00:40:26,160
to generalize machine learning.

586
00:40:26,160 --> 00:40:29,760
Like we're going to have to, you know, it's always going to be a problem that's solved

587
00:40:29,760 --> 00:40:34,960
on a snowflake by snowflake basis on a problem by problem basis.

588
00:40:34,960 --> 00:40:38,880
Dan, you're building platforms, what's your take on that?

589
00:40:38,880 --> 00:40:43,160
What's your experience on that and how close are we to, you know, can we call it an

590
00:40:43,160 --> 00:40:48,360
Irvana of having a generalized platform that we can apply to a wide variety of business

591
00:40:48,360 --> 00:40:50,360
problems?

592
00:40:50,360 --> 00:40:53,400
I think we're getting closer and closer to that.

593
00:40:53,400 --> 00:40:55,800
I think I would, so I'd answer that in a couple of ways.

594
00:40:55,800 --> 00:41:01,560
I think the first is that we still have a view that you still solve machine learning problems

595
00:41:01,560 --> 00:41:02,560
case by case.

596
00:41:02,560 --> 00:41:04,440
So we start with the hypothesis.

597
00:41:04,440 --> 00:41:08,960
What is the core question you're trying to answer and how do you prove that with data?

598
00:41:08,960 --> 00:41:14,600
It's still fundamental to, you know, the scientific method, if you will, that we employ in the

599
00:41:14,600 --> 00:41:19,080
way we go about solving any problem we get from our business users.

600
00:41:19,080 --> 00:41:23,360
What I think is increasing the interest thing is that we are starting in the same way as

601
00:41:23,360 --> 00:41:27,080
the software industry figured out that even when you're solving a problem, you can create

602
00:41:27,080 --> 00:41:28,080
your reusability.

603
00:41:28,080 --> 00:41:31,840
We're seeing that same trend emerging in machine learning.

604
00:41:31,840 --> 00:41:38,520
So you can have generic pipelines which are mostly the same user-consistent set of frameworks

605
00:41:38,520 --> 00:41:44,920
and then ultimately can be deployed to solve different problems within a given space.

606
00:41:44,920 --> 00:41:47,760
So the example I always use is natural language processing.

607
00:41:47,760 --> 00:41:53,120
So I talked about Tesseract, right, Space C is becoming commonplace in that domain.

608
00:41:53,120 --> 00:41:57,080
At the end of the day, what you can do in most cases is stitch together a series of building

609
00:41:57,080 --> 00:42:02,000
blocks with Python, NLTK and those sorts of things and ultimately get to an end product

610
00:42:02,000 --> 00:42:05,760
where you can use that same pipeline to answer multiple questions.

611
00:42:05,760 --> 00:42:10,680
Now, you still need a data scientist that knows how to tweak the knobs if you want to

612
00:42:10,680 --> 00:42:11,680
put it that way.

613
00:42:11,680 --> 00:42:17,120
But at the same time, you can reduce the time to value so you're not rebuilding everything

614
00:42:17,120 --> 00:42:18,640
every time.

615
00:42:18,640 --> 00:42:23,160
And that's really what the theme across my team, we're really looking to say, when I go

616
00:42:23,160 --> 00:42:26,520
to solve that problem, can I take a reusable asset that someone else has built somewhere

617
00:42:26,520 --> 00:42:33,240
else and it reduced the time to value, whether that be at the data level or at the framework

618
00:42:33,240 --> 00:42:37,680
level to allow you to solve the business problem as fast as possible.

619
00:42:37,680 --> 00:42:42,680
Now I think the next generation of that is making that accessible so the end users can

620
00:42:42,680 --> 00:42:44,400
tweak those parameters.

621
00:42:44,400 --> 00:42:48,800
And that's the AutoML development, it's the work in the self-service and actually some

622
00:42:48,800 --> 00:42:52,960
of the things that C3 are trying to do to expose some of their core platform functionality.

623
00:42:52,960 --> 00:42:56,240
And you see that right across the industry right now.

624
00:42:56,240 --> 00:42:59,120
I think the other thing that's going to come and hit us quite hard, though, which is

625
00:42:59,120 --> 00:43:04,760
emerging is how do you infuse data security into all of that?

626
00:43:04,760 --> 00:43:09,680
Because I think there's been an explosion of ideas in this space and somehow we need to

627
00:43:09,680 --> 00:43:13,640
make sure that we do that responsibly and there's a lot of thinking going into that, particularly

628
00:43:13,640 --> 00:43:17,920
within Shell right now, to make sure that we've got the platform right underpin all of

629
00:43:17,920 --> 00:43:22,760
that and that those reusable assets have also security baked in.

630
00:43:22,760 --> 00:43:28,360
So I see the whole industry growing up, I see it becoming much more like the software

631
00:43:28,360 --> 00:43:29,360
industry.

632
00:43:29,360 --> 00:43:30,360
In general.

633
00:43:30,360 --> 00:43:32,160
It's a great analogy.

634
00:43:32,160 --> 00:43:37,440
We started this interview talking about the strategy you've made in four months.

635
00:43:37,440 --> 00:43:39,720
You want to predict us four months out?

636
00:43:39,720 --> 00:43:41,640
That's a great question.

637
00:43:41,640 --> 00:43:43,680
So there's a few things on our roadmap.

638
00:43:43,680 --> 00:43:45,360
I'll talk about some of those.

639
00:43:45,360 --> 00:43:50,280
I think for us, optimization is the next big horizon.

640
00:43:50,280 --> 00:43:55,480
So how do we start to use a combination of traditional optimization techniques, stuff

641
00:43:55,480 --> 00:44:00,880
like Cplex, for example, and traditional solvers, in conjunction with new optimization techniques

642
00:44:00,880 --> 00:44:05,280
like deep reinforcement learning, to start to solve some of the toughest optimization problems

643
00:44:05,280 --> 00:44:06,800
across the energy industry?

644
00:44:06,800 --> 00:44:09,000
That's a really exciting space for us.

645
00:44:09,000 --> 00:44:13,200
I think the other thing about that space is that's when you start ironically getting back

646
00:44:13,200 --> 00:44:18,520
to C3's original mission, which is trying to reduce carbon, because in that whole space,

647
00:44:18,520 --> 00:44:23,440
if you can really optimize, you can start to look at trade-offs between your greenhouse

648
00:44:23,440 --> 00:44:27,640
gas emissions and your production, as well as trying to look at places where you need

649
00:44:27,640 --> 00:44:29,320
to tighten up.

650
00:44:29,320 --> 00:44:32,680
So I think that's going to be a huge thing for us.

651
00:44:32,680 --> 00:44:39,400
I think the other thing that's super interesting is how do you start to look at an emerging

652
00:44:39,400 --> 00:44:47,960
class of energy customer who are typically very green in their outlook, tech savvy, and

653
00:44:47,960 --> 00:44:49,360
wants everything digitally?

654
00:44:49,360 --> 00:44:52,280
So how do we meet that need as an organization?

655
00:44:52,280 --> 00:44:56,120
And how do we make that, make it really easy to interact with Shell, and how do we give

656
00:44:56,120 --> 00:45:02,200
you a range of offerings that allow you to solve your energy needs, and do that taking full

657
00:45:02,200 --> 00:45:03,880
advantage of AI?

658
00:45:03,880 --> 00:45:06,000
And that's the vision that I'm super excited about.

659
00:45:06,000 --> 00:45:08,360
That's where we're really trying to push the envelope next.

660
00:45:08,360 --> 00:45:13,720
Well, Dan, Eddie, thanks so much for taking the time to chat with me, great conversation.

661
00:45:13,720 --> 00:45:14,720
Thank you.

662
00:45:14,720 --> 00:45:15,720
Thank you.

663
00:45:15,720 --> 00:45:21,920
All right, everyone, that's our show for today.

664
00:45:21,920 --> 00:45:26,160
For more information about today's show, visit twimmelai.com.

665
00:45:26,160 --> 00:45:32,480
Be sure to visit twimmelcon.com for information or to register for Twimmelcon AI platforms.

666
00:45:32,480 --> 00:45:36,280
Thanks again to C3 for their sponsorship of today's episode.

667
00:45:36,280 --> 00:45:39,960
To check out what they're up to, visit c3.ai.

668
00:45:39,960 --> 00:45:46,120
As always, thanks so much for listening and catch you next time.

