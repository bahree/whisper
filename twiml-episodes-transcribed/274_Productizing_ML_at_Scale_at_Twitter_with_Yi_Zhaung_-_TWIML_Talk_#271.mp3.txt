Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.
And last week we kicked off the second volume of our listener favorite AI platform series,
sharing more stories of teams working to scale and industrialize data science and machine
learning at their companies.
We've been teasing that there's more to come and today I am super excited to announce
the launch of our inaugural conference, Twimblecon AI platforms.
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices
necessary to scale the delivery of machine learning and AI in the enterprise.
Now you know Twimble for bringing you dynamic practical conversations via the podcast and
we're creating our Twimblecon events to build on that tradition.
The event will feature two full days of community oriented discussions, live podcast interviews
and practical presentations by great presenters sharing concrete examples from their own experiences.
By creating a space where data science, machine learning, platform engineering and ML ops
practitioners and leaders can share, learn and connect, the event aspires to help see
the development of an informed and sustainable community of technologists that is well equipped
to meet the current and future needs of their organizations.
Some of the topics that we plan to cover include overcoming the barriers to getting machine
learning and deep learning models into production, how to apply ML ops and DevOps to your machine
learning workflow, experiences and lessons learned in delivering platform and infrastructure
support for data management, experiment management and model deployment.
The latest approaches, platforms and tools for accelerating and scaling the delivery of
ML and DL and the enterprise, platform deployment stories from leading companies like Google,
Facebook, Airbnb, as well as traditional enterprises like Comcast and Shell and organizational
and cultural best practices for success.
The two day event will be held on October 1st and 2nd in San Francisco and I would really
love to meet you there.
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first 10 listeners
who register the amazing opportunity to get their ticket for 75% off using the discount
code TwimbleFirst.
Again, the conference site is Twimblecon.com and the code is TwimbleFirst.
I am really grateful to our friends over at Sigopt who stepped up to support this project
in a big way.
In addition to supporting our AI Platforms podcast series and next ebook, they've made
a huge commitment to this community by signing on as the first founding sponsor for the event.
App Software is used by enterprise teams to standardize and scale machine learning experimentation
and optimization across any combination of modeling frameworks, libraries, computing
infrastructure and environment.
Teams like Two Sigma, who will hear from later in this podcast series, rely on Sigopt Software
to realize better modeling results much faster than previously possible.
Of course, to fully grasp its potential, it's best to try it yourself and this is why
Sigopt is offering you an exclusive opportunity to try their product on some of your toughest
modeling problems for free.
To learn about and take advantage of this offer, visit Twimbleai.com slash Sigopt.
And now on to the show.
All right, everyone.
I am on the line with Yizhuang, E is a senior staff engineer at Twitter and tech lead
for the machine learning core environment on the core techs team.
E, welcome to this week a machine learning and AI.
Thank you.
My pleasure to be here.
It's great to chat with you.
I'm really looking forward to digging into what you are working on on the platform side
of things.
Before we do that, I'd love to start out with a little bit of your background and how
you started working in machine learning platforms and infrastructure.
Sure, actually, I would say there are two parts to this question.
There's how I, I guess, how I started working on machine learning and also how I started
working on platform and infra.
So I can dive into both.
Does that sound good?
Absolutely.
Yeah, let's do it.
Okay.
So I think ever since I was a kid, I always had an affinity to both math and computer
science, so that's why I got attracted to discipline like machine learning where I get
to practice both.
So I would say the first time I touched machine learning was actually in college.
I was on my college's robotics team and we programmed these robot dogs to play soccer.
Those are Sony eyeballs and the competition was called Robocop and we participate in
this competition and program robot dogs to play soccer against each other.
Unlike what you would have guessed today, machine learning actually wasn't used in, wasn't
used very much in our vision system.
It was actually used for a different case.
We used machine learning to tune the gates of the robots by gate.
I mean, the walking posture, there is a set of parameters that needs to be optimized
and the search space is pretty large.
So what we did was we essentially made the robot dogs walk back and forth on the playing
field and recorded the running speed and then we tune each parameter by a little bit and
then make them walk the playing field again.
Then we can compute the gradient of the walking speed with respect to that parameter we
nudged by a little bit and we do that to all the parameters and then we perform one round
of gradient descent that allowed us to optimize the running speed of our robot dogs.
It's a very tedious process actually.
That sounds super tedious.
Yeah, exactly.
I remember these Sony Ibo dogs from many years ago, I didn't realize they were quite
that programmable.
They were programmable, but yeah, the experience was pretty, it was not as good as today.
Essentially, for example, if we get a segmentation fault, the whole robot turns off, basically
the operating system shuts down.
But one thing that was pretty amazing was our college's robotics team.
We started as the last place in the U.S. open competition in around 2015.
The same team actually won World Championship in 2007 and 2007.
Wow.
We actually beat very reputable teams like Carnegie Mellon 8-1 in terms of score.
I would say it was the definitely hard working team and the techniques we were using, using
machine learning to tune the gate of the robots, they worked out well.
Wow.
And where did you go to school?
I actually went to Carnegie Mellon right after.
I think for basically beating their robotics team, definitely helped me get that offer
from both Carnegie Mellon's robotics institute and computer science department.
What I ended up choosing, though, was I was both a math major and a CS major in college.
I ended up choosing a very mathematical field of computer science at Carnegie Mellon where
I model system performance by building mathematical models, essentially building a mark of models
and analyze queuing, et cetera, to basically predict the performance of computer systems.
That's what I do in grad school.
And I got into platform around, like I would say, system engineering around roughly two
years into my PhD program at Carnegie Mellon, I interned at Google.
So I worked on the basically performance modeling for the next generation storage system
at Google.
And I offered to, for example, build math models predict performance there.
And they were less excited.
They were more, yeah, they were like, we don't want you to build math models.
We want you to actually measure and tune the parameters.
So this is, again, very similar to the robotics case, right?
It's, essentially, we have like a black box storage system, but we have many knobs we can
tune.
Essentially, we're looking for the best set of parameters that can actually perform best.
So that was my Google internship and afterwards.
I started realizing that my PhD program, I do a lot of math, but I don't do a lot of
hands-on engineering.
And I actually found myself to like building stuff.
And that's when I dropped out of my PhD program and came to Twitter.
My first projects were actually on Twitter search.
And this is where I learned to be an actual engineer and learned about system and platform
engineering.
And I took like around 2014 or so.
I led a group of people and built this trillion documents scale search engine at Twitter.
This allowed Twitter to index every single tweet ever published.
And now you can search for Jack's first tweet setting up my Twitter and that tweet would
show up.
Oh, wow.
That's briefly how I got into platform engineering.
And afterwards, I came to cortex building machine learning platform for Twitter.
And so cortex, it was, is cortex a team that grew organically in Twitter or there was an
acquisition that was part of that as well, right?
Yes, actually, there were multiple acquisitions.
There was a tweet from Jack around Jack was our CEO.
He tweeted about this around 2016 and Twitter acquired three companies.
One of them is called Mad Bits.
One of them is called Magic Pony and one of them is called WetLab.
These three companies, the acquisition formed the original cortex org.
And the original focus was deep learning research, essentially, we inherited the DNA from these
three acquisitions.
Got it.
Got it.
And so to what degree was machine learning really heavily used at Twitter or maybe the
right way to ask this is broadly used, it sounds like you, there were some activity around
search and I imagine there were some other kind of point use cases, but was it in broad
use at Twitter before cortex?
So machine learning before cortex was used, but it wasn't practiced in a consistent way.
It was definitely used for us to do advertisements.
For example, CTR prediction to fight spam and adversarial users, bad accounts on our platform
and also used to rank search.
Cortex, essentially, in the past two years or so, transitioned from a deep learning research
org to a machine learning platform org.
And the cortex is basically bringing consistency to how machine learning is used across all
sorts of product teams at Twitter.
So I wouldn't say machine learning wasn't used at Twitter widely before cortex, machine
learning was widely used, but exactly because it was widely used and it was practice, like
there are practitioners across many different teams.
It was a very fragmented landscape and different teams did things differently.
So cortex around 2017, our CTO product took over the org and started focusing the org
around serving customers and being a platform team.
And later our current director sent deep to cover and continued to sharpen our customer
focus, now we are a platform team building machine learning platforms to serve various
product teams.
And as we currently stand, most Twitter product teams are using cortex machine learning platforms
to practice and know at Twitter now.
Is the goal of cortex and building platforms, would you say, it's to drive more consistency
and efficiencies in the way folks use machine learning or is it to broaden the kind of the
base of people that can use machine learning, not that those are strictly a dichotomy,
but I'm wondering if one or the other really drove the establishment of the organization.
I think it's both actually ultimately our goal is to empower practitioners at Twitter
to use and know both more efficiently and empower more people to be able to leverage machine
learning.
So it's, I would say the emphasis is more on the latter.
Bringing consistency itself is a intermediate goal in my mind.
We're hoping to bring efficiency to the company by bringing consistency.
And once everybody is practicing ML in a consistent way using our offerings, it makes our job
easier to bring productivity to them.
I don't know if this answer makes sense.
I can further elaborate.
Yeah, it absolutely does.
And I've got one more question on kind of these meta organizational questions before
we dig into some more detail about the platform there and it, and that is around this transition
from deep learning research focused organization to a platform organization.
It strikes me that those are very different missions, perhaps calling for very different
skill sets.
What was the experience of going through that transition like?
Yeah, it was definitely a difficult transition, I would say, and the most, the biggest
shift is the mindset from doing cutting edge research to serving internal customers.
And the customer focus mindset is the biggest change in the org and it took us, many took
us a lot of effort to get our, for example, engineers and researchers to be aligned on
that.
Part of it comes from, did most people buy into the idea of doing that shift or did the
org turn over quite a bit in order to get there?
Both, both.
We had some turnovers as the transition, but most people actually stayed and bought into
this new vision or focus.
And we are now very customer oriented and we do what our customer asked for.
And we still have a research org, but even that research org is focused on, for example,
improving the production model performance of our customer teams.
So for people who wants to do like machine learning, deep learning research, we still
have a place, but we definitely repurposed the goal of the research.
Yeah, the idea of kind of a customer, a customer centric view in providing platforms, I think
is one that makes a lot of sense is kind of, it's a very kind of straightforward approach.
And I'm thinking specifically about this conversation that I recently had and was published
on the podcast just a couple of shows ago with Eric Colson at StitchFix.
And when he talked about the role of the platform's team in his group, it was the way that
they figured out the features that the platform needed, it was, it was really focused on things
that their customers weren't asking for.
It's like how could they add value that the user doesn't even know about versus the way
I think of traditional kind of product management, your understanding requirements and kind
of organizing those requirements and figuring out how to get there with a platform is either
one of those approaches resonate with the way things tend to happen at Cortex?
I think actually both.
So we can talk about our strategy for last year and this year.
Last year is about adoption and consistency, we need to get our users to use our platform.
And this year we started to look into, for example, ease of use and iteration speed.
This is when we think about what kind of features make our users' lives easier.
So I would say for 2018, getting customers to adopt our product and essentially switching
from their current machine learning tool kits to our platform, a customer-focused mindset,
customer-driven feature development is more valuable.
We really need to listen to our customers to understand what makes them to change from
their current ML tool kits and ML, for example, frameworks that switch to our offering.
And afterwards, when we think about what features makes their life easier, we need to adopt
a strategic thinking and think about what our customers are not asking for.
Does this answer make sense?
So I think it's a gradual shift in the beginning of creating this platform.
We definitely need to put more focus on serving customers and catering to their asks.
And as the platform matures, we gradually increase the strategic bets in our portfolio
and do more work that's not necessarily being asked by customers, but we're anticipating.
So we've talked very abstractly about the platform.
Maybe walk us through the platform now.
What are the major components?
How do you think about it architecturally or how would you describe it to someone?
Sure.
So Quartex right now, we offer multiple solutions in our ML platform.
We offer the core model training and evaluation part, which is based on TensorFlow.
And we offer data preprocessing and featureization, something we call feature store.
It allows users to consistently prepare features for machine learning models, both at training
time and prediction time, which is offline in offline and online context.
And we offer production model serving based on JVMs.
This is like a TensorFlow serving equivalent, except it's specialized and custom-bued for
Twitter.
And we have pipeline orchestration, which is a automation solution that allows people to
run dependency graphs of machine learning workloads and basically chain a dependency
of tasks into a single graph and run them in an automated manner.
And we have also added more efforts in our platform, including embeddings, nearest neighbor
search for candidate generation.
We have added machine learning observability, which allows us to observe feature distributions
and also analyze models.
And in the end, we also started a new team inside Quartex, which is called meta, which
is studying the bias and accountability, fairness, those concerns inside algorithmic decisions.
That's a high level overview of what Quartex comprises of today.
Lots of interesting stuff to dive into.
The first of the things that you mentioned was at the framework level?
Yes, the first one is what I mentioned is called the DeepBird.
It's our model training and evaluation solution.
So maybe let's start there.
What is the goal of DeepBird and how do users use it?
Yes.
So DeepBird started as, so historically, Twitter Quartex uses Torch, which is Lua Torch,
not PyTorch.
It's the initial, I would say it's one of the older established deep learning frameworks.
What we noticed around 2017 was that the Torch community started to lose steam and the
tensor flow and PyTorch started gaining popularity in the community.
And DeepBird and actually DeepBird version 2 is our effort in partnership with Google
to bring tensor flow to Twitter.
So the goal of this component DeepBird V2 is to unlock latest technology backed by Google
mostly tensor flow and its ecosystem and tensor flow extended for use at Twitter.
And specifically to be more specific, Twitter is more of a Java shop like most of our code
are either in Java or Scala, basically JVM languages, whereas tensor flow is mostly Python
and C++.
So we had to build quite a lot of production, gluing logic to actually make tensor flow
work at Twitter.
And also DeepBird provides an additional abstraction layer between tensor flow and our machine learning
practitioners at Twitter.
We do this for multiple goals.
One is to reduce the complexity of using tensor flow.
This tensor flow was actually released, production released was in February 2018.
It's a relatively new thing and we would like to hide complexity whenever possible.
And inside the abstraction layer, we prescribe default values for different knobs and we
also include optimizations that are specialized and customized for the Twitter data centers
and Twitter workload.
A ton and there to dig into, you mentioned needing to build a lot of glue code to bring
this Python oriented system into your primarily JVM based environment.
Can you give some examples of specific things that you had to do?
Yeah, the main example I can give is that tensor flow is serving, right?
So tensor flow serving is actually a C++ app speaking GRPC at Twitter, with GRPC is not
our standard RPC protocol and also C++ is not the main language.
Our engineers don't know how to maintain C++ apps.
So we essentially built an equivalent of tensor flow serving, but inside using Java
and Scala and we ship that app to our customers.
This is mainly for internal maintenance so that our teams knows how to be on call and
fix issues for the serving solution.
And also it's for tighter integration with our internal observability stack.
This allows us to integrate with our monitoring and alerting solution seamlessly because tensor
flow serving doesn't have that integration.
Does that make sense?
Yes.
So deeper V2 is very focused on bringing tensor flow to Twitter.
Does that mean it's a highly opinionated system and user that is interested in using PyTorch
for example, isn't supported with deep bird?
That's currently the case.
So remember what we started with, the goal was we were trying to defragment the machine
learning practices at Twitter.
What we noticed was in 2017, we had users of LuaTorch and we had tensor flow users,
we had scikit learn users, we had XG boost users.
So this fragmentation caused several issues.
First of all, it causes difficulty sharing.
Different teams can't share their machine learning models, they're tooling and resources.
And sometimes it prevents expertise sharing as well, like if an engineer wants to move
from one team to another, he has to learn a new set of expertise in order to be effective.
So that's when we noticed the fragmentation is a problem and then we also noticed work
duplication, right?
So Twitter is a very large scale company and I can introduce our scale maybe separately,
but essentially we have to invest in a lot of resource to duplicate serving solutions.
For example, let's say to serve a scikit learn model versus serving a PyTorch model versus
serving a tensor flow model, if we end up building like three different set of serving solutions,
which is what happened before, it wastes a lot of engineering resource.
So that's why deep bird is an opinionated, like you said, prescribed opinionated way of
doing machine learning based on top of tensor flow that tries to get our machine learning
practitioners to do things in a consistent way that allows different teams to share machine
learning models, share their tools and resources and even knowledge across teams.
You mentioned scikit learn is deep bird also an abstraction for traditional machine learning
workloads and beyond the deep learning workloads.
So there's really not a clear line between deep learning and traditional machine learning.
So yes, deep bird can support traditional machine learning, tensor flow supports, for
example, traditional machine learning methods as well.
One of the why it is to use the traditional machine learning method is actually logistic
regression.
It's very widely used inside Twitter and in fact, there's really not a clear line between
logistic regression and deep learning because we can actually think of logistic regression
as a one layer neural network with a single output.
So sure, but often folks find the overhead of deep learning frameworks relative to the
tools that they might use for traditional ML to be a pretty heavy weight.
Yes.
So for example, tensor flow itself, the estimator API is the main API tensor flow recommends
for productionization in tensor flow one.
And that API we acknowledge that it's very clunky and many of our users don't really like
it.
They think it's too heavy weight.
That's exactly what we provide in our deep bird abstraction.
We're trying to hide the complexity whenever possible and prescribe good defaults.
So I usually use a camera analogy.
Think of tensor flow as a very powerful DSLR camera.
Many users actually prefer like a mobile phone, one button point and shoot camera.
So what we're trying to do is we're trying to wrap tensor flow inside our layer of abstraction.
We try to encapsulate all the knobs and buttons on the digital SR camera.
And we expose a single button point and shoot solution.
So this is a double edge sword, right?
It's most users like it, especially the production ML engineers.
They really like solutions where they type a single command.
It trains a machine learning model for them.
But some of the more advanced users and deep learning researchers actually don't necessarily
like the point and shoot solution.
It's just like how a professional photographer might feel like it's an insult.
If you're giving a point shoot camera, right?
So I would say to answer your question, we added this abstraction layer to hide complexity
of deep learning frameworks such that if you want to do logistic regression using our
ML platform, it's very simple.
And we aim for like a simple solutions where you type a command.
We can launch training and save models on HDFS and you type another command.
We launch hundreds of prediction servers serving the model saved on HDFS.
But we do need to think about how to cater to the more powerful users as well.
To what degree does DeepBird replicate a lot of the work that has been done with Keras
as a front end to TensorFlow, it seems like there are similar goals in terms of increasing
usability, although Keras is certainly not a one button type of solution.
Correct.
So I would say there's definitely some overlap.
Essentially TensorFlow 1.0 made this, basically, in TensorFlow 1.0, there are these two ways of
practicing ML estimators and Keras estimator was more targeting production use, scaling
to large data set, while Keras was targeting ease of use.
So DeepBird is basically building on top of estimator and hoping to improve ease of
use.
Upcoming in TensorFlow 2.0, TensorFlow is consolidating estimator and Keras into a single Keras API.
So users no longer need to choose between scalability and usability.
We also envision that going forward once TensorFlow 2.0 is released, DeepBird will also
most likely adopt the Keras based API.
So that is the training elements.
Do you also, does DeepBird also offer features focused on experiment management, tracking
model parameters, hyperparameter tuning kind of that whole space?
Yes, we do.
So before our ML platform, most of our customers track to these in spreadsheets and we
notice that, so we build a repository where our DeepBird training jobs can automatically
push the hyperparameters used and the experiment name and the resulting metrics like a PRAOC,
accuracy, et cetera, into this model repository.
And then you can query the repository for the experiments you have run and examine their
hyperparameters and metrics.
And do you support visualization with TensorBoard or do you have your own solution or an alternate
solution for visualization?
We mostly visualize using TensorBoard.
So when you launch a DeepBird training job in our internal private cloud, we automatically
start TensorBoard to watch the training process and render loss and other metrics.
So this model repository we just talked about, once you query for experiment that finished,
there's actually a button right in the UI that says launch TensorBoard.
If you click on that button, it launches TensorBoard on an instance in our internal private
cloud and it points the TensorBoard to that experiment that actually finished running
and shows how the loss came down and how the resulting metrics look like.
Is the platform also opinionated in terms of whether users use a notebook experience or
traditional code files or ID?
We're not opinionated on how the user developed code, but we do offer a notebook solution
that's integrated with our internal clusters.
We offer this thing called PyCX where our users can type a single command and it launches
a notebook instance and it tunnels to the instance and gives back a URL that the user can
use.
It's a semi-hosted notebook solution, basically our users can type a single command and
we launch a notebook server on our internal cloud and the user can use the notebook and
it contains most of the dependencies our users would need.
But we don't force our users to use notebooks for development.
Are you doing anything to try to streamline the process of going from notebook to production,
like some kind of automated code pulls or code extraction from the notebooks that kind
of thing?
I've seen that from time to time.
Yeah, not yet.
This is definitely an area that's worth considering.
I've seen in the industry that there's paper mill that allows people to execute notebooks
as a parameterized script and there's also git plugins that allows people to create
very nice looking diffs of notebooks.
Those are not yet explored in PyCortex, but it's a upcoming area that we plan to look
into.
Also, hyper parameter optimization.
Did you mention anything in that regard?
Are you doing that?
Are you providing a solution to automate that?
Yes, definitely.
So this touches on the machine learning workflow component that I talked about.
I briefly mentioned earlier to the pipeline orchestration piece.
Yes, exactly.
One thing we learned about machine learning is it's a very tedious process, and when we
don't automate, our users don't necessarily do things in exhaustive or they don't, for
example, typewriter.
If you don't help them too much, they'll stay on their laptops and do it the way they
used to.
Exactly.
Exactly.
And hyper parameter search is one of them.
We noticed that it often requires very repetitive and tedious repetitions of experiments.
So our machine learning workflow solution allows our users to perform randomized search
and grid search by launching a large array of experiments on our internal cloud and
automatically recording hyper parameters and results.
And they can essentially just pick the best solution.
We also have a solution based on Bayesian optimization, where we have a service called wetlab,
this is from the company we acquired.
We give it the set of hyper parameters that we just tested and the results and the service
tell us back what's the next set of hyper parameters to test.
It automatically takes into account exploration and exploitation and recommends sets of hyper
parameters to try next.
And that's so far seems very effective and our ML workflow solution has completely automated
solution for using wetlab.
Essentially, you just need to say I want to do hyper parameter tuning and configure the
hyper parameter tuner to be wetlab instead of grid search or randomized search.
And the automated solution would take care of the hyper parameter tuning for you.
I guess what's the kind of the fundamental currency in this system?
Is it code that's checked into Git repository?
Some people focus at the code level.
I see others are dealing with containers and checking it and checking out containers.
Is the core artifact for your system?
For our system, the core artifact, I would say, are tensor flow graphs or actually the
code that's defining the tensor flow graphs.
The tensor flow graphs essentially defines the machine learning model.
And as part of it, we also define which features are used.
After that, we send this for training and training spits out another artifact called
saved bundles.
It's a tensor flow concept.
And these saved bundles are stored on HDFS for serving.
So I would say the core artifacts are both the tensor flow graph and the trained models.
And presumably, you're versioning all of these and tracking the versions transparently
to the end user, or is the end user thinking about development workflow explicitly?
The end users do need to think about development workflow and how they version their code.
Essentially, the tensor flow graph is code, so it's version by Git.
But the models are end users need to version themselves.
And have you seen within your user base that you've got ML engineers that are very comfortable
with that kind of workflow, but data scientists that are less comfortable with that workflow
and prefer it to happen transparently?
So we have definitely seen that one of the things we learned was that there's a wide
variety of machine learning practitioners ranging from machine learning engineers all the
way to deep learning researchers and data scientists.
And their use cases differ from production engineering versus more exploration and analysis.
And they prefer quite different solutions.
And what you mentioned is one.
The scientists, they tend to prefer, for example, notebook, centric exploration and development
and machine learning engineers prefer to write code and just git and check into Git.
Right now, we don't have a very opinionated solution or we don't prescribe a development
workflow across this different type of machine learning practitioners.
We are, we start with catering to production ML engineers, we're in the process of starting
to look at how deep learning researchers and data scientists test to use our platform
and how we can make their lives easier.
We jumped over to talking about that workflow or pipeline orchestration layer.
Is that based on something open source like airflow or is it proprietary orchestration
tool?
It's based on top of Apache airflow.
Are you doing both kind of online and offline workflows with that?
Are you like doing offline scoring and or batch scoring using the workflow tool as well
or is it primarily for the experimentation and kind of model development loop?
It's primarily for offline training.
For Twitter, the online part is actually real time, right?
So we have prediction servers for that machine learning workflow is for training the models.
I didn't fully understand what you mean by batch scoring like our predictions are like
this like a user comes to twitter.com.
For example, we need to present advertisement, we immediately need to respond.
So this request hits our prediction servers and we generate scores in a real time manner.
That's not using machine learning workflows.
I think we thoroughly explored the first thing on your list of like seven things and we're
40 minutes in.
I see.
We may need to to to be continued this.
I am very curious about the meta team that you mentioned and maybe we can spend a few
minutes talking about what you're doing there because it is an issue the issues around
bias and accountability are and fairness are ones that are you know folks are starting
to realize they need to pay more attention to and I'm curious how you've staffed up a
team with the charter of that team is what the teams practices are and how they're tackling
this issue.
Sure.
So this started from a year ago RCU Jack publicly tweeted that we're committing to
increasing the collective health openness and civility of public conversations.
And as part of that we're using machine learning to make algorithmic decisions to curate
the public conversations right.
So one thing we realize is we don't yet fully understand the impact of those algorithmic
decisions and for example these algorithms decide what our users see right and what people
see might actually change their behavior in response to the algorithmic recommendations
and as a result their behavior shift and their behavior creates training data which feeds
back to the algorithms which feeds back to the training data we use to train our machine
learning models and this creates feedback loops and it's not yet super clear to us what
exactly these feedback loops cause and for example there's right now there's research
about how machine learning in social networks cause like polarization of opinions.
So we started this team called meta to study like the bias for example and fairness and
accountability and explainability of our machine learning models and we're staffing the
team by partnering up with UC Berkeley professors.
This is for two reasons for one this is the interdisciplinary effort it involves not
just engineering there's also social science concerns there's legal concerns there's I
don't know there's a lot of like essentially human concerns other than engineering concerns
that's why we want diverse perspective and we're partnering with UC Berkeley professors
and researchers the second thing they bring is they bring a like a because we're thinking
about fairness and bias right the third party which is UC Berkeley which is not really a
part of Twitter they bring a I think they bring exactly exactly that's why we staffed
the team to be a partnership between Twitter engineers and UC Berkeley professors and researchers
we want to make sure that we're not we're getting perspectives not just from engineers
because this is not just an engineering problem it's also a social problem and when you when
I think about a role like this or a team or a charter like this and what we want from
them in today's environment it strikes me that a big part of this by necessity is kind
of research and exploration and building understanding of these issues and how they play out in
a network like Twitter's but you also want that to have engineering impact and you want
to create a place where data scientists that are working on a problem or machine learning
engineers that are working on a problem can take advantage of you know some degree of
expertise in a very kind of practical tangible today kind of way how do you manage that
that dichotomy with this team yes so this team is right now the main objective is to provide
for example tooling and resource to increase explainability and transparency of our machine
learning models for now we need to first understand what's going on right before we actually
propose what to change so I think for now there really isn't a dichotomy yet between engineering
impact and research essentially we're trying to measure for the first step and we are providing
tools for different teams to be able to measure well it has been a great conversation
still so much more for us to chat about but I think this was a very very interesting exploration
of deep bird and how you're approaching training and I appreciate you taking the time to chat
with us about it happy to continue the conversation next time if we we get a chance yeah absolutely
thanks so much all right everyone that's our show for today for more information about
today's guest or to follow along with AI platform volume 2 visit twimmelai.com slash
AI platforms to make sure you visit twimmelcon.com for more information order register for twimmel
con AI platforms thanks again to seek out for their sponsorship of this series to check
out what they're up to and take advantage of their exclusive offer for twimmel listeners
visit twimmelai.com slash sigopt as always thanks so much for listening and catch you next
time.
