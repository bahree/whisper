WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:28.120
This week on the podcast, we're featuring a series of conversations from the Nips conference

00:28.120 --> 00:30.440
in Long Beach, California.

00:30.440 --> 00:34.320
This was my first time at Nips and I had a great time there.

00:34.320 --> 00:37.560
I attended a bunch of talks and of course learned a ton.

00:37.560 --> 00:43.720
I organized an impromptu round table on building AI products and I met a bunch of wonderful

00:43.720 --> 00:47.840
people, including some former Twimble Talk guests.

00:47.840 --> 00:52.680
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

00:52.680 --> 00:59.360
take a second right now to subscribe to at twimblei.com slash newsletter.

00:59.360 --> 01:05.040
This week through the end of the year, we're running a special listener appreciation contest

01:05.040 --> 01:09.760
to celebrate hitting 1 million listens on the podcast and to thank you all for being

01:09.760 --> 01:11.720
so awesome.

01:11.720 --> 01:16.400
Tweet to us using the hashtag Twimble1Mill to enter.

01:16.400 --> 01:21.040
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

01:21.040 --> 01:23.040
other mystery prizes.

01:23.040 --> 01:29.840
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

01:29.840 --> 01:32.280
for the full rundown.

01:32.280 --> 01:36.640
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

01:36.640 --> 01:39.880
of this podcast and our Nips series.

01:39.880 --> 01:44.880
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

01:44.880 --> 01:50.600
sessions, their big news this time was the first public viewing of the Intel Nirvana

01:50.600 --> 01:54.400
neural network processor or NNP.

01:54.400 --> 01:59.320
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

01:59.320 --> 02:04.440
primitives while making the core hardware components as efficient as possible, giving

02:04.440 --> 02:09.920
neural network designers powerful tools for solving larger and more difficult problems

02:09.920 --> 02:14.560
while minimizing data movement and maximizing data reuse.

02:14.560 --> 02:20.400
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

02:20.400 --> 02:22.560
Nirvana.com.

02:22.560 --> 02:28.040
In this episode, I speak with Yael Niv, professor of neuroscience and psychology at Princeton

02:28.040 --> 02:29.200
University.

02:29.200 --> 02:34.240
Yael joined me after her invited talk on learning state representations.

02:34.240 --> 02:38.840
In this interview, Yael and I explore the relationship between neuroscience and machine

02:38.840 --> 02:39.840
learning.

02:39.840 --> 02:44.760
In particular, we discuss the importance of state representations in human learning,

02:44.760 --> 02:49.800
some of her experimental results in this area, and how a better understanding of representation

02:49.800 --> 02:54.640
learning can lead to insights into machine learning problems such as reinforcement and

02:54.640 --> 02:56.360
transfer learning.

02:56.360 --> 02:58.520
Did I mention this was a nerd alert show?

02:58.520 --> 03:01.800
I really enjoyed this interview and I know you will too.

03:01.800 --> 03:06.280
Be sure to send over any thoughts or feedback via the show notes page.

03:06.280 --> 03:08.760
And now on to the show.

03:08.760 --> 03:20.400
All right, everyone, we are here in Long Beach at the NIPPS conference and I've got the

03:20.400 --> 03:22.880
pleasure to be seated with Yael Niv.

03:22.880 --> 03:29.880
Yael is a professor of neuroscience and psychology at Princeton University and she delivered

03:29.880 --> 03:33.760
a talk this morning on learning state representations.

03:33.760 --> 03:37.600
And I'm really excited to have her here to talk to us about what she's working on.

03:37.600 --> 03:40.400
Yael, welcome to this weekend machine learning and AI.

03:40.400 --> 03:41.400
Thank you.

03:41.400 --> 03:42.400
Thanks for having me.

03:42.400 --> 03:43.400
Absolutely.

03:43.400 --> 03:44.400
Absolutely.

03:44.400 --> 03:47.360
So you are a neuroscientist, a neuropsychologist.

03:47.360 --> 03:50.240
You're here presenting on machine learning.

03:50.240 --> 03:52.600
You talked about reinforcement learning.

03:52.600 --> 03:54.320
How did all of this come about?

03:54.320 --> 03:58.440
How did you end up at the intersection of these two fields?

03:58.440 --> 04:01.840
So the funny thing is I didn't start as a neuroscientist.

04:01.840 --> 04:09.680
Well, I started as a computational neuroscientist, more interested in AI and in psychology, but

04:09.680 --> 04:14.360
not an experimentalist like I am today, I started as a theoretician.

04:14.360 --> 04:17.160
So my PhD was in reinforcement learning theory.

04:17.160 --> 04:19.960
My main advisor was in computer science.

04:19.960 --> 04:22.400
My secondary advisor was in psychology.

04:22.400 --> 04:26.680
She was, I asked her to be an advisor so that she would ground me in real data and I

04:26.680 --> 04:32.360
wouldn't be making models of things that I've made up rather than the real world.

04:32.360 --> 04:35.920
And my PhD was also completely theoretical.

04:35.920 --> 04:41.200
I was modeling animal behavior with reinforcement learning, submitting my papers to nips.

04:41.200 --> 04:43.120
That's getting rejected by nips.

04:43.120 --> 04:48.320
I got a best paper award finally when I paper was finally accepted at nips.

04:48.320 --> 04:50.160
So I was actually really excited now.

04:50.160 --> 04:52.640
I have drifted away.

04:52.640 --> 04:56.920
I've drifted more into psychology, more into neuroscience, more into doing experiments.

04:56.920 --> 05:02.640
I still do modeling, but it really is now half half with experiments.

05:02.640 --> 05:06.360
And so I haven't been coming to nips for the last few years and I was really excited

05:06.360 --> 05:14.120
to be invited to give a talk here because it felt like closing a circle from all those

05:14.120 --> 05:17.720
papers being rejected now.

05:17.720 --> 05:19.040
Now giving an invited talk.

05:19.040 --> 05:22.880
So I immediately said yes, very nice, very nice.

05:22.880 --> 05:25.920
What do you consider to be your home conference now?

05:25.920 --> 05:29.920
My whole conference now is reinforcement learning and decision making RLDM.

05:29.920 --> 05:32.880
I helped found that conference.

05:32.880 --> 05:35.360
And we have it every two years.

05:35.360 --> 05:39.440
There was one this past year, so the next one is in 2019.

05:39.440 --> 05:45.360
And it's basically a very interdisciplinary conference that tries to bring machine learning,

05:45.360 --> 05:52.520
AI, psychology, etiology, economics, anybody who's interested in reinforcement learning

05:52.520 --> 05:55.280
and decision making over time.

05:55.280 --> 05:57.360
And it's a growing conference last time.

05:57.360 --> 05:59.120
I think we had about 600 people.

05:59.120 --> 06:00.120
Oh wow.

06:00.120 --> 06:01.120
Wow.

06:01.120 --> 06:02.120
So that's my home base.

06:02.120 --> 06:03.120
Nice.

06:03.120 --> 06:04.120
Nice.

06:04.120 --> 06:07.240
And one of the things that I've observed here and this is the first time I've ever been

06:07.240 --> 06:12.760
at nips is that there seems to be of several kind of themes that have emerged for me.

06:12.760 --> 06:19.560
One of the strong themes is kind of integrative approaches and interdisciplinary approaches.

06:19.560 --> 06:24.280
Has nips always been like that or are we seeing more of that now than in the past?

06:24.280 --> 06:26.520
I think nips has always aspired to that.

06:26.520 --> 06:32.080
It waxes and wanes, so people have talked about, you know, the n in nips, is it sometimes

06:32.080 --> 06:33.080
more ips?

06:33.080 --> 06:34.080
Okay.

06:34.080 --> 06:39.320
And whereas the neuro, there have been, you know, worse years and better years, I used

06:39.320 --> 06:40.320
to come regularly.

06:40.320 --> 06:42.280
So I remember this.

06:42.280 --> 06:48.320
And there are also kind of fads and things that become more popular and less popular.

06:48.320 --> 06:52.200
I remember when I was a master student, someone coming back from nips and saying that it should

06:52.200 --> 06:57.440
have been called support vector machine conference that year because it was all support vector

06:57.440 --> 06:58.440
machines.

06:58.440 --> 06:59.440
Right.

06:59.440 --> 07:03.600
Now almost nobody talks about SVM and everybody talks about deep learning.

07:03.600 --> 07:08.640
So it kind of changes focus, but there's always been, I think, a sincere attempt to keep

07:08.640 --> 07:10.720
the neuroscience in the mix.

07:10.720 --> 07:16.680
And it's not super easy because I think it's very clear that neuroscience has a lot to

07:16.680 --> 07:21.320
learn from or not necessarily to learn, but to take from machine learning.

07:21.320 --> 07:26.800
So we use machine learning algorithms to analyze our data to think of computational processes

07:26.800 --> 07:27.800
in the brain.

07:27.800 --> 07:31.640
Really, I feel like when I come to nips, I'm coming is like a shopper.

07:31.640 --> 07:35.160
I want to see what's on the shelf now that I can use in my research.

07:35.160 --> 07:38.520
I don't think it's necessarily that way, the other way around.

07:38.520 --> 07:45.600
I think AI in particular has been inspired by neuroscience, but mostly, you know, takes

07:45.600 --> 07:49.960
quick inspiration and runs, runs off to somewhere else, and that's fine because their goal

07:49.960 --> 07:50.960
is different.

07:50.960 --> 07:52.600
My goal is to understand the brain.

07:52.600 --> 07:54.960
The goal of AI is completely different.

07:54.960 --> 07:56.960
It's almost aspirational inspiration.

07:56.960 --> 07:57.960
Yeah.

07:57.960 --> 08:03.400
I think basically AI needs neuroscience less that neuroscience needs machine learning.

08:03.400 --> 08:04.400
Interesting.

08:04.400 --> 08:08.280
So it's always been kind of a tough thing to keep the mix together.

08:08.280 --> 08:12.520
But in your talk, you, the focus of your talk was that mix, right?

08:12.520 --> 08:13.520
I tried.

08:13.520 --> 08:16.000
I tried my best.

08:16.000 --> 08:20.120
It's not the typical talk that I give because these days, I don't usually talk to this type

08:20.120 --> 08:21.120
of audience.

08:21.120 --> 08:26.320
So I thought, hard, you know, what do I have to sell to AI?

08:26.320 --> 08:28.360
What do I want AI to buy?

08:28.360 --> 08:33.880
And so I was posing this challenge of, you know, real, well, I don't know, call it

08:33.880 --> 08:39.000
real with a capital R, but intelligence that is more like human intelligence.

08:39.000 --> 08:44.480
So with, you know, playing chess or playing go at an expert level, that's a huge achievement

08:44.480 --> 08:48.520
because these are really hard tasks, but they're really hard for humans too.

08:48.520 --> 08:54.120
You can't just become a go expert tomorrow.

08:54.120 --> 08:56.920
But there are other things that we learn super easily.

08:56.920 --> 09:03.240
And those things are very hard for computers as well for AI, you know, maybe, maybe in

09:03.240 --> 09:05.240
some way as hard as playing go.

09:05.240 --> 09:10.280
And I think that's, that's a real challenge because those are, because the amount of flexibility

09:10.280 --> 09:15.760
that the human brain has, the extent to which we learn super quickly, and we can toggle

09:15.760 --> 09:21.560
tasks, do one task, and then a minute later do another and then return back and kind

09:21.560 --> 09:27.600
of toggle the representations of policies that all the machinery that's needed for these

09:27.600 --> 09:30.320
tasks is really, really impressive.

09:30.320 --> 09:33.320
And that's what I have not seen in any AI yet.

09:33.320 --> 09:34.320
Right.

09:34.320 --> 09:37.440
I thought that was a really interesting characterization of kind of the challenge of AI, or what you

09:37.440 --> 09:39.440
believe the challenge of AI should be.

09:39.440 --> 09:40.440
AI challenge.

09:40.440 --> 09:41.440
AI challenge.

09:41.440 --> 09:42.440
One of them.

09:42.440 --> 09:47.160
AI should be, you know, as opposed to kind of these, you know, grand challenges like

09:47.160 --> 09:55.120
AlphaGo or like the game of Go, you propose many simpler problems solved with much less

09:55.120 --> 09:57.120
data as a challenge.

09:57.120 --> 09:59.640
Is that the way did I characterize that right?

09:59.640 --> 10:01.480
I don't know if the problems are simpler.

10:01.480 --> 10:02.480
OK.

10:02.480 --> 10:06.960
So I gave the example in my talk of crossing the street and it seems like a simple problem

10:06.960 --> 10:14.600
because we already think about it as a simple situation, but really the stimuli, the auditory,

10:14.600 --> 10:19.840
visual world around you when you're doing a simple task of crossing the street is very

10:19.840 --> 10:21.440
complex.

10:21.440 --> 10:28.320
And you need, you know, sophisticated visual machinery in our brain to parse out the

10:28.320 --> 10:32.120
objects you need sophisticated auditory machinery, et cetera.

10:32.120 --> 10:37.760
But more than that, you need to know, even if I've parsed everything, most of it is still

10:37.760 --> 10:38.760
irrelevant for the task.

10:38.760 --> 10:40.080
I don't even need to parse it.

10:40.080 --> 10:43.680
I don't need to parse what are the stores on the other side of the street.

10:43.680 --> 10:48.520
I don't need to parse even the colors of the cars because that's irrelevant for the task.

10:48.520 --> 10:53.640
So whether my visual system does that or not is one question, but my learning system

10:53.640 --> 10:58.880
should definitely ignore those aspects, and that's really hard because there are potentially

10:58.880 --> 11:03.720
infinite combinations of things that I might need to ignore or pay attention to.

11:03.720 --> 11:08.280
There's everything that I see and then there's everything in my past because it might be

11:08.280 --> 11:12.080
that the time of day matter, unobservable things, not only in my past, the time of day

11:12.080 --> 11:17.280
matters, the city in matters, what I did yesterday matters, maybe not for crossing the street,

11:17.280 --> 11:23.160
but if I'm driving a car and navigating, like what I know about traffic from past learning.

11:23.160 --> 11:28.560
So potentially, infinite dimensions of the environment and have to narrow them down

11:28.560 --> 11:29.560
to like 345.

11:29.560 --> 11:35.560
And one of those examples of the kind of the state that goes into figuring that out was

11:35.560 --> 11:37.680
Washington DC versus New York?

11:37.680 --> 11:38.680
Yeah, yeah.

11:38.680 --> 11:43.880
The idea is that, you know, I could also say, you know, New York versus London, long

11:43.880 --> 11:48.880
for each versus whatever, the context really affects, let me take one step back.

11:48.880 --> 11:52.800
So on the one hand, we want to generalize broadly and that's why it's really important

11:52.800 --> 11:59.960
to ignore some things and kind of represent and learn about only others because if I'm

11:59.960 --> 12:05.560
really, you know, if my brain is so sophisticated that can take into account everything in my

12:05.560 --> 12:10.400
visual scene, that would seem to be optimal, but it's actually super suboptimal because

12:10.400 --> 12:16.080
it means that when I learned something new, I don't know what to generalize it to.

12:16.080 --> 12:20.760
Is it, is this true only in this intersection only when a red car is coming at me only

12:20.760 --> 12:26.200
when I'm in front of this shop and depending on the task, let's say crossing the street,

12:26.200 --> 12:28.640
what I've learned is probably much more general than that.

12:28.640 --> 12:34.100
I might learn that in Princeton, the minute you step down the curve, all the cars top

12:34.100 --> 12:38.960
and let you go no matter what, that's general to all of Princeton, not only to that place,

12:38.960 --> 12:40.120
but only to Princeton, right?

12:40.120 --> 12:43.080
I wouldn't want to learn that about Long Beach.

12:43.080 --> 12:48.800
So it's all about setting the boundaries of generalization and what I've realized from

12:48.800 --> 12:54.920
this work is that really when you think about real world experience, you never, ever see

12:54.920 --> 12:57.560
two situations that are exactly the same.

12:57.560 --> 13:01.280
Even if you're at that same intersection, it won't be exactly the same crossing the

13:01.280 --> 13:02.600
street tomorrow.

13:02.600 --> 13:08.880
So the fact that we can reuse any past experience, the fact that we can learn at all means that

13:08.880 --> 13:10.320
we have to generalize.

13:10.320 --> 13:15.480
And so the basic thing that I think about all the time is how do we determine the boundaries

13:15.480 --> 13:16.840
of the generalization?

13:16.840 --> 13:23.360
So I've come from reinforcement learning where we think a lot about how do we learn values

13:23.360 --> 13:25.800
and policies.

13:25.800 --> 13:28.520
But I'm thinking that problem is pretty much solved.

13:28.520 --> 13:30.600
I mean, we have a lot of algorithms for that.

13:30.600 --> 13:32.680
We know how it's done in the brain.

13:32.680 --> 13:34.240
The question is on how do I learn values?

13:34.240 --> 13:36.360
The question to what do I generalize this value?

13:36.360 --> 13:38.120
What's the value of what?

13:38.120 --> 13:40.360
What state, what situation?

13:40.360 --> 13:45.720
And I want to go back to you said that my challenge was to solve or the challenge that

13:45.720 --> 13:47.360
my problem was less data.

13:47.360 --> 13:49.840
To solve simple problems with less data.

13:49.840 --> 13:52.120
Let me try to phrase that a little bit differently.

13:52.120 --> 13:58.040
So there's this old story, I don't know if it's a myth or not, that I heard about nips,

13:58.040 --> 14:03.320
that basically, well, it happened at nips, it's not about nips.

14:03.320 --> 14:08.760
So what I heard is that in the 80s, kind of like in the first heyday of neural networks,

14:08.760 --> 14:15.680
now there's second, someone published a nips paper that showed how you can use a

14:15.680 --> 14:19.240
neural network to parallel park a truck.

14:19.240 --> 14:23.600
And you know, this is in the 80s, think of, you know, computing power in those days and

14:23.600 --> 14:24.600
stuff.

14:24.600 --> 14:27.440
This is a really hard challenge to parallel park a truck.

14:27.440 --> 14:34.520
And after all the hurrah and happiness, apparently, or as the story goes, the next year,

14:34.520 --> 14:41.960
someone else published a paper in nips showing that one neuron, so a perceptron, one simple

14:41.960 --> 14:44.280
computing unit, can parallel park a truck.

14:44.280 --> 14:50.120
If you give it the input of the obstacles in polar coordinates rather than Cartesian coordinates.

14:50.120 --> 14:55.360
So what I took from that, I heard this when I was a master student, is basically, if you

14:55.360 --> 14:59.400
ask a question just right, it's really easy to answer.

14:59.400 --> 15:04.680
And that's basically what I'm saying, the decision making could be kind of pair down

15:04.680 --> 15:08.400
to a yes, no question that a perceptron can answer.

15:08.400 --> 15:13.200
But that pairing down is the hard thing, like giving the input just right.

15:13.200 --> 15:17.520
And so that's kind of my whole enterprise, my whole research enterprise is how does the

15:17.520 --> 15:24.080
brain learn how to ask the questions, ask complex questions so that they're made simple.

15:24.080 --> 15:30.760
And you know, how can we use that for AI to take all these complex problems, real world

15:30.760 --> 15:35.960
navigation, et cetera, and make them into easy problems that a perceptron can answer.

15:35.960 --> 15:36.960
Interesting.

15:36.960 --> 15:42.560
Is there evidence in the brain that that process looks like a filtering process, or is

15:42.560 --> 15:49.720
it more like a blindness to certain irrelevant variables, or is it the way things are classified?

15:49.720 --> 15:54.800
It seems like there are potentially a number of ways that you can not consider variables.

15:54.800 --> 15:55.800
Yeah.

15:55.800 --> 15:59.880
So, the evidence that we've seen so far in the brain that I actually didn't talk about

15:59.880 --> 16:06.320
today, and some glad that you asked about this, is that attention processes are strongly

16:06.320 --> 16:09.400
involved in sub-selecting.

16:09.400 --> 16:14.120
So first of all, anatomically, I should say that the areas in the brain that we know are

16:14.120 --> 16:19.080
involved in this kind of reinforcement learning and decision making, get input, the small

16:19.080 --> 16:22.040
area in the middle of the brain, it's called the stratum.

16:22.040 --> 16:26.440
And the stratum gets input from the whole cortex.

16:26.440 --> 16:33.640
So all the brain all around, the sensory, motor, high level, emotional, everything, everything

16:33.640 --> 16:34.960
funnels in.

16:34.960 --> 16:38.960
But it funnels in with a huge dimensionality reduction, so about 1 to 10,000.

16:38.960 --> 16:42.880
So 10,000 inputs into one neuron on average.

16:42.880 --> 16:46.920
So from the start, we know that there has to be huge dimensionality reduction.

16:46.920 --> 16:51.600
But more than that, I'm talking here about dimensionality reduction that's not structural,

16:51.600 --> 16:54.600
that's kind of modulated task specific for one task.

16:54.600 --> 16:57.360
I want to know the colors of the cars for another task I don't.

16:57.360 --> 17:02.200
So the input has to really be modulated on the fly according to our goals.

17:02.200 --> 17:07.280
And what we've seen is that areas in the brain that are involved in attention, in switching

17:07.280 --> 17:12.400
attention, selecting what to attend to are involved in this process.

17:12.400 --> 17:16.880
And it's really interesting for me to think about attention in that way, because we often

17:16.880 --> 17:22.440
think about selective attention or limited attention as a bottleneck or limitation.

17:22.440 --> 17:27.040
We can't attend to everything, so unfortunately, we have to ignore most of the scene.

17:27.040 --> 17:31.480
And I'm thinking about it in a much more normative way, that it's optimal to not attend

17:31.480 --> 17:32.480
to everything.

17:32.480 --> 17:33.720
That's what allows us to function.

17:33.720 --> 17:35.720
It's what allows us to learn.

17:35.720 --> 17:36.720
Right.

17:36.720 --> 17:42.480
So if you attend just right, and you're solving a problem, rather than creating a problem.

17:42.480 --> 17:48.640
So one of the challenges is to understand how do we learn what to attend to.

17:48.640 --> 17:50.120
And we're working on that.

17:50.120 --> 17:51.280
It's not easy.

17:51.280 --> 17:53.680
We have tasks where we can measure people's attention.

17:53.680 --> 17:56.160
We have a multi-dimensional scenario.

17:56.160 --> 17:59.400
We tell people that only one dimension matters.

17:59.400 --> 18:05.000
So these dimensions could be color, shape, texture of different stimuli that could be other

18:05.000 --> 18:06.200
dimensions.

18:06.200 --> 18:09.400
And we tell them that only one determines reward.

18:09.400 --> 18:13.160
And we're trying to see how they learn from trial and error, what to attend to.

18:13.160 --> 18:18.840
And in essence, what we're trying to do is devise a figure out, reverse engineer the

18:18.840 --> 18:24.760
algorithm, the computational algorithm, by which feedback for our actions affects our

18:24.760 --> 18:27.520
representation, affect what we attend to.

18:27.520 --> 18:30.480
And we're kind of in the midst of that.

18:30.480 --> 18:32.000
It's a it's a call order.

18:32.000 --> 18:35.760
We've actually talked to, it turns out that there aren't machine learning algorithms for

18:35.760 --> 18:37.760
modeling those kinds of data.

18:37.760 --> 18:38.760
Hmm.

18:38.760 --> 18:43.240
Now here attention come up a lot in the context of like LSTM networks.

18:43.240 --> 18:49.760
Is it just kind of a word overlap, but not the same kind of mechanism or idea, or are

18:49.760 --> 18:52.120
there some parallels there?

18:52.120 --> 18:56.680
I'm not well enough first in these models to know how direct the parallels are.

18:56.680 --> 19:00.760
I can say that from a psychology point of view, a neuroscience point of view, attention

19:00.760 --> 19:04.120
is almost kind of a dirty word, because we don't know exactly what it is.

19:04.120 --> 19:05.960
Some people hate that word.

19:05.960 --> 19:13.120
A selective filter is really kind of, you know, operationalization of this idea.

19:13.120 --> 19:17.920
I think there's been very little work, both in neuroscience.

19:17.920 --> 19:22.520
And well, I don't know it recently in machine learning, so truth I haven't really followed

19:22.520 --> 19:23.520
it.

19:23.520 --> 19:28.840
But in neuroscience, there's been very little work on how attention is learned from experience

19:28.840 --> 19:30.640
rather than from instruction.

19:30.640 --> 19:35.440
And all the work on attention is you tell the subject what to attend to, and then you figure

19:35.440 --> 19:39.360
out how is the brain doing that, how are they're focusing, how much are they sensitive

19:39.360 --> 19:41.080
to distractors, etc.

19:41.080 --> 19:43.080
And my question is completely different.

19:43.080 --> 19:48.400
And the place where I said that there aren't any algorithms for is to try to, so what

19:48.400 --> 19:52.240
we want is to predict attention, and we want to test our models.

19:52.240 --> 19:54.920
We want to evaluate how well our models are doing.

19:54.920 --> 20:01.720
So with choice data, which a lot of our experiments, we have subjects choosing actions.

20:01.720 --> 20:08.400
There's a whole wide range of models for modeling choices and comparing between models and

20:08.400 --> 20:12.200
saying, you know, this model is better than that model in predicting choice.

20:12.200 --> 20:16.440
What is missing for me is models that predict attention and a way to compare those.

20:16.440 --> 20:19.840
And the difference here, I mean, this is a little bit going into the weeds.

20:19.840 --> 20:24.000
But if you think of attention as a quantity that sums up to one, if I attend more to one

20:24.000 --> 20:30.720
thing, I can't attend more to everything else, this is a quantity that lives on the simplex.

20:30.720 --> 20:36.880
And it's a whole different statistics and geometry and comparison world on the simplex.

20:36.880 --> 20:40.280
And people haven't really worked on that because of this constraint that everything adds

20:40.280 --> 20:41.280
up to one.

20:41.280 --> 20:44.080
It's much, it's, it's, you need a different math.

20:44.080 --> 20:45.080
Okay.

20:45.080 --> 20:49.120
We've found a little bit of this math of all places in geology.

20:49.120 --> 20:55.280
Apparently, in geology, geologists want to ask, is this rock the same as that rock by

20:55.280 --> 21:01.200
looking at its composition and saying, you know, it's 80% this material and 15% this material

21:01.200 --> 21:02.400
is at the same as that one.

21:02.400 --> 21:07.000
So they do these comparisons of percentages and there's a whole book act to the on this

21:07.000 --> 21:09.400
compositional models.

21:09.400 --> 21:11.400
So we're reading that now.

21:11.400 --> 21:12.400
Wow.

21:12.400 --> 21:15.400
That's where we're trying to find our new math.

21:15.400 --> 21:16.400
Okay.

21:16.400 --> 21:17.400
Interesting.

21:17.400 --> 21:21.880
I talked about, you mentioned reinforcement learning in our conversation as well as in

21:21.880 --> 21:23.040
your talk.

21:23.040 --> 21:28.560
But when you, when I hear you say it, it's almost like the context is you're talking about

21:28.560 --> 21:32.400
the machine learning reinforcement learning, but you're also talking about like biological

21:32.400 --> 21:33.400
reinforcement.

21:33.400 --> 21:34.400
Am I reading that right?

21:34.400 --> 21:35.400
Yeah.

21:35.400 --> 21:36.400
Yeah.

21:36.400 --> 21:39.760
So reinforcement learning and psychology is called classical conditioning and instrumental

21:39.760 --> 21:40.760
conditioning.

21:40.760 --> 21:41.760
Okay.

21:41.760 --> 21:46.120
So the classic, you know, Pavlov with a dog, salivating, et cetera, that is learning

21:46.120 --> 21:49.360
values for states as in reinforcement learning.

21:49.360 --> 21:54.720
And then instrumental conditioning rats, lever pressing lever in order to get food, that's

21:54.720 --> 21:59.640
basically learning a policy that maximizes values and rewards.

21:59.640 --> 22:07.440
So there is a very kind of direct isomorphism or translation between the computational reinforcement

22:07.440 --> 22:13.280
learning world of learning policies and values and predictions and the behavioral psychology

22:13.280 --> 22:18.800
world and it goes through neuroscience or it goes to neuroscience because we also know

22:18.800 --> 22:24.720
in the brain where these different signals are computed and and and there's a lot of

22:24.720 --> 22:30.160
evidence this goes back to the 80s, actually, the 80s, no, the 90s.

22:30.160 --> 22:37.080
I think 94 was the first paper really making these parallels in particular dopamine, which

22:37.080 --> 22:42.880
is a really important neuromodulator in the brain that's involved in everything from

22:42.880 --> 22:52.880
Parkinson's disease to schizophrenia, depression, gambling, any drug abuse dopamine is seen

22:52.880 --> 22:59.440
today as representing in the brain, prediction errors, a lot of reinforcement learning.

22:59.440 --> 23:07.720
So literally, yeah, yeah, I'm often amazed that people don't know this because in neuroscience

23:07.720 --> 23:14.280
this is so big, this is basically one of the biggest success stories of taking machine

23:14.280 --> 23:19.200
learning, taking computational models and translating them to neuroscience and behavior

23:19.200 --> 23:25.560
is this is the idea that dopamine calculates a prediction error.

23:25.560 --> 23:30.120
So how different is what I'm getting from the world from what I predicted?

23:30.120 --> 23:34.600
This is a key quantity for learning and reinforcement learning, every reinforcement learning algorithm

23:34.600 --> 23:36.880
relies on prediction errors.

23:36.880 --> 23:39.720
We know behaviorally that animals learn from prediction errors.

23:39.720 --> 23:43.840
We know normally that dopamine represents these prediction errors and broadcast them to

23:43.840 --> 23:45.360
the whole brain.

23:45.360 --> 23:50.880
So anywhere in the brain, what does it even mean for dopamine to represent these prediction

23:50.880 --> 23:55.840
errors, meaning the levels of dopamine correlate strongly with prediction errors?

23:55.840 --> 24:02.360
Yes, it means that when in a task, I can through a computational model say, you probably

24:02.360 --> 24:07.000
just experienced a positive prediction error, so you expected, let's say, three MNM's and

24:07.000 --> 24:08.000
you got five.

24:08.000 --> 24:12.240
So through the model, I can track your learning and save with all the experience that you've

24:12.240 --> 24:14.920
had so far, you should be expecting three MNM's.

24:14.920 --> 24:16.960
You just got five, so it's a positive prediction error.

24:16.960 --> 24:22.360
If I record it from your brain, I would see a kind of positive burst of dopamine.

24:22.360 --> 24:28.200
So above baseline dopamine, whereas if my model says you should predict three MNM's and

24:28.200 --> 24:32.280
you actually get one MNM and I record from your brain, I'll see a negative dip.

24:32.280 --> 24:38.320
So I'll see less dopamine than the baseline dopamine levels at that exact time.

24:38.320 --> 24:41.280
It's a phasic, it's a short lived signal.

24:41.280 --> 24:42.720
It's broadcast all over the brain.

24:42.720 --> 24:46.880
It basically says at every point in time, are things better than expected right now or

24:46.880 --> 24:49.240
are things worse than expected?

24:49.240 --> 24:52.040
And AlphaGo relies on prediction errors.

24:52.040 --> 24:55.240
So all of reinforcement learning relies on prediction errors.

24:55.240 --> 24:56.560
And so we see that in the brain.

24:56.560 --> 25:00.360
So really when I say reinforcement learning, I'm thinking about the brain as much as I'm

25:00.360 --> 25:02.760
thinking about the algorithm.

25:02.760 --> 25:03.760
That's fascinating.

25:03.760 --> 25:04.760
Yeah.

25:04.760 --> 25:11.840
It's really an amazing case of convergence of all these lines of research.

25:11.840 --> 25:16.640
And is this a new observation or did you say 80s for this?

25:16.640 --> 25:17.640
94.

25:17.640 --> 25:20.440
I got not 80s, so 94.

25:20.440 --> 25:26.200
So in the 80s and the late 80s, the idea was that dopamine represents reward.

25:26.200 --> 25:28.680
So it's the brain signal for reward.

25:28.680 --> 25:32.320
And people started looking for that signal by recording from dopamine neurons, from

25:32.320 --> 25:37.160
monkey brains, as monkeys were obtaining rewards in a task.

25:37.160 --> 25:41.600
And they were really confused because sometimes dopamine would respond to the reward.

25:41.600 --> 25:42.920
Sometimes it didn't.

25:42.920 --> 25:47.560
And there were all these abstracts in the society for neuroscience saying, you know, this

25:47.560 --> 25:52.760
is clearly not a reward signal, but we have no idea what it is.

25:52.760 --> 25:58.000
And then Reed Montague and Peter Diane, who were then postdocs with Terry Sinowski at

25:58.000 --> 26:03.760
the Salk, the Salk Institute, read these papers and they'd been reading about reinforcement

26:03.760 --> 26:04.760
learning.

26:04.760 --> 26:08.400
So Rich Sutton and DiBarto's work.

26:08.400 --> 26:10.160
And they basically put two and two together.

26:10.160 --> 26:11.160
Wow.

26:11.160 --> 26:16.960
The story is that one day Reed Montague went to Peter Diane's office and said, look at this.

26:16.960 --> 26:20.160
This looks just like a prediction error signal.

26:20.160 --> 26:25.360
And then they published a paper they started from, you know, the politics of these things

26:25.360 --> 26:26.360
are weird.

26:26.360 --> 26:33.760
They started from a paper in science about bees, not about monkey, about bees navigating

26:33.760 --> 26:39.080
with the idea that a ketopamine, which is the equivalent of dopamine signals in insects,

26:39.080 --> 26:41.000
a ketopamine represents a prediction error.

26:41.000 --> 26:42.000
Okay.

26:42.000 --> 26:46.360
And then later they published a paper about that same thing in monkeys a year later in

26:46.360 --> 26:47.360
96.

26:47.360 --> 26:51.960
So this, this first paper was 95, I think there was an abstract in 94.

26:51.960 --> 26:59.480
And then in 97, the famous paper is Schultz Diane and Montague, 1997, where they published

26:59.480 --> 27:03.600
basically the recording data together with the model and said, and this was published

27:03.600 --> 27:07.480
in science saying dopamine seems to be a prediction error.

27:07.480 --> 27:13.480
And this was, you know, this was the hypothesis with some data and since then it's been tested

27:13.480 --> 27:15.760
a million times over.

27:15.760 --> 27:19.560
And on the one hand, it looks like sometimes it's so amazing.

27:19.560 --> 27:23.480
It looks like, you know, dopamine neurons must have read the text book, like they do exactly

27:23.480 --> 27:25.480
what you expect.

27:25.480 --> 27:30.120
And really, you know, convoluted situations, you set things up so that it should be, you

27:30.120 --> 27:33.680
know, whatever, and it is exactly that.

27:33.680 --> 27:41.640
But, you know, with neuroscience, the deeper you dig the more unexplained gold you find.

27:41.640 --> 27:45.760
So a lot of it fits the theory, some of it doesn't.

27:45.760 --> 27:49.320
And that, some of it is not isoteric stuff that we can ignore.

27:49.320 --> 27:52.600
It's not just noise, it's persistent differences.

27:52.600 --> 27:57.080
So the model, this idea is kind of a simplified.

27:57.080 --> 28:01.920
So dopamine, I believe that dopamine definitely represents a prediction error, but that's

28:01.920 --> 28:04.880
not the end of the story, that's the beginning of the story.

28:04.880 --> 28:10.720
And there's a lot of work trying to understand exactly how timing is represented in this

28:10.720 --> 28:15.960
system, and are these prediction errors only for reward or any kind of prediction that's

28:15.960 --> 28:17.280
violated?

28:17.280 --> 28:20.160
And if it's any kind of prediction that's violated, how do you know what to learn?

28:20.160 --> 28:21.640
How do you know what prediction was violated?

28:21.640 --> 28:27.240
Because when it's reward, it's kind of easy, you just update your prediction of reward.

28:27.240 --> 28:34.240
If it's anything, it becomes, you basically need more information in order to learn.

28:34.240 --> 28:38.040
It's making me wonder are there other chemicals that respond similarly that?

28:38.040 --> 28:42.720
Yeah, there aren't other chemicals in the brain that look like a prediction error signal.

28:42.720 --> 28:46.600
There are four neuromodulators in the brain, so there are lots of chemicals in the brain,

28:46.600 --> 28:52.840
but neuromodulators are signals that rather than communicating neuron to neuron kind of

28:52.840 --> 28:58.400
like a phone call, they're more like a PA system, they like broadcast very, very widely.

28:58.400 --> 29:02.840
So there's dopamine, norpinephrine, acetylcholine, and serotonin.

29:02.840 --> 29:07.720
And people have basically been dying to know what these four do because it's like, you

29:07.720 --> 29:12.560
know, if you had four broadcast systems, those are four things that you can tell everybody,

29:12.560 --> 29:15.400
what would you say with those four?

29:15.400 --> 29:21.000
And it's clear that all four are super critical in the sense that disrupting any of these

29:21.000 --> 29:25.480
systems causes a whole host of problems, which makes sense if they, you know, if they broadcast

29:25.480 --> 29:28.480
everywhere, they must be doing something really important.

29:28.480 --> 29:29.480
So dopamine is the best?

29:29.480 --> 29:33.440
So far only dopamine has been implicated in the learning process, or is that too strong

29:33.440 --> 29:34.440
statement?

29:34.440 --> 29:39.320
Dopamine is really important, that frame learns all the time, dopamine is the best understood.

29:39.320 --> 29:45.680
It seems like the simplest of the stories, but the others have been implicated in learning

29:45.680 --> 29:50.800
as well, so norpinephrine has actually been implicated in the breadth of attention, controlling

29:50.800 --> 29:55.160
the breadth of attention, controlling the gain of the system, controlling what you do or

29:55.160 --> 29:58.720
how do you respond to unexpected changes?

29:58.720 --> 30:05.200
So one of the ideas in learning is that when the world changes in an unexpected way, you

30:05.200 --> 30:10.640
should be able to kind of reset, to not continue to carry on previous learning, but to start

30:10.640 --> 30:15.680
a new, to reset your values, to increase your learning rate to say, okay, like, you know,

30:15.680 --> 30:17.480
I take a break here from the past.

30:17.480 --> 30:19.880
So norpinephrine is implicated in that.

30:19.880 --> 30:25.520
Acetyl colon is also implicated in adjusting learning rates to the variability of the environment,

30:25.520 --> 30:31.480
so an environment that's more variable, basically, it's more noisy, so every bit of information

30:31.480 --> 30:36.280
should carry less weight, so you should have a lower, a smaller step size in learning,

30:36.280 --> 30:42.160
we'll call it a smaller learning rate, so acetyl colon is implicated in that.

30:42.160 --> 30:47.920
And serotonin has been kind of, serotonin is so complicated, there are 20 different

30:47.920 --> 30:52.400
kinds of receptors for serotonin, and they do all kinds of things, I mean, all of these

30:52.400 --> 30:56.760
are complicated, everything that I'm saying is a gross simplification, but yeah, that's

30:56.760 --> 31:02.000
what we pay our neuroscientists for, you know, we're trying to figure it out.

31:02.000 --> 31:07.240
So I don't know, we've kind of strayed into like a lot of background material, which has

31:07.240 --> 31:10.920
been amazing, but you're, you're talk, right?

31:10.920 --> 31:17.080
So you're, I won't even try to like summarize it in a sentence, I'll let you do that.

31:17.080 --> 31:22.640
But it was a, well, no, that's what I would do to introduce it, you can actually kind

31:22.640 --> 31:27.440
of walk us through the framework that you presented and some of the experimental results

31:27.440 --> 31:29.560
and things like that.

31:29.560 --> 31:37.880
So kind of broad strokes, you're, you're kind of applying a Bayesian inference to learning,

31:37.880 --> 31:43.480
and at least what I got out of the talk was trying to identify, you know, these latent

31:43.480 --> 31:49.680
variables that are present in the way we kind of perceive the world through experimentation

31:49.680 --> 31:54.280
and relate that back to machine learning or statistical models.

31:54.280 --> 31:55.280
Yeah.

31:55.280 --> 32:00.160
So this goes back to what we talked about in the beginning of the podcast today, which

32:00.160 --> 32:06.600
is how do we parse the world into these, how do we put boundaries on learning and say

32:06.600 --> 32:10.680
all of these experiences are similar enough, I'm going to learn from one to another, I'm

32:10.680 --> 32:15.040
going to learn from this street to that street, and these are different, this is London,

32:15.040 --> 32:19.480
the cars come from the other side of the road, I definitely, definitely not generalized

32:19.480 --> 32:20.840
between these.

32:20.840 --> 32:26.520
And so what my talk was about is trying to identify and what my research is about, strike

32:26.520 --> 32:31.600
to identify the computational algorithms for putting these boundaries in place.

32:31.600 --> 32:38.000
Because I think that's, this is a computational way of talking about the issue of how do we

32:38.000 --> 32:41.160
take a complex problem and make it into a simple one.

32:41.160 --> 32:46.400
So when I cluster experiences together and say all these are similar, I'm basically saying

32:46.400 --> 32:49.040
I'm going to ignore all their differences.

32:49.040 --> 32:52.800
So that's part of like learning what to ignore, I'm going to say these are essentially,

32:52.800 --> 32:56.560
you know, in reinforcement learning, we would say, this is state one, right, we just

32:56.560 --> 32:59.080
give them a label, all of these things are state one.

32:59.080 --> 33:05.160
So now I don't have to, you know, I don't have to analyze all their minute differences.

33:05.160 --> 33:11.160
So I'm trying to understand how the brain decides what is state one, how the brain does

33:11.160 --> 33:13.360
this clustering.

33:13.360 --> 33:17.560
And it's really, it's not that I'm trying to identify what are the relevant aspects

33:17.560 --> 33:21.080
for each task that's, that's less interesting for me.

33:21.080 --> 33:27.320
I'm trying to, to understand what is this general purpose algorithm that can take complex

33:27.320 --> 33:32.080
input, like we talked about before, it can have potentially infinite dimensions that

33:32.080 --> 33:33.280
are relevant.

33:33.280 --> 33:39.600
And can easily say based on inputs one, two, and three based on these three dimensions

33:39.600 --> 33:43.280
and none, none other, I'm going to call this state one.

33:43.280 --> 33:47.120
And those three dimensions in a different scenario say that that one is state two and

33:47.120 --> 33:48.120
not state one.

33:48.120 --> 33:49.120
So that's what I want to understand.

33:49.120 --> 33:54.920
And I think because this is a NP hard, well, I don't know, NP hard, I haven't proven

33:54.920 --> 33:55.920
it.

33:55.920 --> 33:56.920
It's a very hard problem.

33:56.920 --> 34:00.520
Let's say this way, you can't actually use Bayesian inference for this.

34:00.520 --> 34:05.520
I gave, in my talk, I talked about the Chinese restaurant process prior.

34:05.520 --> 34:12.320
So a way to start from a prior that says there are a few latent causes for all of the observations

34:12.320 --> 34:13.320
that we see.

34:13.320 --> 34:17.360
And I'm going to try to cluster observations according to similarity and each cluster

34:17.360 --> 34:18.760
I'll call it a latent cause.

34:18.760 --> 34:21.360
And that will be, you know, my state one and state two.

34:21.360 --> 34:22.360
Is it?

34:22.360 --> 34:28.040
I was wondering this actually, what's the backstory for Chinese restaurant problem or process?

34:28.040 --> 34:34.160
Well, that comes from Stanford, you know, people who live in Palo Alto and go to Chinese restaurants.

34:34.160 --> 34:41.120
The backstory for this culinary metaphor is the idea that, so imagine you have a really

34:41.120 --> 34:47.560
large Chinese restaurant, like an infinite Chinese restaurant, and you think of each table

34:47.560 --> 34:50.280
as a cluster, a latent cause.

34:50.280 --> 34:52.440
And each customer is an observation.

34:52.440 --> 34:56.400
And the observation, so customers come in and they tend to sit at tables where a lot

34:56.400 --> 34:57.920
of people are already sitting.

34:57.920 --> 35:02.200
And that means that we tend to ascribe everything to a few latent causes.

35:02.200 --> 35:06.280
So a few clusters, we say, and we don't want to use all the tables out there.

35:06.280 --> 35:09.600
But once in a while, someone sits at a new table.

35:09.600 --> 35:11.200
So there's infinite capacity.

35:11.200 --> 35:15.960
It's an infinitely sized Chinese restaurant, but people tend to sit together.

35:15.960 --> 35:21.960
This is, you know, just the metaphor for an infinite capacity mixture model.

35:21.960 --> 35:22.960
There are others.

35:22.960 --> 35:24.560
There's the Indian buffet process.

35:24.560 --> 35:31.320
Because we're the same people, another culinary metaphor, and it's kind of similar to that.

35:31.320 --> 35:33.120
It's the only way she got to keep going, right?

35:33.120 --> 35:34.120
I know, I know.

35:34.120 --> 35:40.040
So in the Indian buffet process, you choose a number of dishes, and again, you're choosing.

35:40.040 --> 35:44.680
You tend to choose dishes, it's like an infinite buffet, and you tend to choose dishes that

35:44.680 --> 35:46.520
people have already chosen.

35:46.520 --> 35:47.720
But you're choosing several.

35:47.720 --> 35:52.080
So now the idea is that every observation has several latent causes, not only one.

35:52.080 --> 35:56.440
So in the Chinese restaurant, you only sit at one table, and here you're taking several

35:56.440 --> 35:57.440
dishes.

35:57.440 --> 35:58.440
Where are we?

35:58.440 --> 36:03.720
So I was saying, okay, so this is a Bayesian process.

36:03.720 --> 36:09.280
But even when we apply it to a really simple experiments, we have to approximate.

36:09.280 --> 36:11.840
It's not tractable.

36:11.840 --> 36:14.080
And I think the brain does even better than that.

36:14.080 --> 36:21.080
I think the brain doesn't even try to approximate closely a optimal Bayesian solution.

36:21.080 --> 36:23.720
I think the brain does something that just works.

36:23.720 --> 36:25.240
It might not be optimal.

36:25.240 --> 36:33.400
It's susceptible to biases or decision-making is not always correct, but it's vastly simplified.

36:33.400 --> 36:38.320
I think for the brain, simple is more important than optimal.

36:38.320 --> 36:45.320
And that's because the biggest constraint is, in my view, on learning a decision-making

36:45.320 --> 36:51.960
in real life, in real life, people and animals is time.

36:51.960 --> 36:52.960
We have tons of neurons.

36:52.960 --> 36:55.880
It's not that we don't have enough computational machinery.

36:55.880 --> 36:58.200
What we don't have is enough experience.

36:58.200 --> 37:03.040
Any task that we need to do 30,000 times to be able to do it correctly, and 30,000 is

37:03.040 --> 37:05.160
small for AI, right?

37:05.160 --> 37:11.160
Like millions and millions of trials, even reinforcement learning algorithms, usually thousands

37:11.160 --> 37:13.400
of trials is normal.

37:13.400 --> 37:16.120
We don't need thousands of trials to learn almost anything.

37:16.120 --> 37:23.600
Yes, to be a world-class chess player or to be a perfect athlete of something.

37:23.600 --> 37:24.920
You need a lot.

37:24.920 --> 37:25.920
That's skill learning.

37:25.920 --> 37:31.280
To learn, to just solve a task way better than chance, to learn to survive and not get

37:31.280 --> 37:33.480
run over by a car.

37:33.480 --> 37:35.640
It's just a handful sometimes.

37:35.640 --> 37:40.360
And I think, so what I'm really interested in and what I was trying to give a flavor for

37:40.360 --> 37:43.920
in this talk was, how does the brain solve this?

37:43.920 --> 37:47.880
And what I showed, I didn't show the how.

37:47.880 --> 37:53.760
I said, you know, in, as we don't know, it's not that I have some secret that I'm not

37:53.760 --> 37:54.760
telling.

37:54.760 --> 38:00.040
Was the idea with bringing up the Bayesian inference piece to say, it sounds like it wasn't

38:00.040 --> 38:05.600
to say this is, you know, a useful model necessarily, but more, whatever it is, it's

38:05.600 --> 38:08.520
much simpler than this, and this is the simplest that we'd have.

38:08.520 --> 38:16.000
Yeah, it's a useful model because it inspired, it's in the way that in our work, it inspired

38:16.000 --> 38:17.440
our experiments that I showed.

38:17.440 --> 38:21.280
So it gave us the idea that similarity is key.

38:21.280 --> 38:25.760
So whether it's that exact algorithm or something else, it gave us the idea that similarity

38:25.760 --> 38:29.920
between different observations is going to be key to clustering them or not, it gave

38:29.920 --> 38:35.080
us the idea that the clustering is key that basically, if you think of the real world,

38:35.080 --> 38:40.600
not an experiment, information comes in all the time, there has to be this meta decision

38:40.600 --> 38:41.600
in the brain.

38:41.600 --> 38:44.080
Is this something I know about, or is this new?

38:44.080 --> 38:48.360
If it's something I know about, what do I know, let me retrieve it from memory, let me

38:48.360 --> 38:52.000
act on it, let me update what I know if I find that things are different.

38:52.000 --> 38:55.520
If it's something new, well, let's observe the world and see what to do or let's choose

38:55.520 --> 38:57.880
an action randomly and see what happens.

38:57.880 --> 39:02.120
So there's always this tension between old and new, and that's what the Chinese restaurant

39:02.120 --> 39:06.200
process basically embodies for us or gave us this idea because in the Chinese restaurant

39:06.200 --> 39:10.400
process, you're asking, is this an old latent cause or a new latent cause when you see

39:10.400 --> 39:11.400
an observation?

39:11.400 --> 39:15.960
So I think that that gave us really deep insight, even though I'm not committed to that specific

39:15.960 --> 39:19.880
algorithm, I think that idea is real.

39:19.880 --> 39:24.720
That's how our memory works, that's how we organize information in our brain, is it

39:24.720 --> 39:26.200
older, is it new?

39:26.200 --> 39:29.600
And so it just made us think of this, like, what is doing that?

39:29.600 --> 39:34.200
This is a cognitive, a new cognitive process if you want because nobody had talked about

39:34.200 --> 39:35.840
this process before.

39:35.840 --> 39:40.800
So that's why the algorithm was so powerful for us, even if it's not exactly that one.

39:40.800 --> 39:46.880
And so the experiments that I showed were trying to probe the general idea, is similarity

39:46.880 --> 39:52.120
key for clustering in humans to answer is yes, is it key for how we organize our memory

39:52.120 --> 39:53.840
the answer is yes.

39:53.840 --> 40:00.960
And then the third experiment that I showed was looking at where are these clusters stored

40:00.960 --> 40:01.960
in the brain?

40:01.960 --> 40:06.000
And this was not so much a yes or no question.

40:06.000 --> 40:09.440
It was really, now it was, can we be our opportunistic about this?

40:09.440 --> 40:14.320
So if there is this clustering, if these states are that are learned by the brain are represented

40:14.320 --> 40:18.640
somewhere that we can read them out, then we can start tracking what is the algorithm

40:18.640 --> 40:20.000
by which they're learned?

40:20.000 --> 40:24.560
Because I still haven't answered the main question that I started my whole, my lab and my research

40:24.560 --> 40:28.960
career with, which is how do we learn these say representation?

40:28.960 --> 40:31.040
So I'm, you know, there are clues on the way.

40:31.040 --> 40:32.960
So now I'm thinking about it in this clustering way.

40:32.960 --> 40:35.440
Closest fiction, we're busy for a while, but.

40:35.440 --> 40:36.440
Yeah.

40:36.440 --> 40:39.320
Thinking about it in this clustering way, now I know where to read them out in the brain.

40:39.320 --> 40:46.520
Now I need to give human participants tasks or animals tasks where they are learning states

40:46.520 --> 40:51.480
on the fly, I can read out what they're representing at each point in time and try to understand

40:51.480 --> 40:56.160
what is the algorithm that modifies these state representations over time.

40:56.160 --> 40:58.160
Is it this Chinese restaurant process?

40:58.160 --> 41:00.320
Is it an approximation to it?

41:00.320 --> 41:02.280
Is it something different?

41:02.280 --> 41:09.480
And the specific example that you gave, I forget the problem, but you had subjects in

41:09.480 --> 41:15.800
a MRI and you were able to read out images from the specific implicated section of the brain

41:15.800 --> 41:20.400
in the orbit of frontal cortex, the area above our eyes, that's a really important area.

41:20.400 --> 41:27.320
And then just use those images as inputs to a predictive model that was shown to be

41:27.320 --> 41:30.280
better than chance, is that the right interpretation?

41:30.280 --> 41:35.000
So what I was doing is I was showing, it wasn't really a predictive model, it was basically

41:35.000 --> 41:40.360
using a classifier, a corrector machine, impact, you know, shopping through algorithms

41:40.360 --> 41:49.280
and nips, a corrector machine that basically takes the activation patterns in a magnet

41:49.280 --> 41:54.400
that measures activation patterns in the brain online as people are performing a task

41:54.400 --> 41:55.400
I could measure.

41:55.400 --> 41:56.400
What was that task again?

41:56.400 --> 42:02.280
The task was, it's kind of, you know, it's a little bit complex on purpose, it's a task

42:02.280 --> 42:07.080
where you have to judge, you see faces and houses that are overlaid on each other, and

42:07.080 --> 42:11.560
you have to judge whether the face is older young or whether the house is older young and

42:11.560 --> 42:14.560
there is this rule about whether you're judging faces and houses and-

42:14.560 --> 42:16.560
The point people to that one is pretty-

42:16.560 --> 42:17.560
Exactly.

42:17.560 --> 42:22.760
And you're basically, you're performing this task over time by keeping in mind which state

42:22.760 --> 42:25.200
of 16 different states you're in.

42:25.200 --> 42:28.400
And what we showed is that you can look at the orbit frontal cortex at the activations

42:28.400 --> 42:36.200
in an orbit frontal cortex and from that activity alone you can classify whether a person

42:36.200 --> 42:40.040
is now in state one or state two or state three of these 16 states.

42:40.040 --> 42:44.200
And so what that tells us is that that's a place where you can read out this state representation

42:44.200 --> 42:49.560
even when the representation is, it involves unobservable information, it's inferred, it's

42:49.560 --> 42:51.400
an internal cluster.

42:51.400 --> 42:56.120
And further you did some work that showed that no other places in the brain are likely

42:56.120 --> 42:57.960
to be storing this state information.

42:57.960 --> 43:02.160
Well there were other areas that were likely to be but we found-

43:02.160 --> 43:07.760
So a state representation for a task, for reinforcement learning task is a kind of a,

43:07.760 --> 43:12.200
it's a specific entity, the state you want it to be Markov so you want to have all the

43:12.200 --> 43:17.320
necessary information in even from the past in your representation right now.

43:17.320 --> 43:22.280
And from what I said before about generalization, you want it to not have any extraneous information.

43:22.280 --> 43:24.280
So it has to be a very specific thing.

43:24.280 --> 43:29.800
And that specific, all the relevant things and nothing but them, we found only the orbit

43:29.800 --> 43:30.800
of frontal cortex.

43:30.800 --> 43:37.520
Other brain areas, I mean the orbit of frontal cortex is pretty much as far from sensory input

43:37.520 --> 43:43.520
as can be in the sense that it doesn't get any sense, well it does get, it gets input

43:43.520 --> 43:50.400
from all of sensory cortex but it's not doing its own primary sensory processing.

43:50.400 --> 43:54.320
So a lot of other areas are representing parts of the state, you know, contributing

43:54.320 --> 43:56.640
that potentially to the orbit of frontal cortex.

43:56.640 --> 44:00.440
But I see the orbit of frontal cortex is kind of the final place that says, okay, now

44:00.440 --> 44:05.760
right now for this task, this is, this is my state, building on everything else.

44:05.760 --> 44:09.960
And yeah, and it was the only place in the brain that we found that kind of representation.

44:09.960 --> 44:12.560
Well, we're running long time.

44:12.560 --> 44:18.480
I am hoping that the videos from nips are going to be made available as recordings and so

44:18.480 --> 44:19.480
folks.

44:19.480 --> 44:22.760
I really encourage folks to take a look at your talk because we've really only kind of

44:22.760 --> 44:23.760
skim the surface.

44:23.760 --> 44:27.520
There are some really great experimental examples that you provided that I think folks

44:27.520 --> 44:33.000
would find interesting, but sort of that any kind of wrap up thoughts or places that you

44:33.000 --> 44:38.160
would want to, you know, send people or places that they should start if they're interested

44:38.160 --> 44:40.080
in this field.

44:40.080 --> 44:43.080
Can I take this to a slightly different place for a wrap up?

44:43.080 --> 44:44.080
Sure.

44:44.080 --> 44:45.080
Sure.

44:45.080 --> 44:47.880
So, I'm going to put one of my activism hats on.

44:47.880 --> 44:48.880
Okay.

44:48.880 --> 44:53.920
So in my field in neuroscience, we have a website called Bias Watch Nero.

44:53.920 --> 44:54.920
Okay.

44:54.920 --> 45:00.880
Bias Watch Nero.com, I think, or dot org, I think dot com.

45:00.880 --> 45:07.600
And in that site, what it does is it tracks the gender composition of conferences.

45:07.600 --> 45:11.880
So basically, the idea is for every conference in the field of neuroscience, including computational

45:11.880 --> 45:17.920
neuroscience, on that site, there will be a post of how many of the invited speakers.

45:17.920 --> 45:20.440
This is not contribute to talks, but invited.

45:20.440 --> 45:23.120
How many of the invited speakers are women?

45:23.120 --> 45:24.120
How many are men?

45:24.120 --> 45:26.840
What is the base rate for that specific conference?

45:26.840 --> 45:33.560
So they have a way in Bias Watch Nero calculating in a transparent way that anybody can recreate

45:33.560 --> 45:34.880
for themselves.

45:34.880 --> 45:39.280
What is the base rate of female faculty in that subfield?

45:39.280 --> 45:43.080
And the idea is, it's scientists, we know what a Bias sample is, right?

45:43.080 --> 45:48.600
We don't want to give our algorithms a Bias sample of what they need to learn from.

45:48.600 --> 45:52.880
So why are we giving our audiences a Bias samples of all the great ideas out there in science?

45:52.880 --> 45:56.840
If there are 20 percent women in a field, yes, you know, it would be better if there were

45:56.840 --> 45:57.840
50 percent.

45:57.840 --> 45:58.840
But there are 20 percent.

45:58.840 --> 46:01.760
Why do we have only 5 percent in our conferences?

46:01.760 --> 46:04.080
Why are we missing out on these great ideas?

46:04.080 --> 46:08.280
And this website has made a huge difference, I think, in neuroscience.

46:08.280 --> 46:12.480
And just a couple of years, the Bias has really gone down.

46:12.480 --> 46:18.360
And I think it would be great if computer science started a similar thing.

46:18.360 --> 46:24.440
So Bias Watch CS or something of that sort, including machine learning, including AI, because

46:24.440 --> 46:33.560
I think really it's even more of a pill climb for women in computer science related fields.

46:33.560 --> 46:36.520
And you know, diversity of ideas is good for everybody.

46:36.520 --> 46:38.480
We all want all the best ideas out there.

46:38.480 --> 46:44.560
We all want to, you know, make our, we want to progress as fast as possible with knowledge.

46:44.560 --> 46:50.360
And so, yeah, so, so it's kind of little, you know, call for activism in computer science

46:50.360 --> 46:51.360
in this field.

46:51.360 --> 46:56.120
And I, I know the people who started Bias Watch Neural, and I'm happy to help anybody

46:56.120 --> 47:00.120
who wants to start Bias Watch computer science, so they can just contact me.

47:00.120 --> 47:01.120
And yeah.

47:01.120 --> 47:02.120
All right.

47:02.120 --> 47:03.120
Great.

47:03.120 --> 47:04.120
What's the best way for them to contact you?

47:04.120 --> 47:05.120
Yeah.

47:05.120 --> 47:06.120
At Princeton.edu.

47:06.120 --> 47:07.120
That is so simple.

47:07.120 --> 47:08.120
Yes.

47:08.120 --> 47:17.200
You can spell Yael, which is Y-A-E-L, and Princeton has an E after the C. So people forget

47:17.200 --> 47:18.200
that.

47:18.200 --> 47:21.480
So yes, Yael, Y-A-E-L at Princeton.edu.

47:21.480 --> 47:22.480
Yeah.

47:22.480 --> 47:23.480
Thank you so much.

47:23.480 --> 47:24.480
Well, yeah, thank you.

47:24.480 --> 47:25.480
Thank you.

47:25.480 --> 47:26.480
It was a great conversation.

47:26.480 --> 47:27.480
I really enjoyed it.

47:27.480 --> 47:28.480
Thank you.

47:28.480 --> 47:32.520
All right, everyone.

47:32.520 --> 47:34.680
That's our show for today.

47:34.680 --> 47:39.640
Thank you so much for listening and for your continued feedback and support.

47:39.640 --> 47:44.800
For more information on Yael or any of the topics covered in this episode, head on over

47:44.800 --> 47:49.160
to twimlai.com slash talk slash 92.

47:49.160 --> 47:56.080
To follow along with the NIP series, visit twimlai.com slash NIPS 2017.

47:56.080 --> 48:02.720
To enter our Twimlai 1 Mill contest, visit twimlai.com slash twimlai 1 Mill.

48:02.720 --> 48:08.480
Of course, we'd be delighted to hear from you either via a comment on the show notes page

48:08.480 --> 48:13.680
or via a tweet to at twimlai or at Sam Charrington.

48:13.680 --> 48:18.200
Thanks once again to Intel Nirvana for their sponsorship of this series.

48:18.200 --> 48:22.920
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

48:22.920 --> 48:27.160
the AI arena, visit intelnervana.com.

48:27.160 --> 48:31.920
As I mentioned a few weeks back, this will be our final series of shows for the year.

48:31.920 --> 48:37.120
So take your time and take it all in and get caught up on any of the old pods you've been

48:37.120 --> 48:38.720
saving up.

48:38.720 --> 48:41.160
Happy holidays and happy new year.

48:41.160 --> 48:43.440
See you in 2018.

48:43.440 --> 49:11.200
And of course, thanks once again for listening and catch you next time.

