Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we're featuring a series of conversations from the Nips conference
in Long Beach, California.
This was my first time at Nips and I had a great time there.
I attended a bunch of talks and of course learned a ton.
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful
people, including some former Twimble Talk guests.
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should
take a second right now to subscribe to at twimblei.com slash newsletter.
This week, through the end of the year, we're running a special listener appreciation contest
to celebrate hitting 1 million listens on the podcast and to thank you all for being
so awesome.
Tweet to us using the hashtag Twimble1Mill to enter.
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and
other mystery prizes.
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill
for the full rundown.
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship
of this podcast and our Nips series.
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster
sessions, their big news this time was the first public viewing of the Intel Nirvana
neural network processor or NNP.
The goal of the NNP architecture is to provide the flexibility needed to support deep learning
primitives while making the core hardware components as efficient as possible, giving
neural network designers powerful tools for solving larger and more difficult problems
while minimizing data movement and maximizing data reuse.
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel
Nirvana.com.
This time around, I'm joined by Matthew Crosby, a researcher at Imperial College London,
working on the kinds of intelligence project.
Matthew joined me after the Nips symposium of the same name, an event that brought researchers
from a variety of disciplines together towards three aims, a broader perspective on the possible
types of intelligence beyond human intelligence, better measurements of intelligence, and
more purposeful analysis of where progress should be made in AI to best benefit society.
Matthew's research explores intelligence from a philosophical perspective, exploring ideas
like predictive processing and controlled hallucination and how these theories of intelligence
impact the way we approach creating artificial intelligence.
This was a very interesting conversation and one that I'm sure you'll get a kick out
of.
And now on to the show.
Alright everyone, I am here in Long Beach, California at the Nips Conference, and I have
the pleasure of being joined by Matthew Crosby, research associate at Imperial College London.
Matthew, welcome to the podcast, thank you very much.
So why don't we get started by having you tell us a little bit about your background
and how you got involved in AI.
So I was always very, very interested in intelligence as a concept, and I had a fairly mathematical
sort of upbringing and start, and so my first thought was, I'm going to explore intelligence
through mathematics and computer science, and so I worked for a while in high level planning
in AI, and over the course of the time working on that stuff, I sort of got a bit disillusioned
with the fact that AI was less about intelligence than I was hoping it would be.
And of course for me, intelligence always spoke in some sense to human-like intelligence,
to this disability to sort of experience the world and plan in the world, that's why
I was working in planning, but to form ideas about what you're going to do and represent
this picture of the world in front of you, and that was a fundamental part of intelligence.
No, not quite there yet.
Well, it wasn't being covered in the part of the field that I was working on, and then
as I was sort of becoming a bit disillusioned with that, I found these theories in philosophy
which I'd been dabbling in, which suddenly spoke to what I felt, you know, my experience
of the world was like, and what intelligence actually was, and it was the first time
when I thought, no, people are actually explaining this at a level where it's making sense to
me, and it's progressing us further to all the better understanding.
Okay.
So I just completely transitioned, went back and re-learned philosophy so that I could
understand these theories and philosophy of mind better.
And while I was doing that, we were seeing all these advances now in neural networks and
in deep learning and machine learning, where we are actually approaching something that
looks like intelligence in a way that we can talk about, especially philosophically,
that's interesting and like makes sense and does speak to what I was interested in in
terms of intelligence.
So now after making this transition to philosophy, I'm back in a sort of partial role in between
the two, looking at intelligence in AI, but from a more philosophical perspective.
And so how does that translate into a research path for you?
Well, one of the problems with that is that's obviously a giant research project.
So the project I'm on right now is the kinds of intelligence project, which is a sub-project
at the Level Hume Center for the Future for Intelligence.
This is a large 10 million pound project in the UK, mainly based at Cambridge, where
the idea is to take people from multiple disciplines and look at the future of intelligence
in the path we're on towards the future of intelligence and take expertise from AI,
but also from philosophy, political science, literature, and all the broad range of subjects
that could have something to say about intelligence.
And use it to shape research in the future, shape policy decisions, and make sure that
we are moving towards a future that's best for everyone in terms of intelligence.
The kinds of intelligence is also the name of a symposium that you could organize here
at NIPS.
Tell us a little bit about that and the goals for it.
So the goals of the kinds of intelligence project are to map this space of intelligence
so that we can better understand the intelligence landscape.
And the kinds of intelligence symposium and NIPS, the idea was to bring a lot of people
who are at the forefront of different types of intelligence together into the same place
and so we can have this conversation and look at the way the intelligence landscape is
shaped, both from plant cognition to animal cognition to child development, which are all
very important parts of understanding intelligence to obviously being at NIPS, AI and machine learning
and the type of intelligence we can now create.
We had people talking like Demis Hassebus from DeepMind, who's talking about alpha zero,
which is now solving, go and chess and show the levels way beyond human intelligence.
And we also had people talking about plant cognition and the way if you drop certain
plants from a height over time, they can learn a response to curl up in protection.
And falling on the floor, you drop them enough times, eventually they learn to anticipate
what is going to happen and curl up ahead of time.
Oh wow.
So this is a form of learning that doesn't have any neurons involved and it's very alien
to the type of learning that we generally think about.
Huh, interesting.
And so your personal kind of slice through this is from a philosophical perspective and
you mentioned some kind of a body of work or research within philosophy that you stumbled
across.
What is that?
So the general term for this is predictive processing and it's the idea that when we take
in sensory data from the world, we have this huge jumble of messy information coming into
the system, right?
We have 130 million or so photo receptors in each eye, all transducing electromagnetic
information and somehow the brain has to make sense of that and understand it.
And at some point, it's, understands it in a way that we sort of experience the world
and that's how that happened.
But the old sort of very old view was that this information comes into the system and
in a very bottom up way, it gets pieced together more and more and more complicated as it goes
up through the system and eventually you get ideas such as tables and chairs and the
kind of objects that we feel like that we see.
Predictive processing idea sort of turns that on its head and says, we're not in the
process of sort of taking these components of information and putting them together.
We're actively trying to work out what we're going to experience.
We're predicting the incoming sensory information and actively doing so, we're always trying
to work out what's going to happen next in the world.
And by turning it that way around and looking at how we could actively predict, we see
that our experience of the world takes the form of what Annalceta is called a controlled
hallucination.
And this phrase is becoming much more, much more popular nowadays.
So the idea is in a hallucination, you're just making stuff up, right?
Maybe your brain is making stuff up that's not really there and that's what you see,
right?
In a controlled hallucination, you're making stuff up just as you were in a real hallucination,
but it's controlled by the actual sensory data.
It's up.
So there's no real difference from me seeing this table in front of me right now to hallucinating
one except for the fact that there's a ground truth from the sensory data that is binding
it together.
So that hopefully when I see a table there, from your perspective, you're also seeing
a similar table.
So what are the implications of seeing, you know, seeing cognition as this controlled
hallucination process?
There's a huge number of implications from this and I think that's one of the beauties
of the theory and probably one of the potential downfalls of the theory too is that it can
apply it to so many different levels across the brain.
And also in relation to machine learning, we're seeing obviously a huge focus on predictive
algorithms and on generative models, which are generating predictions about the world
or the sensory data or the data that they're being input.
So we can think of this as like a very low level, like in the retina at the back of the
eye, we're doing what is called predictive coding, which is whenever I get say a particular
rod cell is hit by electromagnetic radiation, it has the amount of information of the intensity
of the light that it can transfer up further in the brain, right?
That could be a large number of different values that this takes.
But if instead of transferring that value, I look at what I would expect just by looking
locally at all the values of the rod cells or the cone cells around me, I can take the
average of that and see how much that particular cell is different from that average, then you
will get a much smaller number, which increases the bandwidth that you, well, decreases the
bandwidth that you need to use to send the same amount of information.
And so are you describing a theory of what is actually happening physiologically or are
you describing a modeling approach or this is a serious point happening in the eyes
or something?
A bit like that, yes, but so I was starting at the point where this is actually, yes,
we know this is happening at a neurophysiological level.
And I was going to move on to the complete other side where this approach can be applied
to beliefs and desires and where our beliefs and desires are updated in a similar sort of
predictive format.
But before I move on, one thing with this predictive coding approach to the retina is if
you look at it, it is very much like convolutions that we're seeing in machine learning.
And the way they work is a very similar approach.
They're obviously a bit more focused on invariance in the visual field and how we can apply the
same map at different locations, which is another area of this.
But they could also be used to do a similar approach to predicting local variations and
only transmitting information about that variation as opposed to the raw data itself.
So that's one level of it, but at the other level, we have the idea that our beliefs are
updated in a similar way and our high level understanding of the world is based on these
predictions that we're making.
And comparing to the data coming in and when it's wrong, we have two choices.
We either update our prediction and therefore change our model of the world and see the
world differently.
Or we act in the world and move around the world.
And that might make our prediction that was wrong, turn into a prediction that was
correct.
So for example, if I predict that table is off to my right, there's two ways I can make
that true.
Well, I can turn, I can change my prediction, I can be, no, it's wrong, it's to my left
and then update it that way.
Or I could move my head and that would make the say another way of making the same prediction
right.
And I think this is a way that we could bring actions and interacting with the world
into our understanding in, especially in machine learning and robotic, of how we can incorporate
this sort of predictive approach to the whole brain or the whole way of modeling the world
into a system that's not just understanding the world, but also acting in it and performing
task.
And therefore, being intelligent, hopefully.
Is your research kind of approaching this from a theoretical perspective exclusively
or is it there an experimental element or applied element as well?
So I have been running experiments with predictive coding style on your own network.
Okay.
So you coming out recently based on this structure where you have a hierarchical generative network
and each layer of the hierarchy is just trying to predict anything that the lower layers
have so far failed to predict.
So the first layer tries to predict the world, but it doesn't, it's not strong enough
by itself to fully model everything.
So then what it can't predict.
Can you give an example?
Can you give an example?
Kind of the experiment.
Like what it, what specifically is it trying to predict?
So there's been work on this in experiments from video data, from car images for videos
on top of a car moving around, driving around, you give it the first nine frames and ask
it to predict the 10th.
And then you can get fairly good results at predicting, you know, how the road is going
to have moved and the things that will have come into you.
Okay.
I've been experimenting with this in maze like three dimensional domains working on the
raw pixel inputs and predicting how the maze is going to update or how the pixels are
going to update as I move around this maze.
Okay.
I spoke with a researcher working on something very similar like she was looking at it from
the perspective of like embodied computer vision.
So not just fixed frames, but you know, fixed frames plus the ability to move the orientation
of the viewport, if you will, and one of the sets of experiments or use cases was this
kind of sensor mounted on a car that was changing direction and trying to do the
prediction.
Things like that.
Yeah.
Interesting.
And so you've got this scenario with, you know, the sick, the video case, you've got
this scenario where you've got the camera on the vehicle and you're trying to predict
ahead.
How does that then tie into this hierarchical structure that you were describing?
Well, the idea of the hierarchy is that you'll have the first layer of your network would
be not have enough potential space inside it to fully predict everything that's going
on.
It's going to have space to represent the full function of the mapping of the change of
the inputs over time.
So then anything that it can predict, you're not interested in that anymore because that's
sort of, that's done.
That gets, that gets sort of finished at that layer of the system.
But anything that it doesn't get predicted, this is called the prediction error, will get
sent up to the next layer of the system.
And this is the, if everything's working as intended, this is the parts that are a bit
more complex than could be predicted just really easily, like, you know, something this
pixel change always stays the same, so I'm just going to predict it stays the same.
But then something slowly moving across the visual field or the input space in this video
might be impossible to predict at that lower level, but at a higher level, once you've
removed the easy stuff and you've got access to more machinery for sure, layers deep into
the system, you might be able to predict that.
And the idea is that as you move up the hierarchy, you'll get more high level representations
of elements of the world that are changing.
So the low level would be very simple stuff, the high level could potentially be things
changing, you know, very slowly over time or changing, you know, more modeling, like
the intuitive physics going on behind the domain that you might expect.
Oh, this object's hitting something, so now it's going to change direction.
The lower level might think, oh, it's just going to continue going on in the same direction.
And then when it gets that wrong, the higher level, which hopefully has representation
of the physics, will be able to correct for that and say, no, this is how it's going
to change.
So what you're describing sounds like, you sounds like what I think of a, you know, maybe
a very deep convolutional net, right, that's going to figure out different things at different
levels.
How is it different, or are you doing things to kind of force it to learn certain aspects
in certain layers?
Yes, I think the beauty of the research is that at one level, you could think of it as
just as a very deep convolutional net.
But the one thing that is different is your focusing on this particular prediction
paradigm, which has its own nice properties to it.
So for example, if I, if I give you a noisy input, so if the video is full of noise, and
I'm trying to predict the next frame in a noisy video, yeah, if the noise is unbiased,
so that over time the average of the noise is zero, then the prediction automatically
filters that out at the very bottom layer of the structure.
And that doesn't get passed up to the higher levels, so they see less noisy input.
So you get powerful results just from moving to this prediction paradigm.
And also the second thing that is different is the main propagation from layer to layer
is the errors of the stuff that you can't predict, as opposed to just a more complex
combination of everything you've got so far.
Does that result in a network that has fundamental differences in properties than what you
might see in a typical CNN-like in terms of the density of the weights or the way the
layers are interconnected with one another?
Yeah, I think so far.
I mean, the research is fairly early here, and there will be, I don't really know the
answer to that question.
The answer is going to be yes, but I can tell you in good details exactly how this approach
differs from the others.
And I think the more you incorporate from different areas in machine learning techniques,
the more you are going to find that this approach looks like what we've got.
Right, right.
So the layers are trained into N, you're not training individual layer separately.
You can train the individual layer separately because each layer's input is just the error
from the layer.
Right.
Right.
And so when you, you set, you acknowledge this early, have you had any preliminary results
from this line of research?
There are many results that the idea works.
And the suggestion there is that if this is a good philosophical theory of how the brain
might be working, and then when we do implement it, you know, actually implement it in your
networks, we see positive results.
So that's just a sort of a backup like result to say, yeah, maybe the philosophical theory
is onto something, right?
Maybe this is a good idea.
So it can predict future frames with high accuracy, you know, just when moving around
in the main, but it's still early days to say how well that will be when we move it into
reinforcement learning type of experiments where we can compare to say to the art and say
how that learning that kind of representation helped.
Yeah.
The impression that I get from the conversations I've had recently on this and related topics
is that the, you know, our understanding of the brain and the neurophysiology and our
understanding of the machine learning are, you know, we're kind of one leap frogs the
other and then feeds back, you know, learnings to the other.
And it's kind of this, you know, iterative process, is that your sense as well?
Or as one area like, you know, far ahead of the other and, you know, for example, we
understand the brain a lot more than we do the, you know, the machine learning side and
machine learning continues to pull from that or the other way around or I think we understand
them in very, very different ways.
But there are a lot of people that are sort of on the cusp of between the two disciplines
that are grabbing stuff from one and pulling it into the other and grabbing stuff in the
other direction.
We've seen that work like convolutions, as I mentioned before, across these two disciplines
and they work on both side of the spectrum and what side did they come from, do you know?
I think it depends on your view pit.
So I was at the symposium, Gary Marcus was saying that perhaps Janlequin wasn't aware of
all the predictive coding type work in the retina and how convolutions might be applied
in low-level visual systems and he was just trying stuff and found something that works
that happens to be very, very much related.
Okay.
Yeah.
I think that depends who you ask.
Okay.
Interesting.
Interesting.
So you also mentioned earlier in the kind of the conversation of philosophy, theory of mind
more broadly.
Can you describe that and how that fits into all of this?
Yeah.
So at first, my, my intuitions about why intelligence is interesting are that it involves
introspection and thoughts and the ability to reason about the world and in the way we
do, which is in a sense in a sort of symbolic process, right?
We construct sentences, we have language that's obviously a very key component.
We construct sentences in our heads and we understand things through those sentences
sometime.
And it's really interesting to see how like modern work, it's starting to look at representations
of the world that in machine learning where we can answer questions in natural languages
applied to images, for example, we've seen relation nets where you take in an image
containing, you know, a few objects of different sizes and different locations and you answer
a natural language question such as is the red object to the left of the yellow object
to that kind of question.
And the way they work is, is to try and create a representation inside the network that
understands this relational kind of information, which is moving towards, in, in my opinion,
moving towards a more symbolic or at least potential for a symbolic understanding of the
world inside the standard machine learning algorithm.
My reaction to that from very little kind of reading in linguistics is that like part
of that's not the idea of, you know, thinking and sentences is not universally accepted.
Is that right?
Like, it's, you know, a lot of ways thought is more abstract than sentences, their experiments
about trying to remember the, you know, there's this line of thinking around, you know,
whether the degree to which language impacts thought and, you know, I think there's kind
of this popular belief that, you know, people think in their languages, but it's also been
disproven in a lot of ways.
Yeah.
I don't know too much about that area, but I do think that some kind of, not necessarily
language type processing, but symbolic level processing or processing where we understand
objects as entities that persist over time that we can therefore then label is going to
be necessary as we move towards more general types of intelligence, especially if we end
up on this track where it seems, at least there's a lot of key players in the field right
now pulling towards human like general intelligence, that that's going to be a key component.
And I think as we move towards it in AI machine learning, we will be able to answer those
questions better.
But for now, yeah, I'm not really sure.
What's kind of the future of your particular research, both from the philosophical side
as well as the machine learning side?
Yeah.
So we actually ended up talking about a sort of slightly small part of the research I've
been doing, which I think doesn't really necessarily reflect on that.
Let's dive further into your research.
Okay.
So one thing, Subin said at the symposium was that maybe at the moment we're at a pre-compernican
revolution for our understanding of intelligence.
So obviously in the concurrent revolution, we went from having humans at the center of
the universe with everything revolving around it to humans as no longer at the center.
And our understanding of intelligence seems very human-centric at the moment, or at least
the layperson or the everyday understanding of intelligence.
And we're seeing a sort of move away from this with the, you know, ideas of plant cognition
and also these ideas of AI systems where, as Demis was saying at the symposium, Alpha
Zero playing chess played very alien type of chess to him.
It wasn't a human-like way of playing.
It was a new type of playing.
So when we explore this intelligence landscape, humans are going to be a very, very tiny part
of that giant landscape.
And with AI, because everything is artificial, we have the ability to explore way beyond
the scope of this little area that, well, there's a very tiny area that biological life
potentially exists in this landscape.
And there's an even smaller area of the human life exists in this landscape.
But my particular interest, and I think the big question moving forward for AI is, when
will we create intelligences and what type of intelligences have some kind of moral
patience?
You'd have some kind of, some kind of what?
Moral patience.
So they are, patience in our moral understanding of them.
So they can, it can be ethically correct or wrong to put them in certain situations.
So for example, Nick Bostrom has this term, the mind crime where potentially we could create
conscious artificial entities into some kind of slavery because they're just doing task
for us.
Or we could create entities that just live a life of suffering because they're never
achieving their goals and it actually means something to say that they're suffering.
And I think we need to explore their space of intelligence and compare it to the space
of possible mind.
The type of intelligences, not all intelligences are minds.
We know that, that seems obvious.
But some of them are.
And we don't know right now whether it's just this little tiny, and it seems absurd,
you know, it would be very pre-capernica and to say it's just this little human dot that
is the space of possible minds where we need to map that onto the space of possible
intelligences.
So I see that as the broad research goal that's most important.
So beyond the tiny piece of it, we discover what are some of the other kind of concrete
research areas within that broad umbrella.
So I think that the most concrete question, at least for me, is how can we understand
intelligence in a way where we can then say about certain agents that are intelligent,
whether or not they have a mind.
But obviously that itself is a massively huge question and it's going to take results
from philosophy, from neuroscience, to get to the human understanding of, like, we
know humans have minds, so we can learn about minds from them, and then from AI and from
completely other side of the field to think about what different types of alien intelligences
or artificial intelligence could have.
Do we even have a functional definition of mind?
We don't even have a function with our definition of intelligence.
That was one of the great results I thought of their symposium was we've invited all
these people together to talk about intelligence in different ways and they've brought their
own expertise.
But that expertise, each comes with its own assumptions about what intelligence is, and
we even have this debate there with Alpha Zero now playing Go and Chess better than humans.
If I told you, however, Srendan is a really great chess player, you're automatically think
he's intelligent.
But now there's people saying, oh, Alpha Zero is not intelligent, chess is easy.
Which, I mean, there's not even an agreement on that front of what intelligence is, so
yeah, so minds is going to be harder than intelligence, and we haven't sort of intelligence
yet.
From the various folks involved in the symposium, are there patterns in the way they define
intelligence that are easy to characterize?
Like, is there...
I think so, yeah.
So there's a...
Pedro Domingo's tribes of AI, is there an intelligence version of that?
Yeah, because I think there's definitely a count that's...
Very interesting human-like intelligence, and what they tend to do is define intelligence
in terms of human intelligence, and then automatically assume that AGI, as we move to more general
intelligence, is going to be on a path towards human-like intelligence, because that's the
type they're very, very interesting.
And then on the other side of the spectrum, you've got this idea that intelligence is so
much more broad than as humans we could possibly imagine, so I think you have these sort of
two separate camps.
Okay.
Interesting.
So maybe going kind of circling back to, you know, the future and kind of how you push
all this forward.
How are you thinking about that today?
Having just finished your, you know, pulling together the symposium and bringing together
some of the, you know, folks that are kind of pushing this research forward.
So I'm thinking that it's never too early to start these asking these questions.
It might be too early to expect to have concrete answers to these questions, but it's definitely
the time that we can actually bring these different types of people together to have this
conversation.
Because although everyone has different understandings of intelligence, we're getting results
in these different fields that are comparable, and we can start comparing them and talking
about the issue.
So my feeling is very optimistic that this is the important and right area that we should
be working on, and that we are going to get results.
Now that we're at a state where AGI, you know, is as advanced as it is, and our understanding
of intelligence across the animal kingdom is that it is, but we can stop bringing these
things.
Awesome.
Awesome.
Well, Matt, thanks so much for taking the time to chat with me about what you're up
to.
I wish I had an opportunity to attend the symposium.
It sounds excellent, and I know that you had some really excellent speakers and participants.
So I'm looking forward to keeping up with the work of the group.
Thank you.
Thank you.
All right, everyone, that's our show for today.
Thanks so much for listening, and for your continued feedback and support.
For more information on Matthew or any of the topics covered in this episode, head on
over to twimmelaii.com slash talk slash 91.
To follow along with the NIP series, visit twimmelaii.com slash NIPs 2017.
To enter our twimmel 1 mil contest, visit twimmelaii.com slash twimmel 1 mil.
Of course, we'd be delighted to hear from you, either via a comment on the show news page,
or via a tweet to act twimmelaii or at Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series.
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in
AI Arena, visit intelnervana.com.
As I mentioned a few weeks back, this will be our final series of shows for the year.
So take your time and take it all in and get caught up on any of the old pods you've been
saving up.
Happy holidays and happy new year.
See you in 2018.
And of course, thanks once again for listening and catch you next time.
