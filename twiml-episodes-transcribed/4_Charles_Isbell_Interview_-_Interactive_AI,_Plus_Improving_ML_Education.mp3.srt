1
00:00:00,000 --> 00:00:16,480
Hello everyone and welcome to Twimble Talk, the podcast where I interview interesting people

2
00:00:16,480 --> 00:00:21,040
doing interesting things and machine learning and artificial intelligence.

3
00:00:21,040 --> 00:00:22,800
I'm your host Sam Charrington.

4
00:00:22,800 --> 00:00:27,600
We've got another great interview for you this time around but first a quick update

5
00:00:27,600 --> 00:00:30,880
on the drawing we've been running in conjunction with O'Reilly Media.

6
00:00:31,600 --> 00:00:37,040
As you know if you've listened previously O'Reilly Media is holding their first ever AI conference

7
00:00:37,040 --> 00:00:43,520
on Monday and Tuesday, September 26th and 27th in New York City. The conference will span both

8
00:00:43,520 --> 00:00:49,840
low-level talks on implementing AI and high-level talks on the impact of AI in society

9
00:00:49,840 --> 00:00:54,320
and I'm personally looking forward to speeches by AI luminaries such as

10
00:00:54,320 --> 00:01:00,640
Google's Peter Norvig, Facebook's Jan LeCoon, and Intel's Slash Nervana's Navine Row.

11
00:01:01,440 --> 00:01:05,360
And we're giving away a ticket to one lucky winner here two day.

12
00:01:06,400 --> 00:01:13,040
In addition, right after the AI conference on Wednesday and Thursday, the 28th and 29th

13
00:01:13,680 --> 00:01:19,440
is the O'Reilly Stratta Plus Hadoot World Big Data Conference which is one that I've been attending

14
00:01:19,440 --> 00:01:25,440
for years now. You may have heard me mention this one before. Stratta is a much bigger event

15
00:01:25,440 --> 00:01:30,880
and while it's not strictly focused on AI, there are tons of really interesting AI machine learning

16
00:01:30,880 --> 00:01:36,720
talks at Stratta as well. Along with talks focusing on what I consider to be the core topics of that

17
00:01:36,720 --> 00:01:44,080
event, data infrastructure and data engineering. And O'Reilly has been kind enough to offer us

18
00:01:44,080 --> 00:01:49,840
a ticket to Stratta as well, which we'll be giving away today. So about that giveaway.

19
00:01:50,640 --> 00:01:56,480
If you went ahead and entered into the contest via either Twitter or the Twimlai.com website

20
00:01:56,480 --> 00:02:02,080
before the cutoff date, your name or Twitter ID went into a spreadsheet and you actually

21
00:02:02,080 --> 00:02:07,760
had a pretty good chance of winning as far as giveaways go. I chose winners using a random

22
00:02:07,760 --> 00:02:14,320
number generator to pick four numbers in the range of my spreadsheet rows. The first winner

23
00:02:14,320 --> 00:02:21,520
who was lucky number 17 is Lance Pool who entered via Twitter. Lance gets to choose either conference

24
00:02:21,520 --> 00:02:28,800
ticket for his prize. The second prize winner is Yinka also from Twitter and he gets the ticket

25
00:02:28,800 --> 00:02:35,040
remaining after Lance's choice. I've also chosen two runner-ups who may be called upon to

26
00:02:35,040 --> 00:02:41,440
fulfill the duties of one of our winners if either of them can attend the event. Our first runner-up

27
00:02:41,440 --> 00:02:49,040
is Samuel W and our second runner-up is Dennis A who both happened to have entered via the Twimlai.com

28
00:02:49,040 --> 00:02:56,560
site. If you hear this any of you please reach out to me to claim your prize. Now if you didn't

29
00:02:56,560 --> 00:03:02,800
win it's not too late to save 20% on your registration for either conference. You can do that by using

30
00:03:02,800 --> 00:03:10,640
the registration code PCTWIML when registering and I'll include a link to the registration page

31
00:03:10,640 --> 00:03:17,440
in the show notes. On behalf of the podcast and our partner O'Reilly thank you to everyone who

32
00:03:17,440 --> 00:03:29,760
entered and now on to the show. All right folks I am super excited to bring you this interview.

33
00:03:29,760 --> 00:03:35,920
My guess this time is Charles Isbel Jr., Professor and Senior Associate Dean in the College of

34
00:03:35,920 --> 00:03:42,400
Computing at Georgia Institute of Technology. Charles and I go back a bit and in fact he's the first

35
00:03:42,400 --> 00:03:49,360
AI researcher I ever met. His research focus is what he calls interactive artificial intelligence,

36
00:03:49,360 --> 00:03:54,640
a discipline of AI specifically focused on the interactions between AI's and humans.

37
00:03:54,640 --> 00:04:00,160
Charles and I spent a good chunk of time in our interview exploring what this means and some of

38
00:04:00,160 --> 00:04:05,200
the interesting research results in this field. One part of the discussion I found particularly

39
00:04:05,200 --> 00:04:10,160
interesting was the intersection between his AI research and the related fields of marketing

40
00:04:10,160 --> 00:04:16,320
and behavioral economics. Beyond his research Charles is well known in the ML and AI worlds for his

41
00:04:16,320 --> 00:04:22,080
popular machine learning course on Udacity which he teaches with Brown University Professor Michael

42
00:04:22,080 --> 00:04:28,400
Littman. In addition Charles helped launch the online masters of computer science program at

43
00:04:28,400 --> 00:04:33,040
Georgia Tech. We spend quite a bit of time talking about what's really missing in machine learning

44
00:04:33,040 --> 00:04:38,960
education and how to make it more accessible. Of course I'll be linking to Charles and the

45
00:04:38,960 --> 00:04:45,440
resources we mentioned in the show notes which you'll be able to find at twomolei.com slash talk

46
00:04:45,440 --> 00:04:53,920
slash four and now on to the interview. All right everyone so I'm here with Charles Isbell. Charles

47
00:04:53,920 --> 00:05:00,320
is senior associate dean and professor at Georgia Tech and actually Charles and I go way back.

48
00:05:00,320 --> 00:05:05,040
So this has been a weird conversation because we're already like 20 minutes in and just getting

49
00:05:05,040 --> 00:05:10,800
started with the interview. Charles say what's up to everyone and we'll get started.

50
00:05:11,680 --> 00:05:15,360
What's up everyone how are you doing? I'm happy to be here. I'm happy to be having this conversation.

51
00:05:15,360 --> 00:05:23,120
Awesome well thank you so much for joining us for this interview now. I think we figured out that

52
00:05:23,120 --> 00:05:29,760
that it's been like 20 something years since we met and that was pretty interesting in that

53
00:05:29,760 --> 00:05:39,120
we were roommates during a I guess summer internships at Bell Labs and that was when you were at

54
00:05:39,120 --> 00:05:46,640
MIT and studying AI. Tell us a little about your experience at MIT and what you said you're in the

55
00:05:46,640 --> 00:05:53,520
famous AI lab there right? I was although the AI lab no longer exists it merged with the laboratory

56
00:05:53,520 --> 00:06:01,200
for computer sciences now known as CSAIL. So I love my time at MIT and I love my time at Bell Labs

57
00:06:01,200 --> 00:06:07,200
and eventually AT&T labs. Sort of my journey through AI is a I don't know it's a bit of a

58
00:06:07,200 --> 00:06:12,800
wandering one so here I'll just give you my entire history up to now in like 15 seconds and we'll

59
00:06:12,800 --> 00:06:17,840
see how that goes. So as you can tell by my accent I was born in Chattanooga Tennessee but my

60
00:06:17,840 --> 00:06:22,400
earliest memory is arriving in a moving truck at the age of three and a half in Atlanta so I think

61
00:06:22,400 --> 00:06:27,120
of myself as being from Atlanta. But very very early on I cared a lot about computers and computer

62
00:06:27,120 --> 00:06:31,040
science and I knew when I was eight years old that I was going to do computer science although I

63
00:06:31,040 --> 00:06:35,040
didn't know what it was. I knew I was going to be a professor although I didn't know what it was

64
00:06:35,040 --> 00:06:39,360
and I knew I was going to do AI even though I had no idea what that was. Something about building

65
00:06:39,360 --> 00:06:45,120
robots and yeah eight years old. You know it took me a very long time to realize that not everybody

66
00:06:45,120 --> 00:06:49,040
thought they knew what they wanted to do and they were eight years old. I think I was probably a

67
00:06:49,040 --> 00:06:54,720
senior in college before I realized this but I had always sort of wanted to to build intelligent

68
00:06:54,720 --> 00:06:59,040
things although I couldn't have articulated that way when I was eight years old but I always wanted

69
00:06:59,040 --> 00:07:02,480
to build smart things I always thought I thought the computers were great at least what I thought

70
00:07:02,480 --> 00:07:07,600
computers were and I basically just wanted to build you know an intelligent friend that's

71
00:07:07,600 --> 00:07:12,640
basically what I was into at the time and so everything I kind of did from at that point on was

72
00:07:12,640 --> 00:07:20,000
about that. My actual first encounter with Bell Labs long before we met I was things the summer

73
00:07:20,000 --> 00:07:26,480
before ninth grade so I was 13 years old or so and I built a computer at Bell Labs as a part of

74
00:07:26,480 --> 00:07:31,040
this the summer science program. What I say I built a computer I mean there was a kit and another

75
00:07:31,040 --> 00:07:35,200
engineer did all of the work while I stood there and watched it but it felt like I was building

76
00:07:35,200 --> 00:07:45,200
the thing. It was a time X and Claire T 1000 and I remember. Yeah it was a little chicklet thing

77
00:07:45,200 --> 00:07:49,440
and it didn't have an on-off switch so when you turned it off you had to unplug it. It was great

78
00:07:50,320 --> 00:07:57,280
and the first program I ever wrote was a piece of code that would fill up the screen with inverse

79
00:07:57,280 --> 00:08:02,240
spaces and it ran out of memory before it could finish doing it and that was my introduction to

80
00:08:02,240 --> 00:08:07,280
real computer so you know that that's what I figured I needed to fix that and so that whole summer

81
00:08:07,280 --> 00:08:12,000
we spent well the two or so weeks that I was there for that program I spent a lot of time trying

82
00:08:12,000 --> 00:08:16,080
to figure out how how to make computer smart and how to make them do what you wanted to do and it

83
00:08:16,080 --> 00:08:21,040
just verified for me that that's what I wanted to do for all of my life so I kind of dove in from

84
00:08:21,040 --> 00:08:25,360
there and I kept getting you know bigger and better computers and convincing my mom that you know

85
00:08:25,360 --> 00:08:30,480
an Apple 2GS was the right thing and it was the best thing she could do for my education she kind

86
00:08:30,480 --> 00:08:35,360
of nodded politely eventually gave me the things that I wanted and I sort of moved through and one

87
00:08:35,360 --> 00:08:40,800
of the advantages of knowing what you want to do with your life is that you sort of moved towards it

88
00:08:40,800 --> 00:08:46,000
there's some disadvantages we can talk about those but really admit that you know I knew I wanted

89
00:08:46,000 --> 00:08:50,720
to go to Georgia Tech because I wanted to stay in Atlanta and I thought that it was the best place

90
00:08:50,720 --> 00:08:57,280
for me to be so I went to Georgia Tech as an undergrad I completely dove into AI didn't do a lot

91
00:08:57,280 --> 00:09:01,520
of research at the time because that you know in the the 1980s it was a little there weren't as many

92
00:09:01,520 --> 00:09:06,400
places where you could do the kind of research that you can do now as an undergrad that no matter

93
00:09:06,400 --> 00:09:10,560
sort of what you're into and then decided well there was basically one place for me to go to grad

94
00:09:10,560 --> 00:09:19,840
school and I applied to MIT and I went to MIT and I wrote this long essay about building robots and

95
00:09:19,840 --> 00:09:24,400
trying to make them smart and and trying to make certain that they wouldn't run out of memory and

96
00:09:24,400 --> 00:09:32,080
it was a it was a lot of fun so I ended up going to MIT immediately started diving into machine

97
00:09:32,080 --> 00:09:36,480
learning which at the time was sort of new for me I knew about AI and I knew I wanted to build robots

98
00:09:36,480 --> 00:09:42,000
but it didn't occur to me that you needed to do something separate to make machines learn and

99
00:09:42,000 --> 00:09:47,040
I decided almost immediately once I was exposed to it that this was the central question you couldn't

100
00:09:47,040 --> 00:09:51,360
be smart unless you could learn right and our machines were never going to be able to do the

101
00:09:51,360 --> 00:09:55,920
interesting things that I wanted them to do when I was eight nine years old unless they were smart

102
00:09:55,920 --> 00:10:00,800
enough to learn how to do them on their own and so I dove into that became a part of the AI lab

103
00:10:01,600 --> 00:10:06,480
went through a couple of advisors I'm still good friends with with all of them and eventually

104
00:10:06,480 --> 00:10:12,880
ended up where I was the side story where we met is I at the same time that I was going through grad

105
00:10:12,880 --> 00:10:17,840
school I got to go to bill labs every summer so part of this this fellowship program you know

106
00:10:17,840 --> 00:10:23,760
all about this of course and there I did a lot of really interesting things in AI that had

107
00:10:23,760 --> 00:10:29,680
absolutely nothing to do with what I was doing in grad school but it was so interesting what they

108
00:10:29,680 --> 00:10:33,520
were doing they're trying to build these knowledge representations and kind of really understand

109
00:10:33,520 --> 00:10:38,080
how it is you could think and you could represent thought that I just you know at the time it felt

110
00:10:38,080 --> 00:10:41,680
okay that I wasn't making progress in grad school because I was still getting to do these cool

111
00:10:41,680 --> 00:10:49,520
things and so by the time we met I was doing six months out of the year at Bell Labs and six

112
00:10:49,520 --> 00:10:54,960
months out of the year at MIT more or less oh wow I don't think I realized that at the time

113
00:10:55,840 --> 00:11:00,560
yeah because I take four and a half months over the summer I'd start before everyone else and I

114
00:11:00,560 --> 00:11:07,200
would end after everyone else and I would go back during the winter breaks okay okay uh so

115
00:11:07,200 --> 00:11:15,360
the I think the time that you kind of came up in AI was during the quote unquote AI winter

116
00:11:15,360 --> 00:11:20,720
is that right more or less yeah I was just sort of at the tail end of the AI winter nobody told

117
00:11:20,720 --> 00:11:27,280
me that I didn't figure that out until much later so how was that impacted your and your

118
00:11:27,280 --> 00:11:34,160
contemporaries perspective on AI and and the work you've done and how do you like what do you

119
00:11:34,160 --> 00:11:42,400
think about the current popularity of AI and where it's all going so I think basically

120
00:11:43,200 --> 00:11:48,160
what it's mainly done is it the people who are about my age and a little bit older who

121
00:11:48,160 --> 00:11:51,840
live through the AI winter I think basically spend a lot of their time wondering when the next

122
00:11:51,840 --> 00:11:56,720
AI winter is going to come so a lot of us are very very sort of naturally and reflexively worried

123
00:11:56,720 --> 00:12:02,240
that we're overhyping what's going on right it was it wasn't that it was difficult to get funding

124
00:12:02,240 --> 00:12:06,160
it wasn't that it wasn't it was difficult to do work it wasn't that there weren't people

125
00:12:06,160 --> 00:12:10,880
interested in the problem that we're interested in it's that any minute now the federal government

126
00:12:10,880 --> 00:12:14,240
would take away all of the funding and we would you know we would go from having 10 graduate

127
00:12:14,240 --> 00:12:18,000
students to having two graduate students and I kind of think that little fear is always there

128
00:12:18,000 --> 00:12:22,720
in the in the back of our heads and we find ourselves thinking please stop overhyping deep

129
00:12:22,720 --> 00:12:27,760
neural networks or you know getting people convinced that we're gonna uh we're gonna build

130
00:12:27,760 --> 00:12:32,800
the next data or the you know the next android and self-driving cars and any minute it can all kind

131
00:12:32,800 --> 00:12:37,120
of go wrong so I think it's probably made us somewhat more cautious at least it's made me

132
00:12:37,120 --> 00:12:40,720
somewhat more cautious and trying to think a little bit about the hype that's sort where

133
00:12:40,720 --> 00:12:46,160
where it's kind of driven me uh but you know the other advantage of being a part of you a part

134
00:12:46,160 --> 00:12:50,480
of sort of AI when it was during the AI winter is that you knew that you and the people you were

135
00:12:50,480 --> 00:12:54,720
talking to were in it because you were truly passionate and motivated about solving the problem

136
00:12:54,720 --> 00:12:58,960
as opposed to starting a company that would make you really rich or you know this is the hot thing

137
00:12:58,960 --> 00:13:03,200
you were doing it because you you actually cared about it and and I think that you know that's

138
00:13:03,200 --> 00:13:06,640
important right certainly when you're when you're doing research you have to be passionate about the

139
00:13:06,640 --> 00:13:10,800
the things that you you're doing and really believe that somehow it's gonna get you someplace

140
00:13:10,800 --> 00:13:19,200
interesting. And so do you think the fear notwithstanding do you feel like the is the industry

141
00:13:19,200 --> 00:13:25,200
structured in the same way such that the risk is the same or is it different and in particular

142
00:13:25,200 --> 00:13:32,480
I'm thinking about is there you know the funding sources more distributed now is the level of

143
00:13:32,480 --> 00:13:39,120
industrial activity you know more greater now or is it all you know from a research perspective

144
00:13:39,120 --> 00:13:43,840
all still fundamentally the government funding everything and you know when he decided to change

145
00:13:43,840 --> 00:13:52,480
there when the winds changed there at all collapses. Well I think structurally two things have

146
00:13:52,480 --> 00:13:59,440
happened one is computers computing and that sort of way of of crunching things and data are now

147
00:13:59,440 --> 00:14:04,480
ubiquitous they're everywhere so industry is deeply into this it's not going away

148
00:14:05,120 --> 00:14:10,960
Google exists right and everything is driven by data and it turns out that the parts of AI the

149
00:14:10,960 --> 00:14:15,760
parts of computer vision the all the sort of pieces of of building intelligent things they're

150
00:14:15,760 --> 00:14:20,560
driven by data now and since we everyone has access to data and everyone has access to

151
00:14:21,440 --> 00:14:26,480
computing everyone has access to really fast machines I'm not worried about sort of it structurally

152
00:14:26,480 --> 00:14:31,680
going away in fact the problem is sort of the opposite it's that everyone has a piece of it now

153
00:14:31,680 --> 00:14:37,040
it's it's driven as much by commercial interest as it is by sort of pure research and so really the

154
00:14:37,040 --> 00:14:42,240
difficult thing in some ways is that there's so many opportunities to do the what I would have

155
00:14:42,240 --> 00:14:45,680
thought of as AI what we would talk about is machine learning and those kinds of related things

156
00:14:45,680 --> 00:14:51,840
that it's easy for things to become diffuse in a way that wasn't true 25 years ago I don't think

157
00:14:51,840 --> 00:14:57,520
this is a bad thing I mean the the fact that Facebook exists the fact that Google exists the

158
00:14:57,520 --> 00:15:01,600
fact that everything is about your about data and about you know sort of modeling what people are

159
00:15:01,600 --> 00:15:05,840
doing and what things are happening is definitely a good thing and it does mean that there's always

160
00:15:05,840 --> 00:15:10,640
going to be funding for some piece of it even if it's not being called AI or it's not being

161
00:15:10,640 --> 00:15:16,560
called machine learning the kind of ideas metastasized so I'm not really worried about it going away

162
00:15:16,560 --> 00:15:24,000
the only thing that worries me is that people are concerned that bad things will happen because

163
00:15:24,000 --> 00:15:27,840
of what we're doing and for good reasons right they're concerned about their privacy we now have

164
00:15:27,840 --> 00:15:32,480
all these ability it's ability to track everything that you do I guarantee you Google is well aware

165
00:15:32,480 --> 00:15:35,760
that you and I are having this conversation right now they probably know what we're going to say

166
00:15:35,760 --> 00:15:40,560
before we say it you know they've got more data on us than you can possibly imagine and truthfully

167
00:15:40,560 --> 00:15:47,120
we I'm not entirely sure that we mind Facebook knows everything about us their companies out there

168
00:15:47,120 --> 00:15:51,120
neither was it heard of who know kind of everything about us as a people worried about privacy they're

169
00:15:51,120 --> 00:15:56,560
also worried about cars running off the road and killing other people they're worried about robots

170
00:15:56,560 --> 00:16:03,040
you know rising up in terminator style killing us all so the the kind of fears is the hype has

171
00:16:03,040 --> 00:16:07,120
actually gotten to the point of not what you haven't given us what we promised it's that you've given

172
00:16:07,120 --> 00:16:13,200
us more than what we asked for I think that's where the danger is coming from now but in terms of

173
00:16:13,200 --> 00:16:17,760
funding in terms of people being interested in these problems now that's driving everything even

174
00:16:17,760 --> 00:16:25,680
things you don't think of as being AI or being machine learning. It's interesting that in some

175
00:16:25,680 --> 00:16:32,240
ways it's in some ways the industry's given more in some ways like we're still waiting like

176
00:16:32,240 --> 00:16:40,720
you know if you if you survey sci-fi and you know even the Jetsons you know where where sci-fi thought

177
00:16:40,720 --> 00:16:46,800
we would be in you know 2016 and a lot of ways where we're not there yet right like a lot of

178
00:16:46,800 --> 00:16:53,840
movies would have had the self-driving cars all over the street but some of this stuff it takes

179
00:16:53,840 --> 00:17:00,160
longer it takes longer to develop than you think and some of the stuff is happening quicker than

180
00:17:00,160 --> 00:17:04,800
you think. No I think well some of the things are happening that nobody ever thought about I mean

181
00:17:04,800 --> 00:17:08,960
you go back and you start thinking about sci-fi it wasn't self-driving cars or self-driving jetpacks

182
00:17:08,960 --> 00:17:15,760
right it's still haven't got my jetpack yet I'm still waiting for that and it's true we we haven't

183
00:17:15,760 --> 00:17:20,720
gotten the flying machines we haven't gotten the really the smart butlers that are that are

184
00:17:20,720 --> 00:17:24,720
taking us everywhere another thing we've gotten a lot of other things right we've got access to

185
00:17:24,720 --> 00:17:30,160
information that we've never had access to before we can ask questions and we'll get the answers

186
00:17:30,160 --> 00:17:35,360
back we can look up anything we want to we can teach ourselves we've gotten a lot more things we

187
00:17:35,360 --> 00:17:39,520
never thought about than we thought we would and we've gotten less of the kind of obvious things

188
00:17:39,520 --> 00:17:45,760
that that I think people sort of hoped that we would one day get so you know it's a mix I'm okay

189
00:17:45,760 --> 00:17:51,360
with that I mean I I people ask me all the time you know when are the computers going to achieve

190
00:17:51,360 --> 00:17:56,640
sentience and and and take over the world and I think the answer is probably never or at least

191
00:17:56,640 --> 00:18:01,120
probably not for a very very long time not in the way that people think about it but we're going

192
00:18:01,120 --> 00:18:07,520
to have very smart machines and we already do doing a whole lot of things for us that we never

193
00:18:07,520 --> 00:18:12,400
sort of expected them to do and the interesting thing is we won't even notice and it won't seem

194
00:18:12,400 --> 00:18:18,880
like that big of a deal I mean for example with the Tesla and the autonomous cars Uber and all

195
00:18:18,880 --> 00:18:23,600
the things that they're doing that's amazing have you ever been in one of these cars

196
00:18:23,600 --> 00:18:29,600
if you ever to it led to this that's amazing that you can sit in that car and it can drive you

197
00:18:29,600 --> 00:18:36,000
through traffic on a highway at 65 miles an hour that's it's amazing if you would ask me how

198
00:18:36,000 --> 00:18:41,280
you would do something like that 25 years ago I mean I can barely forget how human beings do it

199
00:18:41,280 --> 00:18:45,280
and in fact being on the road it's pretty clear to me lots of human beings don't do a very good

200
00:18:45,280 --> 00:18:50,320
job of it but that's a miracle and we barely notice right every time you get an airplane right the

201
00:18:50,320 --> 00:18:56,240
pilot's not flying the airplanes flying itself right and we just take this everyday miracle is just

202
00:18:56,240 --> 00:19:01,920
a another little thing in fact you know one of the big complaints if you're into AI right is that

203
00:19:01,920 --> 00:19:08,080
you never actually get credit for the cool things that you do right AI is kind of the the science

204
00:19:08,080 --> 00:19:14,080
and the engineering of making computers act the way they do in the movies right but one of the

205
00:19:14,080 --> 00:19:19,600
things that sort of tied into that is if it's got to be intelligent then it's got to be like humans

206
00:19:19,600 --> 00:19:24,560
and if it's got to be like humans it has to be mysterious and something we can't understand so

207
00:19:24,560 --> 00:19:30,000
the problem is every time we do something even if it's amazing once we know how to do it and we

208
00:19:30,000 --> 00:19:34,880
understand it well that can't be real intelligence and so we don't give the credit to AI so AI sort

209
00:19:34,880 --> 00:19:41,440
of has this problem where you you can't ever win because anything interesting you do well

210
00:19:41,440 --> 00:19:47,600
we understand that and that's not real intelligence so it's no longer AI yeah or it's just it's

211
00:19:47,600 --> 00:19:52,640
just computers right it's never this thing where you succeed it it's just oh that's not the real

212
00:19:52,640 --> 00:19:57,360
part the real intelligent part is this thing and then when you can suddenly do be you know people

213
00:19:57,360 --> 00:20:01,280
at jeopardy well that's that's not really intelligence the real intelligence is other things so you

214
00:20:01,280 --> 00:20:06,080
you basically just keep you know innovating your way out of out of business and so AI gets sort

215
00:20:06,080 --> 00:20:10,560
of smaller and smaller and smaller and what it's allowed to to call itself because the mystery gets

216
00:20:10,560 --> 00:20:16,640
smaller and smaller is it smaller smaller or further further well it's always sort of infinitely

217
00:20:16,640 --> 00:20:22,160
far away right all right it's it's it's something that we can always look for but we can never

218
00:20:22,160 --> 00:20:30,080
quite get to sort of Zeno's paradox of AI but there's not like a you know there's not some finite

219
00:20:30,080 --> 00:20:35,440
set of things that we need to do to figure out AI and we're chipping away at it and it's getting

220
00:20:35,440 --> 00:20:43,760
smaller and smaller it's like the goalpost is moving yeah well so I think both of those things are

221
00:20:43,760 --> 00:20:48,080
true I think there are a finite number of things we need to do we're definitely chipping away at it

222
00:20:48,080 --> 00:20:54,080
and so the stuff we need to do sort of gets smaller and smaller though it's still really big but the

223
00:20:54,080 --> 00:20:58,880
goalpost keep moving right we've got cars that can drive themselves more or less and now that's

224
00:20:58,880 --> 00:21:04,080
no longer amazing so it's got to be something else but that's that's amazing and by the way

225
00:21:04,080 --> 00:21:09,920
it's not just amazing it has an amazing impact on the world have you seen this I know you're on

226
00:21:09,920 --> 00:21:14,480
Facebook see you remember this map that was going around for a while that showed the the most

227
00:21:14,480 --> 00:21:21,120
common jobs in every state you remember this yeah and do you remember what the most common

228
00:21:21,120 --> 00:21:26,560
job is in almost every state in the US truck driver right yeah truck driver delivery person taxi

229
00:21:26,560 --> 00:21:32,320
driver right that's something like 42 or 44 this item the right number but it's over 40 for some

230
00:21:32,320 --> 00:21:36,320
reason in other states it's elementary school teacher I don't know why but but mostly it's truck

231
00:21:36,320 --> 00:21:43,280
driver well you know we're five years away from all the cars delivering driving themselves right

232
00:21:43,280 --> 00:21:49,920
Uber is it's not going to have people involved anymore my old advisor my one of my PhD advisors

233
00:21:49,920 --> 00:21:54,960
you know is heading the work at primary right so things are going to be delivered to as by drones

234
00:21:54,960 --> 00:22:00,960
and people are going to be involved anymore well that's the most common job in the country

235
00:22:00,960 --> 00:22:08,480
and it's going away right so the the goalpost removing the the things we have to do or getting

236
00:22:08,480 --> 00:22:12,480
smaller or not and people have this sort of feeling what AI is but whether or not you want to call

237
00:22:12,480 --> 00:22:16,880
it AI or not it's going to have a massive impact on our day-to-day lives and it's going to have

238
00:22:16,880 --> 00:22:20,880
a massive impact on the economy it's going to have a massive impact on sort of how we see ourselves

239
00:22:20,880 --> 00:22:25,520
and how we interact with one another and whether you decide that it's AI or not or it's intelligent

240
00:22:25,520 --> 00:22:30,320
or not whether you move the goalposts or not it's changing everything around us in deep and

241
00:22:30,320 --> 00:22:37,920
profound ways yeah absolutely absolutely so I want to talk about a couple of of really specific

242
00:22:37,920 --> 00:22:46,560
things with you and we'll take these in in turn the first is in the the realm of education

243
00:22:46,560 --> 00:22:53,120
and the second is in the realm of your research focus area and reinforcement learning but let's

244
00:22:53,120 --> 00:22:58,480
start with the first of those we got we got through your grad school experience in MIT then you

245
00:22:58,480 --> 00:23:10,000
went back to Georgia Tech and most recently you've been doing a lot of work in online education

246
00:23:10,000 --> 00:23:18,000
around machine learning maybe walk us through what you're doing there and in particular I'm curious

247
00:23:20,000 --> 00:23:27,920
and maybe as a bit of background here I didn't go through your entire course but I took a look at

248
00:23:27,920 --> 00:23:33,840
the the course that you did with Michael Litman and it was I really enjoyed the presentation

249
00:23:33,840 --> 00:23:42,160
having gone through a number of ML MOOCs and it made me wonder like what you know what unique

250
00:23:42,160 --> 00:23:50,000
views do you bring to you know teaching and or learning machine learning in AI that you surfaced

251
00:23:50,000 --> 00:23:54,320
in the the coursework as well as you know which of the you know are there any views you have that

252
00:23:54,320 --> 00:23:59,120
you think kind of go against the grain of the way people other people are approaching it

253
00:23:59,120 --> 00:24:07,600
yeah so so I'm glad you enjoyed the the the classes Michael and I had a ball just a total blast

254
00:24:07,600 --> 00:24:12,480
doing it and if you haven't you should watch the the Michael Jackson parody video we did about

255
00:24:12,480 --> 00:24:17,440
machine learning you get to see get this you Michael dressed up as Michael Jackson and dancing

256
00:24:17,440 --> 00:24:23,440
which is well worth the price of admission which is free so the you talk about this kind of

257
00:24:23,440 --> 00:24:27,680
interaction we have one of the things that Michael and I I tried to do is we decided that we've

258
00:24:27,680 --> 00:24:30,720
been wanting to do things for a long together for a long time but you know he's on one part of the

259
00:24:30,720 --> 00:24:34,640
country having another part of the country we wanted to do this this machine learning MOOC and

260
00:24:34,640 --> 00:24:38,480
and this gave us the the opportunity to do it and the way we decided to come at it was it's

261
00:24:38,480 --> 00:24:44,400
much like we're doing this now we said you know what education like this should be more like a

262
00:24:44,400 --> 00:24:50,160
podcast you should have a conversation so every time we did one of these these lectures one of us

263
00:24:50,160 --> 00:24:54,240
would be the professor who would try to present the material and the other person would try to be

264
00:24:54,240 --> 00:24:58,640
the student so the professor would do all the preparation and come up with the sort of lesson

265
00:24:58,640 --> 00:25:02,800
and get everything together and the student would do no preparation at all and come in cold so you

266
00:25:02,800 --> 00:25:09,120
know in that way it's just like regular school and we would just talk and of course he's an expert

267
00:25:09,120 --> 00:25:13,280
I'm an expert and this is what we do all day so it's not like we didn't really kind of understand

268
00:25:13,280 --> 00:25:17,680
what was going on but it turned out and I think this really does come out in the conversations that

269
00:25:17,680 --> 00:25:22,720
we had that we actually have very different views of what's important right so Michael is much more

270
00:25:22,720 --> 00:25:27,200
of a theoretician if you asked him what AI and machine learning is he might say something like

271
00:25:27,200 --> 00:25:31,760
computational statistics I'm much more interested in thinking about it and it's kind of practical

272
00:25:31,760 --> 00:25:36,640
applications and you know sort of what you can do as a practitioner to to to use these tools

273
00:25:36,640 --> 00:25:41,360
to to make them work and get synthesis I want people to see that this thing over here is just like

274
00:25:41,360 --> 00:25:45,120
that thing over there which is just like this thing over there and they're all tied together

275
00:25:45,120 --> 00:25:50,000
and I'm much less interested in proving in the abstract what it is that that you actually

276
00:25:50,000 --> 00:25:53,440
can learn in what you actually can't learn it's not that these things are important I just you

277
00:25:53,440 --> 00:25:57,360
know I'm just less interested in them than than Michael is and so we would spend all of these

278
00:25:57,360 --> 00:26:01,520
times kind of arguing some of that sometimes obviously sometimes not about what's going on

279
00:26:01,520 --> 00:26:05,440
and what I hope came out of that and you can tell me if you if you think it's true or not

280
00:26:05,440 --> 00:26:09,680
is that the student was drawn into this conversation and at least got the feeling that not only

281
00:26:09,680 --> 00:26:13,920
were they learning some equation or getting ready for some test or doing some assignment but that

282
00:26:13,920 --> 00:26:18,160
there really is a deep conversation going on about AI and machine learning and there's lots of

283
00:26:18,160 --> 00:26:24,400
different ways to think about it and and really that kind of gets to my larger philosophy about

284
00:26:24,400 --> 00:26:28,800
the way education works and why I'm so excited about the online education that's that we've

285
00:26:28,800 --> 00:26:35,440
been able to do to me what's really missing in education is access right the ability for people

286
00:26:35,440 --> 00:26:41,600
to really to participate in the in the commons that is education that is research that that is

287
00:26:41,600 --> 00:26:46,240
learning and one thing that I think's important for people to understand is that when you say

288
00:26:46,240 --> 00:26:52,000
access some people turn that into affordability you know is it cheap enough you know

289
00:26:52,000 --> 00:26:56,720
tuition is too high you know and that is a part of access but access is actually very different

290
00:26:56,720 --> 00:27:02,080
access is just the ability to be able to participate in the conversation and that if you're

291
00:27:02,080 --> 00:27:06,080
capable of getting through it being able to have the real opportunity to get through it

292
00:27:06,080 --> 00:27:10,800
affordability is only a small part of that so one of the things that we've been doing and

293
00:27:11,840 --> 00:27:15,920
and I'm actually quite proud of this over the last three years is we decided that we

294
00:27:15,920 --> 00:27:20,720
wanted to push on this idea of access and affordability and that online education in MOOCs

295
00:27:20,720 --> 00:27:24,400
were one way of doing it and while we're working on this this machine learning class we wanted

296
00:27:24,400 --> 00:27:28,880
to make it a part of something bigger and so Georgia Tech when when I was there in my senior

297
00:27:28,880 --> 00:27:33,840
social dean role I guess I still am and in my professor role we wanted to build an entire degree

298
00:27:33,840 --> 00:27:41,120
a graduate level degree that anyone who could get access to the internet and then who had the

299
00:27:41,120 --> 00:27:46,240
time and had the desire would be able to get through an actual full-fledged course

300
00:27:47,120 --> 00:27:51,920
a full-fledged and not just a course a full-fledged degree a real program and so we created this

301
00:27:51,920 --> 00:27:57,840
online MS program it's exactly the same as our on-campus program same requirements same degree

302
00:27:57,840 --> 00:28:01,680
you get through this you get a you get a master of science computer science from a top 10

303
00:28:01,680 --> 00:28:06,800
department and it's indistinguishable from the one that you get on campus and here's the thing

304
00:28:06,800 --> 00:28:13,280
that we we did two things to sort of push on this notion of access one is we decided to make it

305
00:28:13,280 --> 00:28:19,760
as inexpensive as possible so the entire the cost of the entire degree is something around $6600

306
00:28:20,800 --> 00:28:24,240
wow depending on how fast you you get through the program so somewhere between $6,000

307
00:28:24,240 --> 00:28:29,520
and $8,000 sort of depending upon what you do you get an entire degree that's pretty inexpensive

308
00:28:29,520 --> 00:28:34,160
if you came on campus and you were out of state student it cost you more like $46,000

309
00:28:34,160 --> 00:28:37,680
so that was the first thing that we did make it affordable but the other thing that we decided

310
00:28:37,680 --> 00:28:42,960
to do is we decided to admit every single student we believed who could succeed

311
00:28:44,400 --> 00:28:49,520
this is a pretty big deal right if you if if we think about our on-campus degree we accept about

312
00:28:49,520 --> 00:28:55,120
10% of our applicants why do we accept 10% of our applicants because it's all the space we have

313
00:28:55,120 --> 00:29:00,000
right I'd estimate somewhere between 60 and 70% of the students who applied our graduate program

314
00:29:00,000 --> 00:29:05,120
are above bar but we only got a room for 10% of them so only 10% of them get in and by the way

315
00:29:05,120 --> 00:29:11,280
it's it's it's basically a lottery right I mean you know when when you've got your place like

316
00:29:11,280 --> 00:29:14,640
Stanford and you're accepting four or five percent of the people coming into your undergraduate

317
00:29:14,640 --> 00:29:18,080
program there's no way that that four or five percent really better than the next four or five

318
00:29:18,080 --> 00:29:22,000
percent the four or five percent after that you're you're almost closing your eyes and just picking

319
00:29:22,000 --> 00:29:25,280
people right yeah and this is but what we were doing at the graduate level we don't like that

320
00:29:25,280 --> 00:29:30,720
for our online degree which again is the same degree as our on-campus degree at this point we're

321
00:29:30,720 --> 00:29:38,240
accepting about 60% of applicants okay we've gone from zero students three years ago to 4,000

322
00:29:38,240 --> 00:29:45,360
students this term 4,000 currently enrolled students or is that a cumulative 4,000 currently

323
00:29:45,360 --> 00:29:52,560
enrolled students okay wow wow that's pretty good how many on campus about two or three hundred

324
00:29:52,560 --> 00:29:59,200
okay in fact right by the way it's not just that we've got 4,000 students they're performing

325
00:29:59,200 --> 00:30:04,080
as well as the on-campus students oh by the way it's not just that we have 4,000 students

326
00:30:04,080 --> 00:30:09,200
who are behaving who are performing as well as the on-campus students they look very different

327
00:30:09,200 --> 00:30:14,160
so if you look at our on-campus degree about 85% of the applicants are far national

328
00:30:14,160 --> 00:30:20,160
faster majority of whom are from India following behind China so about 15% are US citizens

329
00:30:20,160 --> 00:30:26,960
for our online degree it's the compliment about 85 80 to 85% of the applicants are US citizens

330
00:30:26,960 --> 00:30:31,840
or permanent residents okay right they're in their early 30s early in mid 30s not in their

331
00:30:31,840 --> 00:30:39,200
earlier mid 20s most of them are working full-time they've got you know jobs mostly in IT they're

332
00:30:39,200 --> 00:30:44,880
not all of them they've got mortgages they've got kids and they're trying to sort of get through

333
00:30:44,880 --> 00:30:50,480
their day but they can't take the time to get further education or to do that thing they want to

334
00:30:50,480 --> 00:30:54,320
do because again they've got mortgages and kids they've got responsibilities right so what's

335
00:30:54,320 --> 00:30:58,560
interesting is we've done studies of this we partnered with Harvard and looked at it we think that

336
00:30:58,560 --> 00:31:04,400
of the people who are coming through our program almost none of them would have pursued an advanced

337
00:31:04,400 --> 00:31:08,160
degree otherwise they weren't they because they just simply didn't have the option they couldn't

338
00:31:08,800 --> 00:31:14,080
take two years off from their lives to go and pursue a degree because they had too many other

339
00:31:14,080 --> 00:31:18,640
responsibilities and things that they had to do but this gives them the option of doing that and so in

340
00:31:18,640 --> 00:31:23,120
fact the overlap between them and the people who normally we get education is almost zero

341
00:31:23,120 --> 00:31:28,800
current estimate is that we'll add between eight and 10% every year to the number of graduate

342
00:31:29,360 --> 00:31:35,040
IT workers in the United States then we otherwise would have seen and have you looked at what what

343
00:31:35,040 --> 00:31:40,320
they're doing afterwards how long has the program been in place and how long have you been tracking

344
00:31:40,320 --> 00:31:46,160
that into what degree so it's been about three years in fact I think we're beginning our we'll be

345
00:31:46,160 --> 00:31:50,000
we're just ending our third year now and we'll be starting our fourth year so people have just

346
00:31:50,000 --> 00:31:57,760
begun to graduate we had 20 people graduate two terms ago and this semester we're expecting

347
00:31:57,760 --> 00:32:03,120
to see closer to about 250 and we're expecting to see a steady state of closer to a thousand people

348
00:32:03,120 --> 00:32:09,520
graduating a year most of them already had jobs so you know usually the way you measure success you

349
00:32:09,520 --> 00:32:13,840
say okay the people get jobs when they graduate well most of these people already had jobs so they

350
00:32:13,840 --> 00:32:18,160
didn't lose their jobs I guess that's a good thing but it's hard it's hard to know what that

351
00:32:18,160 --> 00:32:24,640
what that impact is because the usual measures don't really make sense but they're all they all

352
00:32:24,640 --> 00:32:30,480
seem to be happy 97% of them said that they would you know recommend this to other people many of

353
00:32:30,480 --> 00:32:34,320
them do get jobs while they're in the middle of the program a lot of them get promotions and they

354
00:32:34,320 --> 00:32:38,560
move through you'll have to ask me in five years what the what the real impact is but right now it appears

355
00:32:38,560 --> 00:32:43,200
that people are happy they're getting a lot out of it some of them are able to change careers get

356
00:32:43,200 --> 00:32:47,440
promotions and to do things they wouldn't otherwise be able to do because they just couldn't take

357
00:32:47,440 --> 00:32:52,880
the time off to do it so I'm very happy with that and happy with the sort of impact it appears to

358
00:32:52,880 --> 00:32:59,200
be having on students let me ask you this a lot of people who listen to the podcaster somewhere along

359
00:32:59,200 --> 00:33:07,040
the progression of learning and and entering machine learning as a field as a profession and

360
00:33:07,040 --> 00:33:16,880
I'm wondering what what do you think the right set of uh set of educational tools to take advantage of

361
00:33:16,880 --> 00:33:25,120
right MOOCs are a piece of that um but there's obviously other pieces that go into making a full

362
00:33:25,920 --> 00:33:32,480
kind of a well-rounded student of machine learning in AI how do you recommend that students

363
00:33:32,480 --> 00:33:37,760
approach that or do you have a philosophy around that well so I sort of do and I do think it comes

364
00:33:37,760 --> 00:33:42,880
out in my in my class if you actually take the class as opposed to watch the lectures you get my

365
00:33:42,880 --> 00:33:46,480
assignments and I'll just describe my first assignment to you because I think it actually captures

366
00:33:46,480 --> 00:33:51,920
a lot of at least what I believe matters in becoming a either a machine learning researcher or

367
00:33:51,920 --> 00:33:56,640
a machine learning practitioner or even AI or more broadly speaking so here's my first assignment

368
00:33:56,640 --> 00:34:01,520
first assignment is go find two datasets I don't care what they are so long as they're interesting

369
00:34:01,520 --> 00:34:05,360
they have to be interesting by themselves and they have to be interesting together and you have

370
00:34:05,360 --> 00:34:11,360
to convince me that they're interesting um then I want you to implement these five or six algorithms

371
00:34:11,360 --> 00:34:15,440
and when I say implement I mean steal the code I don't really care you'll get any credit

372
00:34:15,440 --> 00:34:19,600
whatsoever for implementing and running the code you steal libraries you know go get your

373
00:34:19,600 --> 00:34:23,920
your favorite implementation of K&N or boosting from somewhere else I don't really care

374
00:34:23,920 --> 00:34:28,160
and I want you to run all of those algorithms on those two datasets and I want you to do analysis

375
00:34:28,160 --> 00:34:31,760
and explain to me why you got the behavior that you did why did some of those algorithms which

376
00:34:31,760 --> 00:34:36,640
should all work why did some of them behave better on some data on one of the datasets than the other

377
00:34:36,640 --> 00:34:42,160
what sort of things did you learn by applying those algorithms and doing the data analysis convince

378
00:34:42,160 --> 00:34:47,040
me that you thought about it convince me what experiments you would need to run in order to really

379
00:34:47,520 --> 00:34:51,760
get the answers to the questions and then run those experiments do all of that and then write it

380
00:34:51,760 --> 00:34:58,560
up in 12 pages not 13 pages not 14 pages 12 pages why do I have an assignment like that I haven't

381
00:34:58,560 --> 00:35:03,680
assignment like that because I think much about machine learning much about the field that we're in

382
00:35:03,680 --> 00:35:10,000
is really about the practice of doing it you know theoretically all of these algorithms is

383
00:35:10,000 --> 00:35:13,840
particularly in supervised learning they're all very similar they all can learn the same kinds of

384
00:35:13,840 --> 00:35:18,560
things you know but there's no free lunch right so there has to be built into what you're doing

385
00:35:18,560 --> 00:35:22,960
deep assumptions about your data what is you're trying to accomplish and you have to be able to

386
00:35:22,960 --> 00:35:27,440
surface those things so if somebody want to ask me if I wanted to really do machine learning what

387
00:35:27,440 --> 00:35:31,840
do I need to learn I give them two answers one you need to learn the foundations and the fundamentals

388
00:35:31,840 --> 00:35:35,840
yes you need to know the math you understand information theory you need to understand you know

389
00:35:35,840 --> 00:35:40,000
what linear algebra is you need to not flinch or somebody mentions an eigenvector an eigenproblem

390
00:35:40,000 --> 00:35:44,720
to you you need to get the math yes and you need to get the computing because it's a fundamentally

391
00:35:44,720 --> 00:35:49,600
I'm computing pace discipline and computing is not math computing is not engineering computing is

392
00:35:49,600 --> 00:35:54,480
not science you need to internalize the computing part of machine learning but just as important

393
00:35:54,480 --> 00:36:00,000
and in many ways more important I believe is you have to really dive deeply into the empirical side

394
00:36:00,000 --> 00:36:04,800
of it you have to get dirty with data you have to understand what the difficulties are in and

395
00:36:04,800 --> 00:36:08,400
answering the questions you want to answer and you have to really realize that the questions you're

396
00:36:08,400 --> 00:36:13,680
asking aren't necessarily the right ones most of what traps us in machine learning and in lots of

397
00:36:13,680 --> 00:36:18,640
other things we do are the unspoken assumptions you have to surface what those things are and I

398
00:36:18,640 --> 00:36:24,080
think that the best way of doing that is by getting your hands and your feet dirty so my classes

399
00:36:24,080 --> 00:36:30,480
are designed to do that to force you to get into a messy ill-defined situation and to work your

400
00:36:30,480 --> 00:36:34,880
way out of it so if you want to do data analysis if you want to do machine learning that's great it's

401
00:36:34,880 --> 00:36:39,920
wonderful I can think of nothing more interesting to do but you have to get out of the textbooks you

402
00:36:39,920 --> 00:36:45,200
have to play through the data and understand why it works the way that it does why the algorithms

403
00:36:45,200 --> 00:36:49,760
have the effect that they do why you can learn some things you can't seem to learn other things

404
00:36:49,760 --> 00:36:54,480
and that I think is actually really missing I think people either dive down the empirical side

405
00:36:54,480 --> 00:36:58,240
and just try to get stuff working but with no understanding of the fundamentals they don't even

406
00:36:58,240 --> 00:37:02,960
know how to ask the questions or they get so caught up in the fundamentals they don't worry

407
00:37:02,960 --> 00:37:06,960
about whether it actually works in practice or how you would actually apply your ideas

408
00:37:06,960 --> 00:37:15,760
and you have to do both especially in a field like machine learning they use all the

409
00:37:15,760 --> 00:37:20,320
the social media tools that are out there to build community to talk to each other to talk to

410
00:37:20,320 --> 00:37:25,440
the faculty to talk to the advisors they really build an entire community around what they're doing

411
00:37:25,440 --> 00:37:29,200
and really the people who are in that community do well and the people who are not a part of that

412
00:37:29,200 --> 00:37:35,520
community do poorly so one of the things that's important about the trips that I've been taking in

413
00:37:35,520 --> 00:37:39,520
the traveling around the world I've been doing is making certain that we provide the tools so that

414
00:37:39,520 --> 00:37:43,200
people can build local community that makes sense to them because that's how the learning happens

415
00:37:44,000 --> 00:37:50,640
you guys you guys might be single-handedly propping up Google Plus I'm about helping Google Plus

416
00:37:50,640 --> 00:37:55,600
I think people haven't been nice enough to Google Plus I've never heard of anyone else saying they're

417
00:37:55,600 --> 00:38:02,640
using it well there's no lag because no one else is using it so you got that nice nice nice

418
00:38:02,640 --> 00:38:11,440
so let's switch gears a little bit and talk about your research your research is your research focus

419
00:38:11,440 --> 00:38:18,160
as I understand it anyways primarily around reinforcement learning or maybe you tell me tell us

420
00:38:18,160 --> 00:38:28,880
what your research focus is nowadays and how you think of that area yeah so I I you know I

421
00:38:28,880 --> 00:38:32,960
like I said earlier I really have been into AI machine learning for a very long time and it

422
00:38:32,960 --> 00:38:38,080
took me a while to figure out what it was about it that I I really cared about and it was I was

423
00:38:38,080 --> 00:38:42,960
easier to see when I was reflecting back on it what it is that you know I found interesting what

424
00:38:42,960 --> 00:38:47,680
I didn't the kind of machine learning that I care about the name that we kind of give it in the

425
00:38:47,680 --> 00:38:53,680
field is interactive machine learning and interactive AI I sometimes refer to as interactive AI

426
00:38:53,680 --> 00:38:58,000
because I care about the AI problem as much as I do the the machine learning problem and what it

427
00:38:58,000 --> 00:39:02,960
really is is about what happens when you instead of just saying oh look here's some data and I'm

428
00:39:02,960 --> 00:39:06,880
going to look at that data and then I'm going to build a function and now I can do some prediction

429
00:39:06,880 --> 00:39:10,720
you know that you're going to have a fundamentally incremental and interactive process so I want to

430
00:39:10,720 --> 00:39:15,440
model human beings because I actually care about messy data and there's nothing messier than people

431
00:39:15,440 --> 00:39:20,400
so I want human beings to be a part of the story of how I learned and when I say that I think

432
00:39:20,400 --> 00:39:24,320
that people learn only through social communities or they learn best their social communities

433
00:39:24,320 --> 00:39:28,960
I think that's actually true for our machines as well so that ends up looking a lot like and I

434
00:39:28,960 --> 00:39:33,760
spend most of my time worrying about reinforcement learning so you're right about that and the

435
00:39:33,760 --> 00:39:37,920
reason I care about reinforcement learning is that reinforcement learning is really I think trying

436
00:39:37,920 --> 00:39:42,560
to do something big and hairy which is actually model what it means to be an autonomous agent so

437
00:39:42,560 --> 00:39:47,360
when people ask me for the one sentence description of what it is that I what it is that I do I said

438
00:39:47,360 --> 00:39:52,560
I care about interactive machine learning I care about building intelligent agents that have to

439
00:39:52,560 --> 00:39:57,840
interact with other intelligent agents perhaps hundreds of thousands of them at a time and some of

440
00:39:57,840 --> 00:40:02,880
those intelligent agents might be human they don't all have to be human but some of them will be

441
00:40:02,880 --> 00:40:07,680
and since some of them are human you can't just go around sending XML packets back and forth you have

442
00:40:07,680 --> 00:40:12,400
to actually engage in conversation you have to worry about the fact that human beings change over time

443
00:40:12,400 --> 00:40:16,640
they're inconsistent they're errone they're highly non-marcovian there's all kinds of interesting

444
00:40:16,640 --> 00:40:22,160
things about people and you need to be partners with people and you need to be long lived in order for

445
00:40:22,160 --> 00:40:26,800
you to make progress in the area so that's what I really care about I care about building a system

446
00:40:26,800 --> 00:40:32,560
that doesn't just predict whether you know a car is going to run into the side of the road or not

447
00:40:32,560 --> 00:40:36,320
but actually deals with the fact that there are several million other people on the road at the

448
00:40:36,320 --> 00:40:40,400
same time and you have to interact with those other people and you have to learn by talking to

449
00:40:40,400 --> 00:40:48,880
them and interacting with them and so reinforcement learning is a subset of that yes that's right I

450
00:40:48,880 --> 00:40:52,960
spend a lot of my time worrying about game theory I spend a lot of my time worrying about

451
00:40:54,480 --> 00:40:59,920
marketing believe it or not about social behavior and how people tend to interact and

452
00:40:59,920 --> 00:41:05,520
and work with one another and how you can convince them to to work with you or how you can deal

453
00:41:05,520 --> 00:41:09,680
with them if they're trying to work against you so it's the whole gamut of what it means to interact

454
00:41:09,680 --> 00:41:14,400
with other intelligent beings that have their own set of goals and and interest that might not be

455
00:41:14,400 --> 00:41:21,040
the same as yours so tell me you mentioned marketing tell me more about how that plays into your

456
00:41:21,040 --> 00:41:28,320
research or or maybe even give us an example of some of the research topics you've been looking

457
00:41:28,320 --> 00:41:34,240
into recently so I like the marketing question so so I spend a lot of time with a friend of my

458
00:41:34,240 --> 00:41:41,360
with one of my students is now a professor in North Carolina on something called drama management

459
00:41:41,360 --> 00:41:45,600
so the short version of drama management is well you know you've played video games right

460
00:41:46,240 --> 00:41:51,680
uh yep and you know the thing about video games is the interesting ones are ones where you're

461
00:41:51,680 --> 00:41:57,200
you know involved an entire world and an entire story so what's actually going on is that you're

462
00:41:57,200 --> 00:42:02,240
the person building the system for you is trying to build a story but most stories you just read

463
00:42:02,240 --> 00:42:06,240
and you're a passive participant of and things like games you're actually an active participant

464
00:42:06,240 --> 00:42:10,160
which means there's this tradeoff between your sense of autonomy and agency on the one hand

465
00:42:10,160 --> 00:42:14,880
and me making certain that you have a good experience or a good story so you can actually think of

466
00:42:14,880 --> 00:42:18,560
lots and lots of things like this you can think about conversations that you have in the interviews

467
00:42:18,560 --> 00:42:22,560
and a podcast is like a story where you're negotiating back and forth and trying to

468
00:42:22,560 --> 00:42:26,160
trying to figure out how to tell the the story that you want to tell while still allowing people

469
00:42:26,160 --> 00:42:30,320
to say the things that they that they need to say or that they want to say you can think about

470
00:42:30,320 --> 00:42:34,560
all kinds of examples like those can kind of go on for a while but the the the thing that the

471
00:42:34,560 --> 00:42:41,520
thing there is that it turns out that because your player or the person who's participating

472
00:42:41,520 --> 00:42:46,240
in building the story with you has their own ideas they might take your ideas off track they

473
00:42:46,240 --> 00:42:51,120
might turn your murder mystery into a horror story they might turn your interview where you're

474
00:42:51,120 --> 00:42:54,880
supposed to be going back and forth and having a conversation into a series of you ask me a

475
00:42:54,880 --> 00:42:58,800
questions and I say yes or no and it's not much of an interview for you right so you have to

476
00:42:58,800 --> 00:43:03,760
influence what the player is doing what the human participant is doing or otherwise

477
00:43:03,760 --> 00:43:08,800
you don't end up with a good story that you want to have so there are two ways of doing that one

478
00:43:08,800 --> 00:43:13,280
and I think you know you and most your listeners if you ever heard the expression a game that's on

479
00:43:13,280 --> 00:43:18,640
rails sure so you know that's where well I'm sorry I'm just not going to let you go through this door

480
00:43:18,640 --> 00:43:23,040
because if you do it breaks the video game it breaks the story and so you're on rails and the

481
00:43:23,040 --> 00:43:28,000
thing about being on rails it takes you out of the story takes you out of the experience and that's

482
00:43:28,000 --> 00:43:32,400
what a lot of people do and a lot of the drama management stuff is about that as well but there's

483
00:43:32,400 --> 00:43:36,240
another way of doing and in fact the right way of doing it if you can make it work is you get the

484
00:43:36,240 --> 00:43:40,720
other person the person you're interacting with you're trying to learn with the story you're trying

485
00:43:40,720 --> 00:43:45,760
to get to participate in the story to actually accept your goals as his or her own and it turns out

486
00:43:45,760 --> 00:43:51,840
marketing is very good at this so we build this kind of system where you get people to do the things

487
00:43:51,840 --> 00:43:56,960
that you want them to do by putting them in situations where it's just natural for them to do those

488
00:43:56,960 --> 00:44:04,320
things so rather than lock every door except one door in a room so you go through it I make something

489
00:44:04,320 --> 00:44:09,120
happen maybe some noise or something interesting that makes you want to go through that door right

490
00:44:09,120 --> 00:44:15,440
so um those kind of like themes of behavior like economics and incentives and things like that

491
00:44:15,440 --> 00:44:19,040
coming into play here right yeah oh that's exactly right so in fact the example of this that

492
00:44:19,040 --> 00:44:24,160
everyone's familiar with is one called scarcity uh-huh so that's where it turns out that people

493
00:44:24,160 --> 00:44:30,560
if they believe that something is going away they suddenly find it more valuable right so anybody

494
00:44:30,560 --> 00:44:35,520
with kids knows certainly anyone with kids ten years ago know that Disney has his habit of saying oh

495
00:44:35,520 --> 00:44:40,720
we're going to release on DVD beauty in the beast and then we're never going to release it again uh-huh

496
00:44:41,360 --> 00:44:46,480
and so everybody buys it right because it's about to go away or I mean black fridays like this right

497
00:44:47,280 --> 00:44:51,600
you're gonna every year at the day after Thanksgiving you go to the store to buy a bunch of stuff

498
00:44:51,600 --> 00:44:55,760
it doesn't make any sense whatsoever there are not even things you want to have but they're going

499
00:44:55,760 --> 00:45:00,400
away you're gonna get a price right now it's on sale and so people react to that they can't help

500
00:45:00,400 --> 00:45:05,040
themselves uh it's a scarcity is just one of one of the particular they very easy to understand

501
00:45:05,040 --> 00:45:08,480
there's tons of others of these there's something called liking which is it turns out people will

502
00:45:08,480 --> 00:45:14,560
do things for you if they if they like you uh people react to authority actually my favorite

503
00:45:14,560 --> 00:45:19,920
example is something called consistency where if you can get someone to say something out loud

504
00:45:19,920 --> 00:45:25,040
that they believe something they haven't almost pathological need to be consistent with it over time

505
00:45:25,040 --> 00:45:28,640
so uh you know do you have anybody in your neighborhood who won't mold their lawn

506
00:45:29,360 --> 00:45:34,160
um here's the way you get them to mold the lawn you wait till it's winter right and so all the

507
00:45:34,160 --> 00:45:38,480
grass is you know kind of dead and it's all the the same height and you start up a conversation

508
00:45:38,480 --> 00:45:42,960
and you say man you know it really looks great around here when it's like this you know everything's

509
00:45:42,960 --> 00:45:47,200
the same color everything's the same height if you get the person to agree with that yeah it looks

510
00:45:47,200 --> 00:45:53,680
really nice and it's like this the next summer they'll mold the lawn because they basically believe

511
00:45:53,680 --> 00:45:57,200
that's the way it's supposed to be and you know it's really nice about it it's not that you got

512
00:45:57,200 --> 00:46:03,520
them to mold the lawn it's that they believe that they are in complete control of the idea that

513
00:46:03,520 --> 00:46:08,560
they're the ones who made the decision are in charge so that's a long story but the the short

514
00:46:08,560 --> 00:46:15,280
version is we built systems like this uh that basically convinced people uh to do the things that

515
00:46:15,280 --> 00:46:19,360
we wanted them to do we influenced them so i'm not using machine learning just to predict your

516
00:46:19,360 --> 00:46:23,680
behavior i'm using machine learning to figure out how to intervene to get you to do something and

517
00:46:23,680 --> 00:46:28,160
what i really want to happen is for you to believe it's your own idea so we built this little story

518
00:46:28,160 --> 00:46:33,600
just a quick example we built this little story uh where the whole goal was to get you to buy a fish

519
00:46:33,600 --> 00:46:37,440
at a market it's not the most exciting story in the world and there are lots of ways we can

520
00:46:37,440 --> 00:46:42,000
influence you to do this with scarcity and various other things and and and so we had people

521
00:46:42,000 --> 00:46:47,600
play this game and uh the people we tried to influence were much more likely than the people we

522
00:46:47,600 --> 00:46:53,200
didn't um in buying the fish and doing the things that we tried to get done sure now that's

523
00:46:53,200 --> 00:46:59,520
interesting but what's more interesting is that when you ask the people whether they felt manipulated

524
00:46:59,520 --> 00:47:04,320
or not the people who were not manipulated were much more likely to say they felt they were being

525
00:47:04,320 --> 00:47:08,400
manipulated than the people who actually were manipulated that's interesting why is that

526
00:47:08,400 --> 00:47:13,040
because the whole the whole way this works the whole way the kind of psychology works is you

527
00:47:13,040 --> 00:47:17,520
feel as if you have agency that you're making the decision when something goes on sale and you

528
00:47:17,520 --> 00:47:21,040
decide you're gonna buy it you don't feel that you've been tricked into buying it you made the

529
00:47:21,040 --> 00:47:26,720
decision to do it right and so what's really interesting this is why it's not just running the

530
00:47:26,720 --> 00:47:30,640
data and doing machine learning it's actually understanding about human behavior it's understanding

531
00:47:30,640 --> 00:47:34,960
behavioral economics it's understanding the way marketing tricks work it's it's all about getting

532
00:47:34,960 --> 00:47:41,360
the person to make the decision you know themselves that they want to do this thing and then they have

533
00:47:41,360 --> 00:47:46,000
agency they have control and they're much more likely to see it through the fact that you kind of

534
00:47:46,000 --> 00:47:53,440
trick them into doing it is neither here nor there so quick note uh for listeners anyone that's

535
00:47:53,440 --> 00:47:59,200
interested in digging deeper into some of these ideas uh there's a great book called Influence by

536
00:47:59,200 --> 00:48:05,760
Robert Chaldeini that is super accessible and is covers all the things that you you talked about

537
00:48:05,760 --> 00:48:14,480
consistency and scarcity things like that um but this brings up a uh question for me and that is

538
00:48:15,520 --> 00:48:23,280
a lot of the a lot of the work we read about reinforcement learning nowadays is you're training

539
00:48:23,280 --> 00:48:31,040
these agents to uh navigate a world right and then the work you're describing is you've got this

540
00:48:31,040 --> 00:48:37,280
world that's essentially training the human to navigate it and there's an interesting complementariness

541
00:48:37,280 --> 00:48:42,640
to it and I'm wondering if if that complementariness has been explored at all like the things that I'm

542
00:48:42,640 --> 00:48:46,880
thinking around like adversarial networks like can you have the one training the other thinking

543
00:48:46,880 --> 00:48:50,640
it's training the other does that make any sense is anything happening there oh yeah that's

544
00:48:50,640 --> 00:48:54,960
actually very commonly doing it so the way so the thing that really got me into reinforcement learning

545
00:48:54,960 --> 00:49:00,240
when I was a young graduate student a couple hundred years ago uh was actually playing games so

546
00:49:00,240 --> 00:49:05,440
there was this guy named Saro who had built something called TD Gammon which was a particular way

547
00:49:05,440 --> 00:49:09,920
of doing a reinforcement learning to to learn how to play backgammon and the way it learned to play

548
00:49:09,920 --> 00:49:16,320
backgammon was through self-play so it it played both sides of the game uh and it learned by playing

549
00:49:16,320 --> 00:49:21,360
itself how to get better uh and this is a well I think I'm pretty well understood sort of technique

550
00:49:21,360 --> 00:49:26,720
for learning right you it's difficult to it if it's too hard you can't learn if it's too easy you

551
00:49:26,720 --> 00:49:31,600
can't learn you need to be just about a little beyond your current level of understanding and so

552
00:49:31,600 --> 00:49:35,520
yeah this kind of thing happens all the time now it is true that a lot of people who worry about

553
00:49:35,520 --> 00:49:39,760
machine learning do not think about the kind of complementary nature that rather than there being an

554
00:49:39,760 --> 00:49:45,360
agent that's training in an environment the environment could be in fact training the agent and people

555
00:49:45,360 --> 00:49:50,880
don't always see that um in fact my biggest complaint or complaints not the right word but my

556
00:49:50,880 --> 00:49:55,120
biggest um I don't know let's say complaint my biggest complaint about the way machine learning

557
00:49:55,120 --> 00:50:00,160
is portrayed is that it's portrayed as a supervised learning problem you know I'm going to give you

558
00:50:00,160 --> 00:50:04,640
a bunch of input output examples and you're going to learn the function that maps input output

559
00:50:04,640 --> 00:50:09,440
and that's interesting but I think reinforcement learning is more interesting because it's this

560
00:50:09,440 --> 00:50:14,960
bigger problem you don't have inputs and outputs all you've got is actions you can take and feedback

561
00:50:14,960 --> 00:50:19,600
you get from the world and then from that you have to figure out how to behave that feels richer to

562
00:50:19,600 --> 00:50:24,240
me even though in some sense they're equivalent the unsupervised learning is a very different way

563
00:50:24,240 --> 00:50:28,160
of thinking about the world even though again sort of mathematically they're they're all kind of

564
00:50:28,160 --> 00:50:32,880
equivalent and that kind of breadth of what machine learning and AI is is something that I don't

565
00:50:32,880 --> 00:50:37,280
think we spend enough time really thinking about I think people tend to focus on the supervised

566
00:50:37,280 --> 00:50:41,120
learning part instead of the reinforcement learning in the unsupervised learning part at least in

567
00:50:41,120 --> 00:50:48,800
the kind of popular press okay so maybe taking a step back how do you think about the

568
00:50:49,680 --> 00:50:55,040
current state of reinforcement learning like can you characterize the the major research

569
00:50:55,040 --> 00:50:59,920
efforts or even is it possible to characterize the major research efforts into a handful of

570
00:51:01,280 --> 00:51:07,520
directions and kind of who's doing what so I think there's kind well so the answers no it's

571
00:51:07,520 --> 00:51:11,680
way too broad but there's a couple of things that I think are really interesting one is all this

572
00:51:11,680 --> 00:51:16,640
work on deep networks and deep neural networks which you know is the the current thing that everybody's

573
00:51:16,640 --> 00:51:20,320
really into and by the way it's really good work you know I know the people who've been pushing on

574
00:51:20,320 --> 00:51:24,560
that for years and years and years and and they've really been able to to do a lot of interesting

575
00:51:24,560 --> 00:51:28,960
things they they've got the kind of fundamentals right with the math and they're taking advantage of

576
00:51:28,960 --> 00:51:33,840
the fact that we have insane amounts of data so that we can actually sort of take advantage of those

577
00:51:33,840 --> 00:51:38,080
algorithms and do cool things a lot of what's going on at least in my world that people are paying

578
00:51:38,080 --> 00:51:43,280
a lot of attention to is figuring out how to use the stuff that we know from deep networks and

579
00:51:43,280 --> 00:51:48,320
deep learning and applying it to reinforcement learning okay and and rather than doing the

580
00:51:48,320 --> 00:51:51,920
supervised learning take where you said well okay here's a state of the world tell me what to do

581
00:51:51,920 --> 00:51:56,240
you're actually treating it the way you treat a reinforcement learning problem you're talking about

582
00:51:56,240 --> 00:52:01,200
building value functions over what the states are in the world and what things are better and then

583
00:52:01,200 --> 00:52:06,400
using that to figure out how to make a decision and use what you learn from making the decision

584
00:52:06,400 --> 00:52:11,520
to affect your view of what's valuable in the world and kind of having each one feed into one

585
00:52:11,520 --> 00:52:16,160
another and so recognizing that there's at least two parts of that problem instead of one part of

586
00:52:16,160 --> 00:52:21,040
that problem is a really big deal and being able to marry the kind of math that's come out of

587
00:52:21,040 --> 00:52:26,480
supervised learning has been I think really important that I think has has been really interesting

588
00:52:26,480 --> 00:52:31,360
is push forward a lot of a lot of of what we've been able to to learn in the last couple of years

589
00:52:31,360 --> 00:52:36,320
for sure the second thing which I think is interesting in part because it's it's my own work and

590
00:52:36,320 --> 00:52:40,160
and place where I lived is very related to what we just got through talking about and it's this

591
00:52:40,160 --> 00:52:45,120
interactive machine learning it turns out you know I mentioned earlier that there's no free lunch

592
00:52:45,120 --> 00:52:49,520
right so for those of you don't know the no free lunch there and basically just says that

593
00:52:49,520 --> 00:52:54,240
all algorithms are equally good and in fact not only are all algorithms equally good but none of

594
00:52:54,240 --> 00:52:59,280
them are any better than behaving randomly and the reason that's true is because over all the

595
00:52:59,280 --> 00:53:04,640
infinite number of problems that one could encounter you know any algorithm has just as good a

596
00:53:04,640 --> 00:53:08,960
chance of doing well as that as any other algorithm but it turns out in practice we don't care

597
00:53:08,960 --> 00:53:14,080
about every possible problem in the universe we care about a small set of problems in the universe

598
00:53:14,080 --> 00:53:20,320
and what allows us to get leverage over that small set are built in assumptions about that

599
00:53:20,320 --> 00:53:25,280
about that world so the problem of learning is difficult and in some ways impossible

600
00:53:26,000 --> 00:53:31,280
but it turns out people are really good at solving the problems that people are really good at

601
00:53:31,280 --> 00:53:34,880
what they're really bad about is explaining to you how they do what they do but they're really

602
00:53:34,880 --> 00:53:39,280
good at solving these problems so a lot of what's been going on in the reinforcement learning world

603
00:53:39,280 --> 00:53:44,400
in particular is taking advantage of people learning from getting people to tell you something

604
00:53:44,400 --> 00:53:49,680
or to demonstrate something to you about how to do something so that you can learn much much faster

605
00:53:49,680 --> 00:53:54,160
than you ever would and really what you're getting out of it is you're getting human beings the

606
00:53:54,160 --> 00:53:58,560
human beings assumptions about the way the world works and you're taking advantage of those

607
00:53:58,560 --> 00:54:03,280
assumptions to narrow down the to narrow down the search base so I'll give you really I'll give

608
00:54:03,280 --> 00:54:09,760
you a really quick example so it turns out that people do not think about things and

609
00:54:11,280 --> 00:54:15,680
atomic actions they tend to think about them in these big temporarily extended views of the world

610
00:54:15,680 --> 00:54:20,000
so that takes something like Pac-Man right if you asked if I asked you to explain Pac-Man to me

611
00:54:20,000 --> 00:54:23,360
you would be describing in terms of up down left right or what you would say things like oh well

612
00:54:23,360 --> 00:54:28,400
look you need to get the power pellet you need to avoid the ghosts you need to you know you need to

613
00:54:28,400 --> 00:54:34,000
do these four or five things and we run experiments on this where we ask people to to create buttons

614
00:54:34,000 --> 00:54:38,640
that they would use if to to make Pac-Man go faster and they come up with these interesting

615
00:54:38,640 --> 00:54:43,760
buttons these sort of long term things but dividing the world up like that not from up down left

616
00:54:43,760 --> 00:54:48,640
right but into get the power pellet avoid the ghost is something that is very difficult to learn

617
00:54:48,640 --> 00:54:53,360
from scratch but people have already figured this out so you build systems where people are able

618
00:54:53,360 --> 00:55:00,320
to express to you those tricks those shortcuts those assumptions about the world and then you can

619
00:55:00,320 --> 00:55:07,200
learn so much faster than you would ever be able to do on your own and that's kind of where we're

620
00:55:07,200 --> 00:55:11,040
getting a lot basically taking assumptions from the world and getting them automatically from

621
00:55:11,040 --> 00:55:14,960
humans I think that's incredibly important and one of the reasons I think it's important by the way

622
00:55:14,960 --> 00:55:22,160
is because so many of the problems that we actually care about involve people right they involve

623
00:55:22,160 --> 00:55:26,240
other people they involve interacting with people and so you have to understand the fundamental

624
00:55:26,240 --> 00:55:29,520
assumptions that people are living and and you have to take advantage of them if you're ever going

625
00:55:29,520 --> 00:55:33,520
to learn so those are two areas that I happen to think are are really cool in the reinforcement

626
00:55:33,520 --> 00:55:40,400
learning space right now are we also learning how to enable the machines to make the assumption the

627
00:55:40,400 --> 00:55:47,120
assumptions themselves like what's happening there yeah but the way they do it is they kind of do it

628
00:55:47,120 --> 00:55:52,080
they do it by dint of observing the world right there's a Michael Lippman always says a couple of

629
00:55:52,080 --> 00:55:56,720
things that I really like him and one is that you know if the person who's doing the programming

630
00:55:56,720 --> 00:56:01,280
is doing all the learning and writing down the data structures then you're stuck right you need

631
00:56:01,280 --> 00:56:05,600
the machine itself to be able to learn its own data structures through observation it needs to be

632
00:56:05,600 --> 00:56:10,000
able to to build its own assumptions and its own models if you're always giving it the model then

633
00:56:10,000 --> 00:56:14,400
it's always depending upon you to give it the model it has to be able to to build its its own model

634
00:56:14,400 --> 00:56:19,120
so fundamental to that is this idea that that you're going to learn these you're going to build in

635
00:56:19,120 --> 00:56:22,240
your own assumptions you're going to learn new assumptions and you're going to build models that

636
00:56:22,240 --> 00:56:27,520
you're you're willing to adapt and so yes yes that that's definitely built into it it's definitely

637
00:56:27,520 --> 00:56:33,360
a part of what's going on but the problem is absent nothing number absent anything you you can't

638
00:56:33,360 --> 00:56:38,720
know where to start and so this gets his back full circle to this idea that learning is a

639
00:56:38,720 --> 00:56:43,440
social exercise right as human beings we interact with other human beings that have a bunch of

640
00:56:43,440 --> 00:56:47,600
assumptions they built the world together and they kind of know how it works and a lot of your

641
00:56:47,600 --> 00:56:52,560
job is to figure out what it is they've built into the world as assumptions so that you can begin

642
00:56:52,560 --> 00:56:56,960
to learn and our machines have to be able to do the same thing or otherwise they're not actually

643
00:56:56,960 --> 00:57:03,920
living in the same world that we're living in. Interesting interesting one of the one of the

644
00:57:03,920 --> 00:57:11,200
papers that I pulled up of yours on archive is a paper perceptual reward functions which is

645
00:57:11,200 --> 00:57:17,680
pretty recently published and that goes into I think the the former of these two areas that you

646
00:57:17,680 --> 00:57:24,080
mentioned where you're trying to map kind of the deep learning to you know a broader set of

647
00:57:24,080 --> 00:57:31,680
problems. Can you describe that the paper? That is relatively new stuff there's a bunch of new

648
00:57:31,680 --> 00:57:36,240
things that are coming coming out about about that at the student mine actually Edwards is really

649
00:57:36,240 --> 00:57:45,520
buying into the fundamental idea there is that you know people have people's reward functions so

650
00:57:45,520 --> 00:57:48,080
if you're a machine learning guy right you're particularly reinforcement learning guy you start

651
00:57:48,080 --> 00:57:52,880
talking about rewards and you start talking about states and you divide the world up into the abstract

652
00:57:52,880 --> 00:57:57,120
space and you go that's how you solve problems but we spend most of our time never actually worrying

653
00:57:57,120 --> 00:58:02,400
about where these things come from they're just given to us and this paper is a part of actually a

654
00:58:02,400 --> 00:58:06,800
larger body of work that that I've been I've been paying a little bit of attention to the last

655
00:58:06,800 --> 00:58:11,120
couple of years of trying to figure out where those things come from are there principles about

656
00:58:11,120 --> 00:58:16,000
where reward functions come from are there principles about where state comes from at least with

657
00:58:16,000 --> 00:58:21,840
respect to the way human beings deal with it so that you can actually solve these problems in

658
00:58:21,840 --> 00:58:26,800
general and be more robust to small changes in the environment one of the things that that's true

659
00:58:26,800 --> 00:58:31,840
about reinforcement learning is you know there's a nice little math equation that you need in order

660
00:58:31,840 --> 00:58:36,320
to figure out how to learn and determine value and it's very nice and it's very elegant but it's

661
00:58:36,320 --> 00:58:41,200
actually quite fragile so if I were to build a system let's say a robot and I wanted this robot

662
00:58:41,200 --> 00:58:46,480
to get from one end of a hallway to another and along the way it might do some other interesting

663
00:58:46,480 --> 00:58:52,320
things I can construct all my little alphas and my my learning rates and I can put everything together

664
00:58:52,320 --> 00:58:56,560
so that eventually it will learn and that it will do exactly what you want it to do and it won't get

665
00:58:57,280 --> 00:59:01,760
so scared that something battle happened that it won't move and it won't get so distracted by

666
00:59:01,760 --> 00:59:05,120
some interesting thing over here to the left that it'll never get to the end of the hallway I can

667
00:59:05,120 --> 00:59:10,080
actually do that pretty well but then if I take that robot and all that it's learned and then I

668
00:59:10,080 --> 00:59:15,920
make the hallway five inches longer it will stop working right because the math is very brittle

669
00:59:15,920 --> 00:59:21,280
everything is set up just right so that everything kind of touches one another and what you

670
00:59:21,280 --> 00:59:24,720
want to do is you want to build systems that are robust to that you want to build systems that

671
00:59:24,720 --> 00:59:30,880
adapt to that and it turns out that human beings are very good I mean in fact optimized in some

672
00:59:30,880 --> 00:59:35,280
ways for dealing with you know it's still a niche environment right we we do pretty well on earth

673
00:59:35,280 --> 00:59:40,160
we don't we won't do pretty well on Mars right we don't do pretty well in space but but you know

674
00:59:40,160 --> 00:59:44,720
it's still a rich environment that we're in and you want to build systems that can do that and so

675
00:59:44,720 --> 00:59:52,640
the perceptual reinforcement learning stuff is about using what we get from our perceptions

676
00:59:52,640 --> 00:59:57,600
directly as the as the notion of state and as our notion of reward that we try to get things to

677
00:59:57,600 --> 01:00:02,720
look like what we see we try to imitate the things that we see through through our perceptions

678
01:00:02,720 --> 01:00:09,520
rather than you know build simple or actually complex optimization functions that tell us you

679
01:00:09,520 --> 01:00:13,840
know whether this thing actually is like that thing no you just think about what it is that you see

680
01:00:13,840 --> 01:00:17,600
what it is that you're what it is you're receiving and there's this sort of larger philosophy

681
01:00:17,600 --> 01:00:22,000
around that I'm actually quite excited about the work I think what it allows us to do is to stop

682
01:00:22,000 --> 01:00:28,240
thinking about reinforcement learning as five you know a five tuple where you have to set the values

683
01:00:28,240 --> 01:00:34,480
and start thinking about it as a larger programming problem where the whole thing is it's reinforcement

684
01:00:34,480 --> 01:00:39,040
learning is not the thing that you start with it's the mechanism by which you happen to solve the

685
01:00:39,040 --> 01:00:43,840
problem it is itself a programming language is itself a wave of viewing the world and you've got

686
01:00:43,840 --> 01:00:48,880
a step back to the level of task and problem instead of thinking about solving this particular

687
01:00:48,880 --> 01:00:55,120
equation interesting yeah I thought the example that was provided in the introduction to the paper

688
01:00:55,120 --> 01:01:03,920
was a good one that was tip training robot to to fold origami like what you know what's the state

689
01:01:03,920 --> 01:01:09,840
of an origami and how do you how would you represent that traditionally you know whereas the

690
01:01:09,840 --> 01:01:15,520
what's natural for us as humans is to see a picture of the final result and you know how do

691
01:01:15,520 --> 01:01:22,800
you define a you know a score metric or a distance metric from you know a given current origami

692
01:01:22,800 --> 01:01:28,800
to this target yeah it's and it's a rich problem too because as soon as if I asked you to explain

693
01:01:28,800 --> 01:01:32,080
me how to do origami which by the way I have absolutely no idea how to do something magic with

694
01:01:32,080 --> 01:01:35,840
your hands paper you do a flurry of things and then suddenly there's a dragon I don't really

695
01:01:35,840 --> 01:01:39,760
know what happens but you know you start saying oh well you start thinking about folding and you

696
01:01:39,760 --> 01:01:44,480
start talking at this very high level just like with with Pac-Man right and the way of dividing up

697
01:01:44,480 --> 01:01:48,800
that world is actually important because if you don't divide up the world in the right way you will

698
01:01:48,800 --> 01:01:53,760
never in a million years a billion year in the lifetime of the universe actually solve the problem

699
01:01:53,760 --> 01:01:58,400
because there's just too many possibilities right this goes all the way back to to language learning

700
01:01:58,400 --> 01:02:05,360
and you know it turns out that people do not actually correct their children right so you don't

701
01:02:05,360 --> 01:02:10,560
get any negative examples hardly at all when you're a kid and yet somehow children learn to speak

702
01:02:10,560 --> 01:02:15,600
their particular language even though nobody's telling them when they're you think you are but you

703
01:02:15,600 --> 01:02:19,360
don't actually correct your children and we can prove to you mathematically that you can't learn

704
01:02:19,360 --> 01:02:23,280
under those circumstances so the only way it can be happening is if the world has been divided

705
01:02:23,280 --> 01:02:27,280
up in the nice little ways and there's only a few possibilities and you're searching over those

706
01:02:27,280 --> 01:02:31,520
few possibilities because the world's already been divided up for you if you have to go to the trouble

707
01:02:31,520 --> 01:02:36,160
of dividing up the world yourself then you're just there is enough time there aren't enough examples

708
01:02:36,160 --> 01:02:43,520
there isn't enough time yeah yeah yeah so now at the risk oh go ahead no I'm just gonna say so to

709
01:02:43,520 --> 01:02:48,320
me if you pop up to the AI level instead of the machine learning level right uh that's really the

710
01:02:48,320 --> 01:02:51,680
interesting thing right but what's really exciting about AI right now what's really exciting about

711
01:02:51,680 --> 01:02:56,560
machine learning right now is that we finally have enough computing power we finally have enough

712
01:02:56,560 --> 01:03:02,320
mathematical sophistication and we finally have enough data that we can actually start solving

713
01:03:02,320 --> 01:03:06,960
really hard problems where we're going to be forced to move beyond you know the equation

714
01:03:06,960 --> 01:03:12,560
that we wrote down in 1965 that hasn't changed to thinking about bringing in all of these other

715
01:03:12,560 --> 01:03:17,440
things whether it's marketing and behavioral economics whether whether it's game theory whether

716
01:03:17,440 --> 01:03:21,440
it's well engineering whether it's control you know we actually going to have to bring in tons of

717
01:03:21,440 --> 01:03:25,920
other things in order to solve the problems we're now at the point where we can actually do that

718
01:03:25,920 --> 01:03:29,440
so we're actually meeting in the middle so that so this is why this is an exciting time for me

719
01:03:30,080 --> 01:03:36,000
nice nice so at the risk of asking a question that we've kind of touched on in a couple different

720
01:03:36,000 --> 01:03:42,880
ways already for someone who wants to dig deeper into the kind of stuff we were just talking about

721
01:03:42,880 --> 01:03:48,880
interactive machine learning and AI and reinforcement learning are there any places that you would

722
01:03:48,880 --> 01:03:54,880
point them to get started well I would start with just a basic machine learning class particularly

723
01:03:54,880 --> 01:03:58,240
one that covers reinforcement learning if you really are interested in reinforcement learning

724
01:03:58,240 --> 01:04:04,240
as a topic I mean you know rich sudden's book is freely available online it's a great place

725
01:04:04,240 --> 01:04:08,400
to start to kind of understand what's going on the class that I teach with Michael Litman is

726
01:04:08,400 --> 01:04:13,120
freely online there's lots and lots and lots and lots and lots of examples out there I would

727
01:04:13,120 --> 01:04:18,240
actually start with that and get the basics there's survey papers I mean Google is your friend in

728
01:04:18,240 --> 01:04:23,360
this case but if you're the if you're the kind of person who wants to have someone give you a

729
01:04:23,360 --> 01:04:29,120
nice brief overview of what's going on then you know hey start with my class just pick Michael

730
01:04:29,120 --> 01:04:32,720
Litman you can go to your desk you can get it for free just sort of skim through it and watch through

731
01:04:32,720 --> 01:04:40,480
it and you'll you'll figure out from there where to go and I would really I would really encourage

732
01:04:40,480 --> 01:04:45,920
people to pick a problem that they find interesting if you games are the things for you then start

733
01:04:45,920 --> 01:04:51,360
looking up the deep learn the deep reinforcement learning work on games there's a there's a bunch

734
01:04:51,360 --> 01:04:56,320
of work done recently on solving most of the Atari games using deep learning that's really

735
01:04:56,320 --> 01:05:00,800
interesting stuff the problem with starting there though is that oh now you have to know what

736
01:05:00,800 --> 01:05:04,000
convolution nets are and you know you're you're going to find yourself distracted for nine

737
01:05:04,000 --> 01:05:08,160
months while you learn enough math to figure out what's going on I would actually start top down I

738
01:05:08,160 --> 01:05:13,040
would start thinking about the problems what the issues are before I get so deep into the to the

739
01:05:13,040 --> 01:05:17,200
math that I get lost you don't want to lose the forest for the trees here and it is very easy

740
01:05:17,200 --> 01:05:20,880
to lose the forest for the trees because there's so much kind of interesting and very difficult

741
01:05:20,880 --> 01:05:25,520
math that's underneath all of this but really you want to keep sight of the goal right which is

742
01:05:25,520 --> 01:05:30,640
to build something that can learn over time can adapt over a year it can live for 20 years

743
01:05:30,640 --> 01:05:34,320
and continually learn and adapt and think about what that would mean think about what it would

744
01:05:34,320 --> 01:05:38,240
mean to you as a person and then start asking what kind of background you would need to have in

745
01:05:38,240 --> 01:05:44,240
order to build a system that does that that's great and I'll include links to a bunch of the things

746
01:05:44,240 --> 01:05:50,960
that you mentioned in the show notes um oh so I would let me add one thing to show notes

747
01:05:51,440 --> 01:05:55,680
you mentioned the book influence I would also recommend the media equation

748
01:05:56,160 --> 01:06:01,360
to media equation that is a fantastic book it's one of these it's a short book about how human

749
01:06:01,360 --> 01:06:07,840
beings actually behave and how it turns out that people will treat machines as if they're humans

750
01:06:08,400 --> 01:06:12,480
even though they know better because they'll treat anything that acts like it has intention

751
01:06:12,480 --> 01:06:17,520
as if it has intention and I think that fact alone should influence everyone who's thinking

752
01:06:17,520 --> 01:06:21,360
about building systems that have to interact with humans interesting we're not even all that good

753
01:06:21,360 --> 01:06:27,840
at describing intention other people the thought of applying it to machines is uh and we're going

754
01:06:27,840 --> 01:06:31,280
to have to work on that I actually think it's the other way around I think the problem is we're

755
01:06:31,280 --> 01:06:35,520
incredibly good at describing intentions to other people it's just not always the right intentions

756
01:06:35,520 --> 01:06:44,160
ah yeah yeah yeah um great so I think we uh this has been a great discussion um I appreciate you

757
01:06:44,160 --> 01:06:48,320
getting together with me for it especially on a Saturday morning and don't want to monopolize

758
01:06:48,320 --> 01:06:53,760
your Saturday so we'll wrap things up here anything else you'd like to uh toss out

759
01:06:54,480 --> 01:06:57,680
no just I really enjoy this and we should have this conversation again

760
01:06:57,680 --> 01:07:03,200
absolutely absolutely um and then for folks that want to get in touch with you

761
01:07:03,200 --> 01:07:08,960
they find you on google plus well if you go to google plus I'm the one guy who's still there

762
01:07:08,960 --> 01:07:14,640
so just send me an email it may take me a while to respond that I'm more than happy to respond

763
01:07:15,440 --> 01:07:21,920
just google in bell c.katek.edu okay and are you on twitter or any of the the the lesser use

764
01:07:21,920 --> 01:07:29,200
social networks I have a I have a twitter account uh and occasionally I even use it uh but really

765
01:07:29,200 --> 01:07:32,560
emails the only way to really get to me I'm unless you have my cell number and I'm not giving

766
01:07:32,560 --> 01:07:37,440
you my cell number nice nice all right great uh well thanks so much Charles really appreciate it um

767
01:07:38,240 --> 01:07:41,520
and uh next time look absolutely awesome

768
01:07:48,640 --> 01:07:53,040
all right everyone that's our show for today thanks so much for listening

769
01:07:53,040 --> 01:07:57,280
if you're one of our lucky winners or runners up please reach out to me

770
01:07:57,280 --> 01:08:04,160
at sam at twimlai.com a bunch of you have asked hey what's up with the newsletter

771
01:08:04,160 --> 01:08:09,680
no you haven't missed anything I've just been crazy busy and haven't had a chance to get one out

772
01:08:09,680 --> 01:08:15,600
I'm so sorry about that I'm still working on it and I'll keep you posted thank you so much for

773
01:08:15,600 --> 01:08:27,360
your support and catch you next time

