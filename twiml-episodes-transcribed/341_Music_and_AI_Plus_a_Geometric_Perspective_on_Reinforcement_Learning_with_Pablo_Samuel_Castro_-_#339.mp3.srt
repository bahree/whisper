1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:46,960
I'm your host Sam Charrington.

4
00:00:46,960 --> 00:00:50,400
Thank you very much for having me, this is a real pleasure to be here.

5
00:00:50,400 --> 00:00:51,400
Awesome.

6
00:00:51,400 --> 00:00:52,400
Thanks so much.

7
00:00:52,400 --> 00:00:56,200
I am really excited to jump into this conversation.

8
00:00:56,200 --> 00:01:00,480
You are someone that I follow on Twitter and we've had these kind of back and occasional

9
00:01:00,480 --> 00:01:06,240
back and forths over time and it's great to finally meet you in person.

10
00:01:06,240 --> 00:01:09,280
You've got some pretty varied interests.

11
00:01:09,280 --> 00:01:14,120
You spend a lot of time, your research focus on reinforcement learning.

12
00:01:14,120 --> 00:01:17,480
You also tweet a lot about music and the arts.

13
00:01:17,480 --> 00:01:25,280
Looking at your background, you've done applied ML stuff at Google, add from and other things.

14
00:01:25,280 --> 00:01:29,600
Tell us the story, how do all these threads come together.

15
00:01:29,600 --> 00:01:35,240
Well originally I'm from Ecuador and I moved to Canada after high school to come study

16
00:01:35,240 --> 00:01:36,880
at McGill.

17
00:01:36,880 --> 00:01:42,600
So I did my undergrad and then I eventually did my master's in PhD at McGill with

18
00:01:42,600 --> 00:01:48,480
Dwayna Prikup and Prakash Panagaden and so part of the reason why I stayed in Montreal

19
00:01:48,480 --> 00:01:55,000
and McGill was for personal reasons, I was dating someone who's now my wife and I also

20
00:01:55,000 --> 00:02:01,680
had a band and I also had a band so I've always been heavily involved with music.

21
00:02:01,680 --> 00:02:05,840
I grew up with music, learning music, playing music so that was very important to me and

22
00:02:05,840 --> 00:02:09,080
I didn't want to leave that so I decided to make that choice.

23
00:02:09,080 --> 00:02:14,840
I know it's not the typical thing that suggested to do all your degrees in the same university

24
00:02:14,840 --> 00:02:18,680
but for me it was more important to keep playing music.

25
00:02:18,680 --> 00:02:24,760
So I graduated, I finished my PhD at around 2011 and then I moved to Paris for a post-doc

26
00:02:24,760 --> 00:02:29,040
and this was at a time where AI isn't what we see here with 12,000 people in this conference

27
00:02:29,040 --> 00:02:34,120
I mean NURPS didn't have, back then it was called NIPS, maybe 4,000 people.

28
00:02:34,120 --> 00:02:39,640
So I wanted to stay in academia and I was working at this intersection, that was very theoretical

29
00:02:39,640 --> 00:02:44,000
sort of between Markov decision processes and formal verification.

30
00:02:44,000 --> 00:02:48,320
So I was finding it really hard to find a job because I wasn't formal verification enough

31
00:02:48,320 --> 00:02:52,200
for the formal verification community and I wasn't reinforcement learning enough for

32
00:02:52,200 --> 00:02:54,520
the reinforcement learning community.

33
00:02:54,520 --> 00:02:58,880
And so after my post-doc I just feared, I already had two young kids and I feared that I would

34
00:02:58,880 --> 00:03:02,040
just be going post-doc to post-doc for too long.

35
00:03:02,040 --> 00:03:06,600
So I luckily got a job from Google doing applied machine learning and ads and I actually

36
00:03:06,600 --> 00:03:11,720
said goodbye to academia at that point, I stopped reading papers and then I did a little

37
00:03:11,720 --> 00:03:17,440
quick stint in Chrome doing building machine learning infrastructure, so back end infrastructure

38
00:03:17,440 --> 00:03:22,240
and brain opened up in Montreal and Mark Belmar who I had done my masters with, he kept

39
00:03:22,240 --> 00:03:26,480
in research, he was in deep mind for a while and he was one of the first people to join

40
00:03:26,480 --> 00:03:32,440
brain in Montreal and he put in a good word for me and so then they offered me to join

41
00:03:32,440 --> 00:03:38,240
them and I jumped at that possibility and I hadn't been following the research that also

42
00:03:38,240 --> 00:03:41,960
was a huge shock to come back, I mean when I was doing my research we were all working

43
00:03:41,960 --> 00:03:45,360
on grid worlds and very simple environments because a lot of it was theoretical, we didn't

44
00:03:45,360 --> 00:03:49,120
really use deep networks at all for reinforcement learning.

45
00:03:49,120 --> 00:03:54,600
So there was a lot of catch up trying to familiarize myself with the literature and how the

46
00:03:54,600 --> 00:03:57,320
whole landscape had changed.

47
00:03:57,320 --> 00:04:01,680
So throughout all this time I always kept with music, I had a few different bands, I've

48
00:04:01,680 --> 00:04:06,360
always been performing live and writing music and the other thing is when I started my

49
00:04:06,360 --> 00:04:12,080
PhD I was actually considering doing a PhD with Douglas Eck as well as with Doiner Precup

50
00:04:12,080 --> 00:04:17,720
in something with machine learning and music but at the time what was available for music

51
00:04:17,720 --> 00:04:22,840
generation didn't really excite me very much because it was still in the early days and

52
00:04:22,840 --> 00:04:29,440
I feared that it would taint my love of music and I just wanted to keep my music aside

53
00:04:29,440 --> 00:04:30,440
separate.

54
00:04:30,440 --> 00:04:34,720
But when I rejoined the research world and I saw what the magenta team was doing I was

55
00:04:34,720 --> 00:04:40,600
kind of blown away by the quality of things so then I decided to also start going along

56
00:04:40,600 --> 00:04:47,760
that pathway and I think the day after I joined Brain this artist from Canada, he's called

57
00:04:47,760 --> 00:04:50,400
David Usher, he's pretty well known in Canada.

58
00:04:50,400 --> 00:04:56,400
He approached us wanting to, he approached us, he was actually first had a band in the

59
00:04:56,400 --> 00:05:02,400
90s called Moist and it was really popular and he approached us, he wanted to do an album

60
00:05:02,400 --> 00:05:07,280
using like AI techniques and so we just met and kind of brainstormed and the thing he

61
00:05:07,280 --> 00:05:13,120
gravitated towards the most was lyrics and so Hugo La Hoshel who was my manager at the

62
00:05:13,120 --> 00:05:16,600
time was very generous because I had just joined Brain he's like do you want to take this

63
00:05:16,600 --> 00:05:20,240
project because I like music I said sure it sounds fun.

64
00:05:20,240 --> 00:05:24,720
I had never trained a language model, I was still trying to figure out all this deep

65
00:05:24,720 --> 00:05:30,040
network stuff because I hadn't looked at that but yeah, Hugo gave me that opportunity

66
00:05:30,040 --> 00:05:34,640
and I learned a ton in that project and so still it's still an ongoing project so relative

67
00:05:34,640 --> 00:05:39,720
to the first model that I trained with David which we actually made a video out of that

68
00:05:39,720 --> 00:05:44,160
like he rewrote one of his songs with this first prototype and it worked okay but the

69
00:05:44,160 --> 00:05:47,760
model we have now is so much better and I understand all of this language modeling so

70
00:05:47,760 --> 00:05:52,240
much better than when I did before and that's just that experience kind of showed me to

71
00:05:52,240 --> 00:05:56,080
not be afraid of sort of stepping out of because I'm very familiar in reinforcement

72
00:05:56,080 --> 00:06:01,400
learning which is background to step out of that comfort zone and go into other areas

73
00:06:01,400 --> 00:06:05,400
that I'm not as familiar with because they're all interesting problems and sort of really

74
00:06:05,400 --> 00:06:10,520
trying to dig into the details and for me the way I learned the most is actually trying

75
00:06:10,520 --> 00:06:14,640
to implement some of these models and architectures and play around with them because you read

76
00:06:14,640 --> 00:06:18,720
about them in papers and you kind of get it that's fine but until you're actually trying

77
00:06:18,720 --> 00:06:22,480
to get it to work for yourself it's that's a whole different experience and I've learned

78
00:06:22,480 --> 00:06:27,480
so much just from doing this like jumping from one one problem to to the next and in

79
00:06:27,480 --> 00:06:32,400
a separate kind of field and learning about those architectures but while still maintaining

80
00:06:32,400 --> 00:06:36,160
my research and reinforcement learning well it sounds like you've landed in an incredible

81
00:06:36,160 --> 00:06:43,840
place to do that not just kind of the resources of Google and the people that you're surrounded

82
00:06:43,840 --> 00:06:50,200
with and have an opportunity to interact with but your role seems to be defined as like

83
00:06:50,200 --> 00:06:56,720
advancing research you know via implementation absolutely yeah so I'm a software developer

84
00:06:56,720 --> 00:07:03,200
like that's my official title there's also research scientists at Google and until recently

85
00:07:03,200 --> 00:07:07,200
there were still like most people that are in research want to be research scientists

86
00:07:07,200 --> 00:07:14,600
because that's like then you're officially doing science right right so I my like if

87
00:07:14,600 --> 00:07:19,840
I had graduated say four years after when I graduated yeah likely I would have been applying

88
00:07:19,840 --> 00:07:24,440
for research scientist role back when I applied at Google that wasn't really a maybe

89
00:07:24,440 --> 00:07:30,040
Sammy Benjo was a research scientist but probably that about it and so I entered Google

90
00:07:30,040 --> 00:07:34,560
as a software engineer and sort of advance my career and that and that track and when

91
00:07:34,560 --> 00:07:38,600
I joined Google it was as a software engineer or developed we call it developer in Quebec

92
00:07:38,600 --> 00:07:44,640
because engineer you get an iron ring and I don't have that okay initially I was a little

93
00:07:44,640 --> 00:07:48,800
skeptical because the official description is you're there more supporting research scientists

94
00:07:48,800 --> 00:07:52,760
and so I was worried that I wouldn't have a flexibility to sort of pursue my own research

95
00:07:52,760 --> 00:07:57,600
interest but it's been not at all like that so I leave my own research projects and I

96
00:07:57,600 --> 00:08:02,720
still support a lot of people with the engineering aspects of it because I'm I've been working

97
00:08:02,720 --> 00:08:08,440
on this a lot so I'm more familiar with like Google infrastructure and just coding in general

98
00:08:08,440 --> 00:08:12,920
and it's been I mean a lot of the major advances that we see in machine learning and AI nowadays

99
00:08:12,920 --> 00:08:16,560
is a lot of it is engineering right so there is of course there's still math and there's

100
00:08:16,560 --> 00:08:22,280
still a lot of theory behind it but a lot of it is engineering and I don't think it I think

101
00:08:22,280 --> 00:08:28,240
more and more it is but a few years ago I don't feel like got the credit it really deserved

102
00:08:28,240 --> 00:08:33,680
and so living in this sort of intersection of pure engineering and pure research is for

103
00:08:33,680 --> 00:08:37,680
me super exciting because I kind of get to play around in both worlds and learn from both

104
00:08:37,680 --> 00:08:38,680
world.

105
00:08:38,680 --> 00:08:44,920
I've got a long list of things that I want to talk to you about but you mentioned something

106
00:08:44,920 --> 00:08:50,320
that's got me really curious the you know what it means to evolve a language model you

107
00:08:50,320 --> 00:08:55,680
started this prop project with David and came out with this you know early crappy language

108
00:08:55,680 --> 00:09:01,160
model and have evolved it over some number of years it's been like a yeah no it's been

109
00:09:01,160 --> 00:09:05,200
like a year and a half it's been actually it's been almost like two years I think since

110
00:09:05,200 --> 00:09:11,760
we started it but two years calendar calendar wise but it's not it's not one of my main

111
00:09:11,760 --> 00:09:16,760
projects so yeah exactly so it's when I get a chance that I work on that yeah so as I

112
00:09:16,760 --> 00:09:20,280
said when I started this project I had never trained a language model I like I

113
00:09:20,280 --> 00:09:24,560
knew what LSTM's were because I studied it in school but so the first thing I did was

114
00:09:24,560 --> 00:09:34,240
I actually Andre Carpati has the yeah this famous blog post the surprising reliability

115
00:09:34,240 --> 00:09:39,160
of of recurring neural networks something like that anyway so I brought that blog post

116
00:09:39,160 --> 00:09:44,160
and I got his code and sort of played around with it and that was the the V0 model just

117
00:09:44,160 --> 00:09:50,080
over characters and then I started tweaking that a bit and finding new data sets for lyrics

118
00:09:50,080 --> 00:09:57,880
and that initial model that was basically a variant of Andre Carpati's model was the initial

119
00:09:57,880 --> 00:10:02,120
model that I had and so that was okay like just a milestone like okay I was able to train

120
00:10:02,120 --> 00:10:06,320
this and actually get it to do what I wanted it to do but it obviously was it has all the

121
00:10:06,320 --> 00:10:12,240
shortcomings that these types of models do at this round I mean the attention is all you need

122
00:10:12,240 --> 00:10:17,760
paper had come out not not too much before then and so then I started looking into these

123
00:10:17,760 --> 00:10:23,480
attention models and and so it seemed like the right thing to do so I switched over to

124
00:10:23,480 --> 00:10:30,800
to the transformer model and started playing around with that and so the V2 model was using

125
00:10:30,800 --> 00:10:37,480
an attention model and so I had various versions of a V2 part of the difficulty that I had

126
00:10:37,480 --> 00:10:42,000
with the language with training these language models on lyrics data set is that the lyrics

127
00:10:42,000 --> 00:10:47,440
data set is is not the best what sense so the tricky thing about these language models

128
00:10:47,440 --> 00:10:52,600
is that and and for lyrics in particular is that you're trying to get this model to learn

129
00:10:52,600 --> 00:10:58,080
English kind of so how how to structure English phrases together but in a quote unquote poetic

130
00:10:58,080 --> 00:11:03,520
way and to not be boring right because you're trying to use it for creative purposes and

131
00:11:03,520 --> 00:11:06,880
so you don't want it to be boring so we trained this model and if you look at like perplexity

132
00:11:06,880 --> 00:11:10,400
scores and things like that it was doing pretty well on this lyrics data set but then when

133
00:11:10,400 --> 00:11:16,480
you actually look at the output it was extremely boring so because in pop songs you have lines

134
00:11:16,480 --> 00:11:21,840
that repeat often I mean that's just how songs are written so the model would tend to just

135
00:11:21,840 --> 00:11:27,200
repeat the same thing over and over and over again it also had certain phrases that it would

136
00:11:27,200 --> 00:11:31,760
keep on coming back to like they just had very high likelihood so one of you talked about this

137
00:11:31,760 --> 00:11:37,520
it I say like it's hand wavy but the the average pop line over the last six decades is you know

138
00:11:37,520 --> 00:11:43,200
that I'm the one and so that one came up a lot and you can also add sometimes you get you know

139
00:11:43,200 --> 00:11:49,120
that I'm the one come a baby so that's the average pop line it was boring and so the interesting

140
00:11:49,120 --> 00:11:53,520
thing about working with with David is that I build like variants of these models and I

141
00:11:53,520 --> 00:11:59,920
show him and one of the things he remarked on is that it was very non-specific in the sense

142
00:11:59,920 --> 00:12:05,600
that the nouns that it was using it wouldn't use proper nouns so it would use like me you he she

143
00:12:05,600 --> 00:12:10,960
they so it's very I'm kind of ambiguous if you think of like the Beatles I mean there's me

144
00:12:10,960 --> 00:12:15,760
Mr. Mustard Paul Avine Pam Jude you know there's all these I mean the fictional characters but

145
00:12:15,760 --> 00:12:20,960
they're characters yeah and so then you can sort of the ground the the song and something kind

146
00:12:20,960 --> 00:12:26,720
of real where's if you're just talking about him like hey you so let me down like that's not

147
00:12:26,720 --> 00:12:34,000
even though Pink Floyd has a hey you somewhere so anyway then from that feedback I started looking

148
00:12:34,000 --> 00:12:40,560
at other datasets so for instance the fiction books dataset these are available online so we

149
00:12:40,560 --> 00:12:45,440
traded with that and we got much more diverse vocabulary much more diverse themes than we would

150
00:12:45,440 --> 00:12:50,960
get with with lyrics but the structure of the what was coming out no longer looked like a song

151
00:12:50,960 --> 00:12:56,080
line like a song lyric it looked more just like a sentence that you'd sort of cut in half and

152
00:12:56,960 --> 00:13:03,760
it just looked like a sentence with new line characters thrown in random places so then I thought

153
00:13:05,040 --> 00:13:10,480
why not rather than trying to tackle this problem with a single language model where I'm trying

154
00:13:10,480 --> 00:13:16,320
to get all these constraints of like making being coherent from the English perspective being

155
00:13:16,320 --> 00:13:22,480
kind of creative and also following the structure of lyrics why try to do this all with one model

156
00:13:22,480 --> 00:13:29,680
like there for me there's no where you go to multiple models with the you know the language models

157
00:13:29,680 --> 00:13:36,480
you're working at working with at the time is there a way are you trying to express those constraints

158
00:13:36,480 --> 00:13:41,120
explicitly or just based on the data that you're feeding into it was I was more playing with the

159
00:13:41,120 --> 00:13:46,320
data so with the with the training data that I was using to try to enforce these these constraints

160
00:13:47,920 --> 00:13:53,200
so the the lyrics I mean the idea was well if we want to write lyrics then we should train

161
00:13:53,200 --> 00:13:58,000
on the lyrics dataset to try to replicate that distribution but there's as I'm saying it a lot

162
00:13:58,000 --> 00:14:02,800
of the interesting lyrics are in the long tail of the distribution and the long tail is terrible

163
00:14:02,800 --> 00:14:07,360
for a model because it's going to have low likelihood by definition but that's actually what

164
00:14:07,360 --> 00:14:14,000
we're interested in as like it so it has to be coherent but unlikely and so so that's part of

165
00:14:14,000 --> 00:14:21,360
the reason why I I was considering just going to why why try to do it with one single model

166
00:14:22,800 --> 00:14:30,000
as my ultimate my ultimate goal was to have something that artists could actually use and David

167
00:14:30,000 --> 00:14:36,640
could use for for writing a whole new song from scratch and so for him it doesn't like he

168
00:14:36,640 --> 00:14:40,720
won't care and most artists won't care whether it's a single model or multiple models as long as

169
00:14:40,720 --> 00:14:48,880
it is interesting and makes sense and just to understand the the goal is it start with some prompt

170
00:14:48,880 --> 00:14:54,880
and the thing spits out an entire song or kind of is the artists or is the model one in which the

171
00:14:54,880 --> 00:15:00,160
artist is kind of incrementally promptly in the model and kind of refining the output yeah it's

172
00:15:00,160 --> 00:15:04,160
more of the latter so the way we're starting to build out a website that I'm hoping to release

173
00:15:04,800 --> 00:15:10,720
early next year so it's it's almost ready but it's just as I said it's one of one project that I

174
00:15:10,720 --> 00:15:15,760
work on when I have time and with nirips and all that it's been kind of tight but the ideas yeah you

175
00:15:15,760 --> 00:15:20,640
have kind of like an almost like a notepad where you where you're writing your song and this is

176
00:15:20,640 --> 00:15:27,040
coming a lot from feedback from david with how about how he writes songs and so you have you write

177
00:15:27,040 --> 00:15:31,760
your part of your line or you can even start without a prompt and then just ask the the model for

178
00:15:31,760 --> 00:15:36,880
suggestions and so then the model will give you suggestions for the next line you can specify

179
00:15:36,880 --> 00:15:42,240
rhyming schemes so whether like if you want a bba or if you just want it to rhyme with your

180
00:15:42,240 --> 00:15:46,400
current line and then the model will give you some suggestions and then you can like drag those

181
00:15:46,400 --> 00:15:53,680
suggestions over to your your worksheet or inspire yourself from those the suggestions that the

182
00:15:53,680 --> 00:15:57,680
model gave and and then continue working with with your song this way so it's more

183
00:15:59,520 --> 00:16:03,200
what I what I'm not as interested in is having something where you press a button and it produces

184
00:16:03,200 --> 00:16:09,600
your next top 40 hit it's really like a tool the way of view it is as a tool so one analogy I like

185
00:16:09,600 --> 00:16:15,600
to use is if you think of recording music in the 60s or 70s if you wanted to record a good album

186
00:16:15,600 --> 00:16:20,560
you had to go to a professional recording studio and hire a professional recording engineer

187
00:16:20,560 --> 00:16:25,200
because otherwise it wouldn't sound very good now we have things like Pro Tools and Ableton

188
00:16:25,200 --> 00:16:29,280
that you can do it in your basement and like a lot of people have become very famous from

189
00:16:29,280 --> 00:16:33,840
recording albums completely by themselves in their basement so it's not like we got rid of

190
00:16:34,400 --> 00:16:40,000
recording engineers or recording studios they're they're still around and they're doing the

191
00:16:40,000 --> 00:16:46,560
same thing and you still require them for for many for many purposes but these tools like Pro Tools

192
00:16:46,560 --> 00:16:51,920
and Ableton that kind of democratize the recording industry and so a lot of people can start playing

193
00:16:51,920 --> 00:16:57,600
around with recording and maybe pursue music as a career by leveraging what they can do on their

194
00:16:57,600 --> 00:17:04,240
own and so I view these tools as sort of in the same vein where the idea is not to sort of replace

195
00:17:04,240 --> 00:17:10,880
musicians or songwriters but to enhance them so basically give them something that they can play

196
00:17:10,880 --> 00:17:15,360
with the other thing that I find interesting that that could be quite useful for this is if

197
00:17:15,360 --> 00:17:19,520
your first language is in English and but you want to write a song in English because you're

198
00:17:19,520 --> 00:17:24,160
living in English, the language in country this tool might be helpful for because it does provide

199
00:17:25,200 --> 00:17:29,440
well well structured sentences it might help you write your song because you can provide the

200
00:17:29,440 --> 00:17:35,040
context and sort of the themes and the model would help you write in a way that that's more natural

201
00:17:35,840 --> 00:17:39,440
sounding English than than than what you would be able to come up with on your own.

202
00:17:39,440 --> 00:17:44,960
Okay and you mentioned that you're the one of the things the artist is able to do is put in kind

203
00:17:44,960 --> 00:17:50,880
of the rhyming scheme that you're interested in is this kind of a filter after the model is generating

204
00:17:52,560 --> 00:17:56,480
possibilities or is it some kind of constraint that's introduced into model itself?

205
00:17:56,480 --> 00:18:01,600
It's not even no constraint or filter the way it's happening is by nature of how we're training

206
00:18:01,600 --> 00:18:07,280
the model so I have as I mentioned we move to this this system where we have two models so one is

207
00:18:07,280 --> 00:18:10,960
what I call the structure model so they're both transformer models but the first one is trained

208
00:18:11,920 --> 00:18:17,840
on the lyrics data sets but rather than using the English words I convert them to other parts

209
00:18:17,840 --> 00:18:23,200
of speech so like now an advert that type of thing and it also includes like the number of syllables

210
00:18:23,200 --> 00:18:29,200
in the line and then the final phoneme syllable so like basically this is for rhyming

211
00:18:30,240 --> 00:18:34,560
and so this model if you give it the structure of your current line it will the output of it

212
00:18:35,040 --> 00:18:39,680
what it generates will be the structure of the next line and so then you can feed that

213
00:18:39,680 --> 00:18:44,240
next line structure with the actual English words of your first line into the second model

214
00:18:44,880 --> 00:18:49,120
and this is another transformer model which I call the vocabulary transformer and so this

215
00:18:49,120 --> 00:18:53,440
one is trained on on books on the books data set because we want the diverse vocabulary from that

216
00:18:54,000 --> 00:18:57,440
and so what this model is trained to do is to fill in these blanks so these

217
00:18:57,440 --> 00:19:02,560
cat parts of speech and the condition on the condition on the so yeah so the first half gives it

218
00:19:02,560 --> 00:19:06,400
the context right because it's using English words the second half gives it the structure so

219
00:19:06,400 --> 00:19:10,800
the number of syllables what parts of speech and the last phoneme so we trained it with the books

220
00:19:10,800 --> 00:19:18,080
data set by essentially because we know the ground truth we can specify when you have this context

221
00:19:18,080 --> 00:19:23,280
and this structure this is the last phoneme and so the the the target that we're training to emulate

222
00:19:24,080 --> 00:19:30,000
matches that so it rhymes really well the one thing that we're working on right now is that

223
00:19:30,640 --> 00:19:37,120
the easiest way to rhyme is to just use the same word right so red always rhymes with red right

224
00:19:37,120 --> 00:19:42,960
so that's something that that we're exploring a few different approaches on how to overcome that

225
00:19:42,960 --> 00:19:48,640
so that we've tried a few things playing like the first things we tried just again playing

226
00:19:48,640 --> 00:19:54,000
with the data to see if we could discourage it from from rhyming with the same word so far none of

227
00:19:54,000 --> 00:19:59,520
those things have worked very well so like an easy thing you can do is basically when you have the

228
00:20:00,800 --> 00:20:06,080
the weights for the the possible tokens that the model kind of met just set the weight to zero

229
00:20:06,080 --> 00:20:10,560
for the words that you don't want to rhyme and then the model is forced to pick a different word

230
00:20:10,560 --> 00:20:15,760
when when decoding so we're trying a few other things that are a bit more exciting for us

231
00:20:16,800 --> 00:20:21,200
but yeah that's essentially how we get the rhyming for the model is this project is this has

232
00:20:21,200 --> 00:20:27,120
this been published and are you working on the website but like yeah we had a paper in the workshop

233
00:20:27,120 --> 00:20:32,960
the creativity workshop two years ago okay or last no it was last year sorry in the creativity

234
00:20:32,960 --> 00:20:38,160
workshop nerfs in Montreal last year but it's a two-page viewer I think it's on our archive but it's

235
00:20:38,160 --> 00:20:44,240
very like summarized to remember somebody asked me the one to collaborate on the project I sent

236
00:20:44,240 --> 00:20:49,680
them the link and he's like this looks like a proposal it's not a paper so yeah that paper is

237
00:20:49,680 --> 00:20:55,200
still very preliminary we have a more extended version but we are thinking of submitting it to

238
00:20:55,200 --> 00:20:59,920
somewhere we're just trying to refine the model so the paper itself is not published but we're

239
00:20:59,920 --> 00:21:04,640
working on on basically getting more human feedback because we can we have some quantitative

240
00:21:04,640 --> 00:21:09,600
measures but because it's the purpose is creative it's kind of hard to measure quantitatively so

241
00:21:09,600 --> 00:21:14,800
we want to get more humans to evaluate and measure it compared to other models or even true lyrics

242
00:21:16,080 --> 00:21:21,040
so we're working on that and also we're working on making the code open source so you can take it

243
00:21:21,040 --> 00:21:26,560
and train with whatever dataset you want so training with Wikipedia or something and you get

244
00:21:26,560 --> 00:21:31,600
what's the training time or kind of resource for the training well so we train we've been

245
00:21:31,600 --> 00:21:38,480
training on TPUs and that's really fast so the longest distributed like lots of TPUs

246
00:21:38,480 --> 00:21:45,120
no just yes okay and it's been it's been it's pretty fast on the order of like we were training

247
00:21:45,120 --> 00:21:50,800
less than in less than an hour oh wow so yeah it's great fast the the longest part is the pre-processing

248
00:21:50,800 --> 00:21:54,960
of the of the dataset if your dataset is very large because we do this decomposition into parts

249
00:21:54,960 --> 00:22:00,560
of speech and that type of thing that's typically what what takes the longest but once you do it once

250
00:22:00,560 --> 00:22:06,720
and then you can reuse it for whenever interesting I can I ask the question earlier about

251
00:22:07,520 --> 00:22:13,280
applying external constraints into the modeling process is there does that work with

252
00:22:13,280 --> 00:22:17,760
transformer models or their folks that are doing that kind of thing like almost like a model-based

253
00:22:17,760 --> 00:22:24,560
kind of there are yeah there's some work with that I've seen so there's there's some people

254
00:22:24,560 --> 00:22:29,520
in the Toronto office from brain that have been doing some there's one paper called the insertion

255
00:22:29,520 --> 00:22:34,800
transformer so rather than be coding left to right as you typically do with with language models

256
00:22:34,800 --> 00:22:39,360
I don't know all the details but essentially the the way it works is now you can emit tokens that

257
00:22:39,360 --> 00:22:44,080
say where to insert so not to go from left to right but actually inserting into different parts so

258
00:22:44,080 --> 00:22:49,600
this allows it to potentially start from like a higher level of the structure of the phrase that

259
00:22:49,600 --> 00:22:55,200
it's going to generate and then sort of start filling in more of the details once the structure

260
00:22:55,200 --> 00:22:59,520
is is there and so it's actually something I've been meaning to look at because this might be

261
00:22:59,520 --> 00:23:05,520
kind of useful for lyrics where you're trying to to satisfy some structure so maybe we can

262
00:23:05,520 --> 00:23:10,640
generate a full verse rather than line by line if we have if we start doing in this kind of

263
00:23:10,640 --> 00:23:16,160
incremental incremental way there's also like from the magenta team the the the music transformer

264
00:23:16,160 --> 00:23:22,800
that they've been doing and that I don't know I don't think they explicitly encode constraints

265
00:23:22,800 --> 00:23:28,720
but for instance the way they they encode time is is a bit different than than what we use for

266
00:23:28,720 --> 00:23:35,680
for language modeling and so that just the result of that is is for music at least is a more coherent

267
00:23:35,680 --> 00:23:42,000
and more pleasing we have started looking a little bit in reinforcement learning and seeing how

268
00:23:42,000 --> 00:23:46,080
we could potentially use that I'm just going to ask is there an overlap or interplay with our

269
00:23:46,080 --> 00:23:53,520
there is so we are looking into it a little bit it's tricky it's tricky to get right and uh yeah my

270
00:23:53,520 --> 00:23:59,040
thinking is always I don't want to over complexify it if I don't have to so that's why I always start

271
00:23:59,040 --> 00:24:03,120
with the simplest idea and if that isn't working then okay we start considering a bit more

272
00:24:03,120 --> 00:24:08,560
sophisticated things for many reasons I mean when something breaks it's easier to fix if it's

273
00:24:08,560 --> 00:24:13,920
yeah if it's simpler than if it's if you have a lot a lot of moving parts um and I also feel like

274
00:24:13,920 --> 00:24:18,640
with the reinforcement learning aspect of it um the little I've been playing with it with for

275
00:24:18,640 --> 00:24:24,160
this particular project it's a little harder to make it stable in the sense that you can kind of

276
00:24:24,160 --> 00:24:30,160
reproduce the same type of quality that from run to run so it might be interesting to an instructive

277
00:24:30,160 --> 00:24:35,520
to kind of explore like what's your first step how do you think about okay you've got this one tool

278
00:24:35,520 --> 00:24:41,280
that you know well or al you've got this use case area that you want to apply it to like how do

279
00:24:41,280 --> 00:24:49,120
you start to even formulate the problem um it's uh this is the way I work it's more trying to solve

280
00:24:49,120 --> 00:24:53,040
very specific problems that I have with so this is what I'm saying where I like to start with

281
00:24:53,040 --> 00:24:58,320
just a simple thing um because it probably won't work the way you want it and and there will be

282
00:24:58,320 --> 00:25:04,160
very concrete aspects that or problems that you have with it at the at the moment and so then

283
00:25:04,160 --> 00:25:10,400
you I like to focus on those particular problems and see what techniques I can use so um first

284
00:25:10,400 --> 00:25:15,760
is for for this rhyming constraint uh there is some prior work that that has done similar type

285
00:25:15,760 --> 00:25:21,680
things not with transformers but with with R&N so um Natasha Jacques when when she did a uh uh

286
00:25:21,680 --> 00:25:25,760
an internship with the magenta team she had a paper where she was using RL to

287
00:25:26,560 --> 00:25:32,800
for like an R&N that produces melodies um she was trying to use RL to make it encourage it

288
00:25:32,800 --> 00:25:37,600
to to respect the rules of counterpoint so counterpoint is this set of rules from music theory that

289
00:25:37,600 --> 00:25:43,520
um basically specifies how to write music for polyphonic voices so that it sounds better

290
00:25:44,080 --> 00:25:49,520
and so she was trying to use RL for this because the the the R&N for for melody generation on

291
00:25:49,520 --> 00:25:54,240
its own wasn't respecting this necessarily at all and sometimes it was producing like just repeating

292
00:25:54,240 --> 00:26:00,160
the same note over and over which is a violation of one of the rules of counterpoint so that uh she

293
00:26:00,160 --> 00:26:04,400
got some interesting results but I think part of the the limitation was also just the R&N

294
00:26:04,400 --> 00:26:09,440
and encoding it into the training process of the transformers so we're how are you training

295
00:26:09,440 --> 00:26:15,440
these attention models I think that might be a little challenging and it would likely slow down

296
00:26:15,440 --> 00:26:21,600
our training because uh the technical aspects of it of getting these RL things working on first

297
00:26:21,600 --> 00:26:28,400
is TPUs um is not trivial um just because TPUs work well on on very large batches but not as well

298
00:26:28,400 --> 00:26:33,760
with with smaller batches and with RL when you're dealing with this kind of online yeah so then it

299
00:26:33,760 --> 00:26:38,080
becomes tricky to to make it work well in a way that you because you still want to be able to train

300
00:26:38,080 --> 00:26:43,600
these things quickly um so it sounds like it's much less of a hey I've got you know chocolate

301
00:26:43,600 --> 00:26:49,360
peanut butter let's get them together and see what happens oh yeah no no the specific thing uh

302
00:26:49,360 --> 00:26:54,800
you know what you know start at the simplest possible way to solve it yeah and kind of work your

303
00:26:54,800 --> 00:27:01,280
way yeah up in complexity exactly exactly so I don't I don't want to throw RL at just because I know

304
00:27:01,280 --> 00:27:06,560
well I don't necessarily want to use it for everything because sometimes you don't need RL like

305
00:27:06,560 --> 00:27:13,280
sometimes just I don't know an SVM would work just fine and then you should just use that

306
00:27:13,280 --> 00:27:18,960
and it might be pointed and now you've got a couple of papers I think a poster in a paper maybe

307
00:27:18,960 --> 00:27:26,240
here at NERP so that are using RL yes yeah so we had a poster this morning from our team um so

308
00:27:26,240 --> 00:27:31,520
Mark Belmara was the first author and he was the one manning the poster and and he he so I'm

309
00:27:31,520 --> 00:27:38,080
afterwards he his voice was very tired uh so that one is called geometric perspective on optimal

310
00:27:38,080 --> 00:27:42,320
representations and reinforcement learning and so we've been thinking a lot uh our

311
00:27:42,320 --> 00:27:47,600
nerd team in Montreal about representations for reinforcement learning and myself in particular

312
00:27:47,600 --> 00:27:52,880
I'm quite interested in this topic and I'm doing a lot of work right now in this space um but

313
00:27:52,880 --> 00:27:59,200
this this uh paper is sort of a partner paper to this other work that came out in ICML

314
00:27:59,200 --> 00:28:04,880
um I wasn't on the other paper but um I was along for the voyage um the value value function

315
00:28:04,880 --> 00:28:09,600
polytope so this is a paper that basically shows that the the value functions that you get

316
00:28:09,600 --> 00:28:14,000
in mark of decision processes form this this polytope in this high dimensional space

317
00:28:14,560 --> 00:28:19,520
and so it has this these interesting characteristics so for instance the the vertices of this

318
00:28:19,520 --> 00:28:25,040
polytope correspond to deterministic policies and so the path you can look at the path that's

319
00:28:25,040 --> 00:28:30,240
are the different algorithms will take along this polytope as they try to get to the optimal

320
00:28:30,240 --> 00:28:35,120
policy and there's some really interesting visualizations of when you compare like valley-based

321
00:28:35,120 --> 00:28:40,960
methods versus policy-grading methods and some of them actually leave the polytope in their

322
00:28:40,960 --> 00:28:46,880
trajectory so they're essentially the policies that are in a space where they're not valid policies

323
00:28:46,880 --> 00:28:51,600
in the sense that um they're not consistent with with with the with the system but they eventually

324
00:28:51,600 --> 00:28:57,440
end up coming back and and reaching some near optimal policy so the work we have here is basically

325
00:28:57,440 --> 00:29:04,160
leveraging this polytope and trying to see how we can um use it for for optimal representations and

326
00:29:04,160 --> 00:29:10,720
what I mean by optimal representations is that it's not just useful for the optimal policy but you

327
00:29:10,720 --> 00:29:18,320
can actually use this representation theoretically for for many different policies so there's a whole

328
00:29:18,320 --> 00:29:22,800
theory behind it but essentially you're trying to find a representation that will minimize

329
00:29:23,760 --> 00:29:29,760
the error that you get when you use that representation to express a particular value function

330
00:29:29,760 --> 00:29:37,520
for a policy that doesn't need to be optimal so it ends up being akin to having auxiliary tasks in

331
00:29:37,520 --> 00:29:43,680
a sense where uh we there's a lot of work in in the reinforcement learning field where adding auxiliary

332
00:29:43,680 --> 00:29:49,360
tasks to your to your learning process is helpful and that it serves almost like a regularizer for

333
00:29:49,360 --> 00:29:56,240
for your representations a multi-task yeah exactly and so these these different um policies that

334
00:29:56,240 --> 00:30:01,360
you're optimizing for so you're not just trying to get this optimal policy but sort of build your

335
00:30:01,360 --> 00:30:06,320
representation in a way that you can express just express these other policies quite well

336
00:30:06,320 --> 00:30:12,000
they end up serving sort of as auxiliary tasks and um they can help make the representation a bit

337
00:30:12,000 --> 00:30:17,120
more interpretable but also more expressive so there's some visualizations in grid worlds where you

338
00:30:17,120 --> 00:30:22,080
can see if you do the regular learning process if you look at the representations you you get

339
00:30:22,720 --> 00:30:27,040
certain dimensions of the representation are almost useless in the sense that they're not

340
00:30:27,040 --> 00:30:30,880
very expressive in terms of the state space of what they can represent in the state space but using

341
00:30:30,880 --> 00:30:36,800
these uh these they're we call them adversarial value functions um so these are the value functions

342
00:30:36,800 --> 00:30:42,000
for the other policies as these auxiliary tasks you get representations that are much more expressive

343
00:30:42,000 --> 00:30:48,640
and that they cover the state space a lot more um and so they're able to have a richer expressive

344
00:30:48,640 --> 00:30:55,280
power for representing multiple value functions you mentioned expressiveness are you trying to

345
00:30:55,280 --> 00:31:02,320
have minimal representations in a sense that they're not not necessarily with this but so part of

346
00:31:02,320 --> 00:31:07,200
what we're trying to achieve is that whatever representation you end up learning isn't own

347
00:31:07,200 --> 00:31:12,880
overfit to the optimal policy so let's say you tweak your your reward function a little bit and

348
00:31:12,880 --> 00:31:18,400
the policy you have is no longer um optimal under this new reward scheme um but let's say for

349
00:31:18,400 --> 00:31:22,800
whatever reason you maintain your your representation fixed because you just want to do linear

350
00:31:22,800 --> 00:31:27,600
approximation um if you don't have a good representation then you're not going to be able to express

351
00:31:28,160 --> 00:31:33,920
the new the new policy um properly and so this is what these representations are trying to do

352
00:31:34,560 --> 00:31:38,960
not necessarily reduce the mentionality but just increase the expressive power so that they're

353
00:31:38,960 --> 00:31:44,560
able to pretty well express the your current optimal value function but if you were to want to express

354
00:31:44,560 --> 00:31:49,440
the the value function under a different policy or an under a different reward function perhaps

355
00:31:49,440 --> 00:31:53,520
that it would still have a pretty good expressive power to be able to do that with low

356
00:31:53,520 --> 00:31:59,920
approximation error now the concept of generalizability is applicable to both the policy itself and

357
00:31:59,920 --> 00:32:09,680
the representation and so is there a relationship between the two whereas more generalizable or better

358
00:32:09,680 --> 00:32:16,640
generalized policies have better generalized representations or not necessarily unless you apply

359
00:32:16,640 --> 00:32:22,800
this approach that you've described so by generalizable policies do you mean like policies that are

360
00:32:23,840 --> 00:32:32,400
like if you've learned a suite of policies or um I guess I'm trying uh I don't know if policy

361
00:32:32,400 --> 00:32:37,360
is the right place to apply this is the question I'm asking like if I'm thinking of uh thinking

362
00:32:37,360 --> 00:32:45,040
of it from the perspective of I've trained an agent that uh can maximize some uh reward in some

363
00:32:45,040 --> 00:32:50,240
environment you know and once in generalization is I want to be able to put it in a slightly

364
00:32:50,240 --> 00:32:57,440
different environment and have it be able to perform well right and so that being the case where

365
00:32:57,440 --> 00:33:01,760
where does generalization live in that world is it in the policy that it you know or as well you

366
00:33:01,760 --> 00:33:07,520
you probably end up learning a new policy um but this tells you very closely to representation

367
00:33:07,520 --> 00:33:12,240
because if you're representation it's almost like you're doing fine tuning at that point so you've

368
00:33:12,240 --> 00:33:16,880
trained your agent under a particular reward function and now you have your trained agent and now

369
00:33:16,880 --> 00:33:21,440
you say okay well now I want this new task or this new reward function um but I don't want to

370
00:33:21,440 --> 00:33:26,160
retrain from scratch I want to start from where I started so it's kind of like fine tuning uh and

371
00:33:26,160 --> 00:33:32,720
so there if if your representation is highly overfit to the the first policy that you ended up

372
00:33:32,720 --> 00:33:39,040
learning um it might be very difficult to to sort of switch over to to this new policy if you're

373
00:33:39,040 --> 00:33:42,800
doing something even more drastic where you're saying I I already trained this agent so I'm going

374
00:33:42,800 --> 00:33:47,680
to fix the representation no longer backprop um through through the rest of the network and I'm

375
00:33:47,680 --> 00:33:52,240
only going to be learning learning the the last linear layer if your representation is poor you're

376
00:33:52,240 --> 00:33:57,120
not going to be able to learn the new task and so there uh is where generalization comes comes

377
00:33:57,120 --> 00:34:01,520
into play if you have a representation that that is expressive then when you switch the reward

378
00:34:01,520 --> 00:34:06,960
function or try to learn a new policy um if your representation is expressive you you should be

379
00:34:06,960 --> 00:34:12,080
able to do that reasonably well at least better than than um with a lot of the existing methods

380
00:34:12,080 --> 00:34:17,280
where there is evidence that they do tend to overfit to the current policy okay and so what was the

381
00:34:17,280 --> 00:34:22,160
inspiration for this paper you know coming from the original polytope paper was it driven by a

382
00:34:22,160 --> 00:34:30,720
particular use case or uh so uh Mark uh he's uh introduced the distributional approach to

383
00:34:30,720 --> 00:34:35,520
reinforcement learning so rather than backing up values you're back backing up distributions when

384
00:34:35,520 --> 00:34:41,200
when you're when you're doing learning and so this seems to give um a lot of advantages for

385
00:34:41,200 --> 00:34:46,240
for learning uh in terms of performance and there's a lot of follow-up work I've seen a bunch of

386
00:34:46,240 --> 00:34:51,760
papers today particularly there were a lot of distributional papers um but it's still not well

387
00:34:51,760 --> 00:34:57,520
understood how why they're they're giving that advantage so we had a paper uh triple AI this

388
00:34:57,520 --> 00:35:04,480
year um with Claire Lyle uh where we were investigating where this difference comes from comes from

389
00:35:04,480 --> 00:35:08,960
so the traditional way of of doing the rl backup is we call it expectational because you're taking

390
00:35:08,960 --> 00:35:12,720
expectations and then you get a single number versus the distributional approach where you're

391
00:35:12,720 --> 00:35:18,400
backing up uh distributions and so Claire did a lot of work and and she essentially proved that

392
00:35:18,400 --> 00:35:22,480
the there is no difference under certain mild conditions there is no difference between

393
00:35:22,480 --> 00:35:27,760
expectational distribution neither intitributional neither in the tabular setting nor the linear

394
00:35:27,760 --> 00:35:31,280
function approximator setting so they're essentially identical distributional doesn't give you an

395
00:35:31,280 --> 00:35:36,160
advantage then it's really when you go into deep networks that the the advantage of distributional

396
00:35:36,160 --> 00:35:41,360
comes in and it's not always guaranteed to give you an advantage so it's just going to be different

397
00:35:41,360 --> 00:35:46,320
and so sometimes it might actually hurt you um and um there's a like a counter example in the paper

398
00:35:46,320 --> 00:35:52,080
where it shows that uh distributional can actually provide worse performance than expectational

399
00:35:52,800 --> 00:35:57,920
so we in our team we're also very interested in in trying to understand distributional methods

400
00:35:57,920 --> 00:36:02,800
more and so Mark was looking into this quite a bit and uh he came up with this idea of

401
00:36:03,840 --> 00:36:08,480
basically looking at distributional at the distributional perspective almost as a auxiliary

402
00:36:08,480 --> 00:36:16,080
task and through that um he he had a remember a good lunch meeting with Dale Schoenman's and I

403
00:36:16,080 --> 00:36:21,120
think that's where they came up with this idea of the adversarial value functions um but it came

404
00:36:21,120 --> 00:36:25,760
from this initial idea of let's try to understand distributional methods a bit okay and you've got

405
00:36:25,760 --> 00:36:34,480
another uh paper that is being presented in the financial yeah yeah so that's so this is a

406
00:36:34,480 --> 00:36:39,200
collaboration with the Bank of Canada I mean they they've done most of the work I'm more on an

407
00:36:39,200 --> 00:36:47,120
advisory well on an advisory role so um they a lot of them their team their economists so they're

408
00:36:47,120 --> 00:36:50,880
not as familiar with reinforcement learning so that's where I come in and so I just basically

409
00:36:50,880 --> 00:36:55,440
they show me what they're thinking and and we we had a lot of brainstorms at the beginning to

410
00:36:55,440 --> 00:37:02,160
try to frame the the the problem properly so that it sort of satisfies their does it route up for

411
00:37:02,160 --> 00:37:06,880
for their economic theory but also that it makes sense from a reinforcement learning perspective

412
00:37:07,520 --> 00:37:11,760
and so what they're looking at they're part of the Bank of Canada which is part of the government

413
00:37:11,760 --> 00:37:16,480
and uh one of the tasks the main tasks of of the Bank of Canada is to make sure that the economy

414
00:37:16,480 --> 00:37:22,480
is stable and so one of the things they they look at is is the interbank payments that happen

415
00:37:22,480 --> 00:37:29,280
on a daily basis between different banks in Canada so um Bank A owes Bank B some money and so it

416
00:37:29,280 --> 00:37:34,240
may send it throughout the day and so if Bank B has has that extra equity then Bank B can pay

417
00:37:34,240 --> 00:37:40,400
other banks and so how they how they manage these payments affects how much how many of the payments

418
00:37:40,400 --> 00:37:46,080
they can make and how many payments they receive and so not making payments in time can give them

419
00:37:46,080 --> 00:37:51,920
interest penalties and these types of things and so the Bank of Canada plays a role as an

420
00:37:51,920 --> 00:37:57,680
intermediary to try to regulate these things so that you don't have um problems where where there's

421
00:37:57,680 --> 00:38:01,920
a bank that's not making any of the payments and all of the other banks are stuck and not making

422
00:38:01,920 --> 00:38:06,720
able to make any other payments as you're stuck in the stalemate so it's a very complex problem

423
00:38:06,720 --> 00:38:12,400
it's a very dynamic problem and they're interested in in looking at this obviously from the

424
00:38:12,400 --> 00:38:17,120
economics theory perspective but also they're interested in seeing if you can simulate some of these

425
00:38:17,120 --> 00:38:23,520
uh dynamics via reinforcement learning so we've been looking at um framing it as a multi-agent

426
00:38:23,520 --> 00:38:27,120
reinforcement learning problem where each of the agents are the different banks in question

427
00:38:27,760 --> 00:38:33,520
and uh seeing trying to train them to to learn optimal policies and their co-learning so all

428
00:38:33,520 --> 00:38:38,080
of the agents are sort of learning independently and you get some interesting dynamics as as this

429
00:38:38,080 --> 00:38:43,520
is happening so the workshop the paper that we have here is still very preliminary work um where

430
00:38:43,520 --> 00:38:49,040
we're essentially trying to demonstrate that this is even feasible um so as I said I like to take

431
00:38:49,040 --> 00:38:54,800
things from the simple uh angle and then grow from there so we're we're decomposing the the big

432
00:38:54,800 --> 00:39:01,600
problem of solving this interbank payment system um with decomposing it into smaller sub-problems

433
00:39:01,600 --> 00:39:06,800
that we can analytically find the solution for um that's what the economists know how to do well

434
00:39:06,800 --> 00:39:12,720
um and we can sort of validate that uh reinforcement learning is able to to simulate that faithfully

435
00:39:12,720 --> 00:39:18,000
and so far we've we've had a pretty good success with that and so now we're starting to combine some

436
00:39:18,000 --> 00:39:22,240
of these sub-problems and go into the more challenging tasks what are some examples of the kind of

437
00:39:22,240 --> 00:39:27,600
granularity of the sub-problems so the two sub-problems we're considering in this paper the

438
00:39:27,600 --> 00:39:33,520
the basically the simplest that you can consider one is um each bank at the uh start of the day

439
00:39:34,080 --> 00:39:39,600
can choose how much liquidity it's going to bring into start making payments so it's a prediction

440
00:39:39,600 --> 00:39:43,360
problem because you have to this one is almost like a bandit problem because you choose initial

441
00:39:43,360 --> 00:39:48,000
liquidity so like basically pulling one of the the bandit arms and then based on that initial

442
00:39:48,000 --> 00:39:53,680
liquidity that um determines how much you'll be able to pay so obviously if there were no cost

443
00:39:53,680 --> 00:39:58,400
to that you would just pull all of your liquidity and then you'd have as much money as you need to

444
00:39:58,400 --> 00:40:03,520
to make all the payments but there's a cost to to pulling um initial liquidity because this is

445
00:40:03,520 --> 00:40:07,360
coming from the bank of Canada it's almost like you're borrowing money from the bank of Canada

446
00:40:07,360 --> 00:40:12,080
to be able to make payments and then at the end of the day you return it um to the bank so this is

447
00:40:12,080 --> 00:40:16,480
one of the sub-problems that this was more like a bandit problem so they've run some simulations

448
00:40:16,480 --> 00:40:22,480
with with uh log data historical log data that they have um and then the other problem is the

449
00:40:22,480 --> 00:40:28,960
intraday payment so they the if you think of the day divided into hours at each hour you can

450
00:40:29,520 --> 00:40:35,600
make a decision of paying a particular bank that you owe money to um or not um so you may owe

451
00:40:35,600 --> 00:40:40,080
money to to a bunch of different banks if you don't pay them back then that bank might not have

452
00:40:40,080 --> 00:40:45,680
enough liquidity to pay you back um and so there's is when you start adding adding more agents then

453
00:40:45,680 --> 00:40:50,000
this becomes more and more complex and so this one is is less of a bandit problem it's more of a

454
00:40:50,000 --> 00:40:57,040
sequential decision-making problem and for in order to decompose them for the first bandit problem

455
00:40:57,040 --> 00:41:01,680
we're essentially keeping the intraday payment fixed in the sense that the the the choice that

456
00:41:01,680 --> 00:41:05,840
you made at the beginning doesn't really affect the dynamics of what happens later it's a fixed

457
00:41:05,840 --> 00:41:12,160
policy and for the second problem we keep essentially assuming giving the agents as much liquidity as

458
00:41:12,160 --> 00:41:16,160
they want so the the problem is very simple there all they have to do is make all the payments

459
00:41:16,160 --> 00:41:20,720
that that they need to make but because there's this multi-agent interaction um they don't

460
00:41:21,520 --> 00:41:27,200
necessarily always will find that optimal policy when you first started describing the problem

461
00:41:27,200 --> 00:41:32,080
one of the thoughts that came to mind was kind of a graph of the individual banks I don't know

462
00:41:32,080 --> 00:41:37,840
that I've heard much conversation about kind of the intersection of graph and reinforcement learning

463
00:41:37,840 --> 00:41:42,560
is there a war happening there um there's a little bit there's a little bit especially in the

464
00:41:42,560 --> 00:41:47,280
deterministic I believe I saw a paper come out recently where they're decomposed they're

465
00:41:47,280 --> 00:41:54,640
basically using graph algorithms for solving uh um certain reinforcement learning tasks so you

466
00:41:54,640 --> 00:42:01,440
can do um value function approximation by by using different types of of graph learning algorithms

467
00:42:03,200 --> 00:42:10,080
in this particular case I mean they are basically representing a graph the the connections between

468
00:42:10,080 --> 00:42:16,960
these these uh kind of the actual transactions are not necessarily yeah an actual thing between

469
00:42:16,960 --> 00:42:21,840
the right and and so most of the work I've seen with at the intersection of of graph whether

470
00:42:21,840 --> 00:42:26,400
be with their own networks or not and reinforcement learning is in the single agent setting where the

471
00:42:26,400 --> 00:42:30,880
the graph is more representing the environment where's in this case the graph is is representing

472
00:42:30,880 --> 00:42:36,000
the connection between the agents so you have multiple agents that they're not sharing parameters

473
00:42:36,000 --> 00:42:41,760
so they're kind of independent agents and the multi-agent setting um it's not something I have

474
00:42:42,320 --> 00:42:46,880
done a lot of work on so it's also been kind of interesting for me to learn more about the literature

475
00:42:46,880 --> 00:42:54,560
it's a really challenging problem um you have a lot of game theoretic aspects uh to it um and it's

476
00:42:54,560 --> 00:43:00,400
not a clear for for many problems there's no clear solution you have like Nash equilibria but

477
00:43:00,400 --> 00:43:07,280
it's it's that's as good as you can get for for many problems and uh so I haven't in that space

478
00:43:07,280 --> 00:43:13,520
I haven't seen much with with graphs okay with graph theory cool well uh Pablo thanks for taking

479
00:43:13,520 --> 00:43:18,560
a time to chat with us and share a bit about what you're up to thanks so much for chatting with me

480
00:43:18,560 --> 00:43:28,160
yeah absolutely thank you all right everyone that's our show for today for more information on

481
00:43:28,160 --> 00:43:44,800
today's guests visit twomla.com slash shows thanks so much for listening and catch you next time

