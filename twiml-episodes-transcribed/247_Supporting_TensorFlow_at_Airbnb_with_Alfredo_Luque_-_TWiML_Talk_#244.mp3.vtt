WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.000
I'm your host Sam Charrington.

00:32.000 --> 00:37.680
With today's show we conclude our TensorFlow Dev Summit series which was recorded on location

00:37.680 --> 00:40.320
during the summit a few weeks back.

00:40.320 --> 00:45.040
This overview features my conversation with Alfredo Luquet, a software engineer on the machine

00:45.040 --> 00:48.480
learning infrastructure team at Airbnb.

00:48.480 --> 00:53.800
If you're among the many Twimble fans interested in AI platforms and machine learning infrastructure,

00:53.800 --> 00:59.240
you probably remember my interview with Alfredo's colleague, Airbnb's Otto Kale, in which we

00:59.240 --> 01:01.680
discussed their big head platform.

01:01.680 --> 01:07.040
In my conversation with Alfredo, we dig a bit deeper into big head support for TensorFlow.

01:07.040 --> 01:11.320
Discuss a recent image categorization challenge they solved with the framework and explore

01:11.320 --> 01:15.840
what the new 2.0 release means for their users.

01:15.840 --> 01:20.080
If you're interested in topics like these, TensorFlow, deep learning, you'll probably

01:20.080 --> 01:25.200
also be interested in the awesome swag box that Google gave out to some of the attendees.

01:25.200 --> 01:30.000
They included their new Coral Edge TPU device and the Spark Fund Edge development board

01:30.000 --> 01:32.400
as well as some other really cool goodies.

01:32.400 --> 01:37.360
And I'm excited to give you an opportunity to win one in our latest giveaway.

01:37.360 --> 01:42.800
To enter, just visit Twimbleai.com slash TF giveaway and let us know what you would do

01:42.800 --> 01:46.280
with this kit if you got your hands on it.

01:46.280 --> 01:50.560
We'd like to send a huge thanks to the TensorFlow team for helping us bring you this podcast

01:50.560 --> 01:52.520
series and giveaway.

01:52.520 --> 01:57.080
With all the great announcements coming out of the Dev Summit, including the 2.0 Alpha,

01:57.080 --> 02:01.320
you should definitely check out the latest and greatest over at TensorFlow.org, where

02:01.320 --> 02:05.000
you can also download and of course start building with the framework.

02:05.000 --> 02:09.520
And now on to the show.

02:09.520 --> 02:16.640
All right, everyone, I am here at the TensorFlow Developer Summit with Alfredo Lukay.

02:16.640 --> 02:19.360
Alfredo is a software engineer at Airbnb.

02:19.360 --> 02:22.120
Alfredo, welcome to this week in machine learning and AI.

02:22.120 --> 02:23.120
Hi there.

02:23.120 --> 02:28.800
Let's get started by having you share a little bit about your background and how you came

02:28.800 --> 02:35.000
to work in machine learning and AI and in particular on machine learning platforms and frameworks.

02:35.000 --> 02:36.000
Sure.

02:36.000 --> 02:42.040
So I guess my interesting machine learning started in college actually well before, you know,

02:42.040 --> 02:46.400
there were a lot of applications to do this.

02:46.400 --> 02:52.560
Started with writings of my own like kernels for GPUs by hand.

02:52.560 --> 02:57.480
And exploring some of the really primitive ways we could do ML back then.

02:57.480 --> 02:59.080
Since then a lot of things have changed.

02:59.080 --> 03:03.680
I mean, since, you know, the last five or six years we had a lot of new developments.

03:03.680 --> 03:07.840
What interest drove you to write your own kernels?

03:07.840 --> 03:11.840
It was just very curious about, you know, what could be done there.

03:11.840 --> 03:17.840
There have been a lot of interesting and promising research around image recognition.

03:17.840 --> 03:21.440
First the convolutional neural net papers have been starting to drop.

03:21.440 --> 03:26.400
And there was a lot of really great applications that actually tried to set yourself outside

03:26.400 --> 03:28.600
of a research setting.

03:28.600 --> 03:31.360
So that's what really got me into it first.

03:31.360 --> 03:36.560
I went on to work in the online advertising real-time bidding.

03:36.560 --> 03:42.640
And later to work on my own hedge funds and then we really, we're just a lot of techniques

03:42.640 --> 03:47.640
in like natural language processing, which further kind of drove me into the field.

03:47.640 --> 03:48.640
Okay.

03:48.640 --> 03:49.640
Yeah.

03:49.640 --> 03:50.640
Interesting.

03:50.640 --> 03:53.960
I don't think I've talked to someone that started their own hedge fund before.

03:53.960 --> 03:55.760
Yeah.

03:55.760 --> 03:57.560
So you're at Airbnb now.

03:57.560 --> 04:00.320
What's your focus at Airbnb?

04:00.320 --> 04:04.800
So at Airbnb, I work on our machine learning infrastructure team.

04:04.800 --> 04:10.920
You actually had our engineering manager on here a couple of episodes ago.

04:10.920 --> 04:15.560
Basically we build tooling to make machine learning really easy.

04:15.560 --> 04:20.160
Developing models and productionizing them should be something that everybody can do.

04:20.160 --> 04:22.760
It's kind of our belief.

04:22.760 --> 04:28.800
And I specifically focus on aspects involving the actual creation of models.

04:28.800 --> 04:34.200
You know, actually developing which techniques you're going to be using, making sure that

04:34.200 --> 04:39.320
it's really easy to assemble those things and making sure there's no additional difficulty

04:39.320 --> 04:42.680
in actually making it production ready.

04:42.680 --> 04:49.560
So we have our own libraries that wrap around most of the major ML frameworks and make this

04:49.560 --> 04:51.880
a really, really easy thing to do.

04:51.880 --> 04:56.640
So TensorFlow is one of those, but we support most of the major ML frameworks.

04:56.640 --> 05:02.320
Yeah, that was one of the aspects of what you're doing with Big Head that struck me in that

05:02.320 --> 05:11.280
initial conversation with Atul, the extent to which you've invested in building the platform

05:11.280 --> 05:16.840
and be framework agnostic and supporting many different frameworks that the data scientists

05:16.840 --> 05:19.400
and software engineers want to use.

05:19.400 --> 05:24.200
Is that responsibility falls on you among others?

05:24.200 --> 05:28.520
But yeah, how do you kind of manage the burden of doing that?

05:28.520 --> 05:33.320
Yeah, so we kind of dove in head first as the answer.

05:33.320 --> 05:38.640
The really two approaches that you see to handling this sort of agnostic behavior.

05:38.640 --> 05:42.800
One of them is you basically treat a machine learning model like a black box, right?

05:42.800 --> 05:47.080
There is some interface, you know, you feed it some data and it's going to spit something

05:47.080 --> 05:50.720
out and then you don't really care what happens inside.

05:50.720 --> 05:55.280
And that definitely does make your platform pretty agnostic, but it also means you don't

05:55.280 --> 05:57.680
really have a really nice integration.

05:57.680 --> 06:02.840
So as a data scientist from ML engineer, if I'm actually trying to develop a model, it

06:02.840 --> 06:07.200
becomes much more difficult if all I hand off is just this big black box.

06:07.200 --> 06:10.360
We don't know if it's going to run efficiently.

06:10.360 --> 06:14.680
We don't know if I want a different environment, it's going to behave the same way.

06:14.680 --> 06:18.800
And I'm not really developing using the same tooling that's actually going to be running

06:18.800 --> 06:20.320
my model.

06:20.320 --> 06:25.720
So metronome frameworks have started really working on this issue of integrating everything

06:25.720 --> 06:26.720
nicely.

06:26.720 --> 06:31.000
I think in 2.0 tons of flows are really making strides there.

06:31.000 --> 06:34.520
And there's other frameworks as well that has really started focusing on this.

06:34.520 --> 06:40.240
But our goal was to really build these libraries that cleanly wrap around the major features

06:40.240 --> 06:42.480
of each framework.

06:42.480 --> 06:47.160
That doesn't mean a high support load, but it also does mean that our users, when they

06:47.160 --> 06:50.880
write something really feel like they're interacting directly with the framework, can

06:50.880 --> 06:53.440
use all its features.

06:53.440 --> 06:58.720
And when they go to production lines, their model, then they get exactly what they wrote.

06:58.720 --> 07:01.360
And they have a lot of predictability.

07:01.360 --> 07:06.000
And more importantly, they can just swap out tons of flow for actually boost or something

07:06.000 --> 07:11.520
else and get the exact same results in the earlier parts of their preprocessing.

07:11.520 --> 07:17.440
So that was a really important aspect of Big Head for us was working on that.

07:17.440 --> 07:25.640
I would imagine that swap ability kind of forced you down and approach other than the one

07:25.640 --> 07:31.360
you've taken where it's more black boxy, more kind of least common denominator.

07:31.360 --> 07:37.720
How do you expose the full functionality, but yet still a lot of people that do swap

07:37.720 --> 07:39.560
outs?

07:39.560 --> 07:46.560
We basically dive into the internals of each framework and try to find the interface

07:46.560 --> 07:52.680
point that sort of makes sense to let users develop their model.

07:52.680 --> 07:57.680
But then encapsulate it in a way where we know how to now feed data into that model.

07:57.680 --> 08:03.120
So once we really establish the method, but we can feed data into a model of a given

08:03.120 --> 08:08.640
framework, then whatever is on the other side, we already know how to deal with essentially

08:08.640 --> 08:10.480
what's in the model.

08:10.480 --> 08:14.920
And what comes before we build ourselves or the user route.

08:14.920 --> 08:19.640
So we have a really, really good way of essentially converting all these data types and making

08:19.640 --> 08:22.280
sure the flow of the data works well.

08:22.280 --> 08:27.400
Another aspect we're exploring as well is once you write what we call a Big Head pipeline,

08:27.400 --> 08:31.640
essentially your entire workflow graph of all the preprocessing steps in your machine

08:31.640 --> 08:37.840
learning models, we can actually potentially convert that into, if you're using one framework,

08:37.840 --> 08:41.200
a framework's native operations.

08:41.200 --> 08:46.520
So if you're using purely tenser flow, you can pile it into a tenser flow graph.

08:46.520 --> 08:51.080
And I think this approach is neat because in that case, you'd get basically no loss in

08:51.080 --> 08:52.080
performance.

08:52.080 --> 08:56.440
But again, it does require diving down into the internals of each framework.

08:56.440 --> 09:01.000
I mean, that is a lot of work, but we think it's worth it.

09:01.000 --> 09:06.480
We've also been collaborating directly looking to, as we open source, and agree engineers

09:06.480 --> 09:12.840
from those different platforms and supporting some of these wrappers and contributing directly.

09:12.840 --> 09:13.840
So.

09:13.840 --> 09:20.680
One of the things that was featured here at the summit was a case study video about a

09:20.680 --> 09:25.480
project at Airbnb focused on categorizing listing images.

09:25.480 --> 09:26.480
What's that project about?

09:26.480 --> 09:29.800
Yes, at Airbnb, we have a lot of listing images.

09:29.800 --> 09:36.320
Basically, every listing you see might have between five to maybe more images.

09:36.320 --> 09:41.720
And one of the challenges is showing the relevant images, right?

09:41.720 --> 09:47.400
So when you go try to book an Airbnb, typically the first thing you want to see is, you know,

09:47.400 --> 09:54.800
put you into the living room or the bedroom and not like the bathroom ear.

09:54.800 --> 10:00.880
So it becomes really important to get accurate classification of those images so that we can

10:00.880 --> 10:02.120
choose what to present.

10:02.120 --> 10:07.200
And for host, we can also assist them potentially in reordering them.

10:07.200 --> 10:13.160
So it was a pretty big project because while image classification is a well-known solved

10:13.160 --> 10:17.000
problem, the issue is the skill.

10:17.000 --> 10:21.480
We have almost half a billion images to sit through.

10:21.480 --> 10:26.800
And if you try out a new model and you want to backfill it, that's a lot of time to backfill

10:26.800 --> 10:27.800
it.

10:27.800 --> 10:32.920
So traditional approaches, just kind of running this on, there's things like Spark, you can

10:32.920 --> 10:39.040
run this on, so the CPUs would have taken months with any framework.

10:39.040 --> 10:46.320
And so one of the things we did was really focus on optimizing that and just taking a pre-trained

10:46.320 --> 10:52.240
model on Keras with some of the optimizations we made earlier in the pre-processing, we

10:52.240 --> 10:55.000
were able to get that to a few days.

10:55.000 --> 11:00.640
It's running on a few machines with GPUs, and so that was a pretty big one for us.

11:00.640 --> 11:05.000
And there are many other projects like this that can use a similar approach.

11:05.000 --> 11:09.840
What was it specifically that allowed you to get that to a few days from weeks or months?

11:09.840 --> 11:16.760
Yeah, so I mean, when you have image data, unfortunately, it's never exactly in the format

11:16.760 --> 11:18.760
that you need.

11:18.760 --> 11:24.920
Yeah, an ideal world you have, you know, perfectly resized data sets, everything's like

11:24.920 --> 11:31.280
nice and clean, but we have some images that might just, you know, they might be corrupted,

11:31.280 --> 11:33.880
they're the wrong size.

11:33.880 --> 11:38.880
If you're using something like ResNet 50, you have to do some scaling and kind of normalize

11:38.880 --> 11:39.880
the values.

11:39.880 --> 11:40.880
Get to the T24 by T24 image.

11:40.880 --> 11:46.400
Yeah, and then normalize it to zero and range and reorder the color channels.

11:46.400 --> 11:50.840
A lot of this stuff was written in Python, especially if you use Carrots, there's a lot

11:50.840 --> 11:57.560
of Python in there, and that becomes very, very slow, that ultimately limits, even if

11:57.560 --> 12:04.200
you have the fastest GPUs out there, they can't get images quickly enough to actually want

12:04.200 --> 12:05.720
inference on your model.

12:05.720 --> 12:10.960
Is the primary time savings inference focused or is it the training?

12:10.960 --> 12:11.960
An inference.

12:11.960 --> 12:12.960
Okay.

12:12.960 --> 12:19.760
Maybe in B, most of our scaling issues are always with inference on a really training.

12:19.760 --> 12:25.200
We've found at least that, typically with, you know, the full-todd GPUs or if you use

12:25.200 --> 12:34.600
GPUs, a couple of machines are plenty to reasonably train whatever you have, but for inference,

12:34.600 --> 12:37.040
that's the real key.

12:37.040 --> 12:38.520
If you try to backhole something there.

12:38.520 --> 12:44.080
Every day at huge scale, yeah, and if you try out something new, you know, every day

12:44.080 --> 12:48.960
you get more images, so now it's going to take even longer to backfill all of history.

12:48.960 --> 12:54.360
So we find ourselves backfilling all of history a lot, basically every time we try out something

12:54.360 --> 12:55.360
new.

12:55.360 --> 12:58.800
Let's dig into that particular point a little bit more.

12:58.800 --> 13:06.400
This was something that I don't recall if Attila and I went really deep into this, but

13:06.400 --> 13:10.960
Airbnb has spent a lot of time, like backfilling is a big part of the way you think about

13:10.960 --> 13:14.480
the problems that you're solving, like zipline, a big part of what it's doing is trying

13:14.480 --> 13:23.160
to manage these point in time, features and backfills, and like, yeah, walk us through

13:23.160 --> 13:24.160
like that whole space.

13:24.160 --> 13:29.080
What's the, is backfilling and how does it, where does it arise in your workflows?

13:29.080 --> 13:33.880
Yeah, so I mean, a really simple definition of backfilling would be, you know, if you have

13:33.880 --> 13:41.120
data that's collected every day and saved every day, then, you know, we might train our

13:41.120 --> 13:46.680
model on a couple of days, but if we want to apply our model to all the data that we've

13:46.680 --> 13:50.760
ever collected, we have to go through all of history.

13:50.760 --> 13:53.760
So it might have started being collected two or three years ago.

13:53.760 --> 13:58.720
So what's the specific example where you might want to apply the model to all of history?

13:58.720 --> 14:02.040
So this was actually, that project is actually a great example.

14:02.040 --> 14:04.360
It's categorizing the languages, okay?

14:04.360 --> 14:10.240
Yeah, so if you just categorized last month, it's not too useful because frankly, the

14:10.240 --> 14:15.000
user could click on a listing that hasn't been categorized, right?

14:15.000 --> 14:20.560
Doing that in real time ends up being a lot of wasted effort because Airbnb has, you

14:20.560 --> 14:24.480
know, a limited number of listings, it has many, but, you know, it's a finite number.

14:24.480 --> 14:27.360
And it's a pretty manageable number.

14:27.360 --> 14:29.640
And many users will be viewing the same listing.

14:29.640 --> 14:35.320
So you have a lot of duplicated work there and it's a pretty expensive computation to go

14:35.320 --> 14:38.560
through 15 images every time somebody clicks on it, right?

14:38.560 --> 14:39.560
Okay.

14:39.560 --> 14:43.720
So if we want to categorize these images, we need to do it for all listings, right?

14:43.720 --> 14:44.720
Got it.

14:44.720 --> 14:49.400
And if you want to run an experiment then and see by providing these better labels and

14:49.400 --> 14:53.480
these reordered images on the sites, do we have a revenue increase?

14:53.480 --> 14:55.120
You really need to go through everything.

14:55.120 --> 14:56.120
Okay.

14:56.120 --> 14:58.520
So that's kind of a rationale for it.

14:58.520 --> 15:06.240
And so it's, again, backfilling is, does it only occur in inference scenarios or is

15:06.240 --> 15:10.160
it also relevant to training scenarios?

15:10.160 --> 15:16.120
It could be relevant to training scenarios if you need to generate training data.

15:16.120 --> 15:19.920
So one example of this is there are models that use embedding.

15:19.920 --> 15:28.320
So essentially, instead of a single value, outputting a chunk of your network, right?

15:28.320 --> 15:32.440
And any of these models do need these embeddings and so you have to go run the embeddings model

15:32.440 --> 15:34.840
on all your data.

15:34.840 --> 15:40.120
If that embeddings model changes or something downstream changes, you might need a backfill

15:40.120 --> 15:41.120
everything.

15:41.120 --> 15:42.120
Okay.

15:42.120 --> 15:47.320
So there are scenarios like this where for training, you might need to backfill something.

15:47.320 --> 15:49.920
I'd say the majority are inference.

15:49.920 --> 15:53.880
For most enterprises, it's pretty similar to what we've heard as well.

15:53.880 --> 15:58.280
It's inference where we didn't chuck through a lot of data, right?

15:58.280 --> 16:07.040
You kind of set up this backfill process, well, the big task was to reduce the amount

16:07.040 --> 16:11.960
of time it took to categorize these images so that when you're doing the actual backfilling,

16:11.960 --> 16:14.280
it's a manageable time.

16:14.280 --> 16:17.520
You mentioned kind of a big part of what you do is like digging deep into these frameworks

16:17.520 --> 16:21.080
did, to what extent was that required in this project?

16:21.080 --> 16:22.080
Yeah.

16:22.080 --> 16:28.240
So we actually done that work already in this case for essentially wrapping Keras and

16:28.240 --> 16:32.840
there are different backends for it, but we had specifically also wrapped TensorFlow specific

16:32.840 --> 16:34.800
implementation.

16:34.800 --> 16:41.720
The investment here was also largely in building these reusable primitives that we could use,

16:41.720 --> 16:47.320
whether we were using TensorFlow or not for pre-processing these images.

16:47.320 --> 16:52.920
And we believe this time investment was largely worth it because we have many other models

16:52.920 --> 16:55.240
leveraging these things now.

16:55.240 --> 17:02.160
We did enable our data scientists to experiment using a variety of models and they would

17:02.160 --> 17:07.440
always get the exact same data set after pre-processing.

17:07.440 --> 17:13.960
So we actually ended up writing a lot of these things in C++ largely because we wanted

17:13.960 --> 17:20.400
to use, this is getting a little into the weeds, but we wanted to use CPUs for the pre-processing

17:20.400 --> 17:27.320
because the models then can use all the remaining GPUs, and this provided a pretty significant

17:27.320 --> 17:34.360
performance boost, and it also meant that anything else, you know, like a tree model,

17:34.360 --> 17:40.880
leverage it, so that was kind of the rationale for investing the time there and digging

17:40.880 --> 17:41.880
in.

17:41.880 --> 17:48.560
And so far as Keras' concern, the investment there was really making multi-GPU inference

17:48.560 --> 17:52.520
very easy, our users don't have to think about these things.

17:52.520 --> 17:57.640
Essentially, whatever machine you're running on, we figure out what's on there, configure

17:57.640 --> 18:01.840
your model to run it in optimized fashion.

18:01.840 --> 18:07.920
And that's one thing I think framework authors are starting to really focus on is, you

18:07.920 --> 18:12.720
don't need to think so much about the level of mechanics of how you're running it.

18:12.720 --> 18:20.840
And I'm trying to think of like if you are, if you're kind of starting out here and

18:20.840 --> 18:27.200
you're trying to, you know, say you've got a team that you are supporting, and you've

18:27.200 --> 18:34.200
got folks that want to work in different frameworks, like where do you start, or where would

18:34.200 --> 18:39.320
you start if you were starting all over again, and like providing them some tooling that

18:39.320 --> 18:43.720
would allow them to be most efficient, where do you think like the most value is in the

18:43.720 --> 18:47.200
kind of things that you've been working on?

18:47.200 --> 18:53.400
You know, I think about this a lot, but I think we've made more or less the right calls.

18:53.400 --> 18:58.240
There are two areas that we really identified where people waste a lot of time.

18:58.240 --> 19:05.320
One of those is simply generating your features to train on and to use for inference.

19:05.320 --> 19:11.120
This is a huge time-waster because oftentimes it means a lot of really, really hacky querying

19:11.120 --> 19:13.320
of a database.

19:13.320 --> 19:17.400
You have no guarantees of point-and-time correctness or anything.

19:17.400 --> 19:21.160
If you do want to guarantee that ends up being a lot of work.

19:21.160 --> 19:26.400
And typically when you're building a model, you're going to iterate through a lot of different

19:26.400 --> 19:30.920
data sets, a lot of different variations of features.

19:30.920 --> 19:36.120
So that was one really, really huge area where people were wasting time.

19:36.120 --> 19:41.680
And this has basically been productized at Airbnb in Zipline.

19:41.680 --> 19:43.920
Yeah, that's right.

19:43.920 --> 19:50.280
We like to call it the engineers working on it called a time machine for your data warehouse.

19:50.280 --> 19:51.280
Right.

19:51.280 --> 19:58.160
So actually, let's remember the second one, and let's just dig into this because I think

19:58.160 --> 20:05.560
as I've kind of studied what you've done with Zipline, I mostly get it.

20:05.560 --> 20:10.440
But like the point-and-time correctness, I don't feel like I fully, fully get exactly

20:10.440 --> 20:14.080
the situations where you need that.

20:14.080 --> 20:18.920
And so maybe walk through kind of where the various places that comes up and why it's

20:18.920 --> 20:22.160
important and how you get there.

20:22.160 --> 20:23.160
Yeah.

20:23.160 --> 20:31.040
There's a lot of models that, for instance, will use aggregations, or some metric that's

20:31.040 --> 20:37.360
calculated every minute, or every hour, or even every day.

20:37.360 --> 20:42.640
But the problem is how a data warehouse works is often at midnight, you start feeding

20:42.640 --> 20:44.440
in data.

20:44.440 --> 20:47.840
And at the other midnight, you basically cut it off.

20:47.840 --> 20:53.680
And there may have been updates in that window that you miss.

20:53.680 --> 20:57.920
There may be things that have an afterwards that actually belong to that day.

20:57.920 --> 21:01.440
There are a lot of things that can go wrong, and technically your data is not perfectly

21:01.440 --> 21:02.440
accurate.

21:02.440 --> 21:03.440
Right.

21:03.440 --> 21:10.320
Zipline actually worries about the mutations that happen to a database, and it will actually

21:10.320 --> 21:16.120
look at, for a given window of time, for a given aggregation, how do I do this correctly?

21:16.120 --> 21:22.000
So that the numbers I give you for your model are reproducible, and they're perfectly

21:22.000 --> 21:23.320
accurate.

21:23.320 --> 21:25.840
So this reproducibility is really, really key.

21:25.840 --> 21:31.800
I think that's for somebody building a model, one of the more interesting aspects.

21:31.800 --> 21:35.960
If I'm going to get the same data later on in time, or earlier on in time, I know I'm

21:35.960 --> 21:38.760
always going to get the same thing.

21:38.760 --> 21:42.400
And that really just gives you a lot of ease of mind.

21:42.400 --> 21:48.240
So what's the specific example of a model that needs this point in time correctness?

21:48.240 --> 21:53.040
Yeah, so I can get to you in the specifics, but for many of the fraud models, you really

21:53.040 --> 21:58.440
do care about actions that you use your friends that may have taken during a certain window

21:58.440 --> 21:59.440
of time.

21:59.440 --> 22:02.000
Those windows can get very, very granular.

22:02.000 --> 22:09.960
So if you're off, or if you're missing data, or even worse, if pulling the data to consecutive

22:09.960 --> 22:12.360
times you get different results, that's really

22:12.360 --> 22:16.760
going to throw off your model and the consequences there can be pretty disastrous.

22:16.760 --> 22:20.440
So you really care about the precision.

22:20.440 --> 22:24.560
That's one case where you might have very, very granular time windows, and it's very likely

22:24.560 --> 22:28.360
that you'll miss data just because of how data warehousing works.

22:28.360 --> 22:29.360
Right.

22:29.360 --> 22:34.280
So the data warehouse might have these daily aggregates or something, and you need to say,

22:34.280 --> 22:40.120
tell me the number of times a user has tried to use this credit card in the past five minutes

22:40.120 --> 22:42.320
from point X in time.

22:42.320 --> 22:43.320
Yeah.

22:43.320 --> 22:44.320
I kind of think.

22:44.320 --> 22:45.320
Yeah, but it's aggregated daily.

22:45.320 --> 22:48.640
So if you try to do it yourself, if you have daily aggregates, you're never going to be

22:48.640 --> 22:49.640
quite right.

22:49.640 --> 22:50.640
Right.

22:50.640 --> 22:51.640
Right.

22:51.640 --> 22:55.400
And the other really big feature is that ZIPLINE gives you this same data streaming for

22:55.400 --> 22:56.640
free.

22:56.640 --> 23:01.440
So once you've backfilled your model, and you want to actually use this in production,

23:01.440 --> 23:07.760
when you have real data flowing in, you can actually get that same data feed in the

23:07.760 --> 23:15.960
same exact format streaming, and it'll guarantee that that streaming data, more or less, will

23:15.960 --> 23:21.720
match exactly what you would get at the end of the day, just using ZIPLINE offline.

23:21.720 --> 23:26.520
So users don't really have to duplicate any of this work.

23:26.520 --> 23:31.440
For many models, they end up being hosted both online, so in production, using the website

23:31.440 --> 23:37.320
and offline for analysis, so it's a pretty nice thing to be able to use.

23:37.320 --> 23:44.400
And so the second kind of big category beyond the point in time correctness is what?

23:44.400 --> 23:48.400
Users write a lot of boilerplate, a lot of boilerplate code in general.

23:48.400 --> 23:53.760
Like I said, data is rarely exactly in a format you need, and there's oftentimes a lot of

23:53.760 --> 23:56.680
pre-processing that's done.

23:56.680 --> 23:59.320
For images, it's usually not too bad.

23:59.320 --> 24:04.000
It might be an impact on performance, but there's usually not too much code.

24:04.000 --> 24:09.400
If you're dealing with tags, you have to strip a lot of punctuation outs, and code

24:09.400 --> 24:12.040
that tags a certain way.

24:12.040 --> 24:16.800
Sometimes there's aggregation of features like you might do, if this feature exists and

24:16.800 --> 24:21.080
this one doesn't, then I want to admit both or something like that.

24:21.080 --> 24:24.960
There's a lot of business logic that people in code is a result.

24:24.960 --> 24:29.080
And historically, we just saw that this was copied and pasted.

24:29.080 --> 24:33.960
Basically, you might have 500 lines of copied and pasted code.

24:33.960 --> 24:40.520
With tweaks, so you can't just always copy, paste it, and expect it to work.

24:40.520 --> 24:44.400
This became a really big problem because people can't share this stuff.

24:44.400 --> 24:50.080
They can't compose it very easily for a new model, and it's very, very error-prone.

24:50.080 --> 24:57.880
So instead, we really focus on modularizing this, so we have a psychic learn-like interface

24:57.880 --> 25:04.160
where people can wrap these unique operations that they can define, or they can use our

25:04.160 --> 25:08.680
built-ins, and compose them to build the entire workflow of the model.

25:08.680 --> 25:12.880
And then if they want to share just one piece of that with their teammates, their teammates

25:12.880 --> 25:16.760
can use that chunk as the basis for another model.

25:16.760 --> 25:23.000
This reusability was a pretty big bet, but it's something that we found was a huge win

25:23.000 --> 25:25.200
for many of our users.

25:25.200 --> 25:28.800
And it enables us to do these nice things.

25:28.800 --> 25:33.840
At the end of my model, I'm using XGBoost, but I want to use now a convolutional neural

25:33.840 --> 25:34.840
network.

25:34.840 --> 25:38.920
And on one line of code, I can just swap it out and it works.

25:38.920 --> 25:42.920
And so where does this manifest itself in the system?

25:42.920 --> 25:46.920
Is this the feature store aspect of?

25:46.920 --> 25:48.920
This is a library that these are used.

25:48.920 --> 25:49.920
The big library?

25:49.920 --> 25:50.920
Yeah.

25:50.920 --> 25:51.920
Okay.

25:51.920 --> 25:56.440
If they're working in Python, they just import our library and can search just writing

25:56.440 --> 25:59.320
a model using the big head libraries.

25:59.320 --> 26:06.040
And if you want to use a TensorFlow model, we have Keras and T.O. estimators built in.

26:06.040 --> 26:10.760
And then they can just use their normal TensorFlow syntax to build out their model.

26:10.760 --> 26:14.520
But it now clearly plugs into the rest of their workflow.

26:14.520 --> 26:19.400
And so at the very end, they end up with a pipeline, basically just an object that contains

26:19.400 --> 26:22.960
all of their logic, and they can just save this thing.

26:22.960 --> 26:25.120
And that'll include everything they need to run it.

26:25.120 --> 26:27.680
And we run it for beta and production.

26:27.680 --> 26:28.680
So.

26:28.680 --> 26:29.680
Okay.

26:29.680 --> 26:30.680
Yeah.

26:30.680 --> 26:40.040
This is tie into, do you run it via a graph and the airflow and that as the orchestrator

26:40.040 --> 26:41.880
or how do you?

26:41.880 --> 26:45.760
It used to, no, it has its own graph scheduler internally.

26:45.760 --> 26:49.000
So this little run in a single machine.

26:49.000 --> 26:53.760
So it'll execute its components, you know, possibly a multiple threads.

26:53.760 --> 26:58.000
There's schedulers for different steps they might take.

26:58.000 --> 27:00.440
But it's meant to be real time.

27:00.440 --> 27:04.440
So I mean, the latency is sub-nilosecond in many cases for using this thing.

27:04.440 --> 27:05.440
Okay.

27:05.440 --> 27:10.040
Whereas airflow might be for huge, you know, back full jobs.

27:10.040 --> 27:14.360
So this entire graph, I mean, we'll run and online inference as well.

27:14.360 --> 27:18.480
So every time you submit a request, it's running the whole graph.

27:18.480 --> 27:24.160
And yeah, I mean, in the future, there's plans to add components where we can have steps

27:24.160 --> 27:29.480
run on different machines and use one about GPUs and things like that.

27:29.480 --> 27:32.880
But for now, we've found that even on a single machine, it's sufficient.

27:32.880 --> 27:33.880
Okay.

27:33.880 --> 27:34.880
Okay.

27:34.880 --> 27:39.240
So there's two pieces then summarizing or effectively kind of managing the data and figuring

27:39.240 --> 27:44.680
out how to deal with some of these trickier issues like point-and-time correctness and

27:44.680 --> 27:53.040
efficient backfills and things like that and kind of, I guess that summarizes like raising

27:53.040 --> 27:58.880
the level of abstraction maybe so that the user has to write less boilerplate and can

27:58.880 --> 28:04.680
compose pipelines from things that are already provided for them.

28:04.680 --> 28:05.680
Yeah.

28:05.680 --> 28:07.680
And also whatever they choose to write.

28:07.680 --> 28:08.680
Right.

28:08.680 --> 28:14.640
I think composability of their own code is something that we really care about as well.

28:14.640 --> 28:20.080
Especially if you're just hacking something out in a couple of hours, typically it's not

28:20.080 --> 28:23.240
going to be the nicest code, so at least encapsulating it, right?

28:23.240 --> 28:24.240
Yeah.

28:24.240 --> 28:27.040
And it's a pretty good way to guarantee you can use that in the future.

28:27.040 --> 28:28.040
Okay.

28:28.040 --> 28:29.040
So yeah.

28:29.040 --> 28:30.040
Okay, cool.

28:30.040 --> 28:35.440
So have you started looking at the TensorFlow 2.0 and like how that impacts the way, you know,

28:35.440 --> 28:40.840
some of the things you've already built and the way that your users will be using TensorFlow

28:40.840 --> 28:41.840
in the future?

28:41.840 --> 28:42.840
Yeah.

28:42.840 --> 28:43.840
I mean, it makes our lives a lot easier.

28:43.840 --> 28:44.840
I think so.

28:44.840 --> 28:45.840
I think so.

28:45.840 --> 28:49.840
One of the key things he did in T.Point now is deprecate a lot of really old APIs.

28:49.840 --> 28:53.680
I think that was, that was initially we ran into with, there was a lot of, there was a

28:53.680 --> 29:00.400
lot of incompatibility within the one point X releases, you know, certain APIs that only

29:00.400 --> 29:04.920
work with certain features and that proved to be a really big source of frustration.

29:04.920 --> 29:10.880
I think by removing those, those old APIs and really focusing on kind of starting more

29:10.880 --> 29:11.880
of a clean slate.

29:11.880 --> 29:19.080
It means that developers using TensorFlow will usually, I would expect an easier time.

29:19.080 --> 29:21.680
And I think there are some other neat features in there as well.

29:21.680 --> 29:28.040
I think the fact they've moved to mostly Eager mode by default is something that people

29:28.040 --> 29:30.080
would like to be able to use.

29:30.080 --> 29:36.920
If I'm most of your users prefer Eager mode to kind of the estimator of layers, API?

29:36.920 --> 29:41.360
The users that care about Eager mode, it's historically been using PyTorch.

29:41.360 --> 29:46.400
Just because it, by design, it kind of works in that way.

29:46.400 --> 29:51.880
But I think they would consider TensorFlow as an option now that Eager mode kind of works

29:51.880 --> 29:57.400
in a more easy fashion and they don't have these other sort of static graph complexities

29:57.400 --> 29:59.480
that deal with the most part.

29:59.480 --> 30:03.680
So yeah, I think it's going to emerge as a pretty viable option for, for many users just

30:03.680 --> 30:09.000
looking to prototype, you know, even in Jupyter, it's really nice to just be able to run

30:09.000 --> 30:11.400
a couple of styles into a result.

30:11.400 --> 30:16.520
So yeah, I think that's, it's really open up those features to people that I think would

30:16.520 --> 30:22.000
have not considered them in the past because of just the difficulty of using them.

30:22.000 --> 30:28.640
They've done a lot around 2.0 and recently with integration with Colab and Jupyter Notebooks.

30:28.640 --> 30:34.480
Your team has built quite a bit of infrastructure around Notebooks, like how do you see those

30:34.480 --> 30:35.480
playing together?

30:35.480 --> 30:42.200
Yeah, I mean, there are very much two different approaches, I think.

30:42.200 --> 30:46.080
I can't blame them, but Google does want to integrate with their own offerings.

30:46.080 --> 30:51.560
And we generally try to be really, really agnostic about what you're using.

30:51.560 --> 30:57.200
So while integrating with, you know, their own, they have this really nice Jupyter, a

30:57.200 --> 31:01.760
collaborative editor, using Google Cloud, yeah.

31:01.760 --> 31:04.880
But I think, you know, if users can leverage that, I mean, it's a great product.

31:04.880 --> 31:09.760
I've tried it out myself and it's, I'm sure, I'm sure it's a great option.

31:09.760 --> 31:18.600
Do any of the new capabilities allow you to do more better things in your own notebook

31:18.600 --> 31:19.600
implementation?

31:19.600 --> 31:22.720
Or are we already doing everything you needed to do anyway?

31:22.720 --> 31:25.360
We definitely weren't doing everything we needed.

31:25.360 --> 31:29.600
We belong to features that people like.

31:29.600 --> 31:34.840
One of the things I saw in the demo were, so the ability to better show these types

31:34.840 --> 31:39.080
of photographs and kind of expose parts of your model inside a Jupyter notebook.

31:39.080 --> 31:44.280
I think that would be useful for us, but I would need to take a closer look at what that

31:44.280 --> 31:45.280
offering entails.

31:45.280 --> 31:50.320
So, yeah, I mean, anything they can do to better surface visualizations, I think that's

31:50.320 --> 31:56.880
one area where generally users are very happy if you give them better ways to sort of see

31:56.880 --> 31:59.160
what's happening under the hood.

31:59.160 --> 32:04.880
One sort of board was one really, I think it was basically the only mainstream option

32:04.880 --> 32:08.880
that you could kind of do for visualizing models.

32:08.880 --> 32:13.000
And there just wasn't a very good way to integrate that nicely with Jupyter.

32:13.000 --> 32:19.280
There was a lot of caveat, so I think it'll be really exciting if they start making that

32:19.280 --> 32:23.880
first class and see what they do.

32:23.880 --> 32:29.600
And you use within Big Head TensorBoard pretty extensively, right?

32:29.600 --> 32:30.600
I wouldn't see.

32:30.600 --> 32:31.600
Remembering that extensively.

32:31.600 --> 32:35.320
But you have your own visualization stuff.

32:35.320 --> 32:44.720
We support TensorBoard, not super well right now, but it's on a roadmap.

32:44.720 --> 32:48.320
There's a lot of internal visualizations that we have.

32:48.320 --> 32:53.720
We've run visualization library for data and visualizing these pipelines.

32:53.720 --> 32:57.440
And we generally like to have a cohesive look whenever possible.

32:57.440 --> 33:00.960
So we'll probably continue to use that.

33:00.960 --> 33:06.200
But if a model exposes its own visualizations, we want to just leverage that out of the

33:06.200 --> 33:07.200
box.

33:07.200 --> 33:09.640
We don't want to reinvent the wheel or anything.

33:09.640 --> 33:16.800
So as an example, one thing we did, XGBoost has a feature importance, kind of really nicely

33:16.800 --> 33:17.800
built in.

33:17.800 --> 33:21.680
We can just expose that plot and people are loved using that.

33:21.680 --> 33:28.440
So yeah, I mean, using TensorBoard or whatever the framework builds in is something that we

33:28.440 --> 33:30.120
going forward want to use.

33:30.120 --> 33:36.800
And do you use any of the other kind of components of the TensorFlow, the evolving TensorFlow

33:36.800 --> 33:44.240
family, like the probabilistic programming or TFX or any of those other components?

33:44.240 --> 33:49.800
Yeah, I mean, we would like to be able to use more of the components, but essentially

33:49.800 --> 33:56.040
how we operate is also largely based on what our users need the most right now.

33:56.040 --> 33:57.760
There are other focuses at the moment.

33:57.760 --> 34:03.480
But I think as we plan on going and open sourcing this, it's going to be easier for other

34:03.480 --> 34:07.440
people to add support for these different features that they need.

34:07.440 --> 34:09.880
Yeah, probabilistic programming.

34:09.880 --> 34:13.080
There are a couple of frameworks that are kind of looking into this.

34:13.080 --> 34:15.360
I think Uber released one as well.

34:15.360 --> 34:16.360
Yeah, Pyro.

34:16.360 --> 34:22.440
That'd be really cool to add, but we just haven't had any tangible use case that immediately

34:22.440 --> 34:25.280
needed it right now.

34:25.280 --> 34:34.880
It sounds like your focus is on enabling kind of the core use cases and you haven't had

34:34.880 --> 34:38.680
much of a need yet to support some of these other ones.

34:38.680 --> 34:43.880
Yeah, I mean, that's the kind of thing that if we offered it, you would find use cases

34:43.880 --> 34:44.880
for it.

34:44.880 --> 34:51.440
But yeah, you have to start somewhere and I think giving users really, really good fundamental

34:51.440 --> 34:55.920
building blocks that work first or a good area where we can do that.

34:55.920 --> 35:00.280
I think estimators in TensorFlow are one area where they're really investing heavily

35:00.280 --> 35:06.440
in that and we'd like to have that also be really, really well supported as well.

35:06.440 --> 35:08.840
And is it not currently?

35:08.840 --> 35:13.560
We support some really basic use cases with estimators, so they have some pre-built ones

35:13.560 --> 35:16.440
and you can define your own custom estimator.

35:16.440 --> 35:20.600
And maybe take a second to explain what those are and how they're used.

35:20.600 --> 35:27.200
Yeah, so estimators are basically TensorFlow's, I think, approached as something that's

35:27.200 --> 35:28.920
like it learned like.

35:28.920 --> 35:36.240
We have a very, very simple API, essentially trained and run inference and the model is

35:36.240 --> 35:37.240
very encapsulated.

35:37.240 --> 35:38.240
Right?

35:38.240 --> 35:41.240
You don't have to worry about sessions and graphs and all these things.

35:41.240 --> 35:46.680
And basically define this structure of your model and what its input data looks like.

35:46.680 --> 35:51.560
And then you have something you can just play with.

35:51.560 --> 35:56.760
They provide a lot of built-ins, so like deep neural networks, you can kind of choose

35:56.760 --> 36:00.360
what those look like, linear models.

36:00.360 --> 36:04.600
I think they have a couple of other ones as well, but I'm not mentioning.

36:04.600 --> 36:05.600
And you can also write your own.

36:05.600 --> 36:11.120
So you can also just take TensorFlow code and kind of wrap it in one of these estimators.

36:11.120 --> 36:14.840
This fits really nicely with what we're doing in Big Head, where we're trying to encapsulate

36:14.840 --> 36:17.320
these models.

36:17.320 --> 36:25.200
So being able to allow users to use these more easily and not have to worry about anything

36:25.200 --> 36:30.600
other than the actual design of the model, that would be beneficial for them.

36:30.600 --> 36:36.480
So like I mentioned before, worrying about GPUs or where things are run or that's one

36:36.480 --> 36:39.800
question that we don't want our users thinking about.

36:39.800 --> 36:45.080
That's because we can almost always make more informed decisions at runtime about what

36:45.080 --> 36:50.760
that should look like, so they can get the best performance out of their models.

36:50.760 --> 36:55.480
So yeah, that's something I'm pretty excited to see what they do with.

36:55.480 --> 37:01.800
I think it was already in a pretty nice state, not too long ago, but now that they clean

37:01.800 --> 37:09.240
up TensorFlow more and 2.0, it's probably going to be the defector choice for a lot of people.

37:09.240 --> 37:18.720
We kind of taking a step back and kind of thinking about frameworks and the framework space

37:18.720 --> 37:25.200
from the perspective of someone who has to provide these to a set of users.

37:25.200 --> 37:34.640
Do you have like a wish list or predictions or where you see it all going?

37:34.640 --> 37:42.280
How do you see this evolving or what would you want to see to better support your users?

37:42.280 --> 37:47.640
Yeah, I think focus on developers and focus on the enterprise space is going to be one

37:47.640 --> 37:51.960
area that's pretty interesting.

37:51.960 --> 37:58.280
So I mean Google has been investing a lot in TFX and it is TensorFlow's specific, so

37:58.280 --> 38:00.440
I mean you have to use TensorFlow.

38:00.440 --> 38:06.560
But it's also largely their attempt at handling the entire end end, data management, model

38:06.560 --> 38:09.280
management, all of this.

38:09.280 --> 38:14.280
We haven't seen that too much from other framework providers, but one thing we have seen outside

38:14.280 --> 38:18.160
of TensorFlow is a focus on modularity alone.

38:18.160 --> 38:23.680
So another framework we've been exploring a lot is MxNet and they've had a lot of focus

38:23.680 --> 38:26.120
as well on this.

38:26.120 --> 38:31.280
So I really see, I mean you're probably going to have only a few big frameworks left

38:31.280 --> 38:34.240
after a while.

38:34.240 --> 38:39.520
For general use cases, it's just very, very hard to develop a new framework.

38:39.520 --> 38:45.320
So yeah, I mean I see the main player still has probably TensorFlow and PyTorch and MxNet

38:45.320 --> 38:48.680
with specific frameworks kind of lacking behind.

38:48.680 --> 38:53.920
I mean you have things like SpaceC and you know you'll have things like Pyro for niche

38:53.920 --> 39:00.800
use cases or for NLP specific use cases, but yeah, I mean a consolidation is very likely

39:00.800 --> 39:01.800
to happen.

39:01.800 --> 39:08.480
It's just not sustainable to have 10 of these and you know, developers putting their time

39:08.480 --> 39:14.600
between them, but it's, I think it's healthy to have competition, you know, an example

39:14.600 --> 39:20.600
of that was PyTorch was kind of the first to have the ability to just run these computations

39:20.600 --> 39:27.000
on a fly and TensorFlow then came out with eGermode and MxNet came out with its own like

39:27.000 --> 39:28.000
comparative APIs.

39:28.000 --> 39:34.320
I think that's a really good back and forth that will really help everyone.

39:34.320 --> 39:38.520
I think there's been a lot of focus on the research space too historically.

39:38.520 --> 39:46.160
So making it really easy to do research is nice, but for most people they generally have

39:46.160 --> 39:50.520
a rough idea of where they're going to start and they want really nice higher level APIs

39:50.520 --> 39:57.960
and then when they have those APIs, the other issue right now is usually when you use one

39:57.960 --> 40:03.760
of those things there's such a performance that moving it to production is going to require

40:03.760 --> 40:08.280
more engineers and that's something we at Airbnb don't really have.

40:08.280 --> 40:13.800
I mean we try to basically only have people writing models and maybe a team working on

40:13.800 --> 40:20.320
the infrastructure but nobody actually dedicated purely to converting these models into a performance

40:20.320 --> 40:21.320
version, right?

40:21.320 --> 40:22.320
Okay.

40:22.320 --> 40:24.640
That's something that we think computers can do better.

40:24.640 --> 40:28.640
Well, Afraidio, thanks so much for taking the time to chat with me.

40:28.640 --> 40:30.280
Thank you very much.

40:30.280 --> 40:36.040
Alright everyone that's our show for today.

40:36.040 --> 40:40.720
For more information on Afraidio or any of the topics we covered in this show visit twimmel

40:40.720 --> 40:44.960
AI dot com slash talk slash 244.

40:44.960 --> 40:53.360
As always, thanks so much for listening and catch you next time.

