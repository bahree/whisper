WEBVTT

00:00.000 --> 00:05.600
We think with the right hardware, you know, optimizations, 90%

00:05.600 --> 00:10.080
sparse should give you more than a 10x gain. So it's not, we're not talking

00:10.080 --> 00:14.160
about 50%, 20%, 30%, but we're talking about orders of magnitude here.

00:19.360 --> 00:24.640
All right, everyone. I am here with Subatai Ahmed. Subatai is VP of research at

00:24.640 --> 00:30.560
Numenta. Subatai, welcome to the Twoma AI podcast. Thank you so much for having me, Sam.

00:30.560 --> 00:34.560
Let's get started like we do here on the show by having you share a little bit about your

00:34.560 --> 00:41.840
background and your journey into the world of AI. Sure. So I've actually been in deep learning

00:41.840 --> 00:47.120
and something called computational neuroscience since the late 80s, long before it was called deep

00:47.120 --> 00:54.080
learning and I did research in, you know, understanding models of the brain and, you know,

00:54.080 --> 00:58.480
how that impacts machine learning in the late 80s and early 90s. I couldn't really see how to

00:59.280 --> 01:03.920
do anything practical with that at that time. And so I sort of switched gears and did research

01:03.920 --> 01:09.120
in more traditional machine learning and then did a couple of startups. About 17 years ago,

01:09.840 --> 01:15.920
I ran into Jeff Hawkins and Donna Dabinsky when they were founding Numenta. And it's sort of all

01:15.920 --> 01:22.080
my worlds kind of came together at that point. You know, Numenta as a company is really trying to

01:22.080 --> 01:27.440
show how we can take the neuroscience and really translate it eventually into practical systems.

01:28.400 --> 01:32.720
And so I've been at Numenta doing research for the last 17 years on that topic,

01:32.720 --> 01:40.000
very fortunate to be able to do that. Oh, that's fantastic. When we throw around this idea of deep

01:40.000 --> 01:46.640
learning being biologically inspired all the time, but I think Numenta is one of several places

01:46.640 --> 01:52.480
that's trying to push that idea of biological inspiration even further. Can you talk

01:52.480 --> 01:58.640
about the company and kind of the core ideas that you're researching?

01:58.640 --> 02:04.400
Yeah, I think deep learning has been somewhat biologically inspired and they're definitely,

02:04.400 --> 02:08.160
you know, the idea of modeling a neuron and, you know, some of the ideas have been convolutional

02:08.160 --> 02:13.920
networks and so on are directly taken from the neuroscience. But by and large, you know,

02:13.920 --> 02:18.800
if you look at neuroscience, it's pretty far from, you know, a lot of the details. It's very,

02:18.800 --> 02:25.280
very abstracted out. And so the core idea behind Numenta is, you know, deep learning is great.

02:25.280 --> 02:30.000
If you can really solve a lot of practical problems, but if we really want to understand intelligence

02:30.000 --> 02:35.600
and build truly intelligent systems, we need to go back and see what, you know, what have

02:35.600 --> 02:40.480
neuroscientists learned and how can we, you know, take those properties and create sort of

02:40.480 --> 02:46.160
algorithms, understand it from the algorithmic standpoint and then incorporate those ideas into

02:46.160 --> 02:51.440
deep learning. You know, we feel just, you know, building bigger computers and faster computers

02:51.440 --> 02:55.200
and throwing more data at it is all great, but that's not, though, that's not going to lead us

02:55.200 --> 03:01.520
to intelligent systems. So we think it's, you know, we have an existence proof. There's an

03:01.520 --> 03:06.720
amazing thing, the brain that is super intelligent, it's far more intelligent than any deep learning

03:06.720 --> 03:12.080
system out there. Why not try to understand, you know, what's going on there and see if we can

03:12.080 --> 03:18.400
apply that to practical systems? When you think about some of those core things that neuroscientists

03:18.400 --> 03:24.480
have learned or that neuroscience teaches us that we can put to use in building learning systems,

03:25.120 --> 03:29.360
what are those things? What does that landscape look like? Yeah, there's, there's quite a lot.

03:29.360 --> 03:34.880
But neuroscientists have field has really exploded in the last 20 or 30 years. You know, many,

03:34.880 --> 03:39.760
many people may not realize this, but the experimental techniques have really gotten extremely

03:39.760 --> 03:46.160
sophisticated. And so there's many, many things in the neuroscience now that we can actually

03:46.160 --> 03:52.080
apply to practical systems. At the same time, there's a ton of detail in neuroscience that is not

03:52.080 --> 03:57.200
applicable to practical systems. So it's, it's a bit of a challenge to figure out exactly where to

03:57.200 --> 04:02.240
draw that line. You know, what aspects are really important and what aspects are just sort of more

04:02.240 --> 04:08.720
biological detail that don't impact practical systems. So kind of the, you know, some of the big

04:08.720 --> 04:14.320
items that hopefully we can get into more detail on just ideas like a cortical column idea that

04:14.320 --> 04:21.120
there's a common micro circuit that's underlying all of intelligent function. The idea of the neuron

04:21.120 --> 04:27.120
itself, the neuron neurons in biological brains are much more sophisticated and powerful than the

04:27.120 --> 04:34.080
neurons we use in deep learning. You know, even core representational ideas like sparsity, the

04:34.080 --> 04:39.920
brain is extremely sparse and leads to a lot of efficiencies and other properties. Yeah,

04:39.920 --> 04:47.120
can we extract some of those ideas and incorporate it in? Even some ideas like the, you know, the brain

04:47.120 --> 04:52.720
is inherently a sensory motor system or constantly moving around the world and we learn by movement

04:52.720 --> 04:57.600
and we learn the structure of the world through movement and this ties into how cortical columns

04:57.600 --> 05:03.680
are set up. So all of those ideas are somewhat different than what's in deep learning today and

05:03.680 --> 05:08.880
those are concepts that we think is a lot that's understood now and it behooves us to kind of

05:08.880 --> 05:14.560
incorporate that into deep learning systems today. So this idea of a cortical column as this

05:14.560 --> 05:21.280
fundamental micro circuit is a compelling one is not something that I've heard of previously. Can

05:21.280 --> 05:26.160
you dig into what that means, what this micro circuit looks like and what we know about it?

05:26.880 --> 05:32.320
Sure. It's actually a fantastic idea. It's an amazing idea and this was, I think first

05:33.520 --> 05:39.440
proposed by a neuroscientist called Vernon Mount Castle in the 70s and what he noticed is that

05:39.440 --> 05:44.560
wherever you look in the neocortex. So first of all, the neocortex is kind of the largest

05:44.560 --> 05:49.600
structure in our brain. It's really where most of intelligent function actually happens and,

05:49.600 --> 05:53.920
you know, whether it's visual processing, auditory processing, language, you know, high level thought

05:53.920 --> 05:58.960
that all occurs in the neocortex. So what he saw is that no matter where you look in the

05:58.960 --> 06:07.440
neocortex, you see a very, very similar micro circuit in a very similar connections between the

06:07.440 --> 06:13.120
layers and between neuron similar neuron types and a sort of a prototypical architecture.

06:14.320 --> 06:21.200
And there are entire neocortex has somewhere around 150,000 cortical columns. So each cortical

06:21.200 --> 06:27.360
column is about the size of a grain of rice and it sort of sprinkled throughout our neocortex.

06:27.360 --> 06:34.080
And like I said, they all have this common architecture. And so do you say that when you say

06:34.080 --> 06:40.960
common architecture and connections, the picture that pops up in my mind is kind of the neuron

06:40.960 --> 06:44.960
and the dendrites and these connections. But it sounds like you're talking about a higher level

06:44.960 --> 06:52.720
structure. Is that right? Yeah. So the new cortex, the way it's structured is it's like a

06:52.720 --> 06:59.200
big flat 2D sheet that sort of scrunched up and stuck in your brain. But if you were to flatten it

06:59.200 --> 07:06.080
out, it's about a couple of millimeters thick. And there's several layers, you know, down that

07:06.080 --> 07:12.800
thickness in that dimension. And so there's about, you know, scientists say roughly six layers.

07:13.920 --> 07:19.680
And these layers have a, you know, intricate connectivity between them. So there's different neuron

07:19.680 --> 07:26.880
types that are in different layers and they connect in a recurrent circuit. And that architecture

07:26.880 --> 07:32.400
is what what we mean when we say that's repeated throughout the, you know, cortex. So it is,

07:32.400 --> 07:39.680
it is a little bit complicated to go through. But this, I'd say probably 50,000 or 100,000 neurons

07:39.680 --> 07:45.680
inside a cortical column and a fairly sort of prototypical connectivity structure between them.

07:45.680 --> 07:52.400
And what have we learned about the columns themselves and their behavior? And I'm imagining this

07:52.400 --> 07:57.920
is something that you, you know, might try to model computationally as well. Yeah, there's a lot

07:57.920 --> 08:03.360
we can do computationally. And what neuroscientists have found, you know, there's this common structure

08:03.360 --> 08:10.560
and the proposal by Mount Castle is that, you know, the reason of visual area as a visual area is

08:10.560 --> 08:15.520
not because there's anything really fundamentally different. It's just that the inputs are different.

08:16.560 --> 08:21.920
The learning algorithms and the architecture is largely similar to the auditory cortex or to

08:21.920 --> 08:26.880
the language areas. And neuroscientists have even done this experiment where experiments where you

08:26.880 --> 08:34.240
sort of swap modalities and you take an auditory auditory cortex and you feed it visual information.

08:34.240 --> 08:38.960
And what you see is you actually see visual feature detectors show up in the auditory cortex.

08:39.920 --> 08:45.680
And, you know, so that tells you there's a very similar kind of algorithm. It's it's not about

08:45.680 --> 08:54.320
the sensory modality. It's about the common algorithm. So just to interject the meaning that this,

08:54.320 --> 09:01.360
you know, we talk about the convolution being neurologically inspired. Is the suggestion that

09:01.360 --> 09:07.040
that's a software feature as opposed to a hardware feature? Exactly. Yeah, so to speak. Yeah,

09:07.040 --> 09:11.920
so convolution sort of says basically that you have sort of similar feature detectors, you know,

09:11.920 --> 09:17.200
throughout your visual field. So that's and you do see that in the in the visual cortex. You see

09:17.200 --> 09:24.000
very similar features across what this says is even more it takes that sort of one notch off. It's

09:24.000 --> 09:30.160
like the entire architecture. It's not just one little feature as sort of the entire architecture is

09:30.160 --> 09:35.840
really preserved across sensory modalities. And this is an amazing thing. If this were true and we

09:35.840 --> 09:41.680
believe this is largely true is in order to really understand how to implement intelligent systems

09:41.680 --> 09:46.880
in a new or cortical way, you sort of have to understand how one cortical column works and how

09:46.880 --> 09:51.520
multiple cortical columns interact. And then it's just essentially a scaling problem, you know,

09:51.520 --> 09:57.520
you just build more and more of them. So that really simplifies the process and and from a

09:57.520 --> 10:02.160
computer scientist point of view, that's great. It's like there's one thing we need to understand.

10:02.160 --> 10:06.960
It may be complex, but it's not that complex. You know, we can understand it. And once we understand

10:06.960 --> 10:11.920
it, it's a scaling issue. And so how far along are we in this journey to understand the cortical

10:11.920 --> 10:16.960
column? Yeah, I think we've made a lot of progress in understanding it. There's still a lot of work

10:16.960 --> 10:24.160
to do. You know, one of the so, you know, if you look at convolutional neural networks, they

10:24.160 --> 10:30.080
may make maybe one or two layers of this six layer structure. So they, you know, so they may make

10:30.080 --> 10:36.560
a small piece of that. Some of the things we've learned is that every cortical column is actually

10:36.560 --> 10:42.560
inherently a sensory motor system. And what I mean by that is every cortical column gets sensory

10:42.560 --> 10:49.200
inputs and every cortical column actually sends out motor commands. So when you look at the visual

10:49.200 --> 10:56.080
area, it's not just a feed, it's not just an area which gets visual input. It actually sends out

10:56.080 --> 11:00.800
signals, you know, to your eyes, for example, and your head to move your, you know, head and eyes

11:00.800 --> 11:06.880
around. If you look at the so-called motor areas, they actually get sensory input as well. So

11:06.880 --> 11:11.520
there's no such thing as a sensory area or a motor area. Everything's an inherently sensory motor

11:11.520 --> 11:19.760
area. And what we know is that mammals in particular and humans, we really learn by moving around

11:19.760 --> 11:26.880
the world. So this algorithm is inherently one that's constantly driving action, constantly making

11:26.880 --> 11:32.080
predictions. We see when our predictions are correct and when they're not correct and we learn

11:32.080 --> 11:37.040
from those mistakes. And that's really how we learn about the world. We're not just passive

11:37.040 --> 11:42.560
systems that, you know, getting billions of images as input and then suddenly we can recognize

11:42.560 --> 11:47.440
cats and dogs. You know, we inherently understand the world and the structure of the world by moving

11:47.440 --> 11:54.000
around and making actions and stuff. So that's a really big, you know, component of it.

11:54.000 --> 12:01.600
Another sort of really interesting thing is that, you know, if the cortical column is the same

12:01.600 --> 12:07.280
everywhere, the implication of that is that if some area of the brain is doing something,

12:08.080 --> 12:14.080
it must be doing that algorithm everywhere. So if you look at high-level thought and what's

12:14.080 --> 12:19.280
required for high-level thought, well, the same processes must be occurring also in low-level

12:19.280 --> 12:28.160
visual areas and other areas. And what our theory says is that every cortical column is actually

12:28.160 --> 12:34.720
its own independent modeling system. So every little cortical column is building a structured

12:34.720 --> 12:40.640
model of the world through movement by understanding how movement shapes perception and builds up

12:41.360 --> 12:46.960
a model of the world. And all of these cortical columns, the way they work is they interact

12:46.960 --> 12:52.720
and they're with each other and they come to a consensus about what is the current percept that's

12:52.720 --> 13:00.400
coming in. And through this sort of voting algorithm, that's essentially what we are consciously

13:00.400 --> 13:06.240
aware of. You know, every with thousands and thousands of these cortical columns, each working

13:06.240 --> 13:12.960
independently. And then they come to a consensus about what actually we are seeing and I think.

13:12.960 --> 13:19.600
So it's an extremely distributed system of lots of independent modeling. Again,

13:19.600 --> 13:22.400
this is very different from the way deep learning systems are struggling today.

13:22.400 --> 13:26.480
Right. It raises a lot of really interesting questions for me. Like,

13:28.080 --> 13:34.000
you know, we talk about these distributed system with lots of common components. You know,

13:34.000 --> 13:39.120
one view is like a swarming kind of thing where everything's the same and you have these voting

13:39.120 --> 13:46.320
mechanisms, but you've also described in talking about the visual versus auditory function,

13:46.960 --> 13:53.840
a degree of specialization. And I'm still trying to wrap my head around like, is that hardware

13:53.840 --> 14:00.640
or software specialization, meaning does the, you know, almost like kind of is there like a bone

14:00.640 --> 14:06.560
marrow kind of is it kind of like a bone marrow and it like physically changes to specialize or

14:06.560 --> 14:13.920
is it just changing thresholds or software things? Yeah. Yeah. That's the answers to questions

14:13.920 --> 14:18.640
like these. I mean, we know some of it. You know, that's it's a great question and it's something

14:18.640 --> 14:22.800
that's puzzled nor scientists for a long time. If you look at the auditory area, you see auditory

14:22.800 --> 14:27.040
neurons. And if you look at the visual area, you see visual neurons. So what's, you know, what's

14:27.040 --> 14:33.680
common there? Well, what's so, you know, it's similar to a deep learning system in the sense that

14:33.680 --> 14:37.920
you can take a convolutional network. If you feed at visual information, it's going to learn edges

14:37.920 --> 14:42.640
and, you know, corners and so on. If you feed at auditory information, it's going to learn auditory

14:42.640 --> 14:48.240
features. So it's, it's the same thing. It's these are, there's a learning algorithm that's going

14:48.240 --> 14:55.600
on. And because the input is different, it's just going to learn different things. And you can't

14:55.600 --> 14:59.600
just look at the end of the day and just probe it and see what it's learned. You have to understand

14:59.600 --> 15:06.160
the learning process itself. So it is, you know, the hardware is essentially very, very common.

15:07.040 --> 15:13.040
But the sort of emergent functionality is very different because the inputs against is very

15:13.040 --> 15:19.600
different throughout throughout its life. I mean, there are some differences between auditory areas

15:19.600 --> 15:24.880
and visual areas. So there's some specialized color detectors and things like that in the

15:24.880 --> 15:29.200
visual areas that you don't see in the auditory areas. And so, but those are really in the, in the

15:29.200 --> 15:34.400
details. If you step back and look at the large scale architecture, there's huge commonalities

15:35.280 --> 15:40.080
across them. Yeah, it is, it is interesting. It's fascinating to think through.

15:42.080 --> 15:48.720
Absolutely. So the cortical column is kind of this higher level architecture. You also mentioned

15:48.720 --> 15:56.000
the neuron itself and the way we've been modeling that for deep learning versus the way we think

15:56.000 --> 16:00.960
about that from a neuroscience perspective. Can you elaborate a bit on that? Yeah. So if you look

16:00.960 --> 16:07.920
at a deep learning system, you know, the basic idea of what a neuron does is it takes a very simple

16:07.920 --> 16:13.680
function of the input and computes a very simple outputs and basically takes linear weighted sum

16:13.680 --> 16:19.360
off its inputs and passes it through non-linearity, like a sigmoid or a relu and then outputs and

16:19.360 --> 16:25.840
there's a very simple idea. That idea has been around for more than a hundred years. It's called

16:25.840 --> 16:31.120
the point neuron model. And that basic idea in our mathematical system and practicalism really

16:31.120 --> 16:37.920
hasn't changed. But if you look at the biology, real neurons are nothing like that. Real,

16:37.920 --> 16:42.560
real neurons, you know, you mentioned these dendrites. The neurons have really complicated

16:42.560 --> 16:49.200
dendritic structures and these dendritic structures as where neurons get input. And the neuron actually

16:49.200 --> 16:55.200
does very sophisticated processing of its input through these dendritic structures before actually

16:55.200 --> 17:02.560
computing an output. And so there are many, many layers of this. But you know, neuron is a pretty

17:02.560 --> 17:09.120
sophisticated computing device in the biological brains. And so, you know, we've been sort of thinking,

17:09.120 --> 17:14.800
you know, what, what aspects of that do we actually need to model in deep learning systems?

17:14.800 --> 17:20.480
Are there advantages to these dendritic structures? And when we think about things like

17:20.480 --> 17:25.360
continual learning, you know, the fact that, you know, humans can constantly learn new things

17:25.360 --> 17:31.200
without forgetting stuff, you know, deep learning systems are not good at that. And we think the

17:31.200 --> 17:36.480
dendritic structure and the non-linear processing that goes on in these dendrites and neurons is

17:36.480 --> 17:42.240
actually critical to how we learn new things without forgetting stuff. So that's an example of,

17:43.040 --> 17:47.600
you know, some functionality that that's difficult today in deep learning that could be improved

17:47.600 --> 17:54.320
if we, if we could incorporate some of these properties. I'm curious what the neuroscience says

17:54.320 --> 18:01.840
around like the, are these cortical columns and the neurons? Are they fundamental for

18:01.840 --> 18:10.480
memory as well as kind of the processing? Like, is it uniform across these very different

18:10.480 --> 18:19.200
modes as well? Some of the more recent deep learning research directions are like incorporating

18:19.200 --> 18:24.720
memory into deep learning systems as a way to, you know, get closer to the kind of intelligence

18:24.720 --> 18:31.520
that we exhibit. But I'm wondering if you're saying that, well, there's a single thing underneath

18:31.520 --> 18:36.160
from a biological perspective. Yeah, I mean, it all has to be done by neurons, right? Some are

18:36.160 --> 18:44.560
on. Yeah, exactly. It's nothing else. So it's, you know, in the brain, you know, memory, learning,

18:45.440 --> 18:51.760
language, all of those functions, a large part of it happens in the neocortex and a lot of it

18:51.760 --> 18:57.360
happens in what's called the hippocampus and hippocampus structures as well. And all of those both,

18:57.360 --> 19:02.800
you know, both of those structures have exact same type of neurons that I was talking about,

19:03.360 --> 19:08.560
what's called pyramidal neurons. So they have the same complicated, you know, nonlinear

19:08.560 --> 19:15.760
integration. So those properties of neurons are common for all function, you know, all intelligent

19:15.760 --> 19:21.280
function, you know, memory, language, speech, you know, as you and I are talking and listening to

19:21.280 --> 19:27.120
each other, that's what we're doing. Our dendrites are processing away and creating these representations.

19:27.120 --> 19:34.560
To what degree are the notions that notions around like spiking something that you're focused on

19:34.560 --> 19:41.360
in your research? Yeah, so neurons send signals to each other by initiating a spike. So it's an

19:41.360 --> 19:47.360
electrical signal that, you know, goes from one end to the other. We think we're not sure that

19:47.360 --> 19:52.640
that's really critical to a model or not. You know, we do need to communicate from one neuron to

19:52.640 --> 19:57.200
another, but it may be okay to just, you know, put a number in a memory location. That may be,

19:57.200 --> 20:03.440
that's what happens in deep learning. And that may be just fine. You know, it is true that spiking,

20:03.440 --> 20:08.320
you know, is primarily binary, like it either a neuron, either spikes or it doesn't,

20:08.320 --> 20:12.320
whereas in deep learning system, we tend to transmit really high precision numbers.

20:13.200 --> 20:19.360
And that's part of the reason deep learning systems are so expensive and energy inefficient.

20:19.360 --> 20:24.000
So if you could go to the point where we can transmit essentially binary information, there's a

20:24.000 --> 20:29.680
chance we could make deep learning systems really, really efficient. And so that's one area where

20:29.680 --> 20:35.760
the spiking, it may be important to model spiking, but by and large, you know, in our work,

20:35.760 --> 20:42.800
we haven't found it really necessary to model kind of the details of all the details of spiking as

20:42.800 --> 20:48.560
it is in biology. So, you know, again, with neuroscience, it's always a question of where do you draw

20:48.560 --> 20:52.880
the line? You know, what level of detail do you incorporate and what level you've done? And so

20:52.880 --> 20:58.320
this is where we're kind of on the border of. And I imagine that that is a kind of a combination

20:58.320 --> 21:05.120
of, you know, intuition and experimentation and seeing what works. Exactly. Yeah. Yeah. We try a

21:05.120 --> 21:11.760
lot of stuff. Our bias is that if something is really prevalent in the neuroscience, if it's really

21:11.760 --> 21:17.920
common and a big feature, it probably has some views. And so we do look at all of this stuff pretty

21:17.920 --> 21:23.120
carefully, but we don't incorporate it into our models until we can actually come up with a

21:23.120 --> 21:28.880
functional reason for it. Like there's some, you know, some benefit to be gained, but we do look at

21:28.880 --> 21:38.160
a lot of these details pretty closely and talk to neuroscientists constantly. So we've talked about

21:38.160 --> 21:46.960
cortical columns, the neuron model, kind of from the biological neuroscience perspective,

21:46.960 --> 21:55.680
how do these concepts translate into things that, you know, learning machines for to say broadly?

21:56.240 --> 22:00.880
You know, from a cortical column perspective, you know, if we can, you know, understanding that

22:00.880 --> 22:08.560
architecture helps us, what should help us design better kind of modules and layers and deep

22:08.560 --> 22:13.520
learning system. In the deep learning system today, it's very, very simple, you know, you have a

22:13.520 --> 22:18.960
convolutional layer or maybe even a transformer linear layer or something like that. They're very

22:18.960 --> 22:24.240
simple layers. What the neuroscience tells us is that each layer is a lot more complicated,

22:24.960 --> 22:31.040
has a recurrent structure and it gets sensory input and motor output. So we can actually take the

22:31.040 --> 22:37.280
notion of what a layer is in deep learning and incorporate some of these other elements into it and

22:37.280 --> 22:45.360
make it a lot more, you know, complex. And that will provide a lot of benefits as I mentioned,

22:45.360 --> 22:49.680
the idea of continual learning, for example, you know, the ability to learn new things without

22:50.800 --> 22:57.120
forgetting stuff. But more importantly, if we can make these layers inherently sensory motor,

22:57.120 --> 23:02.800
what we can build in are layers that really understand the 3D structure of the world and the

23:02.800 --> 23:08.960
physical structure of the world. They inherently understand it. They build 3D models at every level

23:08.960 --> 23:15.040
of the hierarchy and this will allow you to have extremely robust neural networks that don't get

23:15.040 --> 23:21.040
fooled very easily with, you know, input that's slightly different from what it's seen before.

23:22.240 --> 23:29.520
It should lead to neural networks that are much more invariant to distortions and things like

23:29.520 --> 23:34.000
that. Today's neural networks, you really have to show it, you know, let's say you're doing a

23:34.000 --> 23:38.800
visual system, you have to show it images of every object in every possible pose and every possible

23:38.800 --> 23:43.600
lighting condition and all of that stuff. Whereas if you can really inherently understand the

23:43.600 --> 23:49.840
structure of stuff, the amount of training data you will need will be dramatically smaller.

23:50.400 --> 23:56.960
And the representations you build will be much more robust and invariant. So these are the

23:56.960 --> 24:02.560
kinds of properties. I think we can really build in at a very fundamental level into deep learning

24:02.560 --> 24:10.240
systems by taking clues from biology. What does it mean for a model to have an inherent 3D

24:11.200 --> 24:17.600
understanding? I mean, it's a number, right? Are we talking about like changing the coordinate

24:17.600 --> 24:26.720
system, the something polar? It's a fantastic question. It's something we've worked on a lot

24:26.720 --> 24:32.560
exactly that question. We published this theory called the 1000 brain theory. So this idea that

24:32.560 --> 24:36.800
cortical column, the thousands of these cortical columns and each cortical column is kind of this

24:36.800 --> 24:43.440
independent modeling system. So what the 1000 brain theory says, and this is a lot of its derived

24:43.440 --> 24:50.400
from neuroscience data, is in each cortical column, you have, you are building up models that are

24:50.400 --> 24:55.440
based on coordinate systems, just like you mentioned. So there's something essentially reference

24:55.440 --> 25:02.720
frames in each cortical column. And you can think of us, think of cortical columns as building maps

25:02.720 --> 25:10.240
of the world. So just like you have a physical map that might show a city and shows how you can

25:10.240 --> 25:17.120
move around a city and what's located at different GPS coordinates along the map. In the same way,

25:17.120 --> 25:27.040
when we inherently build up a structured model of an object, we create a 3D map. And in that map,

25:27.040 --> 25:34.720
we associate locations with features and properties of the object. And we know how you can manipulate

25:34.720 --> 25:39.680
that object, how you can make predictions about what will happen if you go from one part of the

25:39.680 --> 25:46.720
object to the next. And that map is in what we call in the reference frame of the object itself.

25:46.720 --> 25:52.000
It's not in the observer's reference frame. It's in the coordinate system of the object.

25:52.640 --> 25:58.000
And so that's essentially what we mean by inherently understanding the object. You would build up

25:58.000 --> 26:04.480
this really detailed map life structure of objects in its own reference frame. And we may be viewing

26:04.480 --> 26:09.680
it from very different angles in many different ways. But all we have to do is translate that new

26:09.680 --> 26:18.080
viewpoint into this map life structure. And now we can navigate and understand how that object

26:19.040 --> 26:25.280
works. So that was quite a mouthful. There was a lot of stuff in there. But inherently, we're

26:25.280 --> 26:32.800
building up these map life structures that contain the full physical and geometric structure of

26:32.800 --> 26:39.920
objects and concepts and so on. Yeah, it's reminding me in some ways to the, I'm blanking on the

26:41.040 --> 26:46.560
the model, but like Jeff Hinton's, the name of the model, but Jeff Hinton has been talking about

26:46.560 --> 26:54.960
this post convolutional model that has these properties of being kind of more spatial and

26:54.960 --> 27:00.480
translation invariant and things like that. Yeah, so he came up with this idea capsule,

27:00.480 --> 27:06.720
maybe they're the same as capsules. Yeah, so Jeff Hinton has actually been thinking about these

27:06.720 --> 27:12.000
ideas since the 70s. We found papers of him writing in the 70s about how we need to build up

27:12.000 --> 27:16.800
object-centered reference representations that are an object-centric reference frame. So he's

27:16.800 --> 27:22.880
been thinking about this a long time. And so everything I mentioned is definitely very much

27:24.160 --> 27:30.240
in the same kind of line of thinking. I would say what we've learned from the neuroscience is that

27:30.240 --> 27:36.000
these particle columns are much more powerful than we thought. It's much more than what a capsule

27:36.000 --> 27:42.080
does. They're really independent modeling systems and they're inherently sensory motor.

27:42.800 --> 27:50.240
So motor actions and predictions are an inherent piece of this puzzle. And so these are all

27:50.240 --> 27:56.320
aspects that have to be incorporated in as well. But definitely very, I think there's some

27:56.320 --> 28:00.560
truism in deep learning that no matter what idea you think of, Jeff Hinton has probably thought

28:00.560 --> 28:11.440
of it 20 years ago. He's great. A similar question to the last one. What does it mean for

28:12.480 --> 28:20.240
these computational models to be inherently sensory motor? Is that implying like a system level

28:20.240 --> 28:28.880
connection between input sensors and representations or something else? Yeah. So basically

28:28.880 --> 28:34.880
cortical columns send out motor commands. And so there are 150,000 of these cortical columns. They're

28:34.880 --> 28:42.000
all sending out signals to your motor systems. So whether it's manipulating your hands, your

28:42.000 --> 28:48.880
eyes, your head, your speech systems, all of that stuff is getting those what we call sub-cortical

28:48.880 --> 28:57.600
structures. Motor systems are getting sort of input from the neocortex. And there's some sort

28:57.600 --> 29:03.440
of a reconciliation that has to happen. And again, sort of like a voting process. And then the

29:03.440 --> 29:07.920
motor system decide, okay, these are the muscles I have to move in this way to achieve the

29:09.040 --> 29:15.600
the goal that the neocortex is telling me. So basically there's some sort of to use reinforcement

29:15.600 --> 29:21.120
learning terms as an action policy. There's a lot of possible actions that could happen. And there's

29:21.120 --> 29:25.840
some arbiter that's figuring out, okay, what is the best thing I should do at this point in time?

29:26.880 --> 29:34.960
It's still not clear to me how that necessarily what that looks like from a systems perspective,

29:34.960 --> 29:41.200
although it does prompt this really interesting thought that kind of echoes RL or even like an

29:41.200 --> 29:47.200
active learning where, you know, today we collect a bunch of data, throw it at some model and train

29:47.200 --> 29:51.760
it. And the model doesn't really have anything to say about the data that it receives. And this is

29:51.760 --> 29:57.760
kind of suggesting an active learning-esque kind of inherent capability to the model where it is

29:59.520 --> 30:06.960
telling some downstream system what it needs to perform. Is that that sounds aspirational as

30:06.960 --> 30:13.760
opposed to what we're doing today? Yeah, yeah, I mean, it is, it's exactly that. It's an active

30:13.760 --> 30:19.840
system. And it's what brains are doing constantly. You and I are doing this right now. And so we

30:19.840 --> 30:28.160
are not passive systems. And so we are inherently active. And that is a large part of why we are

30:28.160 --> 30:34.480
so able to learn, you know, fundamentally what how our world works. And so, you know, it's a

30:34.480 --> 30:40.000
predictive system. So when we make, when we send out motor commands and send out actions,

30:40.000 --> 30:45.520
we make predictions about what we're going to see. And based on what we actually end up

30:45.520 --> 30:52.080
sensing, we can update our internal models using the error and the predictions. And this is an

30:52.080 --> 31:00.720
extremely efficient way of learning. And so, you know, and we're going to take actions that

31:00.720 --> 31:06.080
are going to tell us most about the world. You know, if I've seen, you know, something over and

31:06.080 --> 31:10.480
over again, I'm just, I'm mostly going to ignore it. I'm going to go for the novel stuff. And,

31:10.480 --> 31:15.120
you know, that's how I'm going to learn most quickly. So it's, you know, as opposed to seeing the

31:15.120 --> 31:20.240
same data, you know, over and over again, you'll, you'll really become efficient at how you learn,

31:20.240 --> 31:31.360
learn stuff. Yeah. Yeah. I think part of the way I asked the last question was trying to get at. Where

31:31.360 --> 31:38.960
are we today with this line of research? You know, granted that is research and it's trying to

31:38.960 --> 31:46.800
to do, you know, big things that are modeled on biology. But what indications you have that it

31:46.800 --> 31:52.640
is a promising direction to go to build the kind of systems that, you know, we want to build whether

31:52.640 --> 31:57.440
that's what today's deep learning is doing or, you know, what, you know, we want it to do in 10

31:57.440 --> 32:02.080
years. Yeah, it's definitely still research. I think we're making really good progress on it.

32:02.720 --> 32:09.840
We are focused on building sort of initial machine learning based models of these cortical columns,

32:10.800 --> 32:15.920
trying to figure out exactly how these reference range transformations should happen. How do you

32:15.920 --> 32:21.200
build up these object centric models? And then, you know, some of the issues we discussed about how do

32:21.200 --> 32:28.160
you, you know, take the motor signal or the action output and then translate it into movements of

32:28.160 --> 32:34.960
your sensor and so on. So we're making a really good, you know, progress on that, but we definitely

32:34.960 --> 32:40.880
don't have, you know, the full system working at. But it, you know, I don't want to put timelines on

32:40.880 --> 32:45.440
it, but it's something we're really excited about and hopefully we'll have stuff to announce on that

32:45.440 --> 32:54.080
soon. Yeah, nice. You mentioned sparsity as kind of this foundational property of the way that

32:54.080 --> 32:59.360
you build out these models. Can you elaborate on that a bit? Yeah, yeah, I can talk quite a bit

32:59.360 --> 33:04.720
about that, you know, in contrast to the cortical column stuff, which is there's still quite a bit

33:04.720 --> 33:09.280
to figure out and we're making good progress on it. I would say the sparsity stuff is something

33:09.280 --> 33:15.680
that's really practical and we've shown use, you know, a lot of use cases today on large scale

33:15.680 --> 33:22.960
models and small scale models and deep learning. So sparsity is basically when you have weights that

33:22.960 --> 33:29.440
are weight matrices or weights that are mostly zero, meaning that there's a very sparse set of

33:29.440 --> 33:35.600
connections between layers. In the brain, you see another type of sparsity, something we call

33:35.600 --> 33:42.880
activation sparsity, meaning very few neurons are firing at a time. So, you know, only about 1%

33:42.880 --> 33:50.400
of neurons in your cortex are about firing at a time. And so there's sort of connectivity,

33:50.400 --> 33:56.800
sparsity in connections and sparsity in activations. And both of those factors can actually lead to

33:56.800 --> 34:04.320
tremendous efficiencies in processing deep learning systems. The brain uses only about 20 or 30 watts

34:04.320 --> 34:10.000
of power, which is pretty amazing when you consider, you know, the billions, tens of billions of

34:10.000 --> 34:14.960
neurons in there. And you can compare that with today's GPU based systems, which are power,

34:14.960 --> 34:20.000
you can probably power a small village with some of the clusters that are out there.

34:21.600 --> 34:26.400
And the way that this works is basically, you know, if you have zeros in your weight matrix,

34:26.400 --> 34:32.320
you can skip that multiplication because you know already that multiplication is going to be zero.

34:32.320 --> 34:38.720
There's no point doing that multiplication. And if you have both activations that are sparse

34:38.720 --> 34:46.800
and weights that are sparse, you get this multiplicative effect where a fractional

34:47.680 --> 34:53.520
percentage of the multiplications actually have to be performed. So one way to think about this

34:53.520 --> 34:59.360
is suppose you have 90% weight sparsity. So only 10% of the weights are non-zero. So you could

34:59.360 --> 35:07.600
imagine like a 10x gain, you know, you can skip, you know, 910s of the multiplications. But if you

35:07.600 --> 35:14.960
also have 90% activations sparsity, only 1% of the products are going to have non-zero on both

35:14.960 --> 35:21.680
sides. So you can skip 100 times the computation. There's this sort of multiplicative effect that

35:21.680 --> 35:28.160
happens. And what we've focused a lot on is how do you translate that into deep learning systems

35:28.160 --> 35:33.760
that are accurate at the same time? How can you change that? That into hardware architectures

35:33.760 --> 35:39.440
and actual implementations that can actually exploit that efficiency? And those are both areas

35:39.440 --> 35:43.280
that we've started to make tremendous progress on and learn quite a bit about.

35:43.920 --> 35:50.240
And how do you go about approaching that? It strikes me as kind of the classical 10 cent for the

35:50.240 --> 35:58.800
not-and, you know, $1,000 for knowing where to put it. Yeah, exactly. That's actually a great way

35:58.800 --> 36:02.720
to say it because, you know, what we've been doing in deep learning is throwing more and more

36:02.720 --> 36:08.080
compute at the problem. And it's great for some companies that are making GPUs. It's fantastic

36:08.080 --> 36:12.000
for them. They just want to throw more compute at it. But it would be even better if you just

36:12.000 --> 36:17.840
didn't have to do the compute, right? At all. And that's what sparsity allows you to do is sort of

36:17.840 --> 36:26.320
knowing where to skip the compute. And so there are two challenging aspects of that. One is like,

36:26.320 --> 36:31.680
how do you even train networks that are accurate and but have all of these zeros flying around

36:31.680 --> 36:38.320
all over the place? And the second piece is how do you actually translate into hardware architectures?

36:38.880 --> 36:44.960
And so with the first part in training, what we have found is that there are a number of a bunch

36:44.960 --> 36:51.040
of different ways you can go about it. One sort of critical aspect is that the way you train

36:51.040 --> 36:56.800
sparse networks is different from the way you train dense networks. And in particular,

36:57.360 --> 37:02.080
you know, with deep learning you have to do really be careful about your parameters and how you set

37:02.080 --> 37:07.440
up the network and how you set up the experiments and really explore, you know, the hyperparameters

37:07.440 --> 37:13.680
and so on. And with sparse networks, what we found is that you need to do your own hyperparameter

37:13.680 --> 37:19.120
exploration in a way that's quite different from the way you do dense networks. And so we've,

37:20.640 --> 37:24.160
you know, that's one of the big things that we've learned coming out of this.

37:24.640 --> 37:32.400
What is it about sparsity versus density denseness that drives this different way that you

37:32.400 --> 37:36.720
need to approach it? So, you know, a lot of deep learning systems are trained using back

37:36.720 --> 37:43.360
propagation and back propagation inherently assumes you're in this dense and dimensional space

37:43.360 --> 37:48.000
and it's trying to move around this end dimensional space, whereas if you have a sparse system,

37:48.400 --> 37:54.240
you know, an exponentially large percentage of that space is just out of bounds. And so you just

37:54.240 --> 37:58.480
can't go there. And so you're really fighting against what back propagation wants to do.

37:59.040 --> 38:06.640
And so we've had to use a lot of tricks and hyperparameter optimization techniques. We use a technology

38:06.640 --> 38:13.760
from a company called Sigopt that has this sort of Bayesian hyperparameter optimization techniques

38:13.760 --> 38:19.600
that's worked really, really well for us. And we've used that to figure out exactly what

38:19.600 --> 38:25.120
combination of hyperparameters really allow back propagation based networks to effectively

38:25.120 --> 38:30.400
kind of navigate that space and find global optimal or something close to global optimal.

38:30.400 --> 38:36.960
Have you kind of measured the, like, or how do you measure even the difference between trying

38:37.680 --> 38:43.760
kind of standard back prop versus more of an optimization type of an approach?

38:43.760 --> 38:51.280
Yeah. So, so back prop, you know, you're typically computing some sort of a loss function or

38:51.280 --> 38:58.720
error function and you kind of measure that. What's important for us is not just that error function,

38:58.720 --> 39:04.320
but also other things like sparsity, like activation sparsity and stuff. So we have to measure

39:04.320 --> 39:11.280
multiple things at once. So it becomes a more complicated optimization process. It's not as simple

39:11.280 --> 39:17.920
as just finding the lowest error. You need to find the lowest error in conjunction with networks that

39:17.920 --> 39:22.720
are as sparse as possible from a connection standpoint and as sparse as possible from an activation

39:22.720 --> 39:29.440
standpoint. And so doing this sort of multimetric optimization is quite tricky and this is where

39:29.440 --> 39:35.440
sort of the sigop technology that I mentioned really helped us and really shines in that respect.

39:35.440 --> 39:40.240
So yeah, it is tricky to figure out, yeah, how do you measure it and how do you go about doing it?

39:40.240 --> 39:46.560
That's, there was a lot of learning that was involved in that. And is the, is the,

39:46.560 --> 39:55.920
this optimization process, is it telling you, is it telling you broadly, like, is it giving you

39:55.920 --> 40:01.680
some broad parameter around sparsity, like, you know, level of sparsity, or is it telling you

40:01.680 --> 40:08.960
specifically, you know, at any given time step in a training loop, like what neurons you don't

40:08.960 --> 40:17.520
need to worry about? Yeah, it can tell us both actually. So yeah, and so in some cases, we know

40:17.520 --> 40:22.240
what level of sparsity we might want in the weights, but we have a lot of freedom in how, how much

40:22.240 --> 40:28.160
activation sparsity can have as one example. And so you want to be able to balance accuracy versus

40:29.200 --> 40:37.520
sparsity in that case, and it can help guide in both areas. I think what I'm trying to reconcile is if

40:37.520 --> 40:46.480
if, if you're, if each of the, each of the weights, or if each of the weights is a, you know,

40:46.480 --> 40:53.520
a metric that you're trying to optimize the, the spaces, the dimensionality of the space is

40:53.520 --> 41:01.360
ridiculous. Right, right. That, is that what you're doing? Yeah, it's something we call,

41:01.360 --> 41:07.760
well, it's not, it's something we're doing. It's also something the brain is doing. So what's,

41:08.320 --> 41:13.040
what people may not realize is that the connectivity in our brain is actually not fixed.

41:13.920 --> 41:18.720
The neurons in our brain are constantly adding and dropping connections. So it's something we

41:18.720 --> 41:25.440
call dynamic sparsity. The connectivity itself is being learned, which is kind of mind boggling

41:25.440 --> 41:32.800
to think about. Something like, I saw a study that in the adult brain, something like 30% of the

41:32.800 --> 41:39.760
connections are different every few days, which is just a staggering number to think about.

41:40.800 --> 41:45.360
It says your brain is going to be quite different a few days from now. And what's going on is that

41:45.360 --> 41:50.800
neurons are constantly trying to learn new things and forget about the stuff that's no longer relevant.

41:50.800 --> 41:56.320
So you have your core memories that are, that are going to be stable. And then you're constantly trying

41:56.320 --> 42:00.560
to learn new things. And this gets back to the continuing learning thing I alluded to earlier.

42:00.560 --> 42:04.240
We're constantly trying to learn new things. And the brain does that by growing new connections

42:04.240 --> 42:10.000
really quickly. And then if something sticks, those connections will become stronger. But most of the

42:10.000 --> 42:14.560
stuff we see day to day are just random connections and curious connections. So those will,

42:14.560 --> 42:19.120
we'll drop off. So that's kind of what's going on. Now we have to translate that to deep learning

42:19.120 --> 42:24.560
where you have to actually learn the mask over the weights, like which weights are going to be

42:24.560 --> 42:31.520
on or off. And yeah, that makes the whole process quite interesting. And I'm still like, is the,

42:32.560 --> 42:38.560
are you learning the mask at the individual weight level or is that parameterized in some way?

42:39.600 --> 42:46.480
There are some smaller dimensionality of like mask patterns that you're optimizing over.

42:46.480 --> 42:51.920
Yeah, it's a great question actually. We just published a paper called two sparsities are

42:51.920 --> 42:56.480
better than one where we showed that when you think about hardware architectures, you actually

42:56.480 --> 43:01.200
have to be careful about the sparsity patterns because some sparsity patterns will map really

43:01.200 --> 43:06.480
well to the hardware and others won't. And so we've come up with some techniques called complementary

43:06.480 --> 43:11.040
sparsity where there's a set of patterns that map really, really efficiently to the hardware.

43:11.040 --> 43:17.840
And so you want your training algorithms to understand the constraints of the hardware

43:18.720 --> 43:23.200
in this optimization process. And if you can do all of that stuff well and balance everything

43:23.200 --> 43:27.600
well and still get really accurate networks, we've shown you can get actually two orders of

43:27.600 --> 43:32.320
magnitude improvement in performance. So you can get networks that a hundred times faster

43:32.320 --> 43:40.160
on the same hardware, then a corresponding dense network would be on that architecture.

43:40.160 --> 43:49.440
By hardware here, we're talking about GPUs or FPGAs or exactly. Yeah, so the paper we published

43:51.200 --> 43:56.640
was a proof of concept on FPGAs and the nice thing about FPGAs is we can really design the

43:56.640 --> 44:02.720
circuits to be exactly what we want. We actually think you can take the same ideas and apply them

44:02.720 --> 44:09.280
to CPUs and GPUs as well. There's some additional tricks that are involved, but we're making good progress

44:09.280 --> 44:14.640
on that. So we think, you know, sort of stepping back a little bit, we think if you can be really

44:14.640 --> 44:20.240
smart about where you place the zeros and what computations devoid, we think there's a potential

44:20.240 --> 44:25.440
of making deep learning orders of magnitude more power efficient and more compute efficient

44:25.440 --> 44:31.920
than it is today. And if you think about kind of the carbon impact of deep learning today,

44:31.920 --> 44:40.160
which is just insane, you know, it sort of behooves us as an industry to really pay attention to

44:40.160 --> 44:46.400
this and really improve the energy usage of our deep learning systems. It's kind of completely

44:46.400 --> 44:52.160
out of control today. And you're saying deep learning and just to be absolutely clear, you're talking

44:52.160 --> 44:59.680
about conventional deep learning for lack of a better term, as opposed to what we were talking

44:59.680 --> 45:05.120
about earlier, cortical columns, different neuron models, things that we're working on in the

45:05.120 --> 45:12.160
research. This is applying sparsity and optimization to, you know, drive greater efficiency and

45:12.160 --> 45:17.600
something that's roughly akin to what we're doing today. Is that fair? Yes, yes. The sparsity

45:17.600 --> 45:23.280
stuff could be applicable in today's deep learning networks. It seems it's becoming more and more

45:23.280 --> 45:27.920
clear that these sparse techniques can be applied to basically all of the network architectures

45:27.920 --> 45:32.960
that are out there today, whether it's, you know, convolutional systems, transformers,

45:32.960 --> 45:38.960
conformers, you know, ResNet, you know, all the different architectures that are common today

45:39.680 --> 45:44.560
can benefit from sparsity. Having said that, the cortical column

45:45.520 --> 45:50.720
implementations when we get there will also be sparse because the brain is sparse and then

45:50.720 --> 45:55.920
sparsity gives you, you know, tremendous benefits. But the good thing about the sparsity work is

45:55.920 --> 46:01.360
it can be actually applied today to today's deep learning systems. And is sparsity,

46:02.880 --> 46:08.320
is it an emergent property of the architecture or is it, you know, the use case or the data?

46:09.120 --> 46:15.280
Is it something that you can, you know, always apply and it has some benefit because it's

46:15.280 --> 46:21.200
broader is it only if your data or your problem looks a certain way? You can apply to almost any

46:21.200 --> 46:27.360
problem domain. Where the thing you do need is that you need networks that are larger.

46:28.080 --> 46:34.160
It's sort of a little bit paradoxical thing about, but you need larger networks in order to

46:34.160 --> 46:41.440
allow really sparse processing. And what happens mathematically is as you get larger networks,

46:41.440 --> 46:46.320
you get these exponentially larger spaces that you're working with. And it's so it's easier and

46:46.320 --> 46:54.640
easier to find solutions that can be extremely sparse in there. So even if you look at the total

46:54.640 --> 46:58.960
number of non-zero weights, if you have a small network, maybe you can get to, let's say,

47:00.640 --> 47:06.400
you know, a thousand non-zero weights at a layer, just to pick a number. If you have a larger

47:06.400 --> 47:13.200
network, you might actually be able to get to 500 or 100 non-zero weights. So it's it's it's

47:13.200 --> 47:20.000
counterintuitive, but by making the dimensionality's bigger, you can actually get by with smaller

47:20.000 --> 47:25.360
absolute number of weights. Yeah. Is that a property of Bayesian optimization in particular or

47:26.160 --> 47:31.360
something else that this is just a space? Yeah, this is just in the mathematics of sparsity.

47:31.360 --> 47:39.520
The way it works is that as the spaces get larger, you can get the same amount of information with

47:39.520 --> 47:46.240
a smaller number of weights. So it's an information theoretic result there. Now from a, I mean,

47:46.240 --> 47:51.200
obviously from a Bayesian parameter optimization standpoint, it just makes it even harder to

47:51.200 --> 47:56.400
find all these hyperparameters and create networks that can train and that's so.

47:58.000 --> 48:04.320
You mentioned in the application of the Bayesian optimization stuff that you were doing,

48:04.320 --> 48:11.200
you know, we talked about the multimetric nature of it. You also says some things that

48:12.320 --> 48:19.680
sounded like kind of constraint, you know, constrained optimization, which I know SIGUP does as

48:19.680 --> 48:26.000
well. Is that something that you're using as part of the formulation? Yeah, yeah. So with the

48:26.000 --> 48:31.600
multimetric, we can optimize multiple quantities simultaneously, like sparsity and error and so on,

48:31.600 --> 48:38.080
with the constraints, we are using constraints. So with SIGUP, you can put all of these constraints

48:38.080 --> 48:43.280
in your parameters. So we make sure that we stay within ranges that we know will work well.

48:45.920 --> 48:50.080
So for example, constraints that we know are imposed by the hardware.

48:51.920 --> 48:55.440
Or there's combinations of hyperparameters that just don't make sense.

48:56.320 --> 49:01.120
And so we want to put those constraints so that the hyperparameter optimization doesn't

49:01.120 --> 49:07.840
visit parts of the hyperparameter space. That's a mouthful. That just don't make sense.

49:09.200 --> 49:14.000
Yeah, I mean, with hyperparameter optimization, every point in the space is an entire training run.

49:14.000 --> 49:18.560
And so you have to be judicious in how you, you know, picking those points efficiently.

49:19.120 --> 49:24.480
And so the constraints, any constraints you can put in that you know will just reduce the

49:25.200 --> 49:28.720
space of possibilities and make the entire process a lot more efficient.

49:28.720 --> 49:35.360
And you mentioned transformers a couple of times. Have you applied any of the sparsity techniques

49:35.360 --> 49:40.720
that we're talking about to transformers and language models and, you know, some of the things

49:40.720 --> 49:45.760
that are potentially causing, you know, the large environmental impact that you alluded to?

49:46.320 --> 49:50.480
Yeah, yeah. It's exactly because of that that we are actually spending quite a bit of time

49:50.480 --> 49:56.800
on transformers. They're becoming very popular. But at the same time, these are just massive

49:56.800 --> 50:03.280
models that are taking up consuming huge amounts of power and, you know, creating a pretty big

50:03.280 --> 50:09.680
negative environmental impact today. We've been, we've having quite a bit of success in sparsifying

50:09.680 --> 50:16.320
transformers. Because these models are larger, it's, it's, you know, it's possible to get

50:16.320 --> 50:20.560
quite sparse with that. So we've been able to get transformer models. We've worked with the

50:20.560 --> 50:28.720
BERT model, which is kind of the canonical transformer model that everyone uses as kind of a template.

50:28.720 --> 50:36.400
We've been able to get to 90% sparse and even higher without losing accuracy in these kind of

50:36.400 --> 50:42.160
models. So there's a potential of really accelerating these transformer models and making them much,

50:42.160 --> 50:47.040
much more power efficient. So we published a blog post on that recently and we should be

50:47.040 --> 50:52.560
having, you know, a lot more on that in the, in the few coming few months. We'll definitely

50:52.560 --> 50:59.600
include a link to that in the show notes pages. Is there a rule of thumb that 90% sparse

51:00.560 --> 51:09.040
translates to, you know, 50% power or something like that? We think with the right hardware,

51:09.920 --> 51:16.560
you know, optimizations, 90% sparse should give you more than a 10x gain. So it's not, we're not

51:16.560 --> 51:24.000
talking about 50%, 20%, 30%, but we're talking about orders of magnitude here. And so with 90%,

51:24.000 --> 51:30.880
you can skip 9 out of 10 computations, but your model also gets really smaller. So you get additional

51:30.880 --> 51:36.560
advantages over that. So if you, in hardware, you know, you have, you know, memory hierarchies,

51:36.560 --> 51:41.600
the some memories are faster than others. And a smaller model means most of your model can fit

51:41.600 --> 51:48.000
in these faster memory areas. And, you know, memory contention gets much smaller. So there's

51:48.000 --> 51:51.920
a bunch of other practical things that come in. So, you know, we eventually think that we can get

51:51.920 --> 52:00.640
more than a 10x for that. The brain is 95 to 98% sparse. So if we can get anywhere close to the

52:00.640 --> 52:05.440
levels of the brain, we're talking, and again, remember, there's multiple types of sparsities

52:05.440 --> 52:11.680
that, that interact and have multiplicative benefits. So we're talking multiple orders of magnitude

52:11.680 --> 52:18.240
efficiency gains if we can really get close to how it is in the brain. And, and, you know, I think

52:18.240 --> 52:23.760
a lot of deep learning researchers used to think that was impossible, but I think the brain shows

52:23.760 --> 52:28.480
that it's not only is it possible, it's actually the best example we have today.

52:28.480 --> 52:39.120
All right. I'm curious about you. We've been talking about training slash learning. What about the

52:39.120 --> 52:46.400
inference side of things with regards to sparsity? Yeah. So, but both inference and training will

52:46.400 --> 52:51.600
speed up with sparsity. We focused actually a little more on the inference side because that's the

52:51.600 --> 52:57.600
more kind of common use case, but training will speed up too. But the fundamental mathematical

52:57.600 --> 53:04.640
operations that occur during, during inference and training, they're very similar. And so they

53:04.640 --> 53:10.880
should both benefit from this. Awesome. Awesome. But we've covered a ton of ground. What are you

53:10.880 --> 53:17.440
most excited about over, you know, say the next 10 months or some other arbitrary time horizon?

53:18.960 --> 53:23.520
Yeah. What's, you know, there's this big question in the machine learning community, like

53:23.520 --> 53:31.920
can brain inspired understandings actually impact machine learning positively. And I think I'm

53:31.920 --> 53:39.200
really excited that the stuff that we've mentioned is now coming into real practical domain. And

53:39.200 --> 53:45.680
with sparsity, I think 10 months from now or year from now, we'll see really dramatic improvements in

53:45.680 --> 53:51.280
what we can do from an efficiency standpoint. And I'm really hopeful in that time frame that we

53:51.280 --> 53:55.440
will be able to take many more of the ideas of the cortical column and start showing really

53:55.440 --> 54:04.000
concrete hard core benefits to machine learning systems. And to really showcase that we can take,

54:05.600 --> 54:11.440
understanding the brain is not just an academic scientific exercise. It can actually have practical

54:11.440 --> 54:16.400
engineering benefits. And to me, that's what I'm really passionate about as a computer scientist.

54:16.400 --> 54:21.440
And hopefully, you know, year from now will be in a dramatically better space with respect to all

54:21.440 --> 54:27.920
that than we are now. Fantastic. Fantastic. Well, Subatai, thanks so much for taking the time to

54:27.920 --> 54:31.840
share a bit about what you're working on. Very cool stuff. Yeah, thank you so much, Sam. It was

54:31.840 --> 54:36.560
a pleasure talking in there. There's a great question. And I'm glad we got it. Thank you

54:36.560 --> 54:46.960
going to a lot of the detail on that. Absolutely. Thank you.

