Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
On today's show I chat with Song Han, assistant professor and MIT's EECS department about
his research on deep gradient compression.
In our conversation, Song and I explore the challenge of distributed training for deep
neural networks and the idea of compressing the gradient exchange to allow it to be done
more efficiently.
Song details the evolution of distributed training systems based on this idea and provides
a few examples of centralized and decentralized distributed training architectures such
as Uber's Horovod as well as the approaches native to PyTorch and TensorFlow.
Song also addresses potential issues that arise when considering distributed training such
as loss of accuracy and generalizability and much more.
A couple of quick notes before we jump in.
First, our next monthly meetup is taking place on Tuesday, June 12th.
Cardiologist level arrhythmia detection with convolutional neural networks, which is
work by researchers in Andrew Ng's lab at Stanford.
For more information on the meetup, visit twimla.com slash meetup.
Next, I've also posted some updated information about our fast.ai study group, which starts
this week.
We've had nearly 300 people sign up to do this together and the Slack group has been buzzing.
To learn more, visit twimla.com slash fastai.
All right, let's do it.
All right, everyone.
I am on the line with Song Han.
Song is going to be starting in July 2018 as an assistant professor in the EECS department
at MIT and recently received his PhD in doubly from Stanford University.
Song, welcome to this week in machine learning and AI.
My absolute pleasure.
I'd like to be here to join this podcast.
Absolutely.
Tell us a little bit about your background and how you got interested in machine learning
and AI.
Okay, of course.
So I recently graduated from Stanford University, advised by professor Bill Daddy.
So initially, I was doing hardware architecture research and very, in my PhD, when I started
my project, my usual goal was to do specialized hardware accelerators to accelerate different
applications since the semiconductor industry has moved from single core, multi-core core,
now it's the era of domain-specific accelerators.
And then when I started my PhD in 2012, that's exactly when Alex and I came and then image
not accuracy, just booming up.
So I feel like that's a great opportunity to explore domain-specific architectures to
accelerate the deep learning and particularly deep neural networks, which used to be pretty
slow and takes a lot of computation, lots of memory.
And I feel it is a perfect alignment between my previous expertise and computer architecture
and also given the ripet progress of deep learning to bridge the gap between machine learning
and computer systems.
So that's how I got into this area.
Fantastic.
You've done some work around FPGA's but also beyond the silicon accelerators to distributed
machine learning, is that right?
Right.
Right, exactly.
So in my PhD thesis, I focused on most on inference.
So focused on most on inference, building specialized architectures for inference and then
prototype it on FPGA due to the low-budget of actually taking out a tube.
And most recently, I've been working on not only inference but also training, since
training in large scale deep neural networks take days or even weeks, which greatly limits
the productivity of machine learning researchers.
That's why I look into training to improve but the efficiency of training as well.
And the technique that you've written about recently is one called deep gradient compression.
Tell us a little bit about the motivation for that research.
So deep gradient compression is a technique to compress the gradient exchange for distributed
training.
So why do we need to distribute the training?
We want to have more parallelism so that we can try to finish the training in a shorter
time.
So previously, we trained immediately that it takes about a week.
And there's work from, for example, visual training image net in one hour.
So increasing the parallelism will decrease the training time.
We can, through more GPUs and more computation, to decrease the time for computation, compute.
But there is another factor that determines how much time it needs to train neural network
that is communication.
Because the more node you have, the more communication you want to have.
Because different nodes, they have to exchange their gradient during distributed training,
which will limit the scalability of distributed training.
Scalability means with, say, with 64 node compared with one node, you ideally want to
have 64 X speed up compared with using just one node, but usually due to the bottleneck
of communication or networking, it's hard to achieve 64 X speed up.
Some previous work has achieved super great scalability, for example, Ubers, Framework
Card, PowerWord, and also PyTorch.
So usually that require a great networking infrastructure, for example, Infinity Band
or 40 gigabit Ethernet.
And I was motivated by how can we enable such kind of distributed training with cheap commodity
networking infrastructure.
For example, I AWS on Amazon and AWS, for example, they have a 1 gigabit Ethernet, but
still we want to benefit millions of machine learning practitioners who cannot afford those
super expensive, dedicated network and infrastructure that Facebook researchers or Google researchers
can use.
So that motivates me to kind of democratize AI training, even on such commodity hardware.
Yeah, that motivates me to work on this deep breathing compression.
Okay.
Let me take a step back and maybe have you walk through on the compute side when we're
trying to distribute training, what the different elements of distributed training are.
So you talked about the need to share gradients, where does that come from?
So let's back up to talk about where the gradients come from.
So training deep neural nets is using a basic algorithm called grid in descent or stochastic
grid in descent most recently.
So it's calculating the first order derivative of the weights.
So first of all, it's using a convex optimization method to solve a large non-comax problem,
which requires us to get the first order graded for each weight.
So think about it as this.
So we are climbing down a hill, for example, which is very similar to an optimization problem.
So you have 360 degrees and you want to choose which direction would you go in order to climb
down the hill.
So a fastest way to go down the hill is to follow the direction that is steepest, right?
That is analogy of the gradient.
The gradient is very analogy to the finding which direction is the steepest.
And then you follow the steepest path at each step, you follow the steepest step.
And then you will go to the bottom of the valley, right?
So that is for a single node.
And when you have multiple nodes, each node will have a bunch of training images.
Say we have four different nodes, and each one is finding their own direction, climbing
up, up, going down the hill, and then how do you merge them together?
So they need to communicate and exchange this gradient through networking.
And exchanging the gradient can be pretty bulky.
Say, Alexander, it has 240 megabytes of weights, so it's also 240 megabytes of gradient.
For example, a resin 50, it has 100 megabytes.
So every iteration, you have different nodes that have to exchange 100 megabytes of gradient
to each other, which makes it a bottleneck for the networking infrastructure.
And so does every node need to know all of the gradients that the other nodes have worked
on?
Is this for the updated zone weights?
Yes, for synchronized training, it is required, and that's exactly why it requires so much
networking bandwidth.
Since each node synchronized with training, each node has to have all the gradient information
for its neighboring nodes.
How is the work distributed among the various nodes that are working on a problem?
Is it, are they each given batches, for example, to work on?
Or is there some other, is the gradients distributed randomly, or does that matter at all?
Oh, that's a good question.
So there are two ways, usually two ways of achieving such parallelism.
One is data parallelism, the other is model parallelism.
So data parallelism is having different chunks of training data to different nodes.
And model parallelism is having different chunks of the model across different nodes.
Data parallelism is a lot more easier, is a lot easier to implement than model parallelism.
So in this case, I'm talking about data parallelism.
And it's a specific, how data parallelism is achieved is by, we have the same model on
sitting on each node, on each training node, the same model, model can be a convolution
neural net or recurrent neural net.
And then we feed different chunks of training data to each node, save the first node may
have a batch of image 0 through 31, and then node 2 may have image 32 through 63, etc.
So all the nodes are sharing the same model, but they are having, they are being fed with
different chunks of training data.
And they calculate their local gradients according to their own piece of data.
And then they exchange the gradient to each other.
So it's a, it can be implementing two ways, one is preemptive server, the other is
more reduced, are reducing simpler, so I will talk about that.
So are reduced is by every node after calculating their own gradient, it absorbs, it takes the
gradient from all the other node.
And in the meantime, it will also send its own gradient to all the other node.
So everyone receives, everyone's all of the gradient and then sum it up and then calculate
an average.
So what's the architecture for doing this, for an implementation perspective, I can imagine
a number of ways of doing this, putting them in some kind of shared storage and having
all of the distributed workers pull from that shared storage or using some kind of message
passing architecture, is that something that you've explored, the different ways that
one could do this?
Yeah, there are in general two ways to do this.
The first one is using a preemptive server, which is centralized, I think of it in that
way.
So everyone will send out all the training nodes, will send their local gradients to the
preemptive server.
And the preemptive server, by the name we know, it's storing the preemptive, storing
the model.
So it receives all the gradient from different node and then sum it together, sum it up and
added to the shared decentralized preemptive server and update the model and then it broadcast
the model to all the training node.
So for example, if you have four training node, all the four nodes, we will send their own
gradient to the centralized preemptive server to one update and then the preemptive server
will broadcast the updated way to all the training node and then all the training node
starting another iteration of gradient calculation.
So it forms a cycle.
So that's the first way using a preemptive server.
And there's a second way using a decentralized way.
So the preemptive server is a centralized way, right, you have a centralized preemptive
server.
What about a decentralized way?
So a decentralized way, we really use this all-reduce operation.
So for example, you have one, two, three, four, four training node and then still you have
a master training node, for example, node one and then all the node will send their own
gradient to node one, for example, node two, send it to node one, node four, send it
to node three, node three, send it to node one.
So in the end, node one will have the gradient information for all the node.
And the next step is node one will broadcast updated model to node two, three, and four.
So node one will first broadcast to node three and then node one will broadcast node two
and node three will broadcast node four.
So in this tree structure, everyone can get, everyone can get everyone's gradient
information and update.
So this is just one basic implementation using a tree structure.
There is a more advanced implementation with using a butterfly structure.
Yes, butterfly, everyone sends a portion of them to the neighboring.
So the idea is still everyone should get everyone's gradient.
But in that way, it's more efficient than a tree structure.
So in summary, there are two ways, centralize the way using parameter server and decentralize
the way using this all-reduced operation.
Circling back to your work in particular around deep gradient compression, what you're
trying to do is reduce the communication overhead of this last step that we've just been
talking about.
How do you do that?
So the basic idea is to reduce the necessary amount of gradient that we need to send.
So surprisingly, we find only 0.1%, only 0.1% of the gradient that is really needed to
be sent out over the network.
So the other 99.9%, you can hold it locally.
You don't have to send it out.
By 0.1%, is the idea that when we talk about these gradients, we're talking about vectors
that need to be moved around, is the idea that these vectors are very sparse, that they're
mostly consistent zeros and you're spending a lot of, using a lot of your time and effort
moving around these zeros, essentially, that aren't adding information.
Right, that's the key idea.
Although initially, without any treatment, they are not sparse.
Although some of the gradient are super small, they are not zero, but they're small.
So the way we deal with it is to sort the gradient and zero away all those 99.9% the smallest
and then only send out those 0.1% the largest gradient over the network.
But simply not simply doing this, only by such kind of thresholding and making a dense
gradient vector to make it sparse, we'll hurt the prediction accuracy.
So we found those small gradients, although some of them may be noise, but if we don't
send it out, it will hurt the accuracy.
So what we do is to locally accumulate those small gradients for more iterations until it
gets large.
When it gets large enough, then we send it out.
So in this way, we can recover a lot of accuracy.
All right, interesting.
So the idea there is that you've got your, you're training across, you know, some number
of nodes, say 10 nodes, you've got each of these nodes is exchanging gradient information via
one of the couple of ways that we've talked about.
You found that most of that gradient information isn't useful, but it sounds like the key
insight is that it may be useful in the future as that node continues to train.
So you don't want to just kind of throw it all away, you know, at each training iteration,
you want to continue to accumulate the other 99% of the, or 99.9% of the gradient information
over time.
And you find that some of that becomes useful later on.
Yes, exactly, that's exactly the idea.
So another way, that's one way to, another way, a mathematical way to interpret that
is to increase the equivalent batch size of the gradient.
For example, rather than calculated gradient and immediately send it out and do the update,
we accumulate them locally for another iteration, maybe for third iteration until it reaches
a threshold.
It's equivalent, say you use three iterations, that is equivalent to increasing the batch size
almost equivalent.
It seems that the way that has changed a little bit, almost equivalent to increase the
batch size by three times.
That's another intuitive way to understand this.
But it strikes me that that's a little different in the sense that in the initial way you
described it, because you're always sorting and thresholding on the communication, but
not on the internal state of a node, the information in a particular part of the gradient could
stay around forever for, you know, much more than three iterations of the, or three batch
iterations.
It's highly possible you accumulated very long, even more than three iterations, say it
can be a 10 iteration or 20 iterations, but you don't send it out until it reaches, it
reaches top 0.1% of the total gradient magnitude of the gradient.
So how do you reconcile that with the batch interpretation that you just mentioned?
So that piece of gradient isn't sent out, it didn't do the update, it's just calculated
and then accumulate across batches, it just accumulates the batches.
So if you exchange the summation, if you exchange the summation, you are doing such calculation
across key iterations.
So if you put a key in the learning rate and also put a key in the denominator, it's
equally to equal with increasing the learning rate by T times and also increase the batch
size by T times.
Okay.
So this batch interpretation is more reflective of what's happening at the system level
than what's happening at an individual node, is that fair?
That's a fair interpretation, although it's not 100% mathematical equal, so it's in
for each iteration, the weight guys changed.
So it's using different weight for different iteration in real case.
Okay.
Yeah, so that's not the key idea, but that's not the only idea to recover the accuracy.
So for example, on image classification, just with this local gradient accumulation, we
still suffered from 1.6% loss of accuracy and on language modeling, we suffered 3.3% loss
of accuracy.
So we were thinking how do we, it's pretty close, like 1%, 3%, but not perfect.
For context, what kind of loss of accuracy did you see with a more naive approach to gradient
compression?
What were you just mentioning?
Pure, right.
Pure stretch holding and not doing the locomotion locally doesn't converge on either image
classification problem or the speed recognition.
So it doesn't, it just doesn't work.
No, it just doesn't work.
But even so we added the local gradient accumulation, and then it covered it is.
And from sci-fi 10 to 110, the original baseline accuracy is 92.9, now is 91.36, it's pretty
close.
And now let's see how can we further close the gap?
Is there any loss of accuracy in moving to distributed training at all, or are we able
to fully capture the accuracy of a single node solution in distributed training?
Less using super large batch size, and people have already shown using large batch size
nowadays, and converge really well.
So it's almost a consensus that using a distributed training compared with a single node
training, we can get very comparable accuracy.
Although generalization ability is still under, under, under discussion, but the accuracy
nowadays, it's almost solved the problem of distributed training using multiple node
achieved by the same accuracy as using the same node.
What does that mean?
Why would generalization be different between single node and multi node training?
That is an unsolved problem.
Why using a large batch size and generalization ability from one training dataset and to
another dataset can't be the same, so that is an unsolved problem to interpret why about
the generalization ability using large batch training.
But it's the generalization challenge with distributed training isn't so much relative
to single node versus distributed.
It's more because in order to do distributed, you increase the batch size than you introduce
this issue around generalization.
Right.
It's only when the batch size is usually super large, say, more than, say, 8K or even more,
then this problem begins to appear.
Okay.
Yeah, but it's not a problem of decreasing a decrease in the network environment.
Okay.
So shall we go back to the session?
Okay.
So how do we recover those 1%, 3% loss of accuracy?
We find we need the momentum.
Momentum SGD is usually dominant in current neural network training.
We are not using the vanilla, the naive SGD, but we are using the momentum, which means
we are using part of the previous gradient together with the current gradient.
We do a weighted average having a discounting factor to avoid the noise.
So say we are seeing, okay, we are going down the hill.
We should go this direction is the steepest, but we don't go directly with this direction.
But it's a, it's a sum of part of the previous gradient, part of the previous direction
we have already gone through and together with the current, the current gradient.
So that's called momentum.
And with momentum, we are adding the sum, we are summing up the previous gradient with
the current gradient, which give a new vector called the code velocity.
And we are multiplying the velocity with the learning rate that subtracted from the original
weight.
And in this case, we found we should do local accumulation of the velocity rather than
local accumulation of the gradient.
So accumulate the velocity, not the gradient.
Are you using momentum and the velocities throughout or are you uniquely using the velocities
in the accumulation, but using vanilla gradient descent when you're doing the distributed
part?
We are using not the vanilla gradient descent, but momentum gradient descent.
With momentum, with that, we used just because there's momentum term in the gradient descent,
we need to add, we need to accumulate the velocity rather than the gradient.
Okay.
Got it.
Right.
And with this technique, there's a mathematical proof in the paper why we need to accumulate
the velocity rather than the gradient, but you can feel free to check out the paper to
read the math right here, just a very intuitive way to understand this is to take into account
of the previous gradient, the momentum term, so that we need to accumulate the final summation
of the gradient and the previous gradient.
So we accumulate the velocity rather than the gradient.
And with this method, we found the previous 1.5% loss of accuracy now is only 0.36% image
classification, which is closer, but still is not the way we want.
Still have 0.3% loss of accuracy, but for speech recognition, surprisingly, we found
after using this momentum correction, it didn't converge.
So there's still some problem we need to solve.
So shall we move on to the third technique to recover the accuracy?
Okay.
Okay.
So why particularly for speech recognition, the accuracy, even didn't converge using
this other one's method because the gradient management problem.
So previously, those hours, times and hours, we're having this technique called gradient
clipping after doing the summing up the gradient for all the node to prevent from gradient
explosion.
As soon as the RSTM has backed propagation through time, it's very easy to suffer from
either gradient management or gradient explosion.
And now we are doing the clipping.
So the denominator will be the square root of the summation of the sum of the sparsely
lottery from all the node.
So we found we need to exchange the sequence of sparselyplication and gradient clipping.
We need to move the clip, so previously, do the pruning first, sparselycation first,
and then do the summation and then do the clipping.
Now what we need to do is do the sparselyplication and do local gradient clipping and then sum
it up, so we exchange the sequence from between clipping and summation.
And in this way, each gradient, even before they get summed up, they are clipped to the
max value.
So it's very unlikely to suffer from local gradient explosion problem.
And with this technique, the RSTM for speech recognition finally converged, and the accuracy
loss becomes 2% compared with previous, it is 3%.
Did you then go back and apply that method to the computer vision problem?
Yes, computer vision problem now, the, so for computer vision problem, it's a greeting
explosion is a particular problem for RSTM.
Right.
It's not a problem, not usually a problem for used in computer vision tasks, convolution
neural nets.
And convolution neural nets already, it has very close accuracy, only 0.36% loss of accuracy.
So that's not a problem.
Right.
I was thinking of it more from the perspective of, you know, are you moving towards a single
approach that you can apply to, you know, both of these types of problems, or are you,
you know, do you fork it, do you determine which problem and then you apply what we previously
talked about for computer vision, and then you make some tweaks for the LSTM, RNN type
of example.
I would say it's a very general, seems, um, for seniors, they're not much balanced in
resource, for example, there's greeting clipping is not needed for convolution neural
nets, but this is really the inherent, inherently, the, um, the way we deal with RSTM, people
usually have used this greeting clipping, then for greeting compression, we also need
to apply, um, corresponding techniques.
Okay.
So it's not the new, um, trouble people have to deal with.
It's all the trouble of people have to deal with the greeting explosion problem for RSTM,
and then during greeting compression, we also have to take care of that part and I have
a problem.
So just to speak to the way you handle that problem previously.
Exactly.
Okay.
Yeah.
So we are very close to success only 0.36 loss of accuracy on the revision and 2.16 loss
of accuracy on speech recognition, now we solve this problem.
So we found, uh, as we mentioned, those, um, accumulating the velocity, it will take a
long time, say 10 iterations, 20 iterations, even 100 iterations.
So we did this profiling of exactly how many iterations are those accumulations and
happens.
And we found it's a really, it has a really long tail.
Some of the velocity gets accumulated and broadcasted after a 2,000, 2,000 iterations.
So you are 2,000 previous previously 2,000 iterations ahead of time.
Now you apply this gradient, which is so obsolete.
Then we found it's very necessary to cut or to throw away, to mask away these kind of
obviously lead velocity terms.
Just like a student, if he didn't turn in his homework for one week, then it's probably
fine.
But if he didn't turn out turning the homework for a whole semester, then just just fail
him.
Rather than continue giving him the challenge and interpreting, uh, and disturbing other,
his estimate, for example.
So by this momentum factor masking, we, uh, moved the loss of accuracy from 0.3% to
0.1% in vision, and for speech recognition, it's from 2% to 0.5%.
How do you do that in practice, you know, you're, uh, I guess the way I'm interpreting
this is you've got this, uh, you're trying to get rid of contributions in a current
time step from a momentum, a velocity vector, you know, that's too old.
But you're, as I understand it, you're not really keeping around that much state.
It's just kind of continual accumulation.
How do you identify and get rid of the, the older velocity vectors?
Uh, so the question is, how do we keep track?
How long they still haven't turned in his homework and, and how old, uh, this, uh, how old
it is, this stillness becomes, right, right, right, um, I forgot the detailed implementation
about, about the tracking.
Maybe we can turn it, uh, turn to the paper to, to see our details trying to, um, uh, keep
track of how, how, how, how still it is.
Yeah.
Okay.
To turn into the paper for that detail, but it, it sounds like then you, you are introducing
some new kind of bookkeeping scheme to keep track of this.
It's not something that, that falls out of the, the prior implementation very easily.
Probably need some counterlocally to count that.
Okay.
And so the impression that I'm getting with this method is that, you know, it requires
a lot of manual kind of massaging of the way that you might otherwise implement, uh, your
training.
Is that the case or, you know, is it possible to, uh, for example, to generically re-implement
some libraries and like a tensor flow or something like that that would, uh, automatically
do this for you?
Oh, let me close up the last technique and then we talk about back question.
I think that's a good question.
Yeah, so we want to close the last small gap of 0.1% loss of accuracy.
And we found that the technique to deal with that is by warm up training.
So in the first 1% of the training, we don't use 99.9% of sparsity, but we use, um, uh,
but we, but we use 75 and then 1995 and then exponentially increased sparsity in the
first couple of epochs until it enriches 99.9% and then in this way, we can, we saw
accuracy actually improved by 0.4% on image classification and then improved by 0.4% on
speech recognition.
So that's called as the whole story that we can fully recover the accuracy.
Sorry.
What was on the computer vision?
What, where did you end up?
I thought we were at 0.36 before previously.
We were losing 0.36, okay, point accuracy.
Now we are better.
We are having a better accuracy by 0.37% on the baseline, okay, yeah, even better than
the baseline.
And see more of us speech recognition, even better than the baseline.
Is this advantage of warm up training independent of the total number of training iterations?
In other words, intuitively for me, uh, I get that the, that kind of backing off over time,
the amount of information you're, you know, throwing away, so to speak, would accelerate
training, um, but part of me thinks that, you know, if you let training go on long enough,
you'd eventually make that up, uh, and then wouldn't be this big difference, you know,
offered by warm up training.
Uh, that's not completely, uh, what I meant.
So warm up training, first of all, we are using the same amount of training iterations,
same amount of training iterations.
Warm up training is just saying for the first 1% of the training epochs, first 1% of
the training epoch, we have a less sparsely enough in that part, but still we are having
the same amount of iterations, we're not increasing the number of epochs.
I understand, I guess the, the question was, if you could achieve the same level of accuracy
with more training iterations as opposed to using warm up training.
In other words, my intuition at least is that the warm up training would, you know, accelerate
convergence, but not necessarily get you better accuracy, it just makes it take the
last time because you're starting with more data.
Uh, the reason we use warm up is due to the first couple of epochs, we need to allow those
drastic changes of the gradient, which is everything has, has a lot of noise, which would
encourage those noise.
Right.
Uh, but in the end, it's getting more and more stable.
Um, we don't need those kind of noise.
And manipulate just increasing the number of training epochs, it just begins to fluctuate
of the accuracy begins to fluctuate in the later epochs, not necessarily increasing
the accuracy.
Okay.
Interesting.
So that's why we need to meet in the first couple epochs to allow a dramatic disturbance
of the weight of the gradient.
Okay.
So presumably you're starting with some kind of randomized weights.
Uh, so this is a way to kind of flush out those randomized weights a lot more quickly
in a sense.
Right.
Exactly.
Okay.
Yeah.
And then so now we have covered everything.
We can go back to your previous question about, uh, how about a knob, so we need to tune
and require lots of tuning on, not lots of knob tuning.
So all the techniques we discovered these, uh, different techniques are due to original
method requires such, um, for example, uh, momentum, SGD, you, you have another term
of momentum, then we have a recipe to deal with that.
If you don't have a momentum, then just, uh, just accumulate the gradient.
Now we accumulate the velocity.
And then recurrent neural nets has this local gradient clipping, which is now due to,
due to us, but every people just use gradient and clipping for recurrent neural nets.
So correspondingly, we, uh, find our counterpart to deal to, um, to make it compatible with
gradient clipping.
Then that's another technique.
And then for warm up, similar, that's a common sense, uh, that during the first couple
of iterations, either you use large batch training, you all need to allow a certain, um, warm
up of the gradient.
So we find just 1% of the training epoch works pretty well for different tasks.
So no, don't need to tune the warm up period.
Just give it 1% of the time, make it have a exponentially growth of the viscosity until
99.99%, that works, uh, that works pretty well.
So all these knobs and tricks are based on the original, uh, requirements of, uh, original
different techniques requirement.
The networks that you used for this, did you, um, presumably you kind of handcrafted
these, you know, these tricks required for distributed use, uh, do you envision or plan
to create standard implementations of this, or have you already done that?
Oh, suddenly a place along me to suddenly jump to your previous question.
I suddenly remembered how I implemented the graded mask, remember you mentioned, do we
have to go to the keep, keeping, right?
So no, we periodically, periodically clean up, clean up all the other, uh, gradient local,
local velocities.
So no matter who delayed by how many iterations, just clear up everything.
Oh, okay.
Just like, yeah, that means there's no bookkeeping required.
Just like every, every year we have two semesters, right?
One is in winter, one is in summer, and if you don't turn your homework by summer, then
you fail.
Yeah.
Yeah.
Any of your, uh, any of your future, any of your future MIT students might be getting
a little scared listening to this podcast and how often you're talking about failing
them.
No, no, just an example, hopefully to help, uh, I usually use this kind of plan and analogy,
but, uh, uh, I'm really pretty nice to students, actually, because the students are doing this
work, we'll, uh, I made them an offer to MIT, uh, this, um, the collaborator, we worked
on this project.
His name is Yijun Lin, um, and then we're at it from Chinhua University, we worked together
last summer, and, uh, I, uh, made him an offer to MIT, and also got him a fellowship.
I'm really pretty nice to students.
Awesome.
Awesome.
Yeah.
Great.
So I think we're coming up, uh, to the end of our time, is there any other element
to this that you'd like to, to talk about?
Yep.
Um, yeah, just now you were mentioning whether to bring up it to a standard, uh, implementation.
I'm really looking forward to collaborate with other researchers or companies, uh, to
bring it to standardize this framework, say in Amazon EC2, or in Google Cloud, or NVIDIA
Cloud, that would be something to benefit, to help democratize, uh, deep learning, uh, training
in, uh, commodity hardware.
So by, uh, compute, not networking, um, and actually, for example, in, uh, I feel there
are a few candidates, um, for example, in Harvard, uh, distributed training framework, uh,
done by Uber.
That's a very good, uh, thing to start with, and if any students or any, uh, researchers
are interested in exploring this, um, habit to collaborate, um, to work, uh, work together
on, on this direction, and in general on this kind of in, in, uh, uh, improving the efficiency
of large-scale distributed training to make it more, uh, scalable.
And by the end of this talk, I would like to give a small advertisement of my future lab,
if you allow.
Absolutely.
At MIT.
So I will be finding the hands-like.
So hand is my last name, and I also represent my research.
So H stands for, uh, high performance, high energy efficiency hardware.
Uh, so lots of hardware research going on in my lab.
And A stands for, um, architectures and accelerators for, uh, artificial intelligence.
So architecture means both the computer architecture and the neural network architecture.
How do we design those smart, compact models that have the same accuracy, um, for neural,
uh, for deep learning, for example?
An instant for, uh, noble algorithms for neural networks, um, and deep learning.
And S stands for, uh, small, machine learning models.
How do we compress the models?
How do we compress the gradients?
Make it less memory footprint, less computation, more efficient, and also scalable systems.
So how do we make deep learning large-scale training more scalable with the linear speed
up as to go?
So that's the vision or that's the mission of Hans Lab at MIT.
And, um, welcome, hope, hope we can have more, uh, collaborators in the future.
Yeah.
Fantastic.
Well, it sounds like, uh, you've got some really exciting things planned, and I'm looking
forward to, um, touching base sometime in the future to, to, you know, check in on what
you've been up to.
Oh, definitely.
That'll be, that'll be great.
Awesome.
Thanks, Song.
All right.
Thank you.
All right, everyone, that's our show for today.
For more information on Song or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 146.
If you're a meetup member, keep an eye on your inbox for some updates on all we've got
going on with that program.
Thanks so much for listening, and catch you next time.
