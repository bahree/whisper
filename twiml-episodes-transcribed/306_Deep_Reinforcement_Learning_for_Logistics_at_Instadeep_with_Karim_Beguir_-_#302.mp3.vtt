WEBVTT

00:00.000 --> 00:04.960
We are just one week out from the premiere event focused on accelerating,

00:04.960 --> 00:09.000
automating, and scaling, machine learning, and AI in the enterprise.

00:09.000 --> 00:12.000
That's right, Twomokon AI Platforms.

00:13.000 --> 00:17.500
Over the past few weeks, we've shared a ton of details about the conference,

00:17.500 --> 00:22.000
including our live keynote interviews where I'll be onstage with Andruing

00:22.000 --> 00:25.000
and leaders from Uber, LinkedIn, and Cruise.

00:25.000 --> 00:30.000
Our Super Size 2 Track Agenda, complete with speakers from Levi Strauss, Zappos,

00:30.000 --> 00:33.000
Capital One, Airbnb, and many more.

00:33.000 --> 00:36.000
Our community experiences like our Day 2 Unconference,

00:36.000 --> 00:41.000
where attendees will propose and vote on topics to further explore in small group sessions.

00:41.000 --> 00:45.000
And the Twomokon Happy Hour, complete with food and drinks,

00:45.000 --> 00:50.000
interview, and photo booths, and AWS Deep Racer Contest, and much more.

00:50.000 --> 00:54.000
It's not too late to get your tickets, but you'll need to act fast

00:54.000 --> 00:56.000
if you don't want to miss out.

00:56.000 --> 01:00.000
Head over to twomokon.com now, and secure your ticket for what's shaping up

01:00.000 --> 01:02.000
to be an amazing event.

01:02.000 --> 01:05.000
And now, on to the show.

01:11.000 --> 01:12.000
All right, everyone.

01:12.000 --> 01:14.000
I am on the line with Kareem Baghear.

01:14.000 --> 01:18.000
Kareem is the co-founder and CEO of InstaDeep.

01:18.000 --> 01:21.000
Kareem, welcome to this week in machine learning and AI.

01:21.000 --> 01:24.000
Hi, Sam. Thank you for having me.

01:24.000 --> 01:27.000
You are very welcome to be on the show,

01:27.000 --> 01:30.000
and I'm looking forward to diving into the conversation.

01:30.000 --> 01:33.000
But before we get into our main topic,

01:33.000 --> 01:38.000
which will be focused on AI innovation in the field of logistics,

01:38.000 --> 01:40.000
I love to learn more about your background.

01:40.000 --> 01:44.000
You are in a, can we call it a unique position

01:44.000 --> 01:50.000
of having co-founded a deep learning company in Tunisia, North Africa.

01:50.000 --> 01:53.000
How did you get to that point?

01:53.000 --> 01:58.000
Yeah, actually, I grew up in southern Tunisia in a city

01:58.000 --> 02:00.000
which is very familiar to Star Wars lovers,

02:00.000 --> 02:02.000
because it's actually Tatooine.

02:02.000 --> 02:05.000
So it's the same name that George Lucas used

02:05.000 --> 02:09.000
for the first movie about Star Wars.

02:09.000 --> 02:11.000
So grew up in southern Tunisia,

02:11.000 --> 02:14.000
and my love of applied mathematics took me then

02:14.000 --> 02:17.000
to graduate education in France,

02:17.000 --> 02:20.000
called Polytechnic, and ultimately to the US

02:20.000 --> 02:24.000
at Courant Institute at NYU in the US.

02:24.000 --> 02:26.000
And, you know, in study,

02:26.000 --> 02:31.000
is a project that, you know, I co-founded with Zora

02:31.000 --> 02:35.000
that I met in Tunisia with the goal of, you know,

02:35.000 --> 02:39.000
basically proving that we can do, you know, advanced AI

02:39.000 --> 02:41.000
and deep learning in Africa.

02:41.000 --> 02:43.000
And it's been a crazy adventure for us.

02:43.000 --> 02:46.000
We started really with like two laptops and no funding.

02:46.000 --> 02:48.000
It was a pure bootstrap.

02:48.000 --> 02:50.000
And we're very excited to, you know,

02:50.000 --> 02:52.000
how much we've come since then.

02:52.000 --> 02:56.000
Awesome. And the company to this day has most of its employees

02:56.000 --> 02:58.000
in Africa, all in Tunisia,

02:58.000 --> 03:01.000
or are there other locations?

03:01.000 --> 03:05.000
So, yes, majority of our employees are in Africa,

03:05.000 --> 03:07.000
something like 70% of the company.

03:07.000 --> 03:10.000
And after the original Tunis office,

03:10.000 --> 03:14.000
we also have opened up setup shop in Lagos,

03:14.000 --> 03:17.000
in Nigeria, as well as Nairobi in Kenya,

03:17.000 --> 03:21.000
and looking forward to opening an office soon in South Africa.

03:21.000 --> 03:24.000
So, it's been a very, very interesting ride

03:24.000 --> 03:26.000
in terms of like discovering the potential

03:26.000 --> 03:28.000
of African talent.

03:28.000 --> 03:31.000
And we also have two offices in London and Paris as well.

03:31.000 --> 03:32.000
Okay.

03:32.000 --> 03:35.000
And so, what's the company's focus?

03:35.000 --> 03:39.000
So, the company is focused on basically building

03:39.000 --> 03:43.000
advanced decision-making systems for the enterprise.

03:43.000 --> 03:45.000
So, if you look at, you know,

03:45.000 --> 03:47.000
deep learning and uses of AI,

03:47.000 --> 03:49.000
historically, you know, most companies

03:49.000 --> 03:52.000
and most startups focus on pattern detection.

03:52.000 --> 03:56.000
So, advanced uses of either machine learning or deep learning.

03:56.000 --> 04:00.000
But at Instady, we actually believe that, you know,

04:00.000 --> 04:02.000
technology is around reinforcement learning

04:02.000 --> 04:05.000
and the like are coming now to a state of fusion

04:05.000 --> 04:09.000
that make them actually deployable for the enterprise.

04:09.000 --> 04:12.000
And we anticipate significant savings.

04:12.000 --> 04:15.000
So, we are focusing on helping large companies

04:15.000 --> 04:18.000
realize the promise of AI, which is, of course,

04:18.000 --> 04:20.000
easier said than done.

04:20.000 --> 04:23.000
So, our role is to help them, you know,

04:23.000 --> 04:25.000
seize the opportunity in AI,

04:25.000 --> 04:28.000
while at the same time, you know, as discussed,

04:28.000 --> 04:30.000
building up a new generation of talent

04:30.000 --> 04:32.000
and making people believe that, you know,

04:32.000 --> 04:33.000
actually their dreams can come true,

04:33.000 --> 04:35.000
they can be part of this.

04:35.000 --> 04:36.000
Nice.

04:36.000 --> 04:39.000
Nice. You mentioned reinforcement learning.

04:39.000 --> 04:42.000
Yeah, that comes up on the show quite a bit.

04:42.000 --> 04:47.000
And there are very, it can be a polarizing topic

04:47.000 --> 04:50.000
in the sense that there are folks that I talk to

04:50.000 --> 04:53.000
that are very excited about it.

04:53.000 --> 04:55.000
And the promise that it offers not just

04:55.000 --> 05:00.000
in these traditional RL applications like game playing,

05:00.000 --> 05:03.000
but for enterprise applications.

05:03.000 --> 05:05.000
And then there are other folks that say,

05:05.000 --> 05:07.000
it's way too complicated.

05:07.000 --> 05:11.000
We're nowhere near being able to put it to good use.

05:11.000 --> 05:14.000
It sounds like you're in this ladder camp.

05:14.000 --> 05:16.000
So, yeah, we believe, actually,

05:16.000 --> 05:18.000
that the technology is sufficiently mature now

05:18.000 --> 05:21.000
to get to practical applications.

05:21.000 --> 05:24.000
I mean, if you see historically, you know,

05:24.000 --> 05:26.000
DeepMind can start to become, like,

05:26.000 --> 05:30.000
a deep RL started to become something impressive

05:30.000 --> 05:33.000
with originally DeepMind and the progress they did

05:33.000 --> 05:35.000
with DQN on Atari Games and the like.

05:35.000 --> 05:38.000
But now with progress on multiple fronts,

05:38.000 --> 05:41.000
it's actually possible to deploy these technologies

05:41.000 --> 05:42.000
in real life.

05:42.000 --> 05:45.000
And, you know, we expect this to become

05:45.000 --> 05:47.000
more and more mainstream.

05:47.000 --> 05:51.000
So, you know, we actually doing work in multiple sectors,

05:51.000 --> 05:54.000
but in particular in logistics that confirms

05:54.000 --> 05:57.000
that these technologies can actually be deployed

05:57.000 --> 05:59.000
and can actually generate savings

05:59.000 --> 06:02.000
versus more classical technologies.

06:02.000 --> 06:05.000
Okay, and so we've referenced a couple of times

06:05.000 --> 06:07.000
the field of logistics,

06:07.000 --> 06:10.000
the connection being a presentation that you delivered

06:10.000 --> 06:14.000
at the recent Nvidia GTC conference

06:14.000 --> 06:17.000
on some of the work that you've seen

06:17.000 --> 06:20.000
and that you're doing on AI and logistics.

06:20.000 --> 06:24.000
Can you maybe frame up the problem for us?

06:24.000 --> 06:27.000
What are some of the specific challenges

06:27.000 --> 06:30.000
that enterprises are seeing in logistics

06:30.000 --> 06:34.000
and what are the gaps between, you know,

06:34.000 --> 06:36.000
logistics obviously has been a problem

06:36.000 --> 06:39.000
for many, many, many hundreds of years.

06:39.000 --> 06:42.000
And we've developed technology-based solutions

06:42.000 --> 06:45.000
to try to solve these problems.

06:45.000 --> 06:47.000
What are the gaps that are leading folks

06:47.000 --> 06:50.000
to look at deep learning

06:50.000 --> 06:54.000
and machine learning-based approaches?

06:54.000 --> 06:56.000
So, you know, on logistics,

06:56.000 --> 06:58.000
you have multiple problems

06:58.000 --> 07:02.000
that actually require making decisions

07:02.000 --> 07:04.000
in a complex environment

07:04.000 --> 07:07.000
where you have a very large number of choices.

07:07.000 --> 07:09.000
So, just a specific example,

07:09.000 --> 07:11.000
if you look at a right-sharing, for example,

07:11.000 --> 07:14.000
for a mobility company such as Uber or others,

07:14.000 --> 07:16.000
effectively you have a situation

07:16.000 --> 07:20.000
where you have hundreds of cabs in like a big city

07:20.000 --> 07:23.000
and at the same time maybe, you know,

07:23.000 --> 07:26.000
hundreds or thousands of riders requesting a ride

07:26.000 --> 07:28.000
and with a right-sharing allowed,

07:28.000 --> 07:30.000
basically you can onboard, let's say,

07:30.000 --> 07:32.000
three to four people in a given cab.

07:32.000 --> 07:34.000
This is a complex decision to make.

07:34.000 --> 07:38.000
Which cab should cater to which requests?

07:38.000 --> 07:41.000
It's not an easy decision.

07:41.000 --> 07:43.000
And this is typical of what we believe

07:43.000 --> 07:45.000
is the opportunity for, you know,

07:45.000 --> 07:48.000
deep reinforcement learning-based systems.

07:48.000 --> 07:50.000
So, you effectively have to look for a needle

07:50.000 --> 07:52.000
in a hay sack and, you know,

07:52.000 --> 07:54.000
one of the ways to do that is actually

07:54.000 --> 07:56.000
to train a modern AI system

07:56.000 --> 07:58.000
to look for good solutions.

07:58.000 --> 08:01.000
So, it really comes down to,

08:01.000 --> 08:03.000
you have a very large search space,

08:03.000 --> 08:06.000
sometimes bigger than the number of atoms in the universe.

08:06.000 --> 08:08.000
And you need to find a clever way

08:08.000 --> 08:10.000
to go through this search space.

08:10.000 --> 08:12.000
So, I'll tell you historically what enterprises

08:12.000 --> 08:15.000
have been doing to solve this problem,

08:15.000 --> 08:18.000
is, you know, if you look at operations research,

08:18.000 --> 08:22.000
OR, typically all the algorithms

08:22.000 --> 08:24.000
that are used in OR are clever ways

08:24.000 --> 08:27.000
to make that search space simpler

08:27.000 --> 08:30.000
and they are more amenable to, you know, algorithms.

08:30.000 --> 08:32.000
So, that is, you know,

08:32.000 --> 08:34.000
that works pretty well in practice,

08:34.000 --> 08:36.000
but it's far from being optimal.

08:36.000 --> 08:38.000
And typically, we're talking about problems

08:38.000 --> 08:41.000
that, you know, have a very large search space

08:41.000 --> 08:42.000
and can be NP-hard,

08:42.000 --> 08:45.000
so it means that there is not an easy solution

08:45.000 --> 08:47.000
to solve them.

08:47.000 --> 08:51.000
So, by making the search space smaller,

08:51.000 --> 08:53.000
you know, OR algorithms effectively

08:53.000 --> 08:55.000
are introducing biases.

08:55.000 --> 08:58.000
So, you know, they're built on heuristics,

08:58.000 --> 09:00.000
and those heuristics mean that a system

09:00.000 --> 09:02.000
that could learn from scratch

09:02.000 --> 09:08.000
could go beyond what is the state of the art at the moment.

09:08.000 --> 09:12.000
So, it's very similar to what happened in the game of chess

09:12.000 --> 09:14.000
before Alpha 0 came,

09:14.000 --> 09:16.000
so Alpha 0, the groundbreaking algorithm

09:16.000 --> 09:20.000
that DeepMind developed in late 2017,

09:20.000 --> 09:22.000
basically before Alpha 0,

09:22.000 --> 09:25.000
the best chess player was a system

09:25.000 --> 09:29.000
that had to, you know, to use lots of rules,

09:29.000 --> 09:31.000
whether coming from the programming side of things

09:31.000 --> 09:33.000
or coming from the chess side of things,

09:33.000 --> 09:35.000
but like there were hundreds of rules

09:35.000 --> 09:38.000
that were pretty much embedded into the program

09:38.000 --> 09:41.000
to make it do, you know, to make it competitive.

09:41.000 --> 09:44.000
Yet, Alpha 0 came and replaced all of that

09:44.000 --> 09:46.000
with just two simple concepts,

09:46.000 --> 09:48.000
searching and learning,

09:48.000 --> 09:50.000
and it was able to outperform, you know,

09:50.000 --> 09:53.000
stockfish this particular chess program

09:53.000 --> 09:55.000
in, you know, as little as four hours

09:55.000 --> 09:57.000
if you use distributed machine learning.

09:57.000 --> 09:59.000
So, I was watching that.

09:59.000 --> 10:01.000
I was actually at Nuribs for the first time

10:01.000 --> 10:04.000
when David Silver presented these groundbreaking results,

10:04.000 --> 10:06.000
and I was like, wow,

10:06.000 --> 10:08.000
this is actually very exciting,

10:08.000 --> 10:11.000
and it has applications that are beyond games.

10:11.000 --> 10:13.000
Like, think about, you know, as I said,

10:13.000 --> 10:15.000
the right-sharing example I gave you

10:15.000 --> 10:18.000
or, for example, you know, how you fit objects

10:18.000 --> 10:20.000
or packages in containers.

10:20.000 --> 10:22.000
All of those have this property

10:22.000 --> 10:25.000
that, you know, you need to look for the right solution

10:25.000 --> 10:28.000
in a very large amount of, you know,

10:28.000 --> 10:29.000
a very large space.

10:29.000 --> 10:31.000
And so, all of those are potentially

10:31.000 --> 10:33.000
amenable to this kind of solutions.

10:33.000 --> 10:34.000
And, you know, since then,

10:34.000 --> 10:35.000
we've been working very hard,

10:35.000 --> 10:38.000
I think, to make this promise a reality for our customers.

10:38.000 --> 10:41.000
When you're approaching a problem like

10:41.000 --> 10:43.000
the right-sharing problem,

10:43.000 --> 10:46.000
how do you transform, you know,

10:46.000 --> 10:48.000
this problem that, you know,

10:48.000 --> 10:49.000
kind of makes sense when you explain it

10:49.000 --> 10:53.000
into something that you can apply directly

10:53.000 --> 10:56.000
deep learning or RL2.

10:56.000 --> 11:00.000
So, effectively, you need to build up

11:00.000 --> 11:02.000
a model of the environment,

11:02.000 --> 11:04.000
first, which comes from, you know,

11:04.000 --> 11:05.000
the real world.

11:05.000 --> 11:09.000
So, for example, what we use at InstaDeep

11:09.000 --> 11:10.000
is open-street maps.

11:10.000 --> 11:13.000
So, we take, actually, the exact, you know,

11:13.000 --> 11:15.000
information on traffic,

11:15.000 --> 11:18.000
on the topology of the city from open-street map.

11:18.000 --> 11:21.000
And we transform the problem of,

11:21.000 --> 11:25.000
you know, going from point A to point B

11:25.000 --> 11:27.000
and selecting different, you know,

11:27.000 --> 11:32.000
passengers into a set of possible actions.

11:32.000 --> 11:34.000
And basically, you know,

11:34.000 --> 11:35.000
build it into an RL framework.

11:35.000 --> 11:37.000
That's exactly the same method

11:37.000 --> 11:38.000
that we would use, for example,

11:38.000 --> 11:40.000
on a problem like dinpacking.

11:40.000 --> 11:42.000
You have a certain volume.

11:42.000 --> 11:45.000
You have certain constants that you need to respect.

11:45.000 --> 11:48.000
And your goal is to find optimal configurations

11:48.000 --> 11:50.000
such that, you know,

11:50.000 --> 11:52.000
the total volume that you're using

11:52.000 --> 11:53.000
is the smallest possible,

11:53.000 --> 11:56.000
while respecting all the other constraints.

11:56.000 --> 11:59.000
So, basically, there is a significant work

11:59.000 --> 12:01.000
which is done on the environment.

12:01.000 --> 12:04.000
But the general principle remains the same.

12:04.000 --> 12:05.000
At the end of the day,

12:05.000 --> 12:08.000
you have, let's say,

12:08.000 --> 12:11.000
a reward function that comes out of this.

12:11.000 --> 12:13.000
Usually, on NP hard problems in logistics,

12:13.000 --> 12:15.000
you know, it's very easy to verify

12:15.000 --> 12:19.000
if a given solution is a good one or not.

12:19.000 --> 12:23.000
What is not so easy is to find what is a good solution.

12:23.000 --> 12:26.000
So, the problem of like scoring the result

12:26.000 --> 12:29.000
is usually not extremely difficult.

12:29.000 --> 12:31.000
You just need to build up the environment

12:31.000 --> 12:33.000
to make, you know, this possible

12:33.000 --> 12:36.000
and then find ways to train this escape.

12:36.000 --> 12:40.000
So, sticking with this right-sharing example

12:40.000 --> 12:43.000
is the assumption that for a given ride,

12:43.000 --> 12:46.000
you kind of start with a starting place

12:46.000 --> 12:50.000
and a destination may be determined by an initial rider

12:50.000 --> 12:53.000
and you're trying to figure out what other riders to pick up.

12:53.000 --> 12:55.000
Yes. Basically, you have to make decisions

12:55.000 --> 12:59.000
about which riders to take into the car.

12:59.000 --> 13:02.000
And, you know, the car can have two states

13:02.000 --> 13:05.000
like it could either have, you know, already people

13:05.000 --> 13:09.000
in the car and given destination where to go

13:09.000 --> 13:11.000
or it could be completely empty.

13:11.000 --> 13:15.000
So, you have to make a decision about who you want to onboard.

13:15.000 --> 13:18.000
And obviously, if you onboard a given person,

13:18.000 --> 13:21.000
a given client, then you have to take that person to destination.

13:21.000 --> 13:25.000
So, and you do this, you know, progressively, you know,

13:25.000 --> 13:28.000
given the capacity that you have left.

13:28.000 --> 13:30.000
In a situation like ride-sharing,

13:30.000 --> 13:33.000
a typical metric of, you know,

13:33.000 --> 13:36.000
that you're trying to optimize for

13:36.000 --> 13:40.000
is the total number, like the total delay

13:40.000 --> 13:43.000
for the passengers you're onboarding,

13:43.000 --> 13:45.000
meaning that, you know, for example,

13:45.000 --> 13:49.000
if an open street map tells you that you could deliver that person

13:49.000 --> 13:51.000
in five minutes,

13:51.000 --> 13:54.000
if you deliver that person in seven minutes,

13:54.000 --> 13:57.000
then it's a two-minute delay.

13:57.000 --> 14:01.000
And your goal is to, of course, optimize the total amount of delays

14:01.000 --> 14:03.000
to minimize it and have, you know,

14:03.000 --> 14:07.000
satisfied, basically, client base.

14:07.000 --> 14:09.000
How have you created a data set

14:09.000 --> 14:13.000
to use to train these models?

14:13.000 --> 14:17.000
Is it all synthetic or simulated data

14:17.000 --> 14:21.000
or is there some set of data

14:21.000 --> 14:25.000
that you're using to kind of bootstrap this process?

14:25.000 --> 14:29.000
So, initially, we started by working with, you know,

14:29.000 --> 14:31.000
New York City and Manhattan,

14:31.000 --> 14:34.000
because there is significant amount of data,

14:34.000 --> 14:36.000
which is available through the New York Taxi

14:36.000 --> 14:38.000
and Limousine Commission.

14:38.000 --> 14:41.000
So, that makes it, you know, interesting to start with.

14:41.000 --> 14:44.000
But the way the OR approach is effectively

14:44.000 --> 14:46.000
to use open street map

14:46.000 --> 14:49.000
to extract as much data as possible

14:49.000 --> 14:51.000
to model the environment of the city

14:51.000 --> 14:52.000
we're looking at,

14:52.000 --> 14:55.000
and open street map has also, you know, many cities available.

14:55.000 --> 14:58.000
It's actually, I believe, the tool that you were used

14:58.000 --> 15:01.000
when they started their operations.

15:01.000 --> 15:04.000
But when it comes to this problem,

15:04.000 --> 15:07.000
it's actually possible to train from zero data.

15:07.000 --> 15:09.000
So, it's very similar in principle

15:09.000 --> 15:12.000
to what DeepMind did with Alpha0.

15:12.000 --> 15:15.000
Zero means that there is zero data.

15:15.000 --> 15:19.000
What you're setting up is basically a full simulated environment

15:19.000 --> 15:22.000
of, you know, of the city.

15:22.000 --> 15:25.000
And you can learn by doing.

15:25.000 --> 15:29.000
So, the system will initially make random decisions

15:29.000 --> 15:32.000
and then progressively realize what is good

15:32.000 --> 15:35.000
and what is less good and learn from there.

15:35.000 --> 15:38.000
So, this is also what is interesting about this problem

15:38.000 --> 15:42.000
and also why, you know, relatively small scale start-up

15:42.000 --> 15:45.000
like in study today can tackle those problems.

15:45.000 --> 15:50.000
It's because, you know, have not having necessarily a lot of data

15:50.000 --> 15:54.000
is not a hurdle provided you can build a realistic environment.

15:54.000 --> 15:57.000
So, the data components is really about, you know,

15:57.000 --> 16:01.000
affecting your ability to build a realistic environment of the city.

16:01.000 --> 16:03.000
But if you find a way to do that,

16:03.000 --> 16:07.000
technically speaking, you actually do not need data.

16:07.000 --> 16:09.000
You've got this environment,

16:09.000 --> 16:14.000
beyond the open street maps, data.

16:14.000 --> 16:17.000
What's the kind of shape of this environment?

16:17.000 --> 16:20.000
Did you build kind of, feel like a custom simulator

16:20.000 --> 16:24.000
or is it, is it just the data?

16:24.000 --> 16:29.000
You know, and not a lot of kind of external kind of tooling

16:29.000 --> 16:32.000
or did you use an open source simulation platform

16:32.000 --> 16:34.000
that you plug the data into?

16:34.000 --> 16:37.000
What does it all look like?

16:37.000 --> 16:41.000
So, actually, we built our own, our own basically simulation

16:41.000 --> 16:44.000
and giant for this problem.

16:44.000 --> 16:46.000
And including visualization,

16:46.000 --> 16:50.000
we actually also have visualization teams at InstaDeep.

16:50.000 --> 16:54.000
So, effectively open street map is used to provide us with, you know,

16:54.000 --> 16:57.000
metrics about, you know, how much time, for example,

16:57.000 --> 17:01.000
it takes from going to point A to point B in a city.

17:01.000 --> 17:05.000
And basically different, different, like, intermediate points.

17:05.000 --> 17:09.000
For example, like, I give and write could have, like,

17:09.000 --> 17:12.000
50 intermediate points or 13 intermediate points, you know,

17:12.000 --> 17:15.000
that kind of order of range.

17:15.000 --> 17:18.000
But once you have that, effectively,

17:18.000 --> 17:21.000
everything else runs on our platform.

17:21.000 --> 17:23.000
So, we've built, you know, our own platform

17:23.000 --> 17:26.000
to tackle these type of problems.

17:26.000 --> 17:28.000
And this is, you know, one of the things

17:28.000 --> 17:30.000
that makes it exciting for us.

17:30.000 --> 17:34.000
Like, you know, it's something that we have full control on.

17:34.000 --> 17:38.000
So, we use basically, you know, Kubernetes and TensorFlow.

17:38.000 --> 17:41.000
And train on Nvidia DGX1.

17:41.000 --> 17:46.000
So, Nvidia super computers to be able to extract results

17:46.000 --> 17:48.000
and continue to learn.

17:48.000 --> 17:52.000
And one is also interesting about this type of, you know,

17:52.000 --> 17:56.000
solution is that when they are deployed in the real world,

17:56.000 --> 17:59.000
you have an ability to, you know, keep learning from the results

17:59.000 --> 18:01.000
you're getting as well.

18:01.000 --> 18:03.000
You're using DGX1s.

18:03.000 --> 18:05.000
Those aren't cheap.

18:05.000 --> 18:09.000
Because your end user need to have the DGX1

18:09.000 --> 18:13.000
to continue to fine tune a model.

18:13.000 --> 18:17.000
Or is that, you know, what you've got to build up these models

18:17.000 --> 18:21.000
and then you can push the models out for inference?

18:21.000 --> 18:24.000
So, these are, yeah, it's exactly like you said.

18:24.000 --> 18:26.000
We basically use those for training.

18:26.000 --> 18:28.000
But once a model is trained,

18:28.000 --> 18:31.000
it would simply be, you know, about inference

18:31.000 --> 18:34.000
and customer does not necessarily have to have, you know,

18:34.000 --> 18:36.000
have you weaponry as such.

18:36.000 --> 18:37.000
Exactly.

18:37.000 --> 18:38.000
Exactly.

18:38.000 --> 18:44.000
So, that kind of leads me to the question about generalization

18:44.000 --> 18:47.000
and transfer learning in a sense.

18:47.000 --> 18:51.000
You know, for example, if you train a model on,

18:51.000 --> 18:54.000
or when you train the model on New York City,

18:54.000 --> 18:58.000
can you apply that model to San Francisco?

18:58.000 --> 19:02.000
If you do need to then look at San Francisco,

19:02.000 --> 19:04.000
do you have to start from scratch?

19:04.000 --> 19:08.000
Or can you do kind of a traditional transfer learning type

19:08.000 --> 19:09.000
of fine tuning?

19:09.000 --> 19:13.000
Does that, is there an analogy of that kind of fine tuning

19:13.000 --> 19:14.000
and reinforcement learning?

19:14.000 --> 19:15.000
How does that all work?

19:15.000 --> 19:16.000
Absolutely.

19:16.000 --> 19:17.000
Absolutely.

19:17.000 --> 19:21.000
Actually, you know, transfer learning works up to an extent.

19:21.000 --> 19:25.000
So, you are much better indeed using a trained model,

19:25.000 --> 19:27.000
even though it's a different city,

19:27.000 --> 19:29.000
to get started in another,

19:29.000 --> 19:32.000
even though you're going to still require, you know,

19:32.000 --> 19:35.000
significant training dependent on how similar the topology

19:35.000 --> 19:36.000
of the city is.

19:36.000 --> 19:39.000
You know, if the topology of the city has really nothing to do,

19:39.000 --> 19:41.000
then of course, that's an issue.

19:41.000 --> 19:44.000
If, for example, to coming back to Manhattan,

19:44.000 --> 19:48.000
if you have another city which has a relatively speaking

19:48.000 --> 19:51.000
rectangular shape for its streets and avenues,

19:51.000 --> 19:53.000
then obviously, you know,

19:53.000 --> 19:56.000
what you have learned in Manhattan would be

19:56.000 --> 19:58.000
to a certain extent applicable.

19:58.000 --> 20:01.000
So, we've seen actually this type of transfer learning

20:01.000 --> 20:03.000
happening in different problems.

20:03.000 --> 20:06.000
I mean, this is the right sharing problem.

20:06.000 --> 20:08.000
When it comes to, for example, packing bins,

20:08.000 --> 20:09.000
it's the same.

20:09.000 --> 20:12.000
If you are packing bins in a certain configuration,

20:12.000 --> 20:14.000
let's say, for 30 items,

20:14.000 --> 20:19.000
what you learn can be directly redeployed to larger dimensions,

20:19.000 --> 20:22.000
like 200 items under like.

20:22.000 --> 20:24.000
Sometimes even without retraining.

20:24.000 --> 20:26.000
So, you know, you nailed it on it.

20:26.000 --> 20:30.000
It's actually, you know, there is the equivalent of transfer learning

20:30.000 --> 20:32.000
in this type of problems as well.

20:32.000 --> 20:34.000
Yeah, I guess when we think about transfer learning

20:34.000 --> 20:36.000
applied to computer vision, right?

20:36.000 --> 20:41.000
You're often training models on image net and you're,

20:41.000 --> 20:46.000
you're trying to teach the lower layers of the model,

20:46.000 --> 20:50.000
kind of primitives like textures and edges and things like that.

20:50.000 --> 20:53.000
Is that a very problem-specific thing?

20:53.000 --> 20:57.000
Or are there, you know, some equivalent of edges

20:57.000 --> 21:01.000
at the, you know, just broad logistics level

21:01.000 --> 21:05.000
such that you would get some transfer benefit going,

21:05.000 --> 21:10.000
taking a model from bin packing to a right sharing, for example.

21:10.000 --> 21:16.000
So, there wouldn't be lots of, you know, intersection between,

21:16.000 --> 21:19.000
let's say, taking a bin packing model to right sharing.

21:19.000 --> 21:23.000
But there would be between different bin packing problems

21:23.000 --> 21:26.000
or different right sharing problems.

21:26.000 --> 21:30.000
So, typically like to take the bin packing example, for example,

21:30.000 --> 21:34.000
like if you're dealing with a problem that has, you know,

21:34.000 --> 21:36.000
like multiple hundreds of items,

21:36.000 --> 21:41.000
this is actually a large combinatorially exploding problem.

21:41.000 --> 21:44.000
However, if you are already having a good performance

21:44.000 --> 21:47.000
with a smaller set of, you know, items,

21:47.000 --> 21:52.000
actually this set could be as small as 10 items,

21:52.000 --> 21:56.000
then indeed the system has figured out something about

21:56.000 --> 22:01.000
how do I fit packages together that can be redeployed

22:01.000 --> 22:02.000
on larger models.

22:02.000 --> 22:06.000
So, it's a very interesting topic, but like at InstaDeep,

22:06.000 --> 22:10.000
we've been able to take models that been trained on,

22:10.000 --> 22:15.000
let's say, 10 items and deploy them with almost no changes

22:15.000 --> 22:19.000
at all to larger sets of problems, like 50 and 100 items,

22:19.000 --> 22:22.000
and still getting, you know, a type of performance

22:22.000 --> 22:26.000
which is better than other OR type algorithms.

22:26.000 --> 22:29.000
So, indeed, you know, there is a transferability,

22:29.000 --> 22:34.000
but I would say it's transferability among certain domains.

22:34.000 --> 22:39.000
So, you know, it's things that still have some level of similarity

22:39.000 --> 22:40.000
between them.

22:40.000 --> 22:43.000
And within, for example, the domain of bin packing

22:43.000 --> 22:48.000
are there dependencies on kind of the,

22:48.000 --> 22:51.000
you clearly mentioned the number of items,

22:51.000 --> 22:55.000
but like the shape of the items or the shape of the containers,

22:55.000 --> 22:56.000
the bins?

22:56.000 --> 22:57.000
Yes, of course.

22:57.000 --> 22:58.000
Yes, of course.

22:58.000 --> 23:01.000
If you have, you know, shapes that are quite similar,

23:01.000 --> 23:05.000
you would get strong transfer learning features.

23:05.000 --> 23:09.000
For example, like if it's the same shape for the general volume,

23:09.000 --> 23:11.000
let's say it's a container,

23:11.000 --> 23:14.000
and you learn to solve the problem for 10 items

23:14.000 --> 23:18.000
and you're just adding up more items that are relatively similar

23:18.000 --> 23:20.000
to the items you already have.

23:20.000 --> 23:24.000
In certain cases, you would have very little extra training,

23:24.000 --> 23:28.000
meaning that starting with the train model on smaller items

23:28.000 --> 23:31.000
would still deliver to you an acceptable performance.

23:31.000 --> 23:33.000
And this is actually surprising.

23:33.000 --> 23:37.000
We were, when we started working on this with my team,

23:37.000 --> 23:40.000
we were not necessarily expecting this type of properties,

23:40.000 --> 23:42.000
and yet we got there.

23:42.000 --> 23:46.000
And these are some of the results we published in an article

23:46.000 --> 23:49.000
that the latest, the last no-rips,

23:49.000 --> 23:51.000
which is called rank rewards.

23:51.000 --> 23:55.000
We found ways to basically go beyond what had been done

23:55.000 --> 23:58.000
for Alpha Zero and the game of chess and go

23:58.000 --> 24:01.000
to apply this to real, to real concrete problems.

24:01.000 --> 24:03.000
And yes, we were surprised that, you know,

24:03.000 --> 24:06.000
the level of transfer, you know,

24:06.000 --> 24:09.000
transfer ability from one problem or another was actually pretty high.

24:09.000 --> 24:14.000
What can you say about the data efficiency

24:14.000 --> 24:18.000
or sample efficiency rather of RL

24:18.000 --> 24:21.000
for this particular type of problem?

24:21.000 --> 24:26.000
Do you, you know, how much, how many,

24:26.000 --> 24:28.000
how, I don't know what the,

24:28.000 --> 24:30.000
if you want to talk about in terms of time

24:30.000 --> 24:35.000
or compute cycles or, you know, runs or batches,

24:35.000 --> 24:40.000
but how complex is, you know, getting to a model

24:40.000 --> 24:44.000
that, you know, starts to perform well

24:44.000 --> 24:49.000
compared to some of the other things that we might apply RL2.

24:49.000 --> 24:52.000
So, I mean, there is definitely some complexity,

24:52.000 --> 24:55.000
but with, you know, modern equipment,

24:55.000 --> 24:59.000
you could get to results in a matter of, like,

24:59.000 --> 25:02.000
something like 24 hours or, you know, a couple of days.

25:02.000 --> 25:05.000
We're not talking about like super, super long jobs,

25:05.000 --> 25:07.000
but it's not a couple of hours either.

25:07.000 --> 25:11.000
So, to give you concrete examples on our Nvidia DGX one,

25:11.000 --> 25:14.000
we would typically train a model overnight

25:14.000 --> 25:16.000
and be satisfied with the results.

25:16.000 --> 25:19.000
So, it's not something unsurmountable, which means that, you know,

25:19.000 --> 25:22.000
if you're not Google or if you're not like a very large player,

25:22.000 --> 25:25.000
you would not be able to compete.

25:25.000 --> 25:30.000
Obviously, it's strongly dependent on the quality of your algorithm.

25:30.000 --> 25:33.000
We spend a lot of time, for example, trying to find ways to,

25:33.000 --> 25:38.000
we inject the concept of competitiveness,

25:38.000 --> 25:41.000
even though these are like one player games,

25:41.000 --> 25:43.000
you're just trying to solve a problem.

25:43.000 --> 25:46.000
You know, one of the secret, the secret source, for example,

25:46.000 --> 25:49.000
in Alpha Zero, is that you are playing against yourself,

25:49.000 --> 25:51.000
and so you're constantly, you know,

25:51.000 --> 25:54.000
it's a two-player game where you're constantly challenging yourself.

25:54.000 --> 25:58.000
So, when you're really bad, like you start with random parameters,

25:58.000 --> 26:01.000
it doesn't matter because the other person in front of you,

26:01.000 --> 26:04.000
which is, you know, a replica of yourself, is equally bad.

26:04.000 --> 26:06.000
What matters is to get slightly better.

26:06.000 --> 26:09.000
And it's the same thing when you get to, like, superhuman performance,

26:09.000 --> 26:12.000
you can still improve, because what matters, in a sense,

26:12.000 --> 26:15.000
is the relative improvement than the absolute improvement.

26:15.000 --> 26:18.000
So, we found ways to re-inject this type of principles

26:18.000 --> 26:21.000
of, you know, adversarial competitiveness into this problem,

26:21.000 --> 26:24.000
and you can get good results, as I said, in, you know,

26:24.000 --> 26:27.000
a recent, a decent amount of time.

26:27.000 --> 26:30.000
Interesting. Is that part of the work that you've published

26:30.000 --> 26:32.000
at the recent NURPS?

26:32.000 --> 26:35.000
Yes, absolutely. So, we have a paper called Rankewarts,

26:35.000 --> 26:39.000
which was accepted at the DPRL workshop at the recent NURPS,

26:39.000 --> 26:44.000
and it describes exactly the type of results and breakthroughs

26:44.000 --> 26:46.000
we are describing.

26:46.000 --> 26:51.000
We talked about how, historically, these types of OR problems

26:51.000 --> 26:54.000
are solved using heuristic methods.

26:54.000 --> 26:59.000
One of the trends that I see in deep learning is,

26:59.000 --> 27:03.000
kind of, there's a pendulum where we kind of started

27:03.000 --> 27:06.000
at these very model-based approaches to things,

27:06.000 --> 27:10.000
and we swung to, like, pure statistical and learned approaches.

27:10.000 --> 27:12.000
And now, there's a lot of effort happening

27:12.000 --> 27:16.000
to try to fuse model-based and heuristics-based approaches

27:16.000 --> 27:18.000
with statistical approaches.

27:18.000 --> 27:21.000
Is that something that you've looked at for these types of problems?

27:21.000 --> 27:24.000
Yes, absolutely. The general idea that you're going to inject

27:24.000 --> 27:28.000
learning ability into a process is a very powerful one.

27:28.000 --> 27:32.000
But, yes, in some cases, it actually can be pretty efficient

27:32.000 --> 27:34.000
to take and existing heuristic,

27:34.000 --> 27:37.000
but add learning ability into the mix.

27:37.000 --> 27:39.000
And it comes to a very natural idea.

27:39.000 --> 27:41.000
If you are a large company, for example,

27:41.000 --> 27:44.000
you have lots of, like, you have a fleet of trucks,

27:44.000 --> 27:46.000
you have lots of packages to deliver tomorrow,

27:46.000 --> 27:49.000
and some of these companies would do overnight runs,

27:49.000 --> 27:52.000
take the result, and use it to, you know,

27:52.000 --> 27:54.000
process their operations for the day.

27:54.000 --> 27:56.000
And the next day, they're doing the same.

27:56.000 --> 28:00.000
It's kind of a waste not to learn anything new.

28:00.000 --> 28:02.000
So, adding learning ability into the mix,

28:02.000 --> 28:04.000
and especially constant learning ability,

28:04.000 --> 28:08.000
as you get, you know, more experience through your operations,

28:08.000 --> 28:10.000
is a natural idea.

28:10.000 --> 28:14.000
So, there are two schools, one which is indeed saying,

28:14.000 --> 28:18.000
you could, you know, you could use existing heuristics

28:18.000 --> 28:22.000
and build learning on top, which, you know, which makes sense.

28:22.000 --> 28:25.000
But, increasingly, and especially with, you know,

28:25.000 --> 28:30.000
compute, compute power for machine learning,

28:30.000 --> 28:33.000
doubling every three months and a half or so.

28:33.000 --> 28:36.000
And this is a study by OpenAI.

28:36.000 --> 28:40.000
In certain cases, it becomes possible to learn from first principles,

28:40.000 --> 28:43.000
meaning that you do not necessarily need the heuristics.

28:43.000 --> 28:46.000
So, the honest answer is very problem-dependent.

28:46.000 --> 28:52.000
Some problems are still too big to be learned completely and to end.

28:52.000 --> 28:57.000
But some of them can be actually solved using this type of techniques.

28:57.000 --> 29:01.000
So, it sounds like your general perspective

29:01.000 --> 29:08.000
is that where you can use pure learning-based solution that's preferable?

29:08.000 --> 29:13.000
Yes, absolutely, because that removes significant sources of bias.

29:13.000 --> 29:18.000
And, you know, by learning through first principles and direct experience,

29:18.000 --> 29:20.000
you can get a lot done.

29:20.000 --> 29:23.000
There is an additional advantage, you know,

29:23.000 --> 29:27.000
sometimes a heuristics-based model would require specific constraints to be in place

29:27.000 --> 29:30.000
and might not tolerate certain number of constraints.

29:30.000 --> 29:34.000
When you're taking, like, a pure learning approach from first principles,

29:34.000 --> 29:37.000
you can actually set all the constraints,

29:37.000 --> 29:40.000
all the real world constraints that you care about.

29:40.000 --> 29:45.000
And this is also an advantage, means, like, if these are the real world constraints,

29:45.000 --> 29:50.000
and actually there are people in the real world running those operations with those constraints,

29:50.000 --> 29:52.000
you know, solutions exist.

29:52.000 --> 29:58.000
So, as a consequence, a model learning end to end could get to that point as well.

29:58.000 --> 30:07.000
You mentioned that one of the considerations is the some inherent bias in the heuristics-based models.

30:07.000 --> 30:10.000
You know, this might come from constraints or other factors,

30:10.000 --> 30:13.000
and that a learning-based approach overcomes those.

30:13.000 --> 30:16.000
Have you seen the flip side as well,

30:16.000 --> 30:22.000
where the learning you've seen bias introduced by the learning-based approach

30:22.000 --> 30:28.000
that disadvantages it relative to what an organization might want to do?

30:28.000 --> 30:32.000
So, I mean, in general, like, if the problem is well formulated,

30:32.000 --> 30:37.000
and, you know, the system will learn to crack that problem.

30:37.000 --> 30:43.000
So, yes, in cases where you see the system not doing what you expect,

30:43.000 --> 30:50.000
it really comes down to, there might have been some constraints that you forgot to include into the mix.

30:50.000 --> 30:57.000
But if you really, you know, give to the system all the constraints that he should care about,

30:57.000 --> 31:03.000
you know, the system ultimately will explore and learn to crack that problem with the tools you have.

31:03.000 --> 31:08.000
So, it's more a question of defining the right set of objectives,

31:08.000 --> 31:12.000
and what's the space of, you know, possible behavior?

31:12.000 --> 31:17.000
If you do this well, there is no reason why a system which has sufficient capacity

31:17.000 --> 31:21.000
would introduce significant biases.

31:21.000 --> 31:24.000
Just to give you an example in the game of chess as well,

31:24.000 --> 31:29.000
you know, there have been some recent publications, I believe the title of the book is Game Changer,

31:29.000 --> 31:34.000
where, you know, chess experts have looked at the behavior of Alpha Zero,

31:34.000 --> 31:37.000
and, you know, some of them said, you know, this is very interesting,

31:37.000 --> 31:40.000
because, you know, we believe this is very close to the truth,

31:40.000 --> 31:43.000
meaning that the system has explored so many configurations,

31:43.000 --> 31:46.000
and developed an intuition which is really based on facts,

31:46.000 --> 31:50.000
that, you know, some people would consider this, you know,

31:50.000 --> 31:56.000
including actually Kasparov as, you know, the best manifestation of truth in chess

31:56.000 --> 31:59.000
that is possible to achieve.

31:59.000 --> 32:03.000
I'm also wondering about, I imagine when you're talking to enterprises

32:03.000 --> 32:06.000
that are considering these types of approaches,

32:06.000 --> 32:12.000
there are concerns like explainability and even robustness.

32:12.000 --> 32:16.000
You know, when you've got a set of rules, the actions that those rules tell you

32:16.000 --> 32:21.000
to take are relatively predictable, whereas with a deep learning solution,

32:21.000 --> 32:28.000
I'm imagining, you know, there can be instabilities or your decision boundaries are very complex,

32:28.000 --> 32:35.000
and you can have the system-recommend actions that are, you know, maybe, you know,

32:35.000 --> 32:39.000
not actions that you would want to take in the real world,

32:39.000 --> 32:44.000
or these behaviors that you see coming out of systems like these?

32:44.000 --> 32:49.000
I mean, it's a fair point. I mean, explainability will always be, you know,

32:49.000 --> 32:52.000
a concern at some level when using deep learning,

32:52.000 --> 32:57.000
and even though there is a promising research going on in terms of, like,

32:57.000 --> 33:00.000
getting better ways to understand what the system is doing,

33:00.000 --> 33:04.000
and how we, how it can explain to humans what it is doing.

33:04.000 --> 33:10.000
But typically, we don't operate into, with customers for whom explainability

33:10.000 --> 33:15.000
would be extremely strong. So coming back to the examples we discussed,

33:15.000 --> 33:20.000
whether it's right-sharing or been-packing, ultimately, the customer will care about,

33:20.000 --> 33:23.000
you know, in the case of been-packing, how many containers he's using,

33:23.000 --> 33:27.000
provided all the constraints about how to pack, have been respected.

33:27.000 --> 33:31.000
Same thing with right-sharing, you know, the client is going to care about, you know,

33:31.000 --> 33:36.000
what's the, you know, average, for example, a number of, number of miles driven

33:36.000 --> 33:40.000
to deliver one customer and the number of customers delivered.

33:40.000 --> 33:47.000
In this case, it's okay if a few decisions are suboptimal, provided the total,

33:47.000 --> 33:52.000
you know, the average decision is better. So in a sense, you're not getting extremely

33:52.000 --> 33:57.000
penalized for taking a decision which locally would look suboptimal.

33:57.000 --> 34:01.000
So that is typically, you know, the kind of, you know, customers re-engaging

34:01.000 --> 34:06.000
and it's a thing studied for logistics, but I mean, to your point, absolutely.

34:06.000 --> 34:12.000
I mean, if you all have certain situations for which where the negative impact

34:12.000 --> 34:16.000
of a bad decision can be massive, I don't know, for example, you know,

34:16.000 --> 34:19.000
like in, you know, drug manufacturing or things like that,

34:19.000 --> 34:24.000
obviously, explainability could be of interest, but we haven't seen this,

34:24.000 --> 34:29.000
you know, with the type of problems we work on, because really about logistics

34:29.000 --> 34:33.000
and it's about optimizing well-known constraints and, you know,

34:33.000 --> 34:36.000
there's not something which, you know, like if you deliver, for example,

34:36.000 --> 34:42.000
a person and it has taken too long for that particular person, you know,

34:42.000 --> 34:46.000
it is a concern, it can be improved, but it's not a massive concern

34:46.000 --> 34:48.000
that it would stop all your operation.

34:48.000 --> 34:53.000
True, wouldn't stop all your operations, but as organizations mature

34:53.000 --> 34:59.000
in their use of these types of technologies, they may want to, you know,

34:59.000 --> 35:05.000
their perception of what's the right reward function evolves and matures,

35:05.000 --> 35:10.000
and at some point, you know, maybe they wouldn't want, you know,

35:10.000 --> 35:14.000
a customer to randomly sit in the car and be driven across town only,

35:14.000 --> 35:18.000
be driven back because that's what the machine said.

35:18.000 --> 35:23.000
Do you have a path to addressing that in this kind of approach,

35:23.000 --> 35:27.000
or are they totally antithetical to one another?

35:27.000 --> 35:33.000
You can, you can, you know, control this type of, like, spurious behavior.

35:33.000 --> 35:36.000
For example, like typically, like in the example you take,

35:36.000 --> 35:41.000
if somebody, if a car would take someone and really drive around town in crazy ways

35:41.000 --> 35:47.000
before delivering that person, you could actually put a very significant constraint

35:47.000 --> 35:51.000
on like in a penalty if you want on the outcome.

35:51.000 --> 35:56.000
So instead of having just, for example, like the total delay

35:56.000 --> 35:59.000
that this person experienced, you could say that, you know,

35:59.000 --> 36:02.000
beyond a certain point, which is statistically driven,

36:02.000 --> 36:05.000
and you can collect the statistics for a given city.

36:05.000 --> 36:08.000
Hey, if you go beyond that, there's really a significant problem.

36:08.000 --> 36:12.000
And as a consequence, let's say, you know, the penalty on the reward system

36:12.000 --> 36:14.000
would be very high.

36:14.000 --> 36:20.000
So that's one of the ways you can, you can actually remove spurious cases from your system.

36:20.000 --> 36:25.000
And when you're engaging with someone around these types of problems,

36:25.000 --> 36:31.000
do you find that there is very unique and specific problem definition

36:31.000 --> 36:36.000
that needs to happen and kind of building out these reward functions

36:36.000 --> 36:40.000
and penalties and things like that, or, you know,

36:40.000 --> 36:44.000
the most people just want a been packing solution that,

36:44.000 --> 36:48.000
for which you've already really defined the problem.

36:48.000 --> 36:53.000
So typically when we work with large corporates,

36:53.000 --> 36:58.000
those companies have already processes in place, including, you know,

36:58.000 --> 37:04.000
and they have a good sense of what type of constraints they are experiencing.

37:04.000 --> 37:08.000
So one of the cool things at INSADEEP is that we will adapt to their problem.

37:08.000 --> 37:11.000
Our process is pretty general.

37:11.000 --> 37:14.000
So we basically put ourselves in the customer's shoes.

37:14.000 --> 37:18.000
So we'll take the problem with, like, look at it the way they look at it.

37:18.000 --> 37:20.000
They tell us the constraints they have.

37:20.000 --> 37:23.000
And yes, those constraints could change from one customer to another

37:23.000 --> 37:27.000
because maybe they're not doing exactly the same type of operations

37:27.000 --> 37:30.000
or maybe because they have different priorities and preferences.

37:30.000 --> 37:34.000
So we'll incorporate their constraints and work from there.

37:34.000 --> 37:38.000
And as I said, you know, what's pretty cool in the way we look at things

37:38.000 --> 37:43.000
and these general systems is that they can actually learn from, you know,

37:43.000 --> 37:45.000
the constraints you give them.

37:45.000 --> 37:48.000
So you're not a prisoner of given constraints.

37:48.000 --> 37:54.000
So it's not a problem in practice to be looking at different customers

37:54.000 --> 37:58.000
and different constraints because the principles that are behind it

37:58.000 --> 38:01.000
and this is the thing which I'm most excited about,

38:01.000 --> 38:05.000
are very general. So no matter how you look, you know,

38:05.000 --> 38:09.000
what are your constraints and some of those could be real physical constraints

38:09.000 --> 38:11.000
and others could be preferences,

38:11.000 --> 38:14.000
there is always a way to solve your problem.

38:14.000 --> 38:18.000
Provided that it is actually a real world problem that has solutions

38:18.000 --> 38:21.000
and it's typically the case and you want to improve your solutions.

38:21.000 --> 38:24.000
And so we spent most of the time talking about reinforcement learning.

38:24.000 --> 38:28.000
Are there other approaches that you use for these types of problems

38:28.000 --> 38:33.000
or is that the primary area you're working with?

38:33.000 --> 38:37.000
So I mean, we also use like, you know, classical algorithms,

38:37.000 --> 38:40.000
but these are more for benchmarking.

38:40.000 --> 38:43.000
You know, I think the people who believe that, you know,

38:43.000 --> 38:46.000
the type of smart systems are there to stay.

38:46.000 --> 38:50.000
But when we say reinforcement learning just to be very clear

38:50.000 --> 38:53.000
and I know the audience is pretty technical.

38:53.000 --> 38:59.000
What we believe the most at in in in in study is what we believe in the most is

38:59.000 --> 39:02.000
the concept of having, you know,

39:02.000 --> 39:06.000
a neural net that's going to work with planning type algorithms.

39:06.000 --> 39:10.000
For each we look at a technology such as Monte Carlo research.

39:10.000 --> 39:14.000
It is, you know, in the secret source of, you know,

39:14.000 --> 39:17.000
what makes, for example, alpha zero or so good.

39:17.000 --> 39:20.000
So, you know, I have a great analogy for this.

39:20.000 --> 39:23.000
It's this idea and I presented this at GTC,

39:23.000 --> 39:27.000
which is this idea of thinking, thinking fast and thinking slow.

39:27.000 --> 39:32.000
So this is the same title as Daniel Kaneman's book, The Noble Price.

39:32.000 --> 39:35.000
It's this idea that with planning, you know,

39:35.000 --> 39:38.000
basically Monte Carlo research into this type of algorithms,

39:38.000 --> 39:41.000
you're going to be able to think slow,

39:41.000 --> 39:44.000
which means you're going to consider many parts

39:44.000 --> 39:47.000
and incorporate those parts of flat planning into your decision.

39:47.000 --> 39:50.000
And the thinking fast part is the neural net.

39:50.000 --> 39:53.000
So to be very clear, it's not just a rel in the sense,

39:53.000 --> 39:58.000
hey, I'm experiencing a reward and I'm kind of looking blindly for that reward.

39:58.000 --> 40:02.000
I'm actually trying to find, you know, interesting paths.

40:02.000 --> 40:06.000
So another way to look at it, which is an analogy I really like,

40:06.000 --> 40:10.000
is, you know, in deep RL, you're like in the dark in a room

40:10.000 --> 40:12.000
and you're trying to find the switch.

40:12.000 --> 40:15.000
And that is the sparse reward, for example, that you're looking at.

40:15.000 --> 40:18.000
But you're like kind of like looking blindly and trying to find the switch.

40:18.000 --> 40:23.000
You know, if you add planning and CTS type approaches,

40:23.000 --> 40:25.000
it's like you have a search light.

40:25.000 --> 40:26.000
It's not a perfect search light.

40:26.000 --> 40:28.000
So it's only covers a little bit of area.

40:28.000 --> 40:30.000
But at least you're not completely blind.

40:30.000 --> 40:32.000
And this will help you be more efficient, much faster.

40:32.000 --> 40:35.000
So these are the two tech that are used.

40:35.000 --> 40:37.000
And it makes a lot of sense.

40:37.000 --> 40:42.000
And in a funny way, it's actually interesting that Daniel Kaneman's book,

40:42.000 --> 40:45.000
by the way, which is a great book, talks about it

40:45.000 --> 40:48.000
from the angle of, you know, human neuroscience

40:48.000 --> 40:52.000
and human, you know, behavioral patterns.

40:52.000 --> 40:56.000
But there are some equivalents in the machine learning and AI world.

40:56.000 --> 40:58.000
So let me ask you this.

40:58.000 --> 41:03.000
We've spent quite a bit of time talking about this topic

41:03.000 --> 41:06.000
and what you presented at GTC.

41:06.000 --> 41:11.000
What were the key takeaways that you were hoping to leave your audience there with?

41:11.000 --> 41:15.000
So, yeah, for me, the key takeaway is that, you know,

41:15.000 --> 41:19.000
machine learning systems are progressively, you know,

41:19.000 --> 41:22.000
going to have an impact when it comes to making decisions

41:22.000 --> 41:23.000
in the real world.

41:23.000 --> 41:27.000
And, you know, while we have mostly seen, you know,

41:27.000 --> 41:29.000
those in the context of games,

41:29.000 --> 41:32.000
and those games have, you know, started like very simple games

41:32.000 --> 41:33.000
at Aristotle.

41:33.000 --> 41:35.000
And now it's more like Starcraft complex games.

41:35.000 --> 41:38.000
I think we are about to see those reads

41:38.000 --> 41:42.000
I think we are about to see those really spread out

41:42.000 --> 41:44.000
and make an impact in the real world.

41:44.000 --> 41:46.000
And this is due to, you know,

41:46.000 --> 41:49.000
a few key properties of these systems.

41:49.000 --> 41:52.000
First, these systems can learn end to end.

41:52.000 --> 41:56.000
So you can, you know, in certain cases go from pixels to behavior

41:56.000 --> 41:59.000
or from raw data to behavior, things like that.

41:59.000 --> 42:02.000
But these systems get better with scale.

42:02.000 --> 42:06.000
And we have reached a point in terms of maturity of, you know, hardware

42:06.000 --> 42:11.000
and also like, you know, algorithms that makes, you know,

42:11.000 --> 42:15.000
you know, deploying these systems at scale a real possibility.

42:15.000 --> 42:19.000
So I would, you know, to the users, you know,

42:19.000 --> 42:22.000
the listeners of the program, I would say that, you know,

42:22.000 --> 42:25.000
expect a lot of action on these in the real world

42:25.000 --> 42:27.000
and not in games anymore.

42:27.000 --> 42:32.000
And to a certain extent, if you are, you know, having a decision-making

42:32.000 --> 42:37.000
system that uses a compute and doesn't have learnability

42:37.000 --> 42:42.000
deeply embedded in it, that means that probably you will have to change,

42:42.000 --> 42:46.000
you know, the way you are operating to include, you know,

42:46.000 --> 42:49.000
AI first approaches and, you know, learnability.

42:49.000 --> 42:53.000
And that means that, you know, lots, lots of industries

42:53.000 --> 42:56.000
and particular logistics, last mile delivery,

42:56.000 --> 42:59.000
are going to be deeply impacted by those.

42:59.000 --> 43:02.000
Also, well, Karim, thank you so much for taking the time to chat with us.

43:02.000 --> 43:04.000
This is really interesting stuff.

43:04.000 --> 43:05.000
Thanks a lot.

43:05.000 --> 43:06.000
It was a pleasure.

43:06.000 --> 43:09.000
And, you know, thanks a lot for having me.

43:13.000 --> 43:14.000
All right, everyone.

43:14.000 --> 43:16.000
That's our show for today.

43:16.000 --> 43:18.000
For more information about this and every show,

43:18.000 --> 43:21.000
visit TwomoAI.com.

43:21.000 --> 43:24.000
Remember, just one week left to register for TwomoCon,

43:24.000 --> 43:25.000
AI platforms.

43:25.000 --> 43:28.000
So head over to TwomoCon.com now.

43:28.000 --> 43:32.000
Thanks so much for listening and catch you next time.

