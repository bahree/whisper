Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we're featuring a series of conversations from the Nips conference
in Long Beach, California.
This was my first time at Nips and I had a great time there.
I attended a bunch of talks and of course learned a ton.
I organized an impromptu round table on building AI products and I met a bunch of wonderful
people, including some former Twimble Talk guests.
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should
take a second right now to subscribe to at twimblei.com slash newsletter.
This week through the end of the year, we're running a special listener appreciation contest
to celebrate hitting 1 million listens on the podcast and to thank you all for being
so awesome.
Tweet to us using the hashtag Twimble1Mill to enter.
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and
other mystery prizes.
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill
for the full rundown.
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship
of this podcast and our Nips series.
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster
sessions, their big news this time was the first public viewing of the Intel Nirvana
neural network processor or NNP.
The goal of the NNP architecture is to provide the flexibility needed to support deep learning
primitives while making the core hardware components as efficient as possible, giving
neural network designers powerful tools for solving larger and more difficult problems
while minimizing data movement and maximizing data reuse.
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel
Nirvana.com.
In this episode, I speak with Yael Niv, professor of neuroscience and psychology at Princeton
University.
Yael joined me after her invited talk on learning state representations.
In this interview, Yael and I explore the relationship between neuroscience and machine
learning.
In particular, we discuss the importance of state representations in human learning,
some of her experimental results in this area, and how a better understanding of representation
learning can lead to insights into machine learning problems such as reinforcement and
transfer learning.
Did I mention this was a nerd alert show?
I really enjoyed this interview and I know you will too.
Be sure to send over any thoughts or feedback via the show notes page.
And now on to the show.
All right, everyone, we are here in Long Beach at the NIPPS conference and I've got the
pleasure to be seated with Yael Niv.
Yael is a professor of neuroscience and psychology at Princeton University and she delivered
a talk this morning on learning state representations.
And I'm really excited to have her here to talk to us about what she's working on.
Yael, welcome to this weekend machine learning and AI.
Thank you.
Thanks for having me.
Absolutely.
Absolutely.
So you are a neuroscientist, a neuropsychologist.
You're here presenting on machine learning.
You talked about reinforcement learning.
How did all of this come about?
How did you end up at the intersection of these two fields?
So the funny thing is I didn't start as a neuroscientist.
Well, I started as a computational neuroscientist, more interested in AI and in psychology, but
not an experimentalist like I am today, I started as a theoretician.
So my PhD was in reinforcement learning theory.
My main advisor was in computer science.
My secondary advisor was in psychology.
She was, I asked her to be an advisor so that she would ground me in real data and I
wouldn't be making models of things that I've made up rather than the real world.
And my PhD was also completely theoretical.
I was modeling animal behavior with reinforcement learning, submitting my papers to nips.
That's getting rejected by nips.
I got a best paper award finally when I paper was finally accepted at nips.
So I was actually really excited now.
I have drifted away.
I've drifted more into psychology, more into neuroscience, more into doing experiments.
I still do modeling, but it really is now half half with experiments.
And so I haven't been coming to nips for the last few years and I was really excited
to be invited to give a talk here because it felt like closing a circle from all those
papers being rejected now.
Now giving an invited talk.
So I immediately said yes, very nice, very nice.
What do you consider to be your home conference now?
My whole conference now is reinforcement learning and decision making RLDM.
I helped found that conference.
And we have it every two years.
There was one this past year, so the next one is in 2019.
And it's basically a very interdisciplinary conference that tries to bring machine learning,
AI, psychology, etiology, economics, anybody who's interested in reinforcement learning
and decision making over time.
And it's a growing conference last time.
I think we had about 600 people.
Oh wow.
Wow.
So that's my home base.
Nice.
Nice.
And one of the things that I've observed here and this is the first time I've ever been
at nips is that there seems to be of several kind of themes that have emerged for me.
One of the strong themes is kind of integrative approaches and interdisciplinary approaches.
Has nips always been like that or are we seeing more of that now than in the past?
I think nips has always aspired to that.
It waxes and wanes, so people have talked about, you know, the n in nips, is it sometimes
more ips?
Okay.
And whereas the neuro, there have been, you know, worse years and better years, I used
to come regularly.
So I remember this.
And there are also kind of fads and things that become more popular and less popular.
I remember when I was a master student, someone coming back from nips and saying that it should
have been called support vector machine conference that year because it was all support vector
machines.
Right.
Now almost nobody talks about SVM and everybody talks about deep learning.
So it kind of changes focus, but there's always been, I think, a sincere attempt to keep
the neuroscience in the mix.
And it's not super easy because I think it's very clear that neuroscience has a lot to
learn from or not necessarily to learn, but to take from machine learning.
So we use machine learning algorithms to analyze our data to think of computational processes
in the brain.
Really, I feel like when I come to nips, I'm coming is like a shopper.
I want to see what's on the shelf now that I can use in my research.
I don't think it's necessarily that way, the other way around.
I think AI in particular has been inspired by neuroscience, but mostly, you know, takes
quick inspiration and runs, runs off to somewhere else, and that's fine because their goal
is different.
My goal is to understand the brain.
The goal of AI is completely different.
It's almost aspirational inspiration.
Yeah.
I think basically AI needs neuroscience less that neuroscience needs machine learning.
Interesting.
So it's always been kind of a tough thing to keep the mix together.
But in your talk, you, the focus of your talk was that mix, right?
I tried.
I tried my best.
It's not the typical talk that I give because these days, I don't usually talk to this type
of audience.
So I thought, hard, you know, what do I have to sell to AI?
What do I want AI to buy?
And so I was posing this challenge of, you know, real, well, I don't know, call it
real with a capital R, but intelligence that is more like human intelligence.
So with, you know, playing chess or playing go at an expert level, that's a huge achievement
because these are really hard tasks, but they're really hard for humans too.
You can't just become a go expert tomorrow.
But there are other things that we learn super easily.
And those things are very hard for computers as well for AI, you know, maybe, maybe in
some way as hard as playing go.
And I think that's, that's a real challenge because those are, because the amount of flexibility
that the human brain has, the extent to which we learn super quickly, and we can toggle
tasks, do one task, and then a minute later do another and then return back and kind
of toggle the representations of policies that all the machinery that's needed for these
tasks is really, really impressive.
And that's what I have not seen in any AI yet.
Right.
I thought that was a really interesting characterization of kind of the challenge of AI, or what you
believe the challenge of AI should be.
AI challenge.
AI challenge.
One of them.
AI should be, you know, as opposed to kind of these, you know, grand challenges like
AlphaGo or like the game of Go, you propose many simpler problems solved with much less
data as a challenge.
Is that the way did I characterize that right?
I don't know if the problems are simpler.
OK.
So I gave the example in my talk of crossing the street and it seems like a simple problem
because we already think about it as a simple situation, but really the stimuli, the auditory,
visual world around you when you're doing a simple task of crossing the street is very
complex.
And you need, you know, sophisticated visual machinery in our brain to parse out the
objects you need sophisticated auditory machinery, et cetera.
But more than that, you need to know, even if I've parsed everything, most of it is still
irrelevant for the task.
I don't even need to parse it.
I don't need to parse what are the stores on the other side of the street.
I don't need to parse even the colors of the cars because that's irrelevant for the task.
So whether my visual system does that or not is one question, but my learning system
should definitely ignore those aspects, and that's really hard because there are potentially
infinite combinations of things that I might need to ignore or pay attention to.
There's everything that I see and then there's everything in my past because it might be
that the time of day matter, unobservable things, not only in my past, the time of day
matters, the city in matters, what I did yesterday matters, maybe not for crossing the street,
but if I'm driving a car and navigating, like what I know about traffic from past learning.
So potentially, infinite dimensions of the environment and have to narrow them down
to like 345.
And one of those examples of the kind of the state that goes into figuring that out was
Washington DC versus New York?
Yeah, yeah.
The idea is that, you know, I could also say, you know, New York versus London, long
for each versus whatever, the context really affects, let me take one step back.
So on the one hand, we want to generalize broadly and that's why it's really important
to ignore some things and kind of represent and learn about only others because if I'm
really, you know, if my brain is so sophisticated that can take into account everything in my
visual scene, that would seem to be optimal, but it's actually super suboptimal because
it means that when I learned something new, I don't know what to generalize it to.
Is it, is this true only in this intersection only when a red car is coming at me only
when I'm in front of this shop and depending on the task, let's say crossing the street,
what I've learned is probably much more general than that.
I might learn that in Princeton, the minute you step down the curve, all the cars top
and let you go no matter what, that's general to all of Princeton, not only to that place,
but only to Princeton, right?
I wouldn't want to learn that about Long Beach.
So it's all about setting the boundaries of generalization and what I've realized from
this work is that really when you think about real world experience, you never, ever see
two situations that are exactly the same.
Even if you're at that same intersection, it won't be exactly the same crossing the
street tomorrow.
So the fact that we can reuse any past experience, the fact that we can learn at all means that
we have to generalize.
And so the basic thing that I think about all the time is how do we determine the boundaries
of the generalization?
So I've come from reinforcement learning where we think a lot about how do we learn values
and policies.
But I'm thinking that problem is pretty much solved.
I mean, we have a lot of algorithms for that.
We know how it's done in the brain.
The question is on how do I learn values?
The question to what do I generalize this value?
What's the value of what?
What state, what situation?
And I want to go back to you said that my challenge was to solve or the challenge that
my problem was less data.
To solve simple problems with less data.
Let me try to phrase that a little bit differently.
So there's this old story, I don't know if it's a myth or not, that I heard about nips,
that basically, well, it happened at nips, it's not about nips.
So what I heard is that in the 80s, kind of like in the first heyday of neural networks,
now there's second, someone published a nips paper that showed how you can use a
neural network to parallel park a truck.
And you know, this is in the 80s, think of, you know, computing power in those days and
stuff.
This is a really hard challenge to parallel park a truck.
And after all the hurrah and happiness, apparently, or as the story goes, the next year,
someone else published a paper in nips showing that one neuron, so a perceptron, one simple
computing unit, can parallel park a truck.
If you give it the input of the obstacles in polar coordinates rather than Cartesian coordinates.
So what I took from that, I heard this when I was a master student, is basically, if you
ask a question just right, it's really easy to answer.
And that's basically what I'm saying, the decision making could be kind of pair down
to a yes, no question that a perceptron can answer.
But that pairing down is the hard thing, like giving the input just right.
And so that's kind of my whole enterprise, my whole research enterprise is how does the
brain learn how to ask the questions, ask complex questions so that they're made simple.
And you know, how can we use that for AI to take all these complex problems, real world
navigation, et cetera, and make them into easy problems that a perceptron can answer.
Interesting.
Is there evidence in the brain that that process looks like a filtering process, or is
it more like a blindness to certain irrelevant variables, or is it the way things are classified?
It seems like there are potentially a number of ways that you can not consider variables.
Yeah.
So, the evidence that we've seen so far in the brain that I actually didn't talk about
today, and some glad that you asked about this, is that attention processes are strongly
involved in sub-selecting.
So first of all, anatomically, I should say that the areas in the brain that we know are
involved in this kind of reinforcement learning and decision making, get input, the small
area in the middle of the brain, it's called the stratum.
And the stratum gets input from the whole cortex.
So all the brain all around, the sensory, motor, high level, emotional, everything, everything
funnels in.
But it funnels in with a huge dimensionality reduction, so about 1 to 10,000.
So 10,000 inputs into one neuron on average.
So from the start, we know that there has to be huge dimensionality reduction.
But more than that, I'm talking here about dimensionality reduction that's not structural,
that's kind of modulated task specific for one task.
I want to know the colors of the cars for another task I don't.
So the input has to really be modulated on the fly according to our goals.
And what we've seen is that areas in the brain that are involved in attention, in switching
attention, selecting what to attend to are involved in this process.
And it's really interesting for me to think about attention in that way, because we often
think about selective attention or limited attention as a bottleneck or limitation.
We can't attend to everything, so unfortunately, we have to ignore most of the scene.
And I'm thinking about it in a much more normative way, that it's optimal to not attend
to everything.
That's what allows us to function.
It's what allows us to learn.
Right.
So if you attend just right, and you're solving a problem, rather than creating a problem.
So one of the challenges is to understand how do we learn what to attend to.
And we're working on that.
It's not easy.
We have tasks where we can measure people's attention.
We have a multi-dimensional scenario.
We tell people that only one dimension matters.
So these dimensions could be color, shape, texture of different stimuli that could be other
dimensions.
And we tell them that only one determines reward.
And we're trying to see how they learn from trial and error, what to attend to.
And in essence, what we're trying to do is devise a figure out, reverse engineer the
algorithm, the computational algorithm, by which feedback for our actions affects our
representation, affect what we attend to.
And we're kind of in the midst of that.
It's a it's a call order.
We've actually talked to, it turns out that there aren't machine learning algorithms for
modeling those kinds of data.
Hmm.
Now here attention come up a lot in the context of like LSTM networks.
Is it just kind of a word overlap, but not the same kind of mechanism or idea, or are
there some parallels there?
I'm not well enough first in these models to know how direct the parallels are.
I can say that from a psychology point of view, a neuroscience point of view, attention
is almost kind of a dirty word, because we don't know exactly what it is.
Some people hate that word.
A selective filter is really kind of, you know, operationalization of this idea.
I think there's been very little work, both in neuroscience.
And well, I don't know it recently in machine learning, so truth I haven't really followed
it.
But in neuroscience, there's been very little work on how attention is learned from experience
rather than from instruction.
And all the work on attention is you tell the subject what to attend to, and then you figure
out how is the brain doing that, how are they're focusing, how much are they sensitive
to distractors, etc.
And my question is completely different.
And the place where I said that there aren't any algorithms for is to try to, so what
we want is to predict attention, and we want to test our models.
We want to evaluate how well our models are doing.
So with choice data, which a lot of our experiments, we have subjects choosing actions.
There's a whole wide range of models for modeling choices and comparing between models and
saying, you know, this model is better than that model in predicting choice.
What is missing for me is models that predict attention and a way to compare those.
And the difference here, I mean, this is a little bit going into the weeds.
But if you think of attention as a quantity that sums up to one, if I attend more to one
thing, I can't attend more to everything else, this is a quantity that lives on the simplex.
And it's a whole different statistics and geometry and comparison world on the simplex.
And people haven't really worked on that because of this constraint that everything adds
up to one.
It's much, it's, it's, you need a different math.
Okay.
We've found a little bit of this math of all places in geology.
Apparently, in geology, geologists want to ask, is this rock the same as that rock by
looking at its composition and saying, you know, it's 80% this material and 15% this material
is at the same as that one.
So they do these comparisons of percentages and there's a whole book act to the on this
compositional models.
So we're reading that now.
Wow.
That's where we're trying to find our new math.
Okay.
Interesting.
I talked about, you mentioned reinforcement learning in our conversation as well as in
your talk.
But when you, when I hear you say it, it's almost like the context is you're talking about
the machine learning reinforcement learning, but you're also talking about like biological
reinforcement.
Am I reading that right?
Yeah.
Yeah.
So reinforcement learning and psychology is called classical conditioning and instrumental
conditioning.
Okay.
So the classic, you know, Pavlov with a dog, salivating, et cetera, that is learning
values for states as in reinforcement learning.
And then instrumental conditioning rats, lever pressing lever in order to get food, that's
basically learning a policy that maximizes values and rewards.
So there is a very kind of direct isomorphism or translation between the computational reinforcement
learning world of learning policies and values and predictions and the behavioral psychology
world and it goes through neuroscience or it goes to neuroscience because we also know
in the brain where these different signals are computed and and and there's a lot of
evidence this goes back to the 80s, actually, the 80s, no, the 90s.
I think 94 was the first paper really making these parallels in particular dopamine, which
is a really important neuromodulator in the brain that's involved in everything from
Parkinson's disease to schizophrenia, depression, gambling, any drug abuse dopamine is seen
today as representing in the brain, prediction errors, a lot of reinforcement learning.
So literally, yeah, yeah, I'm often amazed that people don't know this because in neuroscience
this is so big, this is basically one of the biggest success stories of taking machine
learning, taking computational models and translating them to neuroscience and behavior
is this is the idea that dopamine calculates a prediction error.
So how different is what I'm getting from the world from what I predicted?
This is a key quantity for learning and reinforcement learning, every reinforcement learning algorithm
relies on prediction errors.
We know behaviorally that animals learn from prediction errors.
We know normally that dopamine represents these prediction errors and broadcast them to
the whole brain.
So anywhere in the brain, what does it even mean for dopamine to represent these prediction
errors, meaning the levels of dopamine correlate strongly with prediction errors?
Yes, it means that when in a task, I can through a computational model say, you probably
just experienced a positive prediction error, so you expected, let's say, three MNM's and
you got five.
So through the model, I can track your learning and save with all the experience that you've
had so far, you should be expecting three MNM's.
You just got five, so it's a positive prediction error.
If I record it from your brain, I would see a kind of positive burst of dopamine.
So above baseline dopamine, whereas if my model says you should predict three MNM's and
you actually get one MNM and I record from your brain, I'll see a negative dip.
So I'll see less dopamine than the baseline dopamine levels at that exact time.
It's a phasic, it's a short lived signal.
It's broadcast all over the brain.
It basically says at every point in time, are things better than expected right now or
are things worse than expected?
And AlphaGo relies on prediction errors.
So all of reinforcement learning relies on prediction errors.
And so we see that in the brain.
So really when I say reinforcement learning, I'm thinking about the brain as much as I'm
thinking about the algorithm.
That's fascinating.
Yeah.
It's really an amazing case of convergence of all these lines of research.
And is this a new observation or did you say 80s for this?
94.
I got not 80s, so 94.
So in the 80s and the late 80s, the idea was that dopamine represents reward.
So it's the brain signal for reward.
And people started looking for that signal by recording from dopamine neurons, from
monkey brains, as monkeys were obtaining rewards in a task.
And they were really confused because sometimes dopamine would respond to the reward.
Sometimes it didn't.
And there were all these abstracts in the society for neuroscience saying, you know, this
is clearly not a reward signal, but we have no idea what it is.
And then Reed Montague and Peter Diane, who were then postdocs with Terry Sinowski at
the Salk, the Salk Institute, read these papers and they'd been reading about reinforcement
learning.
So Rich Sutton and DiBarto's work.
And they basically put two and two together.
Wow.
The story is that one day Reed Montague went to Peter Diane's office and said, look at this.
This looks just like a prediction error signal.
And then they published a paper they started from, you know, the politics of these things
are weird.
They started from a paper in science about bees, not about monkey, about bees navigating
with the idea that a ketopamine, which is the equivalent of dopamine signals in insects,
a ketopamine represents a prediction error.
Okay.
And then later they published a paper about that same thing in monkeys a year later in
96.
So this, this first paper was 95, I think there was an abstract in 94.
And then in 97, the famous paper is Schultz Diane and Montague, 1997, where they published
basically the recording data together with the model and said, and this was published
in science saying dopamine seems to be a prediction error.
And this was, you know, this was the hypothesis with some data and since then it's been tested
a million times over.
And on the one hand, it looks like sometimes it's so amazing.
It looks like, you know, dopamine neurons must have read the text book, like they do exactly
what you expect.
And really, you know, convoluted situations, you set things up so that it should be, you
know, whatever, and it is exactly that.
But, you know, with neuroscience, the deeper you dig the more unexplained gold you find.
So a lot of it fits the theory, some of it doesn't.
And that, some of it is not isoteric stuff that we can ignore.
It's not just noise, it's persistent differences.
So the model, this idea is kind of a simplified.
So dopamine, I believe that dopamine definitely represents a prediction error, but that's
not the end of the story, that's the beginning of the story.
And there's a lot of work trying to understand exactly how timing is represented in this
system, and are these prediction errors only for reward or any kind of prediction that's
violated?
And if it's any kind of prediction that's violated, how do you know what to learn?
How do you know what prediction was violated?
Because when it's reward, it's kind of easy, you just update your prediction of reward.
If it's anything, it becomes, you basically need more information in order to learn.
It's making me wonder are there other chemicals that respond similarly that?
Yeah, there aren't other chemicals in the brain that look like a prediction error signal.
There are four neuromodulators in the brain, so there are lots of chemicals in the brain,
but neuromodulators are signals that rather than communicating neuron to neuron kind of
like a phone call, they're more like a PA system, they like broadcast very, very widely.
So there's dopamine, norpinephrine, acetylcholine, and serotonin.
And people have basically been dying to know what these four do because it's like, you
know, if you had four broadcast systems, those are four things that you can tell everybody,
what would you say with those four?
And it's clear that all four are super critical in the sense that disrupting any of these
systems causes a whole host of problems, which makes sense if they, you know, if they broadcast
everywhere, they must be doing something really important.
So dopamine is the best?
So far only dopamine has been implicated in the learning process, or is that too strong
statement?
Dopamine is really important, that frame learns all the time, dopamine is the best understood.
It seems like the simplest of the stories, but the others have been implicated in learning
as well, so norpinephrine has actually been implicated in the breadth of attention, controlling
the breadth of attention, controlling the gain of the system, controlling what you do or
how do you respond to unexpected changes?
So one of the ideas in learning is that when the world changes in an unexpected way, you
should be able to kind of reset, to not continue to carry on previous learning, but to start
a new, to reset your values, to increase your learning rate to say, okay, like, you know,
I take a break here from the past.
So norpinephrine is implicated in that.
Acetyl colon is also implicated in adjusting learning rates to the variability of the environment,
so an environment that's more variable, basically, it's more noisy, so every bit of information
should carry less weight, so you should have a lower, a smaller step size in learning,
we'll call it a smaller learning rate, so acetyl colon is implicated in that.
And serotonin has been kind of, serotonin is so complicated, there are 20 different
kinds of receptors for serotonin, and they do all kinds of things, I mean, all of these
are complicated, everything that I'm saying is a gross simplification, but yeah, that's
what we pay our neuroscientists for, you know, we're trying to figure it out.
So I don't know, we've kind of strayed into like a lot of background material, which has
been amazing, but you're, you're talk, right?
So you're, I won't even try to like summarize it in a sentence, I'll let you do that.
But it was a, well, no, that's what I would do to introduce it, you can actually kind
of walk us through the framework that you presented and some of the experimental results
and things like that.
So kind of broad strokes, you're, you're kind of applying a Bayesian inference to learning,
and at least what I got out of the talk was trying to identify, you know, these latent
variables that are present in the way we kind of perceive the world through experimentation
and relate that back to machine learning or statistical models.
Yeah.
So this goes back to what we talked about in the beginning of the podcast today, which
is how do we parse the world into these, how do we put boundaries on learning and say
all of these experiences are similar enough, I'm going to learn from one to another, I'm
going to learn from this street to that street, and these are different, this is London,
the cars come from the other side of the road, I definitely, definitely not generalized
between these.
And so what my talk was about is trying to identify and what my research is about, strike
to identify the computational algorithms for putting these boundaries in place.
Because I think that's, this is a computational way of talking about the issue of how do we
take a complex problem and make it into a simple one.
So when I cluster experiences together and say all these are similar, I'm basically saying
I'm going to ignore all their differences.
So that's part of like learning what to ignore, I'm going to say these are essentially,
you know, in reinforcement learning, we would say, this is state one, right, we just
give them a label, all of these things are state one.
So now I don't have to, you know, I don't have to analyze all their minute differences.
So I'm trying to understand how the brain decides what is state one, how the brain does
this clustering.
And it's really, it's not that I'm trying to identify what are the relevant aspects
for each task that's, that's less interesting for me.
I'm trying to, to understand what is this general purpose algorithm that can take complex
input, like we talked about before, it can have potentially infinite dimensions that
are relevant.
And can easily say based on inputs one, two, and three based on these three dimensions
and none, none other, I'm going to call this state one.
And those three dimensions in a different scenario say that that one is state two and
not state one.
So that's what I want to understand.
And I think because this is a NP hard, well, I don't know, NP hard, I haven't proven
it.
It's a very hard problem.
Let's say this way, you can't actually use Bayesian inference for this.
I gave, in my talk, I talked about the Chinese restaurant process prior.
So a way to start from a prior that says there are a few latent causes for all of the observations
that we see.
And I'm going to try to cluster observations according to similarity and each cluster
I'll call it a latent cause.
And that will be, you know, my state one and state two.
Is it?
I was wondering this actually, what's the backstory for Chinese restaurant problem or process?
Well, that comes from Stanford, you know, people who live in Palo Alto and go to Chinese restaurants.
The backstory for this culinary metaphor is the idea that, so imagine you have a really
large Chinese restaurant, like an infinite Chinese restaurant, and you think of each table
as a cluster, a latent cause.
And each customer is an observation.
And the observation, so customers come in and they tend to sit at tables where a lot
of people are already sitting.
And that means that we tend to ascribe everything to a few latent causes.
So a few clusters, we say, and we don't want to use all the tables out there.
But once in a while, someone sits at a new table.
So there's infinite capacity.
It's an infinitely sized Chinese restaurant, but people tend to sit together.
This is, you know, just the metaphor for an infinite capacity mixture model.
There are others.
There's the Indian buffet process.
Because we're the same people, another culinary metaphor, and it's kind of similar to that.
It's the only way she got to keep going, right?
I know, I know.
So in the Indian buffet process, you choose a number of dishes, and again, you're choosing.
You tend to choose dishes, it's like an infinite buffet, and you tend to choose dishes that
people have already chosen.
But you're choosing several.
So now the idea is that every observation has several latent causes, not only one.
So in the Chinese restaurant, you only sit at one table, and here you're taking several
dishes.
Where are we?
So I was saying, okay, so this is a Bayesian process.
But even when we apply it to a really simple experiments, we have to approximate.
It's not tractable.
And I think the brain does even better than that.
I think the brain doesn't even try to approximate closely a optimal Bayesian solution.
I think the brain does something that just works.
It might not be optimal.
It's susceptible to biases or decision-making is not always correct, but it's vastly simplified.
I think for the brain, simple is more important than optimal.
And that's because the biggest constraint is, in my view, on learning a decision-making
in real life, in real life, people and animals is time.
We have tons of neurons.
It's not that we don't have enough computational machinery.
What we don't have is enough experience.
Any task that we need to do 30,000 times to be able to do it correctly, and 30,000 is
small for AI, right?
Like millions and millions of trials, even reinforcement learning algorithms, usually thousands
of trials is normal.
We don't need thousands of trials to learn almost anything.
Yes, to be a world-class chess player or to be a perfect athlete of something.
You need a lot.
That's skill learning.
To learn, to just solve a task way better than chance, to learn to survive and not get
run over by a car.
It's just a handful sometimes.
And I think, so what I'm really interested in and what I was trying to give a flavor for
in this talk was, how does the brain solve this?
And what I showed, I didn't show the how.
I said, you know, in, as we don't know, it's not that I have some secret that I'm not
telling.
Was the idea with bringing up the Bayesian inference piece to say, it sounds like it wasn't
to say this is, you know, a useful model necessarily, but more, whatever it is, it's
much simpler than this, and this is the simplest that we'd have.
Yeah, it's a useful model because it inspired, it's in the way that in our work, it inspired
our experiments that I showed.
So it gave us the idea that similarity is key.
So whether it's that exact algorithm or something else, it gave us the idea that similarity
between different observations is going to be key to clustering them or not, it gave
us the idea that the clustering is key that basically, if you think of the real world,
not an experiment, information comes in all the time, there has to be this meta decision
in the brain.
Is this something I know about, or is this new?
If it's something I know about, what do I know, let me retrieve it from memory, let me
act on it, let me update what I know if I find that things are different.
If it's something new, well, let's observe the world and see what to do or let's choose
an action randomly and see what happens.
So there's always this tension between old and new, and that's what the Chinese restaurant
process basically embodies for us or gave us this idea because in the Chinese restaurant
process, you're asking, is this an old latent cause or a new latent cause when you see
an observation?
So I think that that gave us really deep insight, even though I'm not committed to that specific
algorithm, I think that idea is real.
That's how our memory works, that's how we organize information in our brain, is it
older, is it new?
And so it just made us think of this, like, what is doing that?
This is a cognitive, a new cognitive process if you want because nobody had talked about
this process before.
So that's why the algorithm was so powerful for us, even if it's not exactly that one.
And so the experiments that I showed were trying to probe the general idea, is similarity
key for clustering in humans to answer is yes, is it key for how we organize our memory
the answer is yes.
And then the third experiment that I showed was looking at where are these clusters stored
in the brain?
And this was not so much a yes or no question.
It was really, now it was, can we be our opportunistic about this?
So if there is this clustering, if these states are that are learned by the brain are represented
somewhere that we can read them out, then we can start tracking what is the algorithm
by which they're learned?
Because I still haven't answered the main question that I started my whole, my lab and my research
career with, which is how do we learn these say representation?
So I'm, you know, there are clues on the way.
So now I'm thinking about it in this clustering way.
Closest fiction, we're busy for a while, but.
Yeah.
Thinking about it in this clustering way, now I know where to read them out in the brain.
Now I need to give human participants tasks or animals tasks where they are learning states
on the fly, I can read out what they're representing at each point in time and try to understand
what is the algorithm that modifies these state representations over time.
Is it this Chinese restaurant process?
Is it an approximation to it?
Is it something different?
And the specific example that you gave, I forget the problem, but you had subjects in
a MRI and you were able to read out images from the specific implicated section of the brain
in the orbit of frontal cortex, the area above our eyes, that's a really important area.
And then just use those images as inputs to a predictive model that was shown to be
better than chance, is that the right interpretation?
So what I was doing is I was showing, it wasn't really a predictive model, it was basically
using a classifier, a corrector machine, impact, you know, shopping through algorithms
and nips, a corrector machine that basically takes the activation patterns in a magnet
that measures activation patterns in the brain online as people are performing a task
I could measure.
What was that task again?
The task was, it's kind of, you know, it's a little bit complex on purpose, it's a task
where you have to judge, you see faces and houses that are overlaid on each other, and
you have to judge whether the face is older young or whether the house is older young and
there is this rule about whether you're judging faces and houses and-
The point people to that one is pretty-
Exactly.
And you're basically, you're performing this task over time by keeping in mind which state
of 16 different states you're in.
And what we showed is that you can look at the orbit frontal cortex at the activations
in an orbit frontal cortex and from that activity alone you can classify whether a person
is now in state one or state two or state three of these 16 states.
And so what that tells us is that that's a place where you can read out this state representation
even when the representation is, it involves unobservable information, it's inferred, it's
an internal cluster.
And further you did some work that showed that no other places in the brain are likely
to be storing this state information.
Well there were other areas that were likely to be but we found-
So a state representation for a task, for reinforcement learning task is a kind of a,
it's a specific entity, the state you want it to be Markov so you want to have all the
necessary information in even from the past in your representation right now.
And from what I said before about generalization, you want it to not have any extraneous information.
So it has to be a very specific thing.
And that specific, all the relevant things and nothing but them, we found only the orbit
of frontal cortex.
Other brain areas, I mean the orbit of frontal cortex is pretty much as far from sensory input
as can be in the sense that it doesn't get any sense, well it does get, it gets input
from all of sensory cortex but it's not doing its own primary sensory processing.
So a lot of other areas are representing parts of the state, you know, contributing
that potentially to the orbit of frontal cortex.
But I see the orbit of frontal cortex is kind of the final place that says, okay, now
right now for this task, this is, this is my state, building on everything else.
And yeah, and it was the only place in the brain that we found that kind of representation.
Well, we're running long time.
I am hoping that the videos from nips are going to be made available as recordings and so
folks.
I really encourage folks to take a look at your talk because we've really only kind of
skim the surface.
There are some really great experimental examples that you provided that I think folks
would find interesting, but sort of that any kind of wrap up thoughts or places that you
would want to, you know, send people or places that they should start if they're interested
in this field.
Can I take this to a slightly different place for a wrap up?
Sure.
Sure.
So, I'm going to put one of my activism hats on.
Okay.
So in my field in neuroscience, we have a website called Bias Watch Nero.
Okay.
Bias Watch Nero.com, I think, or dot org, I think dot com.
And in that site, what it does is it tracks the gender composition of conferences.
So basically, the idea is for every conference in the field of neuroscience, including computational
neuroscience, on that site, there will be a post of how many of the invited speakers.
This is not contribute to talks, but invited.
How many of the invited speakers are women?
How many are men?
What is the base rate for that specific conference?
So they have a way in Bias Watch Nero calculating in a transparent way that anybody can recreate
for themselves.
What is the base rate of female faculty in that subfield?
And the idea is, it's scientists, we know what a Bias sample is, right?
We don't want to give our algorithms a Bias sample of what they need to learn from.
So why are we giving our audiences a Bias samples of all the great ideas out there in science?
If there are 20 percent women in a field, yes, you know, it would be better if there were
50 percent.
But there are 20 percent.
Why do we have only 5 percent in our conferences?
Why are we missing out on these great ideas?
And this website has made a huge difference, I think, in neuroscience.
And just a couple of years, the Bias has really gone down.
And I think it would be great if computer science started a similar thing.
So Bias Watch CS or something of that sort, including machine learning, including AI, because
I think really it's even more of a pill climb for women in computer science related fields.
And you know, diversity of ideas is good for everybody.
We all want all the best ideas out there.
We all want to, you know, make our, we want to progress as fast as possible with knowledge.
And so, yeah, so, so it's kind of little, you know, call for activism in computer science
in this field.
And I, I know the people who started Bias Watch Neural, and I'm happy to help anybody
who wants to start Bias Watch computer science, so they can just contact me.
And yeah.
All right.
Great.
What's the best way for them to contact you?
Yeah.
At Princeton.edu.
That is so simple.
Yes.
You can spell Yael, which is Y-A-E-L, and Princeton has an E after the C. So people forget
that.
So yes, Yael, Y-A-E-L at Princeton.edu.
Yeah.
Thank you so much.
Well, yeah, thank you.
Thank you.
It was a great conversation.
I really enjoyed it.
Thank you.
All right, everyone.
That's our show for today.
Thank you so much for listening and for your continued feedback and support.
For more information on Yael or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 92.
To follow along with the NIP series, visit twimlai.com slash NIPS 2017.
To enter our Twimlai 1 Mill contest, visit twimlai.com slash twimlai 1 Mill.
Of course, we'd be delighted to hear from you either via a comment on the show notes page
or via a tweet to at twimlai or at Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series.
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in
the AI arena, visit intelnervana.com.
As I mentioned a few weeks back, this will be our final series of shows for the year.
So take your time and take it all in and get caught up on any of the old pods you've been
saving up.
Happy holidays and happy new year.
See you in 2018.
And of course, thanks once again for listening and catch you next time.
