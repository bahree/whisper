Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Earlier this week I had a chance to speak with Chris Shaloo, senior software engineer at
Google AI, about his project and paper on exploring exoplanets with deep learning.
This is a great story.
Chris, inspired by a book he was reading, reached out on a whim to a Harvard astrophysics
researcher kicking off a collaboration and side project eventually leading to the discovery
of two new planets outside of our solar system.
In our conversation, Chris and I walked through the entire process he used to find these two
exoplanets, including how he researched the domain as an outsider, how he sourced and
processed his dataset and how he built and evolved his models.
Finally, we discussed the results of his project and his plans for future work in this area.
This podcast is being released in parallel with Google's release of the source code and
data that Chris developed and used, which will link to in the show notes.
If what you hear inspires you to dig into this area more deeply, you've got a nice head
start.
Chris was a really interesting conversation and I'm excited to share it with you.
Before we jump into the interview, a reminder that the next Twimmel Online Meetup is quickly
approaching.
Be sure to join us next Tuesday, March 13th, for an in-depth review of reinforcement
learning and the Google DeepMind paper, playing Atari with deep reinforcement learning, presented
by Meetup member Sean Devlin.
Get on over to twimmelai.com slash Meetup to learn more or register.
Okay, let's get to it.
Hey everyone, I am on the line with Chris Jaloo.
Chris, welcome to this Weekend Machine Learning and AI.
Thanks Sam, happy to be here.
Awesome.
Why don't we get started by having you tell the audience a little bit about your background
and how you got interested in machine learning?
Sure.
So, I started out studying mathematics actually.
In my undergraduate degree, I did a double major in pure and applied mathematics.
And once I graduated, I then did a year of research in pure mathematics, studying certain
classes of polynomials in fact.
Oh man.
I'm starting to break out and like sweat here, I think in grad school, the real analysis
class I took was like the hardest class.
Right.
Well, I was actually studying these polynomials over finite fields.
So it was more in the abstract algebra space.
But to your point exactly, after I did this, I decided I wanted to do something with more
of a concrete application in the real world.
And so I knew I wanted to do a PhD.
And so I looked at some other options and eventually I decided to pursue a PhD in biomechanical
engineering.
Oh wow.
So I started that PhD, but after about a year I decided that perhaps a career in biomechanical
engineering research wasn't for me.
So I started to look for other opportunities at that point.
So briefly, I went back to mathematics for a little while by teaching classes at a
university, but I decided to apply for a job at Google mainly because I was excited
by some of the really high impact world changing projects that I was hearing about coming
out of Google in particular.
I remember being excited by Project Loon.
So for those who don't know that was a project, it's actually still going that tries to use
networks of flying balloons to bring the internet to users in rural and remote areas.
So I applied to Google and I was hired as a software engineer, but unfortunately I didn't
get to work on Project Loon straight away.
I was actually placed in the Google Display Ads team and I was working on ads for Google
Maps and Gmail.
And this is where I got my first taste of machine learning.
I was actually working on models to try and show the most relevant ads to each user.
And I learned a lot about machine learning in that role.
And then early on in my career at Google, I saw a presentation here at Google by some
researches in the Google Brain team where I currently work.
And they had created a neural network that would automatically generate image captions.
And I thought this was really exciting.
This model that they created could accept raw pixels from photographs and output fully
formed English sentences that would describe the photo.
And so this model was actually trained using only images and captions as input.
So quite amazingly, the model was not actually given any details about the English language
except the captions.
And it actually managed to learn not only how to caption images, but also how to write sentences
with good English grammar, which I thought was pretty amazing as well, just from reading
captions.
And so at that point I decided I wanted to work on the Google Brain team.
And luckily I got the chance to transfer to the brain team after two years in ads.
And since then I've been working on machine learning research here in the brain team.
And I've been lucky enough to work on many different areas of machine learning research
since I joined the team.
Awesome.
And one of the areas that you did get to work on, you got a little bit of publicity on.
This grew out of really a side project or a hobby project, is that right?
Yeah, that's right, actually this project that I've been working on to discover planets
with machine learning really grew out of a random sort of chance idea I had while I was
reading an unrelated book.
There's a book called Human Universe by Brian Cox, which is mainly about exploring our
evolution as a species, but it also digs into the question of whether we are unique in
the universe.
And one of the fundamental pieces to that question is whether there are other planets like Earth
out there and how many there are.
And then one of the things he mentioned in this book was that it's actually quite difficult
to detect these planets in sort of around far away stars.
And one of the difficulties is actually digging through this huge amount of data that
is collected by modern satellites.
And so when I read that, I instantly connected that back to my work at Google where we train
models to dig through large data sets all the time.
And so I wondered if perhaps we could apply some of those same techniques to dig through
these large astronomy data sets and look for exoplanets.
Nice.
And this is interesting to me.
I've interviewed several people who have come from physics, including at least one astronomer
Josh Bloom, who's at GE Digital now, who have started from this kind of deep domain
knowledge and learned machine learning and applied it to what they're doing.
But you're coming at it from almost the complete opposite approach, like you have some machine
learning tools.
And then you hear about this really interesting problem in astrophysics and you get involved
and you actually do something really interesting with them.
How did you get started?
Well, you're right, that I actually don't have very much background or knowledge in astronomy
at all.
In fact, I still don't.
And so the first thing I did was I jumped on Google and started looking for what these
data sets actually looks like and who worked with them.
And I was lucky enough to stumble across the name of an astrophysicist who was at the
time working at Harvard, his name is Andrew van der Berg.
And he seemed to be someone who had dealt with these data sets in which we looked for planets.
And so I literally just sent him an email and said, hi, I work at Google in machine learning.
Would you be interested in collaborating on a project together?
And he actually was interested.
And so this whole project has really been a partnership between Andrew and I where I
have taken care of the machine learning side and the coding side.
And he has taken care of the astronomy side.
Tell me a little bit about the timeline of the project.
How long did you spend, have you been working on this?
So I believe I started working on this project in late 2016.
So around September or October, I believe I first emailed Andrew.
And initially I was working on this as you mentioned at the start as a side project.
I was really just devoting 10 or 20% of my time to this for a few months.
But it was around March or April in 2017.
So perhaps six months later that we had managed to train a model that we thought we might
be able to use to search for new exoplanets.
And when we ran this model on a very small sample of stars, we actually found that we were
able to discover some new signals that probably were exoplanets.
And that's when we started taking this project a lot more seriously.
And then I started working on it probably more towards 100% of my time.
And so then for most of last year, for the rest of 2017, we really went back and carefully
trained our model again and used it to actually carefully search these stars.
Andrew did the validation of our new planets that we discovered and then we wrote the paper
and that was published in January of this year.
We'll dig into kind of how things shifted when you started working on this full time.
But I suspect that there are a lot of folks out there who know some machine learning maybe
have taken a deep learning course or working it and would love to apply it on a project
like this.
And so I'm really curious if you can dig into those first six months or even those first
six weeks.
When you're spending just 10, 20% of your time equivalent to what folks might have to
work on a project like this in their nights and weekends.
How did you approach it and what were some of the things you did?
What did the data set look like and how did you set priorities for making progress with
this?
Sure.
So actually when I started, I had this vague notion that we would use machine learning
to search for planets but I didn't even know what the data looked like.
I thought that what we might be doing is taking a model to input photographs from the
sky and actually look for the actual planets in images on the sky.
But it turns out that's not in fact the way that most exoplanets are discovered.
So the format of the data in this case, it's actually not images of the sky.
But rather we look at how the brightness of a star changes over time.
And so if you're monitoring the brightness of a particular star in the sky and if a
planet happens to pass in front of that star relative to the telescope that's recording
the brightness, you'll actually see the brightness of that star dip down while the planet
is blocking some of the star light and then the brightness will then increase again once
the planet is no longer blocking any of the light from the star.
So the data that we're looking at is actually a time series of brightness and what we're
looking for is certain patterns in that brightness time series that would correspond to a planet
passing in front of the star.
Do you get it as a time series of brightness because it's been pre-processed from the
state that you previously expected it to be, meaning there have some telescopes of captured
images and within those images, individual stars are identified and some brightness value
is calculated and kind of logged off into some time series data set or is there something
else happening to produce that data?
Do you have a sense for that?
Yes, no, you're exactly right.
So what happens is this data set has actually been pre-processed by a team at NASA.
So the data originally comes from the Kepler Space Telescope which was in operation for
the main part of its mission for eight years, beginning in 2009, I believe.
Yeah, the main part of its mission was in operation for eight years, beginning in 2009, but there
was looking at this specific section of the sky that we've been searching for the first
four years.
So there's this four-year data set from the Kepler Telescope and a team at NASA has already
taken that data, has localized the stars in that data and converted that data into these
time series and has actually posted that for free on the internet.
You can actually download it.
It takes up several terabytes and in fact, it takes several weeks to download because
you don't get great download speeds.
Even if you're on Google LAN?
Yeah, even if you're on Google LAN, it's millions of files and we literally just had this
big WGet script that would just download all these individual files and it took about
two weeks to download.
But that was going back to your original question here.
That was one of the ways in which we were very lucky in this project.
A lot of the data pre-processing was already taken care of by NASA.
And so not only that, but we were also lucky in that we had a large training set of labelled
examples from astronomers at NASA and at other universities.
So I mentioned that this telescope was launched in 2009, so over eight years ago.
And in that time, a lot of human astronomers have looked at certain patterns in this data
set and have labelled them essentially as being, this is a signal that is a planet and
this other signal is not a planet.
So that was one of the other ways in which we were lucky.
We didn't have to start from scratch with no training set.
We actually had the data and we had a training set there that was already made for us.
And those two reasons are probably key for us being able to make such rapid progress.
And within about six months having all the pieces together and being able to find some
new planet candidates.
So I know that there are many other problems in, for example, astronomy that do have large
amounts of data available, but perhaps that data is not in a form that is easily ingestable
by a machine learning model and also perhaps there is not a training set of labelled examples
that you can use to train a machine learning model.
So I think those considerations are very important.
If somebody is trying to, in their spare time, train a machine learning model to accomplish
any task, the two things you want to ask yourself is, one, does this data exist in a format
that I can, you know, obtain easily and process myself?
And two, is there a training set that I can use to train my model?
And can you tell us a little bit about the model development process?
How did you approach that?
Sure.
So my process when I develop a model is to start simple and then try and build that up
into something more complicated.
And so the simplest thing that I thought I could try with this model was simply a linear
logistic regression model, which is a very common and well understood type of machine learning
model that has been used for decades successfully in a lot of tasks.
And so I had this data from NASA, this time series of brightness values.
And so the first consideration there was how am I going to feed this data into my model
and we can perhaps get into that a little bit more.
But once I had figured that out, I thought, well, let's just try the simplest model I
can possibly think of.
And you know, I think as a general rule of thumb, you know, your simple model should at
least get you part of the way there, perhaps even most of the way there.
And so we were actually able to train this simple model to actually have relatively good
accuracy, the simple model that we ended up training first actually ended up having an
accuracy of about 92%.
And then several months later when we trained our big complicated neural network model,
that model ended up having about 96% accuracy.
So I think the lesson here is that you can certainly start simple and you'll probably
get most of the way there before you start trying these, you know, really complicated
models.
You talked a bit about the data set, but what did the labels look like?
I'm envisioning, I mean, so you've got these, you've got stars and you're trying to
identify whether the stars have planets, are the labels, you know, yes planets, no planets
or they, you know, numbers of planets, because you, part of what you were able to do with
this research is identify new planets in known solar systems, meaning stars that already
had existing planets, right?
Right.
Right.
So I'm imagining the labels have to be more, somewhat more nuance than binary.
Right.
So, yeah, so if we imagine that the data for a single star is a very, very, very long time
series of brightness measurements, actually the individual inputs to the model are not
that entire time series, but their individual events on that time series that some other,
you know, separate computer algorithm, a very basic algorithm that's not machine learning
at all has gone through and has identified certain events on that time series where the brightness
decreases.
So as you said, certain stars can have many events where the brightness decreases, you know,
it may have many planets, but it may also have something called star spots, which is like
a dark spot on the star, and because the star itself is rotating, those dark spots when
that, when the dark spot rotates into the, into the view of the telescope, that's also
going to cause the brightness of the star to decrease.
Right.
So, so there are, there are many different types of events, some of which are astronomical,
some of which are actually instrumental, there can cause the brightness of the star measured
by the telescope to decrease.
And so when we actually, when I'm actually talking about labels, I'm talking about a label
for a particular event on the, on the star where we've actually observed the brightness
decreasing.
So that label is just brightness decreasing, or is it brightness decrease because of a
planet?
Yeah.
So the label is is the, the second option you provided.
So one of the possible labels is brightness decreased because of a planet.
And the other possible labels are brightness decreased because of some astronomical event,
for example, a star spot.
And then the, the third possible label in, in the data set is that the brightness decreased
for some sort of instrumental reason, perhaps there was some noise, instrumental noise,
or, you know, something like that.
And so when we trained the model, so those were really the only three labels in the data
set that, that we were, that we were given that we started with.
Okay.
But when I train, when I train the model, I actually just, I, I binarized this.
So one label was this event is a planet.
And the other event is this event is not a planet.
Can you give me a sense for, you know, how long did you spend kind of start, you know,
familiarizing yourself with the data set and maybe learning a little bit about the background
of the problem and the domain knowledge you might need to, to solve it versus modeling,
versus applying your model to, to unlabeled data.
That kind of thing.
Sure.
Yeah.
And I think that it was about six months before we'd actually train the model and start
a discovering planets.
And certainly I would say that at least four to five of those months were me trying to
understand this data to actually, you know, not only download it, but being able to open
this file format, right, like, what, what, you know, what, what format does a, there's
a, you know, brightness plot of a star come in from NASA.
It ends up coming in a file with a dot FIT S extension.
And so you need to be able to figure out how to open that extension and how to extract
the data that you want and then actually how to pre-process that.
Luckily, I had Andrew, you know, on the other end of the phone who would, who already
had a lot of experience with this, which, which really helped, which really helped me.
But nonetheless, I spent a lot of time just learning how to actually, you know, use this
data and what it meant.
I spent a lot of that time reading papers as well.
There, there have been other people who have applied machine learning to similar problems.
And so I need to understand what they did, you know, I don't want to reinvent the wheel.
You know, I can try and learn from their mistakes and also the things they did that worked well.
And so, you know, once it actually got to coding the model and training the model and testing
the model, that was really only probably one of one out of the first six months.
Wow.
Wow.
What were some of those things that you learn from other attempts that you applied to your
own modeling process?
Sure, yeah, so there are, there are several, there are several steps that are fairly
standard in astronomy for processing these brightness time series.
The brightness time series are typically called light curves.
So I'll just start using the word light curves for them now.
So the first thing you actually need to do with the light curve is actually, you need to flatten
away what's called the stellar variability.
So it turns out that the variability or the light from a star is not actually going to
be constant over time.
For various reasons, the brightness that you'll measure from a star is actually changing
over time quite naturally.
Meaning independent of an event, the star is getting further or whatever?
Well, yeah, there's so there's several reasons, the star, the star is itself rotating.
So different, perhaps different areas of the star's surface are brighter than other areas.
And so when the star rotates, the brightness that you see actually actually changes.
Also there's instrumental effects here that can also cause the sort of the baseline brightness
to kind of drift over time.
That's actually characteristic of the instrument on the Kepler telescope.
So you need to fit away this kind of variability that you see in the brightness of the star.
But you want to do this in such a way that you don't actually, you know, fit away the
events that we're trying to classify, which is when the brightness decreases kind of more
sharply.
Okay.
And suddenly.
So there are various techniques to do that.
And so we were able to use some of those techniques in order to sort of normalize the brightness
of the star so that outside these events we're looking at, the brightness was totally flat.
So that was certainly one of the key starting points for us.
Were these other techniques machine learning types of techniques or more, you know, simpler
data massaging types of approaches?
Right.
So the current, I guess state-of-the-art system for classifying these events as being
planets or not planets is actually, it's called the RoboVetto, it was developed at NASA.
And this is actually mainly not a machine learning technique.
There is one component inside this RoboVetto that uses a machine learning technique.
But for the most part, it's actually a decision tree that has been sort of crafted and honed
by humans.
So this piece of software, which was developed over a few years, actually runs hundreds
of different heuristics on these events.
And it has all these different thresholds and branches to determine whether this event
should be classified as a planet or not.
So that's the main technique that is actually used at the moment.
There have been some other techniques that took on the problem that we were focused on.
One of those used a random forest model.
And another one used a sort of unsupervised clustering type model.
And so we were sort of one of the first to decide to use neural networks for this problem.
Coming back to the thinking about your labeled data, which you mentioned, you kind of
binarized, you turned into, is this event on the light curve due to an orbiting planet
or not?
How do you get from that to being able to detect new orbiting planets on a known star that
has, on a star that already has known orbiting planets?
Or was that a consequence not necessarily related or produced by your algorithm?
Sure.
So I think I mentioned that these events that we detect on the light curve where the brightness
dips are actually produced by a completely separate algorithm.
And traditionally, what would happen was that, well, the astronomers would run some algorithm
and would take a whole large set of these events that were detected and manually examine
each of them by eye and decide which of those were caused by planets and which of those
were caused by various other events.
And so because this process was, you know, the time required was really dominated by the
fact that the humans had to go in and examine these things, it wasn't possible to examine
every possible signal.
So what they did was they set some threshold and said, okay, we're only going to examine
events detected above some certain signal to noise thresholds.
And so now even with the signal to noise threshold, they still actually had to go through
and examine over 30,000 events from the four-year capital emission.
So it wasn't like they chose a really, really aggressive threshold.
But the possibility still remained that if the threshold was lowered just a little bit
and some of these, you know, lower signal to noise events, you know, could be considered
that there may be some planets that were missed by the previous searches.
And so that's what we looked at in this project.
We took our train model and we used that model to go and look at these events that had not
previously been classified because the signal to noise was below the traditional thresholds.
And so that was how we were able to discover new planets around stars that had already
been searched, you know, multiple times before.
Did you run the algorithm against kind of all of the remaining data and thus you have,
you know, we collectively now have a fairly high confidence that we found all of the possible
exoplanets or was, you know, because of the data volume or whatever, you know, you also
had some threshold or some slice of the data and there's still an opportunity to apply
them or broadly.
So out of the 200,000 stars in this data set, we've actually only run our model over 600
meters.
Wow.
Okay.
So we chose, we chose those 670 because those stars have already had already had multiple
orbiting planets discovered around them.
And it's a lot more likely that you're going to discover more planets around stars that
we've already discovered planets around than if you just chose some random star.
So we really chose these 670 because we just wanted to, you know, have the quickest test
of our model possible.
And so we restricted ourselves to this tiny sample and actually, you know, discovered
two new planets and we were very excited about this and, you know, we decided to stop and
write our paper.
But certainly what we're currently doing right now is working on ramping up our current
pipeline to actually search all 200,000 stars.
And, you know, who knows what we're going to find when we do that.
Talk a little bit about the process and approach you took once you switch gears from the more
traditional approach to applying a neural net.
I think you said you were using a logistic regression originally?
Right.
That's right.
Yes.
So the logistic regression model really just took in, say, one of these light curves, one
of these time series and simply use those as features.
Each independent point on the light curve was an independent feature to the model.
And so the model could learn, for example, if this particular pixel, we fed our model,
we fed the events into the model such that, you know, we'd normalized where we thought
the light, the light was decreasing, we actually sort of like centered the event in the input.
So the model knew where the event was supposed to be.
And so the model could learn things like if this particular pixel has a brightness below
average, you know, that might contribute to a classification of, you know, it being a
planet, or it not being a planet, but essentially in the logistic regression model, each of those
individual pixels was going to be considered independently of all the other pixels, essentially
they were all going to be used as independent indicators of whether this time series,
this light curve was a planet model.
So your input vector to the logistic regression model was a vector of kind of the four years
worth of these brightness values for a particular pixel at whatever time increment, they were
captured at.
So we didn't actually feed in all four years of data in one very, very long vector.
What we did was we actually fed in just one of the events basically that we were looking
at.
So we, we really kind of like zoomed in on the light curve of the particular event that
we wanted to know the classification for.
So our, the dimensionality of our input vector was something like like 1000, 1000 time series
points as opposed to all the time series points over four years.
And did you presumably, like you, you knew where the event was so you bracket it around
it?
Exactly.
We made sure that in our input, the event that we were considering was, was centered.
Got it.
Okay.
So moving from sort of the, the simple model, the logistic regression model, you know,
so, so one, one parallel we, we noticed with this problem was is, is to the problem of
classifying objects and images.
So with an image, your input is actually a two dimensional grid.
But what you're doing is looking for, essentially looking for local patterns within that grid,
you know, local groups of pixels will together form, you know, edges and shapes and images.
And so what we had, we had it, a one dimensional grid or a vector.
But we were also looking for local patterns, you know, in the grid.
We were looking for shapes in which the light would gradually decrease as the planet sort
of passes in front of the star and then gradually increase again, sort of like a U shape.
And so with that parallel to image classification, we decided to use a convolutional neural network,
which has been very successful for image classification in the past.
And the, the only real difference here is that our inputs are one dimensional, rather
than two or three dimensional. And so we, we used one-day convolutions and, and other
than that, our model is very similar to sort of a classic convolutional neural network.
And did you start with a, did you build it up from kind of scratch, or did you start
with a, a deep kind of existing, deep network architecture?
No, it's, it's actually a pretty simple classic architecture that I built up from scratch.
So essentially, it's just convolutional layers, followed by max pooling layers for say 10
or 12, 12 layers deep. I don't remember exactly how many layers we used, followed by some
fully connected layers, followed by the output, which is the probability that this particular
input is applied.
And did the model evolve much at all as you began working with the data, or did that initial
model that you, that you kind of settled on kind of hold throughout the project?
Probably the, the biggest change that we made over the project was actually to feed in
two separate representations of the light curve that we were looking at, of the event.
I should say the event in the light curve that we were looking at.
So this one of these was a, was a wide view of the event.
And the other view was a, a very zoomed in narrow view of the event.
And so what we, what we learned was that the model sometimes needed some information that
was very, very far away from the, the event we were looking at in order to make its classification
decision.
And so that, that was sort of facilitated by feeding in a very, a wide view of the
light curve.
But also the model needs a fairly fine grained look at the, the shape of the light curve
is, of the event as well.
Sorry, as I mentioned, we were looking for U shaped patterns, but there are various,
other astronomical events that can cause, for example, a V shaped pattern.
And so what we did was we, we ultimately ended up feeding in two separate representations
of the event into the model.
And so each representation is treated by sort of individual, convolutional layers that
are totally separate.
And then they're, they're joined up towards the, the end and combined together to, to give
the final output prediction.
Okay.
And maybe tell us a little bit about like how the time has been allocated like since
that six month mark when you started applying the neural network.
Right.
So we spent a lot of time last year writing the paper.
So some of the things that we, we did for that was come up with various ways to visualize
the sort of things that the model had learned.
You know, especially as this paper was going to be published in an astronomy journal rather
than a machine learning journal, where people are perhaps not quite as familiar as, as,
you know, the machine learning community in what exactly a convolutional neural network
is and what it's doing.
So we used some techniques, for example, to visualize the features of the input that
the model had learned.
And that was also a good sanity check for us to make sure the model was, was sort of
looking at the right features in order to make its decision.
So the, the technique we applied there, which is, which is quite a simple one is if, if
you block out part of the input and then fear the input into the model and then you look
at the model's prediction, if the, if the part of the input that you blocked out was really
important for the model's decision, then, then the decision is going to change a lot.
But if you block out sort of an unimportant region of the input, then the model's decision
is, is not going to change, you know, at all perhaps.
And what we found, and what we found was, was when we blocked out the regions that were,
that, that, you know, certainly we thought, indicated that a planet had passed in front
of the star, the model's prediction changed from predicting this was a planet to predicting
it wasn't a planet, which was exactly what we had hoped.
It means that we, we weren't sort of fitting to other aspects of the, of the like curve.
We were really fitting our model was learning the features that we, we expected.
So we spent, we spent quite a bit of time really digging into the model and, and producing
sort of visualizations and explanations of its, of its predictions to, to put in the
paper.
Awesome.
And you're going to be publishing some additional information about that this week is
that to remember correctly, you're going to be publishing some of the code that you
used as well.
Yeah, that's right.
So we published our paper this January, which outlines everything that we did.
But this week, we're actually releasing all of the code that we used to train our model,
which also includes actually the, you know, all the code you need to download the data
from NASA's servers to process the data in the exact way that we did in the paper.
I mentioned like fitting away the, the low frequency variability in the stars brightness,
that's included as well.
You know, all of this, the specifications for our model are included and the hyper parameters
that we use to train the model.
And so anyone will be able to download this data, we'll be able to, to train a model.
And, you know, potentially they'll be able to use this to go searching.
That's awesome.
I hope being, if anything, to see out of this.
Well, a few things.
I'm hoping that some people are able to, perhaps, build on the work that we did.
So as I mentioned, our project has been focused on data from NASA's Kepler telescope, which
has been in operation for eight years now.
But there's another telescope launching this year, or it's at least scheduled to launch
this year by NASA, which is called the, the test, T-E-WS satellite.
And that is going to use sort of a very similar approach to the Kepler satellite for recording
this brightness data for many, many, many more stars even than Kepler did.
And so I'm hoping that, you know, others in the astronomy community will perhaps build
on this, this, this code that I'm releasing, and perhaps even apply it to, to other missions
and, and use it to, to discover planets in, in data collected by the tests.
That's awesome.
Awesome.
Well, that, that is some really amazing work.
I appreciate you taking the time to share it with us.
Great.
Yeah.
Thanks a lot for having me on.
All right, everyone, that's our show for today.
For more information on Chris, or any of the topics covered in this episode, you'll find
the show notes at twimmaleye.com slash talk slash 117.
If you have any questions for Chris, please post them there, and we'll make sure to bring
them to his attention.
If you're new to the pod and like what you hear, or you're a veteran listener and haven't
already done so, head on over to your favorite podcast app and leave us your most gracious
review and rating.
It helps new listeners find us, which helps us grow.
Thanks in advance.
And of course, thanks so much for listening, and catch you next time.
