All right, everyone. I am here with Eric Corvitz. Eric is the chief scientific officer at Microsoft. Eric, welcome to the Twomal AI podcast. Great to be here. Thanks for having me.
I am really looking forward to our conversation and I'd love to start it off by having you share a little bit about your background and how you came to work in AI.
Thanks. Well, I had a long interest in how the brain works, how the mind works. I think this is some of these are some of the earliest questions we ask ourselves as kids.
And going into as an undergraduate, I was interested in biology and the origins of life and basic questions. I did a degree in biophysics. I went off to do an MD PhD in neuro biology in neuroscience.
Larger because as I got into my junior and senior years at college, I started working in a lab where I would a neuro biology lab where I would actually pull micro pipettes, the little electrodes that can get into the neuron of brains in this case of little animals.
And listen in to the clicks and the clacks, actual potentials. I started getting more and more curious about brains. I wanted to go through the MD PhD path. I thought that would give me access to people someday and I was interested also with going interest in biomedicine.
It's Stanford where I started my graduate work in the first six or seven months or so. I started making a commitment to moving more towards computation, because I thought more and more that we wouldn't get great insights through sticking little electrodes in brains for a thousand years that might go to the mathematics side of things.
I remember thinking that what I would do if I was to stay on the MD PhD path in neuro biology would be kind of akin to taking a little copper wire and putting it into a chip.
The motherboard of a most days in Apple to computer and try to figure out the operating system. I just didn't want to be going there. I thought I had better insights to jump ship to its computer science. And I just said, remember the moment where I was in CS and doing classes and I said, this is what I want to be doing.
It's interesting that you describe that transition. I think some of the most interesting conversations I have on the podcast are ones that explore this interplay between the biology and what we're doing on the computational side and the two way intellectual street between the two fields of study.
Absolutely. And you might say that in both cases, and of course, the related, there's this interesting magic. We don't understand the bio and how bio makes minds.
And then we have this mechanism machine computing side that we actually can make progress on and understand and we're always facing off at this big mystery and getting we hope inching towards it.
But sometimes the casm is even even dark matter. We don't know how far away we are from getting getting there.
Among other things, you've been very involved in a number of industry organizations, including the association for the advancement of AI. I think you probably didn't go into all that because if you listed all of them, we'd be, you know, 40 minutes in.
One of the, and I think you are president at one point. And at that in 2009, you as part of your involvement in that organization kicked off a study of AI ethics, which is going to be one of the themes that we dig into in this conversation.
And if you can maybe tell us a little bit about that effort, how it got started. Yeah, context was for it. You know, and it's funny, because it seems so.
Over the top back then when I proposed this, but when I, when I.
I was president of triple AI, you basically, you know, it's kind of a process, your, your president elect, then your president, then your past president. So it's multi your service. But when I came in, it's interesting.
And I was a election of where AI was in 2007, 2009 was my kind of term. And I decided to make the theme of my presidency, AI in the open world. And what I meant by that was, you know, let's take what AI is coming out of the laboratories and it's becoming, it's getting useful, it's getting real.
And the open world as I, as I like to refer to it is pretty complicated, it's much bigger than our systems can represent, it's much more complicated than our laboratory environments.
And the build systems on a technical side that have the ability to understand their limitations such that they're humble and know what they don't know. And might this might not this be one of several technologies, we need to master to get systems to be robust and reliable in the open world that they weren't so overconfident and brittle and fragile.
And so I gave my presidential lecture on that topic of where we had to go as a field, president to triple, I give a lecture one of their years of their office.
And then I also as a proposed that we needed to sort of come together as leading AI scientists and think about the implications of our technologies.
On people on society, the influence is broadly about what it would mean for ethics and law and safety.
And it was this was the time when people were reading books by, and then probably still are by Ray Kurzweil and singularity coming and.
This kind of utopian view of what AI where it might go, as well as a distopian view kind of being bantered about. And I thought, you know, we haven't really weighed in as folks that are committed to the technology as the scientists who you think should be responsible for the technologies go.
All together about 25 people that I chose, especially for diversity and also for knowing them quite well, I mean, these people would be committed to figure things out wouldn't take, wouldn't say I was crazy for asking questions about long term.
AI advances as well as ethical and legal challenges as well short term disruptions. And so we had we booked the this group up into three different working groups, short term disruptions long term AI futures and ethics and law.
And at, and for several months with me, we all came together in February of 2009 at a cinema that was symbolic, that's where the biologists had met a couple of decades before talk about recombinant DNA, what it would mean.
Two days of two and a half days of fabulous meetings covering multiple topics and that was the first time I even heard working with, even heard this phrase, I remember from the third term group, criminal AI.
What is that, you know, criminal uses malevolent uses by nation state and non state actors of how you could use AI now with these new kinds of information feeds coming together from consumer uses of AI, for example, right, and data sets and connectivity.
We also looked at a long term AI futures addressing the concerns of dystopian futures losing control of AI we hear from in books like later that came out like from Nick Postram and others super.
I asked somebody, one of the members was very good, a fabulous scholar, I said, hey, look at Asimov's laws of robotics, you know, and could you, could we implement those laws in constraints and preferences and really feel the system we could completely trust to do the right thing when it came to safety.
And we had other issues as well and law and someone that became, you know, again, we're, I would say foreshadowing of what came of this area, by the way, speaking of disruptions and possibilities on the first day of that asylum are meeting.
I invited two people who I thought would be the best to able to answer this question, it was, it was Andrew Ang at the time, Tom Dean that might then Google, I said, can you tell us, you know, what will, what surprises would, would were on our horizon in the near term, like what would surprise us as AI researchers.
It doesn't, this is basically 2009 in February, and they both gave deep learning talks, which was kind of exotic at the time.
It provides an unsupervised learning and that summer, you know, literally, it was like that summer in July and August deep neural networks were, were found to basically be.
It's an interesting power that was not leveraged in the past because we just didn't have the data set to do it, and that's when we saw the explosion right after they're in a workshop in October and boom, so they predicted a surprise that was going to have big effects.
That's still even three years before what we often think of as the real explosion with ImageNet in 2012 right.
Well, I think what happened was a permit lens on the situation at the time was, this is a funny story behind this involving Jeff Hinton and me.
But I'm not going to go there exactly unless you really problem that, but Jeff and join us at Microsoft, it sounds like a challenge.
Jeff joined us at Microsoft research that summer, and our, we looked at our speech recognition and word error recognition rates, and we saw this jump on a data set called the switchboard data sets that had been flat in terms of the people hammering on it for, for a decade.
We said, like, this was a big deal, and this happened at MSR, you know, building 99 here at Microsoft, and that led to a workshop in that in the fall.
That was really what people together to say what was going on, like how did this happen.
And that was the path up towards ImageNet and so on, but it came out of, and the discovery was, by the way, and we're off the track a little bit of the ethics and responsibility.
You might just hear your listeners.
The discovery was that, because the methods weren't that different from the neural nets of the late 80s and 90s, early 90s.
But the discovery was these neural nets had been famished all these years for data, and we just didn't know it.
And so the, so you kicked off this effort in 2009, and what was the scope of it?
Was it limited to this meeting, or was it a long term study? How, how did you approach thinking about this issue?
Yeah, we, I thought it was exotic enough just to have a multi month single study with a simple report that we wrote this web page on the, on the event.
We did a report out at the Ijkai conference. It's another international meeting.
The following summer.
Got lots of interest in response that briefing led to an article that was surprisingly on the front page of the New York Times about our gathering above the fold as they say.
And I created all this excitement and fear, as well as, you know, questions.
I remember the phone was ringing off the hook when a headline appeared.
Something along the lines. You still find this online as New York Times piece.
Like AI scientists discuss how machines might outsmart man. That was kind of the headline.
And that, you know, we get calls from all over the major news, you know, organizations and so on.
Like, wait a minute. This wasn't like a crack pot. This was a group of scientists saying this.
Should we take this seriously? That was the kind of questions we were getting at the time.
So, you know, again, this meeting is, you know, predates the popularity of deep learning predates Alex net.
And clearly predates kind of our contemporary conversations around trust, trustworthiness and responsibility and bias.
How, how do you compare and contrast what you talked about then and what you talk about now and how do they relate to one another.
You know, Sam, this is, this is like the reality of where we were with AI back then.
So the basic assumption.
My colleagues and I had as grad students and then young, you know, early early career scientists.
We're along the lines of if we can just get this thing to work, you know, it's team coming out of it and huffing and puffing years rotating.
It's a big success.
And, and the things you would build would be like small numbers of things like a prototype or a prototype and its descendant.
You build like a read missions prediction system for health care. And I was like, you just would say, yes, it works.
I get well calibrated probabilities out of this thing that a patient will bounce back in 30 days, for example, the training off electronic health records.
It was always a celebration of the goodness of getting anything to just darn work.
And it was a stretch at the time, start thinking about stuff that not just works with it work poorly or not, but that would be distributed and have huge effects on the planet.
And, and so it's just a different feel for the level of influence and responsibility that became much more obvious.
A few years later, in fact, in 20, 13 or so, just four years later, I thought to myself.
And this again, we should have another meeting like this and get this same group or a bigger group together because it's getting more important now to get the scientific point of view, the scientists who are really committed.
They could understand some of the problems deeply and maybe address them.
And I said, OK, it will be five years in 2014.
And then I thought, you know, it's kind of like induction and gets in plus one. We should do this every five years for like the rest of the history of human civilization is this stuff is going to move fast as technology and it's going to have a huge influence.
And we need to understand our relationship with the technology and that's when I had a discussion with my significant other.
I wonder if we could make our first bit of major, you know, first philanthropy, significant philanthropy, the endowment of some sort of a center that would always do a report every five years and make sure it happened.
That would be proactive that would look out that would have a sort of framing of a set of core topics that will be kind of timeless in terms of concerns.
And this is what led to the 100 year study that's now at Stanford. It's called the 100 year study on artificial intelligence.
But it really is a study that will happen every five years for as long as Stanford University exists.
And John had to see at the time assured me he thought it would last longer than 100 years.
So in thinking about we can maybe take a detour a little bit and talk about your role at Microsoft briefly.
And because I want to explore, you know, the ethics of, you know, the theoretical ethics and what I imagine is much more real and tangible.
Yeah, you know, issues that you must encounter when talking to customers that are actually trying to do things and assessing, you know, risk and responsibility and things like that.
Yeah, so this we're got, yes, I just mentioned some of the framing, which was the Stanford study for me and thinking about this for other colleagues and then getting this, this Stanford University.
Long term study set up over time.
And at Microsoft, there was rising interest and concerns as well, given our role in the world and how central AI was two Microsoft and to our future.
And as far as the, I say the, the modern history of work in this space, Sachin Adele kicked off a discussion in 2016, called together a small group and we asked ourselves and he asked you let the discussion, what are our AI principles.
And it was a very interesting discussion and it converged on six principles that was 20 at one time and 14 but can't converge down to six, which we have, we call Microsoft AI principles, fairness, reliability and safety, privacy and security, inclusiveness, transparency and accountability.
And we have a little internally at Microsoft, we have a little sentence and then click through for bigger paragraph and more detail on each one of those six principles that we've now.
Socialized with the company, we have an annual training about what they mean, but that was the beginning and then around those principles.
Working with Brad Smith, I set up what's called the ether committee, and ether A E T H E R stood for and stands for AI ethics and effects in engineering and research now is the start in 20 late 2016 or 2017 of a committee and then a set of working groups that would focus on specific topics largely aligned with those principles, a few more.
One of them was called for your question, Sam, the sensitive uses panel and this panel became the body by which we would define and then late roll out a process for the company to send sensitive uses for review.
And even defining for Microsoft for our processes for ether committee, what was what was a sensitive use of AI technology is kind of an interesting endeavor in itself early on.
Keeping it simple, but powerful, for example, so we just give you a sense, we said, okay, a sensitive use of AI, one that should come to the sense of uses panel for review is if a product team is thinking about developing a product or has one out there already that can has the risk of denying consequential services to somebody or subgroup health care, financial education.
Or is there a risk of physical or psychological harm or could the technology in fringe upon human rights.
So a few years ago we developed technology and others have come from similar things that we call neural TTS text speech.
That was fabulous in that you could speak to a system, give it a few snippets of how you speak and have the a deep neural net model learn to speak like you so well that it could, you know, it could, if you had ALS and losing your voice, you could rely on this instead of a machine like text of voice to speak to your family.
Or have it be used to narrate in a very natural way broadcast news.
It also can be used from a level in purposes, someone can steal your voice and I have you call your bank and write a check and send it somewhere.
Or could fame to be a politician, a political leader making a statement that could be very dangerous and threatening fraudulence.
So sensitive uses committee deliberated and send a recommendation to our SLT that this technology should be gated should not be freely available.
It should be licensed on a certain restrictions such that like a like a news organization could license it for use.
That's an example of the kind of technology that really it went through a process whereby a panel met with work with the local expert team in our cognitive services area.
And the report which went to the ether board that report was refined typically what's done in those reports is that there's a recommendation that's made an alternate or one or more alternates each one will be a cost benefit analysis.
That's why we're doing this if we don't do this here's some costs here's the upside the downside and then we if if if it's something that has not been precedent that really needs senior top leaders or the company to look at and make a call.
We take it up to the senior leaders.
I say that I don't remember a time where the you know where the top one or two recommendations wasn't the one that was selected from the ether committee for execution by the company.
The responsibility of AI is to educate the folks that are going to use it. I imagine there's quite a lot that goes into internal communications around a effort like ether and you know how training engineers to know when they should raise the flag and start to ask questions about a technology.
Are there a set of educational responsibilities or things that you think that we as an industry, you know need to be committed to from an education perspective around responsible use of AI.
How do you think about that I think so and this you know Microsoft feels and has been running like an AI business school trying to share these principles with with partners.
I I'd like to see more sharing going on at the level of details among all the partnership on AI organizations, including the nonprofits.
To come up with best practices and to educate about them.
Education and training came up as being critical topics and requirements for recommendations being made.
When it came to this we haven't gotten to this this story yet, which is the story of the national security commission on AI, which I just came off of and we just fielded a report after two years, but it really called out the need.
You know for training and best practices when it comes to these to the fielding of AI technologies when it comes to national security agencies of various kinds.
Sorry to jump ahead if you would go there, but no such a central point of the training and sort of getting people on the same page when it comes to engineering, building, fielding, reviewing these systems.
I hope to have you share a bit about that effort and your involvement in it is one of the things that caught my interest.
Because I think there's you know we think you mentioned you didn't use the words, but you alluded to kind of smart cities as one of the ways that folks.
The government wants to apply artificial intelligence and you know sometimes that comes off as a euphemism for surveillance and we.
I think have a you know we're justified in being suspicious of security and security apparatus and their use of technologies like AI tell us about that report that effort, how it came about and some of the key findings.
Yeah, so in.
In 2018 Congress, US Congress, which commissioned this study.
Called for a deep analysis of.
AI and its relationship to where the United States is and could be when it came to applications of AI and this panel, we call the national security commission on AI or NSC AI was established in 2019.
It's gone for two years and on March 1st, it fielded its report that's available at NSC AI dot gob if you want to read the report.
It's a 750 plus page report.
Don't forgive him for not having finished it.
I think I think it's well done.
I think we did sections you want to look at with care. I was mostly involved in terms of my detailed attention in on on for the chapters, as they're called one on the future of R and D, whereas AI heading.
The second one on trustworthy AI and building trustworthy systems, the third on civil liberty civil rights and privacy of AI systems getting into our values.
And the fourth on this really interesting and challenging area that there's lots of emotion about for good reason AI and lethal autonomous weapons.
These four chapters that I think are and the others are really lay out the ideas, directions, challenges with clarity and put a lot of work into writing for both experts and non experts alike.
I was chair of the area we called ethical and responsible AI which led to those two chapters chapter seven and eight seven on trustworthiness and eight on I would say values ethics civil liberties.
Interesting, the original name for my line of effort that I chaired was AI ethics, but you think about what we mean by ethics ethics often brings to mind issues of values and rights and trade offs and
deep questions about beliefs. And but there's a whole other area of responsible AI which is in which which gets into engineering issues like
human AI collaboration methods, the role of the human being when it comes to working with AI systems issues around security and
trustworthiness reliability, how well does an AI system face adversarial attacks.
These new kinds of attacks called machine learning attacks and so on until you wouldn't necessarily call those kinds of issues ethics I would call them like you know engineering challenges with robustness and trustworthiness.
So just splitting up the that are area into two pillars, one focusing on what I would say is value civil rights civil liberties, which I think is very important in a report on national security agencies using AI as you mentioned before, does this interesting tension and we want to be careful to to know who is that we are as the United States and our constitutional principles.
Looking at issues of reliability, what I can say is and I'm very proud of about this report is the report really considered values civil liberties and civil rights and privacy as foundational to everything in the report.
And I remember when somebody in one of the plenary we don't have these plenary sessions with like public and journalists, somebody asked a question at the zoom meeting wasn't this going to be costly all those values and all the reviews you're asking for and the oversight and the committees and the processes in place.
That put us in a non competitive place in the world with AI my reaction was you know this is who we are and it means it's going to be costlier and it may slow us down but it's who we are.
This is quiet at that point because you think about it, the technologies can be dangerous, they can be oppressive, they're already being used by authoritarian regimes to oppress and we need to be cautious and careful and provide an example to the world about a norm for how these technologies can and should be used when it comes to agencies like FBI, NSA, CIA,
Homeland Security, health and human services and so on that we were kind of looking at for the agencies that will be using these technologies.
Well, I want to dig into that report some more and if you don't mind I'll pick your brain as to some folks to talk to the dig deeper into some of the issues that.
Really, by the way, I was curious, well, how did I get selected to do this to be a call commissioners and if you're looking in the way that Congress wrote the.
The charge.
But they said the committee will consider the methods and means necessary to advance the development of AI machine learning and associated technologies to comprehensively address the national security and defense needs of the US, well you look at why I selected they had a whole like a list of how commissioners will be selected in the actual legislation they had.
You know, on record, you know, so the the chair of the House Armed Services Committee gets to vote for one person and that's happened to be Eric Horvitz I said well, that's really.
Adams with, you know, the sent you know I was actually invited directly the Pentagon has and so each commissioner came was invited by a different agency or a different process.
But it was such a wonderful group of people to work with these commissioners fabulous some of some long term colleagues some people had just met.
All, all pretty, you know, you know, deeply passionate about doing a good job and thinking through these issues for the United States.
I want to be sensitive to the time that you've been so gracious to share with us and maybe start to wrap up and have you share a bit about.
Not just your vision for the future, but also how kind of dovetailing to our conversations about.
You know citizenry how we as citizen scientists developers can engage in the process at all the various levels that it's taking place government company organizations any any thoughts on that it's a bit of a broad question.
Yeah, I have lots of thoughts. I mean, look, I mean, I just was invited to give some opening remarks to computer science educators at the sexy technical symposium couple weeks ago, which just brought it's the largest gathering of educators.
My comment was boy, it's it's great to see all this interest into your science also great to have all the influence.
In the world, but with great power and influence now as computer scientists.
Whether it be an AI or systems or networking.
Or HCI, you know, comes incredible responsibility now. You never would think, you know, when I was doing computer science work, we were worried about, you know, algorithms again, again, getting things to work and understanding big O and so on.
You would never would have thought that all of a sudden it's like we have to be teaching in classes now and we are it's been great to see the innovation going on.
You know, you know, you know, even asking ourselves big questions like, is it okay to use machine learning to optimize clicking on an ad or how much time someone engages with a service with a timeline with an Xbox game.
And so we're getting these interesting questions that we have to start thinking about and become very mindful of engineers as we go now.
That we are talking about people's lives, how they spend their time with content they digest.
But we're talking about security issues, we're talking about breaches of data potentially, we're talking about oppression versus freedom in the world, who would have thought that computer science would have become so influential as to require us all as scientists and as engineers and as students to be thinking deeply about
issues that are now not foreign but that there's such that there are a deep part of computer science and computer engineering, whether it be an industry or in other areas.
And that's just I have to take a deep breath sometimes just think of how far we've come and how important these technologies are and how important they will be going forward.
Only more important over time in terms of thinking through how small biases, for example, let's say they're small, they can be quite big, but even a small bias in how a system is working based on small biases in data.
And that come from the way the data was collected from a bias society as we all know historically could now amplify the biases unmass.
And so we have those issues, the issues of privacy, you know data is the fuel of AI systems.
We know and many of us we get our we get our applications on our phones and we see some sort of, you know, no matter how privacy oriented we are, we're kind of busy people, we say okay, okay, okay, okay, sure, sure location.
Yeah, okay, I'll be useful.
And we don't realize that enough.
We see a story in the press that a government agency was able to purchase a large scale database of the location trails of people commercially, which is okay, concurrent laws that is a gray area in particular, I think, actually, and we see new questions coming up.
Meaning we have large scale commercially available data about faces with names that are available in many applications that could be brought together and fusing to databases that are available commercially.
So you can just walk through every one of, for example, Microsoft AI principles, I'll say that again fairness, reliability and safety, privacy and security, inclusiveness, transparency, accountability, you can walk through to those six principles, and you have rich hard questions, incredible opportunities for getting things right.
Also, the ability to stumble. And so let's work to be done.
So, pulling back from asking another question that, oh, for it, we will pull up a few minutes. So, you know, in there, you mentioned, you mentioned HCI in passing, and I know that one of your, one of your topics of interest is kind of complementarity and kind of the way humans in AI will interact.
So, my sense is that that shapes a bit of your vision for how, you know, we should be developing AI, and I thought it would be great to hear you riff on that for a bit.
So, I would say that my, besides the deep principles of how computation leads to minds of the form that we find ourselves thinking with right now, this might, my second most,
the focus has been on human AI complementarity and collaboration and the future of our relationship with AI systems. I think that we can do a lot there.
We're already making some technical advances where it's designed plus there's a science, for example, how machine learning can be harness to understand human blind spots and weaknesses and to complement people.
We work together as a team for greater holes. I think by, we, we can design better futures for, for civilization, for humanity in terms of our well-being and our labor and our work by being deliberate about how we think about how we should be designing AI systems to work with people versus
thinking about an automated agent or fully autonomous technologies. That will happen too, of course, but there's so much opportunity in this space of, for example, helping physicians to recognize when they may have missed something in a patient will crash.
The phrase in medicine, I was just talking about at a Stanford symposium called failure to rescue, which means that you miss something and a patient died in the hospital.
And we've built systems that can actually look out eight hours, 12 hours and say, this patient will need a life saving intervention within four hours, just to alert every staff and the team.
For example, of, of the kinds of complimentary we can have between humans and machines that we have a great partnership.
People are always going to be central in the world and to think through what that means in terms of having supportive nurturing technologies is going to be important.
We have some really fun work on design for you many collaboration and exciting work on mathematical principles and optimizations will change the objective function of the machine learning, you know, the gradient descent such that now it's considering where people are strong and weak.
We just published that piece at triple AI in January and then a piece on optimizing for teamwork in February edge, Kai, so it's fabulous directions.
Maybe to poke in a little bit more on that and maybe be more concrete.
You talked about kind of this human centrism to the way we think about developing AI, do you think as a thought experiment that we should have put the investment that we're applying into autonomous vehicles to more compliment, I don't know what we would call it driver assist technologies and should.
If we, you know, is it the case that by investing in the moonshot of full autonomy, we support or get the technology advances to the driver assist as an interim state or would we have, for example, reduced the, you know, the number of deaths per mile or per thousand miles more quickly if we focused on the human computer centric model directly as opposed to as a.
Byproduct of full autonomy.
Well, no one, no one would argue that having a perfectly autonomous vehicle wouldn't be desirable, you can read a book on the way to Portland from Seattle, for example.
But on the trajectory of where the technologies have gone and the pace, different companies have taken different approaches, so one major manufacturer has said we believe AI should be there as safety nets and to assist drivers to be to drive more safely and to reduce deaths that way.
Other companies have said we're going for it and you'll you'll have to grab the wheel once in a while in the system shouts, but we're trying our best to go for it.
It's, you know, it's hard to know what's the best path, but at any moment, I would think you want to take the technology that you have and think through its best use.
The given the nearly 40,000 deaths in the US, by way, it's been going up, I think because of distraction devices, but the per year US and I think it's over over a million deaths in the internationally per year of driving that we need it's a it's a health health disasters, we need, you know, assistance and safety and nurturing as soon as possible.
But you wouldn't necessarily think that the goals are disparate right you can get into the design and the details of what we were talking about before, how do you design for really great nurturing systems that can enhance safety and let humans remaining control and look at the best of both worlds at the current technical prowess capabilities that we have.
Awesome, well, so much more there, as has been the case with all of the areas that we've explored, but I want to thank you so much for taking the time to chat with us and share a bit about what you're working on and thinking about.
Yeah, thank you, it sounds like we need more time Sam, so fun talking.
Absolutely, thanks Eric.
Thank you very much.
