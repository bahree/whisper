WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.600
I'm your host Sam Charrington.

00:31.600 --> 00:36.600
In this episode I speak with Joseph Conner, founder of Experto Crete.

00:36.600 --> 00:40.880
Joseph has been listening to the podcast for a while and he and I connected after he reached

00:40.880 --> 00:45.400
out to discuss an article I wrote on AI in the healthcare space.

00:45.400 --> 00:49.880
In this conversation we explore his experiences bringing AI-powered healthcare projects

00:49.880 --> 00:55.440
to market in collaboration with the UK National Health Service and its clinicians.

00:55.440 --> 00:59.680
We take a look at some of the various challenges he's run into when applying ML and AI in

00:59.680 --> 01:04.560
healthcare as well as some of his successes such as tackling effective triage of mental

01:04.560 --> 01:09.920
health patients using emotion recognition within a chatbot environment.

01:09.920 --> 01:15.240
We also discuss data protections, especially GDPR and the challenges that come along

01:15.240 --> 01:20.320
with building systems dependent on using patient data under these restrictions.

01:20.320 --> 01:25.920
Finally, we take a look at potential ways to include clinicians in the building of these

01:25.920 --> 01:29.600
applications and the importance of doing so.

01:29.600 --> 01:33.440
And now on to the show.

01:33.440 --> 01:38.080
Alright everyone, I am on the line with Joe Conner.

01:38.080 --> 01:43.560
Joe is an innovation associate in machine learning and AI with the UK's National Health

01:43.560 --> 01:47.200
Service and founder at EXPERTOCREDI.

01:47.200 --> 01:49.920
Joe, welcome to this weekend machine learning and AI.

01:49.920 --> 01:52.200
Hi, it's pleasure to be here.

01:52.200 --> 01:54.040
Well, it's great to have you on the show.

01:54.040 --> 01:55.040
You reached out.

01:55.040 --> 02:01.560
I believe following maybe an article or something I wrote up in my newsletter on the healthcare

02:01.560 --> 02:07.560
space and my interest in further exploring it and offered to help out.

02:07.560 --> 02:11.760
So I'm really glad to have you on the show.

02:11.760 --> 02:18.520
And why don't you start by telling us a little bit about what you're doing at NHS and your

02:18.520 --> 02:23.000
background and kind of how you got involved in applying ML and AI in healthcare?

02:23.000 --> 02:26.440
Well, you know, one of the things about coming on your show is it makes it actually look

02:26.440 --> 02:30.280
back and realise where you've come from.

02:30.280 --> 02:34.680
I'm a self-aminent engineer by first discipline and I joined the time energy authority doing

02:34.680 --> 02:39.600
probabilistic risk assessment back in the 85.

02:39.600 --> 02:46.520
So our 28 years in this particular field and then moved to the Advanced Robotics Research

02:46.520 --> 02:49.720
Lab at the University of Software doing VR and Telepresence.

02:49.720 --> 02:57.720
But then moved out to that in two commercial due diligence in the tech space, I think

02:57.720 --> 03:03.520
it's supply chains and modelling for banks and VCs, but got back into machine learning

03:03.520 --> 03:11.360
making things back in 2006 looking at computer vision based classifiers for emotion recognition

03:11.360 --> 03:13.640
effective computing type things.

03:13.640 --> 03:17.680
And for the past two and a half years I've been embedded with an organisation called NHS

03:17.680 --> 03:24.480
Digital, which is part of the UK National Health Service and been looking at technologies

03:24.480 --> 03:31.520
that it can adopt, but more of late part of an organisation for code for health.

03:31.520 --> 03:37.040
And its primary role is to work with clinicians, find out what they need, and make some open

03:37.040 --> 03:42.040
source software and tools that they can use, and it's obviously the best thing I've ever

03:42.040 --> 03:43.040
done in my life.

03:43.040 --> 03:47.040
I'm really enjoying it, the clinicians are fabulous people and it's nice to be able to help

03:47.040 --> 03:48.040
in that space.

03:48.040 --> 03:49.840
Oh, that sounds great.

03:49.840 --> 03:53.000
You mentioned you've been working on emotion recognition.

03:53.000 --> 03:56.000
Is that something that you're applying in the health care context?

03:56.000 --> 04:00.360
Well, yes, it's important to actually consider, well one of the things I've been looking

04:00.360 --> 04:07.000
at for the past two years is the subjects of mental health and how that can be assisted

04:07.000 --> 04:11.440
by technologies, cell chatbots, etc.

04:11.440 --> 04:15.520
And you can overlay effective computing type technologies in that domain.

04:15.520 --> 04:22.400
So for example, a clinical issue that exists is how do you actually triage people effectively

04:22.400 --> 04:25.880
when the demand for services is so high?

04:25.880 --> 04:30.200
And the clinician approached me and he wanted to know is it possible to do what's

04:30.200 --> 04:37.400
called PHQ9, GAD7, which are sort of scoring mechanisms for looking at mental health in

04:37.400 --> 04:39.360
a chatbot environment.

04:39.360 --> 04:47.440
So we built a tool which you could consider, which looks at scoring those triage processes,

04:47.440 --> 04:50.840
but all the laying on top of that effective computing.

04:50.840 --> 04:57.520
So analyzing emotion in text and all paid to look at suicide ideation words and expanding

04:57.520 --> 05:01.360
on the current models that exist, which are usually just numerical question and answer

05:01.360 --> 05:07.560
type things where you get a rating, but going into dialogue because I think the future

05:07.560 --> 05:14.520
does probably lie in augmenting services using these types of technologies.

05:14.520 --> 05:19.600
Has that project gone into production uses at live and are patients interacting with

05:19.600 --> 05:20.600
it?

05:20.600 --> 05:25.840
It's currently out with clinicians and the supply chain that could potentially use it.

05:25.840 --> 05:30.800
Obviously, it's an open source piece of software, so anybody can use it.

05:30.800 --> 05:36.280
But there are existing providers for mental health services and triage in Britain and we're

05:36.280 --> 05:40.840
saying to them, here it is, play with it, use it, test it.

05:40.840 --> 05:44.760
And if you find a value, here's the code, fill you books, crack on.

05:44.760 --> 05:50.720
But obviously it would need to go through some rigorous testing, not least because it

05:50.720 --> 05:52.920
would be a change of clinical practice.

05:52.920 --> 05:58.280
And so I think it's the early days, I think before to say it's very early days.

05:58.280 --> 06:03.160
You say it's open source, are the models that you're using open source as well or just

06:03.160 --> 06:07.120
the chatbot platform itself?

06:07.120 --> 06:12.240
Everything, the code, the manual, everything is open source.

06:12.240 --> 06:13.240
Okay.

06:13.240 --> 06:16.720
But the role code for health is to actually produce things that people can use quickly.

06:16.720 --> 06:20.120
A clinician comes to me and says, can I do this?

06:20.120 --> 06:23.680
And what's the art of what's possible and I can show them?

06:23.680 --> 06:26.480
So it's great to build things in the NHS.

06:26.480 --> 06:34.040
Just prior you mentioned or alluded to the process of bringing new products, projects

06:34.040 --> 06:42.600
to market in the healthcare sphere, maybe it's worth taking a step back and kind of starting

06:42.600 --> 06:47.080
there and having you talk a little bit about your experiences with machine learning

06:47.080 --> 06:55.880
and AI in general at NHS and in healthcare and getting projects out to clients.

06:55.880 --> 06:56.880
Okay.

06:56.880 --> 07:01.880
Well, I think like most engineers, you think you can solve problems and you go ahead

07:01.880 --> 07:03.200
and solve problems.

07:03.200 --> 07:08.720
I think one of the things that I, big slaining point I have come across is asking the

07:08.720 --> 07:10.720
question, how wrong can you afford to be?

07:10.720 --> 07:13.920
Because this is an environment that's a bit like my early days in the 80s when we're

07:13.920 --> 07:18.240
talking about nuclear power stations, you cannot afford to be wrong.

07:18.240 --> 07:19.240
Right.

07:19.240 --> 07:23.640
And the degree to which you could be wrong needs to be understood by both the person designing

07:23.640 --> 07:28.160
the system and also the person requesting the system.

07:28.160 --> 07:35.160
And I think most people don't, if I were set to be a broad generalisation, most clinicians

07:35.160 --> 07:39.240
do not actually ask that question of themselves before they consider using machine learning

07:39.240 --> 07:40.240
in AI.

07:40.240 --> 07:43.600
So that's the first thing I've learned of one of the things I'm very keen that people

07:43.600 --> 07:47.200
start to do is to recognise that look, how wrong can you afford to be?

07:47.200 --> 07:49.520
So let me give you an example.

07:49.520 --> 07:57.000
We have a system via which we dispatch ambulances and medication and services in Britain.

07:57.000 --> 08:01.480
You could look at that and you could optimise the questions because that's an existing system

08:01.480 --> 08:03.320
that is an expert system.

08:03.320 --> 08:07.200
You could put a Bayesian ranking system in there.

08:07.200 --> 08:11.840
You could optimise that, ask fewer questions, but chances are not because you're asking

08:11.840 --> 08:14.280
fewer questions, you might get it wrong.

08:14.280 --> 08:19.360
So 80% of the time you might get it right, 20% of the time you might get it wrong.

08:19.360 --> 08:23.680
The risk associated with that is significant because in the 20% of the time you don't get

08:23.680 --> 08:26.200
it right, that might be an ambulance.

08:26.200 --> 08:33.760
So there are certain situations where there are highly critical situations for health

08:33.760 --> 08:37.200
that I think machine learning isn't appropriate for.

08:37.200 --> 08:42.640
So early stage triage for low risk areas, I think it isn't appropriate for.

08:42.640 --> 08:46.320
But when you're building these things, the challenge always is how do you get hold of data

08:46.320 --> 08:47.760
to be able to build them?

08:47.760 --> 08:52.880
And I think there's a misconception that because the NHS has 1.7 million people in it and

08:52.880 --> 08:59.520
it's fourth largest organisation in the world, that it is one discreet organisation and

08:59.520 --> 09:01.240
one discreet set of data sets.

09:01.240 --> 09:03.960
And in some cases that's true with public health data, etc.

09:03.960 --> 09:12.720
Most of the time they sit within fairly siloed groups of NHS practices across Britain.

09:12.720 --> 09:18.880
So it's very difficult for me as a machine learning practitioner to say, okay, you, as

09:18.880 --> 09:23.400
in your trusts, will you please show your data with me, personally identifiable data?

09:23.400 --> 09:25.440
It just doesn't going to happen.

09:25.440 --> 09:30.960
So I think the Chitkey skill for myself and people who will follow on from me will

09:30.960 --> 09:36.080
be to actually get some skill base at a local level embedded at a local level, so that people

09:36.080 --> 09:43.240
can build classifiers and prediction mechanisms that can be used locally, built locally within

09:43.240 --> 09:46.360
the boundaries and the firewall of a particular trust.

09:46.360 --> 09:51.880
The only other option is to try and agglomerate all that data together and well, there's been

09:51.880 --> 09:59.520
further successful with that with a group of organisations providing details to Swansea

09:59.520 --> 10:00.520
University.

10:00.520 --> 10:05.160
Not on a national Welsh skill, but I think one of the challenges I, I think I'm going

10:05.160 --> 10:10.560
to look at probably from May onwards is the subject of federated machine learning because

10:10.560 --> 10:15.440
there's a better chance I think of that being used and by federated, it means different

10:15.440 --> 10:20.280
things to different people, but by by federated machine learning, I mean, let's build models,

10:20.280 --> 10:27.760
let's build models external to the NHS and apply them locally and just get the gradient

10:27.760 --> 10:31.600
descent or just get the error back, let's see what these things work, so we're not actually

10:31.600 --> 10:37.400
looking at personal data, we're not actually looking at anything relating to anything

10:37.400 --> 10:43.440
that could define a person in the real world, but just looking for the success or not of

10:43.440 --> 10:47.080
the models, I think there's a better chance of being able to deploy models in that sort

10:47.080 --> 10:50.640
of environment, but it's early days.

10:50.640 --> 10:58.120
So do you have a taxonomy of sorts or mental model for the different application areas

10:58.120 --> 11:03.800
within healthcare broadly or the NHS of ML and AI?

11:03.800 --> 11:07.680
I think within like an organisation this size, you wouldn't be surprised to find out

11:07.680 --> 11:14.440
that it sits within text, it sits within images, and to a lesser degree, it sits within

11:14.440 --> 11:20.520
video, but it's primarily text and images, so for example if you wanted to do some

11:20.520 --> 11:25.200
analysis of drug use and drug use effect, it's very difficult for you to do anything other

11:25.200 --> 11:31.520
than natural language assessment on text within a health record system, and the taxonomy

11:31.520 --> 11:37.120
is associated with doing that sort of analysis that you might expect to see like ICD-10,

11:37.120 --> 11:46.800
UMILs, SNOMED, are not the ways in which people classify a diagnosis, just don't exist

11:46.800 --> 11:50.880
in these systems, they are just free text systems.

11:50.880 --> 11:57.600
So people are busying themselves, developing, um, annotation mechanisms to understand for

11:57.600 --> 12:04.320
a particular medication, these are the likely ways in which a clinician will express those,

12:04.320 --> 12:11.320
and running those algorithms to understand the effect of drugs and the effect of treatment,

12:11.320 --> 12:14.960
particularly in mental health and in mind about in London they're doing some success in

12:14.960 --> 12:20.400
that area using products like Gates and to a lesser degree glove, the natural language

12:20.400 --> 12:28.160
tools, but within the realms of images, it's very difficult because most images have also

12:28.160 --> 12:36.040
got embedded in them, tools, techniques for maintaining personally identifiable data,

12:36.040 --> 12:40.880
so an x-ray will have identifiable data in it, so we've been busing ourselves over the

12:40.880 --> 12:48.280
past three months looking at ways in which we can, um, allow people to label data without

12:48.280 --> 12:54.560
actually, um, so say we had a physiotherapist who wanted to create some label datasets for

12:54.560 --> 12:59.320
shoulder impingement, it'd be very difficult for them to create that dataset, um, if you

12:59.320 --> 13:03.680
don't give them say web-based or mobile-based tools by which they can take pictures label

13:03.680 --> 13:09.520
the data and give that to in a controlled environment to the people who could produce the algorithms.

13:09.520 --> 13:15.040
So yeah, labeling of data and producing data that you can use for machine learning is

13:15.040 --> 13:19.080
one of the biggest challenges I think that we have, and it's not going to go away probably

13:19.080 --> 13:22.560
because we are a very disparate organization.

13:22.560 --> 13:28.960
So it sounds like you're, you kind of think of things in terms of the media type if you

13:28.960 --> 13:36.760
will, text versus images versus video, uh, I'm curious even at a higher level, there are,

13:36.760 --> 13:41.560
we could probably rattle off the different types of applications, but there you've, you've

13:41.560 --> 13:48.120
talked about diagnostic applications, you've talked about with the ambulances, like the

13:48.120 --> 13:54.400
efficiency types of applications, I was just curious what other broad categories of

13:54.400 --> 13:56.640
applications there might exist.

13:56.640 --> 14:00.600
I imagine, you know, I say sometimes look at an org chart and you can figure out where

14:00.600 --> 14:07.800
you can apply it, the same thing applies at the NHS in terms of HR and lots of other types

14:07.800 --> 14:08.960
of applications.

14:08.960 --> 14:19.240
I think we need to be realistic, and the doing machine learning and AI on clinical subject,

14:19.240 --> 14:23.920
I would break it up into two areas, what is clinical and what's process?

14:23.920 --> 14:28.880
If you look at it just as in process and people were not different from any other organization

14:28.880 --> 14:34.120
in the world, somewhat bigger, so there are ways in which you can do process improvement,

14:34.120 --> 14:44.360
whether that's booking people onto of treatment plans, booking people into hospitals, um,

14:44.360 --> 14:48.760
waiting this management, that type of, these are sort of the sort of bread and butter you

14:48.760 --> 14:53.680
would expect machine learning solutions to be, uh, well made for.

14:53.680 --> 14:58.560
And there's not little or no clinical risk associated with some of those, uh, right,

14:58.560 --> 15:04.120
but when you start to look at clinical applications, you come across one thing that is probably

15:04.120 --> 15:08.440
unique to healthcare, which is the subject of a healthcare pathway.

15:08.440 --> 15:12.520
So for every form of treatment, there will be a healthcare pathway, which we'll say you

15:12.520 --> 15:17.840
do this by this, do this, this, this, this, this, and this will be, um, under what's called

15:17.840 --> 15:22.840
the nice guidelines, so it's simply equivalent to the HIPAA compliance, et cetera, you would

15:22.840 --> 15:25.000
have in the stands.

15:25.000 --> 15:29.800
And some of those processes, if they're in a pathway, you could augment.

15:29.800 --> 15:35.320
So if we take the subject of mental health, you could augment the, the process of, um,

15:35.320 --> 15:40.000
triaging by getting better data to people more quickly.

15:40.000 --> 15:46.080
And that's important because, uh, like, not unlike other, um, conditions, the sooner you

15:46.080 --> 15:48.680
treat somebody, the better it's going to be.

15:48.680 --> 15:56.000
So gathering data and analyzing data and scoring that data more efficiently to help a clinician

15:56.000 --> 16:02.280
understand who they should see isn't much better than what currently exists in some areas

16:02.280 --> 16:04.520
of mental health, which is you get a questionnaire.

16:04.520 --> 16:08.640
So if you're, uh, paper questionnaire, in some cases.

16:08.640 --> 16:13.360
So if you've got a condition, they will, uh, they will meet you and they'll ask you

16:13.360 --> 16:16.000
to rate how you feel on certain subjects.

16:16.000 --> 16:19.640
If you work with paper questionnaire, you will see, and then maybe too excited, you have

16:19.640 --> 16:20.640
to fill it in again.

16:20.640 --> 16:24.200
The data gets lost, and that's just not the way to work.

16:24.200 --> 16:28.320
So you can imagine in a chatbot environment, you can capture that data, but person doesn't

16:28.320 --> 16:34.920
have to be there to actually, um, gather that data, and they can control that data and

16:34.920 --> 16:37.760
share that data as and when they see fit.

16:37.760 --> 16:41.760
So it doesn't, it could be a system that sits on a mobile phone, which they control their

16:41.760 --> 16:46.200
user, and it may not necessarily go to the NHS unless the person specifically wants it

16:46.200 --> 16:47.360
to do.

16:47.360 --> 16:53.760
So these sort of solutions, the triage in making better decisions about what to do next.

16:53.760 --> 16:58.680
At the early start stages of a, uh, a diagnosis is worthwhile.

16:58.680 --> 17:05.840
It's very difficult to suggest, um, alternatives, uh, one of the key and the most successful

17:05.840 --> 17:11.040
bits about being involved with code for health is the clinicians involved from the onset.

17:11.040 --> 17:15.160
And I think that's critical for anybody wishing to bring systems into, into any health

17:15.160 --> 17:20.640
care system, anything that's going to affect the current clinical practice should be designed

17:20.640 --> 17:26.480
with clinicians, not presented to them as a solution for increasing efficiency, which

17:26.480 --> 17:33.360
I've seen done, um, and also it's going to use some, some machine learning terms, you

17:33.360 --> 17:38.040
know, tailor it back a bit and explain exactly what these things do, because people need

17:38.040 --> 17:43.240
to understand specifically, because a GDPR, how these things work, and it's clinicians

17:43.240 --> 17:47.640
at the end of the day that will want to either use it or not or refer it or not.

17:47.640 --> 17:48.640
Right.

17:48.640 --> 17:53.040
So it's, it's how you, how you design, how you decode design a product is the most important

17:53.040 --> 17:54.040
thing.

17:54.040 --> 17:58.040
And being close to clinicians is, and clinical practice wherever you were in the world

17:58.040 --> 17:59.600
is, I think, is the, is a key thing.

17:59.600 --> 18:03.400
If you want to do useful things in machine learning, that's why it's such a good thing

18:03.400 --> 18:08.280
to be, we embed it in the NHS, although it's two and a half days a week and the rest

18:08.280 --> 18:10.240
of time I mean, I spread a creed.

18:10.240 --> 18:13.720
Just working with people, working with clinicians is a, is a wonderful thing, because you know

18:13.720 --> 18:21.720
you can remove some of the pain of doing practice, uh, by optimizing it and augmenting it.

18:21.720 --> 18:26.720
Um, and that, maybe that's another subject that's worth discussing is the subject of what

18:26.720 --> 18:29.960
is AI to most people.

18:29.960 --> 18:36.040
The conversations around what is AI and is it going to remove jobs, et cetera, is just

18:36.040 --> 18:37.840
prevalent in the press.

18:37.840 --> 18:43.480
If I see another discussion with a look of a humanoid robot in it, I think I'm going

18:43.480 --> 18:44.480
to scream.

18:44.480 --> 18:45.800
It drives me a party.

18:45.800 --> 18:49.800
It's just, it doesn't help people to understand that really, what are we looking at here?

18:49.800 --> 18:53.000
We're looking at maths stats and computers.

18:53.000 --> 18:56.960
And they are programmed to do things or not by humans.

18:56.960 --> 19:02.080
And they can be used to augment practice in a clever way by doing predictions and classifications

19:02.080 --> 19:04.040
faster than some people can do.

19:04.040 --> 19:05.520
It's not going to remove jobs.

19:05.520 --> 19:09.960
I know, I don't believe it was going to remove jobs in, in healthcare, and it neither

19:09.960 --> 19:15.400
is should it, because you know, unless we get, um, systems that are good at socratic

19:15.400 --> 19:20.440
dialogue, um, listening better, responding better, very quickly.

19:20.440 --> 19:22.440
And I don't see that happening at the moment.

19:22.440 --> 19:25.960
Uh, I think we're, say, safe to say that clinicians are safe.

19:25.960 --> 19:29.280
I can certainly see it changing a lot of jobs.

19:29.280 --> 19:30.280
Yeah.

19:30.280 --> 19:31.280
Right.

19:31.280 --> 19:36.560
Hopefully for the better one would help both from the patient perspective as well as the

19:36.560 --> 19:37.560
worker perspective.

19:37.560 --> 19:38.560
Yeah.

19:38.560 --> 19:43.360
I mean, the National Health Service is about making people better.

19:43.360 --> 19:48.520
It isn't about keeping people well, although I think increasingly people are thinking like

19:48.520 --> 19:49.520
that.

19:49.520 --> 19:52.920
So public health England does exist and there are various initiatives to keep people

19:52.920 --> 20:01.080
well, but I think in economies like ours that are very densely populated regions, um,

20:01.080 --> 20:05.440
we need to think more about how can we build AI systems that keep people well.

20:05.440 --> 20:07.880
That's become a pretty popular topic here.

20:07.880 --> 20:12.080
Uh, the whole population health management idea.

20:12.080 --> 20:13.080
Hmm.

20:13.080 --> 20:18.160
Um, I don't know that we're necessarily any further ahead than the NHS is.

20:18.160 --> 20:20.960
Um, I'm certainly far from an expert in that.

20:20.960 --> 20:25.920
Uh, it's a topic that comes up quite a bit when talking about kind of health care

20:25.920 --> 20:26.920
directions.

20:26.920 --> 20:27.920
Yeah.

20:27.920 --> 20:33.240
I think if we look at what we have in our phones in terms of technology, it's wholly

20:33.240 --> 20:34.240
believable.

20:34.240 --> 20:39.160
And in my experience, when we built emotion classifiers that can sit in a wholly powered

20:39.160 --> 20:44.600
bike, but it's not within the NHS, but within expert degree, we build emotion classifiers

20:44.600 --> 20:50.320
that can sit in a phone and passively understand your emotional state and put it in a, in a diary

20:50.320 --> 20:54.440
that you can subsequently review using computer vision technologies.

20:54.440 --> 20:59.080
So that sort of thing could exist and could that could enable me to reflect about what

20:59.080 --> 21:02.120
makes me happy and what works you sad.

21:02.120 --> 21:06.600
And therefore I could with the right tools that might be chatbot might give me some

21:06.600 --> 21:12.680
ideas to the way in which I can improve my behavior to make me more resilient.

21:12.680 --> 21:18.000
These sort of things can be encompassed within our day to day lives if they, if somebody

21:18.000 --> 21:19.000
wants to do that.

21:19.000 --> 21:26.000
And I know, I mean, there's quite a lot of resilience apps coming out, resilience technologies,

21:26.000 --> 21:33.000
monitoring, sensing technologies that exist, but behavior change, but a population level

21:33.000 --> 21:37.360
isn't just about the technology, it's about a country's willingness and often society's

21:37.360 --> 21:40.400
willingness to be assisted.

21:40.400 --> 21:43.160
And we've got to trust the thing that's assisting us.

21:43.160 --> 21:47.240
So if you've got a phone that's saying, look, Joe, you've not been happy, you've been

21:47.240 --> 21:50.160
grumpy for a couple of days now.

21:50.160 --> 21:53.160
What do you think's causing that and what do you think you could improve that?

21:53.160 --> 22:00.800
Now, if I, if my phone did that, which it could do, I built a bot that actually does that.

22:00.800 --> 22:03.720
Now, I built it, so I know it works.

22:03.720 --> 22:08.840
But if I had, I've got to have that given to me and I didn't involve, wasn't involved

22:08.840 --> 22:11.920
in the process, didn't know it would work, didn't know what its purpose would be.

22:11.920 --> 22:14.880
I might feel like, you know, what is this thing?

22:14.880 --> 22:15.880
It's an issue of trust.

22:15.880 --> 22:20.480
You trust AI, but you can't trust it if you're involved in the process of its design.

22:20.480 --> 22:27.640
And I think most people are designing tech with including people in the process a lot

22:27.640 --> 22:31.520
of the times, because it just doesn't get used.

22:31.520 --> 22:37.240
So a population level behavior change does require, I think, societies and I think it's

22:37.240 --> 22:42.800
incumbent on health services to say, look, we want to, we want to move our country in

22:42.800 --> 22:47.880
this direction, that might be mental health, etc.

22:47.880 --> 22:48.880
Let's do this.

22:48.880 --> 22:52.560
And these are the tools and techniques we're going to use on that.

22:52.560 --> 22:56.640
And that requires governments to want to do that as well.

22:56.640 --> 22:59.400
But also requires people to feel comfortable with the tech.

22:59.400 --> 23:03.680
And I think that's my concern at the moment is that there's too much rubbish being talked

23:03.680 --> 23:08.560
about machine learning and making it frightening when in fact, it isn't.

23:08.560 --> 23:16.600
You mentioned previously and now just the notion of centering this process around the

23:16.600 --> 23:17.600
clinician.

23:17.600 --> 23:20.560
What has to happen to fully enable that, right?

23:20.560 --> 23:24.800
You know, you said machine learning is maths, stats and computers.

23:24.800 --> 23:27.760
Clinicians are typically focused elsewhere, right?

23:27.760 --> 23:34.640
Do they need to, do we need to turn our clinicians into mathematicians, statisticians and computer

23:34.640 --> 23:35.640
geeks?

23:35.640 --> 23:44.440
Or have you come across some interesting ways to pull clinicians into the process?

23:44.440 --> 23:48.240
And do you have any hints at ways to do that at scale?

23:48.240 --> 23:53.720
So let's say a group of people in the NHS want to, a group of clinicians are wedded to

23:53.720 --> 24:00.840
the idea of doing changing customer practice within, after a clinical risk assessment of

24:00.840 --> 24:05.240
system, let's call it a chatbot, just because we've been talking a little bit about that.

24:05.240 --> 24:10.920
If they wanted to deploy that, the next challenge for, not for me, because we're in code for

24:10.920 --> 24:15.600
how we just give it away, but the next challenge is if you're not doing what I'm doing, which

24:15.600 --> 24:21.840
is giving away things free to the NHS, is who's going to buy these products?

24:21.840 --> 24:23.960
Because it won't be the clinician.

24:23.960 --> 24:29.560
The clinician may specify that they should be used and therein lies the other challenging

24:29.560 --> 24:35.080
issues within health boards or within NHS trusts.

24:35.080 --> 24:40.400
There are purchasing processes, typically three to five year contractual process that

24:40.400 --> 24:44.920
you need to go through to get that through the door.

24:44.920 --> 24:47.320
So getting it specified is one thing.

24:47.320 --> 24:52.480
Getting it passed in compliance with nice guidelines and in compliance with clinical

24:52.480 --> 24:56.520
guidelines is another thing, so that might take you two years.

24:56.520 --> 25:00.080
You then have to wait or you have to create a process for which that process, that product

25:00.080 --> 25:05.120
is subsequently specified and purchased, that might take you another three years, and

25:05.120 --> 25:10.400
I know very few SMEs, very few SMEs, and not many large companies.

25:10.400 --> 25:16.040
For particular technologies who have both the cash or the pre-deliction to want to wait

25:16.040 --> 25:23.360
to run for those types of processes to go through.

25:23.360 --> 25:28.560
It's not easy, and quite rightly it's not easy to implement technology in health care.

25:28.560 --> 25:35.480
But once you're in, the people who do it, stay there, but I have a slight problem with

25:35.480 --> 25:39.760
the fact that once they're in, they don't often innovate.

25:39.760 --> 25:45.120
They get there, they don't make any changes, so customer practice, clinical need changes,

25:45.120 --> 25:50.640
the systems don't change with it, and that's why, and that actually causes the thing that

25:50.640 --> 25:55.440
we have in the NHS, which clinicians will want innovation, and they will subsequently

25:55.440 --> 25:59.000
make it happen because they're not happy with what they've currently gone.

25:59.000 --> 26:05.680
So that's the strange cycle, I don't think it can be that unusual for other large organizations,

26:05.680 --> 26:09.760
particularly public sector organizations, it must be common, I would have thought.

26:09.760 --> 26:16.320
It actually calls to mind a lot of what you see on the traditional enterprise side as

26:16.320 --> 26:17.320
well.

26:17.320 --> 26:23.880
Take, for example, the evolution of cloud computing.

26:23.880 --> 26:30.480
A lot of that came out of pent-up frustration in being able to get infrastructure through

26:30.480 --> 26:35.840
the traditional procurement process and IT processes, so you'd have what they call this

26:35.840 --> 26:43.560
whole shadow IT, and it makes me wonder if there are a shadow health care type of thing

26:43.560 --> 26:50.360
arising where individual clinicians or groups are looking outside of these traditional

26:50.360 --> 26:56.240
structures, or as the bureaucracy, if we can call it that, is it so strong so that that's

26:56.240 --> 26:58.520
less possible in healthcare?

26:58.520 --> 27:02.920
You've got to bear in mind once I've been a beneficiary of the NHS for 55 years, I've

27:02.920 --> 27:08.520
only been within its walls for two and a half, but I would have to say that an outsider

27:08.520 --> 27:15.040
looking in what you've said is a fur representation of customer practice.

27:15.040 --> 27:20.000
So we've talked about some of the clinical challenges and along the way you've mentioned

27:20.000 --> 27:21.000
GDPR.

27:21.000 --> 27:28.200
Do you have any particular insights into how that lays out either generally or within the

27:28.200 --> 27:30.080
healthcare confines?

27:30.080 --> 27:35.100
Very much so, I've been pondering on the subject of GDPR since I came in through the

27:35.100 --> 27:43.880
door of the NHS, not least because it directly impacts on black box algorithms, which my

27:43.880 --> 27:51.680
particularly fond, so it's very difficult, it's very difficult for me and I would say

27:51.680 --> 27:57.240
difficult, I'd say impossible for me to recommend the use of a black box algorithm, and I don't

27:57.240 --> 28:01.960
have a problem doing that in some cases because we both know that majority of problems

28:01.960 --> 28:09.160
can be solved with, you know, logistical regression, experts, you're going to get very good

28:09.160 --> 28:16.640
responses on your dataset by using things that people can understand.

28:16.640 --> 28:26.200
But you seem to have accepted the labeling of black box to describe deep learning where

28:26.200 --> 28:32.800
I've come to refer to it as opaque as opposed to black box, it's hard to see, but you

28:32.800 --> 28:34.480
know, you can see some things.

28:34.480 --> 28:35.480
Of course.

28:35.480 --> 28:42.880
I mean, you can apply models on the outputs of a deep learning model and create an approximation

28:42.880 --> 28:49.040
of how it's created, like a decision tree, you can throw lots of data at a deep learning

28:49.040 --> 28:52.720
model and look at the outputs and explain them.

28:52.720 --> 28:59.200
But the worst case scenario from my perspective, and maybe becomes, you know, because of the

28:59.200 --> 29:04.520
background I come from, looking at probabilistic risk assessment type stuff, is this.

29:04.520 --> 29:10.680
I have an algorithm and it dispatches an ambulance to you or it doesn't, let's say it doesn't,

29:10.680 --> 29:14.800
you die, your family have a right to understand what happened.

29:14.800 --> 29:22.280
So let's say, the process by which that happened involved in some cases, a black box algorithm

29:22.280 --> 29:29.400
on opaque algorithm of some form, and you say to me, okay, well, you obviously made

29:29.400 --> 29:32.840
that decision based on this algorithm, how did it work?

29:32.840 --> 29:38.600
Now under GDPR, because of the issue associated with algorithmic responsibility, I have a legal

29:38.600 --> 29:39.600
requirement.

29:39.600 --> 29:42.760
There is a legal requirement on me, if I was the designer of that system, or indeed the

29:42.760 --> 29:46.320
user of that system, to explain that to you.

29:46.320 --> 29:50.760
It has not yet been tested in law as to the degree to which that explanation will be

29:50.760 --> 29:52.240
acceptable.

29:52.240 --> 29:59.440
So, the current process is that all such algorithms will have a person in the loop.

29:59.440 --> 30:03.360
But let's face it, you can put a person in the loop, the degree to which that person

30:03.360 --> 30:09.360
made that decision, or do we switch that algorithm in the decision, isn't always clear.

30:09.360 --> 30:16.160
Knit a couple of those together, and you might have an excellent explanation for the way

30:16.160 --> 30:19.200
in which a call was handled, for example.

30:19.200 --> 30:24.320
You might have some gap in speech type technology, which would understand that that call was

30:24.320 --> 30:28.800
handled very well, and then you might have some technologies which will take you through

30:28.800 --> 30:33.040
a questionnaire that will dispatch an ambulance.

30:33.040 --> 30:38.040
One might, the first one, might be a black box algorithm, I don't know, opaque algorithm,

30:38.040 --> 30:42.560
because the one might be an expert system.

30:42.560 --> 30:47.760
What actually was, played the greatest influence in that decision to dispatch the ambulance,

30:47.760 --> 30:52.320
was it that the system said I handled the call effectively and asked all the questions

30:52.320 --> 30:58.640
correctly, or was it the decision tree that I went through?

30:58.640 --> 31:03.080
It would be very hard to say the two things are linked, and if the former doesn't have

31:03.080 --> 31:10.320
a very good explanation that you will understand, it's quite a large financial hit, I mean,

31:10.320 --> 31:15.240
20 million euros, potentially, put instance of not being able to explain an algorithm.

31:15.240 --> 31:23.680
I don't know any SMEs in the world that prefer to take that degree of risk, and in practicality

31:23.680 --> 31:29.440
is termed, that wouldn't happen, I don't see that happening, but it's not been tested,

31:29.440 --> 31:36.080
so it's very difficult for me to suggest the widespread use of opaque algorithms because

31:36.080 --> 31:37.760
of this issue.

31:37.760 --> 31:44.720
Even though I know, in some cases, some specific ones that the algorithm may be more accurate

31:44.720 --> 31:49.440
and have less risk associated with it, but if I can't explain it, then what do I go

31:49.440 --> 31:50.440
with that?

31:50.440 --> 31:55.400
So it's been, I completely understand GDPR, I think we have to comply with it, we have

31:55.400 --> 32:01.880
no choice, if we want trust in machine learning, we need to be able to explain things well,

32:01.880 --> 32:08.760
so I've been showing a lot of those people running, well, since the days of line, when

32:08.760 --> 32:15.560
line was originally, how many years has that been around there, two, it's been a couple,

32:15.560 --> 32:21.160
so if somebody were able to solve the problem of explaining how that works, in a manner

32:21.160 --> 32:26.280
that would be acceptable to a member of the public who was affected by it, I'd be a very

32:26.280 --> 32:31.560
happy man, because I think these algorithms have a very important role for just speeding

32:31.560 --> 32:36.720
up the process for which we learn about things, not necessarily just doing things, how we

32:36.720 --> 32:38.440
learn about things.

32:38.440 --> 32:44.440
Do you have any views as to, well, I mean, I mean, obviously, this typically is something

32:44.440 --> 32:49.240
that were you ask very useful questions of the person that you're interviewing, and

32:49.240 --> 32:53.440
they hopefully come up with decent responses, but I'm sitting here, left scratching my

32:53.440 --> 32:59.120
head as to the best way to go with deep-lying algorithms in terms of the explanation

32:59.120 --> 33:00.440
of them.

33:00.440 --> 33:07.080
Given this podcast, maybe listen to by people in healthcare, what techniques do you think

33:07.080 --> 33:11.240
we should be considering in healthcare, because let's face it, you have one of the few

33:11.240 --> 33:16.840
people in the world that goes out and speaks to leading people in the area of machine

33:16.840 --> 33:22.680
learning in AI, so I guess from the nature in which you ask questions, you also have

33:22.680 --> 33:25.680
a reader of papers, and you understand how things are coded.

33:25.680 --> 33:29.640
Do you have any idea as to what sort of things we should be considering?

33:29.640 --> 33:37.840
I think the way I think of the explaining deep-learning landscape is that there are a

33:37.840 --> 33:41.000
couple of broad camps.

33:41.000 --> 33:49.000
There's one broad camp that is basically try to fit some explainable model to the output

33:49.000 --> 33:54.120
of your opaque model, and you've mentioned and acknowledged that camp, and the other

33:54.120 --> 34:05.200
is try to gain some insight into what individual neurons are doing by introspecting weights and

34:05.200 --> 34:08.200
things that are happening in the network.

34:08.200 --> 34:11.680
I think that's a step towards explaining, but it's not quite.

34:11.680 --> 34:20.440
I don't think it produces anything yet that necessarily meets the bar of explainability.

34:20.440 --> 34:23.680
I think that's more providing insight.

34:23.680 --> 34:31.040
There have been a couple of conversations with folks that are offering software tools

34:31.040 --> 34:40.240
that aim to one way or another provide the user who's deploying these tools with a dial

34:40.240 --> 34:48.680
that lets them dial in explainability when needed, presumably at the cost of model performance.

34:48.680 --> 34:56.600
But in a case where at the NHS, that dial will be pinned all the way to one end, right?

34:56.600 --> 35:03.640
Then I think you're back to these two types of approaches, and the various methods of

35:03.640 --> 35:10.120
training a decision tree or something like that against a moral-pake model is what I've

35:10.120 --> 35:14.400
heard the most of happening in real life.

35:14.400 --> 35:18.520
I'd love it to happen, but I think the other challenge we've got is when you string

35:18.520 --> 35:23.960
a number of them together, who takes responsibility for the decisions each one of them have made?

35:23.960 --> 35:26.800
That's an interesting thing in itself.

35:26.800 --> 35:31.200
If you've got a dial, how many dials can you afford to be be bothered to press whilst

35:31.200 --> 35:33.360
you're waiting for an ambulance?

35:33.360 --> 35:35.360
Yeah, it's a big issue.

35:35.360 --> 35:39.760
I think time will tell us to whether GDPR had a positive or negative effect on health

35:39.760 --> 35:41.560
care AI.

35:41.560 --> 35:46.920
At the current time, I think we need to spend more time explaining what we're doing and

35:46.920 --> 35:52.320
making less of a bogeyman, so maybe slowing things down as it is doing is a good thing,

35:52.320 --> 35:57.880
so maybe I'm being slightly pragmatic and thinking on the positive, but I think that

35:57.880 --> 36:03.320
wouldn't be a bad thing, and there are plenty of ways in which organisations can make money

36:03.320 --> 36:06.680
with deep learning algorithms and it doesn't have to be in health care.

36:06.680 --> 36:11.920
I think the challenge, however, when it comes when you don't apply, I'll pick algorithms

36:11.920 --> 36:18.400
to problems that are just intractable with any other way, so I'm talking about computer

36:18.400 --> 36:20.600
vision type problems.

36:20.600 --> 36:25.600
So looking at a label data set for say glaucoma images, which is what I'm going to be doing

36:25.600 --> 36:36.320
next month, and building an algorithm that could identify, say, features of those particular

36:36.320 --> 36:40.920
images that might help somebody understand whether glaucoma is early on say glaucoma is

36:40.920 --> 36:45.520
happening, I think the people who would like me to do it need to understand you can't

36:45.520 --> 36:52.160
do it any other way than with some form of a vague algorithm, and therefore the ramifications

36:52.160 --> 36:57.800
are going to be, it needs to be one where it's a very low risk, how wrong can we fall

36:57.800 --> 36:59.960
to be on this particular subject?

36:59.960 --> 37:05.320
I think that's anybody applying AI in this sector needs to think of that first, and I

37:05.320 --> 37:12.160
don't know enough about the subject as I'm expecting to be in May when I get briefed

37:12.160 --> 37:16.240
by the clinician, but that's the first question I'm going to ask is how wrong can we fall

37:16.240 --> 37:17.240
to be?

37:17.240 --> 37:23.640
Yeah, there's a podcast I did recently with Ryan Publin, who's a research scientist or

37:23.640 --> 37:31.640
research engineer at Google who worked on looking at retinal fundus images and trying

37:31.640 --> 37:39.120
to predict, I think it was diabetes in his case, and his research ended up focusing on identifying

37:39.120 --> 37:46.040
potential risk factors, in other words, inferring demographic information from the retinal

37:46.040 --> 37:49.440
scans, like age and...

37:49.440 --> 37:55.920
Yes, right, right, so they looked at a whole bunch of data sets and came up with inferences

37:55.920 --> 38:00.400
about the nature of the person that didn't relate to the condition that they were looking

38:00.400 --> 38:01.400
for.

38:01.400 --> 38:09.240
That's right, and so in that interview, there was an interesting tidbit about how his grappling

38:09.240 --> 38:14.240
with this explainability issue and the context of being able to go back and validate these

38:14.240 --> 38:24.200
results with the clinicians, and what he suggested was that you've got a deep neural network

38:24.200 --> 38:28.880
model, you've got this layer at the end that says yes or no or makes a prediction of some

38:28.880 --> 38:35.480
sort, but the roughest level it sounds like what they were able to do is kind of push back

38:35.480 --> 38:45.240
a layer and maybe train a layer on identifying features that are kind of known contributors

38:45.240 --> 38:53.840
to a particular condition, and it does strike me that maybe that might help us still use

38:53.840 --> 38:57.080
these opaque algorithms, but pass the explainability test, right?

38:57.080 --> 39:01.960
If the algorithm's not making the decision, it's just providing, it's augmenting the intelligence

39:01.960 --> 39:02.960
of the clinician.

39:02.960 --> 39:08.080
I think that I think therein lies an interesting and probably important factor that we need

39:08.080 --> 39:13.280
to consider is that it's different doing healthcare research than implementing systems

39:13.280 --> 39:14.840
that augment practice.

39:14.840 --> 39:21.200
So I'm particularly focused on implementing systems that can augment practice.

39:21.200 --> 39:28.680
The use of deep learning systems or any opaque algorithms that can improve people's understanding

39:28.680 --> 39:33.400
of causation of illness is wholly appropriate.

39:33.400 --> 39:39.120
The difficulty comes as to when somebody needs to understand why that came to that discussion,

39:39.120 --> 39:44.880
but I think it opens up identifying feature sets that people clinicians need to consider

39:44.880 --> 39:51.880
in more detail, as that one did, is highly appropriate for healthcare research and should be

39:51.880 --> 39:58.520
considered, because it wouldn't be implemented in this particular algorithm that's going

39:58.520 --> 40:01.880
to be used for diabetic myopathy.

40:01.880 --> 40:02.880
I believe so.

40:02.880 --> 40:07.480
I'm not sure I got that term right.

40:07.480 --> 40:21.680
Maybe to be more concrete about it to make up some terminology and apply it to this

40:21.680 --> 40:28.760
example, but intuitively there's a big difference between using an opaque algorithm to determine

40:28.760 --> 40:35.600
whether I have the diabetic condition based on an eye image and using an ensemble, let's

40:35.600 --> 40:42.560
say, of opaque algorithms to identify or maybe a single opaque algorithm to identify features

40:42.560 --> 40:48.920
that may be difficult for me as a clinician to identify or time consuming, but to raise

40:48.920 --> 40:54.480
the flags and say, well, there's this kind of vein structure, there are these kind of

40:54.480 --> 40:57.760
spots, there's this kind of coloration.

40:57.760 --> 41:05.440
So this is a candidate that should be explored more fully by the clinician.

41:05.440 --> 41:10.160
So I think the point I'm trying to make is maybe don't throw the baby out with the bathwater

41:10.160 --> 41:12.160
kind of thing here, right?

41:12.160 --> 41:20.240
There's still potential value in these opaque algorithms, but it's really where potentially

41:20.240 --> 41:22.680
where the decision is being made.

41:22.680 --> 41:23.680
Awesome.

41:23.680 --> 41:24.680
Awesome.

41:24.680 --> 41:27.920
Well, Joe, thank you so much for taking the time to chat with me.

41:27.920 --> 41:28.920
It's been great.

41:28.920 --> 41:29.920
It's been awesome.

41:29.920 --> 41:30.920
Well, thank you very much.

41:30.920 --> 41:34.760
I've really enjoyed it and keep up the good work and you're a very good communicator

41:34.760 --> 41:39.720
in the area of machine learning, and if I have my way, lots of people in the NHS will

41:39.720 --> 41:43.600
be listening to this podcast, not to me, but to what you do on a regular basis.

41:43.600 --> 41:44.880
Well, I really appreciate that.

41:44.880 --> 41:45.880
That works.

41:45.880 --> 41:46.880
Enjoy your weekend.

41:46.880 --> 41:47.880
You too.

41:47.880 --> 41:48.880
Have a good one.

41:48.880 --> 41:52.120
All right, everyone.

41:52.120 --> 41:53.840
That's our show for today.

41:53.840 --> 41:59.080
For more information on Joe or any of the topics covered in this episode, head over to

41:59.080 --> 42:03.880
twimmaleye.com slash talk slash 169.

42:03.880 --> 42:07.800
If you're a fan of the pod, we'd like to encourage you to head over to your Apple or

42:07.800 --> 42:14.080
Google podcast app or wherever you listen to podcasts and leave us a five star rating

42:14.080 --> 42:15.680
and a review.

42:15.680 --> 42:20.120
They're super helpful as we push to grow this show and community.

42:20.120 --> 42:34.160
As always, thanks so much for listening and catch you next time.

