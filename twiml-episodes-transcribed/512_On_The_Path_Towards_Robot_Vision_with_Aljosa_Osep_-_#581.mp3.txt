All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,
your host, Sam Charrington. Today, I'm joined by Alyosha Oshep, a postdoc at the Technical
University of Munich and Carnegie Mellon University. Before we jump into today's conversation,
please be sure to take a moment to head over to Apple Podcasts or your listening platform
of choice. And if you enjoy the podcast, please leave us a 5 star rating and review. Alyosha,
welcome to the show. I'm looking forward to digging into our conversation. We'll be talking about
your work in robotic vision and some of the papers you'll be presenting at CVPR. Before we get there,
though, I'd love to have you introduce yourself to our audience and share a bit about how you came
to work in the field. Yes, sure. So first, thank you very much for inviting me. I'm very excited to
be here today and to have this opportunity to present or work that we are going to present
at the CVPR next week. So, I will just start with a little bit of a background about myself.
So, yeah, my name is Alyosha Oshep and I was born in a race and born in Slovenia, quite
small country in Central Europe. As you already mentioned, at the moment, I'm working as a
post-log boat at the Technical University. I'm working with Laura Ljolta-Shea and I'm also still
still working at Carnegie Mellon University. I'm working with Deva Ramana. So, you mentioned that
you would like to hear something about how I have gotten into computer vision research. So,
just curious, would you like to hear a shorter or the longer version?
The medium version is maybe the best. Medium version. Is it something that you always aspire to do?
I would not say always. I think it kind of has gotten gradually to me, but I would say that from
a very early days, I knew that I want to... I was really interested in computer science,
right? And in programming. So, in one way or another, I was interested in that general film.
Okay, what was your first exposure to vision and machine learning in AI in general?
So, I would say that my first exposure to vision, ML and AI was actually at university.
I had some of the courses on computer vision, computer graphics, and the machine learning already
during my bachelor's studies. But even before that, I got interested in computer programming
as a kid. So, at the beginning, I was interested, you know, like as a kid, of course, I was playing
computer games, right? And I was just really, really fascinated with games and all these virtual
worlds that come together with computer games, right? So, first thing that I really want to understand
is how... When I was still quite young, right? How this comes to be, right? What is the logic behind
all this? How one makes computer game and virtual worlds, and you know, like all these characters
that appear in the game and behave like in somewhat intelligent way, or at least that they appear
that they behave in intelligent way. So, this is really how I got into more general body
direction. So, through interesting computer games and computer graphics. But when it comes to
computer vision and machine learning, I think that some of the first memories I have about this
come from the time when I was doing my bachelor's studies in Slovenia. So, you know, that the one
way you can think about graphics is it's synthesis, right? You see in these images, right? A computer
vision is kind of opposite process, right? You get images and you have to understand what these
images are, what in the world has generated those images, right? And I really remember, like,
back in the days I was watching, I know this is a bit nerdy, but I really liked Star Trek, right?
And one of the coolest things, and one of the coolest things besides traveling around space and,
you know, like meeting alien civilization was also this holiday, right? I just find it so
fascinating that they have like this immersive game that just generates context content and,
you know, like you can interact with characters that computer generates and so forth. So,
this seemed really cool to me. But I still remember there was this one episode that I find so
fascinating where they were trying to recreate something and they have some image of, I don't know,
I don't remember exactly what it was, but they have some image and they just fed this to this
holodeck and holodeck kind of created 3D reconstruction of that image, right? Then they were analyzing
what was happening with this and all that, but I just remember this idea, you know, feeding
image in and getting 2D reconstruction out, right? So I thought this was really cool, yeah.
I should mention that you're doing your postdoc with Laura Layout, I say, or you mentioned that,
but I should mention that I interviewed her around this time in 2018. I'm sure it was CVPR
related given the timing and it sounds like in some ways you're carrying on the torch, so to speak.
In talking about your research interests, it's not just computer vision, it's robot vision,
you know, what kinds of problems are you trying to solve?
I would say that in broader sense, I'm really interested in 3D dynamics in understanding,
right? Also similar to research interests of Laura, so you assume that you have some mobile
platforms such as the robot, it's equipped with some sensors, right? It can be camera, one camera,
two cameras or several, or and lighter and based on whatever sensors you have, you should be able to
get as complete understanding of the world as possible, right? So you should understand the
3D geometry of the world, right? Both static and dynamic, right? You should know where objects are,
right? Where cars are, where pedestrians are, and so forth. In addition to that, it's also important
that you know how these objects move over time, right? And the main reason why it's so important to
know how objects move or how objects move in the past, which really comes to multi-object tracking,
but the main reason why we want to know this is that we have to make predictions about what happens
in next seconds, right? You just can imagine that if you are walking around city or if you are driving
a car, what you end up doing most time is navigating into free space, right? So this means that you
don't only have to understand the static scene geometry, but you also have to know where everything
that moves will be in a few seconds, right? So at the high level, this is what interests me a lot,
but if I get more specific, I was really fascinated with one particular question in past years,
already during my PhD, and I'm still working on this. So you probably know that nowadays,
nowadays, I would say that object detection, tracking, and even forecasting models work quite well,
all right, right? We've made a huge progress in recent years, right? But there is one important
limitation here, and this is, it works great as long as you have enough data for particular
semantic classes that you're trying to recognize, right? So for example, if you go out record data,
you will see lots of cars and pedestrians, right? You label them, use them to train models,
and everything just works fantastic, right? You can recognize cars, right? But we also have this
long-tail distribution of semantic classes, right? And lots of semantic classes appear in this long-tail,
which means that most objects are observed very infrequently, or maybe even never when you're
collecting data sets, right? Which just means that during your model training, you don't see
some particular object classes, but you still have to recognize them if you really want to navigate
around the world, right? Otherwise, otherwise, you might, otherwise, it might be really dangerous,
right? If you didn't recognize something because it's, it looks like not like anything you have
seen before. What kind of approaches are you exploring to address this long-tail semantic detection
problem? Mm-hmm. Yeah, so we explored quite quite some approaches, quite some approaches in the
past. So the way I started looking into this, into this back then was so my intuition was that
to track any object, we can start with something that is based on a bottom-up scene understanding,
right? So current standard object detection works more in a top-down fashion, right?
So existing object detection models. And bottom-up approach is more that you get image and you try
to figure out how pixels group together so that you obtain object instance, right? So which pixels
group together to obtain, for example, pedestrian car or something gathered, you don't know quite
what it is, but you know, based on pixel similarity, you should be able to group these pixels together
and then possibly, you know, realize that object is there and then track these objects, right?
And that's bottom-up. What is top-down? Top-down approach is, for example, the most well-known
top-down approach is, for example, faster RCNN, right? You have a bunch of windows,
or we call them region proposals, right? And then based on these region proposals, you try to
estimate what these proposal covers, right? Is it a car, is it a person, or is, you know,
part of a background? So you don't go from bottom-up, from pixels up, but you go from top-down.
So you're looking at region, yeah, exactly, exactly. So we started looking into this bottom-up
approaches, but very, very soon we actually kind of started switching back to top-down data-driven
methods, but instead of trying to look at region proposals and classify them as one of the
object classes that were available in data sets such as, is it a car pedestrian cyclist?
You would just train the tactors to just tell us whether this region likely contains object
or not, right? So basically, data-driven, we started relying on data-driven object proposals
to initialize tracks, right? When the queue that we were also looking into at the very beginning
was depth, depth estimates, right? So when the issue with this object proposals are, is that
they're quite noisy, right? You will end up having like a bunch of object proposals, some of them
will actually detect objects that you have in our scenes, but many of them will also just
might be firing on, you know, certain background region. But the next queue you can look into
is depth. So if, you know, if you have region proposals that is kind of supported with depth,
estimates, this gives you a strong queue that they're actually very likely. It's an object
that you should start to start to trajectory. Maybe kind of popping back up a level or several.
When you think about robots, are you primarily or robotic vision? Are you primarily thinking about
autonomous vehicle types of use cases? Or do you have, do you even have a particular
ideal in mind in terms of the robot itself? Always when I talk about research, I have a
wider vision in mind. So I don't only think about autonomous vehicles, I think about
any type of robots. And I even think that, you know, that we should have perception system
that should be applicable for robots that want to drive on highways, that want to drive in the
intercity, inner city areas, in potentially very crowded areas, right? Then we might have
delivery robots that might even, you know, like merge into more of a pedestrian urban areas,
right? Or robots that might need to navigate around warehouses, airports and so forth, right?
So I kind of think that robots, the robots we need to be everywhere in the future. And I think
that perception systems that we develop should be general. But one thing I definitely have to admit
is that very often when you write papers, we kind of focus on autonomous driving applications
and scenarios. And one reason for that is that we just simply have most datasets that were captured
for benchmarking and training algorithms for autonomous driving. I would love to have more
datasets, actually, more general datasets. Do you have a sense for some of the ways that
focusing on autonomous vehicles based on data set availability, you know, limits applicability
of the work to other types of robots? I think that there definitely is certain bias if you are
only looking at autonomous vehicle datasets, right? So for example, these datasets are obviously
always recorded outside, right? That were inside. You might, for example, also over a lie on the
fact that you can localize yourself outside very well with GPS, for example, right? To localize
your poses might be impossible indoors, right? Very often is. Then very often, if you look at
limited scenarios, such as highways, for example, your diversity of objects that you will observe
will actually be quite limited, right? So you learn how to recognize cars, trucks, buses,
and a few other classes, and you just think you know how to handle everything, right? But
this is definitely not true, right? Our usual word is very richer than that. Speaking of localization,
one of the papers that you're presenting at CVPR is called text-apause, text-to-point cloud,
cross-modal localization. Let's dig into that a little bit. What's the kind of big picture
problem that you're trying to solve with this paper? I would actually start a bit with motivation
for this or before I even do that, let me just already tell you upfront that this paper is about
to localizing position within the map, 3D map of the environment based on an actual description
of what this is surrounding, right? You can imagine yourself that you are somewhere in the city,
you don't know where exactly you see some things around, right? There might be church in front of you,
some trees or something like that, and you're explaining to your friend what you see and your
friend should realize where you are based on this description, right? If your friend of course
knows the rough environment of the city where you're in, right? And yeah, I imagine that in the
future robots will of course be used for many things such as, for example, food delivery, right?
Or no like instead of Uber drivers picking us up to transport somewhere, just robots will come
to pick us up, right? And sometimes GPS tech works great, but not always, right? And one thing that
for example is particularly realized is particularly frustrating usually where we are somewhere at the
campus, right? Let it be technical university of Munich or Carnegie Mellon campus, we often have
meetings and we use food delivery services to deliver us food, right? And you would think that this
is easy to find this, right? But it turns out it's really not. So what we end up doing, for example,
to if you use door dash, for example, you write a detailed description on how to reach us, right?
And even after this detailed description, where, for example, Smith Hall is, where I was working
until recently, they would still very often get lost at the campus and call us and ask for more
directions, right? And so it's always a bit of a hassle. So this why we also envision that
when the robots will take over, we also need to find a way to communicate with them where they
need to go, right? And we want to do this communication in natural language because natural
language is what humans are used to communicate. So this is like more like overall vision, right?
Now, now if I get more to the task, this paper is addressing. So here I have to say upfront that
this is one of the first investigation of this problem, right? So we had to take quite some
short cuts to making this investigation of this problem physically, right?
Just hearing the way you describe the problem strikes me that there are, you know,
of course, multiple ways to come at it. You know, one is, you know, the paper is called
Text-to-Point Cloud can also envision like text to landmark somehow trying to localize not necessarily
where you are, but the landmarks that you're describing and somehow triangulate from that.
You know, talk a little bit more about the way you set up the problem.
Sure, sure. So you exactly, so this comment was just spot on because this is this is actually
part of what the what the method does, right, triangulating landmarks, right? I mean, this is
the second stage. So we have like course localization stage and file localization stage, right?
So what you just mentioned comes into the fine phase. I will just first touch the course
phase and then then I'll get back to this, right? So in the course phase, we basically just
split the 3D point cloud of the city or of some neighborhood into rectangular tiles, right?
They're, I don't remember what exactly the size is, but you know, it's like some
a bit larger area that you first have to find, right? So first, you want to find like this
rectangular area in where you very likely you are located, right? And when you say tiles,
are you thinking like open street map tiles or something along those lines?
No, we actually have in practice a lighter point cloud. So maps that were built from lighter point
clouds and the data set we actually use for that is the new kitty 360 data set from a research
group of Andreas Geiger. But this this data set of course contains only this point clouds maps
and no textual description, right? But this is this how we actually generated data set. It's
it's a different story. I'm curious, you're, you're starting assumption is are you assuming,
you know, no GPS or, you know, course GPS and you're using that for the first phase and then
you're using the text base for the second phase. So this is also a great question and actually
we assume no GPS. The only assumption we make is that we have instance segmentations available
in our map, right? So that we have, you know, we know where the houses are, we know where the trees
are and so forth. So we assume instance and semantic information in our maps.
And how fine-grained are your classes? Like do you just have building building or do you have
church super market, campus building, whatever? No, we don't have that fine-grained information.
It's more like building tree car roads and so forth. Okay, all right, so go on and finish with
the first part of the method. Yeah, so so first part is just course localization. But you could
also use GPS for that. That's true, but since we didn't have GPS, we just pose this as a retrieval
problem, right? We have textual descriptions, we have our point-code patches, and we just learned
joint embedding space for both, right? Similar to how clipworks where they aligned images and textual
descriptions, but you align point clouds and textual descriptions. And based on this learned
joint embedding space, you can then based on textual description get like a list of most
slightly sales that contain your objects. So this is the course localization step.
It strikes me that there's like a I don't know something that I think of as like a bootstrapping
problem like, you know, if I'm giving directions, I assume a certain amount of knowledge that
would be hard to get from instance labels like on the wash you campus or you know, some place
that I am at a Starbucks or something like that. Is that kind of knowledge being introduced
somewhere? Not yet, and I think this is right now the biggest limitation of our method. So
to incorporate such a fine-grained information, we would have to go step forward, step further,
and align our point clouds with something like you mentioned earlier, open street map, right?
And then you actually get access to such fine-grained information, right? What kind of building is it,
a Starbucks sign, and so forth? But in this paper, we didn't have that. I mean, we consider this
as a future work, but we haven't. You have to start somewhere. But it sounds like then
conclusion would be that you're trying to localize within a relatively constrained area.
I'm not sure what with constrains, because in principle, this should work anywhere in the city area
for which we have a map in form of point clouds, right? But of course, there is this restriction
that if I just tell you that I see, I don't know, traffic side in front, church on the right,
sending us here and there, right? This description fits several locations in the city, right?
So you have this uncertainty, right? Of where you are. And if you wanted to make this non-ambiguous,
you would really either have to go, for example, GPS, it coercely localizes you, right?
Then this narrow down the search space, or you would have to have
information such as, is there a Starbucks next to me, or a street name, or something like that,
right? Something that really is a very narrow down the initial search state, right?
Or even a dialogue-based approach where the system can identify
differentiating characteristics of the three places and ask you, do you see a street sign near
you or something like that? Yeah, that's also, I think it's very important that we incorporate
into this process in the future, because of course, this is how conversation with humans work,
right? It's not like you just described where you are and then the other person just, you know,
click those, right? Right, right. Okay, so that's kind of the course part of the course phase of the
method, what's the fine phase, or the second part? When it comes to the fine phase, we do
pretty much exactly what you said earlier, right? So based on descriptions, find the landmarks that
you refer to, right? So, you know, if you mention a sign, then, you know, the network has to realize
it's, there was a sign mentioned in the sentence and align this with instance of a sign within your
rectangular area, right? So you kind of match words that refer to objects with instances,
and once you do this matching, you, you regress offset to, to, to, to, and is success for the model
returning a list of possible locations, or you then further trying to, you know, guess or predict
which of the locations the user, do you apply some kind of confidence to a list, or are you
only predicting one with a confidence level, like what's the output of the model?
Yes, so we predict several possible locations, and the reason for that is the course
localization step that is incurring here in certain, right? So you might be in several cells,
right? So you get ranked list, and then evaluation is, the evaluation is also done with respect to top
K hits. You know, so you look like, you look like a localization, sorry, you look at localization
retrieval success with respect to top five, top 10, or top 15, uh, top 15 matches. How did you evaluate
the method? So we actually, uh, we actually evaluated a method on, um, data set that we, uh,
generated on top of, uh, kitty 360. Oh, right. We've now come back to the data set
conversation. So how did you create the data set? Yeah, so this, this is actually, uh, also one of
the main parts of the paper, right? Because in theory, this sounds like kind of difficult,
because if you need that, uh, list of, um, annotated poses and someone writing descriptions,
this would take lots of time and lots of researches, lots of researches that we don't have, right?
So you kind of have to find, you have to find the heck away around this. And what we ended up
doing was, uh, so you know, I already mentioned we had kitty 360, list of instances and semantic
meaning. And we also have, uh, color channel available, right? Because they, they align images
and, and library points. So what we ended up doing was we sampled a bunch of points all around
cities. And then we looked at what, uh, what, uh, what, uh, instances are in a spatial neighborhood,
right? For this, you really need 3D data, right? And then if you know that, you know, that there is
a house, uh, house in front of you, then you can kind of generate based on some language template,
uh, description, uh, that, um, uh, that mentioned these objects, right? And, uh, you can, uh,
you can also mention something about color, for example, uh, of the object that is front of you
based on, uh, extracted RGB color from, from instances or, or semantic class, right? You also
have semantic classes. So in a sense, it's kind of, uh, synthetically generated dataset. You
pick a point and then you can come up with a list of possible descriptions that refer to that point.
Exactly. Yeah. Oh, nice, nice. Is the, the model kind of end-to-end train from natural
language to the, the list of predicted locations? Or are you kind of tokenizing the natural language
and trying to identify the landmarks as an intermediate step? Yeah. So, so model is actually
trained end-to-end, uh, I mean, okay, sure, uh, the course and the final two steps are trained
independently, but other than that end-to-end. So we have, uh, you know, we have point clouds in
coders, point cloud encoders and, uh, and the language encoders. Are the language or point cloud
encoders are using any off-the-shelf, you know, other models, uh, for, for those tasks? Cannot
think of any right now. I mean, our airing, our encoders are fairly simple. Uh, so for the, for
the text, uh, we, we are actually just using LSTMs, not really any of, uh, strong seat of the art
encoders. And, um, for, for point clouds, use point net. You talked about the, the dataset that you
created to train the model, you know, that suggests that maybe you, you, you didn't have any external
benchmarks to compare your performance to, um, but maybe you're publishing this and hoping to get
other folks kind of using the same data set to, to create, uh, um, you know, kind of compete for,
for best performance. Is that kind of the thinking? Mm-hmm. Yeah, this, this, this is exactly what you did,
right? So when, when we started investigating this, there was no, no really data sets or, uh,
or, uh, you know, prior work to, to compare with. So, um, this, uh, this is, uh, this is kind of a
first investigation into, uh, into this, uh, into this topic. Yeah. And do you think your initial
approach did pretty well? And, uh, you nailed it. And there's not a lot of room for folks to,
to one up you or, you know, is it kind of a rough start and there's lots more room to, to go to,
to do better on the dataset? Uh, yeah, I would say that absolutely the latter. I think that,
I guess that's the humility test, right?
Yeah, I, I actually see it more that this, this investigation, uh, that it actually opens more
questions than, uh, than answers almost, uh, I mean, in the sense that, uh, um, you know,
it, earlier, we just talked about this course localization step, right? For example, here, uh,
this, this was one of the outcomes that this is really the bottleneck, right? If you can, of course,
it localize yourself correctly, then, then you, then you become pretty accurate at localization,
but, uh, uh, this, this is definitely right now on the bottleneck. And I think that this is,
it will be the first thing that people will have to look into how to improve. But of course,
the question is now, I mean, of course, I'm sure that people can come up with better networks,
right? And the given data that we have can, uh, can, uh, uh, uh, localize, can improve localization
scores, right? But there's also here just, there's also just so far you can go because there's
this inherent uncertainty, right? Based on one description and not like very precise information,
you could be in any of, uh, locations if you have big area to cover, right? So I think it will be
really important to, uh, as we talked about earlier, right? To, uh, align this with OpenStreetMaps
and, uh, and rely on, uh, more, uh, more discriminative and unique cues for, for describing
location. Uh, second paper that you are presenting at CVPR is focused on forecasting from LiDAR,
via future object detection, or at least that's the title of the paper and the focus is kind of on
joint detection and trajectory prediction. Um, talk a little bit about the motivation for that one.
Sure. Motivation here, um, comes directly from, uh, from robot navigation, right? So a, a, um,
I was talking already earlier about object detection and object tracking, right? Um, so, you know,
object tracking is all about understanding how, how object moved in the past, right? But if you
really want to navigate, it's not important to know where objects were. You have to know where
objects will be, right? And now one way you can tackle this, and this was already tackled in
the community, is that, you know, you're detecting objects, you're associating them over time to
get tracks and then based on past tracks, you can, you know, like train some possibly
outdoor aggressive model that gives you prediction, your object will be, right? Um, but, um,
there are a bunch of problems associated with that, and one of them is that object tracking is
very difficult by itself, right? And the question is, even do you really need, um, the really needs
object tracks to do forecasting, or could you just, you know, encode a sequence of point cloud and
just train network to directly, um, detect objects and predict where they, where they will be,
right? I mean, I'm not saying that tracking is not important. I think it is, but it might be that
model actually implicitly learns by itself what it has to know about past positions of object.
So tracking kind of becomes implicit in, in this case. In a sense, it's, it's a similar
uh, bottoms up versus top down type of, uh, distinction that we were talking about earlier.
The current approaches are kind of top down and that they're trying to identify object
instances and then they have to track them, you know, and they might be included for periods of
time and things like that. And it gets really hard. And your hope is that, hey, just feed it a bunch
of point data and let the network figure it out. It's, uh, I feel that this is kind of, uh,
kind of a lesson learned over and over, right? Just, uh, feed, feed enough data and let,
let the network figure out things by itself. Don't, uh, don't mess with that too much. And,
I mean, I also have to say that by this, I'm not saying that, uh, track is not important.
Right? Tracking is important for a number of, uh, other applications, but when it comes to,
to navigation, it really might be that, uh, that what we need to look into is, uh, forecasting and
not necessarily so much, uh, so much tracking. Although, on the other hand, it's still nice to have
this interpretability aspect of this, right? Uh, to, uh, that, that you get with, uh, with tracking.
And this is maybe jumping way ahead, but, you know, maybe there's some kind of multitask
objective where you're trying to do forecasting, but have tracking be kind of a byproduct of the,
the network that, um, you know, has some of the benefits that multitask learning can provide.
Yeah, this, this makes perfect sense, especially if this hypothesis, uh, that I mentioned earlier,
that if you know how to do forecasting, you somewhat have to know some idea about tracking implicitly,
right? If this hypothesis is correct, then features that we learn to do forecasting should also
benefit tracking, right? So, um, I don't have answer for this yet, whether this is so, but, uh, I think
this is definitely something that we should pick into. So talk a little bit about your method.
Yeah, so, uh, first thing I should say is that, uh, our method is not the first one that tackles
end-to-end detection and forecasting. Uh, there, there have been very interesting paper in this topic,
before, for example, from a group of Raquel Lourdeson from, um, uh, University of Toronto.
And, um, one thing that is, um, quite unique, uh, with our method is that, um, that it actually
offers multi-future interpretation, right? Forecasting is not something that, um, I mean,
forecasting is as, just as in the paper that we were talking about earlier, right? It's inherently,
uh, inherently, um, ambiguous, right? Uh, based on past velocity, you could make several
guess where objects will be, right? And, uh, or, or method is actually capable of that. And
the way that, the way that we achieve this is actually, in the end, really simple. We just
repurpose object detectors for not just detecting objects in the current frame that we just observed,
but also, you know, future frames that we haven't observed yet. Um, so, you know, by encoding,
by encoding temporal sequence, you can just feed the sequence to the network and then say, hey,
detect objects in current frame, but also in future time steps, right? So you just have,
you will have multiple detection heads for future time steps, time steps, and you have supervision
for that radar, you're available, right? But, um, what is nice with this is that, uh, you can then go
instead of, you know, like doing forecasting from time t into the future, you can do something
different and you can go into the future and backcast, right? So from the last detection,
you can backcast vector to previous frame and from there to the previous frame and so forth,
and then you will also have, um, uh, one to many mapping, right? Um, so it might, it might be that,
uh, to one detection from t to t minus one, two detections might connect to this one, right?
In this sense, you have one to many mapping, and this kind of defines this tree of possible,
possible splitting paths. And if you can connect your backcasts all the way back to particular
detection, then you can say, these paths are my forecasts, right? Maybe I'm reading too much into
this, but does this kind of open up some of the graphical types of tools to you? Actually,
we didn't use these tools, but, uh, this, this is one thing that we were definitely thinking next,
that, uh, this is also something that, uh, that we could do even more rigorously by relying on
the graph neural networks, right? What, what we did was actually something, something rather
simple. So we just have multiple network heads that detect objects in future frames and then we
just from each detection and regress a single offset vector backwards and then just based on
a credient distance, uh, say whether some detection based on back-asset vector connects with some
detection from the previous frame. So in, in the end, the, uh, based on a credient distance
of the regressed offset vector. For what did you use for, for data set here? Um, I'm assuming
based on the description that you needed to have, uh, labeled, uh, objects in the, the frames.
Yeah, that's, that's exactly correct. So we used a new since data set. So this is, uh,
one of the, uh, the popular automotive data sets that, uh, that we're using nowadays.
Um, and, uh, yeah, for this, we, of course, need, uh, supervision in the form of, uh, 3D bounding
boxes and, uh, an object tracks, right? Um, so, so we, we, we built on, we built on that.
So in the case of forecasting from LiDAR, uh, and the object detection, uh, you mentioned earlier
that this is a problem that has been well studied and there are existing methods that, uh, that
have been developed to solve this problem. How did your method perform relative to the existing
work? Um, well, I have short and long answer. So the short answer is, it works better. Uh,
okay. The, the, the long answer is, and this was also a really big part of this paper that, um,
metrics used in the past for this problem, for end-to-end detection forecasting, were actually not
the, the right metrics for studying this, uh, this problem. And we actually have, uh, big, big part
of the paper showing this that this metrics can be trivially fooled. And we also have proposed,
new metrics that, uh, that don't have, uh, that don't have, uh, those issues. Um, so I'm not sure
into how much details you want me to go into this because this is something that, uh, I could,
you're actually talk a lot about, but, uh, uh, uh, I'm, I'm curious. I'd like to hear a little
bit more. I mean, it sounds like if the metrics can be trivially fooled, then your method
performs better on both the old metrics and the new metrics. If nothing else, you could
trivially, trivially fooled old metrics. Yeah. Yeah. So this is, this is correct. Or, or method
work, well, on, on both metrics, what sets of metrics, but, uh, we had a, the reason, the reason
why we were looking into evaluation here is, so, um, previous metrics were adopted from, um,
from more traditional forecasting setting that we're studying in the past years. And the other
setting was that someone gives you ground truth trajectories and based on given perfect ground
truth trajectories, just, you know, uh, predict continuation, right? But this is not the case
in end to end forecasting, right? Because you don't have previous trajectories. Yeah. Yeah. So in
this setting, very half past trajectories, uh, they used metrics that are called, um, uh,
average and absolute displacement errors, right? Um, so absolute displacement errors error is quite
simply looking at you, you, you look at your prediction, you look at the ground truth prediction,
and if they're close enough, then, you know, you need to go to forecast, right? But the farther
way you are, you are from the ground truth, the more you are penalized, right? Uh, and, uh,
another, another, um, uh, metric that is usually used, uh, uh, penalizes for, uh,
penalizes, uh, false, false forecasts. So forecasts that are nowhere close to any ground truth,
truth forecast. And, um, when, uh, one thing that, uh, community did when they started looking
into end to end forecasting was that, um, they, uh, evaluated separately object detection.
And then they evaluated, um, this absolute and average, um, uh, trajectory errors with respect
to certain detection recall, right? For example, for 60 or 90 percent recall, right? And what can
then interiory happens is that you train object detector in a way that the detector will focus
on detecting objects for which forecasting is easy. And there is particular type of, uh,
forecast for that. And this, and this is namely our objects that don't move at all, right?
The, the, are just static there. And what is even more problematic is most objects don't move,
right? If you're driving through the city, you see parked cars everywhere, right? And we actually
have shown that it's possible to trick these metrics by a very simple baseline. And this is,
so called, no object moves. Exactly. Exactly. So, so we showed that this was the best approach,
right? Better than, uh, anything proposed in the literature, which, which is of course not
the case, right? Those methods, uh, those methods are great. It's just that the metric was wrong.
I mean, not wrong, but, um, you put the data set or for real world. Yeah. Yeah. Exactly. Exactly.
So our intuition was that we should, um, somehow find a way to, you know, you know, like,
not, uh, evaluation of detection and forecasting together, right? And, um, uh, intuitively,
you know, if you look at object detection community, they really have great evaluation tools,
right? I mean, I mean, average precision is, you know, great metric that's why this virus has,
you know, survived the past test of time, right? So our intuition was that, uh, we should use
in one way or another, uh, MAP, uh, mean, average precision. And it turned out that this, uh,
this, we can do this quite easily. Everything that we need to do is, we need to change, um,
um, the matching criterion, right? What is considered to be true positive, false positive and false
negative, right? And what we said was, um, if you correctly detect an object and correctly forecast
that, right? And these two, um, so that you both have correct detection and correct forecast,
then you have a true positive. So this, uh, this metric we call forecasting, uh, forecasting,
uh, mean average, mean average precision. So I wanted to, uh, make sure we cover the third paper
as well. Uh, you are a busy guy at CVPR. That one is opening up open world tracking. Um,
yeah, tell us about that paper. What's the motivation there? Yeah, gladly. So this, this is
actually really paper that I'm extremely excited about because, uh, uh, because the motivation
dates way back in days when I was still doing my, uh, my PhD, right? And I already touched earlier,
this problem of, uh, that you need to be able to track any objects, right? We can't expect that, uh,
we have training data for, uh, for everything. And during my PhD, I was working on trackers for
tracking any object, but once, but here we really have fundamental problem, right? You would come
up with the model and then you have to say, how good your model is, right? But how, how you're
going to do that, right? Back then, a few years back, we didn't have data sets or benchmarks or,
you know, even the right metrics to talk about this. So I always had to find some
hacky ways to, you know, make a point that's, uh, what we are proposing works, right? But, uh,
there was never like really a right rigorous way or, you know, benchmark that would really
help community to rigorously evaluate method study progress, compare different methods,
apple to happen. So, um, this, the motivation for this paper is really to come off with tools
to do this so that community can, can make progress. And this is what this paper largely is about.
So this paper doesn't really propose a drastically new method for this. It's more,
more than that, it, uh, rather proposes a test bet for evaluating this and kind of, uh,
we have experimental evaluation that records holidays, different, uh, different contributions
in tracking, but it is stealing out a good tracker, a good simple baseline for, uh, for this task.
The tracker is a metric or a tool and environment. We actually, uh, provide, um, a benchmark for
this and this benchmark consists of a data set, uh, evaluation metric and baselines, right? But
the data itself, it's not something that we, uh, recorded. For, for data set itself,
we repurposed, uh, tau data set. It's called tracking any object data set and it comes from
our, uh, collaborators, uh, back then we were collaborators when we started this, I wasn't
at Carnegie Mellon yet. Uh, so this comes from the group of, uh, of, uh, Deva Ramon, uh, whom I'm,
also now, uh, working with, but, um, um, tau data set by itself was released to study tracking
in the long tail, right? But not in the open world. And with this paper, we repurposed it for studying,
studying object tracking in the open world. And what we did was, um, we, we split training
and test data as follows. We, for training the models, we proposed to use
cocoa data set that has labels for 80 object classes, right? And then in tau data set, we have labels
for, uh, several more, uh, objects, right? For, for hundreds of, uh, object classes. And we then
define a test bed such that, uh, you train your models with knowledge of 80 classes, then you do
validation on additional set of semantic classes, right? Then you have additional test set in which you
have semantic classes that don't even appear on validation set, right? So that you make sure that, uh,
nothing is, you know, kind of leaking from one speed or of another. And here is actually one
important distinction because usually you make sure that when you have a training test and
validation speed that the data does no overlap, right? And you also have to make sure that, uh,
that semantic classes don't don't overlap, right? So that in each set, you have semantic classes that
you can say, okay, these are the unknowns. And I, uh, I didn't train anything or tune any parameters
with knowledge of, of these classes. Got it, got it, got it. So the, when you say open world here,
are you using that term colloquially, or does that have a specific meaning, or does it refer to
a specific environment in this context? So, so the term itself open world is of course borrows
from the community, right? You, you probably know about this, uh, this classic work from, uh,
um, or Bolton, uh, Bendale, right? Where they studied open world recognition. And they're,
they're, they study, study open world recognition in a sense that, uh, you know,
you have some certain close set of object classes in which you train your model. And then, uh,
during the deployment phase, you will see new objects you haven't seen before. You have to
recognize them. Then you kind of have to ask annotators to label them, right? Um, so we borrowed
part of this, um, part of this idea behind this, right? So we also have some, uh, closed world
data set, right? In which we have some finite set of classes labeled. And then we study, study
performance of those models in the open setting, in which classes that we hadn't seen during training
also appear, right? So it's describing the setting. I think I was envisioning like a simulation
environment that was a world that an agent would explore kind of thing, but that's not really what
we're talking about here. Did you also, so you provide this data set, uh, and this kind of benchmarking
approach, did you also, uh, provide, uh, kind of proof of concept models, you know, did you kind of
bootstrap the effort? And what does that look like? Yeah, so this, this I would say it's, it's
something that is rather simple and builds on really on contributions from multi-object tracking
community. So, uh, the tracker itself is at the end of the day really follows this tracking by
detection paradigm, right? Which means that you have some kind of object detector that gives you
possible object detectors in each frame. And then by some means you connect these detections
over time, right? Uh, but they're of course certain important differences. And first one is that, um,
you're, uh, that you cannot really, you know, train object detector for object class for which you
have no labels for, right? So, um, this object detector is a bit more like object proposal generator.
I touch this briefly at, uh, at the very beginning. So we just repurposed mass carsia and then,
so in particular, the region proposal mechanism of mass carsia and then, but, uh, but we also made
sure that we obtain, um, object instant segmentations for, for each proposal, right? And these proposals
are then kind of, you know, you have lots of them in the image and they are like kind of input to
the, to the tracker and then tracker, uh, figures out over time which proposals are temporally
stable, right? Which it can connect over time and those will be then or, or, or object tracks.
When you describe the, the simple method, uh, kind of this, this proof of concept method that you
ran against the benchmark that you're proposing, it sounded like what we described as kind of this
top down, uh, traditional approach where you have these objects and you're trying to track them
across time. Uh, the, you know, objects from bounding boxes that you're trying to, to track across
time. When we previously spoke about the, um, forecasting from LIDAR paper, we kind of contrasted
that top down approach with the bottoms-up approach that, you know, starts at, you know, point cloud
or in another setting, maybe pixels. And I'm kind of wondering if the methods that you developed
in that paper could be a future direction for someone tackling this open world tracking problem
or are they, you know, I'm really trying to test my understanding and see how they fit together
or are they totally unrelated and I'm off here. No, so it's, it's, it's, uh, it's not, uh, it's not
totally unrelated. So, uh, there, there are some gaps, definitely because what I was talking about
earlier at the beginning, it was mainly 3D computer vision and it was, uh, largely, uh, relying on
3D sensors, right? And it's opening up open world tracking is purely image-based. So, uh, here,
here we haven't talked about 3D at all, but there is one thing, you, uh, that, uh, if, if I
understood correctly, um, that, um, you were asking if, um, we could, uh, instead of doing this
separation of detection and then linking things, uh, to do something more like we did in the previous
two papers and just do everything in end-to-end manner, right? Um, and, uh, indeed, the community,
community is definitely moving in direction of doing multi-object tracking in end-to-end manner.
They are, they are several very nice, uh, approaches out there to do that, uh, methods that
base, based on graph neural networks or just, you know, regress, uh, targets or, or use, uh,
end-to-end transformer-based detectors for tracking simulation. So, there are many of them.
Um, but, uh, this particular dataset and challenge, uh, makes things especially difficult,
uh, because most tracking dataset before focused on tracking pedestrians or cars and pedestrians,
right? And you will have a few objects in the scene and, uh, shorter sequences, and this dataset
is really huge and you have jungle of objects pretty much, right? And, uh, we just, uh, haven't
been able to, to apply those methods, methods to this problem, uh, this problem yet. We, you very
quickly run into issue with, uh, memory and, uh, training time and, and so forth. So,
I think that the method that is closer, closest to being end-to-end is, uh, is tractor,
that also comes from, uh, from Laura's group. Uh, but, uh, tractor was, uh, was, uh, um,
was, uh, lagging behind, uh, uh, the baseline that you proposed.
Well, Aliyasha, uh, it's been really wonderful learning a bit about your research and CVPR
papers. Um, we'll, of course, have links to the papers on the show notes page, uh, for folks to
check out the full details, uh, but wanted to thank you for taking the time to join us and share
a bit about, uh, what you've been up to. Yeah, I would, I would also like to thank you for, uh,
for inviting me here. I, uh, really had a great time, uh, discussing, uh, or, or, or recent research,
so it was a really nice discussion and, uh, I'm, uh, um, I hope to see you at CVPR as well and, uh,
also, also encourage audience to come to, or posters or just if you see me anywhere, just
wave and I'm always happy to chat about research and your end of time. Awesome. Thanks so much.
Thanks. Thank you. Bye-bye.
