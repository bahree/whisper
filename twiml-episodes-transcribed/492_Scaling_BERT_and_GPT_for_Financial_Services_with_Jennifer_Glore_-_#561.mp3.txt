All right, everyone. I am here with Jennifer Glor. Jennifer is the VP of customer engineering at
San Bonova Systems. Jennifer, welcome to the Twoma AI podcast. Thanks for having me.
I'm really looking forward to digging into our conversation. We're going to go deep on GPT-3
and Transformers in the Enterprise in industries like banking and finance. But before we do,
I'd love to have you share a little bit about your background and how you came to work in AI.
Sure. So I've been at San Bonova Systems now for three years. The customer engineering team
is responsible for the technical customer journey here from pre-sales all the way to
post-sales deployment and customer success. Prior to being at San Bonova Systems, I was at Sun
and Oracle, where one of our founders came from. And I had a varied background there where I
worked with products, partners, customers. The last few positions I had involved engineered
systems and cloud deployments for mission critical applications. So AI was up and coming for some
time. It was an interesting technology and I was itching to get involved in that. And when
the opportunity at San Bonova presented itself to combine my technical expertise with customers,
it made perfect sense. That's awesome. That's awesome. I've been following the company for a while now.
And a lot of my early conversations with the folks on the team there, Marshall, Rodrigo,
and others have been around the hardware vision. Creating a new chip architecture to support
machine learning applications. But more recently, the company has been focused on this
GPT-3 thing. Where did that come from? Give us some context for that part of the journey.
Yeah. So obviously San Bonova is an AI accelerator company and part of that is our own chip and our
own system. But in all honesty, the premise for the company is a combination of both hardware
and software. And hardware is only as good as the software that it runs on. And so we are actually
a software-divined hardware platform. And when the company started four years ago, we had the
mission that for AI workloads, deep neural networks, you need to reimagine what that platform,
both software and hardware should look like for the type of workload that are coming, right?
CPUs and GPUs are doing a great job, but they were built for more traditional workloads in mind.
And when you look at these deep neural networks, compute is important, memory is important,
but how the data flows through the system is extremely important. And models are changing and
new research is being done. And so the ability to reconfigure and have flexibility and map your
software to your hardware is extremely important. So from the very early days, well, we have our
own chip and we have our own system. The software stack has been crucial. And so we have two products
at Samba Nova Systems. One is Dataflow as a service, which packages our underlying hardware and
software product and put services and solutions around that or focuses in natural language processing
and computer vision. And then we also sell a data scale product, which is more just the hardware
and the software for folks who want to control the full lifecycle of AI development themselves.
So that's just getting the racks of the Samba Nova servers plus the software layer on top.
Exactly. So we think of them in two ways. Dataflow as a service is more data-centric and data scale
is more model-centric. So if customers are looking to consume solutions and services, they want to
accelerate their AI journey and get there faster, then they're looking for a service and solution
that's optimized and that's what Dataflow is a service. We provide the service, they bring their
data. If you look at data scale, that's a much more model-centric platform where customers are
researching models, doing a lot of work with models, as far as hyper parameter tuning or other
things, making changes to their models, they control the full lifecycle and that would be the platform
for them. Got it. So in the former case, or rather in the latter case, you're expecting
the customers to be a lot more sophisticated. They're spending a lot of their time in training
and development. In the latter case, it's meant to be more of a you bring data and will
figure some stuff out for you. Exactly. And I'll be in EV. Exactly. For customers who are consuming
Dataflow as a service, we're actually seeing two different types of consumers. So certainly,
there's those customers that are early in their AI journey. They may have data scientists,
but the team is not significantly large. They feel like they're maybe behind or haven't
made a lot of progress on their AI journey. So they're looking to accelerate that. And so they bring
in some solutions from us that helps get them part of the way and then they can focus their team
to build workflows around that. There are other users of Dataflow as a service that actually do
have larger AI teams. They're quite skilled, but it's just the decision of where to focus their
effort. And so if they can purchase pre-packaged solutions from us that help take care of some of
the NLP and CV problems, then they can go focus on more business critical parts of their workflow
and AI. And so how did the focus on GPT-3 and transformers come to come to be?
Yeah. So at Samba Nova Systems, as we've been working on our journey and the products that we
have to offer, we're always looking at what are the capabilities that our platform can enable
and bring value to customers. At the end of the day, it's all about the customers and how are they
going to use the product and how are they going to be successful on their AI journey. So for us,
some of the advantages of the underlying platform are tied to our large memory configuration,
the flexibility we have in the platform. And so things like transformer models, both BERT and GPT,
you know, they started a few years ago where you would have a hundred million parameter model,
a 300 million parameter model, GPT-2, a 1.5 billion parameter model. And now we're looking at GPT-3,
which is 175 billion parameter model. That is stressing not only the compute capacity,
but also the memory needs of existing architectures. And because our platform has
those capabilities in really shine in that space, it was just a place for us to get started
and start showing the capabilities that our platform has to offer.
When you're out talking to folks about these kinds of models, I'm curious to learn a bit about
what you're seeing out in the real world, outside of academia, in the industries that I mentioned
earlier, banking, finance, traditional organizations, like how far have they come with trying to use
these models and what ways are they trying to use transformers and what's their experience so far?
Yeah, so I think, I mean, obviously it depends. It depends where you are in the world. Different
countries are potentially further along. Different industries within those countries are further along.
And so you have, you know, if you take a look at, for example, hyperscalers, regardless of where
they are, they are obviously very far in their journey with natural language processing.
Many of them are coming out and pushing the boundaries with new models on a regular basis,
right? And, you know, we used to say that hardware, you know, the hardware has a cycle that's usually
years in gestation period. And these models are making significant improvements like every two to
three months as far as the number of parameters and the compute and memory needs required.
But what we're seeing in enterprise customers, regardless really of where they are in the world,
is that, you know, they realize that they need to do something with AI.
They feel like their competitors are ahead of them. And in some cases, there are actually
examples that we know of or that they know of where investments have been made,
significantly in resources, both people, time, machines to go build solutions, right?
How successful those implementations have been, you know, varying degrees.
But they realize that there is a lot of opportunity here. How they translate that opportunity to
value that they bring to their customers and their business is that's where I think some of
the questions still lie, right? Some people have started to figure it out and are deploying things
slightly. Others are just very early on in their journey. And then you have some in the middle
who have been trying things, but just haven't really figured out how to put it all together.
You know, from a language perspective with natural language processing and our data flow as a service
for language, we see this cutting across all industries. And if you take a look at, say,
financial services, it's going to hit community banking. It will hit corporate banking,
capital markets. There's applicability across all parts of banking and financial services as well
as other industries. And it has the ability to be transformational, both, you know, customer
facing and back office for many industries. And are there specific use cases in those industries
that are coming to the fore as being places to places that organizations are starting or
where they're seeing success? Yeah. So I think if you look at banking and this applies to other
industries as well. The first use case that always jumps to people's minds is call center.
And the reason for that is that is an easy one for people to walk through and understand.
And it does apply in multiple industries. And so when you think about call center interactions,
when they come in, you understand who your customer is. You have some information about them.
Maybe they are speaking with you. They're texting with you. They're chatting with you in other
ways. There's lots of ways that, you know, call centers are running these days, whether it's chat
about with a live person. At the end of the day, despite a lot of technology being applied, you know,
most people when you talk to them about how was your call center experience or support experience
with x, y, and z, right? It's not a positive experience. And then you usually get asked for the
survey at the end. And who fills out the surveys, except for the people who are really irritated
about the process that they've been through. So I think, you know, call center is a very easy
problem to understand. And there's a couple of different ways that we can apply natural language
processing and GPT and obviously combined with other bits to improve that experience and drive
customer value. So, you know, bank banking institutions, certainly when you come in and talk to them,
they understand how to figure out, you know, was a customer happy or not. Some of it's through surveys,
some of it's through other mechanisms. But think about as you're interacting, whether it's via text
or actually talking to a live human being or speaking with the automated system, you know, by tone,
by the word choice, word choice specifically for something like sentiment analysis as a downstream
test of GPT, you can start to get live feedback on what the customer is saying and the word choice
that they're using is this a positive experience, is this a negative experience, or is this a
net neutral experience, right? So that's just one example. As you are talking to a customer or
they're inputting things, you know, you can do extraction of text and apply it to rather than
saying, you know, can you fill out this stuff for me or give me this information, you can grab a
lot of that and get things into the system to improve the overall customer success flow. I mean,
the end, the end state is really to try to get the information from the customer about their problem
as quickly as possible to have the call center automated system or person that eventually gets called
in to have the solution at the hand and ready for the problem that they have and then all of that
be a positive experience. So there's lots of different ways that we can combine all of that to make
that work. I'm not sure if this is a pushback or an opportunity for a liberation, but it just
strikes me that there's so much opportunity to do basic things, like I'm very curious your
experience about the readiness to even do advanced things, you know, to the point you mentioned
earlier, ensuring that the rep that you're talking to has the information about your account
in front of them. So you're not repeating things that you told the last rep or, you know,
telling them what products you have. Like it seems like there are some pretty foundational things
that organizations need to figure out. I don't know if it's just banks or rather, I don't
have an opinion on whether banks are further ahead of that or behind on that, but what's the
readiness of the enterprise to receive this kind of technology given the state of call center?
Yeah, I mean, I think, you know, essentially when you think about a customer,
regardless of industry, they're going to have, you know, they should know about your products,
your previous transactions, your history. That's all structured data, right? So when you think
about that, that is an easier problem to solve. And many people across many industries are doing
better in that once they get some preliminary information from you, right? If you give, say,
a key identifier, they just ask you to verify who you are, right? And they know, oh, here is your
name. Here's the transactions you've had with us. And it can go very smoothly from there.
It's this on other unstructured data, right? Which how do we incorporate that into the workflow,
to bring a better synergy between the customer and the business? And so I think,
you know, we are ready to start doing that. But I think one of the key things is when you start
looking forward to how you do implementations, a lot of people get stuck on what is successful.
And maybe they're thinking really big and really grand. And there are some small things and easy
wins that you can make to show success. And so one of the things we talk about with our customers
is, you know, breaking things down into smaller chunks and milestones where you can actually see
this new technology making impact, you know, testing that it has an impact and then continuing to
make adjustments and add additional things over time. So that you don't get stuck in the cycle of,
I'm testing things and I don't know, is it successful? Is it not? It feels like we're what we call
sort of POC purgatory where you're doing a lot of things, but are you actually driving business
value into your core business? And what's an example of how you might walk back from, hey, I want
to apply this to call center to a specific kind of sequence of of wins and things to test.
If you think about accuracy of being able to translate what a customer is either writing to you
or talking to you, right? That's highly important. And so GPT as an example does a great job
at basic language, right? But if you say are in the financial space or maybe you're an airline,
the vocabulary and types of words that you and your customers will use can be significantly
different. So for different industries and different use cases, there are going to be what we
call sort of custom corpuses or industry specific data sets that are important. So if you're
say working in a call center in a bank and you want to inevitably get some workflows there to help
improve it based on NLP, the first step is let's look at taking the base GPT model and improving
that with custom corpuses that are specific to your industry or specific to your customer install
base, right? So are there standard open source data sets or do you have to curate those data sets
internally? But let's refine the knowledge that the AI model has. So as an example, if you say
end of quarter, does that actually mean the end of a business quarter? Or does that mean the end
of a sporting contest, right? And so those specifics look very different by industry. And so that
would be step one. Let's do some fine tuning on this large NLP model. Then let's gauge accuracy
and how did that improve things? We can do sentiment analysis. We can do text generation. There's
other downstream tasks that we can use to evaluate that and to see, are we making progress?
At some point you get to a, this is a great level of accuracy. I think we can do something with
it. And then you would start to say, okay, let's transition that into adding it into a workflow.
What parts of the workflow can we replace with this? What do we have to enhance as sort of the
next phase? Got it. So it sounds like what I heard there was the first step is kind of transcribing
the conversation and enhancing or enriching the notes that CSR might otherwise take. And then
it's applying kind of these higher level capabilities, sentiment analysis or some predictive thing
to maybe guide them in the conversation or allow you to better assess the success of your
interactions? Exactly, right? So that first step is really to say, is the data that we're using
and trained on capable of giving us the accuracy in our interactions that we want?
And then when you get to a level that says, yeah, it can, you know, this AI model is not just
generic to the English language, but it really understands commercial banking or it really understands
capital markets to very different things, right? Then we can say, how do you apply that in workflows?
And for each of those areas, they're going to have different workflows, right?
Community banking certainly has a call center aspect to it. Capital markets may not. And so
then you would take to the specific use cases and how to apply that. Got it. Got it. So as opposed
to jumping to, you know, something in the workflow, start by, say, start by assessing whether
the model can even be used in the context of your industry, either off the shelf or after fine
tuning with whatever data you can access. Got it. Makes sense. Yeah. And I would say that the model
is wonderful and can be applied across multiple industries. What we do at Samba Nova is pre-trained
and optimize it for the English language on open data sets. And that is a hard chunk of work
that not a lot of people can do across the world, right? So we're able to do that and we're able to
get customers up and running quite quickly. But that stage of industry specification or specific
data is really important so that, especially when you're dealing with customers, you know,
you don't want to get them irritated because it's not giving them the right answer or not
understanding the context. So those differences between industry words or words specific to your
business matter and aren't going to show up in a traditional English language corpus. Or if they
do show up, they're not going to be in the volume that the AI model is going to be able to learn
enough about them. When an organization is trying to use a model like GPT-3 or BERT,
you know, I think that the options that come to mind first are, you know, use GPT-3 as a service
from OpenAI or, you know, pull down a model from hugging face and use that. How does someone know
that they should be calling a seminar? Like, what are the hidden challenges of, you know,
those two approaches or other approaches that maybe you're seeing folks take that folks tend to
run into that impede their progress? The few things that come to mind are, first of all,
finding the resources who can do the type of work at GPT scale. That's a very specific skill set.
Those are hard resources to find, regardless of where you are in the world. And so when we've
talked to customers, I have a customer as an example, in the financial services space, they looked
across at a competitor and said, over the past three years, they've built a team of data scientists
of hundreds, you know, spent millions and millions of dollars investing in hardware and then
spent all that time to work on a solution, we cannot catch up with them, even if we started
investing today. By the time that we get the people, if we can find them, and invest the money,
we will have wasted significant time and they will have leapfrog does even more than today.
So for them, they think about the time it will take to put together what we consider a do-it-yourself
approach on a hardware perspective. There's the money to do that. And then also the people resources
that you need to invest in and that can can do that work both on the hardware side and the software
data science side. And so we are resource constrained to find the right type of people. And so
working with a company like Samba Nova Systems that has the resources and experts who are working
in algorithmic improvements at the cutting edge, at the cusp of research is an advantage that they
can leverage. We have a solution that we bring that we have up and running in a day that customers
can start using a pre-trained GPT model and get going. And so we continue as the life of the service
to optimize that for them and keep abreast of new research and deliver new services and solutions
for them. So by partnering with someone at Samba Nova Systems, they get to leverage our expertise
over a period of time. The do-it-yourself approach is also hard. Even if you have the time to go and
do it, doing something like GPT takes a large scale of equipment because of the constraints of
traditional architectures. And so Samba Nova Systems allows people to basically deploy at a
smaller configuration to start because of some of the enhancements we have in both our hardware
and software and scale up in an optimized way without any change in their environment.
And that's not something that you necessarily get with the off-the-shelf do-it-yourself components.
And so you don't have this option of starting at a reasonable size. You just have to go to this huge
scale. And that's a really big sort of roadblock for a lot of people. Now, to go back to your point,
you say, well, they could go to the cloud and they could go use some of those services.
But at the same time, some of the customers we have have said they've been surprised by some
of the bills that they've received from some of these cloud service providers. Training is not
a cheap endeavor. So compute time is certainly one thing and the network bandwidth of data back
and forth. So while it might be a nice playground to start with, having that as a full-on production
environment becomes a little bit difficult from a financial perspective. And they are starting to
think that maybe some of those workloads should be brought back on-prem. That being said,
we are happy to deploy our solution on-prem with hosting providers or in dedicated cloud
environments for our customers. So we're very flexible. At the same time, we also do have many
customers who have requirements to keep data within their own firewalls, within their own country.
And so there's rules and regulations that play here. So there's lots of reasons I think why
customers may start out looking at things that exist today in the market. But in order to deploy
those in a real environment at production, at scale, I think Sombanova provides a unique advantage
for them. Can you talk a little bit about the process for kind of building the solution
on your end? And I guess to elaborate on that a little bit, the GPT-3 data set, for example,
like OpenAI, Crawl, Reddit, the web, all this stuff, like it's not like a five-remembering
crack. It's not like go to GitHub and download that data set. You have to go crawl that yourself,
right? And there are aspects of the, certainly they publish the paper about the model,
but those papers never really have all of the details about the model. Of course.
Here is what all from a research and development perspective had to go into
essentially replicating what they did and published a lot of, but not all of.
There has been significant time and effort as we've looked at how our transformer models
optimize to run on this new hardware and this new software stack, right? So it's not for us.
It's not only with Dataflow as a service, download an industry standard model and run.
We certainly can do that. Go to HuggingFace, download the model, run it on the platform. That is
a great solution for customers who are interested in our data scale product. But for our Dataflow
is a service product where we are packaging solutions. There's a much larger effort that goes into
play there for the optimizations and enhancements that we put in the stack and for the solution around
it. And I will say whether we're doing work internally or customers are doing work as they look
to additional pre-training and fine-tuning. Data curation is one of the hardest aspects of this.
Data may be available. Is it available in the right format? Do you have access to it? Can you
use it? Different countries have different rules for how it's used. And some countries have more
digitized text content than others. And so the one thing I can say whether it's internal to
Samba Nova or external with our customers. Having the right data set to use is always the first
discussion and always a challenge. As you're out talking to and working with customers, again,
banking finance, but also beyond that, how well do they understand GPT-3 and transformers
and maybe even more interesting, like what's surprising to them about the technology? What is it
that they already kind of get and what is it that? Oh, it just blows their mind. It blows their mind
not necessarily from a performance perspective. But what is it that they find interesting and
counterintuitive, I guess, is what I'm trying to get at? Well, so obviously when you can talk to
customers, there's lots of different personnel that you can be talking to. You can be talking to
the technical people who are data scientists. You can talk to technical people who are more
in the data center and administrators of systems. And you can also talk to business
type users, right? So it all comes down to who you're talking to. If there is an established
team of data scientists and data scientists who work in the area of AI focused on deep neural
networks, I'm doing some clarification here. Then most of those people are abreast and have heard
of Bert and they've heard of GPT. Whether or not they've used it themselves personally or
internally is another question. If you have at customers technical people who are more on the
analytic side and the traditional ML side, it's possible that they don't know much about Bert
or GPT. And this is not a everybody that falls in that category, but there are situations where
just because of the type of data and analytics work that they're doing, they're more focused on
other types of problem solving than deep neural networks with transformers.
The business side users, some of them have certainly same things in the press. I've said
we've gone to talk about GPT and then I saw something about this other model that came out that
's even bigger. The acronym is familiar from a they've heard about it and maybe seen some others
are using it, but maybe the details aren't so well known on the technical side, but they're also
interested in understanding what it can do and how they can take advantage of it. And I think
not everybody has the same understanding of what these large transformer models are capable of
and what they can be applied to, because that information isn't so readily available and shared
at a level that everyone can get a common understanding on. So those are just when I go and talk to
people, the levels of understanding are significantly different based on who we're talking to. A lot
of people don't realize the effort it takes to get to an optimized and running configuration that's
accurate. They underestimate it or they haven't underestimated it from a hardware configuration
side, but having the skill set to be able to implement that is also challenging. So as I think
alluded to earlier, there aren't many people in this world who can actually do an optimized GPT
configuration, right? And so that takes expertise across an entire stack, both hardware and software,
to be able to do that. And so I think that that in and of itself has surprised people when I've
gone and talked to them. The other thing that's surprising, you know, and this is just, it's
back to the value of Samba Nova systems is, you know, they're shocked that we can wheel in a
service that within a day they can be using and it's already pre-trained and optimized for them.
Right? And we can scale it from one rack to many racks. And for us, it doesn't really matter.
And so for them that catches them by surprise because this is a hard problem. And so seeing that,
you know, actually in practice is, you know, we get a lot of, wow, you said it, I don't think we
believed you and you actually delivered and it's amazing. One of the issues that I think we're
still trying to wrap our hands around as an industry with language models is, you know,
predictability, controllability, governance, that kind of thing. Lots of examples of, you know,
language models gone wrong. We don't need to, you know, repeat those. How are the folks that
you're working with addressing those kinds of issues? So certainly they come up, right? As far as
discussion points, right? And what should we be doing that about them? And so this comes back to,
I think, a few things. Certainly it comes back to the data set, right? The information that the AI
model learns is only as good as the data set that it's trained on, right? So what is the right
data set to get you to an accuracy level that makes sense for your business? And so it's not only
about the data set, but then how are you going to measure and test that? You are actually within
the bounds that make sense for you. And that's not something necessarily that Samba Nova system can
guide customers on because it's going to be specific to their business and specific to their
industry, but we're happy to work with them and talk to them about some of these steps that we
think are crucial for them to go through in order to make sure when things are deployed,
it's doing what is expected, right? And if things aren't doing what they're expected, you want to
catch them early before you've gone live and you want to understand how are you going to
remediate that problem and ensure it doesn't happen again, right? So there is a life cycle around this.
You hope to do most of it in the early phases of an implementation and really refine what you're
working on so you know what the answers are going to be and how to address them. And then if you
are in a production environment, if something does happen, how quickly can you respond to it
and remediate things and fix things so they don't continue to happen moving forward?
Can you talk through the data collection aspect of that with maybe some use case examples given
a particular problem, you know, where folks collecting the data, what do they have to do with it
in order to make use of it for fine-tuning? Yeah, so I think, you know, some of it is with data
sanitization. So when you think about GPT models as an example or BERT, trained in pre-trained
in English, we're all using sort of open standard corpses, right? So how sanitized are those,
is there bias in them? You know, all the questions that you would you would think to ask people
should be asking. And so as you start to curate additional data sets to do pre-training and fine-tuning,
making sure, you know, it's you have to trust the source of the data. Is it external data?
Is it internal data? If it's internal data, does it need to be cleansed first of sensitive
information that you shouldn't be putting into an AI model? But then also, you know, you need a
team of data experts who are familiar with how to produce data corpuses. And this is it's
own practice in its own industry, right? And not necessarily my expertise, but it's certainly
a focus where our customers are spending a lot of time. So they get the data set right.
And it's not like you have to do it right in one shot. You can start and continually enhance it
over time by doing additional pre-training and fine-tuning. So that allows you to start the hit
some early milestones and deliver a continually improving solution. You mentioned measurement
and testing with regard to these data sets. What exactly does that mean in this context?
Well, so I think, you know, we have ways of, so take sentiment analysis as an example.
I'm a finance customer. I have a GPT model. We'll go back to my end-of-quarter analysis, right?
Is that a positive or is that a negative statement, right? So let's say, you know, the
Bank of America here in the US ended up ended their quarter, you know, up $10 per share or something.
Okay, so that is a positive statement. So from a sentiment analysis perspective, right? So it
would have pre-trained, it would have done that, and then you would have some sanity. You need to
check, right? Are is the analysis that the AI model doing correct or incorrect? When you start to
do things that are very specific to finance and start talking about, maybe it's talking about
puts or calls or things related to the stock market, it may not be able to discern that. And in
fact, we've seen cases, I don't have one in front of me where, you know, models like Bert large
or maybe GPT will classify something as negative or neutral when it's really positive. And so you
need to be able to flag the incorrect assessments and figure out a way to refine the data that you're
training on to get rid of those so that you're more accurate than not, right? So first of all,
you want to start judging accuracy of your data set, but you also need to come in and actually check
is your accuracy what you think it is or you falsely identifying things, right? And so
two aspects to that. So a lot of what has driven the excitement around
Bert and Transformers generally is that unlike kind of prior models in NLP and you know the way we
think about much of machine learning in general, like you haven't needed these labeled data sets,
like you it's you're kind of letting your training on unlabeled data.
Yeah, it sounds like for these measurement and tests requirements, though, you would need
some kind of labeled data set in order to do benchmarking. Is that the case and does that?
Yeah, I'm not, though, so we're not necessarily using labeled data with our customers. I think
there's just a, you know, when you think about deploying some of these solutions and people question
is AI correct and is it, you know, do you get the same answer every time? There's care that goes
into the creation of the data set, even if it's not labeled, right? So thoughts around that.
And then, you know, I don't necessarily think you need to label everything, but there does need to
be some checking and some test put in place to verify, right? And so maybe that's that spot checking
or other things, right? So I don't think you need to go to the everything must be labeled.
We know that's not going to be a scalable solution, but when you think about, you know,
big name brands deploying these solutions in front of their customers or using it to make
business decisions, you do want to put some ways to at least assess how things are looking.
So you understand, you know, are you accurate or not and doesn't make sense to push forward?
Yeah, yeah, that's that's kind of the question that I had and what I was getting at are folks
spot checking and getting a general feel that this looks about right or are folks setting aside,
you know, some labeled test set or validation set that they then, you know, for sentiment,
let's say, run against their model and, you know, produce numerical reports against.
And then how much of that, how hard is that effort, how well prepared are they to go through those
efforts? Yeah, so I think, you know, in the conversations I've been with customers,
you see sort of a little bit of both and questioning about what is the right way. And I'm not sure
that there is a well, you know, people are still debating the approaches here of what should be
taken. You know, obviously we know there when you create a curated data set and everything needs
to be labeled, there's additional challenges there, right? So, but at the same time, people want
some confidence. And so it's a where that will fall might depend on the customer and the industry,
but I will say it's a discussion point. And I'm not sure that the answer, you know, industry best
practice is well known at this point. Got it. And you also mentioned kind of these guard rails that
folks will put up in place to ensure that when slash if a model has a problem, they can avoid
that problem in the future. Can you talk a little bit about the approaches that you've seen for
doing that? Yeah, so I mean, I think it's mainly about, it goes back to the process I've talked about
where there's some initial training that's done, you do more pre-training or fine-tuning,
you try your downstream tasks. At some point, you know, things are looking good, it's deployed,
you know, but data changes things, things morph over time. And so as you think about deploying
these into environments where they're more in production and less in test, you need to be
constantly staying abreast and looking at are there things that I need to retrain? Do I need to
bring additional data in? Do I need to, does the model need to be changed? That could potentially
happen as well, right? And so how are you monitoring for that and the end-to-end lifecycle of things?
It's not, I don't think it will forever be a, we've trained it and deployed it, you know,
how people speak, what's acceptable or not acceptable, common vernacular, those things change
over time. And so how do you keep up with those changes and incorporate them into a pipeline that
get deployed is critical? We spoke a little bit earlier about what your customers find surprising
as they're working with these models. To close out, I'm curious, what have you found surprising
personally as you started to work more closely with DPT and Burton Transformers? Yeah, so, you know,
I think, well, and maybe this is more of an AI thing in mind. When I, when I first joined
sambanova, I certainly felt that this was a transformational technology. But now that I'm three
years in, I'm looking at use cases with customers, we're exploring a variety of areas. I really
understand that this is going to be a very transformative shift in technology and it's going to
affect all industries, right? You can see it applied in so many different ways. And when I think
about my lifetime, there's been very few of these moments. You think maybe of the internet
boom and sort of the mobile phone. And I really think AI has the ability to be that transformational
shift of this next generation. And so for me, I think that's the exciting part. I knew it was the
new technology and there was lots of things to do. But I don't think when I started my journey
at sambanova, I quite understood how impactful it could be across everybody's lives. Yeah, awesome,
awesome. Well, Jennifer, thanks so much for joining us and sharing a bit of your experiences,
deploying GPT3 and related technology out of customers. Thank you so much. It's been a pleasure.
Thank you for having me.
