1
00:00:00,000 --> 00:00:12,560
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:17,920 --> 00:00:23,360
All right, everyone. I am on the line with Emily Bender. Emily is a professor of linguistics

3
00:00:23,360 --> 00:00:27,840
at the University of Washington. Emily, welcome to the Twimal AI Podcast.

4
00:00:27,840 --> 00:00:32,400
I'm really excited to be here. Thanks for having me on. I am super excited to chat with you.

5
00:00:32,400 --> 00:00:37,200
We spent some time speaking earlier, kind of the obligatory, how are you doing?

6
00:00:37,200 --> 00:00:44,000
Kind of age of corona conversation and kind of grounded on some of the things that we'll be

7
00:00:44,000 --> 00:00:52,320
talking about. Ethics, practical considerations around AI and NLP ethics, computational

8
00:00:52,320 --> 00:00:57,760
linguistics and a relationship to linguistics to name a few. But I'd love to start with hearing

9
00:00:57,760 --> 00:01:02,880
a little bit about your background and how you came to work in the field of what linguistics

10
00:01:02,880 --> 00:01:08,960
slash computational linguistics. Happy to. So my background in terms of training is linguistics

11
00:01:08,960 --> 00:01:16,160
all the way. I studied at UC Berkeley. Didn't know what linguistics was until I got there and

12
00:01:16,160 --> 00:01:20,960
he did some wonderful advice the summer before going off to read through the course catalog,

13
00:01:20,960 --> 00:01:25,200
which you know back then was a book, looked like a phone book, and just circle anything that looked

14
00:01:25,200 --> 00:01:29,040
interesting. And there was this one class called an introduction to language. And I thought,

15
00:01:29,760 --> 00:01:33,120
that sounds great, because I really liked my language classes I took in high school.

16
00:01:34,000 --> 00:01:37,280
And so I circled it. And then when I was looking for a general education

17
00:01:38,000 --> 00:01:42,000
requirement class, I saw that it fulfilled one of them. So I signed up for it and I was hooked

18
00:01:42,000 --> 00:01:47,440
on the first day. It was like, this is my thing. But it took me the whole term to convince myself I

19
00:01:47,440 --> 00:01:55,840
could major in something that I perceived to be impractical, which is pretty funny in retrospect.

20
00:01:55,840 --> 00:02:00,720
And the class was an intro to linguistics or? Yeah, intro to linguistics. And I majored in

21
00:02:00,720 --> 00:02:06,720
linguistics. I took one computer science class when I was an undergrad. There was the intro

22
00:02:06,720 --> 00:02:12,720
programming class that was taught in Scheme, which is a dialect of Lisp. And then I went on to do a

23
00:02:12,720 --> 00:02:18,640
PhD in linguistics at Stanford, where I worked on. And did you field work in the whole nine? No,

24
00:02:18,640 --> 00:02:23,120
I did some experimental work, but I didn't do like there's there's linguists who do really

25
00:02:23,120 --> 00:02:27,200
interesting work going out and, you know, recording people speaking to observe something with

26
00:02:27,200 --> 00:02:31,360
a variation or working with consultants to understand the structures of languages that are not

27
00:02:31,360 --> 00:02:37,520
well documented. I'm not either of those. But I did do a little bit of experimental work. So my

28
00:02:37,520 --> 00:02:45,920
dissertation was on the interface between sociolinguistics and syntax. So syntax is the study of

29
00:02:45,920 --> 00:02:52,000
what are the rules behind the grammars of languages? How do we get to the strings that are allowed

30
00:02:52,000 --> 00:02:56,320
and what they mean? How do we write those descriptions? And what do we need to be able to

31
00:02:57,040 --> 00:03:00,720
create a formalism that'll work for any language? Because we know that any human can learn any

32
00:03:00,720 --> 00:03:05,520
language if they're exposed to it. And so those are the questions of formal syntax. And there's

33
00:03:05,520 --> 00:03:10,720
this interest in what knowledge do we have. And then sociolinguistics is looking at language

34
00:03:10,720 --> 00:03:15,920
variation and how people speak differently in different contexts based on sort of their

35
00:03:16,720 --> 00:03:21,120
social address and how they want to present themselves to the world. So register and all that kind

36
00:03:21,120 --> 00:03:26,160
of stuff? Yeah, register code switching, right? Lots of people will have multiple different

37
00:03:26,160 --> 00:03:29,760
varieties that they control and they use it to present themselves in different ways in different

38
00:03:29,760 --> 00:03:36,160
contexts and all of that. And the syntax traditions were saying that's not linguistic knowledge.

39
00:03:36,160 --> 00:03:42,960
Linguistic knowledge is just what it is that lets you know that in English we don't say I see cat.

40
00:03:42,960 --> 00:03:47,120
We have to say I see a cat or I see that cat or I see cats, right? And that's a grammatical

41
00:03:47,120 --> 00:03:54,800
constraint of English. And that kind of stuff is knowledge of language. And all of this other

42
00:03:54,800 --> 00:03:59,840
knowledge that people have that allows them to decide when to speak in which way, that's separate.

43
00:03:59,840 --> 00:04:04,640
And my intuition was, well, we're one person. And yet we know all of this and it's connected

44
00:04:04,640 --> 00:04:10,000
with the same thing. I think it fits together. And so I did some research looking at variation in

45
00:04:11,040 --> 00:04:16,800
what at Stanford, we referred to at the time as av, which is an acronym African-American

46
00:04:16,800 --> 00:04:21,680
vernacular English, also known as ebonics. And black English is a whole bunch of names for

47
00:04:21,680 --> 00:04:27,520
the variety, which is a surprisingly actually well studied variety of English among those that

48
00:04:27,520 --> 00:04:34,080
are not considered standard. And so there's really good social linguistics studies of the ways

49
00:04:34,080 --> 00:04:40,240
in which variation is used in app to different purposes. And so I did a perceptual study looking

50
00:04:40,240 --> 00:04:44,960
at how people perceived the meaning associated with saying or not saying the verb B in different

51
00:04:44,960 --> 00:04:53,760
contexts. So that's pretty far field for a podcast on machine learning. So that's what I was

52
00:04:53,760 --> 00:04:59,440
doing in my studies. And alongside I was working as a research assistant on a project building

53
00:04:59,440 --> 00:05:05,360
a computational grammar of English. Okay. So I go on the job market and don't get picked up as

54
00:05:05,360 --> 00:05:11,520
either a syntactician or a sociolinguist. And so I go out into the startup world. I was working

55
00:05:11,520 --> 00:05:20,160
at a startup called YY Technologies. I started there in 2001, which was not a good time to be going

56
00:05:20,160 --> 00:05:24,640
into the startup world if you remember. It was just in time for the dot com first.

57
00:05:25,600 --> 00:05:30,000
Company was gone within seven months of when I started working there that had been around for

58
00:05:30,000 --> 00:05:37,040
almost a decade. Doing grammar engineering for Japanese. So building a grammar that modeled

59
00:05:37,040 --> 00:05:40,640
the rules of Japanese so that we could get from strings of Japanese to semantic representations

60
00:05:40,640 --> 00:05:46,800
in the context of a customer service application. Okay. Kind of precursor with customer service

61
00:05:46,800 --> 00:05:51,760
chatbot. Yeah. Yeah. It wasn't set up in a chatbot mode. It was meant to be email response.

62
00:05:52,720 --> 00:05:57,440
But very much that same idea. Okay. And the company had a product that worked for English and they

63
00:05:57,440 --> 00:06:02,880
wanted to branch out into the Japanese market. And so they hired me to work on the grammar for

64
00:06:02,880 --> 00:06:09,680
Japanese that would fit into that same product. So then I am on the strength of that basically

65
00:06:09,680 --> 00:06:14,640
and a little bit of research I was doing around generalizing grammars across languages got hired

66
00:06:14,640 --> 00:06:18,160
to start the professional master's program in computational linguistics at the University of

67
00:06:18,160 --> 00:06:26,480
Washington. Okay. And yeah. Which felt like a huge stretch at the time I have to say. I was like

68
00:06:26,480 --> 00:06:30,400
really because if you look at my CV, I'm pretty junior and I look like a syntactician slash

69
00:06:30,400 --> 00:06:35,440
sociolinguists with this thin layer of computational linguistics on top. But part of what was going

70
00:06:35,440 --> 00:06:41,200
on there was that the kind of syntax that I was doing was really grounded in computational

71
00:06:41,200 --> 00:06:45,120
modeling of syntax. It was developed to be able to be written on a computer as well as understandable

72
00:06:45,120 --> 00:06:51,040
by humans. And so the linguists in my department saw all of my syntax work as computational linguistics

73
00:06:51,040 --> 00:06:56,560
too. And was that just the representation, the way you represented the syntaxes or something

74
00:06:56,560 --> 00:07:01,360
about the analysis that you were doing? It's about the analysis and the theoretical framework.

75
00:07:01,360 --> 00:07:06,000
So that HPSG is what it's called and it's a framework that's interested in getting down to all

76
00:07:06,000 --> 00:07:11,200
the details so that we actually model not just the small subset of sentences that are interesting,

77
00:07:11,200 --> 00:07:17,680
but everything you might find in running text eventually. And so that's very coherent with

78
00:07:17,680 --> 00:07:23,920
the goals of doing natural language processing, but from a symbolic perspective. And on the other

79
00:07:23,920 --> 00:07:29,680
hand, it's very formalized. It's precise enough that you can do it on a computer. And a lot of

80
00:07:29,680 --> 00:07:34,880
other work that happens in syntactic theory is much more entrusted in theoretical questions to do

81
00:07:34,880 --> 00:07:40,880
with similarities and differences across languages and doesn't put the same emphasis on getting

82
00:07:40,880 --> 00:07:45,040
down to the nitty-gritty both in terms of all of the data and in terms of precise enough a computer

83
00:07:45,040 --> 00:07:50,960
could do it. Is there a way to create an example of the two different tags that would be

84
00:07:50,960 --> 00:08:00,000
elastic and restrictive? It's not so the best examples I can think of are visual examples in terms

85
00:08:00,000 --> 00:08:07,360
of what the analyses look like. And just to give you a sense of it, when we build a grammar in HPSG

86
00:08:07,360 --> 00:08:14,880
for English, there's English. When we do that, for those people just listening, that was a cat

87
00:08:14,880 --> 00:08:18,640
that just jumped into the frame. That's why we're laughing. And the cat did it earlier and I was like,

88
00:08:18,640 --> 00:08:26,080
I hope the cat does it again. So there's the cat. He's helping me out. So in order to get,

89
00:08:26,960 --> 00:08:30,400
there's a resource called the English Resource Grammar that the best thing that I was involved

90
00:08:30,400 --> 00:08:35,440
with as a research assistant in the 1990s. It's been under continual development since 1993.

91
00:08:35,440 --> 00:08:41,280
It is a hand-built grammar of English on a computer that if you give it well-edited English text

92
00:08:41,280 --> 00:08:49,200
from pretty much any genre or register. So chemistry, academic articles, Wikipedia, Linux

93
00:08:49,200 --> 00:08:54,160
manpages, Norwegian hiking, tourist questioners, like broad range of genres, but well-edited.

94
00:08:55,120 --> 00:09:04,000
Their cats went flying. It can come up with correct analyses that give you a good grammatical

95
00:09:04,000 --> 00:09:08,880
structure and a good semantic representation. I can send text tree of the center or something like that.

96
00:09:08,880 --> 00:09:12,960
Send text tree and then like predicate argument structure, like first-order logic type representation,

97
00:09:13,920 --> 00:09:18,800
predicate logic, not first-order of the semantics, for something like 90% of those sentences.

98
00:09:19,440 --> 00:09:24,160
It's a massive project. But in order to do that, its representations internally are actually

99
00:09:24,160 --> 00:09:28,880
quite complex. And if we try to like print one out, not in sort of the abbreviated form that shows

100
00:09:28,880 --> 00:09:32,560
you the general structure of the tree, but all the details, it's pages and pages and pages for one

101
00:09:32,560 --> 00:09:41,520
sentence. If what you're interested in is sort of making generalizations across languages about

102
00:09:41,520 --> 00:09:46,320
what happens with the expression of pronouns versus you just don't say the word at all

103
00:09:46,320 --> 00:09:50,000
in different languages. And how does that correlate with how much morphology is on the verb?

104
00:09:50,000 --> 00:09:56,480
That kind of representation can be cumbersome. And so that's where the theoretical syntax tends to

105
00:09:56,480 --> 00:10:00,320
work more in broad brush, I think, than the kind of details you need to do it on a computer.

106
00:10:00,320 --> 00:10:06,160
Is there an analogy there to like explainability of machine learning models, or is that too much

107
00:10:06,160 --> 00:10:13,200
of a leap? Wow, it's an interesting question. But I don't think it connects to explainability,

108
00:10:13,200 --> 00:10:18,240
because in both cases, it's humans are creating these things. Yeah, I'm not directly like I'm

109
00:10:18,240 --> 00:10:23,840
envisioning the thing that with the pages and pages is like a, you know, a deep model with a lot

110
00:10:23,840 --> 00:10:28,080
of primers that we don't really understand. I guess in this case, you can understand them if you

111
00:10:28,080 --> 00:10:33,680
zoom in. Exactly. Whereas in the other case, it's a little bit more amenable to interpretation.

112
00:10:34,160 --> 00:10:38,480
Yeah, I think at that level, the analogy works. It's that it, but it's not useful.

113
00:10:40,480 --> 00:10:43,920
You can drill down, but it's hard. So when people approach this resource and haven't been involved

114
00:10:43,920 --> 00:10:47,040
with it for years and years and years, it is opaque. And it's like, well, how do I,

115
00:10:47,920 --> 00:10:52,080
like, why is this feature value this here and that there? And where is it coming from? And

116
00:10:52,080 --> 00:10:56,880
why did you make that analytical decision? And there's a lot of work to be done still about,

117
00:10:56,880 --> 00:11:02,240
how do we document this as an incredible resource? But how do we make it accessible and understandable

118
00:11:02,240 --> 00:11:08,640
to people who haven't already been deeply involved with it? And so did you end up taking another

119
00:11:08,640 --> 00:11:15,840
computer class programming class? That's a big question. I left that hanging. So I did take one

120
00:11:15,840 --> 00:11:20,720
class in the computer science department in graduate school. It was a class with Terry Winigrad

121
00:11:20,720 --> 00:11:27,440
on phenomenology. It was a philosophy class. We did not program anything. And it was a great class.

122
00:11:27,440 --> 00:11:35,200
He made us read Heidegger, which was painful. But it was a really interesting class. And that's

123
00:11:35,200 --> 00:11:42,080
the only other computer science class that I took. I did learn along the way how to code.

124
00:11:43,840 --> 00:11:47,280
Done both working in the grammar engineering is a very high level programming language. That's

125
00:11:47,280 --> 00:11:52,480
sort of declarative linguistic knowledge. But I've also done some work with some of the underlying

126
00:11:52,480 --> 00:11:58,720
algorithms. But I am not, you know, not the person who's going to build or deploy a machine learning

127
00:11:58,720 --> 00:12:07,040
system. Rather, my role in this dialogue about linguistics and NLP has been sort of saying,

128
00:12:07,040 --> 00:12:11,280
okay, I see what you're doing with the machine learning. How does that relate to the questions

129
00:12:11,280 --> 00:12:14,880
that we're asking in linguistics on the one hand? And that was sort of my first entry into it.

130
00:12:14,880 --> 00:12:19,840
Was, okay, you're building systems. I see the input. I see the output. I see you're really

131
00:12:19,840 --> 00:12:23,040
interested in the internals of the system. And that's cool. That's fine. But that's not the part

132
00:12:23,040 --> 00:12:27,760
that I'm working on. Right? I'm interested in the framing of the task, the input, the output,

133
00:12:27,760 --> 00:12:33,680
how it's evaluated. Where did that gold standard data come from? How did you create those annotations?

134
00:12:35,040 --> 00:12:40,400
And then how does that task relate to either scientific questions that someone interested

135
00:12:40,400 --> 00:12:45,440
in say how language works might be interested in? Or how does it relate to the practical

136
00:12:45,440 --> 00:12:50,080
applications that you're selling it as a solution to? Right, right. Yeah, you had an interesting

137
00:12:50,080 --> 00:12:53,920
tweet the other day. I meant to look it up so I could reference it here, but it was something

138
00:12:53,920 --> 00:13:00,560
alone, the lines of, I forget the exact framing, but a lot of your perspective being informed by

139
00:13:00,560 --> 00:13:06,320
the idea that, you know, in NLP, we've got this whole field of linguistics that came before it,

140
00:13:06,320 --> 00:13:10,080
and now we're doing all this cool computational stuff. And that you contrasted that with

141
00:13:10,080 --> 00:13:16,080
computer vision, where maybe we didn't have computer visionology or visionology or whatever the

142
00:13:16,080 --> 00:13:22,320
analogous thing would be. Where were you getting out there? So that came out of a fascinating

143
00:13:22,320 --> 00:13:27,840
conversation that I had with Deb Rajee and Emily Denton, who I only know because we've met

144
00:13:27,840 --> 00:13:31,520
over Twitter. And this is one of the wonderful things about this research environment,

145
00:13:31,520 --> 00:13:34,720
especially now that everyone's stuck at home is like, okay, yeah, sure, I have time to

146
00:13:34,720 --> 00:13:37,520
have a meeting with people who are in very different parts of the country,

147
00:13:37,520 --> 00:13:45,440
cool. And they are interested in the sort of benchmarks and how they're deployed to machine

148
00:13:45,440 --> 00:13:51,920
learning broadly and sort of where, where do they come from? How, and this is also Emily Denton's

149
00:13:51,920 --> 00:13:57,120
working with Alex Hanna on these questions of how does this, how does a benchmark become a

150
00:13:57,120 --> 00:14:03,120
benchmark? Why do people care about some and not others? And I think we all share an interest in

151
00:14:03,120 --> 00:14:07,680
how do these benchmarks relate to the broader thing they're supposed to represent?

152
00:14:07,680 --> 00:14:13,120
Particular benchmarks generally or benchmarks like ethics benchmarks or

153
00:14:14,080 --> 00:14:18,480
So it's a great conversation because all of the people in that conversation really care about ethics,

154
00:14:18,480 --> 00:14:21,280
but that's actually not what we're talking about. So it's always a background and

155
00:14:21,280 --> 00:14:24,560
it's about the, yeah, I was just extrapolating from the names.

156
00:14:24,560 --> 00:14:33,120
So, you know, it's things like, so the super glue benchmarks for natural language understanding

157
00:14:33,120 --> 00:14:37,360
and image net for computer vision and image labeling, right? These become these

158
00:14:38,480 --> 00:14:43,120
tasks that then you try different algorithms on. And so one of the things that came out of that

159
00:14:43,120 --> 00:14:48,400
conversation was a framing that I really like, which is this sort of three-part thing. You have

160
00:14:48,400 --> 00:14:55,120
the task definition sort of in the middle. And on the one hand, you have the data set that the

161
00:14:55,120 --> 00:15:00,640
task is represented by. And on the other hand, you have the thing in the world that the task is

162
00:15:00,640 --> 00:15:07,840
supposed to correspond to, right? So if your task is image labeling and then you can say, okay,

163
00:15:07,840 --> 00:15:13,760
how does the construction of image net actually relate to that task as sort of task internally that

164
00:15:13,760 --> 00:15:18,720
I'm trying to do? But then also, how would that task get deployed? What are we using

165
00:15:18,720 --> 00:15:24,080
image labeling for? And how does this particular conception of the task support or not support those

166
00:15:24,080 --> 00:15:29,760
use cases? So we're talking about things like that. And in the context of that conversation,

167
00:15:31,120 --> 00:15:36,480
I was shocked to hear them say that from their perspective, NLP is better off than other parts of

168
00:15:36,480 --> 00:15:43,520
machine learning because NLP has linguistics, keeping it honest. And I was floored by that because

169
00:15:43,520 --> 00:15:47,680
I feel like, yeah, that's what I'm trying to do. And there's a few of us in there kind of conversation,

170
00:15:47,680 --> 00:15:52,640
but it does not feel like we're generally speaking being listened to, which is maybe a little bit unfair.

171
00:15:54,000 --> 00:15:59,280
I think the dynamic there isn't that people are not listening or ignoring whatever, but it's

172
00:15:59,280 --> 00:16:05,520
rather that the way things are set up right now, NLP for some people serves as an application

173
00:16:05,520 --> 00:16:12,240
area of machine learning, right? So if you're, if what you're interested in is learning algorithms,

174
00:16:12,240 --> 00:16:16,240
and that is a wonderful area of research, it's, you know, I'm glad people are working on it.

175
00:16:17,120 --> 00:16:23,120
In order to test them and refine them and understand, you know, what's good at what you need things

176
00:16:23,120 --> 00:16:28,960
to be learning. And a lot of people who are interested in learning algorithms seem to be interested

177
00:16:28,960 --> 00:16:34,960
in as something that can learn genuinely. So they're interested in applying, you know, RNNs in

178
00:16:34,960 --> 00:16:39,360
different contexts. And so you need different contexts. And it's not just sort of a NLP view of,

179
00:16:39,360 --> 00:16:42,800
okay, different registers, different, you know, are we doing speech recognition or machine translation,

180
00:16:42,800 --> 00:16:50,240
but language is one, vision is one, playing chess, you know, things that you would need in

181
00:16:51,120 --> 00:16:56,880
autonomous driving. Like these are all different application areas. And so we get people coming

182
00:16:56,880 --> 00:17:00,720
into NLP who are really interested in the machine learning, and they're just coming to publish

183
00:17:00,720 --> 00:17:05,440
in NLP venues because they're applying it to NLP. And so there's this constant influx of people who

184
00:17:05,440 --> 00:17:12,320
don't have long or deep training in either linguistics or really thinking about language.

185
00:17:13,920 --> 00:17:19,040
And so I sort of feel like I'm almost like chatting into the void. And then I talk to other

186
00:17:19,040 --> 00:17:23,840
people who say, yeah, but, but at least there is linguistics, like there's a field of study there.

187
00:17:23,840 --> 00:17:30,960
And what would it be for computer vision? And I mean, I guess it's something to do, well,

188
00:17:30,960 --> 00:17:32,880
the thing that's strange about is that, yes, there is.

189
00:17:32,880 --> 00:17:37,440
For thing, no processing is the thing that comes to mind. We study that in DSP classes.

190
00:17:38,000 --> 00:17:42,880
Yeah. And that's used also in speech technology, right, for the speech recognition.

191
00:17:42,880 --> 00:17:47,600
But if you're looking at like trying to do what people do, and you're looking at vision,

192
00:17:49,040 --> 00:17:54,000
there's something around both sort of perception. So how does the, and here I'm talking

193
00:17:54,000 --> 00:17:57,360
way outside my expertise, just want to flag that. This is not anything I don't think about.

194
00:17:57,360 --> 00:18:03,360
How does the, how does the eye and the, and the ocular nerve and all of that, like what happens

195
00:18:03,360 --> 00:18:07,600
when the light hits the retina and what happens, and then what happens in the brain that's processing it,

196
00:18:08,480 --> 00:18:14,960
to create maybe some sort of representation that just has to do with the visual stimulus,

197
00:18:14,960 --> 00:18:17,360
but then that gets connected to categories of things.

198
00:18:18,320 --> 00:18:19,280
Yeah, yeah.

199
00:18:20,240 --> 00:18:26,160
And so it's not just about sort of psychophysics and like perception and stuff like that,

200
00:18:26,160 --> 00:18:31,040
but it's also about categories and ontologies and how we understand our world.

201
00:18:31,040 --> 00:18:36,640
And so I don't think one coherent field of study, like maybe it's different parts of psychology

202
00:18:36,640 --> 00:18:41,520
that they should be talking to. But I don't see that interaction apparently happening,

203
00:18:41,520 --> 00:18:44,240
the way at least NLP and linguistics do talk to each other.

204
00:18:44,240 --> 00:18:50,800
Mm-hmm. Interesting. Interesting. Yeah, when I, when I heard you initially describe the,

205
00:18:50,800 --> 00:19:00,400
kind of how the kind of year take on, you know, machine learning folks doing NLP, it almost sounded

206
00:19:00,400 --> 00:19:06,800
a little bit like the, you know, I guess most recently I've come across this in the context of COVID,

207
00:19:06,800 --> 00:19:10,800
like data scientists running off building models and don't know anything about the

208
00:19:11,520 --> 00:19:17,760
virology. Like does it feel to you as like you might be the virologist and you've got all these

209
00:19:17,760 --> 00:19:23,120
data scientists producing all these models that don't correspond to the thing that you actually

210
00:19:23,120 --> 00:19:30,880
understand? Yes. And, and I've had the same reaction to seeing the data scientists jumping in

211
00:19:30,880 --> 00:19:34,960
around COVID. I'm really glad that you had that panel on that. By the way, those were great

212
00:19:34,960 --> 00:19:42,880
comments that your guests had because there, there seem to be something in CS education, I think.

213
00:19:42,880 --> 00:19:47,120
It's so it's somewhere in the people who are attracted to machine learning or the way we train

214
00:19:47,120 --> 00:19:53,600
them or both that basically says you are problem solvers, here are problems you solve them. And

215
00:19:54,960 --> 00:20:00,320
also the way machine learning gets sold, there's this like packaging around it that's all about

216
00:20:00,320 --> 00:20:06,320
denigrating the work it's supposed to be replacing. And so if we're going to apply machine learning

217
00:20:06,320 --> 00:20:11,040
to NLP, it's because oh, it's far too expensive to do by hand, right? You wouldn't want to hire

218
00:20:11,040 --> 00:20:16,720
somebody to write a grammar when you can just learn a grammar, right? So it's better to do it automatically

219
00:20:16,720 --> 00:20:21,200
and that has a way of devaluing the work of the people who actually understand the shape of the

220
00:20:21,200 --> 00:20:26,160
problem. And so part of my message as a linguist and NLP over these years has been, look,

221
00:20:26,160 --> 00:20:30,080
it's great to do things by machine learning, but if you want to know that you've actually done them,

222
00:20:30,080 --> 00:20:34,960
it has to be in conversation with people who understand the shape of the problem and can see

223
00:20:34,960 --> 00:20:43,440
whether your solution actually works. And so when I see data scientists teaming up with

224
00:20:43,440 --> 00:20:50,240
virologists and epidemiologists and clinicians and saying, hey, I have skills. Where do you need

225
00:20:50,240 --> 00:20:56,400
my help? That's fantastic. When I see people jumping in and saying, I'm going to solve this and

226
00:20:56,400 --> 00:21:05,840
they seem to be on their own, I get really worried. And so I don't know that things like language

227
00:21:05,840 --> 00:21:12,640
models, which have shown incredible, you know, innovation and promise and all this kind of stuff of

228
00:21:12,640 --> 00:21:18,720
late. I don't know the extent to which linguistics were involved in their creation, but they're still

229
00:21:18,720 --> 00:21:24,560
interesting. Like what, how do you kind of parse the fact that innovation is happening and we're

230
00:21:24,560 --> 00:21:31,760
doing, you know, good, interesting, useful, at least in some cases, things with kind of the viewpoint

231
00:21:31,760 --> 00:21:37,840
we previously discussed. Yeah. So there's a saying, I wish I knew who to attribute to that

232
00:21:37,840 --> 00:21:43,920
said refers to the unreasonable effectiveness of n grams. And this is before the transformers,

233
00:21:43,920 --> 00:21:49,440
this is old, right? You can do a lot of useful stuff by just counting co occurrences of words.

234
00:21:50,240 --> 00:21:56,000
And as a linguist, that's kind of a bummer. Because when I look at a string of words, it's like,

235
00:21:56,000 --> 00:22:01,200
yeah, the words occur in some order. That's not the interesting part, but it turns out that if

236
00:22:01,200 --> 00:22:06,400
you have enough data and you count words and look at the orders they occur in, that's really useful.

237
00:22:06,400 --> 00:22:10,720
It's extremely useful for speech recognition. It's extremely useful for machine translation.

238
00:22:11,920 --> 00:22:19,760
It's useful for information retrieval. So there's a lot there. And that's fine. And it's useful.

239
00:22:19,760 --> 00:22:25,920
And it sort of comes back around to this, you have the task. And how does it relate to the world?

240
00:22:25,920 --> 00:22:33,040
Right? So if your goal is better transcription of, you know, open domain, a noisy environment,

241
00:22:33,040 --> 00:22:39,040
and you've set up a task that models that well, and you're honest about which language varieties

242
00:22:39,040 --> 00:22:42,880
are represented, and you don't claim to be solving speech recognition in general when it's actually

243
00:22:42,880 --> 00:22:52,000
speech recognition for a particular variety of English. Exactly. Exactly. That's all fine.

244
00:22:52,800 --> 00:22:58,400
And then there's interesting questions of, okay, why is that working? And there's room for

245
00:22:58,400 --> 00:23:03,600
linguistic knowledge to come in and say, okay, what is it that makes this effective? And also,

246
00:23:04,320 --> 00:23:08,560
let's do some error analysis. When it's not working, is there any pattern to that? Is there anything

247
00:23:08,560 --> 00:23:14,640
about what it's missing by only looking at sentences as strings of words that can predict some of

248
00:23:14,640 --> 00:23:18,800
these things? There's a bunch of really interesting work going on right now under the rubric of

249
00:23:18,800 --> 00:23:25,680
Bertology, looking at the transformer language models and trying to figure out how much linguistic

250
00:23:25,680 --> 00:23:30,160
structure they are picking up by doing this essentially to the language modeling task.

251
00:23:30,160 --> 00:23:37,040
And Bertology, the, when you say that, is that the study of those models, or is it the,

252
00:23:38,720 --> 00:23:43,440
kind of the explosion of related models, Roberta, and things like that?

253
00:23:44,160 --> 00:23:48,400
So, Bertology, I think I usually see it used to refer to the study of the models.

254
00:23:48,400 --> 00:23:53,280
Okay. So, Bert and its kin, how is it that they're working and what is it that they're learning

255
00:23:53,280 --> 00:23:58,400
about language structure? So, there's work by, I can't do this off the top of my head. Sorry,

256
00:23:58,400 --> 00:24:04,160
I could look up references for you, but people do probing tasks. Whether it's a, you know,

257
00:24:04,160 --> 00:24:07,600
we're going to take a slice of this model and we're going to use it as a classifier and we're

258
00:24:07,600 --> 00:24:12,000
going to see if that model just sort of as trained as part of Bert has learned something about

259
00:24:12,000 --> 00:24:16,400
predicate argument structure or has it learned something about syntactic categories or syntactic

260
00:24:16,400 --> 00:24:24,400
dependencies. Interesting. And is that the line of work happening primarily from a linguistic

261
00:24:24,400 --> 00:24:30,880
perspective or primarily from a machine learning perspective, or is it both? So, the folks doing

262
00:24:30,880 --> 00:24:38,080
that tend to be teams of people who are, all of them have an interest in linguistics. I think

263
00:24:38,080 --> 00:24:42,240
primarily they tend to be folks who started off with machine learning training and then learned

264
00:24:42,240 --> 00:24:47,120
the linguistics, some of them have linguistics earlier in their training. And then occasionally,

265
00:24:47,120 --> 00:24:52,160
you'll get people who like bring linguists on as collaborators. So, that's, I mean, that's,

266
00:24:52,160 --> 00:24:57,680
that's sort of what I'm usually pushing for when I'm saying, you know, that you can't do

267
00:24:57,680 --> 00:25:02,320
machine learning without domain expertise. It's not a kind, I could sometimes get accused of

268
00:25:02,320 --> 00:25:06,320
doing gatekeeping, which is not my intention. I think that collaboration is a great, more

269
00:25:06,320 --> 00:25:12,640
perspective is great. And what I'm seeing is a lack of inclusion of the domain expertise perspective

270
00:25:12,640 --> 00:25:19,360
sometimes in the machine learning work. And when you look at something like Bert with the

271
00:25:19,920 --> 00:25:25,760
linguistic perspective, are there, is there kind of a laundry list of obvious things that,

272
00:25:25,760 --> 00:25:31,920
you know, you think need to be done or that, you know, should be looked at or that, you know,

273
00:25:31,920 --> 00:25:37,840
things that, you know, shouldn't have been done like that? Or, you know, I'm wondering, is it,

274
00:25:40,240 --> 00:25:45,680
are you speaking primarily out of kind of opportunity? Like, if we pull domain experts in,

275
00:25:45,680 --> 00:25:51,120
then, hey, maybe we could achieve some, you know, greater thing? Or is it because of the specific

276
00:25:51,120 --> 00:25:58,640
things that you see when you look at Bert? Yeah, yeah. So, I don't, can't say I have specific

277
00:25:58,640 --> 00:26:03,040
examples off the top of my head. And it's not, I mean, this is, this is a, a soap box I've been

278
00:26:03,840 --> 00:26:12,960
creeping from for years now, like, prevert. And in prevert, it's more about the, the claims of

279
00:26:12,960 --> 00:26:16,240
what's happening, right? So, if someone creates Bert and they say, hey, this is helping me do great

280
00:26:16,240 --> 00:26:20,880
speed, right, speed recognition, it's helping me do great machine translation. And then you can,

281
00:26:20,880 --> 00:26:24,400
you know, you can test that. That's a product that's a, you know, sort of a practical application,

282
00:26:24,400 --> 00:26:30,480
and it works better. Excellent. If somebody is saying, hey, Bert understands language,

283
00:26:32,800 --> 00:26:40,640
then that's a scientific claim. And I want to say, okay, show me what your tests are that allow

284
00:26:40,640 --> 00:26:44,080
you to make that claim that it understands language. And also, I can tell you on first principles

285
00:26:44,080 --> 00:26:49,440
that it doesn't. Right. Right. And you talked about this in your paper, climbing towards NLU on

286
00:26:49,440 --> 00:26:55,440
meaning form and understanding and the age of data. Yeah. And the basic premise there is that

287
00:26:56,000 --> 00:27:02,320
Bert isn't at all trained on meaning. So there's no way that it could display, you know, meaning or

288
00:27:02,320 --> 00:27:08,320
understanding of meaning. Yeah. Exactly. Exactly. That was a super fun paper to write with Alexander

289
00:27:08,320 --> 00:27:15,760
Thor. We had a blast. And it came out of actually Twitter arguments. So I had this, I think it's

290
00:27:15,760 --> 00:27:21,680
back in the end of 2018, this unending Twitter thread thread is the wrong word for it. So that

291
00:27:21,680 --> 00:27:27,280
makes it sound very linear. And it wasn't where I was basically stating the thesis of that paper,

292
00:27:27,280 --> 00:27:33,120
which is that if your training is only language modeling. So all of the training data is only form

293
00:27:33,760 --> 00:27:38,960
and your task is to predict the next bit of form or, you know, in Bert style masked bits of form

294
00:27:38,960 --> 00:27:44,160
in the middle, meaning as you say, it wasn't in the input signal. So you're not learning meaning.

295
00:27:44,160 --> 00:27:48,320
And there was an unending parade of people who wanted to pick up the other side of that argument

296
00:27:48,320 --> 00:27:55,760
with me on Twitter. And one new paraphrase, was there a best of those arguments? Because when I've

297
00:27:55,760 --> 00:28:03,920
read that, you know, I have often been in a situation of trying to explain to people, you know,

298
00:28:03,920 --> 00:28:12,800
technical or non-technical, whatever. The extent of these models and what they're really capable of

299
00:28:12,800 --> 00:28:18,560
and what they're not capable of, that kind of thing. And I really appreciated the idea of, hey,

300
00:28:18,560 --> 00:28:24,080
we're not the input, the input is not meaning. It's not understanding. So that's not going to be

301
00:28:24,080 --> 00:28:31,120
what comes out. Yeah. So I don't, I don't have a best of because it is sort of this like,

302
00:28:31,120 --> 00:28:35,200
it's not there. So how are you going to learn it? But it tends to take the form of, well,

303
00:28:35,200 --> 00:28:39,920
what if you give it lots and lots and lots of data, right? Or sometimes the arguments were,

304
00:28:39,920 --> 00:28:45,920
I think if I were to try to argue against that, I would try to say that meaning actually was

305
00:28:45,920 --> 00:28:53,600
there, but not in the form that you're used to seeing it, like in a quality, but rather implicit

306
00:28:53,600 --> 00:28:59,520
meaning as opposed to explicit meaning. Yeah. So that is one shape that the argument sometimes

307
00:28:59,520 --> 00:29:06,560
takes, which is, well, the sentences themselves represent meaning. And that one is, well, yes,

308
00:29:06,560 --> 00:29:11,520
they do, but only if you know the linguistic system behind them so that you can pull it apart.

309
00:29:11,520 --> 00:29:15,680
I was going to say, this is a Sam gets creamed by a linguist's segment of the interview.

310
00:29:18,080 --> 00:29:21,760
So at the end of the paper, we actually have a bunch of the counter arguments.

311
00:29:21,760 --> 00:29:26,960
I'm going to say this, but you're right. And so it's, it's in there. But part of what makes it

312
00:29:26,960 --> 00:29:33,120
difficult is we have to really pin down terms, right? So what do we mean by meaning? And then what

313
00:29:33,120 --> 00:29:37,440
do we mean by language model? Because you can certainly imagine language models that are modeling

314
00:29:37,440 --> 00:29:43,040
meaning as well as form. So we say, hey, language model is the only input data it has is the

315
00:29:43,040 --> 00:29:46,880
form of language. And that could be the written form. It could be spoken. It could be signs and

316
00:29:46,880 --> 00:29:54,000
signed language. And that's, that's all it sees. And meaning is a relationship between those

317
00:29:54,000 --> 00:29:58,800
linguistic forms and something outside of language. And we actually break that down into two parts.

318
00:29:58,800 --> 00:30:03,680
So the sort of biggest relation is between the form that I say and my communicative intent.

319
00:30:04,240 --> 00:30:08,720
I'm trying to communicate something to you. So for example, when the cat went jumping and we're

320
00:30:08,720 --> 00:30:12,160
both laughing, and I'm thinking of the people listening to this, like when they're out running,

321
00:30:12,160 --> 00:30:16,880
like, which is how I usually listen to your podcast, I thought, oh, wait a minute, those people don't

322
00:30:16,880 --> 00:30:21,600
know why Sam and I are both laughing. What's going on? I better say something to clue them in,

323
00:30:21,600 --> 00:30:27,200
right? So I have this communicative intent to make apparent to these people who are only listening

324
00:30:27,200 --> 00:30:32,240
audio that there was a cat in the frame. All right. So that's what I want to get across. And so then

325
00:30:32,240 --> 00:30:35,520
I figured out better podcasts than I am because I were just laughing.

326
00:30:38,640 --> 00:30:44,800
I actually have a lot of experience with online teaching. We've been doing our classes, not just

327
00:30:44,800 --> 00:30:49,920
recently, in the master's program, we've had classes in a hybrid online in person format since 2007.

328
00:30:49,920 --> 00:30:55,280
Oh, wow. So I've long years decades, almost experience with this now.

329
00:30:55,280 --> 00:31:01,920
All right. So that goes flying. I want to get that across. So then I think, okay, how can I convey

330
00:31:01,920 --> 00:31:09,120
that to somebody who's listening? And then I pick a string of words that doesn't directly contain

331
00:31:09,120 --> 00:31:14,400
catflies over my shoulder, right? It doesn't, it doesn't, it's not a picture. It's a string of words.

332
00:31:15,680 --> 00:31:21,680
To provide a clue to the listeners that would allow them to reconstruct my communicative intent.

333
00:31:21,680 --> 00:31:27,840
And so the thing about language is that it is this collaborative experience. And interesting,

334
00:31:27,840 --> 00:31:31,840
like you and I are doing some collaboration right now, right? And we're doing a lot in terms of

335
00:31:31,840 --> 00:31:36,080
the general communication stuff, the turn-taking. It helps that we can see each other, right? It makes

336
00:31:36,080 --> 00:31:40,800
the turn-taking race smoother. But we're also collaborating with our listeners. So people who are

337
00:31:40,800 --> 00:31:48,240
not here in this present moment who are listening later, hi people. They are, they are working with

338
00:31:48,240 --> 00:31:55,600
us across that time to work out, okay, given what I know about English and what these sentences can

339
00:31:55,600 --> 00:32:02,560
conventionally mean. What is Emily trying to get across to me as a listener? What is Emily trying

340
00:32:02,560 --> 00:32:07,920
to get across to Sam in the moment and vice versa? So that's what we're saying meaning is about.

341
00:32:07,920 --> 00:32:12,240
And hopefully that makes it clearer to people that it's not there if all you have are the pixels on

342
00:32:12,240 --> 00:32:21,040
the page. Are you aware of any efforts that are trying to get there? I mean, I'm envisioning

343
00:32:21,040 --> 00:32:25,280
something like, I mean, I guess we're all trying to get there or everyone that's in this research

344
00:32:25,280 --> 00:32:30,000
field of NLU is trying to get there in some way. But more specifically, I mean, I'm envisioning

345
00:32:30,000 --> 00:32:36,000
something like a cross between a, you know, a bird that's kind of anger and focused and a

346
00:32:36,000 --> 00:32:42,000
normal machine translation that is pairing, you know, meaning and sentences. Anything interesting

347
00:32:42,000 --> 00:32:46,960
happening there? So, yes. And I think that a lot of the ways in which Burke gets deployed,

348
00:32:46,960 --> 00:32:51,120
you have the pre-training, which is just language modeling. And then you have the fine tuning,

349
00:32:51,120 --> 00:32:55,680
which is giving a tiny little signal about the kind of meaning that's relevant for the present task.

350
00:32:56,960 --> 00:33:01,600
And so it's there in that sense. There's a wonderful paper, this paper by Jonathan Biskin

351
00:33:01,600 --> 00:33:11,920
colleagues, is talking about sort of the stages of the world scope of NLP. So really early on,

352
00:33:12,720 --> 00:33:18,800
you might have had just like hand constructed word lists. And then you got to corpora. So things

353
00:33:18,800 --> 00:33:25,440
like the pen tree bank, which has Wall Street Journal plus the brown corpus in it. And that's

354
00:33:25,440 --> 00:33:32,480
a couple million words of text. And then you get to like web scale. So enormous corpora.

355
00:33:33,360 --> 00:33:38,080
But it's all that is still just form. And then they're saying, well, where can we go next? And so

356
00:33:38,080 --> 00:33:45,840
there's, there's starting to be work on embedding the, the naturalization processing system in some

357
00:33:45,840 --> 00:33:52,000
sort of embodied context where you are talking with a robot. And the robot has to figure out

358
00:33:52,000 --> 00:33:58,960
what motions in the world it should be taking. So there's sort of, there's small amounts of text,

359
00:33:58,960 --> 00:34:06,480
large amounts of text, text plus embodied interactions and grounding in things like vision and

360
00:34:06,480 --> 00:34:12,480
whatnot. And then the biggest one is adding in the social. And so I think that's a really

361
00:34:12,480 --> 00:34:18,880
fantastic vision. And I'm super excited. This is not actually an ACL paper. This is a preprint

362
00:34:18,880 --> 00:34:22,560
right now. I think they're going to keep working on it and publish it somewhere else later. But ACL

363
00:34:22,560 --> 00:34:27,440
has this wonderful theme this year. ACL is the association for computational linguistics.

364
00:34:27,440 --> 00:34:33,440
Supposed to have been in Seattle in July. And I'm sad that we don't get to host people in person.

365
00:34:33,440 --> 00:34:38,640
They'll be online. And the theme session is taking stock of where we are and where we're going.

366
00:34:39,280 --> 00:34:43,040
And so there's a bunch of papers sort of looking at. Interesting. You know, how do we go in a lot

367
00:34:43,040 --> 00:34:46,960
of them seem to be focused on meaning and semantics. So people are thinking about it.

368
00:34:46,960 --> 00:34:53,120
And do you think that that particular theme is driven by the kind of recent progress with

369
00:34:53,120 --> 00:34:57,840
Bert and the like or is that is something that we talk about at these conferences every once in a

370
00:34:57,840 --> 00:35:05,440
while? I guess I'm trying to get a sense of, you know, does it feel from a linguistic and

371
00:35:05,440 --> 00:35:12,880
computational linguist? I guess as a linguist. And then maybe the next level is computational

372
00:35:12,880 --> 00:35:19,920
linguist. Does it, does it? Do you have the same feeling of excitement around what's happening,

373
00:35:19,920 --> 00:35:25,360
you know, with Bert and language models and the like as, you know, folks from a machine learning

374
00:35:25,360 --> 00:35:30,480
perspective have? Because I think from a machine learning perspective, it's kind of akin to

375
00:35:31,200 --> 00:35:36,080
2012 with, you know, deep models and computer vision. It's like, wow, we're making a lot of

376
00:35:36,080 --> 00:35:43,840
progress here. Or is it just trade? So it's actually as a computational linguist in NLP,

377
00:35:43,840 --> 00:35:51,280
it's kind of shouting for it actually. Because when I when I entered the field,

378
00:35:52,320 --> 00:35:56,800
the what I was doing was considered very old school, right? So I came into computational

379
00:35:56,800 --> 00:36:02,960
linguistics in, you know, early 2000s working on grammar engineering and that was already the

380
00:36:02,960 --> 00:36:07,520
height of statistical processing in NLP. And so everyone would frame their papers as well,

381
00:36:07,520 --> 00:36:11,040
you know, in the olden days, people would have to do this by hand. But now we can do it

382
00:36:12,320 --> 00:36:15,360
and I'm like, some of us are still here, still doing it by hand. And you know,

383
00:36:16,080 --> 00:36:20,400
that's okay to have both. Like we don't have to, you know, this room, this should be, you know,

384
00:36:20,400 --> 00:36:26,400
the world is big enough for all of us, right? And then when deep learning sort of swept through

385
00:36:26,400 --> 00:36:31,760
NLP, starting in about 2015, 2016, a bunch of the statistical learning people who have built up all

386
00:36:31,760 --> 00:36:35,680
this expertise around feature engineering were told, Oh, no, no, no, you don't have to do that by

387
00:36:35,680 --> 00:36:39,680
hand anymore. We can do that automatically. And so that's where the shouting for it comes in a

388
00:36:39,680 --> 00:36:50,400
little bit. So it's not, there was a period where it got a little boring because basically,

389
00:36:50,400 --> 00:36:55,120
you had all these existing tasks and turns out you could build those the model for it. And so

390
00:36:55,120 --> 00:37:00,720
there's all these papers that basically take existing tasks and first word embeddings in general

391
00:37:00,720 --> 00:37:03,840
from like word to back or something and then bird into the mix. And hey, look, say to the art

392
00:37:03,840 --> 00:37:08,560
paper. And it's like, does anybody actually enjoy reading these? Like,

393
00:37:11,120 --> 00:37:14,960
what do you get out of this? Right? That was my reaction to that. So,

394
00:37:14,960 --> 00:37:20,320
Bertology is really welcome. The people who start saying, okay, but why? Right? What is it about

395
00:37:20,960 --> 00:37:24,880
the pairing of this methodology and this task that makes such a big difference? That's interesting.

396
00:37:25,520 --> 00:37:30,000
And sometimes it is, hey, look inside of the deep neural net, you can see that this layer is

397
00:37:30,000 --> 00:37:34,800
picking up this kind of information. And then that sort of opens up questions. Okay, so why is

398
00:37:34,800 --> 00:37:38,960
that information available just from distribution and text? Those are scientifically interesting

399
00:37:38,960 --> 00:37:45,120
questions to me. And in other cases, there's a great paper by Niven and Kau at ACL 2019 that said,

400
00:37:46,000 --> 00:37:53,520
actually, guess what? Bert is cheating. And we can construct a data set that sort of hides

401
00:37:53,520 --> 00:37:58,960
the clues that Bert was using that were just artifacts. And then it falls to chance. Really? Yeah.

402
00:37:58,960 --> 00:38:03,040
Yeah. Interesting. Can you elaborate on that a little bit more? I haven't come across that paper.

403
00:38:03,760 --> 00:38:09,600
So this or maybe I should just interview the. Yeah, that would be great. I recommend it.

404
00:38:09,600 --> 00:38:14,480
But in brief, my non-author non-expert understanding of it was that there was,

405
00:38:15,680 --> 00:38:21,040
this was a question. I think it was, it was like textual entailment or reading comprehension or

406
00:38:21,040 --> 00:38:29,600
something. And the way the task was set up, the there was artifacts and the data that made particular

407
00:38:29,600 --> 00:38:37,520
words really good cues for a certain kind of answer. And if you construct a data set that washes

408
00:38:37,520 --> 00:38:44,080
that out, then those cues are gone. And Bert doesn't do well at all. So it wasn't necessarily a

409
00:38:44,080 --> 00:38:48,720
general statement across all tasks. It was for a particular task. For a particular task. Yeah,

410
00:38:48,720 --> 00:38:53,040
exactly. No sort of saying, hey, look, say to the art with Bert, yay, but wait a minute, that makes

411
00:38:53,040 --> 00:38:56,800
no sense. Why should Bert be doing well in this task? Let's look deeper. And for that task,

412
00:38:56,800 --> 00:39:01,920
they found that problem. Okay. Interesting. Yeah. And I think that that kind of result

413
00:39:01,920 --> 00:39:08,080
informed the design of super glue. So Sam Bowman and his colleagues are, there was the glue

414
00:39:08,080 --> 00:39:11,600
task that had to do with a language understanding. And then super glue was, okay, let's bring together

415
00:39:11,600 --> 00:39:16,880
more tasks, but let's pick the ones that are hard for Bert. Okay. So I think that's a that's a

416
00:39:16,880 --> 00:39:22,320
promising way to try to push towards tasks that better represent the questions that we're interested

417
00:39:22,320 --> 00:39:27,200
in. But you asked, you know, why do we have this theme at ACL? And I think you really have to

418
00:39:27,200 --> 00:39:33,360
ask the program chairs. So that's Joyce, Chai, Natalie, Schluter, and Joel, Tetral, who are the

419
00:39:33,360 --> 00:39:38,080
incredibly hardworking program chairs for this conference. And early on, they said, hey, it will

420
00:39:38,080 --> 00:39:43,120
make this more interesting if we solicit papers on this theme. If we actually encourage people

421
00:39:43,120 --> 00:39:48,320
to sort of lift their heads up and look a little bit more broadly at what's going on here. And

422
00:39:49,600 --> 00:39:54,000
I think I recall them saying that that was actually because of the date, basically. It was like,

423
00:39:54,000 --> 00:40:00,240
hey, 2020 round number, it's time to like sort of take stock and think about it more broadly.

424
00:40:00,240 --> 00:40:11,280
Got it. Got it. Interesting. So I'm trying to think if we've kind of fully captured the relationship

425
00:40:11,280 --> 00:40:16,400
between linguistics and computational linguistics, or maybe that's not possible to do it now, we certainly.

426
00:40:19,840 --> 00:40:25,040
Yeah, probably not possible in an hour, but I think that there's there's lots of ways in which

427
00:40:25,040 --> 00:40:30,320
linguistic knowledge can inform work on NLP. And it doesn't have to be directly in terms of encoding

428
00:40:30,320 --> 00:40:33,760
the linguistic knowledge in the system. And when I do grammar engineering, that's what I'm doing.

429
00:40:33,760 --> 00:40:38,720
And I think that's a valid mode of research, but it's not the only mode of research. But it's also

430
00:40:38,720 --> 00:40:43,360
not the only mode of research that needs linguistic knowledge. So if you're doing statistical

431
00:40:43,360 --> 00:40:47,520
machine learning, or if you're doing neural processing for NLP, there's still room for linguistics

432
00:40:47,520 --> 00:40:51,520
on the one hand in feature design. If you're doing the kind of thing, we're not trying to learn

433
00:40:51,520 --> 00:40:57,920
your features automatically. But especially in task design and error analysis and an understanding

434
00:40:57,920 --> 00:41:05,280
sort of the task data set connection and the task world connection. And we barely talked about

435
00:41:05,280 --> 00:41:10,240
the ethics stuff at all, but there's a connection there too, because one of the things that

436
00:41:10,240 --> 00:41:14,160
linguistics tells us from social linguistics is that variation is the natural state of language.

437
00:41:14,160 --> 00:41:21,200
Number one, number two, the fact that a variety is anointed as the standard is all about power

438
00:41:21,200 --> 00:41:26,000
and nothing else, nothing inherently variety. And so that means that if you've got technology that's

439
00:41:26,000 --> 00:41:31,520
working for the standard variety and not others, then you are likely to be setting up a situation

440
00:41:31,520 --> 00:41:36,160
where you're further disempowering, further disempowering marginalized people. So there's a lot

441
00:41:36,160 --> 00:41:41,760
that linguistics can do to inform the ethical deployment of NLP, especially social linguistics.

442
00:41:42,640 --> 00:41:49,120
And is your research in this area looking at the, I know a big part of it is trying to understand

443
00:41:49,120 --> 00:41:56,080
how to engage with people around this idea of ethics in NLP. Are you also looking at the kind

444
00:41:56,080 --> 00:42:02,400
of fundamental problems themselves? So the fundamental problems are broad and diverse and

445
00:42:03,920 --> 00:42:11,280
so I am interested in understanding like as we sort of get these case reports of what's

446
00:42:11,280 --> 00:42:15,440
what's gone wrong or what might go wrong, I'm always interested in how that connects the

447
00:42:15,440 --> 00:42:20,640
language variation. So when I think I mentioned to you beforehand that I've got some unpublished

448
00:42:20,640 --> 00:42:27,280
work trying to do a typology of the kinds of risks of adverse societal impact of NLP,

449
00:42:27,280 --> 00:42:30,480
and that is very much informed by what I know about language variation.

450
00:42:31,600 --> 00:42:35,120
So yeah, so working on that, but also working on, yeah.

451
00:42:35,120 --> 00:42:38,000
Does unpublished mean that you can't talk about it yet?

452
00:42:38,000 --> 00:42:40,080
No, it just means that I can't point to anybody to it.

453
00:42:43,280 --> 00:42:46,480
When you, how do you, what are some of those categories?

454
00:42:46,480 --> 00:42:52,960
So what I'm looking at it in terms of direct versus indirect stakeholders, and this comes

455
00:42:52,960 --> 00:42:58,080
out of value-sensitive design. So work about your freedom and colleagues sort of conceptualizing

456
00:42:58,080 --> 00:43:01,600
the people that we need to worry about as both the users, the people who actually interact with

457
00:43:01,600 --> 00:43:06,960
the technology directly, and other people who are affected. So we're taking that as the overarching

458
00:43:06,960 --> 00:43:11,520
thing and then thinking about the different ways that you can interact with a system as a direct

459
00:43:11,520 --> 00:43:15,200
or indirect, so the direct stakeholders might be doing it by choice or not by choice.

460
00:43:15,200 --> 00:43:20,320
And then the indirect stakeholders, and it could be, well, my words are part of a broad

461
00:43:20,320 --> 00:43:26,080
corpus, or I am a person who's subject to societal stereotypes that the technology might be perpetuating.

462
00:43:27,360 --> 00:43:30,320
Or, dang it, I got a third one over there right now that I can't do off top of my head,

463
00:43:30,320 --> 00:43:33,760
but that's the kind of category. And none of those are specifically about

464
00:43:33,760 --> 00:43:37,120
sociolinguistic variation, but rather within each of those, I say, okay,

465
00:43:37,120 --> 00:43:40,960
if I'm a direct stakeholder choosing to use something, but it doesn't represent my language

466
00:43:40,960 --> 00:43:45,840
variety well, what happens. So that's sort of an example of that.

467
00:43:47,040 --> 00:43:53,360
So the speech recognition does not work well for all Englishes.

468
00:43:54,560 --> 00:44:00,320
And so if I'm trying to use a virtual assistant and it doesn't recognize my variety,

469
00:44:00,320 --> 00:44:04,640
or it only recognizes the variety that I code switch into usually outside the home,

470
00:44:04,640 --> 00:44:08,640
but I have to use it inside the home with this virtual assistant. How does that make me feel

471
00:44:08,640 --> 00:44:14,080
about my language and how I fit into the world, right? Yeah. So there's the things like that.

472
00:44:15,520 --> 00:44:19,840
So I felt like I was going to go somewhere. So sociolinguistics informs that.

473
00:44:20,880 --> 00:44:24,640
And I think that that's an important kind of linguistics that needs to be in the

474
00:44:24,640 --> 00:44:29,440
conversation and visible to people who are working on machine learning and NLP. And I think it's

475
00:44:29,440 --> 00:44:33,840
pretty remote. It's not the sort of thing that maybe people even necessarily know about. They

476
00:44:33,840 --> 00:44:40,080
may have heard of linguistics and they give linguistics as like Chomsky maybe. So.

477
00:44:42,160 --> 00:44:47,520
And so when you talk about kind of techniques for getting folks to engage in these kind of

478
00:44:47,520 --> 00:44:52,480
discussions and think about these issues, are you primarily

479
00:44:54,480 --> 00:45:00,480
thinking about or targeting kind of the laypeople or researchers and builders?

480
00:45:00,480 --> 00:45:06,480
And so both. And I should I can't give you a pointer to a video of a talk that I gave on this.

481
00:45:06,480 --> 00:45:12,160
And it sort of ends with whose job is this, right? And I've got a bunch of categories of people.

482
00:45:12,160 --> 00:45:17,120
And we all fill multiple categories. So the first one is the researchers and developers, right?

483
00:45:17,120 --> 00:45:22,240
Then there's consumers. So when we use a product, we either buy it or we just choose to use it

484
00:45:22,240 --> 00:45:28,080
in a, like, you know, pain with our eyeballs kind of a way. The third one is for cures. So people

485
00:45:28,080 --> 00:45:34,560
who decide to buy and install and stand up a system based on NLP or machine learning technology

486
00:45:34,560 --> 00:45:39,920
more broadly, I suppose. Then we have the role of members of the public who are advocating for good

487
00:45:39,920 --> 00:45:44,160
policy and then we have policy makers. Okay. So you can see that any individual might be in multiple

488
00:45:44,160 --> 00:45:50,800
ones of those roles. And I think it's important to sort of get this knowledge out in all of those ways.

489
00:45:50,800 --> 00:45:58,400
And so that includes, you know, public engagement, things like this, right? It includes teaching.

490
00:45:58,400 --> 00:46:04,160
So I'm running a tutorial at ACL this summer with their coveans and a scotheld on integrating ethics

491
00:46:04,160 --> 00:46:10,080
into the NLP curriculum so that people hopefully early on in their training will start to see this

492
00:46:10,080 --> 00:46:17,680
stuff. And then yeah, talking about it sort of on Twitter, say. How well do you feel the

493
00:46:17,680 --> 00:46:26,800
core, like failure modes, ethical failure modes of NLP, you know, not even a mentioned machine learning,

494
00:46:26,800 --> 00:46:33,120
but NLP are understood. Like I think we throw around the example, like these word-to-vec examples,

495
00:46:33,120 --> 00:46:38,800
you know, man is to doctor as woman is to whatever. You know, I think we understand those failures

496
00:46:38,800 --> 00:46:43,680
pretty well or at least conceptually. But I'm trying to think of other ones that I've heard

497
00:46:43,680 --> 00:46:50,160
thrown around and they're very few. Yeah. So, so they're right. So you have one failure mode,

498
00:46:50,160 --> 00:46:53,600
failure mode, I'm not so it's one of these like it's actually working as intended and we don't

499
00:46:53,600 --> 00:46:59,360
want that kind of failure mode, right? So of these, when the word vector is feature not a bug,

500
00:46:59,360 --> 00:47:05,840
you commented on that podcast recently. I love that episode. Yes, exactly. So it's like that.

501
00:47:05,840 --> 00:47:09,760
It's a feature not a bug because it's doing what we told it to do and maybe we need to rethink

502
00:47:09,760 --> 00:47:15,600
what we're telling it to do, right? Because there is an awful lot of bias in our conceptualization of

503
00:47:15,600 --> 00:47:19,360
the world that comes out and how we talk about the world. And then if you say learn the patterns,

504
00:47:19,360 --> 00:47:23,600
guess what? That's one of the patterns, right? And it's not even necessarily a pattern in the

505
00:47:23,600 --> 00:47:28,080
world that it's learning. It's a pattern in how we talk about the world. And I think that a

506
00:47:28,080 --> 00:47:33,440
mistake that often gets made in these discussions of machine reading, naturally understanding,

507
00:47:33,440 --> 00:47:38,800
is that the text is the world and it's not. It's how some collection of people decided to talk

508
00:47:38,800 --> 00:47:44,880
about the world. So there's a further distinction there. So that's a quote unquote failure mode.

509
00:47:45,680 --> 00:47:49,920
You have cases, right? I mean, it's right because it's a feature not a bug, but we don't want a feature.

510
00:47:51,360 --> 00:47:58,400
You have cases where the system just doesn't do the thing it's supposed to do, right? So

511
00:47:58,400 --> 00:48:03,840
speech recognition and it gives you gibberish instead of what the person said or speech recognition

512
00:48:03,840 --> 00:48:09,120
and it just doesn't give you any words for a little while. Machine translation and it gives you

513
00:48:09,120 --> 00:48:14,160
something that maybe sounds perfectly fluent because the language model is doing its job,

514
00:48:14,160 --> 00:48:20,160
but it's a bad representation of the source language intent. And that's I think a particularly

515
00:48:20,160 --> 00:48:26,880
insidious kind of failure mode because you don't know that it's failed. It looks fluent. And so if

516
00:48:26,880 --> 00:48:30,640
if as a consumer of that technology, you're not savvy to the fact that it's just making a guess,

517
00:48:30,640 --> 00:48:36,000
you might believe that the person whose words you've translated actually said the thing that you're

518
00:48:36,000 --> 00:48:45,920
reading when in fact they didn't. Interesting. And then so what I'm hearing here is you know,

519
00:48:45,920 --> 00:48:53,520
you're saying things that sound like systems or models just breaking. But then when you take those

520
00:48:53,520 --> 00:49:04,160
and put those, you know, let's call them technical failures into the world, they are having effects

521
00:49:04,160 --> 00:49:09,120
and that creates ethical issues, but they're also potentially caused by things like the

522
00:49:10,080 --> 00:49:14,320
style of language that someone's using or accents or things like that. And that also creates

523
00:49:14,320 --> 00:49:23,600
ethical issues. Yeah. So the general framework that I am working towards for thinking about this

524
00:49:23,600 --> 00:49:28,800
is basically whenever we are moving away from this is a fun abstract toy that we're playing with

525
00:49:28,800 --> 00:49:32,880
to this is something that's going to be in the world. We have a responsibility to think about a few

526
00:49:32,880 --> 00:49:38,000
things and one of them is what are the failure modes? That's a question you're asking. Another one

527
00:49:38,000 --> 00:49:45,920
is when this is working as intended, who benefits and who's possibly harmed? And when it's not working

528
00:49:45,920 --> 00:49:51,360
as intended, who was harmed? Because those harms are not going to be evenly distributed

529
00:49:53,040 --> 00:49:56,320
in many, many cases. So this thing about social linguistics and the fact that people speak

530
00:49:56,320 --> 00:50:00,240
differently and that people who are marginalized tend to then be told that their language

531
00:50:00,240 --> 00:50:04,720
variety is not standard and it's not the one that's getting modeled means that when the system fails

532
00:50:04,720 --> 00:50:09,280
to work because of a difference in in accentual linguistic variety, that's going to fall differently

533
00:50:09,280 --> 00:50:15,840
across different groups of people. But I have to always when I get down this part of the line of

534
00:50:15,840 --> 00:50:22,080
reasoning, refer to Alvin Grissom's talk that I heard last summer where he said, you know,

535
00:50:22,080 --> 00:50:27,120
sometimes working as intended is really not anything we want. And so sometimes it's better to

536
00:50:27,120 --> 00:50:31,600
have it not work for you, right? If it's being used for surveillance, then you're happy when it

537
00:50:31,600 --> 00:50:36,800
can't understand you, provided that it's not understanding you just means it says there's

538
00:50:36,800 --> 00:50:43,520
nothing here, as opposed to misunderstanding and then attributing, you know, bad intent when

539
00:50:43,520 --> 00:50:50,080
you've said something completely innocuous. Right, right. I'll refer folks to my interview with

540
00:50:50,080 --> 00:50:56,880
Alvin from February of last year, mythologies of neuromodils and interpretability.

541
00:50:56,880 --> 00:51:06,400
Great, great talk. So when you talk about, again, kind of going back to this promoting engagement

542
00:51:06,400 --> 00:51:16,960
around these issues, have, you know, part of the challenge I think is, you know, generally getting

543
00:51:16,960 --> 00:51:22,960
folks to care about impacts on classes of people that don't involve them. Like, is that something that

544
00:51:22,960 --> 00:51:29,440
you, you know, study or at least study the people that are studying that and can kind of talk

545
00:51:29,440 --> 00:51:36,480
about how folks are thinking about that or who we should look to. So I don't have the expertise

546
00:51:36,480 --> 00:51:40,320
there. I can tell you just what I've learned by teaching this stuff. So I can tell you sort of my

547
00:51:40,320 --> 00:51:44,480
own experience reports about talking to people about this, but you're absolutely right that we

548
00:51:44,480 --> 00:51:50,160
should be looking to, you know, people who know about movement building and about sort of bringing

549
00:51:50,160 --> 00:51:57,680
people in towards working towards shared causes like that. And that's, that's not me. But certainly,

550
00:51:57,680 --> 00:52:01,920
like there was a wonderful podcast episode of Radical AI with Ruha Benjamin on and she had wonderful

551
00:52:01,920 --> 00:52:06,320
things to say about how to connect with existing work going on in the community rather than thinking

552
00:52:06,320 --> 00:52:11,360
you're the one solving it. So I think there is expertise out there that should be called on.

553
00:52:11,360 --> 00:52:18,240
What I've seen is that most people that I talk to and, you know, sample bias, I tend to talk to

554
00:52:18,240 --> 00:52:25,680
people who are interested in this. Both people that I talk to would like to be sure that the

555
00:52:25,680 --> 00:52:31,680
technology they're creating is making the world a better place and not causing harm and sort of

556
00:52:32,720 --> 00:52:37,120
feel uncomfortable with engaging with these issues. And so it's easier to just not think about it.

557
00:52:37,120 --> 00:52:42,560
It's easier to say, oh, I'm only working on the algorithm. And so one strategy is to say, well,

558
00:52:42,560 --> 00:52:46,000
what can you do to make it a little bit better? And that's what we're going with the data statements

559
00:52:46,000 --> 00:52:51,920
work is if you make clear documentation of what's in the data, then you have set it up so that people

560
00:52:51,920 --> 00:52:56,800
can say, hey, guess what? This user is not a good match for my use case. And so that's sort of a

561
00:52:56,800 --> 00:53:01,360
a positive step that's also close to our own research expertise, this dataset creators,

562
00:53:01,360 --> 00:53:06,880
and ought to be close to a research expertise as system developers who are using the data, right?

563
00:53:06,880 --> 00:53:11,600
You'd better understand what's in that dataset that you're using to test the machine learner if

564
00:53:11,600 --> 00:53:16,880
you're going to claim that you're solving some task. So all of that I think is very in domain

565
00:53:16,880 --> 00:53:24,080
and a positive step. And I have my hope, and this is now just like, you know, pure speculation and

566
00:53:24,080 --> 00:53:29,920
hope, is that when people feel like they can do something like this, they then feel more attached

567
00:53:29,920 --> 00:53:36,240
to the longer term goal of making sure that they're building things that don't exacerbate inequities

568
00:53:36,240 --> 00:53:43,760
and maybe actually start being a force for good overall. Yeah. And you just mentioned the data

569
00:53:43,760 --> 00:53:49,040
statements, right? When I haven't looked into that in detail, but when I hear that, envisioning

570
00:53:49,040 --> 00:53:58,000
something very similar to the data sheets for datasets work, the Timnick Ebru is done.

571
00:53:59,040 --> 00:54:03,920
Similar to that, what are the key differences? Absolutely similar. And in fact, it was sort of

572
00:54:03,920 --> 00:54:08,960
like there was something in the air in 2018. So you have Timnick's group doing the data sheets,

573
00:54:08,960 --> 00:54:13,200
Meg Mitchell's group doing model cards. And there was another group at the MIT Media Lab doing

574
00:54:13,200 --> 00:54:19,200
something called nutrition labels for datasets. And yes, absolutely very similar ideas. I think the

575
00:54:19,200 --> 00:54:24,480
thing that distinguishes data statements is that it was very focused on language datasets. So

576
00:54:24,480 --> 00:54:28,560
we're thinking about what do you need to document about a language dataset. The data sheets work

577
00:54:28,560 --> 00:54:34,320
is absolutely wonderful in terms of thinking about what do you need across machine learning datasets

578
00:54:34,320 --> 00:54:40,960
broadly and very, very congruent. And I think I mean, maybe Timnick and Meg were talking to each

579
00:54:40,960 --> 00:54:43,920
other, but there was sort of this moment we all kind of looked around and went, oh same.

580
00:54:47,280 --> 00:54:52,320
So yeah, absolutely, absolutely that sort of the same idea. Got it. Got it. Awesome. Well, Emily,

581
00:54:52,320 --> 00:54:57,600
it has been so wonderful having an opportunity to chat with you. Any parting thoughts

582
00:54:57,600 --> 00:55:02,640
that you'd like to leave us with? This has been a blast. I'm sorry, we only saw the cat the

583
00:55:02,640 --> 00:55:11,280
once. And I really, you know, I hope that your listeners are remain interested in both how do you

584
00:55:11,280 --> 00:55:16,480
create a clever machine learning solution? And how do you think about how it fits into the world?

585
00:55:16,480 --> 00:55:20,400
And if that second part is hard, then who do you need to talk to to figure out how it fits into

586
00:55:20,400 --> 00:55:29,040
the world for the particular task that you're working on? Great. Thanks so much. Thank you. All right,

587
00:55:29,040 --> 00:55:34,400
everyone, that's our show for today. To learn more about today's guest or the topics mentioned in

588
00:55:34,400 --> 00:55:40,880
this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast, please

589
00:55:40,880 --> 00:55:47,120
subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for listening

590
00:55:47,120 --> 00:55:53,120
and catch you next time.

