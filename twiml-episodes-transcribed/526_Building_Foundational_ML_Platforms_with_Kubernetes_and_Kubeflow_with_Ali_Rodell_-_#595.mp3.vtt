WEBVTT

00:00.000 --> 00:11.760
All right, everyone. Welcome to another episode of the Twimble AI podcast. I am your host, Sam

00:11.760 --> 00:17.280
Charrington. And today I'm joined by Ali Rodel, Senior Director of Machine Learning Engineering

00:17.280 --> 00:22.480
at Capital One. Before we get going, please take a moment to hit that subscribe button wherever

00:22.480 --> 00:27.760
you're listening to today's show. Ali, welcome to the podcast. Thank you. Thank you.

00:27.760 --> 00:32.800
Very happy to be here. I'm super excited to jump into our conversation. We'll be talking about

00:32.800 --> 00:39.520
the platform that you are building there at Capital One to support machine learning at the company.

00:39.520 --> 00:46.000
We had a great chat about old school tech just before we got started and we bring some of that

00:46.000 --> 00:52.720
vibe through it would be a lot of fun. Before we get going, I'd love to have you share a little bit

00:52.720 --> 00:59.040
about your background, your role, how you came to work in the field, all that good stuff. Sure.

00:59.040 --> 01:09.040
Yeah, old school tech. Yeah, I think I am technically old for this industry. Yeah, I'm 49. So,

01:09.840 --> 01:17.440
yeah, I've been working in software engineering or in tech in general for about 26 years. Most of that

01:17.440 --> 01:24.480
in highly regulated industries of various sorts. I've actually technically done ground up data

01:24.480 --> 01:30.880
center builds back way back in the day when we had that thing called data centers. But yeah,

01:30.880 --> 01:35.440
I started as a software engineer. I've worked in a bunch of different areas, including

01:36.880 --> 01:45.760
doing a lot of data movement, like actual migration of data around the world for these data center

01:45.760 --> 01:52.960
builds and whatnot. And just recently, or five years ago, joined Capital One, which was my

01:52.960 --> 01:57.360
actually entry into the machine learning space. Part of that, I was more in the software engineering

01:57.360 --> 02:05.360
and infrastructure space. It was brought in here to help us mature our SDLC life cycle in the

02:05.360 --> 02:11.280
machine learning space, which is now called the MDLC model development life cycle, I suppose,

02:11.280 --> 02:18.640
which I've helped to do since I started, moved in a couple different places, all of which

02:20.080 --> 02:26.960
has been in the machine learning, I guess we'll call it platform space or in the machine learning

02:26.960 --> 02:33.680
infrastructure space in general. And yeah, now I own all the model development platforms here.

02:33.680 --> 02:41.520
Awesome. So you own model development platforms there. Let's jump right into into that and talk

02:41.520 --> 02:49.920
a little bit about your, really the platform journey. Were you at Capital One from the beginning

02:49.920 --> 02:54.560
of the company's involvement in machine learning? How much of that have you seen evolve?

02:54.560 --> 03:08.160
No, I was not. It started before I got here, but it has evolved a lot. And I think in general,

03:11.600 --> 03:16.400
the industry, I mean, I don't need to tell you this industry is moving very quickly.

03:16.400 --> 03:27.520
And evolving is even an understatement, just like gargantuan shifts. When I came in,

03:29.280 --> 03:38.160
we certainly had machine learning in place. As I'm sure you've read, Capital One has made some

03:38.160 --> 03:45.440
very large scale investments in getting off of antiquated infrastructure moving into the public

03:45.440 --> 03:52.560
cloud, like really modernizing how we run everything. And because of that, we've been able to

03:52.560 --> 04:01.600
leverage the ability to redo everything over and over again. We can really evolve quickly

04:02.480 --> 04:07.440
because everything is elastic. And not to move away from the machine learning space, but

04:08.880 --> 04:14.880
at the end of the day, all this stuff has to run somewhere. So when I came in, there was

04:14.880 --> 04:20.720
certainly a fair amount of machine learning that has accelerated significantly. I actually came

04:20.720 --> 04:26.960
from working in one of our lines of business, or I came into a center area, I moved into one of

04:26.960 --> 04:33.840
our lines of business and was building up some capabilities there. We've now consolidated all

04:33.840 --> 04:40.640
of that together and are kind of building a consolidated offering. Like I guess that you could say

04:40.640 --> 04:47.680
we have matured probably faster than others because of that ability to be like elastic and

04:48.960 --> 04:54.880
basically do terraform destroy and kill everything and then create it again, you know,

04:54.880 --> 05:04.400
fairly quickly. So yeah, I was not here in the beginning, but it's been five years, so we've

05:04.400 --> 05:13.600
evolved a whole lot since then. Yeah. Yeah. Yeah. It's interesting to hear you kind of ascribe

05:13.600 --> 05:24.400
to cloud the ability to counteract what I think as two really strong kind of inertial forces,

05:24.400 --> 05:32.720
one kind of large company with lots of existing legacy systems and two highly regulated industry.

05:32.720 --> 05:38.720
And you're kind of in the nexus of both. And I guess they tend to maybe go hand in hand.

05:39.760 --> 05:43.840
Yeah. I mean, I think you're describing why my job is hard.

05:49.600 --> 05:56.880
But yeah, I think traditionally one thinks of a bank, one thinks of a large company and one thinks

05:56.880 --> 06:07.120
of static, not changing. We're afraid to upgrade it. And one definitely thinks of mainframe-based

06:07.120 --> 06:12.160
processes, even if you're using distributed compute, there's obviously a mainframe there somewhere,

06:12.160 --> 06:24.000
right? But yeah, no, that's not the case. And when you look at what we're trying to build

06:24.000 --> 06:30.320
and the speed with which we have to change things, it just wouldn't work.

06:32.000 --> 06:38.000
You're just bolting, you would be building virtualization layers on a system that was not made

06:38.000 --> 06:48.240
to move this quickly. And by that, I mean, let's take an example of, let's say you're training a

06:48.240 --> 06:56.480
very large model. And by a large model, in our case, I would mean 1,000 node spark clusters.

06:57.760 --> 07:02.160
So maybe I think that's large for me, maybe that's not large for Google. I don't know,

07:02.160 --> 07:07.680
I've never worked at Google. But large ish, right? Like large enough that you will break things

07:07.680 --> 07:12.800
when you run a large, like 1,000 node spark cluster for a feature generation run.

07:12.800 --> 07:18.080
You're not going to run that on like a, I mean, I guess you could run it on a VMware cluster,

07:18.080 --> 07:25.040
but it's not really going to scale. And you're not going to be able to leverage the fact that I

07:25.040 --> 07:32.800
can turn it off and go home and not pay for it, right? So it's not uncommon for us to run

07:32.800 --> 07:40.640
many, many, many workloads of that size at the same time. And there's a human there. I mean,

07:40.640 --> 07:44.160
of course, we run these on a schedule matter, but there's a, there's a human there waiting for it

07:44.160 --> 07:51.200
to finish. So like there's a, there's a, a way that you can leverage infrastructure now that

07:51.200 --> 07:56.160
that's par for, for working now, right? The ability to run things at that scale.

07:56.960 --> 08:01.920
In the past, I think that would have been a big deal. Now, that's just par. That's just getting

08:01.920 --> 08:05.840
us through the door. You have to be able to do that. That's the basics. That's just the foundations,

08:05.840 --> 08:09.920
right? Now we're going to be able to build on top of that. But if you don't have those foundations,

08:10.960 --> 08:18.000
you can't, like you don't even know what you can build. So maybe talk us through what it is that

08:18.000 --> 08:25.600
you are trying to build. Like what's the, the, the vision, not really kind of the far vision,

08:25.600 --> 08:32.000
but like what is the product that you are trying to offer the data scientists and machine learning

08:32.000 --> 08:42.080
engineers there at Capital One? I think our job is to, is to make it easy. Like data science is

08:42.080 --> 08:51.440
not easy, but it shouldn't be hard to do your job, right? Like my job is, and my organization's

08:51.440 --> 09:01.360
job is to make doing data science work easy. And that means I, you should be able to log on.

09:03.200 --> 09:07.840
You know, if you're, let's say you're new to the company, you log on and you see how,

09:07.840 --> 09:12.960
how do people do data science here? What, what am I supposed to do? I want to be able to show you

09:14.560 --> 09:19.760
how we do work here, right? We're in a highly regulated industry. We can't just,

09:19.760 --> 09:23.600
you don't get access to everything, right? You can't just be like, oh, I want to look at this

09:23.600 --> 09:29.200
customer record. No, no, no. There's a, there's a, there's a whole way that this is, you know,

09:29.200 --> 09:35.280
manage and dealt with. But like once you do have access to things, I want to make it easy. I want

09:35.280 --> 09:40.160
you to be able to come in, sit down. Oh, this is how we run distributed compute. This is how we

09:40.160 --> 09:45.360
run distributed training. This is how we run, do experiments. This is how we, like, spin up

09:45.360 --> 09:49.200
Jupyter Notebooks. And, and this is what you can do in a notebook. This is what you should not do in

09:49.200 --> 09:54.400
a notebook. Like, I want it to be very, very easy. I want it to be simple. Like, we, we, we try to

09:54.400 --> 10:01.200
incentivize through ease of use versus, like, declaratory, thou must do these things.

10:02.000 --> 10:08.320
Though we also, of course, have to have those two. But we try and just make data science work

10:08.320 --> 10:16.320
easy. And that's not easy. Yeah, absolutely. Does your team scope span from

10:17.120 --> 10:23.280
some of the low, low infrastructure that we've talked about all the way up to the data scientist,

10:23.280 --> 10:28.480
developer experience, you know, managing experiments and launching notebooks and all that?

10:28.480 --> 10:38.880
It does. Yeah. So, um, I, I often joke that you can't really undervalue this

10:38.880 --> 10:44.400
a strong DevOps team. Like, it's just not, actually, I guess that's not a joke. That's just a statement

10:44.400 --> 10:50.560
effect. You can't undervalue a strong DevOps team. Yeah. So we, we own design of the system,

10:51.200 --> 10:57.360
provisioning of the system, day to day running of the system. And that includes user support. So

10:57.360 --> 11:03.360
I've got teams that, you know, we'll go in and be like, have solid, like, infrastructure

11:03.360 --> 11:08.880
experience, very solid AWS experience, very solid Kubernetes, like Docker based,

11:09.520 --> 11:15.520
capability experience. And they start to verge into understanding those data science workloads

11:15.520 --> 11:18.800
and those machine learning workloads, but they're not the folks who are going to go in and help

11:18.800 --> 11:25.360
troubleshoot, like, why isn't my H2O predict function working? And then I've got the other side,

11:25.360 --> 11:32.080
which is folks who are doing, like, hands-on implementation of reusable capabilities that

11:32.080 --> 11:38.880
data scientists will work from and actually utilize. So it kind of runs the gambit. And those,

11:38.880 --> 11:44.640
those teams support each other. I'm not asking a DevOps engineer to necessarily understand the

11:44.640 --> 11:51.520
underpinnings of how machine learning training is executed. But I'm under, I'm asking him to understand

11:51.520 --> 12:00.480
the profile of it and how it executes. So for example, that 1,000 nodes barc cluster I mentioned

12:00.480 --> 12:08.640
in the past, you know, 10 minutes, you can kill a Kubernetes cluster very easily with large-scale

12:08.640 --> 12:15.200
distributed compute. So those DevOps engineers, those platform teams need to know how to scale that.

12:15.200 --> 12:21.440
And one thing that's interesting about the conversation we're having so far is the,

12:23.440 --> 12:30.720
again, kind of the conversation is kind of grounded in infrastructure. And clearly you're coming

12:30.720 --> 12:36.240
at this from like a software engineering perspective where others come at this from a data science

12:36.240 --> 12:45.280
perspective and, you know, they're building the same tools, but worrying a lot less about the,

12:45.280 --> 12:50.480
you know, the infrastructure that's running that. I'm really curious your take on, you know,

12:50.480 --> 12:55.120
the impact that that's had on, you know, what you're building, the way you think about what you're

12:55.120 --> 13:06.000
building, the user experience there. Yeah, it's interesting. It does seem like people usually come

13:06.000 --> 13:11.680
at this from one angle or the other, right? They come into this from either I have an academic

13:11.680 --> 13:19.280
background or I'm like this new machine learning engineering background, but focused on the actual

13:19.280 --> 13:24.320
implementation of machine learning. Or, and I suppose it is less common, they come at it from a

13:24.320 --> 13:31.760
systems background or from a software engineering background. Yeah, I, I have found that

13:34.480 --> 13:42.240
when you boil all of the problems down that we run into on a regular basis,

13:43.280 --> 13:51.360
they really generally fall down to software engineering fundamentals under the hood. And

13:51.360 --> 13:57.840
really, when you really boil it down, things are still constrained by the same constraints

13:57.840 --> 14:07.680
they were 30 years ago. Memory, CPU, and disk IO. I mean, we have many layers of virtualization

14:08.480 --> 14:13.600
before we actually get down to some of these, but at the end of the day, the fundamentals

14:13.600 --> 14:21.600
are still there. So, so now when you're going way up these layers, we're building these platforms,

14:21.600 --> 14:28.240
and you know, I'm not the only person, I'm sure that encounters this, but like when you're building

14:28.240 --> 14:40.800
these platforms, you're, you're trying to optimize a user experience in a very immature software stack,

14:40.800 --> 14:48.800
the machine learning ecosystem, fairly immature. So, with the, like, immature stack that we're

14:48.800 --> 14:56.800
working with, and, and what I mean by that is, like, there's a lot of, what I would refer to

14:56.800 --> 15:04.160
as lazy, lazy engineering. You, you run into a lot of things that, that run it, like, have to run

15:04.160 --> 15:11.040
as root. This has to run as root. Nothing has to run as root. You're just not trying to make it

15:11.040 --> 15:16.160
not run as root. Meaning stuff that your team has created, or stuff that like vendors are providing,

15:16.160 --> 15:22.400
or open source tools. Then vendors are providing open source tools. We use a lot of open source software

15:22.400 --> 15:31.280
as, as do many places, who are, you know, we're here in the machine learning space. Many things need

15:31.280 --> 15:38.960
to run as root. I'm taking an example, a Kotib, a hyper parameter optimization tool, has to run as root.

15:40.400 --> 15:44.400
I'm sure it doesn't have to run as root. I'm sure there is a way to have it not run as root,

15:44.400 --> 15:51.360
but, but out of the box, it does. So, that means, you know, we have to do something to make it. So,

15:51.360 --> 15:57.120
it can't run as root, because I work in a very secure place. I don't let people run things as

15:57.120 --> 16:02.480
root, right? That is a big, big no-no, right? And I programmatically prevent that from happening,

16:02.480 --> 16:12.080
right? So, just as an example, like, coming at this from a systems background, you know,

16:12.080 --> 16:20.240
we're solving these problems, same problems we solved 20 years ago, with the Java ecosystem,

16:20.240 --> 16:26.480
when nothing would run and you couldn't install anything. Here we are today, in the machine learning

16:26.480 --> 16:33.600
ecosystem, by the way, Java ecosystem solved it, right? Sudo app get install Tomcat, and you're good,

16:33.600 --> 16:39.520
right? That didn't work 20 years ago. Nowadays, you can't do Sudo app get install ML platform,

16:40.160 --> 16:44.640
right? So, you have to solve all these problems.

16:45.360 --> 16:51.920
The Java solve that or Linux solve that? Linux solve that. Yeah, you're right. I guess maven to

16:51.920 --> 16:58.320
some extent, but you're right. Linux, all that. I mean, package management has made working in

16:58.320 --> 17:05.520
software engineering just so much more pleasant. It really used to suck.

17:07.200 --> 17:13.600
I referenced us talking about crafty technologies and, like, old school experiences earlier,

17:13.600 --> 17:18.960
and part of that conversation was, like, the JavaScript ecosystem and package management and

17:18.960 --> 17:26.960
all that kind of stuff. I think to your earlier point, like, the same fundamental computing problems

17:26.960 --> 17:38.160
occur in different domains. So, do you think we'll get to app get install, experiment management

17:38.160 --> 17:50.400
system or ML ecosystem? I do. I do. I mean, when you look at the problems that are associated with

17:50.400 --> 17:55.440
installing software, when that software is responsible for generating large amounts of money,

17:56.480 --> 18:02.400
those problems tend to get solved. And the open source ecosystem

18:02.400 --> 18:10.960
has really changed the game as far as the scale of people trying to solve problems.

18:12.000 --> 18:16.960
So, it's kind of like a ratchet. I feel like it's like a ratchet. When things get solved in

18:16.960 --> 18:22.800
the open source community, they don't tend to get unsolved again, right? So, it's like, click,

18:22.800 --> 18:28.800
click forward. Oh, we've solved this package management problem. Okay, we don't have to worry

18:28.800 --> 18:34.960
about package managers anymore. I mean, you do, but you don't really, right? You know, or click,

18:34.960 --> 18:44.080
click, click forward. Oh, we've solved, I don't know, running large scale like app server type stuff,

18:44.080 --> 18:48.400
right? Like Tomcat, whatever you want to insert there. Like, great. We don't have to solve that

18:48.400 --> 18:54.080
anymore because the, you know, there's thousands of engineers that contribute little changes to these

18:54.080 --> 19:02.720
things. So, I think we, I think it will get solved, but it'll just open up other problems. We'll have

19:02.720 --> 19:09.920
to fix, you know, follow on problems, we'll have to fix that. And you also characterize that as

19:09.920 --> 19:21.520
AppGet install ML ecosystem. Do you think that it, it is at that scope ML ecosystem or like individual

19:21.520 --> 19:28.320
tools that people will mix and match into some, some broader thing? Like, how much,

19:29.040 --> 19:35.520
another way to ask that is how customized to your environment and specific needs is the,

19:36.400 --> 19:40.480
the platform that you're building, which you know, still we haven't really talked very

19:40.480 --> 19:48.240
concretely about, we've talked about it pretty abstractly, but how's, you know, customized,

19:48.240 --> 19:54.240
is it to your specific needs? So, that's the second one's a good question, the first one's a good

19:54.240 --> 20:01.040
question. So, I think, you know, would it be at the ecosystem level or would it be at the individual

20:01.040 --> 20:09.520
component level? I don't know. You know, I don't know. I think that vendor tools will allow you to do

20:09.520 --> 20:16.160
that as a ecosystem level. I think eventually you will probably get to an open source ability to

20:16.160 --> 20:21.360
do that. I mean, today, if you don't work in a highly regulated lockdown environment, you can

20:21.360 --> 20:30.160
install Coopflow and it works, right? And it's, and it's pretty good. That being said, in a company

20:30.160 --> 20:37.120
that controls things, you know, either a little bit or a lot, and us being, of course, on the

20:37.120 --> 20:48.240
allot side, nothing will run without being modified. So, you know, I think you'll find in, really,

20:48.240 --> 20:57.120
any super lockdown place, everything will have to be modified. So, from our perspective, you know,

20:57.680 --> 21:02.000
we have to, we have to touch everything. It's one of the reasons why we like open source software

21:02.000 --> 21:07.280
so much because you can pop open the hood and see what's really happening. And you also, you can

21:07.280 --> 21:14.000
change things, right? And, and in some cases, we will actually modify, you know, even the like,

21:14.000 --> 21:20.960
base image stuff in some of these images so that it'll run here. Now, we try to commit some of

21:20.960 --> 21:26.080
those changes, of course, backup stream, but, you know, we have a bit of a build process there.

21:26.080 --> 21:38.000
So, I would say that we modify a lot of the infrastructure side, but we try to enable

21:39.120 --> 21:47.280
capabilities that are fairly standard on the user interface side. Like, if you're using open

21:47.280 --> 21:53.520
source projects, we try not to change how they're used. Like, otherwise, you're just getting too

21:53.520 --> 21:59.040
much into it. And then it makes it very hard to do upgrades. So, we try to keep things fairly vanilla

21:59.040 --> 22:10.000
there. And when you look at the Kubernetes ecosystem with machine learning, you just get this

22:10.000 --> 22:17.200
big wealth of open source capabilities you can use. And what we end up doing is we end up

22:17.200 --> 22:25.440
really modifying how it'll run. But we try to give, we try to make it so that people can apply

22:25.440 --> 22:32.800
their knowledge from elsewhere here, right? I don't know what any of this is, right? We want

22:32.800 --> 22:38.000
them to come in and say, oh, I've used this before and have a, have a familiar interface.

22:38.000 --> 22:46.400
Yeah. So, kind of taking a step back when you think about the platform that you're building,

22:47.520 --> 22:52.800
what are the main components and how do they fit together to create the experience you're

22:52.800 --> 23:00.880
looking to create for your users? Sure. So, when you think about our data science community,

23:00.880 --> 23:10.640
they range wildly from stats, PhD, kind of sort of knows Python to super strong. What I would

23:10.640 --> 23:16.320
even refer to as a full stack data scientist, like, they're very strong engineers. So, we have to

23:16.320 --> 23:25.040
enable all of them to do each of the parts. And when you think about component-wise, maybe I can

23:25.040 --> 23:30.320
focus on like the stages of the, of a model going through the system and all the pieces we have to

23:30.320 --> 23:36.800
enable, you know, when, when, no matter if they're like the stats PhD or the full stack data science

23:36.800 --> 23:42.560
is person, they need to come in and they need to be able to explore. They need to be able to pull in

23:42.560 --> 23:47.040
data, right? So, we've got to provide Jupyter notebooks. We need to provide the ability to do

23:47.040 --> 23:52.240
exploration locally within the constraints of being able to pull data. You're not going to run

23:52.240 --> 23:57.360
pull five terabytes of data down to your laptop. So, we need to provide Jupyter notebooks. We need

23:57.360 --> 24:05.760
to provide an entry point to the ability to run large-scale distributed compute from those

24:06.720 --> 24:15.120
Jupyter notebooks, potentially from local desktop as well from their IDE. And then when they start

24:15.120 --> 24:21.600
to need to really dig into the data, they're going to be running large-scale distributed compute

24:21.600 --> 24:26.000
generally. I mean, maybe small-scale, but either way, not much is done in a single instance

24:26.000 --> 24:33.520
these days that is going to have significant predictive value. So, we then need to enable

24:34.560 --> 24:43.280
basically spark and task capabilities to be able to be run. I'll say easy in an easy way,

24:43.280 --> 24:47.920
which is kind of a counter statement because nothing about distributed compute is easy,

24:47.920 --> 24:54.800
but we need to enable basically spark and task workloads so that people can run those workloads.

24:54.800 --> 25:02.800
We do provide, I call it t-shirt size clusters that people can use. Small, medium, large,

25:02.800 --> 25:10.480
extra, large, extra, extra, extra large, that type of thing so that people can work with it.

25:10.480 --> 25:22.800
And then once they're ready to start to settle on what they really need from a feature set

25:22.800 --> 25:27.840
perspective, they need to be able to put that somewhere securely. They need to be able to put

25:27.840 --> 25:38.800
that somewhere so it can be worked with and honed in on. So, we provide abstractions that we build

25:38.800 --> 25:47.520
for streaming data to our data lake. And by mean abstractions, I want to make it easy,

25:47.520 --> 25:52.560
I want to make it so that they can just dump data. Basically, I don't think that's the actual

25:52.560 --> 26:00.640
method call, but it's similar and dump that data. So, they're working within our ecosystem,

26:00.640 --> 26:04.880
they're working in an notebook. We provide reusable components that they can work

26:06.560 --> 26:12.320
with in each of these. There's a workflow engine in there, of course, for running like a

26:12.320 --> 26:22.160
DAG-based workflows. And then they've got their data. Same mechanisms or same entry points,

26:22.160 --> 26:28.240
they're going to start training models. So, there's reusable components for that. It is not as

26:29.280 --> 26:36.720
easy as just using the same configurations, of course, for every model. Everything is nuanced.

26:36.720 --> 26:42.960
So, we provide customized ability, but we provide base capabilities that people can work from on

26:42.960 --> 26:49.600
that. And then, of course, once they have their model trained, there's a thing along the way,

26:49.600 --> 26:53.360
there's hyper parameter optimization, they're going to run. All this is infrastructure that needs

26:53.360 --> 26:59.600
to be spun off and worked with, ideally, in a way that our data scientists aren't even aware

26:59.600 --> 27:05.920
that it's happening other than spin up time, then they need to, of course, package and deploy

27:05.920 --> 27:11.680
the model. And that's a whole other process that is kicked off and is very controlled. But

27:13.760 --> 27:19.200
those are kind of like the building blocks, I guess. And there's a lot under the hood there. There's

27:20.720 --> 27:29.440
there's a maybe 30, 50 different Kubernetes clusters that we're working with to enable a lot of

27:29.440 --> 27:38.800
these users, very large-scale AWS infrastructure under the hood, all of which, ideally, nobody will

27:38.800 --> 27:50.880
know about. Through those layers of abstraction. And to what degree does Qflow provide the various

27:50.880 --> 27:58.480
functionality that you're exploring or have you had to plug things into Qflow for the various,

27:58.480 --> 28:01.760
you know, user-facing elements. You mentioned like training models,

28:02.560 --> 28:08.800
experiment management, pushing data, different places. These things that you're kind of using

28:09.840 --> 28:15.040
the primitives that the Qflow is providing, or are you using more specialized packages in

28:15.040 --> 28:22.800
that Qflow environment? So we use some capabilities in Qflow, like their notebook provisioner,

28:22.800 --> 28:29.280
we use the capabilities on hyperparameter optimization, so that cotid embeddings.

28:31.600 --> 28:39.920
We use a lot of the visualization capabilities for Qflow pipelines. And the Qflow pipelines

28:39.920 --> 28:48.080
are go abstraction. We use our go. And yeah, Qflow pipelines is basically an abstraction built

28:48.080 --> 28:54.480
on top of our go workflow. Now there is movement, of course, to potentially add other workflow

28:54.480 --> 29:02.560
engines in there, but we're using our go. But then there are other aspects of what Qflow provides

29:02.560 --> 29:09.280
that we do not enable. So there are some of the experiment tracking stuff, artifacts store,

29:10.080 --> 29:16.560
some other capabilities there that we would rather control, and we would rather route data to our

29:16.560 --> 29:22.960
backend systems. So we have other libraries and custom built capabilities there that are either

29:22.960 --> 29:27.120
abstracted away, so you don't even need to know about it. So like I'm gathering metadata on you,

29:27.120 --> 29:32.320
and you're on your model, like you shouldn't have to tell me what you're doing, or I should be

29:32.320 --> 29:38.240
able to correct that, get that myself. And then there are other capabilities that come shipped with

29:38.240 --> 29:45.760
Qflow, but we will swap them out. So like we run our own spark abstractions, or our own

29:45.760 --> 29:50.400
dashed abstractions, that kind of thing. And technically you could use what Qflow comes with, but

29:51.200 --> 29:55.840
we need, we want it to be a little different. We need it to be either a little easier to use,

29:55.840 --> 30:02.640
or we want it to scale better, or something like that. So it's a combination.

30:02.640 --> 30:11.520
And you've referenced scaling and distributed compute quite a bit thus far. I'd love to have

30:11.520 --> 30:15.840
you talk a little bit more about that, particularly from the perspective of the challenges that you've

30:15.840 --> 30:21.600
run into, you know, building and managing that kind of environment. Sure. And this is one of the

30:21.600 --> 30:31.040
harder parts of what we are enabling. And one of the more fun parts probably, just to fix.

30:31.040 --> 30:38.640
So, I mean, spark and dusk are fairly standard, just generally, I think, from a running

30:38.640 --> 30:44.880
distributed compute perspective. They both can kill a Kubernetes cluster.

30:50.400 --> 30:55.440
And I can even, and as can as can Argo, actually, by the way.

30:55.440 --> 31:01.760
And what does that mean? Kill the Kubernetes cluster, right? Because you kill a node and it's

31:01.760 --> 31:08.400
supposed to pop back up. Right. But if you kill the control plane, nothing pops back up.

31:09.760 --> 31:18.480
So yeah, we've had situations with both. So I would say there's a couple of things that you just

31:18.480 --> 31:26.720
have to consider when running any multi-tenant platform like this. One is you're running a multi-tenant

31:26.720 --> 31:32.800
platform. There are a bunch of people working on this platform. And in this case, in a user-interactive

31:32.800 --> 31:39.200
mode, in a Jupyter Notebook, potentially in a different UI, but they're working. This is their

31:39.200 --> 31:46.160
job. This is where they work. So you have to be careful. You don't want to let people run

31:46.160 --> 31:52.880
very, very large-scale stuff that could kill the cluster. I'll answer what I meant, what your

31:52.880 --> 31:58.240
question about. What does it mean kill the cluster in just a second? So you just have to be careful.

31:58.240 --> 32:08.160
What I mean by kill the cluster is Spark, for example, when you shut down a Spark cluster,

32:08.160 --> 32:17.200
the actual Spark destroy hits the Kubernetes, can hit the Kubernetes API server all at once.

32:17.840 --> 32:24.320
It doesn't cascade. So if you run a thousand node Spark cluster, it's a thousand concurrent

32:24.320 --> 32:31.840
requests against the Kubernetes API server. If you don't, message floods blowing up the

32:31.840 --> 32:43.280
controller. Your DDoSIC itself. Yeah. Yeah. Yeah. So there are ways to make that not happen.

32:43.280 --> 32:49.680
There are ways to use different schedulers and whatnot. But at the end of the day,

32:50.320 --> 32:54.720
if you DDoS your API server, there's a chance you're not going to recover the cluster.

32:54.720 --> 32:58.000
There are aspects of the Kubernetes control plane that are a little fragile.

32:58.000 --> 33:09.120
But on the other side, in some other cases, we've seen DASC, for example, basically CRUSH

33:09.120 --> 33:16.560
at CD. The CD is the key value store that Kubernetes uses on the backend. DASC has CRUSHed.

33:18.560 --> 33:23.760
I actually just discussed this the other day. I think at 800 nodes, if a certain configuration,

33:23.760 --> 33:34.240
at CD fell over. And once at CD is down, you're done. You're very, very hard to recover.

33:34.240 --> 33:42.240
So that's what I mean by kill a cluster. It can happen. So we segregate workloads.

33:42.800 --> 33:53.360
We actually, we dubbed the Jet Train cluster because we let people run as fast as they

33:53.360 --> 34:02.720
wanted. We have since killed that cluster. So we're coming up with a new name. But basically,

34:02.720 --> 34:11.840
you segregate workloads. And you ensure that your users don't expect those things to stick around.

34:11.840 --> 34:23.920
We want people to push the boundaries. We want people to run the level of scale they need to do

34:23.920 --> 34:28.560
their job. So it's my job to make sure they don't notice when something falls over.

34:30.000 --> 34:36.640
So we segregate and we make it so that if you have to try again, it's as quick as possible.

34:36.640 --> 34:45.520
But yeah, it's challenging at scale. Like I said before, it all comes down to this fundamental

34:45.520 --> 34:52.080
three things, CPU memory and disk. With so many levels of abstraction on top of those things,

34:52.080 --> 34:58.320
you don't always know which ones falling over, but something falls over. And so it's just

34:59.360 --> 35:04.560
understanding the nature of your workloads and routing things to the right place. I know

35:04.560 --> 35:10.560
I won't say, of course, the names of the models, but I know certain models that require a very

35:10.560 --> 35:18.160
large amount of compute here. And I know when they need to run very large workloads. So we route them

35:20.000 --> 35:24.480
accordingly. So they don't crush their neighbors. Yeah. Yeah.

35:24.480 --> 35:36.240
You talked about the control plane issues. Kubernetes is also, you know, people have been trying to

35:36.240 --> 35:43.920
run stateful workloads on Kubernetes for a long time. A lot of people recommend against it

35:44.720 --> 35:51.840
as well. Like, have you run into issues with the stateful nature of machine learning workloads

35:51.840 --> 36:01.920
and the kind of inherently stateless nature of Kubernetes? Yes, we have. And some of it is,

36:01.920 --> 36:12.240
I think, users being used to being users who are used to the like dumping everything to HDFS

36:12.240 --> 36:25.200
on a persistent like EMR cluster in AWS, for example, versus not doing that. Yes, we have. So

36:26.240 --> 36:38.480
what we've had to balance is speed of nodes spin up and the ability to scale massively with

36:38.480 --> 36:51.040
the ease of dumping to essentially persistent scratch space, or the ease of being able to work

36:51.040 --> 36:59.680
in a stateful manner. And our teams who are running very large distributed capabilities seem

36:59.680 --> 37:09.840
willing to make that trade off. So dumping like snapshots to S3, which is a little slower,

37:11.120 --> 37:18.160
but allows you to recover, you know, if they're failures, still it works. And yes, there are

37:18.160 --> 37:27.040
capabilities you can deploy for intra cluster storage, MINIO, being one of them. I think there are

37:27.040 --> 37:34.080
others out there as well. But we actually haven't, we haven't gone with any of them yet. We haven't

37:34.080 --> 37:40.560
started using them. So, so yeah, I think, I think for us, it was just a trade off, but you're right,

37:40.560 --> 37:48.640
it is, it is an interesting choice for the entire industry to settle on Kubernetes, which is an

37:48.640 --> 37:55.280
inherently stateless execution engine when we are working with inherently stateful

37:55.280 --> 38:06.880
workloads. It is an interesting quandary. You spoke previously about the range of user

38:07.520 --> 38:17.120
personas, let's say that you need to support some folks, you know, their approach to that is

38:17.920 --> 38:24.320
to, you know, for those folks that prefer working in Jupiter notebooks to like try to turn the

38:24.320 --> 38:31.760
Jupiter notebook into this executable, deployable kind of artifact, other folks, you know,

38:32.400 --> 38:37.200
kind of say, hey, we're going to teach all our data scientists how to use, you know, Git and

38:39.120 --> 38:44.640
you know, QCTO and whatever those tools are, low level tools, like how have you managed,

38:45.680 --> 38:51.680
you know, those kinds of decisions and what approach do you take there?

38:51.680 --> 38:57.760
So, I'll just say first, first hand, you have to use Git, you have to use source control,

38:58.640 --> 39:03.280
no matter where you fall on that, on that paradigm or on that range.

39:04.080 --> 39:12.560
And to be clear, I think for some, it's less about, you know, using it or not, but about like

39:12.560 --> 39:18.640
abstracting it so that, you know, every time a notebook is checkpointed, it's automatically

39:18.640 --> 39:22.880
creating a, it's automatically committing and the user doesn't know anything about it.

39:23.920 --> 39:31.520
Are you, are you specifically saying that your users have to use it and are manually committing

39:31.520 --> 39:40.400
stuff? That's a, a non-negotiable in that environment? Yes, yes, I believe I am saying that.

39:40.400 --> 39:49.440
So, and the reason being, you're making me think, actually. So, the reason being that

39:50.160 --> 39:56.480
we need to be able to recreate everything we do, right? Look, we need to be able to explain

39:56.480 --> 40:03.680
how we got from this idea to this model score. And in order to do that, we need to be able to

40:03.680 --> 40:13.360
rebuild this model from scratch. Now, you can snapshot, you know, you can use these notebooks

40:13.360 --> 40:19.840
as first class citizens in, in execution and let, and you can, you know, paper mill, there's

40:19.840 --> 40:25.760
a couple other projects out there. Yeah. And they chose to basically say, well,

40:25.760 --> 40:30.240
data scientists want to work in notebooks. Great. Let's treat this as a first class citizen,

40:30.240 --> 40:41.200
as a really stable, operationally executable body of work. And, and we are, I would say,

40:41.200 --> 40:51.440
we are currently thinking about that. Right now, we, we do not have anything like that in place.

40:51.440 --> 40:57.040
We do allow people to, of course, work in juvenile books. Use these for experiments.

40:57.040 --> 41:02.720
We will leave them running for a fairly long period of time. I don't really like the idea of

41:02.720 --> 41:09.120
this thing being permanent. But, but we will let them, you know, keep them up and come back in,

41:09.120 --> 41:14.640
start working the next day. But at the end of the day, as it is today, we, we do not think of

41:14.640 --> 41:23.360
these things as operationally executable. That may change in the future. We do ask our data

41:23.360 --> 41:34.400
scientists to, to use source control to do some level of local, you know, development work.

41:36.240 --> 41:41.120
Not a, not a ton. You can absolutely work on a single instance compute node with the

41:41.120 --> 41:48.000
Jupyter notebook and have a serialized model object at the end of your process and have a team

41:48.000 --> 41:53.120
do something with that quote unquote. That, that, that is a, that is a way you can work. But,

41:54.400 --> 42:02.800
we, we, we have found that people graduate from that and need more capabilities.

42:03.440 --> 42:10.000
And when you only work in a Jupyter notebook, it is hard to enforce

42:10.000 --> 42:21.200
rigor in code that goes into like building those models. So, we, we have, we're, it's a balance.

42:21.200 --> 42:25.200
It's a balance, right? I don't want to turn my data scientists into software engineers.

42:25.200 --> 42:28.880
Definitely don't want to have to do that, right? I want them to do data science work.

42:29.680 --> 42:34.960
But, I also want to be able to recreate the model they're building. And I want to be able to

42:34.960 --> 42:41.520
have another data scientist come in and work on that model and, and, and be able to, you know,

42:41.520 --> 42:46.720
get clone the code and, and, and, and start working. So, it's a balance.

42:47.600 --> 42:54.000
Early on, you refer, and in fact, a couple of times in this conversation, you referred back to kind of

42:54.000 --> 43:02.800
the, the, these three foundational constraints that ultimately you're trying to deal with,

43:02.800 --> 43:14.720
you know, compute memory disk. Can you, are there, are there kind of a, you know, set of guiding

43:14.720 --> 43:21.200
principles that you apply? Like, when I think about those, you know, those are, you know,

43:21.200 --> 43:28.640
longstanding constraints, you know, computational constraints, you're coming from a software

43:28.640 --> 43:33.440
engineering, kind of a traditional software engineering approach. You know, we've talked a little bit

43:33.440 --> 43:40.800
about how, how that perspective is different from folks that come at it from more of the model

43:40.800 --> 43:46.000
perspective. I'm wondering, you may be abstracting a little bit, are, are there software engineering

43:46.000 --> 43:53.040
principles that, kind of guide the way you approach these problems? It's, it's a little bit of a

43:53.040 --> 44:00.560
joke, but I actually almost reverse, reverse the principle in this concept, in this situation.

44:01.680 --> 44:08.960
A lot of people think that compute is unlimited, right? Because we, we work in the public cloud,

44:08.960 --> 44:17.680
like everybody, everybody read, like compute is infinite in the cloud. So, one of the things we,

44:17.680 --> 44:27.200
we try and do is make sure that people understand that it is not. It is very scalable, but it is not

44:27.200 --> 44:37.120
unlimited. So, so, when you think about, like, software engineering fundamentals, and, and this

44:37.120 --> 44:42.160
is not necessarily applied to data science work, but it's more applied to the productionization of

44:42.160 --> 44:48.960
what you're building and, and, and some of the actual building of those platforms. Testing is a

44:48.960 --> 44:57.520
really big part of the basics here, right? If even, even for data science work, like unit tests

44:57.520 --> 45:02.720
are not a bad idea to write, for, for, you know, some of the, the logic that's being built.

45:03.360 --> 45:09.120
So, testing is a really big thing that we always come back to, and building just solid

45:09.120 --> 45:16.640
build processes and, like standards, implement, like standards enforcement on this stuff.

45:18.400 --> 45:26.400
So, really down to, like, the basics of the SDLC life cycle. A lot of what I think of as my job

45:26.400 --> 45:34.080
is actually to just be a very annoying person for my engineers. Like, I, I, you know, I ask about

45:34.080 --> 45:43.040
how, like, okay, where are your tests? Okay, great, okay. How, how, how are we focusing on our

45:43.040 --> 45:48.800
code quality? Like, like, like, never, these conversations never stop, right? And, and I, I really,

45:49.360 --> 45:57.760
I really focus a lot on those basics on, on, you know, microservices based, like, abstractions,

45:57.760 --> 46:02.560
right? Like, are we doing too much in one unit? Right? Like, just some of these, like,

46:02.560 --> 46:12.000
fundamental principles, we don't really ever get away from them. So, so we focus a lot on that,

46:12.000 --> 46:18.640
even in, like, developing components, for example, like, for, like, reusable components for

46:18.640 --> 46:23.920
executing within, like, a machine learning, like a DAG for running machine learning model training.

46:25.040 --> 46:32.160
Like, there's a discussion going on actually around how much do you put into a reusable component,

46:32.160 --> 46:37.200
because there's overhead in, in splitting between the components. So, so, like, thinking about that

46:37.200 --> 46:42.880
from a microservices based architecture perspective and, and focusing on just those building blocks.

46:44.880 --> 46:49.760
That's, that's me being annoying, basically, with my engineers. Like, we're, we're not focusing on,

46:50.320 --> 46:54.080
on the high-level stuff, we're focusing on, like, the basics of software engineering.

46:54.080 --> 47:02.400
I'm a big, I'm, I'm a big proponent of a lot of testing, automated testing in particular.

47:02.400 --> 47:11.360
Yeah, and you referenced, you referenced DevOps earlier, and some of the, I guess,

47:11.360 --> 47:17.280
CICDs, not strictly a DevOps thing, but I figure, like, the, I'm imagining kind of the,

47:18.000 --> 47:22.080
this idea of continuous delivery, continuous integration of that figure,

47:22.080 --> 47:28.880
you're strongly into the way you say some of the, the discipline that you're trying to create there.

47:28.880 --> 47:36.560
It does, it does, and I think there's two angles to it. There's the building of the platform,

47:36.560 --> 47:44.640
and there's the using of the platform. So, within building the platform, it is, as inherent,

47:44.640 --> 47:51.680
in the platform building, as it is in any software project, right? I have,

47:55.040 --> 48:00.480
myriad conversations with my engineers, and with my, like, distinguished engineer folks,

48:00.480 --> 48:07.680
who are kind of guiding things around, you know, how are the nightly builds going? How are we,

48:07.680 --> 48:13.600
you know, like, are we release candidates in the downstream testing from other systems,

48:13.600 --> 48:21.520
the usual stuff, right? And then there is an element of that in the using the platform,

48:21.520 --> 48:28.400
but it shouldn't be as much, right? At the end of the day, for model training, there isn't,

48:28.400 --> 48:34.000
like, I don't really want my data scientists to have to deal with, like, CICD capabilities,

48:34.000 --> 48:40.240
but at the end of the day, they do, they do generate something at the end of their work that is

48:40.240 --> 48:48.320
then an executable, right? So, that, the building of that needs to be

48:50.160 --> 48:56.800
repeatable. You need to re-build that model in a way that is auditable. So, there is a CICD

48:56.800 --> 49:02.960
component there, but we automate it. So, it happens, but, like, a data scientist shouldn't have

49:02.960 --> 49:08.080
to deal with it, but it does happen. There is a auditable build process that is run on a,

49:08.080 --> 49:13.520
a build server somewhere that generates the serialized model object that is actually run in production.

49:14.400 --> 49:23.600
Okay. Awesome. Awesome. Where do you see this all going? Both at Capital One and, you know,

49:23.600 --> 49:28.480
a little bit more broadly, you know, the industry we've, you know, hinted at it a little bit, but

49:29.520 --> 49:33.200
what are the short, medium and long-term kind of directions for your team in platform,

49:33.200 --> 49:39.200
or make Capital One? We can start there. Wow. Short, medium, and long-term. All right. So, I think,

49:41.120 --> 49:49.280
I think that there are a couple areas that we'll be focusing on around building more and more

49:49.280 --> 49:55.760
reusable capabilities. So, I don't really want each data science team dealing with,

49:55.760 --> 50:05.920
I don't know, we'll choose a modeling framework. XGBoost, some running of, like, XGBoost model training,

50:05.920 --> 50:09.840
right? Like, there are capabilities there that I think that can be reusable, and we have reusable

50:09.840 --> 50:16.240
components today, but I think continuing to rebuild those out and expand those. So, for us, that is the case.

50:16.240 --> 50:28.960
Long-term, I do expect all of this to be easier. I think as an industry, this will get easier. I don't

50:28.960 --> 50:35.760
think we will be in this immaturity space forever. I think we will, of course, there's the next thing

50:35.760 --> 50:41.360
that will be immature, but I, you know, if I knew what that was, I'd probably be very wealthy.

50:41.360 --> 50:51.520
So, but, like, I think, as we'll get easier, I think that the ability to

50:54.960 --> 51:03.360
integrate better explainability capabilities into these models, or into the creation of these

51:03.360 --> 51:16.720
models will allow us to accelerate building capabilities faster, but also, like, it'll make us

51:16.720 --> 51:24.480
more comfortable, like, using pre-packaged capabilities, like, pre-packaged software,

51:24.480 --> 51:31.440
like, we shouldn't have to necessarily go in and hack. That's my terrible phrase for doing work,

51:31.440 --> 51:36.320
right? But we shouldn't have to, like, go in there and, like, hack a lot of this stuff. It should

51:36.320 --> 51:45.600
be able to be run in our environment, but it isn't. So, that's where we kind of get all those

51:45.600 --> 51:50.560
very strong DevOps engineers in there, and kind of getting it all working. So, I do think this will

51:50.560 --> 51:59.520
get easier. How is another conversation? Well, maybe if I do know, I'm not going to tell you,

51:59.520 --> 52:06.720
I don't know, like, maybe that's my future career, but no, I don't know how, necessarily,

52:06.720 --> 52:15.200
but I imagine it will get easier. Awesome. Well, Ali, thanks so much for sharing with us a bit

52:15.200 --> 52:21.280
about what you've been up to and the platform you're building, and of course, you're coming from

52:21.280 --> 52:32.560
your perspective as a software engineer. My pleasure. Thanks for having me.

