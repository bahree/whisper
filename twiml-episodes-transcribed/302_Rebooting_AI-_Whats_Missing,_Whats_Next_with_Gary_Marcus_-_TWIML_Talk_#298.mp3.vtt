WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.200
I'm your host Sam Charrington.

00:32.200 --> 00:37.880
With less than three weeks to go, time is running out to register for the event we are all looking

00:37.880 --> 00:41.840
forward to, Twimblecon AI platforms.

00:41.840 --> 00:47.200
At Twimblecon, you'll hear from industry luminaries like Andrew Aing, Ubers Fran Bell

00:47.200 --> 00:52.080
and Cruises Hussein Mahana on what leading organizations are doing to increase the efficiency

00:52.080 --> 00:57.680
of developing machine learning and deep learning models and getting them into production.

00:57.680 --> 01:02.040
You'll learn about the practices and platforms being put into place by companies like Airbnb,

01:02.040 --> 01:07.480
Capital One, Comcast, Liva, SurveyMonkey, Zappos, and many more.

01:07.480 --> 01:11.880
And you'll get to engage with fellow data science, machine learning, platform engineering,

01:11.880 --> 01:18.000
and MLOPS practitioners and leaders to learn, share and connect on how to accelerate, automate

01:18.000 --> 01:22.480
and scale machine learning and AI in the real world.

01:22.480 --> 01:27.800
We are super excited that we'll get to see you all on October 1st and 2nd in San Francisco.

01:27.800 --> 01:33.240
For more details, visit twimblecon.com and while you're there, send me a message on the

01:33.240 --> 01:42.440
chat for information about our team discounts, and now on to the show.

01:42.440 --> 01:45.320
Alright everyone, I am on the line with Gary Marcus.

01:45.320 --> 01:52.040
Gary is the CEO and founder at roboss.ai, he was also the CEO and founder of the machine

01:52.040 --> 01:58.280
learning startup Geometric Intelligence, which was acquired by Uber in 2016.

01:58.280 --> 02:03.720
Gary is the author of five books, including his latest rebooting AI, which will be available

02:03.720 --> 02:06.040
on the day this podcast is published.

02:06.040 --> 02:08.640
Gary, welcome to this week machine learning and AI.

02:08.640 --> 02:09.960
Thanks for having me.

02:09.960 --> 02:15.320
I am really excited to jump in and chat with you about this book.

02:15.320 --> 02:19.320
I had a chance to dig into it.

02:19.320 --> 02:22.160
And awesome, awesome book.

02:22.160 --> 02:23.400
Let's just jump in.

02:23.400 --> 02:26.920
Before we really dive into talking about the book, I'd love to explore a little bit about

02:26.920 --> 02:32.960
your background you spent quite a bit of your career at NYU as a professor of psychology

02:32.960 --> 02:34.960
and neuroscience.

02:34.960 --> 02:39.920
Tell us a little bit about your background and the perspective that this creates for you.

02:39.920 --> 02:44.000
So I'm trained primarily as a cognitive scientist.

02:44.000 --> 02:48.920
My research for many years and my PhD with Steve Pinker was all about how children learn

02:48.920 --> 02:53.200
language and how children start to understand the world.

02:53.200 --> 02:56.760
So I'm a developmental cognitive scientist by training.

02:56.760 --> 03:00.160
And at the same time, I've been interested in AI since I was about eight years old when

03:00.160 --> 03:02.640
I first learned about programming computers.

03:02.640 --> 03:08.080
And in the last seven years or so, I've focused almost exclusively on answering the question,

03:08.080 --> 03:10.720
what can cognitive science bring to AI?

03:10.720 --> 03:16.520
So AI is currently dominated by certain statistical approaches that from my perspective as a cognitive

03:16.520 --> 03:20.440
scientist, as someone who studies how humans work, seem a little weird to me.

03:20.440 --> 03:25.720
So I don't think of children as giant data machines, but the way that AI is kind of rolling

03:25.720 --> 03:28.280
right now, it's all about big data.

03:28.280 --> 03:33.880
And I've been trying to see what I can contribute to AI from the perspective of cognitive science.

03:33.880 --> 03:39.600
So when you were, when you created geometric intelligence, was that a company that really

03:39.600 --> 03:44.960
commercialized a cognitive science based approach or was there a statistical approach involved

03:44.960 --> 03:47.520
in your work there?

03:47.520 --> 03:52.040
Well, geometric intelligence, which was my first company, was inspired in some ways by

03:52.040 --> 03:53.040
a cognitive science.

03:53.040 --> 03:54.040
It wasn't slavish to it.

03:54.040 --> 03:58.600
So there's always this tension of, you know, if you're building airplanes, you don't want

03:58.600 --> 04:03.280
to fly exactly like birds, because that wouldn't make any sense and who wants to flap their

04:03.280 --> 04:05.480
wings so many times a minute.

04:05.480 --> 04:10.040
But you also want to understand something about the dynamics of flight in my last company

04:10.040 --> 04:15.000
and also in this company, we're trying to take some lessons from biology, in particular

04:15.000 --> 04:19.920
from how humans think, and apply those to AI problems.

04:19.920 --> 04:26.520
So we're not in the last company was not like trying to be neuroscientifically, to perfectly

04:26.520 --> 04:27.520
accurate.

04:27.520 --> 04:28.520
We're not trying to be faithful to the brain.

04:28.520 --> 04:30.960
We're trying to take inspiration from the brain.

04:30.960 --> 04:34.520
The last company, the broad problem that it was trying to address is how do you learn

04:34.520 --> 04:36.400
from small amounts of data?

04:36.400 --> 04:39.480
And that question itself in some ways comes from cognitive science.

04:39.480 --> 04:43.000
I think machine learning is catching up to it now in the last couple of years, but

04:43.000 --> 04:48.320
it's always been clear from cognitive science, especially from the field of language acquisition

04:48.320 --> 04:51.160
that learning from small data is the name of the game.

04:51.160 --> 04:54.800
Children can generalize from tiny amounts of examples.

04:54.800 --> 05:00.120
My dissertation was about how children learn the ADD rule for forming the past tense,

05:00.120 --> 05:02.040
which they sometimes use incorrectly.

05:02.040 --> 05:04.320
They'll say gold or went to things like that.

05:04.320 --> 05:06.360
They learn that from a small amount of data.

05:06.360 --> 05:11.640
Sometimes they make mistakes and over-apply it, but they don't have gigabytes of data the

05:11.640 --> 05:16.160
way that say the GPT system does now.

05:16.160 --> 05:21.760
And so the last company was really focused on one particular way of solving this small

05:21.760 --> 05:22.760
data problem.

05:22.760 --> 05:27.600
And our, I think, most impressive results were we were beating deep learning in terms

05:27.600 --> 05:28.920
of data efficiency.

05:28.920 --> 05:34.160
So we could learn things with half as much data without having specific priors about

05:34.160 --> 05:35.640
the nature of the things we were learning.

05:35.640 --> 05:39.200
So we would take M-NIST, which is benchmarked probably a lot of your audience knows,

05:39.200 --> 05:40.440
recognizing characters.

05:40.440 --> 05:44.920
We could do M-NIST with half as much data without having to build in anything about the nature

05:44.920 --> 05:46.520
of letters or anything like that.

05:46.520 --> 05:50.560
So we were working towards a general way of doing supervised learning and maybe some

05:50.560 --> 05:53.320
other things using less data.

05:53.320 --> 05:55.000
And we were inspired there by humans.

05:55.000 --> 05:58.400
We weren't necessarily doing it exactly the way the humans do.

05:58.400 --> 06:03.720
But I think the core intellectual property is something that Zubin Garamani and I developed

06:03.720 --> 06:07.920
and I sort of set a direction that was based on some things that made sense to me from

06:07.920 --> 06:12.000
a cognitive science perspective and Zubin, who's brilliant mathematician, figured out

06:12.000 --> 06:13.520
how to apply it.

06:13.520 --> 06:21.720
And so, you know, I think a lot of our listeners, when they hear the idea of creating AI on

06:21.720 --> 06:26.960
limited data, we'll think about things like, you know, one-shot learning, zero-shot learning.

06:26.960 --> 06:31.400
But it sounds like your approach was very different from these or was it?

06:31.400 --> 06:35.400
I mean, there's some interrelations and I can't say too much because Uber owns the IP

06:35.400 --> 06:38.400
and there's NDAs and all that kind of stuff.

06:38.400 --> 06:43.680
But I would say that zero-shot learning and one-shot learning, first of all, are names

06:43.680 --> 06:48.440
of problems, they're not names of techniques and people use different kinds of techniques

06:48.440 --> 06:49.440
to do them.

06:49.440 --> 06:53.040
And they're often, I think, narrowly construed.

06:53.040 --> 06:56.280
So, you know, there are lots of problems in the world where you have some data.

06:56.280 --> 07:00.880
It's not the zero data, but you just don't have that much.

07:00.880 --> 07:04.880
Something I often like to talk about is what my daughter did when she climbed through

07:04.880 --> 07:05.880
a chair.

07:05.880 --> 07:08.680
So, we were sitting in a Whole Foods about a year and a half ago, she was about four

07:08.680 --> 07:12.280
and a half years old or four years old at the time.

07:12.280 --> 07:17.280
And we sat in a chair that had a back and then a gap between the back and the base of

07:17.280 --> 07:20.200
the chair, if you can kind of visualize that.

07:20.200 --> 07:22.880
And she'd never seen the TV program, The Dukes of Hazard, where they climbed through

07:22.880 --> 07:23.880
the windows.

07:23.880 --> 07:28.720
So, she didn't have any data from like a model of people doing wacky things, sticking

07:28.720 --> 07:33.160
their bodies through, you know, an aperture inside of a chair.

07:33.160 --> 07:38.400
So, this was not a big data problem, or at least there wasn't a lot of directly relevant

07:38.400 --> 07:39.400
big data.

07:39.400 --> 07:43.480
She had data about how her body worked, the size of her body, and she probably explored

07:43.480 --> 07:45.960
other apertures before.

07:45.960 --> 07:51.440
She did what a lot of people might call abstract least unsupervised learning, but she didn't

07:51.440 --> 07:54.080
use any of the techniques that we would call unsupervised learning.

07:54.080 --> 07:59.400
So, it was unsupervised in the sense that she didn't have training examples saying this

07:59.400 --> 08:06.160
is the right, you know, torque to apply to your torso in order to spin through the chair,

08:06.160 --> 08:11.960
right, in the way like a reinforcement learning robot might try it a million times and get

08:11.960 --> 08:12.960
reinforcement.

08:12.960 --> 08:13.960
I got stuck this way.

08:13.960 --> 08:15.440
I didn't get stuck that way and so forth.

08:15.440 --> 08:18.800
She just did it in the space of like a minute.

08:18.800 --> 08:21.840
And then the second time that she did it, I asked her to reenact it and I took pictures

08:21.840 --> 08:22.840
of the second time.

08:22.840 --> 08:26.920
I wish I had taken pictures the first time or taken a video of the second, but anyway,

08:26.920 --> 08:31.240
you look at this sequence of pictures that I took and she actually got stuck at one point

08:31.240 --> 08:33.240
and then she figured out how to get unstuck.

08:33.240 --> 08:37.840
And so there was problem solving process there and there was also kind of leveraging modest

08:37.840 --> 08:38.840
amounts of data.

08:38.840 --> 08:43.520
She had no direct data on this problem except what she got from trying it herself in that

08:43.520 --> 08:44.520
moment.

08:44.520 --> 08:48.280
And then she had a bunch of background data from other kinds of problems that she had

08:48.280 --> 08:49.280
solved.

08:49.280 --> 08:53.360
And she knew enough, maybe not consciously, but unconsciously, about physics and how

08:53.360 --> 08:56.360
her body moved and so forth that she could integrate all of that.

08:56.360 --> 09:00.280
So that doesn't fall into the paradigm of zero shot learning, although you could sort

09:00.280 --> 09:03.680
of call it a zero shot problem, but it's not like the things that people do in literature

09:03.680 --> 09:08.080
and it doesn't fall into the one shot learning and it doesn't really fit with how people

09:08.080 --> 09:12.000
think about unsupervised learning where they like take clusters of things or predict the

09:12.000 --> 09:13.480
next frames in the video.

09:13.480 --> 09:16.200
It's not really like any of those problems.

09:16.200 --> 09:20.200
And yet it's kind of what little kids like my children do all the time.

09:20.200 --> 09:24.040
They say, here is some challenge that I have never confronted before.

09:24.040 --> 09:25.560
I'm going to figure it out.

09:25.560 --> 09:31.040
It's like 80% I feel like I'm probably exaggerating, but it's a large fraction of what my kids

09:31.040 --> 09:32.440
do is they set new challenges.

09:32.440 --> 09:37.440
So right now my son's a little older, he's six and a half, my daughter's five now.

09:37.440 --> 09:41.720
And they like play games all day long and they don't play existing games, they play games

09:41.720 --> 09:42.720
that they invent.

09:42.720 --> 09:46.000
And so they're like, well, let's pretend you can't fly anymore because you broke your

09:46.000 --> 09:47.000
wing or whatever.

09:47.000 --> 09:51.880
They're constantly making up assumptions and then doing problem solving relative to those

09:51.880 --> 09:52.880
reference points.

09:52.880 --> 09:57.000
It's just completely far away from what people are doing in AI now.

09:57.000 --> 10:00.960
And part of the reason that Ernie Davis and I wrote this book rebooting AI is to like

10:00.960 --> 10:04.560
reorient the field and reboot just like start over.

10:04.560 --> 10:09.200
So we're doing great on all this supervised learning stuff where we have a ton of data,

10:09.200 --> 10:10.200
ton of label data.

10:10.200 --> 10:13.840
But you know, the reality is that's not really what the real world is like.

10:13.840 --> 10:18.120
And it's certainly not like what children do as they come to understand the world.

10:18.120 --> 10:22.840
And there's a gap right now between I think memorizing or doing something is a little

10:22.840 --> 10:25.440
bit better than memorization and understanding.

10:25.440 --> 10:29.400
So deep learning is like a better way of doing memorization, you can interpolate between

10:29.400 --> 10:32.840
examples you've seen before, but it's not really about comprehension, it's not really

10:32.840 --> 10:37.400
about like building a model of chairs and apertures and bodies and understanding how

10:37.400 --> 10:38.400
those interrelate.

10:38.400 --> 10:43.480
And so what Ernie and I are trying to do is to get the feel to look in a different direction

10:43.480 --> 10:46.920
that's more about comprehension and understanding and so forth.

10:46.920 --> 10:50.880
Going back to your question for a second, I mean, did my last company do all of that?

10:50.880 --> 10:55.080
No, I mean, we were a small startup, we were, when we were bought, we were 15 people,

10:55.080 --> 11:00.320
we had one very specific way of solving a supervised learning problem with less data.

11:00.320 --> 11:05.240
There's a lot that goes into the human way approach to less data.

11:05.240 --> 11:08.120
Another thing that goes into it that we didn't work on in the last company at all is an

11:08.120 --> 11:09.120
ateness.

11:09.120 --> 11:14.560
So in Chomsky's arguments, which I think are correct, is that we start with something

11:14.560 --> 11:16.520
that constrains how we learn language.

11:16.520 --> 11:21.480
We're not open to any possible language. We're born knowing certain things about language.

11:21.480 --> 11:25.240
I differ from him a little bit about what those things are, but I would say we're probably

11:25.240 --> 11:30.120
born knowing that you can concatenate symbols in order to express things.

11:30.120 --> 11:31.120
Is it not conscious?

11:31.120 --> 11:32.120
Is it not conscious?

11:32.120 --> 11:33.120
Is it not conscious?

11:33.120 --> 11:34.120
Is it not conscious?

11:34.120 --> 11:38.080
I was about to say, it may not be conscious, but I'll tell you about an experiment that

11:38.080 --> 11:44.040
I did, which is probably my best known result in the psychology literature.

11:44.040 --> 11:50.160
I taught seven-month-old kids in artificial language, and I didn't tell them the rules

11:50.160 --> 11:51.160
for the language.

11:51.160 --> 11:55.120
It just gave them examples, two minutes, and that was enough for seven-month-old babies

11:55.120 --> 11:56.760
to figure out the abstract grammar.

11:56.760 --> 12:02.960
So they heard sentences like La Tata, Ganana, so they had an ABB pattern, of course we're

12:02.960 --> 12:07.760
psychologists, we counterbalance, so others saw the AAB pattern and some saw an ABA pattern,

12:07.760 --> 12:08.760
et cetera.

12:08.760 --> 12:13.000
So you hear one of these grammars, and now you have to hear new sentences that are made

12:13.000 --> 12:14.000
up.

12:14.000 --> 12:18.800
Well, they're all made up of the same words, but some of them, or sorry, the new words,

12:18.800 --> 12:21.400
and some of them also have a new grammar, some of the same grammar.

12:21.400 --> 12:25.400
So you hear ABA sentences, now you get tested on ABB sentences, or not.

12:25.400 --> 12:29.240
And what we found was that the seven-month-olds, they're not paid for their participation

12:29.240 --> 12:33.680
and they're getting course credit for intro psych, they're just sitting there listening,

12:33.680 --> 12:35.720
but they want to know what's going on here.

12:35.720 --> 12:38.320
So they hear two minutes of this stuff, and they can already tell when you change to

12:38.320 --> 12:39.320
a new grammar.

12:39.320 --> 12:40.640
They start paying attention more.

12:40.640 --> 12:44.480
So they habituate, they get bored by hearing the same thing over and over again, and then

12:44.480 --> 12:48.200
they dishabituate, they show interest, when we change the grammar.

12:48.200 --> 12:52.880
I was hoping that this was going to be teaching them Klingon, or Dothraki, although it's

12:52.880 --> 12:55.720
probably going to be a little bit after that time.

12:55.720 --> 13:01.240
My favorite line of all of the Star Trek movies was about hearing Shakespeare in the original

13:01.240 --> 13:03.720
Klingon.

13:03.720 --> 13:14.040
So subjects in our experiment heard something that was even proto-Klingon, proto-human.

13:14.040 --> 13:17.520
In any case, they picked this up.

13:17.520 --> 13:22.120
And then what happens at developmental psychology is if you do an experiment, you show that kids

13:22.120 --> 13:26.000
of a certain age can do things, then if it's interesting enough, people try to extend

13:26.000 --> 13:29.840
it in different ways, as many people have done for this.

13:29.840 --> 13:32.400
And they also try to show that even younger kids can do it.

13:32.400 --> 13:36.040
So somebody's actually shown using brain measures.

13:36.040 --> 13:40.400
It's not a perfect experiment, but a pretty good one, showed that even newborns are doing

13:40.400 --> 13:44.000
the same kind of grammatical analysis that our seven-month-olds were doing.

13:44.000 --> 13:49.240
We show that kids were more likely to do this with speech than with musical tones and

13:49.240 --> 13:50.240
stuff like that.

13:50.240 --> 13:55.360
And if they could get the gist of it from speech, then they could transfer it to musical tones,

13:55.360 --> 13:57.640
but they didn't analyze musical tones in the same way.

13:57.640 --> 14:02.040
It's a very interesting set of results with the bottom line for what we're talking about.

14:02.040 --> 14:07.200
It does actually suggest that some of the roots of grammar are there as early as we can

14:07.200 --> 14:09.000
test children.

14:09.000 --> 14:12.200
As an aside, something can be innate and not be there at birth.

14:12.200 --> 14:16.040
So my capacity to grow a beard was innate, I didn't learn how to do it.

14:16.040 --> 14:20.200
But it wasn't expressed until I was, I don't know, 14 or 15 years old.

14:20.200 --> 14:24.480
So people in developmental psychology literature get confused and think if it's not there

14:24.480 --> 14:26.280
at birth, it's not innate.

14:26.280 --> 14:30.520
But the human brain comes out of the oven before it's fully developed, not everything

14:30.520 --> 14:33.600
that happens afterwards is about learning.

14:33.600 --> 14:36.680
In any case, I would say that we have innately that.

14:36.680 --> 14:41.520
We also have innately probably a distinction about subjects and predicates, like these entities

14:41.520 --> 14:45.840
are undergoing this change or something like that, and various other things.

14:45.840 --> 14:49.480
So when we start getting exposed to language, we have some hooks already built in that we

14:49.480 --> 14:51.200
can attach that to.

14:51.200 --> 14:56.240
Now you compare that to the kind of reinforcement learning for language acquisition experiments

14:56.240 --> 15:00.560
that deep mind has played around with over the last few years.

15:00.560 --> 15:06.240
If I can jump in here, I think to contextualize reinforcement learning is one of the things

15:06.240 --> 15:13.720
that many in the field are really excited about because in a lot of ways, it appears as one

15:13.720 --> 15:19.200
of the closest things we have to learning the way children learn, right?

15:19.200 --> 15:23.360
There's an environment, you know, there's an agent that interacts with that environment

15:23.360 --> 15:28.920
and learns things if we can put that learning around air quotes.

15:28.920 --> 15:31.920
It's all true, but it's missing something and that's sort of where it was going.

15:31.920 --> 15:36.160
So I think you characterize correctly what reinforcement and deep reinforcement learnings

15:36.160 --> 15:37.160
to do.

15:37.160 --> 15:40.400
And at the level of abstraction that you described, it has to be right, right?

15:40.400 --> 15:44.600
I mean, what you do is you try things in the environment and you look for feedback.

15:44.600 --> 15:47.080
And one of the forms of feedback is sort of am I right or am I wrong?

15:47.080 --> 15:50.840
And that's what does this lead to a good outcome or outcome?

15:50.840 --> 15:52.880
And that's what reinforcement learning is all about.

15:52.880 --> 15:57.200
And you know, and saying would argue that some of that happens.

15:57.200 --> 15:58.200
Right.

15:58.200 --> 16:03.440
And there's a lot of conversation around and how difficult it is in practice, you know,

16:03.440 --> 16:09.800
for a variety of reasons, sample inefficiencies and how difficult it is to get the reward function

16:09.800 --> 16:11.280
correctly.

16:11.280 --> 16:15.280
But I think you're saying something more fundamental about the difference between

16:15.280 --> 16:17.080
real learning and reinforcement learning.

16:17.080 --> 16:18.080
Yeah.

16:18.080 --> 16:22.360
You know, in the improv school, the improv school, they teach you, yes, and, right.

16:22.360 --> 16:28.440
So yes, and you need a whole lot of other stuff to make it actually work.

16:28.440 --> 16:34.320
So the problem right now is like a stereotypical version of this is the Atari game system that

16:34.320 --> 16:39.040
DeepMind built, which led to their sale or was part of why Google bought them for all

16:39.040 --> 16:40.040
that money.

16:40.040 --> 16:44.200
And what was cool about it was they could play a lot of different games.

16:44.200 --> 16:47.320
They could play breakout, play space invaders and something they could play at superhuman

16:47.320 --> 16:52.840
level, and there was nothing built in about the rules of any of those games, which by

16:52.840 --> 16:56.240
the way is not true of their go demonstrations and so forth.

16:56.240 --> 17:01.000
But that original demo, it was really like interesting intellectual proof of concept.

17:01.000 --> 17:03.880
However, it didn't really work that well.

17:03.880 --> 17:07.840
In the sense that if you change the circumstances at all, it all fell apart.

17:07.840 --> 17:12.120
So Vicarious had a really nice demo, and actually my company had a similar demo.

17:12.120 --> 17:16.040
My last company that we never published, but it makes the same point.

17:16.040 --> 17:21.160
So Vicarious did was they took breakout and they moved the paddle up a few pixels, and

17:21.160 --> 17:27.080
you have like this famous video on the web of the DeepMind thing playing breakout learning

17:27.080 --> 17:32.840
to break through the wall and people kind of talk in very cognitive language about how

17:32.840 --> 17:35.520
the system has learned to go through the wall or whatever.

17:35.520 --> 17:40.880
And then when Vicarious moved the paddle up three pixels suddenly performance went from

17:40.880 --> 17:43.360
superhuman to mediocre.

17:43.360 --> 17:47.680
As a system hadn't really learned what a ball is, what a paddle is, or what a wall is.

17:47.680 --> 17:51.520
What it really learned was the statistical contingencies that worked on the screen in

17:51.520 --> 17:53.600
which it had been trained.

17:53.600 --> 17:58.560
And that's much more superficial than the human way of playing breakout, which is to learn

17:58.560 --> 18:04.760
about the kind of physics within the game of the ball, the paddle, and the bricks.

18:04.760 --> 18:09.840
And so going back to reinforcement learning, DeepMind's right to set it up as a reinforcement

18:09.840 --> 18:13.560
learning problem, but they're actually wrong to build nothing at all in.

18:13.560 --> 18:18.520
And when they want to, because they, if you start from zero, you just don't get that far.

18:18.520 --> 18:23.920
So in fact, when they play Go, which is in some ways a hard to problem, some ways not,

18:23.920 --> 18:28.360
they build in the rules of Go, they build in Monte Carlo tree search, which sets up the

18:28.360 --> 18:32.440
problem is kind of if I go there, then you go there, they don't learn that stuff.

18:32.440 --> 18:38.840
And they've been putting out all of this kind of PR around a very blank slate approach

18:38.840 --> 18:42.640
and where there's nothing built in, but they don't actually do that when they solve

18:42.640 --> 18:43.640
the hard problems.

18:43.640 --> 18:48.160
And then they had this paper that really kind of aggravated me called mastering Go without

18:48.160 --> 18:49.160
human knowledge.

18:49.160 --> 18:52.720
And in fact, they had like a world class Go player on the team, and there's lots of ways

18:52.720 --> 18:54.480
in which human knowledge was embedded.

18:54.480 --> 19:00.040
I have an archive article, ARXIV, maybe you can link and show notes, taking apart all

19:00.040 --> 19:02.600
the human knowledge that actually went in there.

19:02.600 --> 19:05.960
But they're doing it kind of through the back door, like, hey, nobody pay attention to

19:05.960 --> 19:11.120
the fact that we built in the rules and carefully designed how many layers through a lot of

19:11.120 --> 19:15.080
experimentation and haven't tested it on a different size board.

19:15.080 --> 19:19.120
What we really need to do is to think about the principle information that needs to be

19:19.120 --> 19:24.080
built in in conjunction with reinforcement learning, so it will work.

19:24.080 --> 19:28.200
You could say that what my daughter did was like online reinforcement learning, but she

19:28.200 --> 19:33.360
wasn't just like, what happens if I do this small torque on this limb, she was trying

19:33.360 --> 19:39.080
to figure out how to do this relative to a pretty rich model of her own body in the three-dimensional

19:39.080 --> 19:43.760
geometry of the world and a knowledge about the physics of like rigid objects, you probably

19:43.760 --> 19:48.440
would have done it differently if the back was made of string, and she had more space

19:48.440 --> 19:49.440
and it was flexible.

19:49.440 --> 19:54.600
And I think you said a lot of actually reasoning in the context of reinforcement learning.

19:54.600 --> 19:55.600
Yeah, yeah.

19:55.600 --> 20:00.200
I think it's an important word there in model.

20:00.200 --> 20:07.200
One of the recurring themes on this show that I've talked about quite a bit that really

20:07.200 --> 20:12.480
just kind of jumped out for me after a number of interviews was like this pendulum swing

20:12.480 --> 20:19.280
from a world in which the way we understood the environment that we interact with this

20:19.280 --> 20:24.800
through creating these models around physics and engineering, et cetera, now we've kind

20:24.800 --> 20:30.040
of gone to the other end of the pendulum swing and everything is, there's a lot of focus

20:30.040 --> 20:36.120
and excitement around statistical approaches, and there is a lot of interesting work happening

20:36.120 --> 20:42.360
kind of at really more of an equilibrium point where folks are looking at marrying the

20:42.360 --> 20:47.480
model-based approaches and statistical approaches, and it sounds like what you're really proposing

20:47.480 --> 20:57.280
is something similar to the, you know, in support of AGI, artificial general intelligence

20:57.280 --> 21:04.000
or GAI general artificial intelligence, whichever of those acronyms you prefer, do you agree

21:04.000 --> 21:05.000
with that?

21:05.000 --> 21:09.520
I would slightly rephrase what you just said, but basically I agree with it.

21:09.520 --> 21:14.400
The problem right now is we're mostly focusing on narrow intelligence and deep learning is

21:14.400 --> 21:20.200
a very good tool for that or deeper reinforcement learning for certain narrow problems, but those

21:20.200 --> 21:24.160
tools are not good for general intelligence, whether you want to call that AGI or GAI

21:24.160 --> 21:29.400
or what have you, but the kind of intelligence that is inherent in a flexible person for

21:29.400 --> 21:33.240
example, you can solve problems in different ways if the assumptions change a little bit

21:33.240 --> 21:37.680
from what you started with, and to get there, you need to do exactly what you just said,

21:37.680 --> 21:43.440
you need to marry the modeling approach which may use, for example, symbolic techniques

21:43.440 --> 21:48.120
from classical AI with the more statistical techniques that we have now, and if you look

21:48.120 --> 21:50.040
at humans, that's exactly what they do.

21:50.040 --> 21:54.400
We do some perceptual stuff that seems to be kind of driven by statistics and a lot

21:54.400 --> 21:59.600
of experience, and we do some abstract reasoning and language and so forth that don't seem

21:59.600 --> 22:01.600
to use the same mechanisms.

22:01.600 --> 22:06.560
Connemons cut on that is system one versus system two, the kind of reflexive system versus

22:06.560 --> 22:08.640
deliberative systems, the way I like to talk about those.

22:08.640 --> 22:10.400
I think you fast thinking slow.

22:10.400 --> 22:15.520
And I think that the AI techniques that we have right now are good, I wouldn't even

22:15.520 --> 22:20.440
call it a thinking, but at classifying, which is a little bit like his system one, classifying

22:20.440 --> 22:25.400
it's seen this pattern before, it looks like this other thing that I've seen, but we're

22:25.400 --> 22:30.080
not right now as a community focusing that much on the kind of system two stuff where

22:30.080 --> 22:34.440
you deliberate where you recognize your assumptions are wrong and you change, and that's where

22:34.440 --> 22:35.440
the model stuff lives.

22:35.440 --> 22:39.760
The system one stuff, but you don't have time for a model, you're just relying on kind

22:39.760 --> 22:43.680
of memory traces effectively of things you've seen before.

22:43.680 --> 22:47.040
The system two kind of stuff, you have to have a cognitive model.

22:47.040 --> 22:48.200
What is going on here?

22:48.200 --> 22:51.480
What are the causal relationships between these entities?

22:51.480 --> 22:53.920
If I change this thing, what would happen to this other thing?

22:53.920 --> 22:58.760
That kind of reasoning, which humans do all the time, they do it grad school, but we

22:58.760 --> 23:04.640
also just do it in daily life, that part's not being captured and so much money is going

23:04.640 --> 23:11.400
into the deep learning side that it's kind of perverted, I think, the mission of AI,

23:11.400 --> 23:16.320
which was originally to go after general AI, it was much more, I think, originally sympathetic

23:16.320 --> 23:18.120
to learning from humans.

23:18.120 --> 23:22.040
Right now, the kind of mathematicians and cluster builders have the upper hand, like computer

23:22.040 --> 23:26.520
cluster builders have the upper hand, because these techniques are yielding a lot of short-term

23:26.520 --> 23:31.440
fruit, but I think it's perverting the overall direction of the field, and that's what this

23:31.440 --> 23:34.400
book is about, is to try to correct the shape.

23:34.400 --> 23:37.920
Not to throw that stuff overboard, we need it too.

23:37.920 --> 23:41.600
The deep learning or some success or two, it's going to stick around, but we need to have

23:41.600 --> 23:46.360
some focus on other things, like, how do you do causal reasoning about what happens if

23:46.360 --> 23:48.920
I do this thing to this system, or temporal reasoning?

23:48.920 --> 23:51.120
Where is this system going to be 20 minutes from now?

23:51.120 --> 23:55.800
I just saw a paper showing that people are getting pretty good at using deep learning

23:55.800 --> 24:00.320
system to predict the next clock tick, like the next, say, quarter second in a video, or

24:00.320 --> 24:05.720
the next frame in a video, and they're pretty terrible, just bouncing balls, billiard

24:05.720 --> 24:06.720
balls.

24:06.720 --> 24:10.120
They're good at that, but they had no way of predicting what would happen five minutes

24:10.120 --> 24:11.120
later.

24:11.120 --> 24:14.160
You could look and say, well, they're going to come to rest, and they're going to be scattered

24:14.160 --> 24:18.320
in distribution like this, and the deep learning systems couldn't do that.

24:18.320 --> 24:23.160
They could make these very short-term predictions, essentially, by consulting a library of videos

24:23.160 --> 24:27.160
they've seen before, interpolating over that library, but that's not the same thing as

24:27.160 --> 24:28.160
temporal reasoning.

24:28.160 --> 24:33.760
I tell you that I have an airplane ticket to go to California next week, you can predict

24:33.760 --> 24:37.880
it, I'll be at the airport on Tuesday morning, or if I give you enough information, you

24:37.880 --> 24:41.440
can make these long-term predictions that aren't about looking at frames in a video library.

24:41.440 --> 24:45.680
They're about reasoning about abstract entities like, okay, he needs to be in California

24:45.680 --> 24:49.400
on Tuesday, then he's probably going to have to go to the airport to do that, who probably

24:49.400 --> 24:50.400
have to go through a security line.

24:50.400 --> 24:55.160
You make all these inferences about what's typical and what might happen, what if I missed

24:55.160 --> 24:58.400
my flight, and that will be the alternatives I would consider.

24:58.400 --> 25:02.200
You can do all of this higher-level reason that's very different from predicting the

25:02.200 --> 25:04.120
next frame of a video.

25:04.120 --> 25:07.560
I guess one of the questions I have that's maybe a little bit of a pushback on that is,

25:07.560 --> 25:13.800
is it just okay that we're making a lot of progress in a narrow way in AI right now?

25:13.800 --> 25:18.800
Even in the book, you talk about curing or treating cancer and venting new materials,

25:18.800 --> 25:27.800
addressing climate change, big AI, AGI, could have a huge impact on those areas, but we're

25:27.800 --> 25:33.720
also having small but significant impacts in a lot of those areas today with the narrow

25:33.720 --> 25:35.400
techniques that we have.

25:35.400 --> 25:38.000
There's two things there.

25:38.000 --> 25:40.760
There's two things there, okay, go ahead.

25:40.760 --> 25:46.320
One is, I just hate leaving that much potential for human progress on the floor.

25:46.320 --> 25:48.320
I think we could be doing better.

25:48.320 --> 25:52.360
I think with the amount of money that's being invested right now, if it was spent a little

25:52.360 --> 25:56.960
bit more wisely, we could make huge progress, and I think it's worth thinking about that

25:56.960 --> 26:00.680
rather than just sort of taking the next step because it's the obvious next step.

26:00.680 --> 26:04.440
Sometimes the obvious next step is not the efficient one.

26:04.440 --> 26:05.440
That's one reason.

26:05.440 --> 26:06.440
I deal with this.

26:06.440 --> 26:11.160
I'd like to see us make as rapid progress as possible, and I think we could do that by

26:11.160 --> 26:14.200
compensating where the ship is going a little bit.

26:14.200 --> 26:18.960
The other side of it is we're relying a lot on these techniques right now that are

26:18.960 --> 26:24.040
pretty dumb, and the people who made them more smart, but the techniques are dumb in

26:24.040 --> 26:27.720
the sense that they don't have cognitive models of what's going on.

26:27.720 --> 26:31.720
They're just relying on brute force or variations on brute force.

26:31.720 --> 26:36.200
By using those techniques that don't have rich understanding of the world, we get in

26:36.200 --> 26:37.200
trouble.

26:37.200 --> 26:40.600
People are applying those techniques, for example, to driverless cars.

26:40.600 --> 26:45.320
Within the driverless cars, a couple of days ago from when we were recording this, it

26:45.320 --> 26:49.960
looks like a Tesla drove into a tow truck that was parked on the side of the road.

26:49.960 --> 26:54.920
Because you mentioned this example in the book a couple of times about fire trucks and

26:54.920 --> 26:55.920
tow trucks.

26:55.920 --> 26:56.920
I had not heard this.

26:56.920 --> 26:57.920
This is a thing.

26:57.920 --> 27:01.840
I had mentioned the tow truck actually when we finished the book.

27:01.840 --> 27:02.840
Books go to press.

27:02.840 --> 27:03.840
They take a while.

27:03.840 --> 27:08.000
In the book, we mentioned fire trucks and police trucks who were stopped on the side of

27:08.000 --> 27:09.000
the road.

27:09.000 --> 27:13.400
Then a few days ago in Moscow, a Tesla rose into a tow truck.

27:13.400 --> 27:16.280
There's clearly a pattern there.

27:16.280 --> 27:20.720
It's continuing pattern, even as we're recording the interview.

27:20.720 --> 27:25.280
You want a system where at least if you have a couple of fatalities, oh, and there's tractor

27:25.280 --> 27:27.280
trailers too, that they've run over.

27:27.280 --> 27:33.560
We've had multiple fatalities from the running into tractor trailers, and I don't think

27:33.560 --> 27:39.000
any of these were fatal, but now multiple accidents basically running into emergency vehicles

27:39.000 --> 27:40.000
stopped.

27:40.000 --> 27:44.880
Well, we want to have an AI system where you can specify an abstract language, don't

27:44.880 --> 27:48.240
run into stopped emergency vehicles, and have the system that's smart enough to figure

27:48.240 --> 27:49.240
that out.

27:49.240 --> 27:53.080
We actually have our systems that need a lot of label data, and there aren't a lot of

27:53.080 --> 27:56.280
label data of tow trucks stopped on the side of the road.

27:56.280 --> 27:57.280
Nobody thought to get them.

27:57.280 --> 27:59.760
Maybe they'll collect them now, but there'll be some other case.

27:59.760 --> 28:01.720
This is what I mean by edge cases or outliers cases.

28:01.720 --> 28:05.440
There are going to be some other kind of emergency vehicle that looks a little bit different.

28:05.440 --> 28:08.400
A person's going to be able to reason that must be an emergency vehicle, but it's not

28:08.400 --> 28:09.920
going to be in the data set.

28:09.920 --> 28:13.760
The other part of the answer to your question is we're using this technique now.

28:13.760 --> 28:16.320
These are at stake, so we have to do something.

28:16.320 --> 28:22.120
Either we outlaw driverless cars until we figure out something better, or we work actively

28:22.120 --> 28:26.280
right now to think about what a better approach to AI would be.

28:26.280 --> 28:33.880
I thought this was a really well-stated and important point in the book, and that is

28:33.880 --> 28:36.240
around this core issue of trust.

28:36.240 --> 28:43.520
Other words around what you were just describing, and that we, in particular, in the general

28:43.520 --> 28:49.080
public sense of the word, we don't understand the way the things we're calling AI today

28:49.080 --> 28:56.360
are operating and don't understand the failure modes, and as a result of that, we trust

28:56.360 --> 29:01.040
them too much, and there's a potential as you were just describing that we put them

29:01.040 --> 29:07.040
in the situations where lives are at stake and people don't understand that they're likely

29:07.040 --> 29:08.040
to fail in some ways.

29:08.040 --> 29:09.040
That's right.

29:09.040 --> 29:12.680
And the same thing's happening over and over again, so people put a lot of stock and face

29:12.680 --> 29:16.520
recognition, and it's really not that good, so you have a lot of police departments running

29:16.520 --> 29:23.000
around using it, and effectively adds to our profiling problems, and it's not that good.

29:23.000 --> 29:28.120
All these things, they might be like 90% accurate, but we're using some of them in situations

29:28.120 --> 29:29.120
where we need more.

29:29.120 --> 29:36.440
It's fine if an advertisement recommendation system is 90% accurate, but if we use a face

29:36.440 --> 29:40.800
recognition system in crimes and it's 90% accurate, that's actually pretty bad.

29:40.800 --> 29:44.280
You don't want one in 10 missed calls there.

29:44.280 --> 29:49.880
And a driverless car thing, even if it's 99% accurate, that's not nearly good enough.

29:49.880 --> 29:54.120
And so when we're doing mission critical things with AI, that we don't understand, that's

29:54.120 --> 29:58.720
the interpretability problem you were just referring to, and they themselves don't really

29:58.720 --> 30:03.240
understand the world in which they're operating, that's a recipe for problems.

30:03.240 --> 30:09.840
And that's why the whole book is really about trust, is because we are increasingly assigning

30:09.840 --> 30:14.600
autonomy to systems that don't really understand the world, and that we don't really understand,

30:14.600 --> 30:16.440
we are getting ourselves in a bad position.

30:16.440 --> 30:19.080
The worst case here is AI could get shut down.

30:19.080 --> 30:24.280
We could have a winter kind of firm without, you know, multiple AI winters, because funding

30:24.280 --> 30:25.280
gets dried up.

30:25.280 --> 30:29.400
Like if enough people died on one day in a driverless car thing, like Congress could like say,

30:29.400 --> 30:33.320
you know, enough, no more AI research, and we certainly don't want that.

30:33.320 --> 30:37.480
And so we have to make people's expectations realistic, you and I haven't talked about

30:37.480 --> 30:40.320
hype today, but that's another theme in the book.

30:40.320 --> 30:45.840
And we have to branch out and look more broadly in the space of possible architectures,

30:45.840 --> 30:51.600
including a bunch of things that are really out of fashion, like symbol manipulating

30:51.600 --> 30:55.480
classically AI, not that we should be rebuilding that stuff, we should be borrowing from it,

30:55.480 --> 30:58.680
we should be borrowing from the old stuff, you know, something old, something new, the

30:58.680 --> 31:02.040
right kind of marriage, as you were saying before, but we need to do that if we're going

31:02.040 --> 31:05.840
to be counting on the machines, and we are, and we can't really, we can't put them all

31:05.840 --> 31:07.720
in a box, we got any better.

31:07.720 --> 31:14.440
I'd like to come back to the areas that you see as having promise in, you know, taking

31:14.440 --> 31:24.360
us past where we are today, but before we do that, you mentioned hype, and I think that

31:24.360 --> 31:28.880
is going to, you know, addressing the hype and kind of counter balancing it is going

31:28.880 --> 31:36.240
to be, you know, big contribution of this book, a big part of the first several chapters

31:36.240 --> 31:41.200
of the book is really trying to address that hype, like there are a lot of really good

31:41.200 --> 31:46.440
examples in here of, you know, where the hype underlives the, or where the hype doesn't

31:46.440 --> 31:53.600
live up to, or where the reality rather doesn't live up to the hype, and, you know, in particular,

31:53.600 --> 31:59.120
you spend a lot of time talking about reading, you know, walk us through, you know, that

31:59.120 --> 32:03.360
as an example, and, you know, some of the ways that, you know, you gave some examples

32:03.360 --> 32:07.640
about Microsoft and Alibaba, and the squad results that, you know, were published some

32:07.640 --> 32:08.640
years ago.

32:08.640 --> 32:10.880
There are a bunch of examples in there.

32:10.880 --> 32:11.880
Sure.

32:11.880 --> 32:18.480
So I don't have the book in front of me, but basically the case that we made there is,

32:18.480 --> 32:22.720
there were a bunch of, I mean, in that specific example was a bunch of media accounts saying

32:22.720 --> 32:29.680
that Microsoft had just achieved a superhuman reading or a match human reading, excuse me,

32:29.680 --> 32:34.320
in the reality, and there were even news stories that said, like, you know, humans are running

32:34.320 --> 32:39.600
out of jobs, and like, you know, the sky is falling kind of stuff because of this result.

32:39.600 --> 32:43.280
And what actually happened was Microsoft, it's slightly better than it had done, like,

32:43.280 --> 32:48.320
either anyone else had done a couple weeks earlier, and they now happened to match humans

32:48.320 --> 32:53.920
on this one test, which was called squad, but squad is not really that much of a test

32:53.920 --> 32:54.920
of reading.

32:54.920 --> 32:58.840
It's really a test of, like, can you underline the piece of the passage that corresponds

32:58.840 --> 33:00.840
to a question?

33:00.840 --> 33:04.920
And that's easy sometimes, but a lot of times, or, I mean, that's adequate sometimes,

33:04.920 --> 33:07.800
but most of the time when we're reading, we're trying to figure out things that aren't

33:07.800 --> 33:10.160
in the text, we're trying to go beyond the text.

33:10.160 --> 33:16.080
So if I just want to, you know, read a story about what Donald Trump did today and say,

33:16.080 --> 33:19.920
you know, who's the president? You can probably match the words president and Donald Trump

33:19.920 --> 33:20.920
with a system like that.

33:20.920 --> 33:25.600
And you can call that reading if you want, but a lot of reading is about reading between

33:25.600 --> 33:27.640
the lines and you make inferences.

33:27.640 --> 33:31.720
So I can say somebody walked into a store and you can figure out they're probably a human

33:31.720 --> 33:37.280
being, you know, all kinds of things that are just, like, logically obvious or inductively

33:37.280 --> 33:42.760
obvious to human beings all the time that go into the process of reading.

33:42.760 --> 33:44.880
And that test just didn't happen to measure any of them.

33:44.880 --> 33:50.120
And you would never hire that machine to, like, summarize news stories for you, or, or

33:50.120 --> 33:53.960
to tell you, especially like the implications, even the most obvious implications that aren't

33:53.960 --> 33:56.080
written on the page.

33:56.080 --> 33:59.080
And reading isn't, you know, focused throughout the book.

33:59.080 --> 34:03.480
One of the reasons that Arnie Davis and I wrote this book is we had written op-eds and

34:03.480 --> 34:08.840
things like that, trying to make some of these points, but there's a real mismatch between

34:08.840 --> 34:14.760
what current machines do and what you actually need to do in reading in terms of all the inferences

34:14.760 --> 34:15.760
that you draw.

34:15.760 --> 34:21.760
So we had some worked examples a little bit later in the book, not in the opening chapter,

34:21.760 --> 34:25.680
where we look at a children's story and just show all the things that you figure out when

34:25.680 --> 34:29.640
you're reading something by the woman who wrote Little House on the Prairie.

34:29.640 --> 34:34.840
Just like a paragraph and all the inferences and no system around even cries to do that.

34:34.840 --> 34:40.920
And we wanted to go at length in depth to try to help people better understand why the

34:40.920 --> 34:47.080
direction, the thrust of current AI research is just nothing like what you need to actually

34:47.080 --> 34:48.400
read.

34:48.400 --> 34:51.000
Just wrapping up on high for a second.

34:51.000 --> 34:54.600
One of my favorite parts of the book is just early in the first chapter, we give advice

34:54.600 --> 34:59.520
about questions that people at home can use when they read news stories.

34:59.520 --> 35:04.200
News stories are often like the byproduct of some press release that a big company puts

35:04.200 --> 35:05.200
out.

35:05.200 --> 35:08.040
And, you know, there are some very good people in the media, but there are a lot that just

35:08.040 --> 35:12.360
kind of report the press release and so the press release is often wind up more or less

35:12.360 --> 35:14.560
on editing media.

35:14.560 --> 35:17.120
So we give a set of questions that you can ask.

35:17.120 --> 35:22.400
Like did they do this on some toy problem or a bigger problem, what were the data like

35:22.400 --> 35:28.520
to help people to be able to go and read the news and have a healthy sense of skepticism.

35:28.520 --> 35:30.040
And this should be true for all of science.

35:30.040 --> 35:33.880
Of course, anytime you read science, probably anything that you read in the news, you need

35:33.880 --> 35:38.440
to be careful as we try to arm people with a set of questions to sift through the hype

35:38.440 --> 35:42.440
and figure out like, is this the real result, you know, does this mean systems can read

35:42.440 --> 35:45.880
or just that they passed this one test, what else do they need to do?

35:45.880 --> 35:50.680
And for that matter was any, you know, skeptic like me, for example, consulted to give

35:50.680 --> 35:54.960
any kind of, you know, counterview, if not, that's a sign in itself that this is, you

35:54.960 --> 35:57.960
know, press by press release.

35:57.960 --> 36:05.600
Yeah, I've got this particular one dog year in a note in my notes where I call the Gary's

36:05.600 --> 36:08.200
bullshit detector.

36:08.200 --> 36:14.000
But since I do have it in front of me, the six points were stripping away the rhetoric.

36:14.000 --> 36:16.800
So what did the system actually do?

36:16.800 --> 36:20.920
And so trying to get to, you know, is it actually reading or the squad results?

36:20.920 --> 36:24.960
How general are those results, you know, is there a demo, you know, a lot of times we

36:24.960 --> 36:29.360
see these results, either in academic papers or, you know, commercial and there's not

36:29.360 --> 36:34.320
a demo that anyone can go see.

36:34.320 --> 36:38.640
You know, if there's a comparison about the systems performance relative to humans, then,

36:38.640 --> 36:43.400
you know, which humans in particular, are we talking about and how much better?

36:43.400 --> 36:49.680
How far it is succeeding in this particular task actually take us towards building genuine

36:49.680 --> 36:55.840
AI and then the robustness of the system, which I kind of take as like failure modes, like

36:55.840 --> 36:57.720
where does it fall down?

36:57.720 --> 36:59.600
Is that, is that what you meant there?

36:59.600 --> 37:03.120
You will notice that the direct consequence of writing this book is that I founded a

37:03.120 --> 37:08.720
company called Robust at AI, one of the things that, that, I mean, exaggerate a little bit

37:08.720 --> 37:12.360
for comic effect, but it's also true.

37:12.360 --> 37:16.080
Ernie Davis and I wrote this chapter on reading that I was just alluding to and then we wrote

37:16.080 --> 37:21.560
a similar chapter on robots, like talking about the gap between, you know, a demonstration

37:21.560 --> 37:25.440
of a robot doing a backflip and having a robot actually working your home like Rosie

37:25.440 --> 37:31.360
the robot and the gap is just immense and the biggest problem is robustness, you can make

37:31.360 --> 37:36.840
a lab demonstration of any one action, but you really want the robots to decide for themselves

37:36.840 --> 37:37.840
what to do.

37:37.840 --> 37:42.360
I've, I've started thinking about a kind of distinction between automation and autonomy.

37:42.360 --> 37:47.520
So the field has learned of robotics has learned very well how to automate certain things if

37:47.520 --> 37:51.200
the environment doesn't change at all, but they're not very robust.

37:51.200 --> 37:56.040
So, you know, you can pack a million iPhones into boxes and as long as the boxes are exactly

37:56.040 --> 37:58.880
where you expect them to, it all works out.

37:58.880 --> 38:03.320
But if something unusual happens, then the system may have no idea what, what to do about

38:03.320 --> 38:08.480
it and that's basically what, the problem is with the driverless cars, those are robots.

38:08.480 --> 38:12.880
And they're fine under ordinary circumstances and then, you know, it rains or someone has

38:12.880 --> 38:16.920
a hand-lettered sign or there's a stop-toe truck that isn't in your training set and they

38:16.920 --> 38:18.480
don't work very well anymore.

38:18.480 --> 38:23.800
And so the key challenge, I think, in robotics, which is a great test of how good you're

38:23.800 --> 38:28.040
AI is, is to be robust, to get the same thing to work when the lighting is different, when

38:28.040 --> 38:32.280
there's somebody there that you weren't expecting, who is an object that you weren't expecting,

38:32.280 --> 38:36.400
when your map turns out to be out of date or wrong, all of these kinds of things.

38:36.400 --> 38:40.280
This is really a measure indirectly of intelligence.

38:40.280 --> 38:44.800
An intelligence system will recognize that its assumptions were wrong and compensate for

38:44.800 --> 38:45.800
that fact.

38:45.800 --> 38:48.160
A blind system just keeps doing what it's doing.

38:48.160 --> 38:53.320
So I'm driving on the road, I don't see anything in my little test set, seems good and

38:53.320 --> 38:55.080
I just drive right into the tow truck.

38:55.080 --> 38:57.840
Like that's the opposite of robustness.

38:57.840 --> 39:03.880
One of the chapters in the book is called Insights from the Human Mind.

39:03.880 --> 39:08.680
It jumped out at me that you said mind and not brain and I was curious what was that distinction

39:08.680 --> 39:09.680
for you and what.

39:09.680 --> 39:11.920
Sorry, that was very deliberate.

39:11.920 --> 39:16.520
My view is that neuroscience is a prestige field right now but hasn't come up with the

39:16.520 --> 39:21.920
goods and psychology is the study of the mind and more generally cognitive science is

39:21.920 --> 39:25.920
the study of my and actually already has a lot to offer that isn't really being paid

39:25.920 --> 39:26.920
attention to.

39:26.920 --> 39:32.560
So someday we will build better AI by understanding the wiring diagram of human brain and getting

39:32.560 --> 39:36.040
some insights from it, but right now we don't understand that wiring diagram.

39:36.040 --> 39:40.280
We don't understand even basic things like how does the brain do short term memory?

39:40.280 --> 39:43.920
So if I say let's pause I'll call you back in five minutes, you'll be expecting my

39:43.920 --> 39:44.920
call in five minutes.

39:44.920 --> 39:49.400
You don't need a thousand trials to hammer that into your brain, but we don't know how

39:49.400 --> 39:50.400
the brain does that.

39:50.400 --> 39:54.960
We really don't ask all the neuroscientists, you know, how did we do that?

39:54.960 --> 39:56.120
I'll call you back in five minutes.

39:56.120 --> 39:57.120
How does that work?

39:57.120 --> 39:58.440
What are the brain?

39:58.440 --> 40:02.520
People can sort of say, well, you know, something lights up in your prefrontal cortex

40:02.520 --> 40:05.480
and like it's so hopelessly vague.

40:05.480 --> 40:08.640
So neuroscience is not going to be our salvation in the short term.

40:08.640 --> 40:12.200
On the other hand, we know a lot about, for example, the dynamics of human memory from

40:12.200 --> 40:16.760
a cognitive science perspective and we know a lot about things like goals and plans

40:16.760 --> 40:20.440
and intentions and beliefs and desires, stuff like that.

40:20.440 --> 40:24.760
We don't know how they work in the brain, but we obviously have them and we can build

40:24.760 --> 40:26.760
those things into our machines.

40:26.760 --> 40:31.040
And so the point of that chapter is to look at cognitive science and see what clues it

40:31.040 --> 40:32.040
gives us.

40:32.040 --> 40:36.160
I've already alluded to a couple of those like, it's okay to have an eight structure.

40:36.160 --> 40:39.160
The machine learning world right now is so obsessed with learning.

40:39.160 --> 40:42.480
It doesn't want to build anything in except for convolution.

40:42.480 --> 40:47.840
Well, convolution is actually in a neat prior, your technical people will know exactly how

40:47.840 --> 40:51.720
it works that allows you to recognize an object that's in different places in a visual

40:51.720 --> 40:53.920
field in cognitive science.

40:53.920 --> 40:56.760
They call that translation invariance for a very long time.

40:56.760 --> 41:00.480
And that's a clever way of building in, convolution is a clever way of building that into a neural

41:00.480 --> 41:01.480
network.

41:01.480 --> 41:02.960
There's so much more of that.

41:02.960 --> 41:06.440
But there's this like attitude and machine learning that that's cheating, you shouldn't

41:06.440 --> 41:07.440
build anything in.

41:07.440 --> 41:09.120
There's a crazy attitude.

41:09.120 --> 41:10.120
Look at biology.

41:10.120 --> 41:14.360
You know, it's spent a billion years building in the right set of genes to make us have

41:14.360 --> 41:18.080
brains that could react with the environment in a flexible way, like let's not throw all

41:18.080 --> 41:19.080
that away.

41:19.080 --> 41:22.720
So that's one example of the things that are in the cognitive science chapter.

41:22.720 --> 41:29.760
Yeah, one of my recent guests said, and in fact, I think this became the title of the interview

41:29.760 --> 41:32.960
episode that, and he was talking about something very different.

41:32.960 --> 41:37.840
I want to make that clear, but that, you know, AI is a systems engineering problem.

41:37.840 --> 41:44.120
And it sounds like in a lot of ways you might agree with that that, you know, but a lot

41:44.120 --> 41:48.120
of the pieces of the system that we need to engineer don't exist yet.

41:48.120 --> 41:53.080
I agree with that too, although I would say that we are as a community not doing a great

41:53.080 --> 41:59.240
job of leveraging the ones that do exist, that as a community, we're fragmented.

41:59.240 --> 42:04.400
So there's a small kind of classical AI group that's kind of doing what it's always been

42:04.400 --> 42:06.840
doing and not really paying that much attention to deep learning.

42:06.840 --> 42:11.000
There's a huge deep learning community that's not paying much attention to all to classical

42:11.000 --> 42:12.000
AI.

42:12.000 --> 42:15.440
They're kind of smuggler like we have good equations, we don't need that stuff.

42:15.440 --> 42:22.360
And so the divisiveness between the culture which goes back all the way to the 50s has kept

42:22.360 --> 42:25.840
people from doing the right systems engineering.

42:25.840 --> 42:29.000
But I also agree that we probably need to invent a whole bunch of new stuff too.

42:29.000 --> 42:32.200
So we can do much better even with the existing tools.

42:32.200 --> 42:36.280
And I think if we worked harder to integrate the existing tools we get a clearer idea

42:36.280 --> 42:40.520
of where we actually need the new tools and that's part of the thinking behind my new

42:40.520 --> 42:43.880
company with Rodney Brooks, the robust AI company.

42:43.880 --> 42:47.560
And is this Rodney Brooks, Brooks of Mythical Manman fame?

42:47.560 --> 42:54.640
This is Rodney Brooks best known I guess for the Roomba which he co-invented and a fantastic

42:54.640 --> 42:58.960
set of blogs about kind of what's possible and not that he's been writing recently.

42:58.960 --> 43:05.440
He was the chair of MIT's AI CSAIL laboratory for many years.

43:05.440 --> 43:11.200
He's done many great things and it's fantastic that we have him in the company.

43:11.200 --> 43:15.280
From a kind of concrete technical perspective, what are the opportunities?

43:15.280 --> 43:21.600
And it sounds like maybe a context for this is the things that you think that we have

43:21.600 --> 43:25.600
that we could be integrating into these systems.

43:25.600 --> 43:31.760
So I don't want to say too much in a way to protect details of what I'm thinking about

43:31.760 --> 43:36.800
within the company context, but broadly speaking, deep learning is part of the picture clearly.

43:36.800 --> 43:39.320
We need to do perceptual classification.

43:39.320 --> 43:44.040
But we also need that kind of knowledge representation, the classical AI used, in order to figure

43:44.040 --> 43:48.360
out how to represent common sense in machine interpretable form.

43:48.360 --> 43:52.160
There has to be a way of putting together the vast knowledge that humans have already

43:52.160 --> 43:56.160
accumulated with the kind of experiential knowledge that deep reinforcement learning

43:56.160 --> 43:57.920
is good at acquiring.

43:57.920 --> 44:01.520
That has to be the focus.

44:01.520 --> 44:09.720
And you mentioned a couple of areas in the conversation thus far, calls or reasoning,

44:09.720 --> 44:10.720
temporal reasoning.

44:10.720 --> 44:15.480
Are there any independent of what you're doing with the company?

44:15.480 --> 44:22.000
Are there things that folks that are practitioners that are listening to this and

44:22.000 --> 44:28.160
wondering, OK, how can I buy into this idea of kind of swinging the pendulum back to

44:28.160 --> 44:31.920
the middle and doing more kind of model-based approaches?

44:31.920 --> 44:34.440
You mentioned knowledge representation.

44:34.440 --> 44:35.440
What's there for folks?

44:35.440 --> 44:39.360
Where should folks be looking to really start working in this direction?

44:39.360 --> 44:43.560
I would say they should do a couple of things first, which are to read the new book of

44:43.560 --> 44:50.760
mine, repeating AI with Ernie Davis, because we talk about what common sense is and whether

44:50.760 --> 44:53.080
the challenge is in representing it and so forth.

44:53.080 --> 44:59.280
And I think we do that in a more accessible form that is in the technical literature.

44:59.280 --> 45:00.440
I think people should look at that.

45:00.440 --> 45:05.960
I also think they should look at Judea Pearl's very recent book, The Book of Why, on the

45:05.960 --> 45:08.600
question of causal reasoning.

45:08.600 --> 45:09.600
OK.

45:09.600 --> 45:12.640
So I think those are the two best places to start.

45:12.640 --> 45:15.280
They're not the most technical places.

45:15.280 --> 45:18.800
Both books refer to lots of technical literature from there.

45:18.800 --> 45:23.400
Another thing I would say is everybody in the field should know about the CYC psych system

45:23.400 --> 45:27.920
that Doug Lennett created, which still exists, been working on it for 30 years.

45:27.920 --> 45:33.280
Most people think it's a failure, and I think everybody who wants to work in general AI

45:33.280 --> 45:39.080
should have a view about that system, what it can do, why it didn't entirely work.

45:39.080 --> 45:43.160
What strikes me a lot is a lot of young people in machine learning have never even heard

45:43.160 --> 45:44.160
of it.

45:44.160 --> 45:51.560
It's the most sophisticated and comprehensive attempt to have a machine, understand everyday

45:51.560 --> 45:57.880
reasoning, like how objects work, how people interact with each other, kind of intuitive

45:57.880 --> 46:01.120
physics, intuitive economics, intuitive psychology.

46:01.120 --> 46:05.760
And maybe it didn't work, but I think that the mountain that Doug Lennett was trying to

46:05.760 --> 46:07.960
get across is one that we still have to get across.

46:07.960 --> 46:12.960
I don't see how you get a reading system, for example, that really works if you can't

46:12.960 --> 46:17.400
ask part of the system, what happens when somebody loses their wallet?

46:17.400 --> 46:21.360
You want to know that they care, that their money isn't it, they might try to go find

46:21.360 --> 46:22.360
it.

46:22.360 --> 46:28.440
And you're not going to get all of that by just memorizing movies that you watch, which

46:28.440 --> 46:30.480
is kind of what I think the field is trying to do.

46:30.480 --> 46:34.760
So Lennett had an approach, and I think people need to use that as a mental crucible.

46:34.760 --> 46:39.160
So when you're getting started, you should read, rebooting AI, you should read the book

46:39.160 --> 46:44.160
of why, and you should read something about psych, you know, you can find various review

46:44.160 --> 46:48.400
papers that Lennett has written over the years.

46:48.400 --> 46:51.480
And like you got to start there by understanding what the problem is.

46:51.480 --> 46:55.120
None of the three things that I just mentioned have the answers.

46:55.120 --> 47:00.080
They just have a better cut on the questions, you know, Ernie and I don't know exactly

47:00.080 --> 47:01.960
how to build a general purpose AI.

47:01.960 --> 47:07.800
But I think that we are able to articulate some of the problems that need to be solved,

47:07.800 --> 47:09.160
that are being neglected.

47:09.160 --> 47:10.160
Fantastic.

47:10.160 --> 47:14.160
Well Gary, thank you so much for taking the time to share with us what you're working

47:14.160 --> 47:15.160
on.

47:15.160 --> 47:16.160
Thanks very much for having me.

47:16.160 --> 47:17.800
It's a really fun interview.

47:17.800 --> 47:23.400
All right, everyone, that's our show for today.

47:23.400 --> 47:29.920
For more information on Gary or any of our other guests, visit twimmelai.com.

47:29.920 --> 47:32.760
Register now for Twimmelcon, AI platforms.

47:32.760 --> 47:35.640
You definitely don't want to miss out.

47:35.640 --> 47:39.480
As always, thanks so much for listening and catch you next time.

