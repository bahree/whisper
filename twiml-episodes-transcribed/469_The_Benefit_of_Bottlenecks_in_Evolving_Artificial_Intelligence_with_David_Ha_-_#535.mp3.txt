All right, everyone. I am here with David Ha. David is a research scientist at Google Brain.
David, welcome to the Twomal AI podcast. Thanks for having me, Sam.
Hey, I'm really looking forward to diving into our conversation. I've been a long time follower
viewers on Twitter, and I definitely recommend folks to check you out there at Hard Maroo.
Why don't we get started by having you share a little bit about your background and how you came
to work in AI? Yeah, sure. It's kind of a weird background. I was originally studying
control systems back in the day in university. Eventually, for some one reason or another,
I entered the finance industry. I started off as a quants on Wall Street, actually.
I started working at banks, and then eventually I became worked on a trading desk as a trader,
and I spent around 10 years or so of my life in the derivatives trading, various different investment
banks, but things got a bit old and tried to learn different things. I was always interested in
neural networks because they're always fascinating, especially the biological inspired component,
and I started to do some reading and learning by myself. One thing that to the other, and
around the five years ago, I was able to join Google in one of their research residency programs,
and as a researcher, so that I was able to change careers and became a four-time AI researcher.
And so this is where I am now. That's awesome. That's awesome. As the idea or the attraction,
the initial attraction to the biological inspiration, has that held up for you? Do you
feel like the biological inspiration continues to inspire you, or was it a let down to find out
that the neural networks and computers are not all that similar to the biological ones?
I think to this day is still continues to inspire me and drives some of my work,
but we do have to recognize that modern deep learning or machine learning systems are very
different than biological processes. For one thing, we can scale them up. We have lots of
electricity and compute power, and the trend is actually having more compute resources,
and for machine learning and training to increasingly scale to larger models and larger datasets
and larger environments. And it's a bit different than biology, because in biological systems,
it's more like a biological intelligent life is more like coming from and evolving because we
have not because we have an abundance of resources, but more like we have a lack of it.
And I was fascinated at how evolution seems to select systems that are able to always do more
with less. But in a way, it's not just biological systems, but also the creativity process as well.
Sometimes we see some creative works. It's always like you're able to express more with less.
And I think the good, the interesting thing about being a researcher, especially at Google is
you do have a lot of resources. So you get to see both ends of the spectrum. On one hand,
you do get to see people who are really excited at scaling up the research and making very large
systems to work on large datasets. And on the other end of the spectrum, you have people working
on theory or on coming from theoretical physics backgrounds. And actually, they may not even
do a lot of extensive computational modeling. So it's good to see a balance of the spectrum of
a resource-heavy stuff, and also the things that concentrate on having very low resources.
And ultimately, you need both. You can have large models and you have a smart chips that run them
with less power. You alluded to this idea of constraints as playing a role in the way you think
about machine learning systems. Can you elaborate a bit on that? Yeah. Getting back to the idea of
constraints we see in nature, it certainly plays a role in shaping some of the research work
that we've been doing. In nature, for example, there are lots of examples of these so-called
bottlenecks that shape their development as a species. Just looking at the fascinating way
of how our brain is wired, how our consciousness is able to process abstract thought.
We have a language I'm talking to you in language, even though we have a video feed.
And also how we're able to convey concepts to each other, not just using languages,
but using drawings or gestures like this. And that's developed into languages, stories and
cultures. I guess to me, it's kind of debatable whether these bottlenecks or constraints
from our development is a requirement for intelligence to emerge. But it's also not
deniable that our own intelligence is a result of these constraints. On one hand,
maybe the argument is just because we have constraints that led to us, it doesn't mean we cannot,
I have the development of a general, a strong AI needs to have such constraints.
So there are opposite views of this spectrum. But for my research work, it's more like a
led by the idea of constraints. And you can see this from some of the work I've done even very
earlier, a few years ago, when I started to get into things like generative models.
Back then, GANs were really taking off in 2017 or 2016. They started to take off
and at the beginning, people were really excited at generating C-Fart 10 images, 32 by 32 pixels.
But then they got bigger, 64 by 64, 128 by 128, pictures of a datasets of CD albums or something
really cool. And so there's all this exciting work going on. And I had my share of playing around
with these generative models as well. So a few early works I've done is to build a generative model
for MNIST, the simplest dataset ever. But rather than taking the approach of generating pixels
directly, I try to generate a parameterization of MNIST, which can be very abstract in nature.
And with that led to like an early work, I combine some of the models from another researcher,
Ken Stanley at the time, who designed this network called CPPNs. So if you're familiar with that,
it's more like you take in the pixels. And you take in the coordinates and it outputs a pixel.
So if you have a simple rule that can take in the coordinate and output a pixel,
then you actually don't need to train the network to output the entire pixel. You just train
the network to simply give you the coordinate, I'll give you the pixel value. So I train such a
network to generate MNIST, so it's very elegant. And the end process is you can actually train it
on an MNIST dataset, 28 by 28, into this half-stract generative model, which I call a CPPN,
EAE or GANs. And then you can actually blow it up and generate MNIST digits that are like
1,000 by 1,000 resolution back in 2016. So before people can do, now we can actually model GANs on
1,000 by 1,000 resolution datasets with our exponentially increasing hardware. But I thought at the
time, it was kind of cool to be able to train again in 2016. Right after the EN Goodfellow is
again paid for K-vogue for a few months, and you're able to also produce 1,000 by 1,000 resolution
images by skipping entirely the need to produce such big images. And the key is to abstract the
principles of that image into an abstract representation using the CPPNs. And later work, I kind of
follow the same trend. I looked at creating a generative model of doodles. There's a model,
I don't know, you probably played around with it called a sketch RNN, that you can interactively
draw something on the web browser. And the model is like an anguish model. It'll continue to
predict what you're going to draw, like a stroke by stroke in a vector format. It's also like
an auto encoder model as well. So you can draw a full pig and then you can compress it into a
lean space and then redraw the pig out. So now is a very trivial in retrospect. But a few years ago,
it was one of the different models, because most people working on GANs and generative models
on pixels. And we're trying to do it on doodles. And at the time, the challenging thing was finding
the datasets. And luckily, one of my colleague, Jonas, at Creative Labs, they created a viral
game called a quick draw that collected some of this doodle data that we can use. So I thought
that was kind of fun, man. For that project, it was more inspired by, rather than trying to create
a representation, a minimalist representation, maybe we can use machine learning to study how
we humans ourselves do like a representation learning. Because of our own inductive biases,
you know, we're forced to draw doodles, maybe from the time we're cave people, because we have
a we have hands and we have sticks. So we develop this type of drawing. So maybe it's, you know,
a good idea, a good idea to get machine learning models to analyze how we develop this
representation. And that could lead to other ideas. And one of those, the ideas after
this sketch RNN paper was when I started to get into doing some work on reinforcement learning.
And, you know, like, what I thought was we have all of these cool algorithms that can train agents
to perform tasks when the agents are fed pixels like the entire screen, which was really amazing
at the time. At the time, the DQM model from the mine came out and agents started playing
pong or like Atari games, the entirety of pixels. I thought that was cool. But in a way, I kind of
think that could be information overload as well as most of the pixels are not useful when you're
playing like pong games or Atari games. So what I try to do is like, you know,
have enforced a type of constraints onto the policy or the controller so that it's not allowed
to see the full pixel information or the stream of pixels. And it's only allowed to see a
representation of its environments. So that, that work led to a model, a paper called the
World Models. It's kind of an exploratory paper that I published a few years ago with
JÃ¼rgen Schmittuber. And it was a really fun project. So the idea is we have a really simple
generative model. We use a variational auto-encoder to simply compress all of the screens into a
load-dimensional latent space. And we have another recurring neural network that simply predicts
the future latent space of the environments. So if you have a VAE that's trained on,
trained on your your game that produces a load-dimensional latent vector, your R&M will predict
your future latent vectors. That, that, like, depending on its current.
Are you projecting the future state of the game?
Yes, exactly, exactly. So that was a fun project because we're able to use these two simple concepts
to build a neural simulator of, like, games if we're able to connect enough data on it. And
what, what was fun about the project is we can, we show that we can just feed in
the representations learned from such a model, like the VAE's latent vector and also the
R&M's hidden state and feed it to an agent. And, like, at the time, this, we show that,
this hidden vector, like, this small, this bottleneck allowed the agents to, to discover policies
much, much more easier than compared to, you know, like, having to see the entire pixel information
is from an optimization standpoint. It's easier to figure out what you have to do if you're only
given, like, maybe 200 numbers and give me an action compared to if you're given, like, you know,
a million numbers every time set, give me an action, right? So because of that,
it was able to, like, solve tasks like the car racing game and open the agent.
Back then, it was, like, considered a hard task. Now it's trivial. But no one was able to get
the required score. I'm sure people tried hard enough, they could. But at the time, this was
the first approach that was able to get the required score for that game, you know, which was,
I guess, from, from the machine learning research point of view, it was considered state of the art.
I'm sure it'd be tweaked. Any model of it enough, you can also beat it. But, but apart from that,
with those kind of results, have we seen that idea of, you know, constraining your latent space
who come generally used as part of state of the art approaches in RL and similar areas?
Yeah, yeah, definitely. So from that, from that paper, I think, you know, whether we got state
of the art or not, it doesn't really matter for, in general, in machine learning papers, because
it'll always be beaten by later on. But the idea on that paper of that, you can, you can train
and you can learn a janitor model and train the agents entirely inside of that model to produce
a policy. That, that was the main idea that, that seems to have taken off in subsequent works.
Like, for example, after the world model's paper, there was another paper about, about
the model base learning for Atari. So, and where they literally caught their algorithm simple.
And the idea is basically, okay, you would, you would, you would, you have your agents collect data,
train a generative model of the environment to predict the future. And then, and then, and then train
your policy inside of that environment only. Right? Of course, at the beginning, you're not going to
get a good policy, but then it doesn't matter. You deploy that policy out and you collect
more data. And then you would refine your model and then you would redeploy it. So, so at,
when that work came out in 2019, at the time, that was then the state of the art for, for,
for sample efficiency for various Atari games, because simply because the learning took place in
the model. Because a lot of the sample efficiency, we've been, if we noticed, when we run an
RL algorithm, is, you know, you have the data collection process, but you're also learning
Indian environments. And that, that could, there could be some slippage in the efficiency.
So, if you're able to isolate the data collection from, from policy learning and your interactions
through the environment is strictly for data collection and for evaluation policy of your policy.
And all of your learning is done in the model. Then, then intuitively, that would help,
that would also help the data efficiency. And another line of work done by my colleague, Danny
J.R. Halfner, who is also working at a Google, but based in Toronto is, is a new, he started using
these latent based role models and combined them with planning algorithms. So, like traditionally,
planning algorithms are really useful for robotics. But at the same time, they're kind of flaky as
well, especially when it comes to, when you're getting the, like, for example, four video feeds
of sensory data, maybe traditionally, a lot of the planning algorithms were used on the state
observations. So, you're feeding, like, you know, like a really well engineered measurements
of your robot controller, right? But, like, how, then, how the key question is, how do you get
your robot controller or your control system to work on video feeds? So, then, I guess,
something like a world model or a latent based world model with, with this latent bottleneck
could be useful for planning algorithms, because then you, if they're really good at working with
load-dimensional data. So, then you give it load-dimensional data. So, the key idea behind that line of
work initially started by a model called a planet. So, it's a great kind of name is to, you have
you have this kind of latent, latent spatial temporal world model that is constantly updated as you
get more data, and you have a planner that would, that would get, that would figure out the optimal
action within the model. So, then, you don't actually go into doing any learning.
This is also helped with generalization. You'd think that a lower latent space model
has gotten rid of some of the noise that the full, the full world or environment
contains. And so, the agent might be able to perform, the agent performance might transfer
from one specific environment to another better. Is that actually the case?
That is still like an open question. For instance, if I naively train that world model based
on based on the data that we collect, then like, no, it's not going to generalize to variations.
An example is if we change the background color of your environments, then yeah, your VAE
or your latent model has never seen that before. And that generalization can only be done
in VAE learning. So, that algorithm would need to collect the new data and relearn its world again.
And whether it can generalize or not will be a question of how many shots, how many time
steps it has to generalize rather than a zero shot. But that being said, there is like a line of
work on looking at generalization problems within latent space models. There's actually a few
challenges. There's a deep mind robotic control, have a variation with explicitly introduced
lots of distractions and changing the backgrounds. And then you can employ lots of all sorts of ways,
like rather than training like an image-based latent space, you can do contrasts of learning.
There's a line of work doing that and so on. But for me, around that time, I also step back a bit.
There's all these, I have the same question as you, you know, we can this generalization.
Yeah, of course, there's lots of different ways of doing these latent space models.
But I looked at, along with my colleagues and my team, we started to explore them.
It's maybe latent space bottlenecks is one solution, but it might not be the best solution
for these generalization tasks. So we looked at maybe another bottleneck we can use is
something like attention. Or in our case, we try to use a hard attention. So like in a paper,
we published two years ago called a neural evolution of self-interpretable agents,
which is let by my colleague, Eugene Tang, rather than using a latent space to do this bottleneck.
The idea is we will only allow an agent to see 10 patches, for instance, of the screen.
And its decision is solely based on those 10 patches. Or however, however, number of patches we want.
So it's kind of like, you know, biological vision for our fovea type system where we have to really,
you know, like a one when when we study how humans see things, it's always like, you know,
attending to a bunch of points in front of us. But somehow we have a mental understanding of what
we're seeing. But it's not like we're getting like, you know, full on HD resolution directly in my
eyes. I'm actually seeing a bunch of things. So it's kind of inspired by that.
And in this example or paper is the where the patches, were you trying to emulate like a visual
field and the patches were kind of contiguous in a particular arrangement or were they,
you know, randomly distributed across the image. Oh yeah, so for this work,
it's part of the policy actually. So rather than having a randomized frame, the agent actually
has to learn to decide first which 10 patches to choose. Right. So like it's like it's like how
when you're looking at me, somehow you're deciding where to position your your eyeball on the
screen. So in the same way, the agent has to decide which 10 or it doesn't have to be 10. It could
be one or two that don't still work. But we do a sweep and we can be a bit more general and then
choose five or 10. So it's less like I was originally thinking it was along the lines of masking
for kind of generalization or regularization. This is more learning where to attend to within the
image as a as a constraint. Yes, exactly. And it was it's also inspired by a line of work in
psychology a few decades ago. The whole concept of a script you heard about is this in
in a tentative selective attention. So this is an intentional blindness. So sometimes our brains
just don't see part of the screen or part of what we see. So there was a psychology experiment done
back then where the subjects were asked to to look at a scene and the scene had two two teams of
basketball players. One wearing white shirts and the second one wearing black shirts. And I think
the subjects have to count the number of times that's the ball was passed between the white shirt
the players to the black shirt players and something like that. And then there's a gorilla working
walking up from the background. And most of the time the subjects were not able to to see the gorilla
because they're so focused on the ball and the colors of what people are wearing. So that kind of
helped create some some analogies between okay we have this thing called whether we like it or not
called intentional blindness. What if we try to do something like that with an RIO agents? What are
the pros and cons? Does it give it more abilities or does it actually deduct some of the abilities?
And that's that's what we're trying to explore. And it turns out that's using this simple scheme
we were also able to train some simple agents to do the same task as the world models paper
like getting a pretty good score on a car racing game from pixels and playing the dune game.
But unlike the previous latent space models, this model we found can easily adapt
to to augmentations to the environments. Like for instance, if we in doom in that doom
this in environments if we change the color of the ground it will still work. If we add like a
little blob on the side of the tracks on the car racing game, it will still work. And the reason
is those patches are likely not to be selected by the pre-trained agents. So so it's just simply
it works because it's just not attending to the things that deemed not to be that relevant
to some extent. Of course this is very like a naive way of like approaching the problem
because like in reality it's very nuanced like we do see a bit of it but it's kind of a simple
model that that clearly demonstrates that in attentional blindness if we strictly enforce it
in the context of NRL agents it will have these properties that because it's simply not allowed
to see certain parts of the screen it may lack if you throw away information but you also gain
ability to to generalize to to changes in the environments. Yeah yeah and does it how does it compare
from a sample efficiency to the constrained latent space does it retain that advantage in some way?
For for this one no no because it actually takes a bit more time to train or to evolve the policy
for to be able to perform the task but but the way I think about these issues is that there's
a few dimensions you know you can you can work on optimizing the sample efficiency like maybe reducing
an RL algorithm from you know 200 million time steps to 100 million time steps to achieve some
score or you can think of a sample efficiency in terms of of a zero shot transfer so one can
argue that okay I spent all of this time figuring out the policy using a hard attention in this
paper but if you give it a new environment which is not the same as the original environment but
one that has has some augmentations to it we can argue that that's a new environment and and how
many time steps would it take your agents to adapt to that new environment and here the case is
zero because it's a zero shot so that's also like another way of looking at the sample efficiency
as well not not on the training task but on the task that it has never seen in life kind of arguing
for a global sample efficiency in a sense across multiple problems or versions of a problem
is exactly or in in our case I think I'm really interested in in in sample efficiency across
on the same versions of the problem and that that's basically what what one of the the goals
of of AI is it's like of course given enough compute we're we're gonna solve every known problem
that is well defined but one of the thing is that this thing which is us from machines so far is
like our ability to to solve problems quickly that that we have not seen before with variations
you mentioned earlier your work with Ken Stanley and you just mentioned this concept of evolution
I spoke to him probably several years ago talking about his work in neuro evolution
and this we're using evolution loosely or have you also studied the these ideas of neuro evolution
and kind of evolutionary neural nets and machine learning oh yeah yeah for sure for example the work
that I just talked about the neuro evolution of itself interpretive agents like we actually
used evolution or computational evolution algorithms to train the agents so rather than using
like like reinforcement learning to train them so like in in general I kind of like some of
the evolution algorithms because they're kind of like we can use them as a black box optimizer as
well we we don't necessarily need everything to be nice and differentiable which is one of the
key properties of many many domains right now so one one of the things are differentiable
than you can put it into the machine and you know you're you're you're you're gradient based
optimizer would get the solution but because hard attention is it's kind of difficult to make
it very differentiable or there are methods but there is challenging it's just easier sometimes to
use evolution to solve these problems so one hand we we do like to use evolution and specifically
evolution strategies and genetic algorithms as a tool to to help us find solutions but we I did work
on some research projects where we're also developing these evolution algorithms as well
so there there was another paper with with with these themes of constraints was done with with me
and I was led to my former colleague and intern Adam Geier in a paper called weight agnostic neural
networks so the the key concept in that paper is you know we want to find the neural
network architectures that have a really strong inductive bias for certain like reinforcement
learning or machine learning task and can we go to the extreme and find architectures that can work
even without training weights so so usually when we think of neural networks you think of
having a neural network architecture okay and then let's run the optimization algorithm using
SGD to find the weights but here we okay what can we still find the architecture that can still work
when we don't train the weights like when the weights are chosen from a random distribution
so that that one is when we when we looked at we essentially looked at doing architecture search
where we want to optimize the performance of the architecture with a given weight distribution
right so of course your architecture is not going to perform as well as when all the ways we
refine tune but this is still very useful because as you know a neural architecture search is
extremely computational intensive so you you will have a batch of architectures and then you'll
have to find the weights of all of these architectures and and then you would go on to find the
next set of architectures you use your results but here we can simply find the architectures and
evaluate their performance on on random weights and and then we can find architectures that are
have have a very strong inductive or even like an innate bias for certain tasks so then
the intuition is like is kind of inspired by by the biology sometimes the organisms have some
ability the moment they're born to escape predators right you can imagine like maybe you can
you can have a bipedal walker controller that can already still walk forward when the
weights are not trained but if that's the starting point then then training the weights will be
a lot more efficient if you want to fine tune the network later on so that that's like to answer
your your question earlier is like this is like one example of the work that I was involved in
where we actually try to extend and and improve upon architecture or neuro evolution methods
to find neural networks that that are where we're not just an user of the evolution algorithm
as a black box optimizer and this paper was was apparently like a talk about in the neuro
science community a bit more than the machine learning community it was not so useful to them
like it would be like that we got our best score on M this was like 92% or I forgot
I'm there so it was like the the M this performance is is horrible so it's not going to be so
useful for for the M this committee but but the the papers do still got accepted at the
new groups conference you know and it got like a spotlight but it's probably one of those papers
that where we we massively underperformed the state of the art with like 90% on M this but
somehow still got in kind of luck done on that one nice nice I tend to hear neuro evolution coming
up most in the context of architecture search are there other areas where you see it being used
yeah well like as I mentioned earlier we we use it a lot just for policy search
we also see it used quite often in in robotics as well
huh like for example some of my colleagues in in the robotics team they they like to use
simple evolution algorithms to to quickly find policies like the the one that is
is used most often there's two that is really used often one one is a CMAES so that that is
that kind of like the the defaults evolution strategies algorithm that people like to use as a
black box optimizer the other one is caught a I think it's caught a augmented random search
is basically evolution is a form of random search and then this one is a it's a it's a very simple
random search algorithm that's directed in in a very simple way so so the robotics folks likes
these simple approaches because they're they're they're explainable and they're intuitive so I see
some people are using them to find the policies on on like on like like robots and then using them to
to like a control control these like mini-tower robots that they have in the lab
uh we I but I use them a lot for in general like especially if when I have a I have a neural net
without so many parameters like which is very common in RL like unlike deep learning where you
do have like you know 20 million parameter solutions in RL a lot of the the controllers like my
might work uh when when we only have have like you know the the 10,000 parameters or even 1,000
parameters so so it's like yeah and it could nice to say the RL is like a cherry on a cake
and so so the trend is uh you have all these self supervised models that are trained with
gradient descent with hundreds of millions of parameters but your actual policy network that
could be using all of these things maybe perhaps via a world model and and those networks could
might even just be a few thousand parameters and that can you know wipe out or using gradient descent
to train them when uh like uh if we're able to use evolution to train them we can get away with
doing things like non-differentiable environments and and whatnot so so we we tend to like to use
those as as a baseline in that case in conjunction with with other RL methods and and approach
this to policy or kind of um in isolation we usually like if we're able to get the solution we
want then we can use them like in isolation okay I mean there's some of my colleagues have been
working on ways to to combine the reinforcement learning with evolution so yeah then evolution
can kind of be the outer loop and the RL can be the inner loop but in a lot of the work where I'm
simply using like an end user of an optimizer then I simply use it to to get me get me a set of
weights or get me a set of parameters and go on in a day. Did we talk about the sensory neuron
paper? Oh no no no it's it sounds like I mean it kind of fits right into this idea of uh constraints
and um applying constraints to to make problem solving easier can you talk a little bit about that
paper? Yeah sure so like uh some of the previous work I I discuss it's it's more like you know
the constraint is more like a bottleneck like an information bottleneck and maybe you're doing more
with less but it doesn't always have to be like that so in in this paper we it was a really fun
project also with with my teammates Eugene Tang we we looked at the problem of uh what what if we
we gave an agent's uh an observation space that is shuffled around so like usually in a in these
reinforcement learning environments or in machine learning in general you have to give a model
very well-specified input data like like if if you give it like the the observation space of
of a humanoid or an ant's robots like every single input means something like maybe a tour good
of velocity or the positions or maybe the pixels on the screen this pixel corresponds to
to this has to be this position so we we toyed around with the idea of what if we we can randomly
shuffle the observations and the agent actually has to like figure out what each input like each
sensory input means before you know deciding an action and if an agent is able to to solve
a particular task or environments or or a machine learning problem from from uh shuffled
observations we can also examine the properties whether it has extra benefits that it has compared
to agents that are otherwise trained the normal way of just getting getting the inputs so this
is another type of constraint that I don't consider to be a bottleneck or information bottleneck
because I you're actually giving the agent the same information I guess like the dimensionality
of the information is the same but but here we we try to just shuffle the order and surprisingly
we're able to we're able to get it to work so like uh the the information of this work originally
came from some ideas on in a meta learning space because we we're essentially trying to
to get an agent to adapt to changing environments when it's like the agent will get a shuffled
and re-shuffled screen and it has to re-adapt but also in the in the neuroscience there's the
the area called a sensory substitution and uh it's a psychologist have to measure the human's
ability to adapt to when when what our senses give us suddenly change like like there's this
popular experiment done even a hundred years ago where you you're wearing this uh you're wearing
an upside down goggle I'm not sure you saw that so there's a there's a simple mirror glass
in front of your eyes so what you're seeing is is completely flipped and what people notice is
it requires maybe maybe like ten minutes or half an hour of readjusting and you're able to walk
perfectly fine with with this flip sensory but once you take off the glass then you're messed up
again for another half an hour or so so that that's kind of I guess one of the easier tasks
uh there there's a TED talk a few years ago where someone had a video of an inverted bicycle
so this one is harder when when you turn left you actually go right and when you turn right you
actually go left and they found this one really messed people up so you know like I guess because
a riding a riding a bicycle is more like a it's like a human invention I guess so it takes it takes
a long time for people to to read that because you actually have to balance as well you're like
a complicated control system constantly balancing you so that one really messed people up
I don't know if you've ever had the experience where your your screen controls get flipped
I don't remember what caused it um but you know your you know trackpad right becomes left and
up becomes down and and vice versa and that can be you know infuriated it's very difficult to
to adjust to yeah exactly yeah especially like you know apple like whenever you use like
apple products that okay but sometimes your trackpad goes the other way around when you're on
another person's trackpad or when they have a new model so I booked or MacBook Pros you have
you have a touch bar and you suddenly don't have a touch bar yeah no no we're back to the other day
but hey there's another uh there's another uh neuroscientist uh Paul Paul back Rita who's kind
of a pioneer of sensory substitution and and his claim to fame was uh he he had uh sub he had uh
people who were unfortunately blind like cannot like uh see uh this lack vision and back in the
end of the 60s he had an experiment where uh he put a low-dimensional uh video camera
uh there's analog back in the day and he he uh he fed some of those uh signals from the analog
video camera into a low-dimensional 2D grid of pokes into the person's back I've heard about this
yeah so yeah one of the cool things is uh our our skin or our our touch senses is under
utilized everywhere outside of our hands like maybe from evolution when we're hunter-gatherers our
skin was really important but in kind of modern times that we we wear you know like a clothing and
we we don't really use our touch senses but that does another interesting topic but it's kind of
getting signed like but back for for this particular idea is that he poked a low-dimensional
resolution of uh commissioned to the subjects back within a few weeks or months uh people gain vision
they were able to to see and understand things uh by sitting on his chair uh so so he showed that
through through uh touches or through pokes on a set person's back that person can can learn
to to interpret those signals as if that person was was seeing uh what's in front of the camera
and in the in the late 90s in turn of the century there was a variation of this uh from this team
where they they fed in a higher resolution video feed into a 2D grid of uh electrodes that was
placed on a person's tongue so so from from the the stimulation on the tongue the person's is
able to the the subject is able to interpret uh what the video uh camera mounted on the subjects
head is seeing and this was actually you know gain popularity uh so like uh people were able to
to live their lives then like having a low-dimensional uh vision system simply by learning to
predicting um how um the these sensory uh signals from the from their I guess from from their
tongue but however um these these are incredible it shows how how great we are but they require
like months if not years of training uh to gain mastery so it's it's kind of like okay sure
you can you can change you can switch around your inputs and retrain your machine learning
model from scratch even uh and using the new inputs and of course yeah then then you can deal
with these sensory substitutions so what the what we're trying to ask ourselves is can we actually
get an algorithm to do this without training in the in the traditional sense like with without
like a retraining your model where so uh where the the the the agent is able to explicitly adapt
to to these uh inputs uh so in the end uh even though this work is biologically inspired from
from on the problem side the solution we use has nothing to do with biology uh we're lucky enough
to to build on to previous work that gave us to do the tools to work with uh permutation
invariant networks and some of these uh works have been pioneered by by people working on the
transformer paper the original transformer paper uses the linear attention which was predate
transformer by lots but those those were shown to be uh permutation acrovariants so if if you
change the input order the output order changes the same way but uh there's another paper that came
out later called a set transformer which had a really cool trick on making one of the the
query matrix uh constant and that converted this attention mechanism to be permutation invariant
so suddenly you're able to feed in a signal of like uh of any order and the output will be the
same thing okay so it'll be like um it's it's a method to to take on a sets of an unordered
variable length sets and and you're able to to get uh get a permutation invariant representation
of it so we played around with this idea and applied it to reinforcement learning problems
so it's a weather you can uh you can feed in all of your signals whether those signals could be
the states of a pie bullets like locomotion environments uh or it could be like all of the
the tiles of of uh of an Atari game uh and you can feed them in any order you want uh and you know
you get the same representation coming up so we we tried to uh to to feed these representations
into into a policy network and train the entire system to to perform the task
and uh what we notice is that it is uh after after uh development we have to iterate on this method
it doesn't work at the beginning so uh some some of the the improvements that my my colleague
Eugene discovered is we actually have to feed in things like the previous action and have
have each sensory neuron uh for certain tasks have its own internal state so so for for for example
like your locomotion robot actually you know every sensory uh input goes into its own LSTM
and that LSTM will output a broadcast signal to the attention mechanism that will generate
this permutation invariance representation that can produce the action
so it's fairly expensive yeah yeah but yeah so in a way abstracting like usually uh
you know our in traditionally our networks are we just get the input right away
into into our particular uh input node of a neural network but here we treat every input node
as uh as a neural network itself so that's why uh yes the papers type of is the sensory neuron
as a transformer because these neural networks is has been inspired by the transformer architecture
so to to uh to I guess pay some give some credit to that and what we notice is like uh of course
this it's going to work for permutation invariance observations but we were actually without
additional training these agents tend to work even when we shuffle them during the observations
during an episode like like like for example if you have a locomotion robot plot forward uh
and of course it's going to work when you shuffle to input once at the beginning uh and keep
keep that shuffle order the same for the rest of the one thousand time steps because like by the
condition the the the representations don't change but what we notice is we can shuffle them
many times during the environments like like if you if your episode is one thousand time steps
you can shuffle them you know every one hundred time steps and the performance without additional
training remains roughly the same so so there there's something to be said about the the power of
uh of uh the agent's ability to to quickly re-adapt uh without explicitly learning to re-adapt
uh to the environments um that you look at um or would you expect to see that if you then
uh you train an agent with this capability or this constraint you know as you might say
and then you give it unsuffled data does it perform better because it you know has learned to
attend to important relationships in the scene as opposed to um an agent that you know
hasn't been trained in this way for for this one um if if we give a shuffle or on shuffle data
it'll perform exactly the same way um because the representations are consistent uh but but
that being said uh we we could do things like um like take away information uh from the input
or give it additional redundance information uh into input and have it still kind of work
like like for instance if the agent expects like five input signals to do a task uh you can give it
like you know 20 signals but five of them are the actual important signals and the other 15 are
pure noise uh a little like a small amount of noise and the whole thing can be shuffled
and without actually extra further training like uh it's only trained originally on the five inputs
it's it's still able to identify like it work i guess uh it's able to to somehow learn
that it should identify which signals are important without explicitly training to identify those
so i feel it's it's kind of like um this is somewhere between learning and metal learning
for something like a metal learning you're explicitly training the algorithm to learn an algorithm
that learns uh and if we're for learning you're just getting the algorithm policy for the task
here i think the the method is like indirectly learning uh self identification method
to to identify um like which patches or or which uh inputs are important and
the other uh interesting result is uh is from the robustness standpoint so if we apply this uh
methodology to visual tasks like the car racing game or pong like Atari games uh we know
this that we can do we can change the backgrounds uh of the game and the policy can still continue to
work uh to some extent uh this was not possible in the earlier work on on the on the hard attention
so when we change the background it still fails but here uh when we change the background for
for the car racing task uh without explicitly training on these new backgrounds the policy can
can still work to some extent and the the performance is is almost as well that is good for
these generalization tasks compared to existing works in the literature that are explicitly designed
to do such generalization uh but but here it's like a byproduct of of okay let's train our agent
to work with shuffle inputs and oh by the way you know the generalization abilities are
are like just a byproduct of of this this constraint um so when we duck into further like uh the
hypothesis is if we if we shuffle up all of the the patches or the tiles of the screen uh we we
could force the agents to to learn like the to essence the essential important things for the
task and because it's it's forced to learn the essential properties that may help it generalize
to to variations of the environment with different backgrounds and when we did further analysis
we actually looked at the patches that they learned they learned to attend to like in in the
car racing game and it turns out that even though the patches are all shuffled around
it still learns to to attend mostly to the patches that correspond to the edge of the road
even even when the screen is all shuffled around and so then this can help us explain why
the generalization still works to to environments with with when we change the background
because like it's not seeing it's not really attending to to the positions with with different
backgrounds is still looking at at the road so some of the analysis is done in the paper to to
explain why the the transfer works got it so given this uh you know body of research that
that you've pursued focused on the ideas of constraints incorporating ideas like noravolution
you know what are you excited about looking forward where where do you see your research headed
yeah so like I'm really fascinated with the whole concept of of the self-organization
so especially like I was I'm really inspired by this body of work that that my my colleague
Alexander Mordensev did on a neurosurder automata and also self-organizing
uh class and this classifiers which was uh recent articles on the disto pub platform and
like one of one of the things that excited me about this self uh the the sensory you know neuron
paper is uh the it is sort of like a self-organized system every input goes into uh an identical
neural network with its own hidden recurrent state and somehow these neural networks learn to
communicate via attention mechanism to have this emergent property which is the policy and I'm
really excited about like going forward with exploring more of these uh collective intelligence
themes where you where you have a you have an emergent property from thousands or even like
hundreds of thousands of different unique agents or units that have their own local processing
rules but somehow as a whole you have some global emergent property that that is a that is a result
of maybe some evolutionary optimization and I want to explore like properties of these emergent
prop uh behavior because maybe that will help us address some of the shortcomings we see in
in reinforcement learning like like a like a lot of like some some of the issues in RL has to do
with like robustness generalization um maybe like uh out like a sample efficiency and so on
but uh we can get inspiration from other areas like like like for example swarm
swarm computing swarm optimization uh like a multi-agent systems and and maybe if we look at
if we try to break down a problem into into a large like a complex systems problem where you
have lots of local computation perhaps that might give us some insight or or different types of
solutions to to how we've been able to approach them so far so I'm excited about the general idea
of like a collective intelligence complex systems and going forward we want to see you know how we
can like a bridge between the complex systems uh research and incorporate some some of the good
ideas into machine learning and also maybe look at the other way around maybe we can use machine
learning to also help uh advanced state of complex systems research awesome awesome i'm looking
forward to following along as you uh push forward in that direction david has been wonderful chatting
with you thanks so much for joining us well thanks for having me Sam always thank you
