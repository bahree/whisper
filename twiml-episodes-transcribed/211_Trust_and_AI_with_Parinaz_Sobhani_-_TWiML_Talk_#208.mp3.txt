Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we explore perspectives on trust in AI.
In today's episode, we're joined by Paranaz Sabani, director of machine learning and
Georgian partners.
In our conversation, Paranaz and I discuss some of the key issues falling under the trust
umbrella, such as transparency, fairness, and accountability.
We also explore some of the trust-related projects she and her team at Georgian are working
on, as well as some of the interesting trust and privacy papers coming out of the NURPS
conference.
This week's series is sponsored by our friends at Georgian Partners.
Georgian Partners is a venture capital firm that invests in growth-stage business software
companies in the US and Canada.
Most investment, Georgian works closely with portfolio companies to accelerate adoption
of key technologies, including machine learning and differential privacy.
To learn more about Georgian, visit Twimbleai.com slash Georgian.
And now on to the show.
Alright everyone, I am in Montreal and I've got the pleasure of being with Paranaz Sabani.
Paranaz is the director of machine learning at Georgian Partners.
Paranaz, welcome to this week in machine learning and AI.
Thank you Sam.
It's great to see you here and talk to you.
Absolutely.
It's been a while since we've seen one another and I'm looking forward to diving into the
conversation.
But before we do that, can you share a little bit about your background?
Had you start working on machine learning?
Long story.
It's always a long story.
I know.
It's always a long story.
I started working in AI and machine learning when I was in third year of my bachelor and
I joined the Robocop team.
I don't know if you know what does it mean, Robocop.
Robocop, I just spoke to someone who also did Robocop and that was their kind of entree
into ML&A.
Oh, so cool.
Exactly.
He's a kind of simulation for soccer and in back home in Iran, they were all like the
big fan of soccer and that's why we wanted to do something different, maybe simulation
with robots and then I really found it amazing.
And then in my master, I worked also in machine learning, I worked on data stream classification
and concept rift detection and then I came to Canada to do my PhD and I found one of
the most challenging problems in AI is like natural language understanding.
If you look at the other problems that we are solving using machine learning technologies,
speech recognition, vision and image kind of processing, I feel like out of all NLP and
natural language understanding is one of the most difficult one and it's mainly because
over time, over the course of human evolution, natural languages has adopted all the kind
of human complexities.
So if all I've got to be one of the last things that AI can achieve, truly, I mean like
actual natural language understanding and generation, not simple mapping, sequence models
here.
So and then in my PhD, I've worked on sentiment analysis and stance classification and these
are like kind of baby steps towards natural language understanding, mainly takes classification
for social media and then I've worked for Microsoft research, working on machine translation
and also for National Research Council of Canada, working on natural language understanding
using deep neural networks, mostly three base LSTMs, that is structural LSTMs and stuff
like this.
And then were you working at Microsoft research kind of in the hey day of the neural machine
translation when they were exactly that was a Google starter that they really wanted
to have something similar and they wanted to ship the product as soon as possible.
Okay.
Yeah, exactly.
Oh, wow, exciting.
Exciting.
It was a lot of that happening in Toronto.
No, in Seattle.
Oh, in Seattle?
Yeah, in Redmond, actually.
Okay.
Yeah.
Okay.
The actual headquarter.
Yeah.
And then I joined Georgian Partners two years ago, which was kind of like big decision because
like nobody can, nobody in my area, nobody in Michelinink kind of community can think
of like venture capital as a carrier path and it was, I found, I didn't know about a lot
about venture capital as well, but I found the opportunity pretty unique and the kind of
opportunity that is like you're going to have more exposure to multiple businesses and
you can work with several different startups.
Actually, at some point, I decided I don't want to stay with big corporations.
Okay.
And I was so interested in a startup ecosystem, but I didn't know anything about startup
ecosystem.
So I thought, it's going to be fun.
I'm going to learn a lot and actually I learned a lot.
And at the same time, I was, it was more meaningful for me because I was helping startups
that they didn't have resources or skulls like this cause I had.
And I thought, okay, Google has so many smart people, Microsoft has so many smart people.
So let's work for some places or work on some problems that if I don't work on them,
there is nobody else to work on them.
And so how did you get started working in the domain of trust and fairness in AI?
I guess like most of other machine learning like machine learning centers that are working
on real world problems.
In one of our projects, we realized, oh my god, it's such an important topic.
And I actually realized I don't know anything about it.
So I have focused a lot on the optimization and the choice of machine learning models
on like data processing, future engineering, but I never thought about these kind of fundamental
problems.
And again, it's kind of interesting at least for most of like products that I worked on.
It's not full automation, it's more like a human in the loop that you provide recommendation
and you have to build this trust with the human so that they will act under on, they will
act based on your recommendations.
And I thought, oh my god, even if I build the best model, if I can't build that trust
by having better user interfaces or interpretability and shrink the value of your time and shrink
maybe like for some applications, more critical decision making.
It's also important to show the model is fair.
Then actually the product would not be adopted and all my efforts going to be useless.
So that was the story.
And again, it was working with one of our companies and I can't share a lot about the company
and the project, but it was a pretty important decision making, automation for a pretty important
decision making that could have impacted the individuals and realized that we never evaluated
the performance of the model against fairness, against bias.
And we started to think of like what would be the sensitive attributes for their problem.
And looking at the literature, it was more about gender ethnicity, but then we realized
it's not only limited to gender and ethnicity or religion.
And for every application, we might have a different set of protected attributes that
we have to consider.
And then, so that was the fairness story and then we decided, okay, how we can let's
first of all quantify the bias in the model.
And we used a fair test.
There is a kind of open source tool which is great, very good user interface.
And what's it called?
Fair test.
Fair test?
Yes.
Okay.
I can't remember the authors of the paper, can't check, but I can't remember anyhow.
But then, and we could kind of identify and quantify a couple of sources of the bias.
And for their problem, we could solve this issue by having better sampling strategy.
To as far as, as soon as we identified what are the kind of under-representatives in this
data sets, then we could have kind of, it was only a single iteration, it was several
kind of back-end-forced testing the model, improving the strategy, sampling the strategy,
and then testing it again.
One of the things that you mentioned that came up in the context of trust was user interface,
which is not something that often comes up.
What are the, when you think about kind of the broad issues that fall under trust?
What are the kind of main things that you're thinking about?
When I'm talking about trust, first of all, I'm thinking about transparency.
And it's hard, and again, all these kinds of problems are hard to define, at least in
a mathematical way.
So by transparency, I mean, is it clear for the kind of users or customers, how are you
using their data for what kind of processes you are using their data, and how the data
has been shared across several different kind of use cases or processes?
And then, interpretability, explainability is also, is really, really important.
And again, I remember one, two years ago, when we were talking about interpretability
in machine learning community, everybody were like, why should we focus on interpretability?
We should focus on improving the performance of our machine learning models.
And exactly in one of the projects that we were working on, we realized that users are
not looking for that single probability or prediction.
They are asking for more evidences, and it can help them to combine the knowledge of
the machine or models with their own knowledge and having smarter decisions.
So there are several different reasons that we might need interpretability, that's one
of them.
There are lots of discussion if interpretability can help causality or identifying if the
model is biased or for scientific purposes.
So there are several different reasons that we might need interpretability, that was one
of the examples that I encountered in my kind of experience.
And privacy also is really important.
Again, it's not a secret anymore that machine learning models, they can memorize data.
And especially if you use sensitive data to build your machine learning models, these
models can remember the training in senses and can be reversed engineered to get access
to those sensitive attributes of the customers or users.
As we are using machine learning models for more kind of sensitive applications like
healthcare, the importance of privacy is even higher and higher.
Security is actually more important than any of these because it's actually the fundamental
layer.
If you don't have the secure kind of computation or if you don't encrypt your data in transition
or in rest, and if you have a data bridge, nothing else can work, you know what I mean?
You have lost the trust.
Right.
So it sounds like a lot of these issues that we sometimes think of as disconnected issues
are all kind of come together when it comes to creating a trustful relationship between
the user of an AI system and the system itself.
Exactly.
Yeah, exactly.
And like there are, every day there are new issues, for example, adversarial attacks say
are new issues completely specific to machine learning models.
And like AI safety, like now these days I nips again, there are a lot of interesting papers
about adversarial attacks on AI safety.
Before we weren't even aware of possible risks of using machine learning models.
Are there specific examples of projects that you can talk us through that, you know,
where you kind of explore these various issues?
Again, for sure talk about differential privacy.
We've worked on several different, differential privacy projects.
And as you know, we try to have the message as you can create value by having more private
differential learning and data mining approaches.
As an example, you can convince the user to share their data and you can aggregate data
across multiple customers, building better performing machine learning models at the same
time.
You're going to have guarantees in terms of privacy of the users.
So that's the kind of like one of the most kind of inspiring projects that I've worked
on because again, it's not only about these are the risks and it's also about creating
value.
It's also about win-win kind of like you're going to have private models you shouldn't
and at the same time you don't need to sacrifice the performance of the machine learning model.
Okay.
That's the kind of issue we have also with fairness because normally in fairness people believe
that there should be a trade-off between accuracy and fairness.
Why they really don't believe that's the case.
And at least in our case, when we improved the kind of sampling strategy, we could improve
both the performance of the model at the same time.
We could have root out the bias and kind of discrimination in the system.
So this example where you had to work with improving the sampling strategy.
What was wrong with the model before you did that?
So like we didn't, again, there are several different definitions for fairness.
So we weren't mostly looking at group fairness.
So for protected groups or for microseconds of the population, there was a huge difference
between performance of the model and that's why, again, I know lots of people argued
that should we have individual fairness or group fairness, but what we achieved by having
better sampling strategy was group fairness for protected groups.
And so can you walk us through those two different definitions of fairness?
Like how are people thinking about this problem?
So like, again, like some people argue that even if you have group fairness, you might
not have individual fairness.
For example, me and you having the same qualifications for the application, for the application X, we
might be discriminated, we might not get the same utility out of the machine learning
models.
But at the same time, the model might have a similar performance for microseconds that
we belong to.
So there's a kind of intuition behind group fairness versus individual fairness.
In a space like this where you mentioned that we don't have good definitions for this
stuff in math, it's even like we don't have good definitions for this stuff in English,
let alone math.
There's so many different ways to define all these things.
How does one go about creating, charting a path through such a marquee territory?
It's really difficult.
And you're right.
Every time I'm giving a talk about fairness and masking, has anyone have a good definition
for fairness?
I'm really willing to listen and learn more about it, but still there is no concrete
kind of definition or metric.
And again, we are talking about like we have to quantify the bias, but then if we don't
have any kind of metric that everybody agrees, that's a metric that we have to use.
It's not like if one scored at everyone using machine learning now, we need to have
similar score.
But it doesn't mean there is no research.
I'm really encouraged that we are in the right direction.
In the machine learning community, even again, last night I saw a couple of papers that
they were talking about different metrics about for fairness in Newrips.
So like again, the community is in the right direction.
We are thinking about these problems and we are and the other kind of encouraging thing
that is happening is that we are kind of engaging with other communities as well.
So now in the conferences, every time I'm giving a talk, I saw a couple of people from
legal domain and they are kind of challenging me and asking me questions and I learn a lot
from them.
Also, there are also people from sociology interested in these kinds of problems.
So we are in the right path, but it's still a long way to go.
Yeah, I was at the kind of the exhibit floor here in Newrips earlier and was getting
a demo of some project that IBM research was working on as Fairness 360 Toolkit, I think
is what they called it.
They were going through some examples and they had like six different half a dozen different
fairness metrics.
They touched on this kind of group versus individual and some others and then they had several
different ways that this kit could help someone address these kinds of issues, just like
statistically working with biases that might be in their data.
But even with six, they're like, oh, and it's open, other people could add their own.
There are just so many different definitions and ways that you might want to approach
this.
Yeah, that's right.
One of my favorite papers this year in Newrips is my classifier discriminatory.
It's a really good paper because it also talks about let's root out the bias and let's
find where this bias is coming from.
It's not enough maybe to play with the optimization functional objective function.
It might be better to start trying different sampling strategies or even start capturing
more information about the entities.
For example, there might be a strong predictor that we are not using in our data.
So it's kind of like it's not only about optimization, it's not only about mass, it's
also about figuring out what is missing here.
And I guess it was also in my practical experience more important than the mass or the optimization
part.
Quite a couple of approaches in the literature, but none of them actually gave us a pretty
good result.
So what do you mean by finding other predictors?
I guess what I'm hearing is something that any data scientist is always trying to do
find additional features or variables that might have predictive value.
Are you getting at something different?
You know exactly is the same thing, like that you can understand if you can kind of figure
out what are the holes in your system and can you get access and you're right every time
we're asking more information, we are asking for more resources, time and effort.
And the question mark is that it's worth it to invest in kind of capturing more information
and getting access to that data.
And sometimes it's worth it if we kind of realize it has a huge impact both in the performance
of the model and also on the bias and fairness issues we have.
One of the things that seems a bit paradoxical in this space is that I think I have the impression
this isn't particularly well informed, but my impression is that in a lot of highly
regulated industries, you know take for example banking, they historically haven't collected
information about these protected attributes because they don't want to be accused of
using them to make decisions.
And now we're talking about using statistical approaches that require that need that information.
Have you run into that kind of thing?
Yeah, I was in the panel exactly with a couple of other people, VPs and leaders of those
financial institutes and they were, I was kind of arguing and challenging them, but I need
access.
As a data scientist, I need access to those protected attributes and they were like, why
should we give you access?
And I know there might be lots of privacy issues as well.
So I was actively thinking about what would be the solution.
So if you don't want to give access to those protected attributes, at the same time you
want to give the data scientist the opportunity to test their models against different kind
of biases, what would be the solution, the simplest one I could find was like having
an API.
So you don't access the data scientist attributes, but you can have an API that you can send
your requests to a server and you can get the answer.
So that's kind of like simple, simple maybe kind of solutions, maybe we need to do more
research on what's the best kind of framework or process.
But yeah, that's a kind of huge problem we have with the senior leaderships of the companies
that they are like, why do you need access and it's hard to convince them.
We do want to use them to improve the performance of our models, actually when we use them to
test our models.
And without having access to them, there is no way that we can't test our models.
Is it an absolute requirement to be able to test against a particular kind of bias or
bias against a particular attribute to have those attributes available as labels or are
there other ways that you could infer that from your data or kind of explore from your
data?
That's a very good question.
So you normally need to define the protected attributes and then test the models over
different macro segments based on these protected attributes or sensitive features.
But there are always proxies in data that we know they are a kind of good proxies of
for example, even if we don't have access to gender, the other attributes might be very
correlated with the gender.
Okay, just part of the problem.
Exactly.
It's just like, yeah, that's exactly the problem that two years ago when we were talking
about bias and fairness, they were telling us, okay, just remove those sensitive attributes
from your data.
We were like, okay, don't you know about correlations in features?
But at the same time, we can't leverage those kind of correlations if we don't have access
to it.
And in fact, for that project that I was talking about, we didn't have access to lots of
information like this.
So we used lots of proxies.
But you weren't necessarily able to even test your proxies against any kind of reaction.
You're right.
We don't know how accurate these proxies.
The only possibility is this for example, for example, the address might be a proxy for
ethnicity.
Right?
Sure.
So again, then you can just on public data sets, you can kind of test the correlation between
the address and ethnicity.
That's the kind of possibility, but on your data set, yes, you can do anything.
Yeah.
All this is making me wonder if there's some kind of intersection between the differential
privacy and fairness, not, you know, so setting aside the privacy uses for differential privacy
like, are there ways to, I don't know, maybe mixing up streams, but I talked to someone
else who was working on kind of a, like a cryptographic way to allow people to do testing
against data or share data without giving them the data itself, which is kind of basically
like this API.
Exactly.
It's the same basic thing.
And so I don't know if the only thread between those two is, no, that's a very good point.
And I believe two years ago in KDD, Cynthia, she also had a invited talk and she talked
about maybe we should use techniques similar to differential privacy for fairness.
And she talked, okay, like in differential privacy, actually, if you think of like the
kind of data we have as a kind of matrix, and in differential privacy, we want to mask
a row in the matrix.
So which means like we want our models to not be sensitive to any single user information.
But in kind of like when we talk about fairness, we want our models to be exactly, to be vertically
in sensitive to some features.
So there are some commonalities, but I didn't see any actual research in this domain.
It's a great opportunity for researchers to start working on kind of using techniques
like differential privacy for fairness.
So you mentioned one paper that you were excited about here in NURP's, what was the name
of that one again?
Is my classifier discriminatory?
And were there any other observations that jumped out at you from that presentation or
from the paper?
I guess the presentation is going to be tomorrow or in the poster, maybe tomorrow, but a friend
of mine, she just told me about this paper and found it pretty interesting.
There was another paper that I ran into last night in the poster session, but I can't
remember.
I can just Google it if you want.
No, that's fine, that's fine.
When you're working with startups that are trying to bring a product to market, they're
under all kinds of intense pressures to bring a product to market, to satisfy customers
like how do they, or maybe how do you typically have to convince them to pay attention to this
or are they already thinking about it?
It depends.
Like some companies, they have already some data scientists that they have thought about
those problems, some of them know, and we have to convince them.
So, what I found out is that people are normally more familiar with security box and a normally
kind of frame furnace or biased box as kind of unwanted association, but it's a box
as well.
And it's similar to privacy and security box that they are so hard to identify.
But the risks are also very similar.
So, the same risk of data breach, what would be the risk?
What's going to happen to the valuation of the company?
It's going to be very similar kind of impact on your brand if your model proved to be like
discriminatory or biased.
So, that's kind of high frame it, and I try to convince people to think about the importance
of these issues.
When you frame it like that on the security side and the security bugs that lead to breaches,
there are increasing amounts of hard data about the costs of these kinds of breaches and
the implications from leadership change perspective in that business.
Are there similar examples from a breach of fairness or a breach of trust perspective
or are you familiar with any?
Not in the startup ecosystem on slave.
For example, I guess you all know about a kind of blog post that was about Amazon recruiting
software that they start using it.
This was for folks that may not have come across this, an article that actually thought a
lot of the headlines were disingenuous.
It sounded like they were researching some, using some tool in HR and they found the tool
to have a bias against women in the hiring process and they decided not to use the thing,
which that sounds like what you're supposed to do.
Exactly.
Yeah.
If it wasn't the case, you wouldn't even hear about it.
Right.
So there's that and I guess the Netflix prizes, the one that comes along, that's more privacy.
It was privacy.
Yeah.
It was privacy.
You simple the identification and it failed.
For example, Google example, so that lots of people use it.
I don't know if they fix it or not, but I remember six months ago, if you search for images
of CEO, it was all white male.
And it's a hard question and it's a kind of question that even for Michelle and Ingrid
data scientists can't answer.
It's a kind of question that you have to ask in the community of interdisciplinary researchers
because actually, if you look at the statistics, what's the percentage of non-white male
CEOs and it's a kind of, if you do random sampling, what's the likelihood of having one
of those kind of, for example, women's deals?
Because right now, but it's not about equality, it's more about equity and like that's kind
of like hard to see, that's kind of, as a kind of scientist, you're working on this problem
and you have a random sampling.
You don't know.
So I would take that kind of restate that as part of the challenge is that we want our
models to reflect our values more so than the data that we have to work with to create
the models.
And so model may be performing accurately relative to the data, but that's not necessarily
what.
Yeah, it is reflecting the data, which is reflective of many, many years of biases.
Exactly.
Right.
And that's why we need to have more awareness that the main assumption of any machine learning
model is that future is going to be similar to the past.
So for any reason, if we don't want future to be the same as past, then we shouldn't
rely on data and we shouldn't rely only on machine learning technologies.
So what should we rely on?
Do we know yet?
No.
I was like, again, like in Norepse yesterday, there was a talk, I can't even remember.
There was like a tutorial, actually, Norepse Tutorials.
It was very interesting.
I didn't.
Were you there for that?
No.
It was a very good one.
And surprisingly, there were very, very few people in the room.
So like most of the tutorials, they were packed, but this one, very, very few people.
And I was like, why, these are like questions that we need to think about.
And unfortunately, there are not so many people in our community in that room.
So like, that was the sad part, but there was a tutorial on common pitfalls for studying
the human side of machine learning.
And they had very, very interesting points.
So they were like, maybe explainability is not enough, maybe it's not only about casting
the furnace as optimization, we should also fix the process.
They had lots of interesting points, and one of the interesting problems, if they point
out was data, data are not the truth.
So also we know that there are lots of noise in capturing the data, and even if we talk
about future, the past problem, we have data or machine learning models, but even sometimes
the data is not even reliable because there are so many kind of noise in recording the
data.
So, but my problem with those kind of talks is that they don't talk about the solutions.
And as a kind of practitioner, I'm always like waiting for, like, give me a solution,
give me a solution.
And it might not be a perfect solution, but it can be a starting point.
And okay, we are, we have a long way to go, but they have to start from somewhere.
Now structured is kind of your solution approach when you're engaging with one of these portfolio
companies that, you know, recognizes that there's an issue or that they don't want there
to be an issue.
That's a very good point.
So like, normally, if they don't, sometimes they also are aware of possible problems with
their data and the problem.
They are solving.
But sometimes it's harder to convince them, so I ask them, can you give me the access
to data and the predictions of your machine learning models?
And then I run some tests.
For example, they told you about this fair test.
It can give you all the statistics about kind of performance of the model, accuracy,
different macro segments.
And it's very different combinations of these protected and non-protected features.
And then I come with some kind of proofs, okay, these are my evidences that your model
is biased.
Okay.
And let's think about the possible solutions.
And if they are using any kind of sampling strategy, we would start with improving the
sampling strategy.
If you have lots of data on your only using parts of your data, how can we improve the
model by leveraging better sampling strategies or like other parts of the data that you are
not using?
Is the implication there that the problems are introduced by their sampling strategy?
Or they can use alternative sampling strategies to overcome problems that are already in
their data?
Exactly.
They can use alternative sampling strategies or iteratively try different sampling strategies,
test the model in that fair test kind of framework, kind of evaluating and observing
the result and having the second iteration.
The other kind of solution is that if they are getting their labels, if they're having
a supervised learning problem, getting the labels from the humans, and if they are using
human annotators, how can you define strategies to minimize the human bias?
So like I remember for my PhD, I used mechanical turics to label my data.
So we had all these kind of quality assurance techniques to make sure that we're getting
kind of accurate labels from our annotators, so we can do similar things.
So like quorum types of approaches and that kind of thing?
Exactly.
That's another one.
The other one is like, for example, one of my other favorite papers is the paper by
Rich Zemol, which is about learning a representation for your data that captures as much as possible
the information, but at the same time masks all the sensitive attributes.
So kind of having two competitive goals when you are learning representations for your
entities.
It might not be possible for some applications because like we don't have enough data to
learn the actual representation, we are not using fancy deep neural networks.
So like we have a couple of solutions, but still sometimes we are in a position that
we don't know what can be the solution and we start researching again.
So first step is playing with sampling strategies and seeing if we can solve it there.
And then the next step is just diving in and it sounds like it's a combination of data
science and research and subject matter expertise and it's, you know, like other aspects
of the science.
It's kind of all these two apart, yeah, exactly like any other problem.
You've mentioned how a lot of this work is interdisciplinary, are there specific disciplines
that you kind of look to first when you're looking for practical approaches that you might
be able to incorporate into solving a company's problem?
Normally what I found really useful is engaging with the product managers.
Because they have, they are the, or the product owners because they have the best understanding
of their user requirements and the addressable markets.
So like even their model might be discriminatory because they are not serving part of the population.
So it's not like cohort of the user might not be in the addressable market.
So it's really good to have their insights before having any kind of further experiments
or exploratory kind of analysis.
So like I found like product people are the best people to talk to.
But what is really missing is a kind of trust officer in our companies.
So and we try to, we just started to think about these problems.
Do we need a trust officer and it's not about the person who can solve every problem,
but this is a person that can collect all the information from different stakeholders.
Right.
And he's like the owner of building trust with the customers.
And again, it's not going to be a single person.
It should be education for the whole company.
And even we suggested you should have, you should add another kind of metric in your performance
evaluation when you're evaluating your employees performance.
You should add the trust component or responsible innovation or responsible AI kind of metric
when you're evaluating your customer, your employees performance.
It's not going to be a single person.
But again, you should engage as many as possible people from the company and even outside
of the company.
Because for example, social sociologists, like we normally don't have anybody with sociology,
background in our companies.
So it's really important to engage those people if you have such question like the Google
problem.
Right.
Yeah.
That's interesting.
I can see at larger companies, I wonder where this will end up living, right?
You've got your, you know, chief data officers that are responsible for, you know, both kind
of protecting and securing data or, you know, sometimes it's a CSO, chief information
security officer who owns that, a CDO might own kind of monetizing data or, you know,
the way that they manage it.
But to your earlier point, a lot of the, you know, the challenge and the risk for larger
companies would fall under someone who owns the brand, right?
Yeah.
I guess it says that it needs to be something that companies think about at the highest
levels and have dialogues about.
Exactly, somebody that can have a decision-power and is in all the decision-making discussions,
somebody in a leadership team like that because you want to make sure that for every decision
you make that can impact your users or customers, you have a person thinking about all these
kinds of problems that might happen later when you're, when you're deploying your
systems.
And so are there, when you're kind of looking for research to incorporate into finding
a solution to a given problem?
Is there anything unique about your approach to that in this space as opposed to trying
to solve a vision problem using unique research or is it kind of the same Google or kind
of search or something?
That's a very good point.
I try to not bias myself by reading only papers from Shalani community.
So it's still, it's still its research and I'm scientist and I have to dig into different
kind of papers and articles and blog posts out there, but at the same time I'm trying
to read more papers from other disciplines as well.
So that's the only thing I can also like, I try to be active in the community and having
lots of conversation with people with different backgrounds.
It also helped me a lot, but I really don't have any good answer to your question that
is there any fundamental difference between this problem and any other kind of problem
we are solving?
Right, right.
Are there particular communities that you've found to be very useful and under-recognized
I guess in terms of contributions to this type of work?
Underrecognized, again, ethics in here is also hot, so that's less of a problem.
Exactly, it's like harder than a couple of years ago.
Yeah, exactly.
It's not hot.
Lots of people interested to work in this area.
Lots of conferences, talks, events, yeah, top of my mind I have no example.
Any other things that you're looking forward to seeing here at NURBS on this particular
topic?
I believe what is missing is that our community at least historically wasn't very welcoming
for people with different backgrounds, so I told you that I in my PhD was working on
NLP and I believe I met lots of people with linguistic kind of backgrounds in NLP conferences
and it was more welcoming, so but here if you don't have mass background, you kind of
full intimidated, so and I don't know how we can encourage people with not necessarily
machine learning background to join our community and also give us comments and challenges and
challenge our kind of mathematical optimization techniques.
That's I guess what I'm fully missing right now.
Yeah, I guess that kind of underscores one of the, we are kind of starting to have this
conversation often about the role of diversity, like on teams and helping create awareness
of these kinds of issues and identify, you know, to help an organization be open to understanding
where these issues might lie and it sounds like you're kind of underscoring that at the
level of the community as well.
Yeah, exactly.
Also, you are right in our teams, you also need to have more diverse kind of opinions.
And again, when we talk about diversity, it's not only about ethnicity or gender, it's
also about having people from different backgrounds.
Well, Paranas, thanks so much for taking the time to chat with me about the stuff, super
interesting stuff and as always wonderful to catch up with you.
It was a pleasure and I also really enjoyed the chat.
It was as simple as Chang told me.
Well, we had a good one on the topic of differential privacy, Chang and I and I would encourage
folks who haven't listened to the former series on differential privacy, they should take
a listen to that one because it's good stuff.
Perfect.
Thank you.
All right, everyone, that's our show for today.
For more information on Paranas or any of the topics covered in this show, visit twimmelai.com
slash talk slash 208.
Thanks once again to the good folks over at Georgian Partners for their sponsorship of this
series.
As always, thanks so much for listening and catch you next time.
