All right, everyone. I am here with Jacqueline Nolis. Jacqueline is head of Data Science at Saturn Cloud.
Jacqueline, welcome to the Twoma AI podcast. Thank you for having me.
Hey, I'm looking forward to our conversation. We're going to touch on careers in data science as
well as desk, which is a open source project that your company is working on. But before we dive in,
I'd love to have you share a little bit about your background and how you came to work in data science.
Sure. So I had the odd circumstance where I fell into data science, which is to say, I started
years ago as an undergrad in mathematics. And then I got a master's in mathematics thinking I want
to be a math professor. Then I was, no, I don't want to be a math professor. I want to use math to
help businesses and stuff. So I started a job, which now today would be called a data scientist,
but back then it was business analytics. And I just kind of grew, I worked at an e-commerce
company doing forecasting for them. And then I worked in aerospace for a bit. And then I'm like,
I want to go get more technical skills. I'm going to go get a PhD. So I got a PhD in operations research,
my researchers in electric vehicle networks, like where does Tesla put their charging stations.
And about this time, the field was starting to get called data science. And while I was in my PhD
program, a company, like a boutique consulting firm was looking for data science consultants.
And I'm like, yeah, I'll make some side cash as a PhD student, you kidding me.
And then I finished my PhD and then I did consulting for full time for many years,
to the point where I was a director and like started my own data science team at a consulting firm.
And then I quit and started my own like freelance consulting business. And I did that for a few
years. And then I said, you know, I being a freelancer and working on your own is a lot of work.
And that's very stressful. Like I would like to go back into industry. And so then I ended up,
now I'm here at a Saturn cloud having a lot of fun. So awesome, awesome. Yeah, I think you were doing
the freelance consulting thing or maybe just at the point in transition when we first met. And
the circumstance around our connection was for those who, you know, may remember your name or
the panel that we did, we did a panel discussion on advancing your data science career during
the pandemic just about a year ago. And that was your kind of wrapping up the independent consulting
at the time. That's right. And so it was like, I was giving that, I was part of that panel.
I'm like, I also am advancing my career by trying to get a job again after working independently
for several years. And if I remember correctly, you spent some time talking about your career search
and kind of how that evolved for you and what was, what, how the pandemic made it interesting.
Yeah, because I did start looking for a job right at the beginning of the pandemic when they were
like three data science jobs in the country available, which I think now there are a lot more.
I think we've kind of gotten back to normal from what I've seen. But I was pretty touch and go in
like, what, do you, May of 20, 28, 2020? It was a rough time. Yeah. So, you know, I mentioned
we'll be talking about a couple different things. One of them is you wrote a book on data science
careers. You do a podcast on data science careers and you're generally out talking about
and helping folks kind of build their careers, get jobs in data science. How did that all come about?
Yeah. So that's interesting. So I, you know, I started like a couple years ago going to conferences
and trying to give talks. And I'm like, the thing I really like talking about is like, you know,
how do you become a director? How do you get your first job? Like I kind of always like thought
that stuff was not, or like how do you run a team effectively? And I always thought that stuff
could be talked about more like per hour than maybe some technical topics. And at one point,
I'm manning the publishing company read some blog posts I had and they're like, hey, do you want
to write a book? And I'm like, do I want to write a book? You know, I never considered writing a
book. But yeah, I do want to write a book. And so I reached out to Emily Robinson and we actually
had met several months ago because we're both giving talks at day-to-day Texas. And I said,
Emily, you want to come write a book with me? And she said, maybe give me a month. I'm switching jobs.
And so then a month later, and I just switched jobs too. And then like a month later, we're like,
yeah, let's do it. And so we spent like a year writing this book. And so like each chapter,
like the first half of the book is like, hey, if you're not a data science test yet and you want
to get a job, how do you get the skills? How do you build a portfolio? How do you write your resume?
And then the second half is, okay, cool. You're a data scientist. Now what? Like how do you think
about like making good analysis? How do you know? What do you do when a project is failing? Or
how do you quit a job and get your next one? And so it's really fun. And so Emily and I, we
wrote this book. It did pretty well. We're pretty happy with it. And we're like, we want to talk
about this more. And so then we made a podcast too, where now the podcast is each week, we kind of
talk about the themes of different chapters of the book. So yeah, so I guess the end of the result
is like I fell into it. But also it was a really nice venue because I had been a data scientist
for a bunch of years now. And I have all these thoughts. And I would just like to get them out
of my head and on paper. And I could just hand the paper to other people rather than keeping it in here.
So when you think about the first part of the book, targeting folks that want to get a career
in data science, what are the kind of top insights that you've actually, you know, in the book or
you know, since writing the book for, you know, how you go about that nowadays, you know,
beyond the, the usual, which I imagine is, you know, have some portfolio projects, you know,
get on LinkedIn and network, get on Twitter and network. I what do you find are the, the non-traditional
or the non-obvious, you know, things that folks should be thinking about? Well, I would say,
maybe I'm gonna say the obvious stuff, but I'm gonna think about it. Think about it this way,
this different way, which is the data science field at the entry level. I think it's starting to get
pretty saturated, which is to say, maybe like when I was looking for jobs, it was, you know, like 10
years ago, it was, oh, hey, you got a math degree. Cool. Come on. Oh, come in. And now like it's so
many people want to be data scientists that's getting trickier, which is not to say you can't get a
job, but it's just gonna take more thought and, you know, maybe work to do that. And when people give
the advice of make a portfolio, make a GitHub page, all that, what they're implicitly saying is, hey,
people who don't have data science experience, like pure like data science job experience yet,
companies don't know, hey, can I trust you to be a data scientist? Would you get here and know
what you are doing and how to handle it? And so something like a, you know, a portfolio or
GitHub page, things like that, they are letting the employer know, hey, look, no, you can tell I'm a
data scientist because look at this cool side project I did or whatever. But that's not the only
way you can do it. You could do something like, hey, if you're a, you know, software engineering
job, you can try and start doing data science things within that job, you know, before you look
for your next full data science job, or you can start by getting a data analyst job. And in that
role, which is similar data science, work there for a few years, then transition to data science.
So it's not like the pattern is first you make a GitHub portfolio, then they hire you. It's like
yeah, it's like, think about what are the things that you can do and you know your circumstances
best. What are the things that you can do that will help employers understand you are a good fit
for this position. And that may not be something you can do it over a weekend. It may take years
to really set that foundation to help you out. Given the, the saturation at the early stages of
data science careers now are employers looking for different things or they're different ways
that folks can and should be signaling that they have the required background.
I think that the classic like build a portfolio, that's good. Get something in your job, that's good.
Things like boot camps can also be good, but also you need to like, you need to do the work of like,
hey, I've been to the boot camp, I'm going to take this job and make it like so obvious how,
sorry, this side project I did as part of the boot camp make it so obvious that this is just like
what you're hiring for I can do it. But I think more broadly what I would say is, well,
that's kind of intimidating to know that the field's getting more tight at the entry level.
I would say at the, hey, if you have nine months experience in one data science role, I think it
is very easy right now to get another job, which is to say the hurdle between zero and one unit of
like pure experience is massive. So it may feel really hard to get that first job, but know that once
you get that first job, you stick in a frill, like just a tiny bit, it should be much easier going
forward. So it really is going to like, you are at the hardest part right now, people who are just
trying to get in and it should get easier. Hopefully as, you know, as the your career experience
progresses. Do you find that that kind of tenure or churn in data science jobs, these early data
science jobs is more or less than other areas that like software engineering? I don't know if I
would say data science is more churn than software engineering. I guess it wouldn't surprise me if
it was, but I don't know. Yeah, for the typical tenure for a first, you know, first data science
gig. I will say, I mean, it's probably like, I would guess it's like a year and a half, two
years average, which I think is probably similar to software engineering. And I think the reason
why you leave data science jobs is not that different than the reason you leave software engineering
jobs, right? Like you don't get along with your manager. You don't agree with a couple of these
missions. Be hours aren't good. Like it's kind of like a lot similar. But I do think that
that the people I know who have left after nine months of their first job, it's not like they
left and then they couldn't get another job because they only had nine months experience. It's like,
wow, people see nine months like, cool, you've worked somewhere and done something. Come on in,
you know, like we are not a lot of experience data scientists out there. So we are hiring
pretty quickly at that level. You know, and like disclaimer, what Jacqueline says is not a
career guarantee. This is just what I've seen. Yeah.
Yeah. That's a little bit of a contrast to some of the memes that you see like,
you know, for taking desk, for example, to start our transition over that topic, you know,
data scientists wanted, you know, 10 years of desk experience required, right? Well,
desk hasn't been around that long. Yeah. Well, I think I think this is a separate issue,
which is oftentimes the people writing job posts don't know what they actually want. And that
doesn't mean just like, oh, it's an HR person who's writing this job post. It could be a data
scientist who thinks that they have the ability, like, oh, I'm going to sit the bar so high because
our team needs people that are like, that's fun. You could say this job requires a PhD and
years experience and ask and all that and you will find no one. And then what are you going to do?
Well, you're going to drop it to a reasonable level and then you will find people, you know, like
the fact that those terrible job posts exist is not an indication that the field actually
requires that stuff, not only in an indication that a lot of people don't actually have realistic
expectations for what they should get from an employee. You mentioned that the second part of the
book talks about like why projects fail and what to do if projects are failing, you know,
what are the biggest questions you get from folks that are that are, you know, data scientists
and are at that stage of their career where they're trying to navigate the role.
So I will say on that failure thing in particular. So I give that as a talk, and I've done it a
couple of times at a couple of different places. And when I give the talk on failure, there are
some questions, but in general, what happens after the talk is I get lots of people come up to me
and they're like, oh, thank God, someone else feels the things I feel, which is to say that
especially as a more junior data scientist, when your stuff doesn't work, right? If someone says,
hey, go build a churn model on that customer data and you try and build it and you just can't
get it to work because there's just not enough signal there. It's very easy as a data scientist
to be like, ah, I knew more skills. If I were a better data scientist, I could have made that churn
model actually fit. When in practice, probably that data just has no actual signal in it. So no matter
how experienced of a data scientist, you are, that model won't fit. And I think that that's kind of
a problem for lots of junior data scientists in lots of different ways, which is you have expectations
that the amount of knowledge and skill you have is an indication of how well you will do. But
things like the data isn't actually have a signal in it or your stakeholders don't actually know
what they want this product to do. They're just asking you to build models because they think that's
a good idea. And then when you build them, they're like, wait, what do I do with this? Like,
none of that stuff has to do with your technical abilities and knowing how to like draw that like
emotional boundary and be like, okay, this project failed, but it's not because of me or maybe it
is like related to some stuff that I had to do. But like, what are the things I can take us notes
for the next time I do this project as opposed to I should have known that stuff in the first place
even though this is only my ninth month on the job. That stuff's really important and I don't think
it's talked about very much in data science. You know, I imagine when you first came into the field,
you know, we were certainly a lot closer to this time where, you know, we the data scientist was
also, you know, almost, you know, often preceded by mythical like the unicorn data scientist like the
you know, the someone who knows math, knows stats, can program, can deploy, can do all these things.
And over the past several years, as a field's gotten more mature, there's a lot more
specificity in roles and also roles of kind of bifurcated into now you've got folks that are,
you know, machine learning engineers that are focused on deployment. I'm wondering how,
you know, the evolution of the role has changed or, you know, would change the advice you give to
someone who is, you know, joining the field or early on in their career. Do you, is that something
that kind of impacts the way you you talk to folks and advise folks? Yeah, kind of. I do think,
yeah, I think when I started, there were no, no one had any expectation of data scientists because
they didn't really barely existed. And then I agree they got to the point where your point of,
now every data scientist should do everything. And now we've gotten smarter about that. I think
what I would tell a people entering the field is it is helpful, I think, to have a basic idea of
which of these things you'd rather do. Would you rather build models that are continuously running
that like get hit like APIs and help customers like a recommendation model? Or would you rather be
using, would you rather be using data to help a business make a decision? And like you're,
what you're delivering is a PowerPoint, but it's a PowerPoint filled with interesting ideas that
you found out from data. Or do you want to do just analytics, not just, but do you want to do
analytics, which is like, hey, I'm going to take data from a database, put it in an easy to use
dashboard, get it to you so it's easily digestible, but like just making data easy to use.
Like think about what are the kinds of things that sound the most interesting to you?
That would be the first thing I'd say. And then the second thing I would say is,
but don't feel like you are locked into that and don't feel like you have to, you have to like,
make a full decision before you start as a data scientist, which is to say like one, you may think,
oh, I really want to do machine learning, but then you'd happen to find a job that's your first
jobs, the more decision science, I'm going to make decisions based on this data. You might like
that. And you don't know until you really try these jobs, which ones you like and won't like.
So don't feel like if it's not the one you signed up for, you will not enjoy it.
And then second, it is entirely possible to change. I spent the first 10 years of my career
doing decision science stuff, especially as a consultant, which is give me your data. I will
give you insights out of it. And then in the last couple of years, I've switched to machine
learning, which is, hey, I'm going to build models that continuously run as Docker containers,
blah, blah, blah. And like, was that switch easy? Not really. I mean, it took work, but like,
I could do it. I think other people can do it. And I think we kind of have this weird gatekeeping
as a field of like, oh, you're just a decision scientist. You can't do this stuff. You never will.
And it's like, no, anyone can do any of those stuff. It's just a matter of getting the experience
to learn how to do it. Yeah, yeah. So on the topic of machine learning and Docker containers and
all that jazz, you know, what is, maybe let's start with Dask. What is Dask? Yeah. So, um,
so Dask is basically, here's how I think about Dask, right? You have you're a Python user. You
write your Python code. It's running on your local machine. And you're like, man, I really wish
this Python code was running on a distributed set of machines really easily, right? So Dask is
that tool, which is to say, if you have an operation that could really easily be switched to running
on a bunch of computers at once, Dask can help you do that. Like one instance is, let's say you have
a for loop. And in that for loop, it has a big computation. And it goes up for loop a thousand times.
Why not have 10 computers taking each parts of those for loops and doing them all at once?
Dask will help you do that. Dask also has a lot of built-in libraries and stuff. So you don't even
have to think about how do you distribute this stuff out. You could just do something where it feels
like you're doing normal pandas, but secretly on the back end, the calculations are being farmed
out to this cluster of stuff. So Dask is an open source tool. It's right now. It's kind of like
the alternative you'd use as Spark. But Spark is like, it's written in Scala. It's on the JVM.
It's a hassle set up. And like, Dask is really like, hey, you got Python code, take your Python
code, run it on a bunch of things at once. Nice and easy. Don't have to worry about it. And so Dask is
an open source tool that's been developed since like 2018. And Saturn Cloud, the company I'm at now,
one of the things we do is we make hosted Dask easy use. So if you are doing stuff on your laptop
and you're like, I wish I want to, I wish this was running on 100 computers in the cloud,
you can like in like three clicks have Saturn Cloud be the place where all those computers live.
And it's a really cool tool. I think it's very neat. And I think I am surprised more tools like
this don't exist in other languages and stuff like it just seems such like an intuitive thing
that it should exist. And so I'm just very happy that Dask exists.
And so when you talk about the for loop example, it echoes both stuff that was happening in
high performance computing where you have to be very aware of the program and whether it's
data parallel versus computation parallel, things like that. And on the other side,
infrastructure type things like Kubernetes and distributed computing and things like that.
It sounds like Dask is maybe a little bit of both. Yeah. And okay. So
people who have ever heard my podcast and I think I'm very strongly opinionated about things.
I think that as a data scientist, you should not have to worry about your Kubernetes cluster.
And like, oh no, is that port open on that one work? Like there's a level of abstraction.
You as a data scientist should not have to worry about. You should just be able to use these tools.
I also think that there is for some whatever reason when people talk about the Shabie computing,
it is one of those topics where people love to make it unnecessarily complicated.
Right? Like, ah, but is it a blah blah blah type or a blah blah type for you? Yeah.
And it's like, look, all I do, I have this, I have, it's a for loop. I would like each one of
those for loops to happen all at the same time. Just like that shouldn't be that complicated.
And I think Dask is one of those tools that makes it less complicated to do that.
As opposed to sometimes people would be like, you know, there are some tools that make it more
complicated. I think another great example. I would like for their whole phase with Hadoop where you
had to take that for loop, turn it into a map and a reduce and all that stuff. Yeah.
In order to distribute it. Yeah. And like, this is really a place where it'd be easy to do that
abstraction. And Hadoop did, ah, here's a confession I have, which is when I was a junior data scientist
in like 2012 or whatever, Hadoop was becoming really big. And it was like, oh, you're a
tech scientist, but you work with big data. And I was always like, no, I always work with like 200,000
rows and like linear aggressions and, you know, stuff. And like, I felt really ashamed. I'm like,
should I go learn to do? Should I go learn to do? And thankfully, I never learned to do. And
now I don't need to because people don't really think about that. We have tools like Dask and
Spark that handle that layer of abstraction, which is say, if you're a data scientist and you feel
like, oh, no, I don't use this tool yet. But I feel like I should be using this tool to stay
ahead of the, you know, stay on the curve. You never know that tool might go away by the time.
And it would be simple enough that you don't need that. Another place where I think this happens
a lot is with, I think neural networks for the longest time where like, oh, do you have,
is it a blog layer of this kind of abstraction? And like, like, here's a really complicated,
crazy diagram. And here's eight equations to talk about like neural networks. And like,
you see this, right? You see these like easy to use neural network cheat sheets on LinkedIn that
are just like pages of math equations. And that's crazy. Like a talk I give is literally just,
hey, you can learn how to use do neural networks and are in like 20 PowerPoint slides. And
it's going to be PowerPoint slides filled with pictures of pets. And we're going to be training
a neural network to generate pet names. Like that's a talk I give. And people are like, wow,
I didn't know neural networks are this easy. And I say this so passionately because I did for
the longest time didn't feel like I can learn neural networks because I thought they were too
complicated. And then once I learned them, I'm like, are you kidding me? This is nothing. I could
learn this easily. Other people could too. And I think desk and distributed computing is a thing
that more people could do if we made this easier to understand as opposed to like, look how smart I
am for understanding this complicated thing. Yeah. Yeah. Yeah. So what's the what's the user experience
for a desk user? Are they is it totally transparent? Is it transparent depending on what you're trying to
do? Do you have to, you know, annotate or decorate your functions or like what does the data
scientists need to do to take advantage of desk? So there are a couple different ways you can use it.
So the way I personally like to use it is it is just a decorator on your functions, which is like
you say, hey, does it delayed function? Don't run it right away. And then you create like a desk,
you start your desk cluster, which may be inside our cloud, may just be on your PC, whatever,
like you start a desk cluster. And then you say, hey, use that cluster to map that function or,
you know, basically run those functions on this list in that cluster. Or, you know, there's
other simple operations. Another way you can use desk is like, desk has kind of wrappers. So
there is like a instead of using like PD for pandas, you use like DD for desk and then you use
the same pandas commands. But what that's doing is it for you is doing that figuring out how to make
it into a list and sending that all to the clusters or whatever, like it's doing all that work of
turning it into the distributed code. And so like there's the pandas version of that. There's,
you know, like I think scikit learn, like there's a lot of different packages and stuff that have,
okay, if you want to do desk for your distributed backend, you can do that too. And those are kind
of the different ways you can run it. And I will say you can, like I said, you can on your laptop
using desk and then call like send it over to the satire cloud servers. The one complexity of that
is like if you're using Python 3.6 and that desk cluster is all using Python 3.8 and stuff like
you can run into a little bit of issues. So what we do at satire cloud is you can also we host
Jupiter. So you can be using a Jupiter that has like guarantees that that Jupiter will be consistent
with that desk cluster. Just as like another potential way to use this tool, which is to say there's
a lot of different ways you can try and run this stuff. And you know, a satire cloud, a real
design philosophy is like we don't want to lock you into one way of doing things. You know, I feel
like some like data science tools are like, okay, like you want to use get sure, but you can only push
and you can only commit using this one button. And if you want to do anything complicated, like
branching or all these things you're used to, you won't let you because we're locking you down.
And I don't think those tools usually succeed. Like usually you want your tools to meet data scientists
where they are. On that topic, I guess maybe a couple of ways to go into this conversation,
like where do you think data scientists are with regard to the, you know, the kind of the software
development tool chain? Like that's been something that's been evolving over the past few years.
There was a point in time which Git was a foreign entity to a lot of data scientists. Now it's
becoming more common for data scientists to use Git. And I often see it varies from organization
to organization, you know, some organizations will want all of their data scientists to use Git
and IDE's others are, you know, hey, we're going to figure out how to make the Jupiter notebook
into the Uber IDE and, you know, production allows notebooks and things like that. Any, any observations
on that? Yeah, I would say I've seen a remarkable like compared to like six years ago or
remarkable like gathering around Git. I think six, six, eight years ago, you could be like, we're a
data science team, but all of our code shared in a share point, you know, and like that would,
I've seen it like you laugh, but I've seen it. It might have been a network drive, but it was like
the data wasn't sure. It was not like a good time. I think like the message is now out that like if
you're a data science team and you're not using Git and version control on your stuff that that's
like you, you have a problem. I think that message has successfully gone out. I also do think in
the last couple of years it's becoming more common for data science teams to use things like GitHub
actions, you know, we're docker. And you know, like I think I think data scientists are like like
we're doing a good job of making this stuff more accessible so you don't have to be a software
engineer to use software engineer tools. And I think that's really great. But that said, I also
don't feel like if you don't know those tools, that means you can't be a data scientist merely that
like usually like when you're on your first job, you will pick up a lot of these things. Like maybe
get you should probably learn Git before you like hunt your first job, but beyond that things like
GitHub actions, docker, all that stuff like you don't need to know that when you're applying for
jobs, a job should be teaching you how to use those tools if they need them. Yeah, yeah. And it is
super important to realize that learning Git means some basic things. And it's there's so much to
there's so much to like really, really, really learning Git. But I think a lot of us know enough
Git that we can do the usual things. And if we get into trouble, then it's stack overflow with
everyone else and trying to figure out how to rebase your branches and crazy stuff. Yeah, I've
been using Git. God, I learned it like 2012, like 10 years. And yeah, I feel like this year,
year 10, I'm like, okay, I kind of feel like I get like, yeah, rebasing and stuff. Like I feel
like I kind of like getting more. So like if you've been doing this for less than 10 years,
don't feel like you should know that. And if you've been doing it for 10 years, like,
still probably fine. Yeah, yeah. And you mentioned docker and containers,
Kubernetes, those of all come up in this conversation. What's the relationship between those and
desk? So, um, well, with desk, you can run desk like in a Kubernetes cluster. Like there are ways
to like have that all work. Um, we at sat and clown kind of managed that all for you. So basically,
you just specify, I want this AWS image size. And I want this base Docker image. And then it says,
cool, you know, I have a desk cluster of that. Um, as, but if you don't know docker, it doesn't
really matter that you don't, you don't need to know docker to know, I need this particular base
image. But if you're like, I want to specify exactly what's on these desk clusters when they start.
Like that is a thing with sat and cloud, you'd specify through like a Docker container or,
you know, a post build file. And I think there is, I do think data scientist, regardless of type
of data scientists, it is a good thing to learn, like a little bit of Linux commands. Like what does
pseudo apt get mean? What does, you know, blah, blah, blah, because you start to learn that, then you
can pretty easily make Docker files and Docker containers. And, you know, do like kind of speak
the common language of a lot of these tools. Um, so I would recommend learning that kind of stuff.
But again, I don't think that's something you need to learn. Um, before you get a job that,
that really, I would consider a skill you can learn while you are, you know, growing in your career.
And so does this, is Saturn based on Kubernetes and all that stuff? Or is it using its own
Pixie dust under like under the covers? I am not on the engineering team. So I, I don't want to say,
because I know I'm going to get it slightly wrong, but I believe we do have a Kubernetes cluster.
And like, well, what we usually do is we, you know, because a lot of times we are being hired by
enterprise to like run, like an enterprise doesn't want to have their data pass all the way to some
other persons, AWS, you know, whatever. So we install Saturn cloud in your company's environment.
And I think we set up one Kubernetes cluster within that like BPC or whatever.
But I'm just spouting words. I don't know exactly the intricacies of this. I'm much more on the
using Dask than setting it upside. Got it. Yeah. And we, I think we, we skip right by that.
What is your specific role at the company? What are you focused on?
Oh, yeah. So as head of data science, I help the data science team. So we do things like,
you know, helping customers get up to speed with Dask and Saturn cloud.
We also, you know, develop white papers, blog posts, things like that.
So like we're kind of like the internal team that uses this tool and helps other people use this tool.
And in addition to that, it's a lot of product development too, because this is a tool for data scientists.
So if we're using this product and we're like, oh, we find this part of it, you know,
complex or unintuitive, then we are the ones to kind of say, hey, we should be prioritizing this kind
of solution. Which is again, it's nice because I think compared to a lot of technical products for
data scientists are still built by software engineers. And so you kind of get that feel of like,
are they? I'm not sure the people designing this understand what data scientists do every day.
And I do not feel that that was the case with our product. I feel like, you know, well,
I like to think I'm doing my job, which is making that easy. But, you know, who's, who's to say,
really? Is there a particular use case or a set of use cases that you see more commonly
among desk users? Are there, are there, you know, are there specific
signals or indicators that say, hey, this is probably going to be a good place to apply to ask?
Yeah. So I think that, I mean, it's kind of the same as like any parallel distributed computing,
which is if you're running out of memory, because one machine isn't enough,
then desk is a good solution because it can, it could spin up to arbitrary amounts of memory,
right? Because everything is split over a bunch of computers. So if you're running out of memory,
that's a good sign you need to ask. Or if you have one, like one thing that's running,
but it's taking forever. And it, you know, you're like, man, I could probably chop this up into
smaller tasks. Then desk is great, because you can, if you can swap, chop it up into smaller tasks,
you can then put it in a desk cluster. And including like most common pandas commands and stuff,
pandas, you can switch pandas to desk and then use these sorts of tools. So that stuff's a
pretty good sign. Also, I will say, you know, with Saturn Cloud, we have it all set up to use GPUs
just as easily. Like with no setup, it's just you hit go in the GPU CUDA drivers or whatever
already correctly installed and stuff. So, you know, if you're finding, you're like, hey,
I need a train 10 neural networks and compare how well they do. Well, like wouldn't be nice to train
all those 10 at the same time at one desk cluster. Or, you know, you can, in fact, in PyTorch and
stuff say, hey, I have this one neural network and this neural network is too big for one. Even a
computer with several GPUs is not enough GPUs for this, this neural network to train. And so you can
get a desk cluster where each computer has a bunch of GPUs and like you connect all two. I don't
think that's a common use case. But like, you know what, if you're there, if you're like, I just
do not have GPUs running at the same time to do this kind of work. And I think desk is a really
nice solution for a lot of those. And is desk equal useful for folks that are doing kind of
traditional tabular data and analytics types of workloads as opposed to neural nets,
media files, that kind of thing, images. Yeah, I think it's more one or the other.
No, you can do it. If you're not doing like, you know, networking stuff, that's still totally
useful. It's, it's, you know, there is a penalty. If you're switching stuff to like distributed
parallel thinking, there's a penalty if it's going to take you a little bit of time to like rewrite,
you know, rethink the code of like, hey, this one for loop. Okay, yes, I can split it or whatever.
You know, can I spend this for like, you got to put a little thought into it. And I think, you
know, if you're training on like a 50 kilobyte file, then like, I don't think you're going to get
much benefit out of the effort of switching to desk. But I do think if there's a lot of situations
where it's like, I have a huge data set. And yeah, what I'm doing is training a linear regression.
You know, I have a huge data set, but what I'm doing is simple. Even in that case, because the data
is so big, it is still faster or more convenient to just have it go on desk, rather than trying to
get it to take forever on one machine. Is it used equally in the data processing
transformation part of the workflow or exploratory analysis part of the workflow,
training part of the workflow? Is there any particular sweet spot or? Yeah, so so a couple good
a couple good places training models can be very good on desk because you can train a bunch of
models at the same time or you can train one model across a bunch of them. And we have customers
who do that at Saturn cloud. Another thing that's really useful for is, yeah, like a scheduled daily
processing job, right? Where if every day you're taking your entire company's data and you're trying
to calculate some aggregated statistics and then put that into a new table, you can schedule that
to run every day and have it scheduled to run on a desk cluster and then it will be faster and you
won't run out of memory and things like that. And then at Saturn cloud, we have customers who
don't use desk at all. They just like the ability to just start and stop data science instances
and start and stop GPU instances and start and stop instances that have like 500 gigs of RAM.
So just having for us, just having the ability to have one computer easily accessible in the cloud
has been useful. But that is all just to say that there's a lot of utility in having these
sorts of data science resources readily available for data scientists.
Cool. Any thoughts or thoughts on where the tool is going and what folks should look out for?
So I think it's just one of these things where it only came out in 2018. So it's still quite young,
which is just to say I think there's going to be a lot more taking other packages and
getting them to run within desk directly. I think, you know, continuously making the tool easier
to use, creating new documentation, things like that. And you know, so Saturn cloud uses desk,
but we're not the only company that does. And so there's lots of people contributing to this,
which is kind of cool. I've never worked at a company where we were so directly involved in the
open source community. And that's been, I know it's been a lot of fun. I've been really happy with that.
Mm-hmm. Nice. Nice. Well, Jacqueline, thanks so much for taking the time to chat with us and
bring us up to speed on desk. No problem. No, it's been a lot of fun being back on the show.
Yeah. And now at what point do we become friend of the show, Jacqueline Nollis? Is it like one more?
You are friend of the show, Jacqueline Nollis. Absolutely.
Great. Well, yes, thank you for having me. Thank you.
