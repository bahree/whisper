1
00:00:00,000 --> 00:00:15,520
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:22,320 --> 00:00:28,000
Alright everyone, I am here with Ben Green. Ben is a PhD candidate in Applied Math at Harvard

3
00:00:28,000 --> 00:00:32,640
and Affiliate at the Berkman Client Center for Internet and Society, also at Harvard,

4
00:00:32,640 --> 00:00:38,800
and a research fellow at the AINow Institute at NYU. Ben is joining me for our continued

5
00:00:38,800 --> 00:00:43,760
conversations coming out of the 33rd NURBS conference here in Vancouver. Ben, welcome to the

6
00:00:43,760 --> 00:00:47,440
Tumel AI Podcast. Thanks so much for having me. I'm excited for our conversation.

7
00:00:48,240 --> 00:00:54,640
I am as well. So, as I just said, your degrees are going to be very soon in Applied Math,

8
00:00:54,640 --> 00:01:02,240
but you have applied that application of math to largely an exploration of the intersection of

9
00:01:03,440 --> 00:01:09,120
technology, AI, and social good. Tell us a little bit more about your background.

10
00:01:09,120 --> 00:01:15,520
Yeah, so my background is primarily computer science and data science, but I always,

11
00:01:15,520 --> 00:01:20,240
even going back to my undergraduate years, had a really strong interest in urban policy

12
00:01:20,240 --> 00:01:26,560
and urban government and urban planning. And so, as I started out the PhD, a lot of my emphasis

13
00:01:26,560 --> 00:01:31,600
was on how can we use, you know, the tools of artificial intelligence and data science to

14
00:01:31,600 --> 00:01:38,240
improve society and participated in the UChicago data science for social good program and did some

15
00:01:38,240 --> 00:01:43,760
other work really thinking about how can I as a data scientist contribute to society? I spent

16
00:01:43,760 --> 00:01:49,920
a year working for the city of Boston as a data scientist in the middle of my PhD. But in the

17
00:01:49,920 --> 00:01:54,960
course of doing that work from originally from a more technical perspective, increasingly also

18
00:01:54,960 --> 00:02:03,040
came to see the broader governance political social questions that were at the heart of these

19
00:02:03,040 --> 00:02:10,720
technological endeavors and often were overlooked or ignored and also played a significant role in

20
00:02:11,280 --> 00:02:16,400
shaping the impacts of these systems in some of the projects that I worked on, whether it was

21
00:02:16,400 --> 00:02:21,440
building machine learning algorithms in the city of Boston or for the city of Memphis to help them

22
00:02:22,240 --> 00:02:29,120
prioritize various types of inspections and investments. Often what I found was that the key

23
00:02:29,680 --> 00:02:34,400
factor that shaped the impacts was not the technology itself, but the broader policy,

24
00:02:34,400 --> 00:02:39,760
government, political environment in which that technology was being embedded. And so that shaped

25
00:02:39,760 --> 00:02:47,040
a lot of my thinking on how to integrate technology into these broader social contexts and how to

26
00:02:47,040 --> 00:02:54,400
think about the ways in which what are very well-intentioned efforts to use technology for good can

27
00:02:54,400 --> 00:03:02,160
overlook some key factors and end up failing to achieve those those social goals. And your affiliation

28
00:03:02,160 --> 00:03:06,800
with the applied math department is that I imagine there are several places that you could have

29
00:03:06,800 --> 00:03:11,840
plugged in your research interests at a place like Harvard. Is that the selection of a particular

30
00:03:11,840 --> 00:03:19,600
advisor or is applied math? How does applied math fit into that broader research agenda?

31
00:03:19,600 --> 00:03:25,680
Yeah, so definitely the really the computer science perspective which is sort of where my

32
00:03:25,680 --> 00:03:32,640
more more day-to-day home is at Harvard is both very much a lot of my work is sort of with is all

33
00:03:32,640 --> 00:03:37,760
sort of within the realm of computer science both thinking about those tools in the past more

34
00:03:37,760 --> 00:03:45,280
work on building those building AI systems for various social applications, running different

35
00:03:45,280 --> 00:03:50,160
types of more recently. I've been running a variety of human computer interaction audits to

36
00:03:50,160 --> 00:03:57,200
understand how people interact with algorithms in practice. And but also my work is very much

37
00:03:57,200 --> 00:04:02,560
than about stepping outside of the typical modes of thinking within the field bringing in other

38
00:04:02,560 --> 00:04:08,400
perspectives from science and technology studies and philosophy and government and thinking about

39
00:04:08,400 --> 00:04:14,560
what and law and what those perspectives can do to inform our understanding of artificial

40
00:04:14,560 --> 00:04:21,360
intelligence and its impacts and how to develop it. So my work is definitely very multi-disciplinary

41
00:04:21,360 --> 00:04:25,680
and I've worked with you know even within Harvard at many different departments and with many

42
00:04:25,680 --> 00:04:31,360
different people but the core focus has always been on the application of data and algorithms

43
00:04:31,360 --> 00:04:37,520
in society. So here at NERPS you're presenting a paper called Good Isn't Good Enough

44
00:04:37,520 --> 00:04:43,760
at the AI for Social Good Workshop. Tell us a little bit about the paper and what your objectives

45
00:04:43,760 --> 00:04:50,400
are there. Yeah so this paper is so it's just a it's a short workshop paper so definitely not

46
00:04:50,400 --> 00:04:56,400
a fully in-depth discussion of these topics but it really emerged out of my own experiences

47
00:04:56,400 --> 00:05:01,680
and some of the other broader examples I was seeing of these efforts to do social good

48
00:05:02,320 --> 00:05:07,680
that were well intentioned but often sort of not thinking of the full of the full picture of

49
00:05:07,680 --> 00:05:14,800
what it actually means to do good. And recognizing in particular that efforts to do any sort of

50
00:05:14,800 --> 00:05:22,320
technology for social good are about somehow shaping society somehow changing society for the better

51
00:05:22,320 --> 00:05:28,000
and that's of course an incredibly complex topic and the two things that I really point out

52
00:05:28,000 --> 00:05:34,640
that I have seen missing in the majority of efforts to use AI for social good is first what I

53
00:05:34,640 --> 00:05:40,480
would think of as a normative theory a sort of grounded definition of what good actually means

54
00:05:40,480 --> 00:05:47,520
typically most groups will talk about AI for social good and the social good part is sort of

55
00:05:47,520 --> 00:05:52,880
taken for granted what that might mean but of course as you can sort of if you step out of the AI

56
00:05:52,880 --> 00:05:57,840
space and just think about our broader social political world there are many different definitions

57
00:05:57,840 --> 00:06:03,280
of what's good and many nuances within that type of debate so there was often a lack of sort of

58
00:06:03,280 --> 00:06:09,920
a normative discussion about what are we trying to accomplish and then the second part was a lack of

59
00:06:09,920 --> 00:06:16,080
what I would call a theory of change a theory or sort of a grounded justification for how the

60
00:06:16,080 --> 00:06:22,880
particular technological approach being taken is an effective means to getting to the social good

61
00:06:22,880 --> 00:06:31,040
and whatever that end may be and so a lot of the time even in cases where perhaps the

62
00:06:31,680 --> 00:06:37,360
an important problem is recognize the particular mode in which technologists go about trying to

63
00:06:37,360 --> 00:06:44,000
solve that problem may not be the most effective way of achieving that end if you take the social good

64
00:06:44,000 --> 00:06:50,000
or the social impact as the as the ultimate goal here and think about the technology as a means

65
00:06:50,000 --> 00:06:57,360
to achieving that goal and so both of those things are I think pretty significant challenges

66
00:06:57,360 --> 00:07:02,480
certainly one not ones that cannot be overcome but the types of things that really need to be

67
00:07:02,480 --> 00:07:08,560
incorporated into these into these discussions I like to think about it and in some sense this is

68
00:07:08,560 --> 00:07:12,880
really what the goal of the paper is to do is I like to think about it in terms of rigor

69
00:07:12,880 --> 00:07:19,040
right that when we talk about AI for social good what we're actually doing is really expanding

70
00:07:19,040 --> 00:07:23,760
what we're trying to accomplish with an AI system right we're not simply saying we want to build

71
00:07:23,760 --> 00:07:30,160
a tool that can efficiently predict this or you know efficiently analyze this data but we're

72
00:07:30,160 --> 00:07:37,360
trying to build a tool so that it can achieve or advance this social outcome and what I'm trying

73
00:07:37,360 --> 00:07:43,760
to bring as a sense of here are things that we're overlooking failing in many cases to think about

74
00:07:44,480 --> 00:07:49,520
and trying to frame that as a lack of rigor in these efforts that we're actually not thinking

75
00:07:49,520 --> 00:07:56,480
about factors that are incredibly important in shaping those outcomes and that to the same extent

76
00:07:56,480 --> 00:08:04,400
that we would never accept a system that hadn't done an analysis on some sort of hold out test

77
00:08:04,400 --> 00:08:11,680
dataset we also shouldn't be accepting systems for integration into societal contacts that also

78
00:08:11,680 --> 00:08:16,640
hasn't done some sort of analysis of well what will the impacts of this system be in practice how

79
00:08:16,640 --> 00:08:23,040
is it actually going to affect the system that we're trying to impact here and bringing in more of

80
00:08:23,040 --> 00:08:30,160
those types of sociotechnical analyses into what it means to build and evaluate these types of

81
00:08:30,160 --> 00:08:36,480
systems. The way you've framed those two components establishing kind of a normative definition

82
00:08:36,480 --> 00:08:46,880
for good and theoretical framework for understanding the implications of a particular technology

83
00:08:46,880 --> 00:08:56,960
sound like ideals. Are you holding these up as hurdles that should be overcome in order to

84
00:08:58,400 --> 00:09:04,560
before folks embark on their AI for social good projects or more like conversations that we

85
00:09:04,560 --> 00:09:11,600
need to be having? Yeah, I think it's a bit of both. I wouldn't view those as being in

86
00:09:11,600 --> 00:09:17,440
contrast with one and other. It's definitely I would say centrally about conversations. There's

87
00:09:17,440 --> 00:09:24,000
certainly not a single answer to any of these things both within the field and without the field

88
00:09:25,280 --> 00:09:30,240
philosophers and activists have been debating questions about what is good and how do you get there

89
00:09:30,240 --> 00:09:37,600
for forever essentially? Are there examples from other fields where good or even

90
00:09:37,600 --> 00:09:43,600
something you know as broad as good has been successfully defined and used as a foundation for

91
00:09:43,600 --> 00:09:50,080
kind of further work? What would this be? Ground that we're breaking in in AI? So it's definitely not

92
00:09:50,080 --> 00:09:55,920
ground that's being broken in AI. I would say again the point is not that there are other fields.

93
00:09:55,920 --> 00:10:01,680
It's less about there are other fields that have come up with the definition of good, but there are

94
00:10:01,680 --> 00:10:08,640
other fields that have figured out how to incorporate these sorts of conversations into what it means

95
00:10:08,640 --> 00:10:14,320
to do this practice. So one of the places that I've been looking and this is a paper that I just

96
00:10:14,320 --> 00:10:20,480
finish and will be published at the 2020 Fat Star Conference Fairness Accountability and Transparency,

97
00:10:21,120 --> 00:10:27,120
my co-author and I we look to the law and we look to debates in the law in the 20th century

98
00:10:27,120 --> 00:10:33,040
about how they moved from a system that was very formalistic that said the law has these rules.

99
00:10:33,040 --> 00:10:38,240
We can just deduce from these rules how do we get to the right outcome? And there were a number

100
00:10:38,240 --> 00:10:43,920
of challenges to the law a shift from what was known as legal formalism to what was known as

101
00:10:43,920 --> 00:10:51,200
legal realism that said legal practitioners whether lawyers or judges or scholars need to figure out

102
00:10:51,200 --> 00:10:56,400
how to reason about the law in terms of the impacts that it's actually having in real people's

103
00:10:56,400 --> 00:11:02,080
lives. It's not just about did you apply the principle of liberty correctly, but how will

104
00:11:02,800 --> 00:11:08,000
this application of the law affect people's liberty in their real lives? That's what the

105
00:11:08,000 --> 00:11:14,640
realism is doing here. And so this paper is about shifting from what we might call algorithmic

106
00:11:14,640 --> 00:11:20,720
formalism to a mode of algorithmic realism, recognizing that the law is a place while typically

107
00:11:20,720 --> 00:11:26,560
viewed as being in conflict with technology. And there certainly are conflicts in many ways. The

108
00:11:26,560 --> 00:11:33,760
law is another mechanism for having a structure to a mechanism to structure our decision making

109
00:11:33,760 --> 00:11:39,600
and to structure the rules by which we allocate decisions and resources. And in many ways that's

110
00:11:39,600 --> 00:11:46,480
what AI want applied to city governments or healthcare systems or any number of different

111
00:11:46,480 --> 00:11:52,960
contexts are doing right. It's providing a way to distribute to make decisions to manage

112
00:11:52,960 --> 00:11:57,920
discretion. And so I think that by taking for example some of those lessons of the law which

113
00:11:57,920 --> 00:12:02,640
have always been thinking about how do we understand or not always but have for many years been

114
00:12:02,640 --> 00:12:12,080
thinking about how do we emphasize the the quality of illegal analysis not in terms of it's sort of

115
00:12:12,080 --> 00:12:17,040
whether it not it fits the ideal theory or the principles within ideal theory but how does

116
00:12:17,040 --> 00:12:23,440
it affect society. And I think in many ways that's where we should be moving to in these contexts

117
00:12:23,440 --> 00:12:28,880
of building AI for society right. The goal is not to just build these systems for these systems

118
00:12:28,880 --> 00:12:34,800
own sake but because we're trying to advance some sort of social benefit or social outcome.

119
00:12:34,800 --> 00:12:42,880
And so is the paper is it prescriptive in the sense that it tells a practitioner who buys into

120
00:12:42,880 --> 00:12:48,720
this vision what to do next? Yeah absolutely I mean we're certainly not prescriptive as saying

121
00:12:48,720 --> 00:12:55,280
you know here's all of the answers it's a broad a broad shift but yeah so we we provide and I

122
00:12:55,280 --> 00:13:00,480
think you know and these sort of things that orient we call them orientations that we're putting

123
00:13:00,480 --> 00:13:05,600
forward are very much also aligned with what I recommend at the end of this AI for social good paper

124
00:13:05,600 --> 00:13:12,560
here in the rips but you know one is really thinking about what are those normative commitments

125
00:13:12,560 --> 00:13:18,320
that you want to embody within your application what might you be taking for granted as things that

126
00:13:18,320 --> 00:13:26,720
are good or not good if you're not interrogating those things. How do you bring in a more interdisciplinary

127
00:13:26,720 --> 00:13:32,880
approach recognizing the variety of different perspectives that might come to bear on that

128
00:13:32,880 --> 00:13:40,480
system or the ways in which the interactions outside of what we would think of as the algorithm

129
00:13:40,480 --> 00:13:45,840
itself can affect the impacts of that system so expanding beyond what are the technical

130
00:13:45,840 --> 00:13:51,200
specifications in terms of accuracy and efficiency but what happens when you put this into the middle

131
00:13:51,200 --> 00:13:55,840
of a healthcare system and you need doctors to be working with it in practice. How does you know

132
00:13:55,840 --> 00:14:02,560
what what happens there and that's just as central and then really engaging with the context of

133
00:14:02,560 --> 00:14:09,680
the problem and ultimately from what what we call an approach of agnosticism right recognizing

134
00:14:09,680 --> 00:14:16,720
that the AI can be built in many different ways and that the AI is just one way of approaching

135
00:14:16,720 --> 00:14:23,200
a social problem that if the goal ultimately is not about building the system the goal is about

136
00:14:23,200 --> 00:14:30,080
advancing some social outcome that requires a different sort of stance of understanding okay well

137
00:14:30,960 --> 00:14:36,240
what type of system should I build what is the best contribution that I as an engineer can make

138
00:14:36,240 --> 00:14:42,960
into this problem and how might that be different from building the sophisticated system that

139
00:14:42,960 --> 00:14:49,040
sounds really cool from an engineering perspective but may not actually be the best data analytic

140
00:14:49,040 --> 00:14:56,480
tool for pushing the ball forward on this problem so you know there's a variety of questions

141
00:14:56,480 --> 00:15:00,880
and prompts and sort of things like that to start start thinking about and then there are

142
00:15:00,880 --> 00:15:05,760
you know getting at what that looks like in practice at every single one of those decision points

143
00:15:05,760 --> 00:15:11,040
is certainly an area for further work and one that other people also are thinking about a lot.

144
00:15:11,040 --> 00:15:18,640
A big part of your work is kind of thinking about and providing a framework for thinking about

145
00:15:18,640 --> 00:15:24,400
kind of unintended consequences and the application of technology to social good

146
00:15:25,280 --> 00:15:30,880
are there you know specific examples of these that are most salient for you.

147
00:15:32,320 --> 00:15:38,480
Yeah so you know I think there are a couple I would say the one that I always come back to because

148
00:15:38,480 --> 00:15:44,960
this is the focus of my dissertation work are algorithms in the criminal justice system things

149
00:15:44,960 --> 00:15:53,440
like predictive policing and risk assessments and these are systems that often are deployed

150
00:15:53,440 --> 00:15:57,920
under the idea that these are effective ways of achieving some form some type of criminal

151
00:15:57,920 --> 00:16:03,600
justice reform right of taking this system that is broken in a number of ways and helping to

152
00:16:03,600 --> 00:16:13,120
improve it and I think that that's an example of where thinking about what the typical sort of

153
00:16:13,120 --> 00:16:18,400
we can build an algorithm for this problem approach where that can go wrong because I think

154
00:16:18,400 --> 00:16:23,760
that ultimately what we're doing with these systems is not just improving a particular

155
00:16:24,720 --> 00:16:29,840
aspect of the system with these algorithms but actually affecting the broader landscape of what

156
00:16:29,840 --> 00:16:36,000
it means to do reform what sorts of shifts that we're taking. So the example that comes to mind

157
00:16:36,000 --> 00:16:44,080
I'm sure for many who follow this base is like the compass system that was written about

158
00:16:44,080 --> 00:16:48,960
extensively in a public overport maybe a couple of years ago now you're in 2016 anything.

159
00:16:48,960 --> 00:16:55,120
Oh, 2016, it's been a while. Yeah but that's a system that is being you know developed and

160
00:16:55,120 --> 00:17:05,280
promoted by a you know commercial entity and it's not at all clear that their goals are you know

161
00:17:05,280 --> 00:17:10,000
reform or social good for that matter you know is that the example that comes to mind for you

162
00:17:10,000 --> 00:17:16,400
or are there others and you know certainly there are folks that are you know whose primary goal is

163
00:17:16,400 --> 00:17:23,600
you know social good you know that is not necessarily one of them. Yeah I mean it definitely

164
00:17:23,600 --> 00:17:28,720
varies the the landscape is sort of complex and they're definitely just companies trying to

165
00:17:28,720 --> 00:17:34,960
make money out of this and then I think also definitely computer scientists and other folks who

166
00:17:34,960 --> 00:17:42,720
are genuinely excited about these tools as as a way of improving the system. You know so two two

167
00:17:42,720 --> 00:17:46,720
different very different types of unintended consequences that play out there that I have looked

168
00:17:46,720 --> 00:17:52,080
at in my research. One being again going back to this human computer interaction component

169
00:17:52,080 --> 00:17:57,360
where a lot of my work and other folks have done similar work is looking at okay well you have

170
00:17:57,360 --> 00:18:02,080
this system that seems like it's able to make these decisions what happens when you actually put

171
00:18:02,080 --> 00:18:09,520
it into the context are people using these predictions in the way that you would expect and what

172
00:18:09,520 --> 00:18:16,720
what I and others have found is that people can in can inject their own different types of biases

173
00:18:16,720 --> 00:18:23,120
in terms of how they respond to these systems or they might ignore the recommendations or only

174
00:18:23,120 --> 00:18:29,920
use the recommendations in an unexpected and sort of particular way that has you know that

175
00:18:29,920 --> 00:18:34,960
benefits one group versus another so that's a particular type of other specific examples of that

176
00:18:34,960 --> 00:18:41,600
that come to mind. Yeah so so in my work I've looked at how this is lay people not judges but how

177
00:18:41,600 --> 00:18:47,040
lay people can be more likely to be influenced by higher recommendations of risk from an algorithm

178
00:18:47,040 --> 00:18:51,440
when that defendant is black compared to when that defendant is white so they might be more

179
00:18:51,440 --> 00:18:56,880
susceptible to using it in certain directions in certain cases so they'll have a higher they'll

180
00:18:56,880 --> 00:19:01,200
be pulled in a higher risk prediction for black defendants in a lower risk prediction for white

181
00:19:01,200 --> 00:19:08,480
defendants and then others like me being the the groups that you studied or the research that

182
00:19:08,480 --> 00:19:15,120
you're referring to demonstrates that that folks are more willing to defer to the authority of

183
00:19:15,120 --> 00:19:19,840
an algorithm that they don't understand in the case of a black defendant than a white defendant

184
00:19:19,840 --> 00:19:26,480
is that the essence of it? Yeah so it's that they they differ in different ways depending on

185
00:19:26,480 --> 00:19:33,040
the defendants right so if essentially if the algorithm is telling you to increase your score

186
00:19:33,040 --> 00:19:37,440
you're more likely to follow that recommendation for black if the defendant is black and if the

187
00:19:37,440 --> 00:19:42,320
algorithm is telling you to decrease your score you're more likely to follow that if the defendant is

188
00:19:42,320 --> 00:19:50,640
white so what you know through the point and sort of the you know how this this is this clear

189
00:19:50,640 --> 00:19:56,800
example of how we might think about the system we analyze we can audit the system on its own terms

190
00:19:56,800 --> 00:20:03,360
a great deal but what we're actually doing is incorporating this AI system into a very complex

191
00:20:03,360 --> 00:20:10,720
social political government context and so how it actually gets used in that context may lead it to

192
00:20:10,720 --> 00:20:16,320
have very different impacts even just in terms of the decisions that are made then we might expect

193
00:20:16,320 --> 00:20:24,160
from doing a analysis of the algorithms recommendations on their own in terms of accuracy and false

194
00:20:24,160 --> 00:20:30,160
positives and those sorts of things so you know this is adding even just more dimensions to something

195
00:20:30,160 --> 00:20:36,000
like what the pro-publica article was pointing out in terms of disparate false positive predictions

196
00:20:36,000 --> 00:20:43,040
and saying e above and beyond those types of analyses we can also study how these algorithms will

197
00:20:43,040 --> 00:20:49,520
affect decision-making when you put them closer or in their in their real context also makes me think

198
00:20:49,520 --> 00:21:00,400
of the use of facial recognition technology by law enforcement groups Amazon has been central to

199
00:21:00,400 --> 00:21:06,160
many aspects of this conversation and when I talk to them they defer to you know documenting

200
00:21:06,160 --> 00:21:14,240
proper use of these systems you're suggesting that improper use of of these systems is in fact

201
00:21:14,240 --> 00:21:20,880
like systematic and so maybe it's not enough to just point to the documentation is that a conclusion

202
00:21:20,880 --> 00:21:25,760
that you're making or I would say yeah I think I think you know that's a useful framework for

203
00:21:25,760 --> 00:21:30,960
thinking about some of these systems and facial recognition is a good example here there are questions

204
00:21:30,960 --> 00:21:40,080
of both is the is the actual use proper right so do we get proper improper use in practice so that's

205
00:21:40,080 --> 00:21:46,880
one way of breakdown is that what the we get improper use in practice the other would be that

206
00:21:46,880 --> 00:21:53,040
proper use would itself be troubling right and I think that many of the systems I would I think

207
00:21:53,040 --> 00:21:58,720
there are a lot of systems facial recognition being one of them that proper use is itself troubling

208
00:21:58,720 --> 00:22:04,320
right so and similarly with with risk assessments I would make the same argument that there are

209
00:22:04,320 --> 00:22:09,360
definitely breakdowns that can happen in practice things like the types of bias human biases I was

210
00:22:09,360 --> 00:22:16,000
talking about a minute ago but then there's also the question of you know what happens if the system

211
00:22:16,000 --> 00:22:22,720
is perfect right so with facial recognition there's been great work showing that these systems

212
00:22:22,720 --> 00:22:30,240
are flawed and biased in a variety of ways but the answer there is not to say well we just need to

213
00:22:30,240 --> 00:22:35,440
have proper use and proper systems that are perfectly accurate right because the way that facial

214
00:22:35,440 --> 00:22:42,480
recognition systems are primarily being used is you know law enforcement and corporate surveillance

215
00:22:42,480 --> 00:22:50,320
right so having a system that just works better and is better able to document people is not necessarily

216
00:22:50,320 --> 00:22:56,800
the outcome that we actually would want and that's a place where the proper use is itself a problem

217
00:22:56,800 --> 00:23:04,320
and because why because their perfection and successful use promotes further surveillance and

218
00:23:04,320 --> 00:23:11,600
that and of itself is kind of taken as negative or because there are specific outcomes that are

219
00:23:12,960 --> 00:23:19,360
that you predict or see based on their their use yeah it's definitely that it's you know it's

220
00:23:19,360 --> 00:23:25,280
about thinking again not it's about taking the the system like facial recognition in the broader

221
00:23:25,280 --> 00:23:31,280
context and how it gets used right so if the primary use of these tools are you know think about

222
00:23:31,280 --> 00:23:38,160
law enforcement being able to have better records of where everyone is going in the city more

223
00:23:38,160 --> 00:23:46,880
ability to track people and that to me and to many others as an incredibly dangerous prospect the

224
00:23:46,880 --> 00:23:55,040
idea that you are fully tracked just by stepping foot into you know downtown Vancouver where we are

225
00:23:55,040 --> 00:24:02,080
right now right and of course that type of surveillance typically has the the most significant harms

226
00:24:02,080 --> 00:24:09,280
for minorities in the lower and lower classes and so the idea that we would just want to make those

227
00:24:09,280 --> 00:24:16,000
systems more accurate and more technically sound is only enhancing the ability for those broader

228
00:24:16,000 --> 00:24:22,480
types of systems to be developed so I think that thinking through I think it's really important

229
00:24:22,480 --> 00:24:29,040
for folks who are trying to think about the social consequences of AI to sort of think about it

230
00:24:29,040 --> 00:24:35,120
in this just in this way of you know where problems occurring because of improper use and where

231
00:24:35,120 --> 00:24:40,640
would proper use itself be bad and I think that's a really helpful heuristic for thinking through

232
00:24:40,640 --> 00:24:46,640
what types of challenges we want to make to these systems so you know in my in my work on risk

233
00:24:46,640 --> 00:24:52,880
assessments there are all sorts of improper use critiques to make or sort of improper engineering

234
00:24:52,880 --> 00:24:58,160
critiques to make about flaws in the training data or flaws in the human interactions but then

235
00:24:58,160 --> 00:25:05,040
there are also critiques we made of even a perfect system what itself be bad right and we need to

236
00:25:05,040 --> 00:25:09,920
we need to be able to think about that relationship between those different types of harms and

237
00:25:09,920 --> 00:25:15,920
different types of breakdowns and I think that doing that can inform a lot of both our understanding

238
00:25:15,920 --> 00:25:21,920
of these systems but also our work as computer scientists to understand these systems and actually

239
00:25:21,920 --> 00:25:29,600
understand what are the types of flaws that we should be actively working to fix and what are the

240
00:25:29,600 --> 00:25:35,600
types of flaws that can prompt us to do a larger process of reimagining what types of systems

241
00:25:35,600 --> 00:25:41,920
we're building reimagining what problems we're choosing to work on in the first place and so

242
00:25:41,920 --> 00:25:46,800
this is you know and this comes back to you know something I talk about that comes up a lot

243
00:25:46,800 --> 00:25:54,560
in my conversations I am often sort of advocating for a position of what I would call a political

244
00:25:54,560 --> 00:25:59,360
orientation among computer scientists about thinking about these types of questions that's

245
00:25:59,360 --> 00:26:05,680
obviously an uncomfortable thing as a scientist and an engineer who has been sort of taught

246
00:26:05,680 --> 00:26:12,880
that you are to work from an objective remove and a position of neutrality and I think one of the

247
00:26:12,880 --> 00:26:19,120
key things in sort of thinking about how to do that is to understand what we mean when we talk

248
00:26:19,120 --> 00:26:27,120
about objectivity and to think about how there are many types of decisions that sort of fly

249
00:26:27,120 --> 00:26:31,840
under the radar of what we would consider objective and that we can play with those right so when I

250
00:26:31,840 --> 00:26:39,840
say you know political the point is not to make up data or to make up results that would advance

251
00:26:39,840 --> 00:26:46,000
an empirical finding that you would like to show but to think about what types of problems are you

252
00:26:46,000 --> 00:26:51,280
choosing to work on in the first place who are you choosing as your domain experts who are the

253
00:26:51,280 --> 00:26:58,160
partners that you're trying to work with and build systems for those are all ultimately incredibly

254
00:26:58,880 --> 00:27:04,480
what I would call political questions right that choosing choosing a problem to work on

255
00:27:04,480 --> 00:27:11,040
is in many cases you know that's projecting a vision of a good society or a vision of where

256
00:27:11,040 --> 00:27:16,560
society has gone wrong and then you know building systems within that doesn't necessarily

257
00:27:16,560 --> 00:27:23,280
require rejecting the typical modes that we would use when building and evaluating AI systems

258
00:27:23,280 --> 00:27:29,280
but that's one way that we can think about stepping back from our typical process and recognizing

259
00:27:30,080 --> 00:27:34,320
the decisions that we're making often without realizing it about what we're going to work on

260
00:27:34,320 --> 00:27:40,880
and who we're going to work with but those can be incredibly consequential and simply choosing a

261
00:27:40,880 --> 00:27:47,040
slightly different problem or a slightly different partner can have really big consequences and

262
00:27:47,040 --> 00:27:51,920
can very much shape what the type of system you're building is or the types of impacts that you're

263
00:27:51,920 --> 00:28:01,440
going to have so we've maybe straight a little bit from kind of establishing a basis for what we

264
00:28:01,440 --> 00:28:10,000
mean by good or social good and a theory of the relationship between technology and social good

265
00:28:10,000 --> 00:28:15,520
I think we've kind of spoke to talk a little bit about the first part of that all of this is

266
00:28:15,520 --> 00:28:22,480
definitely very central to exactly those those questions right yeah right right but the you know

267
00:28:22,480 --> 00:28:29,440
the latter of those two points kind of a theory for you know the relationship between technology

268
00:28:29,440 --> 00:28:35,840
and social impact you know what does that what does that look like yeah yeah you know in some

269
00:28:35,840 --> 00:28:41,600
ways it's like what we've been talking about like what you know you do you know you do x that has

270
00:28:41,600 --> 00:28:46,640
implications a and b and kind of just thinking through that but when you talk about it as kind of

271
00:28:46,640 --> 00:28:52,640
a theory and you you may have said formalism or I'm like kind of implying that like you know how

272
00:28:52,640 --> 00:28:59,440
and you said rigor like how how rigorous can we get yeah yeah yeah um yeah so that you know I

273
00:28:59,440 --> 00:29:05,040
I like to talk about it as in terms of a theory of change right and this is something that definitely

274
00:29:05,040 --> 00:29:12,080
comes out of more I guess political activist circles right that you're thinking about okay how do we

275
00:29:13,600 --> 00:29:22,480
tie our actions now to the outcomes that we want to get to in the medium or long term right how do

276
00:29:22,480 --> 00:29:29,120
we think of any particular intervention and making the impacts that we want to make and that's

277
00:29:29,120 --> 00:29:37,280
you know that's an incredibly hard question but one that many different areas of uh you know people

278
00:29:37,280 --> 00:29:43,200
in government political activists and social scientists and others have been thinking about a lot

279
00:29:43,200 --> 00:29:50,240
and in sort of a broader sense and then there's also a great deal of work in STS and critical

280
00:29:50,240 --> 00:29:56,480
algorithm studies and philosophy of science and other fields that are looking at how our

281
00:29:56,480 --> 00:30:03,840
technologies that are built and put into an applied social context actually affecting the world

282
00:30:04,640 --> 00:30:12,960
and I think that by starting to pull all of that together we can start to have some some better

283
00:30:12,960 --> 00:30:20,560
ways of thinking about you know okay if I'm trying to achieve you know x goal how can I

284
00:30:20,560 --> 00:30:27,120
think about what type of system I can build to achieve that goal or how can I think about the ways

285
00:30:27,120 --> 00:30:33,680
that the system that I built that seems to advance that goal could end up sort of splintering

286
00:30:33,680 --> 00:30:39,680
off in any different number of ways and a lot of you know great work in law for example has been

287
00:30:39,680 --> 00:30:48,640
looking at how laws meant to protect against discrimination or protect civil rights can end up

288
00:30:48,640 --> 00:30:56,160
getting wielded and used in unexpected ways that actually have impacts totally counter to what

289
00:30:56,160 --> 00:31:02,640
the developers uh or the creators of that law were intending so you know it's an incredibly

290
00:31:03,840 --> 00:31:10,240
it's a complex area but I think that starting to you know look to some of those areas uh you know

291
00:31:10,240 --> 00:31:17,280
we can look at for example okay how have particular types of anti-discrimination laws and efforts

292
00:31:17,280 --> 00:31:23,520
been effective and been ineffective and how can we uh I think a lot of the work on fairness for

293
00:31:23,520 --> 00:31:33,120
example today uh mirrors a lot of the typical modes of anti-discrimination work in the past

294
00:31:33,120 --> 00:31:39,840
that have not necessarily been as effective as they were intended to be and I've written some

295
00:31:39,840 --> 00:31:44,640
about that and Anna Lauren Hoffman at the University of Washington has a great paper on that

296
00:31:44,640 --> 00:31:49,920
so that's an example of how we can look to some of these other fields and other areas and historical

297
00:31:49,920 --> 00:31:57,920
contexts uh both for coral areas of how certain types of efforts have uh gone wrong or been

298
00:31:57,920 --> 00:32:06,000
effective and also the lessons learned in terms of what it means to uh advance a particular

299
00:32:06,000 --> 00:32:13,440
social project in a way that is is robust but but then to get to get more granular here uh you know

300
00:32:13,440 --> 00:32:17,840
I think a couple of the things that that computer scientists can do you know I think that thinking

301
00:32:17,840 --> 00:32:26,080
about the socio-technical context is super important here uh one of the ways that these unintended

302
00:32:26,080 --> 00:32:31,280
impacts go wrong as we've been discussing has been you know you put it into context and it doesn't

303
00:32:31,280 --> 00:32:40,000
actually get used in uh the ways you might expect and I think that part of this is if we can if

304
00:32:40,000 --> 00:32:47,680
we think about the work that we're doing from this perspective of uh impact and theories of change

305
00:32:47,680 --> 00:32:53,120
those types of things can shift from unintended impacts to things that we've thought about ahead of

306
00:32:53,120 --> 00:33:02,560
time so taking those you know human interaction audit studies and making that an integral part

307
00:33:02,560 --> 00:33:10,640
of building a system that's going to be used in an applied context or thinking about what is the

308
00:33:10,640 --> 00:33:16,640
what is the policy domain that I'm working in and how have other efforts in the past and today

309
00:33:16,640 --> 00:33:22,480
been successful at trying to challenge some of the issues that I'm challenging or been effective

310
00:33:22,480 --> 00:33:29,920
at moving the ball in a certain direction so there are I think sort of there's this larger

311
00:33:29,920 --> 00:33:35,440
conversation to be had and a lot of that is then you know talking to domain experts and talking

312
00:33:35,440 --> 00:33:40,800
to people outside the field but then also I think there are just ways of having a more socio-technical

313
00:33:40,800 --> 00:33:47,440
orientation that can take what today would be called unintended consequences and bringing them

314
00:33:47,440 --> 00:33:54,240
into the design process earlier in terms of okay how do we prepare to test if that consequence

315
00:33:54,240 --> 00:34:00,640
is going to happen and how do we make that central to what it means to evaluate this system rigorously

316
00:34:00,640 --> 00:34:06,480
to bring that back into the conversation a lot of what I'm hearing here is going back to

317
00:34:07,600 --> 00:34:14,640
as practitioners we should be thinking more about these issues you know I wonder you know for

318
00:34:14,640 --> 00:34:19,760
practitioners you know do we know how to think about these issues and do we need more

319
00:34:19,760 --> 00:34:25,600
or yeah obviously we should be thinking more about these issues but do we need more you know structure

320
00:34:25,600 --> 00:34:31,280
frameworks rigor you know do we all have to go back and get degrees in STS or other things in order

321
00:34:31,280 --> 00:34:40,080
to kind of reason through these issues it seems like at some point you know it's it's very

322
00:34:40,080 --> 00:34:47,200
reductionist but like you know we need a checklist or you know some kind of map or some kind of

323
00:34:47,200 --> 00:34:54,160
you know or at least you know a list of you know required reading or something like that to kind

324
00:34:54,160 --> 00:34:58,560
of narrow the scope yeah do you agree with that or do you think that you know we're just being

325
00:34:58,560 --> 00:35:03,840
lazy and we should just think no no no you know the you know and I think one thing that's really

326
00:35:03,840 --> 00:35:11,760
important to to to say on that last point is like the goal of this is not to say you computer

327
00:35:11,760 --> 00:35:19,760
scientists are evil or malevolent is right but to say that the at its foundations this is not

328
00:35:19,760 --> 00:35:25,760
the type of thing that the field is trying to think about right so you know even going back to

329
00:35:25,760 --> 00:35:32,240
the types of things that would be taught in an algorithms 101 class or like your canonical AI

330
00:35:32,240 --> 00:35:36,800
or other CS textbooks and these sorts of things don't come up so I think it really does come back

331
00:35:36,800 --> 00:35:44,000
to this question of education and training and sort of the broader culture and norms within the

332
00:35:44,000 --> 00:35:51,040
field so these are you know these are certainly things that are hard really I would say long-term

333
00:35:51,040 --> 00:35:56,960
shifts for the field to be to be building but I think that starting to think about how to

334
00:35:56,960 --> 00:36:04,000
incorporate these other other disciplines into CS education that that or not just incorporating

335
00:36:04,000 --> 00:36:08,320
but also expanding the boundaries of what it means to study computer science right that studying

336
00:36:08,320 --> 00:36:14,880
computer science means not just studying you know algorithms and their efficiency and what's

337
00:36:14,880 --> 00:36:21,680
an MP hard problem but also studying STS to understand how algorithms are affecting society

338
00:36:21,680 --> 00:36:28,320
and studying at least some form of you know the policy or social domain where you're maybe

339
00:36:28,320 --> 00:36:32,480
interested in applying right like if you're interested in the criminal justice system taking

340
00:36:32,480 --> 00:36:40,000
classes in those areas or finding ways to have rich aspects of those fields or courses brought

341
00:36:40,000 --> 00:36:47,680
into a computer science training world so you know and there are a lot of efforts to to do that

342
00:36:47,680 --> 00:36:54,560
there is you know efforts to incorporate ethics training into courses there's an effort at Harvard

343
00:36:54,560 --> 00:36:58,720
to do that led by Barbara Gross there are efforts at a lot of different universities now there

344
00:36:58,720 --> 00:37:05,040
these sort of AI ethics or variants on that type of class and so and that's this you know that's

345
00:37:05,040 --> 00:37:10,480
sort of the right start to be moving on is how do we expand the training that you're receiving

346
00:37:10,480 --> 00:37:16,480
as a computer scientist but also in maybe a broader sense how do we change what it means to be a

347
00:37:16,480 --> 00:37:23,520
computer scientist through that training so that these types of questions are not external to your

348
00:37:23,520 --> 00:37:29,520
professional responsibility or to the types of things that might come up in peer review if you're

349
00:37:29,520 --> 00:37:35,680
a researcher but are the among the fundamental questions that are going to be missed so I think

350
00:37:36,240 --> 00:37:41,600
there's you know a lot of work to be done at both the scholarship level thinking about what is this

351
00:37:41,600 --> 00:37:46,800
even look like and then at the education level thinking about how do we take these broader lessons

352
00:37:46,800 --> 00:37:54,000
and incorporate that into my undergraduate and graduate curricula for what it means to study

353
00:37:54,000 --> 00:37:58,080
machine learning in a different way than we would have studied machine learning even five years ago

354
00:37:58,080 --> 00:38:02,480
right and also referring back to an earlier part of the conversation implications for

355
00:38:03,360 --> 00:38:09,920
those people that are activists or trying to affect change within not just education but

356
00:38:09,920 --> 00:38:18,640
but practice a way to kind of constructively look back at you know at social change across

357
00:38:18,640 --> 00:38:24,400
you know other fields within and out and without technology and identify you know what's working

358
00:38:24,400 --> 00:38:31,440
what's not working and use that to rethink the way you know we engage in conversations around

359
00:38:31,440 --> 00:38:37,760
AI for social good mm-hmm yeah and you know again like none of this is to say that we can't

360
00:38:37,760 --> 00:38:43,040
you know that that the types of tools or the types of methods that we have that are you know

361
00:38:43,040 --> 00:38:47,760
what I would call them more sort of algorithmic formalist methods are bad or should be completely

362
00:38:47,760 --> 00:38:53,360
thrown away or that every single person needs to do every single one of these things but I think

363
00:38:53,360 --> 00:39:00,080
particularly as the field becomes more oriented around questions of social good and social impact

364
00:39:00,080 --> 00:39:07,600
and thinking about these applied domains that only becomes more and more central so you know it's

365
00:39:07,600 --> 00:39:15,120
one thing to think about building if you're a systems architect and not thinking about the social

366
00:39:15,120 --> 00:39:19,120
impact I mean there are always social impacts and always ways to be thinking about that every

367
00:39:19,120 --> 00:39:24,640
about ethics and architecture that everyone should be having but I think particularly where this

368
00:39:24,640 --> 00:39:32,480
really comes to the fore is in the shift in the field towards applied work towards social good

369
00:39:32,480 --> 00:39:39,920
which is both an incredibly exciting move for the field to be making but also a move that

370
00:39:39,920 --> 00:39:46,960
requires you know a sense of humility and responsibility for what that means and a recognition

371
00:39:46,960 --> 00:39:54,080
that that means incorporating new types of knowledge production and expertise and opening up

372
00:39:54,080 --> 00:40:00,080
the bounds of what expertise even means or who is an expert yeah into the field also now also

373
00:40:00,080 --> 00:40:04,560
well Ben thanks so much for sharing a bit of what you're up to yeah thank you so much I really

374
00:40:04,560 --> 00:40:12,480
enjoyed this thank you same all right everyone that's our show for today to learn more about today's

375
00:40:12,480 --> 00:40:18,880
guest or the topics mentioned in this interview visit twimmel ai.com of course if you like what you

376
00:40:18,880 --> 00:40:25,360
hear on the podcast please subscribe rate and review the show on your favorite pod catcher thanks

377
00:40:25,360 --> 00:40:32,800
so much for listening and catch you next time

