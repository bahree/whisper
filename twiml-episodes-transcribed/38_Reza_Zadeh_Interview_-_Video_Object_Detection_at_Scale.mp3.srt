1
00:00:00,000 --> 00:00:16,000
Hello and welcome to another episode of Swimultalk, the podcast where I interview interesting

2
00:00:16,000 --> 00:00:20,920
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,920 --> 00:00:23,440
I'm your host Sam Charrington.

4
00:00:23,440 --> 00:00:29,320
The show you're about to hear is part four of our five part O'Reilly AI New York series,

5
00:00:29,320 --> 00:00:31,800
sponsored by Intel Nirvana.

6
00:00:31,800 --> 00:00:36,880
As I've mentioned before, I am super grateful to Intel for helping make this series possible,

7
00:00:36,880 --> 00:00:41,480
and I'm excited about the cool stuff they launched at the O'Reilly AI conference, including

8
00:00:41,480 --> 00:00:46,520
version 2.0 of their neon framework and their new Nirvana Graph project.

9
00:00:46,520 --> 00:00:50,960
Be sure to check them out at intonirvana.com if you haven't already listened to the first

10
00:00:50,960 --> 00:00:57,040
show in this series, where I interview Naveen Rao, who leads Intel's AI products group

11
00:00:57,040 --> 00:01:00,600
and Hanlon Tang and algorithms engineer on that team.

12
00:01:00,600 --> 00:01:04,680
It's Swimultalk number 31, and you definitely want to start there.

13
00:01:04,680 --> 00:01:07,840
My guess for this show is Reza Zade.

14
00:01:07,840 --> 00:01:13,360
Reza is an adjunct professor of computational mathematics at Stanford University and founder

15
00:01:13,360 --> 00:01:16,760
and CEO of the startup Matroid.

16
00:01:16,760 --> 00:01:21,280
Our conversation focused on some of the challenges and approaches to scaling deep learning, both

17
00:01:21,280 --> 00:01:26,400
in general and in the context of his company's video object detection service.

18
00:01:26,400 --> 00:01:28,560
All right, on to the show.

19
00:01:28,560 --> 00:01:41,720
All right, hey everyone, I am here with Reza Zade from Stanford University and Matroid.

20
00:01:41,720 --> 00:01:48,800
Reza gave a presentation earlier today at the O'Reilly AI conference, and I'm excited

21
00:01:48,800 --> 00:01:50,320
to catch up with him here.

22
00:01:50,320 --> 00:01:51,320
Hi Reza.

23
00:01:51,320 --> 00:01:52,880
Hey Sam, thanks for having me.

24
00:01:52,880 --> 00:01:54,640
It's great to be here in New York.

25
00:01:54,640 --> 00:02:00,400
I appreciate the time you're taking to talk about machine learning AI and the stuff

26
00:02:00,400 --> 00:02:01,400
I love.

27
00:02:01,400 --> 00:02:02,880
Absolutely, I love it too.

28
00:02:02,880 --> 00:02:07,760
And I've been having a bunch of fun talking to folks over the past couple of days.

29
00:02:07,760 --> 00:02:11,000
Why don't we start with a little bit of introduction?

30
00:02:11,000 --> 00:02:14,920
Tell us a little bit about your background and how you ended up doing what you're doing

31
00:02:14,920 --> 00:02:15,920
in AI.

32
00:02:15,920 --> 00:02:16,920
Sure thing.

33
00:02:16,920 --> 00:02:22,320
So I've been working on machine learning for around 12 years now, since I was 18, started

34
00:02:22,320 --> 00:02:28,960
at Google working on machine translation, language modeling, and word alignments for machine

35
00:02:28,960 --> 00:02:29,960
translation.

36
00:02:29,960 --> 00:02:36,080
Back when it wasn't neural in any way, more traditional phrase-based models, transitioned

37
00:02:36,080 --> 00:02:42,200
into distributed machine learning as a tool, so how do you take, say, 100 machines in

38
00:02:42,200 --> 00:02:47,840
training machine learning model that is either scaled across large model, large data, or

39
00:02:47,840 --> 00:02:48,840
many models?

40
00:02:48,840 --> 00:02:49,840
Okay.

41
00:02:49,840 --> 00:02:53,720
And that manifested itself in the machine learning library in Apache Spark, so that's

42
00:02:53,720 --> 00:02:57,200
one of the projects that I've worked on heavily.

43
00:02:57,200 --> 00:03:02,040
More recently, the Twitter who to follow suggestions, the recommendation algorithms on

44
00:03:02,040 --> 00:03:03,040
there.

45
00:03:03,040 --> 00:03:05,880
So if you go on to twitter.com, there are a lot of things on there.

46
00:03:05,880 --> 00:03:09,280
And one of the things you'll notice is the recommendation pane, it says, who do you

47
00:03:09,280 --> 00:03:10,520
want to follow?

48
00:03:10,520 --> 00:03:15,400
And the algorithm is behind that, we're actually part of a chapter of my PhD thesis.

49
00:03:15,400 --> 00:03:21,880
And then after graduating from Stanford, I got hired by my department to do a distributed

50
00:03:21,880 --> 00:03:27,680
machine learning class, as well as a more data science oriented class with graph theory

51
00:03:27,680 --> 00:03:28,680
in it.

52
00:03:28,680 --> 00:03:29,680
Okay.

53
00:03:29,680 --> 00:03:34,680
And while an adjunct professor at Stanford, I started matroid.

54
00:03:34,680 --> 00:03:39,000
The computer vision has been completely taken over with machine learning.

55
00:03:39,000 --> 00:03:41,720
And I've been working on machine learning my whole career.

56
00:03:41,720 --> 00:03:45,880
So ever since I've had a job, it's been about machine learning.

57
00:03:45,880 --> 00:03:51,000
And because the computer vision is so overtaken with it now, it was the perfect time for

58
00:03:51,000 --> 00:03:55,560
me to get into it, especially since computer vision also went through a revolution.

59
00:03:55,560 --> 00:03:58,400
Many problems that weren't possible before became possible.

60
00:03:58,400 --> 00:04:00,560
It seemed like the perfect opportunity.

61
00:04:00,560 --> 00:04:05,200
And so now, most of my time goes into matroid, the computer vision company, and that was

62
00:04:05,200 --> 00:04:06,200
a talk today.

63
00:04:06,200 --> 00:04:07,200
I gave a talk about matroid.

64
00:04:07,200 --> 00:04:10,040
We recently released matroid.com as their website.

65
00:04:10,040 --> 00:04:15,640
And I'm currently thinking hard about deep learning and computer vision on a day-to-day

66
00:04:15,640 --> 00:04:16,640
basis.

67
00:04:16,640 --> 00:04:17,640
Okay.

68
00:04:17,640 --> 00:04:23,400
Your talks span both the distributed machine learning, scaling machine learning, and matroid

69
00:04:23,400 --> 00:04:26,600
there, kind of in a role conversations, is that right?

70
00:04:26,600 --> 00:04:27,600
That's right.

71
00:04:27,600 --> 00:04:31,600
And matroid is a studio for creating and using detectors.

72
00:04:31,600 --> 00:04:37,040
Now, these detectors are a very heavy duty in terms of computational cost.

73
00:04:37,040 --> 00:04:44,280
And to be able to scale such a system to many detectors and many users and many streams,

74
00:04:44,280 --> 00:04:51,440
we have to build an entire cluster of commodity machines with fancy hardware GPUs in particular,

75
00:04:51,440 --> 00:04:53,240
and scale them up and down dynamically.

76
00:04:53,240 --> 00:04:58,920
So as more people use our system, we need to scale up, scale down so that we're not wasting

77
00:04:58,920 --> 00:05:00,560
resources.

78
00:05:00,560 --> 00:05:05,480
And the way we do that is, essentially, all of it is new.

79
00:05:05,480 --> 00:05:10,120
And we are particularly good at making sure it's also fault tolerant so that if, so just

80
00:05:10,120 --> 00:05:14,640
a little background, as a service, we provide the ability to monitor a stream of media.

81
00:05:14,640 --> 00:05:19,360
So let's say you're watching a TV channel and you want to see when there's a Coca-Cola

82
00:05:19,360 --> 00:05:22,360
logo show up on this TV channel.

83
00:05:22,360 --> 00:05:25,480
And then you can, of course, multiply that by all the TV channels in the world, which

84
00:05:25,480 --> 00:05:28,600
is my very hard to do for an intern in a production studio.

85
00:05:28,600 --> 00:05:32,480
But that's the kind of thing that's possible now when we provide as a service.

86
00:05:32,480 --> 00:05:36,920
That, the fact that there are thousands of streams that could come at us and some of them

87
00:05:36,920 --> 00:05:38,920
could go down and they could come up.

88
00:05:38,920 --> 00:05:39,920
And that could be our fault.

89
00:05:39,920 --> 00:05:40,920
We have to deal with that.

90
00:05:40,920 --> 00:05:45,320
It has to be computational ability to run these models.

91
00:05:45,320 --> 00:05:48,080
There has to be RAM to run these models.

92
00:05:48,080 --> 00:05:54,160
These are, these models have to fit into smaller GPUs.

93
00:05:54,160 --> 00:05:59,800
These GPUs have small amounts of RAM, not smaller GPUs, GPUs with small amounts of RAM.

94
00:05:59,800 --> 00:06:03,760
These are all challenges that have to be solved at the same time to be able to provide a service

95
00:06:03,760 --> 00:06:05,360
that we provide.

96
00:06:05,360 --> 00:06:07,680
On top of that, there's a UI components.

97
00:06:07,680 --> 00:06:13,840
We have our own video player, the video player lets you have a poor man's reinforcement

98
00:06:13,840 --> 00:06:15,320
learning happening.

99
00:06:15,320 --> 00:06:19,480
And all of that tightly integrated provided as a service is matroid.

100
00:06:19,480 --> 00:06:22,480
I don't know how much you want to go into matroid itself as a product.

101
00:06:22,480 --> 00:06:27,480
I'm happy to talk about either the distributed machine learning aspect of more general

102
00:06:27,480 --> 00:06:30,720
distributed machine learning or the distributed machine learning aspect of matroid or machine

103
00:06:30,720 --> 00:06:33,800
learning in general up to you where we go with that.

104
00:06:33,800 --> 00:06:38,160
So one immediate question that I had was, and you describing what matroid is doing, it

105
00:06:38,160 --> 00:06:44,360
made me think of Google's recent, in fact, I think just today they put this, their video

106
00:06:44,360 --> 00:06:47,680
object detection offering into beta.

107
00:06:47,680 --> 00:06:52,960
Are you doing similar things or tell me a little bit about matroid in the context of

108
00:06:52,960 --> 00:06:53,960
that?

109
00:06:53,960 --> 00:06:58,160
So the Google offering is primarily focused towards developers.

110
00:06:58,160 --> 00:07:03,480
So the Google customer or the Microsoft or the other customers, there would be people

111
00:07:03,480 --> 00:07:10,640
who can code, people who are already somewhat familiar with machine learning.

112
00:07:10,640 --> 00:07:11,640
Right.

113
00:07:11,640 --> 00:07:12,640
They're providing APIs.

114
00:07:12,640 --> 00:07:16,040
And the reason the Google sells this is because they want people to come to Google Cloud

115
00:07:16,040 --> 00:07:18,440
Platform and spread and compute.

116
00:07:18,440 --> 00:07:20,720
That is something that is totally not our customer.

117
00:07:20,720 --> 00:07:24,080
Our customer is someone who has a need to watch media.

118
00:07:24,080 --> 00:07:29,200
And that happens where it's the production internet agency or a brand monitoring company.

119
00:07:29,200 --> 00:07:30,200
That's right.

120
00:07:30,200 --> 00:07:31,200
That's right.

121
00:07:31,200 --> 00:07:32,200
So these people who can't code.

122
00:07:32,200 --> 00:07:33,200
Right.

123
00:07:33,200 --> 00:07:34,200
And there's a very different question.

124
00:07:34,200 --> 00:07:38,480
So you're offering a solution as opposed to a set of APIs and yes, that's right.

125
00:07:38,480 --> 00:07:39,480
Okay.

126
00:07:39,480 --> 00:07:42,440
Although there is overlap in the technologies used for sure, we're using convolutional

127
00:07:42,440 --> 00:07:43,440
neural networks.

128
00:07:43,440 --> 00:07:46,240
For sure, we're both scaling machine learning in some way.

129
00:07:46,240 --> 00:07:48,240
But the customer is actually very different.

130
00:07:48,240 --> 00:07:50,240
The UI is very different.

131
00:07:50,240 --> 00:07:51,760
And that's the differentiation.

132
00:07:51,760 --> 00:07:56,640
We don't directly sell to developers, although we have an API that developers can use

133
00:07:56,640 --> 00:07:58,400
and if they're sophisticated enough.

134
00:07:58,400 --> 00:08:01,320
But we're essentially like a Photoshop for computer vision.

135
00:08:01,320 --> 00:08:06,400
You just have to be determined at clicking and pointing.

136
00:08:06,400 --> 00:08:09,720
You don't need to learn programming to use this.

137
00:08:09,720 --> 00:08:13,600
And then the other thing you said that was interesting was poor man's reinforcement

138
00:08:13,600 --> 00:08:14,600
learning.

139
00:08:14,600 --> 00:08:15,600
Yes.

140
00:08:15,600 --> 00:08:17,280
Tell me about that analogy and what that means in your world.

141
00:08:17,280 --> 00:08:18,280
Sure thing.

142
00:08:18,280 --> 00:08:22,480
So we have this whole detector creation flow within matroid.

143
00:08:22,480 --> 00:08:26,000
So if you go to matroid.com, you can create a detector for whatever you want.

144
00:08:26,000 --> 00:08:29,680
And in fact, during the talk today, I made some examples of whatever I want.

145
00:08:29,680 --> 00:08:30,680
Right.

146
00:08:30,680 --> 00:08:33,280
So in the talk today, I made a detector for cars.

147
00:08:33,280 --> 00:08:34,280
Okay.

148
00:08:34,280 --> 00:08:35,280
Like a live demo.

149
00:08:35,280 --> 00:08:36,280
Yeah.

150
00:08:36,280 --> 00:08:37,280
Absolutely.

151
00:08:37,280 --> 00:08:42,280
And most I never I use like five slides and the rest of the 40 minutes was in on matroid.com.

152
00:08:42,280 --> 00:08:43,280
Okay.

153
00:08:43,280 --> 00:08:47,080
So it was almost entirely a live demo that you can do if you make an account on there yourself

154
00:08:47,080 --> 00:08:49,800
now and do everything I did.

155
00:08:49,800 --> 00:08:51,680
There's a flow there to create a detector.

156
00:08:51,680 --> 00:08:56,360
And this detector once it's once it's created, it takes three or four minutes to create.

157
00:08:56,360 --> 00:09:01,280
Once it's created, you can immediately see how well it's performing by running videos

158
00:09:01,280 --> 00:09:02,960
through the video player.

159
00:09:02,960 --> 00:09:06,880
And immediately see, okay, with the video player, you see, okay, it's working on these areas

160
00:09:06,880 --> 00:09:07,880
of the video.

161
00:09:07,880 --> 00:09:10,120
It's not working on these areas of the video.

162
00:09:10,120 --> 00:09:14,480
And because we have our own video player, you go in and you can tag where that, where

163
00:09:14,480 --> 00:09:21,520
those mistakes happen, where you want to reinforce correct attitude and where you want to guess,

164
00:09:21,520 --> 00:09:24,760
give negative examples and say, no, this is incorrect.

165
00:09:24,760 --> 00:09:27,400
You can do that very quickly with our with our video player.

166
00:09:27,400 --> 00:09:32,240
And then that goes back into reinforcing correct attitude in the model.

167
00:09:32,240 --> 00:09:35,560
So then the model can be retrained then and now you have a better model.

168
00:09:35,560 --> 00:09:39,800
That's why I said it's a poor man's reinforcement learning because it's not traditional reinforcement

169
00:09:39,800 --> 00:09:45,240
learning where we have the whole loop right, taking care of, you have to provide us with

170
00:09:45,240 --> 00:09:47,520
examples through the video player.

171
00:09:47,520 --> 00:09:54,160
But it's exactly the same spirit of reinforcement learning that an agent is essentially exploring.

172
00:09:54,160 --> 00:09:57,880
So you create a model, you have some, some policy you think is right.

173
00:09:57,880 --> 00:10:01,640
You run a video through, you observe how the detector is working.

174
00:10:01,640 --> 00:10:03,040
If it's working well enough, you're done.

175
00:10:03,040 --> 00:10:04,960
You go to town, you use that detector.

176
00:10:04,960 --> 00:10:10,040
If it's not working well enough, you can you can iterate on it and reinforce correct behavior.

177
00:10:10,040 --> 00:10:14,720
And the UI is essentially what allows a human to become the reinforcement learner.

178
00:10:14,720 --> 00:10:21,400
And the fact that the training and the training for computer vision, strictly for computer vision,

179
00:10:21,400 --> 00:10:22,720
not for general reinforcement learning.

180
00:10:22,720 --> 00:10:24,640
We are not a deep learning as a service company.

181
00:10:24,640 --> 00:10:27,400
We are not a machine learning as a service company.

182
00:10:27,400 --> 00:10:28,400
Right.

183
00:10:28,400 --> 00:10:29,400
Right.

184
00:10:29,400 --> 00:10:30,920
And is it primarily video or also still images?

185
00:10:30,920 --> 00:10:32,440
Of course, it's also still images.

186
00:10:32,440 --> 00:10:33,440
Okay.

187
00:10:33,440 --> 00:10:38,760
There's something that is in terms of the use cases and like where most of our customers

188
00:10:38,760 --> 00:10:42,720
care about video, because if you remember that value proposition, the value proposition

189
00:10:42,720 --> 00:10:46,400
is you're hiring someone to look at vast amounts of media, right?

190
00:10:46,400 --> 00:10:48,120
You're hiring someone to look at vast amounts of media.

191
00:10:48,120 --> 00:10:51,200
Sure that vast amount of media could be a very large image collection.

192
00:10:51,200 --> 00:10:52,200
Yeah.

193
00:10:52,200 --> 00:10:56,400
But more often than not, you would hire someone to watch very long videos, because then

194
00:10:56,400 --> 00:10:59,160
you would have a need for it to hire a person.

195
00:10:59,160 --> 00:11:02,040
If you have a hundred pictures, chances are you still wouldn't hire a person.

196
00:11:02,040 --> 00:11:05,560
If you have a thousand pictures, chances are you still wouldn't hire a person.

197
00:11:05,560 --> 00:11:10,160
If you have millions of images and photos, yeah, sure, you would hire a person at that

198
00:11:10,160 --> 00:11:11,160
point.

199
00:11:11,160 --> 00:11:14,320
But there are way fewer of those cases than there are people who have asked amounts of

200
00:11:14,320 --> 00:11:15,320
video.

201
00:11:15,320 --> 00:11:16,320
Right.

202
00:11:16,320 --> 00:11:18,440
It's very easy to have a camera on 24 seven.

203
00:11:18,440 --> 00:11:22,400
It's very easy to need to watch TV 24 seven.

204
00:11:22,400 --> 00:11:27,120
There are multiple streams of video ever going amounts of video streams in the world now

205
00:11:27,120 --> 00:11:28,840
with because cameras are cheap.

206
00:11:28,840 --> 00:11:33,560
And we expect them all to have the ability to have eyes on them with with matroid or other

207
00:11:33,560 --> 00:11:35,360
services.

208
00:11:35,360 --> 00:11:39,720
So we talked a little bit about branding use cases and envisioning surveillance style

209
00:11:39,720 --> 00:11:41,480
use cases as well.

210
00:11:41,480 --> 00:11:43,680
What are the kind of the major clusters of use?

211
00:11:43,680 --> 00:11:46,760
So we have we have two big industries that were focused on it.

212
00:11:46,760 --> 00:11:50,280
One is TV and the other is a security.

213
00:11:50,280 --> 00:11:51,280
Okay.

214
00:11:51,280 --> 00:11:52,280
So those are the two.

215
00:11:52,280 --> 00:11:53,280
That's it.

216
00:11:53,280 --> 00:11:56,880
As a startup, we can't be too sure broad, right?

217
00:11:56,880 --> 00:12:01,000
And so yes, you can actually just take your home camera and integrate it with matroid

218
00:12:01,000 --> 00:12:02,000
as well.

219
00:12:02,000 --> 00:12:06,920
So if you want notifications when some particular person is at home, okay, you can set that

220
00:12:06,920 --> 00:12:12,720
up pretty easily to the level of person, person A versus person B versus person C or more

221
00:12:12,720 --> 00:12:14,960
interesting things that that could be funny.

222
00:12:14,960 --> 00:12:18,520
Like, like, maybe you want a detector that just thinks you're whenever someone with a beard

223
00:12:18,520 --> 00:12:19,520
comes in.

224
00:12:19,520 --> 00:12:20,520
I'm being silly here.

225
00:12:20,520 --> 00:12:22,520
That's the level of customizability that we have there.

226
00:12:22,520 --> 00:12:23,520
Oh, that's interesting.

227
00:12:23,520 --> 00:12:26,240
So if someone with a beard walks in the room, you want a notification, it's kind of

228
00:12:26,240 --> 00:12:27,240
silly, right?

229
00:12:27,240 --> 00:12:34,200
But more interesting, you could do something like maybe a kid has a hand in a cookie jar.

230
00:12:34,200 --> 00:12:35,200
Yeah.

231
00:12:35,200 --> 00:12:36,200
That'd be pretty funny.

232
00:12:36,200 --> 00:12:37,200
Yeah.

233
00:12:37,200 --> 00:12:40,760
Or opening up, you know, your car and diet and you're opening up the fridge after a certain

234
00:12:40,760 --> 00:12:41,760
time at night or something.

235
00:12:41,760 --> 00:12:42,760
Oh, yeah.

236
00:12:42,760 --> 00:12:46,880
And you pick up the Coke instead of that's that's that's an answer to take use case, but

237
00:12:46,880 --> 00:12:47,880
absolutely possible.

238
00:12:47,880 --> 00:12:51,120
If you put a camera in front of your fridge, yeah, we'll be able to detect whether you're

239
00:12:51,120 --> 00:12:53,920
picking up a diet Coke or a regular Coke.

240
00:12:53,920 --> 00:12:54,920
Oh, wow.

241
00:12:54,920 --> 00:13:00,800
So it's interesting that the fact I bought a Pepsi and a Coke for the demo today, one

242
00:13:00,800 --> 00:13:05,360
of the how all cards downstairs in New York, I just put them in front of it and very easily

243
00:13:05,360 --> 00:13:07,480
figured out that one's a Pepsi, the other Coke.

244
00:13:07,480 --> 00:13:11,960
I didn't have diet versus regular, but that's that would be a new detector.

245
00:13:11,960 --> 00:13:12,960
Okay.

246
00:13:12,960 --> 00:13:19,880
It's interesting that video is becoming or a computer vision in general, I guess is

247
00:13:19,880 --> 00:13:24,280
becoming almost like, I don't like a lingua franca detector.

248
00:13:24,280 --> 00:13:28,200
The background thought here is, there's a point in time that I wanted to do like a home

249
00:13:28,200 --> 00:13:33,280
automation project and you know, the question was, how am I going to figure out, you know,

250
00:13:33,280 --> 00:13:38,280
who's in the house and who's not and that kind of thing and, you know, I, this was years

251
00:13:38,280 --> 00:13:44,480
ago and I got into, you know, walking around with like, you know, NFC going to work or

252
00:13:44,480 --> 00:13:47,200
Bluetooth tags or things like that.

253
00:13:47,200 --> 00:13:51,840
And the computer vision stuff is advanced so much that now you would just throw up a camera

254
00:13:51,840 --> 00:13:56,600
and use that and it's opened up like so many different avenues.

255
00:13:56,600 --> 00:14:01,480
You mentioned something, well, so another application that you're absolutely right.

256
00:14:01,480 --> 00:14:05,240
Computers are getting eyes and that's incredibly exciting.

257
00:14:05,240 --> 00:14:10,280
It used to be that it was a blob of numbers to the computer and now it can understand

258
00:14:10,280 --> 00:14:13,800
what's going on and to the point where it becomes the ultimate sensor.

259
00:14:13,800 --> 00:14:19,280
So instead of having all these weird sensors, we can just have cheap, really, really cheap

260
00:14:19,280 --> 00:14:24,480
cameras, I mean, these cameras are incredibly cheap and they are incredibly powerful and

261
00:14:24,480 --> 00:14:26,600
we want to give them more and more power.

262
00:14:26,600 --> 00:14:28,120
That's exactly what Matroid is.

263
00:14:28,120 --> 00:14:33,400
We think that the ability to sense things through your eyes is immensely powerful.

264
00:14:33,400 --> 00:14:38,520
When you think about it, a computer can take in our eyes, right, their eyes taken so much

265
00:14:38,520 --> 00:14:39,520
information, right?

266
00:14:39,520 --> 00:14:40,840
Absolutely.

267
00:14:40,840 --> 00:14:44,680
Megabytes and megabytes of information per second is going in through our eyes.

268
00:14:44,680 --> 00:14:50,960
But as humans, we only have the ability to type at like a very slow speed.

269
00:14:50,960 --> 00:14:57,720
So our eyes are giving us much more information than we could give a computer with clicking

270
00:14:57,720 --> 00:15:00,440
and with most sensors too, actually, I would say.

271
00:15:00,440 --> 00:15:04,480
So most sensors out there, they would give a computer a few bites of information per

272
00:15:04,480 --> 00:15:06,080
second.

273
00:15:06,080 --> 00:15:08,400
But cameras, it's not like that at all.

274
00:15:08,400 --> 00:15:13,360
It's tremendous amounts of information and it's been such an overwhelming task to

275
00:15:13,360 --> 00:15:17,760
sit through that until, until now, until deep learning and until CNN's.

276
00:15:17,760 --> 00:15:22,080
And then it's a matter of now taking this power and being able to make it flexible, putting

277
00:15:22,080 --> 00:15:28,680
it in the hands of everybody instead of just developers, going to town with it basically.

278
00:15:28,680 --> 00:15:32,400
It's incredibly exciting time for computer vision.

279
00:15:32,400 --> 00:15:36,360
A couple more questions, you know, I don't want to go too deep into Matroid, but a couple

280
00:15:36,360 --> 00:15:43,160
more questions that came up for me one is, do you have pre-established relationships

281
00:15:43,160 --> 00:15:48,680
with network supply, TV network suppliers so that like a brand agency can just click like

282
00:15:48,680 --> 00:15:53,080
I want to monitor ABC, CNN and that kind of thing or did they have to go find all that

283
00:15:53,080 --> 00:15:54,080
themselves?

284
00:15:54,080 --> 00:15:55,080
No, that's the hard.

285
00:15:55,080 --> 00:16:00,120
We actually have, we have the ability to search many TV channels.

286
00:16:00,120 --> 00:16:04,120
And the way we do that is by making sure that we don't impringe upon their copyright.

287
00:16:04,120 --> 00:16:07,760
So you can never watch these TV channels on our website.

288
00:16:07,760 --> 00:16:12,120
You can never even see much of what's going on on there.

289
00:16:12,120 --> 00:16:17,360
Other than the ability to get a notification when your detector figures out what's going

290
00:16:17,360 --> 00:16:18,360
on.

291
00:16:18,360 --> 00:16:22,960
And then you only get a notification and a really small blurry screenshot so that you

292
00:16:22,960 --> 00:16:26,520
can verify that that actually happened and that stays within fair use.

293
00:16:26,520 --> 00:16:31,880
So we are absolutely dedicated to making sure we don't step on anyone's toes here and

294
00:16:31,880 --> 00:16:33,680
that's something that we figure it out.

295
00:16:33,680 --> 00:16:39,480
So yes, you can essentially say, well, I want to watch these few channels for my product

296
00:16:39,480 --> 00:16:40,480
showing up.

297
00:16:40,480 --> 00:16:43,520
And answer questions like, well, when did this car show up next to this other car?

298
00:16:43,520 --> 00:16:47,280
Like when I don't want to pick out any particular bands here, but you let your imagination

299
00:16:47,280 --> 00:16:52,120
while like, when did car model number one show up next to car model number two or when

300
00:16:52,120 --> 00:16:53,520
did car model number one show up?

301
00:16:53,520 --> 00:16:57,840
And this would then look at all movies that have been playing, you know, movies of cars

302
00:16:57,840 --> 00:16:58,840
in them all the time.

303
00:16:58,840 --> 00:17:02,040
They have brands in them all the time, right?

304
00:17:02,040 --> 00:17:06,360
You can answer questions like when did a particular kind of laptop show up when to kind

305
00:17:06,360 --> 00:17:10,800
of brand show up and TV channels across the US across the world?

306
00:17:10,800 --> 00:17:18,240
And you also have an offer like a database of movies or someone, no, not movies, not

307
00:17:18,240 --> 00:17:19,240
movies.

308
00:17:19,240 --> 00:17:24,320
So we have streams of media and the streams are, the streams are TV channels.

309
00:17:24,320 --> 00:17:27,120
And you can have the ability to hook up your own streams.

310
00:17:27,120 --> 00:17:29,440
So we integrate with many cameras.

311
00:17:29,440 --> 00:17:33,680
You can search YouTube videos, you can search static videos that you own.

312
00:17:33,680 --> 00:17:37,320
We have not curated a large collection of movies.

313
00:17:37,320 --> 00:17:38,320
Okay.

314
00:17:38,320 --> 00:17:44,360
We only have so much resource, you know, and we just, we want to make it easy for people

315
00:17:44,360 --> 00:17:45,920
to search their own libraries.

316
00:17:45,920 --> 00:17:49,960
So we've built in tools to allow that.

317
00:17:49,960 --> 00:17:55,040
But we can't, we can't index all the videos in the world right now.

318
00:17:55,040 --> 00:18:02,400
That's not what we're focused on because that's a bit of a different role for us, you

319
00:18:02,400 --> 00:18:03,400
know?

320
00:18:03,400 --> 00:18:09,040
And if someone has a need to monitor a stream, usually they have the stream themselves,

321
00:18:09,040 --> 00:18:12,240
unless it's a very popular public stream like TV.

322
00:18:12,240 --> 00:18:15,040
If it's movies, that's a different story.

323
00:18:15,040 --> 00:18:16,200
We just haven't gone there yet.

324
00:18:16,200 --> 00:18:21,920
Well, in more general, it sounds like the core problem is one of monitoring and less

325
00:18:21,920 --> 00:18:26,440
of search, which requires kind of the broader database.

326
00:18:26,440 --> 00:18:32,680
So in doing all this, one of the key challenges that you faced is how do you scale all of it?

327
00:18:32,680 --> 00:18:35,000
And that was a big part of your talk today.

328
00:18:35,000 --> 00:18:37,160
Walk us through some of the things that you talked about.

329
00:18:37,160 --> 00:18:38,160
Sure thing.

330
00:18:38,160 --> 00:18:42,560
So these models, these machine learning models that have to decide whether something is

331
00:18:42,560 --> 00:18:47,680
happening in a scene or not are computationally very intensive.

332
00:18:47,680 --> 00:18:55,680
To monitor a stream of video, you have to essentially dedicate around an eighth of a typical

333
00:18:55,680 --> 00:18:59,000
GPU card these days, 24-7.

334
00:18:59,000 --> 00:19:01,560
So you can monitor eight streams with one GPU card.

335
00:19:01,560 --> 00:19:02,560
Okay.

336
00:19:02,560 --> 00:19:03,560
Yeah.

337
00:19:03,560 --> 00:19:04,560
That's a lot of compute.

338
00:19:04,560 --> 00:19:08,280
I mean, once you would expect that you could do a lot more, if that I'd been optimized,

339
00:19:08,280 --> 00:19:10,120
but that's where you can do it.

340
00:19:10,120 --> 00:19:14,200
And this is inference, which is typically much lighter on the GPU than...

341
00:19:14,200 --> 00:19:18,880
Oh, no, actually, we do inference on the GPU as well.

342
00:19:18,880 --> 00:19:25,240
Inference is, it's not typically lighter on the GPU for video, because it's just relative

343
00:19:25,240 --> 00:19:27,640
to training, I guess, is what I was.

344
00:19:27,640 --> 00:19:32,520
I mean, they're almost equally as difficult when video is involved.

345
00:19:32,520 --> 00:19:37,240
Because what's happening is we have a trained model, right?

346
00:19:37,240 --> 00:19:38,240
Training sure.

347
00:19:38,240 --> 00:19:39,240
Training takes...

348
00:19:39,240 --> 00:19:40,680
We do both training and inference on GPUs.

349
00:19:40,680 --> 00:19:44,520
So both of these are computationally intensive.

350
00:19:44,520 --> 00:19:48,120
The reality, though, is actually inference is more computationally intensive for us, because

351
00:19:48,120 --> 00:19:53,760
the training happens within a matter of a few minutes, because we have many, many pre-trained

352
00:19:53,760 --> 00:19:59,120
models, and we can build up a lot of work that has happened in the background, pre-trained

353
00:19:59,120 --> 00:20:07,480
models, in particular, the inference, however, you're just constantly running a CNN on

354
00:20:07,480 --> 00:20:15,520
a video stream, 24-7, and that's infinitely long, whereas training, at least, is finite,

355
00:20:15,520 --> 00:20:16,520
right?

356
00:20:16,520 --> 00:20:20,760
Training is going to take a week, you're done, and then you have a pre-trained model,

357
00:20:20,760 --> 00:20:25,800
you can use it, so inference, the way matroid deals with it, is infinitely long.

358
00:20:25,800 --> 00:20:31,320
So you always, you just have to dedicate an 8th of a GPU to inference for a stream.

359
00:20:31,320 --> 00:20:35,080
So that is something we have to deal with, as a company, we have to deal with that.

360
00:20:35,080 --> 00:20:38,520
So first of all, that is per stream, per detector, or you can't...

361
00:20:38,520 --> 00:20:43,040
Yeah, so it's per stream, per detector, per stream, per detector, yeah, so that makes it

362
00:20:43,040 --> 00:20:44,040
even worse, right?

363
00:20:44,040 --> 00:20:45,760
It's even more computationally intensive.

364
00:20:45,760 --> 00:20:50,040
It's not like you're just, you know, you can have multiple networks kind of peering into

365
00:20:50,040 --> 00:20:52,640
a stream or sharing...

366
00:20:52,640 --> 00:20:54,960
Sometimes, but not often enough.

367
00:20:54,960 --> 00:20:58,800
Sometimes that happens, and we do optimize for that, but it doesn't happen often enough,

368
00:20:58,800 --> 00:21:01,960
because the reason you're using matroid is because you want to customize a detector.

369
00:21:01,960 --> 00:21:06,120
You've made a detector for your own data, usually it's a detector that only you have,

370
00:21:06,120 --> 00:21:10,600
and it's a stream that only you have access to, and so it's a detector stream.

371
00:21:10,600 --> 00:21:17,480
And so we have a whole cluster dedicated for this, and the way it works is, so these models

372
00:21:17,480 --> 00:21:21,480
are quite large themselves too, the models are on order of 200 megabytes, and there are

373
00:21:21,480 --> 00:21:22,480
millions of them.

374
00:21:22,480 --> 00:21:24,080
It's a huge amount of data.

375
00:21:24,080 --> 00:21:28,440
And just storing the models, just storing the detectors, already needs a distributed

376
00:21:28,440 --> 00:21:29,440
file system.

377
00:21:29,440 --> 00:21:30,440
That's three.

378
00:21:30,440 --> 00:21:34,400
But then some of these models are less often used than others.

379
00:21:34,400 --> 00:21:40,560
And so we have this four layer cache that goes from the distributed file system to the

380
00:21:40,560 --> 00:21:46,840
local hard drive of a machine that's dedicated, then has a GPU, to regular RAM that a CPU can

381
00:21:46,840 --> 00:21:52,080
access, and then smaller GPU RAM, GPU memory.

382
00:21:52,080 --> 00:21:57,200
The hot models, the very hot models, sit in GPU memory for a long amount of time, and

383
00:21:57,200 --> 00:22:00,520
we deal with the whole cache infrastructure there.

384
00:22:00,520 --> 00:22:05,960
And that infrastructure, did you have to roll your own, or is there something that is pre-existent

385
00:22:05,960 --> 00:22:07,920
to manage that for you?

386
00:22:07,920 --> 00:22:14,520
There are tools for general distributed computing, for example, we use Kubernetes, and Kubernetes

387
00:22:14,520 --> 00:22:19,600
is at the level of resource management, resource allocation.

388
00:22:19,600 --> 00:22:27,840
You say to Kubernetes, hey, look, I need this many cores and this much memory and a GPU.

389
00:22:27,840 --> 00:22:32,560
And you set up Kubernetes to be able to get those resources from some cloud provider.

390
00:22:32,560 --> 00:22:36,360
And ideally, it's all from the same cloud provider, but we have the ability to go through

391
00:22:36,360 --> 00:22:39,080
multiple cloud providers.

392
00:22:39,080 --> 00:22:45,000
And then you get that resource, and you can run your workload on it.

393
00:22:45,000 --> 00:22:51,040
But the fact that Kubernetes doesn't know that we have millions of models, and that we

394
00:22:51,040 --> 00:22:55,440
have this whole caching strategy, it just gives you resources when you ask them for it.

395
00:22:55,440 --> 00:22:58,760
It handles some of the fault tolerance, and some of the logging, which is nice.

396
00:22:58,760 --> 00:23:03,080
But like any rollout of open source software, there's a tremendous amount of work that

397
00:23:03,080 --> 00:23:08,120
goes into actually productionizing it and making it good for a particular application.

398
00:23:08,120 --> 00:23:11,880
And there's the interface with TensorFlow that has to work out, and the interface to our

399
00:23:11,880 --> 00:23:13,240
web environment, which has to work out.

400
00:23:13,240 --> 00:23:19,400
So our web app is very complicated, it's the video player, it's constantly talking to

401
00:23:19,400 --> 00:23:22,240
the cluster and to other places.

402
00:23:22,240 --> 00:23:26,040
These are all, it's like a big symphony orchestra that comes together.

403
00:23:26,040 --> 00:23:32,200
Yeah, the specific question that I had was around the, I guess the analogy for me is

404
00:23:32,200 --> 00:23:39,520
in the storage arena, there's, you know, the whole idea of, you know, hot storage, cold

405
00:23:39,520 --> 00:23:44,640
storage, near line, all that kind of stuff as well established in there is tons of products

406
00:23:44,640 --> 00:23:49,640
and infrastructure and stuff that's off the shelf that you can put in place to do that.

407
00:23:49,640 --> 00:23:58,680
It sounds like what you're doing, shuffling data between GPU, CPU, local disk, or GPU memory,

408
00:23:58,680 --> 00:24:05,320
you know, RAM, local disk, and distributed storage, not a lot pre-existent to facilitate

409
00:24:05,320 --> 00:24:07,440
that, so you're kind of rolling your own there.

410
00:24:07,440 --> 00:24:10,840
Yeah, absolutely, we built that from the ground up.

411
00:24:10,840 --> 00:24:15,000
Using Kubernetes, I don't want to say we built the cluster management part, but the

412
00:24:15,000 --> 00:24:19,200
rest of it is all, is all around, in fact, my co-founder has a PhD in distributed systems.

413
00:24:19,200 --> 00:24:20,200
Okay.

414
00:24:20,200 --> 00:24:25,560
And, you know, we were, part of my research was the Apache Spark library, from the machine

415
00:24:25,560 --> 00:24:29,440
learning library, so if one of my papers is the ML lib library, which probably a lot

416
00:24:29,440 --> 00:24:34,960
of your listeners know about, and there it's, it's more about the JVM and Apache Spark.

417
00:24:34,960 --> 00:24:37,680
Well, let's definitely, let's come back to that.

418
00:24:37,680 --> 00:24:40,720
Yeah, that is something we've been working on for a very long time, and we understand

419
00:24:40,720 --> 00:24:42,440
those nuances very well.

420
00:24:42,440 --> 00:24:48,440
We did not pick up Apache's practice time because it's not a go-to tool for, for deep learning,

421
00:24:48,440 --> 00:24:54,240
exactly because it GPUs, it's access to GPUs is limited because it sits on the JVM.

422
00:24:54,240 --> 00:25:01,600
So yeah, dealing with the whole stack from customized hardware to having a distributed

423
00:25:01,600 --> 00:25:06,840
cluster, all of that isn't well managed by one tool right now, and so we had to roll

424
00:25:06,840 --> 00:25:08,160
our own.

425
00:25:08,160 --> 00:25:11,960
And a lot of it really does come to the fact that you have to be able to use customized

426
00:25:11,960 --> 00:25:16,560
hardware, like being able to manage a cluster where some of the machines have a GPU and

427
00:25:16,560 --> 00:25:19,880
some of them don't have a GPU, that also matters, right?

428
00:25:19,880 --> 00:25:28,440
So we will use CPUs when we have no GPU machines available, but it whole thing will be slower.

429
00:25:28,440 --> 00:25:34,440
Using that again is yet another corner case that we deal with in our setup.

430
00:25:34,440 --> 00:25:41,640
And are you running all of this on AWS, or do you run some of it on a local cluster, or

431
00:25:41,640 --> 00:25:46,400
right now everything is on AWS, but we've built everything with the mindset that we should

432
00:25:46,400 --> 00:25:48,080
be able to be cloud hybrid.

433
00:25:48,080 --> 00:25:54,120
So we should at the very least be able to access some custom processing power from other

434
00:25:54,120 --> 00:25:55,120
cloud providers.

435
00:25:55,120 --> 00:25:59,160
For example, we're looking forward to being able to use TPUs, intensive processing units

436
00:25:59,160 --> 00:26:01,800
from Google when they're available.

437
00:26:01,800 --> 00:26:07,960
There are multiple hardware providers who are competing on deep learning hardware and

438
00:26:07,960 --> 00:26:12,680
a multitude of startups in the space, is at least five startups that I've counted in

439
00:26:12,680 --> 00:26:13,680
this space.

440
00:26:13,680 --> 00:26:19,840
And all of the chip manufacturers want to get into it, Intel, Qualcomm, Google is not

441
00:26:19,840 --> 00:26:24,680
a traditional chip manufacturer that they will produce chips to rent on GCP.

442
00:26:24,680 --> 00:26:27,320
And of course, Nvidia leads the pack here, right?

443
00:26:27,320 --> 00:26:32,760
All of these folks are essentially reinventing computing from their ground up to use linear

444
00:26:32,760 --> 00:26:37,360
algebra on the specialized co-processors, if you want to call them that, or intensive

445
00:26:37,360 --> 00:26:38,360
processing units.

446
00:26:38,360 --> 00:26:41,680
I think the name is going to become tensor processing unit, instead of central processing

447
00:26:41,680 --> 00:26:46,120
unit, eventually we're just going to be using TPUs much more than CPUs.

448
00:26:46,120 --> 00:26:52,800
That chipset is evolving rapidly and it's changing all of computing with it.

449
00:26:52,800 --> 00:26:57,840
And that's another exciting side effect of deep learning.

450
00:26:57,840 --> 00:27:03,400
For a long time, we didn't know how computing should evolve to bypass the fact that we can't

451
00:27:03,400 --> 00:27:05,320
make clock speeds any faster.

452
00:27:05,320 --> 00:27:08,480
And now we've realized that linear algebra operations are the way to go, because linear

453
00:27:08,480 --> 00:27:13,560
algebra operations can be used in deep learning, they can be used in all of machine learning,

454
00:27:13,560 --> 00:27:17,960
they can be used in so many other applications, wherever linear algebra is useful, these

455
00:27:17,960 --> 00:27:21,200
operations can be made to be useful.

456
00:27:21,200 --> 00:27:25,280
That's yet another area that we can spend hours on, just like, what is the right hardware

457
00:27:25,280 --> 00:27:31,480
software interface between, just what is the right hardware software interface past

458
00:27:31,480 --> 00:27:35,600
the traditional X86 programming model?

459
00:27:35,600 --> 00:27:38,600
It doesn't make sense to have simple operations anymore.

460
00:27:38,600 --> 00:27:43,280
The hardware should make linear algebra a first-class operation, a first-class citizen,

461
00:27:43,280 --> 00:27:44,280
and that's been happening.

462
00:27:44,280 --> 00:27:50,400
It's interesting that you are projecting that TPU becomes the general term for this.

463
00:27:50,400 --> 00:27:52,520
I was talking to Navin.

464
00:27:52,520 --> 00:27:57,520
That's part of the reason I actually named matroid matroid is because I expect the term

465
00:27:57,520 --> 00:28:01,560
tensor to become more and more popular in computing vocabulary.

466
00:28:01,560 --> 00:28:04,000
TensorFlow definitely was a step in that direction.

467
00:28:04,000 --> 00:28:09,080
Even Nvidia now has the word tensor core in their products.

468
00:28:09,080 --> 00:28:17,440
So if the new Nvidia Volta has many tensor cores, which do small 4x4 matrix multiplies

469
00:28:17,440 --> 00:28:19,400
and accumulates.

470
00:28:19,400 --> 00:28:24,240
That word is creeping around, even in Nvidia, and Nvidia has all the reason in the world

471
00:28:24,240 --> 00:28:27,080
to market the term GPU, right?

472
00:28:27,080 --> 00:28:31,640
But even they are using the word tensor, tensor is all over the world in deep learning.

473
00:28:31,640 --> 00:28:37,680
I think that's the correct, more intuitive term for these processors.

474
00:28:37,680 --> 00:28:40,840
And then so this is why we named matroid as a generalization of tensor.

475
00:28:40,840 --> 00:28:46,560
It's also a domain name that sounds good and was available, so it's hard to find those

476
00:28:46,560 --> 00:28:47,560
days.

477
00:28:47,560 --> 00:28:53,400
And having your company named after a concept deep, deep mathematical concept is actually

478
00:28:53,400 --> 00:28:54,400
very heartwarming.

479
00:28:54,400 --> 00:28:57,400
And it excites a lot of my students to come help us.

480
00:28:57,400 --> 00:28:58,720
Awesome.

481
00:28:58,720 --> 00:29:00,360
So we were talking about scaling.

482
00:29:00,360 --> 00:29:01,360
Yes.

483
00:29:01,360 --> 00:29:05,000
We talked about infrastructure level concerns.

484
00:29:05,000 --> 00:29:10,520
Actually, random question, I keep hearing from a lot of folks talking about Kubernetes

485
00:29:10,520 --> 00:29:15,000
that it's actually kind of been a key to get up and running and working well on Amazon.

486
00:29:15,000 --> 00:29:16,000
It's been reasonable for us.

487
00:29:16,000 --> 00:29:17,000
It's been reasonable.

488
00:29:17,000 --> 00:29:18,880
It's been okay.

489
00:29:18,880 --> 00:29:21,840
So infrastructure stuff we've talked about.

490
00:29:21,840 --> 00:29:28,680
What about the kind of the modeling, training, inference, architecture?

491
00:29:28,680 --> 00:29:35,840
Has that evolved or it has to what degrees has that evolved specifically to enable distributed

492
00:29:35,840 --> 00:29:38,560
compute and scale?

493
00:29:38,560 --> 00:29:41,440
So that's a very loaded question.

494
00:29:41,440 --> 00:29:49,360
So how have models in computer vision evolved is a tremendous, long question to answer

495
00:29:49,360 --> 00:29:50,360
in itself?

496
00:29:50,360 --> 00:29:55,800
Well, more specifically, what you guys are doing, to what degree, clearly you're thinking

497
00:29:55,800 --> 00:30:02,440
about scale when you're building your systems above the layer of the infrastructure and all

498
00:30:02,440 --> 00:30:06,000
of the movement of bits and compute.

499
00:30:06,000 --> 00:30:10,880
For folks that are trying to build, that have, that are doing deep learning and need to

500
00:30:10,880 --> 00:30:14,840
make a scale or computer vision even more specifically, like what are the things that

501
00:30:14,840 --> 00:30:21,440
they need to be thinking about in the computer vision domain to enable scale?

502
00:30:21,440 --> 00:30:26,800
So even with computer vision, scaling can come in three ways, right?

503
00:30:26,800 --> 00:30:28,480
Scaling can come in having many models.

504
00:30:28,480 --> 00:30:32,840
It can come with having very large models and it can come with having large amounts of

505
00:30:32,840 --> 00:30:33,840
data, right?

506
00:30:33,840 --> 00:30:38,680
So I think we already talked pretty extensively about having many models, right?

507
00:30:38,680 --> 00:30:40,480
That was the caching system that I mentioned.

508
00:30:40,480 --> 00:30:42,960
So that's, let's leave that aside for a second.

509
00:30:42,960 --> 00:30:49,120
The other way that scaling can come into effect is having large amounts of data in training.

510
00:30:49,120 --> 00:30:52,600
So assuming that the model is still small, you have a large amount of training data.

511
00:30:52,600 --> 00:30:53,600
We do have that.

512
00:30:53,600 --> 00:30:58,800
So when, when we train, sometimes we train a pre-trained model for weeks, by that I mean,

513
00:30:58,800 --> 00:31:02,520
we train it, it starts out untrained and then it becomes one of our pre-trained models.

514
00:31:02,520 --> 00:31:07,760
Now that's, that's sort of well understood in, in literature and it's one of the reasons

515
00:31:07,760 --> 00:31:13,280
GPUs became really popular because AlexNet was one of these first neural network architectures

516
00:31:13,280 --> 00:31:18,360
that was trained with a ton of data from the ImageNet and that, that's sort of well understood

517
00:31:18,360 --> 00:31:21,880
and let's not talk about that too much right now.

518
00:31:21,880 --> 00:31:27,560
The other form of scaling when it comes to models is the model itself.

519
00:31:27,560 --> 00:31:33,000
So you should be able to add more and bells and whistles to the model and scale it up

520
00:31:33,000 --> 00:31:36,600
to learn more things as, as you like.

521
00:31:36,600 --> 00:31:38,800
And that's actually something that we focused on.

522
00:31:38,800 --> 00:31:44,120
It's because as new models come out in open source, we would like to be able to suck them

523
00:31:44,120 --> 00:31:46,320
in and make them part of our detector.

524
00:31:46,320 --> 00:31:51,280
So if there's a new TensorFlow model out there that can detect logos really well or if

525
00:31:51,280 --> 00:31:55,760
there's a new TensorFlow model out there that can detect the make and model of a car really

526
00:31:55,760 --> 00:32:01,000
well, we better be able to integrate that into our system within a matter of an hours.

527
00:32:01,000 --> 00:32:02,720
And that's where we are.

528
00:32:02,720 --> 00:32:06,920
We have built our setup so that if there's a pre-trained model, a subnet, we call them

529
00:32:06,920 --> 00:32:14,480
out there that is available to learn rapidly a large body of things, we can then hook

530
00:32:14,480 --> 00:32:20,640
it up into matroid so that it becomes a new set of features to use when creating a detector.

531
00:32:20,640 --> 00:32:29,000
So our detectors are essentially a cocktail of pre-trained subnets that some of which

532
00:32:29,000 --> 00:32:30,560
are proprietary to us.

533
00:32:30,560 --> 00:32:37,520
I have a team of three PhDs working on them non-stop building these cocktail of pre-trained

534
00:32:37,520 --> 00:32:43,400
models, taking them from whenever open source provides them and using whatever we can do

535
00:32:43,400 --> 00:32:49,200
to whatever our customers ask for and we don't have models for we build them ourselves.

536
00:32:49,200 --> 00:32:55,440
And all of these models are combined together at the end to build that one last detector

537
00:32:55,440 --> 00:32:58,960
and then detector is sort of statically available.

538
00:32:58,960 --> 00:33:04,800
So yeah, the short answer is the ability to morph a model, to give it more capabilities

539
00:33:04,800 --> 00:33:11,000
by adding subnets to it is something that is very valuable if done right.

540
00:33:11,000 --> 00:33:13,600
And that's something that we've focused on.

541
00:33:13,600 --> 00:33:18,240
And I haven't talked about too much because I guess that ties into a little bit of our

542
00:33:18,240 --> 00:33:21,800
proprietary architectures and so we haven't really released much of that.

543
00:33:21,800 --> 00:33:22,800
Okay.

544
00:33:22,800 --> 00:33:29,000
But it does sound like if your goal is to be able to use off-the-shelf pre-trained models

545
00:33:29,000 --> 00:33:32,040
then that's not our goal.

546
00:33:32,040 --> 00:33:33,680
That's not my goal, right?

547
00:33:33,680 --> 00:33:38,200
It's to have that be a small part of our system, to be able to essentially have a super

548
00:33:38,200 --> 00:33:40,240
set of the capabilities of pre-trained models.

549
00:33:40,240 --> 00:33:45,280
Okay, but be able to quickly take advantage of innovations outside of your company.

550
00:33:45,280 --> 00:33:46,280
Exactly.

551
00:33:46,280 --> 00:33:47,280
That's the key there.

552
00:33:47,280 --> 00:33:48,960
We don't think we can take on the open source community.

553
00:33:48,960 --> 00:33:50,520
It would be foolish, right?

554
00:33:50,520 --> 00:33:54,160
We think that the open source community is, first of all, we contribute back.

555
00:33:54,160 --> 00:33:58,920
We commit to Kubernetes and we submit bugs and we're actually running a book on TensorFlow

556
00:33:58,920 --> 00:34:01,160
and so on.

557
00:34:01,160 --> 00:34:05,800
There is no point in not embracing open source wholeheartedly.

558
00:34:05,800 --> 00:34:10,000
So then the question is how do you build a product that can both give back and receive

559
00:34:10,000 --> 00:34:11,000
from open source?

560
00:34:11,000 --> 00:34:15,000
And we think this is the way is to be able to integrate what the open source community

561
00:34:15,000 --> 00:34:19,000
finds and discovers as quickly as possible.

562
00:34:19,000 --> 00:34:22,600
But in doing that, those are pre-trained pre-existing models.

563
00:34:22,600 --> 00:34:28,440
You're not changing, changing the model architecture can't be kind of how you scale, right?

564
00:34:28,440 --> 00:34:34,560
It's your, or it can't be, or to what extent can that be a part of, or is that a part

565
00:34:34,560 --> 00:34:36,480
of the way you scale?

566
00:34:36,480 --> 00:34:41,480
It is a core part of how we scale to detect more things.

567
00:34:41,480 --> 00:34:46,640
So right now we have a Coca-Cola logo detector, say.

568
00:34:46,640 --> 00:34:54,400
If I wanted to create a detector for Coca-Cola on a t-shirt detector, not Coca-Cola in general,

569
00:34:54,400 --> 00:35:00,760
like not on a can, not a side of a truck, but only on t-shirts, I'd probably have to create

570
00:35:00,760 --> 00:35:03,480
a custom neural network architecture for that.

571
00:35:03,480 --> 00:35:08,560
Let's say I create that and I better have an easy way to add that into the matroid model

572
00:35:08,560 --> 00:35:12,040
detection scheme and that's the key that we have already.

573
00:35:12,040 --> 00:35:16,880
So that little subnet can go into matroid without much effort.

574
00:35:16,880 --> 00:35:20,920
And if that subnet happens to be available on an open source, we would pick it out of

575
00:35:20,920 --> 00:35:22,640
there and put it in.

576
00:35:22,640 --> 00:35:26,080
And that's, I mean, if you want to call that scaling, or if you don't want to call that

577
00:35:26,080 --> 00:35:32,120
scaling, it seems like we're having a vocabulary thing here, is that we're increasing a number

578
00:35:32,120 --> 00:35:39,000
of things that can be detected by adding subnets to what is being trained.

579
00:35:39,000 --> 00:35:40,480
I don't know if you want to call that scaling.

580
00:35:40,480 --> 00:35:46,320
Now, I think, you know, that I think the background for this conversation is maybe an interview

581
00:35:46,320 --> 00:35:50,600
I did with Shubos and Gupta at Baidu Labs.

582
00:35:50,600 --> 00:35:57,280
And we were talking about the Baidu net research they did for audio and machine translation

583
00:35:57,280 --> 00:35:59,080
and things like that.

584
00:35:59,080 --> 00:36:03,360
The conversation was around, like, some of the same types of issues, like systems challenges

585
00:36:03,360 --> 00:36:07,760
and he went through, you know, a lot of low-level things like we've talked about, but he also

586
00:36:07,760 --> 00:36:13,920
talked a little bit about how, you know, the model architecture, some of the decisions

587
00:36:13,920 --> 00:36:22,360
they made in architecting their neural nets were made in light of the computational limitations

588
00:36:22,360 --> 00:36:23,680
that they had.

589
00:36:23,680 --> 00:36:28,080
And I was just curious whether you experienced similar things and whether you adapted

590
00:36:28,080 --> 00:36:35,800
model architecture in light of computational network, storage constraints, memory constraints,

591
00:36:35,800 --> 00:36:36,800
things like that.

592
00:36:36,800 --> 00:36:44,480
So for some of our customers who have severe memory restraints and custom hardware that

593
00:36:44,480 --> 00:36:50,480
they would like to run a matroid detector on, we have compresses, yes, exactly.

594
00:36:50,480 --> 00:36:57,640
Essentially, you know, one customer who wants to run detectors on their own camera.

595
00:36:57,640 --> 00:37:04,080
For them, we've had to compress and remove a large number of the subnets that we use

596
00:37:04,080 --> 00:37:06,400
when training a matroid detector.

597
00:37:06,400 --> 00:37:09,440
But that is something that we don't do lightly.

598
00:37:09,440 --> 00:37:13,040
There was a lot of engineering effort there and the client wanted this so much that they

599
00:37:13,040 --> 00:37:15,800
were willing to pay us for that engineering effort.

600
00:37:15,800 --> 00:37:19,640
Part of the reason that we run in the cloud is so that we can afford to have slightly larger

601
00:37:19,640 --> 00:37:20,800
models.

602
00:37:20,800 --> 00:37:25,120
It's a privilege there in that environment to be able to run larger models.

603
00:37:25,120 --> 00:37:31,080
We have, in some cases, had to compress the models and remove the parts that weren't

604
00:37:31,080 --> 00:37:36,440
so relevant for competitions and we are, in some cases, using TensorFlow Lite for some

605
00:37:36,440 --> 00:37:37,760
of these.

606
00:37:37,760 --> 00:37:43,400
But we can be a little bit more lazy when it comes to model compression and more focused

607
00:37:43,400 --> 00:37:49,040
on model power and detecting many things as opposed to one or two things.

608
00:37:49,040 --> 00:37:53,080
Once again, even in model compression, the open source community has been better than

609
00:37:53,080 --> 00:37:54,080
us.

610
00:37:54,080 --> 00:37:59,160
And so the compressed models that we're using for this particular camera did come from

611
00:37:59,160 --> 00:38:00,160
open source.

612
00:38:00,160 --> 00:38:05,440
This one, I don't want to say that we've had an innovation in putting neural networks

613
00:38:05,440 --> 00:38:08,360
on cameras because we haven't.

614
00:38:08,360 --> 00:38:12,720
That innovation came from open source and we're essentially commercializing it.

615
00:38:12,720 --> 00:38:17,760
The things I worry about is how do we make sure that those innovations that are, first

616
00:38:17,760 --> 00:38:23,080
of all, how do we contribute back to open source and make sure that it's always alive

617
00:38:23,080 --> 00:38:24,080
and well?

618
00:38:24,080 --> 00:38:28,800
But then also, how can we make sure to be able to bring innovations there as quickly

619
00:38:28,800 --> 00:38:29,800
as possible?

620
00:38:29,800 --> 00:38:36,800
And the fact that we could bring that innovation in very quickly, this model that was very

621
00:38:36,800 --> 00:38:43,320
tight, very small, computationally efficient, but powerful enough for our user.

622
00:38:43,320 --> 00:38:44,320
That was very promising.

623
00:38:44,320 --> 00:38:46,240
It was the fact that we managed to do that.

624
00:38:46,240 --> 00:38:50,920
So quickly, as a small company, one speaks to the power group in source and two speaks

625
00:38:50,920 --> 00:38:51,920
to our planning.

626
00:38:51,920 --> 00:38:52,920
All right.

627
00:38:52,920 --> 00:38:53,920
All right.

628
00:38:53,920 --> 00:38:57,040
So maybe we can take that step back to talk a little bit about Spark.

629
00:38:57,040 --> 00:38:58,040
Let's do that.

630
00:38:58,040 --> 00:39:01,240
Spark extensively on Spark MLlib in particular.

631
00:39:01,240 --> 00:39:02,240
Yes.

632
00:39:02,240 --> 00:39:05,800
And it didn't choose it as the foundation for your architecture.

633
00:39:05,800 --> 00:39:11,480
Maybe talk a little bit about Spark and MLlib and some of the trade-offs that you looked

634
00:39:11,480 --> 00:39:14,840
at when you were choosing to build up your system.

635
00:39:14,840 --> 00:39:19,520
So we already talked about how the hardware landscape is changing.

636
00:39:19,520 --> 00:39:23,680
And it's definitely completely changed as far as machine learning goes.

637
00:39:23,680 --> 00:39:29,040
Its custom chips are the way to go when it comes to machine learning.

638
00:39:29,040 --> 00:39:36,080
And as you know, Spark, because of legacy reasons, runs on the Java virtual machine, the

639
00:39:36,080 --> 00:39:37,080
JVM.

640
00:39:37,080 --> 00:39:43,040
It runs there because of Hadoop running there initially when Hadoop was created out of Yahoo,

641
00:39:43,040 --> 00:39:48,120
they decided to run it on the JVM because at the time, Java was hot, there were a lot

642
00:39:48,120 --> 00:39:49,120
of developers.

643
00:39:49,120 --> 00:39:50,880
And so it was done.

644
00:39:50,880 --> 00:39:55,000
And the thing about Java is that it makes us promise that you don't have to know what

645
00:39:55,000 --> 00:40:01,640
hardware you're running on, which is an assumption that just totally goes out the window when

646
00:40:01,640 --> 00:40:05,920
a hardware software interface is so volatile right now.

647
00:40:05,920 --> 00:40:11,440
The hardware software interface is no longer an instruction set that is familiar to everybody.

648
00:40:11,440 --> 00:40:17,080
It is a vast number of different instruction sets for different chips that have not been

649
00:40:17,080 --> 00:40:22,760
standardized the way that the CPU has been.

650
00:40:22,760 --> 00:40:32,920
And as a result, the JVM cannot directly and easily make use of these fancy processors.

651
00:40:32,920 --> 00:40:35,840
Of course, there are ways to make native calls in Java.

652
00:40:35,840 --> 00:40:40,840
But those native calls in the libraries that allow you to do those native calls.

653
00:40:40,840 --> 00:40:41,840
Is that still JNI?

654
00:40:41,840 --> 00:40:42,840
Yes.

655
00:40:42,840 --> 00:40:48,680
Those are still usually a year or two behind the hardware coming out.

656
00:40:48,680 --> 00:40:54,120
And so you're in this bad situation where some new hardware is out.

657
00:40:54,120 --> 00:40:58,840
And the software that will let you use that hardware and really only still in the limited

658
00:40:58,840 --> 00:41:03,560
capacity, not the full instruction set, comes out in two years after the hardware.

659
00:41:03,560 --> 00:41:04,560
So you're already two years behind.

660
00:41:04,560 --> 00:41:07,520
And as a researcher, then, that's unacceptable.

661
00:41:07,520 --> 00:41:09,360
Well, it's a commercial user as well.

662
00:41:09,360 --> 00:41:10,360
That's right.

663
00:41:10,360 --> 00:41:13,040
So the hardware changes, it is absolutely unacceptable.

664
00:41:13,040 --> 00:41:17,360
It's the difference between being competitive or not, especially for a company like ours,

665
00:41:17,360 --> 00:41:18,360
right?

666
00:41:18,360 --> 00:41:23,160
Because, like I said, we have to dedicate an eighth of a GPU for infinity to a stream.

667
00:41:23,160 --> 00:41:26,400
Well, and if some new thing comes along that makes that a sixteenth, right?

668
00:41:26,400 --> 00:41:27,400
We better use it.

669
00:41:27,400 --> 00:41:28,400
Exactly.

670
00:41:28,400 --> 00:41:29,400
Exactly.

671
00:41:29,400 --> 00:41:32,920
And if our competitors do, then chances are we wouldn't be so happy and maybe even run

672
00:41:32,920 --> 00:41:34,000
out of business.

673
00:41:34,000 --> 00:41:38,400
So sadly, Spark lives on the JVM.

674
00:41:38,400 --> 00:41:43,880
And so the question is, how do you get all those benefits in the JVM?

675
00:41:43,880 --> 00:41:46,040
You can't easily.

676
00:41:46,040 --> 00:41:54,120
The way the Spark community is evolving is to be able to plug in the neural network frameworks

677
00:41:54,120 --> 00:41:57,200
that are actually not written in the JVM.

678
00:41:57,200 --> 00:42:02,720
So what is the case now, actually, we saw at the early talk right after mine, actually,

679
00:42:02,720 --> 00:42:04,720
was about how to run TensorFlow on Spark.

680
00:42:04,720 --> 00:42:06,240
Is that the Yahoo, folks?

681
00:42:06,240 --> 00:42:07,240
I know this.

682
00:42:07,240 --> 00:42:08,240
No.

683
00:42:08,240 --> 00:42:14,080
Yeah, there are many TensorFlow on Spark packages.

684
00:42:14,080 --> 00:42:16,080
I think I've counted four of them.

685
00:42:16,080 --> 00:42:19,200
It's a good idea, clearly, because so many people are working on it.

686
00:42:19,200 --> 00:42:22,240
And this particular talk that I mentioned was a Databricks talk, but actually, yes,

687
00:42:22,240 --> 00:42:28,080
Andy Fang and some folks who have also worked on putting TensorFlow on Spark in a different

688
00:42:28,080 --> 00:42:29,080
way.

689
00:42:29,080 --> 00:42:30,080
Yeah.

690
00:42:30,080 --> 00:42:36,360
And the various approaches to it, but almost all of them involve Spark handling the data

691
00:42:36,360 --> 00:42:44,160
munching and gringing, meaning ETL and just the distribution of data across many machines.

692
00:42:44,160 --> 00:42:48,000
And then when it comes time to learning, they spin up a TensorFlow process and just let

693
00:42:48,000 --> 00:42:52,680
the TensorFlow process go to town on the data for a long period of time, maybe 10, 20 minutes.

694
00:42:52,680 --> 00:42:54,000
It does its thing.

695
00:42:54,000 --> 00:42:56,960
And then eventually it gets picked out a new model, then Spark does some broadcasting

696
00:42:56,960 --> 00:43:01,360
and some communication between machines and then again, hands back to TensorFlow.

697
00:43:01,360 --> 00:43:08,680
So the majority of the time is spent inside a framework that is written not in the JVM,

698
00:43:08,680 --> 00:43:11,600
written in more closer to hardware.

699
00:43:11,600 --> 00:43:13,600
So then the question is, why do that?

700
00:43:13,600 --> 00:43:19,880
Why not just stick to something that's in, that's always in close to the hardware?

701
00:43:19,880 --> 00:43:20,880
Close to the hardware.

702
00:43:20,880 --> 00:43:21,880
And the answer is there is an integrated answer, right?

703
00:43:21,880 --> 00:43:26,800
So actually, there's this, the lab that Spark came out of at Berkeley is now moving away

704
00:43:26,800 --> 00:43:27,800
from the JVM2.

705
00:43:27,800 --> 00:43:30,280
So it used to be called the amp lab.

706
00:43:30,280 --> 00:43:35,120
The amp lab is, for the sake of creative destruction, they wound down the amp lab and replaced

707
00:43:35,120 --> 00:43:37,680
it now with the Ryze lab.

708
00:43:37,680 --> 00:43:41,360
The Ryze lab on Spark now, well, no, that's not true, Apache on Spark.

709
00:43:41,360 --> 00:43:44,400
But the Ryze lab has a lot of people who work on Spark, but also new projects.

710
00:43:44,400 --> 00:43:47,960
And the new projects are all using C++.

711
00:43:47,960 --> 00:43:54,600
Because we're back, computing has been reset essentially with the resetting of the hardware

712
00:43:54,600 --> 00:43:55,600
software interface.

713
00:43:55,600 --> 00:43:56,600
Computing has been reset.

714
00:43:56,600 --> 00:43:59,880
And so all these chip manufacturers are also worried now because they're a little bit

715
00:43:59,880 --> 00:44:05,000
closer to being competed out, because now that a large body of their moat has essentially

716
00:44:05,000 --> 00:44:10,880
been removed, and that is very powerful for people who are looking to innovate in that

717
00:44:10,880 --> 00:44:11,880
space.

718
00:44:11,880 --> 00:44:17,480
And machine learning engineers are one of them people who work on machine learning libraries

719
00:44:17,480 --> 00:44:18,480
are one of them.

720
00:44:18,480 --> 00:44:22,160
And TensorFlow does this quite well in that there is a, because TensorFlow has supposed to

721
00:44:22,160 --> 00:44:27,040
run on many different chipsets, it has a compiler dedicated to be able to compile TensorFlow

722
00:44:27,040 --> 00:44:31,360
graphs into many different chipsets, like the Qualcomm chipset and Intel chipset and of

723
00:44:31,360 --> 00:44:34,080
course, CUDA and CUDNN.

724
00:44:34,080 --> 00:44:39,960
I suspect what we'll see is after a long battle, maybe over the course of five to ten

725
00:44:39,960 --> 00:44:46,800
years, we'll eventually settle on a new hardware software interface that will look a lot like

726
00:44:46,800 --> 00:44:47,880
linear algebra.

727
00:44:47,880 --> 00:44:48,880
Okay.

728
00:44:48,880 --> 00:44:56,480
There will be a chipset that at its core supports many matrix multiplies, not just many

729
00:44:56,480 --> 00:45:02,040
matrix multiplies, many small matrix multiplies, but a few big matrix multiplies, so multiplying

730
00:45:02,040 --> 00:45:08,280
two very big matrices together, multiplying many small matrices together, which is essentially

731
00:45:08,280 --> 00:45:12,400
what the Nvidia Volta is, and then you know, matrix vector operations and vector operations

732
00:45:12,400 --> 00:45:13,400
as well.

733
00:45:13,400 --> 00:45:17,800
Those are actually reasonably well supported with Bloss, which has been around since the

734
00:45:17,800 --> 00:45:18,800
seventies.

735
00:45:18,800 --> 00:45:24,280
But the many, many small matrix multiplies is not well supported in the CPU and needs

736
00:45:24,280 --> 00:45:30,240
to be done in custom chips, and that's part of what will be this new language, this

737
00:45:30,240 --> 00:45:35,640
new instruction set for the CPU, for the, for the new processing unit, and I'm curious

738
00:45:35,640 --> 00:45:36,640
to see what that looks like.

739
00:45:36,640 --> 00:45:43,120
Once that settles down, then there's time for new JVMs to pop up.

740
00:45:43,120 --> 00:45:44,120
Interesting.

741
00:45:44,120 --> 00:45:49,600
So the audience didn't see me chuckling as you were describing how the Spark ecosystem,

742
00:45:49,600 --> 00:45:55,440
how Spark is running TensorFlow, but it struck me as funny because Spark is like the new

743
00:45:55,440 --> 00:46:01,160
yarn for TensorFlow workloads, and for some of these, in some ways, yes, workloads, which

744
00:46:01,160 --> 00:46:02,880
is somewhat ironic.

745
00:46:02,880 --> 00:46:09,640
At the same time, one would expect that Google expands kind of the landscape around TensorFlow

746
00:46:09,640 --> 00:46:13,760
to more natively support distributed compute.

747
00:46:13,760 --> 00:46:18,480
What's the, I'm not very familiar with the situation there.

748
00:46:18,480 --> 00:46:22,880
I was at trying to remember the name of this, the Google event that I was at where we had

749
00:46:22,880 --> 00:46:29,200
an extensive conversation around like integrating Kubernetes more natively with TensorFlow and

750
00:46:29,200 --> 00:46:33,440
making more easy to do distributed TensorFlow compute.

751
00:46:33,440 --> 00:46:36,560
What's the general landscape there?

752
00:46:36,560 --> 00:46:41,400
So Kubernetes and TensorFlow do play very, very nicely with each other, and TensorFlow

753
00:46:41,400 --> 00:46:47,760
does have a distributed mode where you can have TensorFlow running on many machines or

754
00:46:47,760 --> 00:46:52,560
many cores, many GPUs on a single machine.

755
00:46:52,560 --> 00:46:56,160
Both of those are supported reasonably well with TensorFlow, and it's that the reason

756
00:46:56,160 --> 00:47:01,720
people don't use that is because it's often the case that when they have a lot of data,

757
00:47:01,720 --> 00:47:08,200
they've already set up a Hadoop cluster or a yarn cluster, and so they don't want to

758
00:47:08,200 --> 00:47:10,560
just undo all that engineering.

759
00:47:10,560 --> 00:47:12,960
They just want to be able to use all that data.

760
00:47:12,960 --> 00:47:16,360
And sometimes fault tolerance matters a lot more for them.

761
00:47:16,360 --> 00:47:20,320
TensorFlow by default is not fault tolerant in a serious way.

762
00:47:20,320 --> 00:47:26,080
So if your machines go down, you have to restart the computation, also restart the machines

763
00:47:26,080 --> 00:47:27,600
and everything yourself.

764
00:47:27,600 --> 00:47:31,080
Whereas with Spark, you get the fault tolerance.

765
00:47:31,080 --> 00:47:37,000
But it's actually not clear whether fault tolerance is all that useful for machine learning.

766
00:47:37,000 --> 00:47:42,840
Because usually if you have a training job, it runs for two or three days maybe, and

767
00:47:42,840 --> 00:47:46,560
then it's done, and you may be using 10 machines or 100 machines.

768
00:47:46,560 --> 00:47:49,240
If it's 100 machines for two or three days, chances are one of them will go down.

769
00:47:49,240 --> 00:47:52,360
So there you will care about having fault tolerance.

770
00:47:52,360 --> 00:47:56,200
But then fault tolerance in machine learning can be as easy as just restarting the machine

771
00:47:56,200 --> 00:48:00,400
that died with a version of the models that the other machines had or a version of the

772
00:48:00,400 --> 00:48:02,360
model that is even random.

773
00:48:02,360 --> 00:48:05,920
And so it's actually not a big deal for machine learning for there to be failures.

774
00:48:05,920 --> 00:48:09,200
And so fault tolerance just doesn't seem like an important deal there.

775
00:48:09,200 --> 00:48:11,760
But it is an important deal for some people like us.

776
00:48:11,760 --> 00:48:18,040
If we guarantee to our customers that we're monitoring a stream, we better monitor that

777
00:48:18,040 --> 00:48:20,040
stream all the time.

778
00:48:20,040 --> 00:48:22,520
And so fault tolerance does matter too.

779
00:48:22,520 --> 00:48:27,400
So we actually have to set up our own fault tolerance mechanisms for the sake of making

780
00:48:27,400 --> 00:48:32,440
sure that we always have some detector on the stream.

781
00:48:32,440 --> 00:48:35,600
So the whole landscape is changing dramatically.

782
00:48:35,600 --> 00:48:41,880
And everyone is very interesting and big fight.

783
00:48:41,880 --> 00:48:44,640
Maybe I shouldn't call it a fight because there's not necessarily one winner.

784
00:48:44,640 --> 00:48:51,680
What will probably happen is data workloads, workloads that are of the form, joins, group

785
00:48:51,680 --> 00:48:55,080
buys, and selects and so on.

786
00:48:55,080 --> 00:48:59,800
I think those will forever stay in Spark because Spark doesn't really well.

787
00:48:59,800 --> 00:49:02,360
It doesn't matter that they're in the JVM at that point because the difference between

788
00:49:02,360 --> 00:49:08,520
the JVM and the difference there is much less the CPUs, well, does joins and group buys

789
00:49:08,520 --> 00:49:10,520
well enough.

790
00:49:10,520 --> 00:49:15,200
It's these machine learning operations that these newer network operations that I think

791
00:49:15,200 --> 00:49:18,360
are better suited to hardware, those will be run in TensorFlow.

792
00:49:18,360 --> 00:49:23,600
I don't think TensorFlow will ever evolve to a point where it does joins and group buys.

793
00:49:23,600 --> 00:49:25,360
So I don't think that'll ever happen.

794
00:49:25,360 --> 00:49:29,160
And you always need joins and group buys to manage data.

795
00:49:29,160 --> 00:49:32,280
And so Spark will always have that place.

796
00:49:32,280 --> 00:49:36,400
Not always, but some tool that does joins and group buys will always have that place.

797
00:49:36,400 --> 00:49:39,160
And there's no real serious competitor to Spark.

798
00:49:39,160 --> 00:49:41,960
Well, there are, but you know, there's none of them are popular.

799
00:49:41,960 --> 00:49:48,120
By extension, the general CPU will always have a place alongside the TPU, right?

800
00:49:48,120 --> 00:49:56,080
Yes, unless the instruction sets that the TPUs provide expand and slowly over maybe

801
00:49:56,080 --> 00:50:01,120
a decade take over what the CPU does to and then the CPU is obsolete.

802
00:50:01,120 --> 00:50:03,520
So I don't know if that's going to happen.

803
00:50:03,520 --> 00:50:04,520
Interesting.

804
00:50:04,520 --> 00:50:08,440
Yeah, I don't know if that's going to, it's not, I haven't seen it happening yet.

805
00:50:08,440 --> 00:50:13,560
But it might, one of these hardware manufacturers might decide, hey, I got a good lead in

806
00:50:13,560 --> 00:50:15,200
these co-processor units.

807
00:50:15,200 --> 00:50:20,680
Why don't I just put a little bit more semiconductor into my co-processor and make it a full-on

808
00:50:20,680 --> 00:50:24,840
processor as well as a co-processor or the other way around or the other way around.

809
00:50:24,840 --> 00:50:29,040
So yeah, it's going, it's definitely all of that is happening right now and we're watching

810
00:50:29,040 --> 00:50:30,040
it.

811
00:50:30,040 --> 00:50:32,760
And it seems as if the computing gets faster, computing gets more efficient, so I'm just

812
00:50:32,760 --> 00:50:33,760
happy that it's happening.

813
00:50:33,760 --> 00:50:34,760
Awesome.

814
00:50:34,760 --> 00:50:35,760
Awesome.

815
00:50:35,760 --> 00:50:36,760
Well, we've got to get you to a flight.

816
00:50:36,760 --> 00:50:37,760
Yes.

817
00:50:37,760 --> 00:50:40,360
How can folks find you, learn more about what you're up to, explore these topics more?

818
00:50:40,360 --> 00:50:46,760
I am very Googleable, so matroid.com is for matroid and just Google, Reza, Zadeh and my

819
00:50:46,760 --> 00:50:48,840
home page shows up very easy.

820
00:50:48,840 --> 00:50:49,840
Awesome.

821
00:50:49,840 --> 00:50:50,840
All right.

822
00:50:50,840 --> 00:50:51,840
Thank you for having me.

823
00:50:51,840 --> 00:50:52,840
Thanks so much.

824
00:50:52,840 --> 00:50:53,840
It was awesome.

825
00:50:53,840 --> 00:50:54,840
I enjoyed the conversation.

826
00:50:54,840 --> 00:50:56,840
Learned a ton and we'll be in touch.

827
00:50:56,840 --> 00:50:57,840
All right.

828
00:50:57,840 --> 00:50:58,840
Thanks.

829
00:50:58,840 --> 00:51:05,360
All right, everyone, that is our show.

830
00:51:05,360 --> 00:51:10,360
Thanks so much for listening and for your continued support, comments and feedback.

831
00:51:10,360 --> 00:51:14,840
A special thanks goes out to our series sponsor, Intel Nirvana.

832
00:51:14,840 --> 00:51:19,040
If you didn't catch the first show in this series where I talked to Naveen Rao, the head

833
00:51:19,040 --> 00:51:23,720
of Intel's AI product group about how they plan to leverage their leading position and

834
00:51:23,720 --> 00:51:28,720
proven history in Silicon innovation to transform the world of AI, you're going to

835
00:51:28,720 --> 00:51:30,800
want to check that out next.

836
00:51:30,800 --> 00:51:37,360
For more information about Intel Nirvana's AI platform, visit intelnervana.com.

837
00:51:37,360 --> 00:51:43,960
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference.

838
00:51:43,960 --> 00:51:49,120
To enter, just let us know what you think about any of the podcasts in the series or post

839
00:51:49,120 --> 00:51:54,280
your favorite quote from any of them on the show notes page, on Twitter or via any of

840
00:51:54,280 --> 00:51:56,200
our social media channels.

841
00:51:56,200 --> 00:52:04,360
Make sure to mention at TwomoAI, at Intel AI, and at the AI come so that we know you want

842
00:52:04,360 --> 00:52:06,600
to enter the contest.

843
00:52:06,600 --> 00:52:12,000
Full details can be found on the series page and of course, all entrants get one of our

844
00:52:12,000 --> 00:52:14,960
slick Twomo laptop stickers.

845
00:52:14,960 --> 00:52:20,080
Speaking of the series page, you can find links to all of the individual show notes pages

846
00:52:20,080 --> 00:52:25,640
by visiting TwomoAI.com slash O'Reilly AINY.

847
00:52:25,640 --> 00:52:28,640
Thanks so much for listening and catch you next time.

