Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, in this episode I'm joined by Missy Cummings, head of Duke University's humans and autonomy lab and professor in the department of mechanical engineering.
In addition to being an accomplished researcher, Missy also became one of the first female fighter pilots in the US Navy following the repeal of the combat exclusion policy in 1993.
We discussed Missy's research into the infrastructural and operational challenges presented by autonomous vehicles, including cars, drones and other unmanned aircraft.
We also covered trust, explainability and interactions between humans and autonomous vehicle systems.
This was an awesome interview and I'm glad to be able to bring it to you and now on to the show.
Alright everyone, I am on the line with Missy Cummings, Missy is a professor in the department of mechanical engineering and material science at Duke University and I'm excited to have Missy online to talk to us about her work in autonomous vehicles.
Missy, welcome to this week in machine learning and AI.
Thanks for having me.
Absolutely. As is the tradition here, why don't we get started by having you introduce yourself to the audience and tell us a little bit about
how you got interested in autonomous vehicles and kind of your path to where you are?
So I think the path to how I got to be not just a professor of mechanical engineering and material science, but I'm also the head of Duke Robotics and the Duke Humans and Autonomy Laboratory, which is how I'm tied into artificial intelligence and machine learning in the first place.
But I think what most people find unusual is that I was one of the United States Navy's first female fighter pilots.
Wow.
So yeah, that kind of dates me too, which means that I was flying fighters in the 90s for the US Navy and it was a very tumultuous time.
You know, it was very much an era of social change, but also combined with technology change.
And so when I was flying fighters, I lost about one friend, Pierre, a month for the three years that I flew the planes.
And it was always because of human air and it was human air driven by a lack of human appreciation for what the computers were doing flying fly by wire aircraft, which were still relatively new back in the 90s.
Seeing so many friends die because of effectively what was extremely poor human computer interaction, eventually when I decided to leave the Navy, I decided to go back to graduate school and I decided to focus my research in this area of human machine interaction.
And so it's been around for a little while, but I decided to kind of jump in and ramp this game up. And so MIT found out that I was on the job market, and this is around 2004.
And so then I was at MIT for a decade, running a lab there in aeronautics department. I had a really great time at MIT and basically turned into a diva.
So at Duke University, Duke University treats me like the diva that I think I am. And so then I moved my lab to Duke. And that's a recent move. And we do focus on human interaction with autonomous systems.
And I do a lot of work in drones, but also driverless cars really unmanned vehicle systems of all types where a human is trying to affect change, typically through some kind of remote control station.
But occasionally you'll find me in power plants and in other areas, medical devices, where autonomous technologies are starting to become on the rise.
One of the areas we can explore to get started with is maybe you can kind of compare and contrast the challenges associated with autonomous vehicles that are flying and those that are driving.
You know, we've are focused to date in this series has been on driverless cars. There's, you know, clearly a lot of energy and excitement around that. And I think it's something that we can all relate to as users of cars.
But I imagine the challenges, you know, there may be some unique challenges associated with with flying vehicles as well.
So it was definitely one of the first researchers into drones. I've also become one of the first researchers researchers to generalize to flying cars. So I actually, yeah, we're doing research for NASA right now on flying cars.
I would say it's surprising to most people to find out that actually it's easier to operate drones than it is driverless cars. And what really drives the fact that drones are truly further along in their technological development life cycle is the fact that we have been flying aircraft by them.
And they've been flying themselves being able to land and take off for over 30 years. And so these are actually really mature technologies and aviation drones have been around in various shapes and forms for a long time.
So we have a lot of operational experience.
And the other big discriminator is the obstacle density field. So even though drones have a third dimension, they generally do not have to contend with the number of obstacles and certainly not in the proximity that driverless cars.
I would say driverless cars, I'm using that necessarily right now as a catch all for driverless and self driving, which are not exactly the same thing, but just for the purposes of this discussion, I'll lump them all together.
And I imagine the variety of foreign objects that a drone is likely to encounter is much more limited than a vehicle.
And with 20 other small Subaru's around it, with a couple of drones around it, and with some people that are walking in and around the airspace, right?
I mean you just don't have mixed, you know, this is a problem of mixed equipage. So there's just not that number of heterogeneous vehicles in the sky that planes have to contend with.
And so the radars don't have to be quite as good. LiDars don't have to be quite as good. In fact, you only see LiDars on aircraft when they get close to the ground like in helicopters.
And so there's just a sensor suite that doesn't really even need to apply to drones as like it does in driverless cars.
So if self flying and self landing planes have been around for 30 years, and you know, that's a largely solved problem or at least a mature problem.
What is the research frontier for autonomous drones?
I think it's a great question and it really all comes down to infrastructure. So how are we going to design the air traffic control for the future?
One project that we're actively working on right now is how to design new dispatch centers because air traffic controllers and dispatchers in today's world are going to be able to effectively command these vehicles directly.
This actually switches what would have been the workload to the pilot one or two pilots inside the cockpit over to the person on the ground who is trying to direct traffic.
And so that has a lot of implications. And certainly for flying cars, there's a whole new set of issues that come in.
And how do you design a cockpit, for example, for a driverless car so that a person can't do anything to override the system and either intentionally or potentially force the cybersecurity implications for both drones but also flying cars are significant.
And so there are lots of big research questions in those areas.
You mentioned some research that you're doing with NASA. Can you elaborate on that?
Right. And if you go to my website, you can read, we have a paper that was put up a few months ago. We have another one coming out any day now that really those issues surrounding it.
So just if you want to know just how close we are to having your own flying car, you can read these papers and it will lay out the complex systems engineering issues that span all the way from what are the problems with the technologies on the aircraft, which include things like electric power.
So that's kind of all the rage right now both in cars and flying cars and even drones.
But unfortunately electric vehicles are really untested in the traditional aerospace settings.
And so particularly for passenger carrying, which demands a much higher degree of safety.
So it's okay to have your battery powered drone, but you know, really do you want to send your kid to school and your battery powered drone.
Right. Right. Well, the small consumer drones, the granted the battery smaller, but the weight of the drone is also much smaller.
And you're we're at getting maybe 15, 20 minutes, a half an hour at the, you know, if you're spending a ton of money on those.
I can't imagine the, you know, a drone that's meant to carry people what the weight is there and how much battery that's going to have to require to move that.
And then what the fly time of something like that might be.
Do you have any examples of that?
Yeah, it's doable. So there, there are couple people who are out there, you know, some companies who are aurora flight sciences, who is recently acquired by Boeing.
They've got, they've had an interesting demos that could potentially carry a single person for, you know, I would say about 20 to 30 minutes flight.
And certainly there are people who are looking at these short flights right now.
But the reality is that these systems are so new and they're so untested.
And then part of the other systems engineering issues that have to be contended with are the regulatory issues.
And, you know, how, so the FAA is a very strong regulatory body with a very, very detailed set of guidelines that you must meet to put new technology in the air.
And if that's the case, and this stuff is untested, then, you know, our testing, it's not that we don't have, we know how to do it, we know how to do flying cars.
And having the testing, the technology meet the test requirements that it would take to certify them as safe and fly particularly to put a person with the Asian background in the cockpit.
We're still far away from making that kind of social revolution.
Yeah, I mean, we've been seeing, there have been YouTube videos going around of, you know, various attempts of self driving cars for many years now.
Hey, probably as long as YouTube has been around, there's been, you know, folks that, you know, claim to be, you know, just right there.
But, you know, there's still a long way to go.
Well, you know, and I think that's a good point, especially when you take into consideration that we still don't have the drone situation worked out.
So commercial drones are only allowed if they're under 55 pounds. True revolution will happen when we can get the FedEx, UPS size aircraft to be drones.
And we know how to do that. We actually know how to do that pretty well, but it's a regulatory framework that's an issue, not a technology one for driverless cars that the game changes because they are still very much basic technology issues that have yet to be solved.
And technologies are not mature. And so that's why driverless cars tend to be a much more difficult problem and their timeline is not nearly as close as many companies would like to believe they are because we simply don't have some of the basics worked out.
So you mentioned that you mentioned the, you know, some research challenges associated with building out this infrastructure.
Maybe share what's what some of the, you know, some of your work in this area is imagining, you know, it's touching on things like cooperative systems and kind of these networks of, you know, kind of a network control plan and all these distributed things.
How to, you know, effectively control these distributed things flying around those the kinds of areas that you're involved in or you tackling a different part of the problem.
Yeah, that's true. So we work a lot. So we have a major research thrust in looking at dispatch centers of the future. And that is true for drones, flying cars, driverless cars. In fact, dispatch centers of the future are a wide open area that I think you're going to see really jump up in terms of applicability for driverless cars of the future because if we're going to operate to vehicles and we will eventually get there.
The idea is what's going to happen if you have a fleet of Google cars with no steering wheel and something happens and one or more cars go down.
How are you going to fix that remotely? You're not obviously the person can't push the car over to the side of the road in most cases.
So you're going to have to have somebody remotely intervened. So we have a lot of work going into trying to figure out how do you know what that set of function allocations is going to look like.
How many dispatchers are you going to need for how many numbers of cars? What kind of contingency operations? Is this going to look like a call center, like an on star center? Is it going to look like a dispatch center where, you know, with a set of remote controls where, you know, people can pick up some kind of Xbox controller and drive are out of a stuck situation, for example.
So there's a lot of infrastructure in terms of remote vehicle management, but I would say the other big area that's related to this that we're very passionate about is explainable AI.
And this is the idea of course that you're going to have all of these systems, both drones and driverless cars have a lot of embedded AI in them.
And they're both going to have to get through various hoops in the certification and regulatory cycle and to do that regulators are going to have to have some sense of what the competency boundaries are for the underlying algorithms.
But, you know, and I'm sure most of your audience is going to appreciate this more than anyone, you know, artificial intelligence, machine learning, deep learning, these are very opaque technologies, particularly as the complexity grows. And my students do not understand what is happening in the algorithms that they're designing.
And so trying to get them to communicate what the limits of these algorithms are to people who are not experts in the field is even more challenging. So we're spending a lot of time looking at how can you design the algorithms themselves to be more explainable to people who range and expertise levels from none to some.
And then how could you communicate that information in such a way, you know, is it a visualization component, is it a what if sensitivity analysis component, how can you communicate what that algorithm can and can't do so that you start to engender some kind of trust and confidence in these algorithms.
Alright, that's a really interesting way to think about explainability when I talk about explainability with folks were typically looking at a kind of an after the fact assessment of a decision or prediction and how the network came to that or, you know, doesn't necessarily need to be a neural network in particular.
But, you know, how the machine learning or AI system came to its conclusion, but you're also using it here to encompass our ability to kind of create some kind of bounds around the, you know, the performance or accuracy or the behaviors that these systems drive for us, you know, which are a lot more, you know, doing so is a lot more complex than, you know, the traditional non probabilistic approaches where we can.
Kind of say, well, you know, it's, you know, this is going to be the range of the input, so this is going to be the range of the output and we're very confident about that.
Right, and that's actually how most complex transportation systems are certified today, basically using deterministic tests, which we know quite well that every single time we have a set of inputs, the outputs match every single time we run that test.
And probabilistic reasoning systems like you'll get an AI embedded systems, it just simply is not the case.
And this is a whole new way of thinking and one of the things that I've been very critical about of the US government is it's it's lack of expertise in this area.
And I think that this is true across the Department of Defense, even the Department of Energy.
And this is a largely a result of the sucking sound that's coming out of Silicon Valley and Detroit, taking up every possible engineer and putting a marvelous car problem.
But, you know, our government agencies simply do not have the expertise needed to understand how these autonomous systems are working, much less how to certify or regulate them.
And so this expertise problem I think is going to grow. And so that's one issue, which of course needs to be addressed not just by universities, but I think companies need to be more invested in how they support education programs.
But that's why explainable AI becomes even more important because we need to recognize that these algorithms are difficult to understand, but and maybe to develop, but that doesn't mean that we can't give people inside into how answers came about.
And I think one of the important focus that we are having this lab is we're not as interested in explaining a necessarily aggregate pattern that comes out of the data.
I mean, that's interesting and that might help you understand generally how the algorithm works, but in safety critical systems like transportation systems, what we really want to know is why did an algorithm choose to ignore some cases, right?
And are those cases the edge cases? So looking at the data that gets rejected by these algorithms in many ways is more important than looking at the aggregate data that comes out.
It strikes me that one way to think about this world that you're describing here is if you think about current processes around commercial aviation, when an accident happens and you know the systems are fairly mature, so these accidents happen very infrequently.
But there's a very rigid, regimented, established set of processes that take place that start with finding the black box, finding this thing that's going to tell us what went wrong.
And in this new world, even if we were recording all of the things that are happening and all of the inputs and outputs, that doesn't tell us exactly what went wrong because there's this probabilistic reasoning that's happening in the middle that as you said may have chosen to ignore certain inputs or prioritize other inputs.
And in order for this current regulatory framework to make sense of this new world, we've got to develop new techniques that give them more to work with, I guess, to put it simply.
Yes, you know, give people more to work with, but more, how do we understand what really amounts to huge data sets with, you know, edge, are they edge cases? Are they noise? What can we learn from that? What should we learn from it?
So, you know, it's kind of a mix of trying to understand the limits of what an algorithm is doing. How does it reason about or not about edge cases, but it also has a huge human understanding component to it too.
When we're talking about looking at thousands, tens of thousands, maybe hundreds of thousands of data points, how's a human going to understand something meaningful about that? It's not like they're not going to do this one factor at a time.
So, you know, it's a combination of high dimensional data visualization and understanding in concert with looking at the assumptions underlying many artificial intelligence algorithms and trying to get people to understand how to question that, particularly when they are trying to determine what the edges of an algorithms envelope would be.
That's maybe been a year or more since there was the big autonomous drone delivery frenzy Amazon, you know, showed their kind of concept videos and then soon after that Domino's pizza showed theirs. Do you have a kind of a crystal ball guess as to when we get there and this stuff becomes a reality.
Well, you know, that's interesting because it depends on where we sit in the regulatory cycle. So, you know, about a year ago, I would have said, well, you know, it's still going to be some time off because the regulatory framework is just still so prohibitive that's unlikely that we're going to make a lot of progress anytime soon.
But you wake up today and the Trump administration has basically decided to wipe out a set of regulations that would have prevented that so say what you will about your political affiliation.
But, you know, there's a new sheriff in town and an announcement that we are now going to be allowed to test drones at night and in beyond line of sight with a relaxed regulatory infrastructure. Now whether or not that actually happens remains to be seen.
But if he makes good on those promises of significantly reducing the regulatory barriers, then what would have taken a few years to do could actually be here in a year or less.
So, and it's but it's an interesting argument because, you know, I'm very much pro technology, but I also know the limits of these systems. And so small drones, for example, for Amazon and Domino's pizza.
One big issue that we're going to have are how these vehicles contend with high wind or even even more than high wind gusty wind situations.
So, there hasn't been a lot of work not nearly as much as there needs to be on the kind of impacts that that could have on these operational outcomes.
But, and I say this lovingly as a person who is from Tennessee, I'm a southern gal. I grew up making biscuits and shooting guns at the same time.
You know, I come from I come from the rust belt and my family is the first group of people who are going to let it Amazon drone, bring it a case of beer.
And as soon as the beer is dropped off, they're going to shoot the drone out of the sky.
And I say, why? And everyone in my family would look at me and go, why not? Right?
So, this is the idea of human behavior around these systems too. So, even if the regulatory infrastructure really loosens up for drone delivery.
And I think it is. And I think that you're going to start to see some big progress in these areas that still does not regulate human behavior.
And so, it will be interesting to me to see how humans will tolerate or not tolerate these systems, whether or not it's intentional, you know, benign malevolence pranksters or intentional malevolence.
But that even being said, you know, Amazon is not going to be able to turn over a large market share to these small delivery drones.
You're not going to look up in the sky and see drones, drones, drones. Really, these things cannot operate in bad weather.
The winds are going to be an issue. And so, I think that you will see some breakthroughs, but you're not going to start getting your tacos and pizza by air like you think you might look way.
You know, thinking about folks trying to shoot drones out of the sky, you know, makes me wonder if, you know, our vision for these delivery drones ends up looking, you know, to what degree the everyday delivery drone might need to have the same capability as something that you might expect in a military type of situation, the ability to evade, you know, attack.
Is that, you know, is that something that you are people looking at that and are you involved, especially given your background in some of the kind of research is happening around the military versions of these and is there anything there that you can share with us.
Yeah, so there are people who are looking at that. I think, you know, most of that work is done in something we called concepts of operations. So I doubt we're going to have drones that come along and like shoot you with a taser if you try to take the pizza and it's not your pizza.
Although that does bring up that could be a nice interesting set of films, Quentin Tarotino films around that. But one of the things we are looking at and we just started a big National Science Foundation effort is in the passive deterrence of drones.
So one of the issues that we are concerned about as drones start to rise in numbers both commercially, but also with recreational drones are in public spaces, we're starting to see drones become increasingly annoying.
I have a daughter, 10 year old daughter who plays soccer and there's always, you know, and it makes parents unhappy to see the pervert drone flying around filming everybody. Now, do we say that? I mean, it could be just somebody filming for fun or filming their own daughter, maybe, but the rest of the parents don't like it.
And in fact, it's illegal. And so one of the things that we're working on at Duke is developing both passive detection technologies to let managers of small outdoor spaces to let them know that a drone is nearby and or operating in their vicinity.
And we're thinking we're working with the bull Durham from the movie, the Durham Bulls. We're working with a small minor lead team to start helping them, you know, they have these same issues as well to figure out are the drones around. And then what can be done to dissuade illegal drones from operating in your area? What can you do from a landscape architecture standpoint? What can you put in the environment? How can you design the lighting systems, for example?
What if you layer in mist? So one of the things that we know about any drone with LiDAR and some computer, their cameras, they just don't work well when there's a light layer of moisture in the air.
So maybe one of the things you could do, especially on a hot summer night in North Carolina is just put some mist out there. And that might be enough to keep the drones who shouldn't be there, keep them away from your area.
Interesting.
Yeah. And so, you know, this is of a big importance right now because we need low cost solutions, not every stadium can get jammers that could potentially jam the radio frequencies that these drones are operating on.
And there's actually a lot of debate about whether or not that's actually illegal from a constitutional standpoint. So there is a lot going on in terms of, you know, trying to look at how people either on the ground or with drones are acting in potentially nefarious.
There's not going to be one great solution. There's no silver bullet here, but as a technologist in the academic setting, we really love it when one new technology comes along because the counter to that technology will come and then the counter to the counter.
Right.
It just keeps us in business. So it's all good.
Right. Right. Yeah, it's interesting. I recently saw a video and I'm sure there are many companies working on this type of thing, but it was a company that had a drone detection technology that you can kind of put up around the perimeter of your space, but also like this jamming gun that forces the drone to land.
I guess there's technology pieces of that. I guess the way this thing operated when the drone lost contact with its, you know, it's, you know, whatever its controller was and lost, lost radio contact, its instincts towards, you know, the way it was programmed was to land immediately.
But it struck me that there's that that might not be the right behavior, at least if these jamming guns become popular, you might want the drone to do something else if it loses contactors.
Like you said, it becomes a bit of a cat and mouse game, not just from a technology perspective, but the way we kind of think about and, you know, direct these drones to behave based on the kinds of things they experience in the environment.
Yes. And then of course there's some derivative effects that people don't think of, you know, assuming that the drone does take the safest option and land itself in these cases.
I mean, that's that's a win-win except that that doesn't always happen and oftentimes people will do things like that and the drone will lose its ability to maintain stability and then they fall out of the sky and then that becomes a problem.
But the idea of, you know, jamming drones and other technologies, this is an interesting issue that we're also seeing in driverless cars. And so there's been some work going on at the University of Texas at Austin to look at how easy it is for people to spoof drones GPS signals, which are also true.
It was a similar problem for driverless cars. GPS is a very vulnerable signal and it can be relatively easily tricked.
But there's a researcher, a friend of mine, Todd Humphries, who works at University of Texas at Austin, who told me that they were trying to do these field tests and they were having trouble doing the field test within the city limits of Austin.
Because so many people in Texas were driving around with GPS jammers in the back of their cars trying to make sure their bosses and wives and whoever did not know where they are, that they had trouble getting a good clean signal.
And so I think that that's an interesting case and, you know, one to be taken seriously that you don't even have to be a person who's trying to do anything bad or necessarily trying to do anything to thwart a person specifically on an individual basis.
But yet we still have some kind of paranoia in our society that then could these people driving around with GPS jammers in the back of their cars, what they then do is that they're going to screw up every other driverless car in the near vicinity from doing what they need to do.
So now we have this trickle down effect of okay, you know, how are we going to make sure that cars driverless cars who are legitimately operating the space that they should be operating in are not going to start behaving erratically, because we've got another set of people who are paranoid about what the technology is doing.
Right. Right.
I'm going to be a business for a long time. I was just going to say that's before we even get into cyber security, which you mentioned earlier.
Yeah, right. And so I think the cyber security issues again, the more we do electronically, the more of these fly by wire aircraft and drones and trains and you name it.
You know, CPS and certainly cyber physical systems, it's definitely a huge growth area, not just in academia, but in the public at large.
So I would expect if you're looking wanting to know what's the good field to go in, that's one of them.
Absolutely. One of the areas that you've done a lot of work in is you alluded to this earlier when you describe your background is kind of the human computer interface and interactions between humans and these systems.
And certainly that's been kind of an underlying thread in our conversation art to so far in terms of the way we design systems for this.
But in terms of that, you know, the humans interacting with the autonomous vehicles themselves, what kinds of things are you looking at there?
Right now, it's funny because I've been doing so much drone research for such a long time that I've become bored with it.
That's why I'm like, Lord, looking at defending against drones now because I've done so much of it. And it's funny.
I have a similar issue going on with driverless cars. I'm so bored with the idea of driver distraction and this idea of how to bring the human back into the loop of the driverless car and very fatalistic about it.
Humans are awful or terrible drivers. You just should not assume that they're going to pay attention when you need them to pay attention.
And so unless the technology can be guaranteed to jump in when it needs to jump in, you know, you cannot rely on the humans to do it.
So I've actually moved my area of research now to start looking at people outside the car.
So we have a big research effort in trying to help people who are pedestrians, bicyclists, other people, vulnerable roadside users, which we would consider to be construction crews, and state troopers, for example, first responders who are on the roadside in some way, shape, or form operating in and around self driving and driverless cars.
So looking at how to do pedestrian alerts, how to alert state troopers that they may have a car coming that may or may not be able to see them under certain atmospheric conditions, weather conditions.
So yeah, we're we're I'm really concerned about the other stakeholders in the system other than the person who is in the car who I can assure you is not paying attention.
So does that involve infrastructure like beacons or things like that or what's the direction that you see.
That's one of the things we're looking at. We're looking at how to put very low cost transmitters on a car so that it can so your cell phone can listen for it.
And so, of course, one of the big things that we see are people are heads down crossing streets. There was just a law passed in Hawaii that, you know, they're going to find you 100 bucks if you're caught crossing the street looking at your cell phone.
Wow, which is always interesting to me. I'm like, first of all, the evidence is quite clear that you can legislate that all you want. That's not going to stop people from doing it.
And secondly, good job that you're going to find them 100 bucks, for example, but they're dead because that's not really a helpful solution.
So what we're trying to do is we're trying to get car your cell phones, which are what you're looking at to note up to listen and detect the car before it gets to you and tells you to stop since you're looking at it.
We're working on very many related projects in that area, but our idea is, you know, to try to use the devices that are distracting you to help you.
And so does that is the underlying phenomenon that's driving that research direction, kind of a feeling that in order for humans to feel comfortable around this technology, though, want to know or be alerted when self-driving cars are around, or if you have a similarly fatalistic view of the self-driving cars themselves that they're not going to be able to not crash into people.
Well, no, actually, I mean, I believe that the cars, you know, enabled with this right sensors and the right sensor mix that works in all weather conditions.
I think I'm much, much more trusting of a technology solution than humans.
The real question, though, is making sure that humans, you know, we're talking about humans responding to technology as, and I think that real issue is one of reliability.
So one of the things that we know for sure is, if you're walking across the road and you're looking down at your cell phone like Google Maps, you know, that's a very common problem because you're trying to figure out where you're going and you're crossing the road.
And all of a sudden, your cell phone both showed a big stop sign and screamed at you to stop because there was a car coming and, you know, and it could detect the car from some distance away.
And you looked up and saw this car bearing down on you and jumped out of the road. I mean, that would be a really good, you would say, this technology is great.
But one of the problems that we have is that we can do that and we've actually shown that we have pretty good reliability. In some cases up to about 90%. But the question is, is that enough because humans are incredibly fickle?
So just because 9 out of 10 times, you have something correctly alerting you, it may be, even if it's wrong, 1 out of 10 times, that's enough to cause you to distrust it. So in fact, that's where our big focus of our research is right now is looking at these issues of trust.
So, where do we draw that line? How infallible does the technology have to be for people to turn it off, for example?
A funny thought jumped into my head when you, as you described this, this human looking at Google Maps and getting alerted when, you know, an object was, you know, in this case, a moving object was potentially about to collide with them, you know, we're almost turning the human into kind of a water-based self-driving vehicle.
It's like it's got navigation, it's got object detection or collision avoidance, you know, that's via the, via computing system and we're just using this, the eye is kind of the computer brain interface.
Well, I don't know if I'd call it the interface, but actually say it's the sensor, right? It's your primary sensor that most of us use in the world to get around, but it's not perfect either.
So, we are running up to the, or coming up to the top of the hour, as we close out, are there any thoughts that you'd like to leave the audience with?
No, I mean, I don't have to tell your audience how important it is that the job that they're doing and how popular it is and, you know, it's amazing as I was walking into work this morning with another computer vision professor.
You know, we were just kind of laughing at how awesome it is to be in the in crowd, you know, we're in a very hot field and it's exciting and it's growing.
But I do want people to really try to take a step back and take a deep breath and, you know, these issues around explainable AI, you know, some AI researchers like it, a lot of researchers, you know, kind of take on this, you know, negative attitude, kind of a condescending attitude.
But in the end, we've got to get this technology to a point that we can have true safety guarantees and that is not to say that's not the same thing as saying that an algorithm is provably safe, which has a different academic implication as it does to a real world connotation.
So I do want people in this field to keep pushing like they're pushing, but also take a step back every now and then and remember that in the end, you've got to communicate these results to a range of people.
And so I believe that the ideas that we're working on around explainable AI need to start coming into everybody's vernacular sooner rather than later.
Right. Right. Well, great. Well, thank you, Missy, so much for spending some time chatting with us.
It was my pleasure.
All right, everyone. That's our show for today. For more information on Missy or any of the topics covered in this episode, you'll find the show notes at twomolei.com slash talk slash 128.
Thanks so much for listening and catch you next time.
