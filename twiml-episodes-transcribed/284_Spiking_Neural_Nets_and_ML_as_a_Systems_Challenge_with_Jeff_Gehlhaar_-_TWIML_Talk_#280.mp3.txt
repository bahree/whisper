Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I'd like to start out by sending a huge thanks to Qualcomm for their support of the podcast
and for sponsoring today's episode.
As you're here in my conversation with Jeff, Qualcomm is taking a systems approach to helping
the industry address the challenges associated with AI on mobile devices and at the edge.
In support of their Snapdragon chipset family, which powers some of the latest and greatest
Android devices, Qualcomm provides their own suite of software tools and is also actively
supporting a variety of partner and industry projects, including the Android Neural Network
APIs, TensorFlow Lite, the TinyML initiative, and the Open Neural Network Exchange or Onyx
ecosystem.
To learn more about Qualcomm's AI research, platforms, developer tools, and ecosystem support,
visit twimbleai.com slash Qualcomm.
A quick community update before we dive in.
Many of you are aware that we've been hosting a couple of paper reading meetups in conjunction
with the podcast.
Well, I'm excited to share that Matt Kenny, Duke staff researcher and longtime listener
and friend of the show, has stepped up to help take this group to the next level.
The paper reading meetup will now be meeting every other Sunday at 1 p.m. U.S. Eastern
time to dissect the latest and greatest academic research papers in machine learning and AI.
If you want to take your understanding of the field to the next level, please join us
this Sunday, July 4th, or check twimbleai.com slash meetup for more upcoming community events.
We've also got a couple of study groups currently running, with one group working through
the fast.ai deep learning from the foundations course, formerly known as deep learning for
coders part two, and another working through the Stanford CS224N deep learning for natural
language processing course.
These study groups just started and will be working on these courses through October
and November respectively, so it's not too late to join in.
Sign up on the meetup page at twimbleai.com slash meetup.
Hi everyone, I am on the line with Jeff Galhar.
Jeff is VP of technology and head of AI software platforms at Qualcomm.
Jeff, welcome to this week in machine learning and AI.
Thank you very much.
I'm pleased to be here and thank you for having me, Sam.
It's great to have you on the show.
Let's jump right in and chat a little bit about your background.
You have spent 30 years at Qualcomm as far as that, right?
That's right, not in a linear sort of fashion.
I like to say I'm on my second tour of duty and we can talk about how I came and went.
But yes, I've been here a long time since the long arc of Qualcomm's innovation capacity.
So tell us a little bit about some of the things you've done at the company and how you've
come to get involved in their AI software efforts.
Yeah, perfect, perfect.
So I started as a young engineer a few years after the company got started and so I've
had the pleasure of seeing the company go through ups and downs and challenges and innovate
in first and CDMA before that and other communication vehicles and then move on to 3G and
4G and now 5G wireless standards.
And I spent a lot of my career working in wireless, both in hardware and software.
I worked on ASICs and semiconductors, but most of my career has been in systems and software
work.
And then as a part about Devastrature, the Qualcomm went through in the late 90s.
I left Qualcomm, it was end of two or one and ended up landing at a small startup.
Actually, I didn't know that they were sort of doing what we would today consider sort
of machine learning with support vector machines and so on, but that's what they were doing
and joined them in ran engineering for that company, did commercial work for the government
and for Fortune 500 companies.
And that was my first exposure to sort of AI and machine learning, let's say in the
modern age.
And so I learned a lot there and then came back to Qualcomm and with that company we sold
that company and came back to Qualcomm and then again rejoined wireless and spent a good
chunk of the last, you know, 15 years, 16 years I've been back in wireless.
And then I got a chance to, I was asked to help co lead a project that had gotten started
in spiking neural networks.
So we had made an investment in a small company.
We did a joint research program with this company.
What was that company?
The company has brain corporation, it's still going well here in San Diego.
Qualcomm Ventures took an investment in them and they've since gone on to do sort of
automated robotics and vision systems.
But we did early work with them on the pretext of and in conjunction in some sense with
the DARPA Synapse program, who was actually sort of at the end of the Synapse program,
but we were taking a lot of those similar ideas like IBM was at the time as well.
And really tried to see if we could build sort of biologically accurate systems, vision
systems in this case, in the way that we think of deep learning today, but sort of predating
sort of this Krochevsky kind of, you know, aha moment.
And when that happened, when in 2011, 2012, you know, it kind of became obvious that these
deep neural networks had, you know, hit upon something, right?
It resurrected, you know, back prop and hit upon something.
We moved the program away from spike in neural networks as all kinds of challenges.
And it subsequently, subsequently, we evolved into a deep learning and kind of program.
That was during my time in Qualcomm research.
What has happened in the last few years is that the work that we did there, I left whenever
our commercial group now run our commercial AI software.
And what is, we think of now as Qualcomm AI research has grown and continued based on
that sort of initial work.
And now we're sort of partners.
They work on long-term research and bring innovations that we then bring to products.
And we work with our SOC channels and our customers that are using our SOCs to help them, you
know, do AI on our chips.
Okay. And for folks that are interested in hearing a bit more about what's happening
from our research perspective at Qualcomm, they might be interested in checking out the
recent interview with Max Welling that is Twomel Talk 267 from back in May.
But that was a great conversation, as I'm sure this one will be.
You mentioned the spiking neural networks work.
I don't think Max and I got into any of that kind of stuff.
And it's come up, you know, maybe superficially on the show once or twice.
But I don't think I've ever got anyone to kind of share, you know, who's worked in it
to share a little bit of background on that.
Is that something you can maybe spend a few minutes on?
Sure.
Sure.
I have to dust off some old memories, but I'm going to get to do.
So okay, so the basic idea was, you know, in some sense, this basic intuition that
neural networks are made of, you know, stacks of neurons and they're interconnected
somehow.
And they're the some learning mechanisms where the strength of the connections is, you
know, somehow related to the experience, if you will, that the neurons get just like
an artificial neural network in some sense.
But the gentleman who started Braincore Eugenia Sikavic had written a book about how to mathematically
model the basic 20 or so synaptic behaviors, as we understand them in mammals, I'd say.
And the visual system is the most well sort of studied part of our cortex.
And so that was the premise of, and what I would say is that was kind of our seed moment
for Qualcomm to get started in what has now sort of become a clearly a revolution computation.
And so it was a bat, it was a really early stage bat that the next computing evolution
would be in this general direction.
And we thought, hey, look, well, the brain does this amazing thing in 20 billion neurons
and 20 watts, let's say roughly speaking, gee, could we build some kind of computing machine
that kind of works like that.
And so we did, and we had some early success, and we built a large scale like training and
simulation kind of environment.
And we did work in FPGA's to actually build hardware, made a design hardware for such
a machine.
And what it really, I think if you boil it down to where it didn't work, it worked in
a lot of ways.
But where it didn't work is back prop.
There was no sort of fundamental principled way to train these networks in a way that produced
stable output.
And so you see that maybe a little bit with GANs, you see them maybe with the struggles
of people having RNNs and related things.
But this was on a massive scale.
You could train small scale networks to do very, very simple things like be tricked into
the optical illusions that human visual cortex systems can be tricked into.
But it's an interesting use case.
Yeah.
Well, it was kind of a concept of like where we kind of on the right track.
And so you could do a very simple like, think about an intention mechanism.
If you stare at an object, your visual system will start, you know, looking for changes
in lines and edges and changes in light, and you could get the system to kind of do that
same sort of thing.
If it's stared at an object long enough, the object would sort of vanish because you
don't have any, you know, psychotic eye movement or anything to sort of stimulate the neurons.
But as you tried to build like a real thing to say a visual system or an object recognition
system at that kind of scale, they just were super hard to train.
And so there were limits and they're basically, you know, fundamental limits that systems
like ANNs with back prop basically solve for us today, right?
And what is the spiking refer to and spiking neural networks?
Yeah.
So literally the sort of the idea that in a synapse, right, in a neuron, in a biological
neuron, it is, you know, gets all this input.
Think about it like a sort of sigma delta kind of time coded input.
You get a little bit of stimulus, a little bit of stimulus.
And when it reaches some kind of threshold, depending on the kind of neuron, it fires.
It sends a signal down to the next neuron or next neurons, however it's connected.
And it's got some kind of decay function, right?
So you can think about these not being like a step function, most of them aren't.
They've got some kind of exponential sort of decay function.
So when we think of spiking, one of the intuitions was for low power.
It was like only stimulate the neurons that are implicated in some transaction, right?
And modeling them biologically in a biologically accurate way in terms of how real neurons
collect input and then, you know, fire, right, discharge their output to the next downstream
neuron.
Because of course your brain is not, really neurons are not active all at the same time.
So that's when we think of spiking, what we really mean is, you know, what is that impulse
function when you reach the, you know, exit threshold, if you will, of the neuron, what
is that impulse?
You know, what is the output?
And at what point does it reach that output, right?
Yeah.
Yeah.
And have you followed the work in this field, it still comes up.
So I imagine there's, you know, there's some progress that's been made.
Do you have a sense for where we are today?
Well, I do follow it a little bit when it comes up in various, you know, journals and
blogs and so on.
I still feel like we're at a place where in some sense we don't have a fundamentally
principled way to train these kinds of systems, and I guess over time, do we have that for
regular deep neural networks?
Well, okay.
So we could put some parameters around that, but we have backprop, so we have a structured
way of propagating these errors of tuning, right?
The strengths of biases, if you will, of all these connections.
But I think that part of it is that what I think artificial neural networks have shown
us so far, you don't need anything as complicated as actually modeling the biology of a neuron
to achieve some pretty impressive results.
In the same way that airplanes, this is probably an old analogy, but in the same way that airplanes
don't flap their wings, you know, it's likely that we can build, they're still very complicated
systems, but systems that don't work quite like the brain works and achieve maybe similar
results, right?
And so most of the work I think in spiking has moved in the direction of things like hardware
architectures that are event driven, there are quite a number of efforts in that area,
and using those kinds of architectures to basically map artificial neural networks onto substrates
that are more energy efficient or more computationally efficient in some way by applying like an
attention mechanism, right, by applying a different way of computing these things by using analog
circuitry in a lot of cases.
So different strategies where the angle is more about low power or more about, you know,
activating only the parts of the network that need to be activated at a given point in time.
So that was I think quite a digression in your bio.
Yeah.
There you go, you want the arc of how we end, how I ended up here and that's how I ended
up here.
Awesome.
Awesome.
And so today maybe dive a little bit deeper into your current role and some of the things
that Qualcomm is working on from a software perspective.
Sure.
Sure.
So as part of this going back a little in the bio as part of this early research we were
doing, we started on things like face detection and object detection and this led us to buy
some companies and set up some additional research offices and so on.
And part of that effort is we realized in this, you know, predates a lot of other movers
here, predates TensorFlow Lite and so on, we realized that in order for us even to experiment
internally to do this on Qualcomm Snapdragon SOCs, we needed some kind of toolkit to, you
know, run all these kernels on our GPU, for example, or on our DSP.
And so we started and we built what now you can go to the developer network and download
the Snapdragon neural processing SDK and but an early version and we targeted at internal
use cases mostly and for evaluating our research and it started to become clear that our customers
were coming to us and saying, Hey, you know, I've got an AI thing I want to try to run.
And I saw your demo, I didn't realize you could do this and this and this on an edge device
on a mobile device on a phone.
Can we have that toolkit basically you have a toolkit and so that led us to commercialize
a toolkit and so today my role is to lead a global organization that commercializes our
AI software stacks not just that toolkit, but our overall AI software stacks in order
to make it our hardware basically our high performance SOCs accessible to internal and external
customers who want to run, you know, AI powered solutions on our SOCs.
And so that what that really has taken a form of is that we are architecture and our chips
is heterogeneous, not every core or every use case lends itself to, you know, every piece
of hardware depending on what your use case is.
And so we provide high performance solutions principally targeting our GPU, our hexagon
DSP and in our flagship parts are HTA or tensor accelerator core.
And then we provide access, high performance access to those hardware blocks either through
our SDK or through Android NN API, which is, you know, Google Android's newest sort of
neural network API and framework.
And recently at Google IOW we announced direct work with the TensorFlow Lite team to power
the backend of TensorFlow Lite with our hexagon neural network accelerator library.
So those are examples of where we're building these high performance AI software blocks
and then exposing them via various ecosystem strategies based on what the ecosystem needs
for those use cases.
You know, when you kind of take a step back and look at this landscape with similar functionality
being exposed via these varying APIs, you mentioned a bunch of them, the Android NN APIs, TensorFlow
Lite, you've got your own stack that a developer can interact directly with as well as, you
know, that supports these other things like what's the best way to make sense of all that
if you're a developer or machine learning engineer that needs to deploy stuff out to a device,
how do you know what you should be using?
Yeah, that's a great question and we face a lot of those kinds of questions.
You know, it's hard to make a universal recommendation, but I could give a little bit of guidance.
I would say that maybe 18 months ago the question, if you'd asked a question similar to it,
which training framework do I use, because the question was really about this kind of explosion
of training frameworks that had happened, right?
We do still have questions.
Now are we talking about kind of TensorFlow versus PyTorch versus something else?
Sure, it was PyTorch, which didn't exist at the time, by the way, 18 months ago.
So a cafe to TensorFlow, now PyTorch, MXNet, CNTK from Microsoft, and then if you think
about it, you think even globally, then in Asia, there's a list, there's Chainer and
there's Parrot and there's all kinds of proprietary mace from Xiaomi, proprietary and quasi-proprietary
frameworks that are out there.
And so one of the things that we tried to do early on was to pick a few, we couldn't support
them all and provide converters from those frameworks to our SDK so that customers didn't
have to, in some sense, make some of these choices.
They could train in TensorFlow if they wanted, they could train in cafe, and then you
can convert your network from those frameworks into something we understand and it can accelerate
for you, right?
And so part of it was just making it easier.
Now in the process, Onyx came along, in part to address this issue of, look, I've got
all these training frameworks and I don't have a way to move between them and I sort of
get locked in with the operators or the techniques that that training system TensorFlow
PyTorch cafe use.
And then I have exactly this problem, oh, I want to run on this SOC, but I started in TensorFlow
and they don't support that operator in TensorFlow.
And so what would really, the advice I give people is, look, there are a lot of training
frameworks, but there's maybe three or four Macs that are really robust and popular.
A lot of the toolkits are going to find their way through one of those into either RSDK
or let's say through TensorFlow Lite onto our SOC.
And so, you know, the funnel is getting narrower, you will have a rich choice of training environments.
A lot of them are doing these eager mode, very Python-like kinds of environments.
And when you're ready to deploy to your edge device, your phone, your IoT device, your
automobile, there'll be an on-ramp, right?
And then our job is to make that network run as fast as possible, you know, power, constraint,
kind of environment that a lot of these applications face.
So in other words, the story today is more like, look, choose one of the popular frameworks
that maps to the experience that you want to have as a developer, you know, ALA, the
differences between TensorFlow versus PyTorch versus something else.
And, you know, we'll build essentially middleware that ensures that, you know, that framework
that you're using can take advantage of, transparently to you, you know, all the things that
underlying chipset is capable of.
Yeah, that's a great way to put it.
That's exactly kind of our perspective is to reduce the friction for our users, customers,
partners, to go from, you know, where they're comfortable onto high-performance solutions
on our devices.
Yeah, forget the timeframe you threw out there, but, you know, historically, it was really
all about kind of this training experience is the follow-on to that that today it's
more about inference?
Well, okay, so I'll make a twist on this.
Yes, it's about inference in the sense that there are, you know, billions of devices
that people want to run some kind of AI-powered inference solution, you know, whether it's
a camera, whether it's audio, like translation, like, you know, Google Home, like, you know,
the explosion of audio use cases we're seeing.
I would characterize that training on device and I will characterize this a little bit more
subtly, what I'll call personalization of the experience is something that I think will
become important and we're starting to see and we're starting to make investments in
that direction.
We have done it in the past.
We know it's possible.
I want to personalize like how my gallery is organized or something in my phone, okay,
right?
Pretty straightforward.
I think the next evolution of personalization will be about things like really contextualizing
the experience of my device to me, to my preferences, to how I use the device.
But we're not there yet, but I would characterize, in my view, the training aspect to be less
about, you know, big data and, you know, massive data sets and more about taking like, if
you will, a vanilla experience and then personalizing that vanilla experience out of the box
experience into something that feels a lot more customized, right, to your experience,
right?
On this personalization point, this is a really interesting point that I've been curious
about for a while.
Do you have a sense for kind of how this is done today?
You know, for example, you know, think about an app like, you know, Gmail that is doing
it the predictive replies or even now as you're typing, like predicting the end of your sentences.
You know, presumably they built out some language model there and it's able to do, you
know, it's able to predict based on, you know, tons and tons of data that they've got
of people's emails.
And that's a whole separate issue about like the privacy issues associated with that,
et cetera.
But, you know, but it also is kind of appears to be personalized to me in the sense that
I think it kind of replies in the way that I tend to reply as opposed to like just some
generalization of what everybody does.
And so how does that, you know, how are folks achieving that today, you know, with machine
learning models on devices?
Well, okay.
So I'd say that, you know, you've really hit on what I can still consider sort of a big
data use case.
And I hate to speculate exactly how Google is doing this, you know, on a behind the scenes
in Gmail.
I think the personalization on device is relatively nascent.
I think this is one of these emerging, you know, things to watch and to come back in
a year or 18 months and talk about it where it's moved.
We're seeing some, I'll put them in the personalization category and maybe people don't think
about it this way, but nascent things like, um, like the ability for you to have fingerprint
or face unlock, right?
That's a personalized thing, you have a generic feature face unlock, but it unlocks for
your face or your fingerprint, right?
Speaker identification, right?
So the ability for a device to know that you're Sam and I'm Jeff, because I've, we've
said 10 keywords or five keywords and it picks up builds a pattern, right?
It's doing that with, let's say, building some kind of classifier by using the features
of the built-in model and then running a classification pass over this, right?
These kinds of things we know are possible, we've done them ourselves on device.
What I think the next revolution will be much deeper integration.
What is the, it's not just a single app, it's like what is the sort of experience of, um,
of the device really looking at how you interact with a lot of different kinds of data and
then drawing some personalization experiences out of that, right?
So that ain't going back to the privacy thing so that in the long arc of it, these things
don't require, you know, massive amounts of cloud data, for example, right, to, in order
to get that sort of more intimate experience.
Yeah, I feel that there's like, I'm struggling with the right language to describe this problem
space.
And maybe it's because it's so new we don't have it yet, but, you know, if you or if anyone
else is listening to this, you know, does have it, you know, getting touched, but I guess
it strikes me that there are different classes of use case here, like the face ID and voice
ID strikes me as like, uh, it's a relatively simple use case, uh, you know, where you,
you kind of train this classifier and it's, I don't know, the word monolithic is, it comes
to mind.
You know, but it seems like there are, there's, these other use cases, maybe more like
Gmail, where what you want to do is kind of akin to some kind of hierarchical model where
you've got like the, it's almost like a edge transfer learning kind of thing where you've
got the met the master model, but then you want to fine tune that model based on, you
know, a set of data that is available on the device.
And maybe, maybe blurring a bunch of lines here because my Gmail data is all in the cloud.
Um, uh, but the independent of where the, the data is, there's like a data set that's
purse, that's very personal and then a model that's based on a bunch of people's data.
So Google talked a bit about this kind of approach.
It's not a new idea, but I think, you know, it may be closer to being put into practice.
They discuss a little bit at Google IO about, for example, um, G board, um, and, uh, this
I, you know, the federated learning and how do you do federated learning?
And so what I think you just described in some senses a federated learning use case,
right?
And the example they gave, I thought was, was very, uh, very interesting, you know, you
could think about it like crowdsourcing and it is that, uh, but then, you know, applying
machine learning to it, which is if some new term gets hot and a lot of users start
tweeting a hashtag or some new, you know, term gets coined because of news or politics
or who knows what?
Then, you know, in today, you might have to type that, I don't know, pick a number 50 times
or something before your learning keyboard goes, you know, this guy types his word a lot
over and over and over again.
Maybe I should remember it, right?
Right.
And if you could crowdsource in some sense, right?
If you could like update the predictive model by sourcing and we've done some research
here in privacy preserving, uh, federated learning, um, in a privacy preserving kind
of way, then you can, ideally, you know, climb that sort of hill in terms of the importance
of new terms much more quickly, right?
Maybe in hours or days.
Oh, yeah, that's super interesting, right?
And I think that that kind of application is going to be, and now this is, we're drifting
away from personalization into privacy distributed learning and so on, but there are interesting
applications, I think in like healthcare, for example, uh, similarly, um, you know,
my individual experience, um, is like a tiny, tiny fraction of the collective experience
of a lot of users using the same medicine or with the same symptoms or, you know, whatever
it is, right?
If I can aggregate that data in a privacy preserving, you know, assured kind of way, then
I get a benefit if I can share my experience with others and I can get back personalized
recommendations or I can be told, hey, look, your symptoms are a lot like this other
person over here and we just diagnosed as other person with such and so, um, you know,
you can think about other, you know, life-improving sorts of experiences that rely essentially
on the same property, which is an aggregation of a lot of individual pieces of data and
you could get a personalized in some sense, you know, experience out of that.
So we're talking about inference before, yeah, I got a little feel sorry about that.
No, no, it's awesome.
But I think that led us to, led us to personalization.
One of the questions that I had, I was at the launch for the Cloud AI100 back in April,
which is for those that don't know or don't recall Qualcomm's kind of foray into data
center inference chips or systems and I'm curious is, you know, to what degree, you know,
your teams are starting to think about how all of this, you know, software stack applies,
you know, similarly or differently to the data center devices.
Sure, sure, yeah.
So work with that team a little bit, I think the, that's a complicated, it's a complicated
answer.
So again, like the question we discussed earlier about what does it mean to, you know,
what is a recommendation for when somebody wants to run inference on an edge device and
do they start in TensorFlow or PyTorch?
All those questions and then some come up in the data center because of course you have
a really expensive capital, you know, investment in a data center and a lot of these kinds
of customers are committed to PyTorch or they're committed to TensorFlow or they're committed
to a proprietary framework.
And so we have very analogous issues in order to enable that server chip in the data center
and to provide our customers.
Again, the goal is the same.
You pick your training system, let's say TensorFlow and we want to provide a high performance,
in this case, inference in the first generation product at, you know, high density, lower
power, all the things that you heard about in the pitch and how do we do that kind of
in a frictionless sort of way.
And so let me make it maybe a few points here.
One is the problems are analogous and so we can share our experiences and we do between
how we've seen the edge inference dynamics evolve as the marketplaces become somewhat
more mature and, you know, what kinds of questions and needs customers will have as they bring
their problems to these devices.
So that's one area where I think it's again, it's very analogous and our goal will be
very similar which is provide the lowest friction path that we can for people to bring their
cloud inference tasks to our devices.
But linking a lot of stuff that Qualcomm says does does well and we're invested in and
thinking about Qualcomm as a company that does really well with end end very high complexity
system problems, right?
When you think about a wireless system, it's not just the device, the chip in the handset,
it's the standards, it's the radio, you know, protocols, it's what happens at the base
station versus what happens at the edge device, the whole thing.
When you kind of zoom out and you say, how is this going to fit with AI, then you have
a really, really very complicated and very interesting situation that I think few companies
in the world are really well positioned to sort of look at the whole thing and that's
if I've got an inference chip in the cloud or at the, let's call it heavy edge of the
radio access network and I've got a bunch of edge devices, whether it's their phones
or their IoT devices or their cars and I connect it all with 5G or Wi-Fi or both, now
what happens, right?
And so these are the kinds of problems we're starting to think about, how do we enable
customers to apportion their use case between their edge device, their, you know, maybe edge
compute and the cloud, they're going to have different latencies.
The closer you are to the edge of the 5G network, you have very, very low latencies, super
high bandwidth.
So maybe you can do things like AR and VR split rendering, very easily, right?
So you have lighter weight, you know, headsets that take advantage of massive compute, not
that far away.
Maybe similarly you can do this kind of stuff with, you know, voice translation and other
kind of hard use cases.
You do a certain amount of it on device and then you send the rest of it over 5G to another
inference solution that's not so far away in terms of latency.
These are the kinds of problems we're starting to think about to kind of go beyond just,
okay, how do we enable our customers to run on our devices in the cloud and in the edge,
but then how do we help them start to assemble, you know, real systems that take advantage
of all of these building blocks that we have to offer them?
Yeah, as we've talked about on the software side, it's not just Qualcomm, like there's
tons of initiatives.
One of them that we talked a little bit about before we started rolling here was TinyML.
What's going on with that?
What is it and what are the goals there?
We had a first ever, hopefully annual, I think it'll be annualized, a conference, code
shared between Qualcomm and Google, a couple months back, the TinyML conference and you
can look at for it and LinkedIn and there's some, you know, websites and so on that we
can link you to.
And the idea there is to think about really low power, really simple devices, right?
And so for example, working with Pete Warden at Google, he's made sort of this recent push
on sort of microcontroller-sized devices to run ML and again, at the TensorFlow summit
a few months back, he showed a real simple, you know, microcontroller-powered board that
could do a wake word detection and so on and very, very small footprint.
So this is a like a logical extension of the work that we're already doing, you know,
moving, you know, all the way from the server we were just talking about out to mobile phones
and then out even farther to, you know, microcontroller-sized devices, whether it's a sensor, temperature
sensor, you know, some of the kind of really simple device, you know, what does it take
to do machine learning there, can you run TensorFlow ultra light there, can you do it in, you
know, 30, 40, 50K of memory, you know, how many MIPS does it take?
So this requires innovation in the software frameworks, you know, how do you actually,
you know, build a library that's that small that can do something meaningful and Google
and others, they're working with, are making progress there.
What kind of hardware should you have?
So you think about, again, these really simple low power, maybe you want to device, it'll
run a year or two years or three years on a battery, right, can you do that?
This is the direction that we're doing.
We're partnering with, you know, academia and with people like Pete Warden and his team
at Google and starting to explore this and over the course of time, this will lead us
to hardware innovations and software innovations to solve, you know, machine learning problems
on these really, really small devices.
It also brings me to another point I wanted to mention that kind of going back to my roots
on around Qualcomm AI research and the product side is a lot of our research focus is on
these kinds of problems.
How do you compress a network, quantize a network, you know, compactify it, if you will,
in some way, and how do you make the software systems, the frameworks, what you call the
middleware, I think it was a good term, how do you make those lean and small so they don't
take a ball on a memory and they provide a lot of computation.
And then how do you innovate on the hardware so that it all goes together as a system?
So I've got a compression scheme that leverages something I'm doing on a hardware that takes
advantage of, you know, how my frameworks are set up and then how do I go from TensorFlow
or PyTorch through that whole chain of events to get on to a device to do a meaningful
amount of AI in a power constrained kind of situation?
I guess you said something that suggested the middleware being on device or taking up device
resources versus being part of the development stack, you know, that's running on the, you
know, the developer workstation is to what degree is that the case that, you know, as
you're layering all of these elements, you know, they're all contributing to kind of
the resource taking, you know, using resources on device.
Yeah, that's a great question.
So it's an area we focus on a lot.
And again, I want to highlight for your listeners, I think a really interesting kind of
area of innovation, something to keep an eye out on, is going back to this question
of an inference, it has been mostly the case that the pipeline has looked something like
you training your favorite framework, and this is true, not just of our frameworks, but
the other SSEs in the marketplace have very similar strategies, you training your favorite
framework, you do some amount of quantization, compression, optimization, call it what
you want, and you deploy to your edge device and that middleware, that runtime, whether
it's TensorFlow Lite or it's a proprietary one, has to take that graph, that model, and
interpret it in some way.
So whether it arrives in an open format like TensorFlow Lite's format or it's a proprietary
format, you have to read it, you have to deploy it to your hardware or accelerated whatever
run it over your blast library, whatever your acceleration mechanism is.
And so basically you're interpreting this graph on the fly, which is really handy if
you're an app developer, let's say, and you want to provide an in-store app that has
machine learning, very, very hard to predict, you know, to build an app that's tailor-made
for Snapdragon, this chip or whatever, you really want a pretty general purpose toolkit,
and that's what something like Android and an API provides.
But if I want to hit a microcontroller or I want to hit a purpose built device, like
it's a home speaker, I know what it will do, it will listen to wake words, it'll do translation
or whatever, and it'll talk to the internet for the things they can't do, talk to the
cloud.
I have a very, you know, price-conscious sort of customer, and the bomb is really important.
I want high-performance.
Bomb-gillematerials.
Billematerials, right?
Yeah.
Yeah, how much it costs them to manufacture it, and how much a consumer is willing to spend
to buy, you know, ten of them or five of them at their house, let's say.
And there, what we're heavily involved in now, let's say, particularly for our hexagon
on DSP, is compiler technology, making our middleware a lot more modular so that you can
pull out of the accelerator library just the parts you need for your use case, and compilers,
and making these things all work in conjunction with each other.
So to long answer your question, we're, I think the future world will be a little bit separated.
You'll have these cases where I won't know in advance what my model needs to do.
This is the app store use case, and I need to provide middleware that's as lean as possible,
but yet provides a very generic experience.
So I can essentially run any network that a customer might deploy to my device, and another
world of these from microcontroller up to some, you know, larger device where I have the
network in advance, I know exactly what it needs to do, and so therefore I can spend a lot
of time compiling and optimizing it like you would with a piece of software.
I can distill the network in some way, both architecturally like compressing it or quantizing
it, but then my middleware then is assembled, if you will, based only on the operators that
that network requires for that use case, right?
And so I end up with a very compact, but still high performance experience, right?
So these are other areas in which we're making investments and working with the ecosystem
to innovate, and you can expect, over the course of time, for us to provide tooling that
will allow our customers to essentially compile their networks into libraries that are
sort of custom built for their use case.
Do you envision a future where maybe, you know, the answer is that this has been happening
for years, but as opposed to the step beyond kind of compiling down to standard chipsets
or SOCs might be compiling kind of the neural network down to some kind of HGL that can
be implemented via FPGA or some other kind of hardware description that can then be produced
so that the Silicon itself is optimized for these super cost-sensitive high-volume applications
that you described earlier?
Yeah, sure.
I think, so I got a good answer for you, which is your listeners over to the TVM stack
from University of Washington.
It's now a patchy project, so you can find it there.
The idea there is, in short, there's a lot of really good innovation happening there,
but one of their outputs is, for example, a capacity to generate essentially custom hardware
for the TVM program, if you will, that it's compiling.
So that is already starting to happen, and people like Microsoft today already deploy FPGAs
into their data centers, I don't know exactly what models they run, but targeting specific
models.
Again, so it's happening, and I think we can well expect for certain use cases for
this to continue to happen.
The trade-off, of course, is that you're making a strong commitment to essentially do a
piece of hardware.
Now, an FPGA gives you flexibility, but you're really committing yourself to a particular
instantiation of a network in a market that is highly dynamic, right?
Yeah.
People are thinking that it works all the time.
So that's the trade-off, right?
And so that's the balance we're trying to strike is designing, for example, our tensor
accelerator hardware.
We've got a first generation of the marketplace now, and you can well expect it to continue
to evolve, where we're trying to find that point where we provide very high performance
experiences and the right amount of flexibility.
So going back to what I was saying earlier, OK, I'm an app developer.
I want maximum flexibility on my platform.
I want to run any neural network that has hundreds of operators in it.
OK, that's one use case.
And I want to take my same basic design and workflow.
And I want to design a home speaker, let's say, or a thermostat or whatever.
And I got a much more limited set of use cases.
But I also have more limited computer, more limited memory.
Can we serve both?
And so the compiler, the distiller, will serve the IoT, or embedded, or purpose-built
device use case.
And the same accelerator building blocks that we're building and the same accelerator hardware
will also be available for your smartphone, for your automobile, whatever, where it's not
so clear that the exact use case is kind of boiled down.
And so instead of focusing on, say, FPGA's in a home speaker, I think the direction will
be how do we use tiny ML techniques, how do we use compression, how do we innovate in
the hardware so that we can provide this high performance experience, and a design sort
of approach that meets the sort of memory, power, constraints, cost point for these use
cases?
Well, more thing that I wanted to be sure to cover, and it's related to everything that
we've been talking about, Qualcomm's been pretty active in the Onyx community.
How does that play with all of the other ecosystems that we've been discussing?
And we can take a step back and describe for folks that don't know it Onyx, and maybe
kind of an update on what's been going on with it.
That's good.
That's a good foundation.
So Onyx, I don't know, started maybe roughly a year and a half ago, something like that,
set forth by Microsoft, Facebook, and Amazon, and we were asked at the very early stages
to join, and we were, I think the first mobile SSC vendor to join, and the first mobile
SSC vendor to make Onyx converters into our accelerator framework shipping product.
So, real deep sort of interaction with Onyx, but what is the sort of idea?
The idea was to provide an open standard, if you will.
It's an open source project, interchange format, so that going back to your question about
what do we recommend to our customers in terms of how to bring inference to devices?
Well, again, going back maybe 12 or 18 months, the issue was that all these training frameworks
had different paradigms, different file formats, different ways of interchanging data.
So Onyx's kind of first kind of principal idea was, okay, let's define an interchange format,
so I can move easily from PyTorch to Cafe, or PyTorch to TensorFlow, and so on.
And so by and large, I think it's achieved that translation between these frameworks that
have different sets of operators and different assumptions is always a bit tricky.
But we do have customers that come to us and want to run Onyx produced models.
And the nice thing is that it's helped with this funnel I talked about earlier.
A lot of frameworks, chain, or others, have adopted Onyx support.
And so now the fact that we support Onyx as one of the on-ramps to our SOCs means that
we've actually opened up the number of training frameworks our customers can use.
So long as they can come through that Onyx kind of gateway.
And so we're seeing quite a bit of interest in using that as an on-ramp to our products.
And we're also chairing the Onyx Edge working group.
And so the idea there is to define in broad strokes what this goes also back to the data
center discussion.
We had the set of operators you want in a data center generally a lot richer than the
set of operators that you need in practice on an edge device, whether that's a mobile
device or IoT device.
And so part of the work of the edge working group is to define sort of a subset, a strict
and defined subset of the full set of Onyx operators that we define as sort of a conformance
set for edge.
So if you said I'm Onyx Edge 1.0 although we don't really coin this term but think about
it that way, compliant, then I support these 65 operators and they work in this way and
we can write a sort of compliance test around it, right?
And then interoperability becomes a lot more meaningful because today one of the issues
when somebody says that their Onyx compliant is okay which of the 130 plus operators
whatever the number is today, do you actually support and is that combination meaningful
for use cases that you want to deploy to the edge?
Right.
So that's our involvement.
Again, the big theme here is we want to be able to talk about what does compliance mean,
what does it mean to support AI at the edge?
How do you make it reduce the friction for our customers to do that, right?
We've covered a lot of ground, a lot of very fun digressions, I would say.
Anything else that you wanted to make sure we covered?
Look, I just appreciate the opportunity to be on the show and to on your podcast and
to reach out to your audience.
I'm really excited about where things are going, a bit of a pitch I think maybe for the
company and proud of the company and what we do.
This is a big systems problem, AI, I view it as a real paradigm shift in computing
and it's a big systems problem, but that I mean that I don't think you can just look
at just the edge or just a piece of silicon or just a piece of middleware and deduce
from that, what are the implications across the whole thing?
This is a pervasive kind of technology shift we're undergoing and it's I think fundamentally
going to change how we build computing systems and how we build all the systems that
were in all automated devices that exist today and then all the ones that are going to
be invented.
When we think about this, when your listeners interact with your podcast, I'd encourage
them to be thinking about you're going to have various experts and those experts rightfully
so focus on a slice of this.
When we think about in total an end to end system, I think it's really important if
we're going to like our focus is on high performance, low power, let's say to put it in a sound
bite, that requires huge amounts of innovation in the algorithms and architecture of networks
in the hardware, in the middleware like we talked about, that's a big problem and that's
basically the problem that we're trying to tackle that I'm deeply involved in and excited
to see where this goes, takes a lot of people, we have to work with our ecosystem partners
to make it happen, but really excited at the rate of progress we're seeing and just really
excited about what's going to come as we enable more of these capabilities for people to
innovate on.
Well Jeff, thanks so much for being on the show.
Oh, absolutely, thank you for the invitation, should do it again in some period of time
and we can reflect back on what are these predictions came true and which ones fell flat?
Absolutely, absolutely.
Thanks so much.
Okay, thank you, you have a good day.
Alright everyone, that's our show for today.
If you like what you've heard here, please do us a favor and tell your friends about the
show.
And if you haven't already hit that subscribe button yourself, make sure you do, so
you don't miss any of the great episodes we've got in store for you.
Thanks again to Qualcomm for their sponsorship of today's episode.
Check them out at twomla.com slash Qualcomm.
As always, thanks so much for listening and catch you next time.
