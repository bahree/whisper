WEBVTT

00:00.000 --> 00:11.040
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,

00:11.040 --> 00:17.560
your host, Sam Charrington. Today, I'm joined by Alyosha Oshep, a postdoc at the Technical

00:17.560 --> 00:23.840
University of Munich and Carnegie Mellon University. Before we jump into today's conversation,

00:23.840 --> 00:28.200
please be sure to take a moment to head over to Apple Podcasts or your listening platform

00:28.200 --> 00:35.240
of choice. And if you enjoy the podcast, please leave us a 5 star rating and review. Alyosha,

00:35.240 --> 00:40.040
welcome to the show. I'm looking forward to digging into our conversation. We'll be talking about

00:40.040 --> 00:47.400
your work in robotic vision and some of the papers you'll be presenting at CVPR. Before we get there,

00:47.400 --> 00:52.040
though, I'd love to have you introduce yourself to our audience and share a bit about how you came

00:52.040 --> 00:57.240
to work in the field. Yes, sure. So first, thank you very much for inviting me. I'm very excited to

00:57.240 --> 01:04.200
be here today and to have this opportunity to present or work that we are going to present

01:04.200 --> 01:11.720
at the CVPR next week. So, I will just start with a little bit of a background about myself.

01:11.720 --> 01:19.560
So, yeah, my name is Alyosha Oshep and I was born in a race and born in Slovenia, quite

01:19.560 --> 01:25.640
small country in Central Europe. As you already mentioned, at the moment, I'm working as a

01:25.640 --> 01:31.800
post-log boat at the Technical University. I'm working with Laura Ljolta-Shea and I'm also still

01:31.800 --> 01:39.960
still working at Carnegie Mellon University. I'm working with Deva Ramana. So, you mentioned that

01:39.960 --> 01:45.560
you would like to hear something about how I have gotten into computer vision research. So,

01:45.560 --> 01:49.960
just curious, would you like to hear a shorter or the longer version?

01:49.960 --> 01:57.080
The medium version is maybe the best. Medium version. Is it something that you always aspire to do?

01:59.400 --> 02:05.720
I would not say always. I think it kind of has gotten gradually to me, but I would say that from

02:05.720 --> 02:11.800
a very early days, I knew that I want to... I was really interested in computer science,

02:11.800 --> 02:18.280
right? And in programming. So, in one way or another, I was interested in that general film.

02:18.280 --> 02:23.320
Okay, what was your first exposure to vision and machine learning in AI in general?

02:23.320 --> 02:32.200
So, I would say that my first exposure to vision, ML and AI was actually at university.

02:33.960 --> 02:40.040
I had some of the courses on computer vision, computer graphics, and the machine learning already

02:40.040 --> 02:49.000
during my bachelor's studies. But even before that, I got interested in computer programming

02:49.000 --> 02:54.440
as a kid. So, at the beginning, I was interested, you know, like as a kid, of course, I was playing

02:54.440 --> 02:59.080
computer games, right? And I was just really, really fascinated with games and all these virtual

02:59.080 --> 03:03.640
worlds that come together with computer games, right? So, first thing that I really want to understand

03:03.640 --> 03:11.800
is how... When I was still quite young, right? How this comes to be, right? What is the logic behind

03:11.800 --> 03:17.880
all this? How one makes computer game and virtual worlds, and you know, like all these characters

03:17.880 --> 03:24.120
that appear in the game and behave like in somewhat intelligent way, or at least that they appear

03:24.120 --> 03:29.960
that they behave in intelligent way. So, this is really how I got into more general body

03:29.960 --> 03:39.800
direction. So, through interesting computer games and computer graphics. But when it comes to

03:39.800 --> 03:44.760
computer vision and machine learning, I think that some of the first memories I have about this

03:45.960 --> 03:52.120
come from the time when I was doing my bachelor's studies in Slovenia. So, you know, that the one

03:52.120 --> 03:56.920
way you can think about graphics is it's synthesis, right? You see in these images, right? A computer

03:56.920 --> 04:00.520
vision is kind of opposite process, right? You get images and you have to understand what these

04:00.520 --> 04:06.280
images are, what in the world has generated those images, right? And I really remember, like,

04:06.280 --> 04:12.200
back in the days I was watching, I know this is a bit nerdy, but I really liked Star Trek, right?

04:12.200 --> 04:19.240
And one of the coolest things, and one of the coolest things besides traveling around space and,

04:19.240 --> 04:24.760
you know, like meeting alien civilization was also this holiday, right? I just find it so

04:24.760 --> 04:30.920
fascinating that they have like this immersive game that just generates context content and,

04:30.920 --> 04:37.160
you know, like you can interact with characters that computer generates and so forth. So,

04:37.160 --> 04:41.560
this seemed really cool to me. But I still remember there was this one episode that I find so

04:41.560 --> 04:46.840
fascinating where they were trying to recreate something and they have some image of, I don't know,

04:46.840 --> 04:50.600
I don't remember exactly what it was, but they have some image and they just fed this to this

04:50.600 --> 04:55.000
holodeck and holodeck kind of created 3D reconstruction of that image, right? Then they were analyzing

04:55.960 --> 04:59.800
what was happening with this and all that, but I just remember this idea, you know, feeding

04:59.800 --> 05:04.760
image in and getting 2D reconstruction out, right? So I thought this was really cool, yeah.

05:04.760 --> 05:10.600
I should mention that you're doing your postdoc with Laura Layout, I say, or you mentioned that,

05:10.600 --> 05:19.640
but I should mention that I interviewed her around this time in 2018. I'm sure it was CVPR

05:19.640 --> 05:26.280
related given the timing and it sounds like in some ways you're carrying on the torch, so to speak.

05:26.280 --> 05:30.920
In talking about your research interests, it's not just computer vision, it's robot vision,

05:31.720 --> 05:34.840
you know, what kinds of problems are you trying to solve?

05:34.840 --> 05:43.080
I would say that in broader sense, I'm really interested in 3D dynamics in understanding,

05:43.080 --> 05:50.120
right? Also similar to research interests of Laura, so you assume that you have some mobile

05:50.120 --> 05:54.920
platforms such as the robot, it's equipped with some sensors, right? It can be camera, one camera,

05:54.920 --> 06:01.320
two cameras or several, or and lighter and based on whatever sensors you have, you should be able to

06:01.320 --> 06:08.840
get as complete understanding of the world as possible, right? So you should understand the

06:08.840 --> 06:14.360
3D geometry of the world, right? Both static and dynamic, right? You should know where objects are,

06:14.360 --> 06:20.120
right? Where cars are, where pedestrians are, and so forth. In addition to that, it's also important

06:20.120 --> 06:24.920
that you know how these objects move over time, right? And the main reason why it's so important to

06:24.920 --> 06:30.040
know how objects move or how objects move in the past, which really comes to multi-object tracking,

06:30.040 --> 06:34.200
but the main reason why we want to know this is that we have to make predictions about what happens

06:34.200 --> 06:38.600
in next seconds, right? You just can imagine that if you are walking around city or if you are driving

06:38.600 --> 06:45.400
a car, what you end up doing most time is navigating into free space, right? So this means that you

06:45.400 --> 06:49.400
don't only have to understand the static scene geometry, but you also have to know where everything

06:49.400 --> 06:56.680
that moves will be in a few seconds, right? So at the high level, this is what interests me a lot,

06:56.680 --> 07:05.000
but if I get more specific, I was really fascinated with one particular question in past years,

07:05.000 --> 07:13.160
already during my PhD, and I'm still working on this. So you probably know that nowadays,

07:14.360 --> 07:21.400
nowadays, I would say that object detection, tracking, and even forecasting models work quite well,

07:21.400 --> 07:26.920
all right, right? We've made a huge progress in recent years, right? But there is one important

07:26.920 --> 07:32.920
limitation here, and this is, it works great as long as you have enough data for particular

07:32.920 --> 07:40.280
semantic classes that you're trying to recognize, right? So for example, if you go out record data,

07:40.280 --> 07:44.520
you will see lots of cars and pedestrians, right? You label them, use them to train models,

07:44.520 --> 07:49.960
and everything just works fantastic, right? You can recognize cars, right? But we also have this

07:49.960 --> 07:56.280
long-tail distribution of semantic classes, right? And lots of semantic classes appear in this long-tail,

07:56.280 --> 08:02.200
which means that most objects are observed very infrequently, or maybe even never when you're

08:02.200 --> 08:06.760
collecting data sets, right? Which just means that during your model training, you don't see

08:06.760 --> 08:11.080
some particular object classes, but you still have to recognize them if you really want to navigate

08:11.080 --> 08:16.280
around the world, right? Otherwise, otherwise, you might, otherwise, it might be really dangerous,

08:16.280 --> 08:21.160
right? If you didn't recognize something because it's, it looks like not like anything you have

08:21.160 --> 08:28.120
seen before. What kind of approaches are you exploring to address this long-tail semantic detection

08:28.120 --> 08:36.440
problem? Mm-hmm. Yeah, so we explored quite quite some approaches, quite some approaches in the

08:36.440 --> 08:43.640
past. So the way I started looking into this, into this back then was so my intuition was that

08:43.640 --> 08:51.160
to track any object, we can start with something that is based on a bottom-up scene understanding,

08:51.160 --> 08:56.280
right? So current standard object detection works more in a top-down fashion, right?

08:58.360 --> 09:02.920
So existing object detection models. And bottom-up approach is more that you get image and you try

09:02.920 --> 09:10.280
to figure out how pixels group together so that you obtain object instance, right? So which pixels

09:10.280 --> 09:15.880
group together to obtain, for example, pedestrian car or something gathered, you don't know quite

09:15.880 --> 09:20.920
what it is, but you know, based on pixel similarity, you should be able to group these pixels together

09:20.920 --> 09:25.320
and then possibly, you know, realize that object is there and then track these objects, right?

09:25.320 --> 09:33.880
And that's bottom-up. What is top-down? Top-down approach is, for example, the most well-known

09:33.880 --> 09:40.920
top-down approach is, for example, faster RCNN, right? You have a bunch of windows,

09:40.920 --> 09:47.960
or we call them region proposals, right? And then based on these region proposals, you try to

09:47.960 --> 09:54.040
estimate what these proposal covers, right? Is it a car, is it a person, or is, you know,

09:54.040 --> 10:01.080
part of a background? So you don't go from bottom-up, from pixels up, but you go from top-down.

10:01.080 --> 10:06.280
So you're looking at region, yeah, exactly, exactly. So we started looking into this bottom-up

10:06.840 --> 10:17.160
approaches, but very, very soon we actually kind of started switching back to top-down data-driven

10:17.160 --> 10:24.520
methods, but instead of trying to look at region proposals and classify them as one of the

10:24.520 --> 10:29.320
object classes that were available in data sets such as, is it a car pedestrian cyclist?

10:29.320 --> 10:37.960
You would just train the tactors to just tell us whether this region likely contains object

10:37.960 --> 10:43.080
or not, right? So basically, data-driven, we started relying on data-driven object proposals

10:43.080 --> 10:48.840
to initialize tracks, right? When the queue that we were also looking into at the very beginning

10:48.840 --> 10:57.480
was depth, depth estimates, right? So when the issue with this object proposals are, is that

10:57.480 --> 11:03.720
they're quite noisy, right? You will end up having like a bunch of object proposals, some of them

11:03.720 --> 11:09.160
will actually detect objects that you have in our scenes, but many of them will also just

11:09.160 --> 11:15.960
might be firing on, you know, certain background region. But the next queue you can look into

11:15.960 --> 11:20.920
is depth. So if, you know, if you have region proposals that is kind of supported with depth,

11:20.920 --> 11:25.320
estimates, this gives you a strong queue that they're actually very likely. It's an object

11:25.320 --> 11:32.440
that you should start to start to trajectory. Maybe kind of popping back up a level or several.

11:32.440 --> 11:40.120
When you think about robots, are you primarily or robotic vision? Are you primarily thinking about

11:40.120 --> 11:45.960
autonomous vehicle types of use cases? Or do you have, do you even have a particular

11:46.840 --> 11:55.000
ideal in mind in terms of the robot itself? Always when I talk about research, I have a

11:55.000 --> 12:00.360
wider vision in mind. So I don't only think about autonomous vehicles, I think about

12:00.360 --> 12:05.480
any type of robots. And I even think that, you know, that we should have perception system

12:05.480 --> 12:12.440
that should be applicable for robots that want to drive on highways, that want to drive in the

12:12.440 --> 12:18.200
intercity, inner city areas, in potentially very crowded areas, right? Then we might have

12:18.200 --> 12:28.600
delivery robots that might even, you know, like merge into more of a pedestrian urban areas,

12:28.600 --> 12:34.520
right? Or robots that might need to navigate around warehouses, airports and so forth, right?

12:34.520 --> 12:39.560
So I kind of think that robots, the robots we need to be everywhere in the future. And I think

12:39.560 --> 12:46.520
that perception systems that we develop should be general. But one thing I definitely have to admit

12:46.520 --> 12:53.880
is that very often when you write papers, we kind of focus on autonomous driving applications

12:53.880 --> 13:01.160
and scenarios. And one reason for that is that we just simply have most datasets that were captured

13:01.160 --> 13:05.960
for benchmarking and training algorithms for autonomous driving. I would love to have more

13:05.960 --> 13:10.600
datasets, actually, more general datasets. Do you have a sense for some of the ways that

13:10.600 --> 13:17.160
focusing on autonomous vehicles based on data set availability, you know, limits applicability

13:17.160 --> 13:25.800
of the work to other types of robots? I think that there definitely is certain bias if you are

13:25.800 --> 13:33.720
only looking at autonomous vehicle datasets, right? So for example, these datasets are obviously

13:33.720 --> 13:41.320
always recorded outside, right? That were inside. You might, for example, also over a lie on the

13:41.320 --> 13:49.160
fact that you can localize yourself outside very well with GPS, for example, right? To localize

13:49.160 --> 13:56.520
your poses might be impossible indoors, right? Very often is. Then very often, if you look at

13:56.520 --> 14:02.760
limited scenarios, such as highways, for example, your diversity of objects that you will observe

14:02.760 --> 14:08.600
will actually be quite limited, right? So you learn how to recognize cars, trucks, buses,

14:08.600 --> 14:12.360
and a few other classes, and you just think you know how to handle everything, right? But

14:12.360 --> 14:19.400
this is definitely not true, right? Our usual word is very richer than that. Speaking of localization,

14:19.400 --> 14:25.320
one of the papers that you're presenting at CVPR is called text-apause, text-to-point cloud,

14:25.320 --> 14:32.120
cross-modal localization. Let's dig into that a little bit. What's the kind of big picture

14:32.120 --> 14:37.160
problem that you're trying to solve with this paper? I would actually start a bit with motivation

14:37.160 --> 14:43.560
for this or before I even do that, let me just already tell you upfront that this paper is about

14:43.560 --> 14:51.320
to localizing position within the map, 3D map of the environment based on an actual description

14:51.320 --> 14:56.120
of what this is surrounding, right? You can imagine yourself that you are somewhere in the city,

14:56.120 --> 14:59.720
you don't know where exactly you see some things around, right? There might be church in front of you,

14:59.720 --> 15:05.160
some trees or something like that, and you're explaining to your friend what you see and your

15:05.160 --> 15:09.640
friend should realize where you are based on this description, right? If your friend of course

15:09.640 --> 15:16.840
knows the rough environment of the city where you're in, right? And yeah, I imagine that in the

15:16.840 --> 15:23.560
future robots will of course be used for many things such as, for example, food delivery, right?

15:23.560 --> 15:29.240
Or no like instead of Uber drivers picking us up to transport somewhere, just robots will come

15:29.240 --> 15:37.160
to pick us up, right? And sometimes GPS tech works great, but not always, right? And one thing that

15:37.160 --> 15:42.600
for example is particularly realized is particularly frustrating usually where we are somewhere at the

15:42.600 --> 15:47.640
campus, right? Let it be technical university of Munich or Carnegie Mellon campus, we often have

15:47.640 --> 15:52.440
meetings and we use food delivery services to deliver us food, right? And you would think that this

15:52.440 --> 15:57.320
is easy to find this, right? But it turns out it's really not. So what we end up doing, for example,

15:57.320 --> 16:03.640
to if you use door dash, for example, you write a detailed description on how to reach us, right?

16:03.640 --> 16:09.000
And even after this detailed description, where, for example, Smith Hall is, where I was working

16:09.000 --> 16:14.360
until recently, they would still very often get lost at the campus and call us and ask for more

16:14.360 --> 16:22.680
directions, right? And so it's always a bit of a hassle. So this why we also envision that

16:22.680 --> 16:27.480
when the robots will take over, we also need to find a way to communicate with them where they

16:27.480 --> 16:32.120
need to go, right? And we want to do this communication in natural language because natural

16:32.120 --> 16:38.600
language is what humans are used to communicate. So this is like more like overall vision, right?

16:38.600 --> 16:45.880
Now, now if I get more to the task, this paper is addressing. So here I have to say upfront that

16:46.680 --> 16:51.080
this is one of the first investigation of this problem, right? So we had to take quite some

16:51.080 --> 16:54.920
short cuts to making this investigation of this problem physically, right?

16:54.920 --> 16:58.680
Just hearing the way you describe the problem strikes me that there are, you know,

16:58.680 --> 17:03.400
of course, multiple ways to come at it. You know, one is, you know, the paper is called

17:03.400 --> 17:09.560
Text-to-Point Cloud can also envision like text to landmark somehow trying to localize not necessarily

17:11.000 --> 17:16.040
where you are, but the landmarks that you're describing and somehow triangulate from that.

17:16.040 --> 17:18.840
You know, talk a little bit more about the way you set up the problem.

17:18.840 --> 17:26.600
Sure, sure. So you exactly, so this comment was just spot on because this is this is actually

17:26.600 --> 17:32.840
part of what the what the method does, right, triangulating landmarks, right? I mean, this is

17:32.840 --> 17:37.160
the second stage. So we have like course localization stage and file localization stage, right?

17:38.280 --> 17:42.040
So what you just mentioned comes into the fine phase. I will just first touch the course

17:42.040 --> 17:46.840
phase and then then I'll get back to this, right? So in the course phase, we basically just

17:46.840 --> 17:53.160
split the 3D point cloud of the city or of some neighborhood into rectangular tiles, right?

17:53.960 --> 17:57.960
They're, I don't remember what exactly the size is, but you know, it's like some

17:57.960 --> 18:03.240
a bit larger area that you first have to find, right? So first, you want to find like this

18:03.240 --> 18:09.080
rectangular area in where you very likely you are located, right? And when you say tiles,

18:09.080 --> 18:13.160
are you thinking like open street map tiles or something along those lines?

18:13.160 --> 18:20.840
No, we actually have in practice a lighter point cloud. So maps that were built from lighter point

18:20.840 --> 18:30.280
clouds and the data set we actually use for that is the new kitty 360 data set from a research

18:30.280 --> 18:38.040
group of Andreas Geiger. But this this data set of course contains only this point clouds maps

18:38.040 --> 18:43.560
and no textual description, right? But this is this how we actually generated data set. It's

18:44.280 --> 18:50.760
it's a different story. I'm curious, you're, you're starting assumption is are you assuming,

18:50.760 --> 18:58.040
you know, no GPS or, you know, course GPS and you're using that for the first phase and then

18:58.040 --> 19:03.560
you're using the text base for the second phase. So this is also a great question and actually

19:03.560 --> 19:12.200
we assume no GPS. The only assumption we make is that we have instance segmentations available

19:12.200 --> 19:17.800
in our map, right? So that we have, you know, we know where the houses are, we know where the trees

19:17.800 --> 19:22.520
are and so forth. So we assume instance and semantic information in our maps.

19:23.240 --> 19:28.920
And how fine-grained are your classes? Like do you just have building building or do you have

19:28.920 --> 19:36.120
church super market, campus building, whatever? No, we don't have that fine-grained information.

19:36.120 --> 19:43.400
It's more like building tree car roads and so forth. Okay, all right, so go on and finish with

19:43.400 --> 19:48.120
the first part of the method. Yeah, so so first part is just course localization. But you could

19:48.120 --> 19:53.560
also use GPS for that. That's true, but since we didn't have GPS, we just pose this as a retrieval

19:53.560 --> 19:59.000
problem, right? We have textual descriptions, we have our point-code patches, and we just learned

19:59.000 --> 20:04.120
joint embedding space for both, right? Similar to how clipworks where they aligned images and textual

20:04.120 --> 20:09.800
descriptions, but you align point clouds and textual descriptions. And based on this learned

20:09.800 --> 20:14.040
joint embedding space, you can then based on textual description get like a list of most

20:14.040 --> 20:18.440
slightly sales that contain your objects. So this is the course localization step.

20:18.440 --> 20:23.640
It strikes me that there's like a I don't know something that I think of as like a bootstrapping

20:23.640 --> 20:30.120
problem like, you know, if I'm giving directions, I assume a certain amount of knowledge that

20:30.120 --> 20:38.840
would be hard to get from instance labels like on the wash you campus or you know, some place

20:38.840 --> 20:45.960
that I am at a Starbucks or something like that. Is that kind of knowledge being introduced

20:45.960 --> 20:52.120
somewhere? Not yet, and I think this is right now the biggest limitation of our method. So

20:52.120 --> 20:58.600
to incorporate such a fine-grained information, we would have to go step forward, step further,

20:58.600 --> 21:03.560
and align our point clouds with something like you mentioned earlier, open street map, right?

21:04.440 --> 21:09.480
And then you actually get access to such fine-grained information, right? What kind of building is it,

21:09.480 --> 21:21.160
a Starbucks sign, and so forth? But in this paper, we didn't have that. I mean, we consider this

21:21.160 --> 21:25.560
as a future work, but we haven't. You have to start somewhere. But it sounds like then

21:26.840 --> 21:33.960
conclusion would be that you're trying to localize within a relatively constrained area.

21:33.960 --> 21:41.480
I'm not sure what with constrains, because in principle, this should work anywhere in the city area

21:41.480 --> 21:48.520
for which we have a map in form of point clouds, right? But of course, there is this restriction

21:48.520 --> 21:53.800
that if I just tell you that I see, I don't know, traffic side in front, church on the right,

21:53.800 --> 21:58.920
sending us here and there, right? This description fits several locations in the city, right?

21:58.920 --> 22:06.200
So you have this uncertainty, right? Of where you are. And if you wanted to make this non-ambiguous,

22:06.200 --> 22:11.080
you would really either have to go, for example, GPS, it coercely localizes you, right?

22:11.080 --> 22:15.240
Then this narrow down the search space, or you would have to have

22:16.520 --> 22:23.000
information such as, is there a Starbucks next to me, or a street name, or something like that,

22:23.000 --> 22:28.600
right? Something that really is a very narrow down the initial search state, right?

22:28.600 --> 22:33.240
Or even a dialogue-based approach where the system can identify

22:34.920 --> 22:40.600
differentiating characteristics of the three places and ask you, do you see a street sign near

22:40.600 --> 22:49.000
you or something like that? Yeah, that's also, I think it's very important that we incorporate

22:49.000 --> 22:52.760
into this process in the future, because of course, this is how conversation with humans work,

22:52.760 --> 22:55.880
right? It's not like you just described where you are and then the other person just, you know,

22:55.880 --> 23:04.120
click those, right? Right, right. Okay, so that's kind of the course part of the course phase of the

23:04.120 --> 23:11.480
method, what's the fine phase, or the second part? When it comes to the fine phase, we do

23:11.480 --> 23:16.440
pretty much exactly what you said earlier, right? So based on descriptions, find the landmarks that

23:16.440 --> 23:23.800
you refer to, right? So, you know, if you mention a sign, then, you know, the network has to realize

23:23.800 --> 23:31.160
it's, there was a sign mentioned in the sentence and align this with instance of a sign within your

23:31.160 --> 23:39.400
rectangular area, right? So you kind of match words that refer to objects with instances,

23:39.400 --> 23:50.200
and once you do this matching, you, you regress offset to, to, to, to, and is success for the model

23:50.200 --> 24:00.040
returning a list of possible locations, or you then further trying to, you know, guess or predict

24:00.040 --> 24:07.480
which of the locations the user, do you apply some kind of confidence to a list, or are you

24:07.480 --> 24:11.320
only predicting one with a confidence level, like what's the output of the model?

24:12.200 --> 24:18.200
Yes, so we predict several possible locations, and the reason for that is the course

24:18.200 --> 24:22.920
localization step that is incurring here in certain, right? So you might be in several cells,

24:22.920 --> 24:31.560
right? So you get ranked list, and then evaluation is, the evaluation is also done with respect to top

24:31.560 --> 24:41.400
K hits. You know, so you look like, you look like a localization, sorry, you look at localization

24:41.400 --> 24:48.680
retrieval success with respect to top five, top 10, or top 15, uh, top 15 matches. How did you evaluate

24:48.680 --> 24:57.640
the method? So we actually, uh, we actually evaluated a method on, um, data set that we, uh,

24:57.640 --> 25:04.040
generated on top of, uh, kitty 360. Oh, right. We've now come back to the data set

25:04.040 --> 25:09.960
conversation. So how did you create the data set? Yeah, so this, this is actually, uh, also one of

25:09.960 --> 25:15.160
the main parts of the paper, right? Because in theory, this sounds like kind of difficult,

25:15.160 --> 25:21.080
because if you need that, uh, list of, um, annotated poses and someone writing descriptions,

25:21.080 --> 25:25.640
this would take lots of time and lots of researches, lots of researches that we don't have, right?

25:25.640 --> 25:30.440
So you kind of have to find, you have to find the heck away around this. And what we ended up

25:30.440 --> 25:35.640
doing was, uh, so you know, I already mentioned we had kitty 360, list of instances and semantic

25:35.640 --> 25:40.840
meaning. And we also have, uh, color channel available, right? Because they, they align images

25:40.840 --> 25:46.280
and, and library points. So what we ended up doing was we sampled a bunch of points all around

25:46.280 --> 25:53.320
cities. And then we looked at what, uh, what, uh, what, uh, instances are in a spatial neighborhood,

25:53.320 --> 25:57.800
right? For this, you really need 3D data, right? And then if you know that, you know, that there is

25:57.800 --> 26:02.360
a house, uh, house in front of you, then you can kind of generate based on some language template,

26:02.360 --> 26:10.520
uh, description, uh, that, um, uh, that mentioned these objects, right? And, uh, you can, uh,

26:10.520 --> 26:16.840
you can also mention something about color, for example, uh, of the object that is front of you

26:16.840 --> 26:22.840
based on, uh, extracted RGB color from, from instances or, or semantic class, right? You also

26:22.840 --> 26:27.400
have semantic classes. So in a sense, it's kind of, uh, synthetically generated dataset. You

26:27.400 --> 26:33.480
pick a point and then you can come up with a list of possible descriptions that refer to that point.

26:34.360 --> 26:43.720
Exactly. Yeah. Oh, nice, nice. Is the, the model kind of end-to-end train from natural

26:43.720 --> 26:53.240
language to the, the list of predicted locations? Or are you kind of tokenizing the natural language

26:53.240 --> 26:59.160
and trying to identify the landmarks as an intermediate step? Yeah. So, so model is actually

26:59.160 --> 27:06.200
trained end-to-end, uh, I mean, okay, sure, uh, the course and the final two steps are trained

27:06.200 --> 27:11.000
independently, but other than that end-to-end. So we have, uh, you know, we have point clouds in

27:11.000 --> 27:17.080
coders, point cloud encoders and, uh, and the language encoders. Are the language or point cloud

27:17.080 --> 27:23.880
encoders are using any off-the-shelf, you know, other models, uh, for, for those tasks? Cannot

27:23.880 --> 27:30.200
think of any right now. I mean, our airing, our encoders are fairly simple. Uh, so for the, for

27:30.200 --> 27:36.920
the text, uh, we, we are actually just using LSTMs, not really any of, uh, strong seat of the art

27:36.920 --> 27:45.640
encoders. And, um, for, for point clouds, use point net. You talked about the, the dataset that you

27:45.640 --> 27:51.720
created to train the model, you know, that suggests that maybe you, you, you didn't have any external

27:51.720 --> 27:57.640
benchmarks to compare your performance to, um, but maybe you're publishing this and hoping to get

27:57.640 --> 28:05.720
other folks kind of using the same data set to, to create, uh, um, you know, kind of compete for,

28:05.720 --> 28:11.640
for best performance. Is that kind of the thinking? Mm-hmm. Yeah, this, this, this is exactly what you did,

28:11.640 --> 28:16.680
right? So when, when we started investigating this, there was no, no really data sets or, uh,

28:16.680 --> 28:24.120
or, uh, you know, prior work to, to compare with. So, um, this, uh, this is, uh, this is kind of a

28:24.120 --> 28:31.160
first investigation into, uh, into this, uh, into this topic. Yeah. And do you think your initial

28:31.160 --> 28:37.640
approach did pretty well? And, uh, you nailed it. And there's not a lot of room for folks to,

28:37.640 --> 28:44.280
to one up you or, you know, is it kind of a rough start and there's lots more room to, to go to,

28:44.280 --> 28:50.120
to do better on the dataset? Uh, yeah, I would say that absolutely the latter. I think that,

28:52.920 --> 28:54.920
I guess that's the humility test, right?

28:58.200 --> 29:04.040
Yeah, I, I actually see it more that this, this investigation, uh, that it actually opens more

29:04.040 --> 29:10.200
questions than, uh, than answers almost, uh, I mean, in the sense that, uh, um, you know,

29:10.200 --> 29:15.160
it, earlier, we just talked about this course localization step, right? For example, here, uh,

29:16.680 --> 29:20.840
this, this was one of the outcomes that this is really the bottleneck, right? If you can, of course,

29:20.840 --> 29:25.960
it localize yourself correctly, then, then you, then you become pretty accurate at localization,

29:25.960 --> 29:30.200
but, uh, uh, this, this is definitely right now on the bottleneck. And I think that this is,

29:30.200 --> 29:35.880
it will be the first thing that people will have to look into how to improve. But of course,

29:35.880 --> 29:41.640
the question is now, I mean, of course, I'm sure that people can come up with better networks,

29:41.640 --> 29:47.880
right? And the given data that we have can, uh, can, uh, uh, uh, localize, can improve localization

29:47.880 --> 29:54.120
scores, right? But there's also here just, there's also just so far you can go because there's

29:54.120 --> 29:59.240
this inherent uncertainty, right? Based on one description and not like very precise information,

29:59.240 --> 30:05.480
you could be in any of, uh, locations if you have big area to cover, right? So I think it will be

30:05.480 --> 30:11.960
really important to, uh, as we talked about earlier, right? To, uh, align this with OpenStreetMaps

30:11.960 --> 30:19.800
and, uh, and rely on, uh, more, uh, more discriminative and unique cues for, for describing

30:19.800 --> 30:27.960
location. Uh, second paper that you are presenting at CVPR is focused on forecasting from LiDAR,

30:27.960 --> 30:33.640
via future object detection, or at least that's the title of the paper and the focus is kind of on

30:33.640 --> 30:39.080
joint detection and trajectory prediction. Um, talk a little bit about the motivation for that one.

30:39.960 --> 30:47.640
Sure. Motivation here, um, comes directly from, uh, from robot navigation, right? So a, a, um,

30:49.000 --> 30:55.240
I was talking already earlier about object detection and object tracking, right? Um, so, you know,

30:55.240 --> 31:01.320
object tracking is all about understanding how, how object moved in the past, right? But if you

31:01.320 --> 31:05.640
really want to navigate, it's not important to know where objects were. You have to know where

31:05.640 --> 31:11.400
objects will be, right? And now one way you can tackle this, and this was already tackled in

31:11.400 --> 31:16.680
the community, is that, you know, you're detecting objects, you're associating them over time to

31:16.680 --> 31:21.320
get tracks and then based on past tracks, you can, you know, like train some possibly

31:21.320 --> 31:26.920
outdoor aggressive model that gives you prediction, your object will be, right? Um, but, um,

31:26.920 --> 31:32.600
there are a bunch of problems associated with that, and one of them is that object tracking is

31:32.600 --> 31:40.120
very difficult by itself, right? And the question is, even do you really need, um, the really needs

31:40.840 --> 31:46.040
object tracks to do forecasting, or could you just, you know, encode a sequence of point cloud and

31:46.040 --> 31:52.920
just train network to directly, um, detect objects and predict where they, where they will be,

31:52.920 --> 31:56.680
right? I mean, I'm not saying that tracking is not important. I think it is, but it might be that

31:57.240 --> 32:03.320
model actually implicitly learns by itself what it has to know about past positions of object.

32:03.320 --> 32:09.080
So tracking kind of becomes implicit in, in this case. In a sense, it's, it's a similar

32:09.080 --> 32:16.600
uh, bottoms up versus top down type of, uh, distinction that we were talking about earlier.

32:16.600 --> 32:21.160
The current approaches are kind of top down and that they're trying to identify object

32:21.160 --> 32:25.640
instances and then they have to track them, you know, and they might be included for periods of

32:25.640 --> 32:30.280
time and things like that. And it gets really hard. And your hope is that, hey, just feed it a bunch

32:30.280 --> 32:35.960
of point data and let the network figure it out. It's, uh, I feel that this is kind of, uh,

32:35.960 --> 32:42.920
kind of a lesson learned over and over, right? Just, uh, feed, feed enough data and let,

32:42.920 --> 32:48.120
let the network figure out things by itself. Don't, uh, don't mess with that too much. And,

32:48.120 --> 32:51.880
I mean, I also have to say that by this, I'm not saying that, uh, track is not important.

32:51.880 --> 32:57.160
Right? Tracking is important for a number of, uh, other applications, but when it comes to,

32:57.160 --> 33:04.760
to navigation, it really might be that, uh, that what we need to look into is, uh, forecasting and

33:04.760 --> 33:10.120
not necessarily so much, uh, so much tracking. Although, on the other hand, it's still nice to have

33:10.120 --> 33:17.160
this interpretability aspect of this, right? Uh, to, uh, that, that you get with, uh, with tracking.

33:18.680 --> 33:23.240
And this is maybe jumping way ahead, but, you know, maybe there's some kind of multitask

33:23.240 --> 33:29.960
objective where you're trying to do forecasting, but have tracking be kind of a byproduct of the,

33:29.960 --> 33:36.440
the network that, um, you know, has some of the benefits that multitask learning can provide.

33:36.440 --> 33:42.120
Yeah, this, this makes perfect sense, especially if this hypothesis, uh, that I mentioned earlier,

33:42.120 --> 33:47.640
that if you know how to do forecasting, you somewhat have to know some idea about tracking implicitly,

33:47.640 --> 33:53.160
right? If this hypothesis is correct, then features that we learn to do forecasting should also

33:53.160 --> 33:59.800
benefit tracking, right? So, um, I don't have answer for this yet, whether this is so, but, uh, I think

33:59.800 --> 34:04.440
this is definitely something that we should pick into. So talk a little bit about your method.

34:05.800 --> 34:11.160
Yeah, so, uh, first thing I should say is that, uh, our method is not the first one that tackles

34:11.160 --> 34:17.080
end-to-end detection and forecasting. Uh, there, there have been very interesting paper in this topic,

34:17.080 --> 34:23.800
before, for example, from a group of Raquel Lourdeson from, um, uh, University of Toronto.

34:23.800 --> 34:31.800
And, um, one thing that is, um, quite unique, uh, with our method is that, um, that it actually

34:31.800 --> 34:37.000
offers multi-future interpretation, right? Forecasting is not something that, um, I mean,

34:37.000 --> 34:40.920
forecasting is as, just as in the paper that we were talking about earlier, right? It's inherently,

34:40.920 --> 34:49.400
uh, inherently, um, ambiguous, right? Uh, based on past velocity, you could make several

34:49.400 --> 34:55.320
guess where objects will be, right? And, uh, or, or method is actually capable of that. And

34:55.320 --> 35:02.040
the way that, the way that we achieve this is actually, in the end, really simple. We just

35:02.040 --> 35:09.400
repurpose object detectors for not just detecting objects in the current frame that we just observed,

35:09.400 --> 35:16.520
but also, you know, future frames that we haven't observed yet. Um, so, you know, by encoding,

35:17.160 --> 35:21.400
by encoding temporal sequence, you can just feed the sequence to the network and then say, hey,

35:21.400 --> 35:25.960
detect objects in current frame, but also in future time steps, right? So you just have,

35:25.960 --> 35:30.600
you will have multiple detection heads for future time steps, time steps, and you have supervision

35:30.600 --> 35:37.000
for that radar, you're available, right? But, um, what is nice with this is that, uh, you can then go

35:37.000 --> 35:42.200
instead of, you know, like doing forecasting from time t into the future, you can do something

35:42.200 --> 35:48.360
different and you can go into the future and backcast, right? So from the last detection,

35:48.360 --> 35:54.200
you can backcast vector to previous frame and from there to the previous frame and so forth,

35:54.200 --> 36:02.280
and then you will also have, um, uh, one to many mapping, right? Um, so it might, it might be that,

36:02.280 --> 36:08.680
uh, to one detection from t to t minus one, two detections might connect to this one, right?

36:08.680 --> 36:13.720
In this sense, you have one to many mapping, and this kind of defines this tree of possible,

36:13.720 --> 36:19.160
possible splitting paths. And if you can connect your backcasts all the way back to particular

36:19.160 --> 36:25.720
detection, then you can say, these paths are my forecasts, right? Maybe I'm reading too much into

36:25.720 --> 36:31.320
this, but does this kind of open up some of the graphical types of tools to you? Actually,

36:31.320 --> 36:37.160
we didn't use these tools, but, uh, this, this is one thing that we were definitely thinking next,

36:37.160 --> 36:43.160
that, uh, this is also something that, uh, that we could do even more rigorously by relying on

36:43.160 --> 36:47.480
the graph neural networks, right? What, what we did was actually something, something rather

36:47.480 --> 36:52.600
simple. So we just have multiple network heads that detect objects in future frames and then we

36:52.600 --> 36:57.640
just from each detection and regress a single offset vector backwards and then just based on

36:57.640 --> 37:04.280
a credient distance, uh, say whether some detection based on back-asset vector connects with some

37:04.280 --> 37:11.480
detection from the previous frame. So in, in the end, the, uh, based on a credient distance

37:11.480 --> 37:18.200
of the regressed offset vector. For what did you use for, for data set here? Um, I'm assuming

37:18.200 --> 37:26.120
based on the description that you needed to have, uh, labeled, uh, objects in the, the frames.

37:26.120 --> 37:31.720
Yeah, that's, that's exactly correct. So we used a new since data set. So this is, uh,

37:31.720 --> 37:37.800
one of the, uh, the popular automotive data sets that, uh, that we're using nowadays.

37:38.600 --> 37:44.280
Um, and, uh, yeah, for this, we, of course, need, uh, supervision in the form of, uh, 3D bounding

37:44.280 --> 37:52.440
boxes and, uh, an object tracks, right? Um, so, so we, we, we built on, we built on that.

37:52.440 --> 38:01.320
So in the case of forecasting from LiDAR, uh, and the object detection, uh, you mentioned earlier

38:01.320 --> 38:07.720
that this is a problem that has been well studied and there are existing methods that, uh, that

38:07.720 --> 38:14.360
have been developed to solve this problem. How did your method perform relative to the existing

38:14.360 --> 38:22.920
work? Um, well, I have short and long answer. So the short answer is, it works better. Uh,

38:24.760 --> 38:31.320
okay. The, the, the long answer is, and this was also a really big part of this paper that, um,

38:31.320 --> 38:38.840
metrics used in the past for this problem, for end-to-end detection forecasting, were actually not

38:38.840 --> 38:45.080
the, the right metrics for studying this, uh, this problem. And we actually have, uh, big, big part

38:45.080 --> 38:51.320
of the paper showing this that this metrics can be trivially fooled. And we also have proposed,

38:52.040 --> 38:59.400
new metrics that, uh, that don't have, uh, that don't have, uh, those issues. Um, so I'm not sure

38:59.400 --> 39:04.600
into how much details you want me to go into this because this is something that, uh, I could,

39:04.600 --> 39:09.800
you're actually talk a lot about, but, uh, uh, uh, I'm, I'm curious. I'd like to hear a little

39:09.800 --> 39:18.840
bit more. I mean, it sounds like if the metrics can be trivially fooled, then your method

39:19.800 --> 39:23.880
performs better on both the old metrics and the new metrics. If nothing else, you could

39:23.880 --> 39:30.360
trivially, trivially fooled old metrics. Yeah. Yeah. So this is, this is correct. Or, or method

39:30.360 --> 39:35.720
work, well, on, on both metrics, what sets of metrics, but, uh, we had a, the reason, the reason

39:35.720 --> 39:43.880
why we were looking into evaluation here is, so, um, previous metrics were adopted from, um,

39:44.680 --> 39:50.040
from more traditional forecasting setting that we're studying in the past years. And the other

39:50.040 --> 39:56.040
setting was that someone gives you ground truth trajectories and based on given perfect ground

39:56.040 --> 40:03.160
truth trajectories, just, you know, uh, predict continuation, right? But this is not the case

40:03.160 --> 40:08.760
in end to end forecasting, right? Because you don't have previous trajectories. Yeah. Yeah. So in

40:08.760 --> 40:16.040
this setting, very half past trajectories, uh, they used metrics that are called, um, uh,

40:17.720 --> 40:24.040
average and absolute displacement errors, right? Um, so absolute displacement errors error is quite

40:24.040 --> 40:29.160
simply looking at you, you, you look at your prediction, you look at the ground truth prediction,

40:29.160 --> 40:33.400
and if they're close enough, then, you know, you need to go to forecast, right? But the farther

40:33.400 --> 40:38.920
way you are, you are from the ground truth, the more you are penalized, right? Uh, and, uh,

40:38.920 --> 40:45.400
another, another, um, uh, metric that is usually used, uh, uh, penalizes for, uh,

40:45.400 --> 40:51.480
penalizes, uh, false, false forecasts. So forecasts that are nowhere close to any ground truth,

40:51.480 --> 40:57.720
truth forecast. And, um, when, uh, one thing that, uh, community did when they started looking

40:57.720 --> 41:04.200
into end to end forecasting was that, um, they, uh, evaluated separately object detection.

41:05.080 --> 41:14.840
And then they evaluated, um, this absolute and average, um, uh, trajectory errors with respect

41:14.840 --> 41:21.960
to certain detection recall, right? For example, for 60 or 90 percent recall, right? And what can

41:21.960 --> 41:29.320
then interiory happens is that you train object detector in a way that the detector will focus

41:29.320 --> 41:34.680
on detecting objects for which forecasting is easy. And there is particular type of, uh,

41:34.680 --> 41:39.320
forecast for that. And this, and this is namely our objects that don't move at all, right?

41:39.320 --> 41:45.000
The, the, are just static there. And what is even more problematic is most objects don't move,

41:45.000 --> 41:49.880
right? If you're driving through the city, you see parked cars everywhere, right? And we actually

41:49.880 --> 41:56.040
have shown that it's possible to trick these metrics by a very simple baseline. And this is,

41:56.040 --> 42:02.120
so called, no object moves. Exactly. Exactly. So, so we showed that this was the best approach,

42:02.120 --> 42:05.800
right? Better than, uh, anything proposed in the literature, which, which is of course not

42:05.800 --> 42:09.960
the case, right? Those methods, uh, those methods are great. It's just that the metric was wrong.

42:09.960 --> 42:17.880
I mean, not wrong, but, um, you put the data set or for real world. Yeah. Yeah. Exactly. Exactly.

42:17.880 --> 42:24.040
So our intuition was that we should, um, somehow find a way to, you know, you know, like,

42:24.040 --> 42:30.440
not, uh, evaluation of detection and forecasting together, right? And, um, uh, intuitively,

42:30.440 --> 42:34.120
you know, if you look at object detection community, they really have great evaluation tools,

42:34.120 --> 42:41.080
right? I mean, I mean, average precision is, you know, great metric that's why this virus has,

42:41.080 --> 42:46.760
you know, survived the past test of time, right? So our intuition was that, uh, we should use

42:46.760 --> 42:53.320
in one way or another, uh, MAP, uh, mean, average precision. And it turned out that this, uh,

42:53.320 --> 42:58.760
this, we can do this quite easily. Everything that we need to do is, we need to change, um,

42:58.760 --> 43:04.040
um, the matching criterion, right? What is considered to be true positive, false positive and false

43:04.040 --> 43:12.680
negative, right? And what we said was, um, if you correctly detect an object and correctly forecast

43:12.680 --> 43:18.360
that, right? And these two, um, so that you both have correct detection and correct forecast,

43:18.360 --> 43:24.040
then you have a true positive. So this, uh, this metric we call forecasting, uh, forecasting,

43:24.040 --> 43:30.440
uh, mean average, mean average precision. So I wanted to, uh, make sure we cover the third paper

43:30.440 --> 43:38.120
as well. Uh, you are a busy guy at CVPR. That one is opening up open world tracking. Um,

43:39.080 --> 43:44.440
yeah, tell us about that paper. What's the motivation there? Yeah, gladly. So this, this is

43:44.440 --> 43:52.440
actually really paper that I'm extremely excited about because, uh, uh, because the motivation

43:52.440 --> 43:58.840
dates way back in days when I was still doing my, uh, my PhD, right? And I already touched earlier,

43:58.840 --> 44:03.720
this problem of, uh, that you need to be able to track any objects, right? We can't expect that, uh,

44:03.720 --> 44:10.280
we have training data for, uh, for everything. And during my PhD, I was working on trackers for

44:10.280 --> 44:16.600
tracking any object, but once, but here we really have fundamental problem, right? You would come

44:16.600 --> 44:20.360
up with the model and then you have to say, how good your model is, right? But how, how you're

44:20.360 --> 44:25.240
going to do that, right? Back then, a few years back, we didn't have data sets or benchmarks or,

44:25.240 --> 44:30.440
you know, even the right metrics to talk about this. So I always had to find some

44:30.440 --> 44:35.960
hacky ways to, you know, make a point that's, uh, what we are proposing works, right? But, uh,

44:35.960 --> 44:41.880
there was never like really a right rigorous way or, you know, benchmark that would really

44:41.880 --> 44:46.680
help community to rigorously evaluate method study progress, compare different methods,

44:46.680 --> 44:53.400
apple to happen. So, um, this, the motivation for this paper is really to come off with tools

44:53.400 --> 44:59.640
to do this so that community can, can make progress. And this is what this paper largely is about.

44:59.640 --> 45:04.440
So this paper doesn't really propose a drastically new method for this. It's more,

45:05.720 --> 45:11.960
more than that, it, uh, rather proposes a test bet for evaluating this and kind of, uh,

45:11.960 --> 45:17.720
we have experimental evaluation that records holidays, different, uh, different contributions

45:17.720 --> 45:23.240
in tracking, but it is stealing out a good tracker, a good simple baseline for, uh, for this task.

45:23.880 --> 45:32.200
The tracker is a metric or a tool and environment. We actually, uh, provide, um, a benchmark for

45:32.200 --> 45:39.400
this and this benchmark consists of a data set, uh, evaluation metric and baselines, right? But

45:39.400 --> 45:45.240
the data itself, it's not something that we, uh, recorded. For, for data set itself,

45:45.240 --> 45:50.280
we repurposed, uh, tau data set. It's called tracking any object data set and it comes from

45:50.280 --> 45:54.440
our, uh, collaborators, uh, back then we were collaborators when we started this, I wasn't

45:54.440 --> 46:00.200
at Carnegie Mellon yet. Uh, so this comes from the group of, uh, of, uh, Deva Ramon, uh, whom I'm,

46:00.200 --> 46:08.600
also now, uh, working with, but, um, um, tau data set by itself was released to study tracking

46:08.600 --> 46:14.040
in the long tail, right? But not in the open world. And with this paper, we repurposed it for studying,

46:15.000 --> 46:23.480
studying object tracking in the open world. And what we did was, um, we, we split training

46:23.480 --> 46:29.400
and test data as follows. We, for training the models, we proposed to use

46:29.400 --> 46:34.760
cocoa data set that has labels for 80 object classes, right? And then in tau data set, we have labels

46:34.760 --> 46:41.000
for, uh, several more, uh, objects, right? For, for hundreds of, uh, object classes. And we then

46:41.000 --> 46:47.000
define a test bed such that, uh, you train your models with knowledge of 80 classes, then you do

46:47.000 --> 46:52.920
validation on additional set of semantic classes, right? Then you have additional test set in which you

46:52.920 --> 46:57.640
have semantic classes that don't even appear on validation set, right? So that you make sure that, uh,

46:57.640 --> 47:01.800
nothing is, you know, kind of leaking from one speed or of another. And here is actually one

47:01.800 --> 47:07.240
important distinction because usually you make sure that when you have a training test and

47:07.240 --> 47:12.440
validation speed that the data does no overlap, right? And you also have to make sure that, uh,

47:12.440 --> 47:17.640
that semantic classes don't don't overlap, right? So that in each set, you have semantic classes that

47:17.640 --> 47:25.080
you can say, okay, these are the unknowns. And I, uh, I didn't train anything or tune any parameters

47:25.080 --> 47:32.280
with knowledge of, of these classes. Got it, got it, got it. So the, when you say open world here,

47:32.280 --> 47:38.200
are you using that term colloquially, or does that have a specific meaning, or does it refer to

47:38.200 --> 47:43.960
a specific environment in this context? So, so the term itself open world is of course borrows

47:43.960 --> 47:49.080
from the community, right? You, you probably know about this, uh, this classic work from, uh,

47:49.080 --> 47:56.360
um, or Bolton, uh, Bendale, right? Where they studied open world recognition. And they're,

47:56.360 --> 48:01.480
they're, they study, study open world recognition in a sense that, uh, you know,

48:01.480 --> 48:05.640
you have some certain close set of object classes in which you train your model. And then, uh,

48:05.640 --> 48:09.320
during the deployment phase, you will see new objects you haven't seen before. You have to

48:09.320 --> 48:15.960
recognize them. Then you kind of have to ask annotators to label them, right? Um, so we borrowed

48:15.960 --> 48:26.360
part of this, um, part of this idea behind this, right? So we also have some, uh, closed world

48:26.360 --> 48:32.120
data set, right? In which we have some finite set of classes labeled. And then we study, study

48:32.120 --> 48:37.160
performance of those models in the open setting, in which classes that we hadn't seen during training

48:37.160 --> 48:43.720
also appear, right? So it's describing the setting. I think I was envisioning like a simulation

48:43.720 --> 48:48.520
environment that was a world that an agent would explore kind of thing, but that's not really what

48:48.520 --> 48:54.520
we're talking about here. Did you also, so you provide this data set, uh, and this kind of benchmarking

48:54.520 --> 49:02.920
approach, did you also, uh, provide, uh, kind of proof of concept models, you know, did you kind of

49:02.920 --> 49:09.080
bootstrap the effort? And what does that look like? Yeah, so this, this I would say it's, it's

49:09.080 --> 49:15.480
something that is rather simple and builds on really on contributions from multi-object tracking

49:16.520 --> 49:22.520
community. So, uh, the tracker itself is at the end of the day really follows this tracking by

49:22.520 --> 49:27.720
detection paradigm, right? Which means that you have some kind of object detector that gives you

49:27.720 --> 49:32.680
possible object detectors in each frame. And then by some means you connect these detections

49:32.680 --> 49:38.360
over time, right? Uh, but they're of course certain important differences. And first one is that, um,

49:38.360 --> 49:43.080
you're, uh, that you cannot really, you know, train object detector for object class for which you

49:43.080 --> 49:50.200
have no labels for, right? So, um, this object detector is a bit more like object proposal generator.

49:50.200 --> 49:56.120
I touch this briefly at, uh, at the very beginning. So we just repurposed mass carsia and then,

49:56.120 --> 50:02.200
so in particular, the region proposal mechanism of mass carsia and then, but, uh, but we also made

50:02.200 --> 50:08.920
sure that we obtain, um, object instant segmentations for, for each proposal, right? And these proposals

50:08.920 --> 50:13.480
are then kind of, you know, you have lots of them in the image and they are like kind of input to

50:13.480 --> 50:20.440
the, to the tracker and then tracker, uh, figures out over time which proposals are temporally

50:20.440 --> 50:25.560
stable, right? Which it can connect over time and those will be then or, or, or object tracks.

50:25.560 --> 50:34.040
When you describe the, the simple method, uh, kind of this, this proof of concept method that you

50:34.040 --> 50:39.000
ran against the benchmark that you're proposing, it sounded like what we described as kind of this

50:39.000 --> 50:44.760
top down, uh, traditional approach where you have these objects and you're trying to track them

50:44.760 --> 50:49.880
across time. Uh, the, you know, objects from bounding boxes that you're trying to, to track across

50:49.880 --> 50:58.680
time. When we previously spoke about the, um, forecasting from LIDAR paper, we kind of contrasted

50:58.680 --> 51:04.200
that top down approach with the bottoms-up approach that, you know, starts at, you know, point cloud

51:04.200 --> 51:12.440
or in another setting, maybe pixels. And I'm kind of wondering if the methods that you developed

51:12.440 --> 51:19.640
in that paper could be a future direction for someone tackling this open world tracking problem

51:19.640 --> 51:24.200
or are they, you know, I'm really trying to test my understanding and see how they fit together

51:24.200 --> 51:30.680
or are they totally unrelated and I'm off here. No, so it's, it's, it's, uh, it's not, uh, it's not

51:30.680 --> 51:35.720
totally unrelated. So, uh, there, there are some gaps, definitely because what I was talking about

51:35.720 --> 51:41.320
earlier at the beginning, it was mainly 3D computer vision and it was, uh, largely, uh, relying on

51:41.320 --> 51:47.720
3D sensors, right? And it's opening up open world tracking is purely image-based. So, uh, here,

51:47.720 --> 51:54.760
here we haven't talked about 3D at all, but there is one thing, you, uh, that, uh, if, if I

51:54.760 --> 52:00.920
understood correctly, um, that, um, you were asking if, um, we could, uh, instead of doing this

52:00.920 --> 52:05.720
separation of detection and then linking things, uh, to do something more like we did in the previous

52:05.720 --> 52:12.280
two papers and just do everything in end-to-end manner, right? Um, and, uh, indeed, the community,

52:12.280 --> 52:17.240
community is definitely moving in direction of doing multi-object tracking in end-to-end manner.

52:17.240 --> 52:23.720
They are, they are several very nice, uh, approaches out there to do that, uh, methods that

52:23.720 --> 52:30.440
base, based on graph neural networks or just, you know, regress, uh, targets or, or use, uh,

52:30.440 --> 52:34.200
end-to-end transformer-based detectors for tracking simulation. So, there are many of them.

52:34.760 --> 52:43.080
Um, but, uh, this particular dataset and challenge, uh, makes things especially difficult,

52:43.080 --> 52:48.280
uh, because most tracking dataset before focused on tracking pedestrians or cars and pedestrians,

52:48.280 --> 52:52.680
right? And you will have a few objects in the scene and, uh, shorter sequences, and this dataset

52:52.680 --> 53:00.200
is really huge and you have jungle of objects pretty much, right? And, uh, we just, uh, haven't

53:00.200 --> 53:07.640
been able to, to apply those methods, methods to this problem, uh, this problem yet. We, you very

53:07.640 --> 53:14.040
quickly run into issue with, uh, memory and, uh, training time and, and so forth. So,

53:14.040 --> 53:21.080
I think that the method that is closer, closest to being end-to-end is, uh, is tractor,

53:21.080 --> 53:28.920
that also comes from, uh, from Laura's group. Uh, but, uh, tractor was, uh, was, uh, um,

53:29.960 --> 53:34.920
was, uh, lagging behind, uh, uh, the baseline that you proposed.

53:34.920 --> 53:41.960
Well, Aliyasha, uh, it's been really wonderful learning a bit about your research and CVPR

53:41.960 --> 53:49.160
papers. Um, we'll, of course, have links to the papers on the show notes page, uh, for folks to

53:49.160 --> 53:55.160
check out the full details, uh, but wanted to thank you for taking the time to join us and share

53:55.160 --> 54:01.640
a bit about, uh, what you've been up to. Yeah, I would, I would also like to thank you for, uh,

54:01.640 --> 54:09.320
for inviting me here. I, uh, really had a great time, uh, discussing, uh, or, or, or recent research,

54:10.360 --> 54:20.520
so it was a really nice discussion and, uh, I'm, uh, um, I hope to see you at CVPR as well and, uh,

54:20.520 --> 54:25.640
also, also encourage audience to come to, or posters or just if you see me anywhere, just

54:25.640 --> 54:32.360
wave and I'm always happy to chat about research and your end of time. Awesome. Thanks so much.

54:32.360 --> 54:57.880
Thanks. Thank you. Bye-bye.

