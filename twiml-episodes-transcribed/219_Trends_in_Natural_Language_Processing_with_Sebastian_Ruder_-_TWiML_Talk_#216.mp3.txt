Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington to close out 2018 and open the new year we're excited
to present to you our first ever AI rewind series.
In this series I interview friends of the show for their perspectives on the key developments
of 2018 as well as a look ahead at the year to come.
We'll cover a few key categories this year, namely computer vision, natural language
processing, deep learning, machine learning and reinforcement learning.
Of course we realize that there are many more possible categories than these, that there's
a ton of overlap between these topics and that no single interview could hope to cover
everything important in any of these areas.
Nonetheless we're pleased to present these talks and invite you to share your own perspectives
by commenting on the series page at twimbleai.com slash rewind 18.
In this episode of our AI rewind series we've brought back recent guest Sebastian Rooter,
PhD student at the National University of Ireland and research scientist at alien to discuss
trends in natural language processing in 2018 and beyond.
In our conversation Sebastian covers a bunch of interesting papers spanning topics such
as pre-trained language models, common sense inference data sets, large document reasoning
and more and talks through his predictions for the new year.
Enjoy.
Alright everyone, I am on the line with Sebastian Rooter.
You may remember Sebastian from our recent conversation in tumultalk number 195, Sebastian's
a PhD student in natural language processing at the National University of Ireland as well
as a research scientist at alien, Sebastian welcome back to twimble.
Um, thanks.
Great to be back.
Absolutely.
So, as folks should know by now we're doing something special for kind of the end of
year beginning of the year talking with mostly returning guests with some new guests about
what they found interesting in their fields in 2018 and what they expect to see in the
new year.
This is kind of an interesting conversation for us to have because our last conversation
was kind of this like long term look back in time for natural language processing.
But now we'll kind of focus in on the past year and really looking forward to hearing
what your kind of top, you know top developments were in the field.
So why don't we jump in and start with some of the papers that you thought were interesting
this year?
Sure.
Yeah.
I think we can start.
Yeah.
Let's maybe just start with a very uncontroversial one, I think, which is probably on a lot of
people's highlights list as well.
So that is the paper on Elmo that won the best paper award at knackle and peak conference
in the summer this year on deep contextualized word representations.
And I think that particular paper is remarkable for a lot of people just because it was one
of the first, which really demonstrated how useful these pre-trained language model representations
can be on a very diverse array of tasks.
And so I think for completeness we should definitely mention that paper here.
And for me personally, I just would like to highlight some additional aspects.
Maybe people don't really, maybe might not remember given those really superb empirical
results, which are that I personally found that the empirical analysis and the experiments
in the paper were really like exactly well done.
And I really found those that particular section of the paper really a pleasant experience
to read, particularly because I found that the authors really tried actually to really
make a good effort at understanding what is really going on in the network and watch
those particular layers in the model actually learning.
And in particular, I really liked the experiments they did on word sense dissemination and part
of speech training where they essentially took the features learned in the first and the
second layer of this bidirectional LSTM language model.
And just compared that to essentially really state of the art models and both those tasks.
And we're really able to show that just by training this deep language model, a lot of data,
you get representations that really come very close to the state of the art on, yeah,
tasks that have been widely researched.
And secondly, that highlights also that kind of a difference between the layers that in
the first layer, you would kind of get better performance on this part of speech tagging.
Also this more low level syntactic task, whereas with the second layer, the more high level
layer, you would get better performance on this more semantic words as a dissemination task.
And I think this, like, yeah, this as one particular finding was really exciting for me to
read in that paper.
And so for folks that aren't as familiar with the paper, the main thrust or contribution
of the paper was the the architecture they proposed or was it something else?
So the main contribution of that paper was essentially this kind of overall framework
of using a kind of a state of the art pre-trained language model and using that as features with
learned scalar as a using the pre-trained representations or using a convex combination
of the pre-trained representations as an input into a downstream model.
And observing that those, that this particular approach of this framework leads to, yeah,
exceptionally significant improvements on the viability of tasks like reading comprehension,
yeah, maybe anti-recommissioned sentiment analysis.
Awesome.
Awesome.
And that paper was by researchers at the Allen Institute, is that right?
Yeah, yes, exactly.
So that was building on a paper they had last year where they used these similar language
model representations just as an input into a sequence labeling model.
Yeah.
So most of the authors are from the Allen Institute of the University of Washington with the
lead author being Matthew Peters.
Yeah.
All the papers we'll be talking about are relatively fresh.
They just came out, but if you've seen people yet extending this work, there's been a lot
of conversation about this paper in the community.
Yeah, yeah.
So there's been, I mean, it's really been remarkable seeing the, and the additional developments
and really the interest in this overall direction.
So yeah, I personally had a paper with Jeremy Howard from FASTDI where we proposed a similar
idea, placing more emphasis on how we fine tune these language models for different tasks.
And then I think what has captured a lot of people's attention recently was a paper called
Birch.
So they, from Google with a pre-trained, essentially an even bigger model, a deep transformer model
that was previously proposed for machine translation on even a lot more data for a lot longer
using a lot more TVs.
And they were able just using, yeah, kind of a deeper model with some additional modifications
to outperform these previous results by a large margin again.
So that really shows, yeah, so this paper, I mean, we highlighted as one example in this
overall direction, and I think we'll probably be talking more about that in the kind of predictions
for the next year.
But I think there's still a lot of potential that it's left untapped in this direction.
Awesome, awesome.
So what's next on your list?
Cool, yeah.
So the next, the next paper I want to mention is one in a line of essentially papers on
antivirized cross-lingual models.
And I think the paper that is, yeah, particularly interesting to mention here, which was also
highlighted by the community before, so that one, one best paper award at, in the EMLP,
a large, another large NLP conference at the end of this year is the paper, Freyspace
and Neural Antivirized Machine translation by a group of researchers from Facebook I research
with the Yom Lample pioneering or spearheading that.
And that paper was, to me, particularly interesting, because there had been some earlier papers
at I clear this year, which basically proposed first this idea of antivirized machine translation.
So really, yeah, they proposed kind of three papers independently posed two models that
were maybe not for the first time, and show there's been some classic approaches before,
for the first time, with these more recent deep neural network-based approaches able
to only give a mono-lingual copper into languages, so normally when you train a machine translation
model, you really want to have a parallel, parallel corpus, which is a corpus that contains
pairs of sentences in one language and their translation in the other language.
And the models they proposed were just able to take these mono-lingual copper, which were
not aligned, and just by having these mono-lingual copper learn in alignment and learn a translation
model that is able to produce somewhat okay transitions.
So in those papers, we mainly prove a concept showing that this particular framework
of paradigm actually worked, and the results weren't still quite a bit ways below suffice
approaches, and in this kind of extension or, yeah, building on those ideas from earlier
this year, this new paper kind of took those ideas and made them a lot more robust and
useful in practice, in particular by using many ideas from classical statistical and
phrase-based machine translation models and really getting those ideas to work and really
show very good results on non-different datasets.
And so for this one are the corpora parallel at, for example, the document level, but
just not aligned sentence or phrase by phrase, or are they, let's say random kind of collection
of documents in one language and another random collection of documents in the other language.
So the, yeah, the corpora mostly random collections of documents in both languages, so essentially
I think they crawled corpora, like new scopra online or used unlabeled corpora that were
previously available for machine translation research.
So I think there's maybe some kind of topical alignment in that we have new scopra talking
about different things and probably I would, my intuition would be that if you would train
these models with totally, with corpora coming from totally different domains, like biomedical
and, yeah, new scopra they likely wouldn't do as well, but yeah, still you don't really
have any sort of alignment or the corpora itself are really easy to collect as you just
need to scrape new scopra essentially.
Is there any indication that this kind of approach can extend beyond machine translation
or cross-lingual task?
Did we learn anything in this paper that is broadly applicable to unsupervised NLP tasks?
Yes, so what I really like was that in the paper, in the beginning they took some time
essentially laying down the three key requirements for this sort of unsurvice machine translation,
which are, I think, in a sense, ones that are generally useful for many other unsupervised
tasks in NLP, and these are in particular having a good initialization, doing something
that allows you, doing essentially language modeling, so something allows you to get representations
of the language, and having something that allows you to model the inverse task or, so
in their case, they use back translation, which is a technique of getting, if you don't
have, or if you only have mono-lingual corpora available, to still get that this kind of alignment
pair that you can train your supervised models on, by just using your current or the current
state of your model to translate your mono-lingual data, and that's using that as kind of a weak
supervision signal effectively, and this sort of back translation essentially can be seen
as it allows the model to, given only those data from one of the sources really enforce
something like a cyclical consistency, so it allows the model itself essentially to
learn to be more consistent with itself.
So I guess you would, yeah, kind of people in the computer vision community have seen similar
ideas in cycle-gan, for instance, which can be seen as a similar idea.
We have also non-parallel sets of images from different domains, and the model then learns
to translate or to transfer one image from one of the domains to the other domain, basically,
also having this principle of doing this inverse modeling or enforcing this cyclical consistency.
So I think these notions are generally applicable, and then in particular as well, this notion
of having good initialization that just allows you, because the optimization process in these
unsurveys approaches can be really very noisy or can be very challenging optimization problem,
so you really want to start out from as good of an initialization as possible.
And in their case, they use, essentially, a shared word piece vocabulary, but we've also
seen there was another paper at ACL that I particularly liked by attaching and others
who proposes, basically, a better initialization for learning cross-linked embeddings by effectively
incorporating some sense of his domain knowledge of the problem, that words that are similar
to each other in other languages have similar distributions of words they are similar to
in intern.
I mean, we could talk more about this figure here, but I think we probably want to cover
a few more.
Yeah, I think we could probably do whole shows on each of the papers you're talking about,
but let's move on to the next one.
Yeah, of course, so the next one is, I think, yeah, it's also, I think, for me, in line
or an example of a larger trend that I found particularly interesting this year, which
is kind of this increasing awareness of having to incorporate common sense into our models,
so because many people have observed in different papers or different studies that the current
models are brittle or in many different ways, so that they don't actually encode information
that assumes we find plausible or that makes sense to us, so that they don't really have
this notion of like causality or of common sense, really.
And in this line, there's been some efforts mainly on creating different datasets that allow
us to train models that can exhibit some of these common, common sense inference mechanisms.
And in that line, there's been, yes, and great work coming from the University of Washington
from the Jitren Choice Group, so she had two papers on proposing two datasets, event
two mind and swag, which, yeah, kind of in both instances, create a task that tries
to capture some of these commonalities, so for swag, for instance, they used captions
from images to essentially create a multiple choice task where you have a sentence and
then four possible subsequent sentences to that, and the model then has to decide which
of those four possibilities is the most possible one or which is the actual one that would
plausibly follow the current one, and what I particularly liked about their paper was
that they really tried to take some of the lessons we've seen recently in that many of
the previously proposed datasets exhibited different sort of biases, so often the models
trained in those were able to exploit some patterns in how the human, like how the human
annotators generate the data, because often generally people who would enter those datasets
are on mechanical chalk.
They don't have a lot of time, so they often try to take shortcuts if possible to just
generate those examples faster, and those shortcuts would lead to some patterns in the
data that a model would be able to exploit, and that would then kind of show the spritleness
of these models that a lot of people have criticized, and in this swag paper, a very, very
nice acronym as well, they took some measures to project against these biases by proposing
a new mechanism that uses some other models as well to more effectively select these possible
negative answers in each example, and I think that's a really useful direction, and I think
we'll probably see kind of in future dataset papers, at least some discussion of bias,
and hopefully some measure or some means to prevent that prematurely.
Now, this is the formulation of their problem where the prediction task comes before the
contextual sentences, is that unique or novel, most of the time the prediction task follows
the contextual sentences, is that right?
So typically you would have, I mean, it's kind of rare, so you have like in most cases
in those Q&A datasets, you have some sort of like passage that they're reading and they
try to select some answer based on that, or you have some like challenging datasets
in the past, like the Reno Grad schema where you have a sentence, and then you try to disambiguate
a certain word in that sentence, like it's more kind of akin to word sense disambigation
in context, and in contrast to those, the dataset is more really about kind of this notion
of causality and really events that plausibly follow each other, so for instance in example
here, there's a sentence where which describes that a woman takes seat at the piano, so given
that real world situation, is it more likely that the woman then in the next sentence plays
with the doll, does she just smile, are there dancers in the crowd, or does she actually
put her fingers on the keys to play?
So it's really early, yeah, so this dataset very much is about really understanding kind
of these real world knowledge and understanding what kind of events follow each other in
the real world.
Okay, and just to be clear, are the sample sentences like the ones you described in that
order where you get one or multiple contextual sentences, and then the multiple choice, or
is it the other way around where you, the multiple choices, the context, and then you get
the sentences that follow and the task is to try to guess the context.
Yeah, so it's the first way, so you first have the sentence, and then you have multiple
choice questions, so understand that.
Yeah, exactly, and there have been, I mean, there's been some other datasets before as well,
I think there is one, the Lombada dataset, and a lot of those are based on stories which
give you some sentences or a paragraph of the story, and then ask you to kind of decide
either what is the next sentence or whether certain, what is certain entity in the next sentence.
So these are very kind of automatically generated based on stories, and in comparison to that,
this one really emphasized this kind of more event-based notion.
Cool, and so what's the next one on your list?
Cool, yeah, so the next one was something that flew a bit under people's radar mainly,
so I found that particularly interesting because it combines seamless-wise learning, so
learning, essentially, so these previous approaches to transfer learning that I mentioned,
they essentially learn from a large, unlabeled corpus to get pre-trained representations,
and seamless-wise learning in general often kind of assumes that the unlabeled data is
from the target task you care about, so if you have a seamless-wise learning model,
you might apply that to sentiment task, then you have some additional reviews in that domain,
basically, and a lot of these transfer learning approaches can also be used in that setting,
but this particular paper proposed some ideas that are complementary to these more
language-model-based representations in that it explicitly encourages a model to be more robust
on unlabeled data, basically, so the paper I'm talking about is the seamless-wise sequence
modeling with cross-view training that was presented at MLP 2018 from Clark and collaborators,
and they take some, so you can see their paper in line of some recent seamless-wise learning
approaches, mainly in computer vision, and in a lot of those approaches models try to enforce
some notion of consistency within the model's prediction on unlabeled data. So in a lot of cases,
we had, I think, in 2015, there was a paper called The Letter Networks, and that essentially
enforced that a model's prediction under noise. So if you have your original model and the model
where you add noise to, the predictions of those two variants should be the same on unlabeled data,
and then recently we've had some more similar approaches in that line, which try to make the model,
the model's predictions consistent with the its predictions in the past. So some of these
approaches also called self-assembling approaches. So we can see that in this design of works,
and this paper in particular proposed a very, very simple framework. In that you have your,
say, for an NLP task, you have your bilostium encoder, you feed your sentence through that and get
a prediction for your, for your labeled example. So you still train that as normally on your labeled
examples, and then on unlabeled example, you would again feed your input through that, and you
would get a prediction on your unlabeled example. So this is the prediction of your main model,
and then you would kind of iteratively or alternately obfuscate different parts of the input.
So say if the input sentence was, they traveled to Washington by plane in like a name anti-recognition
task, you would, in one auxiliary model, remove the last part of the sentence. So you would have
an auxiliary model, you would only feed in, they traveled to into your bilostium encoder and get
another prediction. Then you would do that there for other parts where you fuscate other parts of
the input like they traveled to Washington by plane. And for each of these auxiliary models,
you would get a prediction based on the model only seen parts of the input, and in the end,
you would enforce those models, those losses to be similar to the or consistent with the model
seeing the full example. And that essentially enforces that the model is kind of less sensitive
to some of these local features in the input, but really encourages the model to pay attention
to, yeah, particular words like travel that actually travel highlights some highlights that
traveling is to location and really makes model, yeah, also be able to learn from unlabeled data,
these useful patterns effectively. It kind of reminds me of dropout in the language domain as
opposed in the network domain. Yeah, exactly. So it's very similar to like a word dropout essentially,
so people have tried this like dropping out individual words in input rather than dropping
about hidden states like in record dropout. And so I think this is very similar to that. And maybe
the main difference is that in regular or in this word dropout, you're still mostly restricted
to learning on labeled examples. So if you can really only apply the dropout on just making
your model more robust given the label data that you have, and with this additional objective,
you can even, because you're not trying to predict a label, but just enforcing this sort of
consistency, you can also apply that to unlabeled data. Yeah, I think you've got a couple more papers
yet, right? Yeah, so I think maybe one of the last ones I'd like to mention is kind of in line
with, I think, one of the broader or main challenges in MLP going forward, which is really
making sense of very large documents. So we, you have that in summarization, for instance,
where if you want to summarize a very large document, that is still something that current
approaches struggle with. And similarly, if for, say, current question answering systems,
most of those approaches are based on answering questions from on a paragraph. And so really
a kind of a direction and challenge going forward is to expand this context from paragraph to
entire articles. And then hopefully to longer, yeah, larger bodies of information, a collection of
documents, or even whole narratives, like in books or movie scripts. And the paper in that
line that I want to highlight here is from some researchers from DeepMind, the narrative QA
reading comprehension challenge. That was at, yeah, was published in a tackle, the journal of
computational linguistics. And this particular paper I like, so this paper also presented in new
dataset for for question answering. And what I like particularly about this paper is that,
so generally, if you, so the dilemma really is that because current models really are only
able to do really well on, if you have a paragraph as input, if you want to propose a new
dataset or a new challenge that requires models to answer questions based on entire books,
that will be too difficult for current models. So you won't even be able to apply existing models
to that just because the task is so much different to what we have. So you really want to have
something that is both a kind of a challenge and like a more long term direction, but something that
also allows you to apply current models to the task. And I think they did a good job of striking
the balance between those two extremes. In that they had as part of their dataset, they proposed
this kind of overall challenge of answering questions about books and movie scripts.
By the questions, the way they obtained those were based on summaries of those movie scripts
and of the books. So humans were provided with a summary of the of the book or of the movie script
and had to generate possible question answer pairs based on that. So so now to make it
so in the near term to make models so to make this as easier for models, they can then also be
supplied with the summaries that were actually used to generate these question answer pairs,
which makes it then more similarly existing datasets like squat and which allows the application
of existing models to that. And then as we find that current models really are able to do very
well in that, we can then essentially scale up the context, incorporating kind of the the book
or the movie script context as well to get closer to that overall challenge. And yeah,
particularly in the paper, they also talk about so one way that you can already try to leverage
some of this entire content of the book is to use it in a two step way where you first apply
information retrieval approach to that. So you first try to retrieve in like relevant sentences
based to the question from the entire book and then use those for giving an answer. But as I say
in the paper that task because in a book, yeah, book is not something you would typically apply
IR to and you you'd have a lot of sentences that are very similar and particularly when the question
is about some main characters, you have many sentences talking about those. So it's still by itself
even a very challenging IR problem. What kind of experimental results did they present in the
paper? Yeah, I fed all. So essentially they present results both on this kind of simplified
setting where you answer questions based on the summary that the annotators use to generate
the original questions. So to that setting, they just apply existing models like existing,
say if they are reading comprehension models that have achieved good performance on some of the other
tasks. So they show those results and then basically propose this two-step approach for that I
just mentioned for using the entire book and present some results for those as well given
different different base lines and different encoder methods as well as kind of looking if you
had a perfect IR model for instance, I would be able to give the most relevant sentences given
the answer actually how good the models would actually perform then. In addition, they also simplify
the task a bit or try to give like another simpler option of the task rather than generating the
answer selecting the answer from a candid set of 30 different answers. So rather than having actually
the more challenging task of generating the answer, you can just reduce the two classification
setting which also in the near term might make it more accessible to do this sort of QA on full
stories. So some really interesting papers this year? As I said here, I think there's been really a
yeah, a lot a lot of very interesting very diverse papers and a lot of very important directions as
well. Awesome. So our next category is tools and open source projects. What what culture are here?
Cool. Yeah, so in that regard maybe yeah kind of building on what we just this has previously
what I found personally most exciting was just those research groups really open sourcing good
implementations of many of the papers that I just mentioned. So we we had yeah shortly after for
instance the the birch paper which is kind of the current set of the language modeling paper
came out a TensorFlow version together with a collaboratively notebook demo of that which people
then ported into a pie torch. So you can use it in in different frameworks and other people
already using that. So yeah, and similarly to that before you had Elmo as well. So that
being made very easily easily accessible in both an NP as well as in TensorFlow via TensorFlow Hub.
And similarly we also open sourced our old fit model in the faster iLibrary and I think just
the yeah like this whole trend of really researchers and people not taking a lot of time to
share their models with the community really allowed much of this recent kind of accelerated
progress. And I think has really kind of captured a lot of the imagination and really made it
comparatively easy now for people to actually put that put their ideas into practice and apply them.
And similarly, yeah, I was also really excited to see here the groups from
Ateche and from Facebook sharing the implementations of Ansovets machine translation
because also I think that is really very promising particularly for low use of languages.
And yeah, I'm personally really excited to see what people how people are going to build on those
implementation and really use them and apply them to their own applications. And yeah,
and nothing that I just wanted to mention here. So those were just kind of more research paper
open source implementations of those. And another trend that I particularly like this year or
just another development I liked was the one particular tool this Google dataset search.
So more so so I haven't used that tool much recently yet or it doesn't seem that that great
coverage yet for many NLP data sets or many of the most commonly used ones in academia at least.
But I think it's an instance kind of of a larger trend of really making kind of the the lifeblood
of the current models that we have this the data sets that we need to train and evaluate the models
really more easily accessible and placing those really at the fingertips of a lot of people.
And I think just start having like this data set search engine as well as some other yeah,
some other resources. So there's been some great news online on like collections of data sets
recently and also for NLP. I together with other people we've worked on compiling a repository
of different collection of different data sets as well as sales yards results in natural
language processing as well. And there's been some cool extensions and more work in that direction
recently. So I think overall yeah for me that's been really useful direction of just making those
data sets and this knowledge of what is actually the current state of the art and the current
model setup being used more accessible to the wider community. That's great. You mentioned that
this Google data set search isn't very up to date on NLP results. Is it is it manually curated or
is it you know is it kind of a search engine and what do you suppose is the causing the lag?
Yes. So I think it's mostly I mean I'm not exactly sure how it works on the under the hood.
I think a data set so people who host data sets online or so they can embed some Java script
or some schema pattern into their file which would allow the data set search to pick up on that.
But I'm happy to be corrected in that. Yeah, I'm not entirely sure where I think it's my guess
it would be automatic but not entirely sure how it works under the hood. I've just observed.
I mean it's as well kind of related to that not all of the data sets have public pages.
So a lot of the data sets are still you still have to go to
research papers to find to track down the links or actually in some case maybe even reach out
to the authors if there's some particular copyright on the data set. So I think in many cases it's
still not that standardized which would maybe make it easier for this search engine to pick up
on the particular data sets but I'm hopeful that going forward it will be easy like coverage of
a tool that will be broader or that we'd have other tools that make it easier. Jumping into the next
category commercial developments you had a particular announcement or a new product in mind.
Yeah so regarding the commercial development I think the most yeah the most striking development
this year for me the other one I was really most excited about was here from Google this the demo
they showed of Google Duplex so there which was their assistant effectively using Google Assistant
to talk and make appointments or perform certain tasks related to making appointments with
restaurants or the hairdresser so very kind of narrow scenarios and leading like following that
there were some some issues around just privacy and kind of some yeah concerns about the
just the the bot actually sharing that it is not human-operated but it is actually
bot so I think there's still some issues to be addressed around that but more broadly I found
that really kind of a very yeah like really great demonstration of I think yeah particular use case
where these sort of dialect technologies can have a big impact in the new term which is really
focusing on narrow mostly well-defined domains that you can where you can mostly
nominate most of the potential scenarios or most of the potential interactions that you can have
and where you can then apply these models to really make make people more productive or efficient
in general yeah that was absolutely stunning demo and it sounds like they've rolled it out in
at least some limited way this past month maybe yeah I think I mean yeah I've only heard in
the yes that might take place there I think here in Europe we probably have to wait a bit longer
for that yeah awesome awesome and then you've got a couple quick points on kind of top
application areas for the year what caught your eye in terms of the way folks are putting all
this all these research to work yeah so regarding so regarding application areas I think the most
impactful for me this year in like a societal and context really was anything related to dealing
with news and in particular combating the fake news problem that has really has yeah has received
a lot of attention recently and in particular there's been yeah kind of new startups and initiatives
like FAC matter for instance in London were yeah doing a really working closely with journalists
to to to fight fight these problems there's been yeah more increased attention in the community
of instance as part of different workshops so there was a workshop on fact extraction and
verification which really tried to define yeah because in most of these cases really the first step
to fighting combating that is really to build data sets first that allows to to train these models
and sharing those with the community so workshops like that or similarly there's share tasks
next year on hyper partisanship detection so I think initiatives like that will really go
long way themselves and and in addition so news was also on on my personal radar just because of
the work I've been doing here at alien on news analysis and relation extraction is also really
something that we're more trying to focus on as well and I think yeah now fake news maybe
besides addressing additional bias in models that are trained on MLP tasks I think it's really
one of the directions MLP can have the most the biggest impact in society oh yeah that's
become a clear area of opportunity for these kinds of for NLP in general so definitely agree
with that one before we run out of time I'd love to get your take on some predictions for 2019
where do you think the the biggest opportunities lay for NLP researchers and practitioners?
cool yeah so so I think yeah for me personally I think the biggest opportunities are around
transfer learning in general so really as I mentioned before these sort of pre-trained
representations where we're getting we're having right now I think with those we're still very much
at the start there's still yeah I think a lot of a lot of potential that we can leverage there
and in particular I think just for researchers or kind of practitioners who want to
do like yeah work with those who make their own have their own impact in that direction
I think yeah people shouldn't be discouraged by I mean now you've seen with these recent
approaches that like I think bird was trained on like 64 TPU parts on I think four or five days
or something so I think people shouldn't be any even that model what's now the state of the art
I think people shouldn't be discouraged by these like this data or compute requirements and I
think there's a lot of directions that still don't require you to have that many that access to
that much compute so for instance in just better understanding what these models actually capture
and what they if they can yeah if they allow us to capture things like common sense or not and
what those representations actually look like I think that that's really exciting direction and then
just applying those models to different tasks essentially and then relate to that I think
with understanding how like what they can can capture I think there's really a lot of room
for just learning and creating representations that address those particular deficiencies
so for instance there was a recent cool paper I liked from people at Facebook I researched
where they learned dedicated representations dedicated to representations for word pairs
yeah so word pairs just how to words relate to each other in general yeah and these representations
are quite useful for tasks like entailment or even quest answering where you try to relate words
in the question to the words and the paragraph words in the hypothesis towards the premise
because your model in the end has to do some cross sentence inference essentially and I think
really figuring out what is the task to care about and what is something that you that is
necessary for that task to do well on this and how can you capture that and learn that with
representations I think it's really like overall really a useful direction and something so I
think I mean for me personally we've seen now or we have like a lot of in computer vision a lot
of other disciplines things like model zoos where people have access to different sort of pre-trained
models like different types of pre-trained image net models and recently we've seen people
combining different forms of word embeddings and actually showing that different forms of word
embeddings so word embeddings for more tobacco and cloth and other ones are have complementary
information to each other so I think going forward one thing that we probably see is just having
a lot more of these pre-trained representations that encode particular information relating to
syntax, semantics and other aspects that then people can effectively mix and match and combine
for the relative tasks. That's really interesting. We spoke quite a bit about transfer learning in
NLP and the embedding stuff as well in the previous interview that I'd refer folks back to
but I can definitely see us kind of continuing to make progress down this path that's been
pretty astounding what's what we've seen this year. Yeah I totally agree. Yeah and the second
direction that I just want to quickly highlight here is that we've seen this year just a lot more
attention on the importance of evaluating things like on multiple languages and more people are
working on kind of multilingual and cross-lingual applications so I think I'm really confident or
hopeful at least that we'll see more of that next year in particularly particularly looking at
low-resist languages and yeah and even people combining kind of learning some of these
representations in a cross-lingual way so that you have a model like the recent kind of bird multilingual
model that was trained on a lot of languages and that allows them application or almost
zeros at learning for a lot of these other languages. So do you have some kind of top predictions,
some you know specific predictions for 2019 or the near future? Well okay so in terms of specific
predictions I mean I think so we'll see well it's kind of hard to be like particularly specific
I mean we'll see a lot more of like I think almost every paper that is going to achieve state of
the art on a particular NEP task will use some form of contextual or pre-trained representations
that's pretty certain besides that I yeah I would hope that if we look at like keywords of
language like of where it's mentioned in in different papers you would see at least an increase
in other languages they mentioned yeah so that's that's another one I think it's yeah it's
kind of hard to be like too yeah much more specific than that. It sounds like the thrust of your
predictions are in these kind of two main directions more cross-language work more attention to
using transfer learning. Yeah I mean and like I think anything that I mentioned before so also
the work on commonsense inference reasoning with large documents I think all of those could really be
kind of their own predictions or their own directions in the future as well so I would personally
expect to see more work in in those related directions as well and similarly I would expect or
hope to see more work generally on just developing kind of more general seamless learning algorithms
for different NEP tasks. Any other thoughts on what we might expect to see in the coming year?
Yeah so I just want to kind of call out a few just initiatives and organizations that I think
are really worth watching in the next year as well and I'm personally really excited about
yeah kind of local initiatives that are spearheading and evangelizing machine learning in AI
in particularly for kind of underrepresented groups so particularly I think the deep learning
in Daba and the team behind that which is an initiative to strengthen machine learning in
Africa has been doing some amazing work and relates to that also there's been the the Black and
the AI workshop at Neroops this year has really had an amazing attendance or similarly there's
been queer and Latinx in AI so I think yeah I really find it amazing and yeah really fruitful to see
these initiatives really getting more support and getting kind of a wider attendance and yeah
strengthening and yeah and kind of similar to that personally being in Europe as well if
on this exciting that there's been some local European initiatives Alice and Claire in particular
which try to bring kind of Europeans more closely together in kind of a joint
joint organization or joint association effectively and then I hope to see more work in
in the coming year from yeah really like open source initiatives people
open-sourcing more work and in particular yeah particularly things from faster AI for instance
which has really been doing amazing work both on integrating many people as well as even publishing
very high impact research both in computer vision as well as natural language processing.
Well Sebastian thanks so much for taking this time to chat with me and and review this past year
and what we should expect for 2019 definitely an exciting time in NLP.
Cool great yeah thanks for having me was a pleasure.
All right everyone that's our show for today for more information on Sebastian
or any of the topics covered in this episode visit twililai.com slash 216 you can also follow along
with our AI rewind 2018 series at twililai.com slash rewind 18 as always thanks so much for listening
and catch you next time happy holidays
