WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:35.600
Today we're excited to continue the AI for the Benefit of Society series that we've

00:35.600 --> 00:38.400
partnered with Microsoft to bring to you.

00:38.400 --> 00:43.160
Today we're joined by Hannah Wallach, principal researcher at Microsoft Research.

00:43.160 --> 00:48.120
Hannah and I really dig into how bias and the lack of interpretability and transparency

00:48.120 --> 00:50.560
show up across machine learning.

00:50.560 --> 00:55.400
We discuss the role that human biases, even those that are inadvertent, play intaining

00:55.400 --> 01:01.400
data, whether deployment of fair ML algorithms can actually be achieved in practice, and

01:01.400 --> 01:02.920
much more.

01:02.920 --> 01:07.640
Along the way Hannah points us to a ton of papers and resources to further explore the topic

01:07.640 --> 01:09.480
of fairness in ML.

01:09.480 --> 01:13.080
You'll definitely want to check out the show notes page for this episode, which you'll

01:13.080 --> 01:19.200
find at twimbleai.com slash talk slash 232.

01:19.200 --> 01:23.640
Before diving in, I'd like to thank Microsoft for their support of the show and their sponsorship

01:23.640 --> 01:25.640
of this series.

01:25.640 --> 01:30.020
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

01:30.020 --> 01:35.320
people around the world with this intelligent technology to help solve previously intractable

01:35.320 --> 01:41.760
societal challenges, spanning sustainability, accessibility, and humanitarian action.

01:41.760 --> 01:45.080
Learn more about their plan at Microsoft.ai.

01:45.080 --> 01:49.840
Enjoy.

01:49.840 --> 01:50.840
All right, everyone.

01:50.840 --> 01:52.680
I am on the line with Hannah Wallach.

01:52.680 --> 01:57.600
Hannah is a principal researcher at Microsoft Research in New York City.

01:57.600 --> 02:00.480
Hannah, welcome to this week in machine learning and AI.

02:00.480 --> 02:01.480
Thanks, Sam.

02:01.480 --> 02:04.480
It's really awesome to be here.

02:04.480 --> 02:11.680
It is a pleasure to have you on the show, and I'm really looking forward to this conversation.

02:11.680 --> 02:19.240
You are clearly very well known in the machine learning and AI space.

02:19.240 --> 02:24.800
Last year, you were the program chair at one of the largest conferences in the field

02:24.800 --> 02:25.800
NURPS.

02:25.800 --> 02:29.280
And in 2019, you'll be its general chair.

02:29.280 --> 02:34.760
But for those who don't know about your background, tell us a little bit about how you got involved

02:34.760 --> 02:37.400
and started in ML and AI.

02:37.400 --> 02:38.400
Sure.

02:38.400 --> 02:40.120
Absolutely.

02:40.120 --> 02:45.760
So I am a machine learning researcher by training, as you might expect.

02:45.760 --> 02:50.520
And I've been doing machine learning for about 17 years now.

02:50.520 --> 02:57.400
So since way before this stuff was even remotely fashionable or popular or cool or whatever

02:57.400 --> 02:59.640
it is nowadays.

02:59.640 --> 03:05.160
And in that time, we've really seen machine learning change a lot.

03:05.160 --> 03:10.240
It's sort of gone from this weirdo academic discipline, only of interest to nerds like me

03:10.240 --> 03:16.240
to something that's so mainstream that it's on billboards, it's in TV shows, and so on

03:16.240 --> 03:17.240
and so forth.

03:17.240 --> 03:22.080
And it's been pretty incredible to see that shift over that time.

03:22.080 --> 03:25.240
I got into machine learning sort of by accident.

03:25.240 --> 03:27.680
I think that's often what happens.

03:27.680 --> 03:35.080
I had taken some undergrad classes on information theory and stuff like that.

03:35.080 --> 03:39.840
And that to be really interesting, but thought that I was probably going to go into human

03:39.840 --> 03:42.160
computer interaction research.

03:42.160 --> 03:48.480
But through a research assistantship, during the summer between my undergrad degree and

03:48.480 --> 03:53.520
my master's degree, I ended up discovering machine learning and was completely blown away

03:53.520 --> 03:58.280
by it and realized that this is what I wanted to do.

03:58.280 --> 04:04.760
I've been focusing on machine learning in various different forms since then.

04:04.760 --> 04:11.960
My PhD was specifically on Bayesian latent variable methods, typically for analyzing text

04:11.960 --> 04:17.000
and documents, so topic models, that kind of thing.

04:17.000 --> 04:23.840
But during my PhD, I really began to realize that I'm not particularly interested in analyzing

04:23.840 --> 04:26.920
documents for the sake of analyzing documents.

04:26.920 --> 04:32.680
I'm interested in analyzing documents because humans write documents to communicate with

04:32.680 --> 04:33.840
one another.

04:33.840 --> 04:39.920
And it's really that underlying social process that I'm most interested in.

04:39.920 --> 04:47.160
So then during my postdoc, I started to shift direction from primarily looking at text

04:47.160 --> 04:53.440
and documents to thinking really about those social processes.

04:53.440 --> 04:59.480
So not just what are people saying, but also who's interacting with whom.

04:59.480 --> 05:06.480
And thinking about machine learning methods for analyzing the structure and content of social

05:06.480 --> 05:10.520
processes in combination.

05:10.520 --> 05:17.360
I then dove into this much more when I got a faculty job because I was hired as part

05:17.360 --> 05:23.200
of UMass Amherst's Computational Social Science Initiative.

05:23.200 --> 05:29.520
So at that point, I started focusing really in depth on this idea of using machine learning

05:29.520 --> 05:32.160
to study society.

05:32.160 --> 05:37.640
And I established collaborations with a number of different social scientists, focusing on

05:37.640 --> 05:40.000
a number of different topics.

05:40.000 --> 05:45.520
Over the years, I've mostly ended up working with political scientists.

05:45.520 --> 05:51.440
And often study questions relating to government transparency and still looking at this sort

05:51.440 --> 05:58.400
of whole idea of a social process consists of individuals or groups of individuals interacting

05:58.400 --> 06:04.800
with one another, information that might be used in or arising from these interactions.

06:04.800 --> 06:07.960
And then the fact that these things might change over time.

06:07.960 --> 06:14.520
And I often use one of these or two of these modalities, so structure, content, or dynamics,

06:14.520 --> 06:18.880
to learn about one or more of the other ones as well.

06:18.880 --> 06:25.880
Because I continued to work in this space, I started to think more not just about how

06:25.880 --> 06:32.320
we can use machine learning to study society, but the fact that machine learning is becoming

06:32.320 --> 06:36.160
much more prevalent within society.

06:36.160 --> 06:42.280
And about four years ago, I started really thinking more about these issues of fairness,

06:42.280 --> 06:45.760
accountability, transparency, and ethics.

06:45.760 --> 06:49.640
And it was a pretty natural fit for me to start moving in this direction.

06:49.640 --> 06:54.720
Not only was I already thinking about questions to do with people, but I've done a lot of

06:54.720 --> 06:58.960
diversity and inclusion work in my non-research life.

06:58.960 --> 07:02.720
So I'm one of the co-founders of the Women in Machine Learning workshop.

07:02.720 --> 07:08.560
I also co-founded two organizations to get more women involved in free and open-source

07:08.560 --> 07:10.440
software development.

07:10.440 --> 07:16.640
So issues related to fairness and stuff like that are really something that I tend to

07:16.640 --> 07:19.960
think about a lot in general.

07:19.960 --> 07:24.840
So I ended up making sort of this shift a little bit in my research focus, and that's

07:24.840 --> 07:32.280
not to say that I don't still work on things to do with core computational social science,

07:32.280 --> 07:39.120
but increasingly my research is focusing on the ways that machine learning impacts society.

07:39.120 --> 07:43.600
So fairness, accountability, transparency, and ethics.

07:43.600 --> 07:49.680
And we will certainly dive deep into those topics, but before we do, you've mentioned a couple

07:49.680 --> 07:52.360
of times the term computational social science.

07:52.360 --> 07:55.600
That's not a term that I've heard before, I don't believe.

07:55.600 --> 08:04.200
Can you, is that, I guess I'm curious how established that is as a field, or is it something

08:04.200 --> 08:09.280
that is specific to that institution that you are working at?

08:09.280 --> 08:10.280
Sure.

08:10.280 --> 08:19.800
So this is really a discipline that started emerging in maybe sort of 2009, 2008, that

08:19.800 --> 08:21.600
kind of time.

08:21.600 --> 08:29.760
By 2010, which is when I was hired at UMass, it really was sort of its own little emerging

08:29.760 --> 08:37.400
field with a bunch of different computer scientists and social scientists really committed to pushing

08:37.400 --> 08:39.480
this forward as a discipline.

08:39.480 --> 08:44.680
And the basic idea, of course, is that, you know, social scientists study society and social

08:44.680 --> 08:51.280
processes, and they've been doing this for decades, but often using qualitative methods.

08:51.280 --> 08:58.960
But of course, as more of society moves towards digitized interaction methods and online

08:58.960 --> 09:05.080
platforms and other kinds of things like that, we're beginning to see much more of this

09:05.080 --> 09:07.040
sort of digital data.

09:07.040 --> 09:11.440
At the same time, we've seen this massive increase, as I said, in the popularity of machine

09:11.440 --> 09:16.920
learning and machine learning methods that are really suitable for analyzing data about

09:16.920 --> 09:19.920
social processes and society.

09:19.920 --> 09:25.640
So computational social science is really this sort of emerging discipline at the intersection

09:25.640 --> 09:30.440
of computer science, the social sciences and statistics as well.

09:30.440 --> 09:36.200
And the real goal is to develop and use computational and statistical methods, so machine learning

09:36.200 --> 09:43.720
methods, for example, to understand society, social processes, and answer questions that

09:43.720 --> 09:47.680
are substantively interesting to social scientists.

09:47.680 --> 09:53.960
At this point, there are people at a number of different institutions focusing on computational

09:53.960 --> 09:54.960
social science.

09:54.960 --> 10:02.120
So yes, of course, UMass, as I mentioned before, but also Northwestern, Northeastern, University

10:02.120 --> 10:03.120
of Washington.

10:03.120 --> 10:07.640
In fact, we've been doing this for years, and of course, Microsoft Research is no exception

10:07.640 --> 10:08.960
in this regard.

10:08.960 --> 10:14.880
Part of the reason why I joined Microsoft Research was that we have a truly exceptional group

10:14.880 --> 10:20.240
of researchers in computational social science here, and that was really very appealing to

10:20.240 --> 10:21.240
me.

10:21.240 --> 10:23.440
Oh, awesome, awesome.

10:23.440 --> 10:32.320
So you talked about your transition to focusing on fairness, fairness accountability, transparency,

10:32.320 --> 10:37.200
and ethics in machine learning and AI.

10:37.200 --> 10:43.920
Can you talk a little bit about what those terms mean to you and your broader research?

10:43.920 --> 10:46.240
Yeah, absolutely.

10:46.240 --> 10:53.800
So I think the bulk of my own research in that sort of broad umbrella falls within two categories.

10:53.800 --> 11:00.840
So the first is fairness, and the second is what I would sort of describe as interpretability

11:00.840 --> 11:02.720
of machine learning.

11:02.720 --> 11:12.080
And so in that fairness bucket, really much of my research is focused on studying the ways

11:12.080 --> 11:19.520
in which machine learning can inadvertently harm or disadvantage groups of people or

11:19.520 --> 11:25.200
individual people in various different usually unintended ways.

11:25.200 --> 11:30.600
And I'm interested in understanding not only why this occurs, but what we can do to mitigate

11:30.600 --> 11:36.200
it, and what we can do to really develop fairer machine learning systems.

11:36.200 --> 11:44.600
The systems that don't inadvertently harm individuals or groups of people in the intelligibility

11:44.600 --> 11:45.600
bucket.

11:45.600 --> 11:53.760
So there, I'm really interested in how we can make machine learning methods that are

11:53.760 --> 11:59.200
interpretable to humans in different roles for particular purposes.

11:59.200 --> 12:06.000
And there's been a lot of research in this area over the past few years, focusing on oftentimes

12:06.000 --> 12:11.480
developing simple machine learning models that can be easily understood by humans simply

12:11.480 --> 12:18.160
by exposing their internals, and also on developing methods that can generate explanations

12:18.160 --> 12:23.840
for either entire models or the predictions of models, and those models might be potentially

12:23.840 --> 12:25.800
very complex.

12:25.800 --> 12:31.480
My own work typically focuses really more on the human side of intelligibility.

12:31.480 --> 12:38.640
So what is it that might make a system intelligible or interpretable to a human trying to carry

12:38.640 --> 12:40.440
out some particular task?

12:40.440 --> 12:45.880
And I do a lot of human subjects experiments to really try and understand some of those

12:45.880 --> 12:51.360
questions with a variety of different folks here at Microsoft Research.

12:51.360 --> 13:00.920
On the topic of fairness and avoiding inadvertent harm, there are a lot of examples that I think

13:00.920 --> 13:09.040
many of our audience would be familiar with the pro-publica work into the use of machine

13:09.040 --> 13:18.200
learning systems in the justice process and others are there examples that come to mind

13:18.200 --> 13:24.480
for you that are maybe less well-known, but that illustrate for you the importance of

13:24.480 --> 13:25.480
that type of work?

13:25.480 --> 13:26.480
Yeah.

13:26.480 --> 13:33.480
So when I typically think about this space, I tend to think about this in terms of the

13:33.480 --> 13:36.520
types of different harms that can occur.

13:36.520 --> 13:42.600
And I have some work with Aaron Shapiro, Solon Brocus, and Kate Crawford, on the different

13:42.600 --> 13:44.720
types of harms that can occur.

13:44.720 --> 13:49.720
And Kate Crawford actually did a fantastic job of talking about this work in her invited

13:49.720 --> 13:54.480
talk at the Nureps Conference in 2017.

13:54.480 --> 14:00.000
So to give you some concrete examples, so many of the examples that people are most familiar

14:00.000 --> 14:06.000
with are these scenarios, as you mentioned, where machine learning systems are being used

14:06.000 --> 14:12.440
to allocate or withhold resources, opportunities, or information.

14:12.440 --> 14:21.080
And so one example would be of the compass recidivism prediction system being used to make

14:21.080 --> 14:24.880
decisions about whether people should be released on bail.

14:24.880 --> 14:35.760
Another example would be from a new story that happened in November where Amazon revealed

14:35.760 --> 14:44.040
that it had abandoned a automated hiring tool because of fears that the tool would reinforce

14:44.040 --> 14:47.440
existing gender imbalances in the workplace.

14:47.440 --> 14:52.400
So there you're looking at these existing gender imbalances and seeing that this tool is

14:52.400 --> 14:58.520
perhaps withholding opportunities from women in the tech industry in an undesirable way.

14:58.520 --> 15:02.320
And there was a lot of coverage about this very sensible decision that Amazon made to

15:02.320 --> 15:04.880
abandon that tool.

15:04.880 --> 15:13.240
Some other examples would be more related to quality of service issues, even when no resources

15:13.240 --> 15:17.240
or opportunities are being allocated or withheld.

15:17.240 --> 15:22.440
So a great example there would be the work that Joy, Wall and Weenie and Timnick Gebrou

15:22.440 --> 15:32.080
did focusing on the ways that commercial gender classification systems might perform less

15:32.080 --> 15:36.520
well, so less accurate, for certain groups of people.

15:36.520 --> 15:41.040
As another example, you might think of let's say speech recognition systems, and you can

15:41.040 --> 15:46.160
imagine systems that work really well for people with certain types of accents or for

15:46.160 --> 15:50.240
people with voices at certain pitches, but less well for other people.

15:50.240 --> 15:56.040
Certainly for me, I'm British and I have a list and I know that oftentimes speech recognition

15:56.040 --> 16:00.240
systems don't do a great job of understanding what I'm saying.

16:00.240 --> 16:04.280
This is much less of an issue nowadays, but you know, five or so years ago, this was

16:04.280 --> 16:07.720
really frustrating for me.

16:07.720 --> 16:12.000
Some other examples are things like stereotyping.

16:12.000 --> 16:17.840
So here, the most famous example of stereotyping and machine learning is Latanya Sweeney's work

16:17.840 --> 16:26.720
from 2013, where she showed that advertisements that were being shown on web searches for different

16:26.720 --> 16:33.560
people's names would more typically be advertisements that reinforce stereotypes about

16:33.560 --> 16:39.440
black criminality when people searched for sort of black sounding names than when people

16:39.440 --> 16:42.800
searched for stereotypically white sounding names.

16:42.800 --> 16:48.760
So there, the issue is this sort of reinforcement of these negative stereotypes within society

16:48.760 --> 16:53.440
by the placement of particular ads for particular different types of searches.

16:53.440 --> 16:58.920
So another example of stereotyping and machine learning would be the work done by Joanna

16:58.920 --> 17:04.960
Brason and others at Princeton University on stereotypes and word embeddings.

17:04.960 --> 17:10.080
And there's also been some similar work done by my colleague Adam Kali here at Microsoft

17:10.080 --> 17:11.080
Research.

17:11.080 --> 17:16.960
And both of these groups of researchers showed that if you train word embedding methods,

17:16.960 --> 17:23.400
so things like word to veck that try and identify a low-dimensional embedding for word types

17:23.400 --> 17:29.000
based on the surrounding words that are typically used in conjunction with them in sentences,

17:29.000 --> 17:34.520
you end up seeing that these word embeddings reinforce existing gender stereotypes.

17:34.520 --> 17:42.360
For example, so the word man ends up being embedded much closer to programmer and similarly

17:42.360 --> 17:48.840
women women ends up being embedded much closer to homemaker than vice versa.

17:48.840 --> 17:51.920
So there would be another kind of example.

17:51.920 --> 17:56.840
And then we see other kinds of examples of unfairness at harms within machine learning

17:56.840 --> 17:57.840
as well.

17:57.840 --> 18:05.200
So for example, over an underrepresentation, so Matthew K and some others at the University

18:05.200 --> 18:10.720
of Washington have this really nice paper where they show that for professions with an equal

18:10.720 --> 18:17.400
or higher percentage of men than women, the image search results are much more heavily

18:17.400 --> 18:21.600
skewed towards images of men than reality.

18:21.600 --> 18:23.800
And so that would be another kind of example.

18:23.800 --> 18:28.600
And what you'll see from all of these examples that I've mentioned is that the effect

18:28.600 --> 18:35.000
of really wide range of systems and types of machine learning applications and the types

18:35.000 --> 18:40.880
of harms or unfairness that might occur are also pretty wide ranging as well, going from

18:40.880 --> 18:46.960
yes, sure, allocation or withholding of resources, opportunities of information, but moving

18:46.960 --> 18:52.080
beyond that to stereotyping and representation and so on.

18:52.080 --> 18:57.320
So often when thinking about fairness and bias and machine learning and the types of harm

18:57.320 --> 19:06.000
that can come about when unfair systems are developed, the kind of all roads lead back

19:06.000 --> 19:14.640
to the data itself and the biases that are inherent in that data given that machine

19:14.640 --> 19:23.880
learning and AI is so dependent on data and often much of the data that we have is bias.

19:23.880 --> 19:28.520
What can we do about that and what are the kinds of things that your research is exploring

19:28.520 --> 19:30.960
to help us address these issues?

19:30.960 --> 19:31.960
Absolutely.

19:31.960 --> 19:32.960
Yes.

19:32.960 --> 19:37.360
So you've hit on a really important point there, which is that in a lot of the sort of public

19:37.360 --> 19:42.960
discourse about fairness and machine learning, you have people making comments about algorithms

19:42.960 --> 19:49.200
being unfair or algorithms being biased and really I think this misses some of the most

19:49.200 --> 19:54.000
fundamental points about why this is such a challenging landscape and so I want to

19:54.000 --> 19:58.440
just emphasize a couple of those here in response to your question.

19:58.440 --> 20:05.920
So the first thing is that machine learning is all about taking data, finding patterns

20:05.920 --> 20:13.320
in that data and then often training systems to mimic the decisions that are represented

20:13.320 --> 20:19.480
within that data and of course we know that the society we live in is not fair.

20:19.480 --> 20:20.480
It is biased.

20:20.480 --> 20:25.600
There are structural disadvantages and discrimination all over the place.

20:25.600 --> 20:31.680
So it's pretty inevitable that if you take data from a society like that and then train

20:31.680 --> 20:38.280
machine learning systems to find patterns expressed in that data and to mimic the decisions

20:38.280 --> 20:45.280
made within that society, you will necessarily reproduce those structural disadvantages that

20:45.280 --> 20:48.160
bias, that discrimination and so on.

20:48.160 --> 20:52.400
So you're absolutely right that a lot of this does indeed come from data.

20:52.400 --> 20:59.560
But the other point that I want to make is that it's not just from data and it's not from

20:59.560 --> 21:05.480
algorithms per se, the issue is really as I see it and as my colleagues here at Microsoft

21:05.480 --> 21:12.160
Research see it, the issue is really about people and people's decisions at every point

21:12.160 --> 21:15.080
in that machine learning life cycle.

21:15.080 --> 21:21.720
So I've done some work on this with a number of people here at Microsoft.

21:21.720 --> 21:28.320
Most recently I put together actually a tutorial on machine learning and fairness in collaboration

21:28.320 --> 21:34.000
with my colleague, Jen Wartman Vaughn and the way we really think about this is that you

21:34.000 --> 21:39.400
have to prioritize fairness at every stage of that machine learning life cycle.

21:39.400 --> 21:42.360
You can't think about it as an afterthought.

21:42.360 --> 21:49.480
And the reason why is that decisions that we make at every stage can fundamentally impact

21:49.480 --> 21:53.480
whether or not a system treats people fairly.

21:53.480 --> 21:58.480
And so I think it's really important when we're thinking about fairness in machine learning

21:58.480 --> 22:03.720
to not just sort of make general statements about algorithms being unfair or systems being

22:03.720 --> 22:10.240
unfair, but really to go back to those particular points and think about how unfairness can

22:10.240 --> 22:13.880
kind of creep in at any one of those stages.

22:13.880 --> 22:17.600
And that might be as early as the task definition stage.

22:17.600 --> 22:21.640
So when you're sitting down to develop some machine learning system, it's really important

22:21.640 --> 22:28.640
to ask the question of who does this take power from and who does this give power to?

22:28.640 --> 22:33.400
And the answers to that question often reveal a lot about whether or not that technology

22:33.400 --> 22:36.800
should even be built in this first place.

22:36.800 --> 22:40.880
Sometimes the answer to addressing fairness in machine learning is simply no, we should

22:40.880 --> 22:43.760
not be building that technology.

22:43.760 --> 22:48.000
But there are all kinds of other decisions and assumptions at other points in that machine

22:48.000 --> 22:50.240
learning life cycle as well.

22:50.240 --> 22:57.040
So the way we typically like to think about it is that a machine learning model or method

22:57.040 --> 23:01.160
is effectively an abstraction of the world.

23:01.160 --> 23:06.000
And in making that abstraction, you necessarily have to make a bunch of assumptions about

23:06.000 --> 23:07.000
the world.

23:07.000 --> 23:11.520
And some of these assumptions will be more or less justified.

23:11.520 --> 23:16.080
Some of these assumptions will be better fits for the reality than others.

23:16.080 --> 23:21.640
But if you're not thinking really carefully about what those assumptions are when you're

23:21.640 --> 23:27.000
developing your machine learning system, this is one of the most obvious places that you

23:27.000 --> 23:32.640
can inadvertently end up introducing bias or unfairness.

23:32.640 --> 23:34.840
Can you give us some concrete examples there?

23:34.840 --> 23:37.160
Yeah, absolutely.

23:37.160 --> 23:44.360
One common example of this form would be stuff to do with teacher evaluation.

23:44.360 --> 23:49.160
So there have been a couple of sort of high profile lawsuits about this kind of thing.

23:49.160 --> 23:52.000
But I think it illustrates the point nicely.

23:52.000 --> 23:57.560
So it's common for teachers to be evaluated based on a number of different factors, but

23:57.560 --> 24:01.200
including their students test scores.

24:01.200 --> 24:06.720
And indeed, many of the methods that have been developed to analyze teacher quality using

24:06.720 --> 24:13.160
sort of machine learning systems have really focused predominantly on students test scores.

24:13.160 --> 24:19.080
But this assumes that students test scores are, in fact, an accurate predictor of teacher

24:19.080 --> 24:20.240
quality.

24:20.240 --> 24:22.600
And this isn't actually always the case.

24:22.600 --> 24:26.760
A good teacher should obviously do more than test prep.

24:26.760 --> 24:32.560
And so any system that really looks just at test scores when trying to predict teacher

24:32.560 --> 24:38.080
quality is going to do a bad job of capturing these other properties.

24:38.080 --> 24:40.560
So that would be one example.

24:40.560 --> 24:43.920
Another example involves predictive policing.

24:43.920 --> 24:49.680
So a predictive policing system might make predictions about where crimes will be committed

24:49.680 --> 24:52.400
based on historic arrest data.

24:52.400 --> 24:58.160
But an implicit assumption here is that the number of arrests in an area is an accurate

24:58.160 --> 25:00.640
proxy for the amount of crime.

25:00.640 --> 25:05.800
And it doesn't take into account the fact that policing practices can be racially biased

25:05.800 --> 25:11.480
or there might be historic over policing in less affluent neighborhoods.

25:11.480 --> 25:14.080
I'll give you another example as well.

25:14.080 --> 25:21.640
So many machine learning methods work by defining some objective function and then learning

25:21.640 --> 25:27.560
the parameters of the model so as to optimize that objective function.

25:27.560 --> 25:32.520
And so for example, if you define an objective function in the context of let's say a search

25:32.520 --> 25:41.400
engine that prioritizes user clicks, you may end up with search results that don't necessarily

25:41.400 --> 25:44.240
reflect what you want them to.

25:44.240 --> 25:49.880
And this is because users may click on certain types of search results over other search

25:49.880 --> 25:50.880
results.

25:50.880 --> 25:57.320
And that might not be reflective of what you want to be showing when you show users a

25:57.320 --> 25:59.400
page of search results.

25:59.400 --> 26:05.240
So as a concrete example, many search engines, if you search for the word boy, you see

26:05.240 --> 26:08.520
a bunch of pictures of male children.

26:08.520 --> 26:15.360
But if you search for the word girl, you see a bunch of pictures of grown ups, women.

26:15.360 --> 26:18.320
And these are pretty different to each other.

26:18.320 --> 26:23.280
And this probably comes from the fact that search engines typically optimize for clicks

26:23.280 --> 26:25.360
among other metrics.

26:25.360 --> 26:30.280
And this really shows how hard it can be to even address these kinds of fairness issues.

26:30.280 --> 26:35.760
Because in different circumstances, the word girl may be referring to a child or a woman

26:35.760 --> 26:39.000
and uses search for this term with different intentions.

26:39.000 --> 26:43.480
And in this particular example, as you can probably imagine, one of these intentions

26:43.480 --> 26:46.840
might be more prevalent than the other.

26:46.840 --> 26:54.200
You've identified lots of opportunities for pitfalls in the process of fielding systems

26:54.200 --> 27:00.960
going all the way back to just the way you define your system and state your intentions

27:00.960 --> 27:13.720
and formulate the problem that you're going after, beyond simply being mindful of the

27:13.720 --> 27:17.800
potential for bias and unfairness.

27:17.800 --> 27:22.040
And just saying simply, I realize that that's not simple.

27:22.040 --> 27:24.840
It's work to be mindful of this.

27:24.840 --> 27:31.280
But beyond that, what does your research offer in terms of how to overcome these kinds

27:31.280 --> 27:32.280
of issues?

27:32.280 --> 27:35.960
Yeah, this is a really good question.

27:35.960 --> 27:41.400
And it's a question that I get a lot from people is, what can we actually do in practice?

27:41.400 --> 27:46.240
And there are a number of things that can be done in practice, not all of them are easy

27:46.240 --> 27:48.360
things to do, as you say.

27:48.360 --> 27:54.200
So one of the most important things is that issues relating to fairness in machine learning

27:54.200 --> 28:00.360
are fundamentally sociotechnical and they're not going to be addressed by computer scientists

28:00.360 --> 28:02.480
or developers alone.

28:02.480 --> 28:08.840
It's really important to involve a range of diverse stakeholders in these conversations

28:08.840 --> 28:14.440
when we're developing machine learning systems so that we have a bunch of different perspectives

28:14.440 --> 28:15.680
represented.

28:15.680 --> 28:21.360
So moving beyond just involving computer scientists and developers on teams, it's really important

28:21.360 --> 28:28.000
that we involve social scientists, lawyers, policymakers, end users, people who are going

28:28.000 --> 28:33.560
to be affected or impacted by these systems down the line and so on and so forth.

28:33.560 --> 28:37.160
And that's one really concrete thing you can do.

28:37.160 --> 28:43.480
There's a project that came out of the University of Washington called the Diverse Voices Project.

28:43.480 --> 28:50.600
And it provides a way of getting feedback from stakeholders on tech policy documents and

28:50.600 --> 28:51.600
it's really good.

28:51.600 --> 28:55.280
They have a great how-to guide that I definitely recommend checking out.

28:55.280 --> 29:00.280
But many of the things that they recommend doing there, you can also think about when

29:00.280 --> 29:06.280
you're trying to get feedback from stakeholders on, let's say, the definition of a machine

29:06.280 --> 29:07.280
learning system.

29:07.280 --> 29:12.600
So that task definition stage, and some of these could even potentially be expanded to

29:12.600 --> 29:16.840
consider other stages of that machine learning pipeline as well.

29:16.840 --> 29:21.600
So there are a number of things that you can do at every single stage of the machine learning

29:21.600 --> 29:22.600
pipeline.

29:22.600 --> 29:27.840
And in fact, this this tutorial that I mentioned earlier that I worked on with my colleague

29:27.840 --> 29:33.080
Jen Warman-Von actually has guidelines for every single step of the pipeline.

29:33.080 --> 29:37.720
But to give you examples, here are some things for instance that you can do when you're selecting

29:37.720 --> 29:39.400
a data source.

29:39.400 --> 29:45.840
So for example, it's really important to think critically before even collecting any data.

29:45.840 --> 29:49.240
It's often very tempting to say, oh, there's already some data set that I can probably

29:49.240 --> 29:54.920
repurpose for this, but it's really important to take that step back and before immediately

29:54.920 --> 30:01.080
acting based on availability to actually think about whether that data source is appropriate

30:01.080 --> 30:04.120
for the task you want to use it for.

30:04.120 --> 30:06.200
And there's a number of reasons why it might not be.

30:06.200 --> 30:10.600
It could be to do with biases in the data source selection process.

30:10.600 --> 30:15.600
There might be societal biases present in the data source itself.

30:15.600 --> 30:19.400
It might be that the data source doesn't match the deployment context.

30:19.400 --> 30:24.040
That's a really important one that people really should be taking into account.

30:24.040 --> 30:27.400
Where are you thinking about deploying your machine learning system?

30:27.400 --> 30:33.720
And does the data you have availability for training and development match that context?

30:33.720 --> 30:37.080
Because another example is still related to data.

30:37.080 --> 30:42.520
It's really important to think about biases in the technology used to collect data.

30:42.520 --> 30:49.040
So as an example here, there was an app released in the city of Boston back in 2011.

30:49.040 --> 30:51.040
I think it was called Street Bump.

30:51.040 --> 30:57.760
And the way it worked is it used iPhone data and specifically sort of the positional movement

30:57.760 --> 31:03.280
of iPhones as people were driving around to gather data on where there were potholes

31:03.280 --> 31:05.680
that should be repaired by the city.

31:05.680 --> 31:10.000
But pretty quickly, the city of Boston figured out that this actually wasn't a great way to

31:10.000 --> 31:11.960
get that kind of data.

31:11.960 --> 31:18.120
Because back in 2011, the people who had iPhones were typically quite affluent and only lived

31:18.120 --> 31:19.960
in certain neighborhoods.

31:19.960 --> 31:25.360
So that would be an example of thinking carefully about the technology even used to collect data.

31:25.360 --> 31:30.520
It's also really important to make sure that there's sufficient representation of different

31:30.520 --> 31:37.240
subpopulations who might be ultimately using or affected by your machine learning system

31:37.240 --> 31:43.080
to make sure that you really do have good representation overall.

31:43.080 --> 31:47.200
Moving on to things like the model, there's a number of different things that you can do

31:47.200 --> 31:49.520
there, for instance, as well.

31:49.520 --> 31:55.440
So in the case of a model, I mentioned a bit about assumptions being really important.

31:55.440 --> 32:00.720
It's great to really clearly define all of your assumptions about the model, and then

32:00.720 --> 32:07.760
to question whether there might be any explicit or implicit biases present in those assumptions.

32:07.760 --> 32:12.120
That's a really important thing to do when you're thinking about choosing any particular

32:12.120 --> 32:14.720
model or model structure.

32:14.720 --> 32:22.960
You could even, in some scenarios, include some quantitative notion of parity, for instance,

32:22.960 --> 32:27.640
in your model objective function as well, and there've been a number of academic papers

32:27.640 --> 32:33.280
that take that approach in the literature over the past few years.

32:33.280 --> 32:35.880
Can you give an example of that last point?

32:35.880 --> 32:37.720
Yeah, sure.

32:37.720 --> 32:42.920
So imagine you have some kind of a machine learning classifier that's going to make decisions

32:42.920 --> 32:50.520
of the form, let's say, no loan, hire, no hire, bail, no bail, and so on.

32:50.520 --> 32:55.640
The way we normally develop these classifiers is to take a bunch of labeled data, so data

32:55.640 --> 33:02.360
points labeled with, let's say, loan, no loan, and then we train a model, a machine learning

33:02.360 --> 33:09.320
model, a classifier, to optimize accuracy on that training data.

33:09.320 --> 33:14.840
So you end up setting the parameters of that model such that it does a good job of accurately

33:14.840 --> 33:19.120
predicting those labels from the training data.

33:19.120 --> 33:26.800
So the objective function that's typically used is one that considers usually only accuracy.

33:26.800 --> 33:33.600
But something else you can do is define some quantitative definition of fairness, some

33:33.600 --> 33:41.440
quantitative fairness metric, and then try to simultaneously optimize both of these objectives,

33:41.440 --> 33:46.120
so classifier accuracy and whatever your chosen fairness metric is.

33:46.120 --> 33:50.160
And there's a number of these different quantitative metrics that have been proposed

33:50.160 --> 33:51.160
out there.

33:51.160 --> 33:56.000
They're all typically looking at sort of parity across groups of some sort.

33:56.000 --> 33:59.760
So I think it's really important to remember that even though these are often referred

33:59.760 --> 34:05.640
to as fairness metrics, they're really parity metrics, and then neglect many of the really

34:05.640 --> 34:11.960
important other aspects of fairness, like justice and due process and so on and so forth.

34:11.960 --> 34:17.520
But it is absolutely possible to take these parity metrics and to incorporate them into

34:17.520 --> 34:23.720
the objective function of say a classifier and then to try and prioritize satisfying and

34:23.720 --> 34:29.800
optimizing that fairness metric at the same time as optimizing classifier accuracy.

34:29.800 --> 34:34.240
There have been a number of papers that focus on this kind of approach.

34:34.240 --> 34:40.600
Many of them will focus on one particular type of classifier, so like SVMs or neural networks

34:40.600 --> 34:44.560
or something like that, and one particular fairness metric.

34:44.560 --> 34:48.400
And there are a bunch of standard fairness metrics that people like to look at.

34:48.400 --> 34:54.080
I actually have some work with some colleagues here at Microsoft where we have a slightly

34:54.080 --> 35:01.120
more general way of doing this that will work with many different types of classifiers

35:01.120 --> 35:03.960
and many different types of fairness metrics.

35:03.960 --> 35:08.680
So there's no reason to start again from scratch if you want to switch to a different

35:08.680 --> 35:11.760
classifier or a different fairness metric.

35:11.760 --> 35:17.440
We actually have some open source Python code available on GitHub that implements our approach.

35:17.440 --> 35:26.680
So you've talked about the idea that kind of people are fundamentally the root of the

35:26.680 --> 35:35.080
issue that these are societal issues that they're not going to be solved by technological

35:35.080 --> 35:39.880
advancements or processes alone.

35:39.880 --> 35:47.240
At the same time, there's been a ton of new research happening in this area by folks

35:47.240 --> 35:49.680
in your group and elsewhere.

35:49.680 --> 35:55.920
Does that lead to a mismatch between what's happening in academia and on the technical

35:55.920 --> 36:00.600
side with the way this stuff actually gets put into practice?

36:00.600 --> 36:03.720
That's an awesome question.

36:03.720 --> 36:10.240
The simple answer is yes, and this actually relates to one of my most recent research projects

36:10.240 --> 36:13.360
which I'm really, really excited about.

36:13.360 --> 36:20.400
So last summer, some of my colleagues and I, specifically Gen Warman Vaughan, Merodudeck

36:20.400 --> 36:27.800
and Hal Damay, along with our incredible intern Ken Holstein from CMU, conducted the first

36:27.800 --> 36:35.960
systematic investigation of industry practitioners' challenges and needs for support relating

36:35.960 --> 36:39.600
to developing fairer machine learning systems.

36:39.600 --> 36:44.920
And this work actually came about because we were thinking about ways of developing interfaces

36:44.920 --> 36:51.000
for that fair classification work that I mentioned a minute ago and through a number of conversations

36:51.000 --> 36:55.920
with people in different product groups here at Microsoft and people at other companies,

36:55.920 --> 37:02.280
we realized that these kinds of classification tasks, while they're incredibly well studied

37:02.280 --> 37:07.960
within the fairness in machine learning literature, are maybe less common than we had thought

37:07.960 --> 37:10.520
in practice within industry.

37:10.520 --> 37:14.480
And so that got us thinking about whether there might be actually a mismatch between the

37:14.480 --> 37:20.720
academic literature on fairness in machine learning and practitioner's actual needs.

37:20.720 --> 37:25.160
What we ended up doing was a super interesting research project.

37:25.160 --> 37:29.560
It was a pretty different style of research for me and for my colleagues.

37:29.560 --> 37:36.400
So I am a machine learning researcher, so is Gen, so is Hal and so is Merod, Ken, our

37:36.400 --> 37:38.880
intern is an HCI researcher.

37:38.880 --> 37:45.840
And what we ended up doing was this qualitative HCI work to really understand what it is that

37:45.840 --> 37:52.880
practitioners are facing in reality when they try and develop fairer machine learning systems.

37:52.880 --> 38:01.000
And to do this, we conducted semi-structured interviews with 35 people spanning 25 different

38:01.000 --> 38:04.280
teams in 10 different companies.

38:04.280 --> 38:09.800
And these people were in a number of different roles ranging from social scientists, data

38:09.800 --> 38:16.560
labeler, product manager, program manager to data scientist and researcher.

38:16.560 --> 38:21.080
And where possible we tried to interview multiple people from the same team in order

38:21.080 --> 38:27.240
to get a variety of perspectives on that team's challenges and needs for support.

38:27.240 --> 38:34.840
We then took our findings from these interviews and developed a survey which was then completed

38:34.840 --> 38:42.240
by another 267 industry practitioners, again in a variety of different companies and a variety

38:42.240 --> 38:43.480
of different roles.

38:43.480 --> 38:49.400
And what we found at a high level was that yes, there is a mismatch between the academic

38:49.400 --> 38:55.320
literature on fairness and machine learning and industry practitioners actual challenges

38:55.320 --> 38:58.040
and needs for support on the ground.

38:58.040 --> 39:04.920
So firstly, much of the machine learning literature on fairness focuses on classification

39:04.920 --> 39:07.920
and on supervised machine learning methods.

39:07.920 --> 39:14.320
In fact, what we found is that industry practitioners are grappling with fairness issues in a much

39:14.320 --> 39:20.040
wider range of applications beyond classification or prediction scenarios.

39:20.040 --> 39:25.000
And in fact, many times the systems they're dealing with involve these really rich, complex

39:25.000 --> 39:28.640
interactions between users and the system.

39:28.640 --> 39:35.560
So for example, chatbots or adaptive tutoring or personalized retail and so on and so forth.

39:35.560 --> 39:41.800
So as a result, they often struggle to use existing fairness research from the literature

39:41.800 --> 39:46.600
because the things that they're facing are much less amenable to these quantitative fairness

39:46.600 --> 39:47.600
metrics.

39:47.600 --> 39:54.000
And indeed, very few teams have fairness KPIs or automated tests that they can use within

39:54.000 --> 39:55.840
their domain.

39:55.840 --> 39:59.920
One of the other things that we found is that the machine learning literature typically

39:59.920 --> 40:06.520
assumes access to sensitive attributes like race or gender for the purpose of auditing

40:06.520 --> 40:08.320
systems for fairness.

40:08.320 --> 40:13.000
But in practice, many teams have no access to these kinds of attributes and certainly

40:13.000 --> 40:15.840
not at the level of individuals.

40:15.840 --> 40:21.760
So they express needs for support in detecting biases in unfairness with access only to

40:21.760 --> 40:26.200
coarse grained partial or indirect information.

40:26.200 --> 40:31.280
And this is something that we've seen much less focus on in the academic literature.

40:31.280 --> 40:37.600
That last point is an interesting one and one that I've brought up on the podcast previously

40:37.600 --> 40:45.040
and many of the places you might want to use an approach like that is forbidden from a

40:45.040 --> 40:50.160
regulatory perspective to use the information that you want to use in your classifier to

40:50.160 --> 40:54.360
achieve fairness in any part of the decisioning process.

40:54.360 --> 40:55.360
Exactly.

40:55.360 --> 41:01.080
This sets up this really difficult tension between doing the right thing in practice from

41:01.080 --> 41:05.360
a machine learning perspective and what is legally allowed.

41:05.360 --> 41:11.280
And I'm actually working on a paper at the moment with a lawyer, Zach Hanard, actually

41:11.280 --> 41:16.920
a law student, Zach Hanard at Stanford University, on exactly this issue, this challenge between

41:16.920 --> 41:21.480
what you want to do from a machine learning perspective and what you were required to do

41:21.480 --> 41:28.440
from a legal perspective based on humans and how humans behave and hundreds of years

41:28.440 --> 41:30.800
of law in that realm.

41:30.800 --> 41:36.040
It's really challenging and there is this complicated trade off there that we really need to

41:36.040 --> 41:37.760
be thinking about.

41:37.760 --> 41:45.040
It does make me wonder if techniques like or analogous to a differential privacy or something

41:45.040 --> 41:53.920
like that could be used to provide kind of a regulatory, acceptable way to access protected

41:53.920 --> 41:57.800
attributes so that they can be incorporated into algorithms like this.

41:57.800 --> 42:05.000
So there was some work on exactly this kind of topic so at the Fatem L workshop co-located

42:05.000 --> 42:07.680
with ICML last year.

42:07.680 --> 42:15.240
And this work was proposing the use of encryption and such like in order to collect and make

42:15.240 --> 42:20.600
available such information, but in a way that users would feel as if their privacy were

42:20.600 --> 42:26.440
being respected and so that people who wanted to use that information would be able to use

42:26.440 --> 42:30.640
it for purposes such as auditing.

42:30.640 --> 42:34.880
And I think that's a really promising approach, although there's obviously a bunch of non-trivial

42:34.880 --> 42:39.640
challenges involved in thinking about how you might make that a reality.

42:39.640 --> 42:44.600
It's a really complicated landscape, but definitely one that's worth thinking about.

42:44.600 --> 42:48.000
Was there a third area that you were about to mention?

42:48.000 --> 42:49.000
Yeah.

42:49.000 --> 42:55.320
So one of the main themes that we found in our work studying industry practitioners is

42:55.320 --> 43:02.920
a real mismatch between the focus on different points in the machine learning life cycle.

43:02.920 --> 43:09.680
So the machine learning literature typically assumes no agency over data collection.

43:09.680 --> 43:10.680
And this makes sense, right?

43:10.680 --> 43:14.560
If you're a machine learning academic, you typically work with standard data sets that

43:14.560 --> 43:18.480
have been collected and made available for years, you don't typically think about having

43:18.480 --> 43:21.560
agency over that data collection process.

43:21.560 --> 43:26.680
And of course, in industry, that's exactly where practitioners often do have the most

43:26.680 --> 43:27.680
control.

43:27.680 --> 43:31.880
They are in charge of that data collection or data curation process.

43:31.880 --> 43:37.240
And in contrast, they often have much less control over the methods or models themselves,

43:37.240 --> 43:42.720
which often are embedded within much bigger systems, so it's much harder to intervene

43:42.720 --> 43:48.000
from a perspective of fairness with the models than it is with the data.

43:48.000 --> 43:52.720
We found that really interesting, the sort of difference in emphasis between models versus

43:52.720 --> 43:55.680
data in these different groups of people.

43:55.680 --> 44:02.480
And of course, many practitioners voiced needs for support in figuring out how to leverage

44:02.480 --> 44:08.760
that sort of agency over data collection to create fairer data sets for use in developing

44:08.760 --> 44:10.520
their systems.

44:10.520 --> 44:14.000
So you mentioned the FET ML workshop.

44:14.000 --> 44:23.160
I'm wondering as we come to a close, if there are any resources, events, pointers, I'm sure

44:23.160 --> 44:26.440
there are tons of things that you'd love to point people at.

44:26.440 --> 44:32.880
But what are your top three or four things that you would suggest people take a look at

44:32.880 --> 44:40.280
as they're trying to wrap their heads around this area and how to either have an impact

44:40.280 --> 44:45.080
as a researcher or how to make good use of it as a practitioner?

44:45.080 --> 44:46.080
Yeah.

44:46.080 --> 44:47.080
Absolutely.

44:47.080 --> 44:51.840
So there are a number of different places with resources to learn more about this kind

44:51.840 --> 44:53.000
of stuff.

44:53.000 --> 44:57.800
So first, I've mentioned a couple of times this tutorial that I put together with Jen

44:57.800 --> 45:04.080
Wartman Vaughn, that will be available publicly online very soon.

45:04.080 --> 45:08.360
It is in fact being broadcast next week, so it should be up by the time this podcast

45:08.360 --> 45:09.560
goes live.

45:09.560 --> 45:14.600
So I would definitely recommend that people check that out to really get a sense of how

45:14.600 --> 45:19.280
we at Microsoft are thinking about fairness and machine learning.

45:19.280 --> 45:24.560
Then moving beyond that and thinking specifically on more of the academic literature, the FET ML

45:24.560 --> 45:30.320
workshop maintains a list of resources on the workshop website.

45:30.320 --> 45:36.160
And that's again, another really, really great place to look for things to read about

45:36.160 --> 45:38.240
this topic.

45:38.240 --> 45:45.920
The FET Star Conference is a relatively newly created conference on fairness, accountability

45:45.920 --> 45:52.360
and transparency, not just in machine learning, but across all of computer science and

45:52.360 --> 45:54.280
computational systems.

45:54.280 --> 45:58.600
And again, there, I recommend checking out the website to see the publications that were

45:58.600 --> 46:02.520
there last year, and also the publications that will be there this year.

46:02.520 --> 46:07.000
There's a number of really interesting papers that I haven't read yet, but I'm super excited

46:07.000 --> 46:10.120
to read being presented at this year's conference.

46:10.120 --> 46:15.920
That conference also has tutorials on a range of different subjects.

46:15.920 --> 46:20.360
And so it's also worth looking at the various different tutorials there.

46:20.360 --> 46:26.880
So at last year's conference, Ivan Narayanan presented this amazing tutorial on quantitative

46:26.880 --> 46:33.880
fairness metrics and why they're not a one-size-fits-all solution, why there are trade-offs between

46:33.880 --> 46:39.720
them, why you can't just sort of take one of these definitions, optimize for it and call

46:39.720 --> 46:40.720
it quits.

46:40.720 --> 46:44.320
And so I definitely recommend checking that out.

46:44.320 --> 46:49.800
Some other places that are worth looking for resources on this.

46:49.800 --> 46:56.640
The AINow Institute, which was co-founded by Kate Crawford, who's also here at Microsoft

46:56.640 --> 47:02.800
Research and Meredith Whitaker, who's also at Google, also has some incredibly awesome

47:02.800 --> 47:03.800
resources.

47:03.800 --> 47:08.200
They've put out a number of white papers and reports over the past couple of years that

47:08.200 --> 47:13.680
really get at the crux of why these are complicated socio-technical issues.

47:13.680 --> 47:19.120
And so I strongly recommend reading pretty much everything that they put out.

47:19.120 --> 47:25.560
I would also recommend checking out some of the material put out by Data and Society,

47:25.560 --> 47:31.200
which is also an organization here in New York, led by Dana Boyd.

47:31.200 --> 47:37.240
And they, too, have a number of really interesting things that you can read about these different

47:37.240 --> 47:38.240
topics.

47:38.240 --> 47:43.680
And then the final thing I want to emphasize is the partnership on AI, which was formed

47:43.680 --> 47:49.480
a couple of years ago by Microsoft and a bunch of other companies working in the space

47:49.480 --> 47:57.880
of AI, to really foster cross-company collaboration and moving forward in this space when thinking

47:57.880 --> 48:03.960
about these complicated societal issues that relate to AI and machine learning.

48:03.960 --> 48:08.160
And so the partnership has been really ramping up over the past couple of years.

48:08.160 --> 48:11.600
And they also have some good resources that are worth checking out.

48:11.600 --> 48:12.600
Oh, that's great.

48:12.600 --> 48:17.000
That is a great list that will keep us busy for a while.

48:17.000 --> 48:21.200
Hannah, thank you so much for taking the time to chat with us.

48:21.200 --> 48:24.000
It was really a great conversation and I appreciate it.

48:24.000 --> 48:25.000
No problem.

48:25.000 --> 48:26.000
Thank you for having me.

48:26.000 --> 48:28.160
This has been really great.

48:28.160 --> 48:29.160
Awesome.

48:29.160 --> 48:30.160
Thank you.

48:30.160 --> 48:35.200
All right, everyone, that's our show for today.

48:35.200 --> 48:40.840
For more information on Hannah or any of the topics covered in this show, visit twimmelai.com

48:40.840 --> 48:43.680
slash talk slash two, three, two.

48:43.680 --> 48:48.920
To follow along with the AI for the benefit of society series, visit twimmelai.com slash

48:48.920 --> 48:51.000
AI for society.

48:51.000 --> 48:58.000
As always, thanks so much for listening and catch you next time.

