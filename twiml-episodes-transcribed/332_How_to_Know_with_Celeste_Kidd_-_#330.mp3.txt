Welcome to the Twimmel AI Podcast.
I'm your host Sam Charrington.
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded
earlier this month at the 33rd annual NURRIPS conference.
If you've been waiting for the Twimmel pendulum to swing from workflow and deployment back
over to AI and ML research, this is your time.
We've got some great interviews in store for you over the upcoming weeks.
Before we move on, I want to send a huge thanks to our friends at Shell for their support
of the podcast and their sponsorship of this NURRIPS series.
Shell has been an early adopter of a wide variety of AI technologies to support use cases
across retail, trading, new energies, refineries, exploration, and many more, and is doing
some really interesting things, but don't take it from me.
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.
They have a very deliberate strategy of using AI right across their operation from the drilling
operations to safety.
Last year, the company established the Shell.AI Residency Program, a two-year full-time
program, which allows data scientists and AI engineers to gain experience working on
a variety of AI projects across all Shell businesses.
If you're in a position to take advantage of an opportunity like this, I'd encourage
you to hit pause now and head over to Shell.AI to learn more.
Once again, that's Shell.AI, and now on to the show.
All right, everyone.
I am here in Vancouver at NURRIPS, and I've got the pleasure of being seated with Celeste
Kid.
Celeste is an assistant professor of psychology at UC Berkeley.
Celeste, welcome to the Twoma AI podcast.
Thank you so much.
I'm so excited to be here.
I am super excited to dive into this conversation you delivered and invited talk here yesterday
that has been blowing up the Twitter's, and I'm really looking forward to chatting
with you about it.
But before we do that, tell us a little bit about your background.
You operate in the intersection of psychology and machine learning, and we're talking
about you building models and stuff like that.
What are you up to?
I do.
I like pulling a lot of things from a lot of places.
My background was actually in investigative reporting, which sounds not that relevant,
but I was interested in doing big data type analyses.
My first real science was getting together public records and looking for corruption.
I started at UC Santa Cruz, and then I also liked computers, so started in CS.
I ended up transferring and changing my degrees to linguistics, and then I finished the journalism
degree, and while I was doing those, I got very lucky and happened upon science via some
really amazing mentors and professors at USC, and fell in love with it, and liked that unlike
journalism in science, truth is on your side.
I think when you find the truth, you win much more so than I was experiencing at that time
in journalism.
I thought the journalism was going to be like that, but I found...
I think hope journalism is like that.
I think it's becoming more like that, and that's actually an interesting discussion, too,
because that's where it's going is much more machine learning relevant than it used
to be, when I was making the decision about whether to continue in journalism or to transition
to science.
I knew I really loved science, but there were still a part of me that thought that journalism
was a kind of higher calling, and at that time, I had a bunch of stories that I had composed
that I was very proud of, but they contained things that my editors weren't expecting.
They weren't that sophisticated.
I hadn't taken formally a statistics class, but in that era, if I brought something to
my editor and it wasn't a pie chart or a bar graph, they were confused and asked me
to go back and turn it into a pie chart or a bar graph, and I was like regression analyses
or not.
We're not things that editors would even like hear you out on, so I think that's changed
a whole lot.
Well, data journalism is a whole thing now.
It's a thing.
In the era in which I was in college, the thing that was the version of that was obviously
named in the 70s and was called computer assisted reporting.
Oh, wow.
The conventions were really, really disappointing.
Pretty much what everybody was doing is there's all these workshops and there's reporters
that have had high profile stories, and the promise was, we'll tell you how to do something
new and innovative and almost all of them, it was like, what I did is I got some public
records for school bus drivers and I got registered sex offenders, cross reference
them, bam, story.
Now I got some priests and registered sex offenders, cross reference them, story, that was
the only thing that people were really doing.
And that should be like table stakes of, hey, doing some research, right?
Yeah, yeah.
So it wasn't, I was having a hard time finding inspiration in that field and then the
stories I was producing often got, the editor wasn't willing to run them as they were or
they wanted me to simplify them to the point that I thought it wasn't true anymore and
nobody was paying investigative reporters.
I applied to grad school, also had a job that it was very likely that I could have, was
like lined up, that I was considering and I called my journalism mentors expecting them
to talk me out of going to grad school for science and every single one of them said, this
is a terrible time journalism might be dead, can you do something else, go do it.
And in that era, I was like my grad school stipend, those are not known for being generous
but the journalism salaries are really abysmal, wasn't my grad school stipend actually was
higher than my salary would have been if I'd taken the journalism job.
So yeah, I'm happy to be here in science where we can do all sorts of more sophisticated
analyses than, you know, 2000 era journalism.
Yeah, so tell us about the focus of your lab at Berkeley.
We are very interested in belief formation, we're interested in how people form their
beliefs and we apply that to domains that I think people don't think of usually, they
don't use the term belief, it's like they use terms like knowledge acquisition, things
that people think of as you learning and you're done, probably that's not true.
So things like words, if you know the word table, you might think, we talk about a kid who
knows the word table, if I say like, hey, show me a table, they can point to it, they produce
the word table in the right instances.
But what is actually true, but you can't observe directly because the mind is a black box
is you're never done forming your concept of table with every new table you encounter,
your belief about the concept of table changes just a little bit and it updates if you move
to a place where the tables look systematically different, your concept will move in that
direction too.
So we think about what other people might call knowledge in terms of beliefs and we think
of beliefs in terms of being kind of packets of probabilistic expectations.
And thinking about this example of table, like how would how do you experimentally validate
those kinds of ideas?
Oh, that's the fun part.
Like I didn't get to do that much, you didn't get to do as much in reporting.
So we do things like we ask people to, we give them a choice, as we say, here's a concept,
for example, pick a political figure, Donald Trump is Donald Trump more like Richard Nixon
or more like Elizabeth Warren.
And people make a selection, as we collect these comparisons across a whole set of examples
and based on people's responses, we're able to use clustering algorithms in order to infer
the true number of types of categories that exist in the population.
That's a good summary.
And it sounds like from what I've read about some of the work at your lab, you're also,
in addition to kind of an experimental type of approach, you're also building models
and using machine learning to, well, you describe, what's the connection between the model
work that you do and the experimental work?
It depends on the particular type of project, but in general, we're building computational
models that represent formal versions of classic theories from learning science.
So we take a lot of inspiration from people like Jean Piaget and Maria Montessori and Lev
Vagatsky.
All of them had pretty similar ideas about what the relationship should be between what
you currently understand and what you're interested in sampling from next or what you
are most able to learn from next.
All of them said similar things about there being a just right amount of information.
I was like, you want to seek out stuff that's a little bit different from what you currently
understand, but not so different from what you currently understand that you can't gain
any traction.
Those ideas remain largely untested because of the black box problem.
You can't directly, you know, open up a kid's head and see what's in there.
So we use models in order to represent a sort of formal version of those kinds of ideas.
I was like, for that particular work, I was like, we have a set of research about infants
and how they sample from the world.
We were interested in testing whether or not infants are generating probabilistic expectations
in the absence of any specific goal and whether those probabilistic expectations were influencing
their decisions about what to look at, whether or not they should continue looking at something
or they should cut and run and find something else.
So we created a probabilistic model, a Dirichlet multinomial, so pretty standard stuff.
And then I've used that to compute the surprise value for different actions in a sequence.
I'm not sure if this is, I was like, now I'm taking a moment to those, I don't know if
I'm going backwards.
Is this understandable?
No, no.
So the surprise value, it is...
Well, I just realized I forgot to tell you what they're looking at.
I should like...
Oh, yes.
We can fill in the context now, sir.
For these experiments, people have been interested in how infants decide what to look at moment
to moment for a long time for a number of reasons.
Like, one of them is you can't ask infants questions the only way that you can find out.
What they know is to look at what they're looking at, what they're interested in and try
to make inferences. The most common format of infant experiments is you show the most
stimulus A, you show the most stimulus B, and you see if there's a difference in looking
between those two.
Okay.
And from that, you try to draw very rich inferences about what is in that black box.
And as you can imagine, like those are one bit experiments, that's challenging.
You can't really learn anything from a single experiment.
But the people that are great in this area do, like people like Liz Spelke and Renee
Bayajan, is they're not really drawing inferences from just one experiment.
They do a whole series of experiments, and they know other things in the background about
what infants know.
So, I was interested in that method.
That was a method that I used with Toby Mintzi at USC, and it was sort of fascinated
that scientists were inferring things like whether or not infants had object concepts.
It's like that's a pretty rich kind of representation that you're inferring just on the basis
of a kid looking longer over here versus over here.
I was interested in that and wanted to know more about the linking function between expectations
and infant's interests.
And I had that idea and then it took a few years before I could figure out how you might
be able to get at that.
I was watching a kid play whack-a-mole and objects pop out in some order, and if you imagine
whack-a-mole as I imagine there's like three moles and three holes, that's a perfect instance
in which I can imagine a way to model what you think is likely in this very limited toy
space.
So, you walk up to the whack-a-mole machine at the very onset.
You haven't observed any data, but if I ask you like, how often do you think each one
pops up?
People say like, they're all equally likely, okay, great, there's your prior.
Now you put a quarter in the machine, you see one mole pop-up, if I stop you right there
and say like, okay, how likely do you think it is, mole will be, we'll see, you've only
observed one pop-up so it doesn't change your mind very much.
But if mole pops up a hundred times, there's like more data, you come to, that shifts your
beliefs more, so we're using that setup where this is a domain in which if we just care
about the sequence of events, we can actually quantify how predictable or surprising a particular
event is.
That's a way of getting at that linking function question.
So we made a version of-
And sorry, the linking function represents what?
The linking function represents the relationship between surprise value, yeah, surprise value
for an event in a sequence in an infant's interest.
And surprise value is a formal way about trying to start thinking about expectations influencing
infants' beliefs, which is obviously true, but people weren't sure exactly what that
relationship looked like because nobody ever varies it on a continuum for infants.
And when you call this a linking function, are you trying to actually define the function
or get to, okay, yeah, these are probably correlated or these are not correlated or something
like that?
We are trying to understand how infants guide their search for information in the world.
I think it's maybe easy to forget this because we have limited attention, but at every
moment where you or an infant is looking at one thing, you're necessarily not looking
at everything else.
Each decision that you make about where you're going to put your attention or where you're
going to click or what you're going to listen to or who you're going to talk to, because
the world isn't static, comes with a huge opportunity cost.
I'm here in this room talking to you and I'm not at the conference seeing whatever is
happening over there.
I'm not looking at a poster.
So the linking function that we're interested, we're linking interested, the linking function
because we're interested in understanding given how much richness and how vast all of
the information is in the world, how could an infant possibly get started trying to make
the decisions about where they should look and when they should terminate and how could
you design a system that can go from possessing as little information as an infant has to
eventually having a not perfect, but I was like a relatively sophisticated network of
knowledge like an adult that's studying machine learning.
Even calling these decisions suggests a higher level of processing than I might think is
the case in a lot of, particularly for infants and whether they're looking at the banana
or the picture or something like that.
I don't want to, when I use that word, imply that I mean that they're conscious.
I think all of these things are happening automatically.
I use the word decisions, I was like you could use the word choice, me and Ben Hayden have
a paper in Neuron that's about how we don't really care what words people use for.
Use the word curiosity and people.
Yeah, so we've already thought of that question and wrote a paper about it.
Yeah, literally it's part of our research program in the lab to work on, yeah, how people's
concepts vary and how one two people use the same word, they're not activating the same
concept.
So, yeah, I don't worry, I don't get in fights over there.
Right, human languages.
The good point is they're looking at one thing or another thing and that's what you're
calling a decision.
Yeah, well and also.
Whatever the mechanism is.
That's correct.
And I'm using the word decision or choice because I think it's very important that if
you're going to have a smart, intelligent selection, attentional system, if you want that,
you want the same general guiding principles to maybe explain where you put your eyes,
it's like your saccades, your eye movements, where are you going to look, but then that
same system should also guide other ways in which you might sample information from
the world.
So, what you click, what you're willing to pay for if you're buying movies on some
kind of streaming services, like there are decisions that are more or less conscious,
they happen at different time scales, but if you were to design a smart system, what
it should do is seek out information that's valuable.
And what it means to be valuable is that it offers something new, but you can integrate
it with your existing representation.
So, the idea of this infant work is trying to see whether or not probabilistic expectations,
guide infants looking at all, they've been theorized to do that for a long time, and if
they do, what is the relationship between some metric-like surprise and their interest?
And what we found is that, like many people had suggested that it might be, you get a
U-shaped relationship between infants' interest and the surprise value for an event in a sequence,
meaning that infants are most likely to terminate their attention to events that are very low
surprise value, so things that are very expected don't offer you much, you thought that was
going to happen, you did.
You're not learning a lot, but on the opposite end of the spectrum, if it's too surprising,
if it's a high surprise value, you also terminate your attention, infants are most interested
at maintaining their attention when they encounter events in the sequence that are a little
bit surprising, given what they were expecting, but not overly surprising.
And is the overly surprising result, is that counterintuitive to you, or surprising at
all, or is that expected?
I think it is to some people, if you are just coming into these questions and you think,
like, what should I do?
I'm going to design a robot that's going to search for places that it should learn in
the world.
You might think, like, the most new information is the place that I should start.
I like to use the analogy of, like, you're going to pick a book to reader, a movie to watch.
If I go for the most new information that I could find, maybe you pick like a book
in a language you don't understand on a topic you don't know, and you can learn both
theoretically, but really you can't, because you're missing the base levels of representation
to make sense of it.
If you know a lot about the topic, you can probably pick up some of the words from a
language you don't understand, if you know the language and you don't know the topic,
you can make progress on the topic.
But trying to do two things simultaneously, the intuition that Maria Montessori had is
you're not going to make a lot of traction there, you're not going to make a lot of progress,
and yeah, there's something to that.
That's why that French version of Game of Thrones is sitting on my shelf, only 10 pages
having been read.
How could you speak French?
Not a while ago.
Not enough.
I see.
Yeah.
You don't learn about dire wolves in, you know, university, French class.
Yeah.
Yeah.
Most people have had the experience of trying to absorb information.
They like want to go wise at a high level, but it's just like hard to stick on it.
That's the point of all of this.
So what we're theorizing is that you have built in attentional mechanisms that are guiding
you towards material that won't waste your time.
And if you are encountering something that is a little bit below, where you're at, as
like if it's overly redundant, it's really hard to stay on task for those things.
If you encounter stuff that's beyond where you're at, that should also be similarly difficult
to focus on.
From the perspective of you're trying to not waste time moment to moment, you should seek
out stuff where you're making progress, but it's not too overlapping before you know.
And so the models that you're building is the idea or goal to advance or enhance machine
learning by applying these traditional psychological learning models to, you know, create better machine
learning models or more to try to validate concretely the things that you're observing
experimentally, you know, with these models or both or neither.
Yeah.
So it's multiple things.
Our first goal is to just understand how human systems work from a basic science perspective.
I'm very interested in understanding how people come to know the things that they know
and how beliefs that you form early influence the sampling process downstream.
I just talked about an instance in which your previous experiences shape the knowledge
and the beliefs that you have, the beliefs that you have influence what you're interested
in next, which means that little things that happen early could potentially have really
profound downstream effects.
So we're interested in these systems in humans, but we're also interested in understanding
human belief formation because we're interested in making sure that people can design technologies
that interface well with humans.
I was like, if you design a technology without respect to what we know about humans form,
how humans form beliefs, you run the risks of designing something that's pushing people
away from access to reality.
There's ways in which you might push information to form beliefs that are not right that don't
match the ground.
Truth, one of the things that I talked about in the talk was the relationship between your
subjective sense of certainty and your willingness to seek out new information and also encode
it.
Even if you're not choosing it, if you become very certain, a rational agent, shouldn't
waste time there, right?
As I talked about that in infants, the problem is that sometimes people become certain when
they shouldn't be.
John justified certainty is a thing too, and if people become certain, their curiosity
goes way down, I was like, if we present them information after they're certain, they
don't wait it in the same way, they don't attend to it, it counts for a whole lot less.
If you're designing a system that delivers information to people, it's really important
that you're aware of that.
A lot of platforms make decisions to optimize engagement and make sense that you push
content to people that they appear to once as indicated by them reading it or clicking
it or whatever, but it's potentially dangerous if in doing that, you're giving people more
confirmatory evidence than they would encounter if they sampled randomly from the world.
Our systems were not designed to have information presented that optimized our interests.
Our systems were designed to forage for information in the world.
Second reason we're interested in this is kind of from a cautionary perspective.
It's very important that we understand how human belief formation works so that we can
design technologies that don't mess it up in ways that are bad for individuals on society.
It's very bad if somebody logs on to Facebook, not sure of whether or not they should
vaccinate their kids and walks away thinking that they should not vaccinate their kids.
As people, yeah, I'm not going to, I'm not going to, people should definitely vaccinate
their kids that will go on record saying that.
I was saying if they walked away with the impression that no one vaccinate their kids, that
would be the bad thing.
Yes, those are actually, yeah, I didn't, there's something that wasn't the talk and then
I had to cut for time, you're referring to an inference about what is true for other
people in the population and I did have a slide about that, but I didn't present it in
the talk.
We showed that when somebody enters a search pretty neutral, they form beliefs very quickly.
We used the example of searching for activated charcoal.
If you search for activated charcoal, trying to figure out whether or not that's like a
useful thing to use as a start by being equally likely to say like, maybe it's good, maybe
it's bad, but in just three clicks of about two to three videos, people are all the way
up at like 80%, 90% for thinking it's probably a great thing for wellness.
And the slide that I cut, not only are they forming that belief from like, I'm not sure
to like, okay, I think this is probably right pretty quickly, they're also drawing social
inferences about the prevalence of that belief in the world, given the overrepresentation
of, you know, super scientific materials on all of the streaming platforms, this is
potentially dangerous because it runs the risk of people becoming certain before they
have the chance to encounter disconfirming evidence.
That was number two.
Number three.
I think this is the like way future one.
This is like the like we're like not near this yet, the goal downstream, way downstream
in the future, if you want to design truly, truly intelligent, artificial intelligence,
you want to understand how human systems work because we fail sometimes, because sometimes
we form strong beliefs that are not justified given reality as like sometimes we make bad
decisions, you want to understand what the pitfalls are in human belief formation so that
you can design an intelligent system that doesn't, doesn't have those.
Kind of speaking of AI safety types of research or research directions.
What do you mean by AI safety?
The, I mean, in a sense, an aspect of what you described, if we're heading in a direction
we're rebuilding AGI, what are the, how do we build safeguards into the AGI so that
we protect ourselves as humans, I guess?
I'm not so much thinking of that as I'm thinking of situations in which people don't use the
data in the way they really should.
So for example, it's well documented that doctors, although well intentioned, have gender
and racial biases that prevent them from seeing the evidence objectively black women
are much more likely to die giving birth to a child than white women are, the reason
for that is because when they report that they are in pain, when they report the same symptoms
they're not taken as seriously because of racial biases about black women complaining.
We are increasingly looking to AI to make decisions in the medical field.
People do that badly and as we're introducing AI into these processes, we ideally do not
want to replicate those bad parts of the way humans make those decisions.
So those are the three driving goals for your work.
In your talk you listed or reviewed five conclusions of your work, would you call those conclusions
of the, I would call them lessons lessons lessons walk us through those.
Number one lesson is that people are continuously forming probabilistic expectations.
They are constantly monitoring the statistics of their environment and using those statistics
to inform what they're looking at, what they're listening to, what they're integrating
into their new representations.
You don't learn the concept for something and then you're done.
We open with that.
That was the table example.
And so what were you mentioned that you've got graphs supporting all those?
What were the graphs that tell that story?
That one is the infant work showing that you can use, you can compute surprise over sequential
displays and you get a U-shaped trend between their lookaway behavior and the surprise
about you.
Got it.
Got it.
Okay.
Second point, we also covered is that certainty diminishes interest.
So the evidence for that, I just picked one example from a study by Shirley N. Wade where
we look at people's certainty as they're generating answers to trivia questions and that
the take home message is that when you're very certain that you know the right answer,
you're not curious.
You don't want that information a little bit worse than that.
As we present that information, you're less likely to integrate that.
Once you're certain, you cut and run, you move on to something else.
And that is problematic because sometimes people are certain when they should not be.
So this is a potential explanation for why people sometimes get stuck with stubborn beliefs
that aren't justified in the world.
If you're very certain, it's really hard to get people to go back and reconsider.
The third thing was that certainty is driven by feedback.
So this work actually is the project.
I think we've talked about all of the main lines of research except for this one.
This evidence comes from work by Lewis Marti in which we try to figure out when you feel
very certain where that subjective sense of certainty is coming from.
Oh, that's an interesting question.
Yeah, it is an interesting question.
It did not come out the way that we were expecting.
It did not come out the way that we were expecting at all when we first...
What does that even mean?
Coming from within the brain or something else?
When you feel, so just like how certain do you feel?
Does that influence your behavior and is your certainty some reflection of how certain
you should be given the strength of the evidence?
There are these rational models that show that how certain you should be given the evidence
are good at predicting people's accuracy when they're learning new concepts.
These studies were done in which you ask people to learn new concepts by just observing
evidence.
So as I gave them, you say you're going to learn whether or not something is daxi and
then you give them the examples of things that are daxi or not.
Sometimes the concept is...
Or there's something is what?
Daxi.
We're making up a new concept for these experiments.
We're like, not worrying about the like complicities of the world, we're just like, yeah,
experimental psych.
Just like make up something new that doesn't have any of the confounds or problems or
messiness of real world data.
Okay.
So we ask people to figure out what's daxi and then we give them examples.
And how you learn in these tasks is we just ask you a moment to moment to say like, is
this daxi or is this not?
I'm going to show you some shapes and they vary along some numbers of dimensions.
It sounds like there's going to be different colors, different sizes and different shapes.
And the concept you're going to try to infer just by guessing.
So at the very onset, is this daxi yes or no?
You can't possibly.
No.
You take a shot and say like, yes, and then you get a second and you get feedback, you
get a second example and you keep going.
And what the original studies that used these paradigm showed was that the complexity of
the concept made a difference in terms of people's accuracy as you might expect.
So if the concept is simple, if it's something like red, just varies along one dimension,
people are pretty good at learning that pretty quickly.
If the concept is something more complex, like it could be something like triangle and
red and small or it's like big and blue and whatever.
So the more logical operators, the more difficult it was for people to infer the concepts and
the worst their accuracy.
And those cases, these models were pretty good at predicting people's accuracy and the
amount of data that was required before you could figure out what daxi met.
But those same models as we were hoping would be a good predictor of how certain you feel.
We were expecting that people might be more certain than they should be, they may feel
more certain than they should be given the evidence.
But instead, what we found is that how certain you feel about whether or not the concept
you have in mind being right was pretty divorced from the strength of the evidence.
Instead, the best predictor of how certain you are was whether or not you're getting
stuff right, which is a little disturbing because if you're just saying yes or no, it's
pretty easy just by chance to get a string of answers correct.
If you get a string of answers correct, whatever idea you had in mind, when you got that string
of answers correct, you gain high confidence and the reason why that's problematic is you
don't keep sampling.
As if we give you the option of like moving on to something else you do, you leave the
task.
And you don't actually figure out what daxi means.
If we create a circumstance in which you like keep collecting evidence, you don't wait
at the same as you did before you were certain.
So we think that this is a part of the puzzle in understanding how people sometimes had
their e-stubborn beliefs that aren't justified given evidence in the world.
You start with some idea.
If you get a few pieces of feedback that are consistent with that, you may develop a
high degree of certainty and once you've done that, it may be hard to go back and revise.
This links in with like reason number two, why can't we do this kind of research.
This maybe was less common when you were walking around the world, sampling from, I don't
know, I hesitate to use the term natural environment, but an environment that's not optimized
what you want to, like I'll call it that.
Now if you're going online to form your beliefs, if you have some kind of idea and you watch
some YouTube video, maybe you think like maybe the earth is flat, let me search for that.
You get a few videos that are consistent with that.
The risk is that you could develop a high degree of confidence that that's correct and
once you develop a high degree of confidence, you feel very, very certain your curiosity,
your interest in revising plummets and you may get stuck with that wrong belief.
Do you think that these kind of models for belief formation are inherent and therefore
kind of unchangeable or can we possibly as humans adapt to these new environments that
we're in that are kind of optimizing around our attention and priors?
That is an excellent question that I would like to know the answer to.
As my whole lab, let the lab do the answer to, this is a question that one of the lab
members led one of the people that's now in the lab working on this stuff to the lab.
One of the people in the lab that's a research scientist is someone named Adam Conover who
is predominantly his background is in science communication and also comedy and he was
very interested in how people might have a shot at not forming bad beliefs online.
An idea that we plan to test but are still working out right now is whether or not these
tendencies people have may be able to be mitigated if they're aware of human belief formation
processes or maybe alternatively if they're aware that the systems that are giving them
information aren't the same as the natural world outside.
I have extended family members that do not understand that when they go on Facebook,
this is not a true representative sample of opinions in the world.
It's like they, you know, we might think they should know that as like they should know
that they're only seeing content from people that they opted into seeing they don't understand
the algorithms that back the system, they don't understand that what they hang on longer
influences what they're more likely to see next.
So instead, they're drawing inferences under an incorrect assumption which is that what
they see represents what's true in the world, that's how our systems are made.
It's possible that understanding either something about your internal system, you could maybe
make those bad tendencies like to fall beliefs that are not justified, maybe you could
out mitigate some of those, it's also it's also possible that knowing something about
the back end system, you could adjust, but we really don't know.
I'm revisiting like a routine or an experiment that uses the results of surprise all to kind
of train people to shift their internal belief distribution or something like that.
Right, yes, people are sensitive to their capable of comprehending information about distributions
and drawing correct inferences under different assumptions like replace versus not.
So the fact that we as scientists are capable of learning statistics, it's obviously possible
how much of a difference it could make in these day-to-day moment-to-moment decisions
that you're forming beliefs constantly.
So is yet to be seen, but there's some precedent from the implicit bias literature.
If you are trying to make people less racist, the worst thing that they can do is say like
I'm not racist, I don't see color as I'm not considering it.
If you point out to people that they may have implicit biases, they're not conscious,
but they're influencing their decision-making processes, it doesn't undo the biases,
but they're lessened just through that knowledge.
So it's possible that people knowing something about how they work and how they form beliefs
could make a difference, but we don't know.
All right, so we're on number three, number four.
I talked about the influence of feedback and feedback being the primary driver of how
certain you feel and why this is problematic for forming beliefs everywhere, but especially
on the internet, point number four is that in the absence of feedback, maybe you think
like if the feedback's problematic, we'll just remove it, but that's probably not right
either.
In the absence of feedback, people seem to be overconfident.
And our evidence for that is we look at situations in which people don't get very much feedback.
We wanted to do something naturalistic and more kind of real-worldly.
This is work that was led by Louis Marty that was published in OpenMind very recently.
The domain we came up with is when two people use a word, are they activating the same
concept?
So this is relating to the concept stuff that I talked about.
I talked about how the first set of findings is about how when two people use a word,
they have different concepts in mind.
So like two people don't usually have the same concept in mind for abstract political
kinds of concept like Joe Biden, but they also don't have the same kind of concept for
table either.
We were expecting to find more disagreement about abstract things like people in politics
and less disagreement about concrete objects because you can observe them, but even for
things like table and chair, those things also mean different things to different people.
So many questions that I'm not going to ask due to lack of time.
Oh, you can't.
You want to hear about something else.
We were interested in this, so it looks like people have, there's more than one concept
in the population.
I was like we tried to, we used tools from ecology and various clustering techniques for trying
to find in for the true distribution of clusters of beliefs in the population.
And when you do that, you get something like five to ten for most concepts, although
they're not all the same.
I was like, there's more disagreement about it.
What are examples of the five to ten beliefs people have about tables?
Well, so the data from this, you know, the question that I wanted to ask that I said that
I wouldn't ask it, you know, how do we know that it's not just differences in the way
we describe things as opposed to the fundamental inherent belief about this thing?
Because we control the context, actually, I don't know if that, I started saying that
and then I'm not sure if that's actually the answer to your question.
We get this data, we didn't want people, so first of all, that we don't ask about words
that have synonyms because it could be that it looks like two different concepts, but
people, that's like a different weird situation.
We're asking about things like penguins or people in politics or concrete objects or abstract
concepts depending upon the particular experiment.
We're controlling the context to minimize the chance that when people appear to have two
different concepts, it's because they're imagining a different situation.
Like, instead, we're getting this data from like, here's Donald Trump, who is he more
similar to when you just pick one or the other, so we're fitting the data to binary vectors.
And we're thinking about the concrete example, like a table, I might describe it functionally,
it's the thing that you put things on, I might describe it structurally, it's a thing that
has legs.
That variation, I would say, falls under the context in which you're thinking of a table
and we're controlling for that by saying just pick is a table more similar to a glass
or to a tape recorder.
So we're getting around that by just having you make a judgment with respect to the other
objects to control for exactly, well, that problem and then also the like people imagining
a different situation.
We wouldn't want to conclude that people have two different concepts of table because
somebody is picturing a table that's like sitting there and somebody else is picturing
a table that like someone is standing on, so it's more like a stage or something like
that.
So we control the context by just having you judge how similar a table is to like a glass
and then hopefully everybody has, that's like pretty much the same concept in mind.
I should also maybe add that for this work, in the past most people have thought about
concepts like table as being relatively stable.
It's less important exactly how many concepts there are and more important that there's
not one.
So the way people usually think about these things is just one, instead your concept of
table is slightly changed by every table you encounter in your life and what that means
is that what you'd accept as a table as opposed to a stage, maybe table is a weird example
unless you're going to make a distinction between like our coffee table and like a dining
table.
Right or like a high top.
Right, yeah, a cup of cup of bowl is a classic example.
I mean, I'm imagining many conversations I've had with my wife who grew up in a different
area of refers to things slightly differently and you know, we have these circular
commercial, that's not a XYZ, that's a, you know, ABC, you know, it's a XYZ.
Yes, yeah, that's exactly the, there were some classic studies in psychology that thought
of like cups and bowls is happening on a continuum.
So what counts as a cup and what counts as a bowl maybe depends partially on the material
but then also like the width versus the height, but also like the absolute size.
Or if all the cups are dirty, then my daughter was known to have drunk out of a bowl.
Correct.
Correct.
Now it's a bowl.
Yeah, that's great.
All the bowls are dirty.
Now this cup is a bowl and I've also pulled that one with small children.
So yeah, so the where you draw the line, you might expect people had pretty similar
places, but it looks like it's more different than you might expect even for the concrete
objects.
So where I would draw the line at the cup bowl distinction depending on the relationship
of absolute size and height and width is probably different from some substantial portion
of the population and where people draw the line along all of these dimensions that you
can cluster them together and try to come up with an estimate for roughly how many overall
concepts there are.
Okay.
So yeah, big point is there's not one.
Right, right, right.
Like for even things like penguin and cup and bowl and the question we were most interested
in asking is given that there is variation when two people use one word, they're not necessarily
activating the same concept, are people aware of that possibility?
Like do I know when I say something, you may not have the same thing in mind?
And the answer.
Back to points two, three.
Right.
The answer is no.
There's a really fun like sub-finding which is people generally overestimate the degree
to which they believe that their concept will align with somebody else's.
But if you have a weirdo deviant definition of something, if you're activating a concept
that's like very different from other people in the population, you actually have a better
chance at being aware of that, which is not what I would have guessed if you talk to somebody
that's like using words weirdly, like you might expect it's because they are unaware of
that deviant usage as like they're more likely to be aware that that is a weird way of,
that's a weird concept that they have.
Is all this related to the, I forget the name of the law, like people who, you know, experts
perceive there.
Oh, done in Kruger.
Yeah, done in Kruger.
Yeah.
It's possible.
There's definitely connection to this.
So the done Kruger effect is if you are very competent, you're more likely to be aware
of the areas in which you have incompetence, but if you're totally clueless, you are not
aware of all the words.
So if you're estimating your own competence, it doesn't go well given those dynamics.
So I don't know, it's not exactly the same thing, but that is an interesting point.
You might think that it's a bad design if people, their estimates of their own incompetence
are not well matched to how incompetent they actually are, but Charlene Wade's research
from my lab showing that people are most curious when they believe that they're about to
know the answer, when they believe that they're about to know everything.
Those two things taken together might actually indicate that that's a feature, not a bug.
If you at the beginning of learning something new, you were acutely aware of how incompetent
you were.
I was like, what are data from the lab would say is that you would not be motivated to
take that first step.
You may never try to approach it.
It actually may be a good thing from a motivational standpoint that when you first start out
learning something new, you don't know how much you don't know as to have your incompetence
revealed incrementally, we would predict is the gives you the best chance of continuing
support.
It's a very quickly that fifth point.
That fifth point, people form beliefs very quickly.
It's possible for people to go from completely undecided to believing that they should buy
the weird black smoothie bagel, those things that are in the expensive coffee shops that
have charcoal added to them for unclear pseudo-scientific wellness purposes.
It doesn't take very long to form that belief with high confidence and so when we're designing
systems that offer information to people, it's really important that we keep that in
mind when somebody's certain, they stop searching, when they see disconfirming evidence after
that point, it doesn't count for the same.
So I think the takeaway of all of this, the big point that I wanted to make in the talk
was that when you hear people say that this tool or this platform is neutral, that's,
I would say dishonest, we know that there are ways of making decisions behind the scenes
that influence people's behavior.
It's like all of us design things that change behavior as like whether or not you're making
different decisions about how to present different options of suggested items that people might
purchase or whether you are trying to keep people on your site for longer.
If people's behavior is changing as a function of different design decisions you make or
different types of things that you optimize, it's important to remember that the mediating
variable there is human beliefs, you're messing with human beliefs and you're messing with
what people know and they walk away into the real world to make real world decisions with
those changed beliefs.
So there is no neutral platform, I think that's a really ill-conceived way of thinking about
these problems, I think it's irresponsible, it's really important to appreciate that the
way you present information, the order in which you present information, it really matters
and it has profound impacts.
It's a really important result, I think we tend to think about the kind of influences
you're describing as just influencing discrete behaviors or actions as opposed to fundamentally
beliefs.
Right.
And they're very different.
And you can't change behavior without altering people's beliefs and it's important to
keep that in mind and to treat that duty with the respect it deserves.
Right.
Really interesting stuff.
We could go on for another hour, I'm sure.
Yeah.
Well Celeste, thanks so much for taking some time out of your busy nirups to come chat
with us and share a bit about what you're up to.
Of course, thank you so much for having me.
Thank you.
All right everyone, that's our show for today.
For more information on today's guest or our NIRP's podcast series, head over to
twimlai.com slash NIRP's 2019.
Thanks once again to Shell for sponsoring this week's series.
Check out the Shell.ai Residency program by typing Shell.ai into your browser's address
bar.
Thanks so much for listening, happy holidays, and catch you next time.
