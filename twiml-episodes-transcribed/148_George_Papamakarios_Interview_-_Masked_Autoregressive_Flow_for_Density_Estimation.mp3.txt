Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode, University of Edinburgh PhD student George Papa Mercarios and I discuss
his paper, Masked Autoregressive Flow for Density Estimation.
George walks us through the idea of Masked Autoregressive Flow, which uses neural networks
to produce estimates of probability densities from a set of input examples.
We discuss some of the related work that's laid the groundwork for his research, including
inverse autoregressive flow, real MVP and masked auto encoders.
We also look at some of the properties of probability density networks and discuss some
of the challenges associated with this effort.
This Friday, June 1st, we'll begin working through the fast AI practical deep learning
for coders course.
A bunch of you have already signed up and the Slack channel has been hosting some very
lively discussion.
If you'd like to learn deep learning with us, there are just three simple steps to join.
First, sign up for the meetup at tumma.ai.com slash meetup, noting fast.ai in the what
you hope to learn box.
Second, use the email invitation you'll receive to join our Slack group.
And third, once you're there, join the fast AI channel.
I'll be publishing some additional information about how we'll work through the course and
the next day or so, so stay tuned.
If you have questions, feel free to shoot me a message on Twitter at Sam Charrington or
via the contact form at tumma.ai.com.
See you soon.
Finally, we continue to celebrate the second anniversary of the podcast and we want to
hear from you.
Have you found this podcast useful?
If so, we want to hear how.
Post a comment over at tumma.ai.com slash 2av to let us know.
And now on to the show.
All right, everyone.
I am here at Nips and I'm with George Papamacarios.
Good to be here.
George is a PhD student at the School of Informatics at the University of Edinburgh.
And he's here at Nips, having just presented a paper on mast auto regressive flow for
a density estimation.
And I'm looking forward to learning more about what that's all about.
George, welcome.
Great to be here.
Thanks for inviting.
Absolutely.
So, you're in the School of Informatics.
How did you get involved in, you know, what's the intersection between Informatics and,
you know, your research and how did you come to get interested in machine learning?
Informatics is quite a general time.
In Edinburgh, where I'm doing my PhD, the School of Informatics, essentially came to be
by combining computer science, artificial intelligence, and cognitive science, and an
umbrella term for everything that has to with information, either artificial information
or, you know, natural information, came to be known as Informatics.
So my background, though, was in, actually, was in electrical engineering.
So I did my undergraduate studies in electrical engineering.
And then I, after I graduated, I was working as a research assistant doing computer vision.
And I was actually working in a project where it was a fun project.
We were trying to detect activities and classify activities.
So we had, in smart homes.
So we had, we put sensors and cameras in, you know, people's flats.
And we could, you know, take measurements such as temperature and humidity, and we could
track the person's location in the flat.
And we wanted to infer what they were doing.
Were they sleeping, watching television, doing the dishes, eating, et cetera.
And that was a fun project.
But it became obvious to me that it was very difficult to, you know, to figure out what
the person was doing and being certain about it.
Right.
Because they could be doing anything, right?
Does it write in temperature mean, for instance?
And so that's how I got into machine learning and tried to infer information from data in
a more principled way, and trying to quantify the uncertainty of that information.
For instance, a person might be doing, you know, might be doing the dishes, but I was
sure that they're doing the dishes.
Or maybe I'll be cooking.
Or maybe they're cooking.
Or have to be, we have to be able to quantify uncertainty.
That's how I got into machine learning and in particular, based in inference.
And that's what brought me to Edinburgh to a PhD.
How is that interest kind of flowed into your current line of research?
So my, so I got interested in based in inference, which is a principled way of making predictions
at the same time, quantifying uncertainty.
And this has all to do with, you know, probability distributions and probability densities.
And, you know, a lot of probabilistic modeling.
And it's difficult to get these probabilities.
We have to, how do we, how do we obtain the probability of data in the first place?
And that's what brought me into doing research in density estimation, which is what my,
which is what the research I'm presenting here at NIPS is about.
And here we're talking about probability densities as opposed to material.
That's correct.
That's correct.
And so the technique you're using to estimate probability densities is mass, autoregressive
flow.
What does that mean?
That's correct.
So, mass autoregressive flow is, it's a deep neural network.
That's why it is.
So, it's a deep neural network, which takes data point, could be an image, could be a
time series, could be a piece of information, and spits out a number.
And that number is the probability density associated with this data point.
Essentially, it's a score that says how likely is this data point.
For instance, if I'm trying to learn the probability density of our images, an actual
image, like say an image of a person, I would score high so that the network would return
a high score.
And a random image, an image of random noise would return almost zero score.
That tells you how plausible, how likely the image is, so that's what probability density
is.
And you can learn, you can train such a network directly on data and get an estimate
of what their probability density should be.
And so, the probability density that the network is producing is a vector of, what does it
look like?
I guess, when I think of probability densities, I think of them in terms of, from an analytical
sense, like, is it, you know, it could be a Gaussian with a certain set of parameters
or some other kind of probability function.
Is this telling you the distribution as well as its parameters or is it telling you something
else?
Yeah, there are many ways to go about doing this.
So it could be that the neural network spits out the parameters of a distribution.
For example, if that distribution were a Gaussian, it would spit out the mean and the variance
that Gaussian, which defines that Gaussian.
There are many ways of doing this, the challenge here, so if you chose, let's say, a particular
type of distribution, say, a Gaussian, that network would only be able to spit out
Gaussian's.
Right.
The challenge here is to make it more flexible, because the data might follow a different
distribution, right?
So how can we make it flexible enough so that it can spit out whatever distribution, the
data happens to follow?
And that's the challenge here, and that's what this paper was about, essentially, was
proposing a very flexible mechanism, so a particular architecture that would spit out
parameters that would describe a distribution that was very flexible.
Is it picking from some catalog of distributions that it knows about, or is there a type of
distribution that is essentially high-dimensional, very flexible, lots of parameters that every
other distribution is kind of a degenerate version of, if you will?
It's a bit like, well, it's more like the second one.
So it is parametric, and it is described by a set of parameters, but these parameters
can adapt themselves in what ways.
So they can change, so you can imagine.
They can change in value, or they can change in meaning in a patient.
In value.
So you can imagine a distribution being sort of like a rubber sheet that you put over data,
and you can stretch that and squash it in many ways, and that's what the parameters
describe.
The stretching is the squashing of this flexible object, and by transforming this object,
you can form it into a distribution that fits the data well.
So that's the idea.
This parameters describe how I would need to stretch and bend a simple distribution like
a Gaussian in order to get a complicated distribution.
That's essentially the idea.
What's this distribution called?
This distribution is the model, is mass-totary-gressive flow.
So as part of this research, you identified or created this distribution, but also demonstrated
how to calculate it from a data set.
So mass-totary-gressive flow is essentially this neural network that squashes and stretches
a simple distribution to form it into a more complicated distribution.
It's free to learn what transformation to do.
So you don't know what the end distribution is going to look like.
But you know that the more simple distribution, the starting distribution, you know that.
Just so I'm clear on this, is mass-totary-gressive flow your creation or is that an existing thing
that you then applied to density estimate?
This is my creation, which was inspired by very similar models in the literature.
So there was a model before that, which was very similar, which is called inverse autoregressive
flow, another model, which is called real MVP, and another model, which is called massed
autoencoder for distribution estimation.
So all these are, you know, in the same family of models.
And this one is essentially a continuation of the same idea and an application of previous
ideas in the problem of density estimation.
And the paper, did you, did you base your results on a particular input distribution,
such as a Gaussian, or did you do something else?
The paper did use Gaussian's as, you know, as base distribution, but that's fine because
the idea of these models is that they're not limited by what a distribution you start
with, because if you, you know, form it, if you transform it in many ways, you can in the
end get almost any distribution you like.
So in the paper, what, what we did is we took various data sets, some of which were images,
like MNIST or CIFAR, others were time series, and there were two more data sets that
described collisions in particle physics. So these data sets are all publicly available.
And we trained the same model, so same, same neural networks, same deep model to each one of them,
and saw that it works well across a range of, of domains. So it's not particularly engineered
to work well with images or anything. So one, one of the motivations in the paper was to
demonstrate generality as well.
And what was your metric of working well?
Test log likelihood. So having trained this deep neural network on training data and having,
you know, tried to estimate the probability density in training data,
does that probability density assign high scores to held out data? So this is the metric.
So if we, let's say, have a data set of images, and we know they should all have high scores
because they're all plausible images. And I, I don't use like 10% of them for training,
then the model I learn should assign high scores to these images that I held out as well.
And let me take a step back.
In this case, the, what does it mean to have a score for a given?
It's the, the score is the, the value of the probability density.
The value of the problem, the value of the probability density where?
Or for what?
At the, for that particular data point. So the probability density assigns a value to each data point.
And these values should be high for, for, for data points that are typical,
natural looking, if, if we're talking about images, and very small, unlikely objects.
So in the case of your test data, you've got an image, a complete image. And when you talk about
the score, are we looking at like an arbitrary pixel and the, the probability value for,
we're looking at the whole image, the whole image. And so kind of an, an average score across
all of the pixels or kind of, yeah, kind of, kind of each pixel would get an individual score.
And the, the, the product of the scores would be the score of an image.
And so how's this, how would you envision this being applied?
That's a very good question. So, so what, what I described is one
way to do unsupervised learning, right? So in unsupervised learning,
what you have is, what you have data, like unlabeled data, like a bunch of images,
that you downloaded from, from the web. Yeah. And you want to know something about the data.
You don't have a specific, you know, goal in mind. You, you want to learn something about
the structure of the data. So one way of doing this is by density estimation, because the density,
which the probability density that describes the data probabilistically tells you that structure.
So it's one way of doing generative modeling and unsupervised learning.
For example, GANs and VAEs would be similar ways, but slightly different approaches.
But what I'm particularly interested in and the reason I got involved with density estimation
to begin with is for doing inference, as I said at the beginning. The probabilistic way of doing
inference is, you know, recovering distributions of unknown information. So if we want to,
for example, in the, in the examples I was, I was talking about in the beginning,
where you want to infer what activity a person is doing. Mathematically, what you want to recover
is a distribution over possible activities. And you could use these type of models. So these density
estimators, these neural networks that learn densities in order to learn densities over things
that you, that you don't know that you want to recover. So this is, this is my motivation of
using neural density estimation to do probabilistic inference. So I'm thinking of applications that,
you know, we see a lot of, for example, GANs apply to, you've got incomplete images and you're
trying to reconstruct those images in some kind of way and you want to infer some of the missing data.
That's an example of how you might apply this. That would exactly be, yeah. So for example,
you've, you've seen half of the image and you want to know, you want to infer what the other half
would be. So what you want is a distribution or a density. Right. Over the pixels that you haven't
seen, for instance, and that could be done potentially with this kind of models. So the, the density
that you, this, the mass-auto-regressive flow distribution, how many, does that have a fixed
number of parameters or a variable number? It has a fixed number of parameters. This is a
design choice. So it's, it's, it's a neural network, it's a deep neural network, which
essentially implements this transformation of the base density. And the parameters of that neural
network is the parameters of density. So then maybe not parameters in the way we're used to
thinking about them for probability densities, but you know, they are hyperparameters for a neural
network. They are, they are the, the weights and the biases of that neural network. It's, it's
difficult to, to interpret them. Yeah, that was my question. Like, is there like an intuitive,
you know, transformation between those and what we might think of as the, you know, the parameters
of a probability distribution. Yeah. Sounds like no, not really. It's, yeah, yeah. So if we,
let's say if the probability distribution is a Gaussian, then we can interpret the parameters.
So the mean is, you know, the descent multiplication. Right. Yeah, standard deviation is the width.
Right. But now it's difficult to interpret. What, what, what are the parameters of the neural
network that, you know, describes this density do? Yeah, you're right. And so is, is it even correct
to think of this, these parameters as describing a density as opposed to describing a machine that
spits out densities? It's, it's, it's, it's how you think it's, it's, it's, it's, it's, it's mostly
semantics. Yeah. And you can think of them either way. Okay. I think. And so what's the
dimensionality of this parameter space? It's, it's huge. It's, it's, it's, however big you design
your neural network to be. So it's, think of it as, it's, it's, it's a modification over a standard
fit forward neural network. And the parameters are the weights and the biases of this neural network.
So you could have potentially thousands or millions of parameters. And so what, what characterizes
the architecture of this network relative to others? So, um, if I, let's say I take an arbitrary
fit forward neural network, like standard, you know, standard stuff that we use, for example,
for regression. And what we want, if, if we want this network to represent, if we want the output
of this network to represent a probability density, then it has to have some properties.
The first property is that it has to be non negative. And the second property, which,
which is a difficult one, is that it has to, the, or the set of all outputs, the set of all
possible probabilities have to sum to one. And this, these two properties are not, not trivial
to ensure. And that's exactly what the research is about. How, how can we design the architecture
of this neural network? So that's kind of your starting point. Yeah. And you design from there.
Yeah. There are other requirements like, uh, that the outputs have to be continuous. Like,
is that of interest or is that trivial? They will be continuous. And they will be
differentiable because otherwise we wouldn't be able to learn the neural network. So given that,
the, the functions represented by neural networks are differentiable. Yeah. Then, yeah,
this is a requirement for being able to learn it. Right. Well, by learning, I mean, you know,
by training. Yeah. Yeah. So like, what was your, what was your process in
then architecting a network to meet these criteria? There is a, uh, there is a line of research.
For instance, the, uh, the methods I, I mentioned before. So essentially, we, we continued from,
you know, rely on the shoulders of giants. Sure. People have, they've been there before you.
Um, so there were, in the literature, there were two types, um, of models. The first one
is called autoregressive model. And the second one, the second one is called a normalizing flow.
Um, and these were, these are ideas of how to design the neural networks to satisfy these
requirements. So what are these individual ideas mean or represent? Yeah. So the, the, the
autoregressive, um, model is essentially a, it's a neural net that has as many outputs as inputs.
And each output is, uh, the parameters of a simple density of the corresponding input.
So for instance, the, the first input would be a mean and a variance of a Gaussian for the,
uh, so the first output would be a mean and a variance of a Gaussian for the first input.
And the second output would be a mean and a variance of a Gaussian for the second input.
So they essentially break down the problem in estimating simple, simple densities for each
variable for, for each input. If they, if the input were an image, then that would be individual
densities over each pixel. So they break down the problem, the problem of estimating,
enjoying density of all inputs into the simpler problem of estimating a density of one,
of only one input at a time. So that's, that's the idea. Do you keep the parameters at each
level, or are you only keeping ones at the end? Or you keep, uh, keep all of them, right? Yeah,
that's your, yeah, so your information comes from. Exactly. Okay. Got it. Um, yeah. So the collection
of all of them, right, describes your density. Okay. So that's auto-aggressive models. And the,
and, uh, there are many examples. So wave units is, is a model like this. Okay. Pixel RNN is a
model like this. Pixel RNN. So these, these, these, these models are great learning complex
distributions over images or sound or, uh, other types of data as well. Um, so that's one, one
approach. The other approach is the normalizing flow approach. This is essentially what I was
describing before. These, these, these take, these, these neural networks take a, a random
input coming from a simple distribution like a Gaussian, and they transform it and produce a
data point. So essentially, they implement what I was describing before they take a simple density,
for instance, a Gaussian, and transforming it, stretching it, like a rubber band, you know,
in order to form a different density. Um, so real and vaping was this type of model. So our work
is, was based on the idea that we can actually, before you, before you go to your, the, this
latter model normalizing flow is a normalizing flow. Are you, is your, your input data just sampling
from a input distribution or is there some kind of training data set or something? It's the random
noise from an input distribution. It's like the generator of a gun. So it's just random noise.
Um, and the output is data. Okay. Um, and so now work, what, what we, the idea, um, that we use
is that you can actually, it turns out that some autoregressive models are actually normalizing
flows. So there's a deep connection between this two type of model, which was really cool.
And that gave us an, an idea of how to make normalizing flows better. And the idea was to use
autoregressive models as normalizing flows. So that was, and that, that's exactly what we
called this thing, an autoregressive flow. And so when you, you started with this idea of making
them better, what was, what were the particular things that you wanted to improve on? So, um,
the, the challenge with designing, uh, neural density estimators, neural networks that,
estimate densities is exactly that it's difficult to ensure that the outputs are actual densities,
that they satisfy this, this, this properties of summing to one and being non-negative. Um,
and if you try to impose restrictions on the architecture in order to achieve that,
you end up hurting expressivity. So you end up making the networks very rigid, um,
very difficult to express complicated distributions. That's why we're trying to improve, we're
trying to improve, uh, the expressivity of these type of models without hurting, um, without,
violating, violating, well, exactly with, uh, violation of the requirements. And when you hurt
expressivity, um, does that lead to a model that tends to underfit, a model that doesn't generalize?
Exactly. What are, what are all of the above? Yeah. Uh, uh, underfitting could happen because
the model is not expressive enough. Right. To represent a complicated density. So, for instance,
a complicated density can have a, you know, a very strange shape in space, right? If, if the,
if the model can only represent gousians, then it will underfit, it will,
right, you know, uh, force fit everything. Exactly. Yeah. So you will try to sort of cover
everything, in the space, and it will end up, um, assigning lots of probability to
unlikely, uh, objects, like unlikely images, for instance. Yeah, that's what underfitting means
in this, in this context. Are there other areas of your paper or presentation that we haven't
covered thus far? That's well, that was pretty much, uh, everything. And so this work is, uh,
uh, kind of a stepping stone and a sequence of stepping stones, you know, where do you see it going?
Um, I'm hoping that it will, it will prove useful in, uh, doing inference.
Uh, and they're, there are many ways you can use it for doing inference. So last year we,
say, last year nips, we had another paper where we used a neural density estimator
to learn, uh, posterior distributions. So that would be one way we can use it to, to do inference.
What's the difference between learning a density estimator and learning in the posterior?
The posterior is a density. Uh, it's a particular, uh, particular density that describes
your beliefs about something you haven't seen. For instance, if you have an image and you've
observed, you have half of it and the other half is corrupted. Then the density of the corrupted
bit of the image would be the posterior of that, uh, part of the image given the, the part you've
seen. Uh, so that, that would be a posterior. Um, but we use that same example to describe or to
illustrate the work that you've done. Uh, so, uh, but it sounded like you were drawing a distinction
between the work that you done and the work that you did, uh, with this paper and the previous one
that was focused on, uh, uh, past, past areas. What's the distinction between the two?
So the distinction is that the previous paper was, was mainly on how to train it for inference,
how to train, it's a given and you're a density estimator. How can you train it, uh, to learn
posterior? Uh, and this one was, how can we design a good density estimator? Got it, got it, got it,
got it. So if we have a good one, we have a good density estimator and we also have a good way
of training, training it to learn posterior. Right. Maybe we can, then we can actually use the
reference. Yeah, we can use the reference. Exactly. So that's the idea. Okay. Um, how did you benchmark
the performance of the, uh, the, uh, the mast auto regressive flow model? Uh, the, the benchmarking
was, was done on general purpose density estimation tasks. So it was, what I was describing before,
we took with these three different tasks. Yeah. We took a number of data sets. And so how well,
how well mast auto regressive flow can fit the density of, of the data set. The density of, of
data is something we don't know, right? It's, we know that, you know, we, we stipulate that, uh,
that data comes from, from a density, um, but we don't know which, what, what, what it is,
and we try to learn it by, but by these type of models by mast auto regressive flow. And if it has
done a good job at learning the underlying density of the data, then it should assign high density
to unseen data. And that, that's the, uh, that's, that's a, uh, benchmark, uh, we can use to test these
models. Uh, and so presumably you compare the results you saw with the, the previous generations of,
yeah, exactly, perform favorably. Yeah. Um, competitive. Okay. It's, it's not clear what's the best
model for, so given a data set, right? It's not clear what the best model is because depending on
the structure, some, some density models are better at, um, estimating cluster structure, structure,
uh, other, um, models are better at estimating manifold structure. And some models work better in
high dimensions, some models work better in low dimensions. So it's not easy. It's, it's, it's difficult to
design a, a model that rules them all, right? That fits everything. Um, so there is, there's always, um,
the question, what do I use for my data set? Uh, we, we saw that mast auto regressive load does well
across a range of data sets, but it's not necessarily the best for each particular case of steel.
If, if I'm interested in a particular data set, then I, I need to think, um, what I should use.
If I'm interested in a range of data sets, then something like mascot or aggressive flow,
as a starting point is something I would probably use. And did we explicitly cover what the
mast in mast auto regressive load, uh, where that comes from? Um, so this comes from a previous
model that we used ideas from. So that previous model was called a mast auto encoder for distribution
estimation. So we took the mast bit from that. And so that model was an auto regressive model.
So it predicted one density at a time. So one, one density. So it had one output for every input.
And that output was a density over the corresponding input. So the, the, the restriction that these
models have is that the, so the, the fifth output is only allowed to be connected to the first four.
It's not allowed to receive, um, input from the fifth, the sixth, the seventh, uh, input. And the,
in, in that, uh, in that particular paper, they, they enforced this restriction by masking out
weights, connections in the network by removing, uh, connections. And that forces kind of like a
summarization at different levels or, uh, it forces, it forces the concentration or something.
It forces, uh, that the fifth output doesn't change by looking, so by looking at the, at the
actual value of the fifth input. Um, that, that's, that's what, that's what it does. And it's,
in, in technical terms, what it does, it, it, it decomposes a, a joint density into a product of
conditionals, um, each, uh, output density is one of those conditionals. And conditionals are,
um, conditioned on only previously seen inputs and not on future ones. Um, so masking was the,
the technique they used for, for doing the sense, it's a very clever idea. And we, we took this idea
from there. Awesome. Well, George, thanks so much for, uh, taking some time, uh, to chat with me
about this. I really enjoyed learning about what you're up to. It was a pleasure for, uh, for me.
Great. Thank you. Thanks very much. All right, everyone. That's our show for today.
For more information on George or any of the topics covered in this episode,
head on over to twimmolai.com slash talk slash 145. Also, take a moment to show us some love for
the podcast's second anniversary and share how it's been helpful to you over at twimmolai.com slash
2av. And finally, thanks so much for listening and catch you next time.
