WEBVTT

00:00.000 --> 00:16.120
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.120 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:33.840
Contest alert.

00:33.840 --> 00:38.960
This week we have a jam-packed intro, including a new contest we're launching.

00:38.960 --> 00:43.040
So please bear with me, you don't want to miss this one.

00:43.040 --> 00:46.720
First, a bit about this week's shows.

00:46.720 --> 00:51.440
As you may know, I spent a few days at CES earlier this month.

00:51.440 --> 00:56.400
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,

00:56.400 --> 01:01.040
and I'm including you in those conversations via this series of shows.

01:01.040 --> 01:05.440
Stay tuned as we explore some of the very cool ways that machine learning and AI are being

01:05.440 --> 01:08.600
used to enhance our everyday lives.

01:08.600 --> 01:13.680
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered

01:13.680 --> 01:15.280
robot.

01:15.280 --> 01:22.120
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.

01:22.120 --> 01:28.120
Intel, who's using the single-shot multi-box image detection algorithm to personalize video

01:28.120 --> 01:31.680
fees for the Ferrari Challenge North America.

01:31.680 --> 01:36.480
First beat, a company whose machine learning algorithms analyzed your heartbeat data to

01:36.480 --> 01:42.360
provide personalized insights into stress, exercise, and sleep patterns.

01:42.360 --> 01:48.400
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams

01:48.400 --> 01:52.280
or automatically adjusting high beams to the U.S.

01:52.280 --> 01:59.480
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to

01:59.480 --> 02:05.640
enable some really interesting home automation and healthcare applications.

02:05.640 --> 02:11.000
Now, as if six amazing interviews wasn't enough, a few of these companies have been so

02:11.000 --> 02:15.520
kind as to provide us with products for you, the Twimmel community.

02:15.520 --> 02:19.360
And keeping with the theme of this series, our contest will be a little different this

02:19.360 --> 02:20.360
time.

02:20.360 --> 02:25.400
To enter, we want to hear from you about the role AI is playing in your home and personal

02:25.400 --> 02:28.640
life, and where you see it going.

02:28.640 --> 02:36.240
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera, and

02:36.240 --> 02:39.120
tell us your story in two minutes or less.

02:39.120 --> 02:43.400
We'll post the videos to YouTube, and the video with the most likes wins their choice

02:43.400 --> 02:50.480
of great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.

02:50.480 --> 02:55.040
Submissions will be taken until February 11th, and voting will remain open until February

02:55.040 --> 02:56.040
18th.

02:56.040 --> 03:02.360
Good luck.

03:02.360 --> 03:06.560
Before we dive into today's show, I'd like to thank our friends at Intel AI for their

03:06.560 --> 03:09.520
continued support of this podcast.

03:09.520 --> 03:14.760
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving

03:14.760 --> 03:17.200
and VR related announcements.

03:17.200 --> 03:21.440
One of the more interesting partnerships they announced was a collaboration with the Ferrari

03:21.440 --> 03:24.720
Challenge North America race series.

03:24.720 --> 03:29.440
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience

03:29.440 --> 03:35.360
more personalized by using deep computer vision to detect and monitor individual race

03:35.360 --> 03:40.640
cars via camera feeds and allow viewers to choose the specific cars feeds that they'd

03:40.640 --> 03:42.760
like to watch.

03:42.760 --> 03:47.080
You'll learn much more about this application in today's show, which features Intel's

03:47.080 --> 03:49.800
Andy Keller and Emil Chinnicki.

03:49.800 --> 03:55.040
Andy is a deep learning data scientist at Intel, and Emil Chinnicki is senior manager of marketing

03:55.040 --> 03:57.120
partnerships at the company.

03:57.120 --> 04:02.080
In this show, Emil gives us a high level overview of the Ferrari Challenge partnership and the

04:02.080 --> 04:04.560
goals of the collaboration.

04:04.560 --> 04:08.920
Andy and I then dive into the various machine learning aspects of this project, including

04:08.920 --> 04:13.880
how the training data was collected, the techniques they used to perform fine-grained object

04:13.880 --> 04:19.280
detection in the video streams, how they built the analytics platform, some of their remaining

04:19.280 --> 04:21.960
challenges, and more.

04:21.960 --> 04:23.960
And now on to the show.

04:23.960 --> 04:24.960
All right, everyone.

04:24.960 --> 04:31.240
We are here at CES, and I have the pleasure of being seated with Andy Keller, who is

04:31.240 --> 04:37.400
a deep learning data scientist at Intel, and Emil Chinnicki, who is a senior manager of

04:37.400 --> 04:43.040
marketing partnerships at Intel, and we've got an opportunity to chat about one of the

04:43.040 --> 04:48.000
cool announcements that was made yesterday, or today it was kind of announced yesterday,

04:48.000 --> 04:51.040
and then kind of more fully announced today.

04:51.040 --> 04:56.080
I'm going to keep you in suspense for just a little bit longer, and actually ask these

04:56.080 --> 05:01.440
guys to introduce themselves, and then we'll get into what that announcement was.

05:01.440 --> 05:02.840
So why don't we start with you, Andy?

05:02.840 --> 05:05.160
Yeah, thanks for having me.

05:05.160 --> 05:07.840
So I'm a deep learning data scientist at Intel.

05:07.840 --> 05:16.480
I started at Nirvana before the acquisition, working as an intern, doing some of the same

05:16.480 --> 05:25.000
kind of object localization stuff that we're using for this soon-to-be-in-house project,

05:25.000 --> 05:30.000
and specifically I was implementing some other models, like FasterRCNN.

05:30.000 --> 05:31.000
Like what?

05:31.000 --> 05:32.000
FasterRCNN?

05:32.000 --> 05:33.000
Okay.

05:33.000 --> 05:42.800
More recently, I've moved to kind of more similar work that I did for my masters at UCSD,

05:42.800 --> 05:49.800
which is natural language processing and dialogue systems and question-answer systems.

05:49.800 --> 05:50.800
Okay.

05:50.800 --> 05:59.800
And so now I'm started full-time after finishing my degree, and I love being here.

05:59.800 --> 06:00.800
Awesome.

06:00.800 --> 06:01.800
Awesome.

06:01.800 --> 06:02.800
The mail?

06:02.800 --> 06:03.800
My name's Emil Tinnicki.

06:03.800 --> 06:12.680
I joined last year the Artificial Intelligence Products Group at Intel, and I'm responsible

06:12.680 --> 06:13.880
for marketing partnerships.

06:13.880 --> 06:20.600
So what that means is that I work with our partner companies to showcase both partner

06:20.600 --> 06:28.040
technologies, as well as intel technologies, showcasing our efforts on an artificial intelligence.

06:28.040 --> 06:34.840
And in fact, there's one of these partnerships that Intel CEO Brian Krozenich announced

06:34.840 --> 06:41.640
at his CES keynote last night, and it's a partnership with Ferrari Racing.

06:41.640 --> 06:49.160
And I think for me, thinking back to Brian's keynote, it was a little surprising for

06:49.160 --> 06:55.560
me, I guess, you know, I kind of exist in this AI bubble, and I expected it to be just

06:55.560 --> 06:59.400
pure AI consumer and devices stuff.

06:59.400 --> 07:06.800
But Intel as a company is way more all-in into this virtual reality and immersive experience

07:06.800 --> 07:09.480
that then I knew.

07:09.480 --> 07:15.480
I didn't know anything about Intel's efforts around true VR and the other virtual reality

07:15.480 --> 07:20.760
plays in the immersive experience that he talked about.

07:20.760 --> 07:28.200
I mean, it's everything from outfitting the Olympics that are coming up.

07:28.200 --> 07:36.600
You guys built a studio, like a volume metric studio, and there was a partnership with Paramount

07:36.600 --> 07:38.400
Theaters announced.

07:38.400 --> 07:44.560
There was a ton of discussion around like how you would use the stuff in sports.

07:44.560 --> 07:48.840
And then one of the partnerships that was announced in that context was this partnership

07:48.840 --> 07:50.360
with Ferrari.

07:50.360 --> 07:54.960
So tell us a little bit about that partnership and what you're hoping to achieve with it.

07:54.960 --> 08:00.200
Yeah, so what was announced was a three-year partnership with Ferrari Challenge.

08:00.200 --> 08:07.480
Ferrari challenges a race series put on by Ferrari, and it takes place in different regions.

08:07.480 --> 08:10.600
We're specifically partnering with the North America team.

08:10.600 --> 08:16.320
So one of the areas that we're applying artificial intelligence is to the race stream.

08:16.320 --> 08:25.560
So that's basically applying AI techniques around a single shot detection and fine green

08:25.560 --> 08:33.400
classification to recognize the cars that are being captured in each of the camera feeds.

08:33.400 --> 08:42.440
And then the intent is to use that as metadata to curate camera feeds for viewers that are

08:42.440 --> 08:45.840
specific to each car or driver.

08:45.840 --> 08:53.840
So basically a fan could pick their favorite driver and we could deliver a curated feed

08:53.840 --> 08:58.920
specific to that driver as they make their way around the entire track.

08:58.920 --> 09:07.880
As I understand it, it's not just fans, but today, as every all of the folks that are

09:07.880 --> 09:13.680
broadcasting a race like the Ferrari Challenge, they're all operating off of the same feed,

09:13.680 --> 09:15.800
like everybody gets the same feed.

09:15.800 --> 09:17.200
Is that how it's working today?

09:17.200 --> 09:23.320
Yeah, so if you think of a traditional race broadcast or more typical race broadcast,

09:23.320 --> 09:30.880
you have kind of a director's cut of the action and normally the feed is focused on the

09:30.880 --> 09:33.800
first three or four cars in the race.

09:33.800 --> 09:40.520
So if your favorite driver is not among those couple cars, then you may not actually see

09:40.520 --> 09:44.360
too much of them during a broadcast.

09:44.360 --> 09:52.480
We notice that people's consumption of media is changing and we expect that there's no

09:52.480 --> 09:59.440
reason why that shouldn't change for motorsports broadcasts as well.

09:59.440 --> 10:05.960
This basically puts more choice in the hands of the fan to kind of get a more tailored

10:05.960 --> 10:09.840
customized viewing experience.

10:09.840 --> 10:13.360
How would I configure my feed?

10:13.360 --> 10:20.000
Is this something that is this being delivered for me like via a live stream and I'm choosing

10:20.000 --> 10:26.400
a car or something like that or what's the experience of tailoring the feed?

10:26.400 --> 10:32.760
Yeah, so the intention is that this would be a stream that would be broadcast live over

10:32.760 --> 10:40.320
an app or a website and exactly what you said, you could pick your favorite driver and

10:40.320 --> 10:45.480
kind of get that tailored stream according to that driver.

10:45.480 --> 10:51.840
And this isn't being done by just putting traditional cameras around the track?

10:51.840 --> 10:54.160
Well, yeah, so there's two components.

10:54.160 --> 10:59.080
We're using drones to capture the race footage.

10:59.080 --> 11:06.360
So that actually adds a rather dramatic element to the race experience, if you will.

11:06.360 --> 11:09.360
So these are not fixed point cameras.

11:09.360 --> 11:15.640
And then the notion is that you can apply these recognition techniques to each of those

11:15.640 --> 11:20.560
camera feeds so that at any given point in time, you know exactly which cars are showing

11:20.560 --> 11:22.160
up in which camera feeds.

11:22.160 --> 11:23.160
Okay.

11:23.160 --> 11:28.840
So with that information, you can kind of tailor that stream to a end user and viewer.

11:28.840 --> 11:29.840
Interesting.

11:29.840 --> 11:34.240
And how many drones are going to be flying around the Ferrari challenge when this thing

11:34.240 --> 11:35.240
is running?

11:35.240 --> 11:39.680
It's somewhat a function of the track configuration and length.

11:39.680 --> 11:45.800
But for starters, we're looking at between five and six drones.

11:45.800 --> 11:47.240
And we'll kind of go from there.

11:47.240 --> 11:54.240
And are the drones mostly kind of fixed in each one covers an area of the track or are

11:54.240 --> 11:57.720
they like following the cars or something?

11:57.720 --> 11:58.720
Yeah.

11:58.720 --> 12:06.000
So there's, they're not fixed in one location, they have kind of more of like a territory

12:06.000 --> 12:08.280
if you will that cover.

12:08.280 --> 12:10.520
So they may, you may see them like zipping around.

12:10.520 --> 12:11.880
Yeah, they'll be moving around.

12:11.880 --> 12:12.880
Territory.

12:12.880 --> 12:13.880
Yeah.

12:13.880 --> 12:14.880
Exactly.

12:14.880 --> 12:15.880
Okay.

12:15.880 --> 12:20.360
So I'm going to someone on your team yesterday about this at the, like, at the keynote,

12:20.360 --> 12:22.840
all right, before the keynote.

12:22.840 --> 12:28.640
And you know, we're going to talk a little bit about the kind of the AI elements and

12:28.640 --> 12:30.680
challenges associated with this.

12:30.680 --> 12:36.080
But, you know, just thinking about even like the, you know, this day and age, you've got

12:36.080 --> 12:41.960
to be capturing like 4K, 8K streams, like off of a drone and like getting those down

12:41.960 --> 12:45.400
to, you know, some place where you're going to process.

12:45.400 --> 12:49.200
I mean, aside, there's a ton of stuff that, you know, keeping the drones in the air is

12:49.200 --> 12:50.600
going to challenge.

12:50.600 --> 12:54.840
But a ton of technical challenges here, really, really interesting.

12:54.840 --> 12:59.600
So maybe this is a good segue to actually talk about some of the AI stuff.

12:59.600 --> 13:03.280
And Andy, this is where some of your work is come in.

13:03.280 --> 13:04.280
Yeah.

13:04.280 --> 13:09.680
Maybe tell us a little bit about kind of the underlying AI technologies and what the primary

13:09.680 --> 13:12.280
goals are for the project that you worked on.

13:12.280 --> 13:13.280
Yeah.

13:13.280 --> 13:15.920
So we thought a good place to start.

13:15.920 --> 13:21.920
We needed a foundation on which to kind of build other analytics about all of the drivers

13:21.920 --> 13:23.440
and the racing that was going on.

13:23.440 --> 13:30.960
So we realized we needed something else to build some sort of analytics platform on.

13:30.960 --> 13:36.440
And to start that, we really needed to be able to know, okay, we're going to be using

13:36.440 --> 13:37.680
the video footage.

13:37.680 --> 13:41.320
We need to know which drivers we're looking at in any given feed and we need to know where

13:41.320 --> 13:42.320
they are.

13:42.320 --> 13:44.040
We need to actually be able to localize them.

13:44.040 --> 13:48.560
So obviously the first thing that makes sense is some sort of object detection or object

13:48.560 --> 13:55.280
localization model, which can draw boxes around all the cars on the track and then potentially

13:55.280 --> 14:00.920
classify them uniquely as who's in what car.

14:00.920 --> 14:08.400
So kind of as a starting point for that, we had tested out using models that were pre-trained

14:08.400 --> 14:13.800
on some other data sets and realized that pretty quickly realized we were going to need to

14:13.800 --> 14:23.320
gather domain specific data set just because like Emule mentioned, the drone footage is

14:23.320 --> 14:28.360
dramatic in some sense and you don't find a lot of that out on YouTube.

14:28.360 --> 14:31.120
Exactly.

14:31.120 --> 14:33.640
What did you start with?

14:33.640 --> 14:37.720
What data sets did you try to train on initially?

14:37.720 --> 14:43.760
Yeah, we were using the kitty data set, which is kind of what in the self-driving

14:43.760 --> 14:52.160
data sets, but so it has bounding boxes around all of the cars and you can build a basic

14:52.160 --> 14:56.480
car detector off of that, but it's all from the viewpoint of kind of the dashboard of

14:56.480 --> 14:58.240
a car.

14:58.240 --> 15:03.560
So some of those viewpoints apply from a drone, but a lot of our shots are really long

15:03.560 --> 15:10.600
distance and kind of unique angles that we'd never seen before in that data set.

15:10.600 --> 15:15.400
Right, that was kind of our main driving factor in generating this new data set, which

15:15.400 --> 15:20.040
was a lot of the portion of the beginning of this project.

15:20.040 --> 15:28.120
So in the typical race, you'll have these five or six drones kind of zipping around

15:28.120 --> 15:34.680
their territories, capturing data of the field as a dozen or two dozen cars typically.

15:34.680 --> 15:38.440
Yeah, somewhere around there.

15:38.440 --> 15:44.840
And for each of these drones is instrumented with a camera and you're pulling down a

15:44.840 --> 15:49.280
feed and you're doing what otherwise might seem like a typical autonomous driving

15:49.280 --> 15:55.280
task, like putting bounding boxes around the cars that are on the track.

15:55.280 --> 15:59.760
So the first step is building a model based on this kitty data set.

15:59.760 --> 16:03.960
And then that wasn't giving you the accuracy that you're looking for.

16:03.960 --> 16:09.080
So you started building your own data set and was this like buys, you know, running

16:09.080 --> 16:12.800
drones on top of racetracks or yeah, basically.

16:12.800 --> 16:20.320
So we had a substantial amount of footage from across the entire 2017 season of recording

16:20.320 --> 16:23.400
from a bunch of drones at every single race.

16:23.400 --> 16:24.400
Okay.

16:24.400 --> 16:26.400
And this was in 4K.

16:26.400 --> 16:33.360
And so we knew that a lot of this footage maybe wasn't as useful.

16:33.360 --> 16:41.440
So we needed to kind of sort through that as a first step to generating a training set.

16:41.440 --> 16:49.280
So we had probably hundreds of hours of 4K footage and it was some of the data scientists

16:49.280 --> 16:53.120
jobs actually look through and find kind of really high variance shots, shots with

16:53.120 --> 16:57.160
differences in lighting, differences in size of the cars.

16:57.160 --> 17:01.840
Since these drones will move anywhere from 10 feet off the ground to 100 feet off the

17:01.840 --> 17:06.520
ground, that really changes the appearance of the car and what the model is going to be

17:06.520 --> 17:08.200
able to learn.

17:08.200 --> 17:11.680
So we really tried to get as much variance as possible.

17:11.680 --> 17:16.960
And what was the approach to doing that was this something that, you know, that was kind

17:16.960 --> 17:23.600
of manually coming through footage and hand annotating or did you, did you automate it

17:23.600 --> 17:26.320
in some way?

17:26.320 --> 17:34.920
We did have some of the cut from the live broadcast that was kind of curated by the drone

17:34.920 --> 17:38.560
team that was broadcast on the stream during the race.

17:38.560 --> 17:43.840
So we knew a little bit of kind of what shots to follow and what was the general.

17:43.840 --> 17:48.280
I mean, obviously there are at least going to have cars in the shot if it's in the broadcast.

17:48.280 --> 17:55.600
So we were able to use those and then some amount of manually coming through unfortunately.

17:55.600 --> 18:02.720
That wasn't the most exciting way that you spent time.

18:02.720 --> 18:10.680
And with that, did you, the manual coming through, like, what, did you use some, like, some

18:10.680 --> 18:14.680
kind of off-to-shelf tools to facilitate that?

18:14.680 --> 18:21.360
Luckily, we were kind of able to just write down timestamps in videos.

18:21.360 --> 18:26.320
And then I had written some scripts and FFM Peg is kind of the default tool for-

18:26.320 --> 18:30.280
To just snip out snippets and dump them in a drive or something like that.

18:30.280 --> 18:34.960
And so once we had kind of parsed through all of those and extracted some percentage of

18:34.960 --> 18:42.560
the frames, we were able to send those off to a labeling company and then provide them

18:42.560 --> 18:47.560
with a series of guidelines and kind of helpful data sheets about each of the different

18:47.560 --> 18:53.840
cars that we wanted labeled since we had something close to 40 or 50 different cars, or some

18:53.840 --> 18:55.880
of them looked pretty similar.

18:55.880 --> 19:01.480
So it was a, that was kind of the, one of the larger challenges was making sure the labeling

19:01.480 --> 19:06.600
team was actually able to do their job correctly such that the model itself was able to do

19:06.600 --> 19:07.600
it.

19:07.600 --> 19:12.920
It's just, it's, it's, so how long do you have a sense for how long you spent on just this

19:12.920 --> 19:22.680
kind of data collection and pre, like pre labeling task?

19:22.680 --> 19:30.560
The data collection itself, the recording of the video was over the entire 2017 race season,

19:30.560 --> 19:35.240
so I guess that maybe shouldn't be included.

19:35.240 --> 19:41.280
But kind of the curation of what we were going to provide to them, I would say happened

19:41.280 --> 19:46.520
over a week or two, we were able to get a couple of different data scientists looking

19:46.520 --> 19:49.520
at it and people who.

19:49.520 --> 19:55.480
And you started with 100 or so hours, I think you said, what was the, how, what was the

19:55.480 --> 20:01.800
size, like if you added up all the snippets that you sent on to get annotated, how many

20:01.800 --> 20:04.400
hours did you have of that?

20:04.400 --> 20:06.680
It was close to I think one hour.

20:06.680 --> 20:07.680
Okay.

20:07.680 --> 20:08.680
So significant.

20:08.680 --> 20:14.440
So dense, set, yeah.

20:14.440 --> 20:21.400
We even, after that, we obviously, because from frame to frame, there's really not that

20:21.400 --> 20:22.920
much difference.

20:22.920 --> 20:28.960
We were pulling something like maybe 10% of the frames, so that we can get more variety

20:28.960 --> 20:35.800
with a lower number of frames, since it really, every single frame is more effort for the

20:35.800 --> 20:39.840
labeling team and you're really trying to reduce that as much as possible.

20:39.840 --> 20:40.840
Right.

20:40.840 --> 20:41.840
Right.

20:41.840 --> 20:48.720
So meaning you used 10% of the frames in the one hour of footage.

20:48.720 --> 20:54.840
And then you had this label by the labeling team, I would have imagined that you needed

20:54.840 --> 21:02.520
a lot more training examples to achieve, you know, acceptable performance on this.

21:02.520 --> 21:09.960
Were you using these examples in conjunction with the previous, like was this a transfer

21:09.960 --> 21:15.280
learning type of task, or did you train from scratch with these new examples?

21:15.280 --> 21:16.280
Yeah.

21:16.280 --> 21:19.120
We ended up actually going straight from scratch.

21:19.120 --> 21:23.720
We kind of tried to approximate the size of some of the other popular object detection

21:23.720 --> 21:26.080
data sets, like Pascal VOC.

21:26.080 --> 21:27.080
Okay.

21:27.080 --> 21:35.800
And so that was kind of our goal to begin with, and basically from scratch was a little

21:35.800 --> 21:42.240
bit challenging, but we realized we probably needed to do it that way just because of kind

21:42.240 --> 21:51.480
of some of the uniqueness of this problem, the fact that these cars are very small and

21:51.480 --> 21:54.200
we have 50 different classes, but they all look very similar.

21:54.200 --> 21:58.240
So it's more of a fine grain classification problem, which a lot of these typical object

21:58.240 --> 22:03.920
detection data sets don't really cover.

22:03.920 --> 22:11.920
And is it generally the case that for these fine grain types of object identification

22:11.920 --> 22:17.800
problems that you need less data than, you know, if you were training up kind of object,

22:17.800 --> 22:23.160
you know, coarse grain object detection scratch?

22:23.160 --> 22:29.160
I don't know if I'd say less data, I think it depends on also how you develop the model.

22:29.160 --> 22:35.760
So there are some kind of future steps and kind of some of the state of the art and fine

22:35.760 --> 22:41.680
grain detection that we're planning to implement where you'll train a deep network and then

22:41.680 --> 22:46.360
chop off the top and add an SVM on top.

22:46.360 --> 22:51.520
And that was able to do a lot better at these tasks where the features between the two

22:51.520 --> 22:58.640
classes are very similar, but there is a boundary versus kind of the traditional something

22:58.640 --> 23:06.600
with something like what we use for SSD where it's an end-to-end neural network.

23:06.600 --> 23:16.000
So I think for the end-to-end case, you probably need about the same amount of data.

23:16.000 --> 23:26.720
We ended up having probably close to 1,000 labels per car, maybe 2,000, which seemed to

23:26.720 --> 23:29.920
be similar to some of these other data sets.

23:29.920 --> 23:30.920
Okay.

23:30.920 --> 23:34.920
Obviously some cars were more frequent than others and there wasn't a ton we could do besides

23:34.920 --> 23:42.080
rebalancing afterwards, but yeah, it turned out to work pretty well.

23:42.080 --> 23:43.080
Oh, nice.

23:43.080 --> 23:45.720
Thank you.

23:45.720 --> 23:52.520
What kind of model approach model architecture did you end up using and how did you arrive

23:52.520 --> 23:53.520
at that?

23:53.520 --> 23:54.520
Yeah.

23:54.520 --> 24:03.680
So we ended up using a single shot multi-box detector, which I think Andres, who was on

24:03.680 --> 24:07.160
your show a few weeks ago talking about the NASA project.

24:07.160 --> 24:11.840
They also used that for the crater detection.

24:11.840 --> 24:18.960
So because of some of the optimizations that we have in Neon for Intel, that made sense

24:18.960 --> 24:19.960
on that front.

24:19.960 --> 24:24.240
We were able to get kind of the live speed that we were looking for.

24:24.240 --> 24:27.960
And then also kind of the general architecture itself.

24:27.960 --> 24:36.200
It's a one shot, not one shot in one shot learning sense, but in a single shot like it, it's

24:36.200 --> 24:37.200
a single architecture.

24:37.200 --> 24:42.600
You don't have kind of a two-step process like faster or CNN has where first it proposes

24:42.600 --> 24:47.680
boxes and then it does the classification, which kind of happens all at once.

24:47.680 --> 24:54.240
So that is partially one of the reasons why it's significantly faster, which we liked.

24:54.240 --> 24:59.240
And it also has kind of feature maps of multiple different scales.

24:59.240 --> 25:02.960
So what does that mean?

25:02.960 --> 25:10.240
We have a convolutional networks when they operate and you go up and hire layers, the kind

25:10.240 --> 25:14.600
of the feature maps continue to shrink down and down further.

25:14.600 --> 25:22.000
So if we provide all of those to the end classifier, it's able to kind of get the same object

25:22.000 --> 25:23.840
at a bunch of different resolutions.

25:23.840 --> 25:30.120
So it's able to, in the end, basically means we're able to classify objects better at a

25:30.120 --> 25:31.400
lot of different scales.

25:31.400 --> 25:35.480
So small objects, big objects, which was really one of the main challenges of this problem

25:35.480 --> 25:37.640
in this data set.

25:37.640 --> 25:42.560
You mentioned that one of the things that you looked at or are looking at or considered

25:42.560 --> 25:49.520
was doing a network where you kind of chopped off the end of the network and replace it

25:49.520 --> 25:52.240
with an SVM.

25:52.240 --> 25:54.040
How far did you go down that path?

25:54.040 --> 25:56.960
We haven't gotten there at all yet.

25:56.960 --> 26:01.720
So there's a lot of next steps that we're looking into.

26:01.720 --> 26:08.760
I'm not sure how many I can discuss, but it's, yeah, that's definitely something we're

26:08.760 --> 26:09.760
considering.

26:09.760 --> 26:10.760
Okay.

26:10.760 --> 26:18.200
What are the kind of outstanding challenges in trying to productionalize this?

26:18.200 --> 26:25.480
I always kind of change whether I say productize, productionize, productionize, but it's a production.

26:25.480 --> 26:26.480
Yeah.

26:26.480 --> 26:34.240
What's remaining for you to kind of take on with this project?

26:34.240 --> 26:35.240
Yeah.

26:35.240 --> 26:43.360
One of the big ones is the fact that the appearance of the cars isn't necessarily static

26:43.360 --> 26:50.040
in between races or even in between days of a given race.

26:50.040 --> 26:55.960
A driver may change the color of their rims on their tires or even completely change

26:55.960 --> 26:58.880
the wrap on their car or crash the car at one day.

26:58.880 --> 27:07.840
So if we're trying to do a purely supervised kind of one of these, like just pound it with

27:07.840 --> 27:13.520
as much data as possible to get it to learn, then that makes it a little more challenging

27:13.520 --> 27:14.520
or to keep up.

27:14.520 --> 27:15.520
Hard to keep up.

27:15.520 --> 27:16.520
Yeah.

27:16.520 --> 27:20.560
So how do you address that?

27:20.560 --> 27:22.520
So there's a couple ways.

27:22.520 --> 27:27.680
The way that we're thinking to address it at this point is we would like to be able

27:27.680 --> 27:38.000
to just during practice laps, maybe take five, three, four, five pictures of a given car.

27:38.000 --> 27:42.240
We can know when a car has changed appearance slightly.

27:42.240 --> 27:47.200
And then if we have a model that's able to learn to classify cars just based on that

27:47.200 --> 27:52.680
small support set, that is kind of that would be ideal.

27:52.680 --> 27:57.720
So there's some architectures out there that are kind of attempting to do this today and

27:57.720 --> 28:02.520
it's kind of in the one shot, few shot learning field.

28:02.520 --> 28:09.040
And so stuff like matching networks or some of the other metal learning techniques, I think

28:09.040 --> 28:13.040
are what we're going to be exploring in the immediate future.

28:13.040 --> 28:17.480
So much of you dug into that stuff so far.

28:17.480 --> 28:22.080
One shot, few shot, metal learning or things that I, you know, are on my list of things

28:22.080 --> 28:25.280
to dig into a bit more on the podcast this year.

28:25.280 --> 28:32.560
So if you've learned about some of this stuff already, I'd love to get a sense for, you

28:32.560 --> 28:34.880
know, what you've seen out there and what you find interesting.

28:34.880 --> 28:35.880
Yeah.

28:35.880 --> 28:40.280
I mean, I saw a ton of interesting metal learning talks this year at NIPs.

28:40.280 --> 28:41.280
Okay.

28:41.280 --> 28:48.560
And a lot of significant portion of them focused around kind of this few shot learning idea.

28:48.560 --> 28:51.840
And it seems like there's a lot of different ways to approach it.

28:51.840 --> 28:55.120
And metal learning as a framework is pretty general.

28:55.120 --> 29:02.160
And so some people were approaching it from the, just like kind of the transfer learning

29:02.160 --> 29:07.400
perspective where you say, okay, we're actually going to design our optimization function

29:07.400 --> 29:13.240
such that our goal is that we learn a set of parameters that, with a single gradient

29:13.240 --> 29:20.160
step, we're able to achieve a variety of different tasks or achieve high performance on a variety

29:20.160 --> 29:25.760
of different tasks, which is different than kind of just optimize for a single specific

29:25.760 --> 29:27.240
task.

29:27.240 --> 29:32.080
I think that work is called mammal out of Berkeley.

29:32.080 --> 29:37.800
And so that, that's something that I think is generally applicable to, I mean, obviously

29:37.800 --> 29:40.480
any model, which is cool.

29:40.480 --> 29:45.840
And then some stuff like the matching network where it's almost like a can yours neighbors

29:45.840 --> 29:55.240
type approximation of classification where you say, okay, I'm going to give it five images

29:55.240 --> 29:59.840
of this type of dog and now show it a new dog.

29:59.840 --> 30:05.080
And it's going to compare with these images that it knows and see which one is the closest

30:05.080 --> 30:09.600
and then or do some sort of majority vote based on the class using that.

30:09.600 --> 30:14.280
So the cool thing is that they're all kind of differentiable and not necessarily related

30:14.280 --> 30:16.160
to a specific data set.

30:16.160 --> 30:19.400
So I think that is an interesting.

30:19.400 --> 30:25.600
So is the idea that you would have someone like at the track who is, or I guess they

30:25.600 --> 30:30.840
could be remotely, but someone who is looking at these live practice run videos and like

30:30.840 --> 30:40.080
coming up with an annotated sample and shooting that like, are you, are you then triggering

30:40.080 --> 30:46.280
an entire retrain of some model, you know, just before the race, that sounds like that.

30:46.280 --> 30:49.280
I hope as we don't have to do that.

30:49.280 --> 30:50.280
Yeah.

30:50.280 --> 30:51.280
That would be dangerous.

30:51.280 --> 30:53.280
Yeah, sounds dangerous.

30:53.280 --> 30:59.600
I think we had discussed that before and we were like, well, what could go wrong?

30:59.600 --> 31:03.080
What if we just doesn't finish training in front of?

31:03.080 --> 31:04.080
Right.

31:04.080 --> 31:05.080
Yeah.

31:05.080 --> 31:11.600
So I think we want to have an interface where you can kind of just draw, either click

31:11.600 --> 31:15.880
on the cars or draw a couple of boxes around the cars.

31:15.880 --> 31:22.920
Our models will work as a general car detector from drone footage, kind of regardless of

31:22.920 --> 31:24.840
how the appearance of the car changes.

31:24.840 --> 31:30.080
So it should be as simple as just clicking on the new car over a series of frames and

31:30.080 --> 31:33.520
the boxes will already be there.

31:33.520 --> 31:38.600
Hopefully if we use something like a matching network, those can just be cropped out and dumped

31:38.600 --> 31:42.600
into a directory and we actually don't have to do any retraining.

31:42.600 --> 31:50.600
So the model uses those images as part of it's inference procedure, which is kind of the

31:50.600 --> 31:52.320
ideal scenario.

31:52.320 --> 32:00.560
So in what way, how is the model incorporating those images as part of the inference?

32:00.560 --> 32:08.280
So you talked about the five images of the dog and getting a new one, is it?

32:08.280 --> 32:13.080
These annotated car images would be presumably the five images of the dog, but that's happening

32:13.080 --> 32:15.800
at inference time, not training time.

32:15.800 --> 32:16.800
Yeah.

32:16.800 --> 32:22.280
So maybe we have five images of every single car associated with that as the labels.

32:22.280 --> 32:26.400
And this is kind of a really rough overview of the matching network.

32:26.400 --> 32:27.400
They did.

32:27.400 --> 32:32.120
They added some fancy stuff on top of this, but basically like a can yours neighbor.

32:32.120 --> 32:37.840
So all of those images, so maybe you have 10 different cars, you have 15 images total.

32:37.840 --> 32:43.640
They're all embedded with some learned embedding function, just a mapping.

32:43.640 --> 32:48.680
And then an embedding function into what space?

32:48.680 --> 32:54.800
One lower dimensional, so you just have like a fully connected neural network layer and

32:54.800 --> 33:00.600
it goes to some smaller space, so you have new, now features for all of those pictures.

33:00.600 --> 33:05.720
You also do the same mapping on your new input.

33:05.720 --> 33:12.240
And then you can just compare through some distance metric or some similarity metric, that

33:12.240 --> 33:17.080
new input and all of your 50 kind of just loaded images.

33:17.080 --> 33:21.000
So these images were potentially never trained on.

33:21.000 --> 33:24.000
You're really just learning that embedding function.

33:24.000 --> 33:29.600
And then once you compare and find, okay, it's closest to these five or these three or

33:29.600 --> 33:35.400
this one, you can take a weighted combination of those classes and then whichever class

33:35.400 --> 33:40.160
kind of has the highest vote is what you end up going with.

33:40.160 --> 33:46.480
And how does the train model interact with and inform the, you know, this matching process

33:46.480 --> 33:49.200
that happens at inference time?

33:49.200 --> 33:58.200
Is it that in this model, like the model that you've trained, the network is only doing object

33:58.200 --> 34:05.680
detection and then this matching process is doing the object identification or does having

34:05.680 --> 34:11.080
the train model somehow inform the identification as well, but you've got this extra layer that

34:11.080 --> 34:12.080
refines it.

34:12.080 --> 34:15.040
Yeah, so I think we could do it either way.

34:15.040 --> 34:24.160
We would like to be able to keep it as a single model, just for speed reasons, but that's

34:24.160 --> 34:27.200
still kind of in the research phase.

34:27.200 --> 34:33.880
An alternative is to kind of have the SSD model just do the object detection and then

34:33.880 --> 34:40.360
have the whatever additional network that we have on top does kind of the few shop learning

34:40.360 --> 34:44.560
and classification of the different cars.

34:44.560 --> 34:49.160
So I think, yeah, a lot of these are things to explore in the year.

34:49.160 --> 34:50.160
Right.

34:50.160 --> 34:51.760
You mentioned SSD a couple of times.

34:51.760 --> 34:58.800
What is, what is SSD I'm sorry, it's the single shot multi box, single shot multi box

34:58.800 --> 34:59.800
detector.

34:59.800 --> 35:00.800
Right.

35:00.800 --> 35:09.040
Okay, as opposed to your hard drive on, and we have an implementation of SSD that's open

35:09.040 --> 35:15.760
source on the neon get a repo, okay, so that's actually what we ended up using for this

35:15.760 --> 35:16.760
project.

35:16.760 --> 35:17.760
Okay.

35:17.760 --> 35:22.120
And it's pretty much just plug and play with a new data set and a little bit of tuning

35:22.120 --> 35:25.400
and we were able to achieve pretty good performance.

35:25.400 --> 35:26.400
Nice.

35:26.400 --> 35:32.080
And now is the, the data set that you've, like are you publishing this data set or anything

35:32.080 --> 35:34.880
like that?

35:34.880 --> 35:38.880
Not at this point, okay.

35:38.880 --> 35:45.520
Or any of the, any of the models or anything like that, are you publishing them or do you

35:45.520 --> 35:49.520
have like technical blog posts or something that folks can take a look at if they want

35:49.520 --> 35:53.200
to get more detailist into, like how you approach this problem?

35:53.200 --> 35:59.120
Yeah, we have a technical blog post that should be out.

35:59.120 --> 36:04.440
Or will we coming out, describing some of the data collection and the modeling procedure

36:04.440 --> 36:10.040
and preprocessing steps that we used, as well as a little bit of the training, so that

36:10.040 --> 36:12.280
should give people a pretty good idea.

36:12.280 --> 36:18.880
And are there, are there other kind of types or classes of problem that you think that

36:18.880 --> 36:22.640
this same kind of approach would lend itself to?

36:22.640 --> 36:28.080
Or is this, like do you think this is very custom to the unique challenges of having six

36:28.080 --> 36:33.920
drones flying around trying to identify, you know, formula racing cars or Ferrari racing

36:33.920 --> 36:34.920
cars?

36:34.920 --> 36:41.440
Yeah, I definitely, even today, here at CES of the booth, we had a lot of people coming

36:41.440 --> 36:48.080
up and saying, oh, this could work for this sport that I participate in, or we haven't

36:48.080 --> 36:52.480
quite explored those areas in terms of partnerships or how that would work yet.

36:52.480 --> 36:59.640
But I think it's definitely applicable to all sorts of different either broadcast races

36:59.640 --> 37:06.120
or sports or when just when there's a fast moving object that it's difficult for viewers

37:06.120 --> 37:12.000
to follow and it helps a lot to have some sort of either AI assistants or overlay or automatic

37:12.000 --> 37:14.320
broadcast control.

37:14.320 --> 37:19.600
So Andy, so this is kind of, you know, this project was just announced here.

37:19.600 --> 37:22.560
What are some of the other things that you're working on?

37:22.560 --> 37:28.960
Yeah, so I'm working on, obviously, continuing this project and some of the optimizations

37:28.960 --> 37:31.120
that I mentioned.

37:31.120 --> 37:37.880
My other work is more related, like I said, to what I had done my master's on, which is

37:37.880 --> 37:49.360
kind of end to end question-answer systems and dialogue systems, using similar network

37:49.360 --> 37:54.360
topologies if you look at it kind of squint and from a distance.

37:54.360 --> 38:03.240
And stuff like memory networks are a large part of my work.

38:03.240 --> 38:08.920
So I'm really interested in kind of memory augmented neural networks and the role that

38:08.920 --> 38:15.960
can play and not just question answering, but kind of a bunch of different challenges.

38:15.960 --> 38:23.440
And our memory networks and memory augmented networks is that of these related to like LSTM's

38:23.440 --> 38:30.080
and attention mechanisms and that kind of thing or yeah, very similar to attention.

38:30.080 --> 38:41.720
It's typically the memory augmented network means it has kind of some sort of fixed readable

38:41.720 --> 38:45.920
memory that you can either load items into.

38:45.920 --> 38:50.320
So for the case of the memory networks for question answering, you'll load in a story,

38:50.320 --> 38:58.280
like a text story of something that happened and then ask it questions about that story.

38:58.280 --> 39:03.000
And so similar to attention, the memory network will kind of compare whatever its input

39:03.000 --> 39:08.320
is with all of the story that's loaded in memory.

39:08.320 --> 39:09.960
So those are very similar.

39:09.960 --> 39:10.960
Awesome.

39:10.960 --> 39:15.800
Well, thanks so much guys for taking a hike over in the rain and the traffic.

39:15.800 --> 39:20.600
I was great chatting with you and I enjoyed learning more about what you're doing with

39:20.600 --> 39:21.920
the Ferrari challenge.

39:21.920 --> 39:22.920
Absolutely.

39:22.920 --> 39:23.920
Thank you.

39:23.920 --> 39:25.920
Thanks for having us.

39:25.920 --> 39:28.200
Alright, everyone.

39:28.200 --> 39:30.360
That's our show for today.

39:30.360 --> 39:34.920
Thanks so much for listening and for your continued feedback and support.

39:34.920 --> 39:41.440
Remember, for your chance to win in our AI at home giveaway, head on over to twimlai.com

39:41.440 --> 39:45.920
slash my AI contest for complete details.

39:45.920 --> 39:50.680
For more information on Andy, Amille, or any of the topics covered in this episode, head

39:50.680 --> 39:55.200
on over to twimlai.com slash talk slash 104.

39:55.200 --> 39:59.240
Thanks once again to Intel AI for their sponsorship of this series.

39:59.240 --> 40:03.240
To learn more about their partnership with Ferrari North America challenge and the other

40:03.240 --> 40:07.840
things they've been up to, visit ai.intel.com.

40:07.840 --> 40:12.920
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

40:12.920 --> 40:19.240
or via Twitter directly to me at at Sam Charrington or to the show at at twimlai.

40:19.240 --> 40:40.080
Thanks once again for listening and catch you next time.

