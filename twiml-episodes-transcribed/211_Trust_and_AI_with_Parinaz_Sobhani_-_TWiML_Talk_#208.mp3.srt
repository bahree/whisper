1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,320
I'm your host Sam Charrington.

4
00:00:31,320 --> 00:00:36,040
This week on the podcast, we explore perspectives on trust in AI.

5
00:00:36,040 --> 00:00:40,600
In today's episode, we're joined by Paranaz Sabani, director of machine learning and

6
00:00:40,600 --> 00:00:42,600
Georgian partners.

7
00:00:42,600 --> 00:00:47,300
In our conversation, Paranaz and I discuss some of the key issues falling under the trust

8
00:00:47,300 --> 00:00:51,580
umbrella, such as transparency, fairness, and accountability.

9
00:00:51,580 --> 00:00:55,580
We also explore some of the trust-related projects she and her team at Georgian are working

10
00:00:55,580 --> 00:01:00,820
on, as well as some of the interesting trust and privacy papers coming out of the NURPS

11
00:01:00,820 --> 00:01:02,700
conference.

12
00:01:02,700 --> 00:01:06,480
This week's series is sponsored by our friends at Georgian Partners.

13
00:01:06,480 --> 00:01:10,580
Georgian Partners is a venture capital firm that invests in growth-stage business software

14
00:01:10,580 --> 00:01:13,180
companies in the US and Canada.

15
00:01:13,180 --> 00:01:17,800
Most investment, Georgian works closely with portfolio companies to accelerate adoption

16
00:01:17,800 --> 00:01:22,780
of key technologies, including machine learning and differential privacy.

17
00:01:22,780 --> 00:01:27,740
To learn more about Georgian, visit Twimbleai.com slash Georgian.

18
00:01:27,740 --> 00:01:35,380
And now on to the show.

19
00:01:35,380 --> 00:01:40,460
Alright everyone, I am in Montreal and I've got the pleasure of being with Paranaz Sabani.

20
00:01:40,460 --> 00:01:44,220
Paranaz is the director of machine learning at Georgian Partners.

21
00:01:44,220 --> 00:01:47,100
Paranaz, welcome to this week in machine learning and AI.

22
00:01:47,100 --> 00:01:48,100
Thank you Sam.

23
00:01:48,100 --> 00:01:51,540
It's great to see you here and talk to you.

24
00:01:51,540 --> 00:01:52,540
Absolutely.

25
00:01:52,540 --> 00:01:57,460
It's been a while since we've seen one another and I'm looking forward to diving into the

26
00:01:57,460 --> 00:01:58,460
conversation.

27
00:01:58,460 --> 00:02:01,300
But before we do that, can you share a little bit about your background?

28
00:02:01,300 --> 00:02:04,100
Had you start working on machine learning?

29
00:02:04,100 --> 00:02:05,100
Long story.

30
00:02:05,100 --> 00:02:06,100
It's always a long story.

31
00:02:06,100 --> 00:02:07,100
I know.

32
00:02:07,100 --> 00:02:09,100
It's always a long story.

33
00:02:09,100 --> 00:02:15,220
I started working in AI and machine learning when I was in third year of my bachelor and

34
00:02:15,220 --> 00:02:17,540
I joined the Robocop team.

35
00:02:17,540 --> 00:02:20,900
I don't know if you know what does it mean, Robocop.

36
00:02:20,900 --> 00:02:25,860
Robocop, I just spoke to someone who also did Robocop and that was their kind of entree

37
00:02:25,860 --> 00:02:26,860
into ML&A.

38
00:02:26,860 --> 00:02:27,860
Oh, so cool.

39
00:02:27,860 --> 00:02:28,860
Exactly.

40
00:02:28,860 --> 00:02:35,380
He's a kind of simulation for soccer and in back home in Iran, they were all like the

41
00:02:35,380 --> 00:02:42,860
big fan of soccer and that's why we wanted to do something different, maybe simulation

42
00:02:42,860 --> 00:02:49,180
with robots and then I really found it amazing.

43
00:02:49,180 --> 00:02:55,500
And then in my master, I worked also in machine learning, I worked on data stream classification

44
00:02:55,500 --> 00:03:03,700
and concept rift detection and then I came to Canada to do my PhD and I found one of

45
00:03:03,700 --> 00:03:11,300
the most challenging problems in AI is like natural language understanding.

46
00:03:11,300 --> 00:03:17,020
If you look at the other problems that we are solving using machine learning technologies,

47
00:03:17,020 --> 00:03:27,820
speech recognition, vision and image kind of processing, I feel like out of all NLP and

48
00:03:27,820 --> 00:03:32,260
natural language understanding is one of the most difficult one and it's mainly because

49
00:03:32,260 --> 00:03:38,260
over time, over the course of human evolution, natural languages has adopted all the kind

50
00:03:38,260 --> 00:03:41,300
of human complexities.

51
00:03:41,300 --> 00:03:46,740
So if all I've got to be one of the last things that AI can achieve, truly, I mean like

52
00:03:46,740 --> 00:03:53,820
actual natural language understanding and generation, not simple mapping, sequence models

53
00:03:53,820 --> 00:03:55,820
here.

54
00:03:55,820 --> 00:04:02,300
So and then in my PhD, I've worked on sentiment analysis and stance classification and these

55
00:04:02,300 --> 00:04:08,580
are like kind of baby steps towards natural language understanding, mainly takes classification

56
00:04:08,580 --> 00:04:17,380
for social media and then I've worked for Microsoft research, working on machine translation

57
00:04:17,380 --> 00:04:24,500
and also for National Research Council of Canada, working on natural language understanding

58
00:04:24,500 --> 00:04:30,220
using deep neural networks, mostly three base LSTMs, that is structural LSTMs and stuff

59
00:04:30,220 --> 00:04:31,940
like this.

60
00:04:31,940 --> 00:04:40,500
And then were you working at Microsoft research kind of in the hey day of the neural machine

61
00:04:40,500 --> 00:04:44,460
translation when they were exactly that was a Google starter that they really wanted

62
00:04:44,460 --> 00:04:48,780
to have something similar and they wanted to ship the product as soon as possible.

63
00:04:48,780 --> 00:04:49,780
Okay.

64
00:04:49,780 --> 00:04:50,780
Yeah, exactly.

65
00:04:50,780 --> 00:04:51,780
Oh, wow, exciting.

66
00:04:51,780 --> 00:04:52,780
Exciting.

67
00:04:52,780 --> 00:04:54,300
It was a lot of that happening in Toronto.

68
00:04:54,300 --> 00:04:55,300
No, in Seattle.

69
00:04:55,300 --> 00:04:56,300
Oh, in Seattle?

70
00:04:56,300 --> 00:04:57,300
Yeah, in Redmond, actually.

71
00:04:57,300 --> 00:04:58,300
Okay.

72
00:04:58,300 --> 00:04:59,300
Yeah.

73
00:04:59,300 --> 00:05:00,300
Okay.

74
00:05:00,300 --> 00:05:01,300
The actual headquarter.

75
00:05:01,300 --> 00:05:02,300
Yeah.

76
00:05:02,300 --> 00:05:10,620
And then I joined Georgian Partners two years ago, which was kind of like big decision because

77
00:05:10,620 --> 00:05:15,860
like nobody can, nobody in my area, nobody in Michelinink kind of community can think

78
00:05:15,860 --> 00:05:22,380
of like venture capital as a carrier path and it was, I found, I didn't know about a lot

79
00:05:22,380 --> 00:05:28,540
about venture capital as well, but I found the opportunity pretty unique and the kind of

80
00:05:28,540 --> 00:05:34,220
opportunity that is like you're going to have more exposure to multiple businesses and

81
00:05:34,220 --> 00:05:36,740
you can work with several different startups.

82
00:05:36,740 --> 00:05:40,580
Actually, at some point, I decided I don't want to stay with big corporations.

83
00:05:40,580 --> 00:05:41,580
Okay.

84
00:05:41,580 --> 00:05:45,460
And I was so interested in a startup ecosystem, but I didn't know anything about startup

85
00:05:45,460 --> 00:05:46,460
ecosystem.

86
00:05:46,460 --> 00:05:47,940
So I thought, it's going to be fun.

87
00:05:47,940 --> 00:05:51,700
I'm going to learn a lot and actually I learned a lot.

88
00:05:51,700 --> 00:05:56,060
And at the same time, I was, it was more meaningful for me because I was helping startups

89
00:05:56,060 --> 00:06:01,380
that they didn't have resources or skulls like this cause I had.

90
00:06:01,380 --> 00:06:07,580
And I thought, okay, Google has so many smart people, Microsoft has so many smart people.

91
00:06:07,580 --> 00:06:13,060
So let's work for some places or work on some problems that if I don't work on them,

92
00:06:13,060 --> 00:06:15,700
there is nobody else to work on them.

93
00:06:15,700 --> 00:06:23,340
And so how did you get started working in the domain of trust and fairness in AI?

94
00:06:23,340 --> 00:06:28,260
I guess like most of other machine learning like machine learning centers that are working

95
00:06:28,260 --> 00:06:31,340
on real world problems.

96
00:06:31,340 --> 00:06:36,180
In one of our projects, we realized, oh my god, it's such an important topic.

97
00:06:36,180 --> 00:06:38,900
And I actually realized I don't know anything about it.

98
00:06:38,900 --> 00:06:43,980
So I have focused a lot on the optimization and the choice of machine learning models

99
00:06:43,980 --> 00:06:51,980
on like data processing, future engineering, but I never thought about these kind of fundamental

100
00:06:51,980 --> 00:06:53,660
problems.

101
00:06:53,660 --> 00:06:59,740
And again, it's kind of interesting at least for most of like products that I worked on.

102
00:06:59,740 --> 00:07:06,260
It's not full automation, it's more like a human in the loop that you provide recommendation

103
00:07:06,260 --> 00:07:12,540
and you have to build this trust with the human so that they will act under on, they will

104
00:07:12,540 --> 00:07:15,260
act based on your recommendations.

105
00:07:15,260 --> 00:07:20,060
And I thought, oh my god, even if I build the best model, if I can't build that trust

106
00:07:20,060 --> 00:07:26,060
by having better user interfaces or interpretability and shrink the value of your time and shrink

107
00:07:26,060 --> 00:07:31,300
maybe like for some applications, more critical decision making.

108
00:07:31,300 --> 00:07:34,980
It's also important to show the model is fair.

109
00:07:34,980 --> 00:07:41,940
Then actually the product would not be adopted and all my efforts going to be useless.

110
00:07:41,940 --> 00:07:44,380
So that was the story.

111
00:07:44,380 --> 00:07:53,620
And again, it was working with one of our companies and I can't share a lot about the company

112
00:07:53,620 --> 00:07:59,300
and the project, but it was a pretty important decision making, automation for a pretty important

113
00:07:59,300 --> 00:08:09,260
decision making that could have impacted the individuals and realized that we never evaluated

114
00:08:09,260 --> 00:08:14,820
the performance of the model against fairness, against bias.

115
00:08:14,820 --> 00:08:20,300
And we started to think of like what would be the sensitive attributes for their problem.

116
00:08:20,300 --> 00:08:24,940
And looking at the literature, it was more about gender ethnicity, but then we realized

117
00:08:24,940 --> 00:08:29,380
it's not only limited to gender and ethnicity or religion.

118
00:08:29,380 --> 00:08:34,260
And for every application, we might have a different set of protected attributes that

119
00:08:34,260 --> 00:08:37,940
we have to consider.

120
00:08:37,940 --> 00:08:44,180
And then, so that was the fairness story and then we decided, okay, how we can let's

121
00:08:44,180 --> 00:08:48,420
first of all quantify the bias in the model.

122
00:08:48,420 --> 00:08:50,740
And we used a fair test.

123
00:08:50,740 --> 00:08:56,180
There is a kind of open source tool which is great, very good user interface.

124
00:08:56,180 --> 00:08:57,740
And what's it called?

125
00:08:57,740 --> 00:08:58,740
Fair test.

126
00:08:58,740 --> 00:08:59,740
Fair test?

127
00:08:59,740 --> 00:09:00,740
Yes.

128
00:09:00,740 --> 00:09:01,740
Okay.

129
00:09:01,740 --> 00:09:07,340
I can't remember the authors of the paper, can't check, but I can't remember anyhow.

130
00:09:07,340 --> 00:09:15,860
But then, and we could kind of identify and quantify a couple of sources of the bias.

131
00:09:15,860 --> 00:09:24,420
And for their problem, we could solve this issue by having better sampling strategy.

132
00:09:24,420 --> 00:09:29,020
To as far as, as soon as we identified what are the kind of under-representatives in this

133
00:09:29,020 --> 00:09:35,500
data sets, then we could have kind of, it was only a single iteration, it was several

134
00:09:35,500 --> 00:09:40,540
kind of back-end-forced testing the model, improving the strategy, sampling the strategy,

135
00:09:40,540 --> 00:09:42,300
and then testing it again.

136
00:09:42,300 --> 00:09:48,740
One of the things that you mentioned that came up in the context of trust was user interface,

137
00:09:48,740 --> 00:09:51,820
which is not something that often comes up.

138
00:09:51,820 --> 00:09:56,980
What are the, when you think about kind of the broad issues that fall under trust?

139
00:09:56,980 --> 00:10:00,620
What are the kind of main things that you're thinking about?

140
00:10:00,620 --> 00:10:07,940
When I'm talking about trust, first of all, I'm thinking about transparency.

141
00:10:07,940 --> 00:10:15,460
And it's hard, and again, all these kinds of problems are hard to define, at least in

142
00:10:15,460 --> 00:10:17,340
a mathematical way.

143
00:10:17,340 --> 00:10:25,740
So by transparency, I mean, is it clear for the kind of users or customers, how are you

144
00:10:25,740 --> 00:10:32,580
using their data for what kind of processes you are using their data, and how the data

145
00:10:32,580 --> 00:10:37,820
has been shared across several different kind of use cases or processes?

146
00:10:37,820 --> 00:10:44,060
And then, interpretability, explainability is also, is really, really important.

147
00:10:44,060 --> 00:10:48,900
And again, I remember one, two years ago, when we were talking about interpretability

148
00:10:48,900 --> 00:10:53,260
in machine learning community, everybody were like, why should we focus on interpretability?

149
00:10:53,260 --> 00:10:58,100
We should focus on improving the performance of our machine learning models.

150
00:10:58,100 --> 00:11:03,180
And exactly in one of the projects that we were working on, we realized that users are

151
00:11:03,180 --> 00:11:08,900
not looking for that single probability or prediction.

152
00:11:08,900 --> 00:11:15,420
They are asking for more evidences, and it can help them to combine the knowledge of

153
00:11:15,420 --> 00:11:21,100
the machine or models with their own knowledge and having smarter decisions.

154
00:11:21,100 --> 00:11:25,380
So there are several different reasons that we might need interpretability, that's one

155
00:11:25,380 --> 00:11:26,380
of them.

156
00:11:26,380 --> 00:11:31,940
There are lots of discussion if interpretability can help causality or identifying if the

157
00:11:31,940 --> 00:11:37,140
model is biased or for scientific purposes.

158
00:11:37,140 --> 00:11:40,740
So there are several different reasons that we might need interpretability, that was one

159
00:11:40,740 --> 00:11:47,020
of the examples that I encountered in my kind of experience.

160
00:11:47,020 --> 00:11:51,900
And privacy also is really important.

161
00:11:51,900 --> 00:11:58,060
Again, it's not a secret anymore that machine learning models, they can memorize data.

162
00:11:58,060 --> 00:12:03,220
And especially if you use sensitive data to build your machine learning models, these

163
00:12:03,220 --> 00:12:08,980
models can remember the training in senses and can be reversed engineered to get access

164
00:12:08,980 --> 00:12:12,980
to those sensitive attributes of the customers or users.

165
00:12:12,980 --> 00:12:18,620
As we are using machine learning models for more kind of sensitive applications like

166
00:12:18,620 --> 00:12:25,580
healthcare, the importance of privacy is even higher and higher.

167
00:12:25,580 --> 00:12:30,220
Security is actually more important than any of these because it's actually the fundamental

168
00:12:30,220 --> 00:12:31,220
layer.

169
00:12:31,220 --> 00:12:39,020
If you don't have the secure kind of computation or if you don't encrypt your data in transition

170
00:12:39,020 --> 00:12:47,220
or in rest, and if you have a data bridge, nothing else can work, you know what I mean?

171
00:12:47,220 --> 00:12:50,180
You have lost the trust.

172
00:12:50,180 --> 00:12:51,180
Right.

173
00:12:51,180 --> 00:12:56,980
So it sounds like a lot of these issues that we sometimes think of as disconnected issues

174
00:12:56,980 --> 00:13:02,820
are all kind of come together when it comes to creating a trustful relationship between

175
00:13:02,820 --> 00:13:07,060
the user of an AI system and the system itself.

176
00:13:07,060 --> 00:13:08,060
Exactly.

177
00:13:08,060 --> 00:13:09,060
Yeah, exactly.

178
00:13:09,060 --> 00:13:13,660
And like there are, every day there are new issues, for example, adversarial attacks say

179
00:13:13,660 --> 00:13:18,900
are new issues completely specific to machine learning models.

180
00:13:18,900 --> 00:13:24,500
And like AI safety, like now these days I nips again, there are a lot of interesting papers

181
00:13:24,500 --> 00:13:27,620
about adversarial attacks on AI safety.

182
00:13:27,620 --> 00:13:35,540
Before we weren't even aware of possible risks of using machine learning models.

183
00:13:35,540 --> 00:13:41,580
Are there specific examples of projects that you can talk us through that, you know,

184
00:13:41,580 --> 00:13:44,380
where you kind of explore these various issues?

185
00:13:44,380 --> 00:13:48,780
Again, for sure talk about differential privacy.

186
00:13:48,780 --> 00:13:53,500
We've worked on several different, differential privacy projects.

187
00:13:53,500 --> 00:14:03,620
And as you know, we try to have the message as you can create value by having more private

188
00:14:03,620 --> 00:14:07,260
differential learning and data mining approaches.

189
00:14:07,260 --> 00:14:15,080
As an example, you can convince the user to share their data and you can aggregate data

190
00:14:15,080 --> 00:14:20,500
across multiple customers, building better performing machine learning models at the same

191
00:14:20,500 --> 00:14:21,500
time.

192
00:14:21,500 --> 00:14:26,220
You're going to have guarantees in terms of privacy of the users.

193
00:14:26,220 --> 00:14:31,340
So that's the kind of like one of the most kind of inspiring projects that I've worked

194
00:14:31,340 --> 00:14:36,860
on because again, it's not only about these are the risks and it's also about creating

195
00:14:36,860 --> 00:14:37,860
value.

196
00:14:37,860 --> 00:14:42,380
It's also about win-win kind of like you're going to have private models you shouldn't

197
00:14:42,380 --> 00:14:47,460
and at the same time you don't need to sacrifice the performance of the machine learning model.

198
00:14:47,460 --> 00:14:48,460
Okay.

199
00:14:48,460 --> 00:14:53,380
That's the kind of issue we have also with fairness because normally in fairness people believe

200
00:14:53,380 --> 00:14:57,580
that there should be a trade-off between accuracy and fairness.

201
00:14:57,580 --> 00:14:59,980
Why they really don't believe that's the case.

202
00:14:59,980 --> 00:15:07,180
And at least in our case, when we improved the kind of sampling strategy, we could improve

203
00:15:07,180 --> 00:15:09,980
both the performance of the model at the same time.

204
00:15:09,980 --> 00:15:19,420
We could have root out the bias and kind of discrimination in the system.

205
00:15:19,420 --> 00:15:23,660
So this example where you had to work with improving the sampling strategy.

206
00:15:23,660 --> 00:15:27,740
What was wrong with the model before you did that?

207
00:15:27,740 --> 00:15:33,220
So like we didn't, again, there are several different definitions for fairness.

208
00:15:33,220 --> 00:15:36,300
So we weren't mostly looking at group fairness.

209
00:15:36,300 --> 00:15:42,540
So for protected groups or for microseconds of the population, there was a huge difference

210
00:15:42,540 --> 00:15:51,380
between performance of the model and that's why, again, I know lots of people argued

211
00:15:51,380 --> 00:15:56,740
that should we have individual fairness or group fairness, but what we achieved by having

212
00:15:56,740 --> 00:16:02,740
better sampling strategy was group fairness for protected groups.

213
00:16:02,740 --> 00:16:07,020
And so can you walk us through those two different definitions of fairness?

214
00:16:07,020 --> 00:16:10,860
Like how are people thinking about this problem?

215
00:16:10,860 --> 00:16:14,660
So like, again, like some people argue that even if you have group fairness, you might

216
00:16:14,660 --> 00:16:16,260
not have individual fairness.

217
00:16:16,260 --> 00:16:24,380
For example, me and you having the same qualifications for the application, for the application X, we

218
00:16:24,380 --> 00:16:28,300
might be discriminated, we might not get the same utility out of the machine learning

219
00:16:28,300 --> 00:16:29,300
models.

220
00:16:29,300 --> 00:16:34,820
But at the same time, the model might have a similar performance for microseconds that

221
00:16:34,820 --> 00:16:37,900
we belong to.

222
00:16:37,900 --> 00:16:44,980
So there's a kind of intuition behind group fairness versus individual fairness.

223
00:16:44,980 --> 00:16:50,020
In a space like this where you mentioned that we don't have good definitions for this

224
00:16:50,020 --> 00:16:54,660
stuff in math, it's even like we don't have good definitions for this stuff in English,

225
00:16:54,660 --> 00:16:55,660
let alone math.

226
00:16:55,660 --> 00:16:59,340
There's so many different ways to define all these things.

227
00:16:59,340 --> 00:17:05,500
How does one go about creating, charting a path through such a marquee territory?

228
00:17:05,500 --> 00:17:06,980
It's really difficult.

229
00:17:06,980 --> 00:17:07,980
And you're right.

230
00:17:07,980 --> 00:17:13,460
Every time I'm giving a talk about fairness and masking, has anyone have a good definition

231
00:17:13,460 --> 00:17:14,460
for fairness?

232
00:17:14,460 --> 00:17:19,700
I'm really willing to listen and learn more about it, but still there is no concrete

233
00:17:19,700 --> 00:17:22,180
kind of definition or metric.

234
00:17:22,180 --> 00:17:26,180
And again, we are talking about like we have to quantify the bias, but then if we don't

235
00:17:26,180 --> 00:17:31,100
have any kind of metric that everybody agrees, that's a metric that we have to use.

236
00:17:31,100 --> 00:17:35,060
It's not like if one scored at everyone using machine learning now, we need to have

237
00:17:35,060 --> 00:17:36,060
similar score.

238
00:17:36,060 --> 00:17:39,100
But it doesn't mean there is no research.

239
00:17:39,100 --> 00:17:42,260
I'm really encouraged that we are in the right direction.

240
00:17:42,260 --> 00:17:46,340
In the machine learning community, even again, last night I saw a couple of papers that

241
00:17:46,340 --> 00:17:51,700
they were talking about different metrics about for fairness in Newrips.

242
00:17:51,700 --> 00:17:54,940
So like again, the community is in the right direction.

243
00:17:54,940 --> 00:18:01,180
We are thinking about these problems and we are and the other kind of encouraging thing

244
00:18:01,180 --> 00:18:07,180
that is happening is that we are kind of engaging with other communities as well.

245
00:18:07,180 --> 00:18:11,900
So now in the conferences, every time I'm giving a talk, I saw a couple of people from

246
00:18:11,900 --> 00:18:18,180
legal domain and they are kind of challenging me and asking me questions and I learn a lot

247
00:18:18,180 --> 00:18:19,180
from them.

248
00:18:19,180 --> 00:18:25,340
Also, there are also people from sociology interested in these kinds of problems.

249
00:18:25,340 --> 00:18:30,300
So we are in the right path, but it's still a long way to go.

250
00:18:30,300 --> 00:18:36,500
Yeah, I was at the kind of the exhibit floor here in Newrips earlier and was getting

251
00:18:36,500 --> 00:18:43,580
a demo of some project that IBM research was working on as Fairness 360 Toolkit, I think

252
00:18:43,580 --> 00:18:46,100
is what they called it.

253
00:18:46,100 --> 00:18:53,020
They were going through some examples and they had like six different half a dozen different

254
00:18:53,020 --> 00:18:54,340
fairness metrics.

255
00:18:54,340 --> 00:19:01,140
They touched on this kind of group versus individual and some others and then they had several

256
00:19:01,140 --> 00:19:09,300
different ways that this kit could help someone address these kinds of issues, just like

257
00:19:09,300 --> 00:19:14,380
statistically working with biases that might be in their data.

258
00:19:14,380 --> 00:19:19,620
But even with six, they're like, oh, and it's open, other people could add their own.

259
00:19:19,620 --> 00:19:22,940
There are just so many different definitions and ways that you might want to approach

260
00:19:22,940 --> 00:19:23,940
this.

261
00:19:23,940 --> 00:19:24,940
Yeah, that's right.

262
00:19:24,940 --> 00:19:34,020
One of my favorite papers this year in Newrips is my classifier discriminatory.

263
00:19:34,020 --> 00:19:39,560
It's a really good paper because it also talks about let's root out the bias and let's

264
00:19:39,560 --> 00:19:42,420
find where this bias is coming from.

265
00:19:42,420 --> 00:19:49,980
It's not enough maybe to play with the optimization functional objective function.

266
00:19:49,980 --> 00:19:56,260
It might be better to start trying different sampling strategies or even start capturing

267
00:19:56,260 --> 00:19:58,180
more information about the entities.

268
00:19:58,180 --> 00:20:03,180
For example, there might be a strong predictor that we are not using in our data.

269
00:20:03,180 --> 00:20:08,140
So it's kind of like it's not only about optimization, it's not only about mass, it's

270
00:20:08,140 --> 00:20:12,020
also about figuring out what is missing here.

271
00:20:12,020 --> 00:20:17,220
And I guess it was also in my practical experience more important than the mass or the optimization

272
00:20:17,220 --> 00:20:18,220
part.

273
00:20:18,220 --> 00:20:24,020
Quite a couple of approaches in the literature, but none of them actually gave us a pretty

274
00:20:24,020 --> 00:20:25,420
good result.

275
00:20:25,420 --> 00:20:29,820
So what do you mean by finding other predictors?

276
00:20:29,820 --> 00:20:34,900
I guess what I'm hearing is something that any data scientist is always trying to do

277
00:20:34,900 --> 00:20:41,260
find additional features or variables that might have predictive value.

278
00:20:41,260 --> 00:20:43,820
Are you getting at something different?

279
00:20:43,820 --> 00:20:48,860
You know exactly is the same thing, like that you can understand if you can kind of figure

280
00:20:48,860 --> 00:20:56,740
out what are the holes in your system and can you get access and you're right every time

281
00:20:56,740 --> 00:21:01,380
we're asking more information, we are asking for more resources, time and effort.

282
00:21:01,380 --> 00:21:08,500
And the question mark is that it's worth it to invest in kind of capturing more information

283
00:21:08,500 --> 00:21:10,540
and getting access to that data.

284
00:21:10,540 --> 00:21:16,220
And sometimes it's worth it if we kind of realize it has a huge impact both in the performance

285
00:21:16,220 --> 00:21:21,780
of the model and also on the bias and fairness issues we have.

286
00:21:21,780 --> 00:21:31,500
One of the things that seems a bit paradoxical in this space is that I think I have the impression

287
00:21:31,500 --> 00:21:37,240
this isn't particularly well informed, but my impression is that in a lot of highly

288
00:21:37,240 --> 00:21:44,780
regulated industries, you know take for example banking, they historically haven't collected

289
00:21:44,780 --> 00:21:50,020
information about these protected attributes because they don't want to be accused of

290
00:21:50,020 --> 00:21:52,140
using them to make decisions.

291
00:21:52,140 --> 00:21:57,820
And now we're talking about using statistical approaches that require that need that information.

292
00:21:57,820 --> 00:21:58,820
Have you run into that kind of thing?

293
00:21:58,820 --> 00:22:04,780
Yeah, I was in the panel exactly with a couple of other people, VPs and leaders of those

294
00:22:04,780 --> 00:22:09,500
financial institutes and they were, I was kind of arguing and challenging them, but I need

295
00:22:09,500 --> 00:22:10,500
access.

296
00:22:10,500 --> 00:22:14,700
As a data scientist, I need access to those protected attributes and they were like, why

297
00:22:14,700 --> 00:22:16,940
should we give you access?

298
00:22:16,940 --> 00:22:20,620
And I know there might be lots of privacy issues as well.

299
00:22:20,620 --> 00:22:23,620
So I was actively thinking about what would be the solution.

300
00:22:23,620 --> 00:22:28,700
So if you don't want to give access to those protected attributes, at the same time you

301
00:22:28,700 --> 00:22:33,140
want to give the data scientist the opportunity to test their models against different kind

302
00:22:33,140 --> 00:22:38,540
of biases, what would be the solution, the simplest one I could find was like having

303
00:22:38,540 --> 00:22:40,020
an API.

304
00:22:40,020 --> 00:22:45,100
So you don't access the data scientist attributes, but you can have an API that you can send

305
00:22:45,100 --> 00:22:50,180
your requests to a server and you can get the answer.

306
00:22:50,180 --> 00:22:55,220
So that's kind of like simple, simple maybe kind of solutions, maybe we need to do more

307
00:22:55,220 --> 00:23:02,060
research on what's the best kind of framework or process.

308
00:23:02,060 --> 00:23:06,980
But yeah, that's a kind of huge problem we have with the senior leaderships of the companies

309
00:23:06,980 --> 00:23:11,420
that they are like, why do you need access and it's hard to convince them.

310
00:23:11,420 --> 00:23:16,060
We do want to use them to improve the performance of our models, actually when we use them to

311
00:23:16,060 --> 00:23:17,580
test our models.

312
00:23:17,580 --> 00:23:22,020
And without having access to them, there is no way that we can't test our models.

313
00:23:22,020 --> 00:23:28,940
Is it an absolute requirement to be able to test against a particular kind of bias or

314
00:23:28,940 --> 00:23:35,820
bias against a particular attribute to have those attributes available as labels or are

315
00:23:35,820 --> 00:23:40,580
there other ways that you could infer that from your data or kind of explore from your

316
00:23:40,580 --> 00:23:41,580
data?

317
00:23:41,580 --> 00:23:42,860
That's a very good question.

318
00:23:42,860 --> 00:23:50,060
So you normally need to define the protected attributes and then test the models over

319
00:23:50,060 --> 00:23:58,380
different macro segments based on these protected attributes or sensitive features.

320
00:23:58,380 --> 00:24:03,660
But there are always proxies in data that we know they are a kind of good proxies of

321
00:24:03,660 --> 00:24:08,940
for example, even if we don't have access to gender, the other attributes might be very

322
00:24:08,940 --> 00:24:10,460
correlated with the gender.

323
00:24:10,460 --> 00:24:12,220
Okay, just part of the problem.

324
00:24:12,220 --> 00:24:13,220
Exactly.

325
00:24:13,220 --> 00:24:18,020
It's just like, yeah, that's exactly the problem that two years ago when we were talking

326
00:24:18,020 --> 00:24:21,940
about bias and fairness, they were telling us, okay, just remove those sensitive attributes

327
00:24:21,940 --> 00:24:22,940
from your data.

328
00:24:22,940 --> 00:24:28,620
We were like, okay, don't you know about correlations in features?

329
00:24:28,620 --> 00:24:33,100
But at the same time, we can't leverage those kind of correlations if we don't have access

330
00:24:33,100 --> 00:24:34,100
to it.

331
00:24:34,100 --> 00:24:39,220
And in fact, for that project that I was talking about, we didn't have access to lots of

332
00:24:39,220 --> 00:24:40,220
information like this.

333
00:24:40,220 --> 00:24:42,540
So we used lots of proxies.

334
00:24:42,540 --> 00:24:46,740
But you weren't necessarily able to even test your proxies against any kind of reaction.

335
00:24:46,740 --> 00:24:47,740
You're right.

336
00:24:47,740 --> 00:24:49,740
We don't know how accurate these proxies.

337
00:24:49,740 --> 00:24:54,300
The only possibility is this for example, for example, the address might be a proxy for

338
00:24:54,300 --> 00:24:55,300
ethnicity.

339
00:24:55,300 --> 00:24:56,300
Right?

340
00:24:56,300 --> 00:24:57,300
Sure.

341
00:24:57,300 --> 00:25:02,740
So again, then you can just on public data sets, you can kind of test the correlation between

342
00:25:02,740 --> 00:25:04,460
the address and ethnicity.

343
00:25:04,460 --> 00:25:09,740
That's the kind of possibility, but on your data set, yes, you can do anything.

344
00:25:09,740 --> 00:25:10,740
Yeah.

345
00:25:10,740 --> 00:25:17,540
All this is making me wonder if there's some kind of intersection between the differential

346
00:25:17,540 --> 00:25:22,940
privacy and fairness, not, you know, so setting aside the privacy uses for differential privacy

347
00:25:22,940 --> 00:25:30,700
like, are there ways to, I don't know, maybe mixing up streams, but I talked to someone

348
00:25:30,700 --> 00:25:39,300
else who was working on kind of a, like a cryptographic way to allow people to do testing

349
00:25:39,300 --> 00:25:46,580
against data or share data without giving them the data itself, which is kind of basically

350
00:25:46,580 --> 00:25:47,580
like this API.

351
00:25:47,580 --> 00:25:48,580
Exactly.

352
00:25:48,580 --> 00:25:50,940
It's the same basic thing.

353
00:25:50,940 --> 00:25:55,180
And so I don't know if the only thread between those two is, no, that's a very good point.

354
00:25:55,180 --> 00:26:03,340
And I believe two years ago in KDD, Cynthia, she also had a invited talk and she talked

355
00:26:03,340 --> 00:26:08,980
about maybe we should use techniques similar to differential privacy for fairness.

356
00:26:08,980 --> 00:26:13,700
And she talked, okay, like in differential privacy, actually, if you think of like the

357
00:26:13,700 --> 00:26:18,780
kind of data we have as a kind of matrix, and in differential privacy, we want to mask

358
00:26:18,780 --> 00:26:20,820
a row in the matrix.

359
00:26:20,820 --> 00:26:27,260
So which means like we want our models to not be sensitive to any single user information.

360
00:26:27,260 --> 00:26:34,380
But in kind of like when we talk about fairness, we want our models to be exactly, to be vertically

361
00:26:34,380 --> 00:26:37,780
in sensitive to some features.

362
00:26:37,780 --> 00:26:43,620
So there are some commonalities, but I didn't see any actual research in this domain.

363
00:26:43,620 --> 00:26:49,100
It's a great opportunity for researchers to start working on kind of using techniques

364
00:26:49,100 --> 00:26:51,660
like differential privacy for fairness.

365
00:26:51,660 --> 00:26:57,140
So you mentioned one paper that you were excited about here in NURP's, what was the name

366
00:26:57,140 --> 00:26:58,140
of that one again?

367
00:26:58,140 --> 00:27:04,020
Is my classifier discriminatory?

368
00:27:04,020 --> 00:27:09,180
And were there any other observations that jumped out at you from that presentation or

369
00:27:09,180 --> 00:27:10,180
from the paper?

370
00:27:10,180 --> 00:27:17,860
I guess the presentation is going to be tomorrow or in the poster, maybe tomorrow, but a friend

371
00:27:17,860 --> 00:27:24,980
of mine, she just told me about this paper and found it pretty interesting.

372
00:27:24,980 --> 00:27:30,780
There was another paper that I ran into last night in the poster session, but I can't

373
00:27:30,780 --> 00:27:31,780
remember.

374
00:27:31,780 --> 00:27:35,660
I can just Google it if you want.

375
00:27:35,660 --> 00:27:38,580
No, that's fine, that's fine.

376
00:27:38,580 --> 00:27:45,260
When you're working with startups that are trying to bring a product to market, they're

377
00:27:45,260 --> 00:27:51,780
under all kinds of intense pressures to bring a product to market, to satisfy customers

378
00:27:51,780 --> 00:27:58,020
like how do they, or maybe how do you typically have to convince them to pay attention to this

379
00:27:58,020 --> 00:28:00,380
or are they already thinking about it?

380
00:28:00,380 --> 00:28:01,380
It depends.

381
00:28:01,380 --> 00:28:04,860
Like some companies, they have already some data scientists that they have thought about

382
00:28:04,860 --> 00:28:08,940
those problems, some of them know, and we have to convince them.

383
00:28:08,940 --> 00:28:19,060
So, what I found out is that people are normally more familiar with security box and a normally

384
00:28:19,060 --> 00:28:26,340
kind of frame furnace or biased box as kind of unwanted association, but it's a box

385
00:28:26,340 --> 00:28:28,060
as well.

386
00:28:28,060 --> 00:28:33,740
And it's similar to privacy and security box that they are so hard to identify.

387
00:28:33,740 --> 00:28:35,980
But the risks are also very similar.

388
00:28:35,980 --> 00:28:40,260
So, the same risk of data breach, what would be the risk?

389
00:28:40,260 --> 00:28:44,220
What's going to happen to the valuation of the company?

390
00:28:44,220 --> 00:28:50,460
It's going to be very similar kind of impact on your brand if your model proved to be like

391
00:28:50,460 --> 00:28:52,700
discriminatory or biased.

392
00:28:52,700 --> 00:28:59,820
So, that's kind of high frame it, and I try to convince people to think about the importance

393
00:28:59,820 --> 00:29:01,540
of these issues.

394
00:29:01,540 --> 00:29:10,460
When you frame it like that on the security side and the security bugs that lead to breaches,

395
00:29:10,460 --> 00:29:20,580
there are increasing amounts of hard data about the costs of these kinds of breaches and

396
00:29:20,580 --> 00:29:25,260
the implications from leadership change perspective in that business.

397
00:29:25,260 --> 00:29:34,860
Are there similar examples from a breach of fairness or a breach of trust perspective

398
00:29:34,860 --> 00:29:37,300
or are you familiar with any?

399
00:29:37,300 --> 00:29:40,580
Not in the startup ecosystem on slave.

400
00:29:40,580 --> 00:29:48,540
For example, I guess you all know about a kind of blog post that was about Amazon recruiting

401
00:29:48,540 --> 00:29:52,500
software that they start using it.

402
00:29:52,500 --> 00:29:58,060
This was for folks that may not have come across this, an article that actually thought a

403
00:29:58,060 --> 00:29:59,900
lot of the headlines were disingenuous.

404
00:29:59,900 --> 00:30:06,020
It sounded like they were researching some, using some tool in HR and they found the tool

405
00:30:06,020 --> 00:30:12,700
to have a bias against women in the hiring process and they decided not to use the thing,

406
00:30:12,700 --> 00:30:15,420
which that sounds like what you're supposed to do.

407
00:30:15,420 --> 00:30:16,420
Exactly.

408
00:30:16,420 --> 00:30:17,420
Yeah.

409
00:30:17,420 --> 00:30:20,660
If it wasn't the case, you wouldn't even hear about it.

410
00:30:20,660 --> 00:30:23,180
Right.

411
00:30:23,180 --> 00:30:29,020
So there's that and I guess the Netflix prizes, the one that comes along, that's more privacy.

412
00:30:29,020 --> 00:30:30,020
It was privacy.

413
00:30:30,020 --> 00:30:31,020
Yeah.

414
00:30:31,020 --> 00:30:32,020
It was privacy.

415
00:30:32,020 --> 00:30:34,380
You simple the identification and it failed.

416
00:30:34,380 --> 00:30:37,820
For example, Google example, so that lots of people use it.

417
00:30:37,820 --> 00:30:43,540
I don't know if they fix it or not, but I remember six months ago, if you search for images

418
00:30:43,540 --> 00:30:47,100
of CEO, it was all white male.

419
00:30:47,100 --> 00:30:52,780
And it's a hard question and it's a kind of question that even for Michelle and Ingrid

420
00:30:52,780 --> 00:30:54,980
data scientists can't answer.

421
00:30:54,980 --> 00:31:00,900
It's a kind of question that you have to ask in the community of interdisciplinary researchers

422
00:31:00,900 --> 00:31:08,460
because actually, if you look at the statistics, what's the percentage of non-white male

423
00:31:08,460 --> 00:31:15,380
CEOs and it's a kind of, if you do random sampling, what's the likelihood of having one

424
00:31:15,380 --> 00:31:19,500
of those kind of, for example, women's deals?

425
00:31:19,500 --> 00:31:24,980
Because right now, but it's not about equality, it's more about equity and like that's kind

426
00:31:24,980 --> 00:31:30,860
of like hard to see, that's kind of, as a kind of scientist, you're working on this problem

427
00:31:30,860 --> 00:31:32,380
and you have a random sampling.

428
00:31:32,380 --> 00:31:33,380
You don't know.

429
00:31:33,380 --> 00:31:40,660
So I would take that kind of restate that as part of the challenge is that we want our

430
00:31:40,660 --> 00:31:46,260
models to reflect our values more so than the data that we have to work with to create

431
00:31:46,260 --> 00:31:47,260
the models.

432
00:31:47,260 --> 00:31:52,980
And so model may be performing accurately relative to the data, but that's not necessarily

433
00:31:52,980 --> 00:31:53,980
what.

434
00:31:53,980 --> 00:31:59,980
Yeah, it is reflecting the data, which is reflective of many, many years of biases.

435
00:31:59,980 --> 00:32:00,980
Exactly.

436
00:32:00,980 --> 00:32:01,980
Right.

437
00:32:01,980 --> 00:32:06,620
And that's why we need to have more awareness that the main assumption of any machine learning

438
00:32:06,620 --> 00:32:11,700
model is that future is going to be similar to the past.

439
00:32:11,700 --> 00:32:17,460
So for any reason, if we don't want future to be the same as past, then we shouldn't

440
00:32:17,460 --> 00:32:23,660
rely on data and we shouldn't rely only on machine learning technologies.

441
00:32:23,660 --> 00:32:25,420
So what should we rely on?

442
00:32:25,420 --> 00:32:26,420
Do we know yet?

443
00:32:26,420 --> 00:32:27,420
No.

444
00:32:27,420 --> 00:32:36,540
I was like, again, like in Norepse yesterday, there was a talk, I can't even remember.

445
00:32:36,540 --> 00:32:43,980
There was like a tutorial, actually, Norepse Tutorials.

446
00:32:43,980 --> 00:32:44,980
It was very interesting.

447
00:32:44,980 --> 00:32:45,980
I didn't.

448
00:32:45,980 --> 00:32:46,980
Were you there for that?

449
00:32:46,980 --> 00:32:47,980
No.

450
00:32:47,980 --> 00:32:50,220
It was a very good one.

451
00:32:50,220 --> 00:32:54,340
And surprisingly, there were very, very few people in the room.

452
00:32:54,340 --> 00:33:00,780
So like most of the tutorials, they were packed, but this one, very, very few people.

453
00:33:00,780 --> 00:33:05,220
And I was like, why, these are like questions that we need to think about.

454
00:33:05,220 --> 00:33:10,300
And unfortunately, there are not so many people in our community in that room.

455
00:33:10,300 --> 00:33:16,300
So like, that was the sad part, but there was a tutorial on common pitfalls for studying

456
00:33:16,300 --> 00:33:20,460
the human side of machine learning.

457
00:33:20,460 --> 00:33:23,900
And they had very, very interesting points.

458
00:33:23,900 --> 00:33:29,420
So they were like, maybe explainability is not enough, maybe it's not only about casting

459
00:33:29,420 --> 00:33:33,100
the furnace as optimization, we should also fix the process.

460
00:33:33,100 --> 00:33:40,420
They had lots of interesting points, and one of the interesting problems, if they point

461
00:33:40,420 --> 00:33:46,620
out was data, data are not the truth.

462
00:33:46,620 --> 00:33:52,260
So also we know that there are lots of noise in capturing the data, and even if we talk

463
00:33:52,260 --> 00:33:59,220
about future, the past problem, we have data or machine learning models, but even sometimes

464
00:33:59,220 --> 00:34:04,460
the data is not even reliable because there are so many kind of noise in recording the

465
00:34:04,460 --> 00:34:05,460
data.

466
00:34:05,460 --> 00:34:11,060
So, but my problem with those kind of talks is that they don't talk about the solutions.

467
00:34:11,060 --> 00:34:16,460
And as a kind of practitioner, I'm always like waiting for, like, give me a solution,

468
00:34:16,460 --> 00:34:17,460
give me a solution.

469
00:34:17,460 --> 00:34:22,340
And it might not be a perfect solution, but it can be a starting point.

470
00:34:22,340 --> 00:34:27,180
And okay, we are, we have a long way to go, but they have to start from somewhere.

471
00:34:27,180 --> 00:34:33,620
Now structured is kind of your solution approach when you're engaging with one of these portfolio

472
00:34:33,620 --> 00:34:38,580
companies that, you know, recognizes that there's an issue or that they don't want there

473
00:34:38,580 --> 00:34:39,580
to be an issue.

474
00:34:39,580 --> 00:34:40,900
That's a very good point.

475
00:34:40,900 --> 00:34:46,620
So like, normally, if they don't, sometimes they also are aware of possible problems with

476
00:34:46,620 --> 00:34:48,020
their data and the problem.

477
00:34:48,020 --> 00:34:49,020
They are solving.

478
00:34:49,020 --> 00:34:53,340
But sometimes it's harder to convince them, so I ask them, can you give me the access

479
00:34:53,340 --> 00:34:57,300
to data and the predictions of your machine learning models?

480
00:34:57,300 --> 00:34:58,500
And then I run some tests.

481
00:34:58,500 --> 00:35:01,100
For example, they told you about this fair test.

482
00:35:01,100 --> 00:35:06,300
It can give you all the statistics about kind of performance of the model, accuracy,

483
00:35:06,300 --> 00:35:07,300
different macro segments.

484
00:35:07,300 --> 00:35:13,500
And it's very different combinations of these protected and non-protected features.

485
00:35:13,500 --> 00:35:19,420
And then I come with some kind of proofs, okay, these are my evidences that your model

486
00:35:19,420 --> 00:35:20,740
is biased.

487
00:35:20,740 --> 00:35:21,740
Okay.

488
00:35:21,740 --> 00:35:27,060
And let's think about the possible solutions.

489
00:35:27,060 --> 00:35:33,700
And if they are using any kind of sampling strategy, we would start with improving the

490
00:35:33,700 --> 00:35:34,860
sampling strategy.

491
00:35:34,860 --> 00:35:39,500
If you have lots of data on your only using parts of your data, how can we improve the

492
00:35:39,500 --> 00:35:46,020
model by leveraging better sampling strategies or like other parts of the data that you are

493
00:35:46,020 --> 00:35:47,020
not using?

494
00:35:47,020 --> 00:35:54,140
Is the implication there that the problems are introduced by their sampling strategy?

495
00:35:54,140 --> 00:36:00,060
Or they can use alternative sampling strategies to overcome problems that are already in

496
00:36:00,060 --> 00:36:01,060
their data?

497
00:36:01,060 --> 00:36:02,060
Exactly.

498
00:36:02,060 --> 00:36:07,100
They can use alternative sampling strategies or iteratively try different sampling strategies,

499
00:36:07,100 --> 00:36:12,340
test the model in that fair test kind of framework, kind of evaluating and observing

500
00:36:12,340 --> 00:36:16,220
the result and having the second iteration.

501
00:36:16,220 --> 00:36:20,580
The other kind of solution is that if they are getting their labels, if they're having

502
00:36:20,580 --> 00:36:24,660
a supervised learning problem, getting the labels from the humans, and if they are using

503
00:36:24,660 --> 00:36:31,100
human annotators, how can you define strategies to minimize the human bias?

504
00:36:31,100 --> 00:36:38,100
So like I remember for my PhD, I used mechanical turics to label my data.

505
00:36:38,100 --> 00:36:45,140
So we had all these kind of quality assurance techniques to make sure that we're getting

506
00:36:45,140 --> 00:36:49,900
kind of accurate labels from our annotators, so we can do similar things.

507
00:36:49,900 --> 00:36:53,940
So like quorum types of approaches and that kind of thing?

508
00:36:53,940 --> 00:36:54,940
Exactly.

509
00:36:54,940 --> 00:36:56,140
That's another one.

510
00:36:56,140 --> 00:37:02,180
The other one is like, for example, one of my other favorite papers is the paper by

511
00:37:02,180 --> 00:37:10,180
Rich Zemol, which is about learning a representation for your data that captures as much as possible

512
00:37:10,180 --> 00:37:16,300
the information, but at the same time masks all the sensitive attributes.

513
00:37:16,300 --> 00:37:22,060
So kind of having two competitive goals when you are learning representations for your

514
00:37:22,060 --> 00:37:23,420
entities.

515
00:37:23,420 --> 00:37:27,980
It might not be possible for some applications because like we don't have enough data to

516
00:37:27,980 --> 00:37:34,100
learn the actual representation, we are not using fancy deep neural networks.

517
00:37:34,100 --> 00:37:37,980
So like we have a couple of solutions, but still sometimes we are in a position that

518
00:37:37,980 --> 00:37:43,380
we don't know what can be the solution and we start researching again.

519
00:37:43,380 --> 00:37:49,980
So first step is playing with sampling strategies and seeing if we can solve it there.

520
00:37:49,980 --> 00:37:58,060
And then the next step is just diving in and it sounds like it's a combination of data

521
00:37:58,060 --> 00:38:03,620
science and research and subject matter expertise and it's, you know, like other aspects

522
00:38:03,620 --> 00:38:04,620
of the science.

523
00:38:04,620 --> 00:38:09,420
It's kind of all these two apart, yeah, exactly like any other problem.

524
00:38:09,420 --> 00:38:16,540
You've mentioned how a lot of this work is interdisciplinary, are there specific disciplines

525
00:38:16,540 --> 00:38:23,340
that you kind of look to first when you're looking for practical approaches that you might

526
00:38:23,340 --> 00:38:27,940
be able to incorporate into solving a company's problem?

527
00:38:27,940 --> 00:38:34,340
Normally what I found really useful is engaging with the product managers.

528
00:38:34,340 --> 00:38:39,100
Because they have, they are the, or the product owners because they have the best understanding

529
00:38:39,100 --> 00:38:43,460
of their user requirements and the addressable markets.

530
00:38:43,460 --> 00:38:50,660
So like even their model might be discriminatory because they are not serving part of the population.

531
00:38:50,660 --> 00:38:55,060
So it's not like cohort of the user might not be in the addressable market.

532
00:38:55,060 --> 00:39:01,660
So it's really good to have their insights before having any kind of further experiments

533
00:39:01,660 --> 00:39:05,660
or exploratory kind of analysis.

534
00:39:05,660 --> 00:39:10,500
So like I found like product people are the best people to talk to.

535
00:39:10,500 --> 00:39:18,180
But what is really missing is a kind of trust officer in our companies.

536
00:39:18,180 --> 00:39:22,900
So and we try to, we just started to think about these problems.

537
00:39:22,900 --> 00:39:27,460
Do we need a trust officer and it's not about the person who can solve every problem,

538
00:39:27,460 --> 00:39:32,020
but this is a person that can collect all the information from different stakeholders.

539
00:39:32,020 --> 00:39:33,020
Right.

540
00:39:33,020 --> 00:39:38,580
And he's like the owner of building trust with the customers.

541
00:39:38,580 --> 00:39:40,340
And again, it's not going to be a single person.

542
00:39:40,340 --> 00:39:45,260
It should be education for the whole company.

543
00:39:45,260 --> 00:39:51,980
And even we suggested you should have, you should add another kind of metric in your performance

544
00:39:51,980 --> 00:39:56,140
evaluation when you're evaluating your employees performance.

545
00:39:56,140 --> 00:40:05,780
You should add the trust component or responsible innovation or responsible AI kind of metric

546
00:40:05,780 --> 00:40:10,580
when you're evaluating your customer, your employees performance.

547
00:40:10,580 --> 00:40:12,180
It's not going to be a single person.

548
00:40:12,180 --> 00:40:18,420
But again, you should engage as many as possible people from the company and even outside

549
00:40:18,420 --> 00:40:19,420
of the company.

550
00:40:19,420 --> 00:40:25,620
Because for example, social sociologists, like we normally don't have anybody with sociology,

551
00:40:25,620 --> 00:40:27,420
background in our companies.

552
00:40:27,420 --> 00:40:31,740
So it's really important to engage those people if you have such question like the Google

553
00:40:31,740 --> 00:40:32,740
problem.

554
00:40:32,740 --> 00:40:33,740
Right.

555
00:40:33,740 --> 00:40:34,740
Yeah.

556
00:40:34,740 --> 00:40:35,740
That's interesting.

557
00:40:35,740 --> 00:40:41,340
I can see at larger companies, I wonder where this will end up living, right?

558
00:40:41,340 --> 00:40:47,980
You've got your, you know, chief data officers that are responsible for, you know, both kind

559
00:40:47,980 --> 00:40:52,780
of protecting and securing data or, you know, sometimes it's a CSO, chief information

560
00:40:52,780 --> 00:40:58,940
security officer who owns that, a CDO might own kind of monetizing data or, you know,

561
00:40:58,940 --> 00:41:00,380
the way that they manage it.

562
00:41:00,380 --> 00:41:06,740
But to your earlier point, a lot of the, you know, the challenge and the risk for larger

563
00:41:06,740 --> 00:41:09,980
companies would fall under someone who owns the brand, right?

564
00:41:09,980 --> 00:41:10,980
Yeah.

565
00:41:10,980 --> 00:41:15,220
I guess it says that it needs to be something that companies think about at the highest

566
00:41:15,220 --> 00:41:19,220
levels and have dialogues about.

567
00:41:19,220 --> 00:41:27,540
Exactly, somebody that can have a decision-power and is in all the decision-making discussions,

568
00:41:27,540 --> 00:41:33,260
somebody in a leadership team like that because you want to make sure that for every decision

569
00:41:33,260 --> 00:41:41,580
you make that can impact your users or customers, you have a person thinking about all these

570
00:41:41,580 --> 00:41:49,140
kinds of problems that might happen later when you're, when you're deploying your

571
00:41:49,140 --> 00:41:50,140
systems.

572
00:41:50,140 --> 00:41:59,460
And so are there, when you're kind of looking for research to incorporate into finding

573
00:41:59,460 --> 00:42:02,380
a solution to a given problem?

574
00:42:02,380 --> 00:42:07,980
Is there anything unique about your approach to that in this space as opposed to trying

575
00:42:07,980 --> 00:42:16,300
to solve a vision problem using unique research or is it kind of the same Google or kind

576
00:42:16,300 --> 00:42:18,860
of search or something?

577
00:42:18,860 --> 00:42:20,780
That's a very good point.

578
00:42:20,780 --> 00:42:27,700
I try to not bias myself by reading only papers from Shalani community.

579
00:42:27,700 --> 00:42:32,980
So it's still, it's still its research and I'm scientist and I have to dig into different

580
00:42:32,980 --> 00:42:41,300
kind of papers and articles and blog posts out there, but at the same time I'm trying

581
00:42:41,300 --> 00:42:46,700
to read more papers from other disciplines as well.

582
00:42:46,700 --> 00:42:52,220
So that's the only thing I can also like, I try to be active in the community and having

583
00:42:52,220 --> 00:42:56,620
lots of conversation with people with different backgrounds.

584
00:42:56,620 --> 00:43:03,460
It also helped me a lot, but I really don't have any good answer to your question that

585
00:43:03,460 --> 00:43:09,020
is there any fundamental difference between this problem and any other kind of problem

586
00:43:09,020 --> 00:43:10,020
we are solving?

587
00:43:10,020 --> 00:43:11,020
Right, right.

588
00:43:11,020 --> 00:43:20,780
Are there particular communities that you've found to be very useful and under-recognized

589
00:43:20,780 --> 00:43:26,340
I guess in terms of contributions to this type of work?

590
00:43:26,340 --> 00:43:33,940
Underrecognized, again, ethics in here is also hot, so that's less of a problem.

591
00:43:33,940 --> 00:43:36,820
Exactly, it's like harder than a couple of years ago.

592
00:43:36,820 --> 00:43:37,820
Yeah, exactly.

593
00:43:37,820 --> 00:43:38,820
It's not hot.

594
00:43:38,820 --> 00:43:41,300
Lots of people interested to work in this area.

595
00:43:41,300 --> 00:43:49,140
Lots of conferences, talks, events, yeah, top of my mind I have no example.

596
00:43:49,140 --> 00:43:53,620
Any other things that you're looking forward to seeing here at NURBS on this particular

597
00:43:53,620 --> 00:43:55,140
topic?

598
00:43:55,140 --> 00:44:02,740
I believe what is missing is that our community at least historically wasn't very welcoming

599
00:44:02,740 --> 00:44:08,500
for people with different backgrounds, so I told you that I in my PhD was working on

600
00:44:08,500 --> 00:44:15,860
NLP and I believe I met lots of people with linguistic kind of backgrounds in NLP conferences

601
00:44:15,860 --> 00:44:23,260
and it was more welcoming, so but here if you don't have mass background, you kind of

602
00:44:23,260 --> 00:44:32,020
full intimidated, so and I don't know how we can encourage people with not necessarily

603
00:44:32,020 --> 00:44:38,260
machine learning background to join our community and also give us comments and challenges and

604
00:44:38,260 --> 00:44:44,180
challenge our kind of mathematical optimization techniques.

605
00:44:44,180 --> 00:44:47,020
That's I guess what I'm fully missing right now.

606
00:44:47,020 --> 00:44:53,580
Yeah, I guess that kind of underscores one of the, we are kind of starting to have this

607
00:44:53,580 --> 00:45:00,940
conversation often about the role of diversity, like on teams and helping create awareness

608
00:45:00,940 --> 00:45:07,940
of these kinds of issues and identify, you know, to help an organization be open to understanding

609
00:45:07,940 --> 00:45:11,620
where these issues might lie and it sounds like you're kind of underscoring that at the

610
00:45:11,620 --> 00:45:13,700
level of the community as well.

611
00:45:13,700 --> 00:45:14,700
Yeah, exactly.

612
00:45:14,700 --> 00:45:21,380
Also, you are right in our teams, you also need to have more diverse kind of opinions.

613
00:45:21,380 --> 00:45:26,380
And again, when we talk about diversity, it's not only about ethnicity or gender, it's

614
00:45:26,380 --> 00:45:29,940
also about having people from different backgrounds.

615
00:45:29,940 --> 00:45:34,260
Well, Paranas, thanks so much for taking the time to chat with me about the stuff, super

616
00:45:34,260 --> 00:45:37,900
interesting stuff and as always wonderful to catch up with you.

617
00:45:37,900 --> 00:45:41,620
It was a pleasure and I also really enjoyed the chat.

618
00:45:41,620 --> 00:45:44,900
It was as simple as Chang told me.

619
00:45:44,900 --> 00:45:54,980
Well, we had a good one on the topic of differential privacy, Chang and I and I would encourage

620
00:45:54,980 --> 00:46:01,940
folks who haven't listened to the former series on differential privacy, they should take

621
00:46:01,940 --> 00:46:04,180
a listen to that one because it's good stuff.

622
00:46:04,180 --> 00:46:05,180
Perfect.

623
00:46:05,180 --> 00:46:06,180
Thank you.

624
00:46:06,180 --> 00:46:12,940
All right, everyone, that's our show for today.

625
00:46:12,940 --> 00:46:19,300
For more information on Paranas or any of the topics covered in this show, visit twimmelai.com

626
00:46:19,300 --> 00:46:22,220
slash talk slash 208.

627
00:46:22,220 --> 00:46:26,340
Thanks once again to the good folks over at Georgian Partners for their sponsorship of this

628
00:46:26,340 --> 00:46:27,740
series.

629
00:46:27,740 --> 00:46:54,500
As always, thanks so much for listening and catch you next time.

