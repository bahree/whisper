Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Hey everyone, Sam again with another quick Twimble
Con update. One of the things that's been especially exciting to see is the number of organizations
sending multiple people in some cases entire teams to Twimble Con to learn about scaling
and operationalizing machine learning. A full third of the companies attending are sending
groups in many cases three four and five people. This is awesome. Seeing so many teams attending
is a great indicator that folks really see the opportunity associated with improving the
efficiency of their data science and machine learning operations and are excited about
the conversations we'll be curating at the event. If you'd like to attend Twimble
Con with your team just reach out to me at Sam at Twimlai.com and let's make it happen.
Of course you're welcome to reach out to me if you want to attend as an individual or
just head over to twimblecon.com slash register to sign up. All right on to today's show.
All right everyone I am on the line with Anubav Jane. Anubav is a staff scientist and
chemist at Lawrence Berkeley National Lab as well as the group lead for the hacking materials
research group. Anubav welcome to this week in machine learning and AI.
Yeah it's good to be here. I've actually learned a lot by listening to your podcast so
it's really exciting to be able to share my work with with your listeners as well.
That is fantastic to hear. So the group you lead is called the hacking materials research
group. That is such a compelling name. Tell us a little bit more about your kind of focus
and charter and what you're trying to accomplish.
Yeah so what we're really trying to do with the research group is to imply computing to
accelerate the process of finding new materials for functional applications. So new chemical
compositions or new crystal structures that have really interesting properties. And these
days it's actually possible to use computer simulations and also data mining to really
change the way that that process is being done. So my group works both on theoretical
methods to predict materials properties. And it also works on data mining and machine
learning techniques to help accelerate materials design.
Awesome and so what led you to combining computing and machine learning and materials.
What was your path?
Yeah so I got into material science because I really liked kind of its central premise
which is that you know everything whether it's an airplane or a computer processor or
a battery. It's all made of materials and the properties of those materials really determine
the fundamental limits on the performance of every device. And you know many people have
pointed out that you know whenever there's the fundamental materials advance we often
name like an entire historical era after it you know the stone age the iron age the age
of plastics or the silicon age. And so you know I've really been interested in you know
how can we actually.
I hadn't heard that before that sounds amazing.
Yeah I mean really underscores the importance of materials and the opportunity is available
now that we're kind of applying machine learning and data science to the field.
Yeah and there's also sort of new applications like quantum computing you might have heard
of or you know new biological technologies that really require materials advancements as
well.
But now you know typically material science has been very data poor because the only way
conventionally to get information about materials was to perform an experiment. That means
you have to synthesize the material put in the machine and it might be like $10,000
per data point that you're collecting.
And so the way that I got into this field is that during my PhD I worked with someone
named Garrett Saider who was at MIT at the time he's now at Lawrence Berkeley Lab like I
am.
But what we were doing was we were using a particular type of computer simulation that's
rooted in quantum mechanics. And then we were using simulations that actually predict
the properties of materials do kind of a virtual measurement in the computer. And we
could do this for maybe like one to $10 and computing costs per material. We could scale
that over computing cluster so it was easy to parallelize. And then it was easy to create
these databases and materials properties based on simulation data.
So now we had a way to actually generate large materials data sets and maybe even two
large materials data sets where you know conventional analyses were difficult to actually
figure out what were the trends in that data.
So I got into data science and machine learning.
So we're generating some of these large databases of simulated materials properties as a way
to kind of figure out what more information we could extract from some of that materials
data.
That's an interesting trend that I talked to folks about kind of across different domains.
The integration of the use of simulated data and machine learning. I think one of the
most recent conversations on this topic was in the area of astronomy if I'm remembering
correctly, talk to me a little bit more about how you are combining that simulated data
and machine learning.
Yeah, so typically the way that it's done now isn't a kind of a tiered screening process
for new materials.
So when you have an idea about a new materials or a new chemical space that might be used
for functional application, you might first do a machine learning prediction and that machine
learning might be trained on simulated data because there usually isn't enough experimental
data to train on. So you first do like a machine learning prediction to see what might
be the interesting materials candidates within this broad chemical space.
Then you might run some simulations on the things that are interesting for machine learning.
The simulations are a little bit more expensive and complicated and time consuming to do.
And then finally, if it passes both of those tests, you can actually do experiments.
You might want to actually target doing some real experiments on that material.
So that's one paradigm in which the techniques are working together.
There's also kind of active learning and adaptive design type frameworks where you do this whole
thing in a loop.
So you do iterative machine learning and based on results of the experiments, you retrain
your machine learning model, run the computations and do this whole thing in a loop as well.
So there's a few different kind of paradigms in which you can mix these different techniques.
Do you apply different types of domain adaptation techniques when you're trying to use the simulated
data in the materials domain?
Is that a technique that's effective in the kind of for-cure doing?
Yeah, so I'm actually unfamiliar with the term domain adaptation technique.
What does that refer to?
I guess, you know, similar in some ways to data augmentation.
It's really take, for example, in autonomous vehicles, you know, one of the things
that you hear people doing is training on, like, you know, different types of simulations
like video games, but the models that are trained in these simulated environments don't
generalize well to the real world, so they'll do different types of things to adapt the
or modify the simulation to help the model generalize or perform.
So, for example, they might, you know, take, you know, daylight scenes and turn them into
nighttime scenes or manipulate them so they seem more realistic.
I'm wondering if there are any kinds of tools or techniques that you apply to make models
trained on simulated data rather more applicable to the real world interactions?
Yeah, so I would say there's two parallel threads, none of which is maybe exactly what you're
mentioning here.
One of them is just kind of rooted in physics and trying to make the models as realistic
as possible by improving the underlying physics of those models.
So, try to just get them to be closer and closer to reality.
The second kind of approach that's taken that's somewhat similar is kind of a multi-fidelity
approach where you have some experimental data, you have a whole bunch of computational
data, and then you maybe use some of the computational predictions as features to help train
your experimental model, your model training on experimental data.
So, I think those sorts of approaches exist, but I haven't seen any sort of, you know, systematic
transformation between computational data and kind of real world results other than those
sorts of examples.
Oh, really interesting stuff.
Now, I probably should have mentioned this earlier.
We're primarily going to be focused not necessarily on some of the materials applications of machine
learning that you're working on, but rather some work that you recently published on applying
natural language processing to capturing information from academic literature.
So, maybe if you can talk a little bit about that work and its motivations and the connection
between it and what you're doing on the material side.
Yeah, so, you know, there are many, many millions, probably 50 to 100 million research articles,
scientific research articles that have been published, and there's probably been trillions
of dollars of investment in actually funding the research studies that typically the only
output of those research studies is a published research article.
So there is a ton of knowledge and a ton of opportunity in terms of extracting information
from published research articles.
And today, you know, it's kind of crazy, but the main way to actually get information
from research articles is to have, you know, researchers read them.
And that process is really, you know, limited.
I mean, as a researcher, I probably read a few dozen articles, like really read a few
dozen articles per year, maybe for some people, it's a few hundred, but there are, you know,
thousands of articles published on any domain in any given year.
So what we were trying to do in our study was to design some kind of a system that would
be able to, let's say, read these articles in some sense and synthesize information about
what those articles were saying, and be able to conceptualize, you know, what was happening
in material science by reading these articles.
And not only be able to have these internal conceptions of what the articles were talking
about or to make predictions that could be testable and help guide researchers in their studies.
And so in this paper, what we did was we actually collected the abstracts of over three million
articles restricted to material science.
We did some data pre-processing on those articles.
And then we actually trained an unsupervised algorithm called Word2Vec.
So this algorithm didn't receive any label data.
It didn't receive any chemical training.
But we found that by applying this algorithm on just the data set of material science articles,
it was able to conceptualize concepts like the periodic table.
And it could also predict what functional, what materials should be studied for functional
applications.
We found that through this procedure, we were able to predict, you know, materials that
should be studied for various applications many years before they were actually reported
in the literature.
So we did this process where we actually kind of virtually went back in time, made some
predictions about what material scientists should be studying, what materials they should
be studying for different applications.
And then we saw, you know, in reality, that they actually study those materials that we
predicted over the next few years.
And we found that, you know, we could actually predict at a very high rate what researchers
would be studying in the future.
Well, if we can pause here and maybe unpack some of that, I think probably many listeners
are generally familiar with Word2Vec and the concept of Word embeddings.
We've covered it quite a bit on the podcast.
But you say that, you know, in creating an embedding model, you're able to, the, or the
model is able to conceptualize the, the periodic table.
What does that mean for a model to be able to conceptualize the periodic table or what
are you trying to express with that?
Yeah.
So, you know, as you mentioned, there's been a lot of research into Word2Vec and people have
already demonstrated many times that it can, you know, understand grammatical relationships.
It can also understand semantic relationships like the concept of gender or capitals, etc.
What we're trying to show with this study is that Word2Vec also captures scientific
relationships.
So it actually captures knowledge that requires some amount of scientific knowledge in order
to, in order to, to express that relationship.
So for the periodic table, specifically, what we did is we, we trained these Word embeddings
and then we looked at Word embeddings of the chemical elements.
So words like helium, words like sodium, words like lithium.
And so these are 200 dimensional vectors that represent these words.
And then we projected those 200 dimensional embeddings into two dimensions, like the
periodic table.
We used T-SNE as the embedding mechanism, sorry, as the projection mechanism.
And then we found that when we actually projected these Word embeddings, which were trained
just on the way that chemists and material scientists mentioned these elements in the
course of their research, we actually got the same sorts of trends that you saw on the
periodic table.
So without really being explicitly trained on what the structure of the periodic table
was, we could recover that structure simply by reading these articles and, you know,
projecting these embeddings down to two dimensions.
And just to interrupt again, when you say recover that structure, meaning if you somehow
visualize this two dimensional embedding space and squint the right way, you can kind
of see the periodic table in there, or you can systematically, you know, further transform
this projection and get to something very close to the periodic table.
Yeah, so we did kind of two types of tests.
The first one is more similar to what you described at first, which is that when you project
the elements in two dimensions, you kind of see these clusters that correspond to the
types of groups in the periodic table.
So things like the alkali metals are all grouped together.
The halogens are all grouped together, et cetera.
And there's also structure within the clusters that you can see that correspond to moving
to certain directions in the periodic table.
But now, you know, you're, I think you've talked on this show before about how T-SNE can
distort things and distort distances, et cetera.
And so in addition to kind of just that T-SNE visualization and qualitatively looking
at it, we did another thing, which is we actually first took these two hundred dimensional
word embeddings and projected them down to 15 dimensions using just PCA.
And then we saw whether certain directions in that 15 dimensional embedding space corresponded
to various directions in the periodic table.
So for example, we saw whether there were certain directions in this 15 dimensional space
that corresponded to increasing the atomic weight in the periodic table or increasing the
electro negativity in the periodic table.
So what we were really doing is to see are there certain directions in this embedding space
that correspond to moving in certain directions in the periodic table?
And we found high correspondence in that as well, things like our squared values of about
point eight or so.
So we also did a more quantitative test of whether the information in the periodic table
was being captured by these embeddings.
Oh, really interesting.
Now, can you take a moment to kind of review or overview T-SNE and PCA for those that aren't
familiar with those terms and kind of how they play out in these experiments?
Yeah, so T-SNE is 10s for T-Stochastic Neighbor embedding.
It's just a way to take a high dimensional space like the 200 dimensional word embeddings
we've been talking about and projected them into a lower dimension in a way that tries
to preserve distances as closely as possible.
And PCA stands for Principal Component Analysis.
And it's a way to try and figure out kind of new directions, so new dimensions along which
to project our 200 dimensional vectors that try to capture as much information as possible
while reducing the number of dimensions down.
So in this case, in order to do, in order to test whether we were quantitatively reproducing
the periodic table, because there's only 100 elements and there's 200 dimensions in our
word embeddings to do the test fairly to see whether we were capturing these directions
properly, we actually reduce it dimensionally down to 15 dimensions to make a fair test.
Awesome.
Awesome.
And so you created this embedding space, you kind of compared it to your intuition and
knowledge of chemistry and physics and found that there is some structure there that
resembles what we know about the way the world works, i.e. the periodic table.
And then with this as a basis, you're using it to try to predict materials that will be
useful in further research and literature.
How do you make that jump from the embedding space to these kinds of predictions?
Yeah.
So when you train these embeddings, you typically give it some kind of a task in which
you train the embeddings on.
So in the case of word to veck, the task that you train the embeddings on, there's multiple
versions, but we use something called skipgram.
And the task there is that given a certain word, that word could be a chemical element
like helium, it could be an application word like thermoelectric, or it could be just any
old word.
Given a word as an input, the skipgram model tries to predict what words will appear next
to that word in your document corpus.
We use a window size of eight, so plus or minus eight words from our target word.
So what we're training these embeddings on, the information that they represent is what
are the neighbors?
What are the types of neighboring words that occur next to each target word?
So now we have basically for every word in our corpus, an embedding that represents what
sorts of words are expected to appear next to that word in a scientific abstract, in
a material science abstract.
Now the way that we make this predictive is, a lot of times when people train word to
veck or similar embedding algorithms, once they get the embeddings, they throw away
the task.
The task is not really important anymore.
You have the embeddings and then you work with those.
What really the lead author of this paper did, his name is Vahey Chattoyan, he decided
not to throw away the task and he decided to use that task directly.
And what he said is, well listen, I want a thermoelectric material, I have a model that
can predict what sorts of words are likely to appear in a scientific abstract next to
the word thermoelectric, that's directly my word to veck model.
So let me use my word to veck model to actually directly predict, in some sense, what words
will appear next to the word thermoelectric.
And then he filtered those down to just chemical composition words.
So which chemical compositions will likely appear next to the word thermoelectric?
And most of those chemical compositions predicted by the model are things that were already
known to be thermoelectric, so things that occurred in our corpus many, many times.
But then when Vahey got down, so let's say the 333rd prediction, he found something new.
He said, okay, here's a chemical composition that was never explicitly mentioned in our
corpus as a thermoelectric, yet it's ranking pretty high in the prediction of likely to
appear next to the word thermoelectric.
So maybe the model is telling us something, it's telling us that there's a high likelihood
that someone's going to publish a research article with this chemical composition next
to the word thermoelectric, even though no one has ever published that article before.
So he essentially turned the word to veck into a way to identify gaps in the research literature
by using these embeddings.
How do you validate your interpretation of a finding like that?
You know, imagining that that 331rd could be a chemical compound that has another name
or another kind of composition and has appeared, but under this other name or maybe it's just
random.
Like, is there, how do you validate that this is actually a function of the model that
you created and not just noise?
Sure.
Yeah, so I should first say that we're pretty confident that the materials that we predict
as new predictions are actually things that haven't been studied before.
Because first of all, we can check our abstract corpus to make sure that that compound has
not appeared next to the word thermoelectric or any other word that would indicate studying
that application before.
So part of it is that is using our corpus to figure out that this has been studied before
and then we also did a lot of manual checking whenever we published a prediction on the paper
just to make sure that this chemical composition that we're predicting is not actually something
that has been studied.
So I think we're pretty confident that it's actually a new prediction, but then whether
it's correct or not is I think a more complicated question.
The best validation would be to actually do the experiments.
So to make the material to characterize it and then measure the thermoelectric properties,
let's say.
Sure.
Yeah, that sounds expensive.
Yeah.
So we're working with collaborators right now on a couple of those top 50 materials that
we predicted in our paper, but it will probably be like six months or so before we actually
find out what the results are just because they have to first synthesize the material,
they have to purify it, they have to characterize it and they have to optimize and dope it and
all that stuff.
So it's going to take a few months to figure out whether the ones that we picked out of
the 50 are actually interesting or not.
I mean, I imagine it's even interesting if the model can come up with plausible candidates,
even if they don't ultimately work.
I mean, that's what scientists do.
I guess I'm trying to get at a more kind of technical validation that it's not just
kind of random, you're finding these plausible predictions more consistently than just any
combinations of molecules that appear that can be made up from the embedding space.
Does that question make sense?
Yeah, yeah.
So, absolutely.
And so in the absence of experiments, we actually did three types of alternate validation.
So I think experiments would be the best, you know, you can't argue with it type validation.
But then in the absence of experiments, we did three other types of validation.
The first type of validation we did is that we had an independent data set of simulated
data on thermal electrics.
So these are based on, you know, physics-based simulations.
So the two data sets between NLP and the simulations, they don't talk to each other.
And then what we did was we looked at correspondence between the ranking of thermal electrics that
we did using natural language processing technique.
And the thermal electric properties as calculated by the simulated data.
And what we found is like pretty remarkable, which is that if you look at, for example,
the top 10 materials in the NLP rankings, there are a lot of them are like the 99th percentile
99.5th percentile, et cetera, of the actual simulated data properties.
So there's a high correspondence between the word-to-vec rankings, the NLP rankings,
and the simulated data.
And that's not only on known materials where you would expect that the word-to-vec would
pick up that gets a known thermal electrics, so I'm going to rank it highly, but also
on unknown materials.
So we have simulated data on the predicted data and the things that we predict to be high
with NLP are also high in the simulations for predictions.
So that's kind of one validation is just consistency between simulations and the NLP.
The second type of test that we did was against experimental data.
And here what we tried to do is to say, because experimental data would be all on known thermal
electric materials, we actually tried to make sure that the quality of the ranking also
corresponded to the quality of the experimental results, so the quality of the ranking and
the experimental results.
So things that had a higher word-to-vec ranking would also have a higher experimental-based
ranking.
And so that second validation also panned out where there was kind of a rank correlation
between our NLP results and between the actual experimental data.
And then finally the third type of validation that we did was basically a forecasting-based
cross-validation.
And what we did is we created these virtual data sets where we asked the question, well,
if we had invented this technique back in the year 2000.
So we threw out all the data of abstracts from past the year 2000, and we trained the
model only in the year 2000, for example, on data up to the year 2000.
And then we made predictions at a particular point in time.
And then we saw what actually happened in the research community in the next 19 years
from 2001 to 2019.
We did those predictions that we made using the 2000 data set actually pat out over the
next 19 years.
And we repeated this process for every year from the year 2000 to 2018 or so.
Where every year we kind of virtually predicted what the algorithm would have predicted.
And then compared that to what was actually observed in the research literature.
And we found that the rate at which the materials that were predicted by the algorithm were
actually reported in literature was much, much higher than if you had just picked random
materials from the data set at the time.
And even more than if you had used some kind of chemical heuristics and simulation-based
heuristics to pick the materials from at the time.
So something like, you know, within the first five years, you're like eight times more
likely to get a thermal electric material using this algorithm versus just picking a
random material.
So that was like a third type of validation we did.
And we found through that type of validation that often you could find new thermal electric
materials or suggest new thermal electric materials five to ten years before they were first
reported in the literature.
Oh, that's super interesting.
So it sounds like you did find that the algorithm was the predictions that the algorithm
came up with tended to be kind of like low hanging fruit in a sense that they were found,
you know, within the fur- they tended to be found within the first five years as opposed
to, you know, something that I guess I wouldn't expect
us of an algorithm to come up with truly novel compositions, you know, things that to
20 years to figure out is that basically in line with what you saw?
Yeah, well, so when you look at the predictions that the algorithm makes, so if you look
at, for example, the predictions from right now that the algorithm makes, some of them,
a lot of them, I would say, are what I would call adjacent to known thermal electrics.
So these are things that, you know, maybe are fairly likely that someone would find in
the next few years anyway, even if it weren't for the algorithm.
But I will say that, you know, a five-year acceleration is actually a pretty big deal.
The typical timescale for kind of materials development is usually quoted around 20 years
based on an article that was published in the 90s.
And so cutting five years off of that is actually a, would be considered a major milestone,
I would say.
Absolutely.
Yeah, so, you know, so a lot of the predictions are, I would say, you know, chemically
adjacent to some of the known thermal electrics and probably would be found in a few years
regardless.
But then there are also a bunch of predictions in there that look, would look strange.
I would say to someone who is interested in thermoelectric materials.
And those materials, I think, are more of the wild card because, you know, they could
be strange and they could just be incorrect.
But they could also be strange and they could also be correct at the same time, in which
case it would be very interesting because it's finding really unconventional materials
that, you know, may or not have been studied if it weren't for the algorithm even after
10 years or even after 20 years.
So, you know, it will really require someone to really, the only way to really test that
out is to do experimental tests on some of them and see whether they pan out or not.
But yeah, there's a mix of things that, you know, maybe would have been found in the
next five years or so and things that probably wouldn't be studied even after quite some
time.
But I would emphasize that even five years is actually considered to be a very big acceleration.
And is the testing that you're currently doing with your collaborators?
Is that focused on either end of the spectrum or do you have folks looking kind of across
the adjacent materials as well as the ones that are, you know, strange looking?
Yeah, so, unfortunately, we don't have any dedicated funding to pay experimentalists to
just look at whatever materials we'd want them to look at.
If we had that, yeah, if we had that, I think we would try to spread it out a little
bit more and do kind of both.
But because we're kind of working, these experimentalists that are collaborating with us, they're
doing it essentially in their spare time.
And so they typically will work on materials that are similar to things that they're already
interested in, so things that are, you know, more simple to make and things that are, you
know, kind of within their comfort zone of synthesis and characterization.
So I would say that the couple that we're working on are more in the adjacent category
than the wild and crazy category.
And is the predictions that the model is making?
Is it predicting structure in addition to components?
For example, I'm thinking if you gave it a prompt, like liquid or a wet, and it came
up with hydrogen and oxygen, you know, that, you know, it's still missing the, you know,
the two in H2O and that configuration.
Is it giving you that or is it, does it leave that bit to the imagination, so to speak?
Yeah, so it's giving you a full chemical composition.
So it would give you the H2O, but it isn't giving you a structure.
So it isn't telling you whether it's ice or water or water vapor, for example.
And that is actually one of the big limitations of the study, you know, in material science,
one of the main things you study is how the arrangement of atoms, not just the chemical
formula, but the arrangement of atoms affect properties.
And so for example, if you think of just carbon, carbon can exist as diamond, it can exist
as graphite, or it could exist as like a nano tube.
And in our model, all those carbons are the same, same carbon.
And you know, you could imagine that a more advanced embedding technique, so some of the
context-sensitive embeddings, something like BERT, for example, might be able to distinguish
between those carbons and provide different predictions for carbon that are synthesized
or that are, have different types of structures within them.
But within our word-to-back model, where every carbon is the same regardless of whether
it's diamond or graphite, it's really just the chemical formula that's being used to
make the prediction.
How is it coming out with the chemical formula for these different compounds?
Yeah, so that's actually another one of the limitations of the current approach, which
is that the current approach can only rank chemical formulas that are observed in the
abstract corpus for an application.
So it's not inventing new chemical formulas, it's only taking the list of, you know, tens
of thousands of chemical formulas that it's detected and raking them for a particular
application.
So maybe we're talking about, you know, things that have thermal electric properties,
but the only reason why the algorithm, the model, knows about this particular compound
is because someone looked at it for something else at some other time.
Yeah, that's right.
And so, you know, one of the associations that it might make, for example, is that the
algorithm has found that there is some overlap between compounds that are studied for photovoltaic
applications and thermal electric applications.
So it might detect, for example, that there's a particular material that has been studied
for photovoltaic applications.
It might have some other keywords that it likes that are also associated with thermal
electric, like Chalkoginite or, you know, Hoysler, except, for example, and decide that,
you know, here's something that has a lot of keywords that tend to associate with thermal
electric, but hasn't been studied before.
So it's making those sorts of connections across the literature.
And so, in order to train this embedding and build out this model, did you hear where
there are things that you needed to kind of invent or innovate on beyond kind of word
to veck as, you know, it's becoming increasingly common, or was it a fairly straightforward application
of word to veck to get to this?
Yeah, so, you know, unfortunately, one of the barriers to doing scientific literature
mining is that the data sets are not easily available.
Is the scientific literature itself?
Yeah, well, because all the abstracts and everything are under published for agreements.
And so, for example, we couldn't publish the data set of abstracts to share with everybody.
We were legally not allowed to.
And so, and so what that implies is that anytime you do a study like this, you have to spend
a lot of time in collecting the data and pre-processing the data and tokenization and all that
stuff.
And just to be clear, so your entire model is based only on abstracts or the full content
of the articles.
Yeah, so what we showed in the paper was based only on abstracts, so no full text there.
And so, in terms of whether it was just straight to word to veck or, you know, something more
custom, I would say all the customization came from the actual pre-processing of the data.
So things like being able to detect chemical formulas and doing the tokenization properly,
things like that.
And being able to collect the data in the first place was a bit of an effort as well.
And so, because we had to spend a lot of time in that process, we didn't have a lot of
time to actually work with the algorithm itself.
We also spent a lot of time verifying whether the results were correct or not.
So that's where most of our time went.
So this was, I would say, a pretty straightforward application of word to veck.
We did test against glove.
We did do some hyperprimer optimization, things like that.
But definitely, I wouldn't say we made any conceptual leaps in NLP during the course of
this study.
Sure.
Yeah.
And that question was not at all to take away from what you did, but rather see if there
was any hidden element that it would be worth exploring further.
I think what I'm hearing is very consistent with what I hear speaking with lots of folks
is actually applying this stuff involves a ton of work setting up the data pipelines.
And in your case, you've been getting access to the data.
Yeah.
And in some sense, it's kind of interesting to know that even one of the simplest word
embedding algorithms is capable of giving good results on this problem.
And so it really kind of begs the question, well, what if we were to use more advanced
techniques on this problem?
Or if we were to expand the data set to the full text, what more might be possible?
I mean, if we're already seeing such good results using simple techniques, there could be
a lot of opportunity here if we were to actually spend even more time to do this a little
bit more carefully.
And so if you were to kind of think about the opportunities that are ahead of you kind
of in this direction, how do you rank them, what are the things you're most excited
about kind of building on and where do you think the true opportunities lie?
Yeah.
So actually, you know, a lot of the motivation for this study was not necessarily to make
predictive models, but just to really improve the process of information retrieval from
the scientific literature.
And I think there's really a lot of opportunity in going from this mechanism where scientists
are just browsing articles one by one for relevant information to really using algorithms
to getting the information that they need just in time from the research literature.
So to kind of use an example or analogy, you know, back in the early days of the web,
if you wanted to look up Planck's constant, you would have to find a bunch of science websites
browse through them and see whether one of them had the value that you wanted.
And today, you just type Planck's constant into Google and it just immediately returns
the answer.
You don't have to browse anything.
And so I think there's a lot of opportunity, just an information retrieval from these
very scientific abstracts.
In terms of the actual predictive models and hypothesis generation, we already touched
upon the ability to make predictions on materials that are not in the data set already.
So how can we actually invent word embeddings for hypothetical chemical compositions?
That's an area that we're looking into right now.
We're also interested, as we spoke about before, of these more context sensitive embeddings
and these more advanced methods that might help us distinguish between terms and materials
that aren't really the same, but are the same currently in our work-to-vec model.
So these are all areas that I think we will be touching upon in the future.
What haven't I asked you about this research that will be worth exploring?
Yeah, you know, I think one thing that I just want to mention is that somewhat ironically,
this paper is behind the paywall, but the publisher, so the publisher, Springer Nature,
they've actually made the full text available to read for free on research gate, so hopefully
we can put a link to that in the show notes so that people that want to read the paper
but don't necessarily have access to it can actually legally read the paper.
So I think that's one of the main things I just wanted to mention about this.
Okay, that's awesome, and we will definitely include that link in the show notes.
On above, thank you so much for, well, A, listening to the podcast and B, jumping on
to share a bit of what you're working on with all of us.
Thanks so much.
Yeah, it's been a pleasure.
Thanks.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms
Conference.
As always, thanks so much for listening and catch you next time.
