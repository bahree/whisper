1
00:00:00,000 --> 00:00:16,000
All right, everyone. I am here with Edward Wrath. Edward is a Chief Scientist at Boos

2
00:00:16,000 --> 00:00:22,160
Alan Hamilton. Edward, welcome to the Twimal AI podcast. Thank you so much for having me.

3
00:00:22,160 --> 00:00:28,320
Really excited to be here. Looking forward to jumping into our conversation. You lead a

4
00:00:28,320 --> 00:00:35,760
machine learning research group at BAH. What is a machine learning research group at BAH? Focus on.

5
00:00:36,640 --> 00:00:42,080
Yeah, so it's a little different for us compared to many other organizations that have research

6
00:00:42,080 --> 00:00:48,640
teams because we are consulting firms. So our business model is basically based on renting out

7
00:00:48,640 --> 00:00:55,680
people's brains. And you have some hard problem you want to solve. So we need to have smart people

8
00:00:55,680 --> 00:01:02,800
who can work on these hard problems. So we sort of view research both as a way to sort of let

9
00:01:02,800 --> 00:01:07,280
people know like, hey, we actually work on some really cool things that require this level of

10
00:01:07,280 --> 00:01:13,920
difficulty and thought and challenge. But also as a way to train staff like one of the things that

11
00:01:15,280 --> 00:01:22,000
we've been doing a lot of work around is adversarial machine learning where there's you have your

12
00:01:22,000 --> 00:01:27,120
model that you've developed and you put blood sweat and tears into it and it's your baby and

13
00:01:27,120 --> 00:01:34,240
you want it to work well and go into production. And there may be some nefarious actor out there

14
00:01:34,240 --> 00:01:42,880
who wants to subvert your model. So like a lot of my work is in malware analysis, malware detection

15
00:01:42,880 --> 00:01:47,360
where the malware author like actively wants to subvert the model. They don't want to be detected

16
00:01:47,360 --> 00:01:53,520
as malware. They want to be able to run uninhibited. So if there is someone who's going to try and

17
00:01:53,520 --> 00:01:59,600
sort of mess with your model and try to make it produce errors, how do you prevent that or how do

18
00:01:59,600 --> 00:02:07,280
you even accurately quantify what's happening. And there's no course that you can sign up for yet

19
00:02:07,280 --> 00:02:13,680
on adversarial machine learning and school that's not part of anyone's curriculum yet. So if we want

20
00:02:13,680 --> 00:02:18,640
people of that skill set, we sort of have to grow them organically and research is one of the ways

21
00:02:18,640 --> 00:02:24,320
to do that and to blow people up of these really deep technical skills that we need but are not

22
00:02:24,320 --> 00:02:31,280
sort of off the shelf yet. Yeah, yeah. I'm glad you brought up adversarial ML because I know

23
00:02:31,280 --> 00:02:36,880
that's an area of personal interest for you and that's one of the things that I wanted to dig

24
00:02:36,880 --> 00:02:44,240
into with you as a way to get that conversation going. It's actually been quite a while since I've

25
00:02:44,240 --> 00:02:52,400
had someone on the show focused on this intersection of AI and cyber security which has continued

26
00:02:52,400 --> 00:02:59,360
to be of interest particularly as we see more and more activity let's say in the cyber security

27
00:02:59,360 --> 00:03:10,320
realm. Can you give us an overview or what's your kind of temperature read your take on the space

28
00:03:10,320 --> 00:03:16,400
and where it is and how it's evolved over the past couple of years. Yeah, I mean it's interesting,

29
00:03:16,400 --> 00:03:22,880
it's if you go back to like the first like applications of machine learning for malware detection

30
00:03:22,880 --> 00:03:33,680
goes back to like 1985 people have been looking at this intersection for a long time and in many

31
00:03:33,680 --> 00:03:39,760
ways it's still nascent despite having been a problem and been something that like people were

32
00:03:39,760 --> 00:03:45,760
really looking at for a long time in part because it's so different from normal data like okay

33
00:03:45,760 --> 00:03:52,000
convolutional neural networks and deep learning have come in and eat and everyone's a lunch at

34
00:03:52,000 --> 00:03:58,400
image classification at natural language processing a signal analysis all these different problems

35
00:03:59,200 --> 00:04:06,400
but all these problems have some underlying similarity of like things near each other are

36
00:04:06,400 --> 00:04:11,280
related to each other the words that I'm saying right now like they have meaning based on their

37
00:04:11,280 --> 00:04:17,600
order and the things that I say tomorrow really have no relationship to what I'm saying today.

38
00:04:17,600 --> 00:04:23,760
There's this correlation in time pixels in an image if you look at this sort of pixel on my shirt

39
00:04:23,760 --> 00:04:28,240
and it's blue you look at the pixels around it they're probably going to be a very similar shade of

40
00:04:28,240 --> 00:04:36,960
blue there's this spatial correlation that's true for signals as well malware that does exist

41
00:04:36,960 --> 00:04:46,800
but it's so much more complex than that you've got this sort of arbitrary system designed by humans

42
00:04:46,800 --> 00:04:52,320
that is like instruction code and assembly and how that gets converted into like literal zeros and

43
00:04:52,320 --> 00:04:59,360
ones and how the compiler is optimizing the layout of the code and which functions get put together

44
00:04:59,360 --> 00:05:08,880
and not and inlining and just a huge amount of complexity that is very different from what people

45
00:05:08,880 --> 00:05:15,840
are working on it's a very different scale a single executable like if you go download a new browser

46
00:05:15,840 --> 00:05:21,760
that's like 30 megabytes would be a really small browser download but if you look at that as one

47
00:05:21,760 --> 00:05:27,440
data point that's a massive data point like that's huge like most people like data point you're

48
00:05:27,440 --> 00:05:36,480
working on is maybe like a kilobyte at like the most we're talking about 30 plus megabytes is

49
00:05:36,480 --> 00:05:43,840
like pretty like normal thing to occur anything if you're looking at your you've got some data set

50
00:05:43,840 --> 00:05:50,400
of software programs and you've trained malware detector on software programs your browser would

51
00:05:50,400 --> 00:05:55,920
be like a feature a 30 megabyte feature that you're trying to do inference or classification

52
00:05:55,920 --> 00:06:01,120
against is that what you're that what you're about that it's like a variable size feature and like

53
00:06:01,120 --> 00:06:05,360
maybe there's an image and maybe there's multiple images embedded inside of it maybe there's a

54
00:06:05,360 --> 00:06:10,480
word doc embedded inside of it there's like every other file format could be embedded inside of this

55
00:06:10,480 --> 00:06:18,560
one file format and it's describing like code this like thing that could do anything of arbitrary

56
00:06:18,560 --> 00:06:26,560
complexity like that's what turning completeness is and we have proofs on like how you can't know

57
00:06:26,560 --> 00:06:30,320
what any arbitrary code is going to do about running it well you don't want to run it because it

58
00:06:30,320 --> 00:06:38,160
might be malware so there's there's just sort of this huge explosion of complexity as you dig into

59
00:06:38,160 --> 00:06:46,720
all these details and machine learning in the broader communal sense has not really

60
00:06:47,440 --> 00:06:52,880
developed the tools to deal with data that's so weird and so different in all these unique ways

61
00:06:52,880 --> 00:06:59,120
so there's a lot of just sort of unsolved machine learning problems at this intersection

62
00:06:59,120 --> 00:07:05,680
and that also creates lots of avenues for attack at this intersection because the malware author

63
00:07:05,680 --> 00:07:09,440
they don't need to abide by their rules that's part of the whole point is they're trying to break

64
00:07:09,440 --> 00:07:16,160
the rules if there's a spec that says oh you don't set this flag and executable because it will

65
00:07:16,160 --> 00:07:20,240
behave poorly well if that helps the malware author they're going to do it they don't care

66
00:07:22,000 --> 00:07:30,640
yeah so it's it's it's a very rich and interesting area I think from both like a machine learning

67
00:07:30,640 --> 00:07:35,520
math side of like how do we get the math to work with these new kinds of complexities these new kinds

68
00:07:35,520 --> 00:07:43,840
of relationships but also just from like the low level really technical side of like I could use

69
00:07:43,840 --> 00:07:49,040
some undocumented instruction to try and make my malware work that like oh yeah this only runs

70
00:07:49,040 --> 00:07:53,680
on new CPUs that have this undocumented instruction because it causes some weird side effects

71
00:07:53,680 --> 00:08:02,160
wow and like I've seen things on like research that gets done on a like specter-based malware

72
00:08:02,160 --> 00:08:11,360
where like the malware only works in the like prefetching of the CPU trying to prefetch code and

73
00:08:11,360 --> 00:08:16,080
data so that if you run it on like the wrong type of CPU or like in a virtualized environment it

74
00:08:16,080 --> 00:08:21,280
won't prefetch the same way malware won't run so you sort of hide what's actually happening and

75
00:08:21,280 --> 00:08:25,840
if you just look at the code the code doesn't make it obvious what's going on because the malicious

76
00:08:25,840 --> 00:08:33,120
intent is hidden to the prefetching logic so just a huge richness of complexity that that makes

77
00:08:33,120 --> 00:08:42,400
it interesting and fun and malware is you know just one element or part of this broader kind of

78
00:08:42,400 --> 00:08:55,440
cyber security patchwork or you know set of problems problem domain is it representative do you

79
00:08:55,440 --> 00:09:03,600
think of the other aspects in that other aspects of cyber security kind of suffer from the same

80
00:09:03,600 --> 00:09:13,680
kinds of problems or are they all kind of unique in their own ways I'm definitely I'm a machine

81
00:09:13,680 --> 00:09:21,120
learning man first and learned about cyber as like an application area or malware specifically

82
00:09:21,120 --> 00:09:26,560
so I don't want to speak too authoritatively but I think a lot of these have their own sort of

83
00:09:26,560 --> 00:09:35,920
unique snowflake problems especially around data collection and building a data set that is

84
00:09:35,920 --> 00:09:42,080
again sort of like unique to this problem space in general where if you want to get your data labeled

85
00:09:44,400 --> 00:09:51,760
it's really easy to go like for images like you can a toddler can label images literally like

86
00:09:51,760 --> 00:09:56,800
you could you could set up that app and they put it on the iPad and they would do it for hours

87
00:09:57,920 --> 00:10:04,080
no special training required to be like this is a cat this is a dog like okay here's this

88
00:10:04,080 --> 00:10:08,560
this arbitrary executable tell me if it's malicious or not or like what kind of malware family it's

89
00:10:08,560 --> 00:10:16,480
from that's a huge amount of work or like if here's some keycap data here's some network traffic

90
00:10:16,480 --> 00:10:22,720
data like is anything weird going on on this network I would just stare at you I don't know maybe

91
00:10:24,160 --> 00:10:29,840
what do you what do you think the answer is like it's it's very technically deep and complex

92
00:10:30,720 --> 00:10:33,360
and each each avenue sort of has its own unique problems

93
00:10:36,400 --> 00:10:45,840
one of the ways that you've taken on this malware problem is a recent paper focus on

94
00:10:45,840 --> 00:10:53,200
adversarial transfer attacks can you talk a little bit about that paper and the problem that

95
00:10:53,200 --> 00:10:59,680
it's seeking to address yeah so this was a paper's adversarial transfer attacks with unknown

96
00:10:59,680 --> 00:11:07,120
data and class overlap that a couple of people on our team have worked on Luke Richards was the

97
00:11:07,120 --> 00:11:14,800
lead author on it some great work we also had one of our collaborators from Nvidia and UMBC this

98
00:11:14,800 --> 00:11:20,240
paper we were really looking at a core adversarial machine learning problem which is

99
00:11:21,280 --> 00:11:27,200
been motivated by a lot of work recently but you have some model that you want to defend

100
00:11:28,080 --> 00:11:33,520
and we'll call you the victim because someone's going to try and attack you and so there's some

101
00:11:33,520 --> 00:11:41,040
adversary that wants to attack the victim model and they know that the models there or they know

102
00:11:41,040 --> 00:11:49,360
you're going to do it but they don't have access to the model specifically so quote unquote black

103
00:11:49,360 --> 00:11:55,920
box attack black box it is black box and it's a specific type of black box attack that we

104
00:11:55,920 --> 00:12:00,000
sort of generally call transfer learning because what the adversary does is they build their own

105
00:12:00,000 --> 00:12:06,320
model that sort of does the same task and they attack their own model and the assumption is well

106
00:12:06,320 --> 00:12:11,280
I built a good model you're doing the same thing these attacks should probably work on your model

107
00:12:12,000 --> 00:12:17,040
and the research that's been done to date sort of says yeah that does work

108
00:12:19,360 --> 00:12:25,600
and we were thinking about this and thinking about it from like okay if we were doing this in

109
00:12:25,600 --> 00:12:37,200
really real life does this match reality and maybe maybe sometimes but oftentimes not because

110
00:12:37,200 --> 00:12:43,520
what a lot of this work has done has sort of said I know you're working on this problem I know

111
00:12:43,520 --> 00:12:49,920
you have a model to detect malware because you're the government and malware is coming at you or

112
00:12:49,920 --> 00:12:58,080
you're a large bank or everyone has malware detectors you're supposed to that's part of the game

113
00:13:00,000 --> 00:13:05,280
or you can you can imagine sort of like many different tasks you would expect like the

114
00:13:05,280 --> 00:13:11,120
government should be doing this like that would make sense and if you're an adversary you just

115
00:13:11,120 --> 00:13:16,000
want to like mess with people you're a bad actor and so you build your own model but you're not

116
00:13:16,000 --> 00:13:24,240
probably going to have the exact same data that the victim does and you might not know like you're

117
00:13:24,240 --> 00:13:29,200
know you know you you can guess what they're going to do but you don't know exactly how they've

118
00:13:29,200 --> 00:13:35,440
set it up like there could be reasonable design choices on like well what are my classes exactly

119
00:13:35,440 --> 00:13:43,120
and where is that line so if you're in that situation where you don't have access to their model

120
00:13:43,120 --> 00:13:48,400
you're probably not going to have a perfect match for their data or how they've designed the

121
00:13:48,400 --> 00:13:55,280
class structure and it sounds like a lot of the prior work assumed that the adversary was

122
00:13:55,280 --> 00:14:01,040
operating from the same playbook so to speak it was using the same data and or the same classes

123
00:14:01,760 --> 00:14:08,080
the exact same data and the exact same data and exact same classes guy which is like unrealistically

124
00:14:08,080 --> 00:14:16,160
optimistic which like if you can defend against like an omnipotent adversary good for you

125
00:14:16,160 --> 00:14:24,160
congrats but I want to know what's going to happen in real life so that's for the motivation

126
00:14:24,160 --> 00:14:30,320
and I'm not claiming ours is perfect to real life but I think it's closer so we started building

127
00:14:30,320 --> 00:14:38,080
some tests to sort of vary like from zero percent overlap in the data to like a hundred percent

128
00:14:38,080 --> 00:14:44,320
and how much how much is that data overlap really a factor in the transfer success rate

129
00:14:45,600 --> 00:14:51,440
and also overlapping the number of classes that are in common so you have like all the exact same

130
00:14:51,440 --> 00:14:56,880
classes as each other to you have like two classes in common so like a very small attack

131
00:14:56,880 --> 00:15:04,880
attack surface there we spoke a little earlier about the difficulty of kind of defining the data

132
00:15:04,880 --> 00:15:13,680
in this space as a feature an executable or is it something else in this particular case in

133
00:15:13,680 --> 00:15:20,400
the example that you used in this paper what was a piece of data feature and what did the labels

134
00:15:20,400 --> 00:15:26,720
look like and this we were focusing just on the the adversarial machine learning part so we picked

135
00:15:26,720 --> 00:15:32,320
easy data that everyone has access to so we just did image data like CIFAR 10 CIFAR 100

136
00:15:33,680 --> 00:15:40,560
mini image net and the the classes were the normal classes like cat dog car truck

137
00:15:41,040 --> 00:15:44,160
okay deer and frog I think are also classes in there

138
00:15:44,160 --> 00:15:51,840
I mean we're just focusing on the the adversarial part for for this one all the complexity of

139
00:15:51,840 --> 00:15:56,560
malware just like that's too much extra complexity right now right you're you're you're basically

140
00:15:58,320 --> 00:16:04,800
trying you're hey let's step back take a simple model and just test this fundamental assumption

141
00:16:04,800 --> 00:16:11,440
about transferability different data and classes yeah something that I often

142
00:16:11,440 --> 00:16:19,360
probably probably one of the best skills that I try to like pass on to like employees and students

143
00:16:19,360 --> 00:16:25,120
is like okay like you have this complex goal just figure out some way to cut this down into

144
00:16:25,120 --> 00:16:31,520
different chunks because you can't eat the whole sandwich it's too big forget about an elephant

145
00:16:32,160 --> 00:16:39,760
you can't even eat the sandwich it's too big you got to cut something off so we cut off this part

146
00:16:39,760 --> 00:16:45,440
so we're using just image data's convolutional neural networks that everyone sort of like feels

147
00:16:45,440 --> 00:16:51,440
comfortable with you're going to understand what the results mean and when we set this up

148
00:16:55,040 --> 00:17:04,720
we we saw that both less class overlap and less data overlap both would hurt the attackers

149
00:17:04,720 --> 00:17:12,560
success rate which makes sense but we also saw some odd behaviors that it wasn't sort of as

150
00:17:12,560 --> 00:17:17,520
consistent as you would expect it wasn't sort of like the smooth degradation down like it would

151
00:17:17,520 --> 00:17:27,200
get worse and it would like start to get better again but randomly and as we start like some kind

152
00:17:27,200 --> 00:17:32,560
of generalization property kicking into effect or something that's that's part of what I think

153
00:17:32,560 --> 00:17:39,920
it is because what really became very interesting is from just one of the base results of that

154
00:17:39,920 --> 00:17:44,960
behavior we think a lot of it is like it once you sort of reach a minimum sort of threshold of

155
00:17:44,960 --> 00:17:50,880
like lack of similarity there's a lot more randomness that comes into play that the model just

156
00:17:50,880 --> 00:17:58,800
is not trained to like expect in any possible way and now it's making errors maybe not because

157
00:17:58,800 --> 00:18:04,560
the adversary did such a good job crafting the example but the data is just so different from

158
00:18:04,560 --> 00:18:12,080
what the model itself understands because the amount of overlap has decreased so much and

159
00:18:13,280 --> 00:18:18,240
the the the really sort of scary part is when it comes to like okay you want to defend your model

160
00:18:18,240 --> 00:18:25,680
what defenses work currently the the overall best defense is adversarial training

161
00:18:25,680 --> 00:18:34,880
it's been the best defense basically since it was introduced in like 2017-2018 and it's pretty

162
00:18:34,880 --> 00:18:41,120
simple strategy of like okay you have your model you train it and now as you're training attack

163
00:18:41,120 --> 00:18:46,960
your own model then feed the attacked inputs back into the training and just sort of be doing that

164
00:18:46,960 --> 00:18:56,800
continuously so you're constantly training the model to be better at correctly classifying attacked

165
00:18:56,800 --> 00:19:05,280
data points and our result showed if you do this in this more realistic scenario adversarial training

166
00:19:05,280 --> 00:19:14,480
actually weakens the defender wait what yeah the attack success rate increased on models that

167
00:19:14,480 --> 00:19:22,000
had been adversarially trained I think what's happening there what we what we sort of believe is

168
00:19:22,000 --> 00:19:30,720
going on is when you're doing adversarial training you're in a way overfitting to a very specific

169
00:19:30,720 --> 00:19:39,200
adversary an adversary that has the exact same classes and data you do but when the adversary

170
00:19:39,200 --> 00:19:47,040
has different classes their attacks are going to naturally go in different directions that your

171
00:19:47,040 --> 00:19:55,520
model has just never optimized for because it sort of started from an initial condition and it

172
00:19:55,520 --> 00:20:03,040
got better that it's sort of just converging on whatever initial path worked best and so it's

173
00:20:03,040 --> 00:20:12,320
sort of overlearned to the sort of unrealistic scenario and these new attacks are tightly coupled

174
00:20:12,320 --> 00:20:17,120
like the the data is the same because it's it's baked into the process of creating the model

175
00:20:17,840 --> 00:20:26,400
exactly and so the attacks seem to transfer more successfully if the victim has done adversarial

176
00:20:26,400 --> 00:20:32,880
training in this sort of imperfect knowledge scenario which if you're actually trying to build a

177
00:20:32,880 --> 00:20:38,640
robust model for real life production like that's actually a huge concern now because you say okay

178
00:20:38,640 --> 00:20:44,560
I'm going to keep my model private I'm going to try and sort of mitigate as much information that

179
00:20:44,560 --> 00:20:53,680
the adversary could acquire by sort of keeping this close hold and you want to do all the best things

180
00:20:53,680 --> 00:20:57,360
okay I'm going to do adversarial training like oh actually if you believe that's the correct

181
00:20:57,360 --> 00:21:02,400
threat model you might not want to do that it might actually make you more susceptible

182
00:21:02,400 --> 00:21:13,360
rather than less interesting can we maybe take a second to kind of punch into the degree to which

183
00:21:13,360 --> 00:21:19,520
adversarial training and a lot of these you know techniques around adversarial ML generally

184
00:21:19,520 --> 00:21:26,400
the way the degree to which those are you know practical concerns and implemented by people

185
00:21:26,400 --> 00:21:33,680
that are building actual models versus academic thought exercises that people really aren't

186
00:21:33,680 --> 00:21:37,360
thinking about when they're putting models in a production at this point but what's your read

187
00:21:37,360 --> 00:21:47,200
on that from where you sit I think on average it's more academic for most real most most real world

188
00:21:47,200 --> 00:21:52,240
like usage of it is more academic like people don't necessarily a reason to really believe

189
00:21:52,240 --> 00:21:57,040
they're under attack so a lot of people motivate this with like self-driving cars that

190
00:21:58,240 --> 00:22:02,480
someone's going to trick the self-driving car into plowing through a stop sign

191
00:22:03,760 --> 00:22:10,720
which like I don't want that to be a thing that can happen to my future self-driving car

192
00:22:10,720 --> 00:22:19,920
match true I'm sure maybe some horrible person out there like tries to screw of them but like in

193
00:22:19,920 --> 00:22:23,440
general it's not like a very well-motivated threat like who's going around just trying to

194
00:22:23,440 --> 00:22:33,840
destroy every self-driving car it's like it's important I think it's more important in like

195
00:22:33,840 --> 00:22:38,640
the academic sense of like trying to figure out how to make models robust to just like errors

196
00:22:38,640 --> 00:22:49,600
in general then it is there's an actual adversary trying to trick you and a lot of times a lot of

197
00:22:49,600 --> 00:22:58,000
the research that gets done becomes sort of somewhat cartoonish and like the amount of information

198
00:22:58,000 --> 00:23:02,560
that they give the adversary makes it like why would they do it this way if they were that

199
00:23:02,560 --> 00:23:10,400
powerful like I remember seeing some work on like all adversarial attacks on medical imaging

200
00:23:10,400 --> 00:23:16,400
and they'll they'll adjust the medical image and the to make the AI models would give you

201
00:23:16,400 --> 00:23:20,400
their long prescriptions and the wrong drugs and the wrong medication to like hurt your health

202
00:23:20,400 --> 00:23:25,680
and if they have like that much control to like you get all this data and like access model

203
00:23:25,680 --> 00:23:34,560
it just flip a bit in the database like it seems I'm like if they're that powerful there's

204
00:23:34,560 --> 00:23:41,280
so much easier things they could have done right so sort of the goal of like how we came up with

205
00:23:41,280 --> 00:23:46,240
this work to begin with was like well in a realistic scenario what's it actually going to look like

206
00:23:46,240 --> 00:23:53,840
and for malware it is a really like true true to life realistic problem that people do deal with

207
00:23:53,840 --> 00:24:00,960
like day-to-day which is part of why I like doing work in this space is it's it's not academic

208
00:24:00,960 --> 00:24:10,560
it is like this is really happening meaning not the the broad existence of motivated adversaries

209
00:24:10,560 --> 00:24:20,320
but the specific application of adversarial attacks to in kind of the malware world

210
00:24:20,320 --> 00:24:26,960
I mean a malware you know there are existing malware detectors that are based on machine learning

211
00:24:26,960 --> 00:24:33,200
models and there are people out there that are trying to deploy adversarial attacks against those

212
00:24:33,200 --> 00:24:39,280
models and there are people in that world that are building adversarial robustness into those

213
00:24:39,280 --> 00:24:47,840
models like that's all real and extent today yeah that that that is all real to varying degrees

214
00:24:47,840 --> 00:24:52,720
today it's more complex in the malware space because an adversarial attack in the malware space

215
00:24:54,400 --> 00:25:02,400
can happen earlier in the process so because again like these they're so executables are so complex

216
00:25:02,400 --> 00:25:11,440
you can sort of mess with the executable itself to maybe screw up the way that the antivirus

217
00:25:11,440 --> 00:25:18,640
process is the features to begin with so you can change like the whole floor out from under the

218
00:25:18,640 --> 00:25:28,720
the model that you're trying to fool uh and meaning like like obfuscation of the the malware code

219
00:25:28,720 --> 00:25:34,320
within the broader executable or something different that that's one of many many possible ways so

220
00:25:34,320 --> 00:25:39,520
you could uh there's a thing called packing which is like let me put my executable inside of

221
00:25:39,520 --> 00:25:44,960
another executable so you're sort of you have to figure out how to peel this onion to figure out

222
00:25:44,960 --> 00:25:53,360
what's going on underneath or like a dynamic analysis which is when you do try to run the malware

223
00:25:53,360 --> 00:26:00,880
or to get features a lot of malware will just like initial like step of the malware is just like

224
00:26:00,880 --> 00:26:06,240
wait 24 hours because you're probably not going to run this dynamic analysis for 24 hours you're

225
00:26:06,240 --> 00:26:10,400
going to run it for like three minutes tops so they're just going to try and like outweigh you

226
00:26:12,160 --> 00:26:18,000
and so like you might have features that like oh does it call this api does it call the crypto

227
00:26:18,000 --> 00:26:23,120
functions that's that might be a good sign of ransomware does it call the file delete functions like

228
00:26:23,120 --> 00:26:27,760
okay that's a really good sign of ransomware it has both of those oh they just wait until the clock

229
00:26:27,760 --> 00:26:35,440
runs out then you never see the features to begin with um so there's more complex and interesting

230
00:26:35,440 --> 00:26:43,920
ways that the malware adversary can sort of mess with the model uh and uh I I can go on like lots

231
00:26:43,920 --> 00:26:50,400
of fun tangents on my favorite one that I've seen which is not I've never seen it actually used

232
00:26:50,400 --> 00:26:57,680
by malware but it's more it's just fun that like this is possible uh there is this project that

233
00:26:57,680 --> 00:27:06,880
people released called the mob fiscator uh fiscator mob like MOV until you've instructed yeah because

234
00:27:06,880 --> 00:27:12,160
it's an instruction for people who are to wear it's like this is the assembly code that moves

235
00:27:12,160 --> 00:27:18,960
data from one location in memory to another location in memory okay and so you might say like

236
00:27:18,960 --> 00:27:24,000
please move like please move this memory from like please move this data from like memory into this

237
00:27:24,000 --> 00:27:29,360
register because I'm going to do some work on it or from register like back to disk to save it or

238
00:27:30,000 --> 00:27:38,800
whatever uh this one instruction has so many side effects that is actually turn turn complete

239
00:27:38,800 --> 00:27:45,360
so you can compile any program into one that contains only the move instruction

240
00:27:46,560 --> 00:27:51,120
ostensibly it looks like this program only moves data and never actually does anything with it

241
00:27:51,120 --> 00:27:56,800
mm-hmm but it has so many weird side effects that you can actually and it works you can recompile

242
00:27:56,800 --> 00:28:03,280
any program to contain only one single instruction wow because the program itself is just

243
00:28:04,000 --> 00:28:09,520
putting bits into memory locations and executing them yep right and there's there's

244
00:28:09,520 --> 00:28:15,040
enough special like side effect cases on there's one instruction that that's the only instruction

245
00:28:15,040 --> 00:28:25,120
you technically need to be able to do anything on an x86 computer wow you see weird stuff like this

246
00:28:25,120 --> 00:28:32,000
with malware so often it's just like yeah at anything you can think of like I'm sure there's some

247
00:28:32,000 --> 00:28:36,400
way someone could get around this and then it becomes a numbers game I'm like how many more

248
00:28:36,400 --> 00:28:44,240
machines am I protecting how many more cases am I covering and am I sort of making any new like

249
00:28:44,240 --> 00:28:48,960
gaping holes that I need to address because you're not going to get it all um but you got to

250
00:28:48,960 --> 00:28:52,400
that doesn't mean you sit around and go like well I can't solve it perfectly so I might as well

251
00:28:52,400 --> 00:28:58,480
not do it no you build the best solution you can you sort of get it out there you try and fix

252
00:28:58,480 --> 00:29:06,400
what you can and see how do they adapt what what happens next and sort of this continual back and

253
00:29:06,400 --> 00:29:16,480
forth of you taking a step in the adversary taking a step so malware you know is a place where

254
00:29:16,480 --> 00:29:25,120
these kinds of things are happening today and we kind of got to that from talking about the

255
00:29:27,680 --> 00:29:34,160
adversarial or adversarial training adversarial robustness kind of incorporating into

256
00:29:34,160 --> 00:29:40,960
training and that having an adverse effect on robustness um is there a solution to that

257
00:29:42,240 --> 00:29:47,440
working on it I'm not out of a job yet though thank you

258
00:29:50,800 --> 00:30:00,720
it's it's sort of a double edge sword if we can I mean if like the the gut reaction of everyone is

259
00:30:00,720 --> 00:30:05,280
like these adversarial attacks are definitely a bad thing we don't want them to exist we've got to

260
00:30:05,280 --> 00:30:13,680
get rid of them uh which yes it is definitely a bad thing uh but it's in the context of how things

261
00:30:13,680 --> 00:30:21,600
are being used if someone's using a machine learning model in a bad way then being able to subvert

262
00:30:21,600 --> 00:30:28,400
that model now actually becomes a defense a good thing so if someone is using the machine learning

263
00:30:28,400 --> 00:30:34,160
model to sort of like create like a surveillance like state or something uh like okay that we're

264
00:30:34,160 --> 00:30:39,680
not comfortable with that we don't like that um there's actually there was a recent paper I had

265
00:30:39,680 --> 00:30:49,760
this idea years ago but didn't do it um I get no credit for just having a nice idea but uh this

266
00:30:49,760 --> 00:30:56,960
is a really interesting paper that got published uh or put online at least on uh using like adversarial

267
00:30:56,960 --> 00:31:03,040
attacks to figure out how to put on makeup in such a way that you look normal but the machine learning

268
00:31:03,040 --> 00:31:09,040
model will just be like yeah there's no one there so sort of giving you a way to sort of get some

269
00:31:09,040 --> 00:31:16,800
of your privacy back you don't want to be tracked or looked at so these are things that um they're

270
00:31:16,800 --> 00:31:20,960
complex from many perspectives they're not just from a technical perspective but from like an

271
00:31:20,960 --> 00:31:24,560
ethics perspective I'm like what is and is not an okay use of machine learning

272
00:31:24,560 --> 00:31:28,880
how do we want to deploy these things these are the things that we often have to work about and

273
00:31:28,880 --> 00:31:36,400
think about and be conscious of uh but it also does mean that adversarial attacks and defenses

274
00:31:37,120 --> 00:31:43,760
are like whichever way it goes we we ultimately can or cannot defend against these attacks

275
00:31:44,320 --> 00:31:49,200
also means we ultimately can or cannot subvert people using machine learning for

276
00:31:49,200 --> 00:31:58,960
or inappropriate unethical use cases uh yeah so that's something that I uh think about a lot

277
00:31:59,760 --> 00:32:05,840
the main idea is that hey we don't uh you know the research that's happening here isn't really

278
00:32:05,840 --> 00:32:11,200
considering our real world scenario your paper considered a real world scenario and you found that

279
00:32:11,200 --> 00:32:26,480
um in general the less overlap um the more difficult an attack is but in a kind of weird way where

280
00:32:26,480 --> 00:32:35,360
a weird non-linear way and in addition we could from from the attackers perspective we could

281
00:32:35,360 --> 00:32:44,800
restore some more predictable behavior to the attacker uh so if we sort of simulated uh

282
00:32:46,240 --> 00:32:52,240
attacking a model with unknown classes or if sort of imperfect classes by sort of like

283
00:32:52,240 --> 00:32:57,280
randomly masking them out and we generate the attacks so we're like each time we attack it we're

284
00:32:57,280 --> 00:33:03,680
going to randomly pretend some of these classes don't exist and don't count um so maybe you're trying

285
00:33:03,680 --> 00:33:13,200
to uh maybe the model's first gut instinct is to convert the uh model from predicting truck

286
00:33:13,200 --> 00:33:19,040
to car you say well we've masked out car that's not an option so you're not getting credit for that

287
00:33:19,040 --> 00:33:27,920
you have to do something else uh if we do that we restore a lot of behavior that makes more sense

288
00:33:27,920 --> 00:33:38,400
so it it sort of removes the variance of uh class overlap from the attacker's accessory uh so

289
00:33:38,400 --> 00:33:46,400
rewind this for me this is when you're training your target model you are doing what

290
00:33:47,200 --> 00:33:51,760
so when we're when we're generating the attacks uh generating the attacks okay

291
00:33:51,760 --> 00:33:58,400
okay to sort of transfer to the victim uh so the adversary has their own what we call a surrogate

292
00:33:58,400 --> 00:34:02,320
model they built their own model that they think hopefully matches what you've done

293
00:34:03,120 --> 00:34:08,560
and they're basically going to perturb their own surrogate model in order to simulate that

294
00:34:08,560 --> 00:34:17,280
they don't know exactly what you're doing and that uh the attacker pays a penalty in terms of

295
00:34:17,280 --> 00:34:22,320
attack success rate in order to do that but what they gain is sort of certainty about their

296
00:34:22,320 --> 00:34:32,160
attacks success rate if that makes sense so in the normal situation uh maybe the average

297
00:34:32,160 --> 00:34:38,480
success rate of their attack would have been 40% but they're not sure that it's actually 40%

298
00:34:38,480 --> 00:34:44,400
they're well maybe it's between like 15 and 60 and I can't really tell what exactly it is

299
00:34:44,400 --> 00:34:54,240
but then if you do this modified attack they're pretty sure that it's somewhere between like 30

300
00:34:54,240 --> 00:35:01,280
and 35 so it's lower but they they actually know what they're going to get so kind of greater bias

301
00:35:01,280 --> 00:35:15,600
less variance yeah exactly and on the the adversarial training um the adversarial robustness

302
00:35:15,600 --> 00:35:25,120
training um are you or are you aware of folks that are like how would you go about trying to

303
00:35:25,120 --> 00:35:35,760
um like reformulate that problem in light of kind of the real world thing like you know I'm thinking

304
00:35:35,760 --> 00:35:41,440
of hey is there an analogy to dropout that's like class dropout where you kind of forget about

305
00:35:41,440 --> 00:35:47,360
classes in the adversarial loop and like does that help like are folks working on that specific

306
00:35:47,360 --> 00:35:53,440
problem we're not not the class dropout thing that the broader problem we actually tried the

307
00:35:53,440 --> 00:36:03,120
class dropout thing and unfortunately did you we tried that one I didn't work we were very sad about

308
00:36:03,120 --> 00:36:12,160
that but uh no I I don't know what the answer is yet uh something that I want to look at and

309
00:36:12,160 --> 00:36:17,760
part of the problem with these experiments where they were hugely expensive to run because

310
00:36:17,760 --> 00:36:24,880
instead of sort of attacking one model for one data set we have to attack like 50 models

311
00:36:26,000 --> 00:36:31,760
for one data set because we have to vary the class overlap and the data overlap every single time

312
00:36:32,640 --> 00:36:36,800
and then we want to run it multiple times for each combination because

313
00:36:37,840 --> 00:36:42,720
you can pick different classes each time that might bias the results so it turns into this

314
00:36:42,720 --> 00:36:50,640
computational explosion of model training and attacking so something that we want I want to look at

315
00:36:50,640 --> 00:36:59,040
but I think we might need to figure out something more intelligent first is these approaches that

316
00:36:59,040 --> 00:37:06,640
try to sort of build provably robust models from the onset where you can sort of think of it like

317
00:37:06,640 --> 00:37:10,720
your model makes a prediction and it's sort of making a prediction about like a single data point

318
00:37:10,720 --> 00:37:16,640
like one point comes in and you get one answer and what they try to do is they try to build models

319
00:37:17,840 --> 00:37:23,680
this is one approach anyway there's multiple but they try to basically classify instead of one

320
00:37:23,680 --> 00:37:28,640
data point like a region so you sort of have like the data point is the center and there's like a

321
00:37:28,640 --> 00:37:36,160
sphere around it and you're saying everything in this sphere gets the same answer and if you can

322
00:37:36,160 --> 00:37:42,000
do that then you've sort of you're provably robust to attacking that data point for a certain sized

323
00:37:42,720 --> 00:37:49,760
radius and then as you train you try to sort of increase the size of that sphere that you can do so

324
00:37:49,760 --> 00:37:55,040
that spheres start out sort of infinitely small well you just the data point and you try and push

325
00:37:55,040 --> 00:38:03,200
them wider and wider as you go and that means you're becoming more robust theoretically that would

326
00:38:03,200 --> 00:38:10,400
work better but I'm not sure in theory there's no difference between theory and practice but in practice

327
00:38:10,400 --> 00:38:20,720
there is it sounds like that has strong implications on the the type of model you're using like

328
00:38:23,600 --> 00:38:29,920
you know maybe even as far as not a neural network model or can you incorporate that kind of

329
00:38:29,920 --> 00:38:36,720
technique into a neural network formulation you you can for a neural network it's it's really

330
00:38:36,720 --> 00:38:42,560
expensive so it's sort of like if I wanted to do that for these experiments I'm going to increase

331
00:38:42,560 --> 00:38:49,200
the compute time by like at least another factor of 10 or it took us like four or five months to run

332
00:38:49,200 --> 00:39:04,400
all these experiments I was like all right 40 50 months okay maybe I think in terms of

333
00:39:06,240 --> 00:39:11,600
real world like when you're really thinking through do I need to be concerned about an adversarial

334
00:39:11,600 --> 00:39:18,240
attack am I an entity that is likely to be attacked if I'm a government if I'm a bank if I'm a

335
00:39:18,240 --> 00:39:25,520
large enough corporation then yeah you you are likely to be targeted and attacked then okay what

336
00:39:25,520 --> 00:39:31,440
what are the models that I train that are sort of the most likely to be attacked or that people

337
00:39:31,440 --> 00:39:35,920
are going to try and subvert so like if you're a credit card company like you have fraud detection

338
00:39:35,920 --> 00:39:44,240
models yeah that people are trying to subvert all the time so okay I have this model like I have

339
00:39:44,240 --> 00:39:50,480
these models that I have then identify these are the models at risk and now let's really think

340
00:39:50,480 --> 00:39:56,800
through like the specifics of the threat model for this particular thing not this sort of academic

341
00:39:56,800 --> 00:40:04,080
abstract I can apply this to any problem kind of thing but what do we know about the domain

342
00:40:04,080 --> 00:40:10,800
that we can use to build a robust model for this specific problem that's what I've had the most

343
00:40:10,800 --> 00:40:15,680
success doing anyway that's the way that I generally try to help like our clients approach these

344
00:40:15,680 --> 00:40:22,560
kind of problems is to really focus in on that scope and narrow down like where do we actually

345
00:40:22,560 --> 00:40:26,560
need to do this let's not panic and think that everything's under attack all the time because

346
00:40:26,560 --> 00:40:33,760
that's not realistic and you're going to give yourself a heart attack and we've done that we've

347
00:40:33,760 --> 00:40:38,160
done some of that for malware for specific kinds of malware models and we've done some of that

348
00:40:38,160 --> 00:40:45,360
for a computer vision before and gotten a lot of success much more quickly with that approach

349
00:40:45,360 --> 00:40:49,600
and as you say like part of that can be like well maybe you shouldn't use a neural network maybe

350
00:40:49,600 --> 00:40:54,640
you should be using a linear model and sort of carefully crafting your your features to make a

351
00:40:54,640 --> 00:41:01,840
linear model work well or maybe a shallow decision tree or a random forest or something would be

352
00:41:01,840 --> 00:41:11,520
better because you can sort of understand to what degree can this really be attacked and what's

353
00:41:11,520 --> 00:41:18,960
that envelope and and design around it so like part of how we did that for some of our malware work

354
00:41:18,960 --> 00:41:25,840
was if we if we go back to like the example of dynamic analysis where if you're actually going to

355
00:41:25,840 --> 00:41:32,000
see the thing run let's just assume it's not perfect I don't claim perfection

356
00:41:34,480 --> 00:41:38,000
but if you say okay I'm actually going to see the thing run I'm going to see it

357
00:41:39,680 --> 00:41:45,040
delete the freed in the files encrypt them write them out delete the original files

358
00:41:45,840 --> 00:41:52,800
I know this is malware and for it to be ransomware it has to like actually have those steps

359
00:41:52,800 --> 00:42:05,120
if you build a model that can only sort of normal models look for things that both indicate like one

360
00:42:05,120 --> 00:42:09,200
or the other so I look for things that tell me it's ransomware I look for things that tell me it's

361
00:42:09,200 --> 00:42:17,760
not ransomware that's a design flaw in this scenario I shouldn't be looking for things that tell me

362
00:42:17,760 --> 00:42:23,920
it's not ransomware because if it does those sort of finite things that qualify it's ransomware

363
00:42:25,680 --> 00:42:31,200
and what malware will do is what what they can do is just insert lots of random other benign

364
00:42:31,200 --> 00:42:36,240
activity like oh look at all these benign things I do that way that outweighs all these malicious

365
00:42:36,240 --> 00:42:41,600
things I did and a normal machine learning model go yeah that's right you did do more benign

366
00:42:41,600 --> 00:42:50,160
things than malicious things you are benign that doesn't make sense at all if you do anything

367
00:42:50,160 --> 00:42:58,560
malicious it's malicious and by default everything is benign it's benign until you do something bad

368
00:42:58,560 --> 00:43:04,400
so we can incorporate that into the model that there are no features that contribute to a score of benign

369
00:43:05,120 --> 00:43:09,920
that that's just sort of the default and then you have to do enough sort of malicious indicators

370
00:43:09,920 --> 00:43:14,800
for the model to change its decision to be like no no no you're actually malicious you don't get

371
00:43:14,800 --> 00:43:21,840
to run anymore and is the the implication then that the models that tend to be used in the space are

372
00:43:22,640 --> 00:43:29,280
kind of very heavily hierarchical or ensemble or something like that where you're

373
00:43:30,160 --> 00:43:35,280
identifying you've got modules that are identifying specific features or characteristics and then

374
00:43:35,280 --> 00:43:42,480
kind of bubbling that up like Microsoft has published some of their like strategy for how they

375
00:43:43,040 --> 00:43:47,120
handle Windows Defender and trying to protect computers and they they've published they have

376
00:43:47,120 --> 00:43:52,640
this whole hierarchical strategy of like okay here's the super fast model and sort of

377
00:43:53,840 --> 00:43:58,240
they can handle a lot of things and for the things that can't like bumping that up a stage

378
00:43:58,240 --> 00:44:03,040
to something more complex it's going to do something more sophisticated and I forget how many

379
00:44:03,040 --> 00:44:07,680
stages it had in it but like the end stage was sort of like okay we're actually going to like run

380
00:44:07,680 --> 00:44:11,680
this and like a dynamic environment in the cloud to try and figure out what this is doing

381
00:44:13,600 --> 00:44:18,480
so you yeah people definitely do that of this sort of building this hierarchy of

382
00:44:20,720 --> 00:44:26,080
speed and complexity trade-offs because you can't afford to run everything through dynamic

383
00:44:26,080 --> 00:44:33,920
analysis all the time like that would just be running all computers but again like there's

384
00:44:33,920 --> 00:44:39,760
not a lot of computers be running all the programs that we want to run to make sure the computers

385
00:44:39,760 --> 00:44:45,280
can run them it doesn't make sense right right right yeah when you were talking about dynamic

386
00:44:45,280 --> 00:44:54,640
analysis I imagined it to be uh well maybe not dynamic I imagined it to be like this design time

387
00:44:54,640 --> 00:45:00,320
tool like hey we collect all these samples of things in the wild and like we run them through this

388
00:45:00,320 --> 00:45:05,680
dynamic analysis thing and like maybe we're creating labels or something but it sounds like

389
00:45:05,680 --> 00:45:10,400
from this Microsoft example it's like no you try to run this app and it says hey hold on I'm

390
00:45:10,400 --> 00:45:15,120
going to you know put this in the malware detector in the cloud and I'll be back to you shortly

391
00:45:15,760 --> 00:45:22,640
yep I remember one of my colleagues who's uh he's a malware analyst uh a lot of experience

392
00:45:22,640 --> 00:45:26,800
you're telling me about how like for the for the analysts who are just doing their job like as part

393
00:45:26,800 --> 00:45:32,560
of a manual process to figure out what things are they would have a big sort of a virtual machine

394
00:45:32,560 --> 00:45:40,720
cluster to run things and try and observe them and see what they're doing and uh sometimes there

395
00:45:40,720 --> 00:45:46,480
there is malware that has what's called a VM escape where there's a bug in the VM and it is

396
00:45:46,480 --> 00:45:53,120
possible for the the code to recognize it's being run inside a virtual machine and leave the

397
00:45:53,120 --> 00:45:58,240
virtual machine and infects the host that it's on and occasionally that would happen and they

398
00:45:58,240 --> 00:46:03,920
would just be like all right burn everything down like yeah this is you're complex now just

399
00:46:04,960 --> 00:46:09,680
light everything on fire we build again from scratch this is the only way to make sure it's clean

400
00:46:10,400 --> 00:46:16,320
right right uh that that might be a slightly hyperbolic description but it's the

401
00:46:16,320 --> 00:46:22,880
the complexity is just so great it's never ending yeah awesome awesome um

402
00:46:24,560 --> 00:46:30,240
what what kind of future directions you know as if we haven't already talked about enough of them

403
00:46:30,240 --> 00:46:39,360
are you excited about in this video uh I'm excited about uh graph neural networks

404
00:46:39,360 --> 00:46:47,840
they've been getting more traction the past two years uh that I think is good uh some of it's

405
00:46:47,840 --> 00:46:55,200
been a little contentious on like relating to a theme of uh reproducibility that's also been

406
00:46:55,200 --> 00:47:01,600
picking up on like okay graph neural networks were very nascent a few years ago and then there was

407
00:47:01,600 --> 00:47:06,640
sort of like a small like kickoff of people iterating and publishing oh here's my new fancy this

408
00:47:06,640 --> 00:47:11,120
and then a lot of people were saying like oh actually all this is wrong and really you just

409
00:47:11,120 --> 00:47:18,000
didn't tune things correctly but so there there's still a lot of sort of working things out there

410
00:47:18,000 --> 00:47:24,080
I think uh and are because like the complexity of dealing with graphs like it's now much more arbitrary

411
00:47:24,080 --> 00:47:28,480
connectivity and how much this depends on the specific kinds of graphs you were looking

412
00:47:28,480 --> 00:47:36,480
what looking at versus someone else's graphs uh but I think a graph structure really is going to be

413
00:47:36,480 --> 00:47:43,440
the most at least from like a machine learning modeling perspective the most ideal way to model

414
00:47:43,440 --> 00:47:48,480
a lot of malware you can sort of create this graph of like which code parts connect to which other

415
00:47:48,480 --> 00:47:53,840
code parts you can have features on the nodes of the graph but also features on the edges of the

416
00:47:53,840 --> 00:48:01,600
graph so you can also have uh if there's a chunk of the executable that you couldn't disassemble you

417
00:48:01,600 --> 00:48:06,800
couldn't figure out what's there you could still have that as part of the model with some features

418
00:48:06,800 --> 00:48:12,240
sort of indicating that like this is connected here somehow but we're not sure what's actually

419
00:48:12,240 --> 00:48:18,960
happening at the node um like bailed to parse correctly uh so I think that's something that

420
00:48:18,960 --> 00:48:28,240
is probably the right direction to be headed in but is not yet fast enough and scalable enough

421
00:48:29,120 --> 00:48:34,720
for the malware sort of use case where like our data points are like the size of other people's

422
00:48:34,720 --> 00:48:42,880
datasets um like I wanted one of the datasets that uh I work with regularly the largest file in

423
00:48:42,880 --> 00:48:49,600
the dataset has like 200 megabytes which is like all as if our 100 is 200 megabytes like literally

424
00:48:49,600 --> 00:48:55,520
there's one data point is the size of this dataset everyone's using uh it's interesting that you

425
00:48:56,240 --> 00:49:03,840
provided that context about graph uh graphical models graph uh neural nets has applied to this

426
00:49:03,840 --> 00:49:12,640
space I'm pretty sure one of my first interviews around cyber was on graph stuff and I did not

427
00:49:12,640 --> 00:49:18,800
realize that there was a bit of a graph neural networks and security winter that happened

428
00:49:20,800 --> 00:49:27,200
I'm not sure it's so much as a winter is uh the the graph neural networks for malware just like

429
00:49:27,200 --> 00:49:31,840
hasn't germinated yet in the first place okay so the graph neural networks in the machine

430
00:49:31,840 --> 00:49:38,320
or in the machine learning land people have done graph based things for malware a lot before but

431
00:49:38,320 --> 00:49:46,320
just like normal graphs not neural network based uh got it uh so I I can see like the room for

432
00:49:46,320 --> 00:49:56,000
rules to collide but they haven't collided yet got it very cool very cool well Edward it was

433
00:49:56,000 --> 00:50:01,600
wonderful chatting and learning a bit about what you're up to in this space thanks so much for

434
00:50:01,600 --> 00:50:07,040
joining us yeah thank you again for having me it was a really fun conversation and I really enjoyed

