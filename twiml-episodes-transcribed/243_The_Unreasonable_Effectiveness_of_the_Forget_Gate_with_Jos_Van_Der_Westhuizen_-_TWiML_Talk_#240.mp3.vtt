WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.800
I'm your host Sam Charrington.

00:31.800 --> 00:38.080
Today we're joined by Yas fund of S. Tyson, PhD student in engineering at Cambridge University.

00:38.080 --> 00:44.400
Yas' research focuses on applying LSTMs or long short-term memory neural networks to biological

00:44.400 --> 00:46.720
data for various tasks.

00:46.720 --> 00:52.080
In our conversation we discuss his paper, The Unreasonable Effectiveness of the Forgetgate,

00:52.080 --> 00:57.320
in which he explores the various gates that make up an LSTM module and the general impact

00:57.320 --> 01:01.960
of getting rid of gates on the computational intensity of training the networks.

01:01.960 --> 01:06.960
Yas eventually determines that leaving only the Forgetgate results in a quote unquote

01:06.960 --> 01:11.640
unreasonably effective network and we discuss why.

01:11.640 --> 01:17.800
He also gives us some great LSTM-related resources, including references to you against Schmidhuber,

01:17.800 --> 01:27.040
whose research group invented the LSTM and who I spoke to back in Twimble Talk No. 44.

01:27.040 --> 01:31.280
Before we get to the episode, I'd like to join Pegasystems, this episode's sponsor,

01:31.280 --> 01:38.600
and inviting you to meet me at the MGM Grand in Las Vegas, June 2nd through 5th at Pegaworld,

01:38.600 --> 01:42.560
the company's annual digital transformation conference.

01:42.560 --> 01:48.160
Pegasystems puts AI in the center of its customer engagement software so that it optimizes

01:48.160 --> 01:52.080
every customer test point on every channel in real time.

01:52.080 --> 01:57.000
That way each interaction is relevant and timely to each individual customer, no matter if

01:57.000 --> 02:02.760
it's a sales call, a digital marketing campaign, or a customer service chat either online

02:02.760 --> 02:04.160
or in store.

02:04.160 --> 02:09.800
And the system is always learning in real time to make the next interaction better.

02:09.800 --> 02:15.000
Pegas Customers are the real stars at Pegaworld, there you'll hear great stories of AI applied

02:15.000 --> 02:18.600
to the customer experience at real Pegas Customers.

02:18.600 --> 02:24.080
The event is a great way to learn from a who's who of the Fortune 500, and of course I'll

02:24.080 --> 02:26.600
be there in speaking as well.

02:26.600 --> 02:33.080
To register, visit Pegaworld.com and use the promo code Twimble19 when you sign up for

02:33.080 --> 02:34.080
$200 off.

02:34.080 --> 02:38.320
Again, that's Twimble19, it's as easy as that.

02:38.320 --> 02:39.960
Hope to see you there.

02:39.960 --> 02:43.200
And now on to the show.

02:43.200 --> 02:49.000
Alright everyone, I am on the line with Yas Fundovesthizen.

02:49.000 --> 02:54.160
Yas is completing his PhD in engineering at Cambridge University.

02:54.160 --> 02:57.240
Yas, welcome to this week in machine learning and AI.

02:57.240 --> 02:58.480
Thank you Sam.

02:58.480 --> 03:05.280
So we were joking earlier, you're kind of in this weird intermediary state, intermediate

03:05.280 --> 03:10.160
state between having finished all of the requirements for your PhD and kind of waiting to get the

03:10.160 --> 03:13.160
actual degree conferred.

03:13.160 --> 03:17.920
Tell us a little bit about your background and what you studied at Cambridge.

03:17.920 --> 03:25.120
Yeah, so in all honesty, I fell into the machine learning field a bit by accident.

03:25.120 --> 03:31.680
And I think this story is kind of true for a lot of people with the big current hype.

03:31.680 --> 03:36.720
So where it all started is I started with biomedical engineering undergrad, I'm South African,

03:36.720 --> 03:40.440
and I studied at Statenbach University over there.

03:40.440 --> 03:45.200
And straight out of the undergrad, I realized that I still want to learn a bit more.

03:45.200 --> 03:50.160
So I started the masters in like effectively computational neuroscience.

03:50.160 --> 03:54.280
And this is where I really had like my first little experience with machine learning.

03:54.280 --> 04:00.200
We played with like basic models, like linear discriminant analysis and like self organizing

04:00.200 --> 04:01.680
maps.

04:01.680 --> 04:06.160
And actually like halfway through the masters, I was accepted at Cambridge and actually

04:06.160 --> 04:08.880
Oxford and like received scholarships for both.

04:08.880 --> 04:11.000
So I was quite happy.

04:11.000 --> 04:15.960
And I decided to stop the masters to go start a PhD at Cambridge.

04:15.960 --> 04:22.080
And my initial like plan was to do I was I think a bit ambitious, but my initial plan

04:22.080 --> 04:23.080
was to.

04:23.080 --> 04:27.960
I wanted to effectively create like a wristwatch that could like measure anything about your

04:27.960 --> 04:31.640
body, like kind of look at detail at your blood.

04:31.640 --> 04:35.400
And like tell you, for example, to like today you have to eat a banana, otherwise tomorrow

04:35.400 --> 04:39.440
you're going to get a heart attack that kind of that level of accuracy.

04:39.440 --> 04:42.880
So kind of like a tricorder prize type of device?

04:42.880 --> 04:44.880
Yeah, yeah, pretty much.

04:44.880 --> 04:48.440
And then my supervisor at the time told me like, oh well, there's a range of problems

04:48.440 --> 04:51.000
that you have to solve to get to this end product.

04:51.000 --> 04:55.280
And she said one of the like most fundamental ones is probably like analyzing these signals

04:55.280 --> 04:57.200
and making predictions.

04:57.200 --> 05:02.440
So I kind of started, yeah, I guess looking at ways of like analyzing these signals and

05:02.440 --> 05:08.520
making predictions and just speaking around in a really awesome lab, I heard about like

05:08.520 --> 05:12.200
these temporal machine learning techniques that could easily solve this problem.

05:12.200 --> 05:14.440
And that's kind of where I started playing with them.

05:14.440 --> 05:18.760
And like I played with theater market models, which is like a more conventional technique

05:18.760 --> 05:23.000
and also with some of the new deep learning techniques like recurrent neural networks.

05:23.000 --> 05:31.360
And I guess after seeing my first few lines of like of gradient descent happening, I completely

05:31.360 --> 05:35.200
fell in love with deep learning and that's that's where it all kicked off, yeah.

05:35.200 --> 05:41.800
Have you been seeking to apply what you've been learning about machine learning and deep

05:41.800 --> 05:49.560
learning back to these biomedical applications or have you totally dived into the more theoretical

05:49.560 --> 05:52.360
side of ML and deep learning?

05:52.360 --> 05:55.200
Yeah, so that's a really good question actually.

05:55.200 --> 06:01.120
So this started my PhD was very lean heavily towards biological applications.

06:01.120 --> 06:05.240
And I guess my whole thesis has this whole theme as well.

06:05.240 --> 06:09.360
But pretty early on we realized that well my supervisor and I realized that it's a very

06:09.360 --> 06:13.600
like tough field with a lot of red tape and it's hard to get data, it's hard to get anything

06:13.600 --> 06:14.600
implemented.

06:14.600 --> 06:18.200
There's a lot of like bureaucracy in some areas.

06:18.200 --> 06:21.640
And I didn't like to like struggle with this during my PhD.

06:21.640 --> 06:27.080
So I think halfway through we decided that it might be good to kind of slightly pivot

06:27.080 --> 06:30.600
towards a more theoretical kind of thesis.

06:30.600 --> 06:32.880
So it has like both aspects.

06:32.880 --> 06:38.680
But I feel like I have a strong opinion that like it's the biological field because it has

06:38.680 --> 06:39.680
all these barriers.

06:39.680 --> 06:42.760
We need to like put a lot more effort into it.

06:42.760 --> 06:47.200
So I'm quite happy that I could kind of put a lot more effort into that kind of side.

06:47.200 --> 06:48.200
Yeah.

06:48.200 --> 06:51.200
And you mentioned that you're planning to start a company.

06:51.200 --> 06:56.160
Is it based on the work that you've done at Cambridge?

06:56.160 --> 06:57.160
So not really.

06:57.160 --> 07:04.520
It's it's based on Cambridge in a way because what I had to because I'm from South Africa,

07:04.520 --> 07:09.400
what I had to do quite often in Cambridge is do like a Skype call back home or a WhatsApp

07:09.400 --> 07:12.000
call or all of these internet-based calls.

07:12.000 --> 07:17.160
And like after the PhD, well, I guess like even before the PhD, I knew I wanted to try

07:17.160 --> 07:19.000
and start a company.

07:19.000 --> 07:24.920
And having like I'm sure you and a lot of other people have experienced the like the

07:24.920 --> 07:29.960
poor quality in any internet-based call like Skype or a WhatsApp voice call.

07:29.960 --> 07:33.680
And essentially I thought well why don't we just try and fix this like machine learning

07:33.680 --> 07:39.040
currently has has achieved remarkable things in terms of generating super-resolution images

07:39.040 --> 07:41.880
and like doing video interpolation.

07:41.880 --> 07:47.480
So essentially that's kind of how how my colleague and I started out with the new idea that

07:47.480 --> 07:48.800
we're trying to pursue.

07:48.800 --> 07:52.760
You're not calling a pie-piper are you?

07:52.760 --> 07:57.000
I'm glad you made that connection.

07:57.000 --> 08:00.760
That's probably a good thing.

08:00.760 --> 08:08.240
One of the papers that came out of your PhD is called the unreasonable effectiveness of

08:08.240 --> 08:09.400
the forget gate.

08:09.400 --> 08:11.000
How did that work come about?

08:11.000 --> 08:12.000
Yeah.

08:12.000 --> 08:15.120
So well, maybe I'll just quickly say where the name came from.

08:15.120 --> 08:20.400
I have to give homage to AndrÃ© Carpathy, who he had a blog post called the unreasonable

08:20.400 --> 08:24.320
effectiveness of RNNs, Ricardo Networks.

08:24.320 --> 08:27.400
And I think a lot of people really love this blog post and I also love it.

08:27.400 --> 08:28.400
I read it.

08:28.400 --> 08:31.520
So that's kind of where the name inspiration came from.

08:31.520 --> 08:38.360
But to to jump back to the question, essentially in the the final parts of my PhD, I started

08:38.360 --> 08:42.120
working a bit like I looked at two things at the same time.

08:42.120 --> 08:47.960
One was this biological application that we had, which was to essentially infer from peripheral

08:47.960 --> 08:55.120
neural signals what an agent or what a human or some animal is doing based on those signals.

08:55.120 --> 09:00.760
And essentially if you want to create a prosthetic device that can like in real time react to these

09:00.760 --> 09:04.480
neural signals, it has to be very low powered and very resource efficient.

09:04.480 --> 09:09.640
So we'd already had a solution that could infer what the human is doing based on those

09:09.640 --> 09:10.640
signals.

09:10.640 --> 09:15.080
But now we needed to make it much more computationally efficient.

09:15.080 --> 09:17.600
So that was like the one driver.

09:17.600 --> 09:22.200
The second or the thing I was working on in parallel was I looked at initialization techniques

09:22.200 --> 09:28.200
for in general kind of, I guess neural networks or deep learning models, but also specific

09:28.200 --> 09:34.240
more specifically for LSTMs or the long short term memory recurrent neural network.

09:34.240 --> 09:44.200
And I guess I played around with various approaches to initialization for LSTMs and with ways

09:44.200 --> 09:49.360
of like I guess quantizing them or pruning them or making them more efficient.

09:49.360 --> 09:55.680
And then I happened to stumble upon this architecture that I've explained in the paper, which

09:55.680 --> 10:01.320
essentially did like, first of all, we were surprised, okay, it kind of saves computation.

10:01.320 --> 10:06.040
But then yeah, we were really surprised when it did a lot better than the original LSTM.

10:06.040 --> 10:07.040
So yeah.

10:07.040 --> 10:08.040
Interesting.

10:08.040 --> 10:12.520
And before we get too far, I should probably note because there's somebody that's jumping

10:12.520 --> 10:18.480
up out of their chair on the on the bar or something like that, that the whole unreasonable

10:18.480 --> 10:24.920
effectiveness thing dates back before Carpathy to a guy Eugene Wigner who wrote a paper,

10:24.920 --> 10:30.160
the unreasonable effectiveness of mathematics and the natural sciences back in 1959.

10:30.160 --> 10:34.960
But we were joking a little bit before we started the interview that I noted that I've

10:34.960 --> 10:42.560
gone through the math of RNNs several times and it's not quite intuitive to me.

10:42.560 --> 10:48.200
You mentioned that you have the same experience having studied it for your degree as well.

10:48.200 --> 10:55.200
Maybe a good place to start is to talk about the, or LSTMs in particular, the role of

10:55.200 --> 10:57.200
gates in LSTMs?

10:57.200 --> 10:58.200
Yeah, yeah.

10:58.200 --> 11:02.320
So I guess I could give a little history of how it all developed.

11:02.320 --> 11:07.920
Essentially recurrent neural networks were designed because they share parameters across

11:07.920 --> 11:09.440
this time dimension.

11:09.440 --> 11:11.160
So it makes them a lot more efficient.

11:11.160 --> 11:16.120
Kind of like the same way that convolutional neural networks share parameters across the

11:16.120 --> 11:19.280
spatial dimension, the 2D spatial dimension.

11:19.280 --> 11:23.600
But one problem you run into with recurrent neural networks is that because you're kind

11:23.600 --> 11:29.600
of over time you're multiplying the same weights or the same information with each other the

11:29.600 --> 11:34.680
whole time, you run into, you usually run into gradient problems.

11:34.680 --> 11:41.920
And back in, I think it's 97, Hawthorite Renshmit Hoover realized that, okay, I guess they

11:41.920 --> 11:48.680
approach it from a slightly different direction, but they thought that, or argue that you have,

11:48.680 --> 11:52.960
when you do back propagation, you have through a recurrent neural network.

11:52.960 --> 11:56.400
Some of the, like if you, if you want to remember something and you want to forget something

11:56.400 --> 12:00.800
of the input sequence, there's kind of conflicting updates through the same edges.

12:00.800 --> 12:05.480
So they thought or proposed that if you use an input and output gate, this could kind

12:05.480 --> 12:10.480
of solve that conflict and kind of protect, so in there are an answer, it could kind of

12:10.480 --> 12:16.760
protect the cell or the memory from these conflicting updates.

12:16.760 --> 12:18.000
And that's worked pretty well.

12:18.000 --> 12:27.320
And then I think a few years later, it was Gares in 2000 who, he realized that this works

12:27.320 --> 12:35.720
well, but you can have, if the memory cell of the RNN or the, like, I guess the state doesn't

12:35.720 --> 12:40.240
have a mechanism of forgetting some of the information, there's a possibility that it could

12:40.240 --> 12:44.720
grow indefinitely and kind of break down the network.

12:44.720 --> 12:50.560
So he proposed a forget gate, and this would then allow the cell to kind of forget some

12:50.560 --> 12:53.280
of the information over time.

12:53.280 --> 12:56.320
And that's kind of the LSTM that we know today.

12:56.320 --> 13:02.840
That's kind of how the gates help the LSTM to prevent these gradient problems during training.

13:02.840 --> 13:03.840
Got it.

13:03.840 --> 13:11.080
So forget gate is part of the typical LSTM that is commonly in use nowadays.

13:11.080 --> 13:12.080
Yes.

13:12.080 --> 13:17.920
So the typical LSTM has three gates, the input gate, which controls how much information

13:17.920 --> 13:19.920
is input at each time step.

13:19.920 --> 13:24.120
The output gate, which controls how much information is output to the next cell or to your output

13:24.120 --> 13:26.000
layer at each time step.

13:26.000 --> 13:29.880
And then the forget gate, which essentially just says, how much should I forget at each

13:29.880 --> 13:31.160
time, time step?

13:31.160 --> 13:36.720
I'm trying to get that kind of your observations about the forget gate and what kind of caused

13:36.720 --> 13:43.040
you to start looking at that as an interesting part of this approach.

13:43.040 --> 13:48.200
In 2015, there were two papers that kind of at the same time said, or concluded that the

13:48.200 --> 13:52.080
forget gate is the most important part of the LSTM.

13:52.080 --> 13:56.200
They did like these ablation studies that removed the few gates at a time and they kind

13:56.200 --> 14:02.280
of found that every time you remove the forget gate, like the performance just drops drastically.

14:02.280 --> 14:07.200
And this was, I think, just aphowitz and a graph.

14:07.200 --> 14:09.760
And essentially, that was kind of my starting point.

14:09.760 --> 14:14.400
I realized, like, okay, let's, if they say it's this important, let's remove everything

14:14.400 --> 14:18.920
we can and just keep the forget gate and see where we go from there.

14:18.920 --> 14:23.960
And that was kind of, that was, yeah, it had decent performance.

14:23.960 --> 14:29.880
But like on a typical data set called MNIST, that I think everyone knows this pretty well

14:29.880 --> 14:31.360
now.

14:31.360 --> 14:37.640
If you kind of, if you process that in scanline order, you get many subsections that are

14:37.640 --> 14:40.880
like have 10 or 20 consecutive zeros.

14:40.880 --> 14:44.720
And essentially, this makes it super hard for the network if it's only forget gate to

14:44.720 --> 14:48.960
remember anything by the end of those zero, like 10 or 20 zeros.

14:48.960 --> 14:53.160
So that's where I had to like, I realized, okay, we need to kind of initialize this better

14:53.160 --> 14:58.760
to be able to kind of retain memory over those periods of zero.

14:58.760 --> 15:01.480
You know, I guess what's interesting about this is that it's counterintuitive.

15:01.480 --> 15:07.520
So you use the forget gate and the memory, but not the input and output gates.

15:07.520 --> 15:12.680
And found that you had this performance improvement.

15:12.680 --> 15:13.680
Yeah, yeah.

15:13.680 --> 15:16.680
So essentially, I saw this example recently.

15:16.680 --> 15:20.600
So if you like take an image and you want to classify what's in it, essentially what

15:20.600 --> 15:24.960
like a network or what you can visualize it as is like, you remove every single element

15:24.960 --> 15:27.880
of that image except the part that you care about.

15:27.880 --> 15:31.280
For instance, if it's a panda, you remove like all the buildings, branches and like trees

15:31.280 --> 15:32.280
behind it.

15:32.280 --> 15:34.680
And you just have to the panda face or something.

15:34.680 --> 15:38.480
So essentially like that could be summarized as to learn something, you just need to know

15:38.480 --> 15:41.320
what you need to forget.

15:41.320 --> 15:45.920
And I guess like that's kind of kind of in a way what happens here, but it's not completely

15:45.920 --> 15:51.480
true because we have only one gate, which is the forget gate, but this gate at the moment

15:51.480 --> 15:56.760
or in the new network controls both how much information is forgotten and also how much

15:56.760 --> 15:58.040
new information comes in.

15:58.040 --> 16:04.320
So it's kind of it's coupled with this gate just like performing both of those roles.

16:04.320 --> 16:11.240
Is that observation that learning is primarily about forgetting at odds with the emphasis

16:11.240 --> 16:17.400
on attention-based methods that are, you seem less about forgetting and more about trying

16:17.400 --> 16:19.200
to figure out what to remember?

16:19.200 --> 16:24.000
I think it's all just semantics in a way, it's just how you phrase or perceive it, yeah.

16:24.000 --> 16:28.680
So it could be either way and I think attention mechanisms could also be said as like, oh

16:28.680 --> 16:32.360
well attention mechanisms, just make sure that you kind of forget about the rest and focus

16:32.360 --> 16:35.360
on something that's like relevant.

16:35.360 --> 16:37.360
So it's just how you perceive it, I guess.

16:37.360 --> 16:42.280
I'm curious, have you tried applying them in concert with one another, you know, I'm

16:42.280 --> 16:45.320
thinking of forgetting all the way down?

16:45.320 --> 16:46.320
All right.

16:46.320 --> 16:47.320
No, I haven't.

16:47.320 --> 16:53.080
So you mean like applying this new network with attention mechanisms?

16:53.080 --> 16:54.080
Right.

16:54.080 --> 16:58.040
No, I haven't tried that, but that's a pretty cool idea.

16:58.040 --> 17:06.440
Based on this, you were inspired to create another network architecture called Janet.

17:06.440 --> 17:07.440
Yeah.

17:07.440 --> 17:09.720
What is Janet about?

17:09.720 --> 17:13.800
So Janet is essentially the network that we've been discussing now.

17:13.800 --> 17:14.800
Okay.

17:14.800 --> 17:15.800
Yeah.

17:15.800 --> 17:21.280
So the Janet is just the name I gave for this new, I guess, way of processing information

17:21.280 --> 17:24.920
or this, I guess, simplified version of the Alistair.

17:24.920 --> 17:28.120
We chose the name kind of in a lab meeting.

17:28.120 --> 17:32.320
I don't know if I'm allowed to say this, but I'll try.

17:32.320 --> 17:38.920
Essentially, I don't think that this is like a network that is the best.

17:38.920 --> 17:43.560
And because people make so many new networks every day, it's just like, this is just another

17:43.560 --> 17:44.560
one of those.

17:44.560 --> 17:46.400
So it's just another network in that sense.

17:46.400 --> 17:52.560
But it happened to also be a nice little acronym for Yosses Awesome Network.

17:52.560 --> 17:55.400
So we kind of have this little lab joke about it, but yeah.

17:55.400 --> 17:56.400
Okay.

17:56.400 --> 18:03.160
I think what threw me off in thinking that it was a different network was the pictures

18:03.160 --> 18:08.120
that I've seen of it still have these other gates in them, or at least what I look like

18:08.120 --> 18:13.520
to me, these other gates, am I reading those pictures incorrectly?

18:13.520 --> 18:16.880
What pictures have you seen?

18:16.880 --> 18:18.880
So, yeah.

18:18.880 --> 18:25.040
So the gates in the Janet are essentially, it's just one gate, which is the forget gate

18:25.040 --> 18:26.040
of the Alistair.

18:26.040 --> 18:30.640
So it's like, the Janet is just the LSTM with every thing I move except for the forget

18:30.640 --> 18:31.640
gate.

18:31.640 --> 18:39.640
So your paper was kind of presenting an analysis of this network and its performance on MNIST

18:39.640 --> 18:43.040
and potentially other data sets?

18:43.040 --> 18:44.040
Yeah.

18:44.040 --> 18:50.680
So I guess we, there are a few papers who like that bring up new types of recurrent neural

18:50.680 --> 18:51.680
networks.

18:51.680 --> 18:57.240
This memory problem, or I guess both the memory problem over very long sequences and the

18:57.240 --> 19:00.880
problem of exploding and vanishing gradients is quite a, quite a big problem.

19:00.880 --> 19:07.680
So you have quite a few of these papers that just kind of have these benchmark memory tests.

19:07.680 --> 19:09.520
And that's kind of what we try to follow over here.

19:09.520 --> 19:14.760
So one of them is MNIST, which has this hard problem, as I mentioned before, of like consecutive

19:14.760 --> 19:15.760
zeros.

19:15.760 --> 19:20.840
Then there's perturbed MNIST, which we also tested on, which essentially just does a random

19:20.840 --> 19:26.840
permutation of all the pixels in the MNIST image.

19:26.840 --> 19:31.760
And this kind of creates longer dependencies over time if you process this in scanline order.

19:31.760 --> 19:33.440
And then there are a few others.

19:33.440 --> 19:36.840
The two we test is the ad and the copy task.

19:36.840 --> 19:42.880
And essentially it's just for the ad task, it's two sequences where one is zeros and ones.

19:42.880 --> 19:45.440
And the other one is continuous values.

19:45.440 --> 19:51.680
And wherever you have ones in the binary sequence, you need to add those corresponding numbers

19:51.680 --> 19:54.960
in the continuous valued sequence.

19:54.960 --> 19:58.760
So it's like it kind of tests the memory of this network.

19:58.760 --> 20:02.760
And then the same for the copy task just needs to remember everything at the start of the

20:02.760 --> 20:07.520
sequence all the way to the end and be able to regenerate that at the end.

20:07.520 --> 20:12.600
So yeah, and then obviously we've tested it on a few other datasets since then.

20:12.600 --> 20:18.080
For the paper, we also tested it on one of my biological datasets called MIT, B-I-H,

20:18.080 --> 20:24.320
arrhythmia dataset, which is like a dataset of ECG heartbeats that are classified into

20:24.320 --> 20:27.320
five different types of heartbeats arrhythmias.

20:27.320 --> 20:30.440
And it worked better on that dataset as well.

20:30.440 --> 20:34.680
And when you say better relative to whatever the state of the art is on these datasets or

20:34.680 --> 20:38.480
relative to vanilla LSTMs.

20:38.480 --> 20:46.960
So for the dataset, I mean the MIT, the ECG dataset, there aren't that many, I guess, state

20:46.960 --> 20:47.960
of the art results.

20:47.960 --> 20:51.600
So we just compared it to the LSTM and it does better than the LSTM.

20:51.600 --> 20:54.360
On the other datasets, it's kind of, it's harder to say.

20:54.360 --> 20:58.840
So in all cases, the Janet does better than the LSTM on those datasets.

20:58.840 --> 21:03.040
But then there are like different, I guess, models that do even better than the Janet.

21:03.040 --> 21:07.120
So like the wave net does, I'm sure you're familiar with there.

21:07.120 --> 21:11.160
Yeah, the wave net is a new, very good model by DeepMind.

21:11.160 --> 21:15.600
And that model does quite well on perturbed MNIST, but then does slightly worse than Janet

21:15.600 --> 21:17.400
on the normal MNIST.

21:17.400 --> 21:22.040
And yeah, you have quite a few of the similar nuances for other models that have also been

21:22.040 --> 21:23.040
proposed.

21:23.040 --> 21:29.600
What was the role of the way you initialized the network in the results you saw?

21:29.600 --> 21:33.680
For the Janet to work on datasets like MNIST, you have to initialize it using this

21:33.680 --> 21:37.400
Corona initialization scheme that we proposed in the paper.

21:37.400 --> 21:42.120
But the initialization scheme was initially proposed by Talek and Oliver.

21:42.120 --> 21:48.160
It's also a really, really cool paper in 2018, published that I clear.

21:48.160 --> 21:52.360
But so if you don't use that initialization scheme for MNIST, it would, like, the Janet

21:52.360 --> 21:53.360
wouldn't learn anything.

21:53.360 --> 21:59.240
You would kind of have this accuracy around 5% throughout training for, and then for

21:59.240 --> 22:03.600
tasks that are slightly easier to learn that don't have, like, long consecutive periods

22:03.600 --> 22:09.640
of zero, the Janet, like, can still learn, but it definitely does a lot better if you

22:09.640 --> 22:12.120
use this initialization scheme.

22:12.120 --> 22:17.880
For the LSTM, you also get some benefits, I guess, like, more stability during training

22:17.880 --> 22:24.440
if you use Corona initialization, but the benefits aren't as clear as it is for the Janet.

22:24.440 --> 22:26.920
And how does the initialization scheme work?

22:26.920 --> 22:31.800
Yeah, so essentially, what Talek and Oliver found was that.

22:31.800 --> 22:40.040
If you, or the, I guess, the characteristic for getting time of the RNN is, sorry, of

22:40.040 --> 22:44.080
the LSTM, is one over the Forgetgate value.

22:44.080 --> 22:49.240
So at the start, if you're, if we initialize it according to the way we always initialize

22:49.240 --> 22:55.520
for Getgate, you essentially have a really small value for the Forgetgate or relatively

22:55.520 --> 22:57.080
small value.

22:57.080 --> 23:01.880
This means that you're forgetting the amount of time steps you can go through before you

23:01.880 --> 23:07.920
forget everything that you, that you saw before it is, like, is limited to probably four

23:07.920 --> 23:10.280
or five time steps.

23:10.280 --> 23:16.440
And this is problematic if you have, if you need to kind of retain memory over longer periods.

23:16.440 --> 23:20.440
And they kind of said, well, okay, we can, we can be smarter about how we initialize the

23:20.440 --> 23:26.640
Forgetgate, well, all the Gates, in fact, and that is essentially to, like, at the start

23:26.640 --> 23:32.560
of training, the values that matter a lot for, for the LSTM is the, the biases.

23:32.560 --> 23:36.760
And initially, or traditionally, we've always just initialize them to be zero.

23:36.760 --> 23:43.360
But Talek and Oliver kind of said, we should initialize them as the log of a uniform distribution

23:43.360 --> 23:47.040
between one and the maximum number of time steps we have.

23:47.040 --> 23:53.200
And that kind of gives you that, make sure that some of the Forgetgate kind of allow

23:53.200 --> 23:58.400
the network to remember very long periods up to the end of the sequence.

23:58.400 --> 24:02.600
And some of them are still, like, short term kind of cells, which kind of can't forget

24:02.600 --> 24:05.160
at every second or third step.

24:05.160 --> 24:12.600
What's the intuition for why this specific kind, well, first of all, did you try other initialization

24:12.600 --> 24:15.760
schemes with Janet?

24:15.760 --> 24:22.640
And if so, and there was kind of a binary, you know, work didn't work kind of situation.

24:22.640 --> 24:27.960
What's your intuition for why this one, you know, performed, whereas others didn't?

24:27.960 --> 24:28.960
Yeah.

24:28.960 --> 24:33.000
So I did, I did try a few other initialization schemes.

24:33.000 --> 24:37.000
I'm sure I've forgotten a few of them as well, because this was kind of, you're done quite

24:37.000 --> 24:38.000
a while ago.

24:38.000 --> 24:43.320
But the, like, one of the things I tried quite a lot is, or I saw from, I saw from

24:43.320 --> 24:49.920
Alchreiter and Schmidt Hooper that they tried this thing in 97 as well, which they just,

24:49.920 --> 24:53.120
they randomly guess some of the initial parameters between, like, certain values.

24:53.120 --> 24:57.640
And if you do that enough, you can actually get a network that does as well as a trained

24:57.640 --> 25:02.840
one without actually ever training it, you just initialize it to this perfect, like, network.

25:02.840 --> 25:07.480
I tried something similar where I would, I know kind of what our current initialization

25:07.480 --> 25:15.080
schemes are, and I would kind of guess random parameters in those ranges and see, kind

25:15.080 --> 25:21.320
of, if this kind of yields any better networks that you can then train from or, like, improve

25:21.320 --> 25:22.320
from.

25:22.320 --> 25:27.760
But this unfortunately didn't work or it gave the same results or, like, the same performance

25:27.760 --> 25:31.080
as you would get with the normal other steam initialization.

25:31.080 --> 25:36.240
And then beyond that, I, yeah, I don't think I can't remember anything else I tried.

25:36.240 --> 25:44.520
And how did you contextualize this within the sphere of biological applications for your

25:44.520 --> 25:45.520
thesis?

25:45.520 --> 25:50.920
Yeah, so I think I mentioned before that we were like, we were searching for a variant

25:50.920 --> 25:56.320
of the LSTM that is computationally more efficient than just the normal one.

25:56.320 --> 25:58.480
So there are various ways you can do this.

25:58.480 --> 26:04.840
Some of them are, like, it's called pruning, which I, it comes from way back.

26:04.840 --> 26:11.160
And another one is quantization, where essentially you just, you kind of quantize the weights to

26:11.160 --> 26:17.160
be kind of, yeah, certain values that don't take up too much memory and computation.

26:17.160 --> 26:22.800
And then I guess, like, I was kind of impressed by the gated recurrent unit, which is like

26:22.800 --> 26:28.200
another version of recurrent neural networks, which only used two gates instead of the three

26:28.200 --> 26:29.360
that the LSTM uses.

26:29.360 --> 26:33.840
So I was like, oh, can we do, can we do one better than the GRU?

26:33.840 --> 26:37.760
And we just have one gate and still do as well.

26:37.760 --> 26:38.760
So yeah.

26:38.760 --> 26:42.120
And then sorry, I just remembered something from the previous question, which is in terms

26:42.120 --> 26:48.520
of an initialization, one thing that I kind of found at the end was when we started using

26:48.520 --> 26:54.680
more units in the layers of the Janet and the LSTM, we found that the Janet kind of, instead

26:54.680 --> 26:59.160
of just doing better at the end, it kind of also learned a lot quicker.

26:59.160 --> 27:05.520
So it would get to the highest point of accuracy a lot quicker than a word with less units,

27:05.520 --> 27:07.600
which is kind of, yeah, that makes sense.

27:07.600 --> 27:11.920
But then I thought like a really cool thing we can try is to play around with this as

27:11.920 --> 27:16.240
in like, it's kind of, if you have a really big Janet, you can kind of just guess the right

27:16.240 --> 27:20.760
solution or very close to the right solution from the start and then remove a lot of units

27:20.760 --> 27:25.800
to say compensation and then just train the rest to kind of give you that perfect model.

27:25.800 --> 27:30.800
So that's something in terms of an initialization that I think is also worth pursuing.

27:30.800 --> 27:35.560
Is that a potential future direction or did you get specific results with that?

27:35.560 --> 27:37.120
That's a potential future direction.

27:37.120 --> 27:41.560
I have, yeah, I've been slightly busy with other things since then, but that's something

27:41.560 --> 27:42.560
I want to play with.

27:42.560 --> 27:43.560
Got it.

27:43.560 --> 27:52.440
What's the, in general, the impact of getting rid of the other gates on the computational

27:52.440 --> 27:56.000
intensity of training one of these networks?

27:56.000 --> 27:59.960
In terms of like computational time, it doesn't improve things that much.

27:59.960 --> 28:05.840
And also I guess in just the feed forward processing time during testing, because a lot of

28:05.840 --> 28:11.040
your like computational time is limited by the sequential nature of the LSTM, which in

28:11.040 --> 28:15.240
the case of the LSTM and the Janet are essentially the same, because you need to like process

28:15.240 --> 28:17.440
things at every time step.

28:17.440 --> 28:23.080
But the part where it does save, which also saves on I guess battery usage if this is a device

28:23.080 --> 28:29.560
that's to be used in a portable device, sorry, a model to be used in a portable device.

28:29.560 --> 28:34.240
Then having using less memory is quite helpful.

28:34.240 --> 28:40.760
So the Janet uses half the memory that the LSTM uses and kind of gets better accuracies

28:40.760 --> 28:43.920
for the datasets that we tested on, which is quite good.

28:43.920 --> 28:49.960
And that also meant that we could train much bigger Janet's on the same GPU, whereas

28:49.960 --> 28:54.760
for an LSTM, you would quickly run out of memory on the GPU.

28:54.760 --> 29:02.960
Did you look at the same memory and compute and power implications from an inference perspective

29:02.960 --> 29:04.920
with the train Janet model?

29:04.920 --> 29:06.560
Is there any difference there?

29:06.560 --> 29:09.720
No, so yeah, so I'm like, it's the same on that side.

29:09.720 --> 29:17.840
So for both like training a model and for doing inference, I guess it saves half the parameters

29:17.840 --> 29:21.040
and computational time is roughly the same.

29:21.040 --> 29:24.200
It's slightly shorter for the Janet, but nothing significant.

29:24.200 --> 29:32.280
You noted that you, you know, that compared to WaveNet this, you know, this underperforms.

29:32.280 --> 29:38.480
But is there a place for it within the domain of general purpose mobile network?

29:38.480 --> 29:44.480
So is it, you know, do you, are there other reasons why it's kind of a research experiment

29:44.480 --> 29:46.720
and not necessarily practical?

29:46.720 --> 29:50.000
It's hard to say off the bat.

29:50.000 --> 29:55.440
I think, yeah, I don't know, this, I guess at the time very much trapped in this very

29:55.440 --> 29:58.600
research focused, I guess, domain.

29:58.600 --> 30:02.280
I think like it's not the ultimate best solution that there is yet.

30:02.280 --> 30:06.960
I think it's just, it was kind of interesting to show that kind of, this is how much importance

30:06.960 --> 30:09.840
the forget gate has in this network.

30:09.840 --> 30:12.080
It's kind of interesting to see.

30:12.080 --> 30:18.040
I think there, yeah, for each specific problem, you would have to have a specific like network

30:18.040 --> 30:22.480
and a specific solution that's best suited for that problem.

30:22.480 --> 30:25.240
There are some where the Janet could be, could be that.

30:25.240 --> 30:30.880
I think specifically in like anything where you have very long term memory requirements

30:30.880 --> 30:33.480
and only a single output at the end.

30:33.480 --> 30:39.960
We kind of have that requirement plus a, I guess, a resource efficiency requirement.

30:39.960 --> 30:45.520
So yeah, I think like the wave net is quite like, it's quite a large network in terms

30:45.520 --> 30:48.960
of number of layers, but it uses very little parameters.

30:48.960 --> 30:52.680
So yeah, there are pros and cons in both.

30:52.680 --> 30:59.040
Are there specific examples that come to mind of scenarios where you'd have this, you

30:59.040 --> 31:02.040
know, long sequence in time with a single output?

31:02.040 --> 31:07.240
Yeah, so I guess like a lot of biological signals have that feature.

31:07.240 --> 31:13.080
So like, if you have, if you're measuring a heartbeat for a few seconds, that's quite

31:13.080 --> 31:18.800
a lot of time steps that you have to analyze and then you kind of have to make a single

31:18.800 --> 31:21.280
prediction at the end of all those time steps.

31:21.280 --> 31:27.240
So that's like a typical scenario where you would have a really long signal with a single

31:27.240 --> 31:28.560
output at the end.

31:28.560 --> 31:33.360
An example where it's not that like that is, for instance, when you're generating, when

31:33.360 --> 31:39.880
you're doing like translation, you have to generate the, or yeah, maybe just, I guess,

31:39.880 --> 31:41.640
sentence generation.

31:41.640 --> 31:47.000
You have to generate a word at each time step or you have to classify it's part of speech

31:47.000 --> 31:48.280
at each time step.

31:48.280 --> 31:53.240
That's kind of not a very long term dependency, or yeah, a long term dependency with a single

31:53.240 --> 31:56.040
output because you're outputting something at each time step.

31:56.040 --> 32:02.200
Okay, cool, you mentioned in our conversation a couple of things that you'd like to play

32:02.200 --> 32:03.200
with in the future.

32:03.200 --> 32:09.560
Are there areas that you would like to see someone continue working on beyond the ones

32:09.560 --> 32:10.560
we've discussed?

32:10.560 --> 32:12.320
No, I think I mentioned all of them.

32:12.320 --> 32:17.960
I think just applying this to more datasets, specifically datasets with long term dependencies

32:17.960 --> 32:21.040
and a single output at the end, that's cool.

32:21.040 --> 32:26.000
So if anyone can get more types of data than are like that, I think that would be really

32:26.000 --> 32:27.680
interesting to see.

32:27.680 --> 32:32.520
And then obviously also like playing around with this new initialization feature where

32:32.520 --> 32:39.440
you can kind of almost, in the first instance, guess the correct, well, very close to the

32:39.440 --> 32:44.720
correct network and then just do a few more steps of training to get there, like a K-shop

32:44.720 --> 32:46.280
learning approach.

32:46.280 --> 32:48.480
I think those would be interesting to see.

32:48.480 --> 32:49.480
Cool, interesting.

32:49.480 --> 32:52.240
Well, thanks for taking the time to chat with me about this.

32:52.240 --> 32:53.240
Cool stuff.

32:53.240 --> 32:54.240
Anytime.

32:54.240 --> 32:55.240
Thank you.

32:55.240 --> 33:01.000
All right, everyone, that's our show for today.

33:01.000 --> 33:07.240
For more information on Yoss or any of the topics covered in this episode, visit twomelai.com

33:07.240 --> 33:10.640
slash talk slash 240.

33:10.640 --> 33:27.800
As always, thanks so much for listening and catch you next time.

