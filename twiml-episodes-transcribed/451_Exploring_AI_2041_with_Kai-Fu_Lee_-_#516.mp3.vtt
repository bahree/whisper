WEBVTT

00:00.000 --> 00:18.640
All right, everyone. I am here with Kaifu Lee. Kaifu is chairman and CEO of Innovation Ventures,

00:18.640 --> 00:25.200
the former president of Google China, an author of the New York Times bestseller AI Superpowers.

00:25.200 --> 00:30.640
And we're here to talk about his new book, which will be released next week, AI2041.

00:31.200 --> 00:38.400
Kaifu, welcome to the Twomo AI podcast. Thank you. Thanks, Sam. It is great to have an

00:38.400 --> 00:44.320
opportunity to speak with you. I'm looking forward to digging in and talking more about the book.

00:44.960 --> 00:49.040
Before we do, though, I'd love to have you share a little bit about your background and

00:50.080 --> 00:53.920
how you came to work in the field of AI. Sure.

00:53.920 --> 01:02.480
I started with my excitement in AI back in 1979 when I started my undergraduate at Columbia,

01:03.120 --> 01:09.760
and I worked on natural language and vision at Columbia, and then I went to Carnegie Mellon for

01:09.760 --> 01:17.200
my PhD, at which I developed the first speaker independent speech recognition system,

01:17.200 --> 01:25.840
based on machine learning, actually, one of the earlier thesis in machine learning in 1988.

01:26.640 --> 01:33.520
I also developed a computer program that beat the world's off-the-all champion. It's all in the 80s,

01:33.520 --> 01:41.680
very early years. After my graduation from CMU, I taught there for two years, then I joined Apple,

01:41.680 --> 01:49.760
and led a lot of apples, AI, speech, natural language, and multimedia efforts. Later, I joined SGI,

01:50.560 --> 01:57.440
and then Microsoft, where I started the Microsoft Research Asia in Beijing in 1998,

01:57.440 --> 02:05.040
which kind of became one of the best AI research labs in Asia. Later, I joined Google,

02:05.040 --> 02:13.040
and ran Google China for four years between 2005 and 2009. We did do a little bit for Hanei,

02:13.040 --> 02:20.480
but mostly it was really developing Google's presence in China. In 2009, I left Google and started

02:20.480 --> 02:28.080
my venture capital firm, Sinovation Ventures. At Sinovation Ventures, we invested in about 40 AI

02:28.080 --> 02:36.480
companies. We were about the earliest and probably invested in the most companies. We invested in

02:36.480 --> 02:44.240
about seven unicorns in AI alone, and with a few more yet to come. So, very excited to be in the

02:44.240 --> 02:51.920
era of AI. It was not so hot during much of my career, but I'm glad to be able to catch the

02:51.920 --> 03:00.000
recent wave and participate in it. Fantastic. Fantastic. So, let's maybe jump into the book.

03:00.000 --> 03:07.200
The title is AI2041. If you just read that, having heard nothing of the book, you might think that

03:07.200 --> 03:15.600
it's kind of a straight-up, you know, your vision for AI in 2041. But, and to some degree,

03:15.600 --> 03:21.200
that is the case, you're asking interesting questions on that time horizon, but there's a little

03:21.200 --> 03:27.440
bit of a twist. Tell us about that twist and the way the book is, you know, organized.

03:29.040 --> 03:37.040
Sure. The twist is, we, I call this book, Scientific Fiction, because I collaborated with a

03:37.040 --> 03:43.760
science fiction writer who wrote most of the book, probably three quarters. And they are 10 stories,

03:43.760 --> 03:51.040
we call it, 10 visions of the future. I find that the impact of AI is misunderstood by a lot

03:51.040 --> 03:57.920
of people. Some are too conservative, others are too optimistic, and others are just naive,

03:57.920 --> 04:04.720
and some explanation, I think, could be helpful. AI will change our future, and more people need

04:04.720 --> 04:11.680
to understand it. And having a fictional writer, right, in terms of stories, will make it all

04:11.680 --> 04:18.080
more accessible to people. So, the book is organized in 10 stories, of which takes a place in a

04:18.080 --> 04:24.640
different country, and in a different industry. So, we can see how AI will impact all countries

04:24.640 --> 04:31.760
and all industries. And then, after each story, I write an analysis of the technologies embedded

04:31.760 --> 04:40.000
in the chapter, how they will progress, and what challenges they may bring, and how we might

04:40.000 --> 04:46.720
solve them, or what we should do now, to deal with the externalities or potential challenges that

04:46.720 --> 04:55.840
they bring about. So, it's 10 stories going from relatively simple uses of AI to extremely

04:57.840 --> 05:03.920
challenging and somewhat futuristic uses of AI, but in the whole set of 10 stories, I try to

05:04.960 --> 05:12.880
at least have a high degree of confidence, like 80% or more, that this would work in the 10 to 20

05:12.880 --> 05:20.240
year timeframe. You mentioned that when you talk to people, you get a range of

05:22.560 --> 05:28.320
reactions or perspectives on AI ranging from very conservative to over optimistic. A lot of that

05:28.320 --> 05:36.320
has to do with the time horizon that you're thinking about. You chose 20 years as the kind of central

05:36.320 --> 05:43.760
time horizon for this book. Why is that? Because on the one hand, 20 years is pretty long. A lot

05:43.760 --> 05:50.720
can happen in 20 years. 20 years ago, we didn't have the iPhone or the mobile internet, and look how

05:50.720 --> 05:59.920
things have changed. So, imagine 20 years ago, if someone were to write AI 2021, it would be pretty

05:59.920 --> 06:07.040
interesting and fantastic 20 years ago if it accurately described today. So, it's long enough,

06:07.040 --> 06:14.080
futuristic enough, exciting enough, but not so long that we could hand wave and say, you know,

06:14.800 --> 06:22.240
bring download as possible and we become cyborgs or we're doing teleportation or time travel,

06:22.240 --> 06:29.040
so we stay away from that. And also, I factor in that the time it takes to develop the research

06:29.040 --> 06:36.560
to perfect it, to reduce the cost, to implement it, to productize it, to make it acceptable to the

06:36.560 --> 06:48.800
market, and also to deal with potential legal regulatory and accountability issues. So, it's not,

06:48.800 --> 06:53.920
so some of the stories may look like, hey, we could almost do that today, but there are a lot of

06:53.920 --> 07:00.560
other issues that we're coming to play. Sure, sure. I think one of the things that challenges

07:01.200 --> 07:11.840
folks the most on this, you know, conservative optimistic from a mass perspective is autonomous

07:11.840 --> 07:18.480
driving. Do you have a story in the book that talks about autonomous driving? Yes, yes, of course,

07:18.480 --> 07:24.240
can't write the book in 20 years without it. Of course, by then, L5 will have worked.

07:25.840 --> 07:33.520
It's kind of in transition, so I think that describes my view is that L5 is quite challenging,

07:34.240 --> 07:42.320
and the path towards L5, as I describe in the story, will be incremental. As we know,

07:42.320 --> 07:49.680
AI gets better with more deployment, with more data, with more learning, so basically L5 will be a

07:49.680 --> 07:58.480
series of increasingly more challenging environments, perhaps starting with fixed routes, like buses,

07:58.480 --> 08:04.720
and then trucks on highways, and then more and more cities, and that's kind of one path

08:04.720 --> 08:13.520
as more data experience is gathered. It will face still a lot of challenges, even in 20 years,

08:13.520 --> 08:21.200
and one of the predictions I make is that cities will have to modify some existing road infrastructure,

08:21.840 --> 08:30.480
for example, to separate very dangerous cross sections with pedestrians and cars, so that there's

08:30.480 --> 08:37.120
no risk of a car hitting a pedestrian in the most likely environments and crossroads in downtowns,

08:37.680 --> 08:45.600
such as happened with the Uber Autonomous Vehicle in Phoenix, and also roads can be smart and

08:45.600 --> 08:52.560
essentially work symbiotically with autonomous vehicles. Also, I predict that there will still be

08:52.560 --> 08:59.680
environments in which AI will be lost and need a backup driver. Yet, we will need cars that

08:59.680 --> 09:05.600
have no steering wheel, and really no place for a driver, so they can be smaller and

09:05.600 --> 09:14.480
simpler, talk to each other, and even avoiding accidents as they communicate their location and speed,

09:14.480 --> 09:21.360
and if you have a blown tire, you will tell cars around you, so I envision all of these will happen,

09:21.360 --> 09:26.720
but one part of it that my partner Stanley who wrote the science fiction stories thought was

09:26.720 --> 09:34.400
interesting was what would be the life of a backup driver in that case, because if the car got

09:34.400 --> 09:40.640
into a natural disaster where the roads have disappeared, and you have to fall back on natural

09:40.640 --> 09:46.640
instincts of a human driver to survive and navigate how they incredibly dangerous zone,

09:47.920 --> 09:51.760
and obviously the passenger couldn't do it, there's no longer a steering wheel,

09:51.760 --> 09:58.720
so the solution would have to be a remote center where super drivers jump in from one disaster to

09:58.720 --> 10:05.600
another saving people's lives, and then the other interesting dramatic element is well what happens

10:05.600 --> 10:11.840
to the life of such a backup driver, would it create so much stress that they can't live with

10:11.840 --> 10:18.800
themselves because they will be watching people die from day to day, and so how would such an

10:18.800 --> 10:23.600
arrangement be made, so without giving away the story that's kind of the dramatic element and

10:23.600 --> 10:31.600
a technical element weaved into a story. That last note about the drivers and their welfare and

10:31.600 --> 10:38.000
their life, even though we're talking about a scenario 20 years from now that calls to mind

10:38.640 --> 10:44.960
the lives of folks that are working in like content moderation farms and centers today that are

10:44.960 --> 10:54.080
dealing with those kinds of issues, so I imagine that part of what you're trying to do is to

10:54.080 --> 10:59.600
point to future issues but also tie them to contemporary issues as well.

11:02.720 --> 11:08.480
And also there are other interesting dramatic elements, for example, how do you recruit such

11:08.480 --> 11:18.240
a amazing drivers, so part of the story is games are developed and then winners of these games

11:18.240 --> 11:24.640
sometimes teenagers would be approached to see if they would be a backup driver, but of course

11:24.640 --> 11:32.960
that's too much psychological burden for a young teenager to be put into the position of

11:32.960 --> 11:44.320
helping and saving lives, so is it morally a problem to package a real job saving people's lives

11:44.320 --> 11:49.760
as a game and not disclose it to the teenager who happens to be the best backup driver that

11:49.760 --> 11:57.360
one can find, so we're saving lives to the purpose but can you lie to a teenager who's known

11:57.360 --> 12:03.680
to be saving lives but also not fair to put a psychological burden for them to know they're

12:03.680 --> 12:10.880
saving and not saving lives every day and do you tell them or don't you tell them it's a moral dilemma.

12:12.720 --> 12:17.680
And do you answer those questions or do you just raise them?

12:18.960 --> 12:25.600
We just raise them but I think the endings of the stories would give away how we feel

12:25.600 --> 12:32.400
but we don't want to but I think it's an issue where reasonable people can and will disagree

12:32.400 --> 12:37.440
so we don't presume that we know the right answer but I think we need to be aware such challenges

12:37.440 --> 12:44.960
will come up and the book is probably for the people watching this podcast the book is less

12:44.960 --> 12:49.760
about learning about technologies because you probably know most of what I have to say

12:49.760 --> 12:56.320
but but thinking ahead about the externalities and implications that are up ahead and what we

12:56.320 --> 13:02.320
technologists can possibly do about it to educate people and also to develop solutions.

13:05.520 --> 13:13.520
One more question on the autonomous driving scenario you mentioned that you fully expect and

13:13.520 --> 13:23.680
that the story presumes level 5 autonomy is in existence in 20 years does the the book or your

13:23.680 --> 13:35.040
analysis project a degree of deployment or the degree to which it is in use at that time?

13:35.040 --> 13:43.920
Yes yes I think the presumption is that in developed countries it's already popularly in use

13:44.480 --> 13:52.080
and in countries that are proactively changing its transportation ecosystem it gets deployed

13:52.080 --> 13:58.240
earlier and that's part of the technological prediction and hypothesis it's also predicting

13:58.240 --> 14:04.800
that developing countries and underdeveloped countries would need the help of developed countries

14:05.440 --> 14:12.960
to put this technology into place so this story takes place in Sri Lanka and in the story Sri

14:12.960 --> 14:20.320
Lanka gets help from a Chinese technological company that is building a business out of helping

14:20.320 --> 14:28.800
developed countries move from not having autonomous to autonomous and I think part of the

14:28.800 --> 14:35.600
implication is that large countries will continue to have more advanced AI to put into other

14:35.600 --> 14:43.040
countries and another assumption is the world will move towards autonomy one tier at a time

14:43.040 --> 14:51.840
and that I feel first tier of countries may have it in the 15 year time frame with other countries

14:51.840 --> 14:58.080
coming later 20 years plus and Sri Lanka was chosen at the place because not all the roads are

14:58.080 --> 15:05.760
yet quite ready for autonomy with some very backward environments because it would not be

15:05.760 --> 15:11.760
reasonable to put that scenario in US or China where by then I think the infrastructure as well

15:11.760 --> 15:18.800
as the technology would perhaps have a very more minimal use of backup drivers by 20 years

15:22.560 --> 15:30.640
you know it's hard to talk about AI in the future without raising the question of jobs and job

15:30.640 --> 15:37.760
displacement it's probably one of the you know issue of one of the issues he was around which

15:37.760 --> 15:46.960
there's the most concern when talking about the future use of AI do you take that up in you know

15:46.960 --> 15:52.320
one particular story in the book or is this something that cuts across various stories

15:54.080 --> 16:01.200
it cuts across all the stories there are probably three stories in which this covers

16:01.200 --> 16:09.600
the one that's squarely on the topic is a story called the job savior and it's a story that

16:09.600 --> 16:20.320
takes place in the US by watching phases and phases of routine jobs being taken over by AI a new

16:20.320 --> 16:28.960
profession arises called a job reallocator and it's a company that would be funded either out

16:28.960 --> 16:36.080
of government funding instead of paying social security or universal basic income the company

16:36.080 --> 16:43.280
would take out take the funding and basically solve the problem of retraining and redeploying

16:43.280 --> 16:50.640
workers whose jobs are being displaced by AI and this company faces some significant challenges

16:50.640 --> 16:57.120
one is that AI is improving capability so more and more routine jobs are being lost people are

16:57.120 --> 17:02.960
being retrained the three years later losing job again another major challenge that it faces

17:03.760 --> 17:11.200
is that many entry jobs are being hollowed out because AI can do jobs of an entry level accountant

17:11.200 --> 17:17.440
entry level architect entry level reporter but how do you advance people's careers and maintain

17:17.440 --> 17:25.840
their motivation to grow and learn without having entry level jobs so it brings up the possibility

17:25.840 --> 17:33.280
of creating a virtual job in which the person thinks he or she is working but perhaps is only

17:33.280 --> 17:41.040
training is more like a training wheels not creating value to the economy but the training

17:41.040 --> 17:46.800
will will help point out people's individual talents so they can be read that redirected later

17:46.800 --> 17:55.440
so again a potentially a moral question of is it okay to let people work jobs which are not

17:55.440 --> 18:01.840
real or maybe are real but could be done by AI and then how would the job reallocator deal with

18:01.840 --> 18:09.680
these challenges so that's the more direct one there is another one on education is how would AI

18:09.680 --> 18:18.960
be evolved so that it helps young people of home their soft skills skills like communication

18:18.960 --> 18:26.080
teamwork and human to human interaction as well as train their creativity and critical thinking

18:26.080 --> 18:32.160
which become all the more important because those are the skills AI cannot replace and also find

18:32.160 --> 18:39.200
the voice of each individual person so that's kind of related to routine jobs being displaced

18:39.200 --> 18:45.520
so people either have to find something AI cannot do or do things that only humans can do or find

18:45.520 --> 18:51.840
something the individual is good at so that's another story a third story has to do with human

18:51.840 --> 18:59.280
motivation it's it's more of a utopian outcome where so much money is being generated in an era

18:59.280 --> 19:07.760
of plenitude where not only does AI do our much of the routine work for us but also the energy

19:07.760 --> 19:13.440
costs have come down with green energy new materials the cost of goods are reduced and the

19:13.440 --> 19:21.520
meaning of money needs to evolve so it asks the question the work isn't the work and money isn't

19:21.520 --> 19:32.000
just a something to keep us busy but it's kind of people's motivation and and reason for living

19:32.000 --> 19:40.000
so if jobs are largely gone routine jobs are largely gone how do people remain motivated so I think

19:40.000 --> 19:47.600
it pushes the question into why do we live in this world perhaps it isn't just for work and pay

19:47.600 --> 19:58.320
but also for for self-actualization finding what our lives are about and can those be measured

19:58.320 --> 20:06.000
somehow can AI somehow measure whether we are improving ourselves where we're happily growing

20:06.000 --> 20:12.080
where we're finding where we're creating more positive energy because as you move up the

20:12.080 --> 20:21.200
mass law hierarchy it's not just about subsistence and not just about money for security but also

20:21.200 --> 20:29.440
about love and empathy and companionship so can those things be measured and can people have

20:29.440 --> 20:37.760
some kind of metric to improve a different metric than money and a different metric by job by

20:37.760 --> 20:47.120
spending their time perhaps in sustainability volunteer jobs companionship as well as all the

20:47.120 --> 20:55.120
creative professional jobs and it's an exploration of how that could develop in in the market in the

20:55.120 --> 21:03.440
country in this case Australia which is doing a pretty good job in in energy efficient energy

21:03.440 --> 21:11.280
that it might create enough of a small enough population to create and a lot of natural resources

21:11.280 --> 21:18.240
to create a the first science the first economy of where universal basic income

21:18.240 --> 21:24.800
plenitude and the moving people the higher purpose might be explored as an experiment kind of the

21:24.800 --> 21:31.520
gamification of life purpose in a sense that's right I mean our life is a gamification now I mean

21:31.520 --> 21:41.360
money is a virtual it's a silly virtual tool that keeps us you know in the rat race when it's

21:41.360 --> 21:48.720
it's you know it's a fabricated human story and we're playing a big game now in chasing fame

21:48.720 --> 21:56.080
and wealth so I think we need to find another which is perhaps more motivating it's interesting

21:56.080 --> 22:03.920
you're hearing you talk about these these stories and reflecting on the other ones the book kind of

22:03.920 --> 22:12.080
walks this line between you know presenting these potentially dystopic scenarios kind of like

22:12.080 --> 22:20.880
black mirror-esque but you know trying to pull out I think trying to pull out an optimistic note

22:21.680 --> 22:28.960
and for the most part you know to talk more about your your broad perspective on the book

22:28.960 --> 22:35.520
are you you know are you kind of going into these stories looking specifically for the the

22:35.520 --> 22:42.000
optimistic ending or you know does it vary do you have kind of different takes on where we'll go

22:42.000 --> 22:52.080
for different scenarios yeah I'm a huge fan of black mirror and if you know if the book reviews

22:52.080 --> 22:58.720
with find this book to be similar to the black mirror but more positive I there's nothing that would

22:58.720 --> 23:06.320
please me more I think the black mirror does a great job describing possible dangers and they

23:06.320 --> 23:13.200
usually end up with bad endings sometimes good ending so this so this is our effort to try to

23:13.200 --> 23:20.400
also describe the challenges that could arise and but also I want to go an extra step to say

23:20.400 --> 23:27.520
there could be a solution if only right if only we educated our kids differently if only we

23:28.400 --> 23:35.200
regulated large companies in particular ways if only we thought ahead about job displacement

23:35.200 --> 23:42.560
and provide the training if we deeply understand the meaning of money and how we can gradually

23:42.560 --> 23:52.800
move towards a substitute so and and I think probably six stories or so have a happy ending and then

23:52.800 --> 24:00.320
three have an ambiguous ending and one has a somewhat bad ending so I'm not I'm not being naive

24:00.320 --> 24:06.000
to say all the problems will be hand-waved away and solved so yeah really their challenges yeah

24:06.000 --> 24:14.560
yeah I think there's there are challenges with both the dystopic ending and the the utopian ending

24:14.560 --> 24:21.280
the optimistic ending and I think you almost want a you know not quite a choose your own adventure

24:21.840 --> 24:30.880
type of a story but a story that you know where there's a branch that says what the dystopic ending

24:30.880 --> 24:38.000
could look like and what are some of the levers that could put you in down that path and also

24:38.000 --> 24:44.240
presents the optimistic ending and you know what are some of the levers that would kind of drive

24:44.240 --> 24:51.440
society towards an optimistic perspective do you do you take on any of that in your analysis of

24:51.440 --> 25:01.680
the the various scenarios I do so so in the chapter about autonomous weapons it's a story it's

25:01.680 --> 25:08.480
an issue that I think a lot of people in the AI community feel the same way that the physics

25:08.480 --> 25:14.560
community felt about nuclear weapons and the chemistry community felt about chemical weapons

25:14.560 --> 25:24.720
and so on so it is the the the clearest challenge that we face today because the cost of making

25:24.720 --> 25:31.840
an autonomous drone with face recognition that can kill an individual as an automated assassin

25:32.400 --> 25:38.480
that has so many dangers because it lowers the cost of it for the terrorists without having

25:38.480 --> 25:44.800
to risk the terrorists lives and also it's very hard to to regulate because it's not like nuclear

25:44.800 --> 25:51.120
weapons they're the good and bad thing about nuclear weapons is there is a principle of a short

25:51.120 --> 25:56.960
mutual destruction so people have a deterrent countries don't do it because they're afraid of

25:56.960 --> 26:01.920
retaliation and mutual destruction but that isn't the case with autonomous weapons

26:01.920 --> 26:07.760
so so the one of the stories is about a terrorist model after the

26:08.640 --> 26:16.400
unabomber and who decided to take revenge on a particular group of people in this case the elites

26:16.400 --> 26:27.520
of the world as this model after the the unabomber so unabomber sorry so so the story ends up with

26:27.520 --> 26:36.480
challenges of what happens when autonomous weapons are not regulated and the outcome is

26:38.160 --> 26:42.720
somewhat negative one and the world isn't destroyed but it's still a somewhat negative one he

26:42.720 --> 26:50.240
gets away with something of course and and of course he's caught but he creates causes a lot of

26:50.240 --> 27:00.240
damage so in the explanation section I go into detail explaining the additional challenges

27:00.240 --> 27:07.360
of autonomous weapons compared to conventional weapons compared to you know as well as the

27:07.360 --> 27:12.560
nuclear weapons and why they really have to be regulated and what happens when you don't

27:12.560 --> 27:18.880
regulated and and then I point out the challenges of regulating it because unlike nuclear weapons you

27:18.880 --> 27:26.800
can't have UN going to a country and inspect uranium or nuclear facilities because someone can

27:26.800 --> 27:32.480
build this in their garage but nevertheless regulation must take place and I point out a few

27:32.480 --> 27:39.280
possible ways of regulating it as well as most AI scientists believe that it should be

27:39.280 --> 27:48.640
regulated several letters have been written and and also the consequences of not not regulating it

27:48.640 --> 27:56.560
so so I think that describes the both both possible paths of a negative outcome and that's an

27:56.560 --> 28:06.560
example where both paths are are explored. In a section like that where you're talking about

28:06.560 --> 28:15.680
something that's you know very clear and present danger do you is there a concrete call

28:15.680 --> 28:21.600
the action for folks is that part of your perspective here to tell folks how to

28:21.600 --> 28:26.400
you know if this is the issue that they're most passionate about where do they go?

28:26.400 --> 28:37.120
Yeah no it's not a direct call to action but I think it hopefully removes all ambiguity

28:38.000 --> 28:44.160
arguments have been made that autonomous weapons are in the early phases of development

28:44.160 --> 28:49.680
so it's too early to regulate them and I give counter example on why that isn't the case

28:49.680 --> 28:57.280
and also there are huge issues about how difficult it is to regulate it but I also point out

28:57.280 --> 29:04.400
that we mankind have managed to largely control and contain chemical weapons and biological

29:04.400 --> 29:11.040
weapons which are potentially equally difficult to to track and regulate so if we can do those

29:11.040 --> 29:16.880
we should be able to do this one so I think people can draw their own conclusions some are

29:16.880 --> 29:21.440
probably perhaps still not convinced by the argument but I wanted to to make the argument.

29:23.040 --> 29:30.240
I've often had these exchanges with folks where you know they kind of present this scenario of

29:31.440 --> 29:39.360
you know conscious AI that is belligerent in some way kind of like Terminator type of example

29:39.360 --> 29:48.080
and you know or like a you know a Nick Bostrom superintelligence that's potentially you know

29:48.080 --> 29:55.680
dangerous and you know often we'll say you know that is something that is potentially out there

29:55.680 --> 30:03.600
in the future but you know autonomous weapons are much more kind of clear and closer and

30:03.600 --> 30:12.320
concrete and scary for me personally than you know some AI that's you know acting on its own

30:12.320 --> 30:21.280
against human interest. Yeah absolutely I totally agree that's why by absence there is no

30:21.280 --> 30:28.000
singularity story in the book there is no AI with self-consciousness self-awareness that tries to

30:28.000 --> 30:35.040
destroy the human race in the story so by its absence I'm totally agreeing with that in the

30:35.040 --> 30:41.040
analysis section I do bring up the issue of singularity of why I don't rule out its possibility

30:41.040 --> 30:48.080
in the future but I think it's too simplistic to say the exponential growth of compute power

30:48.080 --> 30:53.920
also means an exponential growth that will drop the people behind in many of the stories we

30:53.920 --> 31:01.440
still see many parts of the human intelligence that cannot be replicated by AI the fact that

31:01.440 --> 31:08.160
many stories are saved by the saved by the hero or heroine of the story because of their

31:08.960 --> 31:16.160
emotional and beliefs and conviction and love and that's something unique to people and

31:16.720 --> 31:22.720
and also in stories where there are villains who do terrible things they're the ones who

31:22.720 --> 31:30.800
cause the disaster using AI as a tool AI never in these 10 stories become the villain in itself

31:30.800 --> 31:38.960
and and and then I clearly explained that singularity could happen when breakthroughs in algorithms

31:38.960 --> 31:47.520
enable a fully taking advantage of the exponential growth but today we still have not had breakthroughs

31:47.520 --> 31:54.880
that understand how our brain works why we have self-awareness and emotion and creativity

31:54.880 --> 32:01.520
and can do analysis and strategic thinking so to think we can replicate AI on something that we

32:01.520 --> 32:08.960
don't even know how we do it nor do we see AI approaching it and we know AI do not possess it

32:08.960 --> 32:15.760
so we we don't have to worry about extrapolating the exponential curve and seeing super

32:15.760 --> 32:22.400
intelligence and or singularity within the next 20 years that said I do believe the set of

32:22.400 --> 32:28.400
things that AI can do better than human will grow dramatically and AI will do many things that

32:28.400 --> 32:35.200
humans cannot imagine to do but there will always be a set that is about our core humanity

32:35.200 --> 32:41.200
at least in a 20 year time frame that we can hold on to and it's exactly that set that defines

32:41.200 --> 32:47.360
our humanity that causes are the stories the people in the stories to shine and save the day.

32:49.760 --> 32:56.320
Speaking of the exponential growth in compute one of the stories touches on quantum computing

32:56.960 --> 33:04.240
what you take on where that is in 20 years and the degree to which it enables a more powerful

33:04.240 --> 33:14.160
artificial intelligence. Yeah I think quantum is one of the areas where I needed to make a

33:14.160 --> 33:21.840
not 100% confident prediction right because there's too much understanding and variability

33:23.040 --> 33:29.040
but but I do think looking at the maps that IBM, Google and other companies have

33:29.040 --> 33:33.680
and the progress that's been made particularly in the last two years it seems like we can

33:33.680 --> 33:45.200
extrapolate a story where the improvements in logical cubits will reach thousands probably

33:45.200 --> 33:50.880
less than 20 years there are a lot of issues of how do you maintain stability and how many physical

33:50.880 --> 33:57.680
cubits do you need to support logical a few thousand logical cubits so I'm not an expert in the

33:57.680 --> 34:04.000
area but the experts seem to agree several thousand cubits are possible and there are useful

34:04.000 --> 34:13.120
applications by that time with the major one being insecurity that is the existing asymmetrical

34:13.120 --> 34:21.760
cryptography algorithms will no longer work the flip side of that is quantum computing will provide

34:21.760 --> 34:30.400
a new unbreakable security system so in the story I did not go into how quantum and AI work

34:30.400 --> 34:37.520
together because at a few thousand cubits I don't think it's enough to disrupt AI completely yet

34:37.520 --> 34:44.880
so 20 years would be about when the security challenges would come up so in one of the stories

34:44.880 --> 34:52.880
the villain achieved 4,000 cubits without anyone else knowing it and the villain went after

34:53.520 --> 35:01.360
stealing bitcoins which is one commonly described the largest bank that's waiting to be robbed

35:01.360 --> 35:08.720
so that was a part of the story but in the analysis I do go into the very nature of quantum

35:08.720 --> 35:15.600
that can hold uncertainty in its head and pursue paths in parallel and dramatically reducing

35:15.600 --> 35:22.320
the MP complete search problem will lead to a day where AI algorithms will be disrupted

35:22.960 --> 35:28.880
but I don't think 20 years is quite when that will happen will probably take longer with more than

35:28.880 --> 35:40.800
4,000 cubits so the book of course is focused on this 20 year you know 20 year forward time horizon

35:40.800 --> 35:49.280
but there are a lot of AI technologies around which there are very contemporary issues in the

35:49.280 --> 35:56.080
realm of ethics bias and others computer vision is one that comes to mind facial recognition in

35:56.080 --> 36:05.600
particular it's a very contemporary issue the use of facial recognition by police organizations

36:05.600 --> 36:14.000
the proliferation of cameras in you know quote unquote smart cities you know a lot of people

36:14.640 --> 36:21.120
look at the you know it would be easy to look at the situation now and the frustration that many

36:21.120 --> 36:27.520
people have with the situation now and find it difficult to project forward 20 years do you do that

36:27.520 --> 36:35.920
in the book I do not facial recognition is one I did not speculate because we're in a bifurcated

36:35.920 --> 36:44.800
world some countries are attempting to regulate others are not and it's it's not clear a bifurcated

36:44.800 --> 36:52.880
world can work hopefully will reach a universal consensus at some point I do go into many other

36:52.880 --> 37:01.120
aspects of externalities and I guess you can extrapolate from them to to all of the possibilities

37:01.120 --> 37:09.360
so for example I talk about how objective functions need to be improved to go from

37:09.360 --> 37:18.080
maniacally focusing on something like clickthroughs and revenue generating moving into longer term

37:18.080 --> 37:25.440
metrics so our social media in 20 years ought to be showing us content that is making us better

37:25.440 --> 37:32.240
over time that that we feel we're seeing content that is time well spent as Tristan Harris would

37:32.240 --> 37:39.040
say or we're seeing content that is making us improving in some metric maybe it's our wealth

37:39.040 --> 37:45.840
maybe it's our happiness maybe it's our how much knowledge we've gained and whether AI

37:45.840 --> 37:52.720
objective functions can be turned more long term and more aligned with humans that's one aspect

37:52.720 --> 37:59.280
I explored and I think technologists should spend more time on topics like that another is on

37:59.280 --> 38:09.600
bias and fairness can we ensure that have tools that ensure that AI is being trained reasonably

38:09.600 --> 38:17.040
balanced data so that it's not discriminating against any race gender individual etc.

38:18.240 --> 38:26.880
And and also can can compilers alert warnings right now can AI tools do the same

38:26.880 --> 38:33.840
and also can AI engineers be trained to be aware of the substantial power that they control

38:33.840 --> 38:40.720
and therefore the responsibilities that must come with it so that's another aspect another one

38:40.720 --> 38:48.480
related to privacy and and and and having our cake and eat it too can we have AI trained on a lot

38:48.480 --> 38:55.360
of data but not everybody giving away data privately without consent so the stories in the book

38:55.360 --> 39:02.720
talks about how technologies like federated learning homomorphic encryption and also

39:03.360 --> 39:10.560
hardware environments that are self-contained where where data does not leak can these take these

39:10.560 --> 39:15.920
technologies I predict in 20 years we'll be able to let us have our cake and eat it too

39:15.920 --> 39:24.960
so that our data stays in devices to which we permit say our phone our computer or the computers

39:24.960 --> 39:31.600
at the hospital which has our data but not beyond that so the models from a hospital is trained

39:31.600 --> 39:37.920
on all the patients in that hospital who licensed their data to the hospital but not beyond

39:37.920 --> 39:45.440
then the hospitals can jointly train by pulling their models together so so in the book I

39:45.440 --> 39:50.720
point out the technological areas that I think are promising and the possible technological

39:50.720 --> 39:57.760
solutions that could end up addressing many of the problems we see and I think the call to action

39:57.760 --> 40:04.480
is for the technologists who read the book and watch the podcast to think about whether rather than

40:04.480 --> 40:12.480
doing research on the next deep learning or tweaking a particular model is it useful for a sufficient

40:12.480 --> 40:19.840
percentage of the AI community to think about these technological solutions that solve the problems

40:19.840 --> 40:31.440
caused by our technology AI. On that note there are a number of labs focused on AI

40:31.440 --> 40:39.280
safety as a research focus you know often they take the perspective of you know trying to

40:39.280 --> 40:44.800
prevent the terminator scenario or making sure that we can control the terminator scenario.

40:44.800 --> 40:52.240
Do you have a perspective on on those efforts are they asking the right questions looking at

40:52.240 --> 41:01.680
the right things? Well there are risks that are clear and present danger I think those ought to

41:01.680 --> 41:11.600
be addressed by the largest number of people issues related to bias fairness how to have our

41:11.600 --> 41:18.240
cake in either two with respect to personal data there are a longer term lower likelihood

41:19.520 --> 41:25.120
existential questions that one could ask and I think it makes perfect sense for a small number

41:25.680 --> 41:34.880
of people in more like a think tank than a technology development to basically watch

41:34.880 --> 41:40.800
for that possibility and to alert for the rest of us so yes I do think those labs should

41:40.800 --> 41:46.480
continue to do what they do I don't think those existential threats will happen in the next 20

41:46.480 --> 41:52.960
years but I think we should have think tanks that think about them and tell us when we really do

41:52.960 --> 42:03.600
need to get involved and worried. In the chapter on autonomous weapons you call for regulation

42:03.600 --> 42:12.240
you mentioned the objective function and you know there are many contemporary calls for regulation

42:12.240 --> 42:21.120
of internet companies and advertising methods and you know privacy and many other things

42:21.120 --> 42:27.680
for internet companies but how do you see regulation evolving over the next 20 years?

42:27.680 --> 42:36.400
Yeah I think in the US people talk the most about breaking up companies I think that is

42:36.400 --> 42:44.720
a two brute force and two you know 20th century it's not something designed for this kind of

42:44.720 --> 42:51.200
monopoly I do think regulations are needed but I think we need to come up with newer and better

42:51.200 --> 42:59.280
regulations and why do you think that's an effective for modern companies? Well let's say

42:59.280 --> 43:06.960
Facebook got broken up into whatsapp.com and instagram.com and facebook.com it doesn't stop any of it

43:06.960 --> 43:13.520
it wouldn't have stopped the Cambridge Analytica issue right it wouldn't stop any one of the three

43:13.520 --> 43:19.600
products doing things that we don't want them to do it would reduce it but it's it's too brute force

43:19.600 --> 43:28.800
it was specifically addressing monopoly extension by you know standard oil moving into gas gas

43:28.800 --> 43:38.080
into gas stations and stop yeah and having the big bell company break broken up into baby bells

43:38.720 --> 43:44.400
I think those were perhaps appropriate for telecommunications or traditional industries

43:44.400 --> 43:54.160
but I think the issue that is fundamental is I think people the reason people go into such extreme

43:54.160 --> 44:00.960
measures is they've given up hope that some companies can self-manage I've not given up hope

44:00.960 --> 44:08.400
but I don't think today's reward and punishment systems give the large companies any incentive

44:08.400 --> 44:15.200
to self-manage and those need to be created for example I think an idea called the AI audit

44:15.200 --> 44:20.640
is something that could be pursued right it's very clear that the government can't go in and

44:20.640 --> 44:27.440
and look at the code and data for each of the large internet companies but when there are

44:28.320 --> 44:34.160
sufficiently serious complaints and repetitions of complaints there can be an audit just like

44:34.160 --> 44:40.880
there can be a financial audit or a tax audit so with that as a deterrent I think companies

44:40.880 --> 44:46.480
can be better behaved of course what are the metrics how does a complaint count should the

44:46.480 --> 44:51.200
government get to look at the data in a large company these are all issues that need to be solved

44:51.200 --> 44:57.600
but it seems more I think a more plausible way than just breaking up companies and more effective

44:57.600 --> 45:06.880
another I think ultimately we need to get companies to really have aligned financial

45:07.680 --> 45:14.880
incentives so that if they better behave for example can there be a third party watchdog

45:14.880 --> 45:22.240
that publishes how much fake news how much you know false advertising how much wrong search results

45:22.240 --> 45:27.840
or whatever things we have if we have a third party watchdog that publishes those and

45:28.640 --> 45:35.520
enough consumer advocacy and a corporate ESG pressure for the companies to feel like

45:35.520 --> 45:40.880
every quarter they have to report not just the financial results but how they do on the

45:41.600 --> 45:48.560
you know a fake news metric fake news ranking then they're going to form internal teams

45:48.560 --> 45:54.640
because they're being monitored from the outside so that would be one example but the ideally

45:56.000 --> 46:03.840
we want to somehow have companies that can make even more money by aligning themselves with

46:04.400 --> 46:14.480
user needs so as users become happier or learn more information or become wealthier or whatever

46:14.480 --> 46:21.840
those metrics and it can somehow be attributed to companies that have created or helped

46:21.840 --> 46:28.960
enable that situation then it can make more it can even make even more money so

46:28.960 --> 46:33.760
so in other words are we willing to pay a company a lot more money to make us

46:34.880 --> 46:42.560
more knowledgeable wealthier or happier in a three year horizon compared to the money that

46:42.560 --> 46:52.080
the company would make us clicking and buying things so looking at natural financially aligned

46:52.080 --> 46:58.720
metrics that connect the user and the the company this is a little bit abstract at right now

46:58.720 --> 47:03.120
I was going to ask you say destiny such metrics in the book I suspect answers now

47:04.320 --> 47:11.040
no well it's such like you know 30 years ago would people have come up with the ways that

47:11.040 --> 47:18.160
you know Facebook does advertising or Google does at words or at sense some smart entrepreneur

47:18.800 --> 47:26.720
will come up with some system that will create a new ecosystem that will create a new set of

47:26.720 --> 47:31.920
companies that are even more profitable than Google and Facebook unfortunately I don't I don't

47:31.920 --> 47:38.000
know the answer but entrepreneurs and VCs can take a step back and think about it it's not so much

47:38.000 --> 47:43.600
out of a question right because how do you how do we measure people's happiness well there are

47:43.600 --> 47:50.560
many metrics that can be used on our facial expressions micro expressions measures of our hormones

47:51.520 --> 47:58.800
endorphin etc that could be one beginning of such a way our our wealth can be measured over time

47:59.760 --> 48:06.560
and whether we've learned something and grown I think just like you know gpt3 today can remember

48:06.560 --> 48:11.040
millions of words that it's read and pick out the ones that are relevant for the given current

48:11.040 --> 48:18.080
context perhaps there will be AI that can look at all of our time spent on the internet and pick out

48:18.080 --> 48:24.800
the epiphany moments that have caused us to grow and and and those moments if there is a software

48:24.800 --> 48:31.520
technology or objective function that enable the moment they should be properly compensated for it

48:31.520 --> 48:36.320
so I don't think it's out of the question um the technologies can be developed but I don't know

48:36.320 --> 48:42.000
what the model is if I did I'd be either funding or creating that company myself I was just

48:42.000 --> 48:48.000
going to ask was that part of your motivation for writing the book to kind of signal to

48:48.720 --> 48:54.720
entrepreneurs that hey these are areas that need to be explored and if you are working in these

48:54.720 --> 49:03.200
areas hey reach out to reach out to me that's not the primary purpose but if that were a side

49:03.200 --> 49:10.560
effect I would be happy to look at those business plans awesome awesome well kayfoo thanks so much

49:10.560 --> 49:18.720
for taking the time to chat with us and share a bit about the book congratulations on the book it

49:18.720 --> 49:27.280
is really takes an interesting approach at raising some very important questions in the development

49:27.280 --> 49:57.120
of AI so thanks and and congrats once again on that thank you thanks Sam for having

