WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.920
I'm your host, Sam Charrington.

00:31.920 --> 00:36.200
In today's show, we're joined by Navid Ahmad, Senior Director of Data Engineering and

00:36.200 --> 00:38.920
Machine Learning at Hearst Newspapers.

00:38.920 --> 00:43.760
A few months ago, Navid gave a talk at the Google Cloud Next Conference on how publishers

00:43.760 --> 00:46.640
can take advantage of machine learning.

00:46.640 --> 00:51.360
In this conversation, we dive into the role of ML at Hearst, including their motivations

00:51.360 --> 00:56.160
for implementing it and some of their early projects, the challenges of data acquisition

00:56.160 --> 01:00.880
within a large organization, and the benefits they enjoy from using Google's BigQuery as

01:00.880 --> 01:02.720
their data warehouse.

01:02.720 --> 01:03.720
Enjoy the show.

01:03.720 --> 01:04.720
All right, everyone.

01:04.720 --> 01:07.040
I am on the line with Navid Ahmad.

01:07.040 --> 01:12.480
Navid is Senior Director of Data Engineering and Machine Learning at Hearst.

01:12.480 --> 01:15.760
Navid, welcome to this weekend machine learning and AI.

01:15.760 --> 01:16.760
Thank you.

01:16.760 --> 01:18.360
Thank you for having me on your show.

01:18.360 --> 01:22.200
So you've been in the publishing industry for about 10 years now.

01:22.200 --> 01:26.960
Can you tell us a little bit about both your current role and some of the past things

01:26.960 --> 01:28.960
you've done in publishing?

01:28.960 --> 01:29.960
Sure.

01:29.960 --> 01:33.840
So I'm currently a Senior Director for Data Engineering and Machine Learning at Hearst,

01:33.840 --> 01:36.240
being here for about two years.

01:36.240 --> 01:44.240
My rule has been building the data warehouse, building personalization, and also doing

01:44.240 --> 01:48.960
predictive analysis using that data.

01:48.960 --> 01:54.120
And before Hearst, I was at New York Times.

01:54.120 --> 02:03.120
I worked in the subscription space where I'm working on the CRM system, and also some

02:03.120 --> 02:12.400
of the aspects of machine learning, like churn modeling and email content detection.

02:12.400 --> 02:19.640
And before that, I was in Thomson Reuters, where I also built their data and used a distribution

02:19.640 --> 02:26.960
platform, as well as part of their CMS team contributed to recommendation systems.

02:26.960 --> 02:30.960
And before that, I was mostly in telecommunication.

02:30.960 --> 02:38.120
And so you recently presented at the Google Cloud Next Conference on Applied Machine Learning

02:38.120 --> 02:41.600
for Publishers.

02:41.600 --> 02:47.960
Before we jump into that topic, maybe you should take a second to provide an overview of Hearst

02:47.960 --> 02:51.560
for those who aren't familiar with the company.

02:51.560 --> 02:52.560
First.

02:52.560 --> 02:56.680
So Hearst, I especially work in Hearst newspaper department.

02:56.680 --> 03:05.680
Hearst is a very large organization with more than 300 businesses, which includes magazines,

03:05.680 --> 03:15.240
investment, and television channel, and within even newspapers, there's about 40 plus websites,

03:15.240 --> 03:21.120
which includes names like San Francisco Chronicle and Houston Chronicle and Times Union.

03:21.120 --> 03:29.160
It is, and Hearst had quarters located here at 57th Street in New York.

03:29.160 --> 03:31.720
And that's some background about Hearst.

03:31.720 --> 03:38.360
So one of the things that you mentioned in describing your background and in our conversation

03:38.360 --> 03:47.720
before the interview started was the role of the data warehouse and enabling you to

03:47.720 --> 03:54.680
perform the types of machine learning that you want to be able to perform at Hearst.

03:54.680 --> 04:01.680
Can you talk a little bit about the data warehouse and the process for establishing it?

04:01.680 --> 04:02.680
Yeah.

04:02.680 --> 04:11.680
So one of the first tasks for me when I joined Hearst was to build a data warehouse, especially

04:11.680 --> 04:18.720
I used Google BigQuery as a tutorial data warehouse.

04:18.720 --> 04:28.320
And the idea was that we need all the data sets in one place to be able to look at what

04:28.320 --> 04:36.040
is the relationship between newsletter and web and subscriptions.

04:36.040 --> 04:43.280
And the first use case was to build business and tell it on like regular reporting on

04:43.280 --> 04:45.120
top of this data.

04:45.120 --> 04:52.320
And the same data can be used to, that's used to give data reports about current status

04:52.320 --> 04:55.440
can be used to do predictive modeling.

04:55.440 --> 05:05.440
So using BigQuery and basically we build our data sets include Google Analytics, be our

05:05.440 --> 05:12.160
content, our newsletters, so all sorts of data related to our business is in BigQuery.

05:12.160 --> 05:15.600
And this forms as a foundation for machine learning.

05:15.600 --> 05:20.800
So the data warehouse is kind of central to your ability to perform machine learning and

05:20.800 --> 05:26.560
analytics and it was one of the first things that you established at Hearst.

05:26.560 --> 05:33.840
Can you talk a little bit about any challenges that you experienced trying to centralize all

05:33.840 --> 05:38.080
of the data from these various sources?

05:38.080 --> 05:45.760
The challenges were that our data was sitting in different formats like before the warehouse,

05:45.760 --> 05:54.720
would get either data from individual systems or there's Excel worksheets going around.

05:54.720 --> 06:02.880
So really figure out the data sources and then building a platform for ETL, that was

06:02.880 --> 06:04.120
great challenge.

06:04.120 --> 06:11.680
Some of the bigger data sources like Google Analytics and DFP, they were easier to get into

06:11.680 --> 06:18.960
BigQuery but some of the ones we have to build specific code to ETL that into BigQuery.

06:18.960 --> 06:20.720
And what is DFP?

06:20.720 --> 06:22.400
Double click for publishers.

06:22.400 --> 06:26.400
Okay, so that's an advertising, your advertising system?

06:26.400 --> 06:31.760
Yeah, this is all the log data for each advertisement that impressions.

06:32.640 --> 06:37.040
And so the data warehouse has your analytics.

06:37.040 --> 06:43.280
So the click stream data for people that are visiting the site, it's got information about

06:43.280 --> 06:48.480
the advertising, interactions and maybe those click streams.

06:49.040 --> 06:51.280
Does it also contain content information?

06:51.280 --> 06:55.200
Yeah, so all our CMS content is in BigQuery.

06:55.200 --> 06:59.760
All our newsletter, our bird data, that's in BigQuery.

06:59.760 --> 07:03.200
And is that data replicated to BigQuery for

07:03.200 --> 07:10.160
analytics or is that it's native place and you're publishing directly to and from BigQuery?

07:10.160 --> 07:12.800
Yeah, it is replicated.

07:12.800 --> 07:18.400
Like the other, we have a separate CMS system, it has our own database.

07:18.400 --> 07:20.960
So we're replicating that BigQuery, you know.

07:20.960 --> 07:26.320
And these are very different types of data.

07:26.320 --> 07:31.200
What's the goal for pulling this all into a single place?

07:31.200 --> 07:38.000
Yeah, so each of these data, they have identifiers that link them up with other datasets.

07:38.000 --> 07:44.160
For example, a newsletter with their hashed email address.

07:44.160 --> 07:47.600
You can link it up with subscription data.

07:47.600 --> 07:53.760
And from subscription data, we can link it up to Google Analytics data.

07:53.760 --> 07:57.600
So the idea is to be able to connect all the aspects of a user

07:57.600 --> 08:01.840
using these identifiers.

08:01.840 --> 08:07.360
It sounds like the primary focus from an analytics, a predictive analytics,

08:07.360 --> 08:14.880
and a machine learning perspective is based on this link data, this user-centric link data.

08:14.880 --> 08:16.080
Yes, absolutely.

08:16.560 --> 08:23.520
And so maybe let's talk through some of the different things that you've done with the data

08:23.520 --> 08:29.040
from a machine learning perspective. What are the different challenges that you're trying to address?

08:30.080 --> 08:35.920
So we built a few different machine learning-based products and predictive models

08:35.920 --> 08:38.160
using this data we have in BigQuery.

08:38.800 --> 08:46.560
So number one, the one we are currently is a churn model, churn prediction.

08:46.560 --> 08:54.160
The churn prediction is that be able to predict how likely a subscriber is to cancel their subscription.

08:55.120 --> 09:02.080
And it's very relevant in the media space because it's a bit a challenging environment to keep

09:02.080 --> 09:08.480
subscribers. Since we have subscription data, how people, the different attributes of their usage,

09:08.480 --> 09:18.160
behavior, how many newsletters they signed up to, also customer service data associated with

09:18.160 --> 09:26.880
each subscriber. And the way it works is we take like a year-old worth of data and take their

09:26.880 --> 09:34.880
attributes of month and feed it into a machine learning model. And we know that in the next month,

09:34.880 --> 09:41.040
how many of those subscribers were canceled and versus sudden cancel. So this is a binary

09:41.040 --> 09:48.240
classification problem. And using six months of data, we have training data of subscribers who

09:48.240 --> 09:53.600
didn't cancel versus cancel and we can build a predictive model on it. So one of the things

09:53.600 --> 10:01.440
that we've done is since we're on BigQuery, we've used this new, the launch feature called BigQuery

10:01.440 --> 10:10.800
ML, which was released in Google next back in August. And the great thing about BigQuery ML is that I

10:10.800 --> 10:18.800
can do a training and prediction model right using SQL syntax, i.e., you don't have to write code

10:18.800 --> 10:27.440
or Python code. And it does some of the like machine learning goodies, like normalization of

10:27.440 --> 10:39.360
features and also fine-tuning the model. And one of the things that I emphasize that this enables

10:39.360 --> 10:45.120
people who don't even have machine learning background, like people in business intelligence or

10:45.120 --> 10:54.000
who have background in SQL can easily write machine learning code just by using SQL syntax.

10:54.000 --> 11:02.080
So we built out our first model on San Francisco. And we're just working on

11:02.800 --> 11:11.120
integrating with our marketing and CRM systems. And so this was one of the use cases

11:12.160 --> 11:18.080
for machine learning. Then we've also built one of the things that I've done over here's use.

11:18.080 --> 11:23.280
If I could jump in before you go to the next one, one of the things that really struck me

11:23.280 --> 11:33.440
at the Google next conference and not to turn this into a BigQuery commercial, but I was really

11:33.440 --> 11:39.840
surprised by the enthusiasm for BigQuery. People seem to really love that database. Can you

11:40.960 --> 11:47.360
like maybe net out for me why folks are excited about it and the BigQuery ML piece that they just

11:47.360 --> 11:55.440
announced? Yeah. So in general, BigQuery is a fully managed data warehouse. So you don't have to

11:55.440 --> 12:05.680
have a DBA to fine-tune or optimize the database. And it's based on Google's own internal

12:05.680 --> 12:11.520
Dremel technology. What I've heard is heavily Dremel technologies is heavily used inside of Google.

12:11.520 --> 12:20.240
And so they expose that as BigQuery. And their philosophy is it's full scan. Any data set

12:21.440 --> 12:29.280
basically spawns off machines or compute in the back end and is able to very quickly get your

12:29.280 --> 12:36.880
results. Especially if you have large data sets and terabytes or petabyte scale, it only takes a

12:36.880 --> 12:44.560
few minutes to run SQL. So it's very easy in terms of maintenance. It's based on SQL syntax.

12:46.320 --> 12:54.480
That's why it's very an attractive option for data mining. And BigQuery ML, so they built out

12:54.480 --> 13:01.520
machine learning on top of BigQuery, which even further aids very fast because it's using the same

13:01.520 --> 13:10.080
compute infrastructure. And it abstracts out the machine learning as an SQL SQL.

13:10.800 --> 13:18.000
So it becomes, I foresee, that it'll enable machine learning for a lot of people, especially

13:18.000 --> 13:23.760
who are on BigQuery. And so it's the idea then that you'll have like, you know, where you might

13:23.760 --> 13:29.360
have aggregators in SQL like, you know, average or max or something like that, you can apply

13:29.360 --> 13:35.840
some kind of model to or is it different? Yeah, it's absolutely. So just like these functions,

13:36.560 --> 13:44.480
you mentioned they've introduced a few different functions to be able to train and predict and even

13:44.480 --> 13:51.200
get machine learning and metrics like once the machine learning model is built to get like,

13:51.200 --> 13:57.600
for example, what's the AUC of this machine learning or precision and recall. So you can do all

13:57.600 --> 14:04.640
of those things just by a function called with a SQL syntax. Yeah, as compared to using a tool like

14:04.640 --> 14:12.320
scikit-learn or TensorFlow, the other advantage is you don't have to take the data outside of the

14:12.320 --> 14:18.560
system. You do everything in one place. Like if you were using scikit-learn or TensorFlow, there's

14:18.560 --> 14:25.440
this whole process of extracting the data, massaging into a format that machine learning framework

14:25.440 --> 14:30.880
can understand, build a machine learning model, pull that data back into your data warehouse.

14:30.880 --> 14:37.680
So this whole cycle just gets reduced because you're doing everything in line and BigQuery.

14:37.680 --> 14:44.000
You were about to go into another use case beyond the well, actually back on the on the churn.

14:45.760 --> 14:50.240
Yeah, I'm curious. This isn't necessarily a machine learning question. You know, I've talked

14:50.240 --> 14:58.000
about churn prediction in many cases across many different industries and I understand broadly

14:58.000 --> 15:05.440
how it's applicable. But I'm curious in the case of a publisher, you know, what specifically

15:05.440 --> 15:12.160
is Hurst going to do on the business side once it's able to predict that a user has a

15:12.160 --> 15:20.320
high likelihood to churn? Yeah, that's a good question. And we're working with marketing and people

15:20.320 --> 15:27.280
in the subscription business. What are the different ways? So some of the ways is that we,

15:27.280 --> 15:33.040
in our marketing platform, we were able to give messaging and be able to send emails.

15:34.800 --> 15:41.040
Typically a user who is not engaged a lot are some of the people. Those are some of the attributes

15:41.040 --> 15:46.000
that are indicative that a person is going to cancel their subscription.

15:46.960 --> 15:52.880
So the email to nudge somebody, hey, you know, this, did you know that there's a certain feature

15:52.880 --> 16:00.640
or educating our print subscribers that they have free digital benefits are some of the things

16:00.640 --> 16:08.720
that they can, we can employ on getting people to less or cancel their subscription.

16:08.720 --> 16:16.560
You built this on BigQuery ML using this new SQL-like interface. Can you talk about how

16:17.280 --> 16:26.000
that experience of building these models using a SQL type of interface different from

16:27.040 --> 16:32.480
traditional approaches? You mentioned Scikit-learn, like how were they different? And I guess

16:32.480 --> 16:39.840
I'm particularly curious about the different skill level involved, but also any differences in

16:39.840 --> 16:45.200
the way you need to manage the models or the way you productize them, those kinds of things.

16:46.160 --> 16:51.200
Yeah, that's a good question. So one thing is getting started with BQML is really fast.

16:52.080 --> 16:59.200
I actually built the first prototype within a day. I got a very basic churn model up and running.

16:59.200 --> 17:10.080
I've done this before in my previous job using Scikit-learn and the difficulty really was

17:10.080 --> 17:17.120
getting aggregating data from different sources. Like in my previous job, getting data from

17:17.120 --> 17:25.680
like an Oracle database and, you know, a Hadoop database. So seemed like that took most of the time

17:25.680 --> 17:31.840
getting data from different data sources. Typically, the machine learning technology piece is the

17:31.840 --> 17:40.720
easier part. And the more time it's spent on getting the data and then figuring out the right

17:40.720 --> 17:52.960
features to use in our machine learning model. And so using BigQuery ML, getting used to the

17:52.960 --> 18:00.160
SQL syntax is, again, anybody with no SQL can be trained to do it within, I think, half a day.

18:00.720 --> 18:06.880
I think the tricky part is, like, for someone new is to learn some of the machine learning concepts.

18:06.880 --> 18:13.840
For example, if you train a machine learning model, how do you measure that it's, you've succeeded,

18:13.840 --> 18:24.080
like basic concept like precision recall and accuracy and AUC area and the curve. So those are more

18:25.280 --> 18:33.200
where more time is spent, like figuring out the features and then doing measurement of the result.

18:34.080 --> 18:42.000
And I feel like that doesn't differ from versus using Scikit-learn or BigQuery ML. The feature

18:42.000 --> 18:51.920
engineering and the measurement. But getting used to BigQuery ML is very quick. You don't have

18:51.920 --> 19:01.520
to use any language. And then the cycle I was talking about that data export, train model, data

19:01.520 --> 19:08.960
import, that cycle gets really reduced. So the number of experiments that you can do using BigQuery

19:08.960 --> 19:13.520
ML, you can do a lot more experiments with this technology.

19:14.320 --> 19:20.720
And is part of that because presumably they're scaling out the training behind the scenes and

19:20.720 --> 19:26.720
it's just a lot faster or are there other aspects to reducing that cycle time?

19:27.280 --> 19:34.880
Yeah, A is that it's a lot faster because of the BigQuery technology. And second is the

19:34.880 --> 19:41.520
what I mentioned before exporting data for machine learning model outside of the system,

19:41.520 --> 19:47.520
training it and fetching that data back in. So that gets significantly reduced.

19:48.400 --> 19:52.720
And the third one is in our case, since our data was all sitting in one place,

19:54.480 --> 20:00.400
we didn't have to incur this ETL cost of getting data from different sources. Everything was

20:00.400 --> 20:07.520
sitting in BigQuery. What are some of the other types of machine learning projects that you've

20:07.520 --> 20:14.960
done at Hearst? Yeah. So another one was application of natural language processing.

20:15.840 --> 20:23.360
There's a case study published back in November. I think if you just Google Hearst and Google

20:23.360 --> 20:30.800
that probably the first link. So we've applied Google's natural language processing to all our content.

20:32.080 --> 20:38.320
So each hour tag basically we're using two features of natural language. One is classification of

20:38.320 --> 20:45.920
content. Putting this into broad categories like if content is about food or wine or is it about

20:45.920 --> 20:54.080
real estate? And then also a more detailed version of it that it tags entities and the content.

20:55.280 --> 21:02.080
And the entities could be proper nouns or common nouns with their metadata,

21:02.080 --> 21:10.400
with their how much salient they are to that article and also their Wikipedia link for their

21:10.400 --> 21:19.920
popular entities. So we did that for all our content from all our websites and that stored in

21:19.920 --> 21:28.480
our CMS as well as also replicated into BigQuery for their analysis. So some of the use cases

21:29.680 --> 21:38.480
on this NLP data is so we built out BI business intelligent reports. For example, if you want to

21:38.480 --> 21:48.880
see how is for example a particular personality trending over time. So using Google NLP data and

21:48.880 --> 21:56.800
Google Analytics data in our content data in BigQuery we can build out reports. And also if you want to

21:58.320 --> 22:06.720
see which content categories get more traffic versus the content that we publish. Like which

22:06.720 --> 22:15.280
categories should we be focusing on for publishers? So we build all sorts of reports using these

22:15.280 --> 22:23.520
three data sets. So this is one of the use cases for Google NLP. The other one is which is more

22:23.520 --> 22:31.520
interesting is that we also pass like when we render our ads on thing we've integrated that with

22:31.520 --> 22:38.560
double click for publishers. So whenever an ad is rendered we also pass the key value pair

22:40.240 --> 22:46.240
of which category that ad belongs to. The category of the content that ads being displayed.

22:46.240 --> 22:54.720
So we built over a month this builds a database where we can say show me all the ads that are

22:54.720 --> 23:03.920
displayed on for example Olympics content or ads on our food contents. So if there's a new advertiser

23:03.920 --> 23:10.640
that comes on board and he says that I want to run a campaign on let's say basketball or

23:10.640 --> 23:19.360
Olympics content. We build this capability using our tag tagging technology. So they can just

23:19.360 --> 23:26.080
specify a criteria and double click for publisher said specify for this campaign advertise this

23:26.080 --> 23:34.960
campaign on all the content which is related to let's say tennis or basketball. So that's another

23:34.960 --> 23:40.080
use case for natural language processing. I know you've done some work both with Google's

23:40.080 --> 23:49.360
AutoML NLP that allows you to kind of fine tune some of their models but the classification was

23:49.360 --> 23:56.400
there off the shelf NLP service that you that one if I'm not mistaken you're not able to train

23:56.400 --> 24:04.320
that using your own data is that right? Yeah it's a good question. So the AutoML for NLP which is

24:04.320 --> 24:12.560
another tool released recently it works on top of the classification. So they give a

24:13.600 --> 24:20.240
a taxonomy of about 700 categories by default but let's say if somebody wants to train

24:20.800 --> 24:27.280
their own category like for some unique or novel content they can use AutoML for

24:27.280 --> 24:36.640
a natural language to train their own custom categories. So and that works on top of their

24:37.280 --> 24:44.400
they existing categories. So if you make a web service call to this AutoML it not only returns

24:44.400 --> 24:51.040
their default categories but also the trained version as well. And so you found that for the

24:51.040 --> 24:59.920
types of articles that you were initially trying to categorize the were these 700 built-in

24:59.920 --> 25:08.240
categories were they sufficient were they was it you know what was the experience of trying to

25:08.240 --> 25:17.040
map your business to this pre-canned AI service? So most of most of the time it works very well.

25:17.040 --> 25:25.680
So some of the categories like the like for most of our use cases those categories were fine.

25:26.400 --> 25:32.720
There's a few categories which don't work well especially their sensitive content so they

25:32.720 --> 25:40.880
don't really split out. So any article about crime or you know some violence everything gets

25:40.880 --> 25:48.160
categorized into sensitive content. We would have ideally liked that to be split down into more

25:48.160 --> 25:55.360
granular but Google thinks that they're like some restrictions they just don't want to split that

25:55.360 --> 26:04.080
out. But one of the categories that we wanted that I did a POC was to detect evergreen content

26:04.080 --> 26:11.360
because we had labeled about several thousand articles with evergreen label. So whatever green

26:11.360 --> 26:19.760
means is content that has a life shelf of more than a few days or a few weeks. For example an article

26:19.760 --> 26:29.520
which is a review of a museum or an article about some real estate. So those articles have a longer

26:29.520 --> 26:36.320
shelf life and it's very important in the newspaper if you can detect that type of content because

26:36.320 --> 26:44.000
that kind of content can be used in recommendations. Even an article written two years ago can be

26:44.000 --> 26:52.640
reused which otherwise would just be sitting there and nobody reads that content. So I built using

26:52.640 --> 27:00.960
AutoML for text. I built a classifier which basically detects evergreen versus non-evergreen content

27:01.680 --> 27:09.280
and our editorial they helped us label this content. We asked all the different markets

27:10.000 --> 27:16.960
to take their content and label them with evergreen content. Initially I did a very quick prototype

27:16.960 --> 27:25.680
using TensorFlow and then I had a hunch that their Google is working on this AutoML feature for

27:25.680 --> 27:35.520
NLP and then once this feature came out doing a POC was really quick. Within a day I was able to train

27:36.800 --> 27:42.640
an AutoML model that can differentiate evergreen content. So really the hard part in this is to

27:42.640 --> 27:51.280
get the labeled data. And the interesting thing is once this model is trained I even try to

27:51.280 --> 27:58.160
on CNN and New York Times content it was able to differentiate evergreen and non-evergreen content.

27:59.840 --> 28:06.000
And right now we're incorporating that into one of our recommendation systems.

28:06.000 --> 28:12.560
Yeah one of the things that I noticed a couple of times in our conversation you're the senior director

28:12.560 --> 28:19.200
of data engineering in ML. But it sounds like on a couple of these POCs you just kind of

28:20.000 --> 28:24.640
played around and put some stuff together you make it sound really easy. It can be easy to

28:24.640 --> 28:29.680
experiment but then if you actually want to use this in a way that the business is going to depend

28:29.680 --> 28:36.320
on it there's at least traditional a lot more that needs to go into it in terms of engineering.

28:37.200 --> 28:42.880
I guess there's maybe several questions in here but you know part of it is like does the cloud

28:42.880 --> 28:49.360
change that dynamic in your perspective or you know are these projects that you built and then

28:49.360 --> 28:56.160
kind of threw them over the wall to you know some team that then had to maintain these projects.

28:56.160 --> 29:02.320
Are you a question specifically about auto ML or just about recommendations and

29:03.440 --> 29:10.400
the other stuff I've been talking about? Specifically the I think with term prediction and the auto

29:10.400 --> 29:16.400
ML the impression I got was that it almost sounded like you got bored one weekend and kind of

29:16.400 --> 29:20.320
work on these and kind of you know you came up with these models in a you know a day or so.

29:20.320 --> 29:28.240
Yeah so that they weren't done in a day they were sort of like the turn model we started

29:29.280 --> 29:35.680
when BQMO was an alpha state only available for certain customers but I think it was about

29:35.680 --> 29:44.880
May we start building it so and it took a couple of months like refining and fine tuning it

29:44.880 --> 29:53.120
and so and we're now we're working on another market building out doing this prediction for Houston.

29:55.120 --> 30:01.280
Auto ML it was in phase that we started off like last year we said we need to collect the data

30:01.280 --> 30:09.040
since we want to be able to do saying so one mini project was to figure out how to get this data

30:09.040 --> 30:16.960
and get it labeled and and then once we had it label it was just sitting there for a while till

30:18.240 --> 30:24.320
I did some prototyping work for we're using TensorFlow and then when auto ML came out

30:26.480 --> 30:31.200
it was since we had it did already had done the hard work on labeling that data it was

30:31.200 --> 30:40.080
very something very quick to do so I guess one piece of the project wasn't done like in in a weekend

30:40.080 --> 30:45.520
there were different phases and happen in different times and the other thing is in general the

30:45.520 --> 30:53.920
the newer features in cloud especially the the higher level API that's much more quicker to do

30:53.920 --> 31:00.320
prototyping and getting things out quicker and auto ML is a feature that's meant for

31:00.320 --> 31:08.240
people to do things much quicker like you don't have to learn TensorFlow to be able to do it it's

31:08.240 --> 31:16.640
just upload the data give it label data set and then it takes a few hours to train a model and start

31:16.640 --> 31:24.240
using it you've done some work on recommendation systems as well yeah so as I was talking so one of

31:24.240 --> 31:32.240
the use cases with the NLP data that's sitting in our BigQuery where data warehouses to build a

31:32.240 --> 31:39.760
recommendation system so the the idea is that if two piece of content if they have overlapping

31:40.400 --> 31:48.320
NLP entities they're highly likely to be related to each other so this is content to content

31:48.320 --> 31:54.320
recommendation and since we had this again since we had this in our BigQuery database

31:55.680 --> 32:04.560
we built out a recommendation system which actually works using a big SQL which takes entities

32:04.560 --> 32:11.920
from one set of articles and matches that with another one and corporate saliency score and

32:11.920 --> 32:18.720
so there's there's a whole bunch of rules that we built out in our SQL and produces a table of

32:18.720 --> 32:27.440
recommendations and that's fronted by a web service layer that our website San Francisco Chronicle

32:27.440 --> 32:33.680
is I think three websites that are using that content content recommendation and this was a

32:33.680 --> 32:45.680
release as part of San Francisco's user interface rewrite so a few months ago we released a revamp

32:45.680 --> 32:51.440
of SF Chronicle so this was project this content to content recommendation was part of

32:52.480 --> 32:59.280
that deployment so you've got this content in the in BigQuery and then you're using

32:59.280 --> 33:09.600
Google NLP to essentially you're adding fields for each of the entities that are recognized and

33:09.600 --> 33:19.600
then you're using BigQuery ML to generate a recommendation for from a given piece of content to

33:20.240 --> 33:25.760
other recommended pieces of content based on these shared entities that have been recognized by

33:25.760 --> 33:34.640
the NLP service yeah so and just a minor correction that this one doesn't use BQML this is a plain

33:34.640 --> 33:42.160
SQL running which basically is looking at the number of overlaps of entities between two contents

33:42.160 --> 33:48.000
got it got it so it's basically scoring of how like based on the overlap how

33:49.040 --> 33:54.800
relevant one content is to another one okay and then you said you're frontending that with

33:54.800 --> 34:02.560
a website are you serving the data directly up from BigQuery or do you push it out to some

34:03.520 --> 34:10.880
cache or database yeah so yeah so we'd push it out this computation is done on a periodic

34:10.880 --> 34:18.880
basis about every 15 minutes and that's about the frequency that we're getting your content so we

34:18.880 --> 34:26.320
build out a table in a postgres database and then that web service layer is using a memcache as

34:26.320 --> 34:33.040
well as this postgres database to serve these out and you've also done some video content recommendations

34:33.600 --> 34:41.760
yeah so it's so another project we've done is we've taken our video converted that to sound

34:41.760 --> 34:50.160
and then extracted text from that sound and then applied Google's NLP to extract entities from

34:50.160 --> 34:58.400
the sound so so our content has the NLP tags and our video using voice transcription

35:00.240 --> 35:06.960
and using that text to also extract entities we can recommend video to content based on this

35:06.960 --> 35:17.120
technology and so to transcribe the audio did you use the the Google speech to text API for that

35:17.120 --> 35:24.560
yeah yeah so Google has a Google sound API which given any sound it can convert that to text

35:25.200 --> 35:33.360
and what was your experience getting reliable results from that yeah so our like full objective

35:33.360 --> 35:41.600
was to also have captioning on the videos so we did most of the time it works very well the

35:42.800 --> 35:51.360
sound transcription but in certain cases it doesn't like it if like it there was some few

35:52.720 --> 35:59.120
cases when it didn't work very well so we haven't used this for captioning but we took this text

35:59.120 --> 36:07.840
and it seems to work well if we apply NLP on it if we just want to extract categories and tags

36:07.840 --> 36:14.800
from it for that purpose it works well and those tags are used for it eventually being able to

36:14.800 --> 36:24.880
recommend a contextual content along with the videos any final thoughts or words of wisdom that

36:24.880 --> 36:32.720
you shared as you were wrapping up your presentation on these use cases yeah so some of the advice I

36:32.720 --> 36:41.120
gave was that like some of the things that we want to do in the future is to use applied deep learning

36:41.120 --> 36:50.800
for recommendations there's a lot of research done in the past couple of years for application

36:50.800 --> 37:00.320
deep learning and recommendation we're actually doing some research and we want to launch very soon

37:00.320 --> 37:07.840
with a recommendation system that applies user deep learning so I'd recommend that we should use

37:07.840 --> 37:16.800
like everybody should look into deep learning and TensorFlow so it's really everything you don't

37:16.800 --> 37:26.080
have to build from scratch if a problem is already solved by higher level APIs such as the NLP

37:26.080 --> 37:35.120
image video APIs or BQML and AutoML we should be using that it's really a trade-off do you want to

37:35.120 --> 37:40.960
hire data scientists to rebuild this thing or do you want to just pay that for that service and apply

37:40.960 --> 37:48.800
it and only in cases where the problem is novel or the data set is novel is when you want to build your

37:48.800 --> 37:56.480
in-house machine learning model for example we want to be able to do recommendations using deep

37:56.480 --> 38:03.200
learning so this is something that we're building in-house and other things that we're exploring

38:03.200 --> 38:12.400
as more use cases for NLP we identify that we can use this with our newsletters to be able to

38:12.400 --> 38:20.720
generate newsletters automatically that's something that we're looking at too and also more

38:20.720 --> 38:29.040
uses for BigQuery ML just like churn we can do the converses do propensity modeling how life

38:29.040 --> 38:36.640
be heard what's the likelihood of a user to subscribe to our whenever websites so this is

38:36.640 --> 38:44.560
overall our future plans awesome well Naveed thanks so much for taking the time to share what you've

38:44.560 --> 38:49.680
been up to with us it's been really interesting and I appreciate it okay thank you very much

38:49.680 --> 38:59.200
all right everyone that's our show for today for more information on Naveed or any of the topics

38:59.200 --> 39:06.560
covered in this episode visit twimmelai.com slash talks slash 182 if you're a fan of the podcast

39:06.560 --> 39:11.600
we'd like to encourage you to visit your Apple or Google podcast app and leave us a five-star

39:11.600 --> 39:16.560
rating and review your reviews help inspire us to create more and better content and they help new

39:16.560 --> 39:25.120
listeners find the show as always thanks so much for listening and catch you next time

