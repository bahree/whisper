1
00:00:00,000 --> 00:00:16,000
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:16,000 --> 00:00:29,680
All right, everyone. I am here in Vancouver, continuing my coverage and

3
00:00:29,680 --> 00:00:35,040
conversations at the NURPS 2019 conference. And I've got the pleasure of being with

4
00:00:35,040 --> 00:00:39,280
you again, Schmidt Huber. You again is a co-founder and chief scientist of

5
00:00:39,280 --> 00:00:47,360
Nacense, the scientific director at the SWIST AI Lab Insia, as well as a professor of AI at USI

6
00:00:47,360 --> 00:00:54,960
and SUPSI in Switzerland. You again and I had the pleasure of speaking just over a couple of years

7
00:00:54,960 --> 00:01:02,880
ago now, where he hosted me at his lab in Logano, Switzerland, a beautiful setting and a wonderful

8
00:01:02,880 --> 00:01:10,400
conversation that I encourage you to take a listen to. That was Tumel Talk number 44, almost 300

9
00:01:10,400 --> 00:01:18,640
episodes ago. You again is, you know, regarded by many as the father of AI. He is his lab created

10
00:01:18,640 --> 00:01:26,480
the LSTM. That was a big focus of our conversation. The LSTM is a model that, you know, we all use

11
00:01:26,480 --> 00:01:32,400
all the time on our smartphones. And I'm excited to have an opportunity to catch up with you.

12
00:01:32,400 --> 00:01:37,280
You again, welcome to the Tumel AI podcast. It's my pleasure being here. Thank you so much.

13
00:01:37,280 --> 00:01:43,040
So, you know, I really wanted to start this conversation with a bit of an update. What are some

14
00:01:43,040 --> 00:01:48,080
of the things that you've been doing in your research lab and at Nacense? What's been interesting

15
00:01:48,080 --> 00:01:54,160
and exciting for you? Let me give you a couple of examples of what Nacense is doing. That's a B2B

16
00:01:54,160 --> 00:02:02,640
company and now we're entering this age of active AI where you don't just have passive patent

17
00:02:02,640 --> 00:02:07,760
recognition on your smartphone, but where you have machines that learn to become better machines

18
00:02:07,760 --> 00:02:12,720
by translating the data that is coming in through the cameras and other sensors into actions

19
00:02:12,720 --> 00:02:18,960
that manipulate the industrial processes that they are controlling. For example, we have a project

20
00:02:18,960 --> 00:02:25,440
with Festo which is one of the leading robot makers and they have these hands which are being

21
00:02:25,440 --> 00:02:34,640
controlled by air muscles. So you have a compressor which is generating the pressure that you need

22
00:02:34,640 --> 00:02:40,720
to move these fingers around and nobody knows how to control that. And so Nacense is building the

23
00:02:40,720 --> 00:02:49,760
brains that learn to control these soft robot hands that we have there. Or EOS that is leader

24
00:02:49,760 --> 00:02:56,560
in industrial 3D printing where you don't just do little plastic models of something,

25
00:02:56,560 --> 00:03:04,080
but where you really print engines with metal and they have all the patterns for melting the powder

26
00:03:04,080 --> 00:03:10,480
and then letting it incrementally through additive manufacturing turn into engines of airplanes

27
00:03:10,480 --> 00:03:15,360
and stuff like that. And one of these printers costs at least half a million or something.

28
00:03:15,360 --> 00:03:20,720
So they are much more expensive than the standard cheap printers that you can buy in the supermarket.

29
00:03:21,360 --> 00:03:27,280
It already seems clear that you can further improve these printing processes by understanding

30
00:03:27,280 --> 00:03:34,560
better what exactly is the impact of the laser light on the melting process and on the cooling

31
00:03:34,560 --> 00:03:40,400
process and how can you control the thing in a way that improves the quality of the engines that

32
00:03:40,400 --> 00:03:46,560
are coming out. Then other projects with Salt Sashmeet for example they have little drones and

33
00:03:46,560 --> 00:03:54,800
they are inspecting the wind turbines and then how can you automatize that and how can you learn,

34
00:03:55,600 --> 00:04:02,560
make these machines learn to become better quality inspectors. With Audi a while ago we had a

35
00:04:02,560 --> 00:04:09,680
demo project where a little toy Audi is maybe only one eighth-author size of a real Audi.

36
00:04:09,680 --> 00:04:16,320
They learn to park not by imitating a human teacher but just by reinforcement learning just by

37
00:04:17,040 --> 00:04:23,760
maximizing the pleasure signals that come in and minimizing the pain signals that come in.

38
00:04:23,760 --> 00:04:28,480
Whenever one of these little cars bumps against an obstacle then it gets negative reward.

39
00:04:28,480 --> 00:04:36,640
Whenever it reaches the parking lot then it gets a positive reward and as always in reinforcement

40
00:04:36,640 --> 00:04:42,080
learning it's trying to minimize pain and maximize pleasure so it has to translate the incoming

41
00:04:42,080 --> 00:04:50,560
video and the other data from the radar sensors and lead sensors into actions that lead to successful

42
00:04:50,560 --> 00:04:55,680
parking strategies and that was the first time something like that was done in the real world.

43
00:04:55,680 --> 00:05:02,960
These little Audi's they aren't as big as the as the real thing however they have all these

44
00:05:02,960 --> 00:05:10,400
sensors that you have and the big Audi's the radar the cameras they go up to 120 kilometers

45
00:05:10,400 --> 00:05:16,720
an hour so they are pretty fast too and sounds like a nice toy. It's a very nice toy one has

46
00:05:16,720 --> 00:05:25,520
certainly that and a whole bunch of other projects are I'm happy that now that nasons isn't the

47
00:05:25,520 --> 00:05:33,840
second round of financing we don't only have these these investors from the platform companies

48
00:05:34,400 --> 00:05:41,760
on the Pacific Rim but now also old industry. Old industry is investing in us for example shot

49
00:05:41,760 --> 00:05:48,880
is a leading maker of glass and you probably have them on your smartphone because they are owned

50
00:05:48,880 --> 00:05:55,840
by the size foundation and the glass that they make isn't the lenses are the cameras on

51
00:05:56,720 --> 00:06:03,120
hundreds of millions of them okay as smartphones many people don't know that making glass is

52
00:06:03,920 --> 00:06:11,360
difficult and it's about controlling an industrial pipeline where you have

53
00:06:11,360 --> 00:06:20,720
molten glass and it's 1,200 degrees Celsius and you want to remove the bubbles and at the right

54
00:06:20,720 --> 00:06:26,560
moment you want to inject certain substances and you have turbulent mixing that you want to

55
00:06:26,560 --> 00:06:33,760
control all kinds of knobs that you can adjust there and this has been traditionally done by

56
00:06:33,760 --> 00:06:40,400
humans. Shot is a company which is more than a hundred years old and over the decades they have

57
00:06:40,400 --> 00:06:46,000
learned how to do that well however there is still a lot of room for improvement and for making

58
00:06:46,000 --> 00:06:54,240
even better glass through machines that learn to do that so suddenly investors are not just the

59
00:06:54,240 --> 00:07:01,920
platform companies from the Pacific Rim but old industry which is realizing that in this coming

60
00:07:01,920 --> 00:07:08,960
age of active AI suddenly you can do things that go far beyond passive pattern recognition on

61
00:07:08,960 --> 00:07:15,200
this smartphone and they sense this in the middle of that. That's very exciting very exciting you

62
00:07:15,200 --> 00:07:22,640
mentioned a few interesting use cases with the first one that you mentioned the robotic hand calls

63
00:07:22,640 --> 00:07:28,800
to mind the recent news that you may have come across with open AI and their Rubik's Cube and they've

64
00:07:29,440 --> 00:07:36,080
used reinforcement learning to kind of train the model that controls the you know this highly

65
00:07:36,080 --> 00:07:40,880
dimensional robot essentially. Are you also using reinforcement learning as part of that application

66
00:07:40,880 --> 00:07:48,560
or what are the kind of the main techniques that you're applying to that? In fact the open AI

67
00:07:48,560 --> 00:07:57,360
robot hand is driven by the architectures that we have developed in our labs and Munich and

68
00:07:57,360 --> 00:08:04,160
Switzerland over the decades. What is this open AI reinforcement learner doing? It is basically

69
00:08:04,160 --> 00:08:11,680
a big LSTM network which is surrounded by a couple of smaller pre-processing networks, pre-processes

70
00:08:11,680 --> 00:08:18,080
our inputs and then through lots of simulations there's a pretty good simulation of this hand

71
00:08:18,720 --> 00:08:25,120
through reinforcement learning the LSTM learns to control the hand such that it can do interesting

72
00:08:25,120 --> 00:08:33,760
things such as solving the Rubik's Cube in a certain fraction of the times. So yeah of course we are

73
00:08:33,760 --> 00:08:41,120
using the techniques that are now popular in reinforcement learning which is train and LSTM

74
00:08:41,120 --> 00:08:48,480
through reward maximization to do that. However the big difference between what what this

75
00:08:48,480 --> 00:08:54,960
example of open AI is about and what you have in the real world is you don't have a good model

76
00:08:54,960 --> 00:09:04,240
or see real world. You somehow have to deal with this situation that you have very few interactions

77
00:09:04,240 --> 00:09:10,400
examples with the real world from which you have to build a model of the world and then you have

78
00:09:10,400 --> 00:09:17,760
to do mental experiments in this model which is imperfect in many ways but can learn to improve

79
00:09:17,760 --> 00:09:23,280
itself through further data that is coming in through the actions of the robot that is trying to

80
00:09:23,280 --> 00:09:32,880
learn something new. So are you speaking to the relationship between simulation and having the

81
00:09:32,880 --> 00:09:39,200
model operate in the real world and kind of correlating or integrating rather those two such that

82
00:09:39,200 --> 00:09:43,120
you've got some kind of end-to-end training process that integrates both SIM and real.

83
00:09:43,120 --> 00:09:54,320
So at the moment AI works well in the virtual realm for video games like Dota or Starcraft that's

84
00:09:54,320 --> 00:10:00,720
what deep-minded Dota was done by OpenAI. Again you have an LSTM network surrounded by other

85
00:10:00,720 --> 00:10:09,200
networks which is learning through policy gradient methods reward optimization methods to make

86
00:10:09,200 --> 00:10:14,960
some of these weight strong and some of them weaker such that the whole system over time becomes a

87
00:10:14,960 --> 00:10:23,280
better video game player and there you train the system through exposing it through billions and

88
00:10:23,280 --> 00:10:29,920
trillions of video games. This is something that you can do in the virtual realm and the same

89
00:10:29,920 --> 00:10:38,320
principle was applied to the robot hand of OpenAI where they also had lots of simulations

90
00:10:38,320 --> 00:10:43,680
because they did have a pretty good simulation of the hand and then through certain

91
00:10:44,720 --> 00:10:53,840
tricks it was possible to transfer the behavior from the virtual setting to the real world setting

92
00:10:53,840 --> 00:11:03,760
and then one essential ingredient was this excellent simulation of the hand and normally you don't

93
00:11:03,760 --> 00:11:12,000
have that. In almost all industrial processes you don't have that. In the real world you have all

94
00:11:12,000 --> 00:11:20,000
kinds of slippage and it's not well-modeled in any simulation and you have so many things that are

95
00:11:20,000 --> 00:11:30,480
not modeled. How does a baby learn to deal with that? Well the baby is receiving data as

96
00:11:30,480 --> 00:11:36,880
consequence of the actions that it is generating as it is playing with its toys and from this data

97
00:11:36,880 --> 00:11:42,880
it learns to build a model so it doesn't have a given model of the world. No it learns to build this

98
00:11:42,880 --> 00:11:48,800
model and how does it do that? It learns to predict what's going to happen next what is the next

99
00:11:48,800 --> 00:11:56,560
input given what I have done so far and some of the inputs are predictable others are not predictable

100
00:11:56,560 --> 00:12:03,040
and the baby even does more than just predicting the next thing somehow it learns high level abstractions

101
00:12:03,040 --> 00:12:10,800
and it learns to plan ahead in a way that apparently is not the millisecond by millisecond planning

102
00:12:10,800 --> 00:12:16,240
that you would do in a detailed simulation of the world where you will try to come up with a

103
00:12:16,240 --> 00:12:22,400
sequence of actions that leads to a lot of predicted reward in that simulation where you really

104
00:12:22,400 --> 00:12:28,000
have a simulation that millisecond by millisecond simulates the entire world and such a simulation

105
00:12:28,000 --> 00:12:37,120
is totally dependent on being a good simulation and no errors are allowed there while the models that

106
00:12:37,120 --> 00:12:45,040
humans carry around they are more sophisticated in the sense that they acknowledge that certain

107
00:12:45,040 --> 00:12:50,640
things are not known and other things are rather predictable and they don't do this millisecond by

108
00:12:50,640 --> 00:12:57,680
millisecond planning no they do high level planning like for example when you are going to Beijing

109
00:12:57,680 --> 00:13:04,640
you you make a plan for that and you don't plan the trajectories of all your muscle movements

110
00:13:04,640 --> 00:13:11,680
millisecond by millisecond no you say first I have to find a taxi and then once I'm in the taxi

111
00:13:11,680 --> 00:13:18,320
I'm going to the airport and then I'm entering the plane and for nine hours nothing is going to

112
00:13:18,320 --> 00:13:24,960
happen and then I accident so this high level planning is something that humans are really good at

113
00:13:24,960 --> 00:13:31,600
and probably also some of the animals and our robots are not good at that yet however this is

114
00:13:31,600 --> 00:13:39,200
changing and we are making great progress exactly in that realm generally speaking current AI

115
00:13:39,200 --> 00:13:46,720
works well in the virtual realm video games or robots where you have a really good simulation of

116
00:13:46,720 --> 00:13:56,160
the robot and it doesn't work well yet in the physical real world we are so there is an implication

117
00:13:56,160 --> 00:14:04,320
of this that the model that you developed with the hand was not based on simulation it was all based

118
00:14:04,320 --> 00:14:10,880
on training through experimentation on the physical hand itself that's what we generally do

119
00:14:10,880 --> 00:14:19,600
so we try to learn like a little baby a model of the industrial process that we are controlling

120
00:14:20,320 --> 00:14:27,680
and because in most cases there is no good model of this process we have to learn it from experience

121
00:14:28,480 --> 00:14:35,680
so suddenly we need a system that learns to generate experiments that lead to data that can be

122
00:14:35,680 --> 00:14:42,480
used to build a better model when I think about for example you know maybe this doesn't apply to

123
00:14:42,480 --> 00:14:52,320
hands to the extent that applies to like robotic arms but in with robotic arms they're you know I've

124
00:14:52,320 --> 00:14:59,600
often kind of talked to folks or talked about kind of this tension between you know folks that

125
00:14:59,600 --> 00:15:06,240
want to tackle that problem with end deep learning which any model versus folks that want to take

126
00:15:06,240 --> 00:15:12,880
advantage of kind of traditional control systems and you know robot kinematics that you know if

127
00:15:12,880 --> 00:15:19,840
you're able to integrate those traditional models you know that we do kind of know how to get a

128
00:15:19,840 --> 00:15:25,120
robot an actuator from one position you know in a 3D space to another position in a 3D space

129
00:15:25,120 --> 00:15:30,880
because we have kind of models that do that it sounds like your approaches aren't kind of model based

130
00:15:30,880 --> 00:15:37,920
in that perspective you're learning everything from the ground up using reinforcement learning without

131
00:15:37,920 --> 00:15:43,040
the benefit of simulation is that yeah what are the leaps that I'm making there you know tune that

132
00:15:43,040 --> 00:15:50,240
up for me so in industrial applications we use everything that is already there okay so if there's

133
00:15:50,240 --> 00:15:56,960
a reasonable model of the process then of course we are going to use that it's just that the most

134
00:15:56,960 --> 00:16:04,400
exciting applications are those where you don't have that model hmm sure if you have a factory

135
00:16:04,400 --> 00:16:11,520
full of industrial robots then you know exactly where each of these robots is positioned you know

136
00:16:11,520 --> 00:16:22,080
exactly where it has to grip the chair and how to move it exactly at the right position in the car

137
00:16:22,080 --> 00:16:29,680
where it's going to end up so there you have very precise industrial robots and they they are

138
00:16:30,480 --> 00:16:36,240
extremely good at doing what they are doing because everything is totally on a control there's

139
00:16:36,240 --> 00:16:43,280
a perfect model of where everything is located where the tools are where the cars are coming in

140
00:16:43,280 --> 00:16:50,080
where the chairs are they they can do that without cameras even because everything is already

141
00:16:50,080 --> 00:16:55,840
modeled extremely well right but that's not the future of manufacturing the future of manufacturing

142
00:16:55,840 --> 00:17:02,480
is all the messy things all the messy stuff like when you are assembling a smartphone for example

143
00:17:02,480 --> 00:17:09,120
there are all these little parts of this smartphone and at the moment humans have to do that also

144
00:17:09,120 --> 00:17:14,720
there is no robot that is able to do that at least not in the way that humans can do it and what

145
00:17:14,720 --> 00:17:22,400
you really want is a robot that is confronted with a new task and quickly learns through a few

146
00:17:22,400 --> 00:17:29,920
interactions maybe a few demonstrations of a human but also a few self-generated experiments

147
00:17:29,920 --> 00:17:36,960
how this thing works and how to screw in that screw and how to avoid that the screw driver

148
00:17:36,960 --> 00:17:43,120
slipping and all these super complicated important things that are easy for humans but not yet

149
00:17:43,120 --> 00:17:50,640
for current robots and it wants to to learn through a combination of curiosity and imitation of

150
00:17:50,640 --> 00:17:58,800
what the human teachers are showing to it it wants to learn to execute this new behavior quickly

151
00:17:58,800 --> 00:18:05,840
at some point as well as the teacher who is just through a visual demonstration and through

152
00:18:05,840 --> 00:18:12,160
speech and language trying to explain to the robot what it wants to what it should do and then

153
00:18:12,160 --> 00:18:19,280
once it is able to do that to a certain extent you want to have a robot through self-experimentation

154
00:18:19,280 --> 00:18:25,520
to speed up the process and make it more efficient so that it can do it much faster than any human

155
00:18:25,520 --> 00:18:32,320
and with less energy and then once it's really good you want to switch off the learning algorithm

156
00:18:32,320 --> 00:18:39,520
and make a million copies and license it or sell it yeah as we talked about on this show many times

157
00:18:39,520 --> 00:18:46,960
RL is particularly deep RL is historically extremely simple and efficient right and so a lot of

158
00:18:46,960 --> 00:18:52,640
folks will turn the simulation because if they just try to train in the real world they'll destroy

159
00:18:52,640 --> 00:18:59,920
their robots before they converge to any type of model is the work that you're doing they're

160
00:18:59,920 --> 00:19:07,840
focused on the the simple efficiency side of the equation it is the essential thing which we need

161
00:19:07,840 --> 00:19:14,800
for dealing with the real world because there you can have only so many training examples

162
00:19:15,440 --> 00:19:21,920
and self-invented experiments and that's it you can't have a trillion experiments like in video

163
00:19:21,920 --> 00:19:29,120
games right right so that's the big difference and it's really the single most important issue

164
00:19:30,400 --> 00:19:38,640
the the big gap that exists between the learning abilities of humans and our best reinforcement

165
00:19:38,640 --> 00:19:45,920
learning algorithms is really that so from very few training examples you want to pick up all the

166
00:19:45,920 --> 00:19:52,400
things that are necessary to control that process that you're trying to control and it's what

167
00:19:52,400 --> 00:20:00,720
current reinforcement learning algorithms are not so good at what are the specific things that

168
00:20:00,720 --> 00:20:06,320
or specific research or papers that you're working on in your lab that try to get there is it

169
00:20:06,320 --> 00:20:12,160
you know how would you even did they fall under kind of the banner of one shot few shot learning

170
00:20:12,160 --> 00:20:18,080
meta learning curriculum learning imitation learning like are there are they you know one or

171
00:20:18,080 --> 00:20:24,960
combinations of these things or is it an altogether different approach so since we are interested

172
00:20:24,960 --> 00:20:30,160
in universal learning machines it's all of that okay but let me give you a one concrete little

173
00:20:30,160 --> 00:20:36,400
example of something that we published just a couple of days ago okay in an active paper

174
00:20:36,400 --> 00:20:42,640
something which is called upside down reinforcement learning upside down reinforcement learning okay

175
00:20:42,640 --> 00:20:49,040
upside down already intrigued it's called upside down reinforcement learning because it turns

176
00:20:49,040 --> 00:20:53,840
traditionally reinforcement learning on its head traditionally reinforcement learning works like

177
00:20:53,840 --> 00:21:01,520
that you have a neural network or some other adaptive machine which takes in data from the

178
00:21:01,520 --> 00:21:07,760
environment and produces actions which change the environment which means new inputs are coming in

179
00:21:07,760 --> 00:21:15,200
from the environment and there is something you want to maximize which is the cumulative expected

180
00:21:15,200 --> 00:21:23,440
reward so when the when the robot is acting in a good way then it gets reward and it wants to maximize

181
00:21:23,440 --> 00:21:28,320
that by making some of these connections in the network stronger and others weaker such that

182
00:21:28,320 --> 00:21:34,640
actions come out in response to the inputs that lead to a lot of reward success yes exactly

183
00:21:34,640 --> 00:21:39,920
and then the way it's usually done is there is a second network which is a prediction machine which

184
00:21:39,920 --> 00:21:47,120
sees the actions of the first network and the inputs that are coming in from the environment and

185
00:21:47,120 --> 00:21:54,080
it's trying to predict given this policy that is implemented by the first network what is the

186
00:21:54,080 --> 00:22:02,560
expected future reward so some of all the rewards that is going to come in until the end of the

187
00:22:02,560 --> 00:22:10,240
contrial for example and then through this prediction which depends on the actions and the

188
00:22:10,240 --> 00:22:18,080
information from the environment you can find actions that lead to more reward than what you

189
00:22:18,080 --> 00:22:24,400
have so far and then there are simple ways of adjusting the first network such that the actions

190
00:22:24,400 --> 00:22:33,040
that are being generated by it are more likely to lead to a lot of reward so what is very essential

191
00:22:33,040 --> 00:22:39,760
there is this prediction of future expected reward and upside down reinforcement learning turns

192
00:22:39,760 --> 00:22:46,160
all of that around it doesn't have the rewards being predicted by the outputs of such a network

193
00:22:46,160 --> 00:22:55,280
no the rewards are becoming the inputs to a network which produces the output actions and which

194
00:22:55,280 --> 00:23:02,160
also sees the standard inputs coming from the environment and these incoming rewards are

195
00:23:02,160 --> 00:23:12,800
interpreted as commands so there's this extra input field of this control network which is

196
00:23:12,800 --> 00:23:20,000
not the cumulative reward it's the reward that's a result of the previous action no it's

197
00:23:20,000 --> 00:23:27,440
in fact the cumulative the cumulative reward that you would like to get between this time and

198
00:23:27,440 --> 00:23:33,280
this time in the in the simplest case there are two inputs one is a time input which says between

199
00:23:33,280 --> 00:23:40,560
this time and this time I want to get so much reward that's a command that's a command so now

200
00:23:40,560 --> 00:23:46,320
the network sees this command and tries to come up with actions that between this time and this

201
00:23:46,320 --> 00:23:54,320
time lead to so much reward it just tries to obey the command okay so now it's actually unique

202
00:23:54,320 --> 00:24:00,560
its actions and it turns out that the incoming reward is less than what the command

203
00:24:01,520 --> 00:24:07,120
requested so then we have a training example where the network can learn something from that and

204
00:24:07,120 --> 00:24:15,680
it can learn in the following way let's adjust the command retrospectively let's take the

205
00:24:15,680 --> 00:24:22,800
real reward that came in between these two time steps and use that as a command and let's run

206
00:24:22,800 --> 00:24:27,600
the whole thing again and let's train it to generate this action sequence that it already has

207
00:24:27,600 --> 00:24:33,840
generated in response to these modified commands which are now consistent with what the network did

208
00:24:33,840 --> 00:24:42,720
okay so now it learns to generate an action sequence like what it just had generated in response

209
00:24:42,720 --> 00:24:48,880
to these commands and what you really want to achieve is that you generate action sequences

210
00:24:48,880 --> 00:24:54,880
that lead to more reward so you say let's give a new command to the system let's say we now want

211
00:24:54,880 --> 00:25:01,840
to have twice as much reward as last time so new command everything is the same except that

212
00:25:01,840 --> 00:25:09,280
now we've won twice as much reward as a command input so the network is producing outputs

213
00:25:09,280 --> 00:25:17,520
in response to that and maybe the result is that the reward that is really received from the

214
00:25:17,520 --> 00:25:22,960
environment is indeed twice as much because it's somehow was able to internalize then we are happy

215
00:25:22,960 --> 00:25:28,800
then we say on let's try the same thing with four times as much reward and it sounds like a

216
00:25:28,800 --> 00:25:37,040
binary search of the reward space or something like that maybe not binary but you I'm imagining

217
00:25:37,040 --> 00:25:43,760
that you would start with you know an unachievable reward maybe infinite and you the network

218
00:25:43,760 --> 00:25:48,960
achieves much less than that and then you you know it says okay maybe I'll you know go to half

219
00:25:48,960 --> 00:25:54,640
of infinity or whatever half of the large number that I would love to have but the point is from

220
00:25:54,640 --> 00:26:01,680
each of its failures to to satisfy the command it learns something so it learns each time something

221
00:26:01,680 --> 00:26:12,400
new about how to map commands to action sequences and over time it will learn the structure of

222
00:26:12,400 --> 00:26:21,280
this space of parameters that you need to translate these commands into desired behavior and then

223
00:26:21,280 --> 00:26:27,840
you can do very directed exploration because you just as you said you give new commands that you

224
00:26:27,840 --> 00:26:34,160
never have given before and you hope that the network will know enough about the space of these

225
00:26:34,160 --> 00:26:41,760
parameters and about functions mapping reward commands to output actions such that it can

226
00:26:41,760 --> 00:26:50,880
generalize and yield even more desirable behavior and so whenever these expectations fail it still has

227
00:26:50,880 --> 00:26:56,640
a new bunch of training examples from which it can learn to become a better translator of commands

228
00:26:56,640 --> 00:27:04,960
into action sequences so it's not some random exploration though it is a way of using pure

229
00:27:04,960 --> 00:27:11,040
supervised learning to achieve this reinforcement learning objective which is get a lot of reward

230
00:27:11,040 --> 00:27:18,560
and very little time and it's just learning through supervised learning to map these combinations

231
00:27:18,560 --> 00:27:27,760
of cumulative rewards within certain time intervals to action sequences and supervision comes

232
00:27:27,760 --> 00:27:35,600
from it's kind of a binary supervision achieved or failed to achieve the command based on some

233
00:27:35,600 --> 00:27:42,400
sequence of actions yeah and so there is we know for each behavior for each trial of the agent

234
00:27:42,400 --> 00:27:49,760
we know exactly how much reward did we get we know exactly which action sequence was there

235
00:27:49,760 --> 00:27:58,400
and in retrospect we can adjust the reward and time command such that it is compatible with what

236
00:27:58,400 --> 00:28:07,280
really was achieved if you know power play algorithms or hindsight experience replay algorithms

237
00:28:07,280 --> 00:28:13,600
or if you know a rather algorithm of set poor writer's group then you already know a lot about

238
00:28:13,600 --> 00:28:22,000
the principles of what upside down reinforcement learning is about in the paper how did you compare

239
00:28:22,000 --> 00:28:31,200
the performance of this approach to other approaches and maybe before that is do you envision

240
00:28:31,200 --> 00:28:38,880
this approach being applicable to the same or any you know reinforcing any of the kind of

241
00:28:38,880 --> 00:28:45,760
traditional RL problems or it does this approach with the you know the time and the reward as the

242
00:28:45,760 --> 00:28:52,080
inputs is is it particularly you know interesting or useful applied to a specific formulation of the

243
00:28:52,080 --> 00:29:00,240
RL problem yeah so Hu Pech Kumar Srivasava my coworker at Naysans and a former PhD student

244
00:29:00,240 --> 00:29:08,080
of mine and other colleagues at Naysans and also Editsia they came up with variants of what I just

245
00:29:08,080 --> 00:29:14,560
explained and showed that although we at the moment have just little pilot versions of this

246
00:29:14,560 --> 00:29:20,880
simple new type of reinforcement learning it already is able to beat certain traditional

247
00:29:20,880 --> 00:29:28,880
baselines on interesting problems the thing about this upside down reinforcement learning is

248
00:29:28,880 --> 00:29:35,520
that its limitations are exactly the limitations of supervised learning because basically we are

249
00:29:35,520 --> 00:29:42,000
translating reinforcement learning into supervised learning in a way that depends on these deep

250
00:29:42,000 --> 00:29:48,800
networks that have to learn this complicated mapping from reward commands to action sequences

251
00:29:48,800 --> 00:29:56,240
that lead to the rewards that has been requested within two time steps that I also

252
00:29:56,960 --> 00:30:05,120
given through the command and then this is a complex mapping and these many layers because

253
00:30:05,120 --> 00:30:13,440
you change the reward command just a little bit and this might lead to quite different action

254
00:30:13,440 --> 00:30:19,040
sequence that should come out and so on so we know how to train these deep networks and we have

255
00:30:19,920 --> 00:30:25,840
millions of tricks and supervised learning to do that in a good way and RL Pech and the others

256
00:30:25,840 --> 00:30:32,000
use that and use some of these tricks to make that really work but there is so much that we

257
00:30:32,000 --> 00:30:38,880
haven't even exploited yet so many ways of improving supervised learning which can all be applied

258
00:30:38,880 --> 00:30:45,520
to this in principle a very simple type of new reinforcement learning so it sounds like the

259
00:30:46,080 --> 00:30:52,320
when you you mentioned that you included comparisons on traditional benchmarks you know

260
00:30:52,320 --> 00:30:56,640
what categories of traditional benchmarks like the Atari games and that kind of thing or

261
00:30:56,640 --> 00:31:02,960
yeah in the current experiments the result is it works especially well when there are sparse

262
00:31:02,960 --> 00:31:10,160
rewards so when you don't get rewards all the time like in certain video games but only at the

263
00:31:10,160 --> 00:31:18,240
end of the trial which is actually the most challenging situation because if you don't get a lot

264
00:31:18,240 --> 00:31:24,720
of rewards during all your trials then there's not so much from what you can learn something which

265
00:31:24,720 --> 00:31:30,000
means you have this attribution problem of what was it that you did that resulted in the rewards

266
00:31:30,000 --> 00:31:38,160
that she did get the credit assigned problem exactly and some ways the most challenging type of

267
00:31:38,160 --> 00:31:43,280
reward maximization problem however at the moment we have just these pilot experiments and let's

268
00:31:43,280 --> 00:31:49,760
see where that goes at the moment it's also just an active tech report and it's it's useful already

269
00:31:49,760 --> 00:31:56,400
we see that but who knows how much better it's going to get once we use all the tricks of

270
00:31:56,400 --> 00:32:04,320
supervised learning to to improve further the current pilot implementations. And so is that being

271
00:32:04,320 --> 00:32:10,480
presented here in a workshop or something like that at NURBS? Rupesh Kumar Srivasava is actually

272
00:32:10,480 --> 00:32:16,160
going to present that I think tomorrow. Okay awesome awesome well we will definitely look that up

273
00:32:16,160 --> 00:32:23,600
and link to the archive paper in the show notes and I'm looking forward to digging a little bit

274
00:32:23,600 --> 00:32:28,000
deeper and seeing what that's all about sounds really interesting. All right well you again thanks so

275
00:32:28,000 --> 00:32:32,960
much for taking a few minutes to chat with us about you know what's new what you're up to looking

276
00:32:32,960 --> 00:32:39,200
forward to connecting once again in the future. Thank you so much for having me and it was a pleasure

277
00:32:39,200 --> 00:32:47,840
I'm being here. Absolutely thank you. All right everyone that's our show for today. For more

278
00:32:47,840 --> 00:32:55,840
information on today's show visit twomolai.com slash shows. As always thanks so much for listening

279
00:32:55,840 --> 00:33:10,800
and catch you next time.

