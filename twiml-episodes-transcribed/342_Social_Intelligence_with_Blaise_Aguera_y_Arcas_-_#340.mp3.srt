1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:26,480
I'm your host Sam Charrington.

4
00:00:26,480 --> 00:00:34,720
All right everyone, still here in Vancouver at NERPs, continuing our coverage of this

5
00:00:34,720 --> 00:00:39,880
incredible conference and I've got the pleasure of being seated with Blaz Aguera Yarkas.

6
00:00:39,880 --> 00:00:42,720
Blaz is a distinguished scientist with Google AI.

7
00:00:42,720 --> 00:00:45,120
Blaz, welcome to the Twimble AI podcast.

8
00:00:45,120 --> 00:00:46,120
Thank you so much.

9
00:00:46,120 --> 00:00:47,120
Thanks for having me.

10
00:00:47,120 --> 00:00:48,120
Absolutely.

11
00:00:48,120 --> 00:00:53,880
So, you are doing an invited talk here at the conference tomorrow morning on social intelligence

12
00:00:53,880 --> 00:00:57,800
and we're going to dig into what exactly that means for you, but before we do, I'd love

13
00:00:57,800 --> 00:00:59,960
to get a bit of your background.

14
00:00:59,960 --> 00:01:00,960
Sure.

15
00:01:00,960 --> 00:01:03,480
So, it's a little motley.

16
00:01:03,480 --> 00:01:09,180
I started off in physics, it's an undergraduate at Princeton and I studied physics and applied

17
00:01:09,180 --> 00:01:10,180
math there.

18
00:01:10,180 --> 00:01:18,080
I took a year off between my third and fourth years because I was not a very good student.

19
00:01:18,080 --> 00:01:21,840
And I really started to get into bio physics pretty heavily.

20
00:01:21,840 --> 00:01:24,400
So you're all for after?

21
00:01:24,400 --> 00:01:28,560
During, or a little bit before and then during.

22
00:01:28,560 --> 00:01:32,320
So I worked for a little while in Stan Leibler's lab there.

23
00:01:32,320 --> 00:01:36,680
He was working on bacterial chemotaxis and that actually is going to figure a little bit

24
00:01:36,680 --> 00:01:39,280
into my talk tomorrow morning.

25
00:01:39,280 --> 00:01:45,080
So it's the intelligence behaviors of bacteria and how it is that they find food.

26
00:01:45,080 --> 00:01:50,360
They're obviously a really small, simple system but maybe not quite as simple as people think.

27
00:01:50,360 --> 00:01:59,600
And then from there, my next advisor, Bill Bialik, is somebody with a physics background

28
00:01:59,600 --> 00:02:04,480
as well, but also a computational neuroscientist and he ran this course in Woods Hole at the

29
00:02:04,480 --> 00:02:09,200
Marine Biological Lab called Methods and Computational Neuroscientists, Methods and Computational

30
00:02:09,200 --> 00:02:10,200
Neuroscience.

31
00:02:10,200 --> 00:02:11,200
Sorry.

32
00:02:11,200 --> 00:02:14,840
I don't know if you're familiar or how many of your listeners are with MBL with Marine

33
00:02:14,840 --> 00:02:18,840
Biological Laboratory, but it's this place where a lot of Princeton?

34
00:02:18,840 --> 00:02:23,320
No, no, it's on Cape Cod, so it's right on the elbow of Cape Cod across from Arthur's

35
00:02:23,320 --> 00:02:24,320
Vineyard.

36
00:02:24,320 --> 00:02:28,920
It's this little tiny town, it's very cute, and there's this kind of ramshackle lab that's

37
00:02:28,920 --> 00:02:36,880
been there since the 19th century that a lot of visiting neuroscientists and biologists

38
00:02:36,880 --> 00:02:39,520
have been going to for many, many years.

39
00:02:39,520 --> 00:02:43,840
A lot of really basic discoveries in neuroscience were made there.

40
00:02:43,840 --> 00:02:45,840
So it's kind of this cool place.

41
00:02:45,840 --> 00:02:51,960
And at this course, at Methods and Computational Neuroscience, I met my now wife, Adrian

42
00:02:51,960 --> 00:02:52,960
Fairhol.

43
00:02:52,960 --> 00:02:53,960
Oh, wow.

44
00:02:53,960 --> 00:03:00,440
So she also came up in physics and studied originally chaos and turbulence and fluid dynamics and

45
00:03:00,440 --> 00:03:04,200
things like this and was making the switch to computational neuroscience, so we met

46
00:03:04,200 --> 00:03:06,720
there.

47
00:03:06,720 --> 00:03:11,560
And then she ended up getting a faculty job at University of Washington, which is how

48
00:03:11,560 --> 00:03:14,640
we ended up moving to Seattle.

49
00:03:14,640 --> 00:03:21,280
And around that time, I started a company and it was no longer really part of academia

50
00:03:21,280 --> 00:03:23,560
at that point.

51
00:03:23,560 --> 00:03:26,320
And the company got acquired by Microsoft a couple of years later.

52
00:03:26,320 --> 00:03:29,600
And the company was doing computer vision type of work, or?

53
00:03:29,600 --> 00:03:31,080
Yeah, somewhat.

54
00:03:31,080 --> 00:03:32,080
somewhat.

55
00:03:32,080 --> 00:03:37,160
It was doing sort of multi-resolution representations of documents of various kinds.

56
00:03:37,160 --> 00:03:44,200
So it was a combination of wavelet-ish kind of tricks and UX.

57
00:03:44,200 --> 00:03:46,880
I think wavelet is like Kryptonite for me.

58
00:03:46,880 --> 00:03:50,320
That was the hardest thing that I studied in grad school for whatever reason.

59
00:03:50,320 --> 00:03:52,120
It was very difficult to rock.

60
00:03:52,120 --> 00:03:53,120
It was hard.

61
00:03:53,120 --> 00:04:00,280
Yeah, my advisor in grad school in Applied Math was in Grin Dobeschi, who was one of the inventors

62
00:04:00,280 --> 00:04:01,280
of wavelet.

63
00:04:01,280 --> 00:04:02,280
Oh, wow.

64
00:04:02,280 --> 00:04:03,280
And that might have not.

65
00:04:03,280 --> 00:04:04,280
Yeah, it helped.

66
00:04:04,280 --> 00:04:08,480
She was absolutely wonderful, very, very smart, very kind.

67
00:04:08,480 --> 00:04:13,760
And I think one of the greatest living mathematicians, if I, I don't know, maybe unbiased.

68
00:04:13,760 --> 00:04:18,200
But anyway, yeah, Microsoft acquired it.

69
00:04:18,200 --> 00:04:22,560
And I did immediately turn the team toward more kind of computer visionary things right

70
00:04:22,560 --> 00:04:23,560
after that.

71
00:04:23,560 --> 00:04:29,480
So Photosynth, which started off as the Phototourism project by a University of Washington professor

72
00:04:29,480 --> 00:04:35,800
and a Microsoft research scientist, together with their grad student, Noah Snavley, was doing

73
00:04:35,800 --> 00:04:38,040
3D reconstruction environments from 2D images.

74
00:04:38,040 --> 00:04:40,680
And that was really my introduction to computer vision.

75
00:04:40,680 --> 00:04:42,040
That was very classical.

76
00:04:42,040 --> 00:04:46,120
It wasn't like deep nets or anything like this, it was geometric computer vision.

77
00:04:46,120 --> 00:04:49,240
But I kind of fell in love with that field.

78
00:04:49,240 --> 00:04:53,880
And ended up at Microsoft, you know, sort of doing a lot of leading of teams doing that

79
00:04:53,880 --> 00:04:54,880
kind of work.

80
00:04:54,880 --> 00:05:00,280
So Microsoft's, you know, OCR team and their kind of photogrammetry type teams, the teams

81
00:05:00,280 --> 00:05:04,800
that ended up doing a lot of work for HoloLens, for tracking the head using outward-facing

82
00:05:04,800 --> 00:05:08,320
cameras, all that kind of stuff was part of my team at the time.

83
00:05:08,320 --> 00:05:09,320
Oh, wow.

84
00:05:09,320 --> 00:05:10,320
So I was at Microsoft for seven years.

85
00:05:10,320 --> 00:05:16,480
So it was the CTO of Bing Maps, which also had some kind of computer vision, VR, photogrammetry

86
00:05:16,480 --> 00:05:19,880
kind of stuff going on, and Bing Mobile.

87
00:05:19,880 --> 00:05:21,760
And then I went to Google.

88
00:05:21,760 --> 00:05:23,120
That was six years ago.

89
00:05:23,120 --> 00:05:24,120
Come, of course.

90
00:05:24,120 --> 00:05:28,360
So many people that are in this field that have some connection to Bing.

91
00:05:28,360 --> 00:05:29,360
Yeah.

92
00:05:29,360 --> 00:05:32,200
Yeah, I think I shouldn't bad mouth.

93
00:05:32,200 --> 00:05:37,440
I mean, it was creative and scrappy at the time, you know, whether Microsoft was really

94
00:05:37,440 --> 00:05:41,160
committed to running these things, I guess, you know, as anybody's guess.

95
00:05:41,160 --> 00:05:42,160
Right.

96
00:05:42,160 --> 00:05:43,160
Right.

97
00:05:43,160 --> 00:05:47,160
But yeah, I mean, one of the reasons that I ended up leaving Microsoft was because about

98
00:05:47,160 --> 00:05:52,160
six years ago, they had just kind of lost the phone war and it became clear that they

99
00:05:52,160 --> 00:05:55,360
were going to be moving away from being a consumer-focused company and they were going

100
00:05:55,360 --> 00:05:57,320
to start working on just, you know, enterprise stuff.

101
00:05:57,320 --> 00:05:59,280
And that wasn't that interesting to me.

102
00:05:59,280 --> 00:06:02,600
And that was around the same time also that the whole deep learning revolution was really

103
00:06:02,600 --> 00:06:04,320
getting into full swing.

104
00:06:04,320 --> 00:06:08,760
And I was very excited about sort of machine learning and computational neuroscience

105
00:06:08,760 --> 00:06:10,080
re-converging.

106
00:06:10,080 --> 00:06:14,120
And Google is the obvious place, you know, where the kind of hotbed of a lot of that.

107
00:06:14,120 --> 00:06:15,120
So nice.

108
00:06:15,120 --> 00:06:17,120
So what do you research at Google?

109
00:06:17,120 --> 00:06:22,840
Well, at Google, I started a team called Saribra, which is not a name that we've generally

110
00:06:22,840 --> 00:06:25,560
used in public, but that's not at all heady.

111
00:06:25,560 --> 00:06:28,920
Well, well, that's the flow.

112
00:06:28,920 --> 00:06:30,920
Thank you.

113
00:06:30,920 --> 00:06:32,160
It's the plural of brain.

114
00:06:32,160 --> 00:06:34,960
So there was a brain team already that, you know, Jeff, Jeff Dean started the brain team

115
00:06:34,960 --> 00:06:36,680
a few years before.

116
00:06:36,680 --> 00:06:42,040
And I went to Google to start a team that would take a much more decentralized approach.

117
00:06:42,040 --> 00:06:46,120
So rather than one brain, it would be many brains, everybody would have a little brain.

118
00:06:46,120 --> 00:06:51,240
And I had a kind of very augmentation-focused point of view, you know, that rather than

119
00:06:51,240 --> 00:06:57,200
having, you know, one giant AI running in a data center, these things would have to shrink,

120
00:06:57,200 --> 00:07:01,400
they would have to democratize, they would have to go into devices, run locally.

121
00:07:01,400 --> 00:07:06,520
I had a lot of reasons for really wanting to push in that direction, including privacy,

122
00:07:06,520 --> 00:07:09,800
which I will talk about a bit tomorrow.

123
00:07:09,800 --> 00:07:14,040
So mobile nets and a lot of these kind of efficient ways of running neural nets locally came

124
00:07:14,040 --> 00:07:16,040
from our team.

125
00:07:16,040 --> 00:07:20,960
I, again, am running the, you know, the groups at Google, the two things like OCR and

126
00:07:20,960 --> 00:07:27,120
face recognition and a bunch of other sort of image understanding primitives.

127
00:07:27,120 --> 00:07:32,920
But we also power a lot of AI or ML features or whatever you want to call them in Android.

128
00:07:32,920 --> 00:07:37,040
And also on other kinds of devices, including these little kind of coral boards, which

129
00:07:37,040 --> 00:07:41,080
are sort of an IoT kit for doing, for doing local.

130
00:07:41,080 --> 00:07:46,480
Yeah, those were, I think those were just, well, I guess it's maybe half a year ago at

131
00:07:46,480 --> 00:07:49,440
the TensorFlow Developer Conference, I think I have one.

132
00:07:49,440 --> 00:07:50,440
Yeah, that's right.

133
00:07:50,440 --> 00:07:51,440
That's right.

134
00:07:51,440 --> 00:07:53,200
So yeah, we're very excited about those.

135
00:07:53,200 --> 00:07:54,200
Cool.

136
00:07:54,200 --> 00:07:59,360
You mentioned OCR and of all the things that we've talked about, I think of that, or it's

137
00:07:59,360 --> 00:08:03,160
probably easy to think of that as a solved problem and all the problem.

138
00:08:03,160 --> 00:08:08,280
But there's probably a lot of, you know, I guess even just saying it, there's probably

139
00:08:08,280 --> 00:08:16,640
like this last mile problem where in order to get to usable or better levels of accuracy

140
00:08:16,640 --> 00:08:21,080
and performance, kind of that those last few percentage points are really hard to get

141
00:08:21,080 --> 00:08:22,080
to.

142
00:08:22,080 --> 00:08:25,960
So yeah, you say, I mean, it solves problem and yeah, I mean, it's good enough for practical

143
00:08:25,960 --> 00:08:26,960
use.

144
00:08:26,960 --> 00:08:29,240
There are a lot of engines that are good enough for practical use.

145
00:08:29,240 --> 00:08:33,720
But A, of course, extra percentage points are always useful, you know, so a little bit

146
00:08:33,720 --> 00:08:34,800
more is always better.

147
00:08:34,800 --> 00:08:38,840
But also, the OCR team that I ran at Microsoft was still using a lot of these classical

148
00:08:38,840 --> 00:08:43,280
techniques that would first, you know, they would have a whole pipeline of different stages,

149
00:08:43,280 --> 00:08:47,720
first segmenting out letters and then, you know, doing template matching and then using

150
00:08:47,720 --> 00:08:50,080
language model and all kinds of stuff like this.

151
00:08:50,080 --> 00:08:57,320
And the direction that I think and that the people in the OCR team believe are really the

152
00:08:57,320 --> 00:09:01,400
most fruitful now are much more end-to-end and much more neural.

153
00:09:01,400 --> 00:09:05,880
So imagine it's more like a scanner that scans the entire line maybe bidirectionally and

154
00:09:05,880 --> 00:09:10,040
emits a string of characters, kind of like a speech engine might.

155
00:09:10,040 --> 00:09:15,880
If you do it that way, then, you know, joined letters and ligatures don't matter, cursive

156
00:09:15,880 --> 00:09:21,400
doesn't matter, handwriting and print could be the same, Arabic and other languages that

157
00:09:21,400 --> 00:09:25,880
don't have good, you know, distinctions between letters, I shouldn't say good, but that

158
00:09:25,880 --> 00:09:29,120
don't distinguish clearly between letters and a more cursive sort of approach.

159
00:09:29,120 --> 00:09:30,480
All of those things work.

160
00:09:30,480 --> 00:09:34,320
And that sort of generalness and also just weird fonts, you know, there are a lot of things

161
00:09:34,320 --> 00:09:38,360
that are easy for us to read that a classical OCR engine can.

162
00:09:38,360 --> 00:09:41,920
So thinking about it more like a real vision problem, you know, with a brain behind it as

163
00:09:41,920 --> 00:09:45,880
opposed to just a classical kind of letter clustering problem with the language model

164
00:09:45,880 --> 00:09:46,880
tacked on.

165
00:09:46,880 --> 00:09:53,400
So is the focus of that work today achieving the level of accuracy that we previously

166
00:09:53,400 --> 00:09:58,480
achieve with traditional approaches, with deep approaches, or have we-

167
00:09:58,480 --> 00:10:01,360
Oh, that's not a far surpasser past, yeah, we've already far surpassed.

168
00:10:01,360 --> 00:10:02,360
Okay.

169
00:10:02,360 --> 00:10:03,360
Right.

170
00:10:03,360 --> 00:10:08,160
I mean, the goal now is to be able to do that in a way that is compact, real time runs

171
00:10:08,160 --> 00:10:12,800
on devices, doesn't- it doesn't have to be told what language something is in or what

172
00:10:12,800 --> 00:10:16,960
kind of script has a unified model for every imaginable kind of screen, you know, those

173
00:10:16,960 --> 00:10:17,960
kind of goals, right?

174
00:10:17,960 --> 00:10:20,720
So the kind of things that a person can do.

175
00:10:20,720 --> 00:10:24,120
But yeah, the neural methods have long surpassed the classical methods.

176
00:10:24,120 --> 00:10:25,120
Got it.

177
00:10:25,120 --> 00:10:32,280
Yeah, Jeff has been on the podcast previously and has mentioned that the transition from

178
00:10:32,280 --> 00:10:37,840
traditional machine translation to neural machine translation resulted among other

179
00:10:37,840 --> 00:10:42,480
things in increased performance and a reduction in the size of that code base from half a million

180
00:10:42,480 --> 00:10:47,600
lines of code to- I forget what the number was 50 or something like an astounding number.

181
00:10:47,600 --> 00:10:48,600
Is there a similar-

182
00:10:48,600 --> 00:10:49,600
It's exactly the same.

183
00:10:49,600 --> 00:10:50,600
Exactly the same.

184
00:10:50,600 --> 00:10:51,600
Exactly the same story.

185
00:10:51,600 --> 00:10:55,440
And learning, you know, the code is very- it suddenly becomes very small because all

186
00:10:55,440 --> 00:11:00,440
of the structure, all the statistical structure in the thing is being learned rather than

187
00:11:00,440 --> 00:11:01,440
coded.

188
00:11:01,440 --> 00:11:05,040
And it means that a lot of the assumptions that you might make in that code don't have

189
00:11:05,040 --> 00:11:10,320
to be made at a programming time like letters being distinct or being read from left to

190
00:11:10,320 --> 00:11:16,160
right or, you know, or not being slanted, being able to, you know, to be kind of characterized

191
00:11:16,160 --> 00:11:18,320
in terms of connected components or boxes and so-

192
00:11:18,320 --> 00:11:19,320
Right.

193
00:11:19,320 --> 00:11:22,040
And so you're invited to talk here at NURBS is about OCR.

194
00:11:22,040 --> 00:11:23,040
No.

195
00:11:23,040 --> 00:11:24,040
No.

196
00:11:24,040 --> 00:11:29,240
No, the team is pretty big and OCR is only three or four people I think.

197
00:11:29,240 --> 00:11:30,240
Yeah.

198
00:11:30,240 --> 00:11:31,240
Yeah.

199
00:11:31,240 --> 00:11:37,520
What about social intelligence, which is more related to the concept of cerebris and many

200
00:11:37,520 --> 00:11:38,520
brains.

201
00:11:38,520 --> 00:11:40,240
I imagine.

202
00:11:40,240 --> 00:11:43,720
What exactly are you discussing in the talk?

203
00:11:43,720 --> 00:11:44,720
Right.

204
00:11:44,720 --> 00:11:49,760
Well, it's a wide-ranging talk and, you know, I guess it has the shape of, you know,

205
00:11:49,760 --> 00:11:53,560
like some- a lot of very broad considerations at the beginning and then some specific technical

206
00:11:53,560 --> 00:11:57,240
work in the middle and then maybe broadening it back out a little bit at the end.

207
00:11:57,240 --> 00:12:04,600
The broad themes are that, you know, I guess we've gone to the point like with OCR that

208
00:12:04,600 --> 00:12:08,760
if you know what the goal is, if you're able to score or to make a loss function for

209
00:12:08,760 --> 00:12:13,320
what you're doing and we have lots of training data like we do with OCR or faces or whatever.

210
00:12:13,320 --> 00:12:15,440
A couple of my considerations.

211
00:12:15,440 --> 00:12:16,440
Minor considerations.

212
00:12:16,440 --> 00:12:17,440
Yeah.

213
00:12:17,440 --> 00:12:18,440
Obviously they're not here.

214
00:12:18,440 --> 00:12:19,440
Well, but this is the thing.

215
00:12:19,440 --> 00:12:22,440
I don't think that they used to be considered, you know, all that limiting.

216
00:12:22,440 --> 00:12:23,440
Right.

217
00:12:23,440 --> 00:12:28,160
In the days of, I don't know, the Dartmouth Summer Project or something, you know, people

218
00:12:28,160 --> 00:12:33,080
were like, we don't know how to solve AI, you know, think about it, you know, only brains

219
00:12:33,080 --> 00:12:37,040
can do things like understand language and writing and so on.

220
00:12:37,040 --> 00:12:42,760
Surely if we figure out how to do those kinds of things, we'll crack the secret of intelligence.

221
00:12:42,760 --> 00:12:46,400
And now we're like the dog that caught the car, you know, like we, you know, any of those

222
00:12:46,400 --> 00:12:51,160
things that can be characterized cleanly, we can achieve superhuman performance basically.

223
00:12:51,160 --> 00:12:54,140
I mean, I'm making it slightly for looking statement, not all of these things are superhuman

224
00:12:54,140 --> 00:12:58,160
yet, but it's, you know, like if you, if you say something isn't then like next year

225
00:12:58,160 --> 00:12:59,160
it will be.

226
00:12:59,160 --> 00:13:00,160
Right.

227
00:13:00,160 --> 00:13:05,560
You know, so, so like we've solved it and yet none of the systems that we've built are

228
00:13:05,560 --> 00:13:09,040
intelligent in a way that you or I would recognize as intelligence.

229
00:13:09,040 --> 00:13:10,040
Right.

230
00:13:10,040 --> 00:13:14,000
It's, they're just, it's just functions, you know, it's just progression really in

231
00:13:14,000 --> 00:13:15,000
the end.

232
00:13:15,000 --> 00:13:20,000
And that's why we say, you know, like AI or ML data, ML slash data science in a way, you

233
00:13:20,000 --> 00:13:24,340
know, the, the projects of AI and data science could not be more different, you know, data

234
00:13:24,340 --> 00:13:28,840
science is just about, you know, modeling data and AI is about making minds.

235
00:13:28,840 --> 00:13:32,200
I mean, on the face of it, like really are these the same thing.

236
00:13:32,200 --> 00:13:36,140
So, you know, I think if there's a single theme for my talk, it's like, well, what is that,

237
00:13:36,140 --> 00:13:39,960
what is that gap and why and why is it there?

238
00:13:39,960 --> 00:13:46,640
The reason that I called it social AI or social intelligence was that I've come to believe

239
00:13:46,640 --> 00:13:52,440
in the last few years that, that sociality, which is to say, our interactions with each

240
00:13:52,440 --> 00:13:58,240
other are not incidental to intelligence, but are actually fundamental to it.

241
00:13:58,240 --> 00:14:03,920
In the sense that life isn't a one player game, it's not like we evolved intelligence

242
00:14:03,920 --> 00:14:10,080
as a, as an adaptation in order to get by in a really hard video game environment where

243
00:14:10,080 --> 00:14:13,800
nature is trying to kill us and we have to outwit it.

244
00:14:13,800 --> 00:14:19,300
On the contrary, like an individual human is a lot worse at outwitting nature than our

245
00:14:19,300 --> 00:14:23,200
A-band sisters, like drop one of us naked in the jungle and like only a handful of us

246
00:14:23,200 --> 00:14:28,480
will make it, you know, maybe the piraha, you know, in Brazil or something, but it's

247
00:14:28,480 --> 00:14:29,800
very few people who can make it.

248
00:14:29,800 --> 00:14:31,000
We get to watch it on TV.

249
00:14:31,000 --> 00:14:33,000
Yeah, exactly, exactly.

250
00:14:33,000 --> 00:14:35,680
So, you know, our environment is each other.

251
00:14:35,680 --> 00:14:43,620
And there, there is a famous researcher, Robin Dunbar, who proposed the social

252
00:14:43,620 --> 00:14:47,400
cortex hypothesis way back in the 90s.

253
00:14:47,400 --> 00:14:53,000
And he's not the first France V underval and France devalze and many other researchers

254
00:14:53,000 --> 00:14:59,720
of the same kind of idea that we are essentially the role of intelligence is to model others.

255
00:14:59,720 --> 00:15:04,920
And since you and I are of the same species, it's useful for me to be able to model you

256
00:15:04,920 --> 00:15:08,360
and to understand what you're going to do.

257
00:15:08,360 --> 00:15:12,320
And so my brain will grow bigger in order to model yours, but we share genes.

258
00:15:12,320 --> 00:15:16,200
So in the process, your brain grows bigger as well, and you have a kind of, you know,

259
00:15:16,200 --> 00:15:20,120
arms race or a feedback loop, and that's how you get these kind of intelligence explosions

260
00:15:20,120 --> 00:15:26,040
that we've seen in the apes and in cetaceans and dolphins and whales and some other species.

261
00:15:26,040 --> 00:15:32,360
Echoes in a lot of ways, Yvahar Ari's from sapiens, idea that our kind of core achievement

262
00:15:32,360 --> 00:15:37,720
as a species is the ability to collaborate and communicate goals and stories to one another

263
00:15:37,720 --> 00:15:40,720
and kind of project forward.

264
00:15:40,720 --> 00:15:43,480
Yeah, I absolutely agree with that point of view.

265
00:15:43,480 --> 00:15:49,880
There are a bunch of recent books, Harari's are among them, but also Nicholas Kristakis,

266
00:15:49,880 --> 00:15:55,520
Blueprint, the human swarm, Moffitt's book, Pat Churchill's book, Conscience, a number

267
00:15:55,520 --> 00:15:59,560
of books that I think really, you know, sort of start to expound that point of view.

268
00:15:59,560 --> 00:16:04,960
But I don't think a lot of that has been heard in our community in the kind of AI or

269
00:16:04,960 --> 00:16:05,960
ML communities.

270
00:16:05,960 --> 00:16:14,440
So you're proposing this idea that intelligence is a collaborative idea, how does that manifest

271
00:16:14,440 --> 00:16:18,880
itself or what are some of the kind of next layer points that you're making in the talk?

272
00:16:18,880 --> 00:16:19,880
Sure.

273
00:16:19,880 --> 00:16:25,080
Well, so one of them is that when you expand, when you kind of zoom out to think about

274
00:16:25,080 --> 00:16:30,600
not just the intelligence of an individual, but the intelligence of a group of a society,

275
00:16:30,600 --> 00:16:33,760
which, you know, most of our achievements are, of course.

276
00:16:33,760 --> 00:16:34,760
Right.

277
00:16:34,760 --> 00:16:39,560
And then you are in a multi-agent kind of universe, if you want to think about it in kind

278
00:16:39,560 --> 00:16:41,080
of our terms.

279
00:16:41,080 --> 00:16:47,040
And in a multi-agent universe, even if every individual agent is doing optimization, in

280
00:16:47,040 --> 00:16:51,480
other words, has a clear loss function or objective and is doing, I don't know, gradient descent

281
00:16:51,480 --> 00:16:57,120
to optimize it, when you zoom back and look at the whole, that isn't a longer the case.

282
00:16:57,120 --> 00:17:00,240
And you can see that even in a really simple example, like, you know, the most minimal

283
00:17:00,240 --> 00:17:05,840
ecology you can make is an ecology of two things, and GANs are an example of an ecology of

284
00:17:05,840 --> 00:17:06,840
two things.

285
00:17:06,840 --> 00:17:07,840
Right.

286
00:17:07,840 --> 00:17:11,640
You have an actor and an artist and a critic, and their goals are different, and they're

287
00:17:11,640 --> 00:17:12,640
kind of misaligned.

288
00:17:12,640 --> 00:17:13,640
Right.

289
00:17:13,640 --> 00:17:14,640
The artist is trying to fool the critic.

290
00:17:14,640 --> 00:17:17,880
The critic is trying to winkle out, you know, to sauce out the artist.

291
00:17:17,880 --> 00:17:23,560
And when you look at what happens in the interaction between those two, they pursue each other, and

292
00:17:23,560 --> 00:17:26,640
there's a spiral, you know, so this is a dynamical system.

293
00:17:26,640 --> 00:17:31,360
And dynamical systems have chaos, you know, they have vorticity, they have limit cycles.

294
00:17:31,360 --> 00:17:33,560
And that does not look like gradient descent anymore.

295
00:17:33,560 --> 00:17:38,440
Like, if you look at the dynamics of gradient descent, it looks like a curl free field.

296
00:17:38,440 --> 00:17:39,440
Curl free?

297
00:17:39,440 --> 00:17:40,440
Yes.

298
00:17:40,440 --> 00:17:44,280
Meaning that there's no twist in trajectories.

299
00:17:44,280 --> 00:17:45,680
Everything goes downhill.

300
00:17:45,680 --> 00:17:48,880
So trajectories have no curve in them.

301
00:17:48,880 --> 00:17:53,520
Whereas these GANs, you know, that's why they're so quite hard to train, you know, because

302
00:17:53,520 --> 00:17:56,760
they have predator-prey kinds of dynamics, essentially.

303
00:17:56,760 --> 00:18:01,600
So what happens is a lot more complicated, you know, it so happens that GANs were invented

304
00:18:01,600 --> 00:18:07,280
to have a fixed point that coincides with the optimum of a function that can be written

305
00:18:07,280 --> 00:18:10,720
down, which reproduces a distribution of PFX.

306
00:18:10,720 --> 00:18:14,840
But that's just, you know, that's just an artifact of how we cooked that particular one

307
00:18:14,840 --> 00:18:15,840
up.

308
00:18:15,840 --> 00:18:20,120
And in fact, it's not as if GANs, when they converge or are in general, necessarily at

309
00:18:20,120 --> 00:18:22,680
such a global optimum either, you know.

310
00:18:22,680 --> 00:18:27,720
So that's like the minimal ecology, and when you consider that we're made out of cells,

311
00:18:27,720 --> 00:18:32,240
you know, neurons are cells that each have their own objectives and so on.

312
00:18:32,240 --> 00:18:36,240
It's not just ecologies when you look at, say, multiple people that have that property,

313
00:18:36,240 --> 00:18:40,720
but a single brain or a single thing is in turn composed of some things that have their

314
00:18:40,720 --> 00:18:42,640
own goals and agendas.

315
00:18:42,640 --> 00:18:45,480
So it's, you know, it turtles all the way down and all the way up.

316
00:18:45,480 --> 00:18:49,440
And when you start looking at it that way, then I guess I'm not saying that optimization

317
00:18:49,440 --> 00:18:54,560
is dumb or is a bad way to look at things, but it's more like that's just a local force.

318
00:18:54,560 --> 00:19:00,480
And when you look at it at the system, the behavior of that system cannot be determined

319
00:19:00,480 --> 00:19:03,640
by just looking at those local forces, you have to look at the entire picture.

320
00:19:03,640 --> 00:19:09,600
I mean, it's, it's especially fascinating when you think of it in the context of research

321
00:19:09,600 --> 00:19:15,600
like, you know, how much of our behavior is controlled by our microbiome or I was just

322
00:19:15,600 --> 00:19:21,880
watching some TV show, maybe it was like our Earth type of show that showed, you know,

323
00:19:21,880 --> 00:19:26,840
how some parasite would infect these ants and cause them to do the, like, these zombie-like

324
00:19:26,840 --> 00:19:34,320
behavior and control, which I think underscores your point that, you know, so much wall behavior

325
00:19:34,320 --> 00:19:38,520
and, you know, manifest behavior that we see is controlled by kind of the interactions

326
00:19:38,520 --> 00:19:42,520
of things as opposed to, you know, some optimization function.

327
00:19:42,520 --> 00:19:43,520
Exactly.

328
00:19:43,520 --> 00:19:51,680
Very between parasitism or predation or exploitation and symbiosis is, not only is it a fine line,

329
00:19:51,680 --> 00:19:53,280
I'm not even sure the line exists.

330
00:19:53,280 --> 00:19:54,280
Well, certainly.

331
00:19:54,280 --> 00:19:59,280
If you step back and look at a system, right, right, right, right, what is the system's

332
00:19:59,280 --> 00:20:02,280
optimization function may be very different from that of the ant.

333
00:20:02,280 --> 00:20:03,280
That's right.

334
00:20:03,280 --> 00:20:06,360
But even if you look at things, you know, even if you try to make firm boundaries around

335
00:20:06,360 --> 00:20:10,440
say an individual person, I mean, like there was, there was a recent discovery that I thought

336
00:20:10,440 --> 00:20:13,840
was really cool about the arc virus.

337
00:20:13,840 --> 00:20:17,600
So there are a lot of retroviruses that have been incorporated into our gene, into our

338
00:20:17,600 --> 00:20:20,720
germline, into our genome that are passed on.

339
00:20:20,720 --> 00:20:24,640
Some of them for many, many hundreds of millions of years.

340
00:20:24,640 --> 00:20:27,520
And the arc virus is one of those.

341
00:20:27,520 --> 00:20:32,640
So you know, we've known that it's there for a long time, but nobody knew what it was

342
00:20:32,640 --> 00:20:33,640
doing.

343
00:20:33,640 --> 00:20:34,640
It was finally caught.

344
00:20:34,640 --> 00:20:35,960
So I understand this.

345
00:20:35,960 --> 00:20:41,440
You say past or kind of incorporated into our geneful meaning, the body fundamentally

346
00:20:41,440 --> 00:20:44,040
creates this virus as part of existing.

347
00:20:44,040 --> 00:20:45,240
That's exactly right.

348
00:20:45,240 --> 00:20:47,600
Like the virus is no longer distinct from your genetics.

349
00:20:47,600 --> 00:20:51,600
It's not that it's, you know, endemic in a population and gets passed.

350
00:20:51,600 --> 00:20:52,960
It's literally part of your DNA.

351
00:20:52,960 --> 00:20:53,960
Okay.

352
00:20:53,960 --> 00:20:57,880
So these are retroviruses that inject themselves into the DNA and into the germline and propagate

353
00:20:57,880 --> 00:21:02,480
along with you, just like mitochondria, you know, those are bacteria, of course, right?

354
00:21:02,480 --> 00:21:06,920
We're full of some of these kind of symbiotic things and arc, it turns out, you know, it was

355
00:21:06,920 --> 00:21:11,640
caught in this electron microscope actually forming its viral coat and, you know, forming

356
00:21:11,640 --> 00:21:12,640
its capsid.

357
00:21:12,640 --> 00:21:13,640
Okay.

358
00:21:13,640 --> 00:21:15,960
And if you knock it out in mice, they can't remember anything longer than 24 hours.

359
00:21:15,960 --> 00:21:16,960
Wow.

360
00:21:16,960 --> 00:21:21,440
So like they're involved in some way that we don't fully understand in memory formation.

361
00:21:21,440 --> 00:21:22,440
Huh.

362
00:21:22,440 --> 00:21:23,960
You know, wow.

363
00:21:23,960 --> 00:21:24,960
Right.

364
00:21:24,960 --> 00:21:28,040
Some people think that AIDS, you know, it was kind of on the way.

365
00:21:28,040 --> 00:21:32,400
I mean, hopefully we've now, you know, we've now sort of controlled it.

366
00:21:32,400 --> 00:21:37,080
But we might have essentially been witnessing a kind of event that has happened many, many

367
00:21:37,080 --> 00:21:43,840
times in human history, wherein, you know, a virus begins in very virulent and devastating,

368
00:21:43,840 --> 00:21:49,160
but eventually kind of goes global as it were and becomes incorporated into our genes

369
00:21:49,160 --> 00:21:50,160
for good.

370
00:21:50,160 --> 00:21:51,480
And so to make this more concrete.

371
00:21:51,480 --> 00:21:52,480
Yes.

372
00:21:52,480 --> 00:21:53,480
Yes.

373
00:21:53,480 --> 00:21:54,480
Yes.

374
00:21:54,480 --> 00:21:59,800
Well, it may be before, you know, a pit stop before we go into more concrete and technical,

375
00:21:59,800 --> 00:22:01,600
you know, it calls to mind.

376
00:22:01,600 --> 00:22:06,400
Obviously, I guess kind of a lot of the research in, I think, maybe the 80s or 70s, like

377
00:22:06,400 --> 00:22:15,880
swarming behaviors and things like that, which imagining are very simplistic models relative

378
00:22:15,880 --> 00:22:17,600
to the things that you're thinking about.

379
00:22:17,600 --> 00:22:18,600
Well, yes, yes.

380
00:22:18,600 --> 00:22:19,600
Also, factable.

381
00:22:19,600 --> 00:22:20,600
Yeah.

382
00:22:20,600 --> 00:22:21,600
That's right.

383
00:22:21,600 --> 00:22:25,760
So that's, I was, if I have time tomorrow, I'm going to show two things, two sort of technical

384
00:22:25,760 --> 00:22:31,120
things, one of which is very simple and is very much along those kind of 80s swarming

385
00:22:31,120 --> 00:22:32,120
sorts of lines.

386
00:22:32,120 --> 00:22:34,560
It's a simulation of bacteria.

387
00:22:34,560 --> 00:22:41,160
So trivial system, the learning is very, very simple and so you can characterize it completely.

388
00:22:41,160 --> 00:22:47,080
And the other one is about neural nets and update rules for cells and synapses.

389
00:22:47,080 --> 00:22:50,880
So that one applies to, you know, to the kind of systems that we build.

390
00:22:50,880 --> 00:22:55,400
So the bacteria one, yeah, it's very much inspired by that kind of centiphanes to, you

391
00:22:55,400 --> 00:22:58,160
know, sort of work from the 80s and 90s.

392
00:22:58,160 --> 00:23:03,760
And the idea there is, you know, bacteria have E. coli in particular, can either run or

393
00:23:03,760 --> 00:23:04,760
tumble.

394
00:23:04,760 --> 00:23:07,160
So when they run, they're going in a straight line and when they tumble, they randomize

395
00:23:07,160 --> 00:23:08,160
their direction.

396
00:23:08,160 --> 00:23:11,680
It has to do with the direction the flagella are rotating.

397
00:23:11,680 --> 00:23:15,840
And you know, they're too small to have a sense of like the global environment, you know,

398
00:23:15,840 --> 00:23:19,680
they can just go straight or, or kind of reverse, kind of like those RC cars that have

399
00:23:19,680 --> 00:23:24,040
a like go straight or, you know, reverse and turn, kind of, so it's a one bit output.

400
00:23:24,040 --> 00:23:30,040
And so I did a little simulation of them and they have a food source that moves around

401
00:23:30,040 --> 00:23:32,840
and if they don't get enough food, they die.

402
00:23:32,840 --> 00:23:37,400
And if they, if their food gets, if their energy level gets high enough, they reproduce.

403
00:23:37,400 --> 00:23:43,080
They can also conjugate when they touch, they can swap a little bit of DNA and they, and

404
00:23:43,080 --> 00:23:44,600
they, they hand her go some mutations.

405
00:23:44,600 --> 00:23:46,480
So it's artificial evolution.

406
00:23:46,480 --> 00:23:52,520
The first experiment involves thinking about the genome as just being the Q table.

407
00:23:52,520 --> 00:23:56,000
In other words, you know, in RL language, right, just that the table, you know, stimulated

408
00:23:56,000 --> 00:23:57,400
to behavior.

409
00:23:57,400 --> 00:24:01,680
And you can see how, you know, with this kind of evolutionary pressure, they evolve to follow

410
00:24:01,680 --> 00:24:02,680
the food.

411
00:24:02,680 --> 00:24:05,720
There's a kind of, you know, their algorithms that let you do that, essentially they involve

412
00:24:05,720 --> 00:24:08,840
tumbling more when you're in the food and running more when you're far away from the

413
00:24:08,840 --> 00:24:09,840
food.

414
00:24:09,840 --> 00:24:13,840
And statistically that will, that will make the colony of bacteria follow the food around.

415
00:24:13,840 --> 00:24:20,040
I'm imagining the picture that comes to mind for me is the kind of the classic picture

416
00:24:20,040 --> 00:24:26,080
of an RL agent, I forget the game, but it's like a boat game where the RL agent kind of learns

417
00:24:26,080 --> 00:24:29,720
that it can rack up points by just spinning around in this particular place.

418
00:24:29,720 --> 00:24:30,720
Yeah.

419
00:24:30,720 --> 00:24:31,720
Yeah, that's right.

420
00:24:31,720 --> 00:24:36,120
Well, I mean, one of the fun things, of course, about RL is that any kind of machine learning

421
00:24:36,120 --> 00:24:39,120
really is that, you know, it can, it'll, if there's a way to cheat, it'll figure it

422
00:24:39,120 --> 00:24:40,120
out.

423
00:24:40,120 --> 00:24:41,120
Right.

424
00:24:41,120 --> 00:24:42,120
Like life.

425
00:24:42,120 --> 00:24:43,120
Right.

426
00:24:43,120 --> 00:24:44,120
That's right.

427
00:24:44,120 --> 00:24:46,640
So, yeah, the, it figures out how to follow the food around this way.

428
00:24:46,640 --> 00:24:49,720
Things get a little more interesting if you add an additional output and you let them

429
00:24:49,720 --> 00:24:52,120
emit an extra chemical that they can also sense.

430
00:24:52,120 --> 00:24:53,120
Okay.

431
00:24:53,120 --> 00:24:56,560
So, that, and that's done by bacteria, they have chemo, chemo signaling.

432
00:24:56,560 --> 00:25:00,160
So, you know, in the rules of the game that I set up, if they're signaling, they're losing

433
00:25:00,160 --> 00:25:01,160
health faster.

434
00:25:01,160 --> 00:25:02,160
So, it's costly.

435
00:25:02,160 --> 00:25:03,160
It's costly to signal.

436
00:25:03,160 --> 00:25:06,880
So, you, you would, you would first think that, well, the first thing to learn is to

437
00:25:06,880 --> 00:25:12,760
not signal because, you know, it's, it has no advantage for them, basically, and it,

438
00:25:12,760 --> 00:25:14,520
and it burns energy faster.

439
00:25:14,520 --> 00:25:15,520
But they don't.

440
00:25:15,520 --> 00:25:16,520
They keep signaling.

441
00:25:16,520 --> 00:25:20,480
So, in, you know, generation after generation, you know, restart after restart, they, you

442
00:25:20,480 --> 00:25:23,720
know, almost all of them retain the signaling capability.

443
00:25:23,720 --> 00:25:28,400
And what you realize, of course, is that they become a super organism when they, when they

444
00:25:28,400 --> 00:25:30,160
start to share genes.

445
00:25:30,160 --> 00:25:34,760
And, you know, so, thinking about them as individuals or as a, or as a single, as, as a

446
00:25:34,760 --> 00:25:38,520
single organism, as a tissue, you know, it's kind of like in the eye of the beholder.

447
00:25:38,520 --> 00:25:39,520
There's no, right?

448
00:25:39,520 --> 00:25:42,240
It's, it's, that's not determined by the rules of the game.

449
00:25:42,240 --> 00:25:49,200
And so, are we talking about the observed behavior in the game or in the thing that you

450
00:25:49,200 --> 00:25:50,200
model?

451
00:25:50,200 --> 00:25:51,200
Yes and yes.

452
00:25:51,200 --> 00:25:52,200
Yes, yes and yes.

453
00:25:52,200 --> 00:25:57,040
I mean, in the model, you can reduce everything to very, that's a very, very simple principle

454
00:25:57,040 --> 00:26:01,120
since, and see all of those behaviors emerge, which is, which is fun, because then you,

455
00:26:01,120 --> 00:26:03,240
you know, then it becomes clear that we're understanding something.

456
00:26:03,240 --> 00:26:05,240
So, yeah, they signal to each other.

457
00:26:05,240 --> 00:26:06,920
They obviously are behaving like a super organism.

458
00:26:06,920 --> 00:26:11,520
You can't say what is, you know, what is the agent, you know, is it one, is it many?

459
00:26:11,520 --> 00:26:16,320
And it's also kind of hard to say what they're optimizing, you know, like, if you take a

460
00:26:16,320 --> 00:26:20,040
very Darwinian red of tooth and claw kind of perspective, they're like, well, they're,

461
00:26:20,040 --> 00:26:23,640
they're trying to, they're maximizing their energy intake, well, but who is maximizing

462
00:26:23,640 --> 00:26:24,640
their energy intake?

463
00:26:24,640 --> 00:26:26,480
Is it an individual bacterium?

464
00:26:26,480 --> 00:26:28,640
Is it the whole colony?

465
00:26:28,640 --> 00:26:33,680
How do you think about, you know, the population or the size relative to what is being taken

466
00:26:33,680 --> 00:26:35,480
and what does optimization actually look like?

467
00:26:35,480 --> 00:26:42,400
Because in simulation, you observe behaviors that are sub-optimal, either individually

468
00:26:42,400 --> 00:26:46,840
or globally, but optimal otherwise, or well, even talking about optimal is hard.

469
00:26:46,840 --> 00:26:50,520
So I, you know, you just, you know, you're just going to the optimization that you propose,

470
00:26:50,520 --> 00:26:55,120
that it's all about energy intake and thus kind of speed of reproduction.

471
00:26:55,120 --> 00:26:59,000
Well, all I said was that they die if they go to zero and they reproduce if they get

472
00:26:59,000 --> 00:27:00,000
to one.

473
00:27:00,000 --> 00:27:01,000
I didn't say what was being optimized.

474
00:27:01,000 --> 00:27:02,000
Okay.

475
00:27:02,000 --> 00:27:05,360
So, you know, the thing with evolution is that like, what you see is what makes it, you

476
00:27:05,360 --> 00:27:09,320
know, what persists exists, you know, we're not actually writing down a laws function.

477
00:27:09,320 --> 00:27:13,320
So you can ask what is optimized and that's actually an inverse reinforcement learning

478
00:27:13,320 --> 00:27:14,320
problem.

479
00:27:14,320 --> 00:27:16,840
In other words, you have a queue table, now you back out policy and what's the right,

480
00:27:16,840 --> 00:27:18,440
what's the reward.

481
00:27:18,440 --> 00:27:22,480
And I kind of cheated in order to get, so IRL inverse reinforcement learning is actually

482
00:27:22,480 --> 00:27:27,920
a hard problem in general, but I cheated by switching things up so that rather than the

483
00:27:27,920 --> 00:27:31,880
genome being the queue table, now the genome is the reward map.

484
00:27:31,880 --> 00:27:35,920
So they essentially evolve a reward system or an emotional system if you want to think

485
00:27:35,920 --> 00:27:36,920
about it that way.

486
00:27:36,920 --> 00:27:42,280
And we see like, well, what's the reward system that actually results in survival.

487
00:27:42,280 --> 00:27:45,600
And what comes out is kind of what should expect, which is that in general, you know,

488
00:27:45,600 --> 00:27:51,000
you look at this many, many times, death is bad, signaling is bad, because that's, you

489
00:27:51,000 --> 00:27:57,600
know, that hurts and you lose health, right, mating is good, survival is good, food

490
00:27:57,600 --> 00:28:00,400
is good, you know, those are positive, these are negative, so it's, you know, it's kind

491
00:28:00,400 --> 00:28:03,680
of what you'd expect, but the surprise is the error bars.

492
00:28:03,680 --> 00:28:08,480
So when you look at specific winning solutions, that is solutions that, you know, converge

493
00:28:08,480 --> 00:28:12,640
and that, and that persist, they're all over the map.

494
00:28:12,640 --> 00:28:15,880
So those are averages that I just gave you, but the variance is huge.

495
00:28:15,880 --> 00:28:20,040
They're all these different reward maps that all work, you know, they, some of them result

496
00:28:20,040 --> 00:28:24,480
in, meaning dying off is a lot harder than continuing in a sense.

497
00:28:24,480 --> 00:28:28,880
Well, the, the set of, if you can find a, I mean, the set of, of, of emotional systems

498
00:28:28,880 --> 00:28:34,400
or rewards that work is small relative to the total imaginable space of reward maps,

499
00:28:34,400 --> 00:28:39,520
but it's also very varied, right, and there's not just one, there's not just one, okay,

500
00:28:39,520 --> 00:28:44,520
but there are many and some of them involve lots of exploration and therefore death is

501
00:28:44,520 --> 00:28:49,800
not bad for those, they're not afraid of death in some sense, some of them are very conservative

502
00:28:49,800 --> 00:28:53,400
and therefore have smaller populations, they hate death, they don't signal very much,

503
00:28:53,400 --> 00:28:56,480
they follow the food run really closely and all of those are viable.

504
00:28:56,480 --> 00:29:00,840
So, you know, this is kind of one data point, just so we don't get stuck in time on this

505
00:29:00,840 --> 00:29:03,640
thing, because I can ask you a bunch of questions about this particular thing.

506
00:29:03,640 --> 00:29:08,400
This is one data point that informs a lot, a larger perspective.

507
00:29:08,400 --> 00:29:10,920
What's the other, you mentioned a second example?

508
00:29:10,920 --> 00:29:11,920
Yes.

509
00:29:11,920 --> 00:29:16,680
So, yeah, all of this is really just a kind of extended example of, you know, of some

510
00:29:16,680 --> 00:29:21,560
of the problems that arise when you try and look at a, a real evolutionary system in

511
00:29:21,560 --> 00:29:25,880
terms of optimization and try and ask basic questions like what is it optimizing?

512
00:29:25,880 --> 00:29:31,520
So, I then take that to the home, I guess, to the thing that concerns all of us at this

513
00:29:31,520 --> 00:29:36,080
conference, which is, all right, so how do you train a model?

514
00:29:36,080 --> 00:29:43,200
There's been a bunch of work in recent years on meta-learning, which involves, you know,

515
00:29:43,200 --> 00:29:44,200
learning to learn.

516
00:29:44,200 --> 00:29:49,880
So, learning maybe what the update rule is at a synapse or learning how, you know, not

517
00:29:49,880 --> 00:29:52,480
assuming that the learning algorithm is fixed.

518
00:29:52,480 --> 00:29:56,880
And I really like that approach because, you know, our genome basically has in it the

519
00:29:56,880 --> 00:30:00,600
learning rule for our neurons and synapses and that evolved.

520
00:30:00,600 --> 00:30:06,240
So, I began playing around with systems that evolve, use evolution to determine the rules

521
00:30:06,240 --> 00:30:11,480
for neuron state and synapse state over time.

522
00:30:11,480 --> 00:30:13,360
And therefore, that have to learn to learn.

523
00:30:13,360 --> 00:30:16,920
They learn to learn on an evolutionary time scale and they learn on a behavioral time

524
00:30:16,920 --> 00:30:17,920
scale.

525
00:30:17,920 --> 00:30:18,920
That makes sense.

526
00:30:18,920 --> 00:30:24,800
So, the learning to learning includes instinct and imprinting and other kinds of things.

527
00:30:24,800 --> 00:30:32,520
And the learning to learn, you know, establishes the ability to go from stimuli to generalization.

528
00:30:32,520 --> 00:30:37,260
And if you do that, you basically have a little kind of LSTM at every neuron and every

529
00:30:37,260 --> 00:30:44,160
synapse that has local state and has its own time scales and so on that it learns evolutionarily.

530
00:30:44,160 --> 00:30:49,280
And you can get very, very fast learning of standard kind of machine learning models.

531
00:30:49,280 --> 00:30:51,960
I give, you know, some M-nist type examples.

532
00:30:51,960 --> 00:30:53,960
So learning from very, very few examples.

533
00:30:53,960 --> 00:30:57,520
And that's interesting if you just think about it in terms of optimization.

534
00:30:57,520 --> 00:31:02,600
You know, this is in the spirit of some other work that has been done, a Ravi and La Rochelle

535
00:31:02,600 --> 00:31:08,160
wrote a really nice paper in 2016 or 2017 that did something similar.

536
00:31:08,160 --> 00:31:13,360
My version of this is a little more general in that it's not designed necessarily to optimize.

537
00:31:13,360 --> 00:31:14,360
It's kind of broader.

538
00:31:14,360 --> 00:31:17,600
It's just like any old, you know, it's a very general kind of synapse rule in a very

539
00:31:17,600 --> 00:31:19,880
general cellular evolution rule that I don't say.

540
00:31:19,880 --> 00:31:21,200
So I understand this.

541
00:31:21,200 --> 00:31:28,200
You've got the picture that's forming in my head is kind of multiple agents or entities

542
00:31:28,200 --> 00:31:35,320
that have some kind of embedded, you know, memory, sequence, LSTM based thing.

543
00:31:35,320 --> 00:31:41,040
And in their interactions, they can essentially learn and solve M-nist type problems.

544
00:31:41,040 --> 00:31:45,560
So I do show an example like that, but I mean, if it's similar at all what you're saying.

545
00:31:45,560 --> 00:31:47,800
Well, the simpler version is as follows.

546
00:31:47,800 --> 00:31:48,800
Okay.

547
00:31:48,800 --> 00:31:53,160
Imagine that you just have, you know, in one neural net, a single kind of LSTM, a single

548
00:31:53,160 --> 00:31:57,240
set of weights for an LSTM that live at every cell and every synapse.

549
00:31:57,240 --> 00:31:58,640
It's like your genome, right?

550
00:31:58,640 --> 00:32:03,480
So that determines how the cell responds to inputs and how the weights change.

551
00:32:03,480 --> 00:32:09,720
So it's kind of a dynamic weight model based on the parameters of the LSTM's at each

552
00:32:09,720 --> 00:32:11,400
of the neurons.

553
00:32:11,400 --> 00:32:12,400
Essentially.

554
00:32:12,400 --> 00:32:13,400
Okay.

555
00:32:13,400 --> 00:32:14,400
And it's a common LSTM.

556
00:32:14,400 --> 00:32:17,040
So, you know, it's a small, it's like the difference with the genome, the connectome.

557
00:32:17,040 --> 00:32:19,280
The genome is very small, connectome is very large.

558
00:32:19,280 --> 00:32:22,680
The LSTM parameters are, you know, are the same everywhere, like a convolutional net kind

559
00:32:22,680 --> 00:32:23,680
of.

560
00:32:23,680 --> 00:32:24,680
Okay.

561
00:32:24,680 --> 00:32:29,200
And so they comprise the genome of the neural net and whatever weights it learns over

562
00:32:29,200 --> 00:32:30,200
time or the connectome.

563
00:32:30,200 --> 00:32:32,560
The same everywhere in structure, not in value.

564
00:32:32,560 --> 00:32:33,560
That's right.

565
00:32:33,560 --> 00:32:34,560
That's right.

566
00:32:34,560 --> 00:32:39,280
So different state at every neuron synapse, but the same weights, right, on the LSTM.

567
00:32:39,280 --> 00:32:40,280
Right.

568
00:32:40,280 --> 00:32:45,120
Which in turn determine the rule for updating the weights in the neural net, a little confusing

569
00:32:45,120 --> 00:32:46,120
I know a bit.

570
00:32:46,120 --> 00:32:47,120
Yeah.

571
00:32:47,120 --> 00:32:48,120
It's hard to do without the whiteboard.

572
00:32:48,120 --> 00:32:49,120
It's a little hard without the whiteboard.

573
00:32:49,120 --> 00:32:50,120
We run into that a lot.

574
00:32:50,120 --> 00:32:51,120
I'm sure.

575
00:32:51,120 --> 00:32:52,120
I'm sure.

576
00:32:52,120 --> 00:32:55,120
So, yeah, how do you train, how do you determine the genome?

577
00:32:55,120 --> 00:32:57,240
Well, now you do have to have a population.

578
00:32:57,240 --> 00:33:01,160
So you have a population of neural nets that in the beginning have random genomes and the

579
00:33:01,160 --> 00:33:06,400
attempt to learn, meaning you feed them in these digits, you feed them in error signal,

580
00:33:06,400 --> 00:33:11,040
and the ones that do a better job survive and reproduce, meaning they share their LSTM

581
00:33:11,040 --> 00:33:16,480
weights and yield into generation, and the ones that do poorly die off.

582
00:33:16,480 --> 00:33:21,600
So you can use evolution strategies to do this, I use CMAES, or you can use more classical

583
00:33:21,600 --> 00:33:22,600
kinds of evolution.

584
00:33:22,600 --> 00:33:23,600
CMAES.

585
00:33:23,600 --> 00:33:30,760
CMAES is a, so the ES is evolution strategies, and CMA is a particular variant of this

586
00:33:30,760 --> 00:33:37,400
that assumes that the set of parameters is a Gaussian blob, and essentially estimates

587
00:33:37,400 --> 00:33:42,920
the gradient of the fitness along that Gaussian and uses that to kind of make a new Gaussian

588
00:33:42,920 --> 00:33:43,920
in the next generation.

589
00:33:43,920 --> 00:33:44,920
Okay.

590
00:33:44,920 --> 00:33:51,800
So it's a toy model of evolution, and it's very robust optimizer for moderate dimensionality.

591
00:33:51,800 --> 00:33:57,800
So you do that with the genes, and you end up evolving a kind of net that learns really

592
00:33:57,800 --> 00:34:01,280
fast, if that's what you make your fitness function, like learn from the minimum number

593
00:34:01,280 --> 00:34:03,240
of examples, learn as fast as possible.

594
00:34:03,240 --> 00:34:08,240
So like in a normal, you know, MNIST kind of set up, you know, you measure how much training

595
00:34:08,240 --> 00:34:12,920
data in terms of how many passes over, you know, all 50,000 training examples or something.

596
00:34:12,920 --> 00:34:17,240
This one, you know, you measure its performance in terms of how many digits you show it.

597
00:34:17,240 --> 00:34:21,160
So like after you've shown it 10 or 20 digits, it's already doing pretty well.

598
00:34:21,160 --> 00:34:23,880
So it's, you know, really, really, really small in.

599
00:34:23,880 --> 00:34:29,240
And are you able to infrospect into it and understand what it's learning, what the hell

600
00:34:29,240 --> 00:34:30,240
are you exactly?

601
00:34:30,240 --> 00:34:31,240
Yeah.

602
00:34:31,240 --> 00:34:36,280
Well, I haven't thought enough of that yet, but I'm measuring like the texture and feature

603
00:34:36,280 --> 00:34:38,640
maps of a neural network.

604
00:34:38,640 --> 00:34:39,640
That's right.

605
00:34:39,640 --> 00:34:43,400
Well, I don't think that what it's, what it's learned, what the network is learning,

606
00:34:43,400 --> 00:34:48,440
that the weights are, yeah, weights is an ambiguous term here.

607
00:34:48,440 --> 00:34:53,240
I don't think that what the synapses get set to is particularly different from a normal

608
00:34:53,240 --> 00:34:54,240
neural net.

609
00:34:54,240 --> 00:34:55,240
Okay.

610
00:34:55,240 --> 00:34:56,240
What's different is the update rules.

611
00:34:56,240 --> 00:34:57,240
Yeah.

612
00:34:57,240 --> 00:34:59,080
And the update rules are governed by a little LSTM.

613
00:34:59,080 --> 00:35:02,840
So figuring out what that is doing is, you know, it's kind of like a biology problem.

614
00:35:02,840 --> 00:35:04,240
It's actually not that.

615
00:35:04,240 --> 00:35:06,400
It's not that trivial.

616
00:35:06,400 --> 00:35:10,480
But you can do things like, you know, normally when we do backprop, we assume symmetric

617
00:35:10,480 --> 00:35:14,920
weights, meaning, you know, that though you use the same weights when you're backpropagating

618
00:35:14,920 --> 00:35:18,360
is when you forward propagate, you have to, otherwise you can't take the derivative.

619
00:35:18,360 --> 00:35:21,760
But we know that in real brains, that's not how it works.

620
00:35:21,760 --> 00:35:25,600
Like, you know, signals don't propagate backpropagate through synapses in the top 10.

621
00:35:25,600 --> 00:35:26,600
Right.

622
00:35:26,600 --> 00:35:27,600
Right.

623
00:35:27,600 --> 00:35:31,240
And so usually we think of MNIST as this toy problem, but for someone that works on OCR,

624
00:35:31,240 --> 00:35:37,880
particularly relevant, are these things that you can, you have a path to actually using

625
00:35:37,880 --> 00:35:38,880
putting production?

626
00:35:38,880 --> 00:35:39,880
Oh, yeah.

627
00:35:39,880 --> 00:35:40,880
And for what?

628
00:35:40,880 --> 00:35:46,080
Well, I mean, in this case, the goals are to be able to learn from small data, which

629
00:35:46,080 --> 00:35:48,800
I care about from the point of view of privacy.

630
00:35:48,800 --> 00:35:52,760
And I care about because, you know, I think one of the Achilles' heels of deep learning

631
00:35:52,760 --> 00:35:55,520
as we do it today is its reliance on mass and amounts of data.

632
00:35:55,520 --> 00:36:00,200
And the reason we have that reliance is because we don't have very sophisticated learning

633
00:36:00,200 --> 00:36:04,240
rules, and they don't embed any priors that have been learned over, you know, evolutionary

634
00:36:04,240 --> 00:36:07,160
time the way ours have about the world, right?

635
00:36:07,160 --> 00:36:11,280
It's all of those priors and all of that intelligence that lives in our genome that lets us learn

636
00:36:11,280 --> 00:36:12,280
so fast.

637
00:36:12,280 --> 00:36:16,520
So, you know, this is an attempt to say like, well, can we reproduce what evolution did

638
00:36:16,520 --> 00:36:22,120
in order to learn statistical priors and learning rules that radically outperformed the feature

639
00:36:22,120 --> 00:36:27,600
engineered rules that we have today, you know, Adam and Ada Grad and SGD and all that?

640
00:36:27,600 --> 00:36:30,200
So that's interesting, just from the point of view of the table.

641
00:36:30,200 --> 00:36:31,200
I give what I want it.

642
00:36:31,200 --> 00:36:35,440
The question is, is there a catch, right?

643
00:36:35,440 --> 00:36:40,800
You know, do we just throw LSTMs in everything and, you know, now everything converges way

644
00:36:40,800 --> 00:36:41,800
quickly?

645
00:36:41,800 --> 00:36:42,800
Yeah.

646
00:36:42,800 --> 00:36:51,200
I mean, it's much more computationally expensive to do that, of course, but on the other

647
00:36:51,200 --> 00:36:55,280
hand, if you can learn from many, many fewer examples, then, you know, it's still a good

648
00:36:55,280 --> 00:37:00,240
thing, even computationally, and certainly from the point of view of data.

649
00:37:00,240 --> 00:37:01,240
Is there a trade-off?

650
00:37:01,240 --> 00:37:02,400
Yeah, absolutely.

651
00:37:02,400 --> 00:37:06,040
The trade-off is that, you know, when you're learning from very few examples, that means

652
00:37:06,040 --> 00:37:11,280
that you're bringing much heavier weight and sometimes rather opaque priors to bear on

653
00:37:11,280 --> 00:37:12,280
the problem.

654
00:37:12,280 --> 00:37:16,360
So, you know, you're subject to more cognitive fallacies and all kinds of, you know, all the

655
00:37:16,360 --> 00:37:17,960
things that humans are subject to.

656
00:37:17,960 --> 00:37:23,920
So all the issues that we talk about is bias that's being introduced in, you know, our

657
00:37:23,920 --> 00:37:28,800
data distributions are potentially magnified manyfold because we're training on much less

658
00:37:28,800 --> 00:37:29,800
data.

659
00:37:29,800 --> 00:37:30,800
Possibly.

660
00:37:30,800 --> 00:37:33,760
Although, I guess I would say a couple of things.

661
00:37:33,760 --> 00:37:37,600
I mean, one is that when you have very small, when you can learn from very small amounts

662
00:37:37,600 --> 00:37:43,520
of data, then you can perhaps be a bit more careful about how you curate that.

663
00:37:43,520 --> 00:37:47,120
But, and also, of course, the fact that the genome is very small means that you maybe

664
00:37:47,120 --> 00:37:51,880
have a little bit more hope of understanding, you know, what those biases and meta priors

665
00:37:51,880 --> 00:37:52,880
are.

666
00:37:52,880 --> 00:37:56,400
So I think it's still positive from the point of view of problems like ML fairness and

667
00:37:56,400 --> 00:37:57,400
so on.

668
00:37:57,400 --> 00:38:01,160
But that's definitely something that we have, you know, one has to look at very, very closely

669
00:38:01,160 --> 00:38:07,560
because, yeah, you know, newborn babies, you know, will react with fear to snake

670
00:38:07,560 --> 00:38:08,560
like objects.

671
00:38:08,560 --> 00:38:11,920
So like, even at a very high level, object like ignition kind of level, you know, there's

672
00:38:11,920 --> 00:38:15,600
something in the genome that, you know, that makes snake like things scary, and so you

673
00:38:15,600 --> 00:38:20,040
can imagine the problems that can arise from, you know, from having, you know, priors

674
00:38:20,040 --> 00:38:23,920
like that, hidden priors like that, you know, in the genome for learning that.

675
00:38:23,920 --> 00:38:26,440
So yes, that's definitely an issue.

676
00:38:26,440 --> 00:38:31,920
But at a more meta level also to connect this to the bacteria stuff, you know, what you

677
00:38:31,920 --> 00:38:38,200
can imagine is that this rule, it's not just a learning rule, it's actually a rule for

678
00:38:38,200 --> 00:38:40,720
how brains or how behavior works.

679
00:38:40,720 --> 00:38:42,680
And in that sense, it's like an emotional system.

680
00:38:42,680 --> 00:38:46,400
What has been learned by evolution is what is good or bad as well locally.

681
00:38:46,400 --> 00:38:53,000
And that's something I think that this is a route toward, toward real AI in ways that

682
00:38:53,000 --> 00:39:00,400
I don't think we can do with, I don't think we can get to that by hand writing either

683
00:39:00,400 --> 00:39:04,560
update rules or fitness functions or loss functions.

684
00:39:04,560 --> 00:39:10,920
And so in the, the model we were just talking about, the individual agents, actors, whatever

685
00:39:10,920 --> 00:39:13,880
you want to call organisms are these LSTMs.

686
00:39:13,880 --> 00:39:14,880
Right.

687
00:39:14,880 --> 00:39:18,840
So that's kind of looking at a, looking at a neural net as a society of neurons if you

688
00:39:18,840 --> 00:39:19,840
want to think about it.

689
00:39:19,840 --> 00:39:20,840
Right.

690
00:39:20,840 --> 00:39:25,440
You can also then take these brains and put them together and have them teach each other

691
00:39:25,440 --> 00:39:28,000
you have the kind of things that you were pointing out, right, have them interact socially

692
00:39:28,000 --> 00:39:30,000
and that's also super interesting.

693
00:39:30,000 --> 00:39:34,800
And I think that that's how you actually get emotional systems that, you know, that will

694
00:39:34,800 --> 00:39:35,800
work.

695
00:39:35,800 --> 00:39:41,880
So, so yeah, I have come to the belief that our route, I mean, I don't know what to call

696
00:39:41,880 --> 00:39:47,400
AI, I guess, right, but our route to like real brains has to go through this, like,

697
00:39:47,400 --> 00:39:48,400
Stalin intelligence.

698
00:39:48,400 --> 00:39:49,400
Yeah.

699
00:39:49,400 --> 00:39:53,560
It has to go through the social route and stopping hand defining the loss functions and

700
00:39:53,560 --> 00:39:54,560
the update rules.

701
00:39:54,560 --> 00:39:55,560
Interesting.

702
00:39:55,560 --> 00:40:01,160
One of the things that is common between this warm intelligence approach we were referring

703
00:40:01,160 --> 00:40:07,960
to earlier and what you just described is kind of a, you know, everything's the same.

704
00:40:07,960 --> 00:40:11,960
Like you're building these systems out of components that are the same and groups of components

705
00:40:11,960 --> 00:40:13,600
that are the same.

706
00:40:13,600 --> 00:40:17,720
Whereas, you know, maybe the counter example that we've talked about is GANS where you've

707
00:40:17,720 --> 00:40:20,240
got these two distinct things with different goals.

708
00:40:20,240 --> 00:40:21,240
Yes.

709
00:40:21,240 --> 00:40:26,160
Do you have you started looking at more heterogeneous systems?

710
00:40:26,160 --> 00:40:27,160
Yeah.

711
00:40:27,160 --> 00:40:28,160
I have.

712
00:40:28,160 --> 00:40:32,920
So, even within a single neural net, I've often played with having different genes for

713
00:40:32,920 --> 00:40:38,360
every layer, for example, or having different brain areas that have different, that have

714
00:40:38,360 --> 00:40:43,160
different genes in them that are interacting, you know, with, you know, that are sort of

715
00:40:43,160 --> 00:40:47,760
feeding, feeding into each other with asymmetric weights that are, you know, whose interactions

716
00:40:47,760 --> 00:40:51,040
are learned, or having multiple species together.

717
00:40:51,040 --> 00:40:54,680
And have entirely different kinds of inputs and outputs and that, you know, so this gets

718
00:40:54,680 --> 00:40:59,040
into very a life, a very artificial life kind of paradigm.

719
00:40:59,040 --> 00:41:00,880
So yeah, I think those are all really interesting roots.

720
00:41:00,880 --> 00:41:05,040
I mean, the challenge with a lot of this is, of course, that, you know, we've really relied

721
00:41:05,040 --> 00:41:09,640
on the fact that these things look like scores or in games for a long time in order to talk

722
00:41:09,640 --> 00:41:12,840
about, like, what is state of the art, what is better or worse, you know, it's research

723
00:41:12,840 --> 00:41:18,400
groups competing or collaborating with each other with a well-defined metric.

724
00:41:18,400 --> 00:41:23,960
And it's really hard to take these more social and organic kind of approaches and cope

725
00:41:23,960 --> 00:41:28,880
with the same sort of clear metrics, you know, for what constitutes progress.

726
00:41:28,880 --> 00:41:32,960
So that's one of the big challenges that I want to kind of leave the audience with, you

727
00:41:32,960 --> 00:41:39,760
know, how do we keep that sense of clarity of progress about, you know, how we're advancing

728
00:41:39,760 --> 00:41:44,400
in our understanding and our ability to solve these problems when we, you know, I believe

729
00:41:44,400 --> 00:41:47,880
inherently need to start looking at things that don't have such well-defined scores.

730
00:41:47,880 --> 00:41:54,720
And is that because the things that we should care about are internal metrics of these social

731
00:41:54,720 --> 00:42:02,400
entities or if we want to get kind of near-termish value out of them, we're applying them, the

732
00:42:02,400 --> 00:42:08,400
problems that have some type of score associated with them, you know, the things that we've

733
00:42:08,400 --> 00:42:15,080
been doing, like translation and OCR and, you know, playing games and things like that.

734
00:42:15,080 --> 00:42:18,760
Why aren't those metrics sufficient for these types of systems?

735
00:42:18,760 --> 00:42:19,760
Yeah, yeah.

736
00:42:19,760 --> 00:42:21,880
So both of the things you just said are correct.

737
00:42:21,880 --> 00:42:28,640
I mean, I'm not saying that metrics are bad thing for OCR, you know, or for face recognition

738
00:42:28,640 --> 00:42:32,620
where, like, you know, it's very clear what is good or bad, you know, so the problem with

739
00:42:32,620 --> 00:42:37,360
an alphairness for face recognition, for example, is now much discussed and it's actually

740
00:42:37,360 --> 00:42:38,360
really well posed.

741
00:42:38,360 --> 00:42:44,720
But if it doesn't work as well at distinguishing faces for some group of people that we define

742
00:42:44,720 --> 00:42:47,200
socially, then that's a problem.

743
00:42:47,200 --> 00:42:51,320
And the answer is clear, you know, like sample better in that space, you know, measure

744
00:42:51,320 --> 00:42:56,240
better in that space and, you know, and set a higher bar for, you know, for everything

745
00:42:56,240 --> 00:42:57,720
to performically well.

746
00:42:57,720 --> 00:43:02,880
The problem is that most real stuff doesn't actually fit into that paradigm.

747
00:43:02,880 --> 00:43:08,720
You know, what is the loss function for, you know, for a credit score for the correct

748
00:43:08,720 --> 00:43:14,600
assignment of a credit score or for couples counseling, you know, or for good or bad art,

749
00:43:14,600 --> 00:43:19,040
or for, you know, how to rank notifications on Android for that matter, you know, so

750
00:43:19,040 --> 00:43:22,400
like, really bread and butter stuff, you know what I'm saying, like, if you just optimize

751
00:43:22,400 --> 00:43:25,960
for India, I can tell you a thing or two about the loss function for ranking notifications

752
00:43:25,960 --> 00:43:26,960
on Android.

753
00:43:26,960 --> 00:43:29,440
Well, you know, and actually there are rules.

754
00:43:29,440 --> 00:43:33,000
Yeah, so at the moment, at the moment, it's unfortunately a lot of a lot of hard rules

755
00:43:33,000 --> 00:43:36,400
that, yeah, that themselves, I mean, that's where back to Q tables again, like if you

756
00:43:36,400 --> 00:43:39,920
try and back, back out using enforcement enforcement learning, like, okay, those rules

757
00:43:39,920 --> 00:43:43,720
are all, you know, kind of carefully considered, like, what are they optimizing?

758
00:43:43,720 --> 00:43:45,240
You cannot write down what they're optimizing.

759
00:43:45,240 --> 00:43:46,240
Right.

760
00:43:46,240 --> 00:43:47,240
Right.

761
00:43:47,240 --> 00:43:50,920
They're coming from a whole bunch of different socially constructed intuitions.

762
00:43:50,920 --> 00:43:59,840
So kind of your point is you're proposing this potentially much more powerful paradigm

763
00:43:59,840 --> 00:44:05,400
and with this much more powerful paradigm, we should be able to push into areas that

764
00:44:05,400 --> 00:44:12,200
we can't apply, you know, I always fault our saying traditional, you know, since, right,

765
00:44:12,200 --> 00:44:17,000
but she learning as we know it today, she learning as we know it today, which is very much

766
00:44:17,000 --> 00:44:21,880
based on kind of being able to write down these loss functions, but, you know, we're kind

767
00:44:21,880 --> 00:44:22,880
of in it.

768
00:44:22,880 --> 00:44:25,880
It's a little bit of a chicken and egg like we can't write down a loss function so we don't

769
00:44:25,880 --> 00:44:31,080
know how to apply these new things or even how to measure success with these new things.

770
00:44:31,080 --> 00:44:35,040
Well, and what I would argue is that, you know, it, again, it's social, right?

771
00:44:35,040 --> 00:44:39,240
So the point of ranking, I mean, if you were doing the ranking and, you know, you're

772
00:44:39,240 --> 00:44:42,480
like a little person living in somebody's phone doing the ranking for them and you were

773
00:44:42,480 --> 00:44:45,600
able to consult also with the other rankers, you know, and everybody else's phone using

774
00:44:45,600 --> 00:44:50,960
federated learning or something like that, then, you know, the fundamental tool that

775
00:44:50,960 --> 00:44:55,520
you'd want to be able to do that effectively is empathy.

776
00:44:55,520 --> 00:44:59,360
You know, it's not, it's not like maximized engagement, you know, or some kind of stupid,

777
00:44:59,360 --> 00:45:01,160
you know, quantitative measure like that.

778
00:45:01,160 --> 00:45:02,160
Right.

779
00:45:02,160 --> 00:45:04,360
It's more like, you know, be good for the humans.

780
00:45:04,360 --> 00:45:05,360
Right.

781
00:45:05,360 --> 00:45:06,360
Right.

782
00:45:06,360 --> 00:45:12,320
And that requires modeling the other and using that as a guide for how to behave collectively

783
00:45:12,320 --> 00:45:13,320
and individually.

784
00:45:13,320 --> 00:45:16,640
And there may be conflicts in that, like, you know, whether you model them at the individual

785
00:45:16,640 --> 00:45:20,760
level or the collective level, you know, just like in medical ethics, you have conflicts

786
00:45:20,760 --> 00:45:25,160
about, say, you know, I don't know, giving people antibiotics, right?

787
00:45:25,160 --> 00:45:29,520
And in the aggregate, it's not not necessarily a good thing if you're only optimizing for

788
00:45:29,520 --> 00:45:31,280
the individual, you'll do it more.

789
00:45:31,280 --> 00:45:32,280
And so on.

790
00:45:32,280 --> 00:45:33,880
The same issues come up here.

791
00:45:33,880 --> 00:45:39,240
So, you know, you've got to kind of create an entity that has clear allegiances and clear

792
00:45:39,240 --> 00:45:44,680
kind of, you know, empathic goals that don't quite look like lost functions, but look

793
00:45:44,680 --> 00:45:47,720
more like being able to relate socially to the relevant kinds of entities.

794
00:45:47,720 --> 00:45:52,720
And you're sounding at this from a biological perspective, I imagine that there are connections

795
00:45:52,720 --> 00:45:57,480
to many other, you know, economics and sociology and other things if you were to really get

796
00:45:57,480 --> 00:45:59,120
into the social aspect of this.

797
00:45:59,120 --> 00:46:00,120
That's right.

798
00:46:00,120 --> 00:46:02,400
And dynamical systems theory and various other fields.

799
00:46:02,400 --> 00:46:03,400
Right.

800
00:46:03,400 --> 00:46:05,800
There's a lot of other fields come into it, yeah, exactly.

801
00:46:05,800 --> 00:46:06,800
Awesome.

802
00:46:06,800 --> 00:46:07,800
Awesome.

803
00:46:07,800 --> 00:46:12,600
Well, guys, thanks so much for taking the time to, you know, share what you're doing and

804
00:46:12,600 --> 00:46:16,640
give us a preview of what you're speaking about tomorrow.

805
00:46:16,640 --> 00:46:20,120
Of course, folks that are listening to this can actually go and check out the recording

806
00:46:20,120 --> 00:46:21,120
of your talk.

807
00:46:21,120 --> 00:46:22,120
Absolutely.

808
00:46:22,120 --> 00:46:24,640
If they've listened to all of this, they've gotten a longer version than they've talked

809
00:46:24,640 --> 00:46:25,640
tomorrow morning.

810
00:46:25,640 --> 00:46:27,520
So, thank you for asking such great questions.

811
00:46:27,520 --> 00:46:28,520
Nice.

812
00:46:28,520 --> 00:46:36,600
All right, everyone, that's our show for today.

813
00:46:36,600 --> 00:46:42,880
For more information on today's guests, visit twimmaleye.com slash shows.

814
00:46:42,880 --> 00:47:01,240
Thanks so much for listening and catch you next time.

