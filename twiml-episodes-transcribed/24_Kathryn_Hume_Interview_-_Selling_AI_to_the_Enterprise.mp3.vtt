WEBVTT

00:00.000 --> 00:17.920
Hello and welcome to another episode of Tumel Talk, the podcast where I interview interesting

00:17.920 --> 00:23.120
people, doing interesting things in machine learning and artificial intelligence.

00:23.120 --> 00:26.360
I'm your host Sam Charrington.

00:26.360 --> 00:32.200
As usual, thanks so much to everyone who sent in their favorite quote from a recent podcast.

00:32.200 --> 00:36.520
Another batch of stickers got mailed out this week, so keep those quotes coming.

00:36.520 --> 00:40.400
So far this month, we've sent stickers to five different countries.

00:40.400 --> 00:45.400
If you're new to the Tumel worldwide family, you too can get a sticker.

00:45.400 --> 00:51.400
Just let us know your favorite quote from this or any recent Tumel and AI episode.

00:51.400 --> 00:57.080
You can do that via posting a comment to the show notes page by tweeting us at Tumel AI

00:57.080 --> 01:03.120
or at Sam Charrington or via a post on our Facebook page.

01:03.120 --> 01:07.920
Your feedback is very important to us and we appreciate every bit of it.

01:07.920 --> 01:10.600
We're going to jump right into our show.

01:10.600 --> 01:15.520
You may recall that we spent some time in New York City a few weeks ago at the NYU Future

01:15.520 --> 01:18.320
Labs AI Summit.

01:18.320 --> 01:22.760
While there, I had the opportunity to sneak backstage and interview a few of the great

01:22.760 --> 01:28.120
speakers from the event, including this week's guest, Catherine Hume.

01:28.120 --> 01:33.040
Catherine is the president of Fast Forward Labs, which is an independent machine intelligence

01:33.040 --> 01:37.480
research company that helps organizations accelerate their data science and machine

01:37.480 --> 01:40.080
intelligence capabilities.

01:40.080 --> 01:44.960
If that name sounds familiar, that's because we had their founder Hillary Mason on the

01:44.960 --> 01:47.080
show a few months ago.

01:47.080 --> 01:50.240
We'll link to that show in the show notes, it was a great one.

01:50.240 --> 01:54.920
My discussion with Catherine focused on AI adoption within the enterprise.

01:54.920 --> 01:59.360
She shared several really interesting examples of the kind of things she's seeing enterprises

01:59.360 --> 02:04.560
do with machine learning in AI, and we discussed a few of the various challenges they face

02:04.560 --> 02:08.800
and some of the lessons her company has learned in helping them along.

02:08.800 --> 02:12.680
I really enjoyed our conversation and I know you will too.

02:12.680 --> 02:22.680
And now on to the show.

02:22.680 --> 02:26.760
Hey, everyone.

02:26.760 --> 02:29.040
I am here with Catherine Hume.

02:29.040 --> 02:32.320
Catherine is president of Fast Forward Labs.

02:32.320 --> 02:39.600
If you listen to the podcast, you may recall that I interviewed her colleague Hillary Mason

02:39.600 --> 02:47.520
on the podcast some time ago, and she's here at the Future Labs AI Summit.

02:47.520 --> 02:52.080
She just finished her talk and she's agreed to chat with us for a little bit.

02:52.080 --> 02:53.080
Catherine?

02:53.080 --> 02:54.920
Hey Sam, I'm happy to be here.

02:54.920 --> 02:55.920
Great.

02:55.920 --> 02:56.920
You know what?

02:56.920 --> 03:03.520
You have a really intriguing background, mostly or in part because I'm a linguophile and

03:03.520 --> 03:08.560
you speak eight languages I hear.

03:08.560 --> 03:16.360
But you have kind of a, you're not the PhD in neural networks that I often interview.

03:16.360 --> 03:21.520
So how did you kind of find your way from where you were to in this space?

03:21.520 --> 03:24.200
Through a long and tortured path, I suppose.

03:24.200 --> 03:30.200
Yeah, so my PhD is actually in comparative literature, which is where I spent a lot of time

03:30.200 --> 03:34.480
learning languages and reading literature in those languages in my path life.

03:34.480 --> 03:41.320
So I was a mathematician as an undergrad and just was fascinated by language and cultures

03:41.320 --> 03:44.960
and decided to shift paths and do a PhD in literature.

03:44.960 --> 03:49.680
And I found myself as I was working on it always really interested in what has become natural

03:49.680 --> 03:50.680
language processing.

03:50.680 --> 03:55.200
So thinking about language, not only from a, let's go out and talk with people and communicate

03:55.200 --> 03:58.040
perspective, but also how they work.

03:58.040 --> 04:06.280
And I became increasingly interested in the fact that in using AI systems, they didn't

04:06.280 --> 04:08.000
really treat language like language.

04:08.000 --> 04:13.360
So in some of the statistical developments in the early 2000s, it's my PhD from 2007

04:13.360 --> 04:14.360
to 2012.

04:14.360 --> 04:18.560
It was a lot of N gram or bi gram type work in language processing.

04:18.560 --> 04:22.760
And it was fascinating to me that you didn't need to think about syntax and semantics and

04:22.760 --> 04:26.280
all of the grammatical terms that linguists used to think about language to make sense of

04:26.280 --> 04:28.720
it could just be reduced to a stats problem.

04:28.720 --> 04:31.600
And that was totally bizarre and interesting and weird for me.

04:31.600 --> 04:37.440
So I developed an early interest in AI and I had a couple of jobs in software companies

04:37.440 --> 04:42.760
that weren't using machine learning and then found my way fortunately into this space

04:42.760 --> 04:43.760
eventually.

04:43.760 --> 04:45.760
That's amazing.

04:45.760 --> 04:48.840
So your talk was on selling AI into the enterprise.

04:48.840 --> 04:52.760
What were the main things you were trying to accomplish with the talk?

04:52.760 --> 04:56.920
So I think the main thing was I assume that there were a lot of young entrepreneurs in

04:56.920 --> 05:02.240
the audience and I wanted to help them appreciate that you can't just say, you know, I'm going

05:02.240 --> 05:06.200
to go from my research that I did in graduate school and I'm going to go form a company and

05:06.200 --> 05:08.080
everything's going to work and be magical, right?

05:08.080 --> 05:13.000
There's a lot of tactical hard work you have to do in order to really make money off

05:13.000 --> 05:17.080
of these new technologies and tools.

05:17.080 --> 05:18.640
So I started at three parts in my talk.

05:18.640 --> 05:24.520
The first one was what I called the five axioms of enterprise AI and it just went through

05:24.520 --> 05:27.760
a lot of the trends that we're seeing in the enterprise space across different verticals

05:27.760 --> 05:31.200
fast forward doesn't wear a horizontal consulting and research firm.

05:31.200 --> 05:34.880
So we, you know, we work with companies of all shapes and sizes, but there's some patterns

05:34.880 --> 05:41.560
that show up some the biggest of which I think is companies discomfort with risk and probability.

05:41.560 --> 05:45.560
So there's a cultural and process shift that has to take place for them to really succeed

05:45.560 --> 05:52.280
with AI as they move from big data data analytics where they're counting transactions that

05:52.280 --> 05:57.640
occurred in the past to fill out 10k forms and do reporting to experimenting with data

05:57.640 --> 06:02.320
to build out new revenue streams and products and there's just, it's a massive undertaking

06:02.320 --> 06:08.280
to reshape the mentality that folks have to actually do this well.

06:08.280 --> 06:12.800
And then I talked about what I consider to be some super interesting use cases of real,

06:12.800 --> 06:18.000
you know, real enterprise AI that we've seen in our customer base ranging from building

06:18.000 --> 06:23.040
out personalized, recommender systems to support sales and financial services to building

06:23.040 --> 06:30.680
in context to wear surgical robots that can identify critical points in surgeries to basically

06:30.680 --> 06:36.440
shift to the future of automated robotic surgery somewhere 10 to 15 years down the line.

06:36.440 --> 06:39.200
Well, maybe can we maybe walk through one of those?

06:39.200 --> 06:46.880
Yeah, sure, absolutely, so let's pick one, we do some work with the big four and we

06:46.880 --> 06:48.040
As in accounting firms?

06:48.040 --> 06:49.240
Big four accounting firms, yep.

06:49.240 --> 06:54.560
So they've got a lot of problems where they're trying to audit the financial statements

06:54.560 --> 06:58.000
of their clients for compliance or they'll be giving tax advice or some sort of consulting

06:58.000 --> 06:59.400
advice to their clients.

06:59.400 --> 07:03.560
So one use case is it's not the sexiest in the world, but I think it's a great machine

07:03.560 --> 07:04.560
or any problem.

07:04.560 --> 07:10.240
They came to us and they said today our tax practitioners have a hard time keeping up

07:10.240 --> 07:16.480
with new rulings and opinions and news for lack of a better term in the legal world related

07:16.480 --> 07:18.080
to their particular clients issue.

07:18.080 --> 07:23.520
So there's a big gap between the world of know how, let's say in law and in advising and

07:23.520 --> 07:27.360
the world of applied know how where you take the rules and regulations and then you apply

07:27.360 --> 07:30.240
it to your customer's particular situation.

07:30.240 --> 07:36.080
So they engaged us to try to not go so far as to automate the work of providing tax

07:36.080 --> 07:42.280
advice but provide more dynamic up to date and specific alerts for their tax practitioners

07:42.280 --> 07:48.360
on new legal documents that were coming out that were relevant for their particular

07:48.360 --> 07:49.560
client cases.

07:49.560 --> 07:55.080
So that involved are using some statistical topic modeling techniques where we went in

07:55.080 --> 07:59.640
and we started off actually with regular expressions and just found got the most out

07:59.640 --> 08:04.040
of rules that we could to sort of bootstrap up intelligence in the system and then added

08:04.040 --> 08:08.160
on an additional statistical layer of intelligence to get them to a point where they're now able

08:08.160 --> 08:12.600
to provide dynamic advice to their clients because as new rulings come in they have their

08:12.600 --> 08:18.080
sort of fact specific alerts that are coming out and shift around the product offering,

08:18.080 --> 08:22.040
repricate, repackage it and offer something different than their competitors.

08:22.040 --> 08:23.040
Interesting.

08:23.040 --> 08:31.960
If there are a couple of generalizable rules in there one being do you commonly see folks

08:31.960 --> 08:38.800
in traditional enterprises having their first step being using these systems to augment

08:38.800 --> 08:46.600
their human staff as opposed to replace and the second being you know starting out simply

08:46.600 --> 08:52.120
with you know things like regular expressions and then eventually building up to the statistical

08:52.120 --> 08:53.120
techniques.

08:53.120 --> 08:55.600
Yeah those are both absolutely true.

08:55.600 --> 09:01.800
So I think one of the common mistakes that we see is that when somebody working in the

09:01.800 --> 09:06.320
enterprise who's non-technical not a machine learning specialist and is interested in trying

09:06.320 --> 09:11.400
to do something cool with AI normally their default assumption is that they can go from

09:11.400 --> 09:17.120
manual to completely automated in the first pass which I think is leading to what I call

09:17.120 --> 09:21.960
sort of some of the big distractions and discussions around AI where we assume that in the next

09:21.960 --> 09:25.280
10 years the work place is going to radically shift and everybody's going to lose their

09:25.280 --> 09:29.200
jobs and we're going to be replaced by machines and universal basic income comes up right

09:29.200 --> 09:34.480
so there's all of this mania we see online and you know our sort of humbled experience

09:34.480 --> 09:38.440
in working with companies and practices that that's not happening anytime soon.

09:38.440 --> 09:43.200
How people work will change when they have machines as a you know a companion and component

09:43.200 --> 09:49.360
but most of the time just from a pure technical perspective it's so hard to have the amount

09:49.360 --> 09:53.760
of data that's required to train a system that could really perform with the levels of

09:53.760 --> 10:00.320
accuracy that are required to do risk-oriented tasks like healthcare, diagnostics, treating

10:00.320 --> 10:05.200
stocks whatever it may be that they're just you know they're mistrusting of the systems

10:05.200 --> 10:08.720
and it really takes a long time to actually get enough labeled training data to get a

10:08.720 --> 10:15.640
system to perform and then the second is yeah from a product development perspective we

10:15.640 --> 10:19.400
tend to at fast-forward labs tend to think that you should always start with the simplest

10:19.400 --> 10:24.280
algorithm that will scale so we tend to you know we don't we don't start with the neural

10:24.280 --> 10:28.440
network we start with the linear regression right and if it works then it's got all sorts

10:28.440 --> 10:32.520
of benefits it's if the accuracy is good it's interpretable we know how to debug it we

10:32.520 --> 10:36.600
know how to fix it we can do some feature engineering as opposed to just throwing the big

10:36.600 --> 10:43.600
guns at a problem that might not actually require that so yeah so you want to get touched

10:43.600 --> 10:50.560
on trust and you know what we can kind of wrap up as cultural issues within customers how

10:50.560 --> 10:57.520
do you do you and if so how do you help them kind of wrap their arms around you know these broader

10:57.520 --> 11:03.360
cultural issues that are you know ultimately required for them to successfully introduce

11:03.360 --> 11:08.480
these kinds of technologies into their organizations I think it takes a lot of education and training

11:08.480 --> 11:15.600
in a lot of time so there's sort of two cultural hurdles to primary culture hurdles that we see

11:16.160 --> 11:21.680
the first is on let's say the project planning phase where you know there's a lot of risk

11:21.680 --> 11:27.040
involvement experimentation involved in doing data science and AI projects so they don't lend

11:27.040 --> 11:30.880
themselves to they certainly don't lend themselves to a waterfall methodology and they don't even

11:30.880 --> 11:35.200
really lend themselves to agile because you're not really sure at each step how long it's going

11:35.200 --> 11:39.120
to take if it's going to work if the models are going to converge so we have to do a lot of

11:40.080 --> 11:46.320
just coaching to help people understand how to do the data science phase let's say in a product

11:46.320 --> 11:52.960
development process and and help let's say the the technologists that we're working with coach

11:52.960 --> 11:56.400
them to have the meetings that they're going to have to have with their management who are going

11:56.400 --> 12:00.320
to be skeptical and not understand why they're spending money on a project that might not work out

12:00.320 --> 12:05.760
right so it's a lot of the fail fast risk-based thinking that we hear about in the startup world

12:05.760 --> 12:11.120
that needs to be important and the enterprise the second is on let's say the consumption

12:11.120 --> 12:17.840
and the user like the use of outputs of the system where there's a lot of design thinking that

12:17.840 --> 12:24.320
has to go in to help people use predictions well so if they let's say they're using a more

12:24.320 --> 12:29.600
like complex statistical tool and the output is going to be a distribution that says yeah we think

12:29.600 --> 12:35.520
it's here's our prediction we're 82% confident and the accuracy rate is at you know 36% or something

12:35.520 --> 12:41.440
like that so the average user is not going to know what to do when that's what their system tells

12:41.440 --> 12:47.760
them so you know I think there's there's a lot of work in the bigger problem in the enterprise

12:47.760 --> 12:52.800
today is not is not replacing our workers but actually giving them some sort of basic statistical

12:52.800 --> 12:59.200
intuition so they can they can use machine learning effectively interesting yeah so I spoke

12:59.200 --> 13:06.320
when Hillary was on the show we spent some time talking about the this issue of agile methodologies

13:06.320 --> 13:14.160
not lending themselves to this type of problem which was really counterintuitive to me I thought that

13:14.160 --> 13:19.040
you know I guess thinking of it kind of you know in a bipolar way waterfall and agile hey you'd

13:19.040 --> 13:25.520
wanted you want to use agile on the topic of culture one of the most interesting stories that I've

13:25.520 --> 13:34.160
heard came out of lows I heard a talk by the head of innovation there who's focused on

13:34.160 --> 13:41.040
some of their machine learning AI types of initiatives and they were really struggling to get their

13:41.040 --> 13:47.680
executives kind of on board with some of these types of projects and you know I get there get

13:47.680 --> 13:54.320
their heads wrapped around the these you know the risk issues without them you know just kind of

13:54.320 --> 13:59.120
latching into these problems and taking them you know either taking them too far or you know out

13:59.120 --> 14:06.160
of the gate saying hey you know that'll never work right and so what they did was they they hired

14:06.160 --> 14:14.160
a graphic artist and they produced these comic books that basically illustrated you know a given

14:14.160 --> 14:23.120
future you know at lows and what they found was that the the comic books were interest in a very

14:23.120 --> 14:29.520
interesting way they were you know tangible but they were also constrained so you know they

14:30.800 --> 14:35.840
they didn't necessarily you know that an executive could say yeah I want to do that I'm

14:35.840 --> 14:41.520
willing to put some money behind that so so one of the examples they gave was I think they call

14:41.520 --> 14:47.360
it the low spot it's basically a customer service robot that roams around stores and will you

14:47.360 --> 14:53.120
know can answer questions and things like that and they first documented what they envisioned this

14:53.120 --> 14:57.280
project to be in one of these comic books and eventually got funded and I think it's in a few

14:57.280 --> 15:03.680
stores in northern California now are there any other kind of tricks or interesting things that you've

15:03.680 --> 15:09.680
seen customers do to kind of help drive these cultural shifts I think there's all sorts of

15:09.680 --> 15:14.560
lessons in what you just described from low so when you were when you're singing about the comic

15:14.560 --> 15:21.200
book and the tangibility um the imagination is really important right so um you know there's

15:21.840 --> 15:28.240
one thing that we see is it's and I talked about this in the talk um you know the executives that

15:28.240 --> 15:32.560
companies will have seen that Google is able to classify cats or they'll have seen IBM Watson and

15:32.560 --> 15:37.120
they'll have seen plane jeopardy and they'll have seen Google Deep Mine plane go and they're completely

15:37.120 --> 15:42.480
befuddled as as to how those achievements in the realm of games and in the realm of the sort of

15:42.480 --> 15:47.840
consumer internet and the frivolousness around it right are relevant for the tactical boring

15:47.840 --> 15:52.560
business problems that they have and in part that's because if you don't really understand the general

15:52.560 --> 15:58.400
backend technology it's it's it's hard to make the imaginative leap from you know to do the the

15:58.400 --> 16:03.200
the synthesis between one domain and how that might be applied in another so I think um the

16:03.200 --> 16:08.400
concreteness of the book right where it's not you're not leaving it up to their imagination to

16:08.400 --> 16:13.280
imagine what this technology might do you're playing out scenarios you're using fiction and the

16:13.280 --> 16:17.600
ability of the imagination to sort of to to make that happen and granted it's there's a difference

16:17.600 --> 16:22.720
between who knows if the reality is like the the uh the screenplay that was written but that's okay

16:22.720 --> 16:26.560
the the actual product can be completely different from the origin version it just just have to

16:26.560 --> 16:31.840
have the impetus to want to invest so I think um one of the things that we've seen it fast forward

16:31.840 --> 16:37.120
we have a as i'm sir hillary talked about on the podcast we have a subscription product where

16:37.120 --> 16:43.040
we educate our customers as to what's possible today in realm of AI very fewer those who actually

16:43.040 --> 16:49.520
are applying the algorithms that we teach them about but that's okay because it inspires their

16:49.520 --> 16:57.200
imagination it gets them excited and it's a really useful tool to then to muster a lot of the

16:57.200 --> 17:00.720
you know the organizational energy that's required to then do something that because practical

17:00.720 --> 17:07.280
might look really different um other kind of techniques that i've seen i think there is i think

17:08.000 --> 17:12.240
i talked about this a little bit in the talk too um we have to always remember that we're all

17:12.240 --> 17:18.240
just humans right and um and we respond at work kind of the same ways that respond when we're

17:18.240 --> 17:22.800
outside of work but we sort of we put on this work hat and suddenly think that we're supposed to

17:22.800 --> 17:27.760
use these processes and this stuff that we read about that is a little counterintuitive it doesn't

17:27.760 --> 17:35.040
necessarily feel right and my hunches and and take is that if we can use humor even i i talk

17:35.040 --> 17:39.440
with somebody who's just an excellent uh a bist of person the other day who would start off his

17:39.440 --> 17:44.720
meetings with large serious customers with some sort of gimmick and it would just shift the the

17:44.720 --> 17:50.640
tone in the room and lead to real honest problem solving discussion and i think if you anything

17:50.640 --> 17:54.480
that you can do to get that so that you're not fearful of saying something that's gonna upset

17:54.480 --> 17:59.920
your boss or you know having to go around corners and stuff and even from being a company a startup

17:59.920 --> 18:05.440
company working with a large organization to the extent to which you can create an environment where

18:05.440 --> 18:11.520
it's two equals thinking through something you have a much higher probability of success so

18:12.320 --> 18:20.720
very interesting i'm curious where at what part of the cycle does your typical client come to you

18:20.720 --> 18:27.760
today uh is it is it mania driven hey we want to do something around this AI stuff and we hear

18:27.760 --> 18:33.040
that you folks are good at it or is it we've got a specific problem and we want to solve it or

18:33.840 --> 18:37.920
somewhere in between can you statistically characterize the distribution

18:39.920 --> 18:46.720
is it what's on here i guess so it varies we definitely have a lot of um you know younger

18:46.720 --> 18:54.080
smart bright uh researchers and machine learning who will write in and say this is awesome i want

18:54.080 --> 18:58.640
i want to read your reports i want to work for you um and that's great and it helps us build our

18:58.640 --> 19:02.800
community but it's not necessarily what pays our bills um i think from the you know the real

19:02.800 --> 19:07.600
interest it'll sort of lie in in that depends on the spectrum of maturity that the different

19:07.600 --> 19:13.120
enterprises have sometimes folks will reach out and they're actually really at the beginning of a

19:13.120 --> 19:19.040
data science journey for lack of a better word and they're looking for help building out a governance

19:19.040 --> 19:24.640
plan doing some basic analytics etc and that's not exactly we're not we're not staffed we're not a

19:24.640 --> 19:28.080
heart huge consulting company that can serve those kind of requests so we have sort of a partnership

19:28.080 --> 19:33.280
network will push them off the ones that work um often either have a specific problem so they have

19:33.280 --> 19:39.760
enough uh awareness and understanding of the business potential of AI that they can pose a good

19:39.760 --> 19:45.840
problem but they don't necessarily have the internal resources to staff it and execute or they may

19:45.840 --> 19:51.680
have already they do have data scientists they're doing some work um but they're looking for a

19:51.680 --> 19:57.520
neutral third party who has expertise in the domain which is rare to come in and evaluate what

19:57.520 --> 20:02.720
they're doing these heavy the Googles and Facebook's and IBM's of the world to just make sure they're

20:02.720 --> 20:09.520
on the right track um so so we tend to work well uh if there is an existing data science team

20:09.520 --> 20:14.800
in place um and they're looking to go from you know where they are today to where they might go

20:14.800 --> 20:20.400
and using some more advanced techniques there's not a ton of companies that are sort of at the

20:20.400 --> 20:24.640
you know at that phase there's very few enterprises that are actually using deep learning like

20:24.640 --> 20:28.880
very few right because they don't have the data they I mentioned this in the talk too there's um

20:28.880 --> 20:33.360
we have this uh false impression that just because it's a big enterprise they're going to have lots

20:33.360 --> 20:40.080
of data they do but they haven't been considering data over the last hundred years with an i towards

20:40.080 --> 20:44.800
building machine learning products so they don't have labeled training sets and they don't have it

20:44.800 --> 20:50.400
well composed and and processed and ready to use it's there but it takes years of work to

20:50.400 --> 21:01.520
make it useful um do you have any perspective on um your company has uh made a name for itself

21:01.520 --> 21:08.160
initially I believe as a data science company and you know there's obviously um a lot of again

21:08.160 --> 21:13.840
kind of maniac around AI and I've been trying to kind of wrap my head wrap my head around the

21:13.840 --> 21:19.120
relationship between these two terms do you have a perspective on that it's a good question so

21:19.120 --> 21:25.280
I think uh the even the experts in the AI and machine learning community don't agree on

21:25.280 --> 21:30.560
what the definition of what AI is so Hillary and I will just say it's whatever computers can't do

21:30.560 --> 21:36.560
until they can so it really is sort of a it's our subjective impression on what the stuff is um

21:36.560 --> 21:42.800
I mean we consider AI to be today and if you look through the history one can go back to expert

21:42.800 --> 21:48.640
systems and some of the early attempts to mimic first-order logic and the days of touring and

21:48.640 --> 21:53.840
you know uh Claude Shannon and early information theory so um and I actually think there could be

21:53.840 --> 22:00.320
a resurgence of uh symbolic you know AI techniques sometime in the future as we try to move towards

22:00.320 --> 22:06.800
training systems with with less data but um the inheritance of the early 2000s where you know we

22:06.800 --> 22:12.000
really were collecting data and processing it at scales that we hadn't seen before and with

22:12.000 --> 22:17.360
computational power that was faster than we'd seen before I think leads it to the fact that AI

22:17.360 --> 22:22.880
systems today are just sort of a next extension of what we call data science uh sometimes I think

22:22.880 --> 22:28.880
it's meaningful to say the type of data that AI systems work on is different so we're going from

22:28.880 --> 22:37.120
transactional data to images text speech right so categories that historically intractable because

22:37.120 --> 22:42.880
they require vectors of such high dimensionality that we didn't have the backend functions that

22:42.880 --> 22:47.680
could approximate you know universal functions right so just get to the level of complexity where

22:47.680 --> 22:52.320
they actually work for that kind of data I think that's meaningful um outside of that it's

22:53.040 --> 22:57.040
you know what one person calls AI might be with somebody else calls data science and it's just a

22:57.040 --> 23:02.000
question of it's a question of which which term you want to use to get more bang for your buck yeah

23:03.040 --> 23:09.680
so as we wrap things up any parting thoughts for the listeners sure so we talked a little bit about

23:10.960 --> 23:15.680
you know other past guests that you've had that have been focused on thinking about ethics and trust

23:15.680 --> 23:19.760
and bias in these systems um I think it's a big issue we're actually working on it right now fast

23:19.760 --> 23:25.840
forward lab our upcoming topic is on interpretability so the ability that the standard sort of wisdom

23:25.840 --> 23:30.160
today is that if you're using a model like a neural network it's going to give you great predictions

23:30.160 --> 23:34.880
but you have no idea why um and I think in you know some of the large enterprises financial services

23:34.880 --> 23:39.280
healthcare regulated industries and even ones where consumers are just curious about why

23:40.080 --> 23:43.920
why the machine is telling them to do what they're what's it's telling them to do I think there's

23:43.920 --> 23:50.640
some cool efforts afoot to try to gain a little bit more transparency um try to you know turn a

23:50.640 --> 23:55.760
non-linear complex function into something that we can understand a little bit better um and

23:55.760 --> 24:01.600
you know it's something to the companies that a as a as a machine learning practitioner you have

24:01.600 --> 24:05.760
to keep your eye out for because it can be an obstacle to adoption and be that people just seem

24:05.760 --> 24:10.160
to be generally interested in yeah it's a huge issue and that one that I hear coming up all over

24:10.160 --> 24:14.800
the time yeah yeah yeah great well thanks so much Catherine it was great having you on the show

24:14.800 --> 24:17.600
thanks for having me

24:17.600 --> 24:27.600
all right everyone that's our show for today don't forget to share your favorite quotes for one

24:27.600 --> 24:33.360
of our 20 stickers again you can share them via the show that's paid via Twitter or via our

24:33.360 --> 24:41.200
Facebook page the nudge for this show will be up on truly i.com slash talk slash 20 we will

24:41.200 --> 24:49.920
find links to Catherine in the areas resources thank you so much for listening and catch you next time

