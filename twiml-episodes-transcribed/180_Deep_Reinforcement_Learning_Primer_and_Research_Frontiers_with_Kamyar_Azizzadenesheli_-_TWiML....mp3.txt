Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In today's episode we're joined by Kamyar Azizad Anisheli, PhD student at the University
of California Irvine and visiting researcher at Caltech where he works with Anima Anankamar
who you might remember from Twimble Talk 142.
We begin with a reinforcement learning primer of sorts, in which we review the core elements
of RL, along with quite a few examples to help newcomers get up to speed.
We then discuss a pair of Kamyar's recent RL-related papers, efficient exploration through Bayesian
deep-cune networks, and sample efficient deep RL with generative adversarial tree search.
In addition to discussing Kamyar's work, we also chat a bit about the general landscape
of reinforcement learning research today.
So whether you're new to the field or want to dive into cutting-edge reinforcement learning
research with us, this podcast is here for you.
If you'd like to jump ahead to the research discussion, the primer portion of this show
is about 30 minutes long.
We'll have a more specific timestamp noted in the show's description, so check your podcast
app or the show notes at twimlai.com slash talk slash 177 for that.
A quick note before we get into the show.
One day is Labor Day here in the States, and we won't be publishing a show then.
But this is an extra long episode, which will tide you over until we do publish the next
show, which will be Thursday, September 6th.
Alright, onto the show.
Alright everyone, I am on the line with Kamyar Aziza Denisheli.
Kamyar is a PhD student at the University of California Irvine, as well as a visiting
student researcher at Caltech, where he works with Anima Anankumar, who was a guest of
ours back in May of this year.
Kamyar, welcome to this week in machine learning and AI.
Yeah, thank you, Sam.
Thank you for the introduction.
Why don't you give us a sense of your research interests and some of the work you're doing
at Caltech and Irvine?
Right, my research interests is mainly in the area of a specific area of machine learning,
which is called reinforcement learning.
This is my main focus on my main research interests, but at the same time, I do a lot of
other stuff in the field like TensorFlow methods.
I do optimization, I do generative models, study of generative models.
I do a study of safety and furnace in machine learning, because we build up many theory
in machine learning and we build up many practical, like, evolutionary methods, revolutionary
methods.
The question is, can we use them in real world and the question and answer is, hey, you
need to make sure that your algorithm is robots, your algorithm is safe, it's fair.
There are a lot of questions you can ask when you take your machine learning algorithm
and deployed in real world.
Those are not a part of machine learning field that I'm interested in.
We've talked about reinforcement learning on this podcast a number of times, both from
a theoretical perspective, as well as applications like game playing, AlphaGo, things like that.
For this conversation, I thought we would dig into a couple of recent papers of yours
looking at deep reinforcement learning, but also spend some time up front to refresh
ourselves on some of the foundational research in this area.
Why don't we get started by having you walk us through some of the core elements of
reinforcement learning like deep Q networks, for example.
Great, that's an amazing thing.
I really like to, whenever I want to talk about it and explain reinforcement learning,
I get super excited because it's an amazing framework, an amazing setting that almost
it has a lot of intuition from human behavior and also it has super nice theoretical analysis.
The modeling is crazy, great, and I really love it and I really like to explain to others.
It's like, let's assume that I can give you one example, let's assume that you have
a vacuum cleaner in your house or in your apartment or your place and you just leave it
alone and this vacuum cleaner should know how to clean your place.
So what it does is like, vacuum cleaner goes around if it finds something and if it's
like cleaning that place, when you come back home, you tell the vacuum cleaner, hey,
you did a good job.
And if you, the other day you should come back home and like, you see the vacuum cleaner,
you didn't do anything, you tell the vacuum cleaner, hey, you did not do a good job.
It's like, you punish it, I'm not like, you do not punish it, but it's like, this is
the term we use.
So you give some sort of reinforced feedback to the robot, the agent is like going around
the environment or let's say, it even makes it easier in which might make more sense
is like, a baby, a newborn baby, if the newborn baby is getting closer to like, to fire,
the baby feels like some harm and also parents like tell the baby that, hey, do not do it
or like, if the baby does something good, parents, they give the baby like a reward which
is like, let's say, candy, hopefully not candy, but something like, rewarding to the baby
and baby learns that the thing that he or she did was good things over time, the baby
interacts with the environment, which is like the whole world and parents.
And based on the decision the baby makes, like going toward the fire or like getting
higher grade in the, in school, or I don't know, learning how to like, when, let's say
my father of a baby and I'm teaching my baby like, how to ride a bike, if my baby does
a good job, I give a lot of rewards for my baby is like, my kid is like, has this incentive
to learn this task fast.
So it's like, the baby here or the kid here is like the oral agent, it's interacting
with the environment and based on this signal, this feedback system learns how to do
a best job and find the optimal behavior.
So it's a general framework why I really like reinforcement learning because it's like
almost all the time we have reinforcement in problems because you're learning, human
learns, human runs like reinforcement learning somehow is not a, it's not a exact statement
but somehow you can think of it.
So it's reinforcement is actually, or we, in short, we call it RL, it's actually really
interesting framework that has a lot of root in psychology, a lot of root in neuroscience
and also theoretically we found it from graph theory, which was quite interesting.
So from theory, we analyzed many things and some point we realized that oops, is actually
the way human behaves, there was like the nice intersection between like psychology
and neuroscience and like machine learning, which was quite interesting.
So now, this is a motivation why reinforcement is important and why we work on reinforcement
learning because if you can understand the way human learns and the way mathematically
we can make systems to learn, then we can design a robot which can clean a house or
which can like, I can find, I can build a robot arm which helps me to like build something
or like get grass or something, or if I do not have legs, it can help me to walk or
I can come up with the robot, this can help my doctor and my robot is given a patient
comes to my like a clinic, I ask doctor, hey, what is the best prescription you can give
to this patient and also I ask the robot, hey, what do you think?
And the robot has seen many other like trials over the world, so the doctor cannot see
all the possible experiences that other doctors they have seen, but if I can have a robot
which interacts with all the doctors around the world and I give that robot to my doctor
and my doctor can see what my robot thinks and what my doctor is on his own or her own
is thinking about that new patient and then combine this information and give the better
prescription to the patient and patient gets better over time.
So there are a lot of interesting like application and like necessary and interesting application
of reinforcement learning in like, in real world, but the way the model is, there are
many ways to model and the setting, I just want to talk about one of them because mostly
using deep reinforcement learning, let's assume I am playing a game, okay, when I play
a game, let's say, I don't know, you're familiar with, let's say GTA, okay, and then I've
taught her.
Yeah, granted, I thought her or yeah, this is one example that I can imagine many people
have seen it because either they were like punishing their kids for playing this game
like for 24 hours a day or the kids they played themselves is like, you have this agent
going around the world, its own world and like, need to make a decision to optimize its
final goal, okay.
So the decision the agent makes at each point is like walking to left or right or like going
to the barber shop, this or going to gym, this kind of stuff, these decisions at the time
gives it like some reward, each of these decisions has like long term effect.
If my guy in Grand Theft Auto goes to like gym, it gains more power, can run faster, okay.
So this running faster doesn't have meaning at the time, but the thing is this running
faster feature is useful for the agent for later in like few days later when the agent
is going to run away from the cops, something like that.
So the action you're making, the decision you're making now has long term effect.
That effect of this action is like appeared like later, let's say if I am like sitting
in my office, I'm going to grab a coffee, if I grab my wallet now, it doesn't have any
meaning until I get to the coffee shop and I want to pay, okay.
The action I made at the very beginning has a long term effect or if like I go, I want
to invest in a bank, the bank tells me like, hey, if you put money now, you're going
to get returned like over a year.
So the action I'm going to make has like a reward and this reward is like accumulating
over the whole future.
So it's like this reinforcement has this temporal effect, like if I make some decision now,
I see the effect of this decision like way later in the future, which is like makes the
problem hard and also makes the problem interesting because the real world works like this.
If I break my arm now, I might have serious problem like the whole life, my whole there is
my life.
It's not just I see something claim at that time, I see same thing like that effect for
long.
And so one of the big challenges and reinforcement learning is the idea that you've got this huge
in many cases environment, like if we're thinking about this whole, the analogy to humans,
we've got the entire world of things that you can do.
And so how do we explore, how do we decide how we explore all of these possible actions
and states that we could end in end up that that's actually amazing question and the challenge
for reinforcement learning that we are trying to deal with like for many years is like, so
now I give you the world and you need to explore this world, you don't know what each action
is doing.
So, you don't know if you follow some sequence of actions where you're going to end up.
That's exactly the interesting question that you need to explore.
Let's say you go to coffee shop and I'm bringing this example of coffee shop, I don't
know, it makes it more interesting, you go to coffee shop and there is like latte, you
have like let's say five options, there is latte, there is Americano, there is like
cappuccino and brew coffee and also let's assume that in your local coffee shop they also
sell vodka and you go to like coffee shop in eight in the morning, let's assume that
you are not Russian or like Polish and you go eight in the morning to the coffee shop
and you want to get coffee, you for sure not going to order like vodka, right?
But you don't know that you like latte the most or you like like cappuccino the most.
What you do, you need to know which one you think is the best one now and like you drink
let's say you choose latte but tomorrow when you go you are like, I don't know what is
the taste of like cappuccino, let's try cappuccino and for the first time you try the cappuccino
for first time.
Third time you go you order maybe brew coffee and the fourth time you go you are like, okay,
I think latte was good so I'm going to go with latte today, so you get latte and the
day after you say, well I'm not pretty sure how how like cappuccino tastes compared
to latte, so next time you try like cappuccino, so over time you do this type of expression
you are trying different things but you do not like try vodka, so you try some actions
that they make most sense, right, so this is the way we want the RL agent or robot to
do exploration, we don't want to have a RL agent to explore everything uniformly which
is not possible, I don't want to have a GCA organic auto agent to just go left, left
right, like without any reasoning of like what it does, I want to have the agent which
does exploration, in such a way that it maximizes the amounts of information it gains from
the environment while trying to, I mean by information I mean it can build better understanding
of the world, while do not forgetting that the agent is there to collect like more like
rewards, it's like those money in grant us also getting, so my agent wants to, my GCA agent
wants to go around and understand how the world works, while it wants to maximize the understanding
of the world or minimize the uncertainty around the world while trying to maximize the
reward of the game, maximize the score of the game or like get the game to be done, if
you have a kid playing like GTA, the kid doesn't explore the whole year and then start
playing games, the agent while playing games like try to learn and build a model and see
who is friend, what is good, going to gym is good, at the very beginning the kid might
not know that going to gym is good, maybe if you go there they kill you, so something
that, so there are a lot of things, characteristics of the environment that you need to learn,
you need to explore, but you need to explore it carefully, so this way you do exploration
is like actually the key factor in reinforcement learning that most of my works are based on
like how do you do exploration efficiently, and so one of the simple concepts that comes
up in reinforcement learning is this idea of explore, exploit and you've talked a little
bit about that trade off via examples and then one of the algorithms that encodes that
is this idea of epsilon greedy, what is that?
epsilon greedy is actually super powerfully interesting like algorithm and it's super
simple as well, what it does is that again I'm going to talk about coffee shop, you're
at the coffee shop, you have these five options like I told you, you have lots of cappuccino,
americano and like blue coffee and vodka, okay, so now let's assume that so far based on
the, let's say you've seen that you have been in the coffee shop for many times and you know
that you feel that you are a fan of latte demos, you like them all latte demos, so your greedy
decision, which is the decision which maximizes your satisfaction is choosing latte, okay?
So epsilon greedy exploration and exploitation strategy says hey, with probability like,
let's assume epsilon is like 0.1, it says with probability 1 minus 0.1 which is like 1 minus epsilon,
I'm going to choose the most greedy decision which maximizes my satisfaction which is latte,
so with probability 90%, I'm going to choose latte and also I say hey, I'm also not
pretty sure about other other decisions, I'm not sure, very sure about like cappuccino,
so what it does with probability 1 minus epsilon which is like 10%, the agent randomizes
over all these five actions, I think it says like with probability, but with probability 10%,
it's randomizes over all the actions means that sometimes it's like choosing cappuccino,
sometimes choosing americano and sometimes choosing latte when it does exploration and also
it sometimes chooses vodka, so it's like the probability that you choose vodka is exactly equal
to the probability that you choose brute coffee or like cappuccino, which is actually the part
that epsilon greedy sales because you know that vodka is not good, why you choose it again and again,
so this is one part that in one of my papers we address and we resolve this issue which is quite
interesting, so epsilon greedy is like the powerful algorithm, it does choose like the best action
with high probability, the best action I mean the best action that agent so far knows is the best,
might not be, might not be the best, and with probability 1 minus epsilon, sorry with probability
epsilon is gonna just uniformly choose other action, even though the agent might know that some
actions are really bad, but it does choose them, so this is the epsilon greedy and it's been
used in like area of deep, it was a prominent like algorithm for making tradeoff between
exploration and exploitation in area of deep reinforcement learning, and one of the
famous algorithm which uses this is deep q network, so I can just briefly tell like what deep
q network does is, so let's go back again to the grandest auto game, deep q networks are one of
the algorithms for deep reinforcement learning that really popularized the space and
launched a lot of the efforts to solve video games and things like that, it was created at deep
mind, is that right? Yeah, deep q network was created at deep mind and it was like one of the
main reason why like there are many researchers are working on deep reinforcement learning because
this first paper made it somehow possible to go beyond like a small grid work, small games
that theoreticians we were like working on the small work and like this paper was the first paper
that we were able to apply reinforcement learning method on the games like Atari's and like the
games, the video games, the games that we were not even thinking that possible to solve
using deep reinforcement learning algorithm at the time, but this paper was like a kind of
revolutionary paper that brought a lot of attention to the field of reinforcement learning and
people start working on this field, a lot of practitioners and like scientists they start
working on these games and on these algorithms because this third paper made it somehow, it's not
I'm not claiming that this was the it was like the huge gap between the previous works and this
work, there were like many works before like deep q networks they were able to do many things on
Atari games, but this one was the simplest one which had like simple idea and before it
neural networks in order to solve Atari games, so it was quite interesting. And so what's the
relationship between epsilon greedy and deep q networks? So the very deep q network works is like
let's get back to the grand test auto, the agent walks around the city and for each action it
knows that if the agent choose that let's say action going forward or shooting that person,
it has the follow up return, like if it does something it might win the game or like it might get
some reward or some money, some score, so each action has like like upcoming and like forwarding
reward, so it's like each action has value, how good is that action? If I kill that person,
sorry for my language, but if I go to the gym, how much value it has for me, how much I get
full fit if I do this action now, so what deep q network does for when it plays game is like
given a state of the game, even given the image of the game, which is like the frame of the game,
it decides how good is each action, it's like compute the value of each action, okay?
For the time if I play sequence, which is like a submarine is like in the sea and it's
getting out of oxygen, it needs to go on the top of the sea and like get some oxygen,
if I see I'm running out of oxygen, I know that going up has the most value for me,
it's like I'm staying around the bottom of the sea has lowest value, so what deep q network does,
deep q network finds their value associated with each action, okay? The way it constructs this
value is like the agent needs to interact with the environment, the way the agent interacts with
the environment collect samples and explore the environment is excellent, so what deep q network does
builds the model, which somehow gives the value of each decision, and with probability like
one minus epsilon, it goes with the best decision, the agent so far thing is the best,
and with probability epsilon, it randomizes over the all the action, okay? So if I run deep q
network on myself, when I go to the coffee shop, it probably one minus epsilon, I'm gonna choose
latte, and with probability epsilon, I'm gonna randomize over the all the action, okay? So this is
like the way deep q network works, it learns how good is each action, it learns the value associated
with each action, how good is making the like action up at the current time step, how good is
this action? So it learns this function, and when it learns this, while the way it learns this
function is like collect samples to learn this function better and better. And so we can think of
deep q networks as essentially an algorithm for accounting for the various values of these actions
over a series of steps, is that fair? It's fair, it's fair that it's like counting the
amount of reward, it's gonna receive in the future, and somehow in stuff memorizing all of them,
it learns a function which approximates this count. And so in the best case, there are many
theoretical analysis, we show that it doesn't do it exactly, but for this conversation, we can
assume that it's done. Does deep q networks specify a particular type of neural network architecture,
or can it be implemented with multiple different types of networks? Oh, deep q network is actually
an algorithm, and it can be used for, and you can anyone can design his or her own like our
neural network architecture. Neural network here are like the machinery used to solve this problem,
but deep q network it's own is like a generic algorithm, doesn't care that you're using neural
network, you're using Canon machine or using linear models, I mean, the objective is being used
there is generic and can be applied on the on the variety of different models and can be applied
on variety of different architectures, but the first paper used this objective function
on using like deep neural networks, so that's why we call it deep q network. So the thing is like
it can be applied on any architecture design for the deep network. I can design my own
deep, I design also like for different tasks, I design different neural network architectures,
but I still use deep q networks like machinery, and I'll go down to optimize and learn this value
of each decision. So it's kind of generic algorithm, it's not just for a specific neural network,
it's like it works, I mean, it's generic and it's applicable to a variety of almost all the
neural networks. Are there specific neural networks that folks tend to use with deep q networks?
Yeah, for Atari games, when we are like dealing with Atari games, we are using a specific neural network,
mainly people have tried different neural networks, but mainly we use the same neural networks that
give mine paper back in a day, not back in a few years ago, there's a lot. I mean, nowadays the
time is, when you say back in the day, you mean like five years ago in the field of AI.
So far, like most of the researchers, they use the architecture designed in the original deep
q network paper, which is interesting because we use the same architecture and we design better and
better algorithms on the top of deep q network. So we have a variety of extensions to deep q network,
we have double deep q networks, we have my work, which is like Bayesian deep q network,
is like we use the same architecture, but we are developing better and better algorithms.
And once again, what makes an algorithm better and better in this context is its ability to
make better decisions about what elements of the space to explore or what decisions to make
in any particular juncture so that it's more sample efficient. That's a concept that comes up
a lot in here. How efficiently are we using our time in the environment to train an algorithm?
That's a super interesting point that you brought up is I want to learn the optimal behavior for
let's say, playing games in minimum number of interaction with the game, right? I don't want to
like play game for 25 billion years in order to, by game, I mean, let's say game pond or game
and sequence. I don't want to play this game for like 25 billion years in order to be able to
play years, I mean, like time for the for the for the entire game. So I don't want to play,
I want to like play these games for like, I don't know, half an hour. By hour, I mean,
by this time I'm talking is like the time that actually you need to play that game.
It's not like the time that your RL agent is going to use is going to be the number of
interactions with the environment you have. You don't want to have the number of times you
play the game. You don't want to be like 20 billion in order to stop the game. You want to
play this game 100 times and be able to learn it or you want to play this game like 200 times
and being able to learn. So the sample complexity is really issue in reinforcement learning and
the goal mainly is to design an algorithm which makes the optimal balance between exploration
and exploration and minimize the sample complexity or some other notions that we call regret,
which is like, you don't want to lose a lot before getting to the good performance.
Another thing is that these environments, they can effectively have like local optima,
meaning you could going back to your Starbucks coffee shop example, you can kind of get into a
rut where you settle on the latte and that's what you choose. But you don't know that in one
particular day, you happen to order the cappuccino, the person behind you also orders the cappuccino
and you find out that your soulmates and like we're destined to be together. If you didn't order
the cappuccino, you would have never have had that experience. The games that these environments are
so dynamic that unless you're really careful about the way that you explore them, you miss opportunities.
Yeah, it's like your tour ride, it's like these environments are super complicated,
like the dynamic is like really complicated and that's a part that makes the whole our life
really hard and also interesting. So like if the setting might have a lot of
weird and complicated situation and it makes me to explore all of them. But if I'm gonna for the
first time I go to coffee shop, I don't care that much that there is a person behind me is gonna
be my soulmate. But if I go to coffee shop and I learn that which coffee actually I like,
then I'm certain then I start to explore other stuff. I see who is behind me. So for example,
first time I came to United States, I was not able to speak English very well. So when I went to
coffee shop, I was like just focused to get my coffee and pronounce things correctly and like
say my name correctly and be ever and pay correctly. But nowadays when I go to coffee shop,
I just while ordering coffee, I talk to the person behind me. So over time when I get more
confident and confident about the state of the environment, the way I need to make a decision,
I start to do more complicated exploration. So it's like a very human does and I was like when
the first came to United States as a person who was not speaking English before coming here.
So I had this experience that when I start going to coffee shops, I was not even care who is behind me.
I was like I was just focused to talk to the lady or the person, the guy there and just make my
mission accomplished. I wanted to just order my coffee and make this that task done correctly.
And so you've got these two different metrics for what makes a good algorithm. One is it sample
complexity, sample efficiency. The other is the degree to which it fully explores the environment.
There are many times but for simplicity, we all let's call it like sample complexity,
like how many samples I need to come over the good strategy. Okay, so in other words,
what I'm here you say is you can kind of boil all of that stuff down into sample complexity
at the end of the day, whether it's the algorithmic, like the computational element of it,
whether it takes a long time to converge on anything or not, whether it takes a long time to
figure everything out or not. But all of this stuff is ultimately related to the sample complexity.
Yeah, all of them are happening like the goodness of the sample in the sense that I get
lower uncertainty about my world. And also how much that knowing that sample is going to help me
to come up with a better like strategy or better decision. This combination drives me to come
with a sample complexity. So with all that in mind, maybe you can walk us through a couple of
your papers on this topic. You know, one of them is the one that you mentioned earlier,
the Bayesian DeepQ Networks. What are you trying to do there? Yeah, the Bayesian DeepQ Network is
actually quite interesting. I really like that work. It has some theoretical like guarantee
about sample complexity, but also it has interesting behavior in like in real but on Atari game.
Let's go back again on the coffee shop example. Epsilon really what we're saying is like it's
going to choose latte with high quality and randomized over all the actions, all the other
decisions uniformly. So Epsilon duty is also going to choose the vodka with the same number of times
that it's choosing other like cappuccino or good coffee. But the thing is what it's happening
is like it doesn't care that how confident you are about the other action. It just cares how much
information you know about the best decision, which is like latte. So even for DeepQ Network,
DeepQ Network is like computing the value of each action. If the action related to the vodka,
it has a really low value, the Epsilon duty action is going to choose that. So what we do in DeepQ
Network is we say hey, instead of just estimating the by we do in Bayesian DeepQ Network is we are
saying that instead of estimating the value of each action, how good is each action, also
estimate how confident how confident you are about the value of that action. Let's say I'm really
confident that value of like the latte is like high, but let's say I'm not confident that the value
of the cappuccino is high. So I have estimation of the value of the cappuccino. I know how cappuccino
should be good. And I also know that I'm not that certain. So it worth trying. So if I know that
the value of the cappuccino is like high, but it's not as high as latte, but I'm really uncertain
about this value, like how good is the cappuccino, then I would like to try it. And also if I know the
value of the vodka is really low, and also I know that with high confidence, I know that the value
of vodka is low, I'm not going to ever like try it, right? So what DeepQ Network does,
Bayesian DeepQ Network is like I'll algorithm on the top of DeepQ Network, but it says in sub
estimating just the value of each action, also estimate how confident you are about each action.
And then you want to make a decision, see how good is the value of that action and how much
you're confident. And if you're not confident about the action with high value, let's try that one.
Okay, so it's like what it does is like the exploration doesn't happen in epsilon giddy setting.
The exploration happens in a setting that it tries to maximize a combination of the uncertainty
and expected like a goodness of that action. If the action, the agent thinks it's good,
but the agent is certain about that, but there's another action, you choose a slice, it's
worse, but you're really uncertain about it, you want to try that one. Let's squeeze this one.
You go to coffee shop, you go, you always get lottery, and you never got like cappuccino,
but your grandma every day talks about cappuccino, okay? When you go to coffee shop,
you have, you, you believe that the cappuccino has high value because your grandma is always
talking about it, and but you never tried it. So you have a gigantic uncertainty about it,
okay? But still you love lottery. So at this situation, you better to choose like the cappuccino
because you think is a really good drink is not as good as lottery, you think, but you're not sure
that's how good it is. Maybe it's way better than lottery, but you don't know, okay? But the
expectation, your grandma, the way your grandma explains it to you, you think that it's a good drink,
it's comparable to lottery, but it's, but it's still you like lottery a bit more because,
but based on your grandma's explanation, you don't know that cappuccino is how good it is,
but you roughly speaking, you know that it's not, you believe that it's not better than lottery,
but you are not certain about it. So you just tried that one. So it's like kind of balanced
between like uncertainty over the your leap about the value of each decision and makes the
decision based on the expected value of each action and also on certain your over that
expected value. And so are you in the epsilon greedy, you are choosing your primary, the,
the one that you believe has the most value at a probability of one minus epsilon and then
say you've got your five choices, the probability of you choosing one of those other ones is
epsilon over four, right? Yeah. And epsilon over five. I randomize over the whole thing.
Are you randomize over the whole thing? And so that was my question with, with the approach
we're describing, we're waiting, are you, are you doing this confidence waiting over just the other
four or over all of all of you just kind of evaluating each of them based on this confidence
weighted metric. Exactly. So I just for each action, I have estimated value and also I have
uncertainty. And I, what I do, I, so somehow it's like, I have a belief about each action.
I am like, I know how good it's going to be, but this goodness is not a number. It's like
somehow a distribution over each action. So I'm like, when I say I believe that this should be,
okay, a means of high probability, I think it's good, right? So for each action, I have a
distribution about how good each action it is. And the mean of this distribution is going to be
my expected like, expected like value. And also this distribution has some variance,
it's going to be somehow my uncertainty. Okay. So for each action, I have a belief and my
belief is somehow a distribution over the, over the goodness of each action. Or it's going to be
uncertainty over each action. And thinking about it in terms of distributions is where the
concept of basing comes in the body. Exactly, exactly. So this distribution is like my belief,
my posterior belief about each action. So if I know the posterior belief about each action,
what I can do for each action, I can sample out of this belief. I get some, like, let's assume
that for Lasse, I am my expected, expected value is like five. And my variance is like one,
is like, I'm somehow uncertain that this is between four to six. Okay. So what I do, I sample
a number between four and six. And I assume that the value of the, the Lasse is that number.
I do the same thing for other, other actions like for Lasse, for Lasse, I know that my grandma told
me the value of the lot, sorry, for, for Kappuccino, my grandma told me the value of the Kappuccino
is like four. But I'm like my uncertainty is like the variance of my... She thinks it's higher
than the Lasse. So maybe like eight. No, no, she tells me that the Kappuccino is good. But for me,
she thinks that Kappuccino is the best. But for me, I think the Lasse is the best.
Ah, okay. Okay. So for me, Kappuccino is like four. I never tried it. My grandma told me,
I put four for Kappuccino. But the variance for Kappuccino is like 10. So the Kappuccino can be
between 14 to minus like six. Got it. Got it. So if I sample, there is a high chance that the
Kappuccino is going to get a number above like five. If the Kappuccino gets a number of five,
I'm going to, I'm going to choose Kappuccino. So this is the way I do exploration. This is called
Council Sampling. There was a guy back in like 1930s and he developed this idea of sampling.
If you have a belief about the environment, you sample out of that and just do just act based on
that sample. Also in psychology, the literature is called, they call it, they do not call it
Council Sampling. They call it Beijing Sampling because like they are from psychology backgrounds
so they they define their own term. But they have a cool setting that they say even human,
that's why I bring up this coffee shop example. It's like they say in psychology that human also
does Beijing Sampling. The human come up with a belief about the world, a belief about each
decision and sample out of that belief and do the thing, the human thing. It's like the human
randomizes over its behavior. It doesn't go always with treaty. We do not always get a lot
of it. Sometimes randomize. The way we do randomization is sampling through our belief.
This is playing in psychology. I'm not making the discreet. I'm just fitting the thing they say.
I'm not sure how exactly to articulate this but it strikes me that there's one of the important
assumptions in here is this idea that, I mean, I guess the whole idea of sampling, that you can
just pick a number and even though relative to the distribution, that number could be an extreme
outlier. We're still just going to use this number as to make our decision like how,
what tells us that we can do that? So that's an interesting question.
So if I draw samples and one sample is suddenly it's extremely high and like the question is should
I go with that or not? So I guess maybe to start to answer my own question, are we just kind of
reverting to law of large numbers here? If we do this enough, we're basically sampling around,
you know, we'll kind of converge to our distribution. It's slightly related to that but it's not
exactly that. Here it's about concentration of the measure mainly. It's like if I have uncertainty
about the latte and cappuccino and others, if I sample them a lot, then I am going to be sure
how good they are, right? If I'm, let's say in the like not realistic world, I'm able to drink
latte 10 billion times and I'm allowed to drink cappuccino 10 billion times, then I can say
which one is better, right? So it's like over time, my belief concentrates over its actual value
because latte for me has a goodness. I don't know what is that goodness and I need to try it many,
many times to understand what is that goodness and if you allow me to drink latte billions of times,
then I can over time I can like shrink down my uncertainty about latte and at some point I say,
hey, latte is actually a bit, even though might not be, but for other people, for me, let's say
is like when I drink latte, a lot and cappuccino and a lot, over time I am certain about how
which one is good. So it's not just law of large numbers because we're not just sampling a lot
from this distribution to learn its parameters. We're also updating the distribution,
the parameters of the distribution as we go along to reflect our increased confidence.
Yeah, and here is like the distribution, my belief is my distribution,
right? So this distribution at the very beginning has a like fact tail, but over time when I collect
more samples, I'm going to make sure that, so what is this distribution? This is the distribution
over the value of latte, right? So the value of latte in expectation is a fixed number,
so it's five, right? So, but I don't, I'm not sure about it, so I am uncertain, so this uncertainty,
so this distribution represents my uncertainty and over time when I collect more samples,
I'm reducing my uncertainty and this uncertainty is going to be shrink down and over time if you
give me, if you allow me collect more samples, I'm going to shrink down my uncertainty to zero
and claim that I exactly know what is the value of the latte. So it's going to be like a measure
of uncertainty, like if I, if I get more experience, if I try same thing many times, I get more,
I get more certain about it and then I reduce somehow the variance of my belief about that,
that action, that decision. So it's going to reduce that the uncertainty over the world for me,
so and it's been like a study that if you use tons of sampling, you actually balance between,
nicely balanced between exploration and exploitation and you actually get a like ordered off
and all like sample complexity. So a part of your work then is applying this, this Thompson sampling
algorithm to the deep Q networks and does the analytical results follow as well into the reinforcement
learning realm? Yeah, it does. So I'm not applying Thompson sampling on deep Q networks,
I'm extending deep Q networks to have this Bayesian property. Deep Q networks, as I said,
they just compute the value of each action, but Bayesian deep Q networks use the same machinery as
deep Q networks, but in self computing, the value of each action, it also, not only is
estimated value of each action, it also estimate the uncertainty of that value. So it's like
collecting more information. And on the other hand, for exploration, Bayesian deep Q network
doesn't use epsilon greeting, but it used tons of sampling, which is like super interesting.
I mean, the first time I applied this algorithm, this method on the Atari game, it was like,
when I was showing this result to my colleagues, no one was like, like believing that was going on,
because it was doing super great, like 1000 times better in the performance, like 100 times better
in sample complexity. It was like crazy, really good. It can learn the game that like deep Q network
learns in like, I don't know, 100 million times that it can learn it like less than 5 million,
or less than 2 million for some games. It's crazy, really cool. It was like
interestingly, interesting observation and made us to think more about exploration. If you
like the literature in the deep Q networks, and learning almost like more than 99 percent of them,
they use epsilon greeting. They're few of them, they are using more sophisticated exploration
algorithm. And here we show that if you just end up doing architecture design, if you just
come up with better exploration strategy, you're going to gain a lot.
Has that result stood the test of time? Is this still state-of-the-art for certain games,
or has it been extended by other folks? Where does it sit in the landscape of extensions to
deep Q networks? Interesting, interesting question. After deep Q networks, there have been many,
many extensions to design better cost functions, design better, like sampling, the design
better handling of the memory. There are a lot of extensions that they advanced deep Q networks,
but they're almost all using epsilon greeting. So we are not comparing against those advanced
architecture. We are just saying, just take the simple deep Q network, and instead of doing epsilon
greeting, do top of that. And we show that it does way better, and also we show that it does
better than many, many other algorithms, many, many advanced algorithms, but we did not compare
against those algorithms that they are advancing the architecture or advancing some other like
v-warding functions, because the point of its work was like, hey, if we just change the exploration
algorithm, what's going to happen? If those algorithms, those algorithms, they are advancing deep
Q network. If they, if I applied this top of sampling on those algorithms, I'm going to get
like better performance. Got it, got it. So you kind of left it as an exercise to the reader to
take their favorite extension to deep Q networks, and try this as a way to get even better sample
complexity. Yeah, it's like just this is like the way we say to do exploration and exploitation
instead of epsilon greeting. If you have a sophisticated like deep reinforcement learning algorithm,
you're, if you're using epsilon greeting, if you apply this one, this approach, which has a
theoretical analysis, and a theoretical guarantee, you hope to get better performance.
What you've done here is you've proposed an alternative to epsilon greeting based on
Thompson sampling, so based on uncertainty waiting. Presumably, there are other ways that you could
change the exploit mechanism further. How well explored is that space? Have a lot of people
proposed different algorithms for dealing with exploitation? That's a super interesting question.
Is from land of theory, like the main question we are asking on everyday is to come up with
the better exploration and exploitation algorithm. From the theory land, we developed a lot of
nice, amazing, and sweet, I would say algorithms, they, they balance this exploration
for different settings. So, theoretically, this area is nicely studied. There are a lot of
rooms to explore more and to prove and study new things and find the optimal way of exploration.
But the thing is extending those methods. Even I have seen like a theoretical way of doing
exploration, exploitation in the theory land, but it's not easy to extend those ideas and those
settings to deep neural networks. So, from the theory, we have a lot of algorithms and we have
a massive amount of studies that we know how to do exploration, exploitation to get some sort of
order optimal, like safer complexity. But the problem is that those algorithms are not easily
extendable to deep reinforcement learning. Is it simple to explain or give an example of an
algorithm that isn't easy to apply or how or why they tend to break? So, one of the major
portion of the theory land in reinforcement learning is like model-based reinforcement learning.
What we do is like we literally learn the model dynamics. For each state, we store the
transition from each state to another state. If I'm receiving, I'm going to coffee shop,
I get a latte and after that, I'm going to go back. So, we need to store all these possible
transitions, going from one situation to another situation. We need to store all these things
and do our optimization on the gigantic space of possibility. It's not possible to store
all these things in the memory and do your computation. It's not possible. Also,
it's like most of the algorithms we designed for the area from the theory land,
they are not always. Most of the time, we were really concerned about the worst case scenario.
We want to compute, we want to come up with the algorithm, which no matter what is the model,
is going to perform the best. So, it makes almost all the algorithms we developed so far,
makes all of them such that they are the best for worst case scenario. Because we want to
provide a theory. Why this is a case is a case? Because we are going to provide theoretical
guarantee. When I say this algorithm for short works, it means that for no matter what is the world,
what is the environment, it should work. But that doesn't mean it's practical for
any given sense. Exactly. Those are the algorithms that they give you bounds and they give you
theoretical proof that they are going to work. But the thing is, if you know that your environment
is not that bad, it's not adversarily chosen or it's not the worst case scenario ever can happen
to your algorithm, then you can come up with a better and more sensible algorithm.
Reinforcement learning is kind of young. We need many, many more people to spend time and do
and build up that theory to kind of stop reinforcement learning. So we are like the community of
deep reinforcement. The community of the theoreticians in reinforcement learning is not as big as
I cannot even say like it's like the proportion is almost getting to zero. If you look at the
number of people they do theory of like reinforcement learning compared to the people they do
empirical study of reinforcement learning. This ratio is like super small. You cannot even see
theoreticians. Which is like, I'm not, I mean, it's almost nice and also is a concerning. We should
do both. And so just to make sure I understand the first of the two impediments you mentioned,
it is that in practically speaking with deeply reinforcement learning models like deep Q, they make
simplifications that lend themselves to, you know, practical concerns, computational concerns.
And so for example, they are looking at, they're kind of rolling up the value into a single
state, if you will, or element, if you will. Whereas some of these algorithms that are, you know,
you could argue are more optimal, you know, they may be looking at a wider, you know, wider
set of observations of the state that you couldn't really operate on practically. Is that what
you're kind of getting at? So the more sophisticated algorithm they design, more sophisticated
cost function. So they are like, they are designing the advancing the objective function that
deep Q network model is solving. So it's like, they are advancing that. And the idea behind
deep Q network was not giving the best algorithm. They, the Q network came out to show that it is
possible to play games. It doesn't claim that it's the best. And the aim of that work from deep
mind was not heavier doing the best reinforcement learning algorithm. They wanted to just want to show
that, hey, it is possible to use current algorithms to in order to beat human or like, in order to
learn simple games. And a big part of that possibility was the computational feedback. Yeah, they didn't
yeah, they didn't care that much about like, like, like sample complexity. They didn't want to minimize
the number of samples that the agencies, they wanted the main goal was, hey, we want our
else from like other algorithms and we are going to ask from other like human. That was the
goal behind the nature paper they had was like, hey, this is the case that you can come with the
algorithm, which does better than human in certain games. Or let's say for alpha, go like the
goal game, you do not care about the damage about the number of samples that your like goal players,
like your agency, you just at the end wants to like, like, win against the best goal player.
Right. Right. So it's like, this is really important. This is really interesting to do, to be
make it feasible for first time. But after that, you need to design the algorithm to do the thing
that reinforcement learning actually requires. It's like reducing number of samples, like learning
better policy. So they made it feasible. The rest is going to be like, first make it better.
By making it better, it's like coming with better sample complexity, coming with a better value
estimator. In the second work that I want to talk is like, you can easily show that the QN
objective is actually biased objective function. So you better off not to do it. If you want to be
really serious in reinforcement learning, you better like do something else than get QNF. But if
deep QNF is the only thing you can do, we just go with it. If you can't, I mean, if you're able
to do better, it's better off not to deep QNF because the cost function is easily biased and can
it can screw you off. So elaborate on that. You said the second cost function.
No, in the second work that I get chance to talk about here.
In that second paper, walk us through the results. So you started with looking at the cost
function for deep QNF works. So I started to looking at the cost function at deep QNF work,
and I have many friends from theory lands. They have many amazing papers and astonishing papers
that are analyzed back in like 10 years ago or 20 years ago that they analyze this cost
function that one of them has been used by deep QNF work. And they analyze this constantly.
They show that hey, they are biased. And also there is another work we show that for any function
approximation method, there is a subset of problems that if you run this functional approximation
method, they either diverge or are biased. So it's like, there is no hope that you can get
algorithm, which is like, you think that it can do something reasonable almost everywhere.
But the hope is that they do not break in the simple setting. So if you look at the deep QNF work
objective, it's like it minimizes two terms where you just care about one of them. The second term,
you don't want to minimize. But it's naturally minimizes. And also it has some other issues of like
like it somehow looks at, and stuff like like averaging the errors, it just always looks at the
maximum error and like back propagate that maximum error, which is not the thing you want. If you
have a Gaussian random variable, you don't want, you don't care about it max, you care about
like it's mean. But there's the thing that deep QNF work setting passes through its back
propagation is actually the max of the sample, which is not the thing you want. So it's kind of like
biased in many, many different senses. And the question is, if it's biased, and if we know that
any function approximation method can be biased or diverges, can we do anything for that? That was
the question I was asking myself when I start to work on the second work, which we call it
Generative Adversarial 3 Search, or in short we call it Gats. Probably you have watched the movie
Gats B. So the name has some relation to that guy as well. But yeah, the algorithm is called Gats
and it's trying to address the fact that if I use functional approximation for deep reinforcement
learning, and if my functional approximation is biased, can I do anything with respect to that
or not? And presumably you found that you could do something using a generative adversarial network
based approach. Yeah, that's an interesting idea. So there's a line of research in psychology,
which says when human wants to make decision, it forecasts what's going to happen in the future.
Or people say imagine I would, I would like avoid using word imagine, but I can't, I would say
human like when I want to go to coffee shop from my office, I just imagine or forecast or
predict what's going to happen in the future. I can think of going there talking to the coffee man,
an order and pay and think about what I'm going to get and come back. I can imagine and think
about it. So this is a line of research in psychology which says sometimes when human
makes decision, human builds the model of the environment, model of the world, and in that world
does some analysis. It's not always human does analysis based on just observation. When human
interact with the environment with the world, it has some model of that some abstract model of
the environment and the world in his or her brain, okay? I mean, baby, it doesn't make sense, but
I think to me it makes sense because when I'm going to do something and when I want to plan for
future or for my like like a trip of I want to do a road trip, I plan everything and I think of
what's going to happen, what are the situations, all these scenarios, I check all of them in my brain,
I do not start my road trip and then see what's going to happen, I just plan everything.
So this is a thing that we they say, of course there are like many decisions we do not plan for,
like let's say there's a mosquito, it just bites us, like we do not plan to just scratch that
part of body that happens. But part of it happens through this planning, through like the model of
the world that we have in our brain. So we were saying like let's get some idea from this part
of psychology and also see what theory tells us. Theory tells us that functional approximation
like is can be bias or diverse. So now if I have let's say bias like value function, okay. And if I'm
going to use it and also if I'm able to learn the model dynamics, the dynamic, the way model works,
I can build the model of the world right in my brain. Or somehow the agent can based on
interaction the agent has with the world here by world, I mean let's say Atari game, given interaction
that the agent has with Atari games going left, right up shooting left kind of things,
it can learn that given the current situation, if it does some sort of sequence of like decisions,
what's going to happen in the future. So in this work, we used this an interesting framework
out there, which is called generative adversarial networks. These methods, they're like used to
generate images out of like random noises. So they are like the kind of generative model. And we
use this machinery to be able to make ourselves able to like come up with a model of the world.
So now given the samples, we designed an algorithm and a neural network which is like able
given like current frame of the Atari game is able to tell you what is going to be the next 20
frames. If you follow up, down, down, up some sequence of action, it's able to tell you what
actually is going to happen in the future, which is quite interesting. So if I am able to see
like 20 steps from now, okay, then I can say, hey, whenever I'm going to make a decision, I just
see all possible all possibilities from now in 20 steps in my brain. And then in the lift node,
so it's going to be I'm going to construct a tree. I say, if I am at the current time step,
if I am my agent, my like my Atari agent is at some location in the game, if it choose action
up, very it's going to go, if it choose action down, very it's going to go, if it goes forward,
way it's going to go. So I can ask my this generative model, because it's generative dynamic model,
as short like GDM. I can ask GDM, hey, if I if I am at this kind of state and if this and my action
is going up, what's going to happen? And GDM tells me you go to the new place and I ask GDM, hey,
now if I am in just new place, if I go action down, where I go. So I can build up a tree for
different possible actions and see where I go. It sounds like, I mean, in a regular
DRL type of problem, they're also kind of evaluating the expected reward for each of the
possible actions, but that the notion of expectation is like you're you've lost a lot of
information. And so is the idea that by using GANs to project the future state instead of or
an expectation, you retain more information? Okay, that's a great point is if I run my DRL algorithm
to compute this value, what I'm claiming here is that value is can be really, really off.
It means that can be arbitrary bad. Okay, so if I run DeepQ network for, let's again,
pond for a given situation, the pond agent, if I run, if I run DeepQ network, it tells me
going up gives you return of like value of like 10. Okay, if I run Deep, like double DeepQ
network, which is another algorithm, it gives me 5. It's like the DeepQ network, the value
was estimating was like off square factor of 5, by another factor, by bias of 5, at least.
So it's like my point is like any deep RL algorithm can be arbitrary bias.
And now the question is like if it's bias, it means that as you said, it's supposed to tell me
what is the future reward, right? But if this estimator is like wrong, then the question is what
we can do. So what we can do is like we train a generative model in order to be able to like
generate future. And if I am able to generate future, if I'm like, if I'm, if I have an agent
is playing in the playground and it's a ditch somewhere. And if I imagine, if I like, if the agent,
if I use this generative model and see what's going to happen in the future, I see if I choose
action up, up, up, I'm going to fall in that ditch, right? So I do not go. So it's like,
somehow I'm seeing what's going to happen in the future. Yeah, I think maybe the question I'm
that I'm asking is what is it about GANS maybe that allows us to be any more accurate looking
into the future than the machinery used for, you know, for predicting a future reward?
Okay, so it's like, so we theoretically show that it's actually exponentially can improve the error
in the, in the Q function. Okay, so it's, theoretically, it's like exponential improvement,
but practical, I can tell you something. So the way we compute the reward,
the way we compute the value and return is through discount factor, right? So there's a discount
factor. It's like the reward I get that kind of time step worth more than reward I receive in
like next 10 times steps, right? There's a discount factor. So now, if I have my, if my Q function
has a bias of five, okay? And if I am able to do rollout in my generative model up to depth of
up to depth of 20, then the Q value I'm going to use at depth of 20 is going to be
discount factor, power of 20, discount factor is less than one. So the power of 20 is going to be
super small times that five. So the effect of the bias is not going to appear. So actually,
this machinery has been used for AlphaGo as well. What AlphaGo does AlphaGo is like,
runs Monte Carlo 3 search on the board game for some depth, let's say depth of age. And when it goes
to the like leaf node of that tree, so it builds a tree and then the leaf node of that tree
use the learn Q value, okay? If the Q value is biased, since I'm like rolling out for like depth
of age, the effect of that bias is going to exponentially go down. I can imagine I need to draw
three in length. No, I think it makes sense. So your the argument is that with the deep Q networks,
you are predicting the future reward at a given point based on a given action. But by using
GaNS to project into the future, what the board is going to look like, you can then discount
out the errors by the cause of the you're predicting further into the into time.
That one thing is like the first part of your statement. When I use deep reinforcement
and algorithms to compute to see what is the cumulative reward of future at the given point
and given action, you said, my statement here is any deep reinforcement and algorithm use
this error error in this estimation can be arbitrary wrong. It can be arbitrary B. So the
actually you think that you are actually learning the expected return of that state and action
given that the specific point that a specific action. But I can prove of the show that there are
problems that this estimator can be arbitrary wrong. So now I'm saying that if I had a perfect
estimator, there was no need. I mean, there isn't there isn't that much of need for having a
generative model or GaNS. But I know that this error is big for like for deep Q networks,
these errors are gigantic that sometimes like shockingly gigantic. People have studied this
by bias. Because the bias in the estimator, people have studied this biases and they observe
that these biases are like huge. It's like hundreds of percentage are like bigger than the actual value.
So it means that the estimator you get, when you compute the expected return condition
on the specific point and the space and the action, that estimation can be arbitrary wrong.
Okay, so if it's arbitrary wrong, you cannot do anything. But if it's like wrong but not super
wrong, the question is can we do anything? And one of the answer is like doing this generative
adversary or research is like if it's wrong, I can build a tree. I can look at the future and I can
say I want to look at the future and see where I go after like 20 times steps. And then I go to
at the 20 times steps, time step, I'm going to use the this bias value function I learned.
Okay, but the thing is whatever the value of that 20 step is, I'm going to multiply it by
discount factor to power of like 19 or 20 something that. So whatever is going to be the error,
that error is going to be a squash down exponential in depth, which is the thing I really like to have.
Are you using the GAN to project the state only at 20 or is it 2019, 18 all the way down to one
and they're discounting each of them? I literally build the whole tree and then scan all of them.
Okay, and so it's other word, I literally build the model of the world. I literally build the
NDP. If there exists NDP, I literally build the Markov decision process there.
Right. So another way to think about this is like, you're basically trying to do what AlphaGo
did with Go, but for a game like Go, you've got like fixed positions and fixed states. And while
there are a lot of them and we can't enumerate them efficiently, we know what they are given a
particular state of the game now and a particular move and we can enumerate a tree.
For an Atari game, they're more continuous. They're not like these very discrete moves and
positions. So how do we, how might we apply this idea of getting the tree at a bunch of future
states? Well, we can use GANs to do that. Okay, there are three points I need to make it.
First of all, first of all is AlphaGo does this tree search on the board game, right?
And AlphaGo agent has the board game, has the model. But for Atari, I do not have the model.
Yep. So I need to actually learn the model. So there's one difference from like this GANs
and AlphaGo has actually the model of the environment. But for GANs, I need to learn the model
of the environment. Second is for Monte Carlo tree search, in order to work, you do not need to have
discrete like state space. They also work for continuous like state space. So the GAN tea
like the holds before the continuous state space, it doesn't need that. But the thing which makes
it easy for Atari again is like state space, hypothetically it's like continuous, but the transition
is deterministic somehow. It's like if I play sequence, if I choose action left, I go to left,
I'm not jumping somewhere else. So it's like, if we assume that this is a
hypothesis, this hypothesis is that for Atari games, the state of space is continuous,
but still the transition is deterministic. It's like going from one state to another,
I mean, condition of current state and current action, I know which state I'm going to end up.
So this is, it makes the life of the GANs easier, but the GANs is also able to handle the
stochastic state transition. If I am at the state at some frame, if I choose action one action,
if there is distribution of going to next state, GANs is able to handle that issue as well,
but generally Monte Carlo tree search or opera confidence bound tree search, these algorithms,
they do not require neither deterministic state transition nor finite state space. So they
are able to handle continuous one as well. And so one thought that occurs to me is both the
question as well as thinking through the implications of the question. You know, this process of
generating this tree using GANs, what are the computational implications of this? And
you know, assuming they're or imagining that they're significant, you know, I wonder if
maybe it makes sense to do this like to bootstrap the deep reinforcement learning algorithm,
but then once we gain more confidence, switch to something that's more like the approach we
talked about before, you know, the Bayesian deep q networks or something like that.
Sweet. You brought an interesting question. It's like, how how hard is to do this
Monte Carlo tree search? How bad is the competition cost of doing this? The competition cost is bad.
It makes the algorithm slow. Yeah, okay. Yeah. And but if I can get order of magnetic
to sample complexity, then if I'm going to have a self-driving car, I would more compute to be
able to not kill anyone. Okay. So here, the this GANs paper, that GANs work adds like a lot of
competition cost. Like I think makes it like five times more compared to normal DQN,
normal deep q network. But the thing is it's supposed to give you first of all better
performance, second of all, like better sample complexity. Okay. So the general thing is we use
Atari GANs as a test bed in order to see what our algorithms are doing. We are not using Atari
GANs, some people they do, but I personally do not use Atari GAN, Atari GANs as a competition
to say, hey, my numbers are better than yours. Or like my agent gets better scores than
your agent. So mine is better. I'm not using like Atari GANs as to do this type of research,
which is like probably you might not even call it research. I use so I try to develop algorithms
and test them on Atari GANs to see what is their behavior. Yeah, sure. Generative
adversaries research are really bad in the sense of competition costs, but they are supposed to
give you better sample complexity and also better estimation of the value function.
Can you define maybe formally sample complexity for me, because for you to say that
something takes five times as long, but has better sample complexity to me,
it sounds like a contradiction in terms. Oh, I see. So in reinforcement learning,
there are like many components that you might be interested. One is like how much
like wall clock hours you need to spend to find the good policy. So in this study, you don't care
how many times you interact with the environment. You can have form of CPUs and GPUs,
all of them are playing games and like they just give all the feedback to you and you come
with a better algorithm. So if you just care about wall clock time, you don't care about
the sample complexity. You just want to get to high performance in less number of hours
of your wall clock. This study is about just engineering study, it's just about computation.
So how much time you want to spend per day to be able to learn a better policy. This is an
interesting line of study, but it's not my specialty and I'm not focusing on this one. There is another
one is, so now there's another one is like says what objective function for DRL algorithms,
I can define such that I get the best policy and also get the best, more reasonable policy
and also reasonable value estimation. So there's another line of study that people have done a lot
of research on it to come up with a better cost function or objective function. TQN uses the
most naive objective function because the goal of deep-tune network paper was like hey,
to show that it's feasible and it's possible to ask for human or be able to run something on
a target. So there's not a line of research tries to come with a better objective function in order to
estimate better policy and also estimate better value function. Even in this case, you don't care
about the number of, in this setting, you don't care about, in the extreme case of this setting,
you don't care about computation cost and also you don't care that much about the sample complexity.
You are like, the people run the algorithms for like billions of time steps, okay? But there's
another third line which is the most interesting part for me is like, can I get the same performance
in stuff in one billion times of interaction with environment in like one million time steps?
So one million times, so this is like literally I'm gonna, so if I have two kids, they play in the
Atari's and they are playing that, that game, I just see which one is learning faster, but learning
faster means that in the amount of hours they put to play game. If one kid is playing that game
like like from 18 a morning to 9 at night and then does better than the other kid which plays
it one hour per day, then I just, the way I compare the smallest of these two kids somehow might
be like how many hours they play exactly the game, how many hours they interact with them with
the game, right? How many times they press up and down? This is going to be the notion of something
type of complexity. How many times my RL agent like interacted with environment in order to get to
the optimal policy? So in other words, in this method you may take five times as long to figure
out each step on a wall clock, but ultimately you're able to converge to a better policy
taking less steps. Less steps in the game. In the game, right? Because actually the main goal of
reinforcement learning is like when I deploy reinforcement learning for like autonomous vehicles,
I would rather to use the whole AWS like compute to do not give anyone, right? So I'm in one
content, it's for sure a complete compute, but main content is like developing a better algorithm
which has less number of samples required to come with the optimal behavior. So this is like
notion of sample complexity for this setting. I'm not using the exact theoretical notion because
the exact theoretical notion has another meaning, but here somehow it's like how many samples
I need to, if deep QNetwork solves a game in 200 minutes samples, I mean, get some performance
in 200 minutes samples, am I able to reach the same performance in less than one million time
steps and less than one million samples? The answer in the Bayesian deep QNet for paper is like
custom games yet. You can reduce the number of interaction you need to make with the environment
100 times less in order to get to the same performance as deep QNetwork, getting after 200
minutes samples. This is a notion of sample complexity and this is the main part of theory that
I'm also seeing in reinforcement learning in theory land that we mainly care about to reduce
the number of samples we need in order to get to the nice and useful and reasonable performance.
And so along the same lines, can you directly compare the sample efficiency of, you said,
you know, deep Q might be 2 billion, Bayesian, you might be 1 billion. If you throw in the GANs or
GATs approach, can you compare it directly in that same kind of metric? Yeah, of course, I can
definitely compare it in the same metric. I have, like, I have compared it in just for one game
in the paper, but as I said, seems like the composition complexity of GATs is a bit high,
it's a bit beyond the power of academic research labs to do massive experiments. So if I was a
deep mind, I would definitely have all the game. But since I am, like, using my advisors,
AWS credit, probably I rather not burn the whole, the GPU clusters we have just for this
board. So I just try to show a few games. Before those few games, like, how did it compare to the
other couple of methods? Compared to deep Q network, for that game we tried, it reduced the sample
complexity by half, like it converts to the same performance is like half of the samples required
for deep Q network. But in order to make a empirical claim, definitely we need to try more than
for sure one game. Right. So we are trying to come up with more experiments for more games
and to come up with better analysis, better empirical analysis. We are not going to compare against
EDQM or Bayesian deep Q network because the gap is exploring deep Q network inside of it.
So because I have deep Q network and I'm building something on the top of deep Q network,
I'm going to compare against deep Q network. I could use Bayesian deep Q network and use
Gats on the top of that which is totally like this one line change in the code. I could be in the Gats
on the top Bayesian deep Q network and compare against Bayesian deep Q network. This comparison is
meaningful. But if I compare Gats with deep Q network and compare it with the Bayesian one,
it's not that reasonable. The comparison is because we are comparing to somehow orthogonal
effects. You gave the example. You said that like for a given game, you would expect that
the Bayesian deep Q networks is roughly half the number of samples as regular deep Q networks.
No, no, it's like 100 times some games better. So 100 X better. And so it was this that is the Gats
that was half. Yeah, for one game. So for one, right, for the one game that you
but I'm just saying something that Bayesian deep Q network just we studied to come with a
better exploration strategy for deep reinforcement learning problem. At Gats, we are coming,
we came up with the algorithm to reduce the error and bias in the learned value function. So
these are kind of orthogonal effects. So the thing is I'm not going to compare these two together
because these are two studies, not two orthogonal things. And I personally don't care that much
which one does better because both of them can become blind together and come with new algorithm.
Yeah, these are like we are in deep reinforcement learning. We are like far from claiming that we
can use them in real world. So what we do, we are now going to hold the like majority of the
fields. You are trying to study and come up with better and better algorithms. And these are
algorithms are like studying because reinforcement learning like M4 is too big. And we need to do a
lot of study to come up with to study different effects of different components in the setting.
So we are not competing like if like in the vision community, we have ImageNet. Everyone
tries on ImageNet and compare the results of one algorithm to another one. But here we are not doing,
I mean many people they do compare their numbers but I personally do not do compare my numbers to say
this is better than the other. We are just studying the effects of different components in the
environment. And we are in the baby stage of like reinforcement learning. We are like getting
things to see what what what each change would do. We have very very far from to get to real
competition to say these algorithms better than the other ones. So in other words, you know,
it's just kind of exploring ideas about you know what levers we even have available to us to tweak
if we cared about maximizing performance. But we're so early on there's really nothing to maximize
performance against. There's no ImageNet or something that we're you know someone who's building
a self-driving car, you know, they might want to take all of these different approaches and
you know, apply them together. But that's a lot of work and that's their issue, right?
Yeah, they're going to combine everything together. You can imagine like like coming up with
better like exploration and strategy like console sampling. It's obvious that someone should have
tried this one like five years ago, right? But no one has tried it because we are like very very
like this is a fundamental component in the reinforcement learning algorithms. Like the exploration
is like one of the most important and actually I can say even main component of the reinforcement
learning algorithms. But no one has tried this one. Why? Because we are in the early stages. We are
still don't know and don't get to know like what are the fundamental components of the algorithm
is. Like for ImageNet, we know mainly that we need to have this convolution layers. We need to have
like softmax outputs. We can use cross entropy loss. These are the common thing that everyone uses
and it works very well. But in reinforcement learning, we don't have this all components
all together. We don't know what is the best way of doing exploration and exploration together.
We don't know what is the best function approximation we should use. We don't know what is the best
memory we should use. We don't know what we don't expect. We are like really really far from
making. So we're just reading that even before the baby stage, we are not even got born. We were not
born. Awesome. Well, Camyor, you've been very, very generous with your time. Thanks so much for
taking the time to chat with you about all this stuff. It was a lot of fun and I think folks will
enjoy this and learn a lot. I really appreciate it. Awesome. Thank you. Oh, thank you. Have a great day.
All right, everyone. That's our show for today. For more information on Camyor or any of the
topics covered in this episode, head over to twimmolai.com slash talk slash 177.
If you're a fan of the podcast, we'd like to encourage you to visit your Apple or Google
podcast app and leave us a five star rating and review. Your reviews help inspire us to create
more and better content and they help new listeners find the show. As always, thanks so much for listening
and catch you next time.
