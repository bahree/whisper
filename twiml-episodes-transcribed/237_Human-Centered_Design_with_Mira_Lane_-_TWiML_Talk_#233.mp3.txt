Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we're excited to present the final episode in our AI
for the Benefit of Society series, in which we're joined by Mira Lane, partner director
for Ethics and Society at Microsoft. Mira and I focus our conversation on the role of
culture and human centered design in AI. We discuss how Mira defines human centered
design, its connections to culture and responsible innovation and how these ideas can be scalably
implemented across large engineering organizations.
Before diving in, I'd like to thank Microsoft once again for their sponsorship of this series.
Microsoft is committed to ensuring the responsible development and use of AI and is empowering
people around the world with this intelligent technology to help solve previously intractable
societal challenges, spanning sustainability, accessibility, and humanitarian action.
Learn more about their plan at Microsoft.ai. Enjoy.
Alright everyone, I am here with Mira Lane. Mira is the partner director of Ethics
and Society at Microsoft. Mira, welcome to this weekend machine learning and AI.
Thank you Sam, nice to meet you. Great to meet you and I'm excited to dive into this
conversation with you. I saw that you are a video artist and technologist by background.
How did you come to your looking away? Is that correct?
No, that's absolutely true.
So I noted that you're a video artist. How did you come to work at the intersection of
Ethics and Society and AI?
For sure. So let me say let me give you a little bit of a background on how I got to this
point. I actually have a mathematics and computer science background from the University
of Waterloo in Canada and so I've had an interesting journey and I've been a developer, a program
manager and designer. And when I think about video art and artificial intelligence, I'll
touch artificial intelligence first and then the video art. But a few years ago I had the
opportunity to take a sabbatical and I do this every few years. I take a little break,
reflect on what I'm doing, retool myself as well.
So I decided to spend three months just doing art. A lot of people take a sabbatical and
they travel but I thought I'm just going to do art for three months and it was luxurious
and very special. But then I also thought I'm going to reflect on career at the same
time. And I was looking at what was happening in a technology space and feeling really unsettled
about where technology was going, how people were talking about it, the way I was seeing
it affect our societies and I thought I want to get deeper into the AI space. And so when
I came back to Microsoft I started poking around the company and said is there a role
in artificial intelligence somewhere in the company and something opened up for me in
our AI and research group where they were looking for a design manager. So I said absolutely
I'll run one of these groups for you. But before I take the role I'm demanding that we
have an ethics component to this work because what they were doing was they were taking
research that was in the AI space and figuring out how do we productize this because at
that point research was getting so close to engineering that we were developing new techniques
and you were actually able to take those to market fairly quickly. And I thought this
is a point where we can start thinking about responsible innovation and let's make that
formalized practice. So me taking the role for the design manager was contingent on us
creating a spot for ethics at the same time. And so backing up a little bit the video
part comes in because I've traditionally been a really analog artist, been a printmaker,
a painter and during my sabbatical I saw some more digitized, like looked at digitizing
some of the techniques that I was playing with on the analog side. I thought well let me
go play in the video space for a while. And so for three months I just like I said I retooled
and I started playing around with different ways of recording, editing and teaching myself
some of these techniques. And one of the goals I set out at the time was well can I get into
a festival you know can I get into a music or a video festival. And so that was one of my goals
at the end of the three months. Can I produce something interesting enough to get admitted
into a festival. And I want a few actually. So so I was super pleased. I'm like okay well that
means I'm I've got something there I need to continue practicing. But that for me opened up a
whole new door. And and one of the things that I did a few years ago also was to explore art
and with AI. And and could we create a little AI system that could mimic my artwork and become
a little co-collaborator with myself. So we can dig into that if you want. But it was a really
interesting journey around can AI actually complement an artist or even replace an artist.
And and so I there's interesting learnings that came out of that experience. Okay interesting
interesting. We're accumulating a nice list of things to to touch on here. Absolutely. Ethics
and your views on that was at the top of my list. But before we got started you mentioned
work that you've been doing exploring culture and the intersection between culture and AI.
I'm curious what that means for you. It's certainly a topic that I hear brought up quite a bit
particularly when I'm talking to folks in enterprises that are trying to adopt AI
technologies. And here all the time oh well one of the biggest things we struggle with is culture.
And so maybe I don't know if that's the right place to start but maybe we'll start there.
What does that mean for you when you think about kind of culture and AI? Yeah no that's a really
good question and I agree that one of the biggest things is culture. And the reason why I say that
is if you look at every computer scientist that's graduating none of us have taken an ethics class
and you look at the impact of our work it is touching the fabric of our society like it is
touching our democracies and our freedoms our civil liberties and those are powerful tools
that we're building yet none of us have gone through an formal ethics course. And so the discipline
is not used to talking about this it's you know a few years ago you're just like oh I'm just
building a tool I'm building an app I'm building a platform that people are using. And we weren't
super introspective about that it wasn't part of the culture. And so when I think about culture
in the AI space because we're building technologies that have scale and power and are building on
top of large amounts of data that empower people to do pretty impressive things this whole question
of culture and asking ourselves well what could go wrong how could this be used who is going to
use it you know directly or indirectly and those are parts of the culture of technology that
I don't think has been formalized it's usually here designers talking about that kind of thing
it's part of humans and our design but even in the humans and our design space it's really about
like what is my ideal user or my ideal customer and not thinking about well how could we exploit
this technology in a way that we hadn't really intended and and we've talked about that from an
engineering context the way we do you know threat modeling how could a system be attacked how do
you think about denial of service attacks things like that but we don't talk about it from how
could you use this to harm communities how could you use this to harm individuals or how could
this be inadvertently harmful and so those parts of cultures are things that we're grappling right
with right now and you know we're introducing into our engineering context so my group sits at
an engineering level and we're trying to introduce this new framework around responsible innovation
and there's five big components to that one is being able to anticipate look ahead anticipate
different futures look around corners and try to see where the technology might go how someone
could take it insert it into larger systems how you can do things at scale that are powerful
that you may not intend to do there's a whole component around that this you know responsible
innovation that is around reflection and looking at yourselves and saying well where do we have
biases or where we assuming things what are our motivations can we have an honest conversation
about our motivations why are we doing this and can we ask those questions how do we create the
space for that we've been talking about you know diversity and inclusion like how do you bring
diverse voices into the space especially people that would really object to what you're doing and
how do you celebrate that versus tolerate that there's a big component around just like our
principles and values and how do you create with intention and and how do you ensure that they
align with the principles and they align with their values and they're still trustworthy so
there's a whole framework around how we're thinking about innovation in the space and at the end
of the day it comes down to what you're like the culture of the organization that you're building
because if you can't operate at scale then you end up only having small pockets of us that are
talking about this versus how do we get every engineer to ask what's this going to be used for
and who's going to use it or what if this could happen and we need people to start asking those
types of questions and then start talking about how do we architect things in a way that's
responsible but I'd say like most engineers probably don't ask those types of questions right
now and so we're trying to build that into the culture of how we design and develop new technologies.
One of the things that I often find frustrating about this conversation particularly when talking
to technology vendors is this kind of default answer while we just make the guns we don't shoot
them right we just make the technologies you know it's they're you know they can be used for good
they can also be used for bad but we're focused on you know the the good as aspects it sounds
like yeah maybe you while I'm curious how do you articulate you know your responsibility
with the tools that you're creating or Microsoft's responsibility with the tools that's creating
do you have a well I have a very similar reaction to you when when I hear oh we're just making
tools I think well fine that's one perspective but the responsible perspective is we're making tools
and we understand that they can be used in these ways and we've architected them so that they
cannot be misused and we know that there will be people that misuse them so I think
and you're hearing a lot of this in the technology space and you know there's every year there's
more and more of it where people are saying look we have to be responsible we have to be accountable
and and so I think we'll hear fewer and fewer people saying what you're hearing what I'm hearing as
well but one of the things we have to do is we have to avoid the ideal path and just talking only
about the ideal path because it's really easy to just say here's the great ways that this technology
was going to be used and not even talk about the other side because then again we fall into that
pattern of well we only thought about it from this one perspective and so one of the things that
my group is trying to do is to make it okay to talk about here's how it could go wrong
so that it becomes part of our you know daily habit and and we do it at various levels you know we
do it at our all hands so when people are showing our technology we have them show the dark side
of it at the same time so that we can talk about that in an open space and it becomes okay to talk
about it no one wants to share the bad side of technology right no one no one wants to do that
but if we make it okay to talk about it then we can start talking about how do we prevent that
so we do that at like you know larger forums and then you know this I know this is a podcast
but I wanted to show you something so I'll talk about it but we created it's almost like a game
but it's it's a way for us to look at different stakeholders and perspectives and what could happen
and so how do we create a safe environment where you can look at one of our ethical principles
you can look at a stakeholder that is interacting with the system and then you say well if the
stakeholders you know for example it was a woman in a car and your system is a voice recognition
system what would she say if she gave it a one-star review she would probably say I had to yell
a lot and didn't recognize me because we know that most of our systems are not tuned to be diverse
right and so we start creating this environment for us to talk about these types of things
so that it becomes okay again really how do we create safe spaces and then as we develop our scenarios
how do we bring those up and then track them and say well how do we fix it now we've excavated
these issues well let's fix it and let's talk about it so that's again part of culture like how
do we make it okay to bring up the bad parts of things right so it's not just the ideal path do
you run into or run up against engineers or executives that say you know introspection safe spaces
you know granola you know what about the bottom line what does this mean for you know us as a
business how do we you know think about this from a shareholder perspective you know it's it's
interesting I I don't actually hear a lot of that push back because I think you know internally
at Microsoft there is this recognition of who we want to be really thoughtful and intentional
and and I think the bigger issue that we hear is just like how do we do it it's not that we don't
want to it's well how do we do it and how do we do it at scale and so what are the different things
you can put in place to help people bring this into their practice and so you know there isn't a
pushback around well this is going to like it's going to affect my bottom it's going to affect
my bottom line but there's more of a understanding that yeah if we build things that are thoughtfully
designed and intentional and ethical that it's better for our customers I mean our customers want
that too but then again the question is well how do we do it and where is it manifest so there's
things that we're doing in that space I mean when you look at AI a big part of it is data so how do
you look at the data that's being used to power some of these systems and say is this the diverse
data set is this well-rounded do we have gaps here what's the bias in here and so we start looking
at certain components of our systems and helping to again architect it in a way that's that's
better I think all of our customers would want a system that recognized all voices right and
because again to them they wouldn't want a system that just worked for men it didn't work for
women so again it's like better product as a result and so if we can couch it in terms of
better product then I think it makes sense versus if it's all about us philosophizing and only
doing that I don't know if that's the best you know only doing that is not productive right do you
find that the uncertainty around ethical issues related to AI has been an impediment to customers
adopting it does that get in the way do they do they need these issues to be figured out before
they dive in I don't think it's getting in the way but I think it's what I'm hearing from customers
is help us think about these issues and you know a lot of people a lot of customers don't
understand AI deeply right it's it's a complex space and a lot of people are ramping up in it
and so the question is more about well what should I be aware of what are the questions
that I should be asking and how can we do this together we know you guys are thinking about
this deeply we're getting just involved in it you know a customer might say and so they
it's more about how do we educate each other and for us if we want to understand like how do you
want to use this because sometimes we don't always know the use case for the customer so we want
to deeply understand that to make sure that what we're building actually works for what they are
trying to do and from their perspective they want to understand well how does this technology work
and where will it fail and where will it not work for my customers and so the question of ethics
is more about we don't understand the space well enough help us understand it and we are concerned
about what it could do and can we work together on that so it's it's not preventing them from
adopting it but there's there's definitely a lot of dialogue it comes up quite a bit around
well we've heard this we've heard bias is an issue what does that mean right and so we and so I
think that's an education opportunity when you think about ethics from a technology innovation
perspective are there examples of you know things that you've seen either that Microsoft is doing
or out in the the broader role that strike you as innovative approaches to this problem yeah you
know I'll go back to the data side of things just briefly but there's this concept called
data sheets which I think is super interesting yeah you're probably really familiar with that
and I've written about some of the work that Timnick Gebru and some others with Microsoft have
done around data sheets for data sets exactly and the the interesting part for us is well how do
you put it into the platform how do you bake that in and and so what one of the pieces of work
that we're doing is we're taking this notion of data sheets and we are applying it into how we
are collecting data and how we're building out our platform and so I think that that's I don't
know if it's super novel because it to me it's like a nutrition label for your data like you
won't understand how is it collected what's in it how can you use it but but I think that that's
one where now as people leave the group you know you want to make sure that there's some history
and understanding the composition of it there's some regulation around how we manage it internally
and how we manage data in a thoughtful way I think that's just a really interesting concept that
we should be talking about more as an industry and then can we share data between each other in a
way that's responsible as well yeah yeah I don't know that the the data sheet I mean I think inherent
to the idea was the hey this isn't novel in fact look at you know electrical components and all
these other industries that do this it's just common sense quote unquote yeah but what is a little
novel I think is actually doing it so since that paper was published several companies have
published kind of similar takes model cards and there there have been a handful and every time
I hear about them I ask okay so one is this you know what are you going to be publishing these
for your services and the data sets that you're publishing and no one's done it yet so
it's intriguing to hear you say that you're at least starting to think in this way internally
do you have a sense for what the you know the path to publishing these kinds of you know whether
it's a data sheet or a card or some kind of set of parameters around and bias either in a
data set or a model you know for a commercial public service yeah absolutely we're actually
looking at doing this for facial recognition and we've publicly commented about that we've said
hey we're going to be sharing for our services what they're what it's great for and what it's not
where it's and so that stuff is actually actively being worked on right now you'll probably see
more of this in the next few weeks but but there is public comment that's going to come out
with more details about it and I'll say that you know on the data sheet side I think a large
portion of it is it needs to get implemented in the engineering systems first and you need to find
the right place to put it and so that's that's the stuff that we're working on actively right now
um can you comment more on that I it does as you say that it does strike me a little bit as one
of these iceberg kind of problems like it you know it you know looks very manageable kind of above
the water line but if you think about what goes into the creation of a data set or a model there's
a lot of complexity and certainly at you know the scale of Microsoft is working at it needs to be
automated what are some of the challenges that have come into play and in trying to implement
an idea like that well um let me think about this for a second so I can frame it at the right way
the biggest challenge for us on something like that is um really thinking through
the data collection effort first and spending a little bit time there that's where we're actually
spending quite a bit of time as we look at um so let me back up for a second I I work in an
engineering group that touches all the speech language vision technologies and we do an enormous
amount of data collection to power those technologies one of the things that we're first spending
time on is looking at exactly how we're collecting data and going through those methodologies and
saying is this the right way that we should be doing this we want to change it in any way do we
want to optimize it and then we want to go and apply that back in so you're right this is a big
iceberg because there's so many pieces that are connected to it and the spec for data sheets and
the ones we've seen are large and um and so what we've done is how do we grab the core pieces of
this and implement and create the starting point for it and then scale over time adversioning
being able to add your own custom schemas to it and scale over time but what is like the minimum
piece that we can put into the system and then make sure that it's working the way we want it to
and so it's just it's about decomposing the problem and saying which ones do we want to prioritize
first um for us we're spending a lot of time just looking at the data collection
methodologies first because there's so much of that going on and at the same time what is the
minimum part of the data sheet spec that we want to go and put in and then let's start iterating
together on that it strikes me that these will be most useful when there's kind of broad
industry adoption or at least coalescence around some you know standard whether it's a standard
minimum that everyone's doing and potentially growing over time or you involved in or where
of any efforts to create something like that well I think that that's um that's one piece where
it's important I would say also in a large corporation it's important internally as well because
we work with so many different teams um and we're interfacing with the you know we're a platform
where we interface with large parts of our organization and um and being able to share that
information internally that is a really important piece to the puzzle as well I think the external
part is as well but the internal one is not um not any less important in my eyes because that's
where we are we want to make sure that if we have a set of data that this you know group A is
using it in one way if group B wants to use it we want to make sure that they have the rights
to use it they understand what it's composed of where its orientation is and um and so that if
they pick it up they do it with full knowledge of what's in it so um for us internally it's a
really big deal externally um I've heard pockets of this but I don't think I could really comment
on that yet with you know like full authority I'm really curious about the intersection between
ethics and design and um you mentioned human centered design earlier my sense is that that that
phrase kind of captures a lot of that intersection can you elaborate on what that means for you
yeah yeah um so when you look at traditional design functions when we talk about human centered
design there is there's lots of different humans in our design frameworks the one I typically
pick up is um Don Norman's you know emotional design framework where he talks about behavioral
design reflective design and visceral design and um and so behavior is you know how is something
functioning what is the functionality of it um reflective is how does it make you feel about
yourself you know how does it play to your ego and your personality and um visceral is you know
the look and feel of that that's a very um individual oriented approach to design and when I think
about these large systems you actually need to bring in the ecosystem into that so how does this
object you're creating or this system you're creating how does it fit into the ecosystem
and so one of the things we've been playing around with is we've actually reached into adjacent
areas like agriculture and explore like how do you do sustainable agriculture what are
that some of those principles and methodologies and how do you apply that into our space so a lot
of the conversations we're having is around ecosystems and how do you insert something into the
ecosystem and what happens to it what is the ripple effect of that and then how do you do that in a
way that keeps that whole thing sustainable for so it's not um it's a good solution versus one
that's bad and um and causes other downstream effects so I think that those are changes that we
have to have in our design methodology we're looking we're looking away from the one artifact and
thinking about it from a you know here's how the one user is going to work with it versus how is
the society and going to interact with it how are different communities going to interact with
it and what does that do to that community um it's a larger problem and so there's like this shift
in design thinking that we're trying to do with our designers so they're not just doing UI
they're not just thinking about this one system they're thinking about it holistically um and there
isn't a framework that we can easily pick up so we have to kind of construct one as we are going
along yeah yeah for a while a couple of years ago maybe I was having I was in search of that framework
and I think the the motivation was just really early experiences of seeing kind of AI shoved
into products in ways that were frustrating or annoying like for example a nest thermostat like
you it's intended to be very simple but it's making these decisions for you in a way that you
can't really control and it's starting me down this path of you know what does it mean really
build out uh you know a discipline of design that is aware of AI and intelligence I've
jumped on the podcast before that you know I I call it intelligent design but that's overloaded
term total is but is there a term for that now or people thinking about that how far have we
come in building out um you know you know a discipline or a way of thinking of what it means to
build intelligence into products yeah um we have done a lot of work around education for our designers
because uh we found a big gap between what our engineers were doing and talking about and what
our designers had awareness over so we actually created a deep learning for designers workshop it
was a two-day workshop and it was really intensive so we took um you know neural nads
convolutions like all these concepts and we introduced them to designers in a way that designers
would understand it um we you know brought it to here's how you think about it in terms of
Photoshop here's how you think about it in terms of the tools you're using and the words you use
there here's how we'd apply here's a exercise where people had to get out of out of their seas and
we created this really simple neural net with human beings and um and we had them coding as well
and so they were coding in Python and um in notebooks and so they were uh they were exposed to it
and and we exposed them to a lot of the techniques and terminology in a way that was concrete
and they were able to then say oh this is what style transfer looks like oh this is what
this is how we constructed a bot and so um first on the design side I think having the vocabulary
to be able to say oh I know what this word means not just I know what it means but I've experienced
it so then I can have a meaningful discussion with my engineer I think that that that was an
important piece and then understanding how AI systems are just different from regular systems
they are more probabilistic in nature the defaults matter they are they can be self-learning
and so how do we think about these and starting to showcase case studies with our designers to
understand that these types of systems are quite different from the deterministic type of systems
that you may have designed for in the past um again I think it comes back to culture because
it was there's a and we keep doing these workshops every quarter we'll do another one because
we have so much demand for it and we found even engineers and PMs would come to our design workshops
but um kind of democratizing the terminology a little bit and making it concrete to people
was an is an important part of this it's interesting to think about
what it does to a designer's design process to have more intimate knowledge of these concepts
at the same time a lot of the questions that come to mind for me are you know much higher level
concepts in the in the domain of design for example you can we talk about user experience
to what degree should a user experience AI if that makes any sense should we be trying to make AI
or or you know this notion of intelligence invisible to users or very visible to users um you know
this has come up you know recently in for example I'm thinking of like Google Duplex when they announced
that that system was going to be making phone calls to people and there was a big
kerfuffle about whether that should be disclosed um yeah and I don't know that there's a right answer
I get in some ways you want some of this stuff to be invisible in other ways you know time
back to the whole ethics conversation it does make sense that you know there's some degree of
disclosure yeah absolutely I imagine as a designer this notion of disclosure is can be a very
nuanced thing what does that even mean yeah yeah and it's all context dependent and it's all
norm dependent as well because if you were to look into the future and say are people more comfortable
like I mean look at airports for example people are walking through just using face ID using the
clear system and a few years ago I think if you ask people would you feel comfortable doing that
most people would say no I don't feel comfortable doing that I don't want that um and so I think in
this space because it's really fluid and new norms are being established and things are being
tested out uh we have to be on top of how people are feeling and thinking about these technologies
where so that we understand where some disclosure needs to happen and where things don't and um
and in a lot of cases you almost want to assume disclosure for things that are very consequential
in high stakes um where there is opportunity for deception in the duplex case you have to be
thoughtful about that um and so it's this isn't one where you can say okay you should always
disclose it just depends on the context and so we have this notion of consequential scenarios
where things are you know if there's automated decision making if there are scenarios where there
is um there are high stakes scenarios those ones we think about um in a we just put a little bit
more due diligence over those and start to be more thoughtful about those and then we have you
know other types of scenarios which are um more systems oriented and here's some things that are
operationally oriented and they end up having different types of scenarios but we haven't been
able to create uh here's the exact way you do every single you know you approach it in every single
way so it is so super context dependent and expectation dependent um maybe after a while you get
used to your nest thermostat and you're fine with the way it's operating right and so um so I
don't know these social norms are interesting because they are someone will go and establish
something or they'll test the waters you know google glass tested the waters and um that was
a cultural response right people responded and said I don't want to be surveilled I don't want
I want to be able to go to a bar and get a drink and I'll have someone recording me and um and so
I think we have to understand where society is relative to what the technologies are that we're
inserting into them and so again it comes back to are we listening to users are we just putting
tech out there I think we have to really start listening to users um my group has a fairly large
research component to it and we spend a lot of time talking to people especially in the places
where we're going to be putting some tech and I'm understanding what it's going to do to the dynamic
and how they're how they're reacting to it yeah it strikes me that uh and maybe it's kind of
the engineer background in me this looking for like a framework uh you know a flow chart for how
we can approach this problem and uh I need to embrace more of the designer that's like well every
you know product every situation is different and it's more about a principled approach as opposed
to a process absolutely it's more about a principled and intentional approach so what what we're
talking about is everything that you're choosing are you intentional about that choice and are
you very thoughtful about things like defaults because we know that people don't change them
and so how do you think about every single design choice in being principled and in very
intense intentional and evidence driven and so we push this on our teams and I think some of our
teams maybe don't enjoy being with us sometimes as a result but we say look we're going to give you
some recommendations that are going to be principled intentional and evidence driven and we want to
hear back from you if you don't agree on your evidence and why you're saying this is a good or
bad idea um and that's that's the way you have to operate right now because it is so context-driven
I wonder if you can talk through some examples of how you know humans enter design and all these
things come together in the context of kind of concrete problems that you've looked at yeah I
was thinking about this because a lot of the work that we do is um fairly confidential but
there's one that I can touch on which was shared at build earlier this year and that was a meeting
room device and I don't know if you remember this but there's a meeting room device that we're
working on that um recognizes who's in the room and um does transcription of that meeting
and um it to me as someone who is a manager I love the idea of having a room a device in the room
the captures action items and who was here and what was said and uh and so we started looking at
this and we said okay well let's look at different types of meetings and people and let's look at
categories of people that um this might affect differently and so how do you think about
introverts in a meeting how do you think about women and minorities because there are subtle
dynamics that are happening in meetings that um make some of these relationships they can
um reinforce certain types of stereotypes or relationships and so we started um interviewing
people in the context of this sort of meeting room device and um and this is research that is
pretty well uh it's well recognized it's not um it's not novel research but but um it reinforced
the fact that when you start putting in things that will monitor anyone that's in a room certain
categories of people behave differently and you see larger discrepancies um and impact
with women minorities more junior people and so we said wow this is really interesting because
as soon as you put a recording device in a room it's going to subtly shift the dynamic where some
people might talk less or some people might feel like they're observed or depending on if there's a
manager in the room and there's a device in a room they're going to behave differently
and does that result in a good meeting or a bad one we're not sure but that will affect the dynamic
and so then we took a lot of this research and we went back to the product team and said well how do
we now design this in such a way that we design with privacy first in mind and um make users
feel like they're empowered to opt into it and so we've had discussions like that especially around
these types of um devices where we've seen big impact to how people behave but it's not like a hard
guideline or it's not really a hard set of rules around what you have to do but you know because
all meetings are different right you've brainstorming ones that are more of a fluid ideas you don't
really care who said what it's about getting all the ideas out you have ones where you're shipping
something important and you want to know who said what because they're clear action items that go
with them and so um trying to create a system that works with so many different nuanced
conversations and um different scenarios is not an easy one so what we do is we we all run
alongside with the product team and while they're engineering you know they're developing their work
we will take the research where that we've gathered and we'll create alternatives for them at
the same time so that we can run alongside with them we can say hey here's option A, B, C, D, and E
let's play with these and maybe we come up with a version that mixes them all together
but um but it gives them options to think about because again it comes back to well I might not
have time to think about all of this so how do we empower people with ideas and um and concrete
things to to look at yeah I think that examples a great example of the complexity or um maybe
complexity is not the right word but the the idea that your initial reaction might be like the
exact opposite of what you need to do yeah as you were saying this I was like oh just hide the
thing so no one knows it's there it doesn't change the dynamic it's like that's exactly wrong
you don't want to do that don't hide it right right yeah and maybe that's another piece I'm sorry
interrupt that but but one of the things I've noticed is the our initial reaction is often wrong
and so how do we hold it at the same time that we give ourselves a space to explore other things
and then keep an open mind and say okay I have to adjust and change because hiding it would
absolutely be an interesting option but then you have so many issues with red right um but again
like it is about being able to have like an open mindset and be able to challenge yourself in
the space do you have a sense for where you know if we kind of buy into the idea that folks that
are working with AI need to be more thoughtful and more intentional and and maybe incorporate more
this into more this design thinking element to their work do you have a sense for where this you
know does or should or needs to live within a customer organization yeah I think it actually
and this is a terrible answer I think it needs to live everywhere in some ways because
what one thing that we're noticing is we have with corporate level things that happen we have
an you know an ether board it's our it's an advisory board that looks at AI technologies and
advises and that's at a corporate level that's a really interesting way of approaching it
but it can't live alone and so the thing that we have learned is that if we pair it with groups
like mine that sit in the engineering context that are able to translate principles concepts guidelines
into practice that sort of partnership has been really powerful because we can take those principles
and say well here's where it really worked and here's where it kind of didn't work and then we can
also find issues and say well we're grappling with this issue that you guys hadn't thought about
how do you think about this and can we create a broader principle around it so I think that there's
this like strong cycle of feedback that happens if you have something at the corporate level
or you establish just what your values are what are our guidelines and what are our approaches
but then at the engineering context you have a team that can problem solve and apply
and then you can create a really tight feedback loop between that engineering team and your
corporate team so that you're continually reinforcing each other because the worst thing would be
just to have a corporate level thing and just be PR speak right you don't want that right right
and the worst thing would also be just to have it in the engineering level because then you would
have a very distributed mechanism of doing something may not cohesibly ladder up to your
principles and so I think you kind of need both and to have them work off each other to really
have something effective and maybe there's other things as well but so far this has been a really
productive and iterative experiment that we're doing. Danny pointers come to mind for folks that
want to explore this space more deeply do you have a top three favorite resources or
initial directions well it depends what you want to explore so I was reading the AI now report
the other day it's you know a fairly large report 65 page report around the impact
of AI in different systems different industries and so if you're looking at
getting up to speed on well what areas is AI going to impact I would start with some of these
types of groups because I found that they are super thoughtful and how they're going into each
space and understanding each space and then bubbling up some of the scenarios so if you're
thinking about AI from a you know how is it impacting those types of things that are really
interesting on the engineering side I actually spent a lot of time on a few Facebook groups where
they have there's some big AI groups in Facebook and they're always sharing here's the latest here's
what's going on I've tried this technique and so that keeps me kind of up to speed on some of those
that are happening and also archive just to see what research is being published the design side
I'm sort of mixed I mean I haven't really found a strong spot yet I wish I had like something
my back pocket where I could just refer to but the thing that maybe has been on the theory side
that has been super interesting is to go back to a few set a few people that have made commentaries
just around sustainable design so I refer back to Wendell Berry quite a bit the agriculturalist
and poet actually who has really introspected how agriculture could be reframed Ursula Franklin is
also a commentary from Canada she was she did a lot of podcast or radio broadcast a long time ago
and she has a whole series around technology and its societal impact and if you replace a few of
those words and put in some of our new age words it would still hold true and so I think there's
a lot of theory out there but not a lot of like here's really great examples of what you can do
because we're all still feeling out the space and we haven't found a perfect patterns yet that you
can democratize and share out broadly we're very thanks so much for taking the time to chat with us
about this stuff is a really interesting space and one that I enjoy coming back to periodically
and I personally believe that there's you know this intersection of AI and design is one that
is as wide open and should and will be further developed and I'm kind of looking forward to
keeping an eye on it and I appreciate you taking the time to chat with me about it thank you so
much Sam was wonderful talking to you thank you all right everyone that's our show for today
for more information on Mira or any of the topics covered in this show visit twimmelai.com slash
talk slash two three three to follow along with or catch up on our AI for the benefit of society
series visit twimmelai.com slash AI for society as always thanks so much for listening and catch you
next time
