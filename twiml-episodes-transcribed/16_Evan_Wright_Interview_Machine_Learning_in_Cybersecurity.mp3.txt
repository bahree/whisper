Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Thanks so much to everyone who sent in their favorite quote from last week's podcast.
Your stickers are on the way.
We had a bunch of fun with this contest and we've decided to continue it while our sticker
supplies last.
So definitely send us your favorite quote from today's show as well.
You can do that via a comment or post on Facebook, Twitter, YouTube or SoundCloud as well as
the show notes page for any episode of the show.
Okay, and now about today's show.
I've got a very special guest this week.
His name is Evan Wright and he's principal data scientist at Anomaly.
If that name sounds familiar, it's because Evan was the winner of our O'Reilly strata
Hadoop World ticket giveaway earlier this month.
Evan and I met up at the conference last week and got to chat about a number of topics
in the realm of machine learning and cybersecurity.
We discussed the three big problems in cybersecurity that ML can help with.
The challenges of acquiring ground truth and cybersecurity in some ways to do it and
the use of decision trees, generative adversarial networks and other algorithms in that field.
I think you really enjoy this show.
Speaking of cybersecurity, that's just one of the many topics that will be covered in
the future of data summit event that I'm hosting May 15th and 16th at the inter-op conference
in Las Vegas.
At the event, Diana Kelly, a global executive security advisor with IBM, will be talking
about the future of securing cloud, IoT and big data systems.
We'll also be giving you a glimpse into the future of machine learning in AI, so-called
fog or edge computing, augmented and virtual reality, blockchain, algorithmic IT operations
and much more.
You can learn more about the summit at twimmolai.com slash future of data.
And now on to the show.
All right, hey, everyone.
I am here with Evan Wright, Principal Data Scientist with Anomaly.
You may remember Evan's name because we announced him as the winner of our O'Reilly Stratta
Hadoop ticket giveaway.
Evan and I decided to get together and record a little show.
Evan, say hi.
Good morning, everybody.
Good afternoon or good evening as it applies.
Awesome.
Evan, why don't we start by having you talk a little bit about the company you're at
and what you do there?
Sure.
Sure.
And Anomaly, we are focused on threat intelligence.
I think there's really three big problems in cybersecurity that are really ripe for using
machine learning to improve.
One problem is malware detection.
Another problem is threat intelligence, which is really the focus of our work.
And another one is sort of stream detection of when you've got a series of events like
for example, network traffic, trying to sift through that and find the security relevant
concerns out of that stream.
So we're focused on the latter two the most, especially as far as understanding threat
intelligence better.
So threat intelligence is this idea of imagine you're conducting an investigation like if
you're a law enforcement, you would collect pieces of evidence and try and stitch together
the story of what happened.
And if possible, try and stop things, stop bad things happening before they actually
occur.
So something similar is the case in organizations.
They run these security teams and security teams want to know things like of all the things
that our network is talking out to on the internet are some of those malicious, right?
And they want to be able to separate that.
So threat intelligence, fundamentally, is understanding the tools and the infrastructure
out on the internet and in your network to be able to separate those, right?
So very pragmatically, it's something like a list of IPs or domains to start with,
which if your network has activity going to these IPs or domains, then it would be malicious.
So a lot of our main focus at anomaly is we create a threat intelligence platform.
The idea is there's all these security vendors out there which kind of give us this ground
truth.
And in security, ground truth is so, so important.
It is really the fundamental problem and why machine learning is hard in cybersecurity
because ground truth is really expensive.
We're not exactly spoiled in some sense, like having image data, like having video data.
Because in those spaces, you have a very clear sense of what data should I collect and
what data do I care about.
If you look at an image, it's easy to tell if it's a cat or not to have that ground truth
for labeling images.
In cybersecurity, it's much, much harder.
Instead of just being a layman who can look at an image and tell you it's a cat, in the
case of cybersecurity, usually you need a very specialized expert to be able to tell you
if you want really good confidence.
They often need to be even more specialized because the best ground truth comes from analyzing
reverse engineering pieces of software.
So reverse engineering how it works, maybe there's, maybe the software calls out to components
out on the internet that are malicious.
And you need to be able to pull those out.
So all of these indicators are essentially pieces of evidence that we try and put together
and organizations want to alert on and stop or at least investigate if it's happening
in their organization.
So our company provides a stream of this threat intelligence.
And that product is incidentally called threat stream.
Okay.
So it sounds analogous to maybe the kind of signatures that an antivirus type of company
would produce, is that a fair assessment or maybe even another way of asking that.
Like what makes the stream streamy?
So different people can get this ground truth in different ways.
So some of them are, in fact, antivirus companies.
Some of them are organizations that do sort of a forensic investigation on incidents that
happen after the fact.
Some of them are, in fact, applying machine learning given their different data sets.
So everyone has sort of a different strategy, both in their data collection and in how they
validate it.
Some of it is extremely qualitative by hiring cybersecurity experts.
Some of it is extremely data driven and quantitative and pretty much everything you can imagine
in between.
And organizations really struggle from this problem of which one do I pick?
And then if they can decide that, then they still have this issue of how reliable is that
stream.
So one of the things that in our platform we do, our machine learning use case, is to give
people an assessment of how confident are we in this indicator from the stream.
So we're able to, you buy sort of a big database of, this is what all the badness is.
You pay a particular provider.
Then we run our own largely regression based, like ensemble regression, on the input feeds
that they give us.
And then we use that to sort of augment a lot of the incoming feeds, especially when
we go out to the internet, collect free and open source data.
Then we pull back data and how reliable is this particular indicator?
How reliable is this feed in general?
So, so that's where we use a lot of our scoring algorithms to help separate, you know, how
seriously should you take an action on this particular indicator from this feed?
Because there's a drowning in data problem in cybersecurity.
Yes, I'd like to get more concrete on what the specific feeds are.
I know some examples of things that I've seen folks doing are, you could have, you know,
blacklisted IPs, for example, you know, so that may be one, is that an example of a feed
in this case?
Sure, yeah, so you could have, so we scrape a bunch of open source data.
So that's part of it that we just sort of provide with the platform.
Because, you know, intuitively, it kind of makes sense that that needs more vetting, you
know, and so the ML scoring is more important in that role, because if it's just an open source
intelligence feed that comes down, having the scoring is particularly important, whereas
if you're paying for a particular feed, then you intuitively probably have a bit more
confidence in it.
So certainly, you may be tipped off by honeypots, right?
So that's what honeypots are one example of how to collect this data, and that's basically
we put a very vulnerable looking machine on the internet and just try and advertise that
we're vulnerable and get people to attack us.
So we have an open source project called the Modern Honeynet, which is MHN, which is
used to collect a lot of these, no matter what type of honeypot you have, then we can
take this data and sort of centralize it, and it simplifies the data collection piece,
no matter how many different types of honeypots you have.
So that's an example of one of them.
So we've got the kind of the IP, the black listed IPs, like what are some other data
sources that you're collecting, cleaning, providing that feed into, you know, making ultimately
you're trying to make a determination whether, you know, some activity is, you know, likely
to be a threat.
Yeah, absolutely.
And so some organizations may choose to immediately block it.
Other organizations may just want to monitor it and see what happened.
Some organizations get breached and then need to do a retrospective evaluation of how
did this get in?
How did this happen?
What was our whole, what was our chink and our armor?
And so with the retrospective evaluation, we've got another, another tool, anomaly enterprise
that's very focused on applying, basically taking the work off of your existing information
aggregation tools, call the SIM.
Taking the workload off that and applying some ML, applying these known pieces of evidence
like IPs and domains and URLs and file hashes when they occur in the computer network.
Yeah, I'm still trying to wrap my head around like concretely the specific, the specific
types of data that you're ultimately be running machine learning algorithms over.
Right.
Sure.
So there's two kind of main buckets.
Host-based data and the other is network-based data.
So on the host-based side, we have, the most common is something like file hashes, right?
So unique identifier of a file because it's easy to rename a file if it's a piece of malware
that's a really bad indicator.
So we hash the contents of the file or in more sophisticated cases, we hash subsets of
the file to indicate maybe that there's a piece of the malware that's, you know, getting
passed around, for example.
So another kind of lower volume, host-based piece of information that we track is things
like, you know, function names and regaxes and so these types of sub pieces of a piece
of software that could be used in a piece of malware because it's, when you start tracking
by a file hash, so much of cybersecurity is an evolution, right?
So we have an active adversary trying to elude us, which is part of what makes it a bit
more of a unique space.
And so one way this manifests is the type of data that we collect, for example, hashing
files.
When adversaries know that we do that to track their malware, they're going to try and
find methods to get around it.
So one way to get around hashing of files is to just randomly add a little bit of gibberish
in a non-executed portion of the file.
And you can get into, in the ecosystem, like the economic ecosystem of malware, you can
have automatic systems to just add, pad, garbage in the end of the file, just to change the
file hash, just to mislead you.
And is the idea with that to just render tools like yours not useful because there's
so much noise out there, or are you trying to replicate the hash of, like, replicate
the known good hash?
Well, the idea is that file hashes are a very commonly used strategy in cybersecurity
in general.
And so it's a very popular way to uniquely identify a file.
And so adversaries, they're not doing to elude us, they're doing it to elude everyone.
And so then we get into smaller parts of the file.
So there's, in like a PE32, like an executable on Windows, you might have different sections.
If you break down the assembly code, you might hash the import table for all the libraries
that you import.
So these more granular notions of hashing make it harder for the adversaries to avoid,
but it requires a larger context of piecing together the different pieces to the puzzle.
So you've got the host base data, you've got the network, net flow data, presumably,
and perhaps other types of network data.
That stuff is all coming off of ultimately routers, firewalls, network devices.
And then you are talking about the difficulty of kind of coming to identifying ground truth.
Like you have all this data that you've collected, how do you go about labeling or using unsupervised
techniques machine learning wise to try to identify the bad actors?
So we're using, most of our work is definitely in the supervised space.
So like XG boost, random forest, that sort of a space, ensembles, we see a lot of benefit
from.
What's interesting in the threat and intelligence space, I've tried, I've personally looked
at a few different problems and pretty reliably ensemble decision trees or ensemble regression
trees end up being the best strategy.
So maybe can you talk a little bit about some of the different types of problems that
you've looked at?
And how do you go about defining the problems in a way that leads you to being able to solve
them using ML?
Sure.
Sure.
So one of the important rules of machine learning, I think, is only use machine learning
when you actually need it, because most machine learning has false positives and false negatives.
If you can come up with maybe logical expressions that don't have those, then you should use
other methods.
And so when I think of the order of the tools that we use, if we can use the very first
most naive version of a tool, a sort of a white list and black list, just big, big lists
of exact matching, then the next level of tool is a bit more fuzzy matching, starting
to get into space of reg X's and maybe signatures that are capturing patterns with the typical
sort of programming language style of logical expressions, right?
If we see these group of things, possibly followed by another group of things.
If those two methods fail, then your problem is really ripe for machine learning.
And that's really the space we're interested in.
I feel like you need a little bit of intuition to believe that machine learning would work.
So one example is domain generation algorithms.
So for a long time, the strategy is when you have malware infecting your computer network
and it calls out to maybe an IP or domain, then you block that and you're good.
So first we did that with IPs and then the bad guys became aware of it.
So then they started calling out to domains and then once those were getting blocked and
the strategy was successful, then the bad guys evolved yet again.
And so the innovation after that was in one sense fast flux, which is sort of moving
around IPs, but the one that really stuck is something called domain generation algorithms.
So what that is is in a piece of malware itself, you have a pseudo random function that
is known to the computer that's infected with the malware and it's known to the adversary
maybe out on the internet, right?
The guy that's running the whole botnet.
So both of those two points know what the pseudo random algorithm will predict for tomorrow.
And the problem is in between those two points, no one knows.
So the guys that are responsible to defend the network have no idea what the domains might
be tomorrow or the next day.
And so this asymmetry that gets created between the infected malware and the bad guy in
the internet know what's going to happen and the good guys at the perimeter that have
to stop it creates a pretty big disadvantage, right?
Because there's no way they're going to know what domains need to be blocked.
So the problem gets compounded, the asymmetry is compounded specifically because you not
only have one domain like per day, right?
These domains would change every day, for example, but it's not just one.
Maybe that the malware calls out to 1,000 domains each day.
And the bad guys only have to find one chink in your armor and the good guys have to defend
100%.
And so exploiting that asymmetry, the bad guys just maybe pick one or two of the domains
that the malware will call out to tomorrow.
They register that ahead of time and that's how you utilize this communication mechanism.
Because all malware in order to be useful needs to have some network connectivity, right?
There's very few exceptions.
And so, you know, if you're stealing someone's credit card information, you need to send
that back so it can be resold and you can monetize it, right?
If you're doing economic espionage, you need to get the data out of the network and back
into your organization.
So this is a sort of fundamental rule of attacks is that nearly every type of attack is about
some sort of taking information.
So then since the bad guys are able to use this domain generation algorithm to get the
information out of the network, then this is clearly a weak point in the defense of
the good guys perimeter, right?
And so since it's a pseudo random algorithm, the intuition is that humans can look at it
and say, hey, that domain looks like it's gibberish, right?
It might be XLJQBZF2, right?
Absolutely.
And so it's usually the case that these very random sort of high entropy domains are affiliated
with domain generation algorithms.
There's a few exceptions, but the vast majority is like that.
And so what's fascinating is that human analysts, when they see it, they're like duh.
I can pick that out really easy.
Why is this really a problem?
Well because these algorithms may sort of tune down how aggressively they call out, start
blending in with other traffic.
And if you have to look through maybe hundreds of millions of domains that your organization
is called out to yesterday, are you going to be able to look through all of them and
pick this out?
The answer is no.
We need to automate.
Because you can't really automate this very well with logical expressions.
And even if you did, you might be able to reconstruct maybe like a regax that describes
this one particular algorithm.
Oh, but by the way, there's different types of malware.
They all use very different algorithms.
They're often have different seeds to their randomization.
And so you really need an effective way to be able to distinguish between these random
looking domains and normal user traffic and activity.
So this space is really what motivated my first project in this space of machine learning
applied to cybersecurity.
So we saw these things happening.
This was in 2009 actually.
So in 2009, I was talking to some colleagues.
And they, we had realized this was a problem that all these domains were happening out
there in the internet.
And so we put together a supervised prototype and turned out it was able to detect them
pretty effectively.
So that was in about 2009.
And then I think an interesting story that's kind of unique to security is that in a little
over a year later, we, someone, about a year and a half later, someone published a very,
very similar strategy, a little less scalable, but a very similar strategy.
So this was focused.
This was, I can give you the name, this was from Texas AMU.
And they published this paper, which was some good work.
But we intentionally chose to not publish our strategy at the time.
Because there's this question of, if you tell an adversary, you can detect them.
What will happen?
So the upside to the situation is, we had been monitoring them.
We had been monitoring the overall activity of these domain generation algorithm domains
out on the internet.
And so when the report was released at UsNix, it took about two to three weeks or so.
And you can clearly see the points when the information from the conference got back
to the adversaries.
And we can see this, first it drops off, nearly completely.
And that happened maybe lasted about two months or so.
And then after that, you see a significant change in the variance of these.
So it might have been something like, you know, maybe like the weekend activity, you
know, was much higher, but the weekday was lower, something, something like that.
You see a big change in the variance of day-to-day activity.
Then before the paper was released, I have a chart of this, too.
All right.
So, so you did this project, tell, tell us about how, how you wanted about solving the problem.
You know, what techniques did you use and what you learned in the process of, of deploying
them?
Sure.
So, one thing that was very interesting was, I thought was interesting, was understanding
how many, which different models were more effective.
And so, I remember at the time I was using WECA APIs and instrumenting various type of algorithms.
I was very interested in a set-up across fold validation and figure out which algorithms
are going to work better.
The one that ultimately I found worked the best was a mix of adabust with adabust on
Ripper.
So if you've ever heard of the Ripper algorithm was made by William Cohen.
It's a rule-based learner, very similar to, very similar to a decision tree.
But it's a non-boosted algorithm and then boosting it after the fact ended up being more
effective than something like, you know, maybe rent and forest out of the box.
Well, let's dig into all of this.
So, WECA, I've heard of a bunch of times.
I don't know a whole lot about it.
I get the impression that it has declined in popularity relative to newer things, but
maybe tell, what's your take on that?
Let me, you know, what it is to you and the role that, you know, where would you, you know,
what kind of situations would you turn to it?
I think the biggest sweet spot for WECA in my opinion is people who aren't real comfortable
in programming.
So it has a really good point and click GUI.
And so it does have APIs.
It certainly does.
They're Java APIs and it has a lot of algorithms implemented.
So it's going to be much less than or implemented in R, but for being a machine learning framework,
it's got a pretty good selection of APIs, includes feature selection, it includes clustering,
it includes a bit of neural nets.
They've got a sort of repository for prototype code, which includes a lot of bioinformatics
work.
But I think in my perspective, I think it's the, all the tools I've seen, I think it's
my favorite for someone who doesn't do programming.
If you do do programming, you can also use it.
So there's a nice sort of stepping out, right?
So you can go from just using it in GUI to, it's also got a command line where you can
call, you know, like, you know, call it C45 classifier, for example, just from the command
line with the CSV file.
So it's got a nice sort of transition path to programming, and of course, it's also got
Java APIs as well.
So I feel like the transition path from, I don't program to like a program a little bit,
I think in WECK is really strong.
I think like, if you want to compare it to some, like, it's a little bit similar to orange.
So I, what I liked about orange is that it has a canvas format, so you can have sort
of a drag and drop GUI, where you can assemble a pipeline.
The orange is primarily orange is Python based, and WECK is Java based.
With orange, you can assemble sort of a pipeline of what operations you want to do with purely
click and drop.
You can also do this with RapidMiner.
Okay.
And all right, so that's WECK, and then you mentioned Addabust.
Yeah.
Yeah.
Actually, boosted decision trees in general.
Yes, talk through kind of those in the role that they play and soften problems like
that.
Yeah.
So I think that when you're getting introduced to machine learning, I think a very sort
of helpful progression is to start with an ID3, which is a really basic decision tree,
and then kind of move to like a C45, which is a little bit more of a robust decision tree,
and then to understand a bit more about ensembles.
And so ensembles became a pretty hot topic in around the early 2000s, something like
that.
Well, maybe we should take a step back and talk about decision trees for folks that
don't might not know about even a decision tree.
Sure.
Sure.
So the general intuition with the decision tree is that we have a metric like information
gain, so that might be when we, so so when we say decision trees, we actually don't
mean decision trees.
They're actually decision tree induction algorithms.
So they take the data and from the data, they induce a decision tree.
And that's what's a little bit confusing, because when you talk to maybe folks in the
business side, you'll say a decision tree, and it's all about like action, and then
there's an arrow for decisions, and then which way should we go?
That is the end product of these decision tree algorithms.
So the ability to induce those decision trees is really the key observation.
So if you're really in the ML space and you say, oh, you know decision trees, right?
It may mean a completely different thing than you think it means, right?
So so there's special ways these trees are constructed.
I mean, I think a good, a good conceptual understanding is take a metric like information
gain, where we play around with drawing out a bunch of leaves in the tree.
And then at the end of each leaf, we calculate how many, you know, so it's maybe like if
feature a is above three, right?
And that's one leaf.
And if feature b is below two, right, and if feature three is true.
So we build out these levels of of trees, and we assign a, you know, information gain
the idea is if the likelihood is very high.
So how many, how many instances do we label correctly versus inc, how many instances
do we label correctly because of this branch?
And when you have a high metric of usefulness with these, particularly with an individual
branch, then it tends to sort of move up the tree.
So to make this more concrete in the example we were discussing earlier, you may have,
for example, you know, feature a is is IP on such, such and such blacklist feature b
might be, you know, is, you know, hash on x file incorrect.
And so you have some, some grouping of, of these features, and you're basically trying
to create a tree that uses these features to determine whether a, whether the likelihood
of a particular pattern is likely malicious, right?
And then the information gain metric is, you know, as your, your decision tree induction
algorithm is basically trying a bunch of permutations of these different features, and information
gain is essentially asking the question, you know, is this tree adding anything that,
you know, all the other ones that I've looked at, you know, didn't tell me.
Yeah, yeah, so, so you draw out the tree, and then what's important, right, so so much
of data science is then about how do we control overfitting, right?
And I feel like overfitting needs, I think it would be helpful in the field if people
would spend some thought on more precise subgroups of overfitting, because I think there
are subcategories of how things can be overfit, and it's not quite as, not always necessarily
such a binary observation.
So tree pruning on the decision tree is usually one of the parameters that one would tune,
and when you use, when you use tree pruning, the idea is you're, you're trying to appreciate
this trade off between model fit versus model complexity, right?
Since we have a data set that we're using to teach our model, then we could, if we could
build out our tree infinitely, well, we could memorize the data effectively, and that's
not what you want to do.
It's important to generalize, so you don't overfit, and so one of the, one of the important
measurements to tweak with the decision tree is pruning, and so essentially what you're
doing is adding sort of a regularization or a penalty to when you grow out the tree
too much.
So you're trying to find the sweet spot between having a tree that can correctly classify
everything according to the data that it saw, versus having a really complex tree that
might have just memorized all the data.
And so that's, that's a bit of an overview and decision trees, and then when we start
talking about data boosting, we're talking about ensembles.
And I think in the space of decision trees, Leo Bryman's work is really the seminal work
here.
He's, he's the creator of random forests.
And so the, I like to try and explain ensembles in a more general way.
There's, there's variations, so bagging and boosting and stacking are specific types
of ensembles.
But the general idea with ensembles is you have some sort of classifier.
For example, it could be a decision tree, could be an SVM.
Some of the recent work is actually looking at ensembles of deep learning models.
And so, so the idea with an ensemble is that you construct multiple different trees.
So in our decision tree case, you would construct a number of different estimators.
And effectively, they, and you're able to combine the knowledge from the different estimators.
For example, one strategy is to use sort of a voting, right?
And so every different estimator gets a vote toward the ultimate prediction.
And so we talk about ensembles of these.
And so you might use, you know, dozens or hundreds or maybe even thousands, but usually
like low hundreds is, you know, or a few dozen is kind of a good starting place for how
many ensembles to use.
And it's almost always the case that when you combine these ensembles, you get better
performance than you would with the decision tree alone.
And why is that?
It's because some of them, you can imagine sort of variations of some trees might overfit
just a little bit.
Some might underfit just a little bit.
And when you vote, I think it exploits more of a law of large numbers.
And so you get more of an aggregate function, right?
You see in society, you see these interesting things like in crowd voting, where maybe
the intelligence, like wisdom of the crowds, right?
The intelligence of the group is more valuable than the intelligence of any one individual.
So I think that's some of the intuition of why ensembles are better.
And how are the different trees in the ensemble different, are they different sets of features,
are they different hyperparameters, are they, you know, different, I know, when I've
seen ensembles, I've seen them in the case of different models, like you might have
a linear regression, you know, that's good at figuring out one piece of your problem
and, you know, some other type of model that's figuring out another subset of your problem
and then you kind of ensemble them together or at least create a hierarchical model.
When you're doing ensembles of decision trees, like what differentiates tree A from tree
B?
Sure.
And it, of course, depends on the parameters that are set, right?
So it could be that you send different, different data to certain trees.
It could be that you use different initialization parameters, right?
And so, you know, especially if you're creating your own algorithms, there's really no limit,
you know, and some of them you could use information gain as the metric of what is good and
others, you could use, you know, like genie entropy, for example.
What's that?
I don't think I really want to try and define genie entropy.
Like, I've looked at a bunch of definitions, and I'm not surprised I can like succinctly
and clearly describe it with, yeah, it's another metric like information gain that can be
used to rate how good a model is, right, or contributions to a model R.
Yeah.
Okay.
So, add a boost.
You've got a bunch of trees that you've created from sending them different subsets of
your data, different initialization parameters, what does the add a boost algorithm do, and
how's that different from, well, you were talking about actually boosting and bagging and
all that.
Yeah.
Kind of walk us through all that.
Yeah.
So, bagging, boosting, stacking, I think very, very practically.
These are different ways to squeak a little bit of better performance out of your models.
So you have these initial models that you believe are pretty good, and I think it's one
of the single most straightforward ways.
How do I improve my accuracy without changing my data?
Do ensembles, right?
So try different parameters, try boosting, try bagging, try stacking, and you're nearly
guaranteed to improve your performance, whether you're measuring error rates or accuracy
or MCC or anything, AUC.
Yeah, I know, I don't remember the exact stats, but some very large portion of recent
Kaggle contest winners are all gradient boosted decision trees of one sort, you know, ensembles
of one sort or another.
Yeah.
So, XG Boost is really, I think actually the Kaggle is one of the greatest practical sources
to go to for learning about this stuff.
I mean, we all need multiple tools, ensembles, if you will, ensembles of information that
we can pull in, right?
Podcasts like yours, and I think the Kaggle forums are a really valuable space.
That's, you know, a few years back.
That's where, you know, I got first exposed to XG Boost and how sort of it was able to
add a gradient boosted component to our existing strategy of an ensemble decision tree.
And in many cases, XG Boost performs better than random forest.
But it involves a bit more tuning, right?
So it involves basically all the tuning you would have in random forests, you know, maybe
you've got something like a half a dozen to a dozen parameters in random forest.
But when you're tuning XG Boost, you know, you're really looking at one to maybe three dozen
different parameters to tune.
And that can really take some time if you're doing sort of a grid search to try out a bunch
of different levels, you know, it's sort of a combinatorial problem, right?
It's if you have three different settings for Feature One, three for two, three for three,
then it's all multiplicative of how long it will take.
And at the same time, you often want to do it with pretty large data sets because usually
you'll get better performance, if you're doing something like cross validation, cross
whole validation, you'll do, you'll probably get better performance out of it.
So that can really take some time to compute.
So we were kind of walking through applying this stuff to the cyber security use case.
And in that project you were describing, you ended up using decision trees to kind of
get to basically to e-couch your increase performance.
Yeah, how do you even think about performance in this context like what, you know, what,
I don't know if you can quote specific like, you know, error rates or something like that
or what are, what are the key metrics, you know, business metrics, I guess.
And then how do those tie to, you know, like metrics that you would be thinking about
as a data scientist?
Well, actually I can come back to describing that for how we score indicators and just
pass along the confident indicators to our users.
But I think there's an interesting story there when talking about, when talking about the
domain generation algorithm use case because we have this situation of, you have a few
different families of malware.
Let's say you have three families of malware and they generate domain names, right?
It's the domains that they'll call out to in the next day or two or whatever, right?
And so intuitively the first thought you would have is, okay, well, I'm going to measure,
you know, my performance metric, maybe it's, maybe it's accuracy, maybe it's a UC or
whatever.
I'm going to measure that, I'm going to train on some of those domain names, but then
I'm going to test on other domain names and, you know, cross-fold validation and that's
very straightforward.
That's not the right metric to describe it, no matter what, whether you use AUC or Accuracy
or MCC or whatever metric you're using, that's simply the wrong problem you're measuring.
This is what I was meaning about kind of defining, I think there's value to us as a community
to think about better definitions of overfitting, because if I can predict new domains from
the same malware that I've already looked at, what is the value?
Even if it's 100% confidence or 100% accuracy, that's great, but I'm predicting different
domains from the same malware families.
What you want to be doing is predicting new domains from different malware families.
And so when the research was coming out in the space, I was sort of surprised that all
the metrics that was being done were always on detecting new domains from the same malware
families and people weren't measuring other malware families and the accuracy there.
Does that point make sense?
So it's maybe sort of that you need to look in a larger business case to observe what
you should be measuring.
Yeah, so what you're saying is more like, it's almost like a different approach to cross
validation.
You're cross validating across malware families instead of just cross validating within
one malware family.
Yeah, I think that's a great assessment of it, is to think about the scope of the cross
validation and what you're ultimately trying to solve.
Any other interesting use cases or other projects that you guys are working on in the cybersecurity
domain?
Well, I think one thing that's an interesting thing that we are tracking is that how
well are adversaries sort of evolving to our methods, right?
We could talk about this in the space of adversarial machine learning, right?
The idea I think with adversary machine learning is the, so in the cybersecurity domain,
it's pretty common that good guys will first find strong signals of how to detect the bad
guys.
Eventually, the bad guys will know that the good guys are using that particular strong
signal and they'll try to avoid it.
We see this case, you know, this back and forth game happening.
And so what machine learning allows us to do is potentially pick up on a lot of weak
signals that we didn't quite realize were there.
And then use those weak signals to detect the adversary.
And that's kind of the state we're at now.
There's a lot of cyber security companies trying to employ machine learning on obviously,
you know, weak signals to try and get good predictability to detect the bad guys, especially
invariants, right, invariant mechanisms that the bad guys aren't changing and moving
around.
Now the concern is are the bad guys using machine learning, right, or will the bad guys
use machine learning to try and blend in, right?
The most straightforward way is, you know, maybe they would try to model how we do it, for
example.
If the bad guys are modeling how the good guys do it, then constructing an outlier example
isn't too hard.
I mean, that's just the space of generative models.
So if you can fit a generative model to approximate the algorithm the good guy is using, then
this is, you know, this is a great way to sort of evade what the good guys are using.
And that's one way that the space of generative adversarial networks, I think, is a little
bit exciting because most of the generative adversarial networks, the GANs, are used,
so Ian Goodfellow was really a big pioneer in the space.
And so he had some interesting observations and was able to put together a system where
you have sort of a generator function as a generative model.
And then you have a discriminator function.
The discriminator function is trying to detect when, in the case of maybe classifying images.
So you have a generative function that's trying to generate an image and the discriminative
function is looking at the generative model.
And then, or the images that it produces, right, exactly, yeah, it's looking at the
images that the generative model produces, right?
And then the discriminative function, its goal, is to determine, was this determined
by like a human or a computer, right?
So most of this gets used because there's really impressive results.
You end up seeing things like, you know, you can use this in a few different ways.
You can use it to, like, as a better mechanism to interpolate.
So for example, if you have a gap in an image and you want to fill in the image gap, you
could use interpolation and try and sort of guess what it would be.
The point is you've lost some information.
And so when you try and algorithmically fill it in, it looks like it's algorithmically
filled in.
Right.
Meaning if you're averaging pixels or something like that, it looks it just doesn't look
right.
Exactly.
In case you're training a generator or you have a generator that's generating, you
know, proposed, this is a proposed way that I might fill this image in.
And your discriminator is saying, ah, that doesn't look good, ah, that looks good.
And so using the generative adversary on network approach works tends to produce better
looking in fill than, you know, your algorithmic approach is your, you know, averaging and
stuff like that.
That's a, yeah, that's, that's a good assessment of it.
And so a lot of times how it's used is just to try to produce something that's somewhat
convincing.
And the cybersecurity space, we can, we can imagine how that could be used for a type
of like poisoning and evasion.
So, so if we want to generate maybe network data that looks believable, but also gets
around your model, we can use the generator, generator and the discriminator together
to, um, to optimally have the right sort of balance of these two components.
And so in the long term, this is definitely a concern in the security space, you know,
as far as the maturity of the field, you know, we're really just starting to get production
machine learning to do a lot of the, the detection for network and malware and so forth,
you know, the past two, three, four years have been pretty hot in this space.
And so we're really just getting, maturing our process of machine learning, deployed
in cybersecurity.
And so it's a little bit premature to talk about adversaries evading our model, but we
definitely see the adversaries when I was talking about strong signals and weak signals.
We definitely see the adversaries picking up on the strong signals that the humans are
finding and evolving.
And so it's just the case that the way that the adversaries are evolving is suboptimal
because if they had, you know, if they were using machine learning, they could learn
how, learn more optimal strategies to avoid the good guys.
Interesting.
So the good guys all hire these black headers to try to, you know, act like the bad guys
and, and, you know, break them to the bad guys all hire white headers to try to, um, give
them information about what the, the good guys might know about.
So so far, um, as far as what we see adversaries doing to avoid good guys machine learning detection
for the ones we actually catch, right, because we're limited to just what we can catch, um,
we haven't really seen deep model understanding coming from the bad guys.
It's really still at that sort of subtle signal point.
We do in a less ML type, in a less machine learning type of way, we do see bad guys, you
know, testing their malware code, maybe by running it against a bunch of antivirus, right,
because that's, frankly, that's just QA to the bad guys, right?
Right.
Right.
Right.
Um, so Gans, I've only ever seen that in the context of, um, you know, images and deep
neural nets, um, have you seen Gans in, is it, does Gans make sense, uh, in the context
other than deep neural nets?
Um, so yeah, so yeah, well, yeah, that's what's interesting about it is, um, so you, Ian
Goodfeld pretty much invented this space and, uh, I think it was at, I think was at nips
this year.
He gave a pretty famous tutorial on it.
And in that, he mentions, um, you can plug in any supervised algorithm.
Um, but I, we, before there was Gans, there was work in adversarial machine learning.
There was some folks at Berkeley that worked on it.
There was folks, um, at university of, as it pronounced, Caligari, it's that Island
in, uh, the middle of the Mediterranean.
Yeah.
So I think it's the university of Caligari, um, they created a framework for just testing,
um, you know, uh, simple algorithms like SVMs and, you know, decision trees and these types
of things.
So they created a framework for testing those and creating sort of an adversarial space.
So in, in my mind, those types of testing frameworks are examples of adversarial machine
learning that are not Gans.
Yeah, I wasn't aware of any of that stuff, so that's super interesting.
Um, and so in your context, um, are you applying Gans per se or non neural network, uh, uh, models
to, to this and, uh, are you applying neural nets elsewhere in the stuff you're doing?
So we are watching pretty closely as far as do we need to start thinking of an adversarial
component.
And if we do, we'll certainly, the first indication we get, we're going to reorient
our work, um, because it could be a month out, it could be 10 years out, right?
So, um, at the moment, it's, there's not enough data to convince me that it's really worth
that big investment, but it's definitely worth kind of watching and keeping on the radar.
Um, as far as neural nets, um, they're less, uh, advantageous for us for the most part.
Um, I really tend to value interpretable models and I, and I think model interpretability
and cybersecurity is uniquely important because so much of cybersecurity is about, you have
the right data, um, when you understand how your model works, it gives you insights into
what current features are useful, you get a sense of how your model is working, which
you don't really get with neural networks.
And when you get that insight, then you can, um, if you've got some subject matter and
knowledge about how networks work and how operating system work, systems work, you can
start to have the conversation of, well, we should instrument at this level or collect
this new type of data.
So I think the biggest innovations in cybersecurity space are being able to interpret a model
to go back to the data collection, make suggestions of new data to collect and then, uh, improve
the process and start all over there because, um, you know, we've kind of seen a pretty
long history of, for the most part, having good data generally tends to be better than
having really good algorithms.
Of course, you want both, if you have bad data, good algorithms, don't do that much, right?
And even deep neural nets usually require tremendous amounts of data.
And so one of the nice things about our, um, about our company is that, um, when we're
rating all these different feeds that come in, um, in order to help our end users, you
know, we can, we can learn from all these different sources of labeled malicious data,
which includes, you know, all sorts of different techniques for, you know, how do they detect
malware, right?
How much of it is an automated malware of maybe executing and watching the execution patterns
versus, um, statically analyzing a piece of code and reverse engineering and walking
through it or, you know, um, or, or methods like setting up honeypots or, um, other types
of like network-based measurements may be monitoring the collection outside of a tornoid
and just generally tracking this maliciousness.
There's so many different ways to get insight into what malicious is.
We do actually have a nice amount of data.
So for that reason, I think deep networks, deep neural nets might be on the horizon,
but really understanding what data to use to judge the incoming streams of data is very,
um, is important.
So model interpretability is huge for us.
Right.
Right.
Yeah, that comes up in so many conversations I have with folks about, um, as being, uh,
you know, real challenge for employing neural nets right now, at least, um, although, you
know, as those, as that technique matures, you know, there are some signs that will start
to, you know, that it's not necessarily, you know, as I'm not necessarily, uh, antithetical
to have interpretability with neural nets.
It's just, we're not there yet.
Absolutely.
Absolutely.
So, I mean, I think once, once we can cross that threshold of interpretability, I think
it's, um, going to be much more productive in the security space, at least for the companies
that have enough data.
Yeah.
Yeah.
Do you want to touch on before we wrap it up?
So how I got into machine learning, uh, was that in late 2008, I saw this, like, 60
minutes news story, uh, it was titled Reading Your Mind.
And it was, uh, some work happening at Carnegie Mellon, uh, with Tom Mitchell and Marcel
Just and they were using some of the early folks to, to put people in FMRI images, FMRI
brain imaging systems and then use machine learning algorithms to uniquely identify various
thought patterns.
So the FMRI measures blood flow in your brain.
So it's your data source.
And then the ML algorithms were able to separate, um, particular words and concepts.
So they were able to show them a picture of a house or a hammer or something like that.
And then FMRI brain image, the blood flow in the brain and then using the ML to predict
what the person was thinking about.
And so I saw that, it's a little creepy, but it's really cool.
And I just got to thinking, wow, we should, we should be using this for cybersecurity.
And so, um, that was a little bit earlier than a lot of the hot, um, hot, exciting times
when people got, you know, started getting into, so I was a little, a little early to the
game in some, in, in some extent, um, but I ended up a few years later end up publishing
a paper with Tom Mitchell as well and, uh, and Ellen Ritter, uh, at, who's now at Ohio
State.
Okay.
And what we were doing is we had, um, sort of a special, so we applied expectation regularization
to, uh, to detect, um, detect patterns in Twitter stream.
So there's this space of weak supervision in machine learning and the idea is doing supervised
learning when you have very few examples.
And so it was mostly an NLP problem and we were harvesting from data from the Twitter
verse.
And, um, what we were trying to do was predict security, um, pull out the security events.
So create sort of a security news source out of just the general Twitter stream.
And so with just a few hand labeled events, we were able to pull out, you know, events
on like DDoS and account hijacking.
I think we labeled like a couple dozen events and it had reasonably, um, reasonably high
accuracy given the, given the weak supervision space, right, and very few labeled examples.
So then we were able to sort of create these news streams of, um, these news streams of
security events, right?
So this, like, these are the events of people getting, uh, people's accounts getting hijacked
today.
And so able to track, um, news stories.
And I just thought that might be useful in security socks, right, to know a security operation
centers to know, uh, you know, what are the big events of the internet?
Because in cybersecurity, you go into any fancy high-end, um, cybersecurity operation center
and they've always got like CNN on the view screen.
Why?
Because if 9-11 happens, it's really going to change the network characteristics, right?
So you just need a basic exposure to the news.
And so the thinking with some of this work was, um, if we can pull out the security news
from the Twitterverse, then we can track some of these cyber events that are happening
near the cyber news from Twitter.
Mm-hmm, right.
And the technique that you mentioned using was what?
Expectation, regularization.
So we're, um, we're looking at the, the log likelihood, um, so we, we do a bunch of sort
of n-gram analysis of looking at the tweet and the particular words that are involved.
And, um, we're able to, um, take, take the log likelihood and then have a term for regularizing,
um, the label, so sort of a, uh, a penalty for being wrong, right?
Um, and then also use, um, use L2 regularization as well.
So two types of regularization as a penalty, um, the sort of basic ideas in the same way
we don't want to grow our decision tree really large, we want to prune it back.
So we don't fit too tightly to the data by having a lot of strong regularization.
The intuition is, so we'll try and pick up on, on the key patterns.
And in this case, the key patterns were like nearby words in the tweet, um, we used hash
tags as, as well.
Um, and so from that, we're able to sort of create, uh, security news service of sorts.
Cool, does that still exist?
Um, it existed about a year ago.
I could try and email Allen and let you know if it gets back up.
We, we had the security tweets domain and security tweets.org and, uh, that just pointed
to like one of his test servers at Ohio State, um, right now it's down.
So I could ping him, ask if you could turn it back on.
Oh, it sounds like interesting work.
Maybe we'll be able to include a link to the paper or something like that.
Yeah.
So it was presented at the, uh, the World Wide Web Conference in 2015.
Okay.
Yeah.
Great, great.
Uh, well, Evan, thanks so much for, uh, for meeting with me for taking the time to join
us here on the show.
I think this is that this may be the first time I've had a kind of a diehard Twoma listener
on the show.
So it's, it's super exciting for me from that perspective.
Um, and I'm so glad you're able to join us here at the, uh, at the strata event.
Um, maybe before we go, uh, have you, what have you learned so far at the conference?
Anything interesting?
Yeah.
So, uh, so there was two talks.
This is pretty early in the conference, but, uh, two talks was sitting in, um, uh, some
folks from Microsoft were talking about, um, they were talking about how they use machine
learning and their security operations.
Um, in the Q and A section, we had a little bit of discussion of data imputation because
that's, that's an area that's been on my radar lately.
So in cybersecurity, we often have to make like a bunch of external API calls because
that's part of our feature space.
And so for whatever reason, if those API calls fail, then you're forced to impute that data.
And can that impute data then cause you to make significant errors?
For example, if you were an adversary and you knew how to get me to impute my data, then
I could misclassify because of that.
So, um, so we ended up chatting about that a little bit in the Q and A section.
So valuable so far?
Yeah.
Okay, absolutely.
I love that there's, um, a little bit of an, uh, there's multiple sessions that are
covering the cybersecurity use case now, um, because it used to just be, um, basically
media studies of video, audio, or images, or NLP, right?
And so kind of the, the minority of us that are outside of those two areas, um, I don't
think it quite as much attention in the machine learning space.
Awesome.
Awesome.
Thanks again, and, uh, thanks for being on the show.
Great.
Thanks for having me.
All right.
Bye.
All right, everyone.
That's our show for today.
Once again, thanks so much for listening and for your continued support.
Don't forget to share your favorite quotes for one of our hot, new, tumult stickers.
You can share them via the show notes page, via Twitter, via our Facebook page, or via
a comment on YouTube or SoundCloud.
The notes for this week's show will be up on twomlai.com slash talk slash 16, where you'll
find links to Evan and the various resources mentioned in the show.
Thanks so much for listening and catch you next time.
