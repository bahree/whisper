Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Did you miss TwimalCon AI platforms?
If so, you'll definitely want to check out our TwimalCon Video Packages.
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more
about their experiences automating, accelerating, and scaling machine learning and AI.
In each video package, you'll receive our keynote interviews, the exclusive team Teardown
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.
Once again, visit twimalcon.com slash videos for more information or to secure your advanced
purchase today.
Alright everyone, I am on the line with Tom Dietrich.
Tom is a Distinguished Professor Emeritus at Oregon State University.
Welcome to the Twimal AI Podcast.
It's nice to be here.
Thank you for inviting me.
Absolutely.
I'm really looking forward to digging into this conversation.
I'd love to, before we dive in, get a little bit of your...
I'm really looking forward to diving into our conversation, which is going to be focused
on a recent blog post that you wrote.
There's really exploring what it means for a machine to understand.
But before we do that, I'd love to learn a little bit about your background.
And the context to which you bring to this conversation.
Okay, well, I'm started out as one of the very first graduate students working in machine
learning.
My advisor, Richard McCalsky, was one of the three professors, along with Tom Mitchell
and Jaime Carbonell at Carnegie Mellon, who launched the series of workshops and now
conferences, the International Conference on Machine Learning.
So back in 1980, there were about 30 of us in a classroom in Carnegie Mellon, and I've
been working in the field since then.
Wow.
Over that time, I made some technical contributions in ensemble methods and in hierarchical
reinforcement learning and various applications, for example, in pharmaceuticals, and I work
in applications in weather network data and data cleaning, internet of things, kinds
of things, but my primary research focus these days is on reliable, robust artificial
intelligence, particularly in safety critical applications.
So I've done a lot of service activities for the field I edited the machine learning
journal for six years, and then moved into a position as the founding president of the
International Machine Learning Society, which is the organization that runs the machine
learning conference, the ICML conference.
I also chaired the NURP's conference in 2000, and I served a term as president of the
Association for the Advancement of Artificial Intelligence.
My main service activity these days is I'm one of the moderators on archive for machine
learning, which is between machine learning and computer vision, the two most active categories
for the archive pre-print server.
And so I mentioned this in the opening, but we're really digging into this topic of what
it means for a machine to understand, a recent blog post of yours, and I thought to get
things kicked off, I'd read your couple of the opening sentences.
You wrote critics of recent advances in artificial intelligence complained that although these
advances have reduced remarkable improvements in AI systems, these systems still do not
exhibit real true or genuine understanding.
The use of words like real true and genuine imply that understanding is binary, a system
either exhibits genuine understanding or it does not.
The difficulty with this way of thinking is that human understanding is never complete
and perfect.
So certainly the way you've laid that opening argument out, it resonates with me.
I recently had Gary Marcus on the show, this was back in September, and we spoke about
the book that he recently launched rebooting AI, and he's very outspoken as a critic of
deep learning, and maybe that's not the way he would put it, maybe he'd say a critic
of deep learning as a standalone path to artificial general intelligence, but reading
the blog post, I couldn't help but think of Gary Marcus as being, although you didn't
name him kind of the person in absentia that you were writing this to.
Maybe talk a little bit about the broader context for this post and maybe how what prompted
you to write it, if Gary was part of what you were thinking about or not, I'd love
to kind of get a sense for where you were coming from here.
Well certainly Gary and I have had an opportunity even to engage in formal debates, and Gary's
as I was saying, I think Gary's main points I generally agree with, which is that there
are lots of obvious shortcomings in our existing AI systems and in particular these systems
based on deep learning, but Gary and other people can't seem to stop saying things like
well, when we look at the behaviors say of Google Translate, it's clear that it doesn't
have, it's not exhibiting real understanding of the language it's translating, or when
we talk about Siri, that Siri doesn't really understand what we're talking about.
And I've been making the counter argument, yes, these systems are understanding and it's
real understanding, but it is narrow understanding.
So I am criticizing the use of the word real to mean deep and complete understanding, because
that denies that these systems are doing anything that is intelligent or that is exhibiting
real understanding, and I think that puts you in the position that you will never be
happy with any AI system because no matter how good it gets, it will make mistakes and
exhibit failures and it's understanding.
And are you going to say, well, when it understands 95% of what people say, that it's still
not real understanding, I mean, you're pushing yourself into a belief that there's some
magic threshold, that if you could somehow cross it, you would have a system that had
real understanding.
And I don't think that's the way it works.
I think that the way it works is that we make incremental progress, sometimes bigger
leaps, sometimes no progress for periods of time, as we were doing in speech recognition
for a while in the 90s, but our systems get better, they are able to understand something.
So as I say, when I tell Siri, please call Dan and it calls the right person, it has understood
me for the purposes of that utterance for that task.
Now if I said, you know, Siri, tell me what Dan means to me, and it doesn't really know
anything about what it means, say, to be a best friend, but many people, of course, have
remarked that it's impossible for two human beings to fully understand each other.
So that brings me back again to what is it we're really trying to achieve when we build
AI systems.
And as an engineer, I would say, I want systems that can make the appropriate response when
I ask them to do something, or if they're warning me of some situation in the world that
I should be paying attention to, and so on.
And to the extent that they do that correctly, I would say they understand what I want them
to do.
Yeah, when we have these kinds of conversations, I think it's, you know, there's a slippery
slope of kind of devolving to defining every term in the argument.
But in this case, I wonder the extent to which we're talking about different types of
understanding.
Do you think that that is the case at all here?
Well, I don't know that there are different types, but certainly different definitions
or degrees.
Well, obviously we are arguing about definitions.
And in my blog post, I was supporting the view that instead of arguing about our definitions,
we should be trying, we should be asking ourselves, well, what tests would you give to
a system in order to evaluate whether it's understanding, doing a particular type of
task well?
Right.
If you say, well, the system is narrow, show me all the things that you would like it
to do that it is failing to do now.
And I drew the analogy to test driven development in software engineering, write the tests first,
and then use those to decide how to engineer the system to try to meet those tests.
And then keep writing more tests.
And I think Gary has actually jumped on that.
And on Twitter, he's been asking people, you know, what's wrong with our current natural
language understanding tasks.
And because it seems that we can often get an AI system to do well on a particular benchmark
task.
And yet, again, it turns out that it's very narrow and it's not doing well on any like
immediately adjacent tasks that we would like it to do.
And so some people have been, there's been a bit of a discussion now about that.
And I think that's really where the discussion needs to go is, you know, and Gary himself
in our Twitter conversations, I thought articulated beautifully, said, okay, I want the computer
to be able to say, read a story and tell me the answers to the journalist questions.
Who, when, where, why, how?
So why did this person do this?
What did they do?
When did they do it?
You know, ordered these events for me correctly in time.
And those are way beyond what we can, the state of the art in AI and natural language understanding.
Some of the people in the natural language community said, just stop using the word understanding
at all, we just call it language processing, because we know that, that this word understanding
sets expectations for something that is very broad and deep.
You know, Doug Hofstetter had a very interesting piece that came out last year, where he analyzed
Google Translate and showed how many, many cases where Google translates understandings
clearly extremely surface oriented.
And often it can't understand anything about did, you know, did John do something to Mary,
was Mary doing something to John, it just knows that John and Mary need to both be translated
into the different language.
And it certainly doesn't have any of the, you know, connotations and depth to say that
would be required to translate more poetic language or metaphorical language.
It really has no understanding of human social relationships, what might make Mary angry,
what might make John happy, you know, just, it's completely clueless about that, because
all it has been taught to do is to translate from say Chinese into English or English into
Chinese for fairly straightforward everyday sentences, and certainly not trying to translate
Shakespeare at all.
And one can imagine that it could make very serious mistakes as a result in say highly
emotional and complex social situations.
It's fine for where is the nearest bus stop, but not so good for, you know, why aren't
you talking to me anymore or something.
I do want to encourage us to move beyond saying, well, it either understands or it doesn't,
and this understanding is true or it isn't to say, well, this understanding is incomplete
in these important ways and what we would need these to do.
And so for example, when we think about reading a story or just engaging in a dialogue,
we need AI systems that can be building and maintaining an interpretation of the dialogue.
And this is well known in the natural language community, we just don't know really how
to do it at scale.
We can build applications in a narrow domain, say purchasing airline tickets or something,
where we can cover a lot of different linguistic phenomena and have quite good performance,
but as soon as you step out of that narrow domain, that breaks down.
Yeah, maybe kind of returning to the definition or debate over what understanding a self
means, you refer to Cyril's Chinese room argument and an article by Cole in your blog.
Can you talk a little bit about those?
Well, yes, so there is this famous article in which Cyril put forward the sort of following
thought experiment, right, which was his Chinese room.
So a person is in a room and in this room are, you know, files and files or books and
books full of rules that basically say if you are given these Chinese characters as input,
you should produce these Chinese characters as output and maybe their intermediate rules
and so on.
And there's a human being in there and what that human being is essentially like the central
processing unit of a computer, it takes the input from the outside world that matches
it somehow against all these rules, follows the rules until it produces an output and outputs
it.
And Cyril was arguing that obviously this room could not, that was not truly or generally
understanding Chinese, even though to an outside observer, it looked like it was doing
just fine.
And several people criticize this at the time and it has been a source of lots of interesting
analysis over the years, but one of the main points I would like to link this back to
my previous discussion is that the person inside the box, who he, Cyril asserts is just
an English-only speaker and doesn't understand any Chinese at all, will still not understand
any Chinese even though they are executing this computer program.
And that's what we should expect, I mean, he doesn't make this point, he tries to pull
out the intuition, you know, why doesn't this human understand Chinese and even argues,
well, suppose the human wouldn't actually be using books full of rules but had memorized
all of these rules, but still they would just be executing rules and they wouldn't be understanding
Chinese.
Well, a lot of people then criticize this as saying, well, the intuition is somehow that
I'm looking, if the box is the entire system and I'm looking at some subpart of the box
like the central processing unit of a computer, I'm not going to be able to find a locus of
understanding, looking at any single component inside the box, it's a sort of system-level
property that, given inputs, the box produces appropriate outputs, which would be my sort
of functional definition of understanding.
And I would say that understanding is deeper and broader to the extent that you can ask
it a wider and wider of more challenging and deeper questions and it produces the appropriate
responses to those also.
But we shouldn't expect when we open up, let's say some day we have produced an omni-intelligence
system that is broad and deep intelligence and we open it up, what are we likely to find
inside?
Well, at some level, we might find things we would describe as reasoning and memory and
knowledge and if we go down deeper, we might find pattern matching and probabilistic, you
know, random sampling or something.
If we go down lower, we're just going to find, you know, and in or gates turning on
and off, why should we expect to look inside the box and find intelligence there?
It's going to be the entire system that exhibits this behavior.
Just as with us, when we open up our brains, we just see neurons firing and we keep trying
to find, well, how does that produce intelligent behavior?
Yeah, I think part of this, you know, that most recent argument reminds me of my conversation
with Greg Brockman of OpenAI, where I think towards the end of that conversation, you know,
we pause it.
I forget whether this was, you know, his perspective or mine or, you know, how we arrive at it,
but, you know, one way of kind of projecting what AGI might look like is that it's not
some single universally intelligent system, but something that looks more like an ensemble
of narrower things.
And I think that kind of ties into your functional argument in the sense of, you know, at the
end, the individual subcomponents of this thing won't necessarily have broad understanding,
but they could produce something that, you know, if you look at it, it functionally
has a broader understanding.
Yes.
And of course, this has also been a topic of long discussion in the AI community.
I think researchers and artificial intelligence want to believe that there are some common core
mechanisms that apply across a wide range of intelligent behaviors.
And certainly we, one of the things we have a commitment to or have had since the 50s
in reaction to behaviorism is that there should be some sort of mental models or mental representations
of the world.
And for instance, I should have a representation, someone in my head of you and your goals
and questions in this conversation, what we've already talked about, what we might talk
about, and so on.
And there's been a almost a reflex reaction against the idea that intelligence is just
a big switch that, given the task, I switch on a different expert in my head, and I just
have all these narrow experts.
There should be some shared mechanism.
On the other hand, you know, when you look at, say, Nobel Prize winners in physics, who
decide to start talking about artificial intelligence, and they don't have any deeper insight
than any of the rest of us when they move out of their field of expertise, and this
has been established across many different, you know, fields of expertise is that, you
know, you can be an expert in one thing, but you're basically a novice in other things.
And yet, of course, we do see that some people seem to be faster learners than others.
And so that argues, well, maybe there are some shared mechanisms, and certainly our
perception, as we get older, maybe we're fooling ourselves, is that as we learn more and
more about more things, we're able to go into a new area and learn faster because
we have some frameworks that we can plug new knowledge into that help us learn faster.
So there's definitely this conflict in AI between generality and narrow specificity.
And I don't think it's a big switch that just completely switches from one expert to
another.
But nor do I think that there is some universal module that if we just get that right,
we'll be able to learn everything, because certainly we see in ourselves and in other
people that different people have different strengths and weaknesses in terms of the kinds
of knowledge that they can master, the kinds of skill they can exhibit.
So there's a lot of heterogeneity, and that suggests that there isn't a single thing
like our heart or our lungs that has the same function across everyone.
There's certainly elements of your argument that resonate with me very strongly, the idea
that understanding isn't binary just makes sense to me.
At the same time, when I think about what Gary is reacting to and trying to speak out
against, it's this hype engine that wants to portray with intent or not.
Maybe it's not the initiators of the news that want to intent, but once you put it through
the media filter starts to portray this idea that we're at already systems that exhibit
some kind of intelligence or that we should be worried about, or that have understanding.
I think that's really what he's trying to put a damper on for the benefit of the industry
as a whole, and certainly when I talk to laypeople about what's happening in artificial intelligence
and what a theory really is, what I most immediately refer to as a misconception that these
systems do have understanding our intelligent, and my immediate reaction is to try to contextualize
that, and most often I'm saying, no, they're not really, they don't really understand.
It seems like the right counterbalance to what the hype creates or just a lack of understanding
of what's really happening in these systems conveys.
It seems like the right way to kind of guide their understanding of the degree of understanding
of modern AI, so why do you kind of object to that?
Well, I don't object to that, and I am 100% with Gary on the trying to dampen down the
hype.
I think it creates those misconceptions that you're talking about.
I think it's leading investors to put money behind things that are not going to work out.
I think that it's also an issue of intellectual honesty in the computer science field that
as we do research on these things, we need to point out not only the new capabilities
that we can create, but also their weaknesses.
I totally agree with Gary, and I think Gary would make another point in his argument against
a sort of all deep learning approach.
He's making essentially the ladder to the moon argument, which you may have heard before,
which is that if our goal is to get to the moon, and we can demonstrate that we went
from six-foot tall ladders to ten-foot tall ladders to fifty-foot tall ladders, and
we say, look, we're making progress and extrapolate from our ladder technology to claim that
it's going to get us to the moon, and it just isn't.
We have to, of course, as researchers and engineers, we place our bets on certain technologies,
and we want to see how far we can push them.
In the past, people have placed bets on explicit knowledge representation in reasoning systems
where they're hand-coding as much of human knowledge as they can, and now deep learning
is placing its bets on deep neural networks and, more generally, undifferentiable programming.
This certainly gives you a way of writing programs that you can now train in an end-to-end
fashion, or you can train individual modules, and then glue them together with end-to-end
fine-tuning, and there's a feeling among the connectionist or deep learning community
that there's still a lot of headroom to go in the things that we could do with this technology.
So people are putting in memory, and they're connecting this technology to external knowledge
sources. They're doing the meta-learning, which allows them a system that's been trained
on some initial tasks to also then very rapidly learn new tasks and transfer their knowledge
from one to another, and so partly the people inside the deep learning community feel
like this is still a productive research paradigm or research program, which is another
another thing I mentioned in my blog post is this idea that if we think about the development
of science in terms of Thomas Coons analysis in terms of paradigms, or this analysis in
terms of research programs, that these programs continue until they cease to be productive
and fruitful, and Gary, on the one hand, is arguing, well, there are all these things
that deep learning systems, today's deep learning systems cannot do, and that he doesn't
see how they're ever going to do.
So he's arguing, well, we know we have these other systems, these symbolic reasoning systems
that can do some of these other things, so the obvious path forward is to build hybrid
systems that combine both symbolic and connectionist components.
But I would say that the connectionist deep learning reply to that is, well, we're going
to keep pushing, because we see that our methods are still fruitful in giving us new capabilities,
we're going to keep pushing on them, and we want to maintain an open mind.
I mean, maybe we can't reach the moon with only connectionist ladders, but maybe we can
build rocket ships out of connectionist material, and maybe we can get there.
And so, of course, the connections have from, for decades, criticized symbolic systems
for their inability to capture nuances and similarities, and because they're very symbolic
and Boolean, and I don't think we have a good, that the symbolic community has a good
response to that criticism.
So both methods have their weaknesses, and maybe, I mean, if I were building a system
today, I would build a hybrid system.
And certainly, if you look at the successful AI systems out there, like let's look at
Google search or Bing, these systems are hybrids of deep learning and symbolic reasoning
systems.
They are also collections of experts, so that an incoming question is routed to multiple
subquery engines that say, well, is this a question about stock prices?
Is this a question about a product for sale?
Is this a question about geography, right, and so on?
The candidate answers bubble up and are ranked and assessed, and then one or more of them
are displayed to the user.
And so these are certainly, if you look at what is the state of the art in terms of actual
practical systems, they are all hybrid systems.
It strikes me that the critique of deep learning isn't going to get us to AGI is similar
to, or there's maybe an adjacent critique that kind of touches on your own area of research
into robust and safe AI that kind of says that, you know, a lot of the research in that
area kind of presupposes some super intelligence, ALA, Nick Bostrom.
And we're nowhere near there.
We have no idea how we're going to get there.
And therefore, how useful really is that whole line of reasoning is kind of that the parallel
critique there, part of what you're responding to or in what ways do you see that playing
out?
Well, so my interest in robust AI is much more immediate than Bostrom's paperclip maker
or any of those things.
Okay.
I think those are all fantasies, basically.
But we have very, very practical issues confronting us right now, because all of the AI systems
that we build today are limited to fairly closed worlds.
I mean, I guess I'd have to say the Google search engine is much more open, but it has
a fallback, which is to just go and do a search of the web and try to find matching documents.
So if it can't understand something, that's kind of what it falls back on, just like Siri
does.
But when we think about, say, a self-driving car, and we train it to recognize basketballs
bouncing across the street, children on bicycles and tricycles and dogs and deer and what
have you, but there's always the possibility that something new, there'll be some new kind
of obstacle that the car hadn't seen before.
And so I used to use the made up example, well, suppose we train the system in North America,
but then we deployed in Australia, what does it do the first time it sees a kangaroo?
It turned out that was actually a real case, and the kangaroos were confusing some of
the self-driving car systems that had been engineered in Europe, and then were being
tested in Australia.
So this is known as kind of the open world problem or the open category problem.
There's some new kinds of objects out there.
And our machine learning systems, say for supervised learning, are trained on some fixed
set of categories.
So the most famous benchmark, which is ImageNet, has 1,000 categories of objects, and it's
trained to discriminate among those 1,000 categories.
So it basically assumes every new image it sees contains one of those, objects from
one of those 1,000 categories.
And so if there's something new there, it will just classify it as one of the thousand
things it knows about.
So if it hadn't been trained on kangaroos, maybe it classifies it as a rabbit or something,
maybe it's confused about the scale, I don't know, or a deer, who knows what kind of mistake
it might make, maybe because of the way it moves it classifies it as a paper bag blowing
across the road.
And in that case, the automatic car might make a mistake and not break to stop for it.
So I'm interested in this question of can we build systems that can work in open environments?
Can we get probability estimates, confidence estimates out of the system that we can
trust in open worlds?
And in close worlds too.
So if we think about, there's been a lot of discussion of face recognition and these
face recognition systems, and it's well established that they are not equally accurate on all
people, right, and particularly black people and darker skinned people, the systems are
not nearly as accurate on, and particularly black women are very inaccurate on these, and
partly that is because the images of black people are underrepresented in the training
data, but they are also just a minority of the population.
So if you belong to a subpopulation that is a tiny minority, machine learning systems
tend to go for the common case, and they tend to be less accurate on the edge cases.
You know, you can say, well, I'm 98% accurate.
It's just that every one of my mistakes turns out to be a black woman.
And so if you're a black woman, you're really going to have problems with these face recognition
systems.
And you know, this has been really called to attention, I think, really nicely, by a
joy-blown lenient MIT and Timnick Gibru, who I think is still with Google, and their collaborators.
And I want these systems to be able to give confidence scores that say, well, when I see
an image of a black woman, I have a confidence score that is very low, so that we know not
to trust these systems in these situations.
Even for the self-driving car, same for, say, using computer vision in medicine, in reading
X-rays and so on, we need systems that can give us confidence numbers that we can trust.
The computer vision system that Amazon sells for face recognition gives confidence numbers,
but they don't tell you what they mean.
And in any case, you would need to calibrate your confidence numbers to the data you're
using the model on, which is different from the data it was trained on.
And so that's one of the things I'm studying in my work.
Yeah, I think at the end of the day, it strikes me that, you know, what Gary's saying is,
hey, you know, we're calling these systems intelligent, talking about their understanding,
but at AGI, deep learning, you know, almost certainly isn't going to get us to AGI by
itself, which is kind of the benchmark of, quote unquote, you know, capital I intelligence
capital you understanding at the same time, what you're saying is, hey, let's not throw
the baby out with the bathwater, there's still significant value in deep learning as well.
And also a lot of room for additional research and exploration and uncovering future value
there.
And it, you know, it sounds like both of these are correct, both of these are great and
right perspectives to take.
Do you agree with that?
Well, I mean, I'm not willing to say, oh, well, you know, deep learning style technology
can't get us to say broadly intelligent systems.
I don't like them from AGI because it's an undefined term.
But so I think we should continue to push in that direction with our deep learning systems.
But we obviously should not be declaring victory prematurely.
And we do get a lot of press releases out of companies and out of academic labs that
exaggerate the significance of the work and, and, you know, maybe they don't explicitly
say it, but they are open to the misinterpretation that, that, that, you know, broad, I'm the intelligence
systems are right around the corner when, when, in fact, of course, we are nowhere near
having those.
Yeah, and I should be clear, Gary's perspective is that deep learning as a style of computation
won't get us there.
And it needs to be supplemented by symbolic computing and, or other techniques.
My personal view is that, you know, there's some discontinuous innovation that's going
to be required to get us there.
Deep learning may be a big part of it, but there's something else.
And we don't really know what that something else is.
It may be some other style of compute that, you know, quantum computing, you know, heaven
forbid or something else that allows us to, you know, to, to far surpass where we are
today.
But I don't think we know what that is today.
And, you know, it's certainly, I think it's a very safe bet that we're going to need
more innovation, discontinuous innovation, as you say.
Exactly.
And then, you know, but also there, there's, you know, it's very clear that we're just
at the beginning of unlocking the value that deep learning offers to society and that,
you know, there's more work to be done and that we should be doing that work.
Yes.
I think we could continue this discussion for quite some time, but I think we've covered
a lot of good ground and it was great checking with you on your perspective on this.
And I really appreciated the blog post, Tom.
Well, thank you very much.
It's always fun to talk about these, these challenging questions in artificial intelligence.
Awesome.
Thanks so much.
That's our show for today.
To learn more about today's show, visit twomolei.com slash shows.
Once again, if you missed twomolei or want to share what you learned with your team, be
sure to visit twomolei.com slash videos for more information about twomolei.com video
packages.
Thanks so much for listening and we'll see you next week.
