WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.920
I'm your host Sam Charrington.

00:31.920 --> 00:36.760
You are invited to join us for the very first Twimblecon conference which will focus on the

00:36.760 --> 00:41.200
tools, technologies and practices necessary to scale the delivery of machine learning

00:41.200 --> 00:43.720
and AI in the enterprise.

00:43.720 --> 00:48.600
The event will be held October 1st and 2nd in San Francisco and early bird registration

00:48.600 --> 00:58.520
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

00:58.520 --> 01:03.520
All right everyone, I am on the line with Bill Felman, Bill is the Director of Data Science

01:03.520 --> 01:08.080
at USAAA, Bill, welcome to this week in machine learning and AI.

01:08.080 --> 01:10.080
Thank you Sam, thank you for inviting me.

01:10.080 --> 01:16.720
Absolutely, so we were originally planning to connect at the Stradda Data Conference

01:16.720 --> 01:24.920
in New York, but you unfortunately weren't able to make it due to hurricane Florence actually.

01:24.920 --> 01:28.280
Did everything work out okay for you with the hurricane?

01:28.280 --> 01:33.440
It actually did, I mean at the beginning of the week we had a zone of uncertainty that

01:33.440 --> 01:39.680
included where I live and then as a week progressed we shifted south so luckily it missed

01:39.680 --> 01:45.960
my area but still it created a lot of uncertainty and drama and preparation for it.

01:45.960 --> 01:50.880
I bet, I bet, so at the Stradda conference you were going to talk about topic modeling

01:50.880 --> 01:57.520
and we're going to dig into that in our conversation today, but before we do that why don't you

01:57.520 --> 02:00.680
tell us a little bit about your background?

02:00.680 --> 02:02.080
Sure, absolutely.

02:02.080 --> 02:07.280
So as you mentioned I'm the Director of Data Science team at USAAA which is the United

02:07.280 --> 02:14.560
Services Automobile Association and my team conducts research and applies advance analytical

02:14.560 --> 02:22.000
methods in support of our context center operations that serves 12.4 million members and how

02:22.000 --> 02:25.320
I got to that position.

02:25.320 --> 02:31.920
My background is in mathematics and I hold a bachelor's in mathematics, a master's in applied

02:31.920 --> 02:39.280
mathematics and a PhD in applied science and my applied science PhD dissertation was

02:39.280 --> 02:45.160
an area of computer vision and so what I did in my dissertation I developed a method

02:45.160 --> 02:52.880
to classify non-heat generating objects in thermal imagery and that work actually led

02:52.880 --> 02:59.560
me to an opportunity to work in an autonomy incubator at NASA Langley Research Center

02:59.560 --> 03:06.520
after I retired from the US Army and while in the autonomy incubator I worked on a multidisciplinary

03:06.520 --> 03:12.840
team I was doing computer vision and what we were doing was developing machine intelligence

03:12.840 --> 03:19.320
software for uncrewed systems and that actually that computer vision work is what led me to

03:19.320 --> 03:27.680
my efforts at USAAA involving natural language processing and so what I'm doing at USAAA

03:27.680 --> 03:35.880
is really looking in large corpus of documents and under trying to identify topics what I

03:35.880 --> 03:40.200
call unknown unknowns really the things we don't know what we don't know but if you go

03:40.200 --> 03:46.560
back to my computer vision really computer vision is looking at frames of imagery in each

03:46.560 --> 03:51.880
frame as a matrix where each value within the cells in the frame are either gray level

03:51.880 --> 03:57.720
values or RGB red, green, blue and when you look at natural language processing I'm really

03:57.720 --> 04:04.720
dealing with large volumes of documents and along with those documents the terms or vocabulary

04:04.720 --> 04:09.720
that are involved so I'm going to take those large corpus documents along with their vocabulary

04:09.720 --> 04:14.600
and I'm going to put them in a matrix and so I'm really either when I'm doing computer

04:14.600 --> 04:19.920
vision or if I'm doing natural language processing I'm applying the similar types of mathematics

04:19.920 --> 04:25.600
which involves the calculus and matrix algebra of probability and statistics to both types

04:25.600 --> 04:32.480
of application so it was a really good fit from what I was doing with computer vision and

04:32.480 --> 04:36.960
then transitioning over to natural language processing type work that I'm doing at USAAA.

04:36.960 --> 04:46.040
Oh nice nice and what is the what's the motivation for doing that kind of work in support

04:46.040 --> 04:51.240
of a context center what are the kinds of documents and data that you're dealing with and what

04:51.240 --> 04:55.360
business outcomes do they help drive for you.

04:55.360 --> 05:01.440
Good question so in our context center so we got a call channel we also had where agents

05:01.440 --> 05:07.000
are talking to our members on the phone we also have a chat channel where members are

05:07.000 --> 05:12.600
talking to our agents as well but also we have digital channels such as.com and mobile

05:12.600 --> 05:19.760
so these are very large volume of both unstructured and structured data and what we want to be

05:19.760 --> 05:25.400
able to do is gain insights in this data to better serve our members and this is where

05:25.400 --> 05:31.600
actually topic modeling comes into play and what you can think of topic modeling as a way

05:31.600 --> 05:36.960
to what I like to say uncover the unknown unknowns you know there's there's the things

05:36.960 --> 05:42.400
we don't know what we don't know and what topic modeling is is an unsupervised method

05:42.400 --> 05:46.080
that helps us discover those topics that are emerging.

05:46.080 --> 05:55.840
You mentioned a chat channel are you using AI as part of delivering that chat and I'm

05:55.840 --> 06:02.920
mostly asking because this concept of discovering the unknown unknowns seems like it would be

06:02.920 --> 06:10.080
really helpful in identifying you know for example topics that come up in chat conversations

06:10.080 --> 06:17.640
that you might need to create you know intense and support for in your chat system.

06:17.640 --> 06:23.960
Yes absolutely so what we want to be able to do and what we're doing is you know you

06:23.960 --> 06:30.040
have chat in the chat channel you have conversations between our agents and our members and

06:30.040 --> 06:35.800
same thing with the call channel and what we want to be able to do is identify topics

06:35.800 --> 06:43.120
that are emerging before we reach a peak call volume and that way we can say mitigate

06:43.120 --> 06:50.280
members concerns before we hit that peak call volume as an example if if there's some

06:50.280 --> 06:59.240
kind of force fire for example and in California or a hurricane you know as we start seeing

06:59.240 --> 07:04.160
conversations about that we can use topic modeling to get a better understanding of our

07:04.160 --> 07:09.320
the needs of our members and we can be able to be ready to provide the services before

07:09.320 --> 07:15.120
we even reach any kind of peak call volume it's really being more proactive than reactive

07:15.120 --> 07:24.760
to the after the fact okay okay so you've got this data set that comes from the is it

07:24.760 --> 07:34.680
coming primarily or solely from these chat and email interactions or are you also transcribing

07:34.680 --> 07:41.480
voice support calls what all how all does your or how do you generate your data set.

07:41.480 --> 07:46.160
So the the chat of course that's through the chat channel and that's that's tax that's

07:46.160 --> 07:51.600
written between you know the conversation between an agent and a member and then the actual

07:51.600 --> 07:58.360
call is a transcription so the call audio is transcribed to text and so we would be

07:58.360 --> 08:04.200
able to obtain that data which when we get it we it's redacted so it's all any kind

08:04.200 --> 08:08.080
of personal information or identifiable information or confidential information of our

08:08.080 --> 08:12.120
members is removed and then we'll be able to run topic modeling on that to be able

08:12.120 --> 08:14.240
to gain those insights.

08:14.240 --> 08:18.160
What approach is to topic modeling do you use.

08:18.160 --> 08:23.680
So the approach or the methodology that I use mainly is non-negative what's called

08:23.680 --> 08:30.880
non-negative matrix factorization so traditionally within the area of topic modeling there has

08:30.880 --> 08:36.560
been mainly like three methods which what the first method is is one of the original methods

08:36.560 --> 08:43.600
which is latent semantic indexing or lean semantics analysis and what that method does it takes

08:43.600 --> 08:48.120
a document term matrix so you can imagine these very large and by the way when I say

08:48.120 --> 08:55.840
document what I'm meaning is a any kind of unit of text under analysis it could be a it

08:55.840 --> 09:02.560
could be one chat it could be a chat or it could be a a call transcript it could even be

09:02.560 --> 09:09.040
a tweet or or some other kind of one structure data but so we're take you can imagine this

09:09.040 --> 09:15.040
document term matrix and along the say rows are all these documents along the columns

09:15.040 --> 09:21.200
are all these terms so there's different methods that you can use to decompose that matrix

09:21.200 --> 09:26.000
and to other matrices and be able to gain insights and like I mentioned the one of the original

09:26.000 --> 09:32.600
methods is latent semantic indexing and what that does it uses singular value decomposition

09:32.600 --> 09:39.680
to decompose that document term matrix into three other matrices and so but the criteria

09:39.680 --> 09:47.200
with that method is that it's creating or the matrices that are decomposed into our orthogonal

09:47.200 --> 09:55.600
vectors within each of those matrices and so that is a concern because a conversation is usually

09:55.600 --> 10:01.360
includes more than one topic you know we could be talking you know about hurricane Florence but

10:01.360 --> 10:05.760
we could also be talking about other topics involved as well and we want to be able to distinguish

10:05.760 --> 10:12.160
those type of topics and with latent semantic indexing it usually has an issue doing that because

10:12.160 --> 10:20.160
of the orthogonality in the in the matrices that it decomposes into another method is before we

10:20.160 --> 10:28.480
continue does that suggest that this latent semantic indexing might be better used for smaller

10:28.480 --> 10:36.400
documents like a tweet or an individual chat message as opposed to an entire transcript of a

10:36.400 --> 10:42.880
call or an entire chat interaction it could be a tweet it could but it's usually it's probably

10:42.880 --> 10:49.760
best applied when you don't have overlap within a given of different topics within a document

10:49.760 --> 10:53.680
which would be sort of it could be a tweet because you're restricted to the number character

10:53.680 --> 11:00.240
so you're usually talking about one topic is that kind of inherent to the the document or

11:00.960 --> 11:08.960
to the set of classes that you are looking to associate these documents with for example

11:08.960 --> 11:19.120
or a meaning if I've got a news site and I've got and I have like some you know strictly delineated

11:19.120 --> 11:26.320
like a set of like categories of news like you know health and finance and sports that are

11:27.120 --> 11:35.200
you know orthogonal does that mean that I can use LSIers it more have more to do with the content

11:35.200 --> 11:40.560
of the documents themselves it is usually it's probably more to do with the content of the document

11:40.560 --> 11:48.880
themselves themselves because you could you could be talking about if you had like a large corpus of

11:48.880 --> 11:56.240
tweets and let's suppose that they're talking about world cup and and the Olympics if you take all

11:56.240 --> 12:03.920
of those tweets and you just put them into one bucket then the latent semantic indexing might

12:03.920 --> 12:11.040
do okay with the the tweets since they may have be talking about different or the different types

12:11.040 --> 12:17.600
of topics and it may be able to distinguish them better than a say a call transcript that could

12:17.600 --> 12:24.080
include multiple different topics and so it makes sense. Just taking a step back these

12:25.200 --> 12:32.720
I'm thinking about the example that I gave where I'm trying to put documents into topics and that's

12:32.720 --> 12:37.760
a different kind of a problem that's more of a categorization type of a problem whereas what we're

12:37.760 --> 12:42.880
doing what you're we're talking about here with topic modeling is like an unsupervised learning

12:42.880 --> 12:50.000
like you just have the data and you're trying to identify what the topics are in the data itself

12:50.000 --> 12:58.240
is that is that right? Absolutely and you can think of it as a form of soft clustering okay so with

12:58.240 --> 13:05.120
the key means clustering for example that's a form of hard clustering where it's actually taking

13:05.120 --> 13:13.360
a specific document and categorizing it within a specific cluster but because we know that a document

13:13.360 --> 13:19.440
could include more than one type of conversation or topic or theme we want to we don't necessarily

13:19.440 --> 13:27.360
want a hard place that do a hard clustering to place that document within one specific cluster

13:27.360 --> 13:34.160
what we would rather do is see how well see how many topics are existing when that document

13:34.160 --> 13:40.480
and then say what is the probability that this document is associated with say topic number one

13:41.200 --> 13:46.640
what's the probability associated with topic number two etc so that's it's more of a soft clustering

13:46.640 --> 13:53.200
with is what topic modeling will do for you okay all right so latent semantic indexing

13:53.200 --> 13:58.400
it's kind of a traditional way that folks might approach this but it's got this limitation

13:58.400 --> 14:05.920
because it enforces that these terms be orthogonal to one another so it doesn't perform while when

14:05.920 --> 14:13.040
your documents have multiple topics in them yes absolutely and so another couple other methods

14:13.040 --> 14:17.440
that work pretty well is one of traditional method it's called latent dirtual allocation which is

14:17.440 --> 14:25.600
more of a probabilistic type method based on the dirtual distributions and it does pretty good

14:25.600 --> 14:34.240
but what we have found is that because it uses Gibbs sampling which is more of a random selection

14:34.240 --> 14:42.880
of the terms and the documents we see a tendency that the topics results vary from one run to another

14:42.880 --> 14:50.160
and so that's where I sort of lean more towards the third method which is non-negative matrix

14:50.160 --> 14:56.960
factorization and what that does it takes the original document term matrix and decomposes it

14:56.960 --> 15:03.840
into two other matrices one being a document topic matrix and a topic term matrix presumably

15:03.840 --> 15:11.760
because this isn't using Gibbs sampling your results given a particular corpus are more repeatable

15:11.760 --> 15:18.640
yes yes so with the non-negative matrix factorization what it's doing is it's more of an optimization

15:18.640 --> 15:25.120
process so what it's doing is decomposing that original document term matrix into the two other

15:25.120 --> 15:32.560
matrix matrices and the goal is to minimize the error between the original matrix and the product

15:32.560 --> 15:38.160
of the other two and it's pretty repeatable I mean it does a pretty good job at providing good

15:38.160 --> 15:47.920
results and also that you can run over continuously run and now when I hear matrix factorization I hear

15:47.920 --> 15:57.440
hard does that is that necessarily the case or you know maybe hard to scale is maybe more

15:57.440 --> 16:05.120
particular do you find that to be the case or are there methods for doing this matrix factorization

16:05.120 --> 16:13.600
that are you know computationally tenable at large scale yeah that so large scale it does pretty

16:13.600 --> 16:21.760
good is you know with using very large corpus sizes I guess the challenge can be when you start

16:21.760 --> 16:27.600
getting it running it on like near real-time data online data okay and and that can be a challenge

16:27.600 --> 16:33.680
but there are different methods that can use the original topics and then do some kind of

16:33.680 --> 16:38.080
resampling of the new topics or new documents that are coming in and integrating them in

16:39.360 --> 16:48.240
so scalability to large corpus isn't really actually a big deal and you know the time it

16:48.240 --> 16:54.640
takes to do this can be a big deal but there are some methods to overcome that yes yes

16:56.000 --> 17:02.080
okay so interesting so you know we've talked about this document term matrix as being the

17:02.080 --> 17:08.000
starting place for you know all three of these topic modeling methods but we haven't talked about

17:08.000 --> 17:15.760
how we even get to that how we represent the documents in this term matrix that's there are

17:15.760 --> 17:22.800
different ways to do that right sure so there's a there's a pipeline for this process and so what we

17:22.800 --> 17:29.840
do is we want to do some kind of pre-processing so we may want to of course tokenize the documents

17:29.840 --> 17:39.200
so each term is a single dimension but we also want to remove things like the stop words so the

17:39.200 --> 17:46.560
common terms like a the high frequency terms that may not be able to give us give us very good insights

17:46.560 --> 17:53.840
with the topics or the subjects so we remove those as well and then we may want to also apply some

17:53.840 --> 18:00.320
dimensionality reduction techniques so there's of course when you're working with a large corpus

18:00.320 --> 18:06.480
of documents the dimensions can be very large and that interferes with the processing time as well

18:06.480 --> 18:12.320
so there's dimensionality reduction techniques that we can apply what I have done in the past is

18:12.320 --> 18:20.560
you know do a single phase or stage where I'm running topic modeling to help reduce the

18:20.560 --> 18:27.920
dimensions across all the the documents as a pre-processing step as well and so what's an example

18:27.920 --> 18:38.400
of a dimensionality reduction technique that you might apply to this document term matrix so what

18:38.400 --> 18:45.360
I what I do usually as far as a dimensionality reduction technique is I will run

18:45.360 --> 18:53.520
non-negative matrix factorization on the entire corpus using two techniques one one is term

18:53.520 --> 19:01.360
frequency forming a term frequency matrix and the other frequent or method is using term frequency

19:01.360 --> 19:06.960
inverse document frequency and so term frequency what that's really doing is forming the original

19:06.960 --> 19:13.600
document term matrix that based on the frequency of terms whereas term frequency inverse document

19:13.600 --> 19:19.840
frequency is is looking at the frequency of terms within each document but also looking at the

19:19.840 --> 19:26.160
the frequency of a term across all the documents so it's sort of like offsetting so a very high

19:26.160 --> 19:33.520
frequency term that is existing across all the document may not receive as much weight as if

19:33.520 --> 19:39.440
it's high frequency but only occurring across a few documents and so what I'm going to do is I'm

19:39.440 --> 19:48.320
going to run the non-negative matrix factorization on the entire corpus at first and get and reduce my

19:48.320 --> 19:57.680
vocabulary size from say hundreds of thousands of terms to possibly down to a few hundred or a

19:57.680 --> 20:06.000
thousand terms and that is just the pre-processing and then I can go ahead and run get more refinement

20:06.000 --> 20:13.520
running topic modeling further on on those documents so how does doing your matrix your

20:13.520 --> 20:24.320
non-negative matrix factorization on the larger document term matrix gets you to reducing the

20:24.320 --> 20:32.320
number of terms so the it goes back to the original decomposition so the non-negative matrix

20:32.320 --> 20:40.400
factorization is decomposing the original document term matrix into the two other matrices which are

20:40.400 --> 20:47.520
the document topic matrix and the topic term matrix and so what it's going to do is it's you can

20:47.520 --> 20:56.080
reduce the number of terms based on the the number of topics so if you have so what it's going to do

20:56.080 --> 21:04.080
is if you look at the topic term matrix it's giving you the strength of association between

21:04.080 --> 21:11.200
the topics in the terms where so that if you have a term that has a higher rate it has a

21:11.200 --> 21:16.000
more of a higher strength of association with a given topic so if I say look at the

21:16.000 --> 21:26.320
top 100 terms within a given topic and just consider those terms as part of my vocabulary

21:26.320 --> 21:31.040
that I'm going to use then that will give me some kind of reduction dimensionality reduction

21:31.040 --> 21:38.240
as opposed to using all of the terms if you're looking at say these top 100 terms on a for a given

21:38.240 --> 21:46.560
topic are you then kind of zeroing out terms back in your document term matrix and kind of

21:46.560 --> 21:52.320
creating more of a sparse matrix or are you changing the shape of that matrix based on this new

21:52.320 --> 21:58.400
information that you have so it sort so it does change the shape so what it's doing is I'm going to

21:58.400 --> 22:07.200
be my initial run with the non-negative matrix factorization is going to create a new document term

22:07.200 --> 22:13.920
matrix but only include those terms that will have the highest association between that I had

22:13.920 --> 22:22.400
originally from the the preprocessing run with topics so it's what it's really doing is it's given

22:22.400 --> 22:32.400
me terms that are more relevant when I run subsequent topic modeling so just to kind of pull on this

22:32.400 --> 22:37.520
one you know one more step so you've got these terms you zero out you know say you've got 100,000

22:37.520 --> 22:44.160
terms and you've got well how many topics would be typical in your case so that's a good question

22:44.160 --> 22:50.320
actually that's a that's a sort of an open problem the number of topics but I do have we have a

22:50.320 --> 22:57.760
technique that we run it's to compute what is a coherent score interpretability of the topics

22:57.760 --> 23:04.240
and what that that running the coherence will do is allow us to identify sort of like the optimal

23:04.240 --> 23:10.720
number of topics that we need to run and so is this on the order of tens or hundreds or thousands

23:10.720 --> 23:18.720
for you know a you know a corpus of documents that well maybe give us kind of general numbers for

23:18.720 --> 23:24.320
each of these things how many documents do you tend to have tend to see you know rough order

23:24.320 --> 23:31.280
magnitude how many terms how many topics oh so it could be I mean we could easily be running

23:31.280 --> 23:40.400
say 50,000 documents through the through the topic modeling and in one run it could be more

23:40.400 --> 23:46.800
but usually around 50,000 the number of terms of course you know that's it's going to vary I mean

23:46.800 --> 23:54.080
if we're running creating a topic term matrix on a call conversations then you know the calls

23:54.080 --> 24:00.320
can vary from you know a couple minutes to many minutes so the size of the corpus is can vary

24:00.320 --> 24:06.800
as well or the documents can vary as well but it's all it's all I mean the only limitation of course

24:06.800 --> 24:14.240
is the the processing speed you know to that we're working with when you are identifying the terms

24:14.240 --> 24:24.720
that are most meaningful is that on a topic by topic basis or is that across the corpus specifically

24:24.720 --> 24:28.720
I'm asking about this this step where we're doing the dimensionality reduction that's so much

24:28.720 --> 24:34.880
like the meaning of the topic itself and the the coherence um specifically when we're looking

24:34.880 --> 24:40.240
doing that dimensionality reduction and trying to get from our 100,000 terms down to 100

24:40.240 --> 24:47.360
is that on a per topic basis or is that are you looking at the 100 terms that have the most

24:48.560 --> 24:55.840
weight across all topics does that question make sense yeah so the the final vocabulary is based

24:55.840 --> 25:03.520
on the the terms across all the topics that were used in the pre-processing phase okay so you've

25:03.520 --> 25:15.440
got these uh you identify these hundred say terms that uh most strongly correlate to these topics

25:15.440 --> 25:20.480
that you've identified and that kind of answered my question I guess that I've been trying to get to

25:20.480 --> 25:26.400
the long way around if you're doing it across the all of the topics then you can just kind of get

25:26.400 --> 25:31.600
rid of all the other terms in your document term matrix as opposed to if you were doing it on a

25:31.600 --> 25:39.040
topic by topic basis then you still have kind of this potentially large sparse matrix yes okay

25:40.480 --> 25:47.280
and and so this this resulting set of vocabulary in this pre-processing that we just ran based on

25:47.280 --> 25:55.440
this um um topic modeling to identify the the top terms across all topics across the entire

25:55.440 --> 26:01.440
corpus this vocabulary says what's going to be used in the subsequent topic modeling and that's the

26:01.440 --> 26:08.560
dimensionality reduction you mentioned tf idf and and term frequency how do those play into here again

26:09.600 --> 26:15.680
so in the in the subsequent steps I'm just going to be using ti tf idf um there's really

26:16.480 --> 26:21.600
so in the pre-processing when I do the initial run for non-negative matrix factorization I run

26:21.600 --> 26:26.080
usable techniques to identify the vocabulary the term frequency and the term frequency inverse

26:26.080 --> 26:32.800
document frequency um but once I identify the vocabulary in the subsequent steps I'll just be running

26:32.800 --> 26:39.280
the the tf idf which the difference between the two methods is that the term frequency it's looking

26:39.280 --> 26:48.080
at it's really assigning a equal weight to each term and so it um it's it's looking at the how

26:48.080 --> 26:54.560
for a given term how frequent that term occurs across the entire corpus but really there's no

26:54.560 --> 27:02.000
difference on how it's applying the the weight that weight value to term a as it is to term b but

27:02.000 --> 27:09.200
you may have a term that it's a high frequency not only in a given document but a very high

27:09.200 --> 27:16.800
frequency across all the documents so that specific term may not really give you much insights on

27:16.800 --> 27:23.440
the topic of the conversation so the other method which is term frequency inverse document frequency

27:23.440 --> 27:29.680
sort of offsets that it'll look at not only the frequency of the terms uh within a document but

27:29.680 --> 27:35.840
also how frequent that frequent that term occurs across other documents so it will give a high value

27:35.840 --> 27:42.080
when terms for example occurs many times within a small number of documents and that's what I

27:42.080 --> 27:48.320
they're using the subsequent topic modeling as well uh then you mentioned the coherence score

27:49.280 --> 27:58.720
what does that help you get to so you can think of coherence as a measure that captures the

27:58.720 --> 28:05.120
semantic interpretability of topics and it's it's based on the co-occurrence of topics or topic

28:05.120 --> 28:11.600
words and where that's important is that when you when the topic model or say non-negative matrix

28:11.600 --> 28:18.160
factorization is creating these categories consisting of these topic categories that are

28:18.160 --> 28:24.400
consisting of terms we want the analyst to be able to look at those topics and say oh I get it

28:24.400 --> 28:31.120
I understand what that topic is about and that starts getting into interpretability and you want

28:31.120 --> 28:39.680
to be able to generate a coherence score that is going to provide you a good semantic interpretability

28:39.680 --> 28:50.240
of the topics and that's it's that sort of like coherence score is a function of the number of topics

28:50.880 --> 28:58.400
and so what we have done is um identify the number of topics that will maximize the average coherence

28:58.400 --> 29:08.400
across the corpus and what's the intuition for kind of the the math you know the score itself what

29:08.400 --> 29:17.760
the coherent score is saying so it's based on co-occurrence of words so there's there's a few methods

29:17.760 --> 29:28.160
that are are used but what it's doing is it's it's looking at the counts of the number of documents

29:28.160 --> 29:36.960
containing like a set of terms words and it's using that and it's also taking consideration

29:36.960 --> 29:46.080
the the logarithm of that of that count and so it's it's you the closer for the specific coherent

29:46.080 --> 29:54.160
score um I thought I'm using the closer that value is to zero the more coherent your topic is

29:54.800 --> 30:01.440
and the more negative the value is the less coherent it is and so what we want to be able to do is

30:01.440 --> 30:09.600
identify the one number of the topics that will maximize coherence and then that's the the

30:09.600 --> 30:14.560
number of topics that we'll use and and be able to compute the the topic categories and also the

30:14.560 --> 30:21.760
coherent scores for each topic category okay and that number of topics that maximizes coherence

30:21.760 --> 30:30.000
is going to be ultimately based on the relationships between these terms in the given documents

30:30.000 --> 30:35.680
in the corpus yes the co-occurrence yes the relationship between terms okay do you have any kind

30:35.680 --> 30:42.560
of rules of thumb that you found for I guess it really depends a lot on the the types of documents

30:42.560 --> 30:49.280
but for Isaiah kind of a standard you know say for call transcripts do you have any rules of thumb

30:49.280 --> 30:55.040
where it you know it ends up being on the order of you know hundreds of hundreds of topics as

30:55.040 --> 31:01.120
opposed to you know tens or thousands so I've actually I mean this is probably surprising but I've

31:01.120 --> 31:09.360
actually a rule of thumb is between five and eight topics is a pretty good value to get

31:09.360 --> 31:15.360
into good interpretability if you start getting larger than that number of topics then you start

31:15.360 --> 31:21.840
losing the coherence how does it fail like are there patterns in the way it fails like does it pick

31:21.840 --> 31:30.640
up words that you know just aren't meaningful in the sense of you know not particularly additive

31:30.640 --> 31:39.680
to understanding what the topic is of a given document or is it trying to you know split topics

31:39.680 --> 31:44.960
to you know find you know find grain so there are words that are kind of similar that should be

31:44.960 --> 31:52.400
one topic like is there a GC patterns in the way it tends to fail if you have too many topics I

31:52.400 --> 32:00.080
think what it will try if you have too many topics what it will try to do is spread yes spread those

32:00.080 --> 32:10.320
terms across more more of those topic categories and therefore you start losing the interpretability

32:10.320 --> 32:16.400
but you know that when you when you do a topic modeling I mean it goes back to the the

32:16.400 --> 32:21.360
purpose of it you know it's doing soft clustering so it's it's definitely possible that you're

32:21.360 --> 32:31.440
going to see a term existing in more than one topic category you know for example if I if I

32:31.440 --> 32:37.840
have seven documents and you know four of the documents are talking about financial banks and

32:37.840 --> 32:44.800
three of the documents are talking about river banks more likely you're going to see the word bank

32:45.360 --> 32:52.800
across those all seven or all all the topics but you're probably going to be able to still be

32:52.800 --> 32:58.480
able to distinguish the topics because you're going to see things that are talking about river

32:58.480 --> 33:03.680
banks and certain topics and you're going to see things that are terms that are associated with

33:03.680 --> 33:10.320
um financial banks and other topics and the reason is is because it's based topic modeling is

33:10.320 --> 33:16.640
based on co-occurrence of terms so you're going to see a lot of um commonality of terms that are

33:16.640 --> 33:23.600
co-occurring together and those given documents placed in a specific topic and it and it does

33:23.600 --> 33:31.680
pretty good it does really good is there a way to identify what the primary topic of a given

33:31.680 --> 33:38.080
document is is that just kind of ranking the the topics based on some score probability

33:39.360 --> 33:43.760
so that that's a very good question and that's actually one of the purposes of doing the topic

33:43.760 --> 33:50.960
modeling as well so if you go back to the the original decomposition it's taken that document

33:50.960 --> 33:59.680
termatrix and decomposing it into a document topic matrix and also the topic term matrix so the

33:59.680 --> 34:05.600
the topic termatrix is giving you the strength of association between each of the terms and a given

34:05.600 --> 34:13.200
topic so that if I say I take the top 10 terms for a given topic which top 10 terms being the ones

34:13.200 --> 34:17.040
that have the highest weight of association with a given topic you know that gives me information

34:17.040 --> 34:22.960
about what that topic or what is the theme of that given topic but the other matrix the document

34:22.960 --> 34:30.000
topic matrix what that is doing is giving you the strength of association between each topic and

34:30.000 --> 34:36.880
a given document and where that's important is it goes back to a conversation so if you and I

34:36.880 --> 34:41.600
are having a conversation we're probably going to be talking about more than one topic but we

34:41.600 --> 34:47.040
may but want to identify well what's the primary topic and that's where that that document topic

34:47.040 --> 34:53.280
matrix is important because it will allow us to identify the topic that has the highest value or

34:53.280 --> 34:58.800
weight which is the highest strength of association between that topic and that that given document

34:58.800 --> 35:05.280
and I can look across a given corpus and run topic modeling on open to transcripts across a given

35:05.280 --> 35:12.640
corpus and I can be able to identify not only the topics of conversations across all the documents

35:12.640 --> 35:20.800
but also I can identify which documents had topic one as the primary topic of conversation.

35:21.520 --> 35:26.720
You've done this factorization you've got this document topic matrix and this topic term

35:26.720 --> 35:34.240
matrix can you then take this topic term matrix and kind of use it as a model that you can run

35:34.240 --> 35:42.240
inference against when you have a new document you kind of put that through your your term pipeline

35:42.240 --> 35:48.400
to get the terms in that document and then you kind of apply you multiply it by this topic term

35:48.400 --> 35:56.560
matrix to get the the topics in that document. So it's sort of like subsequent modeling like subsequent

35:56.560 --> 36:03.920
right right so trying to get the topics in a document that you didn't or maybe the the question

36:03.920 --> 36:10.080
that you know the question before this is does this topic term matrix tend to have strong predictive

36:10.080 --> 36:18.480
value for documents that don't exist within the corpus that it was created on is is not like a

36:18.480 --> 36:24.240
technique like a supervised learning where you're built into the process you're testing against

36:25.280 --> 36:32.560
data that you haven't built your model on. Yeah so I what you can do is so the non the topic

36:32.560 --> 36:38.880
modeling is definitely an unsupervised machine learning method and but you can think of it as being

36:38.880 --> 36:48.480
a precursor to a supervised method so you could really take the top the topic term matrix that is

36:48.480 --> 36:54.800
produced from this non-negative matrix factorization and look at each one of those topics that are

36:54.800 --> 37:02.160
as an output and and label them you can provide a label for each of the topic and then then create a

37:02.160 --> 37:13.120
supervised model say with a deep learning model that is able to classify a new call into one of

37:13.120 --> 37:21.520
those topic categories and then if you say go below a given threshold of probability perhaps

37:21.520 --> 37:27.280
maybe it's a new topic and you might want to put that new topic or that document associated with

37:27.280 --> 37:33.280
that one it fell below a threshold into a bucket and rerun topic modeling on it so it's sort you

37:33.280 --> 37:40.160
can almost think of this process as a feedback you're you're learning new topics and integrating them

37:40.160 --> 37:44.880
into a new model updating the model and then that model is making supervised models making predictions

37:44.880 --> 37:52.000
and then if it isn't able to predict a new document based on a given probability it puts it into

37:52.000 --> 37:59.440
a bucket and reruns topic modeling on it given that you have this limitation of five to eight

37:59.440 --> 38:09.120
topics in order to achieve interpretability or coherence is there is there a process through which

38:09.120 --> 38:15.360
you might you know if you've got a large corpus of say several hundred thousand documents

38:15.360 --> 38:23.440
or even a you know this fifty thousand document corpus cluster your documents or partition your

38:23.440 --> 38:31.680
documents and create separate topic models and so as to maximize the number of coherent topics that

38:31.680 --> 38:38.800
you end up with yeah I think you could you could definitely do as a one of the initial steps

38:38.800 --> 38:46.480
you could create rules that would separate the documents look based on the structure data the

38:46.480 --> 38:53.520
metadata and you know you you might be able to for for example calls you know you if the calls

38:53.520 --> 39:01.920
are going to the bank within the bank you know they might be going to deposits or or checking

39:01.920 --> 39:08.080
or something like that so as an initial step you might be able to categorize those calls and then

39:08.080 --> 39:15.600
runs separate topic modeling on each one of those categories you might be able to do really

39:15.600 --> 39:20.400
just it's a matter of looking at the structure data before you would separate them and then

39:20.400 --> 39:24.640
once you got those bucket of categories run topic modeling on them so you would have different

39:24.640 --> 39:29.360
topic models so what you're suggesting is as opposed to trying to do it in a machine learn kind

39:29.360 --> 39:36.560
of way take advantage of the the metadata if you will that's inherent in your problem to partition

39:36.560 --> 39:44.480
up your documents into you know I guess there's a sweet spot right a small a small of a

39:44.480 --> 39:51.760
corpus as possible but that still allows you to create a robust topic model absolutely you really

39:51.760 --> 39:56.640
what you're working with is semi structure data you know you're you're working with the structure

39:56.640 --> 40:02.080
data in the metadata and also of course the unstructured data which is the text and you're

40:02.080 --> 40:10.080
really combining the two to be able to do the topic modeling effort so you do this topic modeling

40:10.080 --> 40:19.440
you get these topics you know five to eight topics from a corpus of 50,000 transcripts or chat

40:19.440 --> 40:27.200
call or chat transcripts how do you then integrate these back into the call center operations to

40:27.200 --> 40:36.800
improve them so for it goes back to the example I used if we're identifying topics that are

40:36.800 --> 40:44.800
emerging before we reach a peak call volume what we're able to do is be more proactive with the

40:44.800 --> 40:52.320
topics we may be able to create better or inform our agents of these these certain topics that are

40:52.320 --> 40:59.520
coming in so they can be more prepared to mitigate any concerns or better serve our members

41:00.800 --> 41:07.840
we we may be able to even make adjustments if there's if there's confusion about something that's

41:09.200 --> 41:17.920
say on the mobile channels you know either dot com or or mobile we may able to make adjustments

41:17.920 --> 41:22.960
on those on those those different channels because we're starting to see topics and they're

41:22.960 --> 41:30.560
associated with those and then that also in turn could decrease any call volumes associated

41:30.560 --> 41:37.520
with those concerns but it's really being trying to be more proactive to better serve our members

41:37.520 --> 41:44.400
what rather than you know reacting after the fact and do you find it the initial steps of this

41:44.400 --> 41:51.360
process where you're doing dimensionality reduction end up being at odds a bit with trying to use

41:51.360 --> 41:57.600
this as kind of an early warning system because they get rid of emerging terms that might not have

41:57.600 --> 42:07.840
yet achieved some kind of critical mass no not really what I think the how we approach it it's

42:07.840 --> 42:17.680
able to identify the most significant terms and it's able to capture the most relevant topics

42:18.800 --> 42:23.840
it does a pretty good job it's able to surface the things that we didn't even know you know I

42:23.840 --> 42:28.160
go back to the unknown unknowns you know the things we don't know what we don't know you know it's

42:28.160 --> 42:34.480
able to surface those things so we're able to take action on them awesome awesome are there any open

42:34.480 --> 42:41.920
challenges or problems that you see recurring as you try to use these types of models you know I

42:41.920 --> 42:48.800
would say there's definitely open problems I think that choosing the number of topics is still

42:48.800 --> 42:54.640
an open problem I mean I mentioned that we you know use the coherence score to try to maximize

42:54.640 --> 42:59.360
coherence you what is the number of topics that maximize coherence I still think that's an open

42:59.360 --> 43:08.080
problem an appropriate corpus size I think that's still an open problem as well as the the document

43:08.080 --> 43:13.120
term matrix you know there's there's different weights that are used in a document term matrix such

43:13.120 --> 43:19.440
as the term frequency and term frequency inverse document frequency but I think there's more room

43:19.440 --> 43:26.080
for developing new weights and doing more research in that area and then of course you know I talked

43:26.080 --> 43:31.760
about coherence you know coherence is the interpretability of topics but I think also

43:32.720 --> 43:38.000
determining the relevancy of topics is really important because you know something could be

43:38.720 --> 43:45.040
interpretable but is it really relevant and we need to take action on it well Bill thanks so much

43:45.040 --> 43:50.080
for taking the time to chat with us about this I'm sorry that you didn't get a chance to present

43:50.080 --> 43:55.840
this as stratum sure folks would have enjoyed it but it's really interesting stuff well thank you

43:55.840 --> 44:05.280
Sam once again thank you for inviting me absolutely all right everyone that's our show for today

44:05.280 --> 44:11.920
for more information about today's show visit twimmel ai.com be sure to visit twimmelcon.com for

44:11.920 --> 44:18.160
information or to register for twimmelcon ai platforms thanks again to c3 for their sponsorship

44:18.160 --> 44:25.200
of today's episode to check out what they're up to visit c3.ai as always thanks so much for

44:25.200 --> 44:27.200
listening and catch you next time

