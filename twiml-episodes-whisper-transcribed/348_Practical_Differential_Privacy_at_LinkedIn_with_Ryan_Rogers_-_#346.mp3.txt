Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
Before we get going, I'd like to send a huge thanks to LinkedIn for sponsoring today's show.
LinkedIn Engineering solves complex challenges at scale to create economic opportunity for every member
of the global workforce. AI and ML are integral aspects of almost every product the company builds
for its members and customers. LinkedIn's highly structured data set gives their data scientists and
researchers the ability to conduct applied research to improve member experiences. To learn
more about the work of LinkedIn Engineering, visit engineering.linkedn.com slash blog. And now onto the show.
All right, everyone. I am here at the 33rd NURPS in Vancouver and I am with Ryan Rodgers. Ryan
is a senior software engineer in data science at LinkedIn. Ryan, welcome to the Tumel AI Podcast.
Yeah, thanks so much for having me. I'm a huge fan of the show.
Awesome. Awesome. I am glad to glad to hear that and glad to have you here. You're pretty active
in federated learning, differential privacy. That's in fact what we're going to be talking about.
You know, tell us about your background and how you got interested in those topics.
In my grad school during my PhD, I was very interested in sort of the fields of
game theory with computer science. And I was working with Michael Kerns and Aaron Roth there.
And I started learning more and more about differential privacy from Aaron Roth. He wrote
the textbook on it. And so he was a great teacher and we started working a lot on the topic.
And then toward the end of my PhD, like the last year, Apple announced that they were doing
differential privacy in iOS 10. So that was like really exciting to see that like, wow,
the research that I'm doing can be actually useful to companies that are interested in privacy.
So it seemed like a logical next step to just after my PhD to go immediately to Apple.
And it kind of continued some of the research that I had done during my PhD, but now at Apple.
And so they're kind of worked on what's called the local model of differential privacy where
any records that are submitted are privatized on device prior to being sent to Apple in that case.
So use cases like, you know, finding popular emojis that people are typing, popular new words,
that people are typing on the keyboard, things like that. And then toward the end of my time at Apple
started exploring more private federated learning, local differential privacy in that case,
machine learning tasks. And then I started learning about more and more about some of the work
that's happening at LinkedIn with differential privacy. And it was really exciting to me.
It was like kind of a different model of differential privacy that was going on there.
Something that's called the central model of differential privacy where data is collected and
things that we want to do with the data set is privatized. So rather than in the local model,
which can be restrictive in some settings, the global model can kind of open up a whole new
class of problems that you can do. So that was really exciting to me. So after about two years
at Apple, I then moved to LinkedIn. And that's where I am now working on differential privacy there.
Awesome. Awesome. Yeah. We first started covering differential privacy on the podcast, maybe a couple
of years ago with a series of shows, which included Aaron. And so much has happened since then.
I think it's like the census is now rising differential privacy. So many more organizations
are talking about how they're incorporating it into kind of the way they do business
whereas before differential privacy and in particular, differentially private machine learning.
Oh, and also there's a TensorFlow library now. Yeah. I hope it stores projects now.
A lot of open source stuff. Exciting time to be in the field. Yeah. So in 2016, when Apple
announced they were doing differential privacy, that spiked interest in the area. And now,
like Census Bureau announcing that they're doing differential privacy is even more recognition
into the area. So it's really important to get practical things to be differentially private
and for more people to get engaged with the topic. Yeah. Yeah. So you have a spotlight talk here
at the conference on paper that you contributed to practical, differentially private top-case
selection with pay what you get composition. Yeah. Yeah. I feel like we can spend the interview
like just dissecting what all those words mean in this context. Right. Yeah. What's the best way
to go through that? So yeah. This was a really, really fun project with my colleague at LinkedIn,
David Durphy. So when we first started working on a project at LinkedIn, we wanted to kind of
use them off the shelf techniques from differential privacy. It's been an active research field for
close to 15 years. So we were kind of handling a very basic task like what are the top 10 articles
that members are engaging with across LinkedIn. And so for that type of query, we were just like,
okay, well, we can take existing algorithms and use that and it'll be differentially private.
Unfortunately, these existing techniques require you to have extra information about what you're
searching for. For instance, if I want to know top 10 articles that are being engaged with among
all data scientists in the Bay Area, I would need to also provide all the possible articles
that are actually out there, which is kind of contradictory to how a data scientist would
actually want to run that query. You would want to ask the data set, hey, what are the top 10
articles? Because I don't know what the articles are. As opposed to which of these
many articles? Yeah. Exactly. So my algorithm would have to know all the possible articles that are
out there, which could be millions. Maybe I don't even know about it, without all the possible articles.
You know, you would presume that those articles are in the data set is the requirement to know
all of those in the data set is that the issue there that it breaks the privacy aspect of it itself.
Exactly. So like even articles that nobody clicked on in differential privacy, you would have to
think of this hypothetical world, this hypothetical data set, where one new member might be added to
the data set, which might actually click on these other articles that didn't exist in the real
data set that you're running things in. So only in the analysis to prove that it's actually
differentially private, do you have to consider like all possible articles that can show up?
Okay. So that same contradictory to like how data science is one to work with it. So we're like,
okay, there's got to be a better way of doing this. More of like a black box approach. Ask
top 10 articles, top 10 skill sets among these users, something like that. You wouldn't have to feed it
also. Like here are all the articles that you should search for. Here are all the skill sets to look
for. So we're talking about an approach. The essence of this is that the differential privacy
at its core is providing a theoretical guarantee of privacy, but all the theory in these theoretical
guarantees was based on this having every the knowledge of everything. Exactly. And in practice,
that's in practical. Exactly. Yeah. So whenever you would like to run these things, like you just
want the data set to tell you these sort of things, sort of like exploratory data analytics.
So before like, yeah, you would have to give exactly those things beforehand. But then with these
new algorithms, you can just ask top 10 articles, top 10 countries, top 10 skill sets, and not have
to provide, oh, here are all the articles, here are all the countries, here are all the skill sets
that you need. And furthermore, still have your differential privacy guarantees. It's not that you
couldn't ask your database, you know, select top 10 from whatever it's that you can still get
your differential privacy. Yeah. Subjected differential privacy is always the constraint with these
these issues. Yeah. So the practical aspect of these algorithms are that you don't have to feed it
the what we call the domain of the data set into these algorithms. And they could work on top of
existing infrastructure, say that you already have built at your company some very efficient
top-cave solver, but you didn't we weren't considering differential privacy to begin with.
So you're taking this highly distributed data set that's located in lots of different areas,
and you built this system that can run queries incredibly fast. So now when you want to introduce
differential privacy, you don't want to just totally bypass this existing infrastructure.
Right. You want to be able to leverage that still with all of your queries. So that's where we
wanted to apply our differential privacy algorithms, sort of as a layer on top of this existing
infrastructure. So we wouldn't slow things down. You could still get the efficiency that you asked for
while still getting privacy guarantees. So that's why what was so appealing about these
algorithms, like the mathematics behind it is incredibly difficult, but the algorithms themselves
are fairly straightforward. So in essence, like you would ask a top-k problem, and rather than
asking top-k from the original existing system, you would ask like maybe top 2k, maybe a little bit
more fetch a little bit more data, get those results, and then add noise to those results,
and only release at most k results from that. And so that's essentially the algorithms that we
propose, but proving that it's differentially private is a lot more difficult.
It's surprising that your multiplier just needs to be 2 or on that order.
Yeah, so that's kind of a parameter that you can tune. So depending on how much slower your
computation be while still being usable. So ideally, you would just go to this as existing
infrastructure and say, give me all the data. But that might not be practical in some sense,
because we think of the problem as privacy as a constraint to the problem, but also run time,
also data storage. So we want to be mindful of all those things in terms of a practical
system. So if you can run things, if things can take a little bit longer, go and fetch as much
data as you possibly can. But if you can't, then maybe just a factor of 2 is enough, but utility
will improve if you fetch more results. And so then the paper is primarily establishing this,
this kind of the relationship between the number that you're requesting and this multiplier
and your ability to still guarantee differential privacy. Yeah, so there are kind of two components
to the paper. First is here are some algorithms that we consider very practical that you can put
a layer on top of existing systems and get differential privacy. It might slow things down a
little bit because we have that extra parameter of fetching more data if you need. The other aspect
of it is we also provide a way to sort of budget how much privacy you're allowing if somebody can
interact with the system multiple times. So if I ask like a top-k query followed by another
query followed by another query, you're gaining more and more information about the data set. And so
you want to sort of track how many accesses somebody has made about your data set while still
preserving some privacy. So what second part specific to the top-k framing of this problem,
because it's one of the things I recall from previous conversations is that a big part of
you know doing this in practice is like trying to figure out what this epsilon needs to be.
Exactly. And so it's always a problem. It's not just in this top-k sense. Yeah,
so the kind of is impacted by the multiple dips at the
time. Exactly. So this privacy loss parameter, this epsilon, that's something that we need to track
as we're interacting with it. So differential privacy, one of the great features about it is
what's called composition so that if I run one differentially private query followed by another
one, it's still a differentially private, although it's slightly worse parameters because I've
gained more information about it. So you can sort of budget for if I am comfortable this much
privacy loss over my entire interaction with the system, then I'm just going to track how many
interactions they make and just shut off access after a certain amount of time. But what's the
new contribution of this work is what we call pay what you get composition, which our algorithms,
you can think of it as like maybe a deficiency of them, is if you ask for top 100 results,
it might return only 20 results. And the reason why is because there might be lots of ties in your
results so that it doesn't know how to rank them. And so rather than releasing a full 100 results,
it might return only 20. And so typically with differential privacy, your privacy budget would
have to deplete by the number of things that you requested, not how many things you actually got.
So if I ask for 100 things, my privacy budget would deplete by 100, even if I got only 20 results.
So we provide a new composition technique that allows you to only pay for the number of results
that you get so that if I have a total budget of maybe 100 accesses, I can ask like a top 100.
And then if I only get 50 results, now my budget only dropped by 50, I can ask another query.
And maybe I only get like 25 results. And so I can keep asking until a full 100 is exhausted. Whereas
before, if I asked for a top 100 and only 10 were returned, I would be done with my interaction.
Are there constraints imposed on the way the presented results of a tie are selected?
Yeah, so the ties are an issue for the algorithm because we have to introduce a threshold as well.
Which kind of intuitively for privacy constraints, not even thinking about differential privacy,
but privacy in general, if counts are sort of below some value, you would think of those as being
really sensitive. Maybe only a few people actually use that or viewed that article or type of
URL. So we inject some threshold and it might be the case that there's like several articles
that are actually at or below that threshold value. And so if that's the case, then if there's
sort of lots of small counts, we're only going to release fewer results. Okay, okay.
You mentioned that the first part of the paper is presenting several algorithms. How are the
different algorithms differentiated? Yeah, so that's a great question. So the privacy guarantees
that we can give with these algorithms are what's called user level. And so this is in distinction
with some of other practical works of what's called event level differential privacy. And so if you
think about it, event level differential privacy is a lot maybe an easier constraint to the problem
because it's protecting the privacy of any one action a user makes, maybe like one click or one
page view. But with user level privacy, what you want to do is you want to protect all actions
that a user could possibly make over the entire thing. So if a user is actually engaging with
multiple articles, then you want to protect the privacy of all of those articles that that user
viewed. So that because we're after a user level privacy guarantee, what that means is that we have
to sort of know in advance whether a user can impact counts in lots of different places. Maybe they
can view every possible article that's ever on LinkedIn. And in that case, we call that unrestricted.
So we don't know in advance how many things they can touch. And so we have a particular algorithm
that will give you user level privacy. But for something like top 10 countries that have a particular
skill set, you know in advance that any one user could only be in one country. So if you take
that user out of the data set or add them, you know only a single count can change. And so the
algorithm that we have in that case is slightly different. And so in either case, we're ensuring a
user level privacy guarantee. But that's sort of the distinction to ensure user level privacy. We
need to know in advance either how many things they can possibly touch, how many counts they can
change. And if you don't know how many things they can change, then you're going to use a sort of
default algorithm, which is well, they could change everything. So this is the algorithm that you
should run. So practically speaking, who are the users in these in the scenarios? This
Sam as the user using the LinkedIn app, you know, web or mobile, where I don't tend to see
counts or something like that. Or it's just like an internal LinkedIn user who has access to do,
you know, reporting on articles and things like that. But you still want to protect the privacy
of the user base. Right. So the existing like differential privacy system at LinkedIn
sort of handles data analytics or reporting for users so that we can provide like add analytics,
content analytics, profile view, statistics, that sort of thing. So it's sort of we're providing
products for people to get sort of reporting information. And because of that, we want to be mindful
of the member's privacy, their data. So we might call them like private actions that users are doing.
Things that they don't want to like deliberately share with people outside, unlike, you know,
a share of an article or a like or a comment. And so these aren't things that I might, you know,
be exposed controls that might be, you know, surface via the feed, but more like the, you know,
the business tools add interfaces, things like that. Exactly. Yeah. You know, there's more of a risk
that I might run some report. And you're trying to make sure that I'm not able to get to
individually identifiable. Exactly. I want to provide only aggregate statistics about how
people are interacting across LinkedIn, but still maintain privacy. Even in the aggregate statistics,
there's been studies showing like even with aggregates, you can get individual level
information. So that's kind of why differential privacy is so great.
So the different types of algorithms that you mentioned in the first part of the paper
is it both the event base and the user base for this kind of problem or you're saying the event
base is already solved and you're doing different flavors of user base. Yeah. So what we're going
after is user level privacy. And so because of that, we have to be mindful of how many things one
user can change. If we're going for event level, we know that only a single click can only change
a single count. Right. So in that setting, like we also provide an algorithm for that case. So even
though we're after user level privacy guarantees, we have an algorithm that would also give you
a event level, which is also new. Does this paper describe something that you've been able to
put into production at LinkedIn or is it more kind of ongoing research that is driving towards
something that's usable for you? Yeah. So with the existing differential privacy system,
that's providing noise on aggregate statistics and providing a event level privacy.
And so with these extra tools, extra algorithms, we wanted to sort of incorporate that into our
existing suite of tools so that we can also provide differentially private results for exploratory
data analytics, kind of like the top K. And we could provide user level privacy guarantees.
So this is sort of ongoing work that we're doing and the differential privacy efforts at LinkedIn
is we're pushing forward. You mentioned the kind of the compositionality of
composability, I forget which words he is. Yeah, one of those.
Composition, because of the possibility. That's great.
Elements of differential privacy. Does the user level controls to those,
like sit on top of the event-based controls, or are they a totally separate system or approach
to this? Yeah, so with the composition, we have to think of not just algorithms that are providing
a differential privacy, but we also have to have sort of a tracking of how many accesses are they
making with my system, and then deducting things based on whatever queries they're asking.
So you can kind of think of these of two components. There's sort of the algorithm side of things,
and then there's also the budget tracking. So that these together can provide a very global
differential privacy system. You also are active in kind of the federated learning and the use
of differential privacy and federated systems has LinkedIn published anything in that area?
No, so that was my my prior work at Apple. Yeah, so with that, we were very interested in
federated learning because there's lots of iPhones that are out there. It'd be great if we could
just push training on device because there's lots of sophisticated hardware on the iPhone, so
why don't you just push all the intensive work to the peripheral devices.
What's great about is that you're not submitting a data back, but you're sending the weights of
your neural net that you were training on device, which those themselves are computed on the data
so they contain information about what data they were computed on. And so the work that I did
there, we worked on sort of local privatization of those model weights prior to submission,
so that on the server we can aggregate them and update the model with only these privatized weights
and iterate on that process. And it's actually some of the work that I that I did was actually
featured in the expo at Sunday or Sunday, which is why you're able to tell us about it. Exactly.
Yeah, so Apple just talked about it. Yeah, so they presented some of the work at WWDC this year
and they then talked about the private federated learning system. Some of the use cases, I think
we're like with Hey Siri and some other ones too, so yeah, really cool work. Yeah, where do you see
some of the big kind of, you know, frontiers for differential privacy, you know,
at LinkedIn and beyond in the industry, you know, given the progress that's been made over the
last few years? Yeah, so I think within differential privacy, you can kind of break it up as local
and central. And then within those two models, you can break up the task as data analytics and
machine learning. So with all of those, like I think we need to still push for practical
deployments. And all the stuff we talked about previously is data analytics. We're not talking
about differential private machine learning. Yeah, so in the central model, I was only talking about
data analytics with the federated learning and the local model that's more machine learning
with the emojis one that's more data analytics and the local model. So really, like the book is not
closed on any of these things, like we need to keep pushing in these areas and some of the great
work that's going on in the open sourcing of these projects, I think is a great step forward
because it will bring more people to the forefront of this field and really push the frontiers of
it. Because I think it is the future of how we think about data use. We need to be mindful of,
you know, what algorithms we're picking, how we can track, how users data is following.
And so some of the open source efforts, like there's the work from Microsoft collaborating with
Harvard, pushing this differential privacy open source platform. And so we're actively collaborating
with with that team because LinkedIn is a within Microsoft now. So that's a project.
Is that focused on centralized central? Yeah, so yeah, so I believe like it wants to cover like
all of potential use cases, but initially it will be central model working on top of data stores.
A bit hesitant to try to poke at the details because it's difficult to poke at, you know,
the details of kind of a theoretical paper. But, you know, I wonder if you can kind of give us
a sense for, you know, what was the hard part here? And, you know, where you started and,
and, you know, what that, what the effort was like. Yeah, so some of the difficulties with it were,
there's like sort of standard differential privacy algorithms, like adding liposinoise,
is a very standard differential privacy algorithm. And then there's something called the
exponential mechanism, which you typically don't think of adding noise, you kind of think of
random sampling things instead. And so one of the really nice connections that we made in our
paper was that actually exponential mechanism can be implemented by adding something called
gumbled noise rather than like liposinoise. So, and gumbled noise actually pops up a lot of
machine learning. It's something that you would do to report like the category that has the
highest weight. So typically how you would think about doing this is you would have weights for
different like, you know, dog, cat, orange, whatever, for a particular image classification task.
And so you'd have weights, they would all be positive, but they wouldn't be like a probability
distribution. They wouldn't all add up to one. So what you would do is you'd do a soft max
layer where you would take all those weights, take each of the weight and divide by the sum of the
exponentials of the weights. Now you have a probability distribution and you would sample
according to that probability distribution to get like them. This is the label for that particular
task. Now, typically you don't do this in practice. What you do is you stop at the
weights and you add gumbled noise to all of those weights and report the category that had the
highest noisy value. And this is called the gumbled max noise trick. And so it turned out that
we could use that with the exponential mechanism to get a differentially private algorithm.
Now, one of the limitations of it is because it's differential privacy, gumbled noise itself,
you cannot release the counts and satisfy the differential privacy condition. You can only release
the element that was the max. So you don't know why it was the max. It was the max because the
value was 45 or something. You only know that this was the most popular one. This one was the one
with the highest weight. And so with that connection of exponential mechanism with gumbled noise,
we were able to take existing algorithms, typically how you would solve top K, you would use the
exponential mechanism K different times. You could now do this in one shot by just adding gumbled
noise to everything and reporting the K values that are in the top. And so that connection made
made things a little bit easier in our analysis. We analyzed things according to the exponential
mechanism, but we were using gumbled noise in the actual implementation which made it a lot more
efficient and practical in the title. And so just the use of gumbled noise and kind of a noise
oriented approach as opposed to the exponential approach to that allow the differential privacy
guarantees to be able to kind of use the existing noise-based privacy guarantees, but with a
different distribution. Yeah, so the connection of noise with exponential mechanism was really
helpful because we wanted to incorporate a threshold. We wanted to say if counts were too small,
we didn't want to report them. So we wanted to incorporate a threshold, but incorporating a
threshold when you're randomly sampling things like we didn't know where a threshold could be
implemented. But now when you're talking about adding random noise to values, now it makes sense
to add a threshold to those things so that you would only report things that were above some
threshold value. Whereas with exponential mechanism, if you were just to look at it, you wouldn't
know where to necessarily say like, oh, a threshold really needs to be this value.
Awesome. Any other kind of interesting things we should be kind of looking forward to coming out
of the differential privacy community? Yeah, so I'm really excited about the open source efforts
in the area. I think this is kind of the future of private data analytics. It's really important
to be transparent with how you're doing things. Otherwise, if you're just touting that you're
private and you're not revealing what it is, then is it really private how do people verify
these certain things? So transparency is really important and this will bring more people to the
community. So that's really got to be the movement forward. I think there's going to be there's
great open problems in central DP with data analytics with the machine learning. There's lots
of open problems. I think that still needs to have a lot of consideration from people in industry
to make them practical rather than trying to go after just pure theory, try to make things
actually work in practice and at scale. And in the local model, I think there's lots of work
to do as well there. There's also very interesting work sort of in between the two,
taking sort of a little bit of local with a little bit of central to get like a hybrid model.
So I think that's an incredibly interesting push forward to think of like various models of privacy.
It sounds like it would be a lot harder in that case to kind of provide these, you know, general
privacy guarantees. Yeah. So even in the local model like the guarantees are local as opposed to
you know, local relative to some some centralized thing. Yeah. So I think for that you kind of take
a step back and you're like, well, where's the privacy in my system because maybe the privacy
parameters the same and all of these things. But one just feels more private than the other. And
it's because you're sort of shifting this trust barrier. In the local model like the trust is like
on your device, like, yeah, I know that I'm privatizing it before I'm sending it. And the central
model, the trust boundary is not a data submission, but more wherever I want to build an application,
that's where my privacy trust barrier is going to be. And the model in between, there's like, well,
maybe there's some things internal to my company that I want to protect the, you know,
malicious internal employees from accessing these data sets. So I want to add, introduce some
noise at that level, but maybe not as much as what I would be comfortable with as if I want to
just publicize this model and have deploy this on all devices out there. So it's sort of the trust
boundary, like you can sort of think of various models of privacy, whereas the privacy parameter
might be the same in all of them, you're shifting the trust. All right. Thanks so much for taking
the time to share with us, you know, a little bit about what you're up to. Absolutely.
Thanks for really appreciate you having me. Thank you.
All right, everyone. That's our show for today. For more information on today's show,
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.
