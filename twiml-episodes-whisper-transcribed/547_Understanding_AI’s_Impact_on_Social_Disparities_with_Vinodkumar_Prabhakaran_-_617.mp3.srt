1
00:00:00,000 --> 00:00:06,600
All right, everyone. Welcome to another episode of the Twimal AI podcast. I am your host, Sam

2
00:00:06,600 --> 00:00:12,920
Charrington. And today, I'm joined by Vinod Purpakran. Vinod is a senior research scientist

3
00:00:12,920 --> 00:00:18,080
at Google Research Vinod. Welcome to the show. Thank you, Sam. Thank you so much for having

4
00:00:18,080 --> 00:00:23,440
me here. I'm looking forward to digging into our conversation. We'll be talking about some

5
00:00:23,440 --> 00:00:29,840
of your research, which looks at both the way that AI impacts social disparities, but also

6
00:00:29,840 --> 00:00:35,520
how AI can be used to study social disparities. To get us started with that, I'd love to have

7
00:00:35,520 --> 00:00:39,200
you share a little bit about your background and how you came to work in the field.

8
00:00:39,200 --> 00:00:47,040
Absolutely. So, yeah, I come from a more traditional computer science background, like my PhDs

9
00:00:47,040 --> 00:00:55,120
in computer science, where I worked on sort of more research at the intersection of

10
00:00:55,120 --> 00:01:02,720
natural language processing and social sciences or society. So, that's where I kind of started.

11
00:01:02,720 --> 00:01:10,080
So, I come from a more kind of technical background in that sense. But over time, I've sort of

12
00:01:10,640 --> 00:01:17,920
got to this place where researching at the intersection of AI in general and society. And

13
00:01:17,920 --> 00:01:25,040
where I kind of have two different research profiles. One is where I use AI or NLP

14
00:01:25,040 --> 00:01:32,480
or machine learning technologies to sort of look at societal disparities, use them as tools to

15
00:01:32,480 --> 00:01:39,280
look at societal disparities. And that's the kind of work that I was doing mostly in the PhD

16
00:01:40,160 --> 00:01:48,400
work, as well as afterwards as a postdoc at Stanford. But afterwards, like in the last three or

17
00:01:48,400 --> 00:01:55,120
four years, Google as a research scientist, I've been mostly looking at how social disparities

18
00:01:55,120 --> 00:02:01,920
influence these tools or how these social disparities are captured, reflected and maybe even

19
00:02:01,920 --> 00:02:07,600
propagated or amplified through these machine learning and natural language processing tools.

20
00:02:07,600 --> 00:02:12,560
So, it's kind of like looking at both directions of this intersection.

21
00:02:12,560 --> 00:02:19,200
And you referenced some of the work you were doing for your PhD. What are some examples of the

22
00:02:19,200 --> 00:02:26,640
way that you used AI and NLP to look at social or societal disparities? Yeah. So, I mean,

23
00:02:27,200 --> 00:02:32,960
some of the work was like looking at how workplace interactions, how social power manifest,

24
00:02:32,960 --> 00:02:37,760
you know, like workplace interactions, like that was my PhD work. So, looking at email conversations

25
00:02:37,760 --> 00:02:42,960
at a workplace and like looking at just by looking at a language used and the structure of

26
00:02:42,960 --> 00:02:48,880
these conversations, can you tell who is the superior or who is the subordinates in that conversation?

27
00:02:48,880 --> 00:02:53,840
Like how does power manifest in like how people interact with one another? So, that was my kind

28
00:02:53,840 --> 00:03:02,880
of PhD research thesis topic. That sort of got me into this work, which is kind of I'm really excited

29
00:03:02,880 --> 00:03:08,560
about I continue to be excited about the work is I don't actively work on it, but right now,

30
00:03:08,560 --> 00:03:16,960
but that was where we used this NLP tools to look at social racial disparities in police

31
00:03:16,960 --> 00:03:22,720
community interactions. This is work I did as a postdoc at Stanford with a bunch of amazing

32
00:03:22,720 --> 00:03:29,120
researchers there in collaboration with social psychologists and linguists and computer scientists

33
00:03:29,120 --> 00:03:36,320
where we had access to body camera videos of about one year worth of data from the Auckland

34
00:03:36,320 --> 00:03:42,240
police department. So, it was in collaboration with the police department and we used the NLP tools

35
00:03:42,240 --> 00:03:49,360
to sort of look through this lot of data. This usually this data is looked at as like, you know,

36
00:03:49,360 --> 00:03:55,760
something goes wrong. It's looked at as evidence whereas we were kind of demonstrating that it could

37
00:03:55,760 --> 00:04:02,240
be used as like data to learn from and like understand what is going wrong potentially in the

38
00:04:02,240 --> 00:04:09,360
in the in the department or in that particular city. So, yeah, we used NLP tools to sort of look at

39
00:04:09,360 --> 00:04:15,440
like things like the level of respect, the level of politeness, the way the conversations are

40
00:04:15,440 --> 00:04:22,240
structured when police officers stop community members for traffic jobs, for instance.

41
00:04:22,240 --> 00:04:28,240
So, presumably the body camera videos were transcribed and that's what you applied NLP tools to.

42
00:04:28,240 --> 00:04:33,280
Yes. So, the first paper that came out of that work like we did look at the transcribed data like

43
00:04:33,280 --> 00:04:40,000
we had them manually transcribed and looked at like signals for politeness and respect and so on.

44
00:04:40,000 --> 00:04:45,760
But there's also work on looking at the audio signals like the prosody. I was not actively involved

45
00:04:45,760 --> 00:04:52,720
in it but the the the kind of frequency and pitch and all those factors might be signals that

46
00:04:53,360 --> 00:04:58,320
so this is a huge project with like, you know, so many people looking at interesting signals there.

47
00:04:58,880 --> 00:05:03,360
I also had some work where we are actually looking at the structure of these conversations like

48
00:05:03,920 --> 00:05:09,040
where police officers or when did the police officers like give the reason for the stuff,

49
00:05:09,040 --> 00:05:14,080
right? Like when you stop someone, do you start by saying, hey, I'm stopping you for this

50
00:05:14,080 --> 00:05:19,120
in this reason, where is your diverse license and registration versus basically

51
00:05:19,120 --> 00:05:24,240
stopping someone and saying like, hey, give me your license and registration and then telling.

52
00:05:24,240 --> 00:05:30,320
So, that sets the conversation in a very different kind of path and that also be empirically

53
00:05:30,320 --> 00:05:36,880
we're able to analyze and look at these kind of differences in this kind of subtle ways,

54
00:05:37,600 --> 00:05:43,120
subtle ways in which like conversations can be different and what how that can affect

55
00:05:43,120 --> 00:05:48,480
like the later parts of the conversation. So, this is ongoing work like the researchers

56
00:05:48,480 --> 00:05:54,800
at Stanford continue to work on this and I work on like I kind of wrapping up it's been like four

57
00:05:54,800 --> 00:06:00,960
years now, but some of the work I'm still in in in in bolden. So, there's like some work that's

58
00:06:00,960 --> 00:06:06,080
going to be coming out from that like in the near future, hopefully. But yeah, that's that's an

59
00:06:06,080 --> 00:06:14,080
example of a place where these AI tools are used to sort of look at social disparities.

60
00:06:14,080 --> 00:06:20,640
I'm curious how much of the social science side of your of this work you were

61
00:06:22,240 --> 00:06:28,960
involved in or kind of got into or were you collaborating with social scientists and working

62
00:06:28,960 --> 00:06:33,520
on the more machine learning technical sides of it? Yeah, I love this question because it's

63
00:06:33,520 --> 00:06:38,640
something that that particular project was such a learning experience for me to sort of having

64
00:06:38,640 --> 00:06:44,400
deeper collaborations with social scientists like to and also like having that deep respect for

65
00:06:44,400 --> 00:06:51,600
each disciplines like you know engaging with the social scientists. It was a very close collaboration

66
00:06:51,600 --> 00:06:55,680
to answer your question. It was a very close collaboration. We had like weekly meetings and like

67
00:06:55,680 --> 00:07:02,000
I went to the you know it was not that it's a typical computer science project that I have

68
00:07:02,000 --> 00:07:07,040
been part of like prior to that. You get this data from somewhere and then you go into your lab and

69
00:07:07,040 --> 00:07:11,440
like you know go through munch through the numbers and like try to come up with like you know some

70
00:07:11,440 --> 00:07:16,800
results and then like you know put it in a paper. But this was a very um involved collaboration

71
00:07:16,800 --> 00:07:21,360
with social scientists but also with the police department and communities. I didn't actively

72
00:07:21,360 --> 00:07:25,280
involve with and interact with the communities there say but like you know our collaborators did

73
00:07:25,280 --> 00:07:31,760
have like community workshops and stuff. But I did go to the police you know precinct and like

74
00:07:31,760 --> 00:07:37,040
you know talk to the police department, police chief and like sort of understood like you know

75
00:07:37,680 --> 00:07:46,400
the context of this data and I even went on like right along to sort of like be part of to see

76
00:07:46,400 --> 00:07:53,840
how these conversations happen in reality. That I mean that really changed or shapes the way I

77
00:07:53,840 --> 00:07:58,240
kind of look at this data. It's not no longer just like ones and zeros like it actually

78
00:07:58,240 --> 00:08:03,920
have has a lot more brings a lot more meaning to like the data that we are working with and

79
00:08:05,040 --> 00:08:10,720
so yeah it was and to answer your question about like the disciplinary collaboration also like

80
00:08:10,720 --> 00:08:17,120
it was a very as I said like we had weekly co-a meetings and we had like multiple papers I was

81
00:08:18,720 --> 00:08:25,520
working with like social psychology PhD students and yeah and we were in another podcast recently

82
00:08:25,520 --> 00:08:31,200
me and another social psych substance colleague of one of these papers about like how interdisciplinary

83
00:08:31,200 --> 00:08:35,360
works like in these kind of collaborations right like said I think in that particular project like

84
00:08:35,360 --> 00:08:40,320
the I think one thing that we both mentioned was that there was a deep respect for these disciplines

85
00:08:40,320 --> 00:08:45,120
rather than like you know the computer scientist coming and like taking away like the you know

86
00:08:45,120 --> 00:08:50,160
the data and like going and doing magic like it was not that we were very actively involved in

87
00:08:50,160 --> 00:08:55,920
framing the research questions how we ask the research questions how we interpret the results

88
00:08:55,920 --> 00:09:01,840
all of that was like in deep collaboration and so the more recent work that you're doing

89
00:09:03,440 --> 00:09:11,840
how did the work that you were doing previously kind of take you to the more recent work?

90
00:09:11,840 --> 00:09:18,400
I think the work that I was doing at Stanford especially in this particular context kind of got me

91
00:09:18,400 --> 00:09:28,960
a lot more sort of aware of like a lot more sort of yeah aware of like the kind of the

92
00:09:28,960 --> 00:09:33,280
justice angle of like these social science work like they know the disparities and like that

93
00:09:33,280 --> 00:09:40,320
that human angle to it and that sort of like and it was around the time that there was conversations

94
00:09:40,320 --> 00:09:45,840
around fairness and bias in machine learning models that was just like you know beginning in 2015

95
00:09:45,840 --> 00:09:53,600
1617 period so I was in that like I just happened to be in that space where like I was already working

96
00:09:53,600 --> 00:10:00,160
on understanding social disparities and this is like an important or like a really cool sort of

97
00:10:00,160 --> 00:10:05,840
like way of like looking in the other direction how does this disparities get into or get captured

98
00:10:05,840 --> 00:10:12,000
into these machine learning models and that's what like sort of like pivoted or like it wasn't

99
00:10:12,000 --> 00:10:16,560
a really pivot because and I still kind of like have this I still work on like social science

100
00:10:16,560 --> 00:10:26,000
see kind of flavored work but that realization is like how I go into the Google research

101
00:10:26,640 --> 00:10:33,200
job that I'm where I am right now where kind of like the focus at that time was on sort of

102
00:10:33,200 --> 00:10:37,520
understanding like this how these disparities get captured in machine learning models and that

103
00:10:37,520 --> 00:10:42,320
was like the transition because I was already working at that space in the other direction

104
00:10:43,200 --> 00:10:50,000
and since joining Google most of my Google work as I said have been on the other direction where

105
00:10:51,360 --> 00:10:59,600
you know looking at how disparities in data kind of get captured in these models and

106
00:10:59,600 --> 00:11:07,680
you know clearly one key place where some of those disparities will get introduced into

107
00:11:07,680 --> 00:11:16,480
models is through the that human interface which is data labeling and your more recent work is

108
00:11:16,480 --> 00:11:26,960
kind of focused on on that in particular can you introduce us to your paper your 2021 paper the

109
00:11:26,960 --> 00:11:33,360
issues aggregating human labels what was the kind of the broad set of issues that you were exploring

110
00:11:33,360 --> 00:11:40,320
there absolutely so I think yeah there's a lot of work within the machine learning and OP field

111
00:11:40,320 --> 00:11:46,240
in the last few years looking at like various sources of biases very sources through which like

112
00:11:46,240 --> 00:11:50,960
you know biases get creeped into the models it could be coming through data it could be coming

113
00:11:50,960 --> 00:11:59,120
through the humans who are building these models and their perspectives or you know limited

114
00:11:59,120 --> 00:12:07,200
perspectives about society but in this particular so I was always curious about how diverse

115
00:12:07,200 --> 00:12:12,720
perspectives in data beat like you know data beat and like you know people who are building these

116
00:12:12,720 --> 00:12:18,320
like models how does like diverse perspectives like get captured in these like pipeline right you

117
00:12:18,320 --> 00:12:24,400
know there's no that's not always like this like one single answer I mean we live in a world of like

118
00:12:25,520 --> 00:12:30,160
you know pluralistic world with like so many value systems if you think about like across the

119
00:12:30,160 --> 00:12:36,320
world there's no one single answer for many of these like questions and ethics and fairness

120
00:12:37,040 --> 00:12:42,960
what is fair for me may not be what is fair for like someone in a different cultural context and

121
00:12:42,960 --> 00:12:48,160
having a different cultural history so that kind of motivated that motivates my current work like

122
00:12:48,160 --> 00:12:53,760
largely most of my research currently isn't that kind of motivated from that question of like you

123
00:12:53,760 --> 00:12:58,160
know how do we different perspectives like all disagreements between like what is right and wrong

124
00:12:58,160 --> 00:13:04,000
gets captured in these kind of interventions so it's like even when like you know we intervene or

125
00:13:04,000 --> 00:13:10,800
like making things fairer like how do we do that like in a more pluralistic way so that that's

126
00:13:10,800 --> 00:13:16,480
where like this particular paper work on this particular paper that you referenced I came about

127
00:13:16,480 --> 00:13:25,360
and so we had a paper in 2021 where we looked at this one core thing as you said data labeling

128
00:13:25,360 --> 00:13:30,800
which becomes like a such a core thing for the machine learning pipeline where you actually

129
00:13:30,800 --> 00:13:38,880
you know have human writers annotate like you know be it like images or text content like for like

130
00:13:38,880 --> 00:13:43,840
whatever whatever social constantly whatever construct that you're trying to model in cases where like

131
00:13:43,840 --> 00:13:49,200
you know whether labeling whether something is a cat or a dog you can see that like most people

132
00:13:49,200 --> 00:13:53,440
would agree like you know there might be cases where like you know people disagree whether something

133
00:13:53,440 --> 00:13:59,760
is a cat or not but like it's it's a it's a relatively more objective kind of task but when

134
00:13:59,760 --> 00:14:06,640
you ask like somewhere is this piece of text offensive that's you're actually leaning on a lot of sort

135
00:14:06,640 --> 00:14:13,360
of aspects of that individual humans like you know social cultural context and their lived

136
00:14:13,360 --> 00:14:20,000
experiences shape how they feel or how they perceive something to be offensive and not and so

137
00:14:20,000 --> 00:14:28,320
we looked at like a bunch of datasets which kind of capture or attempts to capture such sort of

138
00:14:29,120 --> 00:14:35,120
subjective or relatively more subjective tasks such as like sentiment whether this is a

139
00:14:35,120 --> 00:14:42,320
positive sentiment or a negative sentiment or offenseiveness like or hate speech or emotions

140
00:14:42,320 --> 00:14:47,600
like whether this is like something expressing happiness or sadness or that sort of so

141
00:14:47,600 --> 00:14:54,320
this like a we had around three datasets around seven or eight different like tasks like these

142
00:14:54,320 --> 00:15:01,600
kind of like social constructs that were like labeled and we looked at how people's like different

143
00:15:01,600 --> 00:15:06,800
annotators like perspectives like matched or not so the traditional way of dealing with like when

144
00:15:06,800 --> 00:15:13,680
people disagree for for a long time like you know in machine learning traditionally how you deal

145
00:15:13,680 --> 00:15:19,280
with it when people disagree is that you take a majority vote like you you have especially when you

146
00:15:19,280 --> 00:15:23,680
collect data from the crowd from the crowd work kind of platforms like mechanical talk and all

147
00:15:23,680 --> 00:15:28,960
that you it's like relatively cheap so you get like three annotations for all in or three or five

148
00:15:28,960 --> 00:15:36,160
or ten depending on how much money you have and you basically get the more number of annotations

149
00:15:36,160 --> 00:15:41,520
and then you take a majority vote to say that like oh this is the majority of people in our pool

150
00:15:41,520 --> 00:15:47,120
agreed and stuff we basically were questioning like how does what does that mean for like

151
00:15:47,120 --> 00:15:53,360
perspectives or people with specific social cultural backgrounds which maybe underrepresented

152
00:15:53,360 --> 00:16:00,160
within the annotator pool so we had a paper like kind of like precursor paper first where we kind

153
00:16:00,160 --> 00:16:05,600
of did this analysis on this eight different datasets or eight different tasks annotation tasks

154
00:16:05,600 --> 00:16:12,320
like where we looked at like if you just take the majority vote and then compare or look at like how

155
00:16:12,320 --> 00:16:16,880
does each individual annotator agree with this majority vote like because we also have like the

156
00:16:16,880 --> 00:16:22,640
annotations that this individual voters gave like how many times like their vote really made it to

157
00:16:22,640 --> 00:16:29,440
the majority right so we looked at like that or was equal to the majority right like we looked at that

158
00:16:30,640 --> 00:16:35,920
for these tasks like that varied significantly across tasks and sometimes it is like all over the

159
00:16:35,920 --> 00:16:42,400
place some some tasks like such as hate speech if I remember correctly was a lot more kind of like

160
00:16:42,400 --> 00:16:46,720
you know most people agree and most people with the majority vote like agreed with like a most

161
00:16:46,720 --> 00:16:53,840
people but like the things like disgust is an emotion like it had like very wide range of like

162
00:16:53,840 --> 00:16:59,200
interpretations and like people disagreed on that or one thing that jumps out at me just hearing

163
00:16:59,200 --> 00:17:08,400
you describe that setting is in some context you might want to you might be tempted to look at the

164
00:17:09,520 --> 00:17:16,240
degree to which a particular annotator annotates along with the majority as like a measure of

165
00:17:16,240 --> 00:17:22,720
quality but here you're pointing out that it's in fact you know maybe a measure of diversity or

166
00:17:22,720 --> 00:17:28,960
different context or something like that absolutely I think predominantly machine learning community

167
00:17:28,960 --> 00:17:35,680
have been using that quality kind of framing in doing the majority vote like the reason why they

168
00:17:35,680 --> 00:17:41,840
do majority vote is to actually remove the noise right so I think it probably comes from the

169
00:17:41,840 --> 00:17:48,640
place that you know machine learning researchers like you know I want to ensure the quality of

170
00:17:48,640 --> 00:17:53,360
the data that they're working with and this is like comes across as like noise but there's

171
00:17:53,360 --> 00:17:59,040
there's even there's two different things there's like in the context of an individual label

172
00:17:59,040 --> 00:18:05,600
data point the you know taking the majority allows you to eliminate the noise but then

173
00:18:05,600 --> 00:18:14,480
from the context of the kind of the labeling operations right you know often you're looking at the

174
00:18:14,480 --> 00:18:22,160
label or over time and and you're trying to understand you know which of the labelers that are

175
00:18:22,160 --> 00:18:29,040
best I've seen work that also looks at like this kind of agreement with majority as a way to like

176
00:18:29,040 --> 00:18:35,760
you know assess the quality of the greater themselves right that's that's what I'm alluding to and

177
00:18:35,760 --> 00:18:46,480
that this now the this additional angle that hey it's not just quality depending on the type of

178
00:18:46,480 --> 00:18:53,520
question that we're asking it's also a measure of you know different degrees of diverse perspectives

179
00:18:53,520 --> 00:19:02,000
yeah yeah and I think one I do want to note that like yeah it's it's possible that it could be

180
00:19:02,000 --> 00:19:08,080
a unreliable reader but if that reader is like internally consistent with their their own annotators

181
00:19:08,080 --> 00:19:12,640
like if they see similar things like again if they're like so then then they are actually bringing

182
00:19:12,640 --> 00:19:18,800
in a different perspective like that is kind of like it's not that it is lower quality it is just

183
00:19:18,800 --> 00:19:25,040
a different value system maybe is reflected in their annotations so that's what we were going

184
00:19:25,040 --> 00:19:29,520
after right like so this is an ongoing project it's not just one paper would not answer all the

185
00:19:29,520 --> 00:19:35,920
questions so in this particular case we were basically trying to tease a part or disentangle

186
00:19:35,920 --> 00:19:42,720
these disentanglements and sort of understand what factors might be contributing to why people

187
00:19:42,720 --> 00:19:47,600
are disagreeing right like is it just this person being unreliable just like you know randomly

188
00:19:47,600 --> 00:19:54,160
selecting things or is there a systematicity to the way they disagree with the majority right

189
00:19:54,160 --> 00:20:01,200
and one and one way in that paper that we looked at was looked at like there was one dataset where

190
00:20:01,200 --> 00:20:08,080
we also had access to the social demographic characteristics of these annotators so we looked

191
00:20:08,080 --> 00:20:14,720
at whether there is a difference in across like different genders this particular dataset had

192
00:20:14,720 --> 00:20:20,560
only binary gender no I think there was only one non binary gender person so it wasn't like

193
00:20:20,560 --> 00:20:28,240
big enough to like analyze all the different kind of gender categories but it was a you know

194
00:20:28,240 --> 00:20:37,280
gender was one thing and then a political affiliation or ethnic ethnicity or race

195
00:20:39,040 --> 00:20:42,560
and in this particular study we show that there was no difference in gender there's

196
00:20:42,560 --> 00:20:49,840
in like men and women had like or male and female annotators had similar rates of sort of like

197
00:20:49,840 --> 00:20:56,720
having rate rateers who disagreed with the majority with the in across annotations but it was

198
00:20:56,720 --> 00:21:03,760
interesting to note that like when it came to race African-American writers annotators like

199
00:21:03,760 --> 00:21:10,400
who identified as African-American in that dataset had significantly lower agreement with the

200
00:21:10,400 --> 00:21:18,160
majority rating than white American this was the dataset collected in the US so white American

201
00:21:18,160 --> 00:21:25,680
Asian-American Raiders so that is a problem that is a problem when a from a fairness perspective

202
00:21:25,680 --> 00:21:30,720
when you take majority then you're actually sidelineing a particular perspective like potentially

203
00:21:30,720 --> 00:21:36,400
there is like a you know perspective that is being sideline what was the particular

204
00:21:36,400 --> 00:21:45,440
the specific dataset and task so this is a dataset for determining sentiment like positive negative

205
00:21:45,440 --> 00:21:51,120
neutral sentiment leveling I do want to note though that this dataset was collected with the

206
00:21:51,120 --> 00:21:56,240
intention to study by so this was not a case where like there was a dataset and someone just like

207
00:21:56,240 --> 00:22:01,680
did majority vote and then moved on with like in the in an incorrect way this dataset had the

208
00:22:01,680 --> 00:22:08,480
social demographic information in them precisely because they wanted to study these this is a

209
00:22:08,480 --> 00:22:16,400
dataset built by Mark Diaz my colleague for his PhD thesis so he now works with me so that's how

210
00:22:16,400 --> 00:22:21,200
this dataset had this information usually machine learning researchers when they collect data

211
00:22:21,200 --> 00:22:27,360
for various reasons like do not collect the social demographic information because it's a much

212
00:22:27,360 --> 00:22:34,240
harder to get that information there's all sorts of risk involved with it yeah so we were able

213
00:22:34,240 --> 00:22:41,440
to see these these disagreements with the majority that differs across different social demographic

214
00:22:41,440 --> 00:22:49,840
kind of groups which is even a bigger problem from a fairness perspective because this is like a

215
00:22:49,840 --> 00:22:55,600
human or like a researcher's decision to sort of take the majority vote and by doing that you're

216
00:22:55,600 --> 00:23:02,480
actually actively sidelineing certain perspectives like in your data labels so yeah that was the work

217
00:23:03,280 --> 00:23:08,720
that you referenced and then we had a follow up paper that I can talk about where we kind of look

218
00:23:08,720 --> 00:23:14,720
at like how we can sort of deal with that like how we can constructively sort of deal with this

219
00:23:14,720 --> 00:23:21,600
like diverse perspectives so let's dig into the let's dig into that second paper you've identified

220
00:23:21,600 --> 00:23:28,800
this challenge and the second paper wanted to propose some potential solutions yeah so what

221
00:23:29,520 --> 00:23:38,240
we did there was basically rather than sort of trying to find this single ground truth for like

222
00:23:38,240 --> 00:23:44,560
these kind of subjective tasks ahead of the time like you know ahead of training these models

223
00:23:44,560 --> 00:23:49,600
at the data collection stage itself like taking the majority out vote rather than doing that

224
00:23:49,600 --> 00:23:56,240
we built a sort of like an approach or like we built a kind of a mission learning pipeline where

225
00:23:56,240 --> 00:24:03,120
we use a multi task approach where it's it's traditionally used for like you know you want to train

226
00:24:03,120 --> 00:24:09,440
a model for like multiple tasks that are similar so use use the same mission learning network like

227
00:24:09,440 --> 00:24:14,880
you know the network that you train like you use the same data but then you kind of like have this

228
00:24:14,880 --> 00:24:21,520
final layers that are trained for specific tasks so that you have like a shared embedding or shared

229
00:24:21,520 --> 00:24:27,680
kind of network for the most of the part and then like you have like this specific task specific

230
00:24:27,680 --> 00:24:35,760
like parts of that network so we looked into whether we can actually model separate annotators

231
00:24:35,760 --> 00:24:41,840
and their systematicity in which like they annotate like using this sort of like shared network

232
00:24:41,840 --> 00:24:46,960
like a multi annotator model so that like you have all the data is being used for training like

233
00:24:46,960 --> 00:24:52,720
you know this model but like this model does not output one label it outputs like okay if it was

234
00:24:52,720 --> 00:25:00,080
this person that person would have said the label X or so you have like if you had like 10 different

235
00:25:00,080 --> 00:25:04,960
labels a labelers like 10 different Raiders you actually are modeling these 10 different

236
00:25:04,960 --> 00:25:12,000
lip perspectives in the output and then in the output time you can actually take the choice of like

237
00:25:12,000 --> 00:25:16,240
if you still want to do majority voting like you can do the majority voting at the output like

238
00:25:16,240 --> 00:25:20,560
you know the machine learning model gave like 10 outputs and you can just take the majority

239
00:25:20,560 --> 00:25:29,440
vote there or you could say that oh we want to take the kind of outputs produced by these

240
00:25:29,440 --> 00:25:35,920
subset of annotators or subset of like these predictions that models like annotators from a

241
00:25:35,920 --> 00:25:40,080
particular background it this you know you can imagine a scenario where like you have annotators

242
00:25:40,080 --> 00:25:43,920
from like different countries let's say there are three countries that we have annotators from

243
00:25:43,920 --> 00:25:50,240
India, US and like say Germany and then when you are actually rolling out like products

244
00:25:50,240 --> 00:25:56,720
into the societies like you may want to actually use the majority vote or like use the vote from

245
00:25:56,720 --> 00:26:03,440
a particular region or like you know or the machine learning models predictions that

246
00:26:04,320 --> 00:26:15,120
reflect that annotator those annotators to kind of to be used in that region right you know so

247
00:26:15,120 --> 00:26:19,440
there are like so many different ways that you can be used you can use this approach so you

248
00:26:19,440 --> 00:26:24,800
basically captures this like multitude of perspectives in the prediction pipeline rather than

249
00:26:24,800 --> 00:26:31,600
sort of like suppressing it in the in the beginning of the pipeline it also gives you an additional

250
00:26:31,600 --> 00:26:38,320
ability to sort of know when there is uncertainty like when there is disagreement like because if you

251
00:26:39,520 --> 00:26:44,480
make a label as a bit majority labels if you call that okay this sentence is offensive

252
00:26:44,480 --> 00:26:49,120
just based on like in a majority vote like in the beginning of the data collection itself

253
00:26:49,120 --> 00:26:55,280
we would never know whether that was like you know agreed on by 10 people or like you know two people

254
00:26:56,640 --> 00:27:02,480
right like and it was an unanimous decision or not whereas in this case like you have these

255
00:27:02,480 --> 00:27:07,600
like multiple perspectives at the prediction time so you kind of know that oh multiple annotators

256
00:27:07,600 --> 00:27:12,720
would have disagreed on this particular case so maybe the machine should not predict here

257
00:27:12,720 --> 00:27:17,280
it should be like a human label or from that particular social context should like make a call

258
00:27:17,280 --> 00:27:22,560
whether this particular piece of text is offensive or not so that's another way I could imagine

259
00:27:22,560 --> 00:27:28,240
being used this being used is that like it gives you a handle on how much uncertainty is potentially

260
00:27:28,240 --> 00:27:33,840
here how much diverse perspectives potentially is here for this given piece of text it's kind of

261
00:27:33,840 --> 00:27:41,840
consistent with the general idea in dealing with neural networks to just give it all the data and

262
00:27:41,840 --> 00:27:48,720
don't try to clean it up too much because you're not hiding what you think might be noise that

263
00:27:48,720 --> 00:27:54,080
the network can actually find some signal in probably I mean that was definitely not the motivation

264
00:27:54,080 --> 00:28:01,520
for our like as approaching it but I think yeah the current neural networks are powerful

265
00:28:03,760 --> 00:28:09,520
in sort of like you know bringing in these signals like you know because prior to this sort of

266
00:28:09,520 --> 00:28:15,680
multi-task architectures which is like a two or three years maybe a little bit longer old prior to

267
00:28:15,680 --> 00:28:21,680
that like you would have had to like train if you have like five different like set you know annotators

268
00:28:21,680 --> 00:28:25,280
you have to train like five different models like you there's no way that like they have a shared

269
00:28:25,280 --> 00:28:30,640
network and they can take signals like even the smaller signals like from each of these

270
00:28:30,640 --> 00:28:35,360
raters to be like you know model differently or separately that would have not been possible so

271
00:28:35,360 --> 00:28:43,200
I think the current neural networks ability to sort of like pick up on these signals are definitely

272
00:28:43,200 --> 00:28:51,280
contributing to being able to sort of capture this diversity without like affecting like sort of

273
00:28:51,280 --> 00:29:00,240
like the end performance if that's what you're going for. And so is the the model that you're

274
00:29:00,240 --> 00:29:07,920
creating based on the kind of the annotator signal is this then kind of use separately as part of

275
00:29:08,640 --> 00:29:16,960
a downstream training task or are you kind of embedding it end-to-end in the ultimate task.

276
00:29:16,960 --> 00:29:21,760
That's a great question. I think that there is potential to actually use this as kind of like a

277
00:29:21,760 --> 00:29:29,200
lead-in for like other downstream tasks and so to take a step back like so this is ongoing work

278
00:29:29,200 --> 00:29:34,480
like you know we have not sort of like rolled it out and like into any products or anything this is

279
00:29:34,480 --> 00:29:40,240
like I work in research so this allows in in Google research that allows us to sort of like do

280
00:29:40,240 --> 00:29:45,760
this foundational research without having it to be tied to any particular products. So in that

281
00:29:45,760 --> 00:29:52,400
sense like we have not sort of like rolled it out yet on any of the products but like I think

282
00:29:52,400 --> 00:29:57,520
I envision this both as like a potential kind of like first step to sort of like you know

283
00:29:57,520 --> 00:30:06,240
tease apart these differences but it could also work as a sort of end-to-end scenario where like you

284
00:30:06,240 --> 00:30:14,480
know I could imagine like an online platform using this model to sort of have multiple answers for

285
00:30:14,480 --> 00:30:22,000
like multiple perspectives for sort of you know choosing whether something some particular content

286
00:30:22,000 --> 00:30:28,000
needs to be removed or art for instance like it could it could help in the content moderation kind

287
00:30:28,000 --> 00:30:38,000
of pipeline to sort of queue content for review appropriate like you know reprioritize content

288
00:30:38,000 --> 00:30:42,400
and so I could see this being used in like that kind of production scenario as an end-to-end

289
00:30:42,960 --> 00:30:49,840
when I say end-to-end like I often envision like a human label or a human human sort of like

290
00:30:49,840 --> 00:30:55,840
intervention it's a human in the loop or machine in the loop kind of situation rather than sort of

291
00:30:55,840 --> 00:31:01,520
like button click and like you kind of like you know get the label and like that's that decide

292
00:31:01,520 --> 00:31:07,040
like whether something is offensive or not. So there's often a human in the loop is needed

293
00:31:08,880 --> 00:31:14,720
and yeah in those settings like I could imagine this model being useful. How did you evaluate the

294
00:31:14,720 --> 00:31:21,360
the performance of the model? Traditionally for this dataset like if you had built like a single

295
00:31:21,360 --> 00:31:27,760
kind of label model like you would basically go and evaluate you know the typical machine learning

296
00:31:27,760 --> 00:31:32,800
evaluation pipeline like you know you have like a test set and you basically look at like the accuracy

297
00:31:32,800 --> 00:31:41,520
or precision recall and so on. So we evaluated first in terms of basically if you choose to just

298
00:31:41,520 --> 00:31:45,840
still do the majority word like if you had taken the majority in the beginning versus like

299
00:31:45,840 --> 00:31:52,080
this whole pipeline of like this multi multi task or multi annotator architecture which has like

300
00:31:52,080 --> 00:31:57,520
these multiple perspectives and then you take the majority word and looking at like sort of

301
00:31:59,040 --> 00:32:05,360
how well this does you know in that evaluation. So using the traditional pipeline to evaluate

302
00:32:05,360 --> 00:32:14,320
it and in that case we were able to see that like the performance was almost the same or sometimes

303
00:32:14,320 --> 00:32:19,760
in some data it's even better because better in the sense that like because it's it's modeling

304
00:32:19,760 --> 00:32:24,960
each annotator kind of like individually and it allows it to sort of like look at internal

305
00:32:24,960 --> 00:32:32,000
consistency of their labels better than when you actually merge them. So it doesn't happen across

306
00:32:32,000 --> 00:32:37,520
board but like in in in in some tasks like it performed even better in that particular evaluation

307
00:32:37,520 --> 00:32:44,960
strategy. But we also looked at sort of how well are we doing on calculating this uncertainty?

308
00:32:44,960 --> 00:32:53,360
We looked at are we able to or the the sentences where the model said something is offensive and

309
00:32:53,360 --> 00:32:57,840
like you know there was a lot of disagreement in the end like in the end predictions are multiple

310
00:32:57,840 --> 00:33:04,960
there's more disagreement. We compared that with like how much did annotators for this particular

311
00:33:04,960 --> 00:33:11,600
text disagreed with each other right? Like so we compared when the model has this high disagreement

312
00:33:11,600 --> 00:33:16,880
is that also the cases where like the annotators originally disagreed and that turned out to be the

313
00:33:16,880 --> 00:33:25,360
case there was a significant correlation between sentences where the annotators disagreed

314
00:33:25,360 --> 00:33:31,520
and the model disagreed and that kind of and that was a lot more than traditional ways of calculating

315
00:33:31,520 --> 00:33:37,760
uncertainty. So this is another kind of way we evaluated so that was not about model performance

316
00:33:37,760 --> 00:33:44,320
because our focus is not getting like the model performance from like 72% to 74% like that's like

317
00:33:44,320 --> 00:33:51,760
the typical sort of like the mission learning kind of research kind of objective and for good

318
00:33:51,760 --> 00:33:58,560
reason like you know it's it is important to improve the performance but in this work we are

319
00:33:58,560 --> 00:34:03,040
we are interested in like how well we are capturing diverse perspectives and are when we are

320
00:34:03,040 --> 00:34:08,400
kept having these disagreements in the predictions are they reflecting disagreements in the actual

321
00:34:08,400 --> 00:34:15,360
data and which turned out to be significantly correlated. So yeah has this research led you to

322
00:34:15,360 --> 00:34:23,600
kind of a set of axioms that or that you would recommend to folks as they're kind of building

323
00:34:23,600 --> 00:34:31,920
these labeling pipelines and you know wanting to create models that are as robust as possible

324
00:34:31,920 --> 00:34:37,200
as fair as possible like how should they think about all these issues? Absolutely yeah I was

325
00:34:37,200 --> 00:34:44,880
this like so this work is ongoing and there's like a lot more work that needs to be done and this

326
00:34:44,880 --> 00:34:52,160
particular model that I proposed is not going to solve all the problems in this space and so we had

327
00:34:52,160 --> 00:34:58,880
like laid out like a bunch of like sort of recommendations in the first paper actually that like

328
00:34:58,880 --> 00:35:03,680
you know the traditional practice is that you take a majority vote and sort of like that's what

329
00:35:03,680 --> 00:35:07,840
is in the data set most people when they collect labels from like multiple people they just take

330
00:35:07,840 --> 00:35:12,480
the majority vote and that one label is the only thing that's even in the data set. So you know we

331
00:35:13,680 --> 00:35:18,400
recommend or we argue that people should like when people collect data should sort of

332
00:35:20,160 --> 00:35:26,160
release annotator level that sorry individual annotator level kind of labels so that like you know

333
00:35:26,160 --> 00:35:32,000
you're not treating these annotators as interchangeable anymore also where possible like you know

334
00:35:32,000 --> 00:35:38,240
sort of being able to collect social demographic information about annotators as much as possible

335
00:35:38,240 --> 00:35:44,480
to do it responsibly being able to sort of like release that along with the data set would only

336
00:35:44,480 --> 00:35:50,640
enrich that and sort of the downstream users of this data set could basically use that information

337
00:35:50,640 --> 00:35:55,920
to account for any fairness failures or any of these kind of analysis which is very hard for us to

338
00:35:55,920 --> 00:36:02,160
do because most data sets in our community do not have even annotator level labels or like social

339
00:36:02,160 --> 00:36:09,360
demographic information. In addition we also argued in favor of sort of like documenting about

340
00:36:09,360 --> 00:36:15,200
the recruitment selection and assignment processes that are followed for this annotation kind of

341
00:36:15,200 --> 00:36:20,000
pipeline so that like it gives more information about like how these annotation labels like you know

342
00:36:20,000 --> 00:36:26,160
what was the diversity of Raiders like you know what like social demographic kind of factors or like

343
00:36:26,160 --> 00:36:30,800
groups were represented and what was underrepresented so those information is super useful

344
00:36:30,800 --> 00:36:38,160
for the downstream user of the of these data sets. So on that note like I want to also give a call

345
00:36:38,160 --> 00:36:48,480
out to the recent paper by my colleague Mark Diaz on sort of providing a sort of comprehensive

346
00:36:48,480 --> 00:36:56,480
framework for communicating these things about annotator diversity and annotator these processes

347
00:36:56,480 --> 00:37:03,520
through kind of a transparency artifact called crowd work sheets which capture these kind of

348
00:37:03,520 --> 00:37:10,000
information and that just got published like a couple of months ago at the fairness and accountability

349
00:37:10,000 --> 00:37:15,680
and transparency conference. So I encourage people to check that check out that work as well and I

350
00:37:15,680 --> 00:37:21,360
also want to call out that like this. The two papers that I discussed at length is work led by

351
00:37:21,360 --> 00:37:27,680
my intern Ida Davani who was at USC at that time last summer and so this is like almost a year

352
00:37:27,680 --> 00:37:32,880
long of work after internship like she continued as a researcher student researcher with us

353
00:37:33,600 --> 00:37:39,280
and she has now joined back as our team as a full-time researcher so this work is an ongoing

354
00:37:39,280 --> 00:37:45,120
effort like and one of the core sort of themes of my research going forward. Awesome awesome

355
00:37:45,120 --> 00:37:49,600
well Vinod thanks so much for joining us share a little bit about what you've been working on.

356
00:37:50,400 --> 00:37:55,760
Absolutely this was a pleasure to chat about this research and it was lovely chatting

357
00:37:55,760 --> 00:38:23,920
with you Sam. Same thanks Vinod.

