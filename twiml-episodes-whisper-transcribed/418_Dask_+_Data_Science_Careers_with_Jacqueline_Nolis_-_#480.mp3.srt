1
00:00:00,000 --> 00:00:17,200
All right, everyone. I am here with Jacqueline Nolis. Jacqueline is head of Data Science at Saturn Cloud.

2
00:00:17,200 --> 00:00:21,120
Jacqueline, welcome to the Twoma AI podcast. Thank you for having me.

3
00:00:21,920 --> 00:00:28,080
Hey, I'm looking forward to our conversation. We're going to touch on careers in data science as

4
00:00:28,080 --> 00:00:37,120
well as desk, which is a open source project that your company is working on. But before we dive in,

5
00:00:37,120 --> 00:00:41,440
I'd love to have you share a little bit about your background and how you came to work in data science.

6
00:00:42,320 --> 00:00:49,600
Sure. So I had the odd circumstance where I fell into data science, which is to say, I started

7
00:00:49,600 --> 00:00:54,480
years ago as an undergrad in mathematics. And then I got a master's in mathematics thinking I want

8
00:00:54,480 --> 00:00:58,640
to be a math professor. Then I was, no, I don't want to be a math professor. I want to use math to

9
00:00:58,640 --> 00:01:03,520
help businesses and stuff. So I started a job, which now today would be called a data scientist,

10
00:01:03,520 --> 00:01:10,080
but back then it was business analytics. And I just kind of grew, I worked at an e-commerce

11
00:01:10,080 --> 00:01:15,520
company doing forecasting for them. And then I worked in aerospace for a bit. And then I'm like,

12
00:01:15,520 --> 00:01:23,760
I want to go get more technical skills. I'm going to go get a PhD. So I got a PhD in operations research,

13
00:01:23,760 --> 00:01:28,800
my researchers in electric vehicle networks, like where does Tesla put their charging stations.

14
00:01:28,800 --> 00:01:34,160
And about this time, the field was starting to get called data science. And while I was in my PhD

15
00:01:34,160 --> 00:01:38,240
program, a company, like a boutique consulting firm was looking for data science consultants.

16
00:01:38,240 --> 00:01:41,840
And I'm like, yeah, I'll make some side cash as a PhD student, you kidding me.

17
00:01:43,040 --> 00:01:48,240
And then I finished my PhD and then I did consulting for full time for many years,

18
00:01:49,120 --> 00:01:53,200
to the point where I was a director and like started my own data science team at a consulting firm.

19
00:01:53,200 --> 00:01:59,440
And then I quit and started my own like freelance consulting business. And I did that for a few

20
00:01:59,440 --> 00:02:04,240
years. And then I said, you know, I being a freelancer and working on your own is a lot of work.

21
00:02:04,240 --> 00:02:09,120
And that's very stressful. Like I would like to go back into industry. And so then I ended up,

22
00:02:09,120 --> 00:02:14,720
now I'm here at a Saturn cloud having a lot of fun. So awesome, awesome. Yeah, I think you were doing

23
00:02:14,720 --> 00:02:24,000
the freelance consulting thing or maybe just at the point in transition when we first met. And

24
00:02:24,800 --> 00:02:31,840
the circumstance around our connection was for those who, you know, may remember your name or

25
00:02:31,840 --> 00:02:38,880
the panel that we did, we did a panel discussion on advancing your data science career during

26
00:02:38,880 --> 00:02:48,080
the pandemic just about a year ago. And that was your kind of wrapping up the independent consulting

27
00:02:48,080 --> 00:02:51,360
at the time. That's right. And so it was like, I was giving that, I was part of that panel.

28
00:02:51,360 --> 00:02:56,240
I'm like, I also am advancing my career by trying to get a job again after working independently

29
00:02:56,240 --> 00:03:02,160
for several years. And if I remember correctly, you spent some time talking about your career search

30
00:03:02,160 --> 00:03:09,120
and kind of how that evolved for you and what was, what, how the pandemic made it interesting.

31
00:03:09,120 --> 00:03:13,440
Yeah, because I did start looking for a job right at the beginning of the pandemic when they were

32
00:03:13,440 --> 00:03:17,760
like three data science jobs in the country available, which I think now there are a lot more.

33
00:03:17,760 --> 00:03:23,120
I think we've kind of gotten back to normal from what I've seen. But I was pretty touch and go in

34
00:03:23,120 --> 00:03:33,360
like, what, do you, May of 20, 28, 2020? It was a rough time. Yeah. So, you know, I mentioned

35
00:03:33,360 --> 00:03:39,200
we'll be talking about a couple different things. One of them is you wrote a book on data science

36
00:03:39,200 --> 00:03:44,240
careers. You do a podcast on data science careers and you're generally out talking about

37
00:03:44,240 --> 00:03:50,160
and helping folks kind of build their careers, get jobs in data science. How did that all come about?

38
00:03:50,160 --> 00:03:56,880
Yeah. So that's interesting. So I, you know, I started like a couple years ago going to conferences

39
00:03:56,880 --> 00:04:01,040
and trying to give talks. And I'm like, the thing I really like talking about is like, you know,

40
00:04:01,040 --> 00:04:04,640
how do you become a director? How do you get your first job? Like I kind of always like thought

41
00:04:04,640 --> 00:04:08,640
that stuff was not, or like how do you run a team effectively? And I always thought that stuff

42
00:04:08,640 --> 00:04:14,240
could be talked about more like per hour than maybe some technical topics. And at one point,

43
00:04:14,240 --> 00:04:18,160
I'm manning the publishing company read some blog posts I had and they're like, hey, do you want

44
00:04:18,160 --> 00:04:22,960
to write a book? And I'm like, do I want to write a book? You know, I never considered writing a

45
00:04:22,960 --> 00:04:28,080
book. But yeah, I do want to write a book. And so I reached out to Emily Robinson and we actually

46
00:04:28,080 --> 00:04:32,320
had met several months ago because we're both giving talks at day-to-day Texas. And I said,

47
00:04:32,320 --> 00:04:37,840
Emily, you want to come write a book with me? And she said, maybe give me a month. I'm switching jobs.

48
00:04:37,840 --> 00:04:42,560
And so then a month later, and I just switched jobs too. And then like a month later, we're like,

49
00:04:42,560 --> 00:04:48,320
yeah, let's do it. And so we spent like a year writing this book. And so like each chapter,

50
00:04:48,320 --> 00:04:52,400
like the first half of the book is like, hey, if you're not a data science test yet and you want

51
00:04:52,400 --> 00:04:57,520
to get a job, how do you get the skills? How do you build a portfolio? How do you write your resume?

52
00:04:57,520 --> 00:05:02,480
And then the second half is, okay, cool. You're a data scientist. Now what? Like how do you think

53
00:05:02,480 --> 00:05:07,840
about like making good analysis? How do you know? What do you do when a project is failing? Or

54
00:05:07,840 --> 00:05:12,960
how do you quit a job and get your next one? And so it's really fun. And so Emily and I, we

55
00:05:12,960 --> 00:05:17,040
wrote this book. It did pretty well. We're pretty happy with it. And we're like, we want to talk

56
00:05:17,040 --> 00:05:21,600
about this more. And so then we made a podcast too, where now the podcast is each week, we kind of

57
00:05:21,600 --> 00:05:27,600
talk about the themes of different chapters of the book. So yeah, so I guess the end of the result

58
00:05:27,600 --> 00:05:34,240
is like I fell into it. But also it was a really nice venue because I had been a data scientist

59
00:05:34,240 --> 00:05:37,920
for a bunch of years now. And I have all these thoughts. And I would just like to get them out

60
00:05:37,920 --> 00:05:42,480
of my head and on paper. And I could just hand the paper to other people rather than keeping it in here.

61
00:05:44,080 --> 00:05:50,000
So when you think about the first part of the book, targeting folks that want to get a career

62
00:05:50,000 --> 00:05:58,720
in data science, what are the kind of top insights that you've actually, you know, in the book or

63
00:05:58,720 --> 00:06:04,640
you know, since writing the book for, you know, how you go about that nowadays, you know,

64
00:06:04,640 --> 00:06:12,480
beyond the, the usual, which I imagine is, you know, have some portfolio projects, you know,

65
00:06:12,480 --> 00:06:19,280
get on LinkedIn and network, get on Twitter and network. I what do you find are the, the non-traditional

66
00:06:19,280 --> 00:06:23,840
or the non-obvious, you know, things that folks should be thinking about? Well, I would say,

67
00:06:24,560 --> 00:06:27,520
maybe I'm gonna say the obvious stuff, but I'm gonna think about it. Think about it this way,

68
00:06:27,520 --> 00:06:32,160
this different way, which is the data science field at the entry level. I think it's starting to get

69
00:06:32,160 --> 00:06:37,040
pretty saturated, which is to say, maybe like when I was looking for jobs, it was, you know, like 10

70
00:06:37,040 --> 00:06:41,440
years ago, it was, oh, hey, you got a math degree. Cool. Come on. Oh, come in. And now like it's so

71
00:06:41,440 --> 00:06:45,440
many people want to be data scientists that's getting trickier, which is not to say you can't get a

72
00:06:45,440 --> 00:06:51,200
job, but it's just gonna take more thought and, you know, maybe work to do that. And when people give

73
00:06:51,200 --> 00:06:58,160
the advice of make a portfolio, make a GitHub page, all that, what they're implicitly saying is, hey,

74
00:06:59,040 --> 00:07:03,360
people who don't have data science experience, like pure like data science job experience yet,

75
00:07:04,240 --> 00:07:09,840
companies don't know, hey, can I trust you to be a data scientist? Would you get here and know

76
00:07:09,840 --> 00:07:14,080
what you are doing and how to handle it? And so something like a, you know, a portfolio or

77
00:07:14,080 --> 00:07:19,200
GitHub page, things like that, they are letting the employer know, hey, look, no, you can tell I'm a

78
00:07:19,200 --> 00:07:23,680
data scientist because look at this cool side project I did or whatever. But that's not the only

79
00:07:23,680 --> 00:07:28,480
way you can do it. You could do something like, hey, if you're a, you know, software engineering

80
00:07:28,480 --> 00:07:32,560
job, you can try and start doing data science things within that job, you know, before you look

81
00:07:32,560 --> 00:07:38,080
for your next full data science job, or you can start by getting a data analyst job. And in that

82
00:07:38,080 --> 00:07:42,560
role, which is similar data science, work there for a few years, then transition to data science.

83
00:07:42,560 --> 00:07:48,320
So it's not like the pattern is first you make a GitHub portfolio, then they hire you. It's like

84
00:07:48,320 --> 00:07:53,840
yeah, it's like, think about what are the things that you can do and you know your circumstances

85
00:07:53,840 --> 00:07:58,480
best. What are the things that you can do that will help employers understand you are a good fit

86
00:07:58,480 --> 00:08:03,040
for this position. And that may not be something you can do it over a weekend. It may take years

87
00:08:03,040 --> 00:08:12,560
to really set that foundation to help you out. Given the, the saturation at the early stages of

88
00:08:12,560 --> 00:08:18,640
data science careers now are employers looking for different things or they're different ways

89
00:08:18,640 --> 00:08:24,560
that folks can and should be signaling that they have the required background.

90
00:08:24,560 --> 00:08:30,080
I think that the classic like build a portfolio, that's good. Get something in your job, that's good.

91
00:08:30,720 --> 00:08:37,040
Things like boot camps can also be good, but also you need to like, you need to do the work of like,

92
00:08:37,040 --> 00:08:40,960
hey, I've been to the boot camp, I'm going to take this job and make it like so obvious how,

93
00:08:40,960 --> 00:08:44,240
sorry, this side project I did as part of the boot camp make it so obvious that this is just like

94
00:08:44,240 --> 00:08:48,880
what you're hiring for I can do it. But I think more broadly what I would say is, well,

95
00:08:48,880 --> 00:08:52,640
that's kind of intimidating to know that the field's getting more tight at the entry level.

96
00:08:52,640 --> 00:08:57,680
I would say at the, hey, if you have nine months experience in one data science role, I think it

97
00:08:57,680 --> 00:09:03,760
is very easy right now to get another job, which is to say the hurdle between zero and one unit of

98
00:09:03,760 --> 00:09:12,000
like pure experience is massive. So it may feel really hard to get that first job, but know that once

99
00:09:12,000 --> 00:09:16,800
you get that first job, you stick in a frill, like just a tiny bit, it should be much easier going

100
00:09:16,800 --> 00:09:21,040
forward. So it really is going to like, you are at the hardest part right now, people who are just

101
00:09:21,040 --> 00:09:27,280
trying to get in and it should get easier. Hopefully as, you know, as the your career experience

102
00:09:27,280 --> 00:09:37,680
progresses. Do you find that that kind of tenure or churn in data science jobs, these early data

103
00:09:37,680 --> 00:09:44,880
science jobs is more or less than other areas that like software engineering? I don't know if I

104
00:09:44,880 --> 00:09:50,000
would say data science is more churn than software engineering. I guess it wouldn't surprise me if

105
00:09:50,000 --> 00:09:56,640
it was, but I don't know. Yeah, for the typical tenure for a first, you know, first data science

106
00:09:56,640 --> 00:10:03,520
gig. I will say, I mean, it's probably like, I would guess it's like a year and a half, two

107
00:10:03,520 --> 00:10:08,000
years average, which I think is probably similar to software engineering. And I think the reason

108
00:10:08,000 --> 00:10:12,240
why you leave data science jobs is not that different than the reason you leave software engineering

109
00:10:12,240 --> 00:10:15,280
jobs, right? Like you don't get along with your manager. You don't agree with a couple of these

110
00:10:15,280 --> 00:10:21,200
missions. Be hours aren't good. Like it's kind of like a lot similar. But I do think that

111
00:10:21,200 --> 00:10:26,960
that the people I know who have left after nine months of their first job, it's not like they

112
00:10:26,960 --> 00:10:30,080
left and then they couldn't get another job because they only had nine months experience. It's like,

113
00:10:30,800 --> 00:10:34,400
wow, people see nine months like, cool, you've worked somewhere and done something. Come on in,

114
00:10:34,400 --> 00:10:39,040
you know, like we are not a lot of experience data scientists out there. So we are hiring

115
00:10:39,040 --> 00:10:43,360
pretty quickly at that level. You know, and like disclaimer, what Jacqueline says is not a

116
00:10:43,360 --> 00:10:47,360
career guarantee. This is just what I've seen. Yeah.

117
00:10:47,360 --> 00:10:52,800
Yeah. That's a little bit of a contrast to some of the memes that you see like,

118
00:10:54,320 --> 00:10:59,040
you know, for taking desk, for example, to start our transition over that topic, you know,

119
00:10:59,920 --> 00:11:04,960
data scientists wanted, you know, 10 years of desk experience required, right? Well,

120
00:11:04,960 --> 00:11:11,200
desk hasn't been around that long. Yeah. Well, I think I think this is a separate issue,

121
00:11:11,200 --> 00:11:17,040
which is oftentimes the people writing job posts don't know what they actually want. And that

122
00:11:17,040 --> 00:11:22,560
doesn't mean just like, oh, it's an HR person who's writing this job post. It could be a data

123
00:11:22,560 --> 00:11:26,160
scientist who thinks that they have the ability, like, oh, I'm going to sit the bar so high because

124
00:11:26,160 --> 00:11:31,680
our team needs people that are like, that's fun. You could say this job requires a PhD and

125
00:11:31,680 --> 00:11:35,440
years experience and ask and all that and you will find no one. And then what are you going to do?

126
00:11:35,440 --> 00:11:38,720
Well, you're going to drop it to a reasonable level and then you will find people, you know, like

127
00:11:39,520 --> 00:11:43,360
the fact that those terrible job posts exist is not an indication that the field actually

128
00:11:43,360 --> 00:11:47,440
requires that stuff, not only in an indication that a lot of people don't actually have realistic

129
00:11:47,440 --> 00:11:54,720
expectations for what they should get from an employee. You mentioned that the second part of the

130
00:11:54,720 --> 00:11:59,680
book talks about like why projects fail and what to do if projects are failing, you know,

131
00:11:59,680 --> 00:12:05,280
what are the biggest questions you get from folks that are that are, you know, data scientists

132
00:12:05,280 --> 00:12:09,280
and are at that stage of their career where they're trying to navigate the role.

133
00:12:09,280 --> 00:12:16,160
So I will say on that failure thing in particular. So I give that as a talk, and I've done it a

134
00:12:16,160 --> 00:12:23,440
couple of times at a couple of different places. And when I give the talk on failure, there are

135
00:12:23,440 --> 00:12:27,280
some questions, but in general, what happens after the talk is I get lots of people come up to me

136
00:12:27,280 --> 00:12:32,480
and they're like, oh, thank God, someone else feels the things I feel, which is to say that

137
00:12:32,480 --> 00:12:37,040
especially as a more junior data scientist, when your stuff doesn't work, right? If someone says,

138
00:12:37,040 --> 00:12:41,200
hey, go build a churn model on that customer data and you try and build it and you just can't

139
00:12:41,200 --> 00:12:44,880
get it to work because there's just not enough signal there. It's very easy as a data scientist

140
00:12:44,880 --> 00:12:50,080
to be like, ah, I knew more skills. If I were a better data scientist, I could have made that churn

141
00:12:50,080 --> 00:12:56,640
model actually fit. When in practice, probably that data just has no actual signal in it. So no matter

142
00:12:56,640 --> 00:13:01,360
how experienced of a data scientist, you are, that model won't fit. And I think that that's kind of

143
00:13:01,360 --> 00:13:06,800
a problem for lots of junior data scientists in lots of different ways, which is you have expectations

144
00:13:06,800 --> 00:13:11,120
that the amount of knowledge and skill you have is an indication of how well you will do. But

145
00:13:11,120 --> 00:13:17,040
things like the data isn't actually have a signal in it or your stakeholders don't actually know

146
00:13:17,040 --> 00:13:20,000
what they want this product to do. They're just asking you to build models because they think that's

147
00:13:20,000 --> 00:13:23,600
a good idea. And then when you build them, they're like, wait, what do I do with this? Like,

148
00:13:23,600 --> 00:13:28,240
none of that stuff has to do with your technical abilities and knowing how to like draw that like

149
00:13:28,240 --> 00:13:33,840
emotional boundary and be like, okay, this project failed, but it's not because of me or maybe it

150
00:13:33,840 --> 00:13:40,080
is like related to some stuff that I had to do. But like, what are the things I can take us notes

151
00:13:40,080 --> 00:13:44,160
for the next time I do this project as opposed to I should have known that stuff in the first place

152
00:13:44,160 --> 00:13:48,880
even though this is only my ninth month on the job. That stuff's really important and I don't think

153
00:13:48,880 --> 00:13:56,640
it's talked about very much in data science. You know, I imagine when you first came into the field,

154
00:13:56,640 --> 00:14:02,480
you know, we were certainly a lot closer to this time where, you know, we the data scientist was

155
00:14:02,480 --> 00:14:08,880
also, you know, almost, you know, often preceded by mythical like the unicorn data scientist like the

156
00:14:10,640 --> 00:14:17,200
you know, the someone who knows math, knows stats, can program, can deploy, can do all these things.

157
00:14:17,200 --> 00:14:22,960
And over the past several years, as a field's gotten more mature, there's a lot more

158
00:14:22,960 --> 00:14:33,440
specificity in roles and also roles of kind of bifurcated into now you've got folks that are,

159
00:14:33,440 --> 00:14:38,320
you know, machine learning engineers that are focused on deployment. I'm wondering how,

160
00:14:39,200 --> 00:14:47,200
you know, the evolution of the role has changed or, you know, would change the advice you give to

161
00:14:47,200 --> 00:14:54,800
someone who is, you know, joining the field or early on in their career. Do you, is that something

162
00:14:54,800 --> 00:15:00,240
that kind of impacts the way you you talk to folks and advise folks? Yeah, kind of. I do think,

163
00:15:01,040 --> 00:15:06,000
yeah, I think when I started, there were no, no one had any expectation of data scientists because

164
00:15:06,000 --> 00:15:10,240
they didn't really barely existed. And then I agree they got to the point where your point of,

165
00:15:10,240 --> 00:15:14,640
now every data scientist should do everything. And now we've gotten smarter about that. I think

166
00:15:14,640 --> 00:15:20,800
what I would tell a people entering the field is it is helpful, I think, to have a basic idea of

167
00:15:20,800 --> 00:15:25,440
which of these things you'd rather do. Would you rather build models that are continuously running

168
00:15:25,440 --> 00:15:30,400
that like get hit like APIs and help customers like a recommendation model? Or would you rather be

169
00:15:30,400 --> 00:15:35,840
using, would you rather be using data to help a business make a decision? And like you're,

170
00:15:35,840 --> 00:15:39,200
what you're delivering is a PowerPoint, but it's a PowerPoint filled with interesting ideas that

171
00:15:39,200 --> 00:15:43,040
you found out from data. Or do you want to do just analytics, not just, but do you want to do

172
00:15:43,040 --> 00:15:47,360
analytics, which is like, hey, I'm going to take data from a database, put it in an easy to use

173
00:15:47,360 --> 00:15:52,400
dashboard, get it to you so it's easily digestible, but like just making data easy to use.

174
00:15:52,400 --> 00:15:55,520
Like think about what are the kinds of things that sound the most interesting to you?

175
00:15:55,520 --> 00:15:58,320
That would be the first thing I'd say. And then the second thing I would say is,

176
00:15:58,320 --> 00:16:03,200
but don't feel like you are locked into that and don't feel like you have to, you have to like,

177
00:16:03,200 --> 00:16:08,880
make a full decision before you start as a data scientist, which is to say like one, you may think,

178
00:16:08,880 --> 00:16:12,560
oh, I really want to do machine learning, but then you'd happen to find a job that's your first

179
00:16:12,560 --> 00:16:15,920
jobs, the more decision science, I'm going to make decisions based on this data. You might like

180
00:16:15,920 --> 00:16:19,760
that. And you don't know until you really try these jobs, which ones you like and won't like.

181
00:16:19,760 --> 00:16:22,880
So don't feel like if it's not the one you signed up for, you will not enjoy it.

182
00:16:23,680 --> 00:16:28,640
And then second, it is entirely possible to change. I spent the first 10 years of my career

183
00:16:28,640 --> 00:16:32,720
doing decision science stuff, especially as a consultant, which is give me your data. I will

184
00:16:32,720 --> 00:16:35,840
give you insights out of it. And then in the last couple of years, I've switched to machine

185
00:16:35,840 --> 00:16:40,080
learning, which is, hey, I'm going to build models that continuously run as Docker containers,

186
00:16:40,080 --> 00:16:45,040
blah, blah, blah. And like, was that switch easy? Not really. I mean, it took work, but like,

187
00:16:45,040 --> 00:16:49,440
I could do it. I think other people can do it. And I think we kind of have this weird gatekeeping

188
00:16:49,440 --> 00:16:54,800
as a field of like, oh, you're just a decision scientist. You can't do this stuff. You never will.

189
00:16:54,800 --> 00:16:58,560
And it's like, no, anyone can do any of those stuff. It's just a matter of getting the experience

190
00:16:58,560 --> 00:17:04,880
to learn how to do it. Yeah, yeah. So on the topic of machine learning and Docker containers and

191
00:17:04,880 --> 00:17:13,200
all that jazz, you know, what is, maybe let's start with Dask. What is Dask? Yeah. So, um,

192
00:17:14,320 --> 00:17:19,840
so Dask is basically, here's how I think about Dask, right? You have you're a Python user. You

193
00:17:19,840 --> 00:17:23,440
write your Python code. It's running on your local machine. And you're like, man, I really wish

194
00:17:23,440 --> 00:17:28,880
this Python code was running on a distributed set of machines really easily, right? So Dask is

195
00:17:28,880 --> 00:17:33,920
that tool, which is to say, if you have an operation that could really easily be switched to running

196
00:17:33,920 --> 00:17:38,800
on a bunch of computers at once, Dask can help you do that. Like one instance is, let's say you have

197
00:17:38,800 --> 00:17:43,600
a for loop. And in that for loop, it has a big computation. And it goes up for loop a thousand times.

198
00:17:43,600 --> 00:17:48,000
Why not have 10 computers taking each parts of those for loops and doing them all at once?

199
00:17:48,000 --> 00:17:52,560
Dask will help you do that. Dask also has a lot of built-in libraries and stuff. So you don't even

200
00:17:52,560 --> 00:17:56,480
have to think about how do you distribute this stuff out. You could just do something where it feels

201
00:17:56,480 --> 00:18:00,480
like you're doing normal pandas, but secretly on the back end, the calculations are being farmed

202
00:18:00,480 --> 00:18:06,320
out to this cluster of stuff. So Dask is an open source tool. It's right now. It's kind of like

203
00:18:06,320 --> 00:18:11,520
the alternative you'd use as Spark. But Spark is like, it's written in Scala. It's on the JVM.

204
00:18:11,520 --> 00:18:16,400
It's a hassle set up. And like, Dask is really like, hey, you got Python code, take your Python

205
00:18:16,400 --> 00:18:22,400
code, run it on a bunch of things at once. Nice and easy. Don't have to worry about it. And so Dask is

206
00:18:22,400 --> 00:18:28,080
an open source tool that's been developed since like 2018. And Saturn Cloud, the company I'm at now,

207
00:18:28,080 --> 00:18:33,200
one of the things we do is we make hosted Dask easy use. So if you are doing stuff on your laptop

208
00:18:33,200 --> 00:18:36,640
and you're like, I wish I want to, I wish this was running on 100 computers in the cloud,

209
00:18:36,640 --> 00:18:40,800
you can like in like three clicks have Saturn Cloud be the place where all those computers live.

210
00:18:42,320 --> 00:18:46,800
And it's a really cool tool. I think it's very neat. And I think I am surprised more tools like

211
00:18:46,800 --> 00:18:51,360
this don't exist in other languages and stuff like it just seems such like an intuitive thing

212
00:18:51,360 --> 00:18:54,720
that it should exist. And so I'm just very happy that Dask exists.

213
00:18:54,720 --> 00:19:06,000
And so when you talk about the for loop example, it echoes both stuff that was happening in

214
00:19:06,000 --> 00:19:11,760
high performance computing where you have to be very aware of the program and whether it's

215
00:19:11,760 --> 00:19:19,840
data parallel versus computation parallel, things like that. And on the other side,

216
00:19:19,840 --> 00:19:25,680
infrastructure type things like Kubernetes and distributed computing and things like that.

217
00:19:26,880 --> 00:19:31,200
It sounds like Dask is maybe a little bit of both. Yeah. And okay. So

218
00:19:32,080 --> 00:19:35,840
people who have ever heard my podcast and I think I'm very strongly opinionated about things.

219
00:19:36,480 --> 00:19:41,840
I think that as a data scientist, you should not have to worry about your Kubernetes cluster.

220
00:19:41,840 --> 00:19:45,840
And like, oh no, is that port open on that one work? Like there's a level of abstraction.

221
00:19:45,840 --> 00:19:50,560
You as a data scientist should not have to worry about. You should just be able to use these tools.

222
00:19:50,560 --> 00:19:55,440
I also think that there is for some whatever reason when people talk about the Shabie computing,

223
00:19:55,440 --> 00:19:59,040
it is one of those topics where people love to make it unnecessarily complicated.

224
00:19:59,040 --> 00:20:02,000
Right? Like, ah, but is it a blah blah blah type or a blah blah type for you? Yeah.

225
00:20:02,000 --> 00:20:06,000
And it's like, look, all I do, I have this, I have, it's a for loop. I would like each one of

226
00:20:06,000 --> 00:20:10,640
those for loops to happen all at the same time. Just like that shouldn't be that complicated.

227
00:20:10,640 --> 00:20:14,320
And I think Dask is one of those tools that makes it less complicated to do that.

228
00:20:14,320 --> 00:20:18,320
As opposed to sometimes people would be like, you know, there are some tools that make it more

229
00:20:18,320 --> 00:20:22,000
complicated. I think another great example. I would like for their whole phase with Hadoop where you

230
00:20:22,000 --> 00:20:26,560
had to take that for loop, turn it into a map and a reduce and all that stuff. Yeah.

231
00:20:26,560 --> 00:20:30,240
In order to distribute it. Yeah. And like, this is really a place where it'd be easy to do that

232
00:20:30,240 --> 00:20:35,040
abstraction. And Hadoop did, ah, here's a confession I have, which is when I was a junior data scientist

233
00:20:35,040 --> 00:20:39,600
in like 2012 or whatever, Hadoop was becoming really big. And it was like, oh, you're a

234
00:20:39,600 --> 00:20:44,560
tech scientist, but you work with big data. And I was always like, no, I always work with like 200,000

235
00:20:44,560 --> 00:20:48,480
rows and like linear aggressions and, you know, stuff. And like, I felt really ashamed. I'm like,

236
00:20:48,480 --> 00:20:52,800
should I go learn to do? Should I go learn to do? And thankfully, I never learned to do. And

237
00:20:52,800 --> 00:20:57,040
now I don't need to because people don't really think about that. We have tools like Dask and

238
00:20:57,040 --> 00:21:00,720
Spark that handle that layer of abstraction, which is say, if you're a data scientist and you feel

239
00:21:00,720 --> 00:21:04,640
like, oh, no, I don't use this tool yet. But I feel like I should be using this tool to stay

240
00:21:04,640 --> 00:21:08,800
ahead of the, you know, stay on the curve. You never know that tool might go away by the time.

241
00:21:08,800 --> 00:21:12,560
And it would be simple enough that you don't need that. Another place where I think this happens

242
00:21:12,560 --> 00:21:16,720
a lot is with, I think neural networks for the longest time where like, oh, do you have,

243
00:21:16,720 --> 00:21:20,720
is it a blog layer of this kind of abstraction? And like, like, here's a really complicated,

244
00:21:20,720 --> 00:21:24,480
crazy diagram. And here's eight equations to talk about like neural networks. And like,

245
00:21:24,480 --> 00:21:28,080
you see this, right? You see these like easy to use neural network cheat sheets on LinkedIn that

246
00:21:28,080 --> 00:21:34,080
are just like pages of math equations. And that's crazy. Like a talk I give is literally just,

247
00:21:34,080 --> 00:21:40,720
hey, you can learn how to use do neural networks and are in like 20 PowerPoint slides. And

248
00:21:40,720 --> 00:21:44,880
it's going to be PowerPoint slides filled with pictures of pets. And we're going to be training

249
00:21:44,880 --> 00:21:48,160
a neural network to generate pet names. Like that's a talk I give. And people are like, wow,

250
00:21:48,160 --> 00:21:53,600
I didn't know neural networks are this easy. And I say this so passionately because I did for

251
00:21:53,600 --> 00:21:56,400
the longest time didn't feel like I can learn neural networks because I thought they were too

252
00:21:56,400 --> 00:22:00,240
complicated. And then once I learned them, I'm like, are you kidding me? This is nothing. I could

253
00:22:00,240 --> 00:22:04,720
learn this easily. Other people could too. And I think desk and distributed computing is a thing

254
00:22:04,720 --> 00:22:09,840
that more people could do if we made this easier to understand as opposed to like, look how smart I

255
00:22:09,840 --> 00:22:17,120
am for understanding this complicated thing. Yeah. Yeah. Yeah. So what's the what's the user experience

256
00:22:17,120 --> 00:22:25,360
for a desk user? Are they is it totally transparent? Is it transparent depending on what you're trying to

257
00:22:25,360 --> 00:22:31,840
do? Do you have to, you know, annotate or decorate your functions or like what does the data

258
00:22:31,840 --> 00:22:38,480
scientists need to do to take advantage of desk? So there are a couple different ways you can use it.

259
00:22:38,480 --> 00:22:43,440
So the way I personally like to use it is it is just a decorator on your functions, which is like

260
00:22:43,440 --> 00:22:48,560
you say, hey, does it delayed function? Don't run it right away. And then you create like a desk,

261
00:22:48,560 --> 00:22:52,640
you start your desk cluster, which may be inside our cloud, may just be on your PC, whatever,

262
00:22:52,640 --> 00:22:58,800
like you start a desk cluster. And then you say, hey, use that cluster to map that function or,

263
00:22:58,800 --> 00:23:04,160
you know, basically run those functions on this list in that cluster. Or, you know, there's

264
00:23:04,160 --> 00:23:11,280
other simple operations. Another way you can use desk is like, desk has kind of wrappers. So

265
00:23:11,280 --> 00:23:16,560
there is like a instead of using like PD for pandas, you use like DD for desk and then you use

266
00:23:16,560 --> 00:23:22,080
the same pandas commands. But what that's doing is it for you is doing that figuring out how to make

267
00:23:22,080 --> 00:23:25,520
it into a list and sending that all to the clusters or whatever, like it's doing all that work of

268
00:23:25,520 --> 00:23:30,480
turning it into the distributed code. And so like there's the pandas version of that. There's,

269
00:23:31,280 --> 00:23:36,960
you know, like I think scikit learn, like there's a lot of different packages and stuff that have,

270
00:23:36,960 --> 00:23:41,600
okay, if you want to do desk for your distributed backend, you can do that too. And those are kind

271
00:23:41,600 --> 00:23:46,400
of the different ways you can run it. And I will say you can, like I said, you can on your laptop

272
00:23:46,400 --> 00:23:53,280
using desk and then call like send it over to the satire cloud servers. The one complexity of that

273
00:23:53,280 --> 00:23:58,640
is like if you're using Python 3.6 and that desk cluster is all using Python 3.8 and stuff like

274
00:23:58,640 --> 00:24:03,280
you can run into a little bit of issues. So what we do at satire cloud is you can also we host

275
00:24:03,280 --> 00:24:07,840
Jupiter. So you can be using a Jupiter that has like guarantees that that Jupiter will be consistent

276
00:24:07,840 --> 00:24:13,040
with that desk cluster. Just as like another potential way to use this tool, which is to say there's

277
00:24:13,040 --> 00:24:18,160
a lot of different ways you can try and run this stuff. And you know, a satire cloud, a real

278
00:24:18,160 --> 00:24:22,960
design philosophy is like we don't want to lock you into one way of doing things. You know, I feel

279
00:24:22,960 --> 00:24:28,960
like some like data science tools are like, okay, like you want to use get sure, but you can only push

280
00:24:28,960 --> 00:24:32,560
and you can only commit using this one button. And if you want to do anything complicated, like

281
00:24:32,560 --> 00:24:36,160
branching or all these things you're used to, you won't let you because we're locking you down.

282
00:24:36,160 --> 00:24:41,520
And I don't think those tools usually succeed. Like usually you want your tools to meet data scientists

283
00:24:41,520 --> 00:24:53,120
where they are. On that topic, I guess maybe a couple of ways to go into this conversation,

284
00:24:53,120 --> 00:25:01,040
like where do you think data scientists are with regard to the, you know, the kind of the software

285
00:25:01,040 --> 00:25:06,240
development tool chain? Like that's been something that's been evolving over the past few years.

286
00:25:06,240 --> 00:25:11,360
There was a point in time which Git was a foreign entity to a lot of data scientists. Now it's

287
00:25:11,360 --> 00:25:18,000
becoming more common for data scientists to use Git. And I often see it varies from organization

288
00:25:18,000 --> 00:25:22,960
to organization, you know, some organizations will want all of their data scientists to use Git

289
00:25:22,960 --> 00:25:28,480
and IDE's others are, you know, hey, we're going to figure out how to make the Jupiter notebook

290
00:25:28,480 --> 00:25:37,040
into the Uber IDE and, you know, production allows notebooks and things like that. Any, any observations

291
00:25:37,040 --> 00:25:43,120
on that? Yeah, I would say I've seen a remarkable like compared to like six years ago or

292
00:25:43,120 --> 00:25:48,720
remarkable like gathering around Git. I think six, six, eight years ago, you could be like, we're a

293
00:25:48,720 --> 00:25:54,000
data science team, but all of our code shared in a share point, you know, and like that would,

294
00:25:54,000 --> 00:26:00,960
I've seen it like you laugh, but I've seen it. It might have been a network drive, but it was like

295
00:26:00,960 --> 00:26:06,480
the data wasn't sure. It was not like a good time. I think like the message is now out that like if

296
00:26:06,480 --> 00:26:11,120
you're a data science team and you're not using Git and version control on your stuff that that's

297
00:26:11,120 --> 00:26:15,360
like you, you have a problem. I think that message has successfully gone out. I also do think in

298
00:26:15,360 --> 00:26:19,200
the last couple of years it's becoming more common for data science teams to use things like GitHub

299
00:26:19,200 --> 00:26:25,760
actions, you know, we're docker. And you know, like I think I think data scientists are like like

300
00:26:25,760 --> 00:26:29,040
we're doing a good job of making this stuff more accessible so you don't have to be a software

301
00:26:29,040 --> 00:26:36,560
engineer to use software engineer tools. And I think that's really great. But that said, I also

302
00:26:36,560 --> 00:26:40,880
don't feel like if you don't know those tools, that means you can't be a data scientist merely that

303
00:26:40,880 --> 00:26:46,000
like usually like when you're on your first job, you will pick up a lot of these things. Like maybe

304
00:26:46,000 --> 00:26:49,680
get you should probably learn Git before you like hunt your first job, but beyond that things like

305
00:26:49,680 --> 00:26:54,320
GitHub actions, docker, all that stuff like you don't need to know that when you're applying for

306
00:26:54,320 --> 00:27:00,160
jobs, a job should be teaching you how to use those tools if they need them. Yeah, yeah. And it is

307
00:27:00,880 --> 00:27:10,080
super important to realize that learning Git means some basic things. And it's there's so much to

308
00:27:10,080 --> 00:27:18,800
there's so much to like really, really, really learning Git. But I think a lot of us know enough

309
00:27:18,800 --> 00:27:23,920
Git that we can do the usual things. And if we get into trouble, then it's stack overflow with

310
00:27:23,920 --> 00:27:29,680
everyone else and trying to figure out how to rebase your branches and crazy stuff. Yeah, I've

311
00:27:29,680 --> 00:27:34,720
been using Git. God, I learned it like 2012, like 10 years. And yeah, I feel like this year,

312
00:27:34,720 --> 00:27:40,560
year 10, I'm like, okay, I kind of feel like I get like, yeah, rebasing and stuff. Like I feel

313
00:27:40,560 --> 00:27:45,520
like I kind of like getting more. So like if you've been doing this for less than 10 years,

314
00:27:45,520 --> 00:27:48,960
don't feel like you should know that. And if you've been doing it for 10 years, like,

315
00:27:49,360 --> 00:27:56,160
still probably fine. Yeah, yeah. And you mentioned docker and containers,

316
00:27:56,160 --> 00:28:01,280
Kubernetes, those of all come up in this conversation. What's the relationship between those and

317
00:28:01,280 --> 00:28:08,000
desk? So, um, well, with desk, you can run desk like in a Kubernetes cluster. Like there are ways

318
00:28:08,000 --> 00:28:14,960
to like have that all work. Um, we at sat and clown kind of managed that all for you. So basically,

319
00:28:14,960 --> 00:28:22,080
you just specify, I want this AWS image size. And I want this base Docker image. And then it says,

320
00:28:22,080 --> 00:28:27,120
cool, you know, I have a desk cluster of that. Um, as, but if you don't know docker, it doesn't

321
00:28:27,120 --> 00:28:31,360
really matter that you don't, you don't need to know docker to know, I need this particular base

322
00:28:31,360 --> 00:28:36,640
image. But if you're like, I want to specify exactly what's on these desk clusters when they start.

323
00:28:36,640 --> 00:28:40,960
Like that is a thing with sat and cloud, you'd specify through like a Docker container or,

324
00:28:40,960 --> 00:28:46,480
you know, a post build file. And I think there is, I do think data scientist, regardless of type

325
00:28:46,480 --> 00:28:51,120
of data scientists, it is a good thing to learn, like a little bit of Linux commands. Like what does

326
00:28:51,120 --> 00:28:55,440
pseudo apt get mean? What does, you know, blah, blah, blah, because you start to learn that, then you

327
00:28:55,440 --> 00:29:00,800
can pretty easily make Docker files and Docker containers. And, you know, do like kind of speak

328
00:29:00,800 --> 00:29:04,800
the common language of a lot of these tools. Um, so I would recommend learning that kind of stuff.

329
00:29:04,800 --> 00:29:08,880
But again, I don't think that's something you need to learn. Um, before you get a job that,

330
00:29:08,880 --> 00:29:12,960
that really, I would consider a skill you can learn while you are, you know, growing in your career.

331
00:29:14,400 --> 00:29:20,160
And so does this, is Saturn based on Kubernetes and all that stuff? Or is it using its own

332
00:29:20,160 --> 00:29:28,960
Pixie dust under like under the covers? I am not on the engineering team. So I, I don't want to say,

333
00:29:28,960 --> 00:29:33,200
because I know I'm going to get it slightly wrong, but I believe we do have a Kubernetes cluster.

334
00:29:33,200 --> 00:29:37,360
And like, well, what we usually do is we, you know, because a lot of times we are being hired by

335
00:29:37,360 --> 00:29:41,760
enterprise to like run, like an enterprise doesn't want to have their data pass all the way to some

336
00:29:41,760 --> 00:29:48,160
other persons, AWS, you know, whatever. So we install Saturn cloud in your company's environment.

337
00:29:48,160 --> 00:29:52,240
And I think we set up one Kubernetes cluster within that like BPC or whatever.

338
00:29:52,960 --> 00:29:58,240
But I'm just spouting words. I don't know exactly the intricacies of this. I'm much more on the

339
00:29:58,240 --> 00:30:03,040
using Dask than setting it upside. Got it. Yeah. And we, I think we, we skip right by that.

340
00:30:03,040 --> 00:30:06,400
What is your specific role at the company? What are you focused on?

341
00:30:06,400 --> 00:30:10,560
Oh, yeah. So as head of data science, I help the data science team. So we do things like,

342
00:30:11,360 --> 00:30:16,400
you know, helping customers get up to speed with Dask and Saturn cloud.

343
00:30:16,400 --> 00:30:19,920
We also, you know, develop white papers, blog posts, things like that.

344
00:30:19,920 --> 00:30:24,320
So like we're kind of like the internal team that uses this tool and helps other people use this tool.

345
00:30:24,800 --> 00:30:29,040
And in addition to that, it's a lot of product development too, because this is a tool for data scientists.

346
00:30:29,040 --> 00:30:32,480
So if we're using this product and we're like, oh, we find this part of it, you know,

347
00:30:32,480 --> 00:30:37,040
complex or unintuitive, then we are the ones to kind of say, hey, we should be prioritizing this kind

348
00:30:37,040 --> 00:30:43,520
of solution. Which is again, it's nice because I think compared to a lot of technical products for

349
00:30:43,520 --> 00:30:47,840
data scientists are still built by software engineers. And so you kind of get that feel of like,

350
00:30:47,840 --> 00:30:52,160
are they? I'm not sure the people designing this understand what data scientists do every day.

351
00:30:52,160 --> 00:30:55,920
And I do not feel that that was the case with our product. I feel like, you know, well,

352
00:30:55,920 --> 00:31:00,640
I like to think I'm doing my job, which is making that easy. But, you know, who's, who's to say,

353
00:31:00,640 --> 00:31:13,040
really? Is there a particular use case or a set of use cases that you see more commonly

354
00:31:15,280 --> 00:31:22,080
among desk users? Are there, are there, you know, are there specific

355
00:31:23,360 --> 00:31:27,600
signals or indicators that say, hey, this is probably going to be a good place to apply to ask?

356
00:31:27,600 --> 00:31:33,600
Yeah. So I think that, I mean, it's kind of the same as like any parallel distributed computing,

357
00:31:33,600 --> 00:31:38,000
which is if you're running out of memory, because one machine isn't enough,

358
00:31:38,800 --> 00:31:42,880
then desk is a good solution because it can, it could spin up to arbitrary amounts of memory,

359
00:31:42,880 --> 00:31:46,640
right? Because everything is split over a bunch of computers. So if you're running out of memory,

360
00:31:46,640 --> 00:31:51,200
that's a good sign you need to ask. Or if you have one, like one thing that's running,

361
00:31:51,200 --> 00:31:55,440
but it's taking forever. And it, you know, you're like, man, I could probably chop this up into

362
00:31:55,440 --> 00:32:00,560
smaller tasks. Then desk is great, because you can, if you can swap, chop it up into smaller tasks,

363
00:32:02,000 --> 00:32:07,040
you can then put it in a desk cluster. And including like most common pandas commands and stuff,

364
00:32:07,040 --> 00:32:12,320
pandas, you can switch pandas to desk and then use these sorts of tools. So that stuff's a

365
00:32:12,320 --> 00:32:17,440
pretty good sign. Also, I will say, you know, with Saturn Cloud, we have it all set up to use GPUs

366
00:32:17,440 --> 00:32:22,480
just as easily. Like with no setup, it's just you hit go in the GPU CUDA drivers or whatever

367
00:32:22,480 --> 00:32:26,640
already correctly installed and stuff. So, you know, if you're finding, you're like, hey,

368
00:32:26,640 --> 00:32:30,880
I need a train 10 neural networks and compare how well they do. Well, like wouldn't be nice to train

369
00:32:30,880 --> 00:32:36,480
all those 10 at the same time at one desk cluster. Or, you know, you can, in fact, in PyTorch and

370
00:32:36,480 --> 00:32:41,440
stuff say, hey, I have this one neural network and this neural network is too big for one. Even a

371
00:32:41,440 --> 00:32:47,520
computer with several GPUs is not enough GPUs for this, this neural network to train. And so you can

372
00:32:47,520 --> 00:32:51,760
get a desk cluster where each computer has a bunch of GPUs and like you connect all two. I don't

373
00:32:51,760 --> 00:32:55,680
think that's a common use case. But like, you know what, if you're there, if you're like, I just

374
00:32:55,680 --> 00:33:01,200
do not have GPUs running at the same time to do this kind of work. And I think desk is a really

375
00:33:01,200 --> 00:33:10,160
nice solution for a lot of those. And is desk equal useful for folks that are doing kind of

376
00:33:10,160 --> 00:33:17,360
traditional tabular data and analytics types of workloads as opposed to neural nets,

377
00:33:17,360 --> 00:33:22,320
media files, that kind of thing, images. Yeah, I think it's more one or the other.

378
00:33:22,960 --> 00:33:26,400
No, you can do it. If you're not doing like, you know, networking stuff, that's still totally

379
00:33:26,400 --> 00:33:32,000
useful. It's, it's, you know, there is a penalty. If you're switching stuff to like distributed

380
00:33:32,000 --> 00:33:36,880
parallel thinking, there's a penalty if it's going to take you a little bit of time to like rewrite,

381
00:33:36,880 --> 00:33:41,440
you know, rethink the code of like, hey, this one for loop. Okay, yes, I can split it or whatever.

382
00:33:41,440 --> 00:33:45,120
You know, can I spend this for like, you got to put a little thought into it. And I think, you

383
00:33:45,120 --> 00:33:49,520
know, if you're training on like a 50 kilobyte file, then like, I don't think you're going to get

384
00:33:49,520 --> 00:33:54,240
much benefit out of the effort of switching to desk. But I do think if there's a lot of situations

385
00:33:54,240 --> 00:33:58,720
where it's like, I have a huge data set. And yeah, what I'm doing is training a linear regression.

386
00:33:58,720 --> 00:34:02,400
You know, I have a huge data set, but what I'm doing is simple. Even in that case, because the data

387
00:34:02,400 --> 00:34:08,400
is so big, it is still faster or more convenient to just have it go on desk, rather than trying to

388
00:34:08,400 --> 00:34:17,760
get it to take forever on one machine. Is it used equally in the data processing

389
00:34:19,680 --> 00:34:26,400
transformation part of the workflow or exploratory analysis part of the workflow,

390
00:34:27,440 --> 00:34:32,640
training part of the workflow? Is there any particular sweet spot or? Yeah, so so a couple good

391
00:34:32,640 --> 00:34:36,800
a couple good places training models can be very good on desk because you can train a bunch of

392
00:34:36,800 --> 00:34:42,000
models at the same time or you can train one model across a bunch of them. And we have customers

393
00:34:42,000 --> 00:34:48,000
who do that at Saturn cloud. Another thing that's really useful for is, yeah, like a scheduled daily

394
00:34:48,000 --> 00:34:52,560
processing job, right? Where if every day you're taking your entire company's data and you're trying

395
00:34:52,560 --> 00:34:57,360
to calculate some aggregated statistics and then put that into a new table, you can schedule that

396
00:34:57,360 --> 00:35:02,000
to run every day and have it scheduled to run on a desk cluster and then it will be faster and you

397
00:35:02,000 --> 00:35:07,200
won't run out of memory and things like that. And then at Saturn cloud, we have customers who

398
00:35:07,760 --> 00:35:12,400
don't use desk at all. They just like the ability to just start and stop data science instances

399
00:35:12,400 --> 00:35:16,880
and start and stop GPU instances and start and stop instances that have like 500 gigs of RAM.

400
00:35:16,880 --> 00:35:22,400
So just having for us, just having the ability to have one computer easily accessible in the cloud

401
00:35:22,400 --> 00:35:28,640
has been useful. But that is all just to say that there's a lot of utility in having these

402
00:35:28,640 --> 00:35:32,320
sorts of data science resources readily available for data scientists.

403
00:35:34,320 --> 00:35:44,400
Cool. Any thoughts or thoughts on where the tool is going and what folks should look out for?

404
00:35:46,160 --> 00:35:51,920
So I think it's just one of these things where it only came out in 2018. So it's still quite young,

405
00:35:51,920 --> 00:35:57,120
which is just to say I think there's going to be a lot more taking other packages and

406
00:35:57,120 --> 00:36:03,600
getting them to run within desk directly. I think, you know, continuously making the tool easier

407
00:36:03,600 --> 00:36:10,080
to use, creating new documentation, things like that. And you know, so Saturn cloud uses desk,

408
00:36:10,080 --> 00:36:14,480
but we're not the only company that does. And so there's lots of people contributing to this,

409
00:36:14,480 --> 00:36:18,480
which is kind of cool. I've never worked at a company where we were so directly involved in the

410
00:36:18,480 --> 00:36:22,480
open source community. And that's been, I know it's been a lot of fun. I've been really happy with that.

411
00:36:22,480 --> 00:36:29,040
Mm-hmm. Nice. Nice. Well, Jacqueline, thanks so much for taking the time to chat with us and

412
00:36:29,040 --> 00:36:33,840
bring us up to speed on desk. No problem. No, it's been a lot of fun being back on the show.

413
00:36:33,840 --> 00:36:37,840
Yeah. And now at what point do we become friend of the show, Jacqueline Nollis? Is it like one more?

414
00:36:40,880 --> 00:36:43,680
You are friend of the show, Jacqueline Nollis. Absolutely.

415
00:36:43,680 --> 00:36:53,520
Great. Well, yes, thank you for having me. Thank you.

