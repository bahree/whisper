WEBVTT

00:00.000 --> 00:17.120
All right, everyone. I am here with Conrad Tolmar. Conrad is a research director at Electronic

00:17.120 --> 00:24.080
Arts or EA, as you may know it, as well as an associate professor at KTH. Conrad, welcome

00:24.080 --> 00:31.920
to the Twimal AI podcast. Thanks Sam. Thanks for inviting us. It's lovely to be here. I'm really

00:31.920 --> 00:37.600
looking forward to digging into our conversation. We'll be talking about as the audience might imagine,

00:37.600 --> 00:44.240
the intersection of AI and games. Before we do, I'd love to have you share a little bit about your

00:44.240 --> 00:52.160
background. I mentioned KTH. What is KTH? KTH is Royal Institute of Technology in Stockholm. It's

00:52.160 --> 01:03.040
a technical university where I did my undergraduate as well as my PhD. I think my interest for AI started

01:03.040 --> 01:11.280
quite a long time ago, starting with computer vision. I've always been passionate about photography,

01:12.160 --> 01:19.600
and I saw them there was an opportunity to combine my kind of interest for photography with my

01:19.600 --> 01:33.360
academic. That's my starting point here. Nice. Tell us a little bit about the kind of research

01:33.360 --> 01:43.840
that interests you in your professorship and in your graduate studies. My PhD was about media

01:43.840 --> 01:52.800
spaces and we built different kind of interactive environments to connect places with video streams,

01:52.800 --> 02:00.000
but also being able to use sensors to convey other kinds of information if you're close or if you're

02:00.000 --> 02:06.320
in the proximity of the space and so forth. That led me eventually to explore that further

02:06.320 --> 02:13.520
after my I graduated and I spent some time working in smart and interactive environments.

02:14.560 --> 02:22.080
Some of these were for play and some were for more like everyday use and I think some of us could

02:22.080 --> 02:31.520
remember recall the kind of demos you saw out of MIT's Media Lab on the late 90s. A lot of

02:31.520 --> 02:38.880
this was kind of cool demos, but we were not really able to build any kind of more intelligent

02:38.880 --> 02:45.840
environments in this way. I then got the opportunity to do my postdoc at MIT AI Lab with Trevor

02:45.840 --> 02:53.200
Dorrell where I saw an opportunity to dwell into more the underlying technologies and start to

02:53.200 --> 03:00.320
really use computer vision and start to understand how you can track movements in the room, for example,

03:00.320 --> 03:08.800
and then see how this could kind of be come as an input in these kind of interactive spaces.

03:10.080 --> 03:17.280
Nice. That reminds me a little bit of as an undergrad, I went to RPI, Rensselaer, in New York,

03:17.280 --> 03:25.440
and they had interactive design program and they would put these installations up in the library.

03:25.440 --> 03:32.240
This was before a lot of modern things, like connect and other things, but they were always

03:32.240 --> 03:41.760
really interesting. And imagining now as you're describing this intersection of those kinds

03:41.760 --> 03:48.560
of installations and intelligence and machine learning like we've got now,

03:48.560 --> 03:53.680
is that what kind of connected you to games?

03:54.960 --> 04:01.760
Somewhat. I think we need to take a few more steps here, because I guess one of the

04:01.760 --> 04:08.560
really great opportunities, even if we didn't have AI as today. I mean, this was 20 years ago,

04:08.560 --> 04:14.800
almost. So we were just in the beginning of like starting to apply. I mean, the first versions

04:14.800 --> 04:21.680
of OpenCV came out, and we started to play around with this. But we were also able to get

04:21.680 --> 04:29.600
some kind of new cold hardware. So one of the products where we got box from Nokia, which was

04:30.240 --> 04:36.400
the very first camera phones that came out. We also need an integrated web browser.

04:36.400 --> 04:42.000
And this box was basically labeled feel free to play. So what do you do with the camera phone

04:42.000 --> 04:47.120
with off? And then we scratch our head for a bit. And we came up with this idea of, well,

04:47.120 --> 04:52.160
maybe you can search with images rather than keywords. And you take a picture of something.

04:53.040 --> 05:00.240
And that become your sort of research here. And these kind of things. And we also had a very early

05:00.240 --> 05:07.440
version of something similar to the connect camera, like a 3D camera, where we can track body

05:07.440 --> 05:13.040
movements. And we looked upon, okay, how can you use these body movements to interact with this

05:13.040 --> 05:21.040
3D world? And I think that was one of the first kind of things where I saw a bit of a connection

05:21.040 --> 05:27.920
to gaming, because we were trying to set up experiments around this and started to study sort of

05:27.920 --> 05:36.080
how people felt about navigating in a 3D world with your body gestures instead. And we played

05:36.080 --> 05:41.840
with different kind of tools that were available back then. But sort of in the end of the day,

05:41.840 --> 05:48.400
the one that we picked was actually a half-life. So we hacked half-life. And that become like the

05:48.400 --> 05:56.720
engine that we used for to create this more like 3D experiments. And this is something that I've

05:56.720 --> 06:04.400
been carrying with me ever since. And even fairly recently, we did studies on simulators for

06:04.400 --> 06:10.960
self-driving cars. We have been using game engines rather than these kind of physics simulators

06:10.960 --> 06:18.400
that is quite typical in automotive industry. Because somehow, if you're no more looking into

06:18.400 --> 06:27.280
sort of the user experience of these environments, they are the best. And maybe this has kind of

06:27.280 --> 06:33.280
brought me here at the EA to start to then look into more like what could we do with AI into the

06:33.280 --> 06:40.720
games as it is now. Got it, got it. And so tell us a little bit about your role at EA and what

06:40.720 --> 06:48.800
your focus is there. Well, we look into, I'm part of an applied research team called SID.

06:50.080 --> 06:55.680
And we look into all kinds of different emerging technologies that eventually could be used

06:55.680 --> 07:04.560
in the games. So we follow state of the art. We most of well several laws has PhD background.

07:05.680 --> 07:11.280
We research and we also contribute them. We publish papers and so forth. Maybe not the same

07:11.280 --> 07:17.840
extent as you would if you would be in there like an academic research lab. But so we stake

07:17.840 --> 07:23.120
connection with the community. We look upon these techniques and see how we can eventually use

07:23.120 --> 07:30.640
them into the games. And there are lots of different ways that AI can be used in games. What

07:31.680 --> 07:38.480
area of focus do you, what's the kind of more specific area of focus for you and your group?

07:39.440 --> 07:47.200
Well, as you say, there is plenty to do or there is plenty of opportunities nowadays. I mean,

07:47.200 --> 07:55.440
with this kind of formal explosion of AI over the last decade, it's rather a bit hard to pick

07:55.440 --> 08:04.080
what to do and what to not to do because you can do so many things. But I guess, I mean, people

08:04.080 --> 08:13.360
maybe think about AI more into the games. And this is something that we also look into. But for us,

08:13.360 --> 08:23.360
AI has also a fundamental role when it comes to creating the games. I mean, the kind of games that

08:23.360 --> 08:32.080
we do, they're massive. And the production of them are also very long and very costly. And

08:32.080 --> 08:41.600
there is a lot of things that could be done with AI into this. So we can use AI to create assets

08:41.600 --> 08:48.720
of all kinds. We can create, I mean, different kind of media speech. You can create animations

08:50.240 --> 08:56.400
and so forth. But you can also then use AI to test these assets. If they look okay,

08:56.400 --> 09:01.520
if they behave okay. And then you put them into the game and then you can start

09:01.520 --> 09:07.840
continue to look into, okay, how do they behave? How do they affect the gameplay experience?

09:07.840 --> 09:17.440
And then you can march on. And when you eventually have a game, I mean, collecting all kinds of

09:17.440 --> 09:22.320
information around the game, how people experience the game, what they like about the game, what they

09:22.320 --> 09:29.200
don't like about the game, how the game works out. Doing that data analytics is also, of course,

09:29.200 --> 09:39.760
a big part of how we apply AI at EA. When I think about games, this is perhaps true for many folks

09:39.760 --> 09:47.440
in audience, I think about deep reinforcement learning. To what degree are the, is the work

09:47.440 --> 09:55.680
that you're doing centered on the application of reinforcement learning? Yeah, thanks for asking.

09:55.680 --> 10:02.160
And yeah, it's definitely reinforcement learning is on the top of our agenda as well.

10:02.160 --> 10:06.880
However, it's not that easy to use. I mean, we play around with it and you know,

10:08.720 --> 10:15.520
the work that has been going on the past, you can show where you can demonstrate the feasibility.

10:16.560 --> 10:22.400
But then turn it into something that actually works in the game. So that's a bit of a different

10:22.400 --> 10:35.200
matters here. So you mentioned that testing is one of the areas that you focus on and you've

10:35.200 --> 10:42.880
published several papers on the application of machine learning to testing games. Can you give us

10:42.880 --> 10:54.480
an overview of that general area game testing and how AI fits in? Yeah, of course. So game testing

10:54.480 --> 11:02.240
came out actually out of some experiments we did a couple of years back when we looked into how to use

11:04.000 --> 11:11.600
reinforcement learning into the games. And we were successful in actually creating some NPCs,

11:11.600 --> 11:18.000
non-playable characters and put them into the game. And they were trained with reinforcement

11:18.000 --> 11:25.920
learning. But we also learned that it's quite hard actually to fine tune their behavior. So

11:26.800 --> 11:36.640
they actually behave in a way that is believable and that gamers like. And I think here you have

11:36.640 --> 11:43.200
a bit of a different twist as well. I mean, typically when you work with AI, you would like to

11:43.200 --> 11:50.560
make AI that if you have it in a self-driving car, for example, the car should behave like we drive

11:50.560 --> 11:58.960
them or preferably even better. AI into the game have somewhat different constraints. They all

11:58.960 --> 12:05.200
don't always have to work perfectly like that, but they need to be believable. And that is kind of

12:05.200 --> 12:13.280
one thing that we learned and that we can create agents that behave well, but still there is

12:13.280 --> 12:21.360
the subtle qualities or lack of qualities in how they behave that still gamers reacted towards.

12:22.240 --> 12:29.760
So we decided then to take a bit of a step back and order and look upon, okay, how could we use

12:29.760 --> 12:37.440
this agent maybe to start at least to explore testing the games with the agents. So this is

12:37.440 --> 12:43.920
something that we've been doing then for like two years or something like that now with in different

12:43.920 --> 12:54.400
flavors. What does it mean for the characters to be believable? Is it simply a matter of them not

12:54.400 --> 13:04.080
being too good and kind of having their own, you know, foibles or are there nuances to that aspect

13:04.080 --> 13:13.760
of believability? Yeah, so I think there's two parts of that. One is how they behave individually.

13:13.760 --> 13:25.200
And they need to walk as you expect them to do. So I mean, of course, I think we all tend to

13:25.200 --> 13:31.520
behave somewhat different in a game, but I mean, if you walk out in the real world, I mean,

13:31.520 --> 13:39.040
you follow a path and you typically don't walk over a piece of grass. And even in games, that behavior

13:39.040 --> 13:47.600
is happens again. So if this agent just walks straight and doesn't kind of have any

13:48.320 --> 13:56.400
tendency of variations so that we expect humans to have, then they start to appear a bit oddly.

13:57.360 --> 14:02.800
So that is one part, but then it's also, and this is even harder, how they act in the group.

14:02.800 --> 14:12.320
So if you have a group of agents, they need to kind of somehow understand each other. So we

14:13.280 --> 14:19.920
think that they have some kind of shared context or shared goal. If they're just running around on

14:19.920 --> 14:27.280
their own, they will behave also quite odd. At least the behavior of them will look like a bit odd.

14:27.280 --> 14:34.640
And when you think of them in groups, I'm imagining you were talking about both groups of these

14:35.680 --> 14:39.120
NPCs as well as when they're collaborating with humans.

14:40.560 --> 14:48.880
True. Collaborating with humans is also quite difficult because to read the gamers' intent

14:48.880 --> 14:55.600
and to participate in that gameplay, I think that it's still a bit further down the road.

14:55.600 --> 15:02.560
You can have them as companions in the games. And I think you see more and more games now

15:02.560 --> 15:12.800
that start to adapt that. So some of the players are AI agents and some of real humans.

15:12.800 --> 15:22.160
But again, to make this believable, it's depending on the game and depending on what task they have.

15:22.160 --> 15:28.560
If you have a crowd that follows you is one thing. If you should sort out some collaborative task

15:28.560 --> 15:36.560
that is very hard to do today. But on the other hand, if you twist this around, again, if you look

15:36.560 --> 15:44.640
into game testing, some of these constraints you can relax tremendously. Because even you can say

15:44.640 --> 15:51.600
that, well, it's sometimes good that these agents doesn't behave like humans or like everyone would

15:51.600 --> 15:58.480
do. Because you know, the way we test games is too folded. Firstly, I mean, and the most part

15:58.480 --> 16:05.840
of the testing is done by having a notch amount of human testers sit and testing through the games

16:05.840 --> 16:14.000
and through different scenarios or different parts of the game. But then we also work with

16:14.000 --> 16:20.560
some kind of automated tools. So we can create agents, script the agents that could solve some

16:20.560 --> 16:28.000
tasks. And then you can sort of drop them into the game and they you can see that the navigation

16:28.000 --> 16:33.520
mesh, for example, in the game works. So they can take they can go from one position to another

16:33.520 --> 16:41.040
position, but then when you work with reinforcement learned agents, sometimes it's good that they

16:41.040 --> 16:46.880
don't behave as humans. Because then they will find bugs that typically humans will not find.

16:46.880 --> 16:53.120
So they won't walk through a wall. That human testers might not do. You see a wall

16:54.320 --> 16:59.840
and then you'd walk around it, but the agents doesn't care. So they just walks straight

16:59.840 --> 17:05.520
through the wall instead. And that's how you sort of find this exploits in the game.

17:06.320 --> 17:14.320
Interesting, interesting. So when we're talking about testing in the context of,

17:14.320 --> 17:21.840
well, specifics of some of the articles that you share that we'll be talking about,

17:21.840 --> 17:28.080
the improving play testing coverage paper and the augmenting automated game testing paper,

17:28.080 --> 17:37.040
are we talking about using deep learning agents, for example, to test the games broadly,

17:37.040 --> 17:44.240
or are we talking about testing the NPCs in the games using other AI?

17:46.240 --> 17:52.320
I think what we need to realize there, there is a couple of challenges that we need to work on.

17:53.280 --> 17:59.600
And one of them is like a lot of the academic research about reinforcement learning agents

17:59.600 --> 18:09.120
and testing or playing games are still fairly simple games. And if you go into and what

18:09.120 --> 18:18.320
would like to apply that in like a full 3D game with a lot of other things, a lot of other

18:19.200 --> 18:29.120
agents and a lot of dynamics, that is way harder. And then you also need to make this work

18:29.120 --> 18:35.520
on to the game engines that we are using. So it eventually could be used in runtime.

18:38.720 --> 18:45.440
Got it. So I think you're suggesting that the Atari games that we associated with

18:46.240 --> 18:54.480
reinforcement learning are far simpler than the typical game that EA is producing today.

18:54.480 --> 19:03.040
And I get that. I think you mentioned I'm trying to confirm what I thought I heard,

19:03.040 --> 19:10.560
which was a couple of different use cases. One is that you want to introduce these NPCs,

19:10.560 --> 19:14.880
these automated characters into the games, and you need some way to test them.

19:16.480 --> 19:23.360
And then I think I heard separately that you want to be able to test the game

19:23.360 --> 19:28.160
to generally make sure that the walls don't let you walk through them. And that's potentially

19:28.160 --> 19:35.040
another area for the use of machine learning or reinforcement learning. And I'm curious which of

19:35.040 --> 19:44.240
those are you working on? Are you working on both of those? And what that, you know, so I'm curious

19:44.240 --> 19:52.080
to explore those areas. Yeah. So we have published a couple of papers recently and one was on

19:52.080 --> 19:58.720
called last year, where we basically just introduced this notion of using reinforcement learning

19:58.720 --> 20:07.520
agents into 3D games and looked upon what kind of test you can do with them. And then we saw

20:07.520 --> 20:13.680
there was a couple of things you can do. I mean, you can see what kind of coverage they have.

20:13.680 --> 20:23.360
They could find exploits in the games. You can find spaces, for example, where players might get stuck.

20:24.960 --> 20:31.280
But also other things like that. So that was the first stepping stone. And then from that,

20:31.280 --> 20:39.360
we have more looked into how you can generalize this in a couple of different ways. One is like,

20:39.360 --> 20:50.000
like we talked a bit about like an agent. If you train an agent, a typical behavior is then that

20:50.000 --> 20:58.720
it would maximize the sort of the how it goes from A to B. Not on the straight line maybe,

20:58.720 --> 21:07.040
but like as quick as possible. And this is typically not the way we play games. So a more

21:07.040 --> 21:13.440
human-like way to play games is to explore the game somewhat. And maybe you would like to check

21:13.440 --> 21:19.840
whether a place to go is safe enough and you kind of sneak in and sneak out and go around it

21:19.840 --> 21:28.960
in different ways. So that has been one of the explorations to look upon a model that is driven

21:28.960 --> 21:40.560
by curiosity instead of an optimized path as it is. And so tell us a little bit about that work

21:40.560 --> 21:51.200
and the idea of these models that are curiosity driven. How does that differ from kind of this

21:51.200 --> 21:58.400
classical idea of tuning your explorers, type of parameters in reinforcement learning?

22:00.480 --> 22:07.600
It goes along those lines of course, but what we needed to try out here is first kind of to what

22:08.480 --> 22:17.040
sort of what coverage these agents could have. But then as another problem that the rise here is like

22:17.040 --> 22:23.600
when you train these agents and you test the games with these agents, the agents themselves will

22:23.600 --> 22:31.440
not tell you really what they have done. So if an agent goes through a wall for example or if an

22:31.440 --> 22:39.200
agent doesn't find a spot, they will not tell you this. So a second part on this work was really

22:39.200 --> 22:49.120
about also how could you visualize the behavior of these agents. So a game designer eventually could

22:49.120 --> 22:55.680
look upon these visualizations and check whether their map or the world that they have created

22:56.240 --> 23:04.560
works as they wanted it to be. And so that is also become a very important part of that work.

23:04.560 --> 23:13.680
So collect all this data, creating different kind of visualization representations. So you can

23:13.680 --> 23:22.080
find trajectories through the maps, you can find like heat maps where people where agents are

23:22.080 --> 23:35.040
or haven't been and so forth. When you're creating an agent to explore a video game, are you and in

23:35.040 --> 23:49.760
your paper in particular, are you substituting some notion of score based reward system for one

23:49.760 --> 23:55.040
that's purely curiosity based? Do you like do you not care about the score if the agent you know it

23:55.040 --> 24:01.280
gets great coverage of the the game or is there also do you also need to build in some notion of

24:01.280 --> 24:07.280
score so that you're you know testing the things that humans would more likely do.

24:10.560 --> 24:17.840
Yes and no. I think still we are in the face of that this exploration that these agents are doing.

24:17.840 --> 24:26.720
It's fairly early on and make them actually to play the game in full. So they collect

24:27.280 --> 24:34.240
like levels and scores as you would normally do in a game. That is still research to come.

24:34.960 --> 24:41.920
So at this point we more looking into how we can make them navigate around and explore the

24:41.920 --> 24:50.720
space in the best way and also being able to I would say to if you look upon games it's not

24:50.720 --> 24:57.440
just running around in the games anymore. I mean typically in the game you have other kinds of

24:57.440 --> 25:07.040
ways to navigate. You have different kind of instruments. You might have elevators that could

25:07.040 --> 25:14.560
take you from one level to another. You might need to climb. Some of the new games also enable you

25:14.560 --> 25:22.960
to fly through the game so you have a jetpack or something like that and this more advanced

25:22.960 --> 25:30.880
form of navigation is still I would say very much work in progress on our side.

25:30.880 --> 25:38.640
I think I hear you trying to temper my enthusiasm or my sense of where things are with the

25:38.640 --> 25:47.280
class of games that you're working with. Maybe take us back and walk us through where you are

25:47.280 --> 25:56.400
with the agents and the types of games that you are focused on at EA. How far along are you and

25:56.400 --> 26:04.880
what are the key challenges that you run into when you kind of scale from Montezuma's

26:04.880 --> 26:16.400
Revenge to a modern video game? Right. Like we talked a bit about the way you navigate around in

26:16.400 --> 26:26.720
the space kind of comes down to the person now and what kind of game archetype you are if you're

26:27.360 --> 26:35.120
an aggressive player or if you're a cautious player. So this is very things that we are very

26:35.120 --> 26:42.960
much interested in where we look into now. Whether you with some form of imitation based learning

26:42.960 --> 26:51.280
demonstration by game testers or even the game designers could pick up some of these

26:51.920 --> 26:59.440
archetypes and then use this data when we train the agents. So the agents actually tend to

27:00.480 --> 27:07.200
kind of get this kind of more multitude of character when they test or play the game.

27:07.200 --> 27:18.080
So that's that's one thing but another thing that we also look into is one of the problems with

27:18.080 --> 27:22.960
reinforcement learning and training of reinforcement learning agents. I think a lot of us

27:24.000 --> 27:29.360
have discovered is that you do play, you train it on a fairly static environment.

27:29.360 --> 27:40.800
So and if you change something in the environment, you need to retrain the old sort of model again.

27:40.800 --> 27:49.680
And this is kind of very unpractical if you're in a game development cycle where you continuously

27:49.680 --> 27:58.000
change the games and the maps and you make updates and fix bugs and so forth. So this is something

27:58.000 --> 28:07.440
that we also are very much interested in how we can get models that is more adaptive to dynamic

28:07.440 --> 28:18.000
environments. Yeah. And how do you go about doing that? Well, I mean, there is a couple of

28:18.000 --> 28:28.240
different ways you can do that. I mean, one of the known techniques is called poet and that is

28:28.240 --> 28:38.480
kind of used to kind of dynamically change the complexity in the environment. So the agents encounter

28:38.480 --> 28:46.240
more and more complex environment to train on. Our approach is somewhat has drawn quite a lot

28:46.240 --> 28:53.040
of inspiration from poet, but has a bit of a different take in that we take two reinforcement

28:53.680 --> 29:02.560
agents. One is we call the generator and another one is called we as solver. So the generator

29:02.560 --> 29:12.160
basically creates games. So using procedural generated algorithm to create game elements.

29:12.160 --> 29:22.160
And the other agents try to solve this. And then they could work together as a pair in different

29:22.160 --> 29:32.240
ways. So one is you can create models that handle more dynamic environment. But you can also use

29:32.240 --> 29:40.800
this to kind of create levels of various kinds. So think about core ways, for example, where I

29:40.800 --> 29:47.520
have one of the agent is kind of creating the track for you. And the other agent is try to drive

29:47.520 --> 29:56.000
this track. And then they could work in a pair. And you can in that also try out how difficult this

29:56.000 --> 30:04.160
track is by having different agents test the track. And then over manipulation, you can then

30:04.160 --> 30:11.600
determine, well, this is a track level 10 or this is a track level five or something like that.

30:11.600 --> 30:20.000
So this is how you can eventually also then implement it in the game to not only test the game,

30:20.000 --> 30:29.600
but you can also hear tests sort of the complexity in the game. And then appropriately

30:29.600 --> 30:37.760
set different scores on how difficult the tracks are. Got it, got it. So it sounds like that last

30:37.760 --> 30:47.760
example ties into it's got this element of you training up an agent that can allow you to

30:47.760 --> 30:54.880
assess the complexity of a given environment. But you could also start to leverage it for

30:54.880 --> 31:04.880
creating new environments either in the studio or from the game itself. Am I hearing that?

31:04.880 --> 31:12.000
From the game itself, yeah. And I think this is for us a tool that could be used in different ways.

31:12.000 --> 31:19.680
I mean, you can imagine that this could eventually be a stepping stone into real game AI where you

31:19.680 --> 31:30.000
start to sort of adapt the environment towards the actual player and how the player plays the games.

31:30.000 --> 31:38.480
So you get a better gaming experience out of that. But you can also eventually think that this

31:38.480 --> 31:48.240
could be put into the tools when you create games. So if you put this agents into the editor,

31:48.240 --> 31:59.200
you can test sort of the map for or the environment in real time. So if you put two things to

31:59.200 --> 32:05.120
far apart, the agent will tell, well, I will never be able to jump from this to that. But then you

32:05.120 --> 32:10.480
can just in the editor, put them a bit together. And then you see, well, then you have a map that

32:10.480 --> 32:19.360
works. So you don't even have to test it eventually. Besides one of the things that we talked about

32:19.360 --> 32:30.960
this far, there are other aspects of using RL for game testing that you've kind of learned along

32:30.960 --> 32:40.320
the way. I'm curious about the practicalities of trying to apply RL in the ways that you

32:40.320 --> 32:47.520
are and are there, you know, their particular kind of, you know, lessons earned or tricks that

32:47.520 --> 32:54.400
you've come across or anything that will be worth sharing there. I think, and I heard that on

32:54.400 --> 33:01.760
the podcast here before, I mean, if you relate to self-driving cars a bit, there is one thing to

33:01.760 --> 33:09.360
test things in the lab or in an experiment. But then, of course, how you deploy it in the real world

33:09.360 --> 33:16.880
is very different. And of course, a game doesn't need to be bulletproof as a car or a self-driving

33:16.880 --> 33:23.120
car, but this should still work. I mean, the games are used by millions and millions of users,

33:23.120 --> 33:30.720
and they will find all kinds of bugs and problems in there, for sure. So the games need to be very

33:30.720 --> 33:39.520
robust and very functional. And also here, more to that, that some of these techniques are still a

33:39.520 --> 33:46.480
bit controversial, I would almost say, among game designers, because you don't really know what

33:46.480 --> 33:53.600
you get. I mean, AI is still much a black box. So if you train a model of some kind,

33:53.600 --> 34:04.800
you need to make sure that there is a way for a game designer or a tech artist that work with

34:04.800 --> 34:11.840
his assets or content that they could sort of add their flavor to it, and they could feel that

34:11.840 --> 34:24.960
they could control this, because getting to the game experience, it's a delicate piece of art,

34:24.960 --> 34:36.160
actually, to make games that feels good and are playable and are fun. And I think if you introduce

34:36.160 --> 34:43.680
a lot of AI things that kind of work, but still doesn't really behave as you think they should,

34:44.320 --> 34:50.960
I think that could rather ruin the game experience than add to it. So this is something that

34:50.960 --> 34:59.920
we need to work a lot more on how to kind of be able to control this AI agent or do its models

34:59.920 --> 35:10.720
and adapt them to different sort of context and usage. Beyond the work in reinforcement learning,

35:10.720 --> 35:20.400
you've also done some work in the application of CNNs to detecting glitches in games.

35:20.400 --> 35:26.400
Yeah, a little bit about that work. What is a glitch in the context? A glitch is,

35:26.400 --> 35:32.800
well, a glitch could be many different things. I mean, low level glitches are problems

35:33.600 --> 35:42.560
in the hardware or in the drivers. So suddenly you get the white space or a white screen or

35:42.560 --> 35:49.200
something like that. But then you have a lot of other kinds of glitches. Typically, it's more

35:49.200 --> 35:58.720
that when you create these games, there's tons of content and assets. And someone just forget

35:58.720 --> 36:06.800
to put the texture on something somewhere. So we started to then look into, okay, how could we use

36:06.800 --> 36:14.960
in reinforcement learning agent to kind of more walk around in the space to, yeah, look if everything

36:14.960 --> 36:23.680
looks okay or not. And then on top of that, that's kind of then fairly straightforward to use

36:23.680 --> 36:32.160
like a competition detector of some kind to detect when you get these glitches like a missing

36:32.160 --> 36:40.160
texture or something like that. And this is also something that is taking a considerable

36:40.160 --> 36:48.080
amount of time when we test the games. Because the games are huge and it takes a lot of time and

36:48.080 --> 36:56.560
a lot of man hours to sit and look through this kind of walkthroughs that we create over the games.

36:58.640 --> 37:05.840
So to be a little bit more concrete, it sounds like maybe an application of the work that we've

37:05.840 --> 37:13.920
talked about previously where maybe you've got this curiosity driven agent. But now the agent

37:13.920 --> 37:22.000
ends up in a position and a particular orientation. And you are able to capture an image from what it

37:22.000 --> 37:28.000
sees and then feed that into a CNN that's maybe, is it a classifier like glitch no glitch?

37:28.000 --> 37:34.800
Well, depending on the game and depending on how we trained, of course. But it's basically a

37:34.800 --> 37:43.040
classical classifier. And you also get a confidence score. So you kind of know how like

37:43.040 --> 37:52.560
accurate the detection is. And then you flag this and then you kind of walk around and find

37:52.560 --> 37:57.200
these glitches in to get the game. But then again, I mean, it depends on the game. I mean,

37:57.200 --> 38:05.120
the games are different. I mean, if you take some of the more open world games, there is a lot of like

38:05.120 --> 38:16.160
assets like houses, trees, I mean, all of that cars. But if you think about the sport games,

38:17.280 --> 38:23.200
there is other kinds of textures that you really need would like to have there. I mean,

38:23.200 --> 38:31.760
if you have longer types into the games, the closing and so on. And then you train the model

38:31.760 --> 38:39.360
for that kind of game. And you mentioned all these different classes of

38:41.040 --> 38:43.920
textures and things that you didn't insert into a game. Are you

38:43.920 --> 38:53.360
to what is your, I'm trying to think about if your model is looking for those things specifically,

38:53.360 --> 39:00.400
is it some kind of hierarchical model? Or is it just kind of looking broadly at an entire image?

39:01.600 --> 39:07.360
And just based on the training data looking for, for these glitches.

39:07.360 --> 39:14.800
Now, it's more looking broadly for glitches in general into the scene. So you have a specific

39:14.800 --> 39:21.200
camera position. And you change this camera position throughout the object. And you rotate around

39:21.200 --> 39:29.120
the object. You walk around taking different scenes. And that is then how you kind of detect this

39:29.120 --> 39:39.680
missing the textures or the glitches in the pictures. And in terms of your training data for that,

39:39.680 --> 39:47.760
do you have a historical database of glitches? Or I imagine there's a huge opportunity here to

39:47.760 --> 39:53.520
do synthetic data creation, creating artificial glitches or something like that.

39:53.520 --> 40:00.880
Where does the training data come from? I guess that's one of the upside of working at EA

40:00.880 --> 40:06.640
and work within the company. Because we have tons of data. We have tons of data from the game

40:06.640 --> 40:15.760
teams. We have tons of data from game tests. So there is a rich set of data that we can train for.

40:15.760 --> 40:24.640
But of course, we need to label them. And so forth. So in that case, we also annotate and synthetically

40:24.640 --> 40:32.480
generate glitches of various kinds. So we can train a better model, simply that perform better.

40:32.480 --> 40:39.120
So it's a bit of a mix there, actually. So we start with like what kind of glitches are

40:39.120 --> 40:48.240
typically encountered into the games. And then we augment them and sort of build up our data.

40:48.240 --> 40:53.360
That's because in the end, when you train this model, if there should be reliable,

40:54.560 --> 40:56.400
you need quite a lot of data now.

40:56.400 --> 41:10.160
Where do you see AI going in games? I think AI will go into the games in a multitude of ways.

41:11.840 --> 41:20.000
Like we talked quite a lot about here today, in how we create the games, how we create the assets,

41:20.000 --> 41:28.560
how we create the worlds, how we create the behaviors, how we create, if you look into games,

41:30.480 --> 41:39.840
that there is so many things, animations, speech, et cetera, that you can create with AI.

41:39.840 --> 41:48.320
But then eventually, we will more and more look into how AI could will be used in the games.

41:50.320 --> 41:56.720
And how it also could drive the game experience in different ways.

41:57.680 --> 42:07.440
You can imagine, for example, you introduce some AI physics. So you can create a bit more

42:07.440 --> 42:15.360
believable worlds by the parts of this that is driven by an AI model instead.

42:17.200 --> 42:23.920
And there is other things like that you can do into the games. You can, like we talked a bit

42:23.920 --> 42:32.320
earlier about, you can test assets as they are used in the games. So if you have

42:32.320 --> 42:39.600
user-generated content of some kind, that's of course not possible to test beforehand.

42:39.600 --> 42:45.520
But then you can check whether this asset or this kind of thing that someone has created

42:45.520 --> 42:52.960
works in the game. And you can help the user then to create a piece of content that actually

42:52.960 --> 42:59.680
do work in the game. I think that is also a very interesting kind of future challenge to look into.

42:59.680 --> 43:08.160
Awesome. Well, Conrad, thanks so much for joining us. It's great hearing a little bit about what

43:08.160 --> 43:18.400
you're working on there at EA and KTH and looking forward to kind of learning more as you continue

43:18.400 --> 43:31.840
to push forward in this area. Thanks for having me, Sam.

