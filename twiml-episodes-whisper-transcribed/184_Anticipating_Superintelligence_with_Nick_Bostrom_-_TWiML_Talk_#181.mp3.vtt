WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.080
I'm your host Sam Charrington.

00:31.080 --> 00:35.400
In today's episode we're joined by Nick Bostrom, professor in the Faculty of Philosophy at

00:35.400 --> 00:41.560
the University of Oxford, where he also heads the Future of Humanity Institute, a multidisciplinary

00:41.560 --> 00:46.320
institute focused on answering big picture questions for humanity with regards to AI

00:46.320 --> 00:48.120
safety and ethics.

00:48.120 --> 00:54.520
Nick is of course also the author of the book Super Intelligence, Paths Danger Strategies.

00:54.520 --> 00:59.360
In our conversation we discussed the risks associated with artificial general intelligence

00:59.360 --> 01:04.240
and the more advanced AI systems Nick refers to as super intelligence.

01:04.240 --> 01:08.720
We also discussed Nick's writings on the topic of openness in AI development.

01:08.720 --> 01:13.000
In particular the advantages and costs of opening closed development on the part of nations

01:13.000 --> 01:15.440
and AI research organizations.

01:15.440 --> 01:20.680
Finally, we take a look at what good safety precautions might look like and how we can create

01:20.680 --> 01:27.240
an effective ethics framework for super intelligent systems.

01:27.240 --> 01:32.320
Before we dive in, a final announcement for Wednesday's America's online meetup.

01:32.320 --> 01:38.240
At 5pm Pacific time, David Clement will present the paper Deep Mimic, example guided deep

01:38.240 --> 01:44.240
reinforcement learning of physics-based character skills by researchers from UC Berkeley, including

01:44.240 --> 01:47.640
former guests Peter Rebeel and Sergei Levine.

01:47.640 --> 01:52.800
For more info or to register, visit twimmelai.com slash meetup.

01:52.800 --> 01:57.720
Okay, enjoy this show.

01:57.720 --> 02:01.840
Alright everyone, I am on the line with Nick Bostrom.

02:01.840 --> 02:07.400
Nick is professor in the faculty of philosophy at the University of Oxford, as well as being

02:07.400 --> 02:12.720
director of the Future of Humanity Institute and director of the Governance of Artificial

02:12.720 --> 02:14.360
Intelligence Program.

02:14.360 --> 02:17.040
Nick, welcome to this weekend machine learning in AI.

02:17.040 --> 02:18.520
Thanks for having me.

02:18.520 --> 02:25.080
So Nick, from what I read, you had four undergraduate majors, eventually settling on physics and neuroscience

02:25.080 --> 02:28.960
for your graduate work, and now you're a philosopher.

02:28.960 --> 02:35.360
Can you start us off by telling us a little bit about your journey and your background

02:35.360 --> 02:37.400
and what you're focused on nowadays?

02:37.400 --> 02:44.600
Well, I studied a bunch of different things as an undergraduate that seemed to me to possibly

02:44.600 --> 02:49.080
be useful for eventually trying to understand big important things.

02:49.080 --> 02:53.680
I didn't know exactly which things were important to understand, but philosophy seemed to

02:53.680 --> 02:58.880
be one area, physics, another neuroscience, and AI, also useful to have.

02:58.880 --> 03:04.960
So I kind of just studied around a lot of different things to make a long story short.

03:04.960 --> 03:10.040
The thing that I'm now running the Future of Humanity Institute is a multidisciplinary

03:10.040 --> 03:12.960
research center at Oxford University.

03:12.960 --> 03:17.360
We have people from a number of different disciplines, mainly mathematics and computer

03:17.360 --> 03:23.840
science, but also some philosophers, political scientists, trying to think hard about big

03:23.840 --> 03:30.400
picture questions for human civilization, things that could affect the trajectory of Earth

03:30.400 --> 03:36.880
originating intelligent life, and AI has been a big focus, I'd almost say, obsession of

03:36.880 --> 03:39.360
ours for a number of years now.

03:39.360 --> 03:45.160
So we have both one group that is doing technical work on AI safety, AI alignment, and another

03:45.160 --> 03:48.720
group that is looking at AI governance issues.

03:48.720 --> 03:52.800
And did you study AI as part of your graduate work?

03:52.800 --> 03:54.040
I did a little bit.

03:54.040 --> 04:04.000
Yeah, this was back in the 90s, I took some AI courses back then, I then wrote a master's

04:04.000 --> 04:09.800
thesis in computational neuroscience, and then I kind of drifted away from the field a

04:09.800 --> 04:17.680
little bit, but came back to it as a focus area when I began working on my book, The Book

04:17.680 --> 04:18.680
Superintelligence.

04:18.680 --> 04:21.840
So this would have been maybe 10 years ago.

04:21.840 --> 04:26.560
However, those are not familiar with the book, you know, what were the major themes in

04:26.560 --> 04:27.560
superintelligence?

04:27.560 --> 04:36.920
The book tries to think through what happens if AI succeeds one day at its original goal,

04:36.920 --> 04:42.560
which has all along been to produce general intelligence in machine substrate, not just

04:42.560 --> 04:50.040
to substitute for human cognition in specific tasks, but to figure out ways, I think, of achieving

04:50.040 --> 04:54.840
the same powerful general learning ability, planning ability, reasoning ability that make

04:54.840 --> 04:57.840
us humans unique.

04:57.840 --> 05:03.280
So at least when I was beginning working on that book, there had been a surprisingly

05:03.280 --> 05:07.880
small amount of attention paid to what would happen if the ultimate goal were achieved.

05:07.880 --> 05:12.720
There's a lot of work trying to make AI slightly more capable, but a little thought to what

05:12.720 --> 05:17.560
would happen if we achieved human level general artificial intelligence one day.

05:17.560 --> 05:21.440
And I argued in the book that that probably would be followed within relatively short

05:21.440 --> 05:26.960
order by superhuman levels of machine intelligence.

05:26.960 --> 05:32.400
And I then, to bulk of the book, then it explores different scenarios and tries to introduce

05:32.400 --> 05:37.760
different concepts and analytic tools that one can use to try to begin to think more

05:37.760 --> 05:42.360
systematically about the issues that will arise in that kind of radically transformative

05:42.360 --> 05:43.360
context.

05:43.360 --> 05:50.600
Yeah, I'm wondering, do you consider yourself, or do you think of yourself as either pessimistic

05:50.600 --> 05:55.160
or optimistic about this, the direction that AI is taking?

05:55.160 --> 05:57.960
I'm full of both hopes and fears.

05:57.960 --> 06:05.920
I think sometimes my public persona is often believed to be more on the negative side than

06:05.920 --> 06:07.440
I actually am.

06:07.440 --> 06:14.280
That is I'm actually very excited about the myriad beneficial applications both in the near

06:14.280 --> 06:21.840
term and also over a longer period of time that one could hope to get from AI.

06:21.840 --> 06:27.680
But I think because the book had a significant focus on what could go wrong if we failed

06:27.680 --> 06:32.600
to get this transition to machine superintelligence right.

06:32.600 --> 06:37.040
Because the book spent quite a number of pages trying to get the more granular understanding

06:37.040 --> 06:40.640
of exactly where the pitfalls are there so that we could avoid them.

06:40.640 --> 06:47.640
I think then, say, journalists often come to me to get the negative scoop and once

06:47.640 --> 06:53.080
you're sort of in that pigeonhole, the journalist might ask you a bunch of questions.

06:53.080 --> 06:56.400
You say, various things, some positive, some negative, they cut out all the positive

06:56.400 --> 06:58.480
things you say.

06:58.480 --> 07:02.920
And then you appear at saying this negative sound bite and then other journalists hear

07:02.920 --> 07:07.000
that and it kind of gets self-reinforcing loop.

07:07.000 --> 07:11.400
To answer your question, no, I wouldn't consider myself as either an optimist or pessimist

07:11.400 --> 07:17.400
but I'm trying to just better understand what the spectrum of possibilities are and most

07:17.400 --> 07:25.080
importantly how actions people take or can take here and now will affect likely long-term

07:25.080 --> 07:27.120
outcomes for humanity.

07:27.120 --> 07:35.480
When you think about the kind of spectrum of outcomes and the things that could go wrong,

07:35.480 --> 07:38.200
then how do you categorize them?

07:38.200 --> 07:43.480
Is there kind of a high-level categorization that you've developed of the different ways

07:43.480 --> 07:50.320
that you think about the safety issues that we're trying to safeguard?

07:50.320 --> 07:52.960
You can carve it up in different ways.

07:52.960 --> 07:59.600
If you want a very high-level division, you could distinguish risks that come from the

07:59.600 --> 08:03.000
AI itself, that is ways that AI could harm humans.

08:03.000 --> 08:08.560
So there we have, in the context of machine super intelligence, the possibility of a failure

08:08.560 --> 08:14.480
to solve the alignment problem, failure to get this hypothetical future super intelligent

08:14.480 --> 08:17.240
system to do what we wanted to do.

08:17.240 --> 08:21.600
We can go into that in more detail later if you want, but that's kind of one category

08:21.600 --> 08:22.600
of risk.

08:22.600 --> 08:27.120
Then another would be the risk that not that the AI would do something bad to humans but

08:27.120 --> 08:30.800
that humans would use the AI to do bad things to other humans.

08:30.800 --> 08:34.360
So use it on wisely, recklessly or maliciously.

08:34.360 --> 08:38.600
The way we've used a lot of other technologies throughout human history, not just for beneficial

08:38.600 --> 08:42.400
purposes, but to which war or to oppress one another.

08:42.400 --> 08:48.040
Then I actually think there is a third category of concern, which has received a lot less

08:48.040 --> 08:54.640
attention, but this would be risks of us humans doing bad things to the AI.

08:54.640 --> 09:01.240
I think at some point these digital minds that we're building might come to process various

09:01.240 --> 09:07.800
degrees of moral status, just like, say, many non-human animals have degrees of moral status.

09:07.800 --> 09:13.840
And there is then a risk that we will maltreat, say, an AI system that is capable of some

09:13.840 --> 09:19.600
form of sentience, more of the relevant interests, capable of suffering, at least if we draw

09:19.600 --> 09:25.760
a logical scheme that should be in there as one category of potential harm that could occur.

09:25.760 --> 09:33.840
It strikes me that in order to fully wrap our heads around this issue, it's fundamentally

09:33.840 --> 09:45.440
a long-term, broad time horizon issue, there's a lot of the response that you see to these

09:45.440 --> 09:50.240
issues being raised is that we're nowhere near AGI, we have no idea how we're going to

09:50.240 --> 09:51.600
get there.

09:51.600 --> 09:58.480
When you think about the timeframe or time horizon of your research, do you put a number

09:58.480 --> 09:59.480
on that?

09:59.480 --> 10:05.680
Do you have a sense as to what's the timeframe that we need to be worried about this kind

10:05.680 --> 10:06.680
of thing?

10:06.680 --> 10:12.640
There is a lot of uncertainty about the timeline, so in organizing one's thinking, it's

10:12.640 --> 10:17.720
sometimes better not to do it with reference to, say, calendar years, but rather relative

10:17.720 --> 10:22.440
to some set of capabilities, like if and when we reach this level of technical capability

10:22.440 --> 10:26.760
than these social issues will arise.

10:26.760 --> 10:32.200
I think a lot of the confusion and some of the controversy surrounding these questions

10:32.200 --> 10:38.040
comes from completing two different contexts, the context of long-term, radically transformative

10:38.040 --> 10:42.720
machine super-intelligence or human level intelligence on the one hand, and on the other hand,

10:42.720 --> 10:48.280
the near-term context of what we can do now or will be able to do over the next few years

10:48.280 --> 10:53.000
that will impact, say, national security or impact the economy.

10:53.000 --> 11:00.440
And I think both of these contexts are important, are legitimate things to have conversations

11:00.440 --> 11:04.320
about, but they are very different and why needs to keep them apart.

11:04.320 --> 11:10.080
Otherwise, I think one simultaneously tends to over-hype the present and under-hype

11:10.080 --> 11:12.560
the longer-term future.

11:12.560 --> 11:17.120
And so is your research particularly focused on the longer term?

11:17.120 --> 11:22.120
Yeah, that's what I'm most interested in, because I think ultimately that will matter

11:22.120 --> 11:24.600
more and make a bigger difference in the world.

11:24.600 --> 11:30.600
Do you work on both these near-term issues as well as the longer-term issues?

11:30.600 --> 11:36.080
Well, a little bit on the near-term, but really our heart is in the longer term, trying

11:36.080 --> 11:41.720
to figure out whether there are things at some point that might affect the trajectory

11:41.720 --> 11:45.600
of human civilization, as opposed to just be bumps in the road.

11:45.600 --> 11:47.280
I think it's also more neglected.

11:47.280 --> 11:53.160
There are more groups interested in near-term issues, so our relative ability to add value

11:53.160 --> 11:57.440
there is smaller than I think on the more neglected longer-term issues.

11:57.440 --> 12:02.960
I'm curious what your experience is talking to folks about these longer-term issues.

12:02.960 --> 12:08.640
Given the challenges that we have with things like global warming and humans impact on the

12:08.640 --> 12:18.280
habitability of the Earth, which seems to be more present than AGI, how do you get people

12:18.280 --> 12:21.520
to care, I guess, is the question?

12:21.520 --> 12:29.600
There's been a huge amount of interest actually in our work in general, but on AGI in particular,

12:29.600 --> 12:34.640
so maybe it's a pricing, but yeah, that hasn't been a problem.

12:34.640 --> 12:41.760
I think the key challenge now is not so much to create a greater level of interest on

12:41.760 --> 12:46.840
AGI and long-term AGI, but rather to try to channel the existing level of interest and

12:46.840 --> 12:49.200
concern in constructive directions.

12:49.200 --> 12:55.240
A lot of people have this general sense, well, AGI, maybe it could be really big, it could

12:55.240 --> 12:58.400
be good or bad or scary, we don't even know how to think about it, so how do you take

12:58.400 --> 13:06.320
that and then use that amount of activation energy to produce actually constructive work

13:06.320 --> 13:11.840
in the world, let's say research that will give us better tools for scalable control

13:11.840 --> 13:18.000
or advances in our ability to politically organize or to think of governance arrangements

13:18.000 --> 13:21.320
that could result in a better outcome down the road.

13:21.320 --> 13:23.080
I want to take a step back.

13:23.080 --> 13:29.320
You mentioned that along this thinking about the timeline, that it's often better to

13:29.320 --> 13:37.840
think about it in terms of capability, what are their specific pivot points or inflection

13:37.840 --> 13:46.440
points in the capability timeline that are notable, you know, achieving AGI, I imagine

13:46.440 --> 13:53.680
is one and achieving superintelligence is another, are there inflection points between where

13:53.680 --> 13:58.120
we are today and AGI that you think are interesting milestones?

13:58.120 --> 14:06.160
It's actually quite hard to think of what would be a reliable indicator that AGI is going

14:06.160 --> 14:14.600
to happen, say, five years down the line and I think it's quite possible that it's

14:14.600 --> 14:20.200
that what will happen is it will look like we're lost in a thick forest for some unknown

14:20.200 --> 14:25.520
period of time and then maybe we stumble on a clearing and the finish line is just a

14:25.520 --> 14:27.000
few words ahead of us.

14:27.000 --> 14:31.160
That is, we shouldn't have a great deal of confidence in our ability to be able to see

14:31.160 --> 14:35.920
a long time in advance that AGI will occur.

14:35.920 --> 14:40.560
You know, there are some milestones that maybe if you had asked people 20 years ago, they

14:40.560 --> 14:47.320
would have thought would be pretty impressive, say, the successes with AlphaGo.

14:47.320 --> 14:53.120
Now that we passed them, I mean, there is a risk, I guess, of just kind of gradually

14:53.120 --> 14:59.360
taking for granted things that really X and D were hugely impressive and at our expectation

14:59.360 --> 15:03.640
level just adjusts so that there will be no point maybe at which we will be more shocked

15:03.640 --> 15:11.120
and odd than we were with AlphaGo until we're all most all the way there and you already

15:11.120 --> 15:17.280
have weak forms of AGI running around and doing and at that point it's kind of a little

15:17.280 --> 15:18.280
bit late at the day.

15:18.280 --> 15:20.800
It's the start thinking about the safety issues.

15:20.800 --> 15:27.640
I think we want to use the time we have available now to put ourselves in the best possible

15:27.640 --> 15:33.840
position for the coming transition to the era of machine superintelligence.

15:33.840 --> 15:39.040
So you mentioned a big part of your work is trying to come up with these concrete strategies

15:39.040 --> 15:43.040
or agendas that folks should be taking up.

15:43.040 --> 15:47.640
What are some of those things that we should be doing to prepare?

15:47.640 --> 15:56.400
Well, one is this research field of AI safety, which when the book was being written hardly

15:56.400 --> 16:01.320
existed, there might have been 10 people in the world who were doing that and it was

16:01.320 --> 16:03.240
very, very far from mainstream.

16:03.240 --> 16:09.400
So now there is a research community working on this with groups in a number of different

16:09.400 --> 16:10.400
places.

16:10.400 --> 16:17.160
There is a group at Berkeley, some work in Montreal, we are doing a joint technical research

16:17.160 --> 16:23.240
seminars with DeepMind, who also has an AI safety group, OpenAI, so it's kind of become

16:23.240 --> 16:28.400
a small little research field in its own right, and that seems constructive and probably

16:28.400 --> 16:31.840
there should be more of that kind of work.

16:31.840 --> 16:36.560
Then I think we're still at an earlier stage with respect to the governance challenges.

16:36.560 --> 16:40.440
Maybe we are with respect to the governance challenges where we were with respect to AI

16:40.440 --> 16:45.360
safety work five years ago, like there is some sense that it's important that somebody

16:45.360 --> 16:50.560
should work on it, but not yet a very clear conception about just what kind of work

16:50.560 --> 16:51.880
would be helpful.

16:51.880 --> 16:58.240
So maybe a few years down the road, we will have a clearer sense of what kind of work

16:58.240 --> 16:59.720
on governance would be productive.

16:59.720 --> 17:05.200
Well, the issue there is slightly different, the AI safety is a technical challenge, ultimately

17:05.200 --> 17:10.680
the more people hammering away at it, the better, at worst they produce nothing and at

17:10.680 --> 17:14.960
best they produce some insight that could actually be useful, when it comes to political

17:14.960 --> 17:19.640
problems it's not always obvious that having more people work on it will produce a better

17:19.640 --> 17:24.640
outcome or producing more insight or knowledge or shared understanding will always produce

17:24.640 --> 17:25.640
a better outcome.

17:25.640 --> 17:30.800
You also have to worry there about things like arms race dynamics and so forth.

17:30.800 --> 17:35.280
So it gets more strategically complicated to figure out what would actually be a helpful

17:35.280 --> 17:42.080
intervention when we are talking about things that are more in the political domain.

17:42.080 --> 17:47.000
Maybe as an example of that you wrote a paper, I guess it was last year some time on the

17:47.000 --> 17:52.080
implications of openness in AI development.

17:52.080 --> 17:58.440
With the kind of cursory view of the paper it was, your results were a little bit counterintuitive.

17:58.440 --> 18:01.760
Can you talk a little bit about that paper and kind of what led to it?

18:01.760 --> 18:07.560
There is this widespread view that openness in AI development is a good thing.

18:07.560 --> 18:12.120
Openness is almost one of these words that like freedom or democracy or fairness that

18:12.120 --> 18:15.280
is kind of almost like just an applause like, right?

18:15.280 --> 18:18.840
It sounds good, we should have more of it.

18:18.840 --> 18:24.000
I think it's actually not that all obvious that ultimately that is what we want to have

18:24.000 --> 18:25.000
more of with AI.

18:25.000 --> 18:29.800
I think the short term impact of more openness or positive that is more people more quickly

18:29.800 --> 18:34.960
get access to state of the art techniques and can use them more widely and I think on

18:34.960 --> 18:37.200
balance that is positive.

18:37.200 --> 18:43.560
But if we're thinking about this hypothetical future strategic context where we are getting

18:43.560 --> 18:48.680
close to developing machine superintelligence and you think maybe there will be several groups

18:48.680 --> 18:53.840
or countries or firms competing to try to get there first.

18:53.840 --> 18:57.760
In that context openness could be extremely dangerous.

18:57.760 --> 19:03.320
You would want, it seems to me, whoever develops superintelligence first to have the ability

19:03.320 --> 19:09.600
at the end of that development process to pass for six months, let us say, or a year

19:09.600 --> 19:15.640
to test their system very carefully, double check all their safety mechanisms, maybe to

19:15.640 --> 19:21.960
slowly boost its intelligence through the human range and into the superintelligent range.

19:21.960 --> 19:29.160
But if you have, say, 20 different research groups running neck to neck with almost

19:29.160 --> 19:35.240
indistinguishable technology, then if any one of them decides to take it slow and be careful,

19:35.240 --> 19:37.840
they'll just be surpassed by one of their competitors.

19:37.840 --> 19:44.440
So it seems in an extreme race condition, the race would go to whoever takes the fewest

19:44.440 --> 19:50.640
precautions and it's least cautious, that seems to be a risk-increasing situation.

19:50.640 --> 19:57.560
So you'd be looking for ways to maybe increase the lead of whichever AI developer isn't

19:57.560 --> 20:02.560
the lead at the time when you're getting close to superintelligence.

20:02.560 --> 20:03.560
So that's the backdrop.

20:03.560 --> 20:08.960
Think about what openness does, it kind of equalizes one variable that could cause dispersion

20:08.960 --> 20:10.120
in AI capabilities.

20:10.120 --> 20:15.400
So if you are open about the general science, well, then everyone has access to the same

20:15.400 --> 20:16.720
general scientific ideas.

20:16.720 --> 20:22.480
If you're open about your source code, let us say, then you equalize the software base

20:22.480 --> 20:28.880
that different developers have, that would mean that any remaining dispersion would have

20:28.880 --> 20:33.120
to come from, say, difference in hardware or different data sets or something like that.

20:33.120 --> 20:36.960
It would one less source of dispersion and capability.

20:36.960 --> 20:42.400
So that would tend to equalize the race, make it more tightly competitive and therefore

20:42.400 --> 20:48.040
tend to reduce the lead time that the lead developer has in the end to go slow for the sake

20:48.040 --> 20:49.760
of caution.

20:49.760 --> 21:01.400
You are in this paper proposing that there are some costs to open this in that they accelerate

21:01.400 --> 21:10.760
AI development and more specifically eliminate the opportunity to put in checks and balances

21:10.760 --> 21:13.480
kind of in the end game.

21:13.480 --> 21:21.440
I'm also wondering if there's a cost to lack of transparency or closeness that you factored

21:21.440 --> 21:31.360
into the analysis that strikes me as mostly around the danger of not knowing where

21:31.360 --> 21:44.680
AGI is, right, if AGI is much closer but it's closed and you don't see it and it's potentially

21:44.680 --> 21:50.080
in a more advanced state in your competitors, how do you factor that into the analysis

21:50.080 --> 21:51.080
here?

21:51.080 --> 21:55.960
Well, so the full analysis, there are a number of other important considerations as well

21:55.960 --> 22:00.600
besides the one I mentioned concerning the racing dynamic.

22:00.600 --> 22:07.120
You might think about whether, say, an open development context tends to, say, attract

22:07.120 --> 22:14.360
a different kind of participant, maybe with better motives than products done in secret.

22:14.360 --> 22:20.760
But in terms of being able to know the capabilities even of different products, let's set aside

22:20.760 --> 22:25.720
their actual algorithms or ideas but even just knowing how far along different product

22:25.720 --> 22:26.720
is.

22:26.720 --> 22:31.280
Not a simple model we have, this is in an earlier paper with a couple of colleagues of mine

22:31.280 --> 22:33.600
called racing to the precipice.

22:33.600 --> 22:40.960
You actually get the higher level of overall risk taking if competitors can see more precisely

22:40.960 --> 22:44.760
how far along each other is.

22:44.760 --> 22:48.920
The intuition being roughly that in this very simple gap theoretic model, if you have a

22:48.920 --> 22:54.320
winner takes all dynamic, that if you see that you are behind, you sort of know for sure

22:54.320 --> 23:00.560
that you will lose and you'd be willing to take any extra amount of risk, if that helps

23:00.560 --> 23:05.720
you have at least some chance of catching up, whereas if you are unsure about your relative

23:05.720 --> 23:09.520
position, then that would be a limit to the amount of risk you would take, because you might

23:09.520 --> 23:14.360
be ahead and you wouldn't want to then take more risk just to get farther ahead.

23:14.360 --> 23:19.280
If that then means you'd be likely to destroy the world if you succeeded.

23:19.280 --> 23:25.840
Now one can construct different kinds of models of this type of situation and get different

23:25.840 --> 23:31.520
outcome, but I think there are some general lessons that seem relatively robust.

23:31.520 --> 23:39.360
So one is that the greater the degree to which there is a commonality of purpose between

23:39.360 --> 23:43.480
different competitors, that is the greater the degree to which it wouldn't matter trying

23:43.480 --> 23:49.840
a body who got the first, the more investment in safety you're likely to get.

23:49.840 --> 23:53.400
In the limit where it's completely indifferent, who gets the first, then you don't have

23:53.400 --> 23:54.400
a race.

23:54.400 --> 24:00.000
You'd be quite happy to drop out of the race if that allows another competitor to spend

24:00.000 --> 24:03.840
more time and be more cautious in the relevant stages.

24:03.840 --> 24:08.440
So that fostering cooperation, fostering a kind of commitment to the common good would

24:08.440 --> 24:12.840
seem to not just be good from a fairness point of view, making sure everybody gets the

24:12.840 --> 24:17.880
slice of the upside, but also be good from an AI risk point of view in taking some of

24:17.880 --> 24:21.920
the pressure of this possibility of a racing dynamic.

24:21.920 --> 24:26.840
And it's not an all or nothing thing, but the more you can kind of ingrain early on

24:26.840 --> 24:31.520
incredible commitment to use AI for the common good of all of humanity, rather than for

24:31.520 --> 24:36.240
narrow, factional purposes to just enrich one company or strengthen the military of one

24:36.240 --> 24:37.240
nation.

24:37.240 --> 24:43.960
I think the more one can reduce this competition dynamic and obviate some of the problems associated

24:43.960 --> 24:44.960
with that.

24:44.960 --> 24:52.480
So does that present a paradigm of sorts in this particular research area that a more

24:52.480 --> 25:01.920
collaborative environment reduces this competitive race, but also tends towards openness, which

25:01.920 --> 25:07.480
increases the competitive nature of it or the risks associated with it?

25:07.480 --> 25:09.280
Yeah, that could be some trade off there.

25:09.280 --> 25:14.160
I think you might want to make a distinction between collaboration and cooperation.

25:14.160 --> 25:19.120
So if collaboration means actively working together on one and the same product side

25:19.120 --> 25:25.200
by side, that would be maybe one way of cooperating, but you could also imagine at least theoretically

25:25.200 --> 25:30.160
the possibility of having entirely separate products that have no communication, but are

25:30.160 --> 25:35.560
both committed to helping each other out or to sharing the spoils.

25:35.560 --> 25:41.720
So that might be a highly operative development regime, but one that lacks actual active collaboration.

25:41.720 --> 25:47.000
And what it seems that what you ultimately primarily want here is cooperation, and whether

25:47.000 --> 25:53.600
that involves collaboration as well is a more tactical question of what seems feasible

25:53.600 --> 25:55.120
at the given time.

25:55.120 --> 26:01.560
But returning to the work in an AI safety, which is a bit further along, what would you

26:01.560 --> 26:11.920
say are some of the interesting directions there and any early, noteworthy results that

26:11.920 --> 26:14.200
folks have had?

26:14.200 --> 26:20.800
Well, I think maybe it's obvious, but it took a little while for it to become kind of

26:20.800 --> 26:27.240
common knowledge that the approach, or best illustrated by laws of robotics, the idea

26:27.240 --> 26:33.880
that you would handcraft a few principles to guide what AI is allowed to do does not seem

26:33.880 --> 26:35.480
to scale.

26:35.480 --> 26:40.080
And therefore seems entirely unpromising as an approach to solving superintelligence

26:40.080 --> 26:41.080
alignment.

26:41.080 --> 26:47.600
And that what instead you'll need to do somehow is to leverage the intellectual capability

26:47.600 --> 26:51.920
of these hypothetical future systems to help us solve the alignment problem, maybe by

26:51.920 --> 26:56.840
having AI's that learn human preferences from human behavior or brain directing with

26:56.840 --> 26:57.840
us.

26:57.840 --> 27:03.840
Or that otherwise leverage their intelligence to help us figure out what it is that we

27:03.840 --> 27:05.560
are trying to get them to do.

27:05.560 --> 27:08.480
So now the question then becomes how can you actually do that?

27:08.480 --> 27:14.160
And then there is a number of different ideas for how to go about researching that.

27:14.160 --> 27:18.400
With different researchers having different judgments and intuitions about which of these

27:18.400 --> 27:20.200
is more promising.

27:20.200 --> 27:24.760
Some of these research avenues are more continuous with current research that is of interest

27:24.760 --> 27:29.920
quite independently of any application in a future context of superintelligence.

27:29.920 --> 27:35.400
So you have, say, inverse reinforcement learning and human preference modeling that

27:35.400 --> 27:40.000
this is a few, even if what you're trying to do is to get like say a recommender system

27:40.000 --> 27:41.000
to work better.

27:41.000 --> 27:47.320
I see various customers have bought these different books and films and rated them thus what

27:47.320 --> 27:49.320
other books and films are the most likely to use.

27:49.320 --> 27:53.720
That is a simple example of how you try to build an AI system that can infer what humans

27:53.720 --> 27:55.040
want.

27:55.040 --> 27:58.720
But if you're trying to move that technology in a direction that could also work in this

27:58.720 --> 28:02.920
context of superintelligence, then there are some distinct challenges that arise that

28:02.920 --> 28:04.560
you could try to do work on.

28:04.560 --> 28:09.280
But there is a bunch of other ideas as well of research that seems useful to give us

28:09.280 --> 28:14.120
a better understanding of the possible safety challenges that could arise when you have

28:14.120 --> 28:16.680
kind of superhuman systems that you need to control.

28:16.680 --> 28:22.200
So just to give you one flavor of that, so one thing that a human level system could

28:22.200 --> 28:25.200
do is to engage in strategic behavior.

28:25.200 --> 28:31.080
It could like humans do it, can anticipate what other humans do, it could be deceptive.

28:31.080 --> 28:36.760
A superintelligence system could do that presumably to superhuman degree of competence.

28:36.760 --> 28:42.480
So once you have a sufficiently capable system, you might no longer be able to just assume

28:42.480 --> 28:44.440
that you could easily test its capabilities.

28:44.440 --> 28:48.520
It might kind of pretend to be less competent than it really is.

28:48.520 --> 28:54.400
If it predicts that that will then result in a certain behavior on the part of the

28:54.400 --> 29:01.360
programmer, sorry, it's human keepers, it might conceal its true goals if it perceives

29:01.360 --> 29:04.280
that there is a strategic rationale for doing so.

29:04.280 --> 29:08.200
So that kind of qualitatively different behavior that you wouldn't necessarily expect

29:08.200 --> 29:11.480
to see in some human artificial intelligence systems.

29:11.480 --> 29:14.920
Also when you have a superintelligence system, there might be a little part of the system,

29:14.920 --> 29:19.840
some internal optimization process that might itself be highly capable and maybe smarter

29:19.840 --> 29:21.440
than human.

29:21.440 --> 29:25.560
And you need to think about whether there could be some agent processes emerging from

29:25.560 --> 29:29.080
a larger system that you hadn't built in there by design.

29:29.080 --> 29:35.280
So there are some ways in which the control problem looks qualitatively different when

29:35.280 --> 29:41.160
one is thinking about the challenge of controlling a superintelligence, as opposed to the challenge

29:41.160 --> 29:46.720
that we currently have of getting more limited AI systems to perform to expectation.

29:46.720 --> 29:54.040
How as an AI safety researcher, how would you attack those types of problems in the absence

29:54.040 --> 29:56.640
of the actual superintelligence?

29:56.640 --> 30:02.120
What are some of the approaches folks are taking to kind of define and make progress in

30:02.120 --> 30:03.120
these areas?

30:03.120 --> 30:07.480
Well, so again, it comes down part of to taste the subjective judgment.

30:07.480 --> 30:11.560
So you could, on the one hand, try to do more theoretical work, try to think through

30:11.560 --> 30:12.960
more from first principles.

30:12.960 --> 30:16.600
What are some of the issues that could arise with very powerful systems?

30:16.600 --> 30:20.680
Or to a different personality, you might prefer to try to do things that kind of build on

30:20.680 --> 30:25.080
existing techniques and develop them in a direction that could seem to be useful.

30:25.080 --> 30:29.240
So a lot of these examples of the latter would be do all you, so say better techniques

30:29.240 --> 30:35.840
for understanding what is going on inside a deep neural network, interpretability tools.

30:35.840 --> 30:40.000
It seems like that would be useful, not just for making faster progress today, if you're

30:40.000 --> 30:44.160
a researcher, like you want to see exactly what's going on in your network and why it's

30:44.160 --> 30:48.920
doing what it's doing, but it also seems like that could lead into over time things that

30:48.920 --> 30:54.040
would be useful for AI safety in the longer term, if you could kind of have better tools

30:54.040 --> 30:59.800
for monitoring the cognitive processes occurring inside the system.

30:59.800 --> 31:03.600
So there's like on one hand, you had these things like transparency, trying to better

31:03.600 --> 31:09.080
understand adversarial examples and countermeasures to that, better ways of doing reinforcement

31:09.080 --> 31:12.000
learning or imitation learning.

31:12.000 --> 31:15.840
And on the other hand, these more kind of conceptual or purely mathematical studies of

31:15.840 --> 31:20.640
hypothetical systems that we can't currently build, but that you can nevertheless reason

31:20.640 --> 31:25.960
about a little bit like you would about some mathematical object that you can't actively

31:25.960 --> 31:29.160
compute, but you could have some theoretical understanding about.

31:29.160 --> 31:35.960
I guess I'm wondering how, you know, when we reason about these systems mathematically

31:35.960 --> 31:44.160
in that way, is there a clear mapping from those more theoretical, from the work we're

31:44.160 --> 31:51.040
doing today, theoretically, to, you know, what might need to be done far off in the future

31:51.040 --> 31:57.120
in the case of these systems coming into existence, or is it, you know, more an issue of

31:57.120 --> 32:04.800
hay work exploring these different areas, theoretically, and it's providing a foundation

32:04.800 --> 32:11.760
for future iterations of work in the same area to build on each other with the hopes that

32:11.760 --> 32:16.760
we keep pace with the advances that are happening on the AI side itself.

32:16.760 --> 32:23.080
It's more the latter, so AI safety, I think, should still be regarded as a pre-powered

32:23.080 --> 32:29.880
igmatic science that it's not clear what the best or most relevant way to address these

32:29.880 --> 32:30.880
problem is.

32:30.880 --> 32:33.480
It's not even generally agreed exactly what the problem is.

32:33.480 --> 32:38.760
So you have different smart people trying out different things coming up with problem

32:38.760 --> 32:43.000
formulations or concepts or sub-products that should be explored.

32:43.000 --> 32:45.240
Some of them might turn out to be useful.

32:45.240 --> 32:49.080
It's possible none of them would be really useful, but what would be useful is to have built

32:49.080 --> 32:55.160
up a research community that is kind of continuously engaging with these questions and gradually

32:55.160 --> 32:59.520
over the years refining their insights and having them kind of then being able to apply

32:59.520 --> 33:05.040
that skill to the systems that are eventually built.

33:05.040 --> 33:09.080
I think it's kind of the sense that this is important enough that we should try our best

33:09.080 --> 33:10.080
to make progress on it.

33:10.080 --> 33:12.160
And here are some cool ideas for how to make progress.

33:12.160 --> 33:15.960
We don't yet know whether or not those will actually turn out to be useful.

33:15.960 --> 33:23.520
Are there some things that folks that are working in the broader AI field should be thinking

33:23.520 --> 33:28.080
about, meaning that folks that aren't working specifically in AI safety or AI governance

33:28.080 --> 33:35.480
but are developing machine learning and AI systems today, how do you advise those folks

33:35.480 --> 33:44.920
to contribute to this broader issue of AI safety without being fully dedicated to this

33:44.920 --> 33:45.920
kind of research?

33:45.920 --> 33:50.760
Well, I mean, it's like how you contribute to any cost in the world that you're interested

33:50.760 --> 33:51.760
in.

33:51.760 --> 33:52.760
You could try to work directly there.

33:52.760 --> 33:54.680
Or you could try to support other people working there.

33:54.680 --> 34:00.440
You could recommend to talented friends or young who wants to go into this to actually do

34:00.440 --> 34:01.440
so.

34:01.440 --> 34:05.760
You can donate money or give prestige by kind of legitimizing it and so forth.

34:05.760 --> 34:06.760
Sure.

34:06.760 --> 34:10.400
I guess I was wondering if there are like things that you wish every person working in

34:10.400 --> 34:13.080
AI was thinking about or something like that.

34:13.080 --> 34:17.520
It does seem to me that what would be quite robustly valuable across a wide range of

34:17.520 --> 34:26.960
different scenarios is to have a more cooperative approach to AI to try to as far as possible

34:26.960 --> 34:32.440
ingrained into the community, a commitment to the common goods principle that AI is super

34:32.440 --> 34:33.440
intelligence.

34:33.440 --> 34:38.080
If it actually were one day achieved should be for the benefit of all of humanity and

34:38.080 --> 34:42.240
in the service of why they shared ethical ideals.

34:42.240 --> 34:48.600
And I think the more that that kind of becomes embedded within the machine learning community,

34:48.600 --> 34:52.160
the greater the chance that that will actually ultimately happen as well.

34:52.160 --> 34:58.400
I think the research community has a non-trivial amount of power.

34:58.400 --> 35:06.160
Certainly today, if you want to do cutting edge AI research, say your firm and you want

35:06.160 --> 35:10.680
to be able to hire from the first tier of talent, it helps a lot if you can credibly claim

35:10.680 --> 35:16.680
that you're going to use this for ethical acceptable purposes.

35:16.680 --> 35:20.600
If you want to do something nasty, like figure out a better way to generate spam or something

35:20.600 --> 35:25.840
like that, or do some kind of highly unpolitable military application, chances are you're going

35:25.840 --> 35:27.800
to have a much harder time to recruit the best.

35:27.800 --> 35:32.040
You might have to go to the second or third tier talent.

35:32.040 --> 35:39.440
So this kind of big commitment to idealism and cosmopolitan values, I think could exert

35:39.440 --> 35:43.040
some shaping influence over how AI is developed and used.

35:43.040 --> 35:47.920
And so I think anybody in the community can do their part to strengthen that.

35:47.920 --> 35:51.960
And I think that cumulative, it could be quite valuable.

35:51.960 --> 35:57.880
There is a sort of a more esoteric issue as well, which relates to this idea of digital

35:57.880 --> 36:04.960
minds again, but as our AI systems become more and more complex and capable, and at some

36:04.960 --> 36:13.360
point maybe have the same behavioral repertoire of capabilities as animals, like a mouse say

36:13.360 --> 36:15.080
or a dog.

36:15.080 --> 36:20.040
At that point, I think the question of the moral status of these digital minds becomes

36:20.040 --> 36:22.640
increasingly relevant.

36:22.640 --> 36:23.840
And it's a hard thing to discuss.

36:23.840 --> 36:28.160
It still feels a little bit like one of those silly things that is kind of, you know,

36:28.160 --> 36:34.480
you can't say with a strange face, but that's where AI safety was five years ago.

36:34.480 --> 36:37.400
It was also this fringe thing that a few people on the internet talked about.

36:37.400 --> 36:43.000
And you couldn't really, and to some extent, these current conversations about AI governance

36:43.000 --> 36:48.720
and the wider impact of society, that's also been moving from a fringe science fiction

36:48.720 --> 36:52.920
saying people on the internet to something that people who think of themselves as serious

36:52.920 --> 36:57.080
people now acknowledge as a legitimate thing to work on.

36:57.080 --> 37:02.400
And I think that the moral status of digital minds needs to start making this migration

37:02.400 --> 37:07.520
as well, from suitable topic for the philosophy seminar room into the kind of thing that you

37:07.520 --> 37:12.000
can talk about in some mainstream forum with different views on it, but without it being

37:12.000 --> 37:14.760
a silly thing or something that you kind of snicker about.

37:14.760 --> 37:19.200
And again, yeah, you can contribute to that by just not being afraid of talking about

37:19.200 --> 37:22.680
that if the topic comes up with your friends or colleagues.

37:22.680 --> 37:27.800
One of the interesting things that I came across in some of your writing was this notion

37:27.800 --> 37:37.040
of how ethics itself is this dynamic force over time, this dynamic picture over time.

37:37.040 --> 37:45.320
And we need to, I forget the specific construct, but it was part of the way we approach AI safety

37:45.320 --> 37:48.120
is to build in some notion of ethics.

37:48.120 --> 37:53.000
It's almost like we need to go back in time to being in, you know, ancient Greece or something

37:53.000 --> 37:58.520
like that and trying to build a system that could map itself from that, you know, ethical

37:58.520 --> 38:03.440
system to our current ethical system, which is, you know, pretty dramatically different.

38:03.440 --> 38:10.040
And that's kind of the way we need to think about building a system today.

38:10.040 --> 38:11.440
Can you kind of elaborate on that?

38:11.440 --> 38:16.200
Did I give you enough to spark some recognition, which we're actually saying?

38:16.200 --> 38:22.440
Yeah, so I think it would be a mistake if one thinks about how one would use super

38:22.440 --> 38:27.840
intelligence to try to list all our current object level ideas and conceptures and moral

38:27.840 --> 38:34.160
principles, then sort of try to hardwire those into the AI to then forever characterize

38:34.160 --> 38:35.440
how the future should pan out.

38:35.440 --> 38:40.040
We should recognize that just as every other earlier age that we can now look back on by

38:40.040 --> 38:47.520
our lives were severely misguided, had huge moral blind spots in terms of, I don't know,

38:47.520 --> 38:54.160
you could go through the list, like slavery, status of women, how it did, animals, social

38:54.160 --> 38:59.960
inequalities, causes for war, that presumably we have now not yet attained full moral

38:59.960 --> 39:00.960
enlightenment.

39:00.960 --> 39:04.920
And it's quite likely that if there are leader stages of human history that looks back

39:04.920 --> 39:10.280
on 2018, they might also shudder to think about the atrocities that we were committing

39:10.280 --> 39:11.280
right now.

39:11.280 --> 39:15.600
And so we want to leave open the possibility for moral growth.

39:15.600 --> 39:20.840
And not just moral growth narrowly conceived, but for in general, there to be development

39:20.840 --> 39:26.760
in how we think about human values and what life can involve and how we can organize society.

39:26.760 --> 39:32.560
And so that one perhaps attractive vision for how super intelligence would be used would

39:32.560 --> 39:40.280
be to enable a deeper deliberation on the part of humanity informed, say by super intelligent

39:40.280 --> 39:46.560
advice, and maybe AI could help us increase human intelligence and safeguard us while

39:46.560 --> 39:47.560
we were doing this.

39:47.560 --> 39:51.800
But some kind of deliberative process that could plunder these things for a long time before

39:51.800 --> 39:57.960
we made any irrevocable decisions about exactly what kind of post human future we would want

39:57.960 --> 39:59.760
to create and move into.

39:59.760 --> 40:08.040
To kind of summarize, it's tempting for us to say, hey, in order to ensure the ethical

40:08.040 --> 40:12.360
behavior of these systems, let's bake in our ethics.

40:12.360 --> 40:19.280
But in the future, our ethics will undoubtedly be proven to be inferior.

40:19.280 --> 40:27.000
And so we need to navigate, build almost an ethical calculus, maybe, and build systems

40:27.000 --> 40:31.720
that can navigate kind of changing ethical standards.

40:31.720 --> 40:37.880
Well, okay, yeah, so I don't think an ethical calculus in the sense that we would lay down some

40:37.880 --> 40:41.360
moral axioms and then the AI would compute things from those.

40:41.360 --> 40:45.960
But more that there is some process whereby we do moral thinking or in general do thinking

40:45.960 --> 40:49.240
about what we want in life.

40:49.240 --> 40:54.320
And if the AI could learn to do that same kind of thinking or to maybe learn to extrapolate

40:54.320 --> 41:00.400
our thinking, that might be one way of getting indirectly at this thing that we really want,

41:00.400 --> 41:04.240
as opposed to the thing we would say we want if we had to make up some answer of the

41:04.240 --> 41:05.240
cuff.

41:05.240 --> 41:07.600
So I think you're right for two reasons.

41:07.600 --> 41:12.160
One is that you wouldn't want to just call in our current misconceptions and superficial

41:12.160 --> 41:14.240
misunderstandings.

41:14.240 --> 41:17.920
You would want this possibility of learning and developing and doing something better

41:17.920 --> 41:20.240
that we could do at the spur of the moment.

41:20.240 --> 41:26.600
Also, I think this harkens back to the earlier idea of a commitment to the common good,

41:26.600 --> 41:32.040
but the more you can conceive of this as a very inclusive process, we're not just one

41:32.040 --> 41:37.920
person or one country, so one country's values would achieve total dominance, but something

41:37.920 --> 41:42.760
that could incorporate many different interests, many different value perspectives.

41:42.760 --> 41:47.920
Maybe not an absolute degree, but a widely generous and inclusive degree.

41:47.920 --> 41:52.320
I think that would also make it possible to conceive of a future that is more widely

41:52.320 --> 41:59.640
appealing and that would reduce some of these incentives to race to the precipice that

41:59.640 --> 42:06.440
would arise if every participant in the development process were just have been done in pulsing

42:06.440 --> 42:09.840
their own idiosyncratic views on the entire future.

42:09.840 --> 42:17.040
So I think that two reasons for doing it more indirectly and in this more inclusive way

42:17.040 --> 42:18.600
to the extent that it's possible.

42:18.600 --> 42:22.480
And that, again, I think is something that anybody who's a member of this community in small

42:22.480 --> 42:28.400
ways, not so much in the specific work that do day-to-day, but in terms of being ethical

42:28.400 --> 42:33.360
members of this community that can sometimes talk about and express views about how their

42:33.360 --> 42:37.160
little contributions should be used and the overall purpose for which this technology

42:37.160 --> 42:42.080
should be developed, where there's space for a lot of voices to cumulatively make a

42:42.080 --> 42:43.600
big difference.

42:43.600 --> 42:51.520
For folks that have listened to our conversation and want to learn more or dig deeper or begin

42:51.520 --> 42:57.320
to even contribute or support some of the work happening in this area, what are good

42:57.320 --> 43:06.160
places to start in terms of resources or organizations to follow?

43:06.160 --> 43:15.520
Well, I have maybe an obvious bias.

43:15.520 --> 43:20.000
The book that we talked about earlier is like maybe best expresses my perspective on

43:20.000 --> 43:26.840
this, but there is a kind of overlapping set of communities that are interested in this.

43:26.840 --> 43:34.000
So one, quite interesting one is the effective altruism community that is concerned about

43:34.000 --> 43:38.440
the wider range of cost areas, but AI also increasing being one of them.

43:38.440 --> 43:41.880
It's a community of people who are trying to take care of how you can have the greatest

43:41.880 --> 43:48.080
possible positive impact on the world in terms of what you do with your career or in terms

43:48.080 --> 43:51.400
of where you donate your charitable giving.

43:51.400 --> 43:56.520
So checking out their resources, they have a career guide, they have various blogs, we

43:56.520 --> 43:58.560
want a good place to start.

43:58.560 --> 44:04.440
If you want to do technical AI safety, then you can Google technical AI research agendas

44:04.440 --> 44:08.400
and you'll find different ideas, or you can see some of the latest publications coming

44:08.400 --> 44:17.320
out from deep-bind AI safety or open AI or FHI or MIRI, or the group at Berksey, to see

44:17.320 --> 44:20.800
kind of concrete examples, so I kind of think people are working on.

44:20.800 --> 44:24.760
Well, Nick, thank you so much for taking the time to chat with me.

44:24.760 --> 44:27.080
Is there anything else that you'd like to leave folks with?

44:27.080 --> 44:32.800
Now, I think the bottom line is, we don't know how long it'll take, but if we ever do get

44:32.800 --> 44:38.760
to create super intelligence, it's such a big thing that our bottom line must be the

44:38.760 --> 44:44.600
sense of enormous humility that we are just in way over our heads, but we have no choice

44:44.600 --> 44:49.480
to make our best effort to get through this somehow in a responsible, wise and generous

44:49.480 --> 44:50.480
way.

44:50.480 --> 44:51.480
You're great.

44:51.480 --> 44:52.480
Thank you.

44:52.480 --> 44:58.160
All right, everyone, that's our show for today.

44:58.160 --> 45:04.120
For more information on Nick or any of the topics covered in this episode, visit twimmelai.com

45:04.120 --> 45:06.560
slash talk slash 181.

45:06.560 --> 45:11.240
If you're a fan of this podcast, we'd like to encourage you to visit your Apple or

45:11.240 --> 45:15.880
Google podcast app and leave us a five-star rating and review.

45:15.880 --> 45:20.520
Your reviews help inspire us to create more and better content, and they help new listeners

45:20.520 --> 45:22.640
find the show.

45:22.640 --> 45:50.760
As always, thanks so much for listening and catch you next time.

