1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,320
I'm your host Sam Charrington.

4
00:00:31,320 --> 00:00:35,960
This week on the podcast, we feature a few of my many conversations from last week's AWS

5
00:00:35,960 --> 00:00:38,880
Reinvent Conference in Las Vegas.

6
00:00:38,880 --> 00:00:44,200
For today's show, I'm excited to present our second annual Reinvent Roundtable Roundup.

7
00:00:44,200 --> 00:00:48,920
I had a blast last week learning about all the new ML&AI products and services announced

8
00:00:48,920 --> 00:00:50,600
by AWS.

9
00:00:50,600 --> 00:00:54,360
If you missed the news coming out of Reinvent, or you want to know more about what one

10
00:00:54,360 --> 00:00:59,200
of the biggest AI platform providers is up to, you'll want to stay tuned because we discussed

11
00:00:59,200 --> 00:01:03,960
many of their new offerings in this show.

12
00:01:03,960 --> 00:01:09,200
This year, I'm joined by my friends Dave McCrory, VP of Software Engineering at Wise.io

13
00:01:09,200 --> 00:01:15,040
at GE Digital, and Val Berkevici, founder and CEO of pencil data.

14
00:01:15,040 --> 00:01:21,040
We cover all of AWS's most important ML&AI announcements, including SageMaker Groundtruth,

15
00:01:21,040 --> 00:01:27,520
reinforcement learning and Neo, DeepRacer, Inferentia, and ElasticInference, ML Marketplace

16
00:01:27,520 --> 00:01:31,320
Personalized, Forecast, TextTrack, and more.

17
00:01:31,320 --> 00:01:35,120
We don't do these kinds of discussions very often on the show, but I always enjoy it when

18
00:01:35,120 --> 00:01:38,360
we do, and I hope you do too.

19
00:01:38,360 --> 00:01:43,960
Before we dive in, I'm at Neurips this week in Montreal, and CubeCon next week in Seattle,

20
00:01:43,960 --> 00:01:47,840
and I'd love to connect with any listeners and attendants or in the area.

21
00:01:47,840 --> 00:01:52,560
Feel free to shoot me a message via at Sam Charington on Twitter via email or the Twimble

22
00:01:52,560 --> 00:01:57,920
AI website, or, if you see me, don't be afraid to say hi.

23
00:01:57,920 --> 00:02:02,800
If you're heading to Neurips, look for the listener meetup and AI platform's meetup topics

24
00:02:02,800 --> 00:02:04,880
I've posted in the Hooba app.

25
00:02:04,880 --> 00:02:06,240
See you around.

26
00:02:06,240 --> 00:02:08,400
And now, onto the show.

27
00:02:08,400 --> 00:02:14,240
Hey, what's up, everyone?

28
00:02:14,240 --> 00:02:21,120
I am here at ReInvent, and this is becoming a bit of a tradition, isn't it Dave?

29
00:02:21,120 --> 00:02:22,720
It is indeed.

30
00:02:22,720 --> 00:02:29,360
And the tradition that I will unveil is our second annual ReInvent Roundup Roundtable,

31
00:02:29,360 --> 00:02:35,320
in which we discuss all of the cool things that happened here at ReInvent, all of the

32
00:02:35,320 --> 00:02:37,720
announcements, etc.

33
00:02:37,720 --> 00:02:44,360
And so before, or as kind of our first step in jumping in, I would like the two of you

34
00:02:44,360 --> 00:02:52,840
to introduce yourselves, and I guess Dave, since you are our veteran panelist, why don't

35
00:02:52,840 --> 00:02:53,840
you go first?

36
00:02:53,840 --> 00:02:56,480
Certainly, my name is Dave McCrory.

37
00:02:56,480 --> 00:03:06,480
I am the VP of Engineering for WISEIO, a division of GE Digital, that works on machine

38
00:03:06,480 --> 00:03:09,480
learning and industrial internet of things.

39
00:03:09,480 --> 00:03:10,480
Awesome.

40
00:03:10,480 --> 00:03:11,480
And Val.

41
00:03:11,480 --> 00:03:12,480
Yes, I am.

42
00:03:12,480 --> 00:03:16,480
And Dave, it's my honor to be here on the second Roundup ReInvent Roundup.

43
00:03:16,480 --> 00:03:19,680
My name is Val Burkivichi, and it's very wise if you say I'm going to let me pronounce

44
00:03:19,680 --> 00:03:24,080
my name first, because it always ships up.

45
00:03:24,080 --> 00:03:25,880
Always ships up some people.

46
00:03:25,880 --> 00:03:29,440
I am CEO and co-founder of pencil data.

47
00:03:29,440 --> 00:03:33,240
And our tagline is, we tamper-proof digital transformation.

48
00:03:33,240 --> 00:03:36,280
But that has a lot of deeper meanings as well.

49
00:03:36,280 --> 00:03:40,600
Before that, some people might remember me from your podcast when I was CTO of Net

50
00:03:40,600 --> 00:03:42,600
App and Solid Fire.

51
00:03:42,600 --> 00:03:48,800
And you know, I was thinking earlier about what is always so cool for me about ReInvent

52
00:03:48,800 --> 00:03:53,640
is that I get to see so many people that I've known for so long.

53
00:03:53,640 --> 00:03:58,160
Were both of you guys also at the very first ReInvent?

54
00:03:58,160 --> 00:04:02,240
I was not at the first, but I think I've been at every ReInvent since.

55
00:04:02,240 --> 00:04:03,240
Okay.

56
00:04:03,240 --> 00:04:04,240
I think I'm a lot like Dave.

57
00:04:04,240 --> 00:04:08,080
I have to check my records. I know I lobbied hard at Net App to be at the first one.

58
00:04:08,080 --> 00:04:11,480
I don't remember whether we succeeded the first year or that second year for sure.

59
00:04:11,480 --> 00:04:12,480
Okay.

60
00:04:12,480 --> 00:04:13,480
Nice.

61
00:04:13,480 --> 00:04:16,920
I was at the first and second, and I think I missed one or two in between, and I've

62
00:04:16,920 --> 00:04:19,480
been at the last three, I think there are.

63
00:04:19,480 --> 00:04:20,640
I don't know.

64
00:04:20,640 --> 00:04:27,280
I think we do not have enough time to talk about everything that was unveiled and announced

65
00:04:27,280 --> 00:04:28,880
here at ReInvent.

66
00:04:28,880 --> 00:04:34,880
And of course, we'll want to kind of wait our discussion points a little bit to the

67
00:04:34,880 --> 00:04:43,800
ML and AI side of things, but maybe a good way to start is to just ask each of you, what

68
00:04:43,800 --> 00:04:49,760
are you most excited about based on what you heard announced here at ReInvent?

69
00:04:49,760 --> 00:04:55,320
In my opinion, there are a couple of things that are really exciting.

70
00:04:55,320 --> 00:05:06,360
I think the Lambda layers function, which really comes down to being able to run different

71
00:05:06,360 --> 00:05:12,680
runtimes on Lambda is probably the biggest and most exciting thing to me.

72
00:05:12,680 --> 00:05:18,960
I think that's going to have a profound impact on the use of Lambda and the growth of Lambda,

73
00:05:18,960 --> 00:05:25,720
but I see it plugging into a lot of the machine learning kind of problems that we had that

74
00:05:25,720 --> 00:05:31,960
were much harder to use Lambda pre-being able to bring your own libraries, dependencies

75
00:05:31,960 --> 00:05:33,640
and execution environments.

76
00:05:33,640 --> 00:05:35,280
I think that's big.

77
00:05:35,280 --> 00:05:44,440
I also think it really adds, I guess, another layer of difficulty if you're a platform

78
00:05:44,440 --> 00:05:51,160
is a service vendor, not that there weren't already some difficulties in some forms or

79
00:05:51,160 --> 00:05:55,080
others, but I think layers definitely adds to that.

80
00:05:55,080 --> 00:06:02,880
So is layers the ability to support multiple or arbitrary languages for Lambda functions

81
00:06:02,880 --> 00:06:05,000
or is it more than that?

82
00:06:05,000 --> 00:06:10,520
It is indeed the ability to do the runtimes, so that would be safe if you wanted to run

83
00:06:10,520 --> 00:06:19,360
or go or Python or Java or pick your favorite language, Rust, it doesn't matter.

84
00:06:19,360 --> 00:06:24,320
But then along with that, being able to have the libraries and dependencies that go with

85
00:06:24,320 --> 00:06:30,400
that, so that formerly, if you wanted to run a Lambda, it was very, very basic code that

86
00:06:30,400 --> 00:06:33,720
you were essentially just gluing two things together with.

87
00:06:33,720 --> 00:06:38,840
You might have something trigger, something else, et cetera, but it was kind of limited

88
00:06:38,840 --> 00:06:42,280
from that perspective, very scalable, but also very limited.

89
00:06:42,280 --> 00:06:49,400
With layers, you end up with so many different abilities that you wouldn't have before, prior

90
00:06:49,400 --> 00:06:57,440
with just regular Lambda, and I would say that makes it incredibly attractive in my mind.

91
00:06:57,440 --> 00:07:04,720
You can now do much more complex things even within the Lambda functions themselves because

92
00:07:04,720 --> 00:07:07,960
of the ability to do this.

93
00:07:07,960 --> 00:07:14,520
What you still cannot do is directly tie it to persistence, so there's still no guarantee

94
00:07:14,520 --> 00:07:16,520
of persistence.

95
00:07:16,520 --> 00:07:20,040
If that makes sense, that's kind of the one missing thing.

96
00:07:20,040 --> 00:07:23,800
Other than that, I guess sky's the limit, so to speak.

97
00:07:23,800 --> 00:07:28,320
They also added some other functionality, but the layers is the most exciting to me.

98
00:07:28,320 --> 00:07:32,680
I'd say the other thing, since I mentioned that there was more than one, the other thing

99
00:07:32,680 --> 00:07:36,200
was the Kafka streaming capability.

100
00:07:36,200 --> 00:07:43,800
But now, basically AWS will run Kafka for you, so Amazon Managed Streaming for Kafka is

101
00:07:43,800 --> 00:07:49,720
pretty exciting in my mind, being able to just use Kafka instead of having to set it

102
00:07:49,720 --> 00:07:53,160
up, configure it, manage it, and maintain it.

103
00:07:53,160 --> 00:07:54,360
That's pretty exciting.

104
00:07:54,360 --> 00:08:01,320
If I was someone that was running Confluent Kafka on my own, and I used AWS, this would

105
00:08:01,320 --> 00:08:06,600
really make me pause and think about, do I want to keep doing that, or would I rather

106
00:08:06,600 --> 00:08:08,720
just let Amazon do it?

107
00:08:08,720 --> 00:08:18,080
Kafka is a big part of a lot of data pipelines, but I also recently interviewed a woman

108
00:08:18,080 --> 00:08:25,040
named Lima Nasseri from Comcast on the podcast to build out a data pipeline for a recommendation

109
00:08:25,040 --> 00:08:27,600
system using Lambda.

110
00:08:27,600 --> 00:08:32,480
I thought it was really cool that her team was able to take advantage of Lambda functions

111
00:08:32,480 --> 00:08:38,800
to do data transformations and things like that, so I think that'll be increasingly part

112
00:08:38,800 --> 00:08:45,320
of the machine learning toolkit and not just the traditional app dev toolkit.

113
00:08:45,320 --> 00:08:50,400
That's right, and Nasseri has personalized, so for me, if we take us all the way back

114
00:08:50,400 --> 00:08:57,240
to Wednesday yesterday, I'm thinking of Andy's keynote, and we were talking about the

115
00:08:57,240 --> 00:09:01,680
very first couple of reinvents where the big announcement was a new instance type, or some

116
00:09:01,680 --> 00:09:07,800
new lower pricing for EC2, or RS3, et cetera, and we've come such a long way since then

117
00:09:07,800 --> 00:09:11,760
where the major highlights, even though I think there were some pretty cool storage highlights

118
00:09:11,760 --> 00:09:15,760
for me, were definitely all the machine learning introductions.

119
00:09:15,760 --> 00:09:20,680
There's so many, but I think my personal favorite is the SageMaker Ground Truth, but that's

120
00:09:20,680 --> 00:09:24,400
one of the things I wanted to have when I first got into machine learning, and I just couldn't

121
00:09:24,400 --> 00:09:26,160
believe it wasn't there.

122
00:09:26,160 --> 00:09:29,840
But I don't know if you guys might be more current with the Google Suite or the Google

123
00:09:29,840 --> 00:09:34,680
Cloud portfolio than me, but I'm not sure if they have a similar service yet, so it just

124
00:09:34,680 --> 00:09:39,840
strikes me as AWS continues to just be more in tune with what their customers really

125
00:09:39,840 --> 00:09:43,480
want, and they're executing and delivering really well.

126
00:09:43,480 --> 00:09:52,640
And so SageMaker Ground Truth is a new offering that basically allows users to create these labeling

127
00:09:52,640 --> 00:09:56,280
pipelines.

128
00:09:56,280 --> 00:10:02,600
And how do you want to go through the details of that, how, how, okay.

129
00:10:02,600 --> 00:10:07,200
So such a big part of an average data scientist's work, if there is such a thing as an average

130
00:10:07,200 --> 00:10:11,880
data scientist, is actually setting up supervised training, right?

131
00:10:11,880 --> 00:10:16,480
That supervised learning that's effectively what most machine learning is nowadays, and

132
00:10:16,480 --> 00:10:22,520
there's a very manual human dependency ironically on that part, which is to actually label data.

133
00:10:22,520 --> 00:10:27,120
So a lot of the captures and recaptures that we see sometimes are asked to identify objects

134
00:10:27,120 --> 00:10:32,080
on a four by four frame of pictures, is us being mechanical turks of doing labeling

135
00:10:32,080 --> 00:10:38,120
for other people to label, you know, features in an image and so forth, the ability to apply

136
00:10:38,120 --> 00:10:43,800
machine learning intelligently, and use inference to automatically label the huge portion

137
00:10:43,800 --> 00:10:46,840
of these data sets now without all that manual effort.

138
00:10:46,840 --> 00:10:51,160
I think it's a huge step forward in the productivity of data scientists and the ability

139
00:10:51,160 --> 00:10:54,720
to generate, you know, more knowledge models going forward.

140
00:10:54,720 --> 00:10:59,800
It's part of that theme, I think Samu and I discussed yesterday, which is that AWS ironically

141
00:10:59,800 --> 00:11:05,040
better than Google at the moment is doing a really good job of democratizing AI, democratizing

142
00:11:05,040 --> 00:11:10,400
machine learning for people, democratizing the data science field itself.

143
00:11:10,400 --> 00:11:13,640
And that's, that's good for the industry, right, that brings more people into it.

144
00:11:13,640 --> 00:11:16,840
It brings more projects forward.

145
00:11:16,840 --> 00:11:20,200
It promotes more knowledge models, and I, you know, I've got a selfish motivation as well

146
00:11:20,200 --> 00:11:25,280
and that my company helps with the reproducibility of all that once we have more candidates

147
00:11:25,280 --> 00:11:27,960
for, you know, promotion into production.

148
00:11:27,960 --> 00:11:34,560
The thing that I found most interesting about ground truth is that it's the labeling,

149
00:11:34,560 --> 00:11:39,560
you know, it kind of ties into these third party labeling services.

150
00:11:39,560 --> 00:11:45,560
So mechanical turk is one example, but they, I think they announced like seven different

151
00:11:45,560 --> 00:11:53,200
partners, including figure eight, but it's not just that there's a pipeline that includes

152
00:11:53,200 --> 00:11:54,280
active learning.

153
00:11:54,280 --> 00:12:04,120
So they are optimizing what needs to be labeled basically there, they're going to choose

154
00:12:04,120 --> 00:12:09,400
what needs to be manually labeled according to some optimization function as opposed to

155
00:12:09,400 --> 00:12:17,000
randomly at the idea being that you kind of manage and keep low the costs of the labeling

156
00:12:17,000 --> 00:12:18,000
process.

157
00:12:18,000 --> 00:12:19,000
That is pretty cool.

158
00:12:19,000 --> 00:12:20,000
Yeah.

159
00:12:20,000 --> 00:12:23,000
That's by far the exciting part to me, you know, the fact that there's better integration

160
00:12:23,000 --> 00:12:27,440
with mechanical turks is an important sort of workflow improvement.

161
00:12:27,440 --> 00:12:32,000
But the fact that we're finally being able to sort of, you know, go, go native, if you

162
00:12:32,000 --> 00:12:36,960
will, or eat our own dog food and actually automate the function of labeling by and large

163
00:12:36,960 --> 00:12:41,640
is, you know, even if it's not a complete solution today, the fact that that's the goal

164
00:12:41,640 --> 00:12:45,280
and the fact that we have it initial operating is very exciting to me.

165
00:12:45,280 --> 00:12:50,640
So now I've got to say you surprised me here, given what your company's focused on nowadays,

166
00:12:50,640 --> 00:12:57,520
I had a thought that your favorite announcement was one of a couple of others.

167
00:12:57,520 --> 00:13:02,440
Well, there's definitely on the blockchain side there was, there was the expected, right?

168
00:13:02,440 --> 00:13:08,120
So AWS is kind of playing catch up to IBM and Oracle with regards to offering managed

169
00:13:08,120 --> 00:13:13,360
hyper ledger blockchains, but also having the nice option of managed Ethereum blockchain,

170
00:13:13,360 --> 00:13:15,040
which I thought was super cool.

171
00:13:15,040 --> 00:13:19,600
The wild card, which I think has got a lot of people buzzing, definitely myself included,

172
00:13:19,600 --> 00:13:24,000
is QLDB, the, I'm not sure why they named it quantum ledger, but certainly the quantum

173
00:13:24,000 --> 00:13:26,920
ledger database is super exciting.

174
00:13:26,920 --> 00:13:32,040
And it actually corresponds to talks I've been giving it or Riley this year around has

175
00:13:32,040 --> 00:13:37,800
this very famous blog in the, just before the peak of the, the cryptocurrency media called

176
00:13:37,800 --> 00:13:43,800
a fat protocol blog, which argued that there'd be a lot of network effect value in a protocol

177
00:13:43,800 --> 00:13:49,720
more than the apps and that was before there were 2,000 versions of Ethereum, so, 3,000 potential

178
00:13:49,720 --> 00:13:50,720
network effects.

179
00:13:50,720 --> 00:13:55,480
So the sad protocol blog was brilliant, you know, in 2016, but did not stand a test of

180
00:13:55,480 --> 00:13:56,480
time.

181
00:13:56,480 --> 00:13:59,880
And at least the enterprise reality is very much that, you know, there will be a few

182
00:13:59,880 --> 00:14:04,920
key platforms, blockchain, and secure ledger platforms, such as QLDB, that enterprise

183
00:14:04,920 --> 00:14:08,640
is standard, a standardized on, but there won't be 2,000 of them.

184
00:14:08,640 --> 00:14:12,680
And that the network effect will indeed go to the platform and the application layers

185
00:14:12,680 --> 00:14:14,840
where the value is closer to the user.

186
00:14:14,840 --> 00:14:21,240
Well, it's my understanding that the quantum and QLDB comes from the name of the internal

187
00:14:21,240 --> 00:14:28,760
AWS system that they kind of extracted this from, whether, you know, encode or in idea.

188
00:14:28,760 --> 00:14:34,360
And what's so exciting about that relative to, you know, the managed blockchain that,

189
00:14:34,360 --> 00:14:37,640
you know, folks kind of expected them to come out with.

190
00:14:37,640 --> 00:14:41,360
So what most people don't realize when you're talking about a managed blockchain service

191
00:14:41,360 --> 00:14:46,440
is it addresses 2 of the 3 impediments towards adoption and enterprise.

192
00:14:46,440 --> 00:14:51,200
That first one being sort of the design and creation or building of a blockchain.

193
00:14:51,200 --> 00:14:55,480
And the second one, which is very important after that, the ongoing operations and management

194
00:14:55,480 --> 00:14:57,240
of that blockchain.

195
00:14:57,240 --> 00:15:01,400
But the third one that, you know, the actual buyers or, you know, budget owners of these

196
00:15:01,400 --> 00:15:06,360
technologies, the executives that would want to see results that isn't addressed by this

197
00:15:06,360 --> 00:15:10,520
or any other managed blockchain service is the usability of the blockchain.

198
00:15:10,520 --> 00:15:16,080
It's actually tying into your application that's actually seeing either, you know, operational

199
00:15:16,080 --> 00:15:20,720
efficiencies or greater compliance or ideally even, you know, new business models that were

200
00:15:20,720 --> 00:15:22,240
impossible before.

201
00:15:22,240 --> 00:15:26,920
And that's the part that, you know, wasn't announced really by a managed blockchain service.

202
00:15:26,920 --> 00:15:32,520
But they inched a lot closer with QLDB, where now you've got, you know, a fully ready-made

203
00:15:32,520 --> 00:15:34,160
solution to just consume.

204
00:15:34,160 --> 00:15:38,360
It's not about thinking how you would build QLDB or how you'd manage it.

205
00:15:38,360 --> 00:15:42,240
Obviously, AWS is greater than president doing that for you.

206
00:15:42,240 --> 00:15:43,520
It's ready for consumption.

207
00:15:43,520 --> 00:15:46,600
So that's what excited me very, very much about QLDB.

208
00:15:46,600 --> 00:15:52,080
And it certainly just is another instantiation of soon to be popular service that validates

209
00:15:52,080 --> 00:15:53,400
the market to pencil data.

210
00:15:53,400 --> 00:15:54,400
Right.

211
00:15:54,400 --> 00:15:55,400
Right.

212
00:15:55,400 --> 00:16:01,200
So for me, and I guess I'm sure you guys felt like this as well, it's kind of like a kid

213
00:16:01,200 --> 00:16:04,560
in the candy store, so many cool new things.

214
00:16:04,560 --> 00:16:06,960
It's really hard to pick just one.

215
00:16:06,960 --> 00:16:12,320
But if I really, really, really had to pick just one, the thing that I'm super excited

216
00:16:12,320 --> 00:16:18,120
about is the SageMaker reinforcement learning announcement.

217
00:16:18,120 --> 00:16:24,160
And I've been a fan of and talking about and learning about reinforcement learning for

218
00:16:24,160 --> 00:16:29,480
a bit now, folks on the podcast have heard a lot of conversations about that.

219
00:16:29,480 --> 00:16:37,120
But what is always part of those conversations is how hard it is and how, you know, unstable

220
00:16:37,120 --> 00:16:43,560
the algorithm training processes, how difficult it is to get the right reward function, all

221
00:16:43,560 --> 00:16:52,600
of these things, and how hard it is to ultimately apply it in a real enterprise environment.

222
00:16:52,600 --> 00:17:00,960
And we've seen folks take swipes at making that easier, bonsai, has been a sponsor of

223
00:17:00,960 --> 00:17:08,200
this podcast that in our industrial AI series last year, and they were acquired by Microsoft

224
00:17:08,200 --> 00:17:10,120
because they did such a great job at this.

225
00:17:10,120 --> 00:17:15,440
But AWS has such a massive presence in the marketplace.

226
00:17:15,440 --> 00:17:21,000
There's a real opportunity for them to bring a lot of attention to reinforcement learning.

227
00:17:21,000 --> 00:17:26,960
And as a result, kind of start this virtuous cycle of, you know, more eyes on it, you

228
00:17:26,960 --> 00:17:34,760
know, lower the impediments to doing it and make it much more easy to extract value out

229
00:17:34,760 --> 00:17:35,760
of it.

230
00:17:35,760 --> 00:17:37,440
So I'm super excited about that.

231
00:17:37,440 --> 00:17:43,880
It's an ascension to SageMaker, which is their kind of data science as a service platform.

232
00:17:43,880 --> 00:17:50,440
It supports a wide variety of 2D and 3D simulation physics environments, including open

233
00:17:50,440 --> 00:17:56,880
gym and Sumerian, which is their 3D environment, RoboMaker, which is another one of their

234
00:17:56,880 --> 00:18:02,080
announcements, Roz, which is the robotics operating system.

235
00:18:02,080 --> 00:18:04,760
It's got a lot of the pieces in place.

236
00:18:04,760 --> 00:18:10,800
You know, the one thing that's kind of interesting about this relative to AWS is typical MO.

237
00:18:10,800 --> 00:18:15,760
They tend to be, you know, this is a lot further out in front, I think, than they tend to like

238
00:18:15,760 --> 00:18:16,760
to be, right?

239
00:18:16,760 --> 00:18:22,280
They kind of pride themselves on being very responsive and only building stuff when like

240
00:18:22,280 --> 00:18:25,440
real customers are asking for it.

241
00:18:25,440 --> 00:18:29,800
And oftentimes commoditizing something that already exists, right?

242
00:18:29,800 --> 00:18:31,880
Like S3, we knew what storage was.

243
00:18:31,880 --> 00:18:33,800
They didn't invent that.

244
00:18:33,800 --> 00:18:37,840
EC2, they, you know, made servers more accessible.

245
00:18:37,840 --> 00:18:41,600
Yeah, they're not inventing reinforcement learning either.

246
00:18:41,600 --> 00:18:44,720
So I don't know that this analogy, you know, fits very well.

247
00:18:44,720 --> 00:18:48,920
But it's certainly, we're certainly way out ahead of the market in some ways.

248
00:18:48,920 --> 00:18:54,760
And hopefully, you know, what I'm excited about is just accelerating that process because

249
00:18:54,760 --> 00:18:59,200
the challenge with deep learning is that it's so data-intensive, labeled data-intensive

250
00:18:59,200 --> 00:19:06,480
and reinforcement learning plus simulation promises to fix that for a good many applications.

251
00:19:06,480 --> 00:19:12,040
And this is a quick question I'd have for you, Sam, is, you know, empirically I'm seeing

252
00:19:12,040 --> 00:19:17,120
a ton of just, you know, image recognition, text and voice recognition and the related

253
00:19:17,120 --> 00:19:19,520
neural networks that correspond with that.

254
00:19:19,520 --> 00:19:24,680
I haven't seen a lot of successful production, you know, transfer learning or reinforcement

255
00:19:24,680 --> 00:19:26,600
learning projects in place.

256
00:19:26,600 --> 00:19:30,560
You know, do you think that would kickstart that and would open up a new front, new frontier

257
00:19:30,560 --> 00:19:33,760
actually in the home machine learning world?

258
00:19:33,760 --> 00:19:38,760
So I would say, you know, I would kind of pick apart transfer and reinforcement learning.

259
00:19:38,760 --> 00:19:44,560
I think transfer learning is one of those things that people maybe don't talk about as much.

260
00:19:44,560 --> 00:19:50,000
It's kind of receded into the background, but it is, I think foundational to a lot of

261
00:19:50,000 --> 00:19:56,920
what folks are doing in the image domain, like it's very popular to pick up a pre-trained

262
00:19:56,920 --> 00:19:58,440
image net model.

263
00:19:58,440 --> 00:20:03,760
And if not use that out of the gate, like tweak that, use, you know, or fine tune it,

264
00:20:03,760 --> 00:20:10,920
they'll say, and kind of start with that to avoid having to train everything from scratch.

265
00:20:10,920 --> 00:20:16,920
And in fact, there's a lot of work that's been done recently by Jeremy Howard and Sebastian

266
00:20:16,920 --> 00:20:22,200
Rooter, who have both been on the podcast relatively recently to apply that same transfer

267
00:20:22,200 --> 00:20:27,040
learning concept to natural language processing.

268
00:20:27,040 --> 00:20:34,040
But reinforcement learning, on the other hand, like it's still largely the domain of like

269
00:20:34,040 --> 00:20:39,920
playing video games, it's like the thing that research labs are pursuing as a stepping

270
00:20:39,920 --> 00:20:48,200
stone to general artificial intelligence, the only folks that very, I've seen very few

271
00:20:48,200 --> 00:20:54,040
folks applying deep reinforcement learning in a kind of commercial enterprise business

272
00:20:54,040 --> 00:20:58,040
kind of way, the bonsai again being the one.

273
00:20:58,040 --> 00:21:05,440
Are you talking about, so you changed it to deep reinforcement learning versus reinforcement

274
00:21:05,440 --> 00:21:06,440
learning?

275
00:21:06,440 --> 00:21:11,640
And we also touched on the idea of transfer.

276
00:21:11,640 --> 00:21:16,040
I tried to be careful there because, yes, reinforcement learning does have a history

277
00:21:16,040 --> 00:21:22,960
that goes back to, I don't know when, I think the 60s, like it's been around for a while

278
00:21:22,960 --> 00:21:30,320
and it's been used as a kind of standard optimization algorithm.

279
00:21:30,320 --> 00:21:33,920
I think it's related to like multi-arm bandits and things like that.

280
00:21:33,920 --> 00:21:39,920
I don't know a ton about it and how it's been used, but what's different is deep reinforcement

281
00:21:39,920 --> 00:21:41,000
learning.

282
00:21:41,000 --> 00:21:45,760
It's promised, but also the challenge is associated with making it work.

283
00:21:45,760 --> 00:21:46,760
Got it.

284
00:21:46,760 --> 00:21:51,840
And I've seen a lot of Elon Musk headlines around how open AI is beating all sorts of professional

285
00:21:51,840 --> 00:21:55,800
e-sports gamers and stuff like that, but to your point, I haven't seen a lot of commercial

286
00:21:55,800 --> 00:21:57,280
applications ever yet.

287
00:21:57,280 --> 00:21:58,280
Yeah.

288
00:21:58,280 --> 00:22:05,960
And if Amazon scale says anything, maybe we'll see that change over the next couple years.

289
00:22:05,960 --> 00:22:11,520
I did want to hit on something interesting since we were talking about different things

290
00:22:11,520 --> 00:22:17,120
that we believed were the most significant.

291
00:22:17,120 --> 00:22:24,800
I've been kind of perusing and I know I had mentioned as my favorite and most impactful

292
00:22:24,800 --> 00:22:34,240
being the Lambda Layers functionality, but I did kind of a interesting perusal on Twitter

293
00:22:34,240 --> 00:22:43,000
and it appears that the most popular announced thing out there is the outpost announcement

294
00:22:43,000 --> 00:22:44,680
out of all of them.

295
00:22:44,680 --> 00:22:54,800
And by a fairly large margin, like three-ish X roughly compared to everything else, actually

296
00:22:54,800 --> 00:22:55,800
that's not true.

297
00:22:55,800 --> 00:23:05,480
Looking, the runtimes to Lambda is about, let's say, two thirds as popular as the outpost

298
00:23:05,480 --> 00:23:06,480
announcement.

299
00:23:06,480 --> 00:23:11,040
Although taking into account it was announced six hours ago, maybe it'll surpass it,

300
00:23:11,040 --> 00:23:14,240
but outpost is pretty darn popular.

301
00:23:14,240 --> 00:23:21,040
And I think it is pretty significant both for ML and such and outside of ML, really.

302
00:23:21,040 --> 00:23:25,640
The other announcement that we didn't talk about that I do think has big impact for ML

303
00:23:25,640 --> 00:23:30,920
is the, and I don't remember the name of it now, but it's the elastic something that

304
00:23:30,920 --> 00:23:38,120
you can attach a GPU to an EC2 instance, I see that is pretty big as well.

305
00:23:38,120 --> 00:23:44,200
So let's start there and then kind of work our way back to outpost.

306
00:23:44,200 --> 00:23:48,600
I was in a conversation with someone yesterday, maybe it was you now, when we were talking

307
00:23:48,600 --> 00:23:53,040
about what was kind of, what was our most exciting, what were the things we were most exciting

308
00:23:53,040 --> 00:23:54,040
about.

309
00:23:54,040 --> 00:23:59,880
And in that conversation, I prefaced it by, there's the thing I'm kind of geeked about,

310
00:23:59,880 --> 00:24:04,440
kid in a candy store style, and the thing I think is going to be most impactful.

311
00:24:04,440 --> 00:24:08,760
And that thing is the elastic inference.

312
00:24:08,760 --> 00:24:15,360
So Andy, Jesse, I heard a couple of numbers thrown around, and in fact, this was a question

313
00:24:15,360 --> 00:24:23,240
that I was asking folks to share data points on Twitter several weeks ago, but the data

314
00:24:23,240 --> 00:24:31,120
points that AWS folks were throwing around were that anywhere from 10 to 20%, 10 to 30%

315
00:24:31,120 --> 00:24:42,320
of the cost of doing machine learning is training, and 70 to 90% of the costs is inference.

316
00:24:42,320 --> 00:24:49,560
And the example that someone pointed me to when I was first looking for this, that's really

317
00:24:49,560 --> 00:25:01,640
hit home for me was Google, it turns out that their investment in the TPU building chips

318
00:25:01,640 --> 00:25:08,920
and as much as they're investing in that now on multiple versions of these chips, that

319
00:25:08,920 --> 00:25:13,200
all started when Jeff Dean kind of did this back of the envelope calculation that said

320
00:25:13,200 --> 00:25:20,400
that our user base does just three seconds of voice search per day, the inference would

321
00:25:20,400 --> 00:25:27,160
basically eat us alive and have us needing to build some multiple of them needing to build

322
00:25:27,160 --> 00:25:33,200
some multiple of the number of data centers that they already had.

323
00:25:33,200 --> 00:25:43,720
And so attacking that inference costs and AWS is throwing down numbers like 75% reductions

324
00:25:43,720 --> 00:25:52,640
in inference costs is potentially quite huge for folks that are doing ML and a cloud.

325
00:25:52,640 --> 00:25:58,080
Yeah, my question actually curiosity more than question is I always have this mental

326
00:25:58,080 --> 00:26:02,440
model of training being done in the cloud, but inference at the edge and a bunch of the

327
00:26:02,440 --> 00:26:06,520
use cases for customers who are working with autonomous vehicles, driverless taxis and

328
00:26:06,520 --> 00:26:08,440
so forth are very much that.

329
00:26:08,440 --> 00:26:12,360
But do you have any idea what the percentage is of inference if there is a breakdown roughly

330
00:26:12,360 --> 00:26:15,800
between cloud and edge?

331
00:26:15,800 --> 00:26:16,800
That is a good question.

332
00:26:16,800 --> 00:26:19,000
I don't have any data.

333
00:26:19,000 --> 00:26:26,960
I think what everyone's excited about for edge is that even if whatever the number is

334
00:26:26,960 --> 00:26:32,320
today, there's kind of, you know, whenever you see like somebody joked about it on did Andy

335
00:26:32,320 --> 00:26:38,080
Jesse joke about this on stage or maybe it was at the analyst summit, but like you always

336
00:26:38,080 --> 00:26:44,080
see this exponential curve in the number of devices that are going to be out in the edge.

337
00:26:44,080 --> 00:26:50,760
And there is a presumption that a lot of inference is going to be moving to the edge for, you

338
00:26:50,760 --> 00:26:54,600
know, latency and bandwidth reasons and all other things.

339
00:26:54,600 --> 00:27:01,520
And in fact, they had an announcement there, I think, yeah, actually so it's SageMaker

340
00:27:01,520 --> 00:27:03,240
Neo did.

341
00:27:03,240 --> 00:27:05,480
I don't think it showed up in the keynote.

342
00:27:05,480 --> 00:27:07,520
Did you guys catch that one?

343
00:27:07,520 --> 00:27:08,520
I did not.

344
00:27:08,520 --> 00:27:10,520
I did not see it in the keynote.

345
00:27:10,520 --> 00:27:13,920
I don't remember seeing it anywhere.

346
00:27:13,920 --> 00:27:21,600
So SageMaker Neo is a essentially it's, it's a, they're open sourcing it.

347
00:27:21,600 --> 00:27:25,080
It's an open source model compiler.

348
00:27:25,080 --> 00:27:32,800
And so they kind of pitching it as this train wants run anywhere, but they've built in some,

349
00:27:32,800 --> 00:27:41,720
some deep optimization into this model compiler so that they're touting two X performance and

350
00:27:41,720 --> 00:27:51,760
one tenth the size for your models and they're targeting a wide variety of hardware targets

351
00:27:51,760 --> 00:28:00,240
including edge devices and some of these like compute and memory constrained device targets.

352
00:28:00,240 --> 00:28:01,520
That one should be pretty cool.

353
00:28:01,520 --> 00:28:06,040
And it kind of begs the question to Dave's point, you know, how low or how far can they

354
00:28:06,040 --> 00:28:08,040
take out post and future generations?

355
00:28:08,040 --> 00:28:12,480
And they bring it down to you know Raspberry Pi style and maybe not the elasticity of it

356
00:28:12,480 --> 00:28:16,880
still have the exact same control and management flow that you'd enjoy in a cloud.

357
00:28:16,880 --> 00:28:19,520
So I never thought of outpost quite like that.

358
00:28:19,520 --> 00:28:25,680
So maybe we should jump into outpost and what it is and why are folks excited about it.

359
00:28:25,680 --> 00:28:27,240
I think that's a good idea.

360
00:28:27,240 --> 00:28:34,440
I think it's going to be the bridge that gets people that that had reasons why they needed

361
00:28:34,440 --> 00:28:37,680
physical equipment on premises.

362
00:28:37,680 --> 00:28:44,960
This doesn't give them the excuse that Amazon is only available in Amazon data centers.

363
00:28:44,960 --> 00:28:53,960
You now have flexibility to have equipment running on premises that is the same as what

364
00:28:53,960 --> 00:29:02,880
AWS runs in its data centers will to you all on site as well as offering the capability

365
00:29:02,880 --> 00:29:06,080
to run and be either in the cloud or on site.

366
00:29:06,080 --> 00:29:08,880
I see that as a pretty big deal.

367
00:29:08,880 --> 00:29:10,480
It really isn't.

368
00:29:10,480 --> 00:29:15,040
You know, here's a couple of the maybe non intuitive questions that come to mind for me is

369
00:29:15,040 --> 00:29:20,400
how are Amazon and partners like we are going to prove compliance?

370
00:29:20,400 --> 00:29:25,120
You know with data, the necessity laws or other privacy laws internationally with regards

371
00:29:25,120 --> 00:29:29,800
to what kind of metadata and what kind of control, you know, really flows back and forth

372
00:29:29,800 --> 00:29:34,800
between one of their regions and an on-prem implementation of outpost.

373
00:29:34,800 --> 00:29:36,840
I'd love to get more details on that.

374
00:29:36,840 --> 00:29:46,400
Well, one of the things that was not in abundance around this announcement was details.

375
00:29:46,400 --> 00:29:49,600
What we know is that this is AWS Design Hardware.

376
00:29:49,600 --> 00:29:55,440
It's the same hardware they say that runs in their data centers, but it's delivered and

377
00:29:55,440 --> 00:29:59,960
installed on the customer premises.

378
00:29:59,960 --> 00:30:05,520
There are supposedly two flavors of it.

379
00:30:05,520 --> 00:30:09,440
What's not clear at all are the hardware specs, and they said they're not talking about

380
00:30:09,440 --> 00:30:11,160
that until next year.

381
00:30:11,160 --> 00:30:14,680
There are two kind of flavors of it.

382
00:30:14,680 --> 00:30:21,680
One of them is, you know, runs as VMware kind of flavor of AWS.

383
00:30:21,680 --> 00:30:26,600
And the other is kind of traditional EC2 instances.

384
00:30:26,600 --> 00:30:32,520
So it appears like an extension of your AWS Availability Zone.

385
00:30:32,520 --> 00:30:33,720
That begs my questions.

386
00:30:33,720 --> 00:30:37,760
Yeah, I would want to feel like as a developer is an operator, I'd want to operate like

387
00:30:37,760 --> 00:30:38,760
that.

388
00:30:38,760 --> 00:30:42,200
But you know, when I think about overseas deployments or even things like China, there's

389
00:30:42,200 --> 00:30:46,040
a lot of those detailed questions that need to be answered to see just how broadly applicable

390
00:30:46,040 --> 00:30:47,040
this could be.

391
00:30:47,040 --> 00:30:52,680
You know, this is not new in the kind of the world of cloud.

392
00:30:52,680 --> 00:30:57,680
Azure Soft has Azure Stack, which is kind of the same idea.

393
00:30:57,680 --> 00:31:04,840
AWS is saying that I guess they talked about EC2.

394
00:31:04,840 --> 00:31:10,560
They talked about cloud formation templates and AMIs being available.

395
00:31:10,560 --> 00:31:15,680
They said that some of the other services like RDS and SageMaker will be supported.

396
00:31:15,680 --> 00:31:21,160
It's not clear when that will be is could take a while.

397
00:31:21,160 --> 00:31:26,760
I'd love to see how much of some of these cool new, you know, Lambda services and runtimes

398
00:31:26,760 --> 00:31:32,480
are available without post as well, because quite naturally, me like most SaaS companies

399
00:31:32,480 --> 00:31:37,720
would love to be able to answer yes when certain customers ask to run your functionality

400
00:31:37,720 --> 00:31:38,960
on Prem.

401
00:31:38,960 --> 00:31:42,840
This offers a promise of that and based on some of the answers to the detailed questions

402
00:31:42,840 --> 00:31:47,960
now or later, this could be game changing in terms of really helping the entire SaaS

403
00:31:47,960 --> 00:31:51,960
industry penetrate deeper into the enterprise, you know, on the back of AWS.

404
00:31:51,960 --> 00:31:56,160
I would argue you could already do that with the green grass.

405
00:31:56,160 --> 00:32:01,040
You can already run Lambda functions and several of the other capabilities green grass

406
00:32:01,040 --> 00:32:06,720
on premises, along with some of the capabilities that Snowball provides.

407
00:32:06,720 --> 00:32:09,040
And that was another announcement that was made.

408
00:32:09,040 --> 00:32:16,240
I don't know if it was pre the conference or not, where the new specs of Snowball, which

409
00:32:16,240 --> 00:32:22,440
were pretty impressive, and that's another thing that you can run on premises along with

410
00:32:22,440 --> 00:32:23,440
green grass.

411
00:32:23,440 --> 00:32:28,600
And the green grass stuff, people are running all sorts of Lambda functions and such already.

412
00:32:28,600 --> 00:32:30,080
And that's cool.

413
00:32:30,080 --> 00:32:35,800
If I could see DynamoDB and S3 and obviously Lambda as we've mentioned, and just a few basic

414
00:32:35,800 --> 00:32:41,160
easy to instance types available on prem with the same control flow and the same developer

415
00:32:41,160 --> 00:32:45,720
interfaces that that's a game changer at least for from my SaaS offering.

416
00:32:45,720 --> 00:32:54,800
So before we kind of run down the rest of the ML and AI announcements, there are a few

417
00:32:54,800 --> 00:33:05,640
candidates for the wackiest new product or new offering or the most unexpected Val, what's

418
00:33:05,640 --> 00:33:07,880
on that list for you?

419
00:33:07,880 --> 00:33:08,880
I wouldn't call it wacky.

420
00:33:08,880 --> 00:33:11,120
I'd call it super geeky cool, which is deep racer.

421
00:33:11,120 --> 00:33:12,120
Right?

422
00:33:12,120 --> 00:33:16,440
I'll put you guys on a spot and say if you've pre-ordered yours already, I think I remember

423
00:33:16,440 --> 00:33:25,680
your answer Sam, not sure, but I have not ordered that although I did, I did order the deep

424
00:33:25,680 --> 00:33:29,760
lens when it was available and got that played with that quite a bit.

425
00:33:29,760 --> 00:33:31,080
Very cool.

426
00:33:31,080 --> 00:33:36,320
The other wacky thing and it belongs in the rumor mill was that Amazon AWS, we're doing

427
00:33:36,320 --> 00:33:40,800
something with all the photos of the attendees that appear on the badges with regards to

428
00:33:40,800 --> 00:33:48,040
just propping up their own image training data sets for their services.

429
00:33:48,040 --> 00:33:49,040
Yeah.

430
00:33:49,040 --> 00:33:52,000
Again, that's just a rumor us on Twitter that caught my attention.

431
00:33:52,000 --> 00:33:53,000
Okay.

432
00:33:53,000 --> 00:33:58,040
I would say wackiest for me is AWS station.

433
00:33:58,040 --> 00:33:59,040
Yes.

434
00:33:59,040 --> 00:34:05,680
And so that caught me off guard.

435
00:34:05,680 --> 00:34:14,000
A fully managed ground station as a service for when you want to launch a satellite.

436
00:34:14,000 --> 00:34:18,200
That I was not seeing, I did not see that one coming at all.

437
00:34:18,200 --> 00:34:23,800
I can finally plan that trip to the Sahara desert.

438
00:34:23,800 --> 00:34:31,000
They also announced RoboMaker, which is this environment for like building robotics applications

439
00:34:31,000 --> 00:34:35,640
that I kind of, I mean, they did some stuff with with with Ross.

440
00:34:35,640 --> 00:34:38,280
Last year, I think.

441
00:34:38,280 --> 00:34:42,480
And so maybe this is an extension of that, but I, you know, they're doing like it's a full

442
00:34:42,480 --> 00:34:47,560
robotics development environment, it's got simulation environment, they're doing fleet

443
00:34:47,560 --> 00:34:48,560
management.

444
00:34:48,560 --> 00:34:54,000
That's that's one of the biggest things that I thought was exciting about that was the

445
00:34:54,000 --> 00:35:01,160
simulator where you can, where you can see not only a single robot and interfacing with

446
00:35:01,160 --> 00:35:07,280
that, but you can simulate that fleet of robots and how they will behave without needing

447
00:35:07,280 --> 00:35:08,640
all that equipment.

448
00:35:08,640 --> 00:35:14,680
I think that's a game changer in and of itself for people doing robotic projects.

449
00:35:14,680 --> 00:35:15,680
Mm-hmm.

450
00:35:15,680 --> 00:35:19,720
And that's actually cool, but ironically, I think that was a Tuesday announcement, right?

451
00:35:19,720 --> 00:35:23,720
So it almost got wrapped up by me.

452
00:35:23,720 --> 00:35:28,120
I'd love to see again whether or not so much reference customers, but just see examples

453
00:35:28,120 --> 00:35:32,840
of how people are applying this, even in small ways, you know, whether it's interesting enough

454
00:35:32,840 --> 00:35:38,360
for, you know, robotic assembly and assembly lines, whether it's helping with logistics

455
00:35:38,360 --> 00:35:44,040
of shipping or other interesting applications, particularly ones where there's a lot of

456
00:35:44,040 --> 00:35:49,320
health hazards to humans and robots could do those tasks much more safely.

457
00:35:49,320 --> 00:35:50,320
Mm-hmm.

458
00:35:50,320 --> 00:35:56,520
Speaking of early announcements that kind of got, you know, swept up and forgotten in the

459
00:35:56,520 --> 00:36:05,320
melee here, there was an interesting announcement in what has now become known as pre-invents,

460
00:36:05,320 --> 00:36:09,000
the, you know, all of the announcements before reinvent.

461
00:36:09,000 --> 00:36:15,760
We've got a name for that now, so as one of the pre-invent announcements, they announced

462
00:36:15,760 --> 00:36:16,760
predictive scaling.

463
00:36:16,760 --> 00:36:18,760
Did anyone catch that?

464
00:36:18,760 --> 00:36:22,320
I think out of the sight of my eye, but I never actually internalized it until you

465
00:36:22,320 --> 00:36:27,080
brought it up right now, that's stupid.

466
00:36:27,080 --> 00:36:30,120
Was that tied to some of the automated tiering in S3 that they announced?

467
00:36:30,120 --> 00:36:31,920
Ah, so that's another interesting one.

468
00:36:31,920 --> 00:36:40,240
So I don't know, you know, maybe kind of ideologically tied, well, definitely ideologically tied,

469
00:36:40,240 --> 00:36:41,560
and that's what's kind of interesting here.

470
00:36:41,560 --> 00:36:47,200
So predictive scaling is basically, I'm assuming it works, you know, in conjunction with like

471
00:36:47,200 --> 00:36:53,640
cloud formation and some of the other auto-scaling mechanisms, but the idea is that they've got

472
00:36:53,640 --> 00:37:02,080
machine learning models now that are making predictive scaling decisions as opposed to

473
00:37:02,080 --> 00:37:07,920
the operator having to define a set of rules around, you know, memory or CPU utilization

474
00:37:07,920 --> 00:37:09,760
or what have you.

475
00:37:09,760 --> 00:37:15,920
And so this is particularly interesting for folks that have applications that maybe have

476
00:37:15,920 --> 00:37:23,680
a long warm up time or take a long time to, you know, stand up for whatever reason.

477
00:37:23,680 --> 00:37:28,880
Yeah, I was thinking, yeah, Lambda cold start, right, that sounds like an ideal application

478
00:37:28,880 --> 00:37:30,480
for that.

479
00:37:30,480 --> 00:37:37,760
And so this, the S3 thing, I forget what that one is called, but it's similar like they're

480
00:37:37,760 --> 00:37:47,920
going to be predictive tiering or something like particularly staging data across these.

481
00:37:47,920 --> 00:37:51,800
Storage tiering is like a good decade, 15 year old technology that's been applied, you

482
00:37:51,800 --> 00:37:57,760
know, particularly in the days of high cache systems like the MC Symmetrics when it first

483
00:37:57,760 --> 00:38:02,920
came out and then in the area of flash now with memory tiers and flash and disk tiers.

484
00:38:02,920 --> 00:38:09,480
But they've always been fairly, you know, static models or very simplistic algorithms.

485
00:38:09,480 --> 00:38:13,280
And I think some people are trying to, you know, I remember, because it's not hopefully

486
00:38:13,280 --> 00:38:16,600
not confidential and it's been more than 12 months at NetApp, we were definitely trying

487
00:38:16,600 --> 00:38:20,920
to do some more clever things with regards to on tap caching, but this seems to take

488
00:38:20,920 --> 00:38:24,720
it to a whole other level because of obviously the resources available for both the training

489
00:38:24,720 --> 00:38:29,760
and the inference to apply, you know, the right decision to whichever data needs to move

490
00:38:29,760 --> 00:38:30,920
between tiers.

491
00:38:30,920 --> 00:38:38,120
So these two announcements collectively for me are a strong candidate for kind of a most

492
00:38:38,120 --> 00:38:46,320
exciting thing at re-invent and it's the idea that machine learning is becoming part

493
00:38:46,320 --> 00:38:52,600
of the infrastructure, right, part of the way that we're able to deliver, you know, scalable

494
00:38:52,600 --> 00:38:53,600
infrastructure.

495
00:38:53,600 --> 00:38:58,320
So, you know, Jeff Dean, obviously Google does a lot of this and AWS does a lot of this

496
00:38:58,320 --> 00:39:05,320
stuff internally that we don't know so much about, but Jeff Dean at Google, for example,

497
00:39:05,320 --> 00:39:13,240
has been doing talks on this for quite a while about how, you know, we can use machine learning

498
00:39:13,240 --> 00:39:19,560
for a ton of things to make infrastructure more effective, like query optimization and

499
00:39:19,560 --> 00:39:25,400
cache optimization and, you know, predictive scaling and tiering.

500
00:39:25,400 --> 00:39:30,400
But I think this is just the beginning of this wave.

501
00:39:30,400 --> 00:39:33,440
And that's one of my favorite topics actually.

502
00:39:33,440 --> 00:39:37,000
Probably remember, I think the title of Jeff's paper was the case for learned indexes.

503
00:39:37,000 --> 00:39:41,880
It was actually a eco-authored, it was a bunch of other researchers, but it was fascinating

504
00:39:41,880 --> 00:39:49,360
that it ties into a blog Andre Carpathy from Tesla Rotor on software 2.0 and how, you

505
00:39:49,360 --> 00:39:54,440
know, incredibly valuable, you know, web properties such as the actual Google homepage, the

506
00:39:54,440 --> 00:40:00,520
search page is anywhere between 15 to 20 percent machine-generated code now, no longer

507
00:40:00,520 --> 00:40:02,520
human-generated code.

508
00:40:02,520 --> 00:40:06,680
And you know, more to Jeff's point, you're able to take a lot of these low-level routines

509
00:40:06,680 --> 00:40:12,480
with a storage or compute resource management index management and so forth and be better

510
00:40:12,480 --> 00:40:17,360
at it with thousands of instances of machine inference versus just a few smart DBAs or,

511
00:40:17,360 --> 00:40:22,120
you know, other content managers and that's interesting, but also potentially disturbing

512
00:40:22,120 --> 00:40:24,800
trend if it's not managed carefully.

513
00:40:24,800 --> 00:40:30,600
So I'm going to run down these announcements and just jump in if you've got, if it strikes

514
00:40:30,600 --> 00:40:34,640
you as interesting or you've got anything you want to chime in on.

515
00:40:34,640 --> 00:40:40,400
So we talked about SageMaker ground truth, SageMaker reinforcement learning, deep racer,

516
00:40:40,400 --> 00:40:44,480
minus on order, valve is on order.

517
00:40:44,480 --> 00:40:50,200
I wonder, well, by the time this podcast is published, it will be more expensive.

518
00:40:50,200 --> 00:40:54,040
There is a special on these things during re-invent.

519
00:40:54,040 --> 00:41:01,960
So I tweeted about it, hopefully you saw that, if you're interested, EC2, P3, DN instances,

520
00:41:01,960 --> 00:41:06,120
one of you guys referred to this earlier.

521
00:41:06,120 --> 00:41:07,120
Yes.

522
00:41:07,120 --> 00:41:08,120
Yes.

523
00:41:08,120 --> 00:41:17,520
So this is the AWS's largest compute instance, P3, but with now 100 gig networking and VD

524
00:41:17,520 --> 00:41:26,440
V100, 32 gigs per GPU, Skylake, VCPUs with the AVX 512 instruction set.

525
00:41:26,440 --> 00:41:34,200
That sounds like a mother of all instance for today for this week.

526
00:41:34,200 --> 00:41:37,720
It's a massive amount of compute power and an instance type that you can spin up on

527
00:41:37,720 --> 00:41:38,720
demand.

528
00:41:38,720 --> 00:41:39,720
Yeah.

529
00:41:39,720 --> 00:41:40,720
Yeah.

530
00:41:40,720 --> 00:41:44,920
It all comes back to the ability to rent a supercomputer right for as little time as you

531
00:41:44,920 --> 00:41:45,920
need.

532
00:41:45,920 --> 00:41:49,360
It's amazing, one of the amazing attributes of cloud overall.

533
00:41:49,360 --> 00:41:51,920
So we talked about elastic inference.

534
00:41:51,920 --> 00:41:58,560
We did not talk about another interesting announcement, AWS Inferentia.

535
00:41:58,560 --> 00:42:00,840
Anyone catch that one?

536
00:42:00,840 --> 00:42:01,840
I did not.

537
00:42:01,840 --> 00:42:03,800
What is that mean either?

538
00:42:03,800 --> 00:42:12,880
AWS Inferentia is their forthcoming high performance machine learning chip.

539
00:42:12,880 --> 00:42:19,800
So this is a product of their acquisition of Annapurna Labs.

540
00:42:19,800 --> 00:42:22,480
It is slated to come out next year.

541
00:42:22,480 --> 00:42:30,640
It is inference, I believe, well, Inferentia is focused on inference, right?

542
00:42:30,640 --> 00:42:37,960
And each of these chips will do hundreds of tensor operations per second.

543
00:42:37,960 --> 00:42:43,360
And you can cluster these chips to scale that to the thousands.

544
00:42:43,360 --> 00:42:49,440
And so there's kind of this interesting one, two punched between elastic inference, which

545
00:42:49,440 --> 00:42:54,560
is, I think this is GA now, right?

546
00:42:54,560 --> 00:43:01,800
And Inferentia, so elastic inference is going to promises to lower inference cost by up

547
00:43:01,800 --> 00:43:04,640
to 75%.

548
00:43:04,640 --> 00:43:11,400
GA when it comes online, they're saying offers further reduction of cost by 10X.

549
00:43:11,400 --> 00:43:12,400
Wow.

550
00:43:12,400 --> 00:43:13,400
Wow.

551
00:43:13,400 --> 00:43:14,400
Wow.

552
00:43:14,400 --> 00:43:17,600
So this actually corresponds to two related announcements this week.

553
00:43:17,600 --> 00:43:23,440
One was obviously support for, I don't know, initially, or just more ARM instance types.

554
00:43:23,440 --> 00:43:26,840
And the other one that was in an Amazon announcement, but just happened the same week, was the

555
00:43:26,840 --> 00:43:30,120
risk five announcement by the Linux Foundation.

556
00:43:30,120 --> 00:43:33,920
And some of the potential opportunities that offers people like Amazon to offer really

557
00:43:33,920 --> 00:43:38,400
interesting customized chips, that's like this for almost any high volume application

558
00:43:38,400 --> 00:43:40,240
with a 10X impact.

559
00:43:40,240 --> 00:43:41,240
Interesting.

560
00:43:41,240 --> 00:43:42,240
Interesting.

561
00:43:42,240 --> 00:43:48,640
And so clearly, AWS is not alone in chasing down this custom inference chip, custom inference

562
00:43:48,640 --> 00:43:49,640
hardware.

563
00:43:49,640 --> 00:43:57,920
We talked about the Google TPU, Microsoft, is there's brain wave, or is that something different?

564
00:43:57,920 --> 00:44:03,280
Yeah, I know they've had, was it, these are certainly custom FPGA with Azure, but maybe

565
00:44:03,280 --> 00:44:07,720
even some custom A6, but am I the only one now that starts to feel like this is the

566
00:44:07,720 --> 00:44:12,440
mainframe error all over again, with everyone running completely custom hardware and some

567
00:44:12,440 --> 00:44:17,600
thin shame of compatibility, maybe at the container level between the clouds themselves.

568
00:44:17,600 --> 00:44:24,840
Well, or the model compiler level, I'll say maker Neo, wait, are we saying that AWS is

569
00:44:24,840 --> 00:44:33,080
becoming IBM that all of them are actually, except for maybe IBM ironically, you're

570
00:44:33,080 --> 00:44:40,240
doing with power, but yeah, I was talking to someone and we were saying that AWS was becoming

571
00:44:40,240 --> 00:44:51,040
more like an IBM in Microsoft of the 90s and that we were seeing, I think we said Microsoft

572
00:44:51,040 --> 00:44:57,880
was behaving more like, like Apple, Apple was behaving, who's Apple behaving, Apple

573
00:44:57,880 --> 00:44:59,760
was behaving more like,

574
00:44:59,760 --> 00:45:09,040
Oracle, Dell, in the 90s, I lived through those errors of Microsoft could kill an industry

575
00:45:09,040 --> 00:45:15,160
with a pre-announced press release and it does seem like Amazon, particularly AWS has

576
00:45:15,160 --> 00:45:19,480
that power now in the tech industry to just announce or pre-announce something in freeze

577
00:45:19,480 --> 00:45:22,240
like an entire new market segment.

578
00:45:22,240 --> 00:45:28,240
I mean, if you look at outpost, I wouldn't be looking to buy hardware to build my new

579
00:45:28,240 --> 00:45:34,840
VMware or a competitive cloud thing until I was able to see what they were going to offer

580
00:45:34,840 --> 00:45:37,560
and what the cost structure and such looked like.

581
00:45:37,560 --> 00:45:41,920
Yeah, at a macro level, you can probably start the countdown clock as to how many more

582
00:45:41,920 --> 00:45:46,720
hardware refresh cycles there will be for people if, as you say, Dave, if the specs and

583
00:45:46,720 --> 00:45:50,280
the price performance match up to expectations.

584
00:45:50,280 --> 00:45:54,600
The machine learning offerings that we ran through are kind of under the broad category

585
00:45:54,600 --> 00:46:01,440
of new stuff and then there's more like improvements and incremental stuff and tools and there

586
00:46:01,440 --> 00:46:04,680
are a bunch of those as well.

587
00:46:04,680 --> 00:46:12,760
So one of those was, they announced earlier in the week, a marketplace for machine learning.

588
00:46:12,760 --> 00:46:20,600
So kind of like an ML app store to anyone that catches your eye, but I'd actually caught

589
00:46:20,600 --> 00:46:21,600
my eye.

590
00:46:21,600 --> 00:46:26,600
I noticed, of course, they weren't the first to do this, but just the fact that in combination,

591
00:46:26,600 --> 00:46:30,880
with integrated workflow for a data scientist and all the other cool tools that are either

592
00:46:30,880 --> 00:46:36,000
maturing, which I think the maturing of all these services announced in prior years might

593
00:46:36,000 --> 00:46:41,160
be the unsung hero of these announcements because they're really usable now and very functional.

594
00:46:41,160 --> 00:46:46,360
But the fact that now you have this kind of marketplace also available on AWS is with

595
00:46:46,360 --> 00:46:50,640
partners as well as natively from the rest of Amazon is very, very cool.

596
00:46:50,640 --> 00:46:56,000
If they do share some of the models they use in the proper Amazon retail side, then that

597
00:46:56,000 --> 00:47:00,360
could also be a game changer with regards to e-commerce everywhere.

598
00:47:00,360 --> 00:47:03,960
One of the things that I thought was interesting about this, if I'm not mixing it up with

599
00:47:03,960 --> 00:47:09,840
another announcement is that it's all based on when I first heard it describe, I thought,

600
00:47:09,840 --> 00:47:16,760
okay, well, how is this different from labeling an amy as machine learning, but this is all

601
00:47:16,760 --> 00:47:18,280
based on containers?

602
00:47:18,280 --> 00:47:24,360
Yeah, I forgot that detail, but it makes it a bit more granular for sure.

603
00:47:24,360 --> 00:47:25,360
Yeah.

604
00:47:25,360 --> 00:47:34,720
So it's interesting, it's based on containers, they've got some ability to, like the partner

605
00:47:34,720 --> 00:47:41,080
amies, you can monetize them, and they've got folks like Figure 8 and Deep Vision, H2O,

606
00:47:41,080 --> 00:47:48,960
H2O and some others that are kind of already part of this new ecosystem.

607
00:47:48,960 --> 00:47:52,720
And yeah, how is this going to tie into the comments you made earlier in the podcast on transfer

608
00:47:52,720 --> 00:47:58,680
learning and the ability to just buy a model, maybe tweak it or not and just apply it to

609
00:47:58,680 --> 00:48:03,560
some other thing and be super productive much early on in a data science cycle?

610
00:48:03,560 --> 00:48:11,800
I mean, I think that's where this heads, I think the interesting thing will be, to me,

611
00:48:11,800 --> 00:48:16,640
can they evolve it where I can see what the specific specs of the performance of this

612
00:48:16,640 --> 00:48:22,680
model are and compare similar models and look at the performance and accuracy that they

613
00:48:22,680 --> 00:48:29,440
offer and decide which one is best for me, and at that point, there's not that much incentive

614
00:48:29,440 --> 00:48:33,920
for me to go out and build my own model from scratch.

615
00:48:33,920 --> 00:48:37,160
How about a recommendation and engine for we just described Dave?

616
00:48:37,160 --> 00:48:38,560
Of course.

617
00:48:38,560 --> 00:48:46,320
I'm like slowly wrapping my head around what this is, and I've talked to folks at AWS

618
00:48:46,320 --> 00:48:48,120
about this yesterday.

619
00:48:48,120 --> 00:48:55,560
I think the thing that caught me up is that the folks that they are rolling out as partners

620
00:48:55,560 --> 00:49:00,280
like TIPCO and figure eight, I think of them as like application and tools and platform

621
00:49:00,280 --> 00:49:05,640
vendors, and so I think that these are like entire systems that we could already put those

622
00:49:05,640 --> 00:49:10,360
in AMIs and spin them up, and so what's the big deal?

623
00:49:10,360 --> 00:49:16,680
But I guess the reason why this container thing is important, going back to what was

624
00:49:16,680 --> 00:49:22,520
clear to you and is sinking in for me is that those are just companies that are publishing

625
00:49:22,520 --> 00:49:32,120
into a model marketplace and repository that you can access from SageMaker as opposed

626
00:49:32,120 --> 00:49:38,520
to like spinning up some new product or spinning up some whole product in an instance.

627
00:49:38,520 --> 00:49:41,040
It's broke-growing algorithms.

628
00:49:41,040 --> 00:49:47,320
So there are folks that play in this space, algorithmia is one, a company who I've got

629
00:49:47,320 --> 00:49:50,560
a lot of respect for, they're doing really interesting things.

630
00:49:50,560 --> 00:49:55,120
It'd be interesting to see, you know, how this plays out for them.

631
00:49:55,120 --> 00:50:02,360
So the next set of announcements are, I think, all around these, like the highest tier

632
00:50:02,360 --> 00:50:14,640
and the ML stack, these cognitive APIs or these AI as a service APIs, and there are a couple

633
00:50:14,640 --> 00:50:18,360
of these that I found really interesting.

634
00:50:18,360 --> 00:50:27,640
So there's one that's called Personalize, and the idea is that it is a recommendation

635
00:50:27,640 --> 00:50:33,200
system kind of as a service, recommendations as a service, and then there's one that's

636
00:50:33,200 --> 00:50:38,160
called Forecasts, and that is Forecasting as a service.

637
00:50:38,160 --> 00:50:43,880
And, you know, their pitch for this is that recommendations and forecasts are both, forecasting

638
00:50:43,880 --> 00:50:50,680
are both these examples of things that are really hard to do generically.

639
00:50:50,680 --> 00:50:58,320
Like your product catalog and data sources, you know, whether it's click stream or logs

640
00:50:58,320 --> 00:51:01,840
or whatever, that's very customized to you.

641
00:51:01,840 --> 00:51:06,400
And likewise, things that you need to forecast are very customized to you.

642
00:51:06,400 --> 00:51:12,400
And so it's very difficult to offer just a generic personalization API or forecasting

643
00:51:12,400 --> 00:51:17,560
API, but, and this is what's really interesting.

644
00:51:17,560 --> 00:51:21,440
And they didn't explain it like this, but it was kind of a light bulb as soon as they

645
00:51:21,440 --> 00:51:23,160
set it for me.

646
00:51:23,160 --> 00:51:29,760
What they've really done with these is like, it's an application level version of AutoML.

647
00:51:29,760 --> 00:51:36,200
Like so what Google did with AutoML is like, it's a machine learning thing, right?

648
00:51:36,200 --> 00:51:42,160
You give it a bunch of images, and, you know, they're going to do architecture

649
00:51:42,160 --> 00:51:50,520
search and optimization to give you to give you a better, you know, image labeling API.

650
00:51:50,520 --> 00:51:53,000
But this is, that's same idea.

651
00:51:53,000 --> 00:51:58,160
You're giving it your data, and it's going to do architecture search and data cleansing

652
00:51:58,160 --> 00:52:05,440
and like a whole set of steps that is doing is basically doing AutoML for you, but at

653
00:52:05,440 --> 00:52:12,760
at a much higher level, at an application or really a business level, which is really cool.

654
00:52:12,760 --> 00:52:13,760
I agree.

655
00:52:13,760 --> 00:52:17,280
I would love to try this, you know, with very common businesses that have to forecast,

656
00:52:17,280 --> 00:52:19,720
you know, how much supply they're going to buy.

657
00:52:19,720 --> 00:52:24,240
Forecasts were kind of staffed to get a need to manufacture or assemble something.

658
00:52:24,240 --> 00:52:25,240
Forecasts were kind of shipping.

659
00:52:25,240 --> 00:52:28,960
They're going to have to reserve ahead of time forecast how long it's going to take

660
00:52:28,960 --> 00:52:32,520
for customers to pay them, and you know, is that going to be quicker than when the payments

661
00:52:32,520 --> 00:52:34,160
have to make their suppliers.

662
00:52:34,160 --> 00:52:38,520
This can impact literally almost every aspect of a business, and it's going to be exciting

663
00:52:38,520 --> 00:52:41,200
to see what people do with it next year and year after that.

664
00:52:41,200 --> 00:52:42,200
Mm-hmm.

665
00:52:42,200 --> 00:52:44,480
So I pulled up my list of the things that they're doing.

666
00:52:44,480 --> 00:52:49,480
So they're loading your data, they're inspecting your data, they're identifying features,

667
00:52:49,480 --> 00:52:53,920
they're selecting algorithms, they're selecting hyperparameters, they're training and optimizing

668
00:52:53,920 --> 00:52:59,080
models, they're building a feature store, they're deploying and hosting these models or

669
00:52:59,080 --> 00:53:03,560
so serving them up for you, and they're caching, they're creating real-time caches.

670
00:53:03,560 --> 00:53:09,560
So they're doing all of this stuff, you know, specific to your data, that is pretty cool.

671
00:53:09,560 --> 00:53:10,560
Yeah.

672
00:53:10,560 --> 00:53:15,400
So a couple of other services that they announced, Comprehend Medical, so Comprehend

673
00:53:15,400 --> 00:53:27,320
is their, like, text extraction service, and Comprehend Medical is a vertical medical,

674
00:53:27,320 --> 00:53:31,120
obviously, oriented version of that.

675
00:53:31,120 --> 00:53:38,640
And then another one that I think is kind of the, maybe like the low-key, super high

676
00:53:38,640 --> 00:53:45,600
utility, you know, not sexy, announcement of this whole, you know, of all of these is

677
00:53:45,600 --> 00:53:48,680
a new one called Textract.

678
00:53:48,680 --> 00:53:58,640
And it's basically a souped up OCR system that you give it a PDF, or they say almost

679
00:53:58,640 --> 00:54:03,640
any document, and it'll extract the text and data from that document, but it does it

680
00:54:03,640 --> 00:54:09,200
in a way that's like, if there are tables in that document, it will extract those tables.

681
00:54:09,200 --> 00:54:17,000
If there are, you know, you know, kind of lists, you know, it'll extract key value pairs.

682
00:54:17,000 --> 00:54:23,800
It, I heard someone say this, it wasn't in any of the slides I saw, but like it, it sounds

683
00:54:23,800 --> 00:54:29,840
like it will retain some of the structure of the documents, it'll deliver bounding

684
00:54:29,840 --> 00:54:36,680
boxes for where, you know, document features are like tables and stuff like that.

685
00:54:36,680 --> 00:54:41,880
And you don't need to know any machine learning to do any of that stuff.

686
00:54:41,880 --> 00:54:49,800
I can see, I can envision, you know, whole businesses kind of just built on kind of commercializing

687
00:54:49,800 --> 00:54:50,800
this service.

688
00:54:50,800 --> 00:54:56,200
Yeah, like I remember still having PTSD from doing expenses at an app with like an antiquated

689
00:54:56,200 --> 00:55:00,440
1990s Oracle, like expense system, and so this could revolutionize that, right?

690
00:55:00,440 --> 00:55:05,800
If you have expenseify or concur, integrate this, hopefully very quickly, it could just

691
00:55:05,800 --> 00:55:10,800
increase productivity and employing morale so much just for expense reporting alone.

692
00:55:10,800 --> 00:55:13,600
I was curious how that would work.

693
00:55:13,600 --> 00:55:18,840
You would upload receipt images and have it generate the expense report.

694
00:55:18,840 --> 00:55:21,600
Exactly, zero touch almost.

695
00:55:21,600 --> 00:55:27,880
The use case that they gave for this was tax forms.

696
00:55:27,880 --> 00:55:36,480
Like, you know, there's one canonical W-9 form, but there are, you know, hundreds of

697
00:55:36,480 --> 00:55:39,520
different kind of versions of W-9 forms.

698
00:55:39,520 --> 00:55:43,560
Like as long as you capture the information and the lines, you could come up with your own

699
00:55:43,560 --> 00:55:46,640
custom version of that form and apparently everyone has.

700
00:55:46,640 --> 00:55:56,600
And so automating, pulling, extracting the data from those forms into a form that you

701
00:55:56,600 --> 00:56:02,000
can then dump into a database is difficult because you have all these variants of these

702
00:56:02,000 --> 00:56:04,160
forms.

703
00:56:04,160 --> 00:56:10,200
You know, if you think of like invoices, you know, it's structured or semi-structured if

704
00:56:10,200 --> 00:56:12,000
you prefer data, right?

705
00:56:12,000 --> 00:56:16,600
It's not a picture, it's not a bunch of texts, but OCR just treats it, you know, it starts

706
00:56:16,600 --> 00:56:22,040
with a picture and gets out a bunch of texts, but it loses all of the semantic meaning in

707
00:56:22,040 --> 00:56:23,040
these forms.

708
00:56:23,040 --> 00:56:29,560
And so what text extract is meant to do is to preserve all of that using, of course, machine

709
00:56:29,560 --> 00:56:32,480
learning, which is pretty cool.

710
00:56:32,480 --> 00:56:35,920
I see text, but the bigger one that I see would be medical.

711
00:56:35,920 --> 00:56:36,920
Yeah.

712
00:56:36,920 --> 00:56:38,920
Talk about that.

713
00:56:38,920 --> 00:56:40,520
That's huge.

714
00:56:40,520 --> 00:56:47,600
Think about all of the medical forms, things that you would submit to insurance.

715
00:56:47,600 --> 00:56:53,040
All of that, if you were a hospital or medical, medical group and you deal with submitting

716
00:56:53,040 --> 00:57:01,200
to different insurance companies, I see lots and lots of applicability for that.

717
00:57:01,200 --> 00:57:07,000
Let alone the myriad of different, as you pointed out, text forms and such.

718
00:57:07,000 --> 00:57:08,200
I would see that as big.

719
00:57:08,200 --> 00:57:15,600
The other thing that would be big would be legal in maintaining the forms because a lot

720
00:57:15,600 --> 00:57:20,880
of different legal documents that you file with courts and such also follow a standardized

721
00:57:20,880 --> 00:57:27,040
format for that court or maybe even for that state, depending, but at least for, say,

722
00:57:27,040 --> 00:57:34,200
that specific county, but it will change for others, but they, but all of those documents

723
00:57:34,200 --> 00:57:39,640
for that specific court will follow the same format, at least for long periods of time,

724
00:57:39,640 --> 00:57:41,320
say, decades.

725
00:57:41,320 --> 00:57:47,120
So being able to maintain that would also be huge, but those are just a few examples

726
00:57:47,120 --> 00:57:50,640
that come to mind that are very, very large have broad applicability.

727
00:57:50,640 --> 00:57:51,640
Mm-hmm.

728
00:57:51,640 --> 00:57:54,000
Yeah, the semantic processing of images is one thing.

729
00:57:54,000 --> 00:57:59,520
I'm wondering how they'll be able to extend this to other localized languages, so to speak,

730
00:57:59,520 --> 00:58:04,880
because I just came back from some trips to China and that massive Belt and Road Initiative

731
00:58:04,880 --> 00:58:10,480
they have, touches so many countries and so many cultures and languages and written

732
00:58:10,480 --> 00:58:15,440
characters and scripts that as this mature, this could be another game changer for how

733
00:58:15,440 --> 00:58:19,520
you process that kind of very heterogeneous supply chain.

734
00:58:19,520 --> 00:58:22,400
Yeah, so it's, you're coming about medical.

735
00:58:22,400 --> 00:58:26,840
Does raise an interesting question about the relationship between the new comprehend

736
00:58:26,840 --> 00:58:33,640
medical offering and textract and do they already, you know, talk to one another, does

737
00:58:33,640 --> 00:58:39,120
textract use comprehend medical for that very domain specific thing or can you do that

738
00:58:39,120 --> 00:58:42,800
or is that something that's coming in the future?

739
00:58:42,800 --> 00:58:44,800
Interesting stuff there.

740
00:58:44,800 --> 00:58:52,320
So I think we made it through the ML related stuff amazingly in an hour, but there's

741
00:58:52,320 --> 00:58:57,840
a bunch more stuff does anyone have or do you guys have like your favorite thing that

742
00:58:57,840 --> 00:59:00,280
we haven't talked about yet?

743
00:59:00,280 --> 00:59:01,280
We covered a lot.

744
00:59:01,280 --> 00:59:06,640
I think there's a whole bunch of as we talked earlier, maturing security offerings just

745
00:59:06,640 --> 00:59:12,640
for better overall governance and supervision of the various policies you have and the monitoring

746
00:59:12,640 --> 00:59:17,480
tools and the probably endless dashboards now that are available to you in terms of

747
00:59:17,480 --> 00:59:24,080
monitoring activity, so the whole notion of just a maturity overall of security and compliance

748
00:59:24,080 --> 00:59:30,440
tools and tool sets for AWS customers is one of those unsung heroic set of announcements

749
00:59:30,440 --> 00:59:32,640
this year for me as well.

750
00:59:32,640 --> 00:59:33,640
How about you, Dave?

751
00:59:33,640 --> 00:59:34,640
I don't know.

752
00:59:34,640 --> 00:59:41,840
I think for me, the, I think the biggest thing is the absolute number of deeply innovative

753
00:59:41,840 --> 00:59:48,560
announcements with lots of meat behind them, lots and lots of things in the ML and the

754
00:59:48,560 --> 00:59:56,720
infrastructure space, both and it makes me wonder if it's not time for them to change

755
00:59:56,720 --> 00:59:58,000
the conference.

756
00:59:58,000 --> 01:00:03,720
I think that we're at the point where there are too many announcements.

757
01:00:03,720 --> 01:00:09,560
The topics are too broad, they cover too much and the conference itself, it's not possible

758
01:00:09,560 --> 01:00:16,400
to go if you have an interest that spreads even a couple of these for you to successfully

759
01:00:16,400 --> 01:00:22,880
go and attend the workshops and sessions and so when a conference reaches that, it's

760
01:00:22,880 --> 01:00:29,240
usually time for the conference to split out into more separate conferences that cover

761
01:00:29,240 --> 01:00:34,360
each of those kind of domains and I think reinvent is at that point.

762
01:00:34,360 --> 01:00:35,360
That's my opinion.

763
01:00:35,360 --> 01:00:40,160
As an attendee, I would say I agree with Dave, right, I'm not sure if you're a presenter

764
01:00:40,160 --> 01:00:44,840
or you know, exhibitor or actually the one that hosts the conference that the economies

765
01:00:44,840 --> 01:00:49,520
of scale would be attractive but as an attendee, I couldn't agree more with Dave and I think

766
01:00:49,520 --> 01:00:54,280
moreover, we noticed earlier on that there's more and more pre-announcements happening

767
01:00:54,280 --> 01:00:58,880
because you can only squeeze in one keynote a year for major announcements and spreading

768
01:00:58,880 --> 01:01:05,040
the conferences out, both location wise, venue wise chronologically would let them announce

769
01:01:05,040 --> 01:01:09,360
I think more timely availability of these cool new services that they keep announcing.

770
01:01:09,360 --> 01:01:13,800
I also have a meta comment but I'm going to hold that for one second because I do have

771
01:01:13,800 --> 01:01:20,920
one more product announcement that I'm really excited about and it's kind of like adjacent

772
01:01:20,920 --> 01:01:23,800
to machine learning stuff.

773
01:01:23,800 --> 01:01:26,400
Did you catch the Lake Formation announcement?

774
01:01:26,400 --> 01:01:27,880
Yes.

775
01:01:27,880 --> 01:01:30,480
Lake Formation is pretty cool.

776
01:01:30,480 --> 01:01:39,920
So I guess the meta to Lake Formation is that AWS this year, I think for the first time,

777
01:01:39,920 --> 01:01:45,240
has been super aggressive about positioning S3 as a data lake.

778
01:01:45,240 --> 01:01:48,960
I have not heard them talk about it like that before.

779
01:01:48,960 --> 01:01:56,280
And so Lake Formation is this new offering that basically automates the data pipeline for

780
01:01:56,280 --> 01:02:01,400
getting data into S3.

781
01:02:01,400 --> 01:02:13,200
So talking about like crawling your on-premises data stores and cataloging and cleansing data,

782
01:02:13,200 --> 01:02:25,520
getting it into S3, organizing it, deduplication and it can crawl relational databases as well

783
01:02:25,520 --> 01:02:32,160
as semi-structured and unstructured data sources.

784
01:02:32,160 --> 01:02:40,840
When I think about and talk to folks about the path to kind of repeatable enterprise machine

785
01:02:40,840 --> 01:02:47,400
learning, like the stumbling block for a lot of people, like kind of stuck in the blocks

786
01:02:47,400 --> 01:02:53,640
is having a centralized data repository, whether you call it a data warehouse or it is a data

787
01:02:53,640 --> 01:03:00,920
warehouse or a data lake or a fabric or whatever, I guess a big deal and it's cool to see them

788
01:03:00,920 --> 01:03:01,920
taking that on.

789
01:03:01,920 --> 01:03:06,880
Yeah, in fact, not only that, but I think they buried the lead a little bit with Lake Formation

790
01:03:06,880 --> 01:03:11,000
and that this can revolutionize things like GDPR and other kinds of privacy compliance

791
01:03:11,000 --> 01:03:16,240
efforts where the discovery part of this alone, particularly as more and more data is either

792
01:03:16,240 --> 01:03:21,400
managed in conjunction with our post at both on-prem and off-prem, the discovery part and then

793
01:03:21,400 --> 01:03:26,160
the cataloging part is that mature as you can automatically categorize data and create

794
01:03:26,160 --> 01:03:29,640
automated master data models and then you can do all sorts of interesting recommendation

795
01:03:29,640 --> 01:03:31,400
engines based on that.

796
01:03:31,400 --> 01:03:35,360
When you tie some of those workflows together that is a really revolutionary in this world

797
01:03:35,360 --> 01:03:37,840
of compliance and spending a lot of time in.

798
01:03:37,840 --> 01:03:38,840
Awesome.

799
01:03:38,840 --> 01:03:39,840
Yeah.

800
01:03:39,840 --> 01:03:40,840
I agree.

801
01:03:40,840 --> 01:03:50,400
I think a lot of people have been using S3 is kind of their mini data lake, so to speak.

802
01:03:50,400 --> 01:03:56,040
They'll dump as much as they can in, but they're still kind of limited in their options with

803
01:03:56,040 --> 01:04:02,360
what they can do and how it works, although they keep expanding S3's functionality constantly.

804
01:04:02,360 --> 01:04:12,200
I still think this taking on the data lake, especially for the impact it has with analysis

805
01:04:12,200 --> 01:04:15,760
and machine learning is a big deal.

806
01:04:15,760 --> 01:04:16,760
It really is.

807
01:04:16,760 --> 01:04:17,760
That's true.

808
01:04:17,760 --> 01:04:27,080
My meta comment that I was saving was one of the things that they've taken some steps

809
01:04:27,080 --> 01:04:33,520
towards addressing with their, oh, they also announced like a whole ML training and certification

810
01:04:33,520 --> 01:04:43,120
curriculum, by the way, or that, oh, that, right, exactly, and they'd previously announced

811
01:04:43,120 --> 01:04:51,040
like ML partner competencies, but one of the things that struck me in going into the Expo,

812
01:04:51,040 --> 01:04:58,400
which is massive, like several hundred exhibiting companies in the Expo, very few of them, even

813
01:04:58,400 --> 01:05:04,840
at this stage, our ML and AI related, like a lot of them have, or do other things that

814
01:05:04,840 --> 01:05:12,680
are kind of, have their ML and AI story as vendors do, tech vendors do, but in terms of

815
01:05:12,680 --> 01:05:22,840
kind of pure play, ML and AI partners, you know, whether services or product ISVs, software

816
01:05:22,840 --> 01:05:28,000
as a service, the pickings were relatively slim.

817
01:05:28,000 --> 01:05:32,200
And it kind of brought to mind a question that I started kind of asking myself last year,

818
01:05:32,200 --> 01:05:37,200
which is like, does AWS have an ecosystem problem around machine learning?

819
01:05:37,200 --> 01:05:44,760
Like, is it not, you know, open enough or inclusive enough or, you know, something that they're

820
01:05:44,760 --> 01:05:51,680
not able to kind of catalyze partners to get on board or make it worth their while?

821
01:05:51,680 --> 01:05:53,080
Or is it just too early?

822
01:05:53,080 --> 01:05:57,320
It's something I noticed actually about the machine learning, you know, industry and

823
01:05:57,320 --> 01:06:02,120
markets, I went as a whole, is by and large, very few of those companies, if any, grow

824
01:06:02,120 --> 01:06:06,680
up to be big, independent successes, and, you know, consequently, there's just a lot of

825
01:06:06,680 --> 01:06:08,880
acryhiring going on.

826
01:06:08,880 --> 01:06:13,880
So I don't know if these companies reach a stage where they can actually get enough capital

827
01:06:13,880 --> 01:06:19,000
to have a branding style of marketing campaign that capital and tons of companies, like

828
01:06:19,000 --> 01:06:24,800
the storage companies, my confirmation bias goes to, I noticed, I noticed so many giant,

829
01:06:24,800 --> 01:06:29,640
you know, storage, you know, logos and branding on the miracle mile screen, you know, across

830
01:06:29,640 --> 01:06:34,640
from the area in the Cosmo, and I just saw a lot of marketing and branding dollars going,

831
01:06:34,640 --> 01:06:36,800
you know, from infrastructure companies.

832
01:06:36,800 --> 01:06:40,560
But to your point, not so much more, a lot of the smaller innovative companies and spaces

833
01:06:40,560 --> 01:06:41,560
like ML.

834
01:06:41,560 --> 01:06:42,560
Hmm.

835
01:06:42,560 --> 01:06:43,560
Interesting.

836
01:06:43,560 --> 01:06:44,560
Take for sure.

837
01:06:44,560 --> 01:06:51,320
I think that just speaks more to my point about splitting the conference into smaller conferences.

838
01:06:51,320 --> 01:06:52,320
Right.

839
01:06:52,320 --> 01:06:53,320
Right.

840
01:06:53,320 --> 01:06:59,200
If you're a ML or AI pure play company, like the, the hit rate here, the signal to noise

841
01:06:59,200 --> 01:07:01,560
ratios got to be pretty low still.

842
01:07:01,560 --> 01:07:02,560
Yeah.

843
01:07:02,560 --> 01:07:03,560
Unfortunately.

844
01:07:03,560 --> 01:07:08,680
Uh, I could get with that, Dave, uh, just, let's start a movement right now.

845
01:07:08,680 --> 01:07:11,880
Don't make it any later in the year.

846
01:07:11,880 --> 01:07:13,680
Oh, yeah.

847
01:07:13,680 --> 01:07:15,480
Agreed.

848
01:07:15,480 --> 01:07:21,520
Um, you know, I think we should, uh, I think we should reach out to Berner and, uh, and

849
01:07:21,520 --> 01:07:26,160
and, and Jesse and tell them that they're awesome.

850
01:07:26,160 --> 01:07:33,480
Well, guys, I've got to pack up and bolt to the airport, uh, but it was awesome or recapping.

851
01:07:33,480 --> 01:07:35,920
Uh, reinvent with you guys.

852
01:07:35,920 --> 01:07:40,000
Thanks so much for, uh, hopping on with me at a great time.

853
01:07:40,000 --> 01:07:41,400
Thanks for including me this year.

854
01:07:41,400 --> 01:07:45,680
And yeah, to Dave's point, maybe we'll have, you know, more mini recaps going forward

855
01:07:45,680 --> 01:07:47,760
when they split up these conferences.

856
01:07:47,760 --> 01:07:48,760
Nice.

857
01:07:48,760 --> 01:07:49,760
Nice.

858
01:07:49,760 --> 01:07:50,760
Awesome.

859
01:07:50,760 --> 01:07:51,760
Take care, guys.

860
01:07:51,760 --> 01:07:52,760
Bye.

861
01:07:52,760 --> 01:07:53,760
Bye.

862
01:07:53,760 --> 01:07:54,760
Bye.

863
01:07:54,760 --> 01:07:55,760
Bye.

864
01:07:55,760 --> 01:07:56,760
All right, everyone.

865
01:07:56,760 --> 01:08:01,880
That's our show for today for more information on Dave, Val, or any of the topics

866
01:08:01,880 --> 01:08:08,000
covered in this episode, visit twimmaleye.com slash talk slash 205.

867
01:08:08,000 --> 01:08:12,080
If you're a fan of the show and you haven't already done so, or if you're a new listener

868
01:08:12,080 --> 01:08:17,360
and you like what you hear, visit your Apple or Google podcast app and leave us a five-star

869
01:08:17,360 --> 01:08:18,760
rating and review.

870
01:08:18,760 --> 01:08:23,040
You reviews help inspire us to create more and better content and they help new listeners

871
01:08:23,040 --> 01:08:24,600
find the show.

872
01:08:24,600 --> 01:08:34,320
As always, thanks so much for listening and catch you next time.

