Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're running a series of shows consisting of conversations
with some of the impressive speakers from an event called the AI Summit in New York City.
The theme of that event and of this series is AI in the Enterprise and I think you'll
find this series really interesting in that it includes a mix of both technical and case
study oriented discussions.
Now I won't actually be attending the AI Summit this week because I'm in Long Beach
California attending the Nips Conference.
There are a bunch of Twimble listeners here and I'm hoping to meet as many of you as possible.
And yes, I have stickers.
If you're here at Nips and you're actually listening to podcasts this week, please
reach out to me either via the event app, the Nips event app where there's a Twimble
listeners thread or via Twitter where my handle is at Sam Charrington.
Before we proceed, let's quickly talk about the podcast schedule through the end of the
year.
Prior to my newsletter, you know that I've been on the road for a couple of weeks now.
After this week's series, we've got two more series coming before we break for the year
with our last show running on December 22nd.
Now, if you're lamenting two weeks without your favorite machine learning and AI podcast,
trust me with these two series.
The first from the Amazon Web Services Reinvent Conference and the next one from Nips.
You will have plenty of great content to tide you over until we get started again on January
8th.
Thanks to you, 2017 was a great year for the podcast and we plan to close it out strong.
So keep your ears open the next few weeks and we hope to hear from you.
Please note that on Wednesday, December 13th, we'll be holding our last Twimble Online
Meetup of the Year.
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion
segment.
And for our main presentation, Bruno Gensalvez, we'll be presenting the paper Understanding
Deep Learning requires rethinking generalization by Shi Yuanjiang from MIT and Google Brain and
others.
You can find more details and register for the meetup at twimlai.com slash meetup.
This AI Summit series is brought to you by our friends at IBM Power Systems.
IBM Power Systems offers servers designed for mission critical applications and emerging
workloads, including artificial intelligence, machine learning, deep learning, advanced
analytics, and high performance computing.
IBM Power Systems benefit from a wide range of open technologies, many stemming from
collaboration with fellow Open Power Foundation members, and they're designed to deliver
performance efficiently, whether deployed in private, public, or hybrid clouds.
To learn more about the IBM Power System AC922 platform for Enterprise AI, visit twimlai.com
slash IBM Power.
Continuing with last show's theme of platforms for Enterprise AI, my guess this time around
is Mazen Gilbert, Vice President of Advanced Technology and Architecture with AT&T.
Mazen and I have a really interesting discussion on what's required to scale AI in the Enterprise,
and you'll learn about a new open source project that AT&T is working on to allow any
Enterprise to be able to do this.
You already know by now that I geek out when it comes to talking about the intersection
of machine learning and cloud computing, and this conversation is no exception.
Be sure to let us know what you think by posting your comments or questions to the show
notes page at twimlai.com slash talk slash 78.
And now on to the show.
All right, everyone, I am on the line with Mazen Gilbert.
Mazen is Vice President of Advanced Technology and Architecture with AT&T.
Mazen, welcome to this weekend machine learning and AI.
Well, thank you very much for having me today.
Yeah, I'm excited to jump into this conversation.
You know, as is the tradition here, I like to get started by having you tell us a little
bit about your background and how you came to get involved in AI projects.
Well, thanks.
So I started looking at AI and when I was just doing my graduate degree in the University
of Liverpool, I got fascinated in the mid 80s about how you could use these AI technologies
to really teach machines and computers and at the time, computers and machines were
very primitive, how you can get these machines and computers to do sort of what humans can
do.
They can think and say and listen the way humans are, either that was extremely fascinating.
It was an area that caught my passion and that started my graduate degree and I've been
involved in AI machine learning ever since.
Oh, wow.
What did you do your graduate degree in?
So my graduate degree, my PhD was on artificial neural networks for computer speech.
So basically at the time, the problem I was trying to solve is how do you get computers
to speak like humans using artificial neural networks?
Okay.
And what did speak like humans mean for you in that context?
It was really, speak like a humans as the way that you and I are speaking today.
The goal was to pass the touring test is that how can you get a machine to speak to you
for so many seconds or half a minute without you knowing that was really a machine?
Clearly in the past 30 years of a lot have changed but that was at the time, computer
speech was extremely, very machine like.
There was a lot of promise at the time of artificial neural networks could be sort of the new
technology that can completely change that because of the association of artificial neural
network with the real neural networks in the brain.
And that's turned out to be, turned out to be true in terms of all of the advances that
are happening now with deep learning.
Yeah.
Absolutely. I mean, it's quite astronomical to see how much deep learning, which is really
based on artificial neural network.
How we've managed in the past 30 years to use those in a lot of different areas beyond
just computer speech, they're very powerful tools.
And I think what they've a lot of had changed in the past 30 years, but the basic algorithm,
the basic technologies from the 70s and 80s are still in use today.
And so did you go directly from grad school to AT&T?
I did.
So I actually came to visit AT&T Bell Labs in the late 80s while I was doing this work.
I was really fascinated by the place.
It was like a kid in a candy store, you know, it's like you get to do your passion and
your hobby and get paid for it, too.
So it was, it was really an amazing experience.
I knew at the time when I met a bunch of the folks that I used to sort of know them, when
I used to write these papers, they used to be like a reference in my book, a reference
in my papers.
And now I get to see them face to face and some of the pioneers that invented, you know,
a lot of amazing technology and digital transformation and transistors and others.
It was an amazing experience and it was very clear that was the place for me and honestly
as soon as I couldn't even wait to finish my graduate degree.
In fact, I ended up doing, I finished my graduate degree at AT&T Bell Labs because it was
an area they were very interested in and neural networks.
So that's how it started.
Oh, wow, wow.
Can you give me a, you know, certainly AT&T and the labs are, you know, doing a ton of
things with regards to AI, you know, how can you give us a kind of a landscape of the
different areas that you're working in?
Yeah, it's AI is a very critical area for us and has been ready for decades.
It is, it's really the way we think of AI.
It's a, it's a way of doing data powered smart automation and when you think about it as
it's just data powered smart automation, it really touches every part of our business.
The machine learning side is what makes it data powered and the AI is what provides this
smart automation.
That's sort of how we think of the two and the relationship with each other.
The way we look at this is that where are we spending a lot of our dollars today and
that we need something new capabilities that are disruptive in the market, disruptive for
AT&T really to try to break down our cost structure and provide better experience to our
customers and better products to our customers.
So we've identified a number of what we call them, major pillars, several years back
and a lot of our activities are really focused on those pillars.
And what are they?
So the pillars are number one is infrastructure and engineering.
So think of that as a company like AT&T that invests significantly in capital and infrastructure
in building 4G and 5G and we're in the homes and we're in your devices and everything.
There's a huge infrastructure that we put in place.
So we spend a lot of time and capabilities in applying machine learning and AI to really
do smarter investment and spending of building out our infrastructure.
Where do we put the small sales, where do we put the macro sales, where do we invest
more in what part of the US, where there is most traffic or we think there will be traffic
over the next year to three years.
So that's infrastructure and engineering.
Number two is in the area of customer experience and I think customer care and customer experience
is the traditional approach as you know, the whole industry went into that with AI when
they started using speech recognition and speech synthesis and some natural language processing
to start doing part of what humans would do today, at least some of the basic things
of let's say understanding the intent of the customer and routing the customer.
So we're doing a lot of that and have been doing that for chat, for voice, for internet,
for email, basically the customer experience side of the game.
The third pillar that we also invest a lot of time is obviously security and when you
look at the size of AT&T and we provide our customers with a secure network, both consumer
and enterprise and those hackers are becoming smarter and smarter and so that we have to
apply the best and the latest technologies to identify even trends or patterns of an
initial attack to any of our customers or to our network so we can address that quickly
before it becomes a big issue and there are millions of those that are attempted every
hour of the day.
So that's a third area.
The fourth area is our network and operation.
As you know, we run one of the most complex networks in the world and that network has
evolved over the past century since the invention of the telephone.
You can imagine that there is a lot of push to try to make it zero touch, to make it
completely automated, to make it completely software defined.
So there's a lot of work we do in that area and the last one I will mention is sort of
the area where we look for a new revenue opportunity.
So we apply AI machine learning for advertising, media analytics and for a lot of our, in
the future, a lot of our enterprise businesses that they sell to customers, what that is
to do with the internet of things, what that is to do with smart cities, healthcare,
the opportunity space is big, AT&T is a very diverse company.
And as you can imagine, there are a lot of opportunities there.
That is a ton of stuff.
So in your role, what's your specific organization's responsibility with regard to these five
pillars is it staying ahead of kind of where this technology is going and then feeding
it back to businesses or are you building out projects and products that these businesses
can take advantage of?
How do you think of the role of your business?
So it's along the line what you said, I lead an organization, the advanced technology
and architecture.
This is the research arm also for AT&T.
And part of my responsibility is to ensure AT&T is always one step ahead and two steps
ahead.
So it's not just understanding the technology, but also understanding where are the opportunities
and the applications where we could provide value add to our customers and better products
and services to our customers.
That is part of my organization, part of the job that we do.
But beyond that, it's also about one thing I think you're just striking about all the
pillars I mentioned, we're not a company trying to build an application for AI.
We're not trying to build a voice personal assistant and make it a variable to our customers.
We're not.
Right.
Right.
The use of AI machine learning for us is across many different services, many different
opportunities where we spend billions of dollars today in each of these pillars.
So there are not just 10 or 20, but they're literally in the thousands of applications
where AI machine learning is required.
So part of my responsibility is to make sure that we can scale and that each of these
application is not yet another service, new technology, new operation of its own, we
have to provide some harmonization to make sure that all of these thousands of applications,
when we get there, they all are built on a common platform, common technology, common
capability, so as technology evolve, you are benefiting everybody.
You're not building just a silo application for one set of users.
One of the challenges in building platforms when we're so early in the technology cycle
is that it's not clear what the pieces of that platform need to be.
How are you addressing that and other challenges that you run into given, you know, the scale
that you described and the fact that you're trying to apply AI very broadly across these
thousands of opportunities?
That's really a great question.
It's you're absolutely right.
I think that, you know, although the technology behind machine learning and AI have been
brewing for decades, but we are just at the tip of the mountain, touching that sort of
our initial, we're getting just a flavor of, well, where are all the opportunities, until
you understand all the opportunities, it's hard to say, well, here is the platform that's
going to support all these opportunities.
So you absolutely, absolutely right.
So the way we're looking at this is sort of a little different.
We're looking at this coming in saying, well, you are absolutely right.
And we have to enable the technology is going to continue to change.
There's going to be new applications.
There's going to be new tools.
There's going to be new capabilities.
The way we need to think about the platform, we have to build an open platform.
It's got to be an open platform that enable rapid innovation, not just within AT&T, but
within the global ecosystem.
And it's a platform in which that our vendors and our suppliers and our partners are part
of the development of this platform, part of the ecosystem.
Because we are all sort of joining hands together to building sort of harmonizing the core
foundation behind AI across the world.
It's because every company out there have the same challenge, the same issue.
And every company is going to end up doing their own thing.
Unless we really build the foundation platform to be open and transparent.
You know, I came across a reference on your website to Acumos, is that, you know, a platform
or the platform that you're describing?
That is absolutely right.
So, how this started is that when about a year ago, when we were looking at the applications,
AT&T has deployed or about to deploy an AI machine learning.
There were tens of, at least the ones that I have tracked, I'm sure there are plenty more.
The one I've managed to track, there are tens of applications already deployed or about
to be deployed.
When I looked at those, is sort of every application supported by every business unit is different.
It's different technology, different vendor, different tools, different operational support.
And it was very clear that it just, if this is the direction we're going as a company
and we absolutely are, it's just not possible to scale.
It's straightforward to build one or to build two.
But to go and build now and scale 20 and take 20 to 2000, you can't.
It's just not possible without having some commonality, okay?
So Acumos was really born as a result of that, thinking of AT&T as a customer, not as
a builder of a platform.
It was really built to try and solve three key problems that AT&T was being challenged.
And we just discovered since then is that pretty much most companies are being challenged
by those three problems.
The first problem is there are a lot of tools out there for machine learning and AI.
And you know, wait another week and somebody else is going to come out with another tool.
And we love some of these tools.
Some of them are very specialized in some markets, some of them are specialized by the company.
They're great tools.
There's no one size fits all.
So the first problem we wanted to solve, even with an AT&T, we found out many people
are using many tools.
First we tried to force everybody to use one tool.
We actually put out, you know, two, three years ago, an open source tool for machine learning
and a statistical modeling called our cloud.
And we were hoping to force everybody in AT&T to use it and frankly we failed.
We ended up finding some people use it and some people like to use other tools that's
you know, even built by AT&T.
Okay.
So so we sort of we sort of moved away from that approach and we decided that the problem
we need to solve with this Akimus number one, Akimus need to harmonize across all the
tools.
So if I use tool A and you use tool B, even though we may not be working with each other,
we may not even talking to each other.
But the output of my tool A and the output of your tool B need to be like Legos and not
snowflakes.
Okay.
These things have to connect.
These things have to have to sort of interoperate with each other.
Not being sort of every tool is its own cycle.
So that's number one thing that Akimus is attempting to sort of solve.
And that's the problem we're trying to solve there, a higher minimization.
The second problem we're trying to solve is that, and I think you know this very well,
is that to build machine learning models or algorithms, it's very difficult.
It's not easy to do.
People talk about hiring PhDs and people will graduate degree.
Well how many of them are there today?
And how many companies require that skill set?
Okay.
I've been talking about lowering the barrier, making it easier.
But you know what, it's never, has been easier.
It's difficult.
It's difficult to do.
It requires some extensive understanding of the data.
It requires extensive cleansing of the data.
Why is you learning about some deep technologies, understanding what an ROC curve is, what a
type one error, what a type two error is, it allows you to understand what is a gradient
descent in neural network is.
These are very deep things.
And unless you go and take some deep courses and play with your favorite tool, whether it's
scikit-learn or TensorFlow or cafe or whatever, you're not going to learn this, okay?
And that's what they do in graduate degrees.
So the second problem we wanted to solve with Akimus was that we need to lower the barrier
to entry.
So that if Mazin goes and build a model, and maybe Mazin takes weeks, uses GPUs and takes
weeks to build that model.
But then if Sam wants to go and use that model, it should be available in a marketplace.
You should just grab and use and plug into somewhere.
And so that the second thing Akimus provides is a marketplace.
Is a marketplace that says, think of that the App Store.
I can go somewhere, I can see what everybody else has built worldwide.
And I could look at the review, the rating, and I can pick what I want.
I don't have to rebuild it.
We don't have to rebuild everything from scratch.
It's very expensive to rebuild.
We have to be thinking about reuse and improve as opposed to rebuild.
So that's what is the Akimus marketplace.
But what we do in the Akimus marketplace is more than an App Store.
But it does also, it says, if I built something and Taylor built something else, then her capability
and mine are microservices that we can just join together.
We can actually put those two together like Legos.
You can come in and connect what she has and what I have from the marketplace create a
new application that didn't exist before.
So these applications, these microservices are interoperable.
And the third thing we wanted to solve is to say, when you build the model with any tool
you want, I want to be able to deploy this model with any cloud and I want it to be minutes.
And I don't want to learn what a Docker container is.
I don't want to learn what Jenkins job is.
I don't want to learn what a G or a ticket is.
And I don't want to learn the internals of a third party cloud.
It should be really like, I build it.
I submit and it takes me all the way to it's running on a third party cloud.
That's sort of, again, what Akumus is trying to do.
Okay.
There's a ton in there to dig into.
I noticed on the Akumus page that there is an affiliation with the Linux foundation
or at least a project.
I'm assuming is the project is being kind of managed under the Linux foundation.
Does that mean that for taking another step back?
It sounds like a lot of the things that you described are almost outside of the realm
of what we think about when we think of traditional AI machine learning tooling.
It's more like the kinds of concerns that platform as a service has come along to try to
help with.
Is the implication of that in the Linux foundation involvement that this is built on cloud
foundry or is taking a similar approach?
So number one, you hit the nail on the head.
This is Akumus is not competing with the tools, the many tools that's out there today.
In fact, we want to encourage communities and companies to innovate and building the
next big, better algorithm.
How do you do deep learning now with a thousand layers instead of 20, 30 layers?
How do you have trillion of neurons as opposed to a billion of neurons?
We absolutely want companies and communities and industries and universities to innovate.
We're trying to fill a gap, a gap that every company in the world, if they want to seriously
look at AI machine learning, they have to fill.
And today that gap is so large that you can hire the best PhDs and you still will never
be able to scale deployment, adoption, and improvement of AI machine learning services.
Linux Foundation, we have an incredible relationship with the Linux Foundation.
We co-created Akumus with a partner of ours, Tech Mahendra, and we're hosting it under
the Linux Foundation.
Linux Foundation will host the marketplace and the platform.
And we will work with the Linux Foundation to grow a community of companies and institutions
and universities to really co-create together the foundation of how we want to scale and
adopt AI globally.
So we are already in discussions with a number of companies.
So we don't consider this as an AT&T or a Tech Mahendra effort.
We consider that we've started a seed and the Linux Foundation is going to help to grow
that seed and really build a lot of beautiful trees with the help of a larger community.
This isn't the first time we've seen things like this.
It really reminds me a lot of, for example, what Intel was trying to do with their TAP platform.
I don't know if you've ever come across that, but I'm not even sure they're still working
on that.
It's a huge problem where you're trying to integrate these many, many disparate components
like lots of tools.
In their case, I think they also built it on Cloud Foundry and then they had Hadoop and
a bunch of data lakes stuff underneath.
These are all very complex enterprise technologies.
They're all evolving very rapidly in their own different directions.
I guess my question is maybe pushing back on the whole idea of doing this.
Why do you think you can succeed at this?
It's such a Herculean task and so many have tried and failed.
It's a great question.
And frankly, if we don't solve this problem, if we don't solve this problem worldwide,
the adoption of what you see in the media about AI and machine learning is very limited.
It's going to be limited to a few companies who can afford to do it, a few companies who
have the expertise and a few companies who can build one or two applications here and
there.
We have to solve this gap.
It is a big gap.
You're absolutely right.
But I'll tell you what we are doing that's very different.
And we have experience in this five years ago.
We started with just as a big challenge as this one, as Akimos, which is we decided to
completely convert the AT&T century sort of young network that's been developed over
many, many decades.
We decided to completely move that to a software defined using the best technology in SDN and
virtualize the network function.
So this is like the core of the network.
You're now saying, I can build the core of a network like a web company where everything
is software running on a cloud environment, commodity cloud hardware.
That's just not possible.
Well we went through that avenue and we started deploying that three years ago, but we
realized a year ago there's something missing in here.
What's missing is that we've invested significantly to making that happen.
Hundreds of people, a lot of expertise, other companies can't do that, okay?
And other operators cannot do that.
And just having us doing something and operators are not, that's going to compromise the experience.
That's going to also compromise the cost to the end customer.
We need all operators, all vendors to be part of this.
And so we made it open source under the Linux Foundation and there's something called
ONAP.
And in the short seven months, we've been at this journey this year.
We have 50 plus companies.
We have 1400 technical people co-developing from these companies.
And we cover 55% of the global sub.
These are the operators, the China mobiles, the China telecom, the Vodafone's, the Orange
and Bel Canada and many of the brands that you basically hear about.
We've cracked the code that the way to do it is that if we can all chip in and if we all
can build thing, it build it together, we can create something better than anyone company
trying it on their own.
This is what we're trying to do the same thing with Akimas.
This is if AT&T ends up the only one or AT&T and Tec Mahandra, the only one doing this,
it will fail.
I'll just tell you that flat face, okay?
But what's going to be different in here, that's not what we're seeing from the companies
we're talking to already.
I can tell you that right now.
What we're seeing is a tremendous momentum.
They all realize the big gap.
They all realize the big challenging gap that you're talking about.
And by having a community come together and each of the companies chip into what they believe
is important to them.
So we will come in and say, well, what's going to work on our native cloud environment?
And it's going to work on a couple of third party cloud that has to work.
And it's going to do X, Y and Z.
Other companies are coming in with slightly similar, what's different also attitude, bringing
it all that together is really what's going to crack this.
It's not going to be AT&T, it's going to be a community building it together.
All of the times that's happened in the past, it's been one company trying to do it, trying
to boil an ocean.
What we're doing in here, there's no one company in the world can do it.
It has to be a community, a global community efforts.
Interesting.
So you mentioned ONAP.
This is maybe going a little bit deeper down this rabbit hole, but what's the relationship
between ONAP, which I hadn't heard of, and OpenStack?
I'm fairly familiar with AT&T's involvement over the years in OpenStack and in particular,
working to build NFV deployments on top of OpenStack.
Yeah.
The two are already like brothers and sisters, the complimentary and brothers and sisters.
OpenStack is the, we are a big user of OpenStack.
We've built our AT&T integrated cloud platform based on OpenStack.
So we take OpenStack, we're a significant part of OpenStack as part of the board as
well.
We ingest OpenStack and we build our AT&T cloud and this is what we use as the infrastructure
to virtualize our network functions.
So when I'm talking about virtualizing the network functions, moving our network to
software defined, it's all using our cloud and sort of a hybrid cloud of our cloud and
third-party cloud and OpenStack is a big part of that.
ONAP, is it, once you have that, once you have these network functions and you have your
cloud environment, well, you need to orchestrate these network functions.
You need to automate the design and the testing and the deployment of those network functions.
You need to do life cycle management of these network functions.
Something could go wrong, they could be a threat, they could be a failure.
So that is what ONAP does, it's an orchestration, automation, platform for both virtual and
physical network functions and life cycle management.
The two go together, you need ONAP on top of OpenStack and our AT&T integrated cloud to
really try to virtualize these functions.
Acumus and AI is a key sort of, a key enabler to enable both the cloud and the ONAP to automate
functionalities.
There are machine learning data driven.
So if ONAP comes in and says, I want to use data driven capabilities to predict there's
going to be a failure in the network in a particular location for a particular customer
an hour from now.
That's where machine learning and acumus comes in is creating these predictive, sophisticated
capabilities to make the ONAP and the integrated cloud data driven and predictable and have
a predictive capabilities.
I actually very keen on digging a little deeper into some of these network and operations
use cases, but I do have one question on acumus before we do that.
I noticed or didn't notice the lack of on the acumus page a link to like a GitHub repository
or something like that.
What stage is acumus in?
Is it a vision or is there more, something more tangible to it today that folks can get
access to?
I always start with a vision, but what it is, it's not just a vision.
If it was a vision, then we wouldn't be having the conversation, I'll be honest with you.
We believe you have to have something real, but this is a community effort.
Like you said, technology is changing a lot, but there's some basic foundation you have
to build.
What we've done is we announced this a couple of weeks ago or whatever, 10 days ago.
Next week, literally next week, we're moving all the software to the Linux foundation.
We built the software, we built the seed capabilities of everything I talked to you about.
The seed software of everything I talked about is being built, and we're moving it to
the Linux foundation next week.
We believe that the Linux foundation, they're going to set it up and set up their CICD
environment and testing environments by December, mid-December.
We believe, and again, that's up to the Linux foundation to make it available, but initially
it will be available for friendly companies who are going to be joining the project, and
early next year, in one queue next year, it's going to be available to the public.
Okay.
Well, I'll definitely be following along.
It sounds like an interesting project, and I can definitely, there's no question that
the need is there.
But it's, again, it's a huge challenge.
So network and operations.
I think maybe the most famous, maybe the most public kind of proof point, example point
here is some of the work that Google has done in this area where they showed that they
were able to reduce their cost of, I don't remember if it was the overall cost of data
center operations, or they're the heating and cooling costs by some 40%.
This is going back a year and change ago.
That's certainly, you know, certainly there are huge opportunities there for AT&T.
How are you attacking that space?
Yeah, it's a key space for us.
So number one, we probably have more data centers than many companies that I know today in
the United States.
We own different types of data centers from small to large to support our customers.
So when you start thinking about how does this all come together, first is that all the
capabilities in these data centers, first have to be software.
They have to be software running on cloud commodity hardware.
If you don't do that, then it's pretty much difficult to do anything else, including
artificial intelligence and acrimus.
You can't, unless you move to a software world.
And we've been doing that for the past five years, and that's part of this own app
effort that I just talked to you about.
We are basically going in that directions.
But where we are applying the technology in our data centers and we will continue to doubling
down in this space from a machine learning perspective and an AI perspective is not just
about energy consumption, which is the example that you gave.
Although that by itself is a big opportunity.
I don't want to minimize that.
But there are tremendous inefficiencies one can apply machine learning and AI to really
do something cheaper, faster, and better.
And combine that with human expertise, you could really sort of combine the best of both
well.
So doing something that we just can't even do today.
So we're not just applying it to looking at energy saving, but there are tremendous opportunities
in terms of operating these data centers.
There are many functions in these data centers.
Just think of, you know, we as a network company, we support the world here.
And there are switches, there are routers, there are firewalls.
This is the core core of our network of how packets go from A to B.
That's sort of the basics here.
All of these have to be automated and it has to be done in such a way that is intelligent.
So I'll give you a simple example.
At every second of the day in the United States, traffic changes.
And in some cases, we know and we predict that.
In some cases, we don't know.
Suddenly, there is a big event happening in a particular location, a particular city,
in a particular corner that just doesn't happen any day or any year, okay?
So part of what we apply machine learning in AI is to really dynamically configure our
network and our resources around the clock.
So I use this metaphor as the think of like we're building a virtual city.
That's the United States where we have limited capacity, but the roads, the roads can become
narrower or wider as long as the physical space is the same.
And how wide or narrow you make that is can change every second of a day.
And that's what we're using machine learning in AI to say, well, how much capacity can I put
in that second of a day there because we're predicting some traffic is going to be there
over the next one hour or five hours, okay?
And where do I take capacity from?
But imagine doing that and optimizing that around the clock.
That's a big, big undertaking.
So that's just one other example where we're applying these capabilities.
Are there some other specific examples or use cases that come to mind within the context
of networking ops?
Yeah.
So another example is that, so imagine a firewall goes down.
What do you do?
All days you, it's failed, somebody has to literally go there, has to remove it, put a new
device in there, new firewall, okay?
And plug everything in, test it, makes it work, could take days, could take weeks depending
if there are extra hardware available, et cetera.
Today, that's completely different.
It's number one, I can spin up the firewall or that switch at any point of the day, okay?
In any location.
That's number one.
That's the vision of what we're going after and we started already excluding on that
vision.
We're reaching about 55% of our target network in terms of virtualization by the end of
this year, okay?
So now, once I can put these capabilities there, the problem happens is that, okay, well
what happens now if it goes down?
Well, what we're implementing today is what we call it an AI closed loop, which means
that we have the capability of extracting data from this, let's say, firewall.
We're able to process and this data using machine learning.
We're able to predict if there are some failures, maybe happening, or some degradation.
We are able to apply policies to say, okay, well, we know there's something going on that's
bad, okay?
We know the driver is this, what do I do?
It could be that, well, rebuild the firewall, okay?
And then we need an action to go and take that action, okay?
Today, that's a manual effort.
Tickets are being, you know, we developed tickets, thousands of tickets around the clock,
hoping that a human is an expert is going to look at those tickets and take an action.
But really, a good portion of those tickets are things we already know.
We've learned from the data.
We know what's a good action, what a bad action.
That's where AI comes in is learning the good action from the bad action.
And being able to automate and do this closed loop in a complete automated way, which could
be reboots, it could be just move the traffic.
There's something chronic about the device, move the traffic and stand up traffic, stand
at the applications in another device or another firewall.
And that's sort of, again, you can think of the same thing we're doing with security.
There are many, many applications like this we are investigating today.
From taking a step back from a portfolio perspective, how do you kind of manage the investments
across all of these various opportunities that you have?
Yeah, that's a good, great question.
I mean, it's a number one, the reason why we are building a platform, we've done this
for our virtualized cloud.
Companies are doing that too for their virtualized cloud.
We're doing this for our orchestration and automation platform, the own app and what
we call it, e-comm inside AT&T.
We're doing this for machine learning and AI.
First, you've got to make sure the foundation is built once, developed by a community in
an open transparent way, but it's built once and evolved.
It's not changed every time from an investment that's just prohibitive, okay?
That's number one.
Number two, anything that sits on top of that investment, we're sort of making sure it
is software and making sure that you could take an investment that we already have and drive
that cost down with this new world of machine learning and software.
So every opportunity I talk to you about, it's not a vision and it's not something we would
like to do.
It's something we're doing today because there is billions of dollars.
We have a business case that says, if we do this through software and through own app
and through machine learning, we can say 50% of the cost.
So we're going after those where there are billions attached and with a technology and
the business case shows we can really shave a lot out of these investments, whether it's
capital investments or expense.
Certainly, AT&T has more resources available to it than many companies, but those resources
nonetheless are not unlimited.
What are the biggest challenges that you're finding in attacking your wish list?
I'm imagining that you've got way more places that you'd like to apply this technology
than you can actually take on at any given time.
Yeah.
Beyond the platform is a gaming like we've talked about that and I totally agree that in
the situation that you've described, building out a platform that lowers the barrier to
entry for other parts of the organization to build on top of is a great place to start.
But then what are the other barriers that if you can figure out how to fix those, you
would be able to accelerate your adoption?
So number one is that every company and where no different has limited resources and that's
number one.
Number two, we're not doing it alone.
We cannot do it alone, independent of even if you have the resources, it's you need the
skill set and subject matter expertise in different areas, network expertise and functions
that we may have some, but not others.
The challenge here is ensuring the ecosystem we work with, the suppliers, the vendors,
the partners that we do active engagement with today, all of us are working with the same
framework, with the same DNA, with the same sort of platform.
That is really how it's a win-win for everybody.
So that's number one challenge.
We've done a lot of progress in that, which led us to where we are today, but that is
definitely a challenge is getting everybody to go into sort of one path and using something
coming.
Imagine that if every car manufacturers build their car without any specifications, without
any sort of blueprints, it's going to be very expensive.
But today, all cars come with literally sort of four wheels maybe, they have a steering,
they have a seat, there are some expectations that every car has to have, but every car
manufacturers compete differently.
Well, imagine if that was not the case, if there were no specifications, then every car
has to be built so differently from scratch.
It costs a lot of money to build that car.
Today, that's not the case and car manufacturers have managed to get all their sort of suppliers
and vendors to align.
We're trying to do the same thing here.
This is a journey, AT&T is a leader in this journey from the networking side.
When you look at all the operators, we went to open source also to bring in other operators
into this game and work with them together so that it's a win-win for both of us.
That's a key challenge is harmonization, which is lead us to the relationship we have with
the Linux Foundation as a partner to help us drive harmonization worldwide.
Well, you certainly shared a lot of jewels with us.
Are there any other thoughts that you want to share with the audience on, if they're
trying to figure this all out, maybe where they should start or what they should do or
any other thoughts on any thoughts that you'd like to close us out with?
Sure.
Number one, I think that no matter which company you're from and no matter what investment
you have, don't build your own thing.
You have to decide what is a value at your company and a competitive edge for you versus
something that is really should be just mainstream.
So number one is that be part of an open source movement.
Join us in the networking side, join us in ONA.
If you are in the machine learning AI side, don't invest in your own silos.
Join us in a community so that your investment is multiplied by a factor of 100 with other
people's investments.
Don't do it alone.
That's sort of number one.
It's be part of the open source and really keep your secret sauce, but trying to use your
investment plus others to build something that's phenomenal.
Number two is as we build the foundation altogether, I think Sky is the limit in terms
of the applications.
We're just touching the surface of that.
I know everyone talks about things like voice recognition and facial detection.
These are wonderful.
These are great applications.
Research in these applications started three, four decades ago.
If you go back to the 70s, 80s, that's how it started.
And today we are at a point where we can commercialize, but there are a lot of opportunities of
applications.
We haven't even thought of applications, especially as we get to 5G and the Internet of
Things and Edge Cloud and applications that none of us have even conceived they existed.
We would love to expedite that innovations and that inventions and we would love everyone
to really be part of the movement and not to take a sideline and watch.
This is a big movement.
This is going to change the future of the industry worldwide.
Get involved.
Well, that's great.
That's great.
Well, Mazin, thank you so much for taking the time to chat with us this afternoon.
Thank you.
And it's my pleasure.
I really appreciate the invitation.
Let's go with the talking to you in the future.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Mazin or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 78.
To follow along with this AI Summit series, visit twimlai.com slash AI Summit.
Of course, you're encouraged to send along your feedback or questions to us by leaving
a comment right on the Shonos page or via Twitter to add Twimlai or add Sam Charrington.
Thanks again to IBM Power for their support of this series.
For more about the IBM Power Systems platform for Enterprise AI, visit twimlai.com slash
IBM Power.
Thanks once again for listening and catch you next time.
