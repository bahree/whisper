WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

00:35.920 --> 00:40.640
to present to you our first ever AI rewind series.

00:40.640 --> 00:45.040
In this series I interview friends of the show for their perspectives on the key developments

00:45.040 --> 00:49.720
of 2018 as well as a look ahead at the year to come.

00:49.720 --> 00:54.360
We'll cover a few key categories this year, namely computer vision, natural language

00:54.360 --> 00:59.200
processing, deep learning, machine learning and reinforcement learning.

00:59.200 --> 01:03.840
Of course we realize that there are many more possible categories than these, that there's

01:03.840 --> 01:08.840
a ton of overlap between these topics and that no single interview could hope to cover

01:08.840 --> 01:12.240
everything important in any of these areas.

01:12.240 --> 01:17.120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

01:17.120 --> 01:26.120
by commenting on the series page at twimbleai.com slash rewind 18.

01:26.120 --> 01:30.280
In this episode of our AI rewind series, we're bringing back one of your favorite guests

01:30.280 --> 01:36.480
of the year, Jeremy Howard, founder and researcher at fast.ai to discuss trends in deep learning

01:36.480 --> 01:38.520
in 2018 and beyond.

01:38.520 --> 01:43.440
We cover many of the papers, tools and techniques that have contributed to making deep learning

01:43.440 --> 01:47.920
more accessible than ever to so many developers and data scientists.

01:47.920 --> 01:48.920
Enjoy.

01:48.920 --> 02:00.160
Alright everyone, I am here in Montreal with Jeremy Howard and I'm speaking with Jeremy

02:00.160 --> 02:07.680
as part of a special series of shows we're doing about reflection of the machine learning

02:07.680 --> 02:13.560
in deep learning world for 2018 and some thoughts on predictions for 2019.

02:13.560 --> 02:15.160
Jeremy, welcome back to the show.

02:15.160 --> 02:19.720
Thanks Sam, nice to be here, I'm worried I'm not nearly cool enough to be able to tell

02:19.720 --> 02:24.640
you much about trends and whatnot but I'll do what I can.

02:24.640 --> 02:28.080
I am sure we're going to have a wonderful conversation.

02:28.080 --> 02:34.800
So this is the first of these that I'm doing and I expect that the format will shift

02:34.800 --> 02:42.200
a little bit as I do them but maybe we can kind of start by just getting your off the

02:42.200 --> 02:48.880
top of the head kind of reflections on 2018 before we focus on some of the things in particular

02:48.880 --> 02:52.840
around deep learning research that you found most interesting.

02:52.840 --> 02:59.480
Well, I guess it's easiest for me to talk about the year and the stuff that I care about

02:59.480 --> 03:08.360
which is increasing accessibility of deep learning for normal people to solve normal problems.

03:08.360 --> 03:16.560
So I guess our main work towards that this year has been trying to make it faster and

03:16.560 --> 03:22.520
easier and less expensive to train neural nets across more different areas.

03:22.520 --> 03:27.280
So there's a lot of people who have been doing work in that area as well which I could

03:27.280 --> 03:28.280
touch on.

03:28.280 --> 03:37.080
One is that when you look at the Stanford competition called Don Bench, it kind of kicked off

03:37.080 --> 03:41.680
increased interest in an area which had been really underappreciated before which is

03:41.680 --> 03:46.680
can you train accurate, large models quickly and cheaply.

03:46.680 --> 03:53.200
So there's a competition to measure those two things and so before the competition it

03:53.200 --> 04:00.560
took a few days generally to train image net to a reasonable accuracy, particularly if

04:00.560 --> 04:07.040
you're using kind of commodity hardware like stuff you would rent from AWS.

04:07.040 --> 04:18.320
And at the end of it we had gotten to first place on the leaderboard with 18 minutes, $40

04:18.320 --> 04:24.680
being nothing but Amazon AWS and then there's been other related work from other researchers

04:24.680 --> 04:28.680
who have got it down not on AWS commodity equipment but on more specialized equipment

04:28.680 --> 04:31.920
down to seven minutes.

04:31.920 --> 04:36.760
And so the kind of stuff that's allowed that to happen has been things like fantastic work

04:36.760 --> 04:45.000
from Leslie Smith on achieving superconvergence so using much higher learning rates, particularly

04:45.000 --> 04:51.480
a approach to learning rate scheduling called one cycle scheduling which we use as the default

04:51.480 --> 04:55.960
all the time now and makes life much faster and easier.

04:55.960 --> 05:03.520
I remember in the course and I believe our last conversation talking about cyclical learning

05:03.520 --> 05:04.520
rates is one cycle.

05:04.520 --> 05:06.840
Yeah, this is the next stage.

05:06.840 --> 05:11.360
So last year I guess Leslie did the CLI paper, cyclical learning rates.

05:11.360 --> 05:19.920
One cycle is just what it sounds like which is to spend about half of your epochs gradually

05:19.920 --> 05:24.040
increasing the learning rate, about half gradually decreasing it.

05:24.040 --> 05:27.840
But there's a few really important insights you have to combine with that.

05:27.840 --> 05:34.200
One in Leslie's paper is that as you increase the learning rate you should decrease momentum.

05:34.200 --> 05:36.160
So you have these two things happening at the same time.

05:36.160 --> 05:39.960
So if you use the first AI library that will happen for you automatically.

05:39.960 --> 05:45.360
And so it means that as you get to those really high learning rates, because the momentum

05:45.360 --> 05:50.240
is a lot lower, you're much less likely to kind of accelerate further than the model

05:50.240 --> 05:52.440
can handle.

05:52.440 --> 06:00.000
Something not quite related to Leslie's work, but stuff from Frank Hudders lab around being

06:00.000 --> 06:04.560
able to handle weight decay properly when you're using dynamic learning rates.

06:04.560 --> 06:09.840
So the momentum and Adam, so their techniques called Adam W, turns out to be super important

06:09.840 --> 06:11.720
as well for this.

06:11.720 --> 06:14.160
Was that Adam W this year?

06:14.160 --> 06:19.880
I can't quite remember, in the last 12 months or not, it may be slightly more, I don't

06:19.880 --> 06:20.880
know.

06:20.880 --> 06:24.160
But the combination I guess of all these things has been very much this year.

06:24.160 --> 06:29.280
So, and we combine those two together along with progressive resizing.

06:29.280 --> 06:34.680
So basically, I don't know if we were the first to do it.

06:34.680 --> 06:37.360
There was a lot of people that kind of did it all about the same time, but basically this

06:37.360 --> 06:43.520
idea of like, why don't you do most of your training on small images and then gradually

06:43.520 --> 06:49.960
resize, because modern convolutional neural nets are all, don't care what size your input

06:49.960 --> 06:50.960
is.

06:50.960 --> 06:53.200
And what's the advantage of doing that?

06:53.200 --> 06:54.720
Well, it's just super fast, you know.

06:54.720 --> 06:59.800
So two things, super fast, you do most of your training at 64 by 64, so you're decreasing

06:59.800 --> 07:02.120
your compute by more than 10x.

07:02.120 --> 07:06.680
But then the nice thing is you can do the last one or two epochs at a larger size than

07:06.680 --> 07:07.680
most people.

07:07.680 --> 07:10.960
So most people do 224 by 224 the whole time.

07:10.960 --> 07:13.760
So we'll go through to 288 or even bigger.

07:13.760 --> 07:19.480
So the intuition there being that you can teach the network, the basic things that it wants

07:19.480 --> 07:22.480
to learn about edges and textures and stuff.

07:22.480 --> 07:24.320
Well, you know, what does the cat look like?

07:24.320 --> 07:25.320
Yeah.

07:25.320 --> 07:30.160
And what a cat looks like at 64 by 64 is basically the same as 288 by 288.

07:30.160 --> 07:33.760
So the last couple of epochs, it's really just learning a few little tweaks around

07:33.760 --> 07:38.840
like, okay, this breed of cat has a, you know, this color of nose and this breed of cat

07:38.840 --> 07:41.560
has this slightly longer hair or whatever.

07:41.560 --> 07:44.040
So it can do most of its learning.

07:44.040 --> 07:52.480
And actually, that's been a real focus for us throughout the year is like this kind of

07:52.480 --> 07:56.680
and same with Leslie Smith, doing a lot more stuff dynamically, changing things during

07:56.680 --> 07:57.680
training.

07:57.680 --> 08:00.880
So for example, something we're going to release next week is doing the same thing for

08:00.880 --> 08:06.200
the GANs. So we've now got GANs training quickly and easily and reliably for the first

08:06.200 --> 08:07.200
time.

08:07.200 --> 08:13.080
And the trick again was basically to pre-train the critic and pre-train the generator using

08:13.080 --> 08:18.960
kind of simpler, fast approaches and small images and then at the very end, you kind of

08:18.960 --> 08:19.960
GANify it.

08:19.960 --> 08:20.960
Okay.

08:20.960 --> 08:24.800
So, you know, that's kind of been a common theme, I think.

08:24.800 --> 08:28.880
And so that all, and so a lot of that ties in then to kind of transfer learning because

08:28.880 --> 08:34.120
all these things of like gradually increasing the image size is kind of just a type of

08:34.120 --> 08:35.120
transfer learning.

08:35.120 --> 08:38.920
It's kind of transfer learning you're doing within the training learning.

08:38.920 --> 08:39.920
Yeah.

08:39.920 --> 08:43.920
So for our GANs, you know, we kind of do a similar thing of this kind of transfer learning

08:43.920 --> 08:47.800
as part of the process of training again.

08:47.800 --> 08:53.560
And so in general, transfer learning lets you generalize better, lets you train faster,

08:53.560 --> 08:57.680
generally lets you use less data.

08:57.680 --> 09:03.360
So we had a particular focus, as you know, on NLP for that.

09:03.360 --> 09:08.520
And so we show it with NLP, you can use like 100 times less data and still get state

09:08.520 --> 09:12.600
at the art results, sentiment classification.

09:12.600 --> 09:14.520
And so that's all about transfer learning.

09:14.520 --> 09:20.720
And then Alec Radford, at OpenAI, you know, built on top of that, replacing our LSTM with

09:20.720 --> 09:29.920
a transformer, and then Google built on top of that, making some tweaks, but mainly just

09:29.920 --> 09:35.520
doing it for longer with more data and, you know, we're now at a point where we know,

09:35.520 --> 09:39.320
I guess we know kind of every time people try and do transfer learning anywhere.

09:39.320 --> 09:40.320
Yeah.

09:40.320 --> 09:46.680
It tends to either let you get way better results on small datasets than people thought

09:46.680 --> 09:51.680
were possible or if you use big datasets like Google did with, but you know, it's kind

09:51.680 --> 09:55.400
of smashed the state at the art of what people thought was possible.

09:55.400 --> 10:01.840
So still really underappreciated area, frankly, still most people don't know how to do it

10:01.840 --> 10:02.840
properly.

10:02.840 --> 10:08.000
Had an interesting conversation here at NURPS, someone approached me and was, we were just

10:08.000 --> 10:12.480
kind of exchanging thoughts on what was interesting at the conference and they said they didn't

10:12.480 --> 10:16.160
see a lot of transfer learning like did that go away.

10:16.160 --> 10:18.320
And I haven't seen any.

10:18.320 --> 10:24.640
I both haven't seen it, but it's also, it's kind of, you know, being baked into a lot

10:24.640 --> 10:25.640
of things.

10:25.640 --> 10:26.640
It's kind of different.

10:26.640 --> 10:32.680
But also like, NURPS is very tainted by its history and culture.

10:32.680 --> 10:39.480
So the papers here, like, very tend to over-represent either things which are very mathematically

10:39.480 --> 10:43.120
intensive or also things that follow certain trends.

10:43.120 --> 10:48.400
So NURPS every second paper is either adversarial, blah or reinforcement learning, blah, you

10:48.400 --> 10:49.400
know.

10:49.400 --> 10:55.160
So like, yeah, it's not necessarily a conference you expect to see the most practically

10:55.160 --> 11:00.440
impactful stuff, unless some of the workshops are a little bit different.

11:00.440 --> 11:05.280
But I think that's been another feature of 2018 is like, people are putting adversarial

11:05.280 --> 11:10.880
into everything and putting RL into everything and not generally for good reasons, especially

11:10.880 --> 11:11.880
adversarial.

11:11.880 --> 11:16.120
This is huge literature now around avoiding adversarial attacks and I've yet to find

11:16.120 --> 11:21.080
anybody who's, and I've asked many researchers in the field, you know, directly, can you

11:21.080 --> 11:27.640
show me a actual practical example of where you would need to use this thing, you know,

11:27.640 --> 11:28.640
whatever they built?

11:28.640 --> 11:29.640
Right.

11:29.640 --> 11:30.640
No one's managed to get.

11:30.640 --> 11:37.040
What about on the generative side of you as bearish on the generative, on the GANS?

11:37.040 --> 11:44.080
So yeah, so I spent two weeks figuring out how to train GANS properly and finally, we

11:44.080 --> 11:51.280
now have something in first AI where you can train them in an hour on a single GPU reliably.

11:51.280 --> 11:54.520
And I'm very proud that we got to that point and we have this really flexible API that

11:54.520 --> 11:59.160
allows researchers to plug in things in ways they couldn't before, but at the same time

11:59.160 --> 12:05.080
I was also researching how to avoid GANS and I've figured out the generative stuff who

12:05.080 --> 12:10.640
actually get GAN level performance without using GANS, so I'm not sure.

12:10.640 --> 12:13.640
Is this stuff that you've written about or published somewhere?

12:13.640 --> 12:14.640
It's coming up.

12:14.640 --> 12:15.640
Okay.

12:15.640 --> 12:21.920
So we're particularly doing some work in microscopy, in collaboration with the SOC institute

12:21.920 --> 12:22.920
at the moment.

12:22.920 --> 12:23.920
Okay.

12:23.920 --> 12:26.160
Through analytic or through the fast idea.

12:26.160 --> 12:28.640
No, no, I haven't had anything to do with endotic for years.

12:28.640 --> 12:29.640
Okay.

12:29.640 --> 12:35.040
This is through a few things, but it's particularly through a new medical and life science

12:35.040 --> 12:41.200
research lab that I've just helped start, and I'm now the chair of, called RAMRI, with

12:41.200 --> 12:49.000
low AI and medical research institute, and it's at USF University of San Francisco.

12:49.000 --> 12:57.320
And we basically invite medical researchers and life science folks to partner with us

12:57.320 --> 13:01.920
and we'll help with the deep learning stuff and they'll help with the domain specific

13:01.920 --> 13:02.920
stuff.

13:02.920 --> 13:06.960
Yeah, so through that collaboration, one of the things we've been working on is helping

13:06.960 --> 13:13.240
the SOC institute to get better results from their microscopy because they're world leaders

13:13.240 --> 13:18.640
in this area and it turns out that if you can get really high resolution microscopy,

13:18.640 --> 13:25.600
then you can literally learn they have been publishing papers showing breakthrough understanding

13:25.600 --> 13:32.080
of how proteins fold and how they actually impact cells and stuff like that.

13:32.080 --> 13:38.600
So yeah, so initially we were kind of looking at GANs and getting pretty good results and

13:38.600 --> 13:43.600
particularly one of our previous students, Jason Antich, has created this awesome thing

13:43.600 --> 13:51.760
called de-oldify, which takes old black and white low resolution photos and turns them

13:51.760 --> 13:54.360
into beautiful color pictures.

13:54.360 --> 13:58.960
And so he's been helping us with some of this work of like getting GANs to work reliably

13:58.960 --> 14:03.840
because he does better practical work with GANs than anybody else that's seen.

14:03.840 --> 14:07.200
And how would GANs plan to this SOC institute use case?

14:07.200 --> 14:13.360
Well, you've got these microscopy results, you want to get the highest resolution outputs

14:13.360 --> 14:18.960
you can from whatever input comes out of your microscope.

14:18.960 --> 14:27.480
And so there's actually been some very high impact work in microscopy recently on using

14:27.480 --> 14:34.880
super res for that purpose, but it turns out not surprisingly that that research was not

14:34.880 --> 14:43.440
at all using kind of modern, deep learning methods, so it's clear we can do a lot better.

14:43.440 --> 14:47.000
But then, you know, as I say, then it turns out we've kind of realized that there are certain

14:47.000 --> 14:50.600
loss functions we can use which avoid the need for GANs entirely.

14:50.600 --> 14:56.000
So I'm now wondering if I wasted all that time.

14:56.000 --> 15:05.400
But I'm very interested in generative models, you know, and to see the stuff that's happening

15:05.400 --> 15:11.720
in the world of life sciences, you know, through better using whatever signal they can get

15:11.720 --> 15:17.440
from their microscopes, it's really exciting, you know.

15:17.440 --> 15:22.720
And you really need domain specialists and deep learning specialists working very closely

15:22.720 --> 15:27.440
together because like there's all these cool things that they can do that I wouldn't

15:27.440 --> 15:29.280
have otherwise known about in vice versa.

15:29.280 --> 15:34.640
So for example, they can like, while they're kind of taking the picture, they can like

15:34.640 --> 15:38.440
change the wavelength of light they're using, they can change the focal length they're using,

15:38.440 --> 15:41.880
they can change the angle that they're using kind of then they can kind of end up with

15:41.880 --> 15:47.080
this like long exposure almost like a video, and so we can then get this whole extra dimension

15:47.080 --> 15:53.600
and this is kind of sub-pixel resolution embedded in that.

15:53.600 --> 15:57.800
I mean this is what, this is exactly what we hope for, honestly, when we start a trust

15:57.800 --> 16:04.360
AI is that kind of domain experts would be able to use deep learning to do a better job

16:04.360 --> 16:08.840
of whatever it is they're doing, so it's nice to see that really happening.

16:08.840 --> 16:16.720
Have you seen any applications of GANs outside of the image domain, I kind of wonder

16:16.720 --> 16:21.280
conceptually you should be able to apply this to text and maybe do some of the things

16:21.280 --> 16:24.480
that people are using RNNs and LSTMs to do?

16:24.480 --> 16:28.600
Yeah, well like I say I feel like people are putting adversarial in everything and I'm

16:28.600 --> 16:35.200
not convinced it's helpful, I think often it's a bit of a lazy shortcut to like actually

16:35.200 --> 16:38.280
thinking about what your loss function should be.

16:38.280 --> 16:44.360
Yeah, I mean you mentioned text, it's a good question, there's, I think the more general

16:44.360 --> 16:52.320
question in text is what kind of like augmentation and stuff can we do in text to be able to use

16:52.320 --> 16:59.760
less data and get better results and I've kind of seen some sign of trying to use adversarial

16:59.760 --> 17:07.560
approaches there which I don't think is necessary, like there was a recent paper in the last

17:07.560 --> 17:14.760
few weeks which was basically, I don't know if you're familiar with the cutout paper,

17:14.760 --> 17:19.040
so you've got dropout which I think everybody knows which is like removing activations

17:19.040 --> 17:26.560
around them and then the cutout paper in vision specifically removed a whole contiguous

17:26.560 --> 17:33.000
sections of an image so kind of cut squares out of it and use that as regularization and

17:33.000 --> 17:35.840
then there's been a more recent paper in the last couple of weeks which is kind of basically

17:35.840 --> 17:41.320
done cut out at every layer of the neural net so it's basically dropout but instead of

17:41.320 --> 17:46.600
removing activations at random, you remove activations at the next to each other, stuff

17:46.600 --> 17:56.760
like that almost certainly will work equally well in text and don't require any of this

17:56.760 --> 18:00.320
kind of adversarial stuff.

18:00.320 --> 18:07.080
So yeah so I think my view is probably cans are a little overhyped and adversarial attacks

18:07.080 --> 18:12.320
are a little overhyped and it's certainly a great way to get published in Europe and you

18:12.320 --> 18:16.560
feel the same way about RL, have you also experimented with RL?

18:16.560 --> 18:26.040
Well, kind of I spent 10 years kind of studying optimization more generally and I never felt

18:26.040 --> 18:33.640
like these standard differentiation based approaches when you've got this kind of long-term

18:33.640 --> 18:41.120
credit assignment issue necessarily make a lot of sense and I still feel like there's

18:41.120 --> 18:46.560
a lot of the optimization literature that's being ignored by the RL community.

18:46.560 --> 18:51.720
You've seen a little bit of like evolutionary algorithms get touched on here and there

18:51.720 --> 18:56.960
like I think everybody's in work there but when I think back to like all the stuff that

18:56.960 --> 19:03.360
was going on in the early 90s, people just started to rediscover some of it.

19:03.360 --> 19:10.920
So for example, in the early 90s combining evolutionary algorithms together with kind

19:10.920 --> 19:15.960
of gradient-based methods was really common and I just saw a paper literally reinventing

19:15.960 --> 19:18.280
it like two weeks ago.

19:18.280 --> 19:22.200
The problem RL is trying to solve is great which is like hey let's not just try to predict

19:22.200 --> 19:27.840
things but let's actually try to figure out what action to take but I feel like currently

19:27.840 --> 19:37.280
the RL community is not quite treating it as enough as differently enough as it should

19:37.280 --> 19:38.280
be.

19:38.280 --> 19:46.360
When I spent a long time on optimization it became clear over time that it's a good idea

19:46.360 --> 19:51.920
to kind of recognize the differences between prediction tasks and more general optimization

19:51.920 --> 19:55.120
tasks.

19:55.120 --> 20:00.040
This is interesting because I had a chance to chat with Sergei Levine last night and

20:00.040 --> 20:08.040
we were talking about generally what he's found interesting over the past year in RL in

20:08.040 --> 20:09.040
particular.

20:09.040 --> 20:12.040
This was informal.

20:12.040 --> 20:17.400
But one of the things that he mentioned was a paper, TD3 paper, Twin Delay, Deterministic

20:17.400 --> 20:24.080
Policy Gradient which sounded like just the kind of hack that you would love.

20:24.080 --> 20:29.720
Like it's a tweak to the way they do the policy that he didn't even go into the details

20:29.720 --> 20:35.520
because it was such a knit but it gave him two X better training times.

20:35.520 --> 20:41.360
So if fast AI was doing RL it would be just the kind of thing that you bake into the library.

20:41.360 --> 20:48.120
Yeah, but I mean we only work on stuff which clearly works in practice for the kinds of

20:48.120 --> 20:54.960
problems most people have and Levine's one of the very few people who's using RL in

20:54.960 --> 20:57.920
very appropriate and useful ways at the moment.

20:57.920 --> 21:03.480
So for stuff involving robotics there's a lot more you can do with RL because any time

21:03.480 --> 21:09.080
you've got some physical system that you can actually model with physics pretty accurately

21:09.080 --> 21:14.800
you can kind of pull it apart and add appropriate constraints and an appropriate kind of

21:14.800 --> 21:18.680
auxiliary losses and there's a lot more that you can do.

21:18.680 --> 21:25.920
So I think the stuff he's doing is interesting and useful but a lot of people are using

21:25.920 --> 21:27.880
RL for stuff I don't know.

21:27.880 --> 21:33.640
I just went to the start of the health workshop here in Europe and there's all these people

21:33.640 --> 21:38.880
tackling various medical problems using RL which just seemed like slightly ridiculous

21:38.880 --> 21:39.880
in my opinion.

21:39.880 --> 21:43.840
Interesting, interesting.

21:43.840 --> 21:50.760
So there's been, we were chatting before we got started and you mentioned some of the

21:50.760 --> 21:57.960
work that's been happening to try to better understand the way some of our tweaks we've

21:57.960 --> 22:01.120
been doing like batch norm and other things are working.

22:01.120 --> 22:03.120
Can you elaborate on some of that work?

22:03.120 --> 22:04.120
Oh yeah, I'd love to.

22:04.120 --> 22:09.400
I feel like that's something where there's been some great progress this year and particularly

22:09.400 --> 22:11.880
in the last couple of months.

22:11.880 --> 22:18.200
There's a fantastic poster here at Europe that's been an archive for a while called Visualizing

22:18.200 --> 22:25.800
the Lost Landscape of Neural Nets and what they basically showed is it's something I've

22:25.800 --> 22:31.360
been talking about for ages but never actually did anything about it which is this idea that

22:31.360 --> 22:38.480
you can have this idea of sharp parts of the lost surface where if you're in a sharp

22:38.480 --> 22:42.880
part of the lost surface then the idea is it probably won't generalize very well because

22:42.880 --> 22:48.880
if you change anything a little bit now you're not in that kind of nice low area anymore

22:48.880 --> 22:59.440
and it's kind of overreaction to what I thought was a kind of obvious and kind of mathematical

22:59.440 --> 23:05.520
issue which is how you can like reparameterize the weights to make anything arbitrarily sharp

23:05.520 --> 23:10.640
which to me like if any time you read can reparameterize something why don't you just normalize

23:10.640 --> 23:12.560
out that reparameterization.

23:12.560 --> 23:17.920
So this Visualizing the Lost Landscape actually did that so they actually did the normalization

23:17.920 --> 23:21.600
and then they did this you know they built some beautiful software and some beautiful visualizations

23:21.600 --> 23:27.920
to show what happens and they found some really interesting things.

23:27.920 --> 23:32.080
One of the most interesting was they found that when you look at the actual trajectory

23:32.080 --> 23:40.880
as you train your own app for example if you take the PCA space of the weights there's basically

23:40.880 --> 23:47.920
only two dimensions you can basically that plot the entire or they said 40 to 90% of the variation

23:47.920 --> 23:55.600
in the direction of the gradient updates lies in just two PCA directions independent of the

23:55.600 --> 24:00.800
dimensionality of your well this was on image net so I mean this is obviously super high-dimensional

24:00.800 --> 24:08.800
so yeah over a hundred million weights and so you know that and you know one of the nice things

24:08.800 --> 24:13.200
that means is you can kind of plot exactly what pretty close to exactly what's going on which

24:13.200 --> 24:19.680
they did and you can also plot the Lance Lost Landscape that's being navigated and so one thing

24:19.680 --> 24:26.400
that they found was if you as soon as you add skip connections so Resnets versus the exact same net

24:26.400 --> 24:34.160
without the identity shortcuts Resnets basically make the whole surface incredibly smooth

24:34.880 --> 24:39.680
and dense nets make it even smoother by the way so when you see their pictures it just

24:39.680 --> 24:45.840
immediately makes you realize like okay you know this is why we've been loving Resnets so much

24:45.840 --> 24:55.040
and so you can kind of see similar stuff with normalization so one of the really interesting papers

24:55.040 --> 24:59.520
to come out in the last couple of months so there's been two papers coming out very similar times

24:59.520 --> 25:06.960
which both both both basically said hey you know how batch norm was meant to help with covariate

25:06.960 --> 25:11.840
shift where it turns out it doesn't help with covariate shift and it's got nothing to do with

25:11.840 --> 25:19.200
covariate shift and actually what it does is it makes the loss surface smoother which actually

25:19.200 --> 25:25.200
if you think about it makes perfect sense you know if your if your current activation is scaled

25:25.200 --> 25:30.960
really poorly then if you don't have batch norm the only way to fix that is to modify all of your

25:30.960 --> 25:36.480
weights on that layer where else if you do have batch norm you only have to modify the batch norm

25:36.480 --> 25:41.680
weights to fix the scaling so just makes the loss surface a lot smoother so then there's been other

25:42.480 --> 25:46.800
very nice kind of follow-ups to that well exactly follow-ups coming out at similar times to that

25:46.800 --> 25:52.080
saying hey here are some different ways of doing normalization which don't focus on the covariate

25:52.080 --> 26:00.080
shift but focus on the scaling so spectral norm and weight norm in particular which are now both

26:00.080 --> 26:05.120
built into pie torch and when you create a conflare in fast AI you can literally

26:05.120 --> 26:12.960
pick from an enum of what norm type you want and yeah they they they help a lot actually with

26:12.960 --> 26:19.520
against stabilizing training so yeah all these all these kind of insights into what's going on

26:19.520 --> 26:24.640
and so the neural network is helping lead to better ways to train the neural networks which means

26:24.640 --> 26:33.120
less hyper parameters less you know more resilient higher learning rates it's all making life

26:33.120 --> 26:40.080
easier in practice which is great so last year the big controversy at least one of them was

26:40.080 --> 26:46.160
Ali Rahimi's kind of call for greater rigor do you feel like a lot of this work is a response to

26:46.160 --> 26:51.600
that or no I never quite figured out what Ali meant and even though I had quite a few private

26:51.600 --> 27:01.280
chats with him I never quite figured out what he meant I tried to dig into it I think yeah so

27:01.280 --> 27:07.120
so I you know since I don't quite know what he was saying and haven't figured it out I can't quite

27:07.840 --> 27:11.920
I mean he's a great speaker and so a lot of people would like I really speak to that talk

27:11.920 --> 27:16.480
when I ask them like what exactly did he mean by rigor and exactly what are you going to do about it

27:16.480 --> 27:24.880
no one had a good answer to that question so for me you know as a kind of an experimentalist I think

27:24.880 --> 27:31.600
rigor is about ablation studies you know so if you look at the your LEMFIT paper that Sebastian and I

27:31.600 --> 27:37.360
did we spend a lot of time doing ablation studies so we said like what if you did this with more

27:37.360 --> 27:43.200
data versus less data what if you removed this training thing from this training thing from this

27:43.200 --> 27:47.040
training thing what if you tried this versus that data set and so we just had lots of tables and

27:47.040 --> 27:52.960
pictures saying here's you know here's the thing showing you which bits help and how much they help

27:52.960 --> 28:00.640
and what they help with so to me that's you know when when people don't have that I don't find

28:00.640 --> 28:05.120
their papers terribly useful because I don't know what's what's actually helping and often the

28:05.120 --> 28:11.360
things they thought were helping weren't helping even worse for me is when people don't use a

28:11.360 --> 28:16.160
strong baseline so they'll have some really crappy model and they'll say like oh look at our technique

28:16.160 --> 28:23.840
X improves that crappy model but when you look at it it's you know their technique X is just a

28:23.840 --> 28:30.320
crappy way of doing what the normal baselines would have done anyway so I think like to me in terms

28:30.320 --> 28:37.280
of experimental work strong baselines and good ablation studies is is what it's about but you know

28:37.280 --> 28:42.880
for the New York's crowd rigor often means great letters you know they want to see like

28:42.880 --> 28:51.360
you know convexity proofs and error bound proofs and all this stuff which I've just

28:52.320 --> 28:59.280
never seen useful like the only thing I've seen those kind of proofs do is to totally mislead people

28:59.280 --> 29:06.080
so like we had what I call the SDS pretty strong it's true we that what I call the SVM winter for

29:06.080 --> 29:13.920
like 15 20 years which is basically you know that Nick did this really to some people compelling

29:13.920 --> 29:20.000
papers it's just kind of like hey all you need is SVMs here's the mathematical proof and

29:20.800 --> 29:26.080
it you know when you actually look at it it's it's it's it's the the things that people took out

29:26.080 --> 29:34.400
of that are rubbish like it's the difference between like in theory here's what ought to work

29:34.400 --> 29:40.080
versus in practice here's what actually it works so it's it's it's nice this year

29:41.280 --> 29:48.160
leonbertos I can't remember the co-author but leonbertos work won a test of time award his 22 2007

29:48.160 --> 29:54.400
paper which basically I mean it's it's it's ridiculous that it was necessary for this paper to

29:54.400 --> 30:00.000
even happen but he basically said look or you people who were spending all this time on like

30:00.000 --> 30:09.920
fancy optimizers for SVMs actually SGD works better and and and like and then here's all the

30:09.920 --> 30:14.560
Greek symbols you need you know like here's all the proof and math and whatever else so like

30:14.560 --> 30:18.720
there's already plenty of experimental evidence to say like it works better which

30:18.720 --> 30:24.560
most of the community ignored and so it really took somebody to come along and like say it in

30:24.560 --> 30:29.920
their own words and to like prove it so to me I'm kind of like I don't like similar thing with

30:29.920 --> 30:34.240
like going back further minskie right minskie like proving that neural nets are a waste of time

30:34.240 --> 30:38.560
because they can't like solve the x-all problem and so the thing is all these mathematical proofs

30:38.560 --> 30:43.600
they're always of oversimplified versions of the problems we're actually trying to solve because

30:43.600 --> 30:48.320
the problems we're actually trying to solve are not amenable to that kind of analytic approach

30:48.320 --> 30:55.120
so you know took 20 years really for somebody to come along and say well we're not just using

30:55.120 --> 31:00.000
one layer you know we actually have a hidden layer if you have a hidden layer then we can solve

31:00.000 --> 31:09.520
any arbitrary problem to arbitrarily close given enough parameters so again like these kind of AI

31:09.520 --> 31:17.760
winters we had were really kicked off by people taking theoretical results far further than they

31:17.760 --> 31:24.160
should have ever been taken and ignoring all of the empirical evidence of like I don't know like

31:24.160 --> 31:28.480
guys like Jan LeCouin who's like saying like hey I've actually written this thing on that five

31:28.480 --> 31:35.680
that actually reads numbers you know it works it's this thing called a convolutional neural network

31:35.680 --> 31:42.960
and you know people like that just weren't getting published because you know because the theory

31:42.960 --> 31:50.000
had already proven that we should be using these other approaches so yeah I think that's um

31:50.720 --> 31:57.680
that's a concern I have about our field in general it also is a real problem for diversity

31:57.680 --> 32:06.160
and inclusion because there's lots more people in the world who know how to code or you don't know

32:06.160 --> 32:12.160
how to do some engineering but won't know how to prove error bounds on something or whatever

32:12.160 --> 32:17.200
and also like we'll be much more focused on like hey I want to actually solve this problem in

32:17.200 --> 32:25.120
in medicine or in you know disaster resilience or whatever and so they'll want to be

32:25.120 --> 32:31.840
publishing papers saying his thing A that works for thing B and the current focus in our field is

32:31.840 --> 32:43.600
yeah very unfriendly to that kind of stuff interesting yeah yeah um I'm curious having launched the new

32:45.040 --> 32:53.040
fast.ai course this year the deep learning course any you know specific 2018 learnings

32:53.040 --> 33:03.520
about deep learning education or what's required you know what's you know what's needed to kind

33:03.520 --> 33:09.200
of broaden the fold of folks that can do deep learning. I think I've been surprised by how far

33:09.200 --> 33:14.640
people have gone on the back of the fast.ai course like a lot of presenters at nureps have come up

33:14.640 --> 33:20.800
to me and said they got into deep learning through fast.ai and if you would ask me three years ago

33:20.800 --> 33:26.000
if our students are going to go on to be researchers presenting at nureps I would have been like

33:26.000 --> 33:31.040
that sounds like me I feel like we're just trying to get people to be reasonable practitioners so I

33:31.040 --> 33:39.040
think I think that's been a pleasant surprise um it's also been I must have been a bit surprised

33:39.040 --> 33:46.480
at how quickly the software has taken off um like for me that's kind of my focus now is like I would

33:46.480 --> 33:52.880
love for software to be as standalone as possible and not really require people to do the course

33:52.880 --> 33:56.960
because there's a lot of investment in time to do the course and yeah I'm increasingly running

33:56.960 --> 34:02.880
into particularly researchers now who have come from backgrounds in TensorFlow or pure pie

34:02.880 --> 34:08.560
torch or China or whatever and kind of saying like oh I started using the software and

34:09.440 --> 34:15.600
my research is going faster than it was before so that's been nice um but I think overall

34:15.600 --> 34:20.240
you know it's we've kept on finding the same things we've found in previous years it's just

34:20.240 --> 34:26.160
accelerating like I went to the black and ii dinner last night and the black and ii workshop which

34:26.160 --> 34:34.080
I saw you there as well um and uh yeah I mean it's just so great how many people came up to me and

34:34.080 --> 34:40.080
said like you know hey I'm from the ivory coast or I'm from Tanzania or whatever and

34:40.080 --> 34:46.640
I had no way to learn deep learning until your course came along and now I've done it and now

34:46.640 --> 34:52.960
I'm here at nureps presenting at this workshop and uh and they're always like kind of inspiring

34:52.960 --> 34:58.160
stories because often that you know they're telling me last night about the steps they had to go

34:58.160 --> 35:04.240
through to get a visa often having to go to other countries and contacting the console general or

35:04.240 --> 35:09.520
you know like as they're they're forging paths no one's ever forged before and now they're

35:09.520 --> 35:15.760
finding communities so like um it's so cool the way like three years ago and we did our first

35:15.760 --> 35:23.360
course we had one student from legos and he was like asking on the forum like hey anybody else

35:23.360 --> 35:29.520
from legos here anybody else from africa here you know tonight you're like can we have a community

35:30.320 --> 35:37.040
no nothing you know and and now uh legos is a second biggest faster your community

35:37.040 --> 35:44.880
long outside of the u.s so a bungalow is still the biggest bangalow that legos is huge so like

35:44.880 --> 35:50.960
it's been really cool to see how just a little bit of um and I know it's certainly not all

35:50.960 --> 35:55.680
thanks to fast ai there's lots of people doing great work here but I know like plenty of the people

35:55.680 --> 36:02.000
involved have got there thanks to fast ai so for example one of the um people presenting at the

36:02.000 --> 36:10.240
black and ai dinner last night uh judy jichoya she's a um a radiologist who got into deep learning

36:10.240 --> 36:18.320
through fast ai and now she's like a incredibly kick ass leader both in the radiology world and

36:18.320 --> 36:24.480
in the deep learning world in the black and ai world so um yeah I feel like there's now enough

36:24.480 --> 36:32.320
momentum going on that these uh underrepresented groups are not going to be underrepresented

36:32.320 --> 36:41.680
too much longer mm-hmm mm-hmm yeah I thought the I forget his full name but one of the presentations

36:41.680 --> 36:51.440
was kareem karen those an interesting uh slide where one of the speakers uh from tenizha

36:51.440 --> 36:59.280
kind of asserted that ai is both this incredible both uh existential threat for africa in some ways

36:59.280 --> 37:05.920
and and an awesome opportunity and kind of uh you know for for him it was a rallying cry to

37:05.920 --> 37:10.800
you know get more people engaged in the process i thought those were really interesting yeah

37:10.800 --> 37:15.680
I mean that's kind of why Rachel and I started this or it's exactly why Rachel and I started

37:15.680 --> 37:22.640
this is we both thought like this has the potential to massively increase inequality that's that

37:22.640 --> 37:28.880
is exactly what will happen if there's just a status quo right because all the people you know

37:28.880 --> 37:34.240
before we started fast ai all the people pretty much studying and working in deep learning were

37:35.760 --> 37:44.560
you know western white men who very very few of whom had any kind of domain expertise background

37:44.560 --> 37:51.440
so they were using deep learning to solve kind of bullshit problems um so we thought okay if

37:51.440 --> 37:56.480
nothing changes that's going to get worse and worse because those people keep getting money thrown

37:56.480 --> 38:03.280
at them and and they keep hiring more of the same people and investing education in the same

38:03.280 --> 38:12.720
places and but at the same time you know um it's deep learning is not at heart and it can be used

38:12.720 --> 38:20.400
to help so many areas with so many problems so if we could get kind of get this the the skills out

38:20.400 --> 38:28.720
there um then maybe uh people who otherwise would not be able to will be able to like make a big

38:28.720 --> 38:35.840
impact so i mean jute is a good example i think she's from Kenya uh and uh you know uh i'm

38:35.840 --> 38:41.200
sure she would have been a great radiologist regardless right but i feel like now you know we have

38:41.200 --> 38:49.680
somebody in the kind of senior thought leader community amongst radiologists who is black and who

38:49.680 --> 38:58.240
is a woman and who is you know both bringing ai to radiology and radiology to ai you know it's

38:58.240 --> 39:05.440
exactly the kind of perspective which yeah hope you know i think can create opportunities but

39:05.440 --> 39:12.640
they didn't exist before um let's talk a little bit about the the tools landscape obviously one

39:12.640 --> 39:20.160
of the big developments in 2018 was the launch of the 1.0 versions of both the fast that ai library

39:20.160 --> 39:30.960
and and uh pie torch is it too early to to talk about kind of momentum from the pie torch launch

39:30.960 --> 39:35.520
or it's not too early because one of the really interesting things is paying to see TensorFlow's

39:35.520 --> 39:41.360
reaction which is they've really got off their ass and doing good stuff you know they made some

39:42.000 --> 39:49.600
tough choices like getting rid of tf.contrib making tfo a much more community given exercise i think

39:49.600 --> 39:56.640
they realized that you know the the direction you know you talk to almost any Google TensorFlow

39:56.640 --> 40:01.600
engineer off the record at least and they'll tell you they all hate that code based you know it's

40:01.600 --> 40:08.880
full of technical debt and it's just not well put together um and it's not focused on the kind

40:08.880 --> 40:17.520
of the developer experience um it's really focused on using as many tp users possible so so tf2 is

40:17.520 --> 40:26.560
looking like a huge step forward in terms of the developer experience and um you know uh just

40:26.560 --> 40:32.160
kind of a piece of software that people will enjoy using breath of the news because they have to

40:32.800 --> 40:38.240
so and i really think that's been i mean i know it's been very heavily a reaction to pie torch

40:38.240 --> 40:44.400
which is not to say pie torch invented this approach uh it's i mean China certainly did it before

40:44.400 --> 40:52.560
pie torch and i don't quite know the history before that but um i think yeah realizing that

40:52.560 --> 40:58.320
the the tools need to be written for developers uh is just a really important insight so i think

40:58.320 --> 41:04.240
that's been so i think the impact on the on the TensorFlow gorilla has been important and interesting

41:04.960 --> 41:11.200
but then um yeah people just using pie torch uh particularly in the research community

41:11.200 --> 41:17.200
you're definitely seeing things you know more innovation as a result you're seeing faster innovation

41:17.200 --> 41:24.640
and then yeah you know the impact of fast ai is probably a little early to really know but um

41:25.520 --> 41:30.480
we're seeing that for a whole nother level for the you know quite a few researchers who are picked up

41:30.480 --> 41:39.760
fast ai um and you know fast ai kind of has two different user groups in mind there's the

41:40.320 --> 41:45.040
you know very much the research user group so that's all about the kind of

41:45.040 --> 41:51.120
lower level of abstractions where so for example with our gans you know we've made it so you can

41:51.760 --> 41:58.000
easily do research around like if you want some dynamic approach to switching between training

41:58.000 --> 42:04.320
the critic and the generator you know you you can just plug in your own gans which are class or if

42:04.320 --> 42:08.800
you want to pre-train a different kind of critic you can plug in a different pre-train critic class

42:08.800 --> 42:16.560
whatever and then of course there's the user group of people who just want to um get something

42:16.560 --> 42:20.960
working at their company so for example one person who came up to me at europe's last night said

42:20.960 --> 42:27.440
oh my company has three million documents um it's a pretty big international company and we've

42:27.440 --> 42:34.880
been using faster ai and urlm fit to basically tag all three million documents and they have a

42:34.880 --> 42:40.800
full-time taxonomist who sits there and classifies documents and then that gets fed off to urlm fit

42:40.800 --> 42:45.840
through fast ai and then they put the results and find you in the model and um so they're not

42:46.560 --> 42:52.640
doing research level customization or whatever they're just getting stuff done so I think yeah I

42:52.640 --> 42:59.120
think both of those groups so uh you know I think previously they would have been using

42:59.680 --> 43:03.760
keras if you wanted to do something like that but keras just doesn't really give you easy

43:03.760 --> 43:10.080
supportive things like nlp and things like that so um I think that yeah maybe that's one of the main

43:10.560 --> 43:18.640
impacts of fast ai in that area is easy deep learning isn't just in computer vision anymore

43:18.640 --> 43:23.120
it's also nlp and tabular and collaborative filtering

43:24.320 --> 43:30.000
any other interesting things on the tool side that you've that have caught your eye

43:30.000 --> 43:36.640
um yeah there's plenty of stuff going on I wish I had more time to dig into them than I did

43:36.640 --> 43:42.880
I mean you know in the pie-torch community well I mean obviously one big thing in pie-torch

43:42.880 --> 43:50.240
version one release which just came out yesterday is the just in time compiler so you can add a

43:50.240 --> 43:57.440
jet decorated to your code and get you know fused you know fast inference version of it so

43:57.440 --> 44:04.960
um it's quite a bit of that stuff going on there's another much less known library for pie-torch

44:04.960 --> 44:10.960
which is the galsion processes library and um so me it's not so much interesting because of the

44:10.960 --> 44:15.120
galsion processes but be interesting because they have to encode a lazy tensor which kind of

44:15.120 --> 44:22.480
takes the jet to a whole nother level which is you can as you basically say these are all the

44:22.480 --> 44:28.480
mathematical operations I want to do on my tensor it it's just storing the computation graph it's

44:28.480 --> 44:33.600
not doing the actual calculation at all until later on when you just say you know compute and then

44:33.600 --> 44:41.120
it compiles a kind of a fused version and then much faster handles kind of stuff like sparsity

44:41.120 --> 44:48.160
and stuff much better and they also have some kind of nice linear algebra identities which they

44:48.160 --> 44:55.360
use to you know just use arithmeticly much better choices when they know that you know you had

44:56.000 --> 44:59.840
these different shape matrices and you did these particular operations to them and you know

44:59.840 --> 45:04.160
this linear algebra identities so therefore we can replace that set of operations with this

45:04.160 --> 45:10.400
single faster one um at least with the lazy tensor is this an example of kind of pie-torch moving

45:10.400 --> 45:17.440
towards tensor flow while tensor flow is moving towards pie-torch I think so um I think so yeah exactly

45:17.440 --> 45:24.160
so the kind of the the the jet in pie-torch is starting to feel a bit like XLA I guess in tensor

45:24.160 --> 45:29.920
flow but with less technical depth yeah and they're kind of a lazy tensor stuff is starting to

45:29.920 --> 45:37.840
look a bit like the um uh kind of static graph approach but without all the horrible boilerplate

45:37.840 --> 45:45.760
the tensor flow so um I mean what we're I'm really excited though is what TNG chain is doing with

45:45.760 --> 45:51.120
TVM like hopefully that stuff with TVM where there you'll have right on that what's TVM

45:51.120 --> 45:58.640
our TVM is is something that's basically at a lot of level which is that you um take a it's

45:58.640 --> 46:06.400
basically a compiler for tensor expressions that will create an an optimized version of your

46:06.400 --> 46:13.760
tensor expression and um because at the moment one of the things I hate I as I hate it when I say

46:13.760 --> 46:18.720
to a student like okay let's dig into this code which calls this which calls this which calls this

46:18.720 --> 46:21.520
and let's understand all the stuff that's going on and they get to a point where it's like oh and

46:21.520 --> 46:28.080
this calls n videos kudian and library so at that point okay after that it's magic um so one of

46:28.080 --> 46:36.160
the cool things TVM does is it um kind of lets you see you know um it doesn't suddenly stop at

46:36.160 --> 46:43.040
kudian and it's now here's the TVM code you know okay and then um TVM ends up believe it or not

46:43.040 --> 46:50.240
faster than kudian and even although TVM is automatically creating that optimized kuda kernel

46:51.360 --> 46:56.880
so is it replacement or is it compiling down to kuda or something i mean it still has to

46:56.880 --> 47:03.840
compile down to what i'm not necessarily kuda but maybe ptx um so it's still you know so it's not

47:03.840 --> 47:10.320
business not just for Nvidia it can also target you know um or you know um various mobile devices

47:10.320 --> 47:19.200
or cpu um so i'm kind of like excited about this because i think it might mean that um particularly

47:19.200 --> 47:24.640
for stuff like swift for TensorFlow which is a project i'm really excited about you know

47:25.840 --> 47:32.640
you know hopefully we can see things where we can write everything we're doing in our kind of

47:32.640 --> 47:38.000
host language and um i was gonna ask are you excited about that because you're excited about

47:38.000 --> 47:43.920
Swift the language or because you want to see deep learning accessible via all the languages

47:44.480 --> 47:51.520
um there's a number of reasons i'm excited about it one is that i hate Python so great to you know

47:52.800 --> 47:58.800
get rid of it um it's just it's just incredibly frustrating to have to write in a language which

47:58.800 --> 48:06.640
has so many klachi things like with global interpreter lock and just so incredibly slow every

48:06.640 --> 48:13.840
time you actually do something in python and um so you know it's it's very very frustrating to

48:13.840 --> 48:19.440
to to work with it's fine if you're doing a web app or something but for numerical programming

48:19.440 --> 48:23.600
you spend all of your time trying to figure out how to have everything not being done in python

48:23.600 --> 48:30.320
you know you're basically always calling to see your kuda or whatever libraries um partly it's

48:30.320 --> 48:36.160
because Chris Latina everything he touches is awesome so it's just nice when somebody like that

48:36.160 --> 48:42.480
comes into your field and um i just can't you know i just love seeing what he's doing

48:43.280 --> 48:50.400
partly because Swift is just you know it's a good language um it's uh you know there's a few good

48:50.400 --> 48:56.080
languages in the world like i think f-sharp and julia and swift uh all examples of just good

48:56.080 --> 49:04.400
languages um but the thing about Swift uh and julia and f-sharp is they can all um you can write

49:04.400 --> 49:11.040
fast code in them and so like and so Chris's approach with with the swift potential flow team is

49:11.040 --> 49:18.800
that you'll be able to write all of your kuda kernels and stuff in swift you know and so be

49:18.800 --> 49:24.720
you know and and because like he's the llvm guy you know you know he's got the compiler chops to

49:24.720 --> 49:32.320
make sure that those swift kernels are not going to be any slower than the c kernels for me

49:32.320 --> 49:38.000
so also from a teaching point of view it'll be really nice to be able to show people every layer

49:38.640 --> 49:42.880
and from a research point of view it'll be really nice because we're we're able to

49:43.760 --> 49:49.920
swap out this lstm cell with our own lstm cell and not have to yeah worry about

49:51.440 --> 49:55.840
you know switching into c++ and comparing an extension and then dealing with a whole different

49:55.840 --> 50:06.000
debugging framework and all that how far along is it um not terribly far along um yeah it's not

50:06.720 --> 50:11.760
some of our students have tried to kind of play with it and it's not really usable yeah

50:12.320 --> 50:17.440
but i'm sure it'll get there okay and similar stuff going on in julia by the way julia also

50:17.440 --> 50:23.840
has similarly exciting stuff going on around writing kernels in julia and all that okay

50:23.840 --> 50:33.120
uh so maybe switching gears to 2019 and things you're excited about uh opportunities

50:34.160 --> 50:41.840
what what do you what are you looking forward to um well what do you think is going to happen if

50:41.840 --> 50:45.680
i don't know if you're one for kind of dusting off of crystal ball but

50:46.800 --> 50:51.520
no i don't know what i'm doing i'm doing another people i mean we're just keep doing what we're

50:51.520 --> 50:59.840
doing i guess um so i'm going to be doing a deep dive into um speech recognition kind of next year

51:00.560 --> 51:06.800
we're going to write a book about faster i and patorch kind of the book version of the course

51:06.800 --> 51:12.480
i guess early in the year with uh silvenko gogo who has been helping us with pretty much

51:12.480 --> 51:24.480
everything this year um you know i i hopefully will keep lowering the bar around um and we hopefully

51:24.480 --> 51:28.560
will start getting to the point sometime soon i don't know if it'll be in the next year where we can

51:28.560 --> 51:35.840
do useful stuff without any code that's that's really the the main bar that i want to get to is

51:35.840 --> 51:43.520
doing useful stuff without any code what does that mean for you so this is a startup i'm involved

51:43.520 --> 51:48.080
in called platform ai where we're trying to do exactly this for one particular sub domain which

51:48.080 --> 51:55.120
is image classification so you can uh don't even need labels you can import some some photos and we

51:55.120 --> 52:01.760
try and provide a uh kind of intuitive representation of what the model is learning to help a domain

52:01.760 --> 52:10.240
expert interact with it in an entirely visual way um so for me it's all about like people using

52:10.240 --> 52:17.520
stuff like that to solve scientific problems or to optimize their logistics or whatever it is

52:17.520 --> 52:22.720
they're trying to do you know like um it's all about recognizing that machine learning is

52:23.520 --> 52:30.960
computers learning from examples and so that should be all about getting rid of code we shouldn't

52:30.960 --> 52:35.760
need for loops and conditionals and stuff you should just be able to say here are my examples

52:35.760 --> 52:38.720
and the computer said here is what i'm learning and then you should say well here is some

52:39.520 --> 52:45.680
feedback about which ones are right and wrong so that's where we want to get to so yeah so we've

52:45.680 --> 52:52.080
uh already got the startup doing that for computer vision and i hope to do similar things in speech

52:52.080 --> 53:00.400
and nlp as well um as long as we rely on people knowing how to code we're missing out on something like

53:00.400 --> 53:08.480
99.9% of local population right right so maybe you know not kind of being so strict about

53:08.480 --> 53:14.560
2019 just you know looking ahead to the near future what um you know where you think the

53:14.560 --> 53:22.960
opportunities for us uh liar or are there specific um ideas you know or even some of the ideas

53:22.960 --> 53:27.120
that we've talked about in terms of you know better understanding of the way some of the training

53:27.120 --> 53:35.680
techniques are working or so the the big opportunity right now is nlp so um you know this year

53:36.800 --> 53:42.960
we and others showed that transfer for learning for nlp works specifically it works with that

53:42.960 --> 53:49.600
requiring an email labeling um um other than your target task and even then doesn't require

53:49.600 --> 53:59.280
much labels um we're gonna see i'm pretty sure similar things being shown for um generative models

53:59.280 --> 54:06.480
for text um so unfortunately that means one of the big opportunities will be that um spammers

54:06.480 --> 54:12.640
and trolls and people interested in disinformation will be able to use this technology to

54:13.680 --> 54:19.200
cause much more mayhem than they've been able to cause before um so i would say that's probably

54:19.200 --> 54:24.640
gonna happen in the next 12 months so i would not be surprised to see massive scale

54:25.440 --> 54:34.400
bots of generative text which are both appropriate enough to the thing it's responding to to seem

54:34.400 --> 54:43.280
reasonable and kind of reasonably believable stylistically that large numbers of people will

54:43.280 --> 54:49.680
be fooled by them large amounts of the time and they will not be easily automatically blocked or

54:49.680 --> 54:56.160
even analyzed to know the scope of them the way that things are now so i think the technology

54:56.960 --> 55:02.560
to fight that is harder than the technology to create it or is it just a will no it's much harder

55:02.560 --> 55:10.880
okay i i i i feel very confident that i you know if i had the reason or motivation to build such a

55:10.880 --> 55:19.120
bot now i feel very confident i could create one at scale which would you know be devastating to

55:19.760 --> 55:26.240
any social media platform uh and i think lots of people i mean not lots lots but you know

55:26.240 --> 55:33.040
anybody involved in the modern nlp kind of uh transfer learning stuff could um i would have

55:33.040 --> 55:38.000
know if somebody said to me like hey Jeremy Sebastian Root has just become an evil genius so this

55:38.000 --> 55:46.320
has written a twitter bot to spread russian disinformation can you go and help block it that'd be

55:46.320 --> 55:54.160
like no probably not you know i don't know that sounds really really really hard uh and that's

55:54.160 --> 56:03.120
often been the problem in these kind of areas is it's normally easier to create mayhem than to

56:03.120 --> 56:09.040
block it um and also when you're trying to block it you're always kind of being reactive

56:09.680 --> 56:14.560
particularly where we're not using more heuristic approaches to do the generative modeling but

56:14.560 --> 56:22.640
using kind of smarter approaches so yeah i think that's going to be a really big problem and one of

56:22.640 --> 56:30.000
the challenges is that the best way to fight that would would be to write your own generative

56:30.000 --> 56:35.520
text bots and you know use them to fight the disinformation but the kinds of people that would

56:35.520 --> 56:39.520
want to fix the problem would be much less likely to be the kinds of people who would be prepared to

56:40.320 --> 56:47.040
launch their own generative text box so i think that'll be difficult um but i then i think you

56:47.040 --> 56:54.560
know there's a lot of opportunities to uh you know in industry for for companies to be able to use

56:54.560 --> 57:02.240
text as a valuable resource i think it can certainly happen in medicine as well um like

57:02.880 --> 57:08.640
one of the challenges and radiology is that the the labels are all buried in radiology reports

57:09.280 --> 57:16.880
right and when you look at stuff like the data sets that the NIH should have provided for medical

57:16.880 --> 57:23.040
imaging the labels they've created have been using classic rules based NLP approaches and they're

57:23.040 --> 57:31.280
just terrible so having um that labeling done with these kind of modern transfer learning

57:31.280 --> 57:39.120
approaches will be awesome so yeah i think that's hopefully going to be one of the big at least

57:39.120 --> 57:45.120
the positive side of it will be one of the big things in 2019 um i think we're going to see

57:45.120 --> 57:56.160
these different approaches to normalization um probably start to take over from batch norm um i mean

57:56.160 --> 58:02.400
it's kind of unfortunate that we don't have a real image net competition anymore because like

58:02.400 --> 58:07.680
image net you know has plenty of flaws so i'm not saying we need image netback but the fact that

58:07.680 --> 58:12.880
there was our competition that quite a few of the more serious researchers decided to invest

58:12.880 --> 58:18.080
significant time in meant that each year you would see you know this year we've got

58:18.800 --> 58:24.320
measured three by three comms this year we've got resonance this year we've got you know

58:25.280 --> 58:31.760
Andrew Howard's data augmentation methods like they were like it made people who would otherwise

58:31.760 --> 58:38.400
be focusing on mathematically pure whatever's like actually focus on solving a problem properly

58:38.400 --> 58:45.360
mm-hmm so i i do worry that you know and also those solutions got published not just as papers but

58:45.360 --> 58:50.560
as you know um pre-trained models so we can download so i do worry a bit that we might continue

58:50.560 --> 58:55.760
to use pre-trained image net models with older architectures and older normalization approaches

58:55.760 --> 59:01.680
and stuff just because we don't quite have an image net competition anymore so i don't know

59:01.680 --> 59:08.800
what the fix to that is i mean there is like tf hub and now there's a torch hub which trans you

59:08.800 --> 59:14.800
know actually provide pre-trained models but people need to start realizing i think yeah that

59:14.800 --> 59:22.960
the image net models are increasingly out of date and then you know skip connections

59:22.960 --> 59:30.960
are still interesting the the unit paper is now the most highly cited mick i medical

59:30.960 --> 59:38.000
image and conference paper of all time resinat obviously is still kind of all powerful

59:38.000 --> 59:44.880
so two really important skip connection stuff so i think hopefully people will keep finding ways

59:44.880 --> 59:51.680
to better utilize skip connections mm-hmm and those two things skip connections and normalization

59:51.680 --> 59:57.920
yeah really help make models easier to train quickly and accurately mm-hmm

59:57.920 --> 01:00:06.640
the i think the rate at which new data sets are coming online is uh been uh increasing there

01:00:06.640 --> 01:00:17.040
have been a ton of these and like various domains in 2018 any do you think we see um or need for

01:00:17.040 --> 01:00:25.920
that matter like a kind of a better image net or a monster you know kind of the image data set

01:00:25.920 --> 01:00:32.560
to and all image data sets or do you think the domain specific direction is more that one better

01:00:32.560 --> 01:00:39.360
yeah yeah you know um so google to their credit just yesterday maybe it's this morning released a

01:00:40.160 --> 01:00:47.360
much more diverse version of open images um and that's really fantastic because the the previously

01:00:47.360 --> 01:00:53.440
the open images and image net data sets were incredibly biased in terms of they're basically all

01:00:53.440 --> 01:01:01.040
came from white western countries and so on that very very difficult to train models that could

01:01:01.040 --> 01:01:10.720
recognize Hindu weddings versus Christian weddings for example um so yeah i think we have a great

01:01:13.280 --> 01:01:20.720
photo object classification data sets um but we don't have great

01:01:20.720 --> 01:01:27.200
my cross-copy you know histopathology data sets we don't have great radiology data sets

01:01:27.760 --> 01:01:39.040
um on the text side and lp side uh on both yeah um particularly thinking of vision um yeah on the

01:01:39.040 --> 01:01:48.880
text side i mean it's ridiculous the it's there are almost no publicly available label text data sets

01:01:48.880 --> 01:01:55.040
that normal people can use as they wish um most of them are locked behind this thing called the

01:01:55.040 --> 01:02:01.280
linguistic data consortium which is part of you pen and um i'm sure originally when it was

01:02:01.280 --> 01:02:07.520
created it had every good plan to help research whatever but today it's basically this um

01:02:08.560 --> 01:02:13.920
thing that is increasing the exclusivity of the field so like it's my students can't replicate

01:02:13.920 --> 01:02:20.800
results in papers because so many of the data sets in an lp are locked behind all kinds of

01:02:20.800 --> 01:02:26.320
licensing agreements and like under the most popular ones the voters corpus you have to like

01:02:26.320 --> 01:02:32.480
download print sign with your organizational affiliation of form and send it to the to the

01:02:32.480 --> 01:02:37.760
government for approval you know you can't have a hundred thousand fast day ice students doing

01:02:37.760 --> 01:02:42.720
that so i can't use those data sets because they just aren't the like they were created in a time

01:02:42.720 --> 01:02:48.480
when people just didn't occur to them that like maybe there's more than just this like little

01:02:48.480 --> 01:02:52.560
research community of people who go to my little workshop but there's actually a there's a

01:02:52.560 --> 01:03:00.080
whole world of people out there who are wanting to do work right you know so that's a huge problem

01:03:01.760 --> 01:03:07.760
so i think um you know i think something else i would really like to see is in areas like

01:03:07.760 --> 01:03:15.520
medical imaging where data sharing is difficult um i want to see more model sharing

01:03:15.520 --> 01:03:19.920
so like there's lots of pre-trained image net models like more than we need

01:03:20.800 --> 01:03:25.520
so where's all the pre-trained histopathology models and the pre-trained

01:03:26.400 --> 01:03:32.320
CT models and the pre-trained MRI models like those are actually much more useful because you know

01:03:32.320 --> 01:03:38.320
if stanford releases their pre-trained prostate MRI model and then Boston picks it up and fine

01:03:38.320 --> 01:03:43.360
tunes it a bit and publishes those weights and then Harvard picks it up and fine tunes out a bit

01:03:43.360 --> 01:03:48.400
and then publishes those weights and then stanford can come back and fine tune that back in the

01:03:48.400 --> 01:03:52.720
circle right end up something better than they started with like it's actually it's been shown

01:03:52.720 --> 01:03:56.880
that you end up with just as good a model as you would have if you actually shared the data

01:03:56.880 --> 01:04:02.320
hmm but more you actually have to do is share weights so i would love to see people

01:04:03.760 --> 01:04:10.960
releasing pre-trained domain specific networks so another thing we saw quite a bit of this

01:04:10.960 --> 01:04:21.200
year was proposals for data sheets for data sets model cards you know different

01:04:21.200 --> 01:04:32.000
representations of the idea that we need to document the biases and and lineage in some cases

01:04:32.000 --> 01:04:40.480
of data sets and models do you see that stuff taking off? I don't see it taking off no I mean

01:04:40.480 --> 01:04:45.920
I think it's really cool the work that Jim Nick Gepparoo and all those folks did with the data

01:04:45.920 --> 01:04:52.480
data sheets for data sets work and it's a really interesting examples of other industries where

01:04:52.480 --> 01:04:58.480
that's happened so I've done some work with electronics and certainly I'm used to as they

01:04:58.480 --> 01:05:03.440
describe in the paper that every electronics component comes with a data sheet and a fairly

01:05:03.440 --> 01:05:12.800
consistent format and you rely on it having said that we're still suffering in the deep learning

01:05:12.800 --> 01:05:20.240
world from people not publishing the data at all or not publishing their code at all so I also

01:05:20.240 --> 01:05:25.360
worry about like if we say like well you can't publish your data set unless you create this data

01:05:25.360 --> 01:05:32.880
sheet maybe there'll be even less people publishing their data I do think that all the conferences

01:05:32.880 --> 01:05:39.520
need to say if you have experiments you need to publish the code and the data okay people claim

01:05:39.520 --> 01:05:46.000
that we have this great peer review system but when you actually look at it it doesn't really work

01:05:46.000 --> 01:05:53.600
like reviewers don't recreate the papers right the code to recreate the papers ever on the other

01:05:53.600 --> 01:06:00.240
hand where people put stuff up an archive there'll be many of our students all at the same time

01:06:00.240 --> 01:06:04.800
trying to replicate it and if they can they'll get up on GitHub and post issues and say what's

01:06:04.800 --> 01:06:10.320
going on which has happened just a couple of weeks ago it turned out that a widely cited paper

01:06:10.320 --> 01:06:15.360
as students went to replicate it and found they couldn't and discovered that the researchers

01:06:15.360 --> 01:06:21.600
had accidentally used the test the test set as part of the training data you know so like I don't

01:06:22.240 --> 01:06:30.800
at all agree with this idea that we have to keep this pure exclusive peer review system what we

01:06:30.800 --> 01:06:36.400
instead need is to be able to publish their code and publish their data and then get it out there

01:06:36.400 --> 01:06:43.840
so that the rest of us can can try it out and I think the academic community still doesn't realize

01:06:43.840 --> 01:06:52.240
how many of us do do that replication like every single algorithm that fast AI teaches or

01:06:52.240 --> 01:06:59.360
implements in our software we always re-implement from scratch and test from scratch for example and

01:06:59.360 --> 01:07:06.880
we're definitely not the only ones so I think like people are lots of people going to start

01:07:06.880 --> 01:07:13.200
publishing the data sheets for data sets let's first of all have the published data sets at all

01:07:14.240 --> 01:07:18.720
it would be great if they you know and I guess it's up to the kind of conference and venues

01:07:18.720 --> 01:07:24.080
and journals and stuff to start saying you know first yes you have to publish your code and data

01:07:24.080 --> 01:07:29.680
and then maybe once that's happening okay you actually have to also publish a data sheet to go

01:07:29.680 --> 01:07:35.360
with it we could at least encourage through the review process in the meantime to like if you've

01:07:35.360 --> 01:07:41.040
got a new data set to kind of say like can you you know reviewers could ask for information about

01:07:41.920 --> 01:07:49.280
how it was collected and you know what the diversity of people involved in was and stuff like that

01:07:49.280 --> 01:07:56.160
it certainly does seem like a good bar in a time when you've got 4,000 submissions to

01:07:56.160 --> 01:08:02.080
nerfs and a thousand papers being published to require that folks that are doing experimental work

01:08:02.080 --> 01:08:09.040
publish the code as well it does yeah well Jeremy it's been so great to chat with you once again

01:08:09.040 --> 01:08:12.960
great to see you here and thanks for taking the time thank you Sam

01:08:12.960 --> 01:08:22.960
all right everyone that's our show for today for more information on Jeremy or any of the topics

01:08:22.960 --> 01:08:30.080
covering in this episode visit twimmalei.com slash talk slash 214 you can also follow along with

01:08:30.080 --> 01:08:39.040
our AI rewind 2018 series at twimmalei.com slash rewind 18 as always thanks so much for listening

01:08:39.040 --> 01:08:46.240
and catch you next time happy holidays

