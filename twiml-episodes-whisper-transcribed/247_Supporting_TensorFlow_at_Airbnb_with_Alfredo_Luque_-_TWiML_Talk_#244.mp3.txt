Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
With today's show we conclude our TensorFlow Dev Summit series which was recorded on location
during the summit a few weeks back.
This overview features my conversation with Alfredo Luquet, a software engineer on the machine
learning infrastructure team at Airbnb.
If you're among the many Twimble fans interested in AI platforms and machine learning infrastructure,
you probably remember my interview with Alfredo's colleague, Airbnb's Otto Kale, in which we
discussed their big head platform.
In my conversation with Alfredo, we dig a bit deeper into big head support for TensorFlow.
Discuss a recent image categorization challenge they solved with the framework and explore
what the new 2.0 release means for their users.
If you're interested in topics like these, TensorFlow, deep learning, you'll probably
also be interested in the awesome swag box that Google gave out to some of the attendees.
They included their new Coral Edge TPU device and the Spark Fund Edge development board
as well as some other really cool goodies.
And I'm excited to give you an opportunity to win one in our latest giveaway.
To enter, just visit Twimbleai.com slash TF giveaway and let us know what you would do
with this kit if you got your hands on it.
We'd like to send a huge thanks to the TensorFlow team for helping us bring you this podcast
series and giveaway.
With all the great announcements coming out of the Dev Summit, including the 2.0 Alpha,
you should definitely check out the latest and greatest over at TensorFlow.org, where
you can also download and of course start building with the framework.
And now on to the show.
All right, everyone, I am here at the TensorFlow Developer Summit with Alfredo Lukay.
Alfredo is a software engineer at Airbnb.
Alfredo, welcome to this week in machine learning and AI.
Hi there.
Let's get started by having you share a little bit about your background and how you came
to work in machine learning and AI and in particular on machine learning platforms and frameworks.
Sure.
So I guess my interesting machine learning started in college actually well before, you know,
there were a lot of applications to do this.
Started with writings of my own like kernels for GPUs by hand.
And exploring some of the really primitive ways we could do ML back then.
Since then a lot of things have changed.
I mean, since, you know, the last five or six years we had a lot of new developments.
What interest drove you to write your own kernels?
It was just very curious about, you know, what could be done there.
There have been a lot of interesting and promising research around image recognition.
First the convolutional neural net papers have been starting to drop.
And there was a lot of really great applications that actually tried to set yourself outside
of a research setting.
So that's what really got me into it first.
I went on to work in the online advertising real-time bidding.
And later to work on my own hedge funds and then we really, we're just a lot of techniques
in like natural language processing, which further kind of drove me into the field.
Okay.
Yeah.
Interesting.
I don't think I've talked to someone that started their own hedge fund before.
Yeah.
So you're at Airbnb now.
What's your focus at Airbnb?
So at Airbnb, I work on our machine learning infrastructure team.
You actually had our engineering manager on here a couple of episodes ago.
Basically we build tooling to make machine learning really easy.
Developing models and productionizing them should be something that everybody can do.
It's kind of our belief.
And I specifically focus on aspects involving the actual creation of models.
You know, actually developing which techniques you're going to be using, making sure that
it's really easy to assemble those things and making sure there's no additional difficulty
in actually making it production ready.
So we have our own libraries that wrap around most of the major ML frameworks and make this
a really, really easy thing to do.
So TensorFlow is one of those, but we support most of the major ML frameworks.
Yeah, that was one of the aspects of what you're doing with Big Head that struck me in that
initial conversation with Atul, the extent to which you've invested in building the platform
and be framework agnostic and supporting many different frameworks that the data scientists
and software engineers want to use.
Is that responsibility falls on you among others?
But yeah, how do you kind of manage the burden of doing that?
Yeah, so we kind of dove in head first as the answer.
The really two approaches that you see to handling this sort of agnostic behavior.
One of them is you basically treat a machine learning model like a black box, right?
There is some interface, you know, you feed it some data and it's going to spit something
out and then you don't really care what happens inside.
And that definitely does make your platform pretty agnostic, but it also means you don't
really have a really nice integration.
So as a data scientist from ML engineer, if I'm actually trying to develop a model, it
becomes much more difficult if all I hand off is just this big black box.
We don't know if it's going to run efficiently.
We don't know if I want a different environment, it's going to behave the same way.
And I'm not really developing using the same tooling that's actually going to be running
my model.
So metronome frameworks have started really working on this issue of integrating everything
nicely.
I think in 2.0 tons of flows are really making strides there.
And there's other frameworks as well that has really started focusing on this.
But our goal was to really build these libraries that cleanly wrap around the major features
of each framework.
That doesn't mean a high support load, but it also does mean that our users, when they
write something really feel like they're interacting directly with the framework, can
use all its features.
And when they go to production lines, their model, then they get exactly what they wrote.
And they have a lot of predictability.
And more importantly, they can just swap out tons of flow for actually boost or something
else and get the exact same results in the earlier parts of their preprocessing.
So that was a really important aspect of Big Head for us was working on that.
I would imagine that swap ability kind of forced you down and approach other than the one
you've taken where it's more black boxy, more kind of least common denominator.
How do you expose the full functionality, but yet still a lot of people that do swap
outs?
We basically dive into the internals of each framework and try to find the interface
point that sort of makes sense to let users develop their model.
But then encapsulate it in a way where we know how to now feed data into that model.
So once we really establish the method, but we can feed data into a model of a given
framework, then whatever is on the other side, we already know how to deal with essentially
what's in the model.
And what comes before we build ourselves or the user route.
So we have a really, really good way of essentially converting all these data types and making
sure the flow of the data works well.
Another aspect we're exploring as well is once you write what we call a Big Head pipeline,
essentially your entire workflow graph of all the preprocessing steps in your machine
learning models, we can actually potentially convert that into, if you're using one framework,
a framework's native operations.
So if you're using purely tenser flow, you can pile it into a tenser flow graph.
And I think this approach is neat because in that case, you'd get basically no loss in
performance.
But again, it does require diving down into the internals of each framework.
I mean, that is a lot of work, but we think it's worth it.
We've also been collaborating directly looking to, as we open source, and agree engineers
from those different platforms and supporting some of these wrappers and contributing directly.
So.
One of the things that was featured here at the summit was a case study video about a
project at Airbnb focused on categorizing listing images.
What's that project about?
Yes, at Airbnb, we have a lot of listing images.
Basically, every listing you see might have between five to maybe more images.
And one of the challenges is showing the relevant images, right?
So when you go try to book an Airbnb, typically the first thing you want to see is, you know,
put you into the living room or the bedroom and not like the bathroom ear.
So it becomes really important to get accurate classification of those images so that we can
choose what to present.
And for host, we can also assist them potentially in reordering them.
So it was a pretty big project because while image classification is a well-known solved
problem, the issue is the skill.
We have almost half a billion images to sit through.
And if you try out a new model and you want to backfill it, that's a lot of time to backfill
it.
So traditional approaches, just kind of running this on, there's things like Spark, you can
run this on, so the CPUs would have taken months with any framework.
And so one of the things we did was really focus on optimizing that and just taking a pre-trained
model on Keras with some of the optimizations we made earlier in the pre-processing, we
were able to get that to a few days.
It's running on a few machines with GPUs, and so that was a pretty big one for us.
And there are many other projects like this that can use a similar approach.
What was it specifically that allowed you to get that to a few days from weeks or months?
Yeah, so I mean, when you have image data, unfortunately, it's never exactly in the format
that you need.
Yeah, an ideal world you have, you know, perfectly resized data sets, everything's like
nice and clean, but we have some images that might just, you know, they might be corrupted,
they're the wrong size.
If you're using something like ResNet 50, you have to do some scaling and kind of normalize
the values.
Get to the T24 by T24 image.
Yeah, and then normalize it to zero and range and reorder the color channels.
A lot of this stuff was written in Python, especially if you use Carrots, there's a lot
of Python in there, and that becomes very, very slow, that ultimately limits, even if
you have the fastest GPUs out there, they can't get images quickly enough to actually want
inference on your model.
Is the primary time savings inference focused or is it the training?
An inference.
Okay.
Maybe in B, most of our scaling issues are always with inference on a really training.
We've found at least that, typically with, you know, the full-todd GPUs or if you use
GPUs, a couple of machines are plenty to reasonably train whatever you have, but for inference,
that's the real key.
If you try to backhole something there.
Every day at huge scale, yeah, and if you try out something new, you know, every day
you get more images, so now it's going to take even longer to backfill all of history.
So we find ourselves backfilling all of history a lot, basically every time we try out something
new.
Let's dig into that particular point a little bit more.
This was something that I don't recall if Attila and I went really deep into this, but
Airbnb has spent a lot of time, like backfilling is a big part of the way you think about
the problems that you're solving, like zipline, a big part of what it's doing is trying
to manage these point in time, features and backfills, and like, yeah, walk us through
like that whole space.
What's the, is backfilling and how does it, where does it arise in your workflows?
Yeah, so I mean, a really simple definition of backfilling would be, you know, if you have
data that's collected every day and saved every day, then, you know, we might train our
model on a couple of days, but if we want to apply our model to all the data that we've
ever collected, we have to go through all of history.
So it might have started being collected two or three years ago.
So what's the specific example where you might want to apply the model to all of history?
So this was actually, that project is actually a great example.
It's categorizing the languages, okay?
Yeah, so if you just categorized last month, it's not too useful because frankly, the
user could click on a listing that hasn't been categorized, right?
Doing that in real time ends up being a lot of wasted effort because Airbnb has, you
know, a limited number of listings, it has many, but, you know, it's a finite number.
And it's a pretty manageable number.
And many users will be viewing the same listing.
So you have a lot of duplicated work there and it's a pretty expensive computation to go
through 15 images every time somebody clicks on it, right?
Okay.
So if we want to categorize these images, we need to do it for all listings, right?
Got it.
And if you want to run an experiment then and see by providing these better labels and
these reordered images on the sites, do we have a revenue increase?
You really need to go through everything.
Okay.
So that's kind of a rationale for it.
And so it's, again, backfilling is, does it only occur in inference scenarios or is
it also relevant to training scenarios?
It could be relevant to training scenarios if you need to generate training data.
So one example of this is there are models that use embedding.
So essentially, instead of a single value, outputting a chunk of your network, right?
And any of these models do need these embeddings and so you have to go run the embeddings model
on all your data.
If that embeddings model changes or something downstream changes, you might need a backfill
everything.
Okay.
So there are scenarios like this where for training, you might need to backfill something.
I'd say the majority are inference.
For most enterprises, it's pretty similar to what we've heard as well.
It's inference where we didn't chuck through a lot of data, right?
You kind of set up this backfill process, well, the big task was to reduce the amount
of time it took to categorize these images so that when you're doing the actual backfilling,
it's a manageable time.
You mentioned kind of a big part of what you do is like digging deep into these frameworks
did, to what extent was that required in this project?
Yeah.
So we actually done that work already in this case for essentially wrapping Keras and
there are different backends for it, but we had specifically also wrapped TensorFlow specific
implementation.
The investment here was also largely in building these reusable primitives that we could use,
whether we were using TensorFlow or not for pre-processing these images.
And we believe this time investment was largely worth it because we have many other models
leveraging these things now.
We did enable our data scientists to experiment using a variety of models and they would
always get the exact same data set after pre-processing.
So we actually ended up writing a lot of these things in C++ largely because we wanted
to use, this is getting a little into the weeds, but we wanted to use CPUs for the pre-processing
because the models then can use all the remaining GPUs, and this provided a pretty significant
performance boost, and it also meant that anything else, you know, like a tree model,
leverage it, so that was kind of the rationale for investing the time there and digging
in.
And so far as Keras' concern, the investment there was really making multi-GPU inference
very easy, our users don't have to think about these things.
Essentially, whatever machine you're running on, we figure out what's on there, configure
your model to run it in optimized fashion.
And that's one thing I think framework authors are starting to really focus on is, you
don't need to think so much about the level of mechanics of how you're running it.
And I'm trying to think of like if you are, if you're kind of starting out here and
you're trying to, you know, say you've got a team that you are supporting, and you've
got folks that want to work in different frameworks, like where do you start, or where would
you start if you were starting all over again, and like providing them some tooling that
would allow them to be most efficient, where do you think like the most value is in the
kind of things that you've been working on?
You know, I think about this a lot, but I think we've made more or less the right calls.
There are two areas that we really identified where people waste a lot of time.
One of those is simply generating your features to train on and to use for inference.
This is a huge time-waster because oftentimes it means a lot of really, really hacky querying
of a database.
You have no guarantees of point-and-time correctness or anything.
If you do want to guarantee that ends up being a lot of work.
And typically when you're building a model, you're going to iterate through a lot of different
data sets, a lot of different variations of features.
So that was one really, really huge area where people were wasting time.
And this has basically been productized at Airbnb in Zipline.
Yeah, that's right.
We like to call it the engineers working on it called a time machine for your data warehouse.
Right.
So actually, let's remember the second one, and let's just dig into this because I think
as I've kind of studied what you've done with Zipline, I mostly get it.
But like the point-and-time correctness, I don't feel like I fully, fully get exactly
the situations where you need that.
And so maybe walk through kind of where the various places that comes up and why it's
important and how you get there.
Yeah.
There's a lot of models that, for instance, will use aggregations, or some metric that's
calculated every minute, or every hour, or even every day.
But the problem is how a data warehouse works is often at midnight, you start feeding
in data.
And at the other midnight, you basically cut it off.
And there may have been updates in that window that you miss.
There may be things that have an afterwards that actually belong to that day.
There are a lot of things that can go wrong, and technically your data is not perfectly
accurate.
Right.
Zipline actually worries about the mutations that happen to a database, and it will actually
look at, for a given window of time, for a given aggregation, how do I do this correctly?
So that the numbers I give you for your model are reproducible, and they're perfectly
accurate.
So this reproducibility is really, really key.
I think that's for somebody building a model, one of the more interesting aspects.
If I'm going to get the same data later on in time, or earlier on in time, I know I'm
always going to get the same thing.
And that really just gives you a lot of ease of mind.
So what's the specific example of a model that needs this point in time correctness?
Yeah, so I can get to you in the specifics, but for many of the fraud models, you really
do care about actions that you use your friends that may have taken during a certain window
of time.
Those windows can get very, very granular.
So if you're off, or if you're missing data, or even worse, if pulling the data to consecutive
times you get different results, that's really
going to throw off your model and the consequences there can be pretty disastrous.
So you really care about the precision.
That's one case where you might have very, very granular time windows, and it's very likely
that you'll miss data just because of how data warehousing works.
Right.
So the data warehouse might have these daily aggregates or something, and you need to say,
tell me the number of times a user has tried to use this credit card in the past five minutes
from point X in time.
Yeah.
I kind of think.
Yeah, but it's aggregated daily.
So if you try to do it yourself, if you have daily aggregates, you're never going to be
quite right.
Right.
Right.
And the other really big feature is that ZIPLINE gives you this same data streaming for
free.
So once you've backfilled your model, and you want to actually use this in production,
when you have real data flowing in, you can actually get that same data feed in the
same exact format streaming, and it'll guarantee that that streaming data, more or less, will
match exactly what you would get at the end of the day, just using ZIPLINE offline.
So users don't really have to duplicate any of this work.
For many models, they end up being hosted both online, so in production, using the website
and offline for analysis, so it's a pretty nice thing to be able to use.
And so the second kind of big category beyond the point in time correctness is what?
Users write a lot of boilerplate, a lot of boilerplate code in general.
Like I said, data is rarely exactly in a format you need, and there's oftentimes a lot of
pre-processing that's done.
For images, it's usually not too bad.
It might be an impact on performance, but there's usually not too much code.
If you're dealing with tags, you have to strip a lot of punctuation outs, and code
that tags a certain way.
Sometimes there's aggregation of features like you might do, if this feature exists and
this one doesn't, then I want to admit both or something like that.
There's a lot of business logic that people in code is a result.
And historically, we just saw that this was copied and pasted.
Basically, you might have 500 lines of copied and pasted code.
With tweaks, so you can't just always copy, paste it, and expect it to work.
This became a really big problem because people can't share this stuff.
They can't compose it very easily for a new model, and it's very, very error-prone.
So instead, we really focus on modularizing this, so we have a psychic learn-like interface
where people can wrap these unique operations that they can define, or they can use our
built-ins, and compose them to build the entire workflow of the model.
And then if they want to share just one piece of that with their teammates, their teammates
can use that chunk as the basis for another model.
This reusability was a pretty big bet, but it's something that we found was a huge win
for many of our users.
And it enables us to do these nice things.
At the end of my model, I'm using XGBoost, but I want to use now a convolutional neural
network.
And on one line of code, I can just swap it out and it works.
And so where does this manifest itself in the system?
Is this the feature store aspect of?
This is a library that these are used.
The big library?
Yeah.
Okay.
If they're working in Python, they just import our library and can search just writing
a model using the big head libraries.
And if you want to use a TensorFlow model, we have Keras and T.O. estimators built in.
And then they can just use their normal TensorFlow syntax to build out their model.
But it now clearly plugs into the rest of their workflow.
And so at the very end, they end up with a pipeline, basically just an object that contains
all of their logic, and they can just save this thing.
And that'll include everything they need to run it.
And we run it for beta and production.
So.
Okay.
Yeah.
This is tie into, do you run it via a graph and the airflow and that as the orchestrator
or how do you?
It used to, no, it has its own graph scheduler internally.
So this little run in a single machine.
So it'll execute its components, you know, possibly a multiple threads.
There's schedulers for different steps they might take.
But it's meant to be real time.
So I mean, the latency is sub-nilosecond in many cases for using this thing.
Okay.
Whereas airflow might be for huge, you know, back full jobs.
So this entire graph, I mean, we'll run and online inference as well.
So every time you submit a request, it's running the whole graph.
And yeah, I mean, in the future, there's plans to add components where we can have steps
run on different machines and use one about GPUs and things like that.
But for now, we've found that even on a single machine, it's sufficient.
Okay.
Okay.
So there's two pieces then summarizing or effectively kind of managing the data and figuring
out how to deal with some of these trickier issues like point-and-time correctness and
efficient backfills and things like that and kind of, I guess that summarizes like raising
the level of abstraction maybe so that the user has to write less boilerplate and can
compose pipelines from things that are already provided for them.
Yeah.
And also whatever they choose to write.
Right.
I think composability of their own code is something that we really care about as well.
Especially if you're just hacking something out in a couple of hours, typically it's not
going to be the nicest code, so at least encapsulating it, right?
Yeah.
And it's a pretty good way to guarantee you can use that in the future.
Okay.
So yeah.
Okay, cool.
So have you started looking at the TensorFlow 2.0 and like how that impacts the way, you know,
some of the things you've already built and the way that your users will be using TensorFlow
in the future?
Yeah.
I mean, it makes our lives a lot easier.
I think so.
I think so.
One of the key things he did in T.Point now is deprecate a lot of really old APIs.
I think that was, that was initially we ran into with, there was a lot of, there was a
lot of incompatibility within the one point X releases, you know, certain APIs that only
work with certain features and that proved to be a really big source of frustration.
I think by removing those, those old APIs and really focusing on kind of starting more
of a clean slate.
It means that developers using TensorFlow will usually, I would expect an easier time.
And I think there are some other neat features in there as well.
I think the fact they've moved to mostly Eager mode by default is something that people
would like to be able to use.
If I'm most of your users prefer Eager mode to kind of the estimator of layers, API?
The users that care about Eager mode, it's historically been using PyTorch.
Just because it, by design, it kind of works in that way.
But I think they would consider TensorFlow as an option now that Eager mode kind of works
in a more easy fashion and they don't have these other sort of static graph complexities
that deal with the most part.
So yeah, I think it's going to emerge as a pretty viable option for, for many users just
looking to prototype, you know, even in Jupyter, it's really nice to just be able to run
a couple of styles into a result.
So yeah, I think that's, it's really open up those features to people that I think would
have not considered them in the past because of just the difficulty of using them.
They've done a lot around 2.0 and recently with integration with Colab and Jupyter Notebooks.
Your team has built quite a bit of infrastructure around Notebooks, like how do you see those
playing together?
Yeah, I mean, there are very much two different approaches, I think.
I can't blame them, but Google does want to integrate with their own offerings.
And we generally try to be really, really agnostic about what you're using.
So while integrating with, you know, their own, they have this really nice Jupyter, a
collaborative editor, using Google Cloud, yeah.
But I think, you know, if users can leverage that, I mean, it's a great product.
I've tried it out myself and it's, I'm sure, I'm sure it's a great option.
Do any of the new capabilities allow you to do more better things in your own notebook
implementation?
Or are we already doing everything you needed to do anyway?
We definitely weren't doing everything we needed.
We belong to features that people like.
One of the things I saw in the demo were, so the ability to better show these types
of photographs and kind of expose parts of your model inside a Jupyter notebook.
I think that would be useful for us, but I would need to take a closer look at what that
offering entails.
So, yeah, I mean, anything they can do to better surface visualizations, I think that's
one area where generally users are very happy if you give them better ways to sort of see
what's happening under the hood.
One sort of board was one really, I think it was basically the only mainstream option
that you could kind of do for visualizing models.
And there just wasn't a very good way to integrate that nicely with Jupyter.
There was a lot of caveat, so I think it'll be really exciting if they start making that
first class and see what they do.
And you use within Big Head TensorBoard pretty extensively, right?
I wouldn't see.
Remembering that extensively.
But you have your own visualization stuff.
We support TensorBoard, not super well right now, but it's on a roadmap.
There's a lot of internal visualizations that we have.
We've run visualization library for data and visualizing these pipelines.
And we generally like to have a cohesive look whenever possible.
So we'll probably continue to use that.
But if a model exposes its own visualizations, we want to just leverage that out of the
box.
We don't want to reinvent the wheel or anything.
So as an example, one thing we did, XGBoost has a feature importance, kind of really nicely
built in.
We can just expose that plot and people are loved using that.
So yeah, I mean, using TensorBoard or whatever the framework builds in is something that we
going forward want to use.
And do you use any of the other kind of components of the TensorFlow, the evolving TensorFlow
family, like the probabilistic programming or TFX or any of those other components?
Yeah, I mean, we would like to be able to use more of the components, but essentially
how we operate is also largely based on what our users need the most right now.
There are other focuses at the moment.
But I think as we plan on going and open sourcing this, it's going to be easier for other
people to add support for these different features that they need.
Yeah, probabilistic programming.
There are a couple of frameworks that are kind of looking into this.
I think Uber released one as well.
Yeah, Pyro.
That'd be really cool to add, but we just haven't had any tangible use case that immediately
needed it right now.
It sounds like your focus is on enabling kind of the core use cases and you haven't had
much of a need yet to support some of these other ones.
Yeah, I mean, that's the kind of thing that if we offered it, you would find use cases
for it.
But yeah, you have to start somewhere and I think giving users really, really good fundamental
building blocks that work first or a good area where we can do that.
I think estimators in TensorFlow are one area where they're really investing heavily
in that and we'd like to have that also be really, really well supported as well.
And is it not currently?
We support some really basic use cases with estimators, so they have some pre-built ones
and you can define your own custom estimator.
And maybe take a second to explain what those are and how they're used.
Yeah, so estimators are basically TensorFlow's, I think, approached as something that's
like it learned like.
We have a very, very simple API, essentially trained and run inference and the model is
very encapsulated.
Right?
You don't have to worry about sessions and graphs and all these things.
And basically define this structure of your model and what its input data looks like.
And then you have something you can just play with.
They provide a lot of built-ins, so like deep neural networks, you can kind of choose
what those look like, linear models.
I think they have a couple of other ones as well, but I'm not mentioning.
And you can also write your own.
So you can also just take TensorFlow code and kind of wrap it in one of these estimators.
This fits really nicely with what we're doing in Big Head, where we're trying to encapsulate
these models.
So being able to allow users to use these more easily and not have to worry about anything
other than the actual design of the model, that would be beneficial for them.
So like I mentioned before, worrying about GPUs or where things are run or that's one
question that we don't want our users thinking about.
That's because we can almost always make more informed decisions at runtime about what
that should look like, so they can get the best performance out of their models.
So yeah, that's something I'm pretty excited to see what they do with.
I think it was already in a pretty nice state, not too long ago, but now that they clean
up TensorFlow more and 2.0, it's probably going to be the defector choice for a lot of people.
We kind of taking a step back and kind of thinking about frameworks and the framework space
from the perspective of someone who has to provide these to a set of users.
Do you have like a wish list or predictions or where you see it all going?
How do you see this evolving or what would you want to see to better support your users?
Yeah, I think focus on developers and focus on the enterprise space is going to be one
area that's pretty interesting.
So I mean Google has been investing a lot in TFX and it is TensorFlow's specific, so
I mean you have to use TensorFlow.
But it's also largely their attempt at handling the entire end end, data management, model
management, all of this.
We haven't seen that too much from other framework providers, but one thing we have seen outside
of TensorFlow is a focus on modularity alone.
So another framework we've been exploring a lot is MxNet and they've had a lot of focus
as well on this.
So I really see, I mean you're probably going to have only a few big frameworks left
after a while.
For general use cases, it's just very, very hard to develop a new framework.
So yeah, I mean I see the main player still has probably TensorFlow and PyTorch and MxNet
with specific frameworks kind of lacking behind.
I mean you have things like SpaceC and you know you'll have things like Pyro for niche
use cases or for NLP specific use cases, but yeah, I mean a consolidation is very likely
to happen.
It's just not sustainable to have 10 of these and you know, developers putting their time
between them, but it's, I think it's healthy to have competition, you know, an example
of that was PyTorch was kind of the first to have the ability to just run these computations
on a fly and TensorFlow then came out with eGermode and MxNet came out with its own like
comparative APIs.
I think that's a really good back and forth that will really help everyone.
I think there's been a lot of focus on the research space too historically.
So making it really easy to do research is nice, but for most people they generally have
a rough idea of where they're going to start and they want really nice higher level APIs
and then when they have those APIs, the other issue right now is usually when you use one
of those things there's such a performance that moving it to production is going to require
more engineers and that's something we at Airbnb don't really have.
I mean we try to basically only have people writing models and maybe a team working on
the infrastructure but nobody actually dedicated purely to converting these models into a performance
version, right?
Okay.
That's something that we think computers can do better.
Well, Afraidio, thanks so much for taking the time to chat with me.
Thank you very much.
Alright everyone that's our show for today.
For more information on Afraidio or any of the topics we covered in this show visit twimmel
AI dot com slash talk slash 244.
As always, thanks so much for listening and catch you next time.
