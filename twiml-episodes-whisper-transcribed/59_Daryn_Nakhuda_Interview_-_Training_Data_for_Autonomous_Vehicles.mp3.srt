1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,240
I'm your host Sam Charrington.

4
00:00:23,240 --> 00:00:27,040
Well team, I guess I'll just jump straight to the bad news.

5
00:00:27,040 --> 00:00:31,080
This week we shared with excitement the news about a special Halloween event that we were

6
00:00:31,080 --> 00:00:34,480
planning for October 30th in New York City.

7
00:00:34,480 --> 00:00:39,920
Well, due to unforeseen events beyond our control, the event is now canceled.

8
00:00:39,920 --> 00:00:45,720
We were really really looking forward to it and are incredibly disappointed about its cancellation.

9
00:00:45,720 --> 00:00:49,960
If you purchase tickets via either the event bright or splash that pages, you should

10
00:00:49,960 --> 00:00:52,280
have been automatically refunded.

11
00:00:52,280 --> 00:00:56,520
The good news though, is that you can still connect with us on Monday evening because

12
00:00:56,520 --> 00:01:01,400
those will be headed to the NYU Future Labs AI Summit Happy Hour.

13
00:01:01,400 --> 00:01:06,040
If you're in New York City, we hope you'll join us at the happy hour and more importantly

14
00:01:06,040 --> 00:01:08,040
the AI Summit itself.

15
00:01:08,040 --> 00:01:13,280
As you may remember, we attended the inaugural Summit back in April and had a great time

16
00:01:13,280 --> 00:01:17,360
and delivered some great interviews, all of which will link to in the show notes for

17
00:01:17,360 --> 00:01:19,200
your listening pleasure.

18
00:01:19,200 --> 00:01:24,400
This year's event features more great speakers, including Karina Cortez, head of research

19
00:01:24,400 --> 00:01:30,520
at Google New York, David Venturelli, science operations manager at NASA Ames Quantum

20
00:01:30,520 --> 00:01:37,360
AI Lab, and Dennis Mortensen, CEO and founder of StartupX.ai.

21
00:01:37,360 --> 00:01:47,000
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the

22
00:01:47,000 --> 00:01:49,800
code Twimmel25.

23
00:01:49,800 --> 00:01:55,520
You can find links to this and more great events on our new events page at twimmelai.events,

24
00:01:55,520 --> 00:02:02,720
and of course, this shows notes page at twimmelai.com slash talk slash 57.

25
00:02:02,720 --> 00:02:08,600
The episode you're about to hear is the first of a new series of shows on autonomous vehicles.

26
00:02:08,600 --> 00:02:13,280
Now we all know that self-driving cars is one of the hottest topics in machine learning

27
00:02:13,280 --> 00:02:18,280
in AI, so of course, we had to dig a little deeper into the space.

28
00:02:18,280 --> 00:02:22,760
And to get us started on this journey, I'm excited to present this interview with Darren

29
00:02:22,760 --> 00:02:26,840
Nakuda, CEO and co-founder of Mighty AI.

30
00:02:26,840 --> 00:02:30,560
Darren and I discussed the many challenges of collecting training data for autonomous

31
00:02:30,560 --> 00:02:36,640
vehicles, along with some thoughts on human-powered insights and annotation, semantic segmentation,

32
00:02:36,640 --> 00:02:39,040
and a ton more great stuff.

33
00:02:39,040 --> 00:02:44,160
You may not realize it, but if you're a long-time listener, you already know Mighty AI from

34
00:02:44,160 --> 00:02:50,840
my interview with their lead data scientist Angie Hugeback for twimmeltalk number six.

35
00:02:50,840 --> 00:02:55,000
It is so hard to believe that that was over 50 shows ago.

36
00:02:55,000 --> 00:02:59,160
Mighty AI was one of the first sponsors of this podcast, and it's great to have them

37
00:02:59,160 --> 00:03:01,680
back as a sponsor for this series.

38
00:03:01,680 --> 00:03:06,360
As you'll hear, the company delivers training and validation data to firms building computer

39
00:03:06,360 --> 00:03:09,000
vision models for autonomous vehicles.

40
00:03:09,000 --> 00:03:13,400
Their platform combines guaranteed accuracy with scale and expertise.

41
00:03:13,400 --> 00:03:18,720
Based to their full stack of annotation software, consulting and managed services, proprietary

42
00:03:18,720 --> 00:03:23,480
machine learning, and global community of pre-qualified annotators.

43
00:03:23,480 --> 00:03:31,760
We thank Mighty AI for being a valued sponsor, so please be sure to visit them at www.mty.ai

44
00:03:31,760 --> 00:03:46,880
to learn more and follow them on Twitter at www.mty.ai.

45
00:03:46,880 --> 00:03:52,680
Alright everyone, I am on the line with Darren Nikuda, the CEO of Mighty AI.

46
00:03:52,680 --> 00:03:58,640
Mighty AI is a company that you've heard from on the podcast before, but in fact they

47
00:03:58,640 --> 00:04:06,480
were formerly called spare five, and we interviewed one of their lead data scientists, Angie Hugeback

48
00:04:06,480 --> 00:04:10,720
back on twimmeltalk number six, just about a year ago.

49
00:04:10,720 --> 00:04:12,400
Darren, welcome to the show.

50
00:04:12,400 --> 00:04:13,400
Thanks Sam.

51
00:04:13,400 --> 00:04:15,120
It's great to have you on.

52
00:04:15,120 --> 00:04:19,480
Why don't we start by having you introduce yourself and talk a little bit about your

53
00:04:19,480 --> 00:04:21,120
role at Mighty AI?

54
00:04:21,120 --> 00:04:26,920
Sure, so my name is Darren Nikuda, I'm the CEO and one of the founders of Mighty AI.

55
00:04:26,920 --> 00:04:30,880
So we've been working at Mighty AI, as you said formerly known as spare five for about

56
00:04:30,880 --> 00:04:36,360
three years, and really trying to harness human insights and human power into building

57
00:04:36,360 --> 00:04:39,960
better training data sets for artificial intelligence.

58
00:04:39,960 --> 00:04:42,240
And tell us a little bit about your background.

59
00:04:42,240 --> 00:04:48,000
Sure, so my background has been in software engineering for about 20 years, mainly in

60
00:04:48,000 --> 00:04:53,960
internet technology, so everything from e-commerce and communications platforms through marketplaces

61
00:04:53,960 --> 00:04:57,720
and yeah, okay.

62
00:04:57,720 --> 00:05:04,040
And is Mighty AI the your kind of first for and to the AI space or have you been doing

63
00:05:04,040 --> 00:05:05,440
that for a while?

64
00:05:05,440 --> 00:05:11,040
So Mighty AI is really a first for in the AI, but really using human insights has been

65
00:05:11,040 --> 00:05:12,280
something I've been doing for a while.

66
00:05:12,280 --> 00:05:17,040
So at both startups as well as when I worked in Amazon, I leveraged mechanical Turk and

67
00:05:17,040 --> 00:05:21,640
other platforms to use humans to augment what we could do with our systems.

68
00:05:21,640 --> 00:05:24,200
Okay, awesome.

69
00:05:24,200 --> 00:05:31,400
So since the conversation with Angie, again, just under a year ago, it sounds like you

70
00:05:31,400 --> 00:05:36,480
guys have gotten a lot more focused and in particular, you're spending a lot of time

71
00:05:36,480 --> 00:05:39,400
in the autonomous vehicle space.

72
00:05:39,400 --> 00:05:42,480
Can you tell us a little bit about what you're up to there?

73
00:05:42,480 --> 00:05:43,480
Sure.

74
00:05:43,480 --> 00:05:48,720
So when we started a few years ago, we were really focused on human powered insights for

75
00:05:48,720 --> 00:05:50,240
almost anything.

76
00:05:50,240 --> 00:05:54,960
And what we realized was what really set us apart was our focus on quality.

77
00:05:54,960 --> 00:05:59,240
So like you talked about with Angie, you know, a year ago, really building our own models

78
00:05:59,240 --> 00:06:05,000
for user reputation and data, data quality predictions was key to our success.

79
00:06:05,000 --> 00:06:09,480
And really that was resonating with customers who were focusing on building training data,

80
00:06:09,480 --> 00:06:13,360
for building, you know, models where they need a really highly accurate data.

81
00:06:13,360 --> 00:06:18,160
And that boiled down to natural language and computer vision and really where we saw

82
00:06:18,160 --> 00:06:22,320
a lot of focus was on the computer vision side, specifically in autonomous driving, which

83
00:06:22,320 --> 00:06:25,440
is, you know, a huge field as you've, as you've seen.

84
00:06:25,440 --> 00:06:30,840
And we had a lot of a lot of demand there for really specialized, really highly accurate

85
00:06:30,840 --> 00:06:31,840
data.

86
00:06:31,840 --> 00:06:34,680
So we decided to focus purely on that area.

87
00:06:34,680 --> 00:06:40,080
And I've mentioned the conversation with Angie and you've mentioned human powered insights

88
00:06:40,080 --> 00:06:41,080
a couple of times.

89
00:06:41,080 --> 00:06:46,880
And I think I may be taking for granted that folks will have heard that podcast, but I

90
00:06:46,880 --> 00:06:48,280
probably shouldn't do that.

91
00:06:48,280 --> 00:06:53,800
So why don't you take a second to kind of step back and really walk through what you

92
00:06:53,800 --> 00:06:58,360
guys do so that we can, you know, make sure everyone's on the same page on that?

93
00:06:58,360 --> 00:06:59,360
Sure.

94
00:06:59,360 --> 00:07:05,240
So what we have a platform called Spare 5, which is basically a community of people around

95
00:07:05,240 --> 00:07:10,200
the world who we give a small microtask to and they are able to perform those.

96
00:07:10,200 --> 00:07:14,880
We have a quality control system in which that we can review and manage that both automatically

97
00:07:14,880 --> 00:07:16,640
and with other people.

98
00:07:16,640 --> 00:07:20,400
And what we deliver to our customers is a high quality result.

99
00:07:20,400 --> 00:07:22,520
So they'll come to us with a requirement.

100
00:07:22,520 --> 00:07:27,400
For example, a photograph and some requirements as far as what types of things in the photo

101
00:07:27,400 --> 00:07:28,400
need to be labeled.

102
00:07:28,400 --> 00:07:32,720
In the case of autonomous driving, that might be drawing bounding boxes around pedestrians

103
00:07:32,720 --> 00:07:37,040
and vehicles on the road, or it could be something like segmenting every pixel of the

104
00:07:37,040 --> 00:07:40,000
image into semantic class.

105
00:07:40,000 --> 00:07:45,400
And we'll build a workflow and we'll go through that and have humans do that and then using

106
00:07:45,400 --> 00:07:49,680
the combination of the humans and our AI deliver back a result to them that they can then

107
00:07:49,680 --> 00:07:51,480
use to build their own models.

108
00:07:51,480 --> 00:07:52,480
Okay.

109
00:07:52,480 --> 00:07:59,440
And you were previously doing this for folks that operated in a variety of market segments,

110
00:07:59,440 --> 00:08:06,320
but you've again focused more tightly on autonomous vehicles for some time now.

111
00:08:06,320 --> 00:08:12,400
Can you talk a little bit about some of what makes that market unique for what you're doing?

112
00:08:12,400 --> 00:08:18,880
Sure. I think autonomous vehicles, especially on the computer vision side, is really a great

113
00:08:18,880 --> 00:08:23,720
example of what needs to happen in order to build highly accurate models.

114
00:08:23,720 --> 00:08:27,960
In a lot of the other use cases that we dealt with in the past, there was a lot of flexibility

115
00:08:27,960 --> 00:08:32,960
or more subjective insights as to taste, like in retail or something like that, where you're

116
00:08:32,960 --> 00:08:38,480
much more focused on things that are not life and death and safety related.

117
00:08:38,480 --> 00:08:43,560
And with the vehicles, it really is about getting as much data as possible, with a lot

118
00:08:43,560 --> 00:08:48,600
of diversity as possible in getting it labeled in an accurate way in which we can feel comfortable

119
00:08:48,600 --> 00:08:54,240
that we can take this model, train a system, integrate all the sensors in the controls

120
00:08:54,240 --> 00:08:59,560
and put a car on the road and have it drive with humans and other cars right next to it.

121
00:08:59,560 --> 00:09:06,600
There are a number of different perspectives on the right way to do autonomous vehicles

122
00:09:06,600 --> 00:09:11,520
in terms of the different types of sensors.

123
00:09:11,520 --> 00:09:18,760
There seems to be one world view that's very heavily computer vision focused and looks

124
00:09:18,760 --> 00:09:24,480
at the camera as the ultimate end all be all sensor and there seems to be another point

125
00:09:24,480 --> 00:09:30,640
of view that's a little bit more integrative and includes LIDAR and other types of sensors.

126
00:09:30,640 --> 00:09:32,920
Do you guys have any perspective on that?

127
00:09:32,920 --> 00:09:38,760
Yeah, so most of the work we're doing is on the image side, camera side, but really our

128
00:09:38,760 --> 00:09:42,800
perspective is that in order to have a car drive like a human it needs to have the sensors

129
00:09:42,800 --> 00:09:43,800
of a human, right?

130
00:09:43,800 --> 00:09:50,320
So there's more than just your eyes, so that's where other sensors come in and maybe humans

131
00:09:50,320 --> 00:09:54,000
don't have a built-in LIDAR system, but we do have a sensor surrounding, right?

132
00:09:54,000 --> 00:09:59,960
So it's not just our eyes, but it's you sound so when you think about radar or ultrasonic

133
00:09:59,960 --> 00:10:06,720
other contexts that's more 360 than just a front camera or a back camera or yeah.

134
00:10:06,720 --> 00:10:10,560
Yeah, I rely pretty heavily on my spidey sense, which is about as close to LIDAR as I'm

135
00:10:10,560 --> 00:10:11,560
going to get.

136
00:10:11,560 --> 00:10:14,400
Sure, I mean, there's things that you pick up as you're driving, you know, like you

137
00:10:14,400 --> 00:10:18,720
see a person way down the road on the sidewalk and you're going to be thinking about will

138
00:10:18,720 --> 00:10:19,720
they cross or not?

139
00:10:19,720 --> 00:10:22,680
Maybe that is, you know, a camera seeing that, but also the intent of which way are they

140
00:10:22,680 --> 00:10:23,680
moving?

141
00:10:23,680 --> 00:10:24,680
What are they doing?

142
00:10:24,680 --> 00:10:26,040
Like in just what is your experience?

143
00:10:26,040 --> 00:10:29,560
Like in downtown Seattle, people usually stop at the crosswalk.

144
00:10:29,560 --> 00:10:31,800
Not really the case, the rest of the world.

145
00:10:31,800 --> 00:10:32,800
Right, right.

146
00:10:32,800 --> 00:10:40,640
Can you talk a little bit about where the service that you are providing fits into kind

147
00:10:40,640 --> 00:10:46,320
of the broader pipeline that your customers are deploying?

148
00:10:46,320 --> 00:10:47,320
Sure.

149
00:10:47,320 --> 00:10:52,440
And maybe as a prelude to that, you can talk a little bit about the customers that you

150
00:10:52,440 --> 00:10:56,400
target and any customers that you can name and kind of what they're working on.

151
00:10:56,400 --> 00:11:01,320
Yeah, so we work with a variety of customers, really everybody you could picture in the

152
00:11:01,320 --> 00:11:02,320
automotive space.

153
00:11:02,320 --> 00:11:07,080
So that could be the OEMs, which are the car manufacturers, the tier one suppliers who

154
00:11:07,080 --> 00:11:10,560
are the people who traditionally have provided parts, but are also now providing integrated

155
00:11:10,560 --> 00:11:11,560
systems.

156
00:11:11,560 --> 00:11:13,760
And then, you know, what we'll call disruptors.

157
00:11:13,760 --> 00:11:18,320
So companies like Uber that, you know, are using autonomous driving, maybe not as their

158
00:11:18,320 --> 00:11:23,560
core business, but as part of their broader offering and then startups who are purely focused

159
00:11:23,560 --> 00:11:27,360
on, we've never been in the automotive space before, maybe we have some individual experience

160
00:11:27,360 --> 00:11:31,120
but now we're going to go straight after kind of full autonomy.

161
00:11:31,120 --> 00:11:34,720
And so that's, you know, a wide range of customers.

162
00:11:34,720 --> 00:11:38,200
Most of them are starting out with a car on the road.

163
00:11:38,200 --> 00:11:40,880
So I cooked with whatever sensors they have.

164
00:11:40,880 --> 00:11:44,960
And then they're taking that data and the requirements coming from their research team,

165
00:11:44,960 --> 00:11:49,440
which might be object detection or might be, you know, semantic segmentation, it could

166
00:11:49,440 --> 00:11:56,240
be a combination of them, and then they give us their raw data, which is video or still

167
00:11:56,240 --> 00:11:58,800
extracted still frames in the requirements.

168
00:11:58,800 --> 00:12:03,280
And then we have to develop a workflow in order to give them back label data.

169
00:12:03,280 --> 00:12:04,520
Okay.

170
00:12:04,520 --> 00:12:10,360
And so object detection, that sounds pretty obvious in terms of what that means on face

171
00:12:10,360 --> 00:12:17,440
value, but are there nuances that are part of the process there that folks don't generally

172
00:12:17,440 --> 00:12:19,840
think of when they hear the phrase object detection?

173
00:12:19,840 --> 00:12:20,840
Oh, absolutely.

174
00:12:20,840 --> 00:12:27,080
I think more so in autonomous vehicles than in other spaces where it's not just about,

175
00:12:27,080 --> 00:12:31,200
you know, what shape is this thing so I can decide whether it's a car or a truck, but

176
00:12:31,200 --> 00:12:36,080
even when you see a truck, you might have, you know, four jeeps.

177
00:12:36,080 --> 00:12:41,760
One is a male delivery van, one's an ice cream delivery truck, one's a passenger vehicle.

178
00:12:41,760 --> 00:12:46,600
And those nuances actually become really important when you think about driving patterns.

179
00:12:46,600 --> 00:12:50,520
A ice cream truck may have kids running out to it, you know, as it drives down the street,

180
00:12:50,520 --> 00:12:54,800
a male man might be driving on the wrong side of the road and, you know, stopping very

181
00:12:54,800 --> 00:12:56,800
often at mailboxes.

182
00:12:56,800 --> 00:13:03,960
And, you know, who knows what a passenger vehicle might do, right, right, right.

183
00:13:03,960 --> 00:13:08,240
And then you also mentioned scene segmentation, tell us about that.

184
00:13:08,240 --> 00:13:09,240
Sure.

185
00:13:09,240 --> 00:13:13,800
So, you know, another part of thinking about computer vision is not just, you know,

186
00:13:13,800 --> 00:13:17,320
what are the objects in front of you, but really what is your context.

187
00:13:17,320 --> 00:13:22,000
So in the segmentation, what we're doing is labeling really every pixel that's in the

188
00:13:22,000 --> 00:13:28,840
field of view, whether that's a road or marking on the road or a vehicle or pedestrian with

189
00:13:28,840 --> 00:13:32,360
different sorts or vegetation and buildings and curbs.

190
00:13:32,360 --> 00:13:35,760
So really making sure that we have enough information about the different types of things

191
00:13:35,760 --> 00:13:40,120
that you're looking at that you can make better decisions.

192
00:13:40,120 --> 00:13:48,520
And so every individual pixel gets a label and the pixels are labeled essentially as objects

193
00:13:48,520 --> 00:13:55,000
or is there a like a fixed vocabulary that you're labeling the pixels with or is it across

194
00:13:55,000 --> 00:13:58,520
a, you know, a broad spectrum of objects.

195
00:13:58,520 --> 00:14:04,040
So the taxonomy of labels changes based on our customer requirements, but you can think

196
00:14:04,040 --> 00:14:09,360
of them in broader terms kind of as classes or as types of things.

197
00:14:09,360 --> 00:14:15,440
So it's not necessarily an object, but something like the sky or vegetation or an individual

198
00:14:15,440 --> 00:14:16,440
car.

199
00:14:16,440 --> 00:14:20,440
And usually from that, what we're doing is labeling everything we can see and then there's

200
00:14:20,440 --> 00:14:22,120
additional labeling steps afterwards.

201
00:14:22,120 --> 00:14:26,800
So we might break down vehicles into like I was describing earlier, various specific types

202
00:14:26,800 --> 00:14:27,800
of vehicles.

203
00:14:27,800 --> 00:14:32,120
And a lot of this also changes based on, you know, the location of the footage because

204
00:14:32,120 --> 00:14:36,840
terminology can change, the types of road markings can change based on what part of the world

205
00:14:36,840 --> 00:14:37,840
you're in.

206
00:14:37,840 --> 00:14:39,600
Hmm.

207
00:14:39,600 --> 00:14:43,960
Can you take us a little deeper into how you do all this?

208
00:14:43,960 --> 00:14:49,600
And in particular, I'm really interested in hearing how that's evolved from when you

209
00:14:49,600 --> 00:14:56,360
were tackling the problem more broadly to doing this specifically for the autonomous vehicle

210
00:14:56,360 --> 00:14:57,360
market.

211
00:14:57,360 --> 00:14:58,360
Sure.

212
00:14:58,360 --> 00:15:02,840
So probably the best example I can walk you through is on that semantic segmentation

213
00:15:02,840 --> 00:15:04,720
that we just spoke about because it's really hard.

214
00:15:04,720 --> 00:15:08,400
I mean, just thinking about the amount of time it would take to figure out how to label

215
00:15:08,400 --> 00:15:11,960
every pixel in an image of regards to tools.

216
00:15:11,960 --> 00:15:15,960
And so when we first started out, you know, especially with the spare five app, we were

217
00:15:15,960 --> 00:15:18,120
a mobile mobile only app.

218
00:15:18,120 --> 00:15:22,160
And what we've done is we we solve that platform, but we've also developed a desktop client

219
00:15:22,160 --> 00:15:25,320
or a web web based desktop view.

220
00:15:25,320 --> 00:15:27,240
And really that was about preference.

221
00:15:27,240 --> 00:15:30,720
So some people really like working on the tablet with their fingers or with the stylist,

222
00:15:30,720 --> 00:15:32,760
other people really like using the large screen.

223
00:15:32,760 --> 00:15:36,560
And either way, you need to be able to zoom in and really get within a couple of pixels

224
00:15:36,560 --> 00:15:40,360
of an edge when you're doing this type of drawing.

225
00:15:40,360 --> 00:15:42,600
And you know, work flow wise, we learned a lot.

226
00:15:42,600 --> 00:15:47,200
When we first started doing this, we said, okay, we've got a list of 75 classes of things

227
00:15:47,200 --> 00:15:48,880
that are in an image.

228
00:15:48,880 --> 00:15:51,720
And we built a couple tools to help you draw polygons.

229
00:15:51,720 --> 00:15:56,120
So you could go click around a shape and make a closed polygon and say this belongs to

230
00:15:56,120 --> 00:15:58,440
the sky or a car.

231
00:15:58,440 --> 00:16:04,440
And what we found was one, it was really hard to instruct humans on 75 different things

232
00:16:04,440 --> 00:16:05,440
at once.

233
00:16:05,440 --> 00:16:09,040
So even if we gave them a long instructional set and quiz them, it's a lot to keep in

234
00:16:09,040 --> 00:16:10,040
your mind.

235
00:16:10,040 --> 00:16:12,080
Two, it takes a long time.

236
00:16:12,080 --> 00:16:17,200
So drawing and labeling any one of these images from start to finish might take an hour

237
00:16:17,200 --> 00:16:18,200
of your time.

238
00:16:18,200 --> 00:16:22,680
And three, if you made mistakes, it was really hard to figure out where the mistake was,

239
00:16:22,680 --> 00:16:26,360
how to pick up on it, how to have somebody else come in and fix it or how to have you,

240
00:16:26,360 --> 00:16:27,360
you fix it.

241
00:16:27,360 --> 00:16:31,080
So we're not just throwing out an hour of effort.

242
00:16:31,080 --> 00:16:35,880
So we iterated on that process several times as far as an overall workflow.

243
00:16:35,880 --> 00:16:41,560
And our first pass of that was, well, instead of doing 75 classes at once, we'll do one.

244
00:16:41,560 --> 00:16:47,720
So let's have people focus purely on pedestrians or purely on vehicles.

245
00:16:47,720 --> 00:16:52,600
And that helped a lot because it made them really focus on the instructions as far as

246
00:16:52,600 --> 00:16:57,600
where to draw the lines, what accounts like do you go around the tires, do you go around

247
00:16:57,600 --> 00:17:01,040
the bumpers, how tight do you have to be.

248
00:17:01,040 --> 00:17:05,480
But it still is very time consuming, especially if you think about something like highway,

249
00:17:05,480 --> 00:17:10,120
highway scene in the middle of rush hour, where there's 50 cars within the field of

250
00:17:10,120 --> 00:17:11,120
view.

251
00:17:11,120 --> 00:17:13,320
And some of them really fire out on the horizon.

252
00:17:13,320 --> 00:17:19,040
And so when you switched to that, that first iterative step, did you go from a model

253
00:17:19,040 --> 00:17:25,680
where you would have like one worker work on all of the various things in an image to

254
00:17:25,680 --> 00:17:32,640
one where the image would kind of pass through steps and be routed to like the pedestrian

255
00:17:32,640 --> 00:17:36,480
team and the tree team and the vehicle team, that kind of thing.

256
00:17:36,480 --> 00:17:37,480
Yeah, that's right.

257
00:17:37,480 --> 00:17:39,720
And I think it's a little foreshadowing of our next step.

258
00:17:39,720 --> 00:17:47,000
But yeah, so what we had was for one person, one image, one person doing one class, so 75

259
00:17:47,000 --> 00:17:50,080
people, roughly for a full image.

260
00:17:50,080 --> 00:17:51,080
Right.

261
00:17:51,080 --> 00:17:55,960
And then the next, what we realized was the time between these different tasks was hard

262
00:17:55,960 --> 00:17:56,960
to predict.

263
00:17:56,960 --> 00:18:01,920
And it was still pretty exhausting to do all every single car in a scene.

264
00:18:01,920 --> 00:18:07,920
So the next iteration was really what we call recursion in our world, but it's basically

265
00:18:07,920 --> 00:18:12,600
we present the image with all the previous activity that's been done to it.

266
00:18:12,600 --> 00:18:18,520
So if there's 30 cars and 25 of them have already been boxed, we'll show you an image with

267
00:18:18,520 --> 00:18:25,400
25 boxed cars and say, are there more cars in the picture that haven't been labeled?

268
00:18:25,400 --> 00:18:30,000
And if they say yes, then we give them the drawing tool and say, draw the shape around

269
00:18:30,000 --> 00:18:31,240
one of the cars.

270
00:18:31,240 --> 00:18:33,400
So just do one at a time.

271
00:18:33,400 --> 00:18:37,440
And so they will outline a car, they'll label it, you know, according to which kind of

272
00:18:37,440 --> 00:18:41,080
class it belongs to and they'll hit next next.

273
00:18:41,080 --> 00:18:45,000
To that point, you know, we've taken something that took an hour or more to do the entire

274
00:18:45,000 --> 00:18:50,520
image to a short, you know, minute or two task to get it right, which allows us both to

275
00:18:50,520 --> 00:18:55,840
let, you know, have that individual unit of work be reviewed, both by our automated systems

276
00:18:55,840 --> 00:18:58,640
as well as by, you know, our reviewers.

277
00:18:58,640 --> 00:19:03,600
And then taking that and aggregating all of those, all the individual cars and all the

278
00:19:03,600 --> 00:19:07,880
individual, you know, different classes into that final composite image, just gives us

279
00:19:07,880 --> 00:19:11,920
a lot more flexibility into actually how quickly things can run because things can run

280
00:19:11,920 --> 00:19:16,800
in parallel as well as, you know, the quality because we have a lot finer green control

281
00:19:16,800 --> 00:19:21,240
as far as what we keep and what we don't keep and even how we, how we edit things.

282
00:19:21,240 --> 00:19:22,240
Hmm.

283
00:19:22,240 --> 00:19:25,640
Now, a couple of questions jump out at me.

284
00:19:25,640 --> 00:19:30,480
The first is have you thought about making this into like a capture?

285
00:19:30,480 --> 00:19:34,480
It seems like the, if you can simplify the UI enough, it seems like the perfect task

286
00:19:34,480 --> 00:19:39,040
to turn into a capture and just let people who are trying to sign into their bank or whatever

287
00:19:39,040 --> 00:19:41,000
do all this work for you.

288
00:19:41,000 --> 00:19:46,200
Yeah, I mean, you know, one thing is there are some really specific instructions as far

289
00:19:46,200 --> 00:19:48,920
as the different types of classes and labels we want.

290
00:19:48,920 --> 00:19:52,920
So in some cases like, you know, once something's been drawn and we just need to have you categorize

291
00:19:52,920 --> 00:19:54,600
it, that would make sense.

292
00:19:54,600 --> 00:19:59,560
But typically there's, you know, enough enough kind of context and we need to train and

293
00:19:59,560 --> 00:20:02,800
instruct people on just for them to really do a good job.

294
00:20:02,800 --> 00:20:08,040
Yeah, and I realized that part of the value proposition that you are bringing to the

295
00:20:08,040 --> 00:20:13,120
table is that you, as opposed to what someone might be able to find with the mechanical

296
00:20:13,120 --> 00:20:18,560
Turk you've got a pool of workers that you've, that you've taught how to do these kind of

297
00:20:18,560 --> 00:20:23,880
classification tasks and so the accuracy is higher and things like that.

298
00:20:23,880 --> 00:20:27,640
And so the, the suggestion is a little bit, you know, tongue in cheek, but it, you know,

299
00:20:27,640 --> 00:20:31,440
I've been getting a lot of these captures recently that are, you know, it'll show you

300
00:20:31,440 --> 00:20:36,840
a scene and it'll say, pick all the squares that have street signs in them and it makes

301
00:20:36,840 --> 00:20:42,880
me wonder if it's, you know, something someone like you folks doing, you know, basically

302
00:20:42,880 --> 00:20:48,120
farming out their object detection to, you know, folks that are trying to sign into websites.

303
00:20:48,120 --> 00:20:49,120
Yeah, certainly.

304
00:20:49,120 --> 00:20:53,960
I'm sure that, you know, recapture, which is owned by Google, that would be very obviously

305
00:20:53,960 --> 00:20:56,960
a use case for them to leverage.

306
00:20:56,960 --> 00:20:57,960
Right.

307
00:20:57,960 --> 00:21:03,600
And, you know, what we found is, like you said, there's a lot of instruction, we have annotators

308
00:21:03,600 --> 00:21:04,600
around the world.

309
00:21:04,600 --> 00:21:08,480
And so we have a community, a really large community, either in 155 countries around the

310
00:21:08,480 --> 00:21:09,480
world.

311
00:21:09,480 --> 00:21:13,440
And really it's us communicating with them and really getting alignment so that when we

312
00:21:13,440 --> 00:21:18,240
have somebody doing these tasks, the same person is going through, you know, the same workflow

313
00:21:18,240 --> 00:21:21,920
multiple times and they really aren't, aren't just a stranger being shown an image and

314
00:21:21,920 --> 00:21:27,520
saying, which one has a street sign, but really given really specific instructions and also

315
00:21:27,520 --> 00:21:33,040
giving them a way to engage with us because inevitably as you have this large data set,

316
00:21:33,040 --> 00:21:36,960
you'll run into, you know, places where the instructions need to be clarified or where

317
00:21:36,960 --> 00:21:43,040
they're confused as to, well, what do I do if there's a car, like partially, including

318
00:21:43,040 --> 00:21:47,400
another car, like how to, which way do I want to draw the boxes, that kind of thing that

319
00:21:47,400 --> 00:21:51,600
really requires us to interact with them a lot more closely.

320
00:21:51,600 --> 00:22:02,720
Okay, so my next question is, you have clearly, or will have clearly accumulated a ton

321
00:22:02,720 --> 00:22:09,920
of, you know, label data sets here is the, is a future step, automate, you know, building

322
00:22:09,920 --> 00:22:18,280
some AI models that automate the, some of this or, you know, for example, predict which

323
00:22:18,280 --> 00:22:25,160
of the pixels are sky or put a bounding box around the sky and ask the humans to correct

324
00:22:25,160 --> 00:22:27,200
as opposed to draw a new.

325
00:22:27,200 --> 00:22:33,920
Yeah, that's absolutely somewhere where we're going to be focusing is how to make the process

326
00:22:33,920 --> 00:22:34,920
more efficient.

327
00:22:34,920 --> 00:22:39,640
So that's both some automation up front as well as assistive tools so that we can make

328
00:22:39,640 --> 00:22:41,560
the community just perform better.

329
00:22:41,560 --> 00:22:46,840
So right now our drawing tools are very manual where you're clicking every point in a

330
00:22:46,840 --> 00:22:51,360
pixel in a polygon so that you can get a really sharp edge.

331
00:22:51,360 --> 00:22:55,080
But there's no reason why with edge detection and other techniques, we can't make that

332
00:22:55,080 --> 00:23:00,480
an easier process for the, for the community members as well as, as you said, if we can

333
00:23:00,480 --> 00:23:04,880
take that process, I was describing that workflow of recursion where we have, you know, for

334
00:23:04,880 --> 00:23:08,040
40 cars, we have 40 people going through doing each car.

335
00:23:08,040 --> 00:23:12,880
If we can skip the first 25 because we can do that in an automated way, that, that'll

336
00:23:12,880 --> 00:23:13,880
be great.

337
00:23:13,880 --> 00:23:18,720
That's not to say we ever want to replace the human because there's a huge value to

338
00:23:18,720 --> 00:23:23,000
kind of having that human eye, that human judgment because ultimately they are only going

339
00:23:23,000 --> 00:23:25,160
to be as smart as the people who are training it.

340
00:23:25,160 --> 00:23:30,560
But over time, if we can kind of up level them into doing more, you know, less of the

341
00:23:30,560 --> 00:23:35,320
road task and more of the task that really involve a human's particular judgments, that's

342
00:23:35,320 --> 00:23:37,080
where we want to get to.

343
00:23:37,080 --> 00:23:38,080
Right.

344
00:23:38,080 --> 00:23:45,560
I think the, you know, the safety of the autonomous vehicle is going to be, will be proportional

345
00:23:45,560 --> 00:23:51,560
to the amount of data that has been, you know, properly labeled and used in training.

346
00:23:51,560 --> 00:23:58,760
And so I think it just strikes me that, you know, there is tons more, you know, seen data

347
00:23:58,760 --> 00:24:00,960
to be processed.

348
00:24:00,960 --> 00:24:06,440
And even if you automated that easy, you know, 40 to 80 percent, there's still going

349
00:24:06,440 --> 00:24:12,960
to be plenty of work for the humans to do to do that, the more difficult task.

350
00:24:12,960 --> 00:24:13,960
Absolutely.

351
00:24:13,960 --> 00:24:17,840
And, you know, this is an industry where 95 percent accuracy isn't going to cut it,

352
00:24:17,840 --> 00:24:18,840
right?

353
00:24:18,840 --> 00:24:19,840
There's lives in the line.

354
00:24:19,840 --> 00:24:20,840
It's about safety.

355
00:24:20,840 --> 00:24:23,480
So you're really trying to get, you know, as perfect as you can get, and that's really

356
00:24:23,480 --> 00:24:24,480
going to take iteration.

357
00:24:24,480 --> 00:24:29,240
That's going to take always having a human in the loop to make sure that, you know,

358
00:24:29,240 --> 00:24:33,120
there isn't a misjudgment at this point where we're talking about training and validation

359
00:24:33,120 --> 00:24:37,040
before we're even talking about putting this onto the road and it's the wild.

360
00:24:37,040 --> 00:24:38,040
Mm-hmm.

361
00:24:38,040 --> 00:24:44,720
Do you think at all about the, any of the research that's happening around adversarial examples

362
00:24:44,720 --> 00:24:50,160
and there are some that are particularly focused on, I guess it's a little bit of a different

363
00:24:50,160 --> 00:24:53,800
context from where you're focused since your focus is on human annotation, but there's

364
00:24:53,800 --> 00:24:58,320
some research that looks at, you know, things that ways that you can manipulate images

365
00:24:58,320 --> 00:25:04,040
so that a neuron that will look at a stop sign and see a giraffe or whatever, is that,

366
00:25:04,040 --> 00:25:05,760
is that on your radar at all?

367
00:25:05,760 --> 00:25:10,280
Yeah, you know, it's definitely an area that we've considered both from that adversarial

368
00:25:10,280 --> 00:25:14,040
side and the generative kind of synthetic data side.

369
00:25:14,040 --> 00:25:20,400
And really, I think the more that we're tied to what's in the wild, whether it is, you

370
00:25:20,400 --> 00:25:25,200
know, running into stop signs that have been vandalized in a way that are intentionally

371
00:25:25,200 --> 00:25:31,640
trying to, you know, confuse models and vision systems or whether it's about getting a greater

372
00:25:31,640 --> 00:25:32,640
diversity.

373
00:25:32,640 --> 00:25:36,720
You know, one of the biggest challenges that auto manufacturers or people who are focusing

374
00:25:36,720 --> 00:25:42,560
on autonomous driving have is just how different, you know, scenarios are around the world.

375
00:25:42,560 --> 00:25:44,080
And also rare cases.

376
00:25:44,080 --> 00:25:47,800
So it's not just about our stop signs being in a different language or a different shape

377
00:25:47,800 --> 00:25:52,080
or the road markings being different, but even the types of vehicles we see on the road,

378
00:25:52,080 --> 00:25:55,880
right, like a pickup truck in the U.S. is pretty different from a pickup truck in parts

379
00:25:55,880 --> 00:25:57,360
of Asia, right?

380
00:25:57,360 --> 00:26:01,320
And, you know, there's a lot of rare cases a few months ago here in the Northwest, there

381
00:26:01,320 --> 00:26:05,120
was like a tractor trailer that turned over in the middle of the highway with a bunch

382
00:26:05,120 --> 00:26:06,920
of like, slime eels in the back.

383
00:26:06,920 --> 00:26:07,920
Right, I remember that.

384
00:26:07,920 --> 00:26:11,720
And, you know, it's like, how would, you know, what would you do if you were the car behind,

385
00:26:11,720 --> 00:26:15,480
you know, the autonomous vehicle behind that truck as that happened?

386
00:26:15,480 --> 00:26:20,120
And currently, even with hundreds of thousands of hours of footage, the odds of getting something

387
00:26:20,120 --> 00:26:23,000
like that on tape is going to be difficult, right?

388
00:26:23,000 --> 00:26:24,320
So, or low.

389
00:26:24,320 --> 00:26:31,080
So really, I think there is a balance of how do we augment what we have with other scenarios

390
00:26:31,080 --> 00:26:35,920
and other things to get that bigger picture of what could possibly happen?

391
00:26:35,920 --> 00:26:37,160
Hmm.

392
00:26:37,160 --> 00:26:41,880
Along those lines, you know, granted that for a lot of the companies in this space, their

393
00:26:41,880 --> 00:26:49,480
data is a core element of their IP and ability to differentiate.

394
00:26:49,480 --> 00:26:57,600
But are you aware of any movements to create like data consortia, for example, where, you

395
00:26:57,600 --> 00:27:03,440
know, OEMs would contribute their data with, you know, all in the agreement that they

396
00:27:03,440 --> 00:27:09,120
would get data back so that, you know, they may have cars operating in North America

397
00:27:09,120 --> 00:27:15,000
and, you know, they can contribute their data and get access to data from, you know, that's

398
00:27:15,000 --> 00:27:19,680
based in other geographies, and is that something that, A, is that something that is, you

399
00:27:19,680 --> 00:27:24,120
know, happening that you're aware of, or, and B, is that something that you might be

400
00:27:24,120 --> 00:27:25,640
able to help facilitate?

401
00:27:25,640 --> 00:27:26,640
Absolutely.

402
00:27:26,640 --> 00:27:32,520
So I think right now, it's, you know, there's a lot of secrecy in this industry, so everybody

403
00:27:32,520 --> 00:27:38,440
keeps their images, their data, even their requirements, as far as what they're labeling

404
00:27:38,440 --> 00:27:39,640
pretty close.

405
00:27:39,640 --> 00:27:44,080
But there are, there are starting to form more partnerships, companies working together.

406
00:27:44,080 --> 00:27:49,000
I think both for the reasons you described, as well as just, you know, everybody's working

407
00:27:49,000 --> 00:27:50,240
on slightly different angles.

408
00:27:50,240 --> 00:27:54,760
So if they can leverage each other's to build a solution and come to market sooner or

409
00:27:54,760 --> 00:27:57,920
be the first, I think they're going to, you know, embrace that.

410
00:27:57,920 --> 00:28:03,160
And where we can fit in is we, there's certainly a place in which we can leverage the data

411
00:28:03,160 --> 00:28:07,280
that we've already labeled and help people distribute that and manage that.

412
00:28:07,280 --> 00:28:11,320
So we're not duplicating as much effort, but we are really thinking about how to build,

413
00:28:11,320 --> 00:28:14,680
you know, a really useful kind of full data set.

414
00:28:14,680 --> 00:28:16,400
Right, right.

415
00:28:16,400 --> 00:28:22,760
So let's maybe dive back into the, you know, the process and, and the lessons learned

416
00:28:22,760 --> 00:28:29,440
and how that's expressed itself in technology that you've developed, anything else in terms

417
00:28:29,440 --> 00:28:35,120
of specifics, you know, things that you've observed specific to the autonomous vehicle

418
00:28:35,120 --> 00:28:36,120
market?

419
00:28:36,120 --> 00:28:37,120
Sure.

420
00:28:37,120 --> 00:28:40,800
You know, a lot of things I think could fit a broader market, but really by focusing

421
00:28:40,800 --> 00:28:46,280
here, it's allowed us to dive deep and not be, you know, distracted by what's going

422
00:28:46,280 --> 00:28:50,600
on in linguistics and natural language processing versus, you know, different parts of robotics

423
00:28:50,600 --> 00:28:51,600
and vision.

424
00:28:51,600 --> 00:28:57,080
But, you know, all of these, all of these approaches that require humans require a lot

425
00:28:57,080 --> 00:29:01,360
of, you know, management of the humans, right.

426
00:29:01,360 --> 00:29:06,040
So as far as really working to make sure that we can translate requirements to something

427
00:29:06,040 --> 00:29:10,760
can be understood, making sure that we understand when people are making mistakes,

428
00:29:10,760 --> 00:29:12,520
what is the, what is the reason behind it?

429
00:29:12,520 --> 00:29:16,920
There's actually, you know, a lot of psychology to, you know, why, why do we get bad data?

430
00:29:16,920 --> 00:29:18,600
Is it because people are being fraudulent?

431
00:29:18,600 --> 00:29:20,760
Is it because we didn't explain it right?

432
00:29:20,760 --> 00:29:23,960
Is it because we didn't even think about the scenario or is it because we explained it

433
00:29:23,960 --> 00:29:27,440
in a way that they're actually being consistent with what we told them to do, but we were, you

434
00:29:27,440 --> 00:29:30,000
know, we were wrong or we misunderstood something.

435
00:29:30,000 --> 00:29:31,600
So it's a really iterative process.

436
00:29:31,600 --> 00:29:35,640
It's, it's not something where you can just say there's a one-size-fits-all tool dropping

437
00:29:35,640 --> 00:29:40,520
your data, use a generic community and get, get good data out.

438
00:29:40,520 --> 00:29:44,120
And we've definitely, I think, learned that more than anything over the past few years

439
00:29:44,120 --> 00:29:48,320
as far as how much we need to understand really specific requirements, as well as how

440
00:29:48,320 --> 00:29:51,520
those fit with data that changes over time.

441
00:29:51,520 --> 00:29:55,760
And how do you, how does your platform express those requirements?

442
00:29:55,760 --> 00:30:02,960
Are they kind of hard-coded in for each project that you take on or do you have element

443
00:30:02,960 --> 00:30:06,040
of the platform that's like a rules engine or something like that?

444
00:30:06,040 --> 00:30:09,040
I'm trying to wrap my head around how I might implement something like that.

445
00:30:09,040 --> 00:30:11,840
So, you know, it's a combination of many things.

446
00:30:11,840 --> 00:30:16,080
So as I said, it's been an iterative process over the past few years as far as us developing

447
00:30:16,080 --> 00:30:17,080
it.

448
00:30:17,080 --> 00:30:21,280
On the instructional side, you know, we spent a lot of time on the instructional design

449
00:30:21,280 --> 00:30:27,320
as far as just making sure that once we internally have understood all the requirements and translated

450
00:30:27,320 --> 00:30:32,160
them into something that our community can understand, we're both giving them enough information

451
00:30:32,160 --> 00:30:37,720
in small enough pieces that they can understand how to use the tool, how to follow instructions

452
00:30:37,720 --> 00:30:41,560
for very specific tasks for a particular customer.

453
00:30:41,560 --> 00:30:47,320
So, you know, even the definitions of how to box or how to, you know, draw a shape around

454
00:30:47,320 --> 00:30:49,880
a vehicle might change from project to project.

455
00:30:49,880 --> 00:30:54,400
And so, making sure that within the context of what they're doing, we're constantly reminding

456
00:30:54,400 --> 00:30:58,640
them of the exact rules and then, you know, and testing them.

457
00:30:58,640 --> 00:31:04,080
So we do have ways to, you know, inject known task and make sure that they are meeting

458
00:31:04,080 --> 00:31:08,400
the right accuracy level as well as getting feedback constantly so we can tell them, you know,

459
00:31:08,400 --> 00:31:13,480
you're doing a great job at, you're drawing, but your labels are consistently or sometimes

460
00:31:13,480 --> 00:31:14,480
off in some way.

461
00:31:14,480 --> 00:31:19,160
Like you keep categorizing a, you know, a box van as a pickup and really they're two different

462
00:31:19,160 --> 00:31:20,160
types of things.

463
00:31:20,160 --> 00:31:25,400
So, you know, we try to have as much feedback as we can as well as the upfront instructions.

464
00:31:25,400 --> 00:31:30,280
And the upfront instructions, it's really, it's written, it's showing photographs and

465
00:31:30,280 --> 00:31:35,360
showing them examples of good and bad and then it's even sometimes going in and producing

466
00:31:35,360 --> 00:31:41,320
videos that really talk about a nuanced detail that is easier to express with words and

467
00:31:41,320 --> 00:31:45,440
in motion than it is with just a, you know, a paragraph and an image.

468
00:31:45,440 --> 00:31:49,920
And part of that too is, you know, we have a international community so making sure that

469
00:31:49,920 --> 00:31:56,360
we're conveying these in the language they understand, you know, there's no reason why

470
00:31:56,360 --> 00:32:01,280
some of these tasks can be done better by one language or one community than another.

471
00:32:01,280 --> 00:32:04,880
And it's really up to us to make sure that we're, we're opening it to the right people.

472
00:32:04,880 --> 00:32:09,400
If we have a community that as it speaks, you know, is natively Spanish speaking, if you

473
00:32:09,400 --> 00:32:13,720
give them very nuanced technical instructions in English, it's going to be harder to understand

474
00:32:13,720 --> 00:32:15,680
than if we give it to them in Spanish, for example.

475
00:32:15,680 --> 00:32:20,000
So that's the type of thing that we have to think about whenever we're doing our targeting

476
00:32:20,000 --> 00:32:24,320
as far as who's going to have access to this task as well as making sure that, you know,

477
00:32:24,320 --> 00:32:31,200
between us and our customers that there's alignment for a given task and to be more specific

478
00:32:31,200 --> 00:32:40,400
for a given scene and an object within a scene, how much redundancy is there in the process

479
00:32:40,400 --> 00:32:45,640
meaning, you know, for a given frame of a video, how many times you're asking someone

480
00:32:45,640 --> 00:32:52,840
to label a given object before you have that confidence level that it's done correctly.

481
00:32:52,840 --> 00:32:57,240
Is there a ton of redundancy in the process or have you managed to kind of filter that

482
00:32:57,240 --> 00:32:58,240
out?

483
00:32:58,240 --> 00:32:59,760
There's not a ton of redundancy.

484
00:32:59,760 --> 00:33:04,080
What early on in the process, we may have multiple people doing tasks in order to get

485
00:33:04,080 --> 00:33:09,320
a better understanding for the types of differences we'll see as people do the task.

486
00:33:09,320 --> 00:33:14,360
But ultimately, that's part of what makes our system work really well is that we get

487
00:33:14,360 --> 00:33:15,640
more efficient over time.

488
00:33:15,640 --> 00:33:18,360
We have less people doing it over time.

489
00:33:18,360 --> 00:33:19,360
Mm-hmm.

490
00:33:19,360 --> 00:33:20,360
Okay.

491
00:33:20,360 --> 00:33:25,960
Yeah, so unlike a traditional crowdsourcing model where your only quality control mechanism

492
00:33:25,960 --> 00:33:30,600
is looking for consensus or asking, you know, 10 people and saying, six of them agreed

493
00:33:30,600 --> 00:33:32,320
so that must be the right answer.

494
00:33:32,320 --> 00:33:36,600
We've tried to be a little bit more intentional and intelligent about how we make these decisions

495
00:33:36,600 --> 00:33:40,520
using our reputation engines and some of our other internal models.

496
00:33:40,520 --> 00:33:41,520
Okay.

497
00:33:41,520 --> 00:33:46,720
Now, this is maybe something that I should have asked earlier, but do you have any, can

498
00:33:46,720 --> 00:33:53,960
you share any data points that can help us contextualize the scope of the challenge

499
00:33:53,960 --> 00:33:59,720
within the autonomous vehicle space or, you know, the volume of data in that space or

500
00:33:59,720 --> 00:34:02,640
that you're focused, that you're working with in particular?

501
00:34:02,640 --> 00:34:03,640
Sure.

502
00:34:03,640 --> 00:34:07,040
So, you know, right now there's a few cars on the road.

503
00:34:07,040 --> 00:34:10,880
I think it's the easiest way to think about it, you know, all these, each company has

504
00:34:10,880 --> 00:34:14,520
a handful of cars at best collecting data.

505
00:34:14,520 --> 00:34:19,560
And even any one of those cars might be collecting, you know, terabyte of video a day.

506
00:34:19,560 --> 00:34:22,240
And most of that doesn't need to be human labelled.

507
00:34:22,240 --> 00:34:26,880
But there is, you know, significant volume, especially when you think about the diversity

508
00:34:26,880 --> 00:34:31,640
problems we were talking about earlier as far as that's one car on one road or one set

509
00:34:31,640 --> 00:34:38,040
of roads in one area of the world, you know, in the valley or in Germany or in Michigan.

510
00:34:38,040 --> 00:34:42,320
And so really, as these fleets develop, that's just going to scale exponentially, right?

511
00:34:42,320 --> 00:34:47,240
We're going to have both the test fleets, which will be hopefully located around the world

512
00:34:47,240 --> 00:34:53,120
in collecting different types of data, so not just images, but light art and other sensors.

513
00:34:53,120 --> 00:34:57,600
And then when we get into production, where we're going to start looking really for validation

514
00:34:57,600 --> 00:35:01,040
and feedback loops, especially when, you know, a system gets triggered.

515
00:35:01,040 --> 00:35:06,040
So for talking about an event-based system and we have emergency braking triggered, you're

516
00:35:06,040 --> 00:35:09,680
going to want to have a human validate was that the right, you know, right thing to do

517
00:35:09,680 --> 00:35:11,160
or not.

518
00:35:11,160 --> 00:35:16,560
And so I think over time, we end up with more and more use cases that are going to require

519
00:35:16,560 --> 00:35:20,920
human insight, even beyond just the raw data that's being captured.

520
00:35:20,920 --> 00:35:27,440
And part of the art will be figuring out what to annotate or what things need better labeling

521
00:35:27,440 --> 00:35:32,080
or what things don't, because obviously we can't take petabytes of data a day and process

522
00:35:32,080 --> 00:35:35,520
that in a meaningful way that's going to really improve things.

523
00:35:35,520 --> 00:35:44,440
Alright, do you often get tasks that are incremental in nature, meaning you've got, as opposed

524
00:35:44,440 --> 00:35:53,000
to processing all of the scenes or objects within, or all of the objects within the scene,

525
00:35:53,000 --> 00:36:01,800
the particular use case calls for only, you know, only the road dividers or signs or

526
00:36:01,800 --> 00:36:06,520
things like, it sounds like that's a typical thing for you to do, is that true?

527
00:36:06,520 --> 00:36:07,520
Yeah, definitely.

528
00:36:07,520 --> 00:36:13,160
I mean, it's actually, might be telling about what parts of the problem any one customer

529
00:36:13,160 --> 00:36:15,320
is focusing on a given time, right?

530
00:36:15,320 --> 00:36:21,840
So lane markings are obviously, you know, a very discrete task as far as, you know, looking

531
00:36:21,840 --> 00:36:27,560
at exits in dashed line, solid lines, road boundaries, and then pedestrians would be

532
00:36:27,560 --> 00:36:32,880
another really good example, as far as trying to understand what, you know, urban scene,

533
00:36:32,880 --> 00:36:35,800
where the pedestrians are located and how they're moving over time.

534
00:36:35,800 --> 00:36:39,800
So are they, you know, likely hit somebody to cross the roadway or cross in front of

535
00:36:39,800 --> 00:36:44,080
the vehicle or just stand around to, you know, to bus stop, we kind of have to understand

536
00:36:44,080 --> 00:36:46,680
that that's a bus stop where people just stand, they're not going to cross the street,

537
00:36:46,680 --> 00:36:51,800
they're not going to move anyway, they'll disappear magically, you know, in a couple of

538
00:36:51,800 --> 00:36:54,040
frames after the bus passes by.

539
00:36:54,040 --> 00:36:59,000
You know, there's things like that that I think really are individual areas of focus.

540
00:36:59,000 --> 00:37:04,360
So beyond the general kind of computer vision, like building a better eye for the camera

541
00:37:04,360 --> 00:37:10,600
is, is building, you know, context and building semantic understanding that I think are involved

542
00:37:10,600 --> 00:37:14,680
involved more of these discrete tasks.

543
00:37:14,680 --> 00:37:22,280
So do you do any labeling, do you do any, for lack of a better term on thinking of this

544
00:37:22,280 --> 00:37:28,440
like first derivative labeling, like as opposed to saying, you know, that's a pedestrian labeling,

545
00:37:28,440 --> 00:37:35,800
the pedestrian is walking in direction, you know, X or at a speed that you can calculate

546
00:37:35,800 --> 00:37:40,040
based on the, you know, the timestamps on different images and things like that.

547
00:37:40,040 --> 00:37:44,520
Yeah, so we definitely do tracking across video.

548
00:37:44,520 --> 00:37:48,840
So in that, that's actually, there's two ways to do that, right?

549
00:37:48,840 --> 00:37:53,000
You can either derive it from two still frames or you can play a video, which can sometimes

550
00:37:53,000 --> 00:37:57,160
be helpful as far as understanding what else is going on in the frame and just getting all

551
00:37:57,160 --> 00:37:58,640
that data at once.

552
00:37:58,640 --> 00:38:03,840
So having one person view five seconds can, you might give you more information as far as

553
00:38:03,840 --> 00:38:09,520
like if the rate of movement changes, like person's walking, then the, you know, the cross-signal

554
00:38:09,520 --> 00:38:13,640
turns to a blinking hand, they start walking faster or something like that.

555
00:38:13,640 --> 00:38:17,760
It's, you know, it's kind of helpful to see that happen or, you know, cars turning into

556
00:38:17,760 --> 00:38:20,400
their lane and so they stop in the middle of the road.

557
00:38:20,400 --> 00:38:23,600
Like, it's a little harder to see that when you're talking about an individual frame one

558
00:38:23,600 --> 00:38:26,880
at a time, even if you're trying to piece that data back together.

559
00:38:26,880 --> 00:38:31,760
And there are definitely nuances where it's not just a box around a person, but like I said,

560
00:38:31,760 --> 00:38:35,040
it's what kind of person, you know, are they, do they have a stroller?

561
00:38:35,040 --> 00:38:36,400
You know, are they, are they walking?

562
00:38:36,400 --> 00:38:39,480
Are they, are they distracted in some way?

563
00:38:39,480 --> 00:38:42,240
And then their orientation, so what direction are they moving?

564
00:38:42,240 --> 00:38:45,320
All of that metadata kind of feeds into it where you end up with an annotation that's

565
00:38:45,320 --> 00:38:50,040
not just an image with a box in coordinates, but it's an image with a box with a coordinate

566
00:38:50,040 --> 00:38:55,360
so it has a lot of metadata that might be related to this point in time versus the same

567
00:38:55,360 --> 00:39:00,000
image, you know, later point in time that has a lot of shared metadata, but also certain

568
00:39:00,000 --> 00:39:02,240
things change.

569
00:39:02,240 --> 00:39:09,400
And it sounds like you're also able to uniquely identify and track not just a person in

570
00:39:09,400 --> 00:39:16,480
a box, but, you know, person X in a box, you know, in frame one across, you know, all of

571
00:39:16,480 --> 00:39:19,720
the frames in a segment in which they're visible.

572
00:39:19,720 --> 00:39:20,720
Yeah.

573
00:39:20,720 --> 00:39:21,920
I mean, that's hugely important, right?

574
00:39:21,920 --> 00:39:24,520
To have that instance level kind of tracking.

575
00:39:24,520 --> 00:39:29,000
So you can say our car is changing lanes or somebody crossing the street or is it just

576
00:39:29,000 --> 00:39:31,720
that there's different people throughout the scene, right?

577
00:39:31,720 --> 00:39:34,800
So it's really important to know that kind of tracking.

578
00:39:34,800 --> 00:39:42,400
Do you have a sense for who is kind of leading the field in terms of data collection?

579
00:39:42,400 --> 00:39:47,360
You mentioned that, you know, most of the folks that are doing this have, you know, one

580
00:39:47,360 --> 00:39:54,600
or two cars out there, but you know, certainly Google's got more cars, at least they've got,

581
00:39:54,600 --> 00:39:59,680
you know, they've got a lot of cars that they've instrumented for maps that are capturing

582
00:39:59,680 --> 00:40:05,320
some of the same types of data and, you know, Tesla's got a lot of cars out there with

583
00:40:05,320 --> 00:40:10,720
cameras mounted and who's your, what's your sense for who's, you know, got the most

584
00:40:10,720 --> 00:40:15,960
data, visual data on vehicles, real life, you know, in the wild vehicles.

585
00:40:15,960 --> 00:40:20,240
Well, you know, I don't know that I would name a particular company, but really think

586
00:40:20,240 --> 00:40:26,200
about companies that have vehicles in the wild, which might be, you know, manufactured

587
00:40:26,200 --> 00:40:29,240
in production vehicles or could be fleets.

588
00:40:29,240 --> 00:40:36,080
So certainly there are companies that are trying to, say, distribute dash cams across, you

589
00:40:36,080 --> 00:40:41,880
know, consumer market so that they can capture video and use it for building, you know, autonomous

590
00:40:41,880 --> 00:40:44,720
systems while also providing value to the end customer.

591
00:40:44,720 --> 00:40:51,480
There's also things like taxis or ubers where there's an inherent value of that data to

592
00:40:51,480 --> 00:40:52,480
the driver.

593
00:40:52,480 --> 00:40:55,920
So there's a reason for them to put this device in their car, but there's also the value

594
00:40:55,920 --> 00:40:56,920
of the data collection.

595
00:40:56,920 --> 00:40:59,480
So I think ultimately there's going to be a couple of different strategies.

596
00:40:59,480 --> 00:41:03,400
It's not necessarily going to be, you have to produce cars and get them out there.

597
00:41:03,400 --> 00:41:04,400
Right.

598
00:41:04,400 --> 00:41:09,640
You have to produce a way to collect this data that's meaningful for people so that they're

599
00:41:09,640 --> 00:41:10,640
willing to do it.

600
00:41:10,640 --> 00:41:17,000
I've not seen the, you know, free dash cam if you give us the ability to, you know, use

601
00:41:17,000 --> 00:41:22,400
the data is who, do you have a, you know, specifically someone who's doing that?

602
00:41:22,400 --> 00:41:27,400
There are a couple of companies I can find the names for, for you later a little bit later.

603
00:41:27,400 --> 00:41:28,400
Okay.

604
00:41:28,400 --> 00:41:31,440
There's one company called Nexar that has a dash cam app.

605
00:41:31,440 --> 00:41:35,240
There's another company that's doing it specifically around ride sharing.

606
00:41:35,240 --> 00:41:37,520
So that's inside outside camera, right?

607
00:41:37,520 --> 00:41:41,360
So the idea of being, you're going to have, you know, you're going to see your customers

608
00:41:41,360 --> 00:41:44,840
in the back in case there's any situation where you have an abusive customer or an accident,

609
00:41:44,840 --> 00:41:49,120
you need to have that liability coverage as well as you've got your, you know, your front

610
00:41:49,120 --> 00:41:53,960
camera for, for actions and that kind of thing.

611
00:41:53,960 --> 00:41:57,840
In formulating that question, I hadn't even really thought about all of the dash cams

612
00:41:57,840 --> 00:42:04,680
and the various cameras that are mounted on public safety vehicles and utility fleets

613
00:42:04,680 --> 00:42:06,160
and things like that.

614
00:42:06,160 --> 00:42:09,960
There's just a ton of image data out there.

615
00:42:09,960 --> 00:42:10,960
Yeah.

616
00:42:10,960 --> 00:42:14,040
Well, you know, if you think about companies that have been working on mapping for a long

617
00:42:14,040 --> 00:42:18,080
time, like you mentioned the Google Street View and the Google Maps cameras, but even

618
00:42:18,080 --> 00:42:23,040
beyond that, you know, any, any fleet or any, you know, all of us who can carry a smartphone

619
00:42:23,040 --> 00:42:27,720
in our, in our pocket, so have some apps running in the background with location awareness.

620
00:42:27,720 --> 00:42:32,640
You know, that's all valid data as far as understanding kind of movement patterns in

621
00:42:32,640 --> 00:42:37,000
that, you know, that alone might not be enough with that in tandem with, with the camera

622
00:42:37,000 --> 00:42:41,480
becomes hugely valuable in that in tandem with, with high, high def maps can tell you

623
00:42:41,480 --> 00:42:43,760
when there's patterns that are changing.

624
00:42:43,760 --> 00:42:50,280
Is anyone doing anything as far as integrating and visual data collected via drones in

625
00:42:50,280 --> 00:42:51,280
the space?

626
00:42:51,280 --> 00:42:55,400
You know, I think that's a whole separate field as far as on the mapping side, for sure,

627
00:42:55,400 --> 00:43:01,240
on the actual vehicle driving systems, not that I know of.

628
00:43:01,240 --> 00:43:08,120
And is that because you really need to kind of be, you know, for the visual data to be,

629
00:43:08,120 --> 00:43:13,000
to have the unique perspective of the vehicle to be useful or because, you know, just hasn't

630
00:43:13,000 --> 00:43:14,000
happened yet?

631
00:43:14,000 --> 00:43:16,160
I think it's probably a little of both.

632
00:43:16,160 --> 00:43:21,600
So certainly companies think that one of their unique advantages is not just the footage

633
00:43:21,600 --> 00:43:25,520
they're collecting, but the way they're collecting it, whether it's using multiple sensors

634
00:43:25,520 --> 00:43:27,960
in a certain way, in certain positions.

635
00:43:27,960 --> 00:43:30,160
So you know, where do they locate their cameras?

636
00:43:30,160 --> 00:43:31,680
Are they using a stereo camera?

637
00:43:31,680 --> 00:43:34,360
Are they using, you know, side cameras as well?

638
00:43:34,360 --> 00:43:38,120
Wide field of view camera in tandem.

639
00:43:38,120 --> 00:43:43,800
So I think there is value to that, you know, uniqueness, but also, you know, I think we're

640
00:43:43,800 --> 00:43:46,800
just going to figure out what the right combination of data is.

641
00:43:46,800 --> 00:43:50,880
Obviously, you know, the more we can get the better, and for certain things like figuring

642
00:43:50,880 --> 00:43:54,760
out, you know, a big picture view of your current area, it would be great if you had something

643
00:43:54,760 --> 00:43:58,800
flying above you the whole time that could see, you know, a wider, further distance and

644
00:43:58,800 --> 00:44:03,800
a wider range than your eyes or your front cameras might see.

645
00:44:03,800 --> 00:44:05,800
Interesting.

646
00:44:05,800 --> 00:44:06,800
Interesting.

647
00:44:06,800 --> 00:44:10,720
What questions should I be asking that I might not have asked yet?

648
00:44:10,720 --> 00:44:15,720
Are there other thing areas that we might want to dig into before we start to wrap things

649
00:44:15,720 --> 00:44:16,720
up?

650
00:44:16,720 --> 00:44:21,200
Well, you know, I think we've covered a lot of a lot of good topics.

651
00:44:21,200 --> 00:44:27,480
I think when we talk about how people approach this problem, so it might be interesting.

652
00:44:27,480 --> 00:44:31,240
Before, you know, before my DAI, when you talk about other crowdsourcing or you talk about

653
00:44:31,240 --> 00:44:35,520
like doing it yourself, one of the biggest challenges is about the quality control, like

654
00:44:35,520 --> 00:44:37,840
I said, the instructions and all of that.

655
00:44:37,840 --> 00:44:41,760
But even when you're trying to do it in-house with your own team who knows all the requirements

656
00:44:41,760 --> 00:44:46,200
and all the instructions, it's really about that scale and diversity.

657
00:44:46,200 --> 00:44:53,440
And so I think, you know, really iterating that or kind of thinking about the fact that

658
00:44:53,440 --> 00:44:57,400
this data in Seattle is different from the data in Detroit, which is different than the

659
00:44:57,400 --> 00:45:01,160
data in Stuttgart or Singapore or any of these parts of the world.

660
00:45:01,160 --> 00:45:02,560
I think that's pretty key.

661
00:45:02,560 --> 00:45:07,200
So I mean, like you asked earlier about how to distribute, you know, how to get collect

662
00:45:07,200 --> 00:45:11,520
more data, it's not just like drive the same car in the same route over and over and over

663
00:45:11,520 --> 00:45:12,520
and over again.

664
00:45:12,520 --> 00:45:13,520
Like you do need to do that for a little bit.

665
00:45:13,520 --> 00:45:14,520
Right, right.

666
00:45:14,520 --> 00:45:17,600
But really it's about the diversity and then the understanding because, you know, your

667
00:45:17,600 --> 00:45:22,160
labelers in the U.S. might not even recognize what does it mean when I see a zigzag line

668
00:45:22,160 --> 00:45:26,280
on the road, on the side of the road in Europe, where, you know, we might go, oh, that's

669
00:45:26,280 --> 00:45:29,120
a no parking zone or that's, you know, that's a merge area, right?

670
00:45:29,120 --> 00:45:32,520
Like there's, so there's so much like context and so much localization that I think

671
00:45:32,520 --> 00:45:35,280
is easy to overlook.

672
00:45:35,280 --> 00:45:38,040
Even if you're somebody who's traveled and you know that you need to go learn the rules

673
00:45:38,040 --> 00:45:42,320
of the road somewhere else, it's like a lot of things that a human can adapt to that.

674
00:45:42,320 --> 00:45:46,160
If you're thinking about a system that really is just looking at what it sees and not having

675
00:45:46,160 --> 00:45:50,520
that higher level understanding, it's a really hard problem, right?

676
00:45:50,520 --> 00:45:54,160
Like I've never seen the zigzag road is that I mean, I have to like, you know, slalom

677
00:45:54,160 --> 00:45:59,200
my way down the road, right, as a car or does it just mean, like, stay away from that

678
00:45:59,200 --> 00:46:00,200
line, right?

679
00:46:00,200 --> 00:46:06,960
But the car figure out wouldn't be surprised to see some autonomous vehicle see one of those

680
00:46:06,960 --> 00:46:12,000
markings and just go go crazy as far as, you know, what it's supposed to do.

681
00:46:12,000 --> 00:46:13,520
Yeah, it's interesting.

682
00:46:13,520 --> 00:46:21,560
There's so much of this problem space that is, you know, the benefits from, from having

683
00:46:21,560 --> 00:46:26,840
that intelligence and the human or the computer intelligence and the human intelligence

684
00:46:26,840 --> 00:46:28,440
kind of melded together, right?

685
00:46:28,440 --> 00:46:32,680
So it's not just this training data labeling problem that we've talked about.

686
00:46:32,680 --> 00:46:40,960
You've made a very strong case for the power of combining human insight with automated,

687
00:46:40,960 --> 00:46:47,040
you know, automated tools, but even within the vehicle itself, there's a, there are folks

688
00:46:47,040 --> 00:46:53,000
doing research on, you know, how the car can benefit from the input of just looking at

689
00:46:53,000 --> 00:46:57,440
the driver and, you know, understanding what their, you know, state is what they're looking

690
00:46:57,440 --> 00:46:58,440
at.

691
00:46:58,440 --> 00:46:59,440
Things like that.

692
00:46:59,440 --> 00:47:00,440
Absolutely.

693
00:47:00,440 --> 00:47:04,120
Yeah, I think ultimately when we get, when we get to a point where we have full autonomy,

694
00:47:04,120 --> 00:47:09,080
we're going to be in a safer world, right, where we don't have distraction or distraction

695
00:47:09,080 --> 00:47:12,160
doesn't matter, you know, it's okay to sit there and start your phone if you're not the

696
00:47:12,160 --> 00:47:15,040
one driving and you don't need to be the one who's ready to grab the wheel.

697
00:47:15,040 --> 00:47:19,200
So we're talking about early stages where you need to be attentive and have your hands

698
00:47:19,200 --> 00:47:21,040
on the wheel or close to it.

699
00:47:21,040 --> 00:47:25,520
But as we get further down the road, you know, you take the best of the humans, which

700
00:47:25,520 --> 00:47:30,400
is the judgment and the vision and the decision-making processes, and you take away, you know,

701
00:47:30,400 --> 00:47:34,360
the fatigue and the distraction and the things that, you know, are going on in our lives

702
00:47:34,360 --> 00:47:39,240
and make it hard for us to stay focused, and I think you're going to end up in a better

703
00:47:39,240 --> 00:47:40,240
place.

704
00:47:40,240 --> 00:47:41,240
Awesome.

705
00:47:41,240 --> 00:47:47,640
Well, that is the hope and, you know, the vision behind autonomous vehicles for is, you

706
00:47:47,640 --> 00:47:54,160
know, as much as people talk about, you know, for example, the economic issues associated

707
00:47:54,160 --> 00:47:59,240
with deploying a bunch of autonomous vehicles in terms of labor and things like that, the,

708
00:47:59,240 --> 00:48:05,720
you know, the promise to stave off, you know, the, you know, huge numbers of vehicular related

709
00:48:05,720 --> 00:48:07,880
deaths, you know, that occur around the world.

710
00:48:07,880 --> 00:48:08,880
That's just huge.

711
00:48:08,880 --> 00:48:09,880
Yeah, absolutely.

712
00:48:09,880 --> 00:48:10,880
Well, awesome.

713
00:48:10,880 --> 00:48:12,880
Well, I really enjoy this conversation.

714
00:48:12,880 --> 00:48:18,360
Thank you so much for taking the time, and I really appreciate, you know, getting a

715
00:48:18,360 --> 00:48:23,720
chance to catch up with, with my DAI and hear about what you're doing in this space.

716
00:48:23,720 --> 00:48:24,720
Great.

717
00:48:24,720 --> 00:48:25,720
Thank you, Sam.

718
00:48:25,720 --> 00:48:26,720
Have a great day.

719
00:48:26,720 --> 00:48:27,720
Thanks, Darren.

720
00:48:27,720 --> 00:48:31,680
All right, everyone.

721
00:48:31,680 --> 00:48:33,520
That's our show for today.

722
00:48:33,520 --> 00:48:39,840
Thanks so much for listening, and of course, for your ongoing feedback and support.

723
00:48:39,840 --> 00:48:44,800
For more information on Darren, or any of the other topics covered in this episode, head

724
00:48:44,800 --> 00:48:49,520
on over to twimlai.com slash talk slash 57.

725
00:48:49,520 --> 00:48:57,320
To keep track of this autonomous vehicle series, visit twimlai.com slash AV2017.

726
00:48:57,320 --> 00:49:02,080
Please, please, please remember to send us any comments or questions you may have for us

727
00:49:02,080 --> 00:49:08,840
or our guests via Twitter, at Twimlai, or at Sam Charington, or leave a comment on the

728
00:49:08,840 --> 00:49:10,440
show notes page.

729
00:49:10,440 --> 00:49:21,080
Thanks again for listening, and catch you next time.

