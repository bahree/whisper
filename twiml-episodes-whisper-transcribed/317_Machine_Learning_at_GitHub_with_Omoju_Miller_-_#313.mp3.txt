Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Sam is back on the road for the next few weeks.
If you're hearing this on October 31st, you can catch him at TensorFlow World in Santa
Clara.
Next week he'll be at Microsoft Ignite in Orlando, and the week after that, a cube
con in San Diego.
If you see him wandering around, please pull up on him.
Say what's up.
Grab one of our awesome new stickers, or maybe even snap a selfie.
He loves that kind of stuff.
But before we move on, last week we published a show featuring an interview with Phoebe
Devries and Brendan Meade about their paper, deep learning of aftershock patterns following
large earthquakes.
It turns out that since that interview was recorded, some questions have been raised
about the research methodology used in the paper.
This was brought to our attention by Raju Shah, a longtime listener and friend of the
show, who initially raised the concerns.
The issue has been covered in numerous articles, links to a couple of which will add to a
note on the show notes page.
Sorry, we missed this one before publishing.
Happy Halloween, enjoy the show.
All right, everyone.
I am here in Santa Clara for the TensorFlow World Conference, and I've got the pleasure of
being seated with Omoju Miller.
Omoju is a senior machine learning engineer at GitHub.
Omoju, welcome to the Tomel AI podcast.
Thank you for having me.
It's great that we finally got this opportunity to have this chat for folks that are listening
in.
We've been trying to connect on this chat for quite some time, maybe about a year, yeah.
And not only do we get to finally have it, but we get to finally have it in person here
in sunny California.
I'm not in my head, yes.
As is typical for us, why don't we get started by talking a little bit about your background?
How did you get started and interested in machine learning?
When I got started in machine learning, it wasn't called machine learning.
It was just computer science, graduate work.
I think this was around 1999, 2000, and this is C++, there were no packages, you have
to write everything yourself.
I was taking graduate classes, I was a graduate researcher.
This is what I walked on.
I used MATLAB, but then I realized that this is too academic.
Now we're going to apply this thing.
So I pivoted to more knowledge representation on the semantic web.
We had a more press and realistic project to work with the Department of Defense.
And so that's what I did.
2012.
This is like owl ontologies and I can't stop.
Yeah, owl ontologies, yes.
I will have you know, we built one of the first sensor ontologies on owl.
Oh really?
Yes, and I think of it by still one of the most cited in that area.
Oh wow.
But I like that kind of stuff.
I like to do like motors, buttons, motors, torrents, first other predicate logic.
I like that.
It's very clean and clear.
It never quite happened for the web the way it was supposed to do.
It never did.
I don't know what happened.
Well, actually, I do know what happened.
Probabilistic models happened.
Probabilistic models happen.
We have lots more data.
We don't have to do that logical reasoning with enough data.
We can understand like the norms of things and it's good enough.
Humans, I good enough first fit model.
We don't need to have the entire thing, but a good enough solution we can take it there.
So probabilistic models were good enough.
And then Google came and existed.
And that was the end of that.
And it works.
Right.
So I went to parental leave and by time I came back, all the packages had been created.
There was a real ecosystem now for machine learning.
And I actually wanted to stop to study more about how to actually acquire technical knowledge.
Luckily enough, I went to Berkeley where they actually had programs where I could do this.
And I created it.
You need to enter the program.
Yeah, at Berkeley.
Okay.
You know, they had this little small program.
I could do that at Berkeley.
And I was, I also was listening to a lot of JZNB on say, to be honest.
So I was like, I need to spark you too.
Yeah.
I like language.
I love rap music.
So I'm just like, you know what?
Wouldn't it be cool if they actually had like an NLP rap unit?
Like why we always didn't poke in and all this stuff?
Like I like rap music.
A lot of what JZ is thinking, that's what I want to do.
My NLP on is the rap corpus.
Okay.
So we were able to like do that unit in classes.
And I just wanted to investigate like cultural approaches to actually acquiring technical
knowledge.
If people are not, if people are either passionate or extremely dispassionate, but there's
some kind of emotional response versus just like nothing boring, and if people already
come into an environment with mathematics and reasoning that they already have in their
heads, then they're able to actually truly understand the machine learning is a statistical
based approach to solving human problems.
It's not some kind of, it's not just derivatives and partial differential equations.
It's not, yes, we use that, but it's not math.
It's math applied to human context and modeling that.
That's what I wanted people to get at.
So I did that, and I was really excited about it, I wanted to continue.
And so wait, the connection between that and the rap music in J.C., the specific project
that you...
Yeah, I called it hip-hoppathy.
It was a very small thing.
Hip-hoppathy.
Okay.
I made a little thing, just like a little amiz-bush of such a work, you know?
Because you have to do a PhD, you have to get out of there, you have to do something,
study it, and get out of there.
And I wanted to continue working around how humans actually acquire technical knowledge,
and better yet, helping humans acquire that knowledge faster and solving their problems
faster.
So I was like, who would, who has the largest data on humans and computation?
GitHub.
Oh, it will also be in their interest to actually solve this problem too, because people are
not on GitHub, technical people are on GitHub, a lot of people come on GitHub also to learn.
And GitHub is where we build that technology that helps us move forward.
So it would be a great place if you want to study that and actually do work in that area
to go to GitHub machine learning, and they could build things to actually help people learn
better and get their work done faster.
So I reached out to people like GitHub, became friends with them.
Luckily for me, they were creating a machine learning team.
They asked me to interview, the rest is history.
Here we are.
Attensive flow.
In Santa Clara.
Right, right, right.
So when you first had the idea of doing this at GitHub, there was no machine learning team
at GitHub.
There was no machine learning team.
I was just like, hey, GitHub.
Hi, it's me.
What are you all doing?
You must have data.
What do you have?
What are the plans for you?
If people must have data now, because this is 2017, you existed since 2009.
That's a little years of data.
What are your plans for this data?
I think you should have enough now that are to start using machine learning to help accelerate
certain things.
So coincidentally, or maybe I think would have it, the company in a couple of months decided
that that's also what they wanted to do.
They said they were like, oh, well, I already was already interested in this.
But you just have to come and interview and see if you can help.
She can help us be part of this thing.
That's awesome.
What's your, what's the scope of the mission of the ML team at GitHub and your role in particular?
So I'm a machine learning engineer.
I'm a senior machine learning engineer.
I'm part of the inaugural machine learning team.
Our mission is to use GitHub data to accelerate the transaction cost of developer.
To reduce the transaction cost of developer corporation.
And how do we do that?
We do that by super charging GitHub features with data.
Because everything that people are doing in GitHub, we have a lot of data.
So if you've written issues, we've seen a lot of those issues.
So we know what issues are going to get close faster than others.
If you're looking at the same repos as I am, you're seeing a lot of the same issues too.
Yeah, we're seeing the same issues over and over again, like bugs.
Maybe things live all as bugs and get treated fast enough.
Maybe, okay, how can we like uproot this things like that?
Right.
Yeah.
And so you're building primarily kind of internally focused.
What's the way to think of it?
Is it GitHub features?
Yes, GitHub features.
They're not internal to GitHub.
They are product that are put on GitHub itself.
Okay.
So one of the products we have is when you open a GitHub repo, create a new GitHub repo.
Sometimes they give you suggestions of labels.
That is a machine learning product.
Like, oh, we see that you open this repo.
Maybe you want to add this level of deep learning to it.
Mm-hmm.
Yeah, things like that.
That is an example.
Like, oh, it's a topic.
Add this topic to it.
Oh, this things look like Azure, but are notebooks in it.
It has this in it.
It has that.
Maybe it's like the rest of everything that has all that stuff.
That is all machine learning.
Maybe you want to put this tag on it.
Things like that.
So imagine when you were a part of getting this effort started,
the first thing to do was like look across, you know, GitHub and figure out
where are the places that you inject machine learning.
You mentioned the tags, like discoverability.
It's got to be a challenge.
So, you know, tags and search are probably a place to start thinking about machine learning.
Yes.
Search as a natural one.
Discoverability is a natural one.
Mm-hmm.
So when I came in, I was, yeah, I think it was the fourth person.
Okay.
And that came to the ML team.
So my predecessors, all everybody is still within months of each other.
But like, okay, one of the first things we wanted to do was around helping people find open source
communities where they want to participate because we're an open source platform.
And one of the problems is you have all these skills.
You come and GitHub.
You want to give back some time.
How do you even figure out what open source community you want to join?
Right.
So understanding user interest.
And using that user interest, do recommendations for like open source communities.
Mm-hmm.
And the ideas are like, ah, we see you do a lot of ML stuff in here.
Maybe you're interested in TensorFlow.
Right.
Well, even beyond that look, they have issues that we think you might be interested in.
Right.
And smicking those kinds of suggestions.
So that's discoverability is one matching with open source is another one.
Another one is triaging.
Like, you know, putting labels on issues, maybe helping that with notifications.
Security.
There are so many places that machine learning fits in.
Mm-hmm.
Natural language, understanding of all the readmys and this and that.
And GitHub is a global platform.
So there are things that hasn't been built yet, like localization.
Perhaps machine learning could actually help.
If I'm reading a repo that is mostly written in Cantonese and Mandarin.
And I want to extract that knowledge.
Maybe there's a way we can actually help to do some kind of machine translation of some of the readmys.
So I have some idea of what is in this repo that I could use.
Okay.
Things like that.
Interesting, interesting.
Maybe steps, which is beginning.
And so when I think about kind of the scope of those types of problems and applying those problems.
I'm imagining a lot of, but not everything that you're doing is looking at the code.
And I'm wondering the types of tools that you use to enable that.
Is it all kind of natural language processing or there are other things that.
It doesn't sound like a solve solve problem.
No, it's not.
What are the types of techniques that you're kind of constantly using when you're thinking about building models based off of code?
Machine learning on code.
We are lucky enough at the machine learning team that we have sister team called semantic code team.
Okay.
And these are the folks that actually build parsers and build representations of code as abstract syntax trees.
It's very exciting to work at GitHub because you actually get to do the computer science you learn.
Like, you learn ASTs.
And then you go into the role of the software engineer.
If you're not writing compilers off when you're not using ASTs.
So we have a team that actually does those kind of representations.
And we can then ingest the things that create to actually model and build them representation and embedding of an AST.
Okay.
So that's something that is experimental now is not something that we actually have inside the product.
But it's these are the kinds of things that we're working on.
Like will ASTs help us get there faster?
Or should we model code as an AST or should we model code and treat it like natural language?
These are all these things you have to think about.
Right.
Because it's not natural language.
Right.
Clearly it's not.
But it's a language and it's rules are different.
So maybe then the question is how do you model a graph as an embedding?
So they're all these different kinds of questions.
And that is now a slow little subset of people in the world who do that kind of research.
And the question is is there research in that area that is fine enough that can be brought into a production environment and applied environment and can scale.
They can scale very, very well gracefully.
So these are all the paths we actually have to now pave.
Because we're not necessarily and it has to also happen very, very fast.
Because we're not building something bundled in an IDE.
Right.
So these are all the paths we're learning how to pave.
It sounds like at least a part of your team's charter is keeping an eye on that research horizon
and what's happening there and trying to figure out which of those things are worth playing around with,
far enough along to play around with.
Yeah. And we have people who help us do that. We have our product managers,
who help us, you know, the product managers help to also shape what we're going to pursue.
Yeah.
We have the ML team itself saying these are things that we think are,
these are no longer on the horizon.
I think we can do this now.
Let's walk with our product managers to actually see if we can all have this joint vision and bring this to the product.
Then we have our like, you know, infrastructure team saying,
the infrastructure is solid enough or robust enough that we think we can scale this.
So it's like a multiple stakeholders that we have our managers, our VPs,
who have the longer term vision and does this actually fall in line with the longer term vision
of what we want machine learning to really help with the product.
So we are a small part of a big hole and we have multiple stakeholders,
but luckily not wearing a place that we can all work together as one finely oiled machine
to bring things to reality with maybe like a 12 month horizon.
So here at the conference, you've already delivered a couple of presentations.
Yes.
Yeah.
So what were those?
What was the first one?
So the first one was understanding what's happening to machine learning communities
because machine learning and GitHub.
So I had this idea for this talk like two years ago,
a year and a half ago, one of my colleagues on an analytics team
and a pink bench was like, well, what do you think about doing this?
And she pushed me, pushed me.
And I wrote half of it and then work calls,
because it's not actually work.
This is just like brain twiddling.
Like, ah, let's see.
Yeah.
Because we realized that machine learning has a field as existed for a very long time.
Right.
But as a somewhat commercially viable field with real tooling around it,
started its ascendancy around the same time of GitHub's invention.
And so did GitHub help ML grow?
Who knows?
Or maybe it's the other way around.
We don't know.
But we do know that a lot of the stuff around GitHub machine learning communities
have happened in GitHub.
And we have that data going back 10 years.
So we can actually do like a longitudinal retrospective study
of the evolution of machine learning communities in GitHub.
That was the first talk.
And that talk was for the contributors themselves to TensorFlow.
OK.
So quick takeaways on that talk.
What did you find in that study?
Two major things happen.
We have 2010, second learn, pandas, ipathun, Kaggle,
all these things were created.
2011 I think is that it might be 2010 to the Stanford AI course around that time.
Yeah.
100,000 people log on to one of them on machine learning.
Including yours truly.
Oh, good.
Yeah.
Because Sebastian Trump, I remember when did I do that Mass Rover Challenge?
Uh-huh.
I did the Andering one.
So the Sebastian was different.
Yeah.
So why do I do this Mass Rover Challenge?
Yeah.
I was also a student.
My advisor, my teacher, that professor of that time was also doing the Mass Rover Challenge.
So it was like an academic thing.
Yeah.
So even I did nothing 100,000 people.
What is this?
Right.
And most people did that.
The following yet, Udacity was created.
Coursera was created.
Right.
So then you're starting to see machine learning become a commercial,
a viable business enterprise.
Mm-hmm.
No one could just academia and NASA.
Mm-hmm.
They're actually human VCs.
Putting money into this thing.
Right.
So it was the first sign of the AI we until we had had thought.
Right.
Now it's a real thing.
And so it started picking up, started picking up, started picking up, picking up, picking up.
Then Andrew Ying does the cat thing at Google, Google X was it?
Where they trained those neural networks to do it, to learn a representation of cats on YouTube.
Right.
I think there was a TED talk and all this.
And everybody was like, oh my god, it's coming.
Then we see another great rise.
And they are no cafe.
All these things are created.
And then Google.
And they got all of these, these, the rise and these inflections that you're speaking to is based on.
Stars and get up.
Right.
So this is using GitHub stars as a proxy of human interest in these packages.
Mm-hmm.
And I did an analysis of quarter of a quarter growth of stars on GitHub repose.
Okay.
Of all these packages from 2019, all the way to 2019.
To actually see if they're, what is the growth of stars on these packages from a quarter to quarter?
Mm-hmm.
And we notice a trend.
I notice a trend.
How big is just three cycles from 2012 Q4 to Q1 2013.
There is a massive spike.
We see the same thing I think around 2015 or in that 2015 to 2016 or 2016 to 2017.
Another massive spike.
The third cycle, which will be around 2019, no spike.
So it's only 10 years worth of data.
Right.
No spike.
And for the first time, if you look at all the stars of all the major repos in machine learning,
we're talking about your TensorFlow's pie torch, cafe.
Right.
You just boost, cycle learn, Jupyter, and so on and so forth.
Interesting.
So the question is why.
Is this the beginning of another AI winter?
I don't think so because you look at VC funding.
I don't think it has accelerated at all.
And if you look at the dot AI domain,
these things are just going up. One more people are going to nerves.
Nerves are longer and academic pursuits.
Right.
It's not becoming like, I don't even know what it is.
So it's not necessarily slowing down.
So somebody asked a question.
I said, did you look at the production framework repos?
Things like Kubernetes, cube flow, things TF serving.
We looked at those ones.
And I realized I hadn't done that.
I'm going to go back to the office and look at that to see maybe those is where that's where we're putting our energy
because we're actually putting things into production now.
Right. Right.
Or is it that it's longer trended because it has become a permanent part of software engineering?
Right.
It's longer trend.
It's just is.
Or it could be that we've solved all the problems.
We've not solved any problem.
Did you come here and I'll tell you my vehicle?
I don't think so.
Exactly.
And what's interesting about these three phases is that they map very cleanly or directly, I guess,
is the better word to my own experience.
Right.
So in that, I think it was 2011 time frame when Andrew did his course.
That was that first wave.
He's first wave.
I was in that wave doing that course and then in the 2015, 2016 wave,
this podcast was started out of kind of feeling the energy that, you know,
is this inflection point that you're talking about?
And, you know, maybe the silent echo, we call it like this last, you know, these last couple of years.
Now, right?
It's this, yeah, Q4 2019 going to cute.
Maybe, maybe something's going to happen over Christmas.
Right.
Well, I think what's happening from my perspective, what's happening is that a lot of the energy and investment
is going into making it real and productionalizing, operationalizing, a lot of the experimentation
that's been happening.
And so I'm very curious to see what you find around these frameworks.
But even then, I think, and what I mean is the higher level like the production framework.
Yeah, the production frameworks.
Yeah, the production frameworks.
But even then, a lot of it is people like building their own stuff off of the low level things
and just kind of demonstrating value of first kind of building that kind of smooth glide path
and then using that to demonstrate value.
And it could be that, you know, what we're seeing now is, you know, ultimately the growth in kind of the broader market
is people kind of reaping value.
And a lot of that value is internal and not shared.
So that's what I was also thinking about.
Maybe it's actually going to private repose in enterprises.
And so we don't look at any private repose.
But one thing, I don't think we're going into a winter.
But I do think we have a solid five to ten years to say that machine learning is not a trend though.
Because some of the challenges are our lovely friends at the media have written all these stories,
this wonderful stories, everybody has high expectation.
In five years, I expect to, I am hoping the car I have now is the last vehicle I will ever own.
Because I'm hoping that by that time there should be an autonomous vehicle that I hope car ownership would actually go away.
Maybe I just buy some Uber credits or some company of the future thing.
And I just have transportation solved.
And the idea of me as a human being owning a car driving it becomes a hobby like polo.
If I want to drive, I want to track.
But it's not like a transportation thing.
That is my idea of what the next five to ten years is.
If that doesn't happen though, machine learning might then just have become truly a fad.
That'll be the downside.
But I doubt that will happen.
Because there's too much.
Look at Google search.
Burp is already helping with Google search.
There's a real real there.
But I want to see other companies outside of the major Silicon Valley.
The big ones really have a real win with machine learning.
Not Google, not Facebook, not Microsoft, not GitHub.
I want to see a company that doesn't exist today.
And in five years from now revolutionized the world.
That for me will be when we've truly arrived.
Yeah, yeah.
I am very confident that we will see that.
And we probably already know some of those companies.
And if they, you know, they're probably already starting these projects that are going to do that.
The other thing that you, the other thing that what you said made me think of is this
kind of the hype cycle technology adoption hype cycle that I think Gartner popularized
but or created.
But it might have come before them or been someone else.
But he had an inevitable part of this hype cycle is the trough of disillusionment.
Yes.
Right.
So the peak of inflated expectations is kind of maybe where we were in 1516 or where we are today.
Who knows.
But at some point, you know, the hype kind of catches up to the market.
Yeah.
Right.
And then, you know, there's a bit of a retrenchment, not necessarily a winter.
But it's, you know, people have a contraction.
A bit of a contraction.
A bit of a contraction.
People working hard to kind of create value.
And then I always forget what the next one is.
Plateau of productivity, I think.
That's where you get value out of it.
And I think we're kind of, you know, the different,
the different sub technologies of ML&AR in different places on this curve.
Yeah.
But I don't think we should be too surprised that there is a trough, you know,
and that we come out of that trough and, you know, ultimately give value out of the stuff.
I hope so.
I really hope so.
Yeah.
So that was the first presentation.
That was the first presentation.
And what was the second presentation?
The second presentation is about machine learning as a product itself.
Okay.
One thing we've noticed is there's so many hobbyists and people really, really getting
the feet wet into machine learning.
And they don't have any path to production.
They're not inside of me company.
They're an individual contributor inside their home right in the model.
And they want to move forward.
And so we realize that we actually have a path to production within GitHub data app ecosystem.
Because we have something called the GitHub marketplace.
And in GitHub marketplace, you can put an unverified GitHub app on the marketplace.
And so my colleague, how I'm all realize that, whoa, I could actually build an ML model
and put it in a GitHub app and push it to marketplace.
And people can actually use it to solve problems and get up itself, especially for open source.
So independent of ML, what are the kinds of things that I might find in the GitHub marketplace?
There are lots of apps.
I think they can ban apps.
But there are lots of app around software engineering.
Yeah.
Just the software engineering process workflows.
There's all kinds of apps.
Okay.
Like maybe you open an issue or a new pull request and something says, hi, or something like that.
They're just any kind of thing they can hook into a GitHub web hook.
You can build an app around.
So there are people who use a lot of stuff.
Okay.
Got it.
And so we realize that we care a lot about open source.
We also do a lot of work around natural language processing.
The kinds of stuff that we are tinkering with, experimenting with within GitHub itself.
One of the ways we can move our experiments forward is actually maybe something is not
all the way quite ready to be a full-on product yet.
You need to work out some of the kinks of it.
One thing you can do, a look-a-metement way to get it continued to go, is to actually push it on the marketplace
and have users use it.
So you can learn the utility of it, you can iterate on it, and anybody can do that.
Especially when it comes to machine learning itself as a community on GitHub.
Like we've been talking about the hype cycles.
What those things actually mean is people's weekends and weeknights.
The maintainers of this of the same frameworks, spending inordinate amount of hours working for no pay
often to build the things that we all use, we take for granted, somebody built pandas.
Somebody built non-party.
Was kidding in that guess.
And sometimes they built this before they joined the corporation.
We write a Python thing, you put your import pandas as NPT.
Somebody's put time for free often to make that happen for you.
And that person is one person having to deal with 600 contributors.
So the amount of maintainers to contribute or flow, it's just not tractable.
So sometimes you don't get the kind of response you want in open source,
which affects the health of the open source community itself.
One thing we realized very, very early on is that machine learning can actually automate a lot of those challenges away.
Machine learning could actually triage some of your issues for you by putting levels that are pushing it to the right person.
And you as a person in the ecosystem can get access to get up data to get up archive,
which is actually inside of Google BigQuery.
You can extract information.
Meaning the code data or the lifecycle data or the behavioral data.
Right.
The behavioral data.
You can use that data to tray models.
We now have code data.
We publish code data in something called code search net.
We just did I think last month. So we have like six million snippets of functions that we've cleaned for you.
We've parsed in multiple languages.
We have JavaScript.
We have PHP.
We have Python.
We have Java.
So we have all these data sets that you can actually start using to build models.
And then better yet we have get up apps that you can then push your app into production.
And people can use it and give you feedback.
And you can be a virtual circle cycle for helping keeping the health of our ecosystem alive.
And we had our partner, Jeremy, who works at CUPLU, who's one of the lead developers of CUPLU,
who is a customer of the issue level bot that, how my colleague wrote.
And so we're talking about that entire virtual cycle.
Like you want to really start contributing to machine learning.
Start by building this apps to help keep the health of the machine learning communities themselves good.
And we're using TensorFlow to do that.
You've got these data sets that you make available both about code,
not to mention the code that's on GitHub itself, but these data sets as well as the interaction information.
And you provide the ability for folks to create applications that interfaces with or interacts with rather the GitHub users and repos,
you know, things like, you know, tagging, uh, tagging issues and.
Notifications. Any kind of event on GitHub.
Yeah.
Whatever it is.
Somebody start a repo.
They could write something to just interact with that.
Somebody forked your repo.
You could just say, oh, thank you. What do you want to use it for?
You could write a bot to do that.
Yeah, right. So it's, I was going to say it sounds a lot like a bot.
Like do you think of them as bots essentially or are their bots?
Yeah.
Bots can have ML in them.
Right.
It's a way to get something to use this.
It's a, it's a path to build their products.
Yeah.
So it's, so as opposed to, you know, if you, if you are playing around with machine learning and want to do something interesting,
as opposed to building your own website or building a UI, something with a UI and having to deal with all that,
you can build a GitHub bot and you've got this marketplace of people that you,
that can connect to your GitHub bot and kind of take advantage of it.
Yeah.
And if you want to, you can get a verified, I don't want to make money.
Yeah.
Nothing stops you, but you can actually start adding value.
You can understand that machine learning is not some delitant pursuit.
Right.
That it is a set of technologies to solve real human problems.
And you can start doing that immediately.
You don't have to wait to be part of a company and then do your ML within that company and then push it to users.
You have the power now to do end to end.
And so did you go into, like, how technical was the presentation?
Did you go into some of the, you know, the details of how you would go about doing that?
Not really, but we have lots.
We're going to release the slides.
We have, if you don't know how to write in flask, there's a course that we recommend.
We put all the events there.
We put snippets of the queries that you can use in BigQuery.
So we get everything that you would need.
We pointed to all the right sources and tutorials that you can actually recreate this.
And they should labor about itself as open source. So you can see every single thing.
So let's talk a little bit about the issue label bot is.
It's a bot.
It's responding to an issues created and it's, you know, has access to the content of that issue.
Yeah. And it's liberated as a bug feature enhancement question.
It's just adding labels to it.
And something as simple as that is so powerful in giving maintenance back hours.
And so the idea is, so GitHub has its own kind of automated issue labeling that it's doing.
Correct or no?
No, not really.
We don't have the automated issue labeling just yet.
Okay.
We're starting to do the automated issue level.
So you think your group, you mentioned earlier, your group is kind of thinking about how you might do that at the scale of GitHub.
But if someone wants, you're not doing it because you can't do it.
It's, you have to deal with the scale, but someone could take this issue label bot.
And are they, what does that mean to take that?
Did they have to take that and then plug their own model in or what are they doing?
No, they don't plug their model.
They just take it.
It's a connection.
They get a repo and the budget starts working.
And what is the bot able to do?
What labels is it able to, is it?
Those three papers.
Really labeling?
Yes.
The issue, it says, is this a bug, it labels it as a bug, if it's a bug, if it records nice, it predicts.
Oh, this looks like a bug.
Okay.
Somebody said not working.
Yeah.
Then it opens it, it puts an issue, it puts another comment in the issue.
This looks like a bug.
If you think it's a bug, say thumbs, thumbs up, or thumbs down.
If you say thumbs up, it adds the label bug to it.
So it does its prediction and asks you to verify.
And you can say, yes, it's a bug.
And that just reinforces it learns.
So it just helps to triage for you.
Got it.
With those simple labels.
Something as simple as that.
The use of the bug and whatever.
The use of the bug, enhancements, or question.
Got it.
So maybe it's a feature request.
I would like this to do that.
Or how do I, whatever, question?
Those three simple things.
And that gives maintenance back hours.
Because this is what humans have to read.
And say, okay, I'm going to, somebody's job was just...
Tagging news.
Yeah, yeah.
This is what everything that we do.
This is how these things get done.
Right.
It's somebody doing all this kind of work.
Right.
But that person, if we can give them back their hours, it's great.
Right.
And especially for building a robust ecosystem of machine learning,
GitHub is the home of all developers, including machine learning developers.
The virtual cycle is you can actually do things to make the ecosystem more robust.
Mm-hmm.
And solve our own problems by leveraging.
If you, by thinking of GitHub as a platform,
instead of just could repository hosting,
it's also a platform to plug and build things on top of.
Got it.
So we ourselves and the ML team,
we build things on top of that platform as open source.
And it's just a way to also just play with things they're experimenting with.
Like, if you're writing a little experiment,
why not open source it to see if it's actually useful?
And maybe it's this useful, they add more time to it,
and you actually build it all the way up.
Yeah, yeah, yeah.
With this issue label bought, it's something that you can immediately start using
and get some value out of.
And it sounds like the cube flow team is using that.
The cube flow team is using it.
Okay.
And they're getting a lot of value out of it.
The person that wrote the bot is downstairs, Hammer.
Yeah.
Here at the bot, Jeremy is also with us in the talk.
He's using that bot to triage.
Yeah.
He's environment, he's a maintenance, he's project,
because they want to keep the health of that project very high.
Right.
They want people to continue to contribute.
And they want to turn around to actually getting things done.
Right.
Nobody wants to go to an issues, listen, see a bunch of issues
that haven't been responded to.
Exactly.
And tagging is the first step in.
That's the first step to figure out where should he go.
Right.
Right.
And so, but this is not just a tool.
It's also an example for other folks that might want to build their own tools.
Absolutely.
Using machine learning.
Yes.
That connect into the GitHub experience.
Yeah.
This was just saying, hey, you want to write code.
You're doing ML and you care about developer productivity.
Or you care about doing stuff around ML on code itself.
Maybe you want to write that plan or have an idea of automated pull request review.
Here's a code base for you that we've cleaned up.
We've built some things ourselves wherever is baseline of what we think the model is.
We've done something here is the baseline we got.
Maybe you want to work on that and beat that baseline.
Right.
And then publish it.
Like get the things down faster.
So I'd be remiss if you just mentioned ML on code.
I'm curious about.
I'm sure others are curious about the generative side of this.
Is that something that you're exploring in your group?
So I am not exploring generative side of it.
I think it's something that we've had ideas on.
To me, that would be the ultimate moonshot.
Yeah.
And part of that code challenge was the GPT2 for things like that.
Yeah.
Exactly.
That is the ultimate moonshot.
So people have said I do work around that.
The thing has to be good enough to be able to compile.
So there's a version of this.
I mean, but you've got that's like an adversarial thing.
Yeah.
But a version of this could just be.
Could completion.
Yeah.
The challenge is so we are not necessarily an IDE.
Right.
From a get up perspective, if I put on a hat of like a get up product manager,
we're not necessarily an IDE.
Right.
The person that would really, really rock and roll on that stuff is the person that is doing an IDE.
Yeah.
Because who is the, sometimes as technologists, we fall in love with the technology.
And now we're trying to shoe horn it in.
Who is the customer for that technology?
Everybody ever sucks about.
I would like to just say I want to do this and it writes the code for me.
Even if that's where you want to get at, who's the customer for that?
Is the person that has like an IDE or some environment where you write code?
We don't necessarily, maybe in the future, we'll right now write code within GitHub.
So from a generated perspective, I guess I can, I can see that from our managers.
I'm not going to give us like the thumbs up to grab the chasing something that we can't,
like that's not our job.
We should be doing other things that are, we are not researchers.
We are not a research.
Right.
Our shop is built something within a 12 month horizon to help our customers today.
It's very, very quick.
We have that those that you are in a position that not many organizations are in terms of the volume of code that you have access to.
Indeed.
Google also has code.
But we don't have as much of all the different languages as GitHub does.
Right.
And structured metadata about the code.
So there is, it does seem that there is a certain, part of this is, do you have the market?
And maybe the IDE has the market for this feature.
But they don't necessarily have the code, right?
They couldn't.
Yes.
Right.
We don't have the code.
So where does GitHub fit?
Right.
I think this is the beginnings of code search net.
If they don't have the code, could we provide some of the code for them?
And so this is code search net is the.
It's a code challenge.
It's a code challenge.
Okay.
Replete with a data set filled with code itself.
Past snippets of code and the sequences of code.
And they're docks strings in natural language.
Put out there to accelerate development on this issues.
Understanding that we actually have customers today that we need to be building stuff for.
Right.
But we also want this long term horizon.
So maybe we have this.
Let's just see if anything interesting happens in this petri dish that we put out there.
But I will make sure that when I get off this talk and I talk to my manager and our VP,
I will tell them that our community might like us to start doing this.
And so if they give us the thumbs up to chase that, then perhaps there's a GitHub research that is created.
And get a research that is tossed with doing this kinds of work.
Yeah.
I mean, maybe this is the beginning of GitHub research.
Yeah, maybe it does seem like it does seem like there's this asset available in open source code
that you're uniquely positioned to take advantage of.
And who knows what the, yeah, it's kind of classic innovators dilemma.
Like this is our current business.
So we kind of have the blinders on or focused on his current business.
But you also are in this very unique position to kind of leapfrog that and do something potentially interesting.
Yeah.
And GitHub machine learning is all of almost three years old.
Yeah.
January 2017.
Yeah.
So that's also what you have to remember.
Right.
Right.
So very different.
I was just curious.
Very different problem for sure.
With the code challenge for those that don't have a team that does ASTs sitting right next to them.
Are there kind of other kind of off the shelf tools that folks are using a play around with that kind of stuff?
There are many parsers that are available that have their own version of ASTs that can parse ASTs for you.
So I can mention any of them off the top of my cough, but they are available.
But that's the thing.
Like look for ASTs and parsers and kind of mashed up.
But we don't know yet.
We don't know is this is research.
So get all the representations.
If I were doing this, I'll get the code represented as an AST.
I will represent the code as natural language.
I'll just do everything.
Yeah.
And then do the science and see which one gets you further.
Yeah.
Yeah.
And then, you know, put your.
Join the leaderboard and submit your model and join the leaderboard.
And I actually see how far you perform.
Like a Netflix prize kind of thing where you've got a million bucks.
I don't think there's a million bucks.
I think there's just bracket rights of I did this.
Okay.
You're prizes.
You're helping your community.
Right.
Awesome.
Well, I'll mold you.
It was so wonderful to finally get a chance to chat with you.
Thanks so much for taking the time to be on the show.
Thank you for having me.
That's our show for today.
To learn more about today's episode, visit twomalead.com slash shows.
If you missed twomalcon or want to share what you learned with your team,
be sure you visit twomalcon.com slash videos for more information about
twomalcon video packages.
Thanks so much for listening.
