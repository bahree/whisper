Alright everyone, welcome to another episode of the Twilmal AI podcast.
I'm your host Sam Charington and today I'm joined by Vidyut Napire, director of AI and
machine learning at PayPal.
Before we get going, be sure to take a moment to hit that subscribe button wherever you're
listening to today's show.
Vidyut, welcome to the podcast.
Thanks Sam.
Happy to be here.
Big fan of usual.
Thanks so much.
I'm really looking forward to this conversation and learning a bit more about what you're up
to both from the perspective of the hat you wear leading applied ML and AI at PayPal but
also some of the work you're doing supporting ML ops and the machine learning platform
teams there.
But before we jump into all that, I'd love to have you share a little bit about your
background and how you came to work in the field.
I joined PayPal three years ago and I was, before that, I used to work in the autonomous
driving area.
I used to work at a company called Neo where my team was working on building kind of very
interesting anomaly detection algorithms for catching crazy driving scenarios out there
in the field.
Before that, I used to work at Qualcomm where I led several interesting projects in modern
design, sensor system design and so on, that's where I started getting my hands and feet
wet with AI ML if you will.
For the past three years, I've been at PayPal where I'm leading, as you said, the AI R&D
function applied AI R&D function and in addition, off late, I also started leading some of
our ML ops efforts in this area.
So really, really interested in sharing what we've found in our journey so far.
Awesome.
Awesome.
Just thinking about some of the places you've been on your personal journey, you kind
of started out in these very, you know, edge scenarios, hardware driven scenarios and
I'm imagining that PayPal is not like that at all.
I'd love to hear you kind of comparing, contrast some of your prior experiences to what
you see now in PayPal.
Yeah.
That's a great question.
Actually, you know, as you rightly pointed out, when I was at Qualcomm, you know, it's
a very different vertical, very different space and the kinds of problems that you encounter
there, you know, have more to do with having very, very limited access to compute, limited
access to memory, you know, so the focus there entirely is on how do you kind of build
the most, you know, energy efficient algorithms from an AIML perspective versus when you move
into a company like PayPal, which is primarily kind of like a cloud-based, you know, platform,
it's a very different situation altogether.
I mean, you're no longer strictly limited by the amount of compute, memory, resources
you have.
It becomes a slightly different environment where you are trying to, you know, bring
AI ML to production, so, you know, just as an example, right?
Like, you know, when at Qualcomm, when we were building, you know, various algorithms,
you know, in the sensor area, right?
So this is where we are trying to use Excel's and gyros, you know, to figure out whether
what kind of activity a person is engaged with, with sensors.
You know, we have a very, very small power budget, energy budget to work with.
So we really, really have to focus on making our algorithms very, very efficient.
In PayPal, it's kind of like a different, I think here it's more about how do we, you
know, make sure that we are deploying the algorithm to solve, you know, to improve performance,
but cost is a consideration, but it's not the most important thing, right?
So it's, it's an interesting, it's an interesting set of experiences I've had.
And then autonomous driving kind of that area falls somewhere in the middle of both of
this, because, you know, on the one hand, you're still doing all of your inferencing on
the edge, because ultimately all your computer vision processing has to happen on the car.
But, but so in that sense, it is energy constraint because you have to do all of that, you
know, kind of battery constraint environment.
But on the flip side, the performance that you need, the bar that you need is very high.
And also, the complexity is extremely high, right?
Because you're processing very, very high bit rate video streams from multiple cameras
concurrently, right?
So the amount of compute you need to solve those problems is, you know, is much higher than
what you would, what I was dealing with when I was at Qualcomm, for example.
So it's just like each area has added its own, you know, difficulties and challenges
to deal with, but it's also made me aware of, you know, very, very interesting perspectives
on solving the same problem.
You know, and I think about, again, financial services or PayPal in particular, fraud is
probably the first thing that, you know, jumps to mind for me and for other folks.
What are some of the other use cases that kind of fall in your portfolio and that are important
at PayPal?
Yeah.
It's a good segue into, into kind of what, what overall data science at PayPal looks
like.
Before going there, maybe I'll just spend a few minutes talking a little about PayPal,
right?
Like what we do, you know, and where we are at in terms of scale and so on.
So as you know, PayPal is probably one of the most widely recognized kind of payments,
payment services company, we have last count, maybe around 395 or so million customers,
maybe another 30 to 35 million merchants on the platform.
So basically what we enable is this two-sided network where we have on the one hand, you
know, all these customers who can use PayPal products to move money, to manage money.
We also offer credit services to customers, for example.
And then on the merchant side of the house, we have, you know, our checkout product.
We have, we have several other, we have a whole laundry of products, right, where we enable
our merchants to improve their own metrics, their own business metrics.
We also help them with risk and fraud management, for example.
And last but not least, we also provide merchants with credit, right?
So as and when they need it, also not to forget, but it's, it's probably not widely known
probably to your audience, but Venmo Zoom, they are also actually PayPal products, very,
very popular.
For example, Venmo with Gen Z, and you know, Zoom is a cross-border, cross-border remittance
is right, like super popular.
Zoom with an X, not Zoom with a Z.
Zoom with an X, exactly.
And several more, right, brain tree, for example, which is, you know, a payment gateway solution
he offers, yeah, which not just a caring payment, but basically it's a payment, you know,
gateway for merchants, right, similar to what stripe offers to merchants.
So anyway, so I think the first thing to understand is PayPal is a very diverse ecosystem with
a lot of different services and products.
And you know, each one of them have their own, I would say, flavor of AIML, you know,
embedded within them.
But again, just zooming out, if you, if I were to broadly categorize all of the AIML,
I think driven products, I think the number one would be definitely fraud prevention, that
is the area where we have the most advanced, most complex AI systems in use and deployed.
After that, you know, there is credit.
So several, several of our credit product also is powered by AIML, especially in the
underwriting process in the marketing of those kinds of products to prospective customers
or merchants.
Then there is things like sales and marketing, you know, when we are trying to essentially
cross sell or upsell, you know, existing products to an existing customer or merchant,
we will typically try to use AIML to identify those customers who are most likely to, you
know, basically propensity models, which will help our business teams to identify the
best set of next customers to target on the both sales and marketing side.
There's also customer service, right?
So we, in our, you know, PayPal is a big company, right?
And many people reach out to us whenever they have some issue with the transaction that
didn't go through or a transaction that was put on hold or stuff like that.
So people contact us a lot of the time.
We actually use NLP bird-like models in many of these use cases to identify, for example,
in a chat context, the intent with which potentially a customer is calling us or contacting
us.
And then there is compliance.
So again, this is where the regulatory side comes into the picture.
So, you know, we are expected to catch, you know, any kind of illegal activity on the
platform, things like money laundering or drug trafficking, and so like there are all
these kinds of use cases that we are supposed to catch and flag to regulatory bodies.
So we again use AIML there as well, because ultimately, if you map all of some of these
problems, they all become AIML problems at some level.
And again, this is just, I would say that probably the top five or six use cases, but there
is many, many, I would say there is again kind of like a longish tale of other areas within
PayPal where there is a lot of exploration going on around where AIML can be used to solve
a certain business problem and so on.
So hopefully that gives you a full view of what AIML looks like in PayPal today.
No, that does.
And now if you could then kind of use that as a segue to map to your applied R&D portfolio
and you think about the projects and investments you're making there and what you think is most
promising for the sets of problems that you have in terms of approaches.
Yeah, it's a great question.
So I'll probably provide my perspective on how I look overall at kind of the area of R&D
where the PayPal director and R&D hat, right? So for me, like the overall AIML landscape,
like I break it into four categories, there's kind of like the hardware compute layer,
you know, where I feel there's a lot of innovation happening.
Then there's the core algorithmic layer, which is kind of the focus of most of the academia
and the Facebook's and Google's of the world.
And finally, there's the application layer, right, where a customer is interacting with
the product and he gets some experience driven by AI, right?
So each of these areas, I feel there's a lot of innovation happening.
And so my role as an artist, say four, yeah, sorry, fourth one is tools frameworks and
platforms, right?
I thought you were going there, but just, yeah, I missed it, sorry, thanks for catching it.
So the tools frameworks and platforms are, you know, this is where I look at the overall
ecosystem, right?
And this is where, for example, MLOPS enters the picture, right?
So how do we make sure that not just each cog in this wheel is doing well?
But as a whole, how does everything kind of work well together in the most efficient
manner, right?
So MLOPS, things like interoperability, platforms, because, you know, there is a, you know,
there are all these different machine learning frameworks, you know, there's TensorFlow,
there's PyTorch.
Now, how do we actually, on X, how do we, yeah, so on X is, for example, something that
we are investigating for our use cases.
So now, let me kind of like, you know, kind of like drive deeper into three main, like
I would say, areas of R&D, right?
The first is the kind of hardware compute layer.
Here I think of two primary initiatives where we already made some investments, we are
really, really hopeful about what, where this is going to go.
The first one is this whole idea of federated learning and how do we essentially benefit
from being able to offload some of our inferencing and hopefully in the future, the future training
workloads directly to the customer on merchant devices, right?
So we have some ongoing work in this area where we feel it's the right time.
It's going to be very important from a cost management perspective, also from a privacy
and regulatory perspective, that the closer we move the compute and the lesser we move
data to our data centers, I think the better it is for everybody in the ecosystem.
The second one is around how, again, this is something that we are extremely positive
about, which is how do we essentially make sure that we use the data that we use, right?
Fundamentally, the data layer and the compute layer in the most efficient manner.
And for example, this could be things like, you know, how do we, for example, store data
at 16-bit resolution, do we really need to store data at 32-bit resolution, do we really
need to do compute at 32-bit, can we do it at 16-bit?
Because again, the impact, the impact that this has on the overall cost of building an
AIML solution, especially for a company like PayPal where we have such humongous amounts
of data and to deal with, right?
So we are investing in this area as well, like how do we optimize our compute layer
and how do we optimize our data layer.
And then, so those two, I think, are the kind of, I would say, primary two things we are
focusing on.
Yeah, the hardware silo, we have, for example, done some pilots in the quantum computing
area.
We have done, we are also looking at, for example, in the area of graph compute, right?
You know, what is the best possible hardware layer for graph?
Because as you know, graph computing, I think the underlying hardware ecosystem today
is not optimized for graph because of the fact that, you know, the memory access patterns
when you are basically trying to do graph compute are not well suited to today's, I would
say, computer architecture.
So, so in this particular instance, for example, we are, you know, working with some external
partners trying to understand, you know, where this might go.
But there's several different lines of things at the hardware acceleration layer that we
are also considering.
So kind of in the context of hardware, you're looking at federated learning, I'm curious
when you think about federated learning or most of your efforts focused on what I think
of as kind of the algorithmic elements of federated learning, like differential privacy
and privacy preserving machine learning or more like deployment challenges, logistical
challenges of just getting models, you know, you know, model slash data to and from devices
and coordinating all of that, you know, distributed training, things like that.
Yeah.
So it's a journey for us, Sam, when federated learning, you know, has many components,
pieces, some of them, as you said, more AIML algorithmic in nature, some of them more
engineering and infrastructure oriented.
So in terms of our journey, right now, we are focusing on first, you know, it's kind
of like, you know, crawl first, then walk, then run right.
The first goal is start to actually do inferencing of some models, you know, on the device.
Gradually, as, you know, in parallel, of course, as you said, it just doesn't stop there.
We have to start thinking about, you know, differential privacy, can we, you know, how
do we make, you know, our models differently private, you know, that's one angle to it,
distributed learning where the full, uh, ambit of edited learning actually is also eventually
doing training on device.
Um, but I think that's a bit far out, uh, from our current, uh, from where we are right
now.
But certain things, there are opportunities here, uh, to do that.
I would, uh, I would just say that, uh, that's where we are at basically, uh, in this particular
area.
mentioned a couple of the kind of accelerated hardware
acceleration opportunities that are out there.
Is there anything in particular that you're finding
exciting or interesting among the things
that you've been looking at?
I would say there are definitely interesting things,
but I feel there are a bit like from an overall technology
maturity standpoint.
I think they're not quite there for prime time.
For example, our recent work on quantum computing,
let me just share some context.
We, one of the steps in basically the AML workflow
is feature engineering.
And one of the sub steps there is feature selection.
So let's say you have a catalog of features
and you're trying to find the best possible subset of features
for a given ML problem.
That problem is fundamentally a combinatorial optimization
problem with the complexity of the number of features
that we have.
Today we use greedy approaches, traditional classical greedy
approaches.
So one of the POCs we did on quantum was,
and this was with some external vendors like IBM and D-Wave
in this space.
And what we found is that we didn't get exactly
very, very conclusive results that demonstrated
that the feature selection that you can get with today's
quantum computers can actually outdo classical approaches
yet.
So that was a sobering, I guess, a revelation for us.
And we feel OK, so there's a few more years
for this particular area to evolve.
That's a specific case in point.
But I feel that's a kind of broader industry trend.
I know there's a lot of startups right now
in overall the hardware acceleration space, companies
are building custom silicon to accelerate AI workloads,
either for training or inference.
But we don't yet feel the need to go there yet,
but maybe in the future.
So that's probably the, I would say,
in terms of overall hardware space,
that's probably the closest to, from a technology
maturity standpoint, custom silicon for AI workloads.
But I think that probably is not what
we are the most interested in right now.
And I've got to imagine if you're a significant user
of cloud computing, you also are, to some degree,
banking on the cloud providers, investments
and custom silicon decreasing your costs
and providing better performance.
So maybe it's not the thing that you
need to spend your efforts for focused on.
Spot on, again, absolutely right.
I think for, especially for quantum,
right, like again, as a case in point,
I think with quantum, it's a kind of steep learning curve
to understand how quantum computing works.
And so we actually went ahead a little bit
and because it's a long lead time activity,
we decided to do some work by ourselves.
But as you said, maybe in a few years' time,
quantum computing and all these graph and other hardware
acceleration silicon might all be abstracted away
by cloud providers.
And we kind of don't have to worry
about all of the early technology maturity issues
that otherwise, you know, that you face,
typically when you go down to the hardware layer.
So absolutely, yeah.
What was that second area that you mentioned?
Was that algorithms?
Yeah, algorithms, yeah.
So in the algorithms bucket, Sam,
I think we have, as you can imagine,
the sets of things that we are doing here
are pretty very familiar to your audience,
things around representation learning,
sequence to label learning, transformers.
How do we use these in our most complex use cases?
So this is one area of focus for my team.
Another, like, I would say,
increasing area of focus for the team is,
you know, around causal ML, causal inference.
I think this is, there are two primary use cases here.
One is many problems that we encountered
typically in the marketing area.
You know, they are more causal in nature
where you're trying to influence behavior
of customers and merchants by taking certain actions.
So we believe that instead of looking
at these problems as predictive problems,
they are much better formulated as, you know,
treatment and effect kind of problems, right?
So we're doing some early research here,
partnering with some teams to, you know, to figure out
if there is, you know, an improvement in performance
by changing our approach to those problems.
The other interesting application of causal ML is,
in what we believe is essentially identifying causal features,
right, in going back to that feature selection problem, right?
A lot of today's feature selection problems
fundamentally are rooted in correlation, right?
You're trying to understand how features are correlated
with labels.
But I think with causal ML, there are approaches,
which have actually been shown
where you can try to learn more causative features
for why a certain thing might be happening,
which in turn helps with making our models more robust, right?
Because to our earlier discussion that we were having
around, you know, the fact that, you know,
data distribution is drifting and, you know,
as the data distribution is drifting,
so is the model score, right?
But if we can fundamentally identify features,
which are causal in nature, we expect the model's output
to be more robust to this kind of noise, right?
So this is another area where we feel causal ML could play
a big role, and it should help, for example,
with keeping, you know, our performance of our models
table over longer periods of time, then it is today, for example.
Are there any references you can point us to on the causal
features work?
Yeah, there is some prior work done by Professor Susan
Atty at Stanford, you know, who actually we had done a pilot
with her team back in 2020 on this area.
But again, that was one single pilot
with one single use case, so we are continuing
to explore other areas and other use cases.
But the basic idea is how do you identify causal features
using some causal ML techniques?
So moving on, maybe to the third area.
I imagine you didn't mention it, but I imagine
under algorithms you're looking at various graph machine
learning techniques as well.
Absolutely, absolutely.
We are looking at graph machine learning.
It's a big area of research for us.
This is where we are, you know, using GCNs to learn embeddings
for several kinds of use cases, like collusion detection,
like one form of fraud is collusion, where the buyer
and the seller collude to essentially gain the system.
So in these kinds of use cases, we have found
that, you know, GCN based embeddings can actually really
help improve the performance, you know,
to catch this kind of fraud.
But going beyond that, right, there is graph is such a rich
representation of our user interaction data, and coupled
that with the fact that graph ML itself is evolving so rapidly
right now in the academia, there's so many great papers
and research is coming out in this area,
that it's a continuous area of exploration for us.
How do we is evolving really quickly?
So we are exploring it in several context,
be fraud detection.
We can also, you know, another use case
that comes to mind is compliance, as I said,
like because there are very, very, when you look at how
money laundering occurs on a graph, it is almost,
you'll immediately see how you can actually use graphs
to catch it, right.
But the problem with graph, which I was
alluding to earlier, is the scale of the graph
that we have at PayPal.
We have like millions, you know, millions of customers, right?
And like billions of transactions happening every year.
So to model all of that, to keep that graph up to date,
to be able to train, you know, models on this kind
of evolving graph and so on, it's a very challenging problem.
So that's one part of it.
And then at the algorithmic layer graph also,
as I keep saying, a lot of interesting work happening
in this area.
So we are doing our own pilots in this area.
And just maybe taking a step back, do you
think about kind of applied R&D in the context of algorithms
as taking the stuff that the Stanford's and Google's
and Facebook's are doing from a research
and academic publishing perspective
and trying to understand how they fit into your problem domains?
Or are you also doing academic style research
into the problems that interest you?
Yeah, I think it's more of the former in the sense
we had an applied R&D group.
So we are not focused on kind of fundamental research.
I would call it, we are not, our goal
is not to, let's say, come up with novel problems
and publish them to conferences and journals.
It's definitely an applied R&D function.
In terms of how we, to refresh your question,
how do we kind of through our backlog?
How do we decide what to work on and what not to work on?
It's a combination of things.
I think we're definitely monitoring what the Google's
and the Facebook's are doing.
We are also monitoring conferences,
like top tier conferences, like ICML, ICLR and so on.
And trying to, I think some of the differences
that we see from academia, when you look at everything
from an academia lens versus a applied R&D lens
is, for example, number one, like a lot of AIML research
is focused on NLP, CV, unstructured, kind of,
I would say data, versus R is focused on,
I would say, some portion of it's structured,
some portion of it unstructured.
And I think the other thing is financial services space,
we have our own unique, hard, I would say,
attributes to our data sets, which are not, like,
typically found in kind of academic conferences.
So one of the key things that we and my team do
is really connecting those dots, right?
So you will see a lot of things that show up in CV
or in NLP land, but the key is like to identify,
how do you map, you know, let's say the problem
and the solution to our domain, right?
And to see if there is some potential, right?
It sounds very easy to say, but it's actually
very hard to do in practice, right?
So, but I think this is really the key,
I would say, role that we play in addition
to running the pilots and doing the R&D itself.
I think the key part is ideating and understanding
what trends are happening in these related,
what separate domains and how do we kind of like map it
to our domain, right?
That is the real secret sauce, I think,
to our group and what we do.
So the third area?
Yeah, the third area is kind of more
on the application side, as I mentioned.
So this is primarily, this is where I think, you know,
most of our focus area here is on the responsible AI side.
So things around explainability, fairness, privacy,
adversarial learning, you know,
how do we essentially look at these problems
and solve them for our customers, basically,
ultimately for our customers,
but also, you know, you know,
make sure that we are complying with regulations
and laws and we have a pretty, I think,
in the last year or two, we've actually taken steps
at PayPal to make sure that we really are, you know,
building our models in a very responsible manner.
You know, we have governance and I would say processes
in place to make sure that as and, you know,
as we build models, we are paying close attention
to how do we make them explainable?
How do we ascertain that they are fair?
In some areas, for example,
we are already doing it because it's required by law,
but we are also now starting to think about all the new areas
where we need to start paying attention to all of these things.
I already mentioned a little bit about privacy.
And then finally, on the security side,
we are starting to really look at adversarial learning
as, you know, how do we, for example,
in a front detection context,
how do we use adversarial learning
to make our models even more robust?
So ultimately, I think our customers interact
with us through the application layer.
And so we really need to make sure that everything
that we are doing as a customer sees it
is done in a responsible manner,
which complies with law and so on.
So this is an area that my team is heavily invested in.
I would say that would probably be the number one.
On the second area here is probably more
around the conversational AI side,
you know, where we want to build,
for example, chat parts, which basically delight our customers
in terms of giving them a very good experience.
And so here, there is some R&D happening within my team
where we are trying to look at how do we use techniques
and reinforcement learning, you know,
to essentially train a chat part
with all the previous chat logs that we have
of our customers with our agents, for example, right?
So that's a very interesting area of research for us as well.
And the fourth area was around tools
and ML ops platform technologies, that kind of thing.
Exactly. So I think that actually is the glue
that eventually tries all these three layers
on the top.
So definitely ML ops, interoperability platforms,
how do we make sure that both on the deployment side
as well as on the training side,
we have an ecosystem that you can train
in any framework of your choice, deploy on any kind of device,
be it like a phone or a browser, right?
So these kinds of things are also,
we have various pilots ongoing in this area as well.
And then the big one is ML ops, of course,
which we have been on our own journey, I guess,
but this is basically the place where we try to automate
as many of the steps involved in model development
and delivery to the extent possible, right?
Like that's the ultimate goal.
So this helps, as you said, with, you know,
making our development cycles shorter
and basically increasing deployment velocity, right?
So this is the primary objective of the ML ops area.
And is ML ops from your perspective
from your perspective, is it still applied R&D into ML ops,
meaning looking at kind of the, you know,
have you characterized the time horizon, you know,
hey, what are we gonna be doing in ML ops
three to five years from now?
And then exploring those things
and transitioning them to other internal teams
that are focused on, you know, one of the three years
or this year is that ML ops component,
like you have all this kind of forward looking stuff
and you're also responsible for the platform
that your teams use today in production.
It's a great question.
I think I would say it's a combination of both.
I think there is some part of that team's bandwidth
that goes into more forward looking R&D type of stuff.
So collecting back, for example,
to our first conversation on, you know, unsupervised
versus semi-supervised approaches to, you know,
for fraud detection.
So that team, for example, will look into something like that
because ultimately it's a way for us to improve,
it's a way for us to automate the process of a model refresh
by looking at a technique like semi-supervised learning, right?
That's the R&D side, but as you can imagine,
this is also a very kind of product and execution heavy function.
So I would say 50% of the bandwidth goes in kind of like
the day-to-day, you know, building off of the pipelines
onboarding, you know, new models and customers
onto the ML ops rails and so on.
But and actually, this was also,
by the way, the reason why this function, you know,
was synergized with the R&D group
because we already see that there is,
there's plenty of synergy in terms of the fact
that some of the work happening in this group is R&D nature.
And so, you know, it makes, it just made a lot of sense
for me to actually, you know, basically look at both
the R&D side of the house as well as the ML ops R&D side
of the house so that we can even synergize more.
Well, I feel like we need a whole separate conversation
just to dig into the details around your platform
and tooling choices and the way you think about
automating the machine learning workflow.
I know.
We didn't even get to it.
Even get to it.
That said, it has been wonderful, you know,
exploring the way you're thinking about applied research
and development at PayPal and some of the use cases
that you're looking at and the things
that you're excited about.
Even there would be great to go into a lot more detail.
So we'll have to make sure to, you know,
stay in touch and have some fall on conversations.
Thank you, Sam.
It was great to be on the show.
