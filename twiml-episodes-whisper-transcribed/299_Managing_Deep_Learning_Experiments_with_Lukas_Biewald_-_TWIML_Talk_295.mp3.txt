Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
We'd like to send a huge shout out to our friends at Waits and Biosys for their sponsorship
and support not only for this podcast, but our upcoming event, Twimblecon AI Platforms.
As you'll hear in my conversation with Lucas, Waits and Biosys believes that the more accessible
transparent and collaborative the process of model training becomes, the safer and better
the models we build will be.
To enable this, they offer an experiment tracking platform for deep learning researchers
and practitioners.
To learn more about Waits and Biosys and get started tracking your modeling experiments,
visit www.wndb.com, w-a-n-d-b.com.
And now on to the show.
Thank you for joining us for this week in machine learning and AI.
You know, so with that in mind, probably a little bit of history and your background
is probably a good place to start so we can start to kind of get the decoder ring in place.
Totally.
So, yeah, you know, I started my professional career at a company called Yahoo.
I don't know if you remember them, but back in Santa Clara.
Yeah, yeah, exactly.
Back in 2004, 2005, I was actually working to convert their kind of rule-based system
into a machine learning ranking system.
So you know, ranking search results is one of the first real applications of ML.
And then I went to a startup called PowerSet to do kind of the same thing that became Microsoft
Bing.
And yeah, this is back in maybe 2007, 2008 and power set initially based in Atlanta.
Am I thinking of the right company?
No, I don't think it was based in San Francisco.
Okay.
I mean, it was a search kind of a natural language search company.
It had a lot of great ideas.
Actually, a lot of the folks working there have gone on to do impressive stuff like the,
you know, the GitHub founders were working there and Descartes Labs is another big kind
of aerial imaging company.
It was kind of, it was a fun place to work.
There were a lot of super smart people there.
And it was definitely way ahead of its time, right, trying to do deep natural language processing
applied to search.
Yeah.
And kind of what I was seeing at both the bigger company and the startup was machine learning
had tons of promise, but tons of obstacles to kind of make it work in the real world.
And that's always kind of what's driven me is the applications of machine learning.
And so I founded a company originally called Crowdflower that became Figure 8, which was
all about getting high quality training data because that's such a bottleneck for building
and deploying machine learning models.
But one of the things that I saw when I was running Crowdflower and later Figure 8 was that
there's a lot of problems that happened downstream, right, as people tried to, you know, take
the training data and turn it into live production models.
And so, you know, I think the next problem that people run into once you have the training
data is figuring out a sane way to manage all the experiments that you do when you train
your models.
And so that was the inspiration behind starting weights and biases.
You know, I think every decade, maybe I'd do another step in the machine learning.
So I'm curious, what is the craziest thing you've seen in terms of the way people are managing
experiments?
Like, you know, I tell people all the time is I'm kind of laying out a, you know, landscape
of, you know, machine learning and the process and how experiment management happens.
Like, you know, sometimes you're better off if it's not even happening.
Like, I've seen like crazy foul names with like hyperparameters in them and stuff like
that.
Like, do you see that kind of stuff?
Oh, yeah.
I mean, I think back to, you know, I remember in grad school, actually, I was, I was fairly
sloppy at managing experiments, which is maybe why I am so passionate about this space.
But I had a huge single file that I added in EMAX and yes, sporadically put notes in
for all the things that I tried and I had, you know, kind of a crazy file naming scheme,
you know, that would kind of evolve over time, if you will.
But you know, actually when I was starting with advices, I, I spent a lot of time with
my friends kind of studying the different ways that people do experiment management and
actually think the most common approach is to have, you know, sets of directors, sets
of files.
And then these days, I think people typically use a Google doc and put notes for, for
each of their runs.
You know, I think everybody, you know, I've looked at lots of different people's, you know,
Google Docs or whatever they use.
And, you know, I mean, these are some of the most incomprehensible pieces of text.
Like, I always wondered if people could actually go back and figure out what they were doing
even, you know, a few weeks before.
Right, right.
So, yeah, I mean, I've come across everything from post-it notes to lab notebooks, to Google
Docs, to spreadsheets, to, you know, the crazy thousand character long founding with all
of the parameters dot pickle, you know, it's like, yeah, yeah, yeah, exactly.
And then you start calling things like dash fixed and then like, dash fixed, dash fixed again.
Right, right.
Dash final, dash final, dash final.
Yeah, yeah, exactly.
That's the real way that they pattern.
Right.
Right.
Hey, everyone.
Sam here.
Our conference, Chumul Khan AI platforms is right around the corner.
So I want to take a minute here to share a bit about the now supersized to track agenda.
On top of our great keynote interviews, including the one with Andrew Ng that I am so looking
forward to, our technical case studies track will feature speakers from companies like
Capital One, Comcast, Levi's and Zappos, all sharing about the architectures and approaches
they've developed to support their machine learning and deep learning workflows.
And the enabling technologies track will focus on tools and solutions that can help organizations
like yours automate and scale various aspects of your machine learning pipelines.
We have a ton of awesome speakers lined up for you.
So head on over to twomulcon.com slash speakers to check out the agenda.
The conference will be held in San Francisco on October 1st and 2nd.
I encourage you to register now as tickets are going fast. The early rate is ending soon
and you definitely don't want to miss out on this event.
PS, stick around to the end of the interview with Lucas and you'll get a discount code
good for 20% off of registration.
Hope to see you there.
So you kind of did some investigation as you're starting the company and found that people
were all over the map kind of tending towards file and directory based experiment management.
What issues did that cause for them?
Or was it just fine but not pretty?
Well, I think the biggest issue is just remembering what you did, right?
So it actually reminds me a lot of, I remember my first job when I was in college at a summer
job programming and honestly, I didn't really know about version control at the time, didn't
really trust it.
You know, not the best and most organized programmer and I remember I would have all these files
and anything.
What was the version control scheme of choice in your day?
Oh, yeah, what was it?
It was before SVN.
It was CVS.
CVS?
CVS, yes, CVS.
That was the first one.
Yep.
I remember when I first discovered CVS, it was just like, oh, man, this is awesome.
Actually, keep track of this stuff and kind of roll back to something I did before.
But the initial pain, I remember this originally trying to write papers, you collect a bunch
of metrics on the run that you want to put in your paper and then you realize, oh, there's
actually another metric that I'd like to include in my table and it's really hard to go
back and recreate all the past experiments that you did.
Even if you snapchat the code, the problem is with machine learning, there's more than
just the code.
There's the code and there's the hyper parameters that you used and there's the data set
that you input into your run.
So if you're not careful, it can be really tricky to even, people talk about the reproducibility
crisis in machine learning, right, because it's hard to reproduce other people's runs.
But forget about reproducing other people's runs, try to reproduce in a year, your past
self, your one month ago self set, runs, you know, I don't know how many people can actually
do that.
Yeah.
And so the big part of that is understanding the hyper parameters and settings associated
with a given run, but you also mentioned the data, are you doing anything there on the
weights and biases side or what are you seeing there?
You know, we actually, in weights and biases, we don't yet like snapchat the data for
your version, the data.
There are some really interesting technologies out there.
We just haven't seen a level of adoption yet that we're sure that something's kind of
becoming the standard, but we're watching that.
So we actually just, we snapchat your, the status of your code.
So we'll take not just your latest Git commit, but any diff from your commit, because, you
know, it flows a little different with ML where I think people are running lots and lots
of experiments and you kind of want to snapchat every point, you don't necessarily want
to have a Git commit or you can't necessarily rely on a user to do that between each run
that you do.
So we, we snapchat with just your latest commit and then a patch.
And then we also, you know, capture hyper parameters.
So I think the, the last step is the, the data, but we don't, we don't actually do that
yet.
Although, of course, if you're using some system where you have some pointer to the,
the data you have, you could input into our system, you know, like a path to your data
or some output of a, a data versioning system.
And I should mention that by the time this podcast is published, the, my ebook will be
out on machine learning platforms and there is a section where I talk about some of the
technologies that are out there for data management and versioning.
Oh, what are you seeing?
I'm curious what, what do you think the ones that I've seen out there are, are the ones
that come to mind at least are, there's a starter called DVC, which is like, I guess
the sense for a data version control, but they also do versioning of models and stuff
like that.
And there's packet arm, I don't have a lot of data on adoption of either.
But from a dedicated to solving this kind of problem, you know, a machine learning problem
from a data perspective, both of those are probably the, the, the pure plays that come
to mind, but they're also a number of kind of these end to end machine learning platforms
that come at it from a data perspective or have like some kind of data versioning capability
that kind of underlies the way that they handle data in their platforms.
And we actually had an interesting conversation the last time.
We spoke about the whole broad structure of this machine learning platform market and
how you have all these kind of specialists and then you have all these end or or kind
of wide folks that are trying to address like all of the problems in machine learning.
And they all kind of start from somewhere or come at it from some angle and for some
of them, the angle that they come at it from is data management, totally.
But so it sounds like your perspective is that you don't need to fully solve that problem
to get to some level of reproducibility or at least kind of useful utility.
Yeah, exactly.
I mean, it's just that back and sort of say that the sort of value prop of the experiment
tracking tool we have and there's sort of three parts to it, right?
So one is this kind of versioning and reproducibility and we want to make it as lightweight as possible.
So we don't to have true reproducibility, we'd have to put a lot of constraints around
you and so we just try to track as much as we can simply and you can add more things
to track.
But that's just one piece of I think experiment tracking is the sort of the versioning.
I think the sort of second piece of experiment tracking is around visualizing what's
happening, right?
So, as your model's training, you want to see typically lots of different accuracies
and lost curves kind of over time and over other axes and you also want to these days
compare across tens, hundreds or we even see thousands or tens of thousands of experiments
and there may be many different ways to look at kind of what's the best model or what's
the best set of input hyper parameters.
That's sort of a second value proposition that you get really well out of our tool and
then the third thing is collaboration, right?
So being able to kind of share the work that you did with a colleague or with someone at
a different organization or even with your future self in a way that they can understand.
So you kind of get all of those things with our tool and you sort of get them to the extent
that you buy in.
So a big thing for me is actually not to build a kind of end-to-end platform that's super
heavy weight that requires a lot of upfront cost.
So we try to make it just two or three lines of code that you add to your working thing
and it can run anywhere, you can run an Amazon, it can run on an Jupyter notebook, it
can run on prem, all these things kind of work and then you get reasonable defaults.
And as you kind of add things like hooking us up to your Git state, hooking us up to your
data version control, hooking us up to your underlying platforms, then we're able to kind
of give you more things.
And sort of the more information that you give and the more that you tell us about your
specific state, the more we can give you full reproducibility.
What are the couple of lines of code that you're inserting to get started doing?
So basically on your client side, we have a Python library, so you basically do a pip install
WNB and then you import a library.
And then actually if you're using Keras, we have a one-liner that will instrument your
code so you'll actually get a lot of value in a single line.
If you're using PyTorch or TensorFlow, it's maybe two or three lines you call WNB.init
and you pass in your configuration and then in PyTorch you call WNB watch on your model.
In TensorFlow, we use a hook so you'll basically use a WNB hook so that during your training
it keeps reporting to our model.
And also if you're using TensorFlow, we have a different way of integrating.
So we basically kind of worked with the sort of status quo of how people monitor things
today and kind of made a really lightweight way to connect to that.
And so you're using whatever native model introspection tools are available through these
frameworks to figure out things like the model type and parameters and all that kind
of stuff.
If you don't have to pass that in explicitly to the library.
Yeah, exactly.
So we figure out what we can.
So Keras and TensorFlow and PyTorch all have different ways of doing this.
So we'll take your TensorFlow flags or your Keras config parameters and we'll save those.
But if you want to save extra stuff, we make that really easy.
It's just a single line of essentially it's a dictionary of inputs and you can add to
that dictionary.
You mentioned PyTorch and TensorFlow and Keras.
These are all deep learning frameworks.
Is deep learning the only use case for weights and biases?
Do you tell folks to look elsewhere if they're trying to experiment tracking for more traditional
models or not using one of these frameworks?
Mostly.
So our application is framework agnostic in the sense that you can use it with any framework.
So you can do the configuration tracking and the logging with, you know, with scikit
learn or XG boost or anything like that. I think that where experiment tracking becomes
more valuable is when your experiments take longer or you want to do complicated hyper
parameter searching.
And we see more of that with deep learning.
So just as a company, we've really focused on deep learning and these frameworks to kind
of give you these magical installs.
If you're using a different framework, it's going to take you some more lines.
Although, you know, we've had people, we've had the community basically submit a integration
for fast.ai.
So we now have a community provided fast.ai integration.
And then we had an enthusiastic employee build a Jack's integration.
I don't know if anyone, do you know Jack's?
What's Jack's?
It's like a newer kind of even lighter weight.
I guess you might call it deep learning framework.
And then we have started to see people use us with XG boost and scikit learn.
So we're working on kind of making that more native.
And we did a study group.
One of our study groups associated with the Twoma Meetup was studying or working through
the full stack deep learning course by Peter Abil and others.
And I guess weight's and biases is like a standard part of that course or something
that you're told about and told to install in the course because we had a bunch of chatter
in our slack about weight's and biases and people sharing screenshots and stuff like
that.
Yeah.
And we saw a bunch of, of Tumofux come in from that, which is, which is fun for us.
Oh, awesome.
And by the way, anyone listening, you should, you should just know if you reach out to
our, our little chat in the bottom right.
It's not, it doesn't go to like some sales rep, it mainly goes to me.
So I'm happy to give you tech support and please reach out and tell us, you know, kind
of who you are.
We're not so big that we don't, you know, kind of want to know what people's issues are.
So the first of these kind of main value propses versioning and we've, we're primarily
talking about keeping track of your model parameters, your models as well.
You're not tracking those, you're just connecting or kind of tracking, uh, get commits of the
models themselves.
Is that right?
So again, everything is sort of, you know, opt in, right?
You know, so, so we work with people that, you know, have various levels of sensitivity.
So, um, and, and we really, you know, we have a strong point of view here where I really
don't want to be a end to end framework where you have to buy into everything to get value.
Yeah.
Um, so, you know, if you want us to, we'll keep track of your, your get shot and we'll
keep a patch against the get, um, your latest get commits.
So we'll know the state of your code if you want us to also, if you want us to, we'll
save your, um, model files either during training or at the end of training.
So, or, or any actually any other artifact that is important for running your model.
So there is a saving component to this, which is important to a lot of our, our users.
If you don't already have a pipeline built out that programmatically saves your model
parameter someplace and commits your code someplace all in the context of a run, you could do that
all through way to buy.
So it sounds like.
Yeah, exactly.
And if you already do have some kind of, um, pipeline where it's somewhere saved, then
maybe the best thing to do is just actually save a link to it, um, in our application.
You know, the important thing is that, um, the run gets associated with all the, the files
you'd need to reproduce it.
Okay. And so the next thing that comes up is, uh, visualization.
And when I think about like versioning and, and like saving model parameters and visualization,
the first thing that comes to mind for me is tensor board.
Are, are you trying to compete with that or replace it or do they complement each other
somehow?
Yeah.
I mean, I think it's, it's super complimentary.
So, you know, one thing that we actually do, which people find useful is we will host
your tensor board.
We send us a TF events files and artifact, um, you know, we recognize that and we'll actually
pop open a hosted tensor board for you in the cloud.
Um, you know, my big issue, there's sort of two things, two ways that we improve on, um,
tensor board.
Which I actually think is an excellent tool, um, you know, the, the, the, the, the first
thing that we improve on is that, um, tensor board tends to be a femoral.
So, you know, you, you typically run it locally.
And you know, one thing we, I saw a lot when I went around, um, and talked to folks about
how they were doing their experiment tracking today is they were literally taking screenshots
of their, um, tensor board and, and posting it into Slack.
And that, that seems a little, um, you know, that seems like a little crazy to me, or,
I guess it seems like there's an opportunity for, um, that to be hosted forever, right?
So if you, um, if you use weights and biases, then all this stuff, all these graphs that
you make and all the, um, all the runs that you do, they're hosted forever, or as until
you delete them, um, in the cloud.
So I think that's a much better practice, right?
Because, you know, if you shut down your tensor board server, um, you know, your colleagues
may still want to look at what you're doing, right?
So that, the, the, the sort of like static, permanent URL is kind of the first improvement.
I think the weights and biases has.
Mm-hmm.
And then the second thing, um, is I think that tensor board starts to struggle when you're
comparing lots and lots of models with lots and lots of data points, uh, elaborate on
that.
Where in particular does it struggle?
So there's kind of two ways that it can struggle, right?
So one is if you, if you run a model over millions and millions of, um, data points, it
doesn't, um, ever really start to sample, right?
So you know, if, if you have a run that, that, you know, you run it for like a couple
of weeks, um, you know, it, uh, it can just be actually literally slow, um, super slow,
right to, to run it even in your browser, um, and the second thing is that if you want
to compare lots of runs, I don't think that tensor board was kind of originally designed
for that.
So there is ways to compare, you know, three, four, five, um, runs, but what we typically
see is people will do, you know, hundreds or thousands of runs with different, um, you
know, kind of hyper parameters and configurations and they want to mark some is, hey, you know,
these were bass lines and, you know, here's what I was doing here and here's what I was
doing here.
And, you know, the typical way people do it in tensor board is they start to do that crazy
long, um, file name thing and then kind of search over them with regular expressions.
And, um, you know, that just, it doesn't scale, right?
So when you do, um, you know, when you, when you're really doing like serious, um, evaluate
equations, I mean, I, I don't want to knock on, um, tensor board, I think it's a, it's
a great tool for regular expressions, or regular expressions, that's a lot of regular
expressions.
Yeah.
Totally, totally.
Um, you know, yeah, good point.
I definitely don't want to, um, insult regular expressions, um, but, um, I think that, uh,
we have a tool that's more designed for a, kind of where you get to down the road when
you have, you know, hundreds of runs and you want to kind of filter and group things, um,
in different ways.
And I should say it is a, it can be a little tricky to monitor things on kind of big distributed
runs.
So when you're running across like multiple machines, um, that's also a case that we
really focused on at, at weights and biases.
Kind of a distributed training scenario where you're, you've got multiple machines training
a single model.
Yeah, exactly.
I mean, it's kind of makes sense if you're running on multiple machines or a single model,
um, to have everybody kind of reporting to like a single centralized place, um, universes
everybody writing out, you know, kind of files locally, you know, I think it's just different,
different design goals.
And so, um, I would say, I would say weights and biases is complimentary with, um, tensor
board, but it is kind of the closest, I'd say it's the most common experiment track
and thing that we, that we see today.
So it is the right place to compare weights and biases.
Uh, and so then the third, uh, element that you mentioned is collaboration.
Uh, I imagine just having that static URL is, uh, not having to screenshot and send it
in Slack.
It probably, uh, is a starting point for like collaboration, but the, are you doing something
explicit, uh, around collaboration?
Yeah, totally.
I mean, I think, um, so I think Adrian, uh, guidance on your podcast from TRI and talked
a little bit about, um, using our tool for collaboration, but, um, you know, they've,
they've sort of talked about it publicly and, you know, opening up is kind of done a case
study with us and how they use us to, um, collaborate.
But I think, I think a lot of it is just around, um, helping people get more systematic
about their training so that it's possible for other people to pick up your work.
And, you know, the regular fashion thing we talked about is a great example, right?
Like if you put in little notes in your run names for your hyper parameters, that might
make sense to you, but it could be really hard for your colleague, um, to come in and do
a similar analysis if they don't know, um, you know, what's your, you know, exactly what
your run names mean and, and what you were doing.
So, you know, we make it a lot easier to have human readable names and then also share
your projects with your colleagues.
So what happens is, you know, you can, you can basically set up a report and you can, you
can put notes in that report and like literally type out, okay, here's what I was doing.
Here, here's the different runs and then a colleague can, you know, go and look at any of
those individual runs and see what happened and then also look at the aggregate, um, statistics
and then also kind of build their own, um, analysis.
So I think where, where waits and biases becomes this like really beloved tool, um, is
when our, our customers start to use it for collaboration because, because that's just
something you kind of can't get out of anything else at, at least right now.
It doesn't sound like though that you're necessarily trying to build the, I don't know what you
would call this thing, the enterprise Facebook for models or something like that where like
every user has a feed and all of their runs going their feed and people are commenting
on each other's runs and that kind of thing.
No, I mean, I think, um, I think we want to do support, you know, discussions outside
of our tool.
I mean, I think there's lots of good ways to, you know, so you have a Slack integration,
you know, so you can, you can post this stuff into Slack.
Um, but I don't think that, that machine learning has necessarily that different of a workflow
that we want to, you know, try to, try to make our own version of a, of a feed, um, although
I would say, you know, we are experimenting with a thing that's been, I'm really excited
about called benchmarks where people can collaborate across, um, organizations.
So we can take an open source, um, you know, machine learning project, um, and then people
around the world can submit their results on it, right?
So they can, they can modify the code and show their accuracy on, you know, their own
data sets or their own setups.
And so, have you done any of those?
What's an example of, of a project that, and some, you know, folks submitting these benchmarks?
Yeah.
So you can find them on our website, but I think I'm one that was kind of fun was, you
know, Giffy, the company gave us a whole ton of gifts of cats.
And so we did kind of a video frame, um, prediction benchmarks.
So, you know, you get the first five frames, I believe, and then you predict the next five
frames.
And so, you know, what we actually saw was all the kind of different strategies that people
use for, um, video frame prediction, how well they work on this data set, but I thought
it was particularly cool about the way the benchmark was set up is that you can actually
go in and look at all the different submissions, um, models and all of their kind of, um, data
in a standard as way, um, or even kind of sort the submissions based on, you know, different
metrics and then pixel distance.
So, you know, different algorithms might work better depending on your, and metric, and,
you know, we have all the different models of people submitted, and then, you know, we're
doing one now on, um, you know, drought prediction, you know, to kind of help, um, to help
folks, you know, figure out where, where droughts are happening.
It's actually sort of a, apparently like a cactus identifier, because, you know, they,
they, the state of the Arctic, it's just like using the amount of green, but the problem
is, you know, cactus is our green, and, you know, that can be like a challenge to, um,
to kind of know where, where, where, you know, droughts are happening on like a small
scale.
And, you know, we have a whole bunch of other benchmarks, it's kind of a new feature,
so it's still, um, we're kind of still seeing how people use it.
But, um, what I like about it is I think there is a lot of room for collaboration across
teams, and there's sort of a rich, kind of culture of it, and machine learning because
so much of it comes out of academia, right?
But, um, you know, I think one of the challenges with, when you get a research paper is that,
you know, you get a small table of results, but you don't get to see all the different
things that the researcher tried and all the, you know, kind of all the paths that they
went down that, that didn't work, but, um, you know, if, if their stuff is instrumented
with weights and biases, you can actually have this really, um, detailed record of, of
lots and lots of different things, and, and kind of start from any point in the process.
Has anyone done that?
Have you seen a paper that cited one of these static URLs with, uh, all of their experimental
results?
Yeah, we just had our first one, actually, so, uh, really knew the, you know, the paper
publishing process is long, but yeah, we actually just, we just had our first one.
What was the paper?
I mean, I think we'll see a lot more, uh, down the road because we did make our, we
make our product, um, free, academics, because we really liked the Seuce case, but this
was called a machine learning techniques for detecting, identifying linguistic patterns
in, um, news media.
Is that literally what they did?
They kind of gave the static weights and biases, Lincoln, you can go in and look at all
of their various experiment runs or, yeah, yeah, exactly.
I mean, we probably should make a more systematic way to do this, but, um, for the emancipation,
um, you know, for their reports, you can go in and look at their, you know, look at
their report and, and get more detail.
When you're talking to folks that are, uh, outside of academia on the enterprise side,
and they're, you know, getting serious with deep learning and starting to figure out
how they can build some more structure around their approach.
What are the, I guess I have a bunch of questions around your experiences there.
Are there, are there, do you find cultural issues, quote, unquote, around, you know, them
adopting a tool like this or some, are there patterns around which teams are more likely
to kind of get it and, you know, want it or, um, is it, is it kind of random?
No, no, I mean, we've, we've designed this tool with a really particular end user in
mind.
Um, so this is a tool for researchers that are working on deep learning.
So we work with companies that have, um, researchers that are training deep learning models.
And we tend to work with, you know, companies that are making a bigger investment in that
today.
You know, you work with folks like, um, you know, GitHub and, and, you know, Blue River,
which is bought by John Deere, um, and, you know, a whole bunch of kind of robotics and,
and aerial imaging companies and, and that's, that's because those are the companies that
are right now making the biggest investments in deep learning.
And so, you know, our strategy has been, you know, rather than kind of focus on the sort
of mass democratization of, of AI to focus on, let's, let's look at what the companies
that are, um, most advanced doing and make the bet that other companies are going to follow
along and do kind of similar techniques.
So, you know, a lot of people disagree with that strategy and they say, well, you know,
you should build, you know, the, the tools that, um, you know, proctor and gamble needs
is very different than the tools that, you know, maybe an open area or a, a Google needs.
Um, but I guess my perspective is, is a little different.
I think that, you know, I think that, that proctor and gamble may not be training lots and
lots of deep learning model sales, they are training some.
I think that over time, they're going to train more and more and they're going to want
to bring that expertise in house.
So, I want my tool to make things, you know, easier for, for machine learning researchers,
but I don't necessarily want to make the machine learning research drops lead or kind
of, um, you know, automate every, every part of the process like, you know, some of these
like, um, you know, auto email types of, of projects.
So we typically go in, we're typically get adopted by researchers inside of companies
and then end up selling a larger license as the, um, business aside, they want to standard
eyes on our tool.
And you, you're describing this target user as a researcher.
Is that, uh, universal or do you find that the user is split across folks that are
formally called researchers and data scientists, machine learning engineers and other things
we see?
Well, of course, I mean, Sam, you know, this market, well, also, I mean, the titles
is total chaos, right?
So, I was kind of getting it there.
Yeah, I mean, you know, as, as you know, right, it's, it's hard to tell, um, you know,
these days from a title, what someone's actually doing, the important thing to us really,
like our, our kind of qualifier is that they're actually training, um, you know, machine learning
models and, and generally training, at least some of the models in one of the frameworks
that we have kind of first order support for.
So that's, you know, as I said, Cara's center flow, pie torture, you know, maybe fast AI.
So those are the folks that are most likely to benefit, um, from our tool.
Have you come across folks that are using fast AI, I don't know if commercially is the
right word or organizationally, like, you know, at a company or research organization,
um, meaning outside of the kind of educational context?
Yeah, I would say yes, it's not, I would say it's not, um, it's mostly in an educational
context.
And sometimes it's a little, um, hard to tell.
I think educational products and some of these orgs can bleed into production products,
you know, yeah, yeah.
Yeah.
You know, gradually, um, so, um, we, we actually honestly, we do see some fast AI and
stuff that looks like it's head and towards production.
I don't know that I could point to something where it's actually going to get deployed,
but, you know, it is really pie torch, um, you know, under the hood.
And I would say one insight that I have is we do see pie torch in a production context
a lot more than I think, um, you know, it's reputation to have you believe.
So, um, people are definitely deploying pie torch successfully into real world applications.
So I don't see why fast AI wouldn't allow you to do that.
I mean, you know, folks have different opinions about fast AI, you know, some, some seem
to love it, some seem to hate it.
But I think it, you know, I think if somebody really loved it, it shouldn't be out of the
question to deploy it.
Yeah.
Um, do you, is there any interaction with, uh, tools like Onyx here?
Um, not really.
We haven't seen a lot of Onyx and we've seen that file format used to save, um, models
occasionally, um, but, um, you know, for whatever reason, we, we, I haven't known
it's a lot of that, you know, if you think about the machine learning pipeline or the,
what are the things that are, you know, I guess we've talked about at some of the things
that are like immediately upstream or downshing from you, you know, there's get repositories
and collaboration and things like that are there, uh, things that, you know, folks really
need to have in place in order to take advantage of experiment management or things that,
you know, once you have experiment management, oh, wow, this whole new world opens up
for you and you can do X, Y, Z, yeah, totally, um, you know, it's funny, it's funny to
say this, but I think some folks, you know, come and don't realize this.
I mean, you do need to actually be doing experiments to take advantage of experiment,
it's not, um, you know, to be fair, it's, it's not totally trivial, right?
So, you know, to actually start training, you know, deep learning models, you need, uh,
you need some folks with experience in that, you know, you need machines that can do that
and, and a lot of people want kind of a software layer that helps them with that.
So we've made a fair amount of effort to integrate, um, with, with, um, the different stuff
that we see.
So, you know, we see a fair amount of number of folks using SageMaker.
So, you know, we built the first class integration with that.
We see a lot of excitement around ML flow and cube flow, so we built integrations with
those tools.
And our, our vision is to just run on top of, um, anywhere the people want to train their
models.
ML flow and cube flow sound so similar when you say them like that, but they're totally
different things.
I would have thought of ML flow, or I, I tend to think of ML flow as more of, uh, alternative
to what you're doing, whereas cube flows like more infrastructure and orchestration.
Yeah, but they've kind of bleed into each other.
It's a cube flow, I think of as more infrastructure and orchestration.
I think ML flow, um, you know, at least what they tell me is that they're kind of trying
to be more of a sort of standard, um, set of APIs around, um, training.
Um, as of course as they become kind of a standard way that different, um, you know, ML services
can talk to each other.
We want to, we want to work with them.
Yeah.
But yeah, good point.
It's funny.
It's like, as I say all these things, I'm thinking this podcast is going to be so out
of date.
Six months.
Well, you know, that is a challenge, I, when I did the Kubernetes ebook in, that was
published last November.
And you know, on the one hand, I was racing to get it ready for CubeCon in, uh, in Seattle.
But then there was a whole bunch of stuff that was announced that totally, uh, made a good
chunk of it obsolete, you know, and there was a bunch of updating that needs to be done.
And this, uh, the ML platforms paper, it probably mentions, I don't know, there's probably
like 30, 50 tools or something that are mentioned in this thing.
And I'm sure, you know, within a month of being published, some of them will be acquired,
some of them will be, will disappear, some of them will evolve as kind of, you know,
defective standards or market leaders or whatever it's, it is such a chaotic, um, and
therefore exciting marketplace and time to be, you know, in and following this marketplace.
Yeah.
Absolutely.
Yeah, I guess you have, you have the hardest job of trying to keep track of all this.
It is crazy.
It is crazy how much activity is happening in this space.
Yeah.
So anyway, you know, I think upstream, you know, we, we try to integrate with people
just as, as folks, you know, ask for us and people typically have, you know, some kind
of platform to do their runs, some kind of, sometimes some kind of, um, pipeline management
or workflow management.
Um, you know, they often have kind of a training data solution, um, I strongly recommend
to figure it.
And I'm totally unbiased.
And then, um, you know, downstream from us, um, you know, there's, um, production deployment,
I think it's kind of the next, um, the next step.
And, um, you know, we mostly see folks doing that, um, on an ad hoc basis, although, you
know, we've started to see a whole bunch of companies around that.
So I'm sure that, um, you know, that's going to start, um, start picking up, you know,
as, as the stuff becomes more real and, and, and people started to care about that more.
Is, is there anything else we should cover before we wrap up or any kind of parting thoughts?
No, you know, I just want to say, you know, the, it's a big goal for me is to make, make
our software really, really easy to try and really low, kind of lock in.
So, you know, I guess my, my asked the folks, you know, if you've listened this far, you
probably get more out of spending five minutes, um, integrating weights and biases, um, into,
into your tool.
So, you know, it, it really is like a, kind of a five minute install, um, and we make the
product free for individuals and free for, you know, academics and, um, easy to export
your data.
So, hopefully it's, it's pretty low commitment because, you know, my goal here is that, you
know, everyone, you know, gives our experiment tracking tool just to try and, and, you know,
if they have any questions or concerns, they, you know, write into our little intercom bubble
and, um, ask for help.
Well, we will be sure to point folks to a, uh, a place where they can find weights and
biases and do that, explore it more.
You, you refer to it as software, but it is software plus, like software as a service,
right?
Yeah.
You're accessing the, the dashboards and the visualizations through your website.
Do you have folks, uh, asking you to make it available, kind of behind the firewall
are on premises?
Yeah.
We're selling that today.
Oh, you are.
Okay.
So, you have, for example, to use it in that way.
Okay.
And we're doing lots of installs like that.
Yeah.
Cool.
Uh, well, Lucas, thanks so much for taking the time to chat very, uh, very fun conversation.
Uh, always learn a ton.
Likewise.
All right, everyone, that's our show for today.
For more information on Lucas or for today's show notes, visit twomalai.com slash shows.
Thanks again to weights and biases for their sponsorship of today's show and twomalcon
AI platforms.
Visit WNB.com to learn more for 20% off of your twomalcon registration.
Make sure to use the code WNB20, W-A-N-D-B20.
As always, thanks so much for listening and catch you next time.
