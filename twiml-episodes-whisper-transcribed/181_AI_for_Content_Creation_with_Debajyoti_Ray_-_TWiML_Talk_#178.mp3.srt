1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,760
I'm your host Sam Charrington.

4
00:00:32,760 --> 00:00:38,440
In today's episode we're joined by Deba Jotire, founder and CEO of Rivet AI, a startup

5
00:00:38,440 --> 00:00:42,440
producing AI-powered tools for storytellers and filmmakers.

6
00:00:42,440 --> 00:00:46,400
Rivet's tools are inspired and part by the founder's collaboration with the team that created

7
00:00:46,400 --> 00:00:51,800
SunSpring, a short AI-written film starring Silicon Valley's Thomas Middleditch, which

8
00:00:51,800 --> 00:00:57,600
you may have seen when it was making the rounds a while back.

9
00:00:57,600 --> 00:01:03,040
Deba and I discussed some of what he's learned in the journey to apply AI to content creation,

10
00:01:03,040 --> 00:01:07,720
including how Rivet approaches the use of machine learning to automate creative processes,

11
00:01:07,720 --> 00:01:13,160
the company's use of hierarchical LSTM models and auto encoders, and the tech stack that

12
00:01:13,160 --> 00:01:16,840
they've put in place to support the business.

13
00:01:16,840 --> 00:01:20,640
Before we dive in, a couple of meetup related updates.

14
00:01:20,640 --> 00:01:25,680
First off, great news for those of you who missed the first round of our fast AI deep learning

15
00:01:25,680 --> 00:01:27,760
for coder's study group.

16
00:01:27,760 --> 00:01:32,720
Sebastian Ween, who participated in the first study group, has stepped up to lead a second

17
00:01:32,720 --> 00:01:35,320
group through the Part 1 course.

18
00:01:35,320 --> 00:01:40,600
Next, if you missed the announcements on email and Twitter, we recently launched a new

19
00:01:40,600 --> 00:01:46,040
online meetup group that's much more conveniently timed for folks in Europe, the Middle East,

20
00:01:46,040 --> 00:01:47,360
and Africa.

21
00:01:47,360 --> 00:01:52,160
This group is being led by Kai Lichtenberg, who delivered a great presentation on capsule

22
00:01:52,160 --> 00:01:56,920
networks earlier this week for that meetups inaugural session.

23
00:01:56,920 --> 00:02:01,480
To learn more or join either of these groups, please sign up for the TwimmelOnline meetup

24
00:02:01,480 --> 00:02:04,880
at twimmelai.com slash meetup.

25
00:02:04,880 --> 00:02:10,400
All right, on to the show.

26
00:02:10,400 --> 00:02:11,400
All right, everyone.

27
00:02:11,400 --> 00:02:14,160
I am on the line with David Jotie Ray.

28
00:02:14,160 --> 00:02:17,480
Dave is founder and CEO of Rivet AI.

29
00:02:17,480 --> 00:02:20,640
Dave, welcome to this week in machine learning and AI.

30
00:02:20,640 --> 00:02:23,520
Thank you very much for having me, Sam.

31
00:02:23,520 --> 00:02:30,120
So Dave, you studied math as an undergrad at the University of Toronto and actually got

32
00:02:30,120 --> 00:02:32,920
a chance to work in Jeff Hinton's lab.

33
00:02:32,920 --> 00:02:37,920
Tell us a little bit about that and how your career evolved from that point.

34
00:02:37,920 --> 00:02:46,200
Yeah, so I was a math major, math specialist at the University of Toronto, and at the same

35
00:02:46,200 --> 00:02:51,200
time, I was really interested in programming and coding.

36
00:02:51,200 --> 00:02:55,480
So I wanted to combine both of my passions.

37
00:02:55,480 --> 00:03:03,560
And University of Toronto, Jeff Hinton was one of the luminaries in the field, so really

38
00:03:03,560 --> 00:03:08,480
what drew me to the University and to math in particular is I wanted to explore the field

39
00:03:08,480 --> 00:03:09,880
of AI.

40
00:03:09,880 --> 00:03:15,640
And in my first year, I actually convinced Jeff Hinton to allow me to sit in one of his

41
00:03:15,640 --> 00:03:16,640
classes.

42
00:03:16,640 --> 00:03:25,720
And then at the end of the summer, I offered to, I offered my programming services as an

43
00:03:25,720 --> 00:03:28,160
undergrad researcher.

44
00:03:28,160 --> 00:03:32,880
And I was really fortunate when they allowed me to do some of the projects.

45
00:03:32,880 --> 00:03:39,240
But I got a taste of machine learning since my undergrad days itself, working with Jeff

46
00:03:39,240 --> 00:03:47,800
Hinton and other researchers in this lab, especially in the field of Bayesian neural networks.

47
00:03:47,800 --> 00:03:53,320
And I had several publications coming out of it during my undergrad.

48
00:03:53,320 --> 00:03:59,080
After that, I wanted to explore the field of computational neuroscience and dive into

49
00:03:59,080 --> 00:04:03,200
neuroscience a little bit more, so I moved to London.

50
00:04:03,200 --> 00:04:07,560
But at that time, there was kind of a boom in the financial sector.

51
00:04:07,560 --> 00:04:14,760
So I made a digression into more hedge funds and quantitative finance.

52
00:04:14,760 --> 00:04:24,800
But in 2008, I also discovered that I wanted to go back to academia and explore my interests

53
00:04:24,800 --> 00:04:32,200
in deep learning and machine learning, again, armed with a new set of applications.

54
00:04:32,200 --> 00:04:40,360
So I went back to grad school, went to Caltech in the Computation and Neural Systems Group.

55
00:04:40,360 --> 00:04:48,880
And there, my focus was more around generative models, because one of the advantages of deep

56
00:04:48,880 --> 00:04:58,600
learning models is it can capture very nuanced statistical behaviors, very complex statistical

57
00:04:58,600 --> 00:04:59,600
behaviors.

58
00:04:59,600 --> 00:05:05,640
So I wanted to see if that could be used to, as a generative model, to study different

59
00:05:05,640 --> 00:05:09,640
types of behaviors, especially behavioral economics.

60
00:05:09,640 --> 00:05:15,200
So that's something that I pursued during my PhD at Caltech.

61
00:05:15,200 --> 00:05:20,960
But more generally looking at generative models for deep learning.

62
00:05:20,960 --> 00:05:29,640
So from there, while you did start a couple of companies between your grad school experience

63
00:05:29,640 --> 00:05:37,960
and starting rivet, there is a connection in that you were working on generative models

64
00:05:37,960 --> 00:05:38,960
there.

65
00:05:38,960 --> 00:05:45,120
And now you're currently working on applying AI to the idea of content creation.

66
00:05:45,120 --> 00:05:50,240
And maybe tell us a little bit about the inspiration for starting rivet, what drove you to look

67
00:05:50,240 --> 00:05:52,240
at that problem.

68
00:05:52,240 --> 00:05:59,440
So from the business point of view, previously, my previous startup was a video amp, which

69
00:05:59,440 --> 00:06:02,560
was in the video advertising space.

70
00:06:02,560 --> 00:06:08,400
It was looking at different behaviors, consumer behavior, and finding the best kind of video

71
00:06:08,400 --> 00:06:10,800
ads to target to people.

72
00:06:10,800 --> 00:06:16,760
But what I saw there is advertising kind of sits on top of content.

73
00:06:16,760 --> 00:06:23,400
If you don't have good content, then you don't get the kinds of engagements.

74
00:06:23,400 --> 00:06:28,920
That's one of the reasons why companies don't get as much ROI out of their video advertising

75
00:06:28,920 --> 00:06:30,400
spent.

76
00:06:30,400 --> 00:06:36,840
And the other thing that I saw is the shift that more and more companies wanted to put their

77
00:06:36,840 --> 00:06:42,800
money instead of putting into video advertising, they wanted to put it into sponsored content.

78
00:06:42,800 --> 00:06:50,040
Because at the end of the day, consumers, humans in general, we resonate much more to stories.

79
00:06:50,040 --> 00:06:53,720
So an ad is basically grabbing attention.

80
00:06:53,720 --> 00:07:00,880
It's 30 seconds to grab somebody's attention and maybe create Pavlovian responses of the

81
00:07:00,880 --> 00:07:03,320
consumer will go and buy something.

82
00:07:03,320 --> 00:07:12,000
But what really resonates with us forms culture, forms behavior is story creation, story-making.

83
00:07:12,000 --> 00:07:18,760
So there are a couple of, in terms of my own interests, I have been very interested in

84
00:07:18,760 --> 00:07:20,920
natural language generation.

85
00:07:20,920 --> 00:07:25,000
So when we look at generative models, especially generative models for language, I wanted to

86
00:07:25,000 --> 00:07:29,520
see how that could be applied to generate language.

87
00:07:29,520 --> 00:07:34,640
We start with a training set, but you also feed a bunch of different parameters.

88
00:07:34,640 --> 00:07:39,760
So can you actually produce narrative structures?

89
00:07:39,760 --> 00:07:46,040
Maybe even stories someday, right, could a model generate a story.

90
00:07:46,040 --> 00:07:49,320
So that was my general kind of research interest.

91
00:07:49,320 --> 00:07:56,560
And also at the same time, noticing this kind of shift in spend by companies from videos

92
00:07:56,560 --> 00:08:03,120
to content, the technology, especially the deep learning technology, was pretty ripe

93
00:08:03,120 --> 00:08:08,800
to be able to analyze content because the tech has matured to the point where you can

94
00:08:08,800 --> 00:08:12,720
analyze videos, you can analyze audios.

95
00:08:12,720 --> 00:08:19,280
These things are not really very possible a few years back, because it's really the perfect

96
00:08:19,280 --> 00:08:24,880
like confluence of tech maturity and the business moving in that direction.

97
00:08:24,880 --> 00:08:31,320
And also it's my personal passion, which is what led me to start Rivet AI.

98
00:08:31,320 --> 00:08:40,880
Maybe as a touchstone for folks, we can reference a project that folks may have come across.

99
00:08:40,880 --> 00:08:42,200
This is SunSpring.

100
00:08:42,200 --> 00:08:51,200
It was a short film that star Thomas Middleditch, who's the star of Silicon Valley.

101
00:08:51,200 --> 00:08:55,800
And it was written by an AI, basically an LSTM.

102
00:08:55,800 --> 00:09:03,880
Tell us about that project and some of the ways you've worked with the team behind that

103
00:09:03,880 --> 00:09:04,880
sense.

104
00:09:04,880 --> 00:09:10,560
Yeah, so the company EndQ produced SunSpring.

105
00:09:10,560 --> 00:09:17,760
And when I was making my transition from video advertising to content, I teamed up with

106
00:09:17,760 --> 00:09:24,720
EndQ because EndQ is a production company that makes feature films, short films, animation

107
00:09:24,720 --> 00:09:26,440
films, and so on.

108
00:09:26,440 --> 00:09:30,280
And really, who creates the best content?

109
00:09:30,280 --> 00:09:31,680
It's Hollywood.

110
00:09:31,680 --> 00:09:36,800
So I wanted to partner up with the folks who really know how to tell good stories, who

111
00:09:36,800 --> 00:09:40,080
know how to create great content.

112
00:09:40,080 --> 00:09:46,560
And EndQ was really the perfect partner because one of the principles of EndQ is a Caltech

113
00:09:46,560 --> 00:09:49,320
trustee, Walter Corkcheck.

114
00:09:49,320 --> 00:09:55,920
And he's also funded projects in AI and at Caltech.

115
00:09:55,920 --> 00:10:03,360
So that's how we had a meeting of minds and wanted to explore the ideas even further.

116
00:10:03,360 --> 00:10:12,920
SunSpring was a project where the tech behind it was essentially a very simple LSTM.

117
00:10:12,920 --> 00:10:17,680
So it's kind of like the same predictive text that you have from your phone.

118
00:10:17,680 --> 00:10:24,920
So imagine you created a story by taking every predictive response from your phone.

119
00:10:24,920 --> 00:10:29,480
So SunSpring was the seed word.

120
00:10:29,480 --> 00:10:33,120
And then you start with SunSpring and imagine putting that into your phone, but something

121
00:10:33,120 --> 00:10:34,640
more complex, obviously.

122
00:10:34,640 --> 00:10:41,640
Imagine your phone's predictive response is now trained on movie scripts and has a little

123
00:10:41,640 --> 00:10:45,600
bit more of a complex LSTM, long short-term memory model.

124
00:10:45,600 --> 00:10:53,600
And you started using very simple, maximum likelihood estimation and picked every word that

125
00:10:53,600 --> 00:10:57,880
came next, every next word that your phone suggested.

126
00:10:57,880 --> 00:10:59,720
So that's how that script was made.

127
00:10:59,720 --> 00:11:07,760
It was very simple, LSTM trained on sci-fi movie scripts and then just maximum likelihood

128
00:11:07,760 --> 00:11:09,160
estimation.

129
00:11:09,160 --> 00:11:17,520
So from there, the real kind of genius in that project was the director Oscar Sharp

130
00:11:17,520 --> 00:11:23,720
was able to take a script like that and turn it into a very interesting watchable movie.

131
00:11:23,720 --> 00:11:27,320
Interesting and watchable, but not necessarily intelligible.

132
00:11:27,320 --> 00:11:30,320
Exactly.

133
00:11:30,320 --> 00:11:37,680
And really kudos to the actors and the director for making something out of it.

134
00:11:37,680 --> 00:11:45,800
But it was a very interesting experiment that very few people would be willing to do.

135
00:11:45,800 --> 00:11:50,800
And so that experiment was like, what if we could actually take some model like that and

136
00:11:50,800 --> 00:11:51,800
turn it into a movie?

137
00:11:51,800 --> 00:11:53,360
What would that look like?

138
00:11:53,360 --> 00:12:00,480
And the response has been very kind of a bi-model in the sense that some people love

139
00:12:00,480 --> 00:12:02,880
it and love the idea of what it represents.

140
00:12:02,880 --> 00:12:07,120
On the other hand, there's hate.

141
00:12:07,120 --> 00:12:11,760
So whenever you get a bi-model response, but lots of responses, that's when you know

142
00:12:11,760 --> 00:12:15,080
you're onto something and that you have to pursue that.

143
00:12:15,080 --> 00:12:22,080
So we made several iterations, especially with RiverDI, we made several iterations on

144
00:12:22,080 --> 00:12:29,880
top of the models before and we came up with the next generation of AI written content

145
00:12:29,880 --> 00:12:30,880
after that.

146
00:12:30,880 --> 00:12:38,920
But also our philosophy changed quite a lot, which was SunSpring where it was completely

147
00:12:38,920 --> 00:12:40,440
unsupervised.

148
00:12:40,440 --> 00:12:47,160
You know, it's completely unsupervised, maximum likelihood estimation, let's create a script.

149
00:12:47,160 --> 00:12:50,440
And there's absolutely no direction involved.

150
00:12:50,440 --> 00:12:55,920
Whereas really, if you're telling a story and the story has to make sense to an audience,

151
00:12:55,920 --> 00:12:59,560
then two ingredients are super important.

152
00:12:59,560 --> 00:13:06,680
One is knowing the preference functions, right, of who your list or who your audiences.

153
00:13:06,680 --> 00:13:10,640
It could be in the form of general audience data or it could be in the form of something

154
00:13:10,640 --> 00:13:16,400
that interacts with the director and tries to learn their preferences.

155
00:13:16,400 --> 00:13:24,080
And the second part is some sort of physical or even a cultural embodiment that AI program

156
00:13:24,080 --> 00:13:31,080
completely lacks, so imagine just an alien showing up or worse and trying to just generate

157
00:13:31,080 --> 00:13:36,640
something with no idea of what the context is at all.

158
00:13:36,640 --> 00:13:43,400
So what we focused on in our next set of models was to really put a sense of context in that

159
00:13:43,400 --> 00:13:48,800
and also learn preference functions, whether it's a human interaction to a human interaction

160
00:13:48,800 --> 00:13:53,000
with the director or writer or audience responses.

161
00:13:53,000 --> 00:13:57,120
And so that kind of evolved into your next set of models.

162
00:13:57,120 --> 00:14:04,640
And our philosophy now is not so much AI-written everything or AI-autonomously generating content

163
00:14:04,640 --> 00:14:13,160
or AI-autonomously writing stories, but AI-augmenting creativity in a couple of different ways.

164
00:14:13,160 --> 00:14:19,280
One is AI-augmenting creativity because a lot of the creative process for those who are

165
00:14:19,280 --> 00:14:25,600
actually engaged in making content, a lot of the process is very tedious.

166
00:14:25,600 --> 00:14:36,120
95% of the time goes into the tedious stuff and only 5% on inspiring creative works.

167
00:14:36,120 --> 00:14:42,600
So our tech is more focused on augmenting the creative aspects and trying to where possible

168
00:14:42,600 --> 00:14:46,240
take away a lot of the or automating a lot of the tedious aspects.

169
00:14:46,240 --> 00:14:53,720
Can you give us some examples of where the TDM comes into play in this particular use case

170
00:14:53,720 --> 00:14:56,920
with this kind of content creation?

171
00:14:56,920 --> 00:14:57,920
Right.

172
00:14:57,920 --> 00:15:04,760
So, you know, for listeners who are not in the content creation process or filmmaking,

173
00:15:04,760 --> 00:15:05,760
what happened?

174
00:15:05,760 --> 00:15:11,720
The typical flow is, you know, you have a story, a script, everything kind of starts there.

175
00:15:11,720 --> 00:15:15,000
And of course, there's a whole lot that goes into making a script.

176
00:15:15,000 --> 00:15:17,760
But let's, as you may start with the script.

177
00:15:17,760 --> 00:15:24,280
Now the first decision is development, like whether you actually decide to work on the

178
00:15:24,280 --> 00:15:25,280
script or not.

179
00:15:25,280 --> 00:15:29,440
So, usually a studio or a production company will get lots and lots of scripts and they

180
00:15:29,440 --> 00:15:34,840
have to evaluate whether they want to produce it or not.

181
00:15:34,840 --> 00:15:42,800
So tip, usually in the industry, it's kind of surprising how, you know, even when you

182
00:15:42,800 --> 00:15:48,640
have these amazingly huge budgets, there's not a whole lot of data that goes into that

183
00:15:48,640 --> 00:15:49,640
decision making.

184
00:15:49,640 --> 00:15:53,440
So, there are different ways you can incorporate data into that decision whether you want

185
00:15:53,440 --> 00:15:58,800
to go with the script or not, whether you want to analyze that script, make some predictions

186
00:15:58,800 --> 00:16:02,160
of how well it would perform and so on.

187
00:16:02,160 --> 00:16:06,120
So let's say you decide to go with the script.

188
00:16:06,120 --> 00:16:13,640
Now the first point is, you know, first step involves a script breakdown.

189
00:16:13,640 --> 00:16:19,640
So a script is essentially just words on a piece of paper, but they need to be depicted

190
00:16:19,640 --> 00:16:23,280
physically or, you know, through an animation.

191
00:16:23,280 --> 00:16:27,600
So it has to go from words to actual physical representation.

192
00:16:27,600 --> 00:16:30,280
So that is a very tedious process.

193
00:16:30,280 --> 00:16:36,960
You know, you have a 200 page script, what happens now is someone goes in with a marker and

194
00:16:36,960 --> 00:16:43,960
starts going through the script word by word, saying, okay, this is, this here is a character,

195
00:16:43,960 --> 00:16:51,400
the character says this, so we do need a person here in a speaking role or it could be,

196
00:16:51,400 --> 00:16:55,160
and they're sitting at a coffee shop talking about this.

197
00:16:55,160 --> 00:17:02,440
So you need to have a set with a coffee shop and the lead actor, he takes a sit from a cup.

198
00:17:02,440 --> 00:17:04,760
So you need to represent that cup as a prop.

199
00:17:04,760 --> 00:17:09,480
So there's so many elements that goes from telling the story to actually putting it in

200
00:17:09,480 --> 00:17:10,800
production.

201
00:17:10,800 --> 00:17:18,160
So that is something that piece itself is hugely time consuming several weeks often, but

202
00:17:18,160 --> 00:17:23,800
that's something that we can solve with natural language understanding because we can,

203
00:17:23,800 --> 00:17:31,480
with a training corpus of previously annotated scripts, we can identify where characters need

204
00:17:31,480 --> 00:17:39,240
to be represented, where props are required, what kind of sets are involved, set dressing.

205
00:17:39,240 --> 00:17:44,000
You know, if you need a vehicle, you need an animal in that screen, an animal handler.

206
00:17:44,000 --> 00:17:48,880
So all of those, like that script breakdown process can be automated, but there needs

207
00:17:48,880 --> 00:17:56,560
to be a better set of training data for that because films always involve analogies,

208
00:17:56,560 --> 00:18:01,120
so you can have a sentence like buzzing like bees, so you don't actually have to have a bee

209
00:18:01,120 --> 00:18:02,720
in the scene, just an analogy.

210
00:18:02,720 --> 00:18:09,360
So those are things that kind of the AI needs to understand.

211
00:18:09,360 --> 00:18:15,320
It sounds like a very specialized, named entity resolution type of problem.

212
00:18:15,320 --> 00:18:21,880
Yes, we've named entity resolution that understands analogies, or where analogies are coming.

213
00:18:21,880 --> 00:18:26,320
So you have to have a context understanding there.

214
00:18:26,320 --> 00:18:33,120
So that's a huge piece that can be automated, like going from like few weeks to just a

215
00:18:33,120 --> 00:18:36,960
few clicks of a button and a few minutes of a script breakdown.

216
00:18:36,960 --> 00:18:42,000
And then from there, you know, the next phase is once you've got, okay, I need for like

217
00:18:42,000 --> 00:18:49,880
over 200 scenes, every scene, I need these, these parts and this actor and this location,

218
00:18:49,880 --> 00:18:54,000
then it's kind of a scheduling problem, a budgeting problem.

219
00:18:54,000 --> 00:18:59,000
So even the first phase, which is a budget approximation, that again is a hugely time consuming

220
00:18:59,000 --> 00:19:07,040
bees, but since we have all these entities extracted and we have training data from budgets

221
00:19:07,040 --> 00:19:11,960
and how these entities map to different budget estimates, we can construct budget estimates

222
00:19:11,960 --> 00:19:14,640
very quickly.

223
00:19:14,640 --> 00:19:19,840
Same thing with schedules, you know, you have so many parameters, you have certain location

224
00:19:19,840 --> 00:19:27,280
where a film needs to be shot and this actor who's extremely, who's time is extremely expensive.

225
00:19:27,280 --> 00:19:34,560
So how do you combine these to get the best schedules in terms of both cost and time effectiveness?

226
00:19:34,560 --> 00:19:38,320
There are billions of parameters to deal with.

227
00:19:38,320 --> 00:19:41,960
That is often done manually in that industry surprisingly.

228
00:19:41,960 --> 00:19:47,800
So just automating a lot of that, just bringing in AI scheduling tools just, you know, gives

229
00:19:47,800 --> 00:19:56,160
you 10-50 percent efficiencies right off the bat, which for these $100 million budget

230
00:19:56,160 --> 00:20:00,120
productions, you know, are substantial savings.

231
00:20:00,120 --> 00:20:08,920
So those are the tedious parts of this content creation where AI can come in and automate

232
00:20:08,920 --> 00:20:12,800
and provide, you know, budget and time savings.

233
00:20:12,800 --> 00:20:23,320
Do you remain involved in the creative content generation side as well or have you mostly,

234
00:20:23,320 --> 00:20:30,800
I think of the various things that you've described as content support, but more on the analysis

235
00:20:30,800 --> 00:20:34,400
side, if you will, do you tend to focus on that as a company now?

236
00:20:34,400 --> 00:20:41,640
Yeah, so we have also been bringing in different kind of tools, but slowly.

237
00:20:41,640 --> 00:20:49,000
So first, it's kind of, whenever we try to talk about AI in the creative industry or content

238
00:20:49,000 --> 00:20:52,840
creation industry, there's a bit of resistance initially.

239
00:20:52,840 --> 00:20:57,760
So it's also a different strategy for kind of entering the market.

240
00:20:57,760 --> 00:21:04,200
So starting with analysis, starting with things that save time and effort, you know, builds

241
00:21:04,200 --> 00:21:12,400
more comfort in allowing people to understand that, yes, AI tools can actually help my work.

242
00:21:12,400 --> 00:21:16,760
You know, the analogy that I draw is AutoCAD, right?

243
00:21:16,760 --> 00:21:22,920
You had architects who, you know, where it was always considered to be a very creative

244
00:21:22,920 --> 00:21:23,920
endeavor.

245
00:21:23,920 --> 00:21:26,360
Nobody touched my clay models.

246
00:21:26,360 --> 00:21:32,280
But then AutoCAD came in and took away a lot of the tedious aspects of architecture.

247
00:21:32,280 --> 00:21:40,520
And now it's hard to imagine, you know, creating these complex architectural models without

248
00:21:40,520 --> 00:21:43,840
using computers and AI tools.

249
00:21:43,840 --> 00:21:50,960
So the same thing, you know, can happen in the field of content creation, but incrementally.

250
00:21:50,960 --> 00:21:58,880
So one of the, you know, features we've added in is things that make the writing better,

251
00:21:58,880 --> 00:22:01,360
but in the sense of, hey, can we make it more readable?

252
00:22:01,360 --> 00:22:03,800
Can we make, you know, improve readability?

253
00:22:03,800 --> 00:22:05,960
Can we improve audience appeals?

254
00:22:05,960 --> 00:22:12,520
So for example, I'm thinking like, I'm thinking like Grammarly for script writers.

255
00:22:12,520 --> 00:22:20,960
Yeah, I mean, Grammarly, you know, improves, it's targeted to a very broad audience.

256
00:22:20,960 --> 00:22:27,520
So imagine for something that where people are creating content where you can take something

257
00:22:27,520 --> 00:22:31,520
that was, let's say, readability levels 11.

258
00:22:31,520 --> 00:22:37,760
And if you turn that into readability level 5 or 6, then you can show that it'll appeal

259
00:22:37,760 --> 00:22:43,240
to a much broader audience while keeping the fundamental idea, the story, everything

260
00:22:43,240 --> 00:22:44,240
the same.

261
00:22:44,240 --> 00:22:47,760
You're just kind of changing some of the words, right?

262
00:22:47,760 --> 00:22:51,960
And simplifying the sentence structures, but all of a sudden you've reached a much wider

263
00:22:51,960 --> 00:22:52,960
audience.

264
00:22:52,960 --> 00:22:59,760
So that's an example of how you can augment the writing, augment the creation.

265
00:22:59,760 --> 00:23:06,680
The other pieces are, you know, with the short films that we have created where the dialogues

266
00:23:06,680 --> 00:23:13,240
are actually generated or some of the AI characters, dialogues are generated with natural

267
00:23:13,240 --> 00:23:14,920
language generation.

268
00:23:14,920 --> 00:23:23,920
So what is different with that from the previous LSTM models is, and I described the script

269
00:23:23,920 --> 00:23:24,920
breakdown.

270
00:23:24,920 --> 00:23:30,880
Now, the next thing you can do is you can also look at character interactions.

271
00:23:30,880 --> 00:23:36,920
So in Scene 1, let's say you have Peter and Amanda interacting and Scene 5, they interact

272
00:23:36,920 --> 00:23:38,400
again.

273
00:23:38,400 --> 00:23:40,560
You can look at the words that they've spoken.

274
00:23:40,560 --> 00:23:45,000
You can look at, you know, the sentiment of their interactions.

275
00:23:45,000 --> 00:23:50,320
You can even look for continuity by analyzing the sentences that they've spoken.

276
00:23:50,320 --> 00:23:54,040
So imagine doing that for a movie or even a TV series.

277
00:23:54,040 --> 00:24:02,040
You can turn a script into a knowledge graph basically, which is characters and their interactions

278
00:24:02,040 --> 00:24:11,880
over a period of time and use that to identify any continuity issues or things that trail

279
00:24:11,880 --> 00:24:12,880
off.

280
00:24:12,880 --> 00:24:19,520
Like it could be just a story sideline that trails off and doesn't really add much to the central

281
00:24:19,520 --> 00:24:20,720
theme.

282
00:24:20,720 --> 00:24:28,000
Now, for writers, for, you know, imagine like Westworld, there's so many little subplots

283
00:24:28,000 --> 00:24:29,000
going on.

284
00:24:29,000 --> 00:24:33,120
And what you're going to a writer's room like that, it was kind of eye opening for me,

285
00:24:33,120 --> 00:24:38,240
going into a writer's room where the wall is literally covered with sticky notes and

286
00:24:38,240 --> 00:24:41,400
strings tying things together.

287
00:24:41,400 --> 00:24:44,280
And the scripts are ginormous.

288
00:24:44,280 --> 00:24:49,040
That's something that's really hard to do where, you know, again, bringing in tools,

289
00:24:49,040 --> 00:24:56,040
like visualization tools and even a step further, like continuity analysis, looking into, you

290
00:24:56,040 --> 00:25:03,320
know, things that trail off or that improves the content generation a lot.

291
00:25:03,320 --> 00:25:08,520
But you can get a step further, which is let's say now I want to make certain changes

292
00:25:08,520 --> 00:25:09,800
to the story itself.

293
00:25:09,800 --> 00:25:15,640
So I have a knowledge graph and I go to a certain node, which is characters interacting.

294
00:25:15,640 --> 00:25:18,320
And I, you know, I want to move around and play with it.

295
00:25:18,320 --> 00:25:21,280
Let's say let's move the story around and let's see what happens.

296
00:25:21,280 --> 00:25:27,440
So there you can actually use a Bayesian inference to figure out what would happen to the rest

297
00:25:27,440 --> 00:25:31,880
of the nodes if you make a particular change in one node.

298
00:25:31,880 --> 00:25:37,840
So those are the different ways that we can give tools in the hands of writers and producers.

299
00:25:37,840 --> 00:25:42,880
So they feel free to change around different parameters of the story.

300
00:25:42,880 --> 00:25:47,720
Now, something we can't do right now, but we're working towards is also mapping that audience

301
00:25:47,720 --> 00:25:49,440
response data.

302
00:25:49,440 --> 00:25:55,160
So perhaps you can make tweaks to a story, you know, in a way that will be more interesting

303
00:25:55,160 --> 00:26:00,840
to the audience, or maybe you want to try something completely new, just to experiment and

304
00:26:00,840 --> 00:26:04,400
see how the audience would react to something.

305
00:26:04,400 --> 00:26:12,760
Can you talk through the knowledge graph and the use of Bayesian inference at the next

306
00:26:12,760 --> 00:26:19,440
level of detail, how do you translate that, for example, translate that, you know, knowledge

307
00:26:19,440 --> 00:26:26,720
graph and Bayesian inference to something that's to an output that a script writer can use.

308
00:26:26,720 --> 00:26:27,720
Right.

309
00:26:27,720 --> 00:26:34,200
So as an analogy, you know, you can use knowledge graphs have been used in a lot of different

310
00:26:34,200 --> 00:26:35,200
domains.

311
00:26:35,200 --> 00:26:43,280
Let's say in the case of, you know, genetics, right, so you look at pairwise interaction

312
00:26:43,280 --> 00:26:48,040
in one place and then you can use Bayesian inference to say, okay, what if I, you know, made

313
00:26:48,040 --> 00:26:54,320
a change here, like the genetic code here, how that's that influence things in other parts,

314
00:26:54,320 --> 00:27:00,160
right, and connected notes in a, in a, in a call in more of a causal model, where you

315
00:27:00,160 --> 00:27:05,320
know the underlying causality, you can make tweaks to one node and you have a causal model

316
00:27:05,320 --> 00:27:09,960
that will tell you what happens as you move one thing and change, and you know, the effects

317
00:27:09,960 --> 00:27:15,160
that it has for their downstream or on the rest of the graph.

318
00:27:15,160 --> 00:27:22,600
So we have certain models of sentiment and models of, you know, emotional interactions

319
00:27:22,600 --> 00:27:25,400
and character interactions.

320
00:27:25,400 --> 00:27:31,960
These are not like models that you can necessarily write down, but more coral, that we've picked

321
00:27:31,960 --> 00:27:35,240
up by breaking down lots and lots of scripts out there.

322
00:27:35,240 --> 00:27:42,560
So there's a universe of like, you know, 50,000 scripts that may have been produced, an

323
00:27:42,560 --> 00:27:46,560
order of magnitude more if you look at scripts that have never been produced.

324
00:27:46,560 --> 00:27:52,440
So you can do this exercise with, you know, produce knowledge graphs for every script

325
00:27:52,440 --> 00:27:56,520
out there and learn some of these interactions.

326
00:27:56,520 --> 00:28:04,280
Now the way we learned our interactions is we had people go and code these interactions

327
00:28:04,280 --> 00:28:11,840
or annotate from one node to another what the sentiment interactions were.

328
00:28:11,840 --> 00:28:18,200
So we were able to use that data to train kind of like a sentiment causality graph.

329
00:28:18,200 --> 00:28:23,960
If you change one, one, the words or the sentiment of one node, how does that impact the

330
00:28:23,960 --> 00:28:25,880
others downstream?

331
00:28:25,880 --> 00:28:31,680
So that's kind of like a high level version of how we can use a knowledge graph to see,

332
00:28:31,680 --> 00:28:33,960
you know, predict what would happen down the road.

333
00:28:33,960 --> 00:28:35,840
How are we representing sentiment?

334
00:28:35,840 --> 00:28:41,480
Is it like trying to apply human emotions to these things, love, hate, whatever?

335
00:28:41,480 --> 00:28:48,160
Now I'm really just trying to wrap my head around maybe a concrete example of how

336
00:28:48,160 --> 00:28:56,560
a script might apply or translate into this model and then how manipulating this model

337
00:28:56,560 --> 00:29:00,320
helps us with the script process.

338
00:29:00,320 --> 00:29:01,320
Yeah.

339
00:29:01,320 --> 00:29:07,600
So, you know, the simplest example is let's say Paul and Andy are in a fight and Paul kills

340
00:29:07,600 --> 00:29:08,600
Andy.

341
00:29:08,600 --> 00:29:14,200
So now all of a sudden that character Andy can no longer exist in the rest of the scenes,

342
00:29:14,200 --> 00:29:15,200
right?

343
00:29:15,200 --> 00:29:16,200
So that's a very simple inference.

344
00:29:16,200 --> 00:29:19,200
That you fly to the rest of the notes.

345
00:29:19,200 --> 00:29:21,080
Let's get more complicated.

346
00:29:21,080 --> 00:29:28,160
Paul and Andy fight, but then they realize they're working towards the same common goal.

347
00:29:28,160 --> 00:29:31,160
So therefore their interactions now are positive.

348
00:29:31,160 --> 00:29:37,040
So now the rest of the nodes, if you have the next node where Paul and Andy interact

349
00:29:37,040 --> 00:29:40,400
were, Nick was a negative interaction.

350
00:29:40,400 --> 00:29:45,440
So for you to change Paul and Andy's interaction to a positive one, you know, that creates

351
00:29:45,440 --> 00:29:48,200
a big difference.

352
00:29:48,200 --> 00:29:54,680
It's basically leave an explanation gap between your node now with Paul and Andy's interaction

353
00:29:54,680 --> 00:29:56,600
and the next one.

354
00:29:56,600 --> 00:30:02,000
So those are basically gaps that you can look at in their interactions, the character's

355
00:30:02,000 --> 00:30:06,200
interactions that are that become fairly easy to identify.

356
00:30:06,200 --> 00:30:11,200
Now you can get a lot more nuanced, but what we see in scripts generally is that's not

357
00:30:11,200 --> 00:30:11,960
necessary.

358
00:30:11,960 --> 00:30:14,600
That's not really necessary.

359
00:30:14,600 --> 00:30:21,600
If you look at successful scripts and look at their knowledge graphs, they tend to cluster

360
00:30:21,600 --> 00:30:28,480
into very fairly predictable storylines.

361
00:30:28,480 --> 00:30:33,920
So if you just map the sentiments, there are certain things like the hero's journey or

362
00:30:33,920 --> 00:30:36,360
the fall from grace.

363
00:30:36,360 --> 00:30:44,520
These are things that even Aristotle wrote about in the poetics identified a certain

364
00:30:44,520 --> 00:30:47,920
set of story archetypes.

365
00:30:47,920 --> 00:30:51,200
And things haven't really changed even after we analyze all these scripts.

366
00:30:51,200 --> 00:30:53,480
These common themes reappear.

367
00:30:53,480 --> 00:30:59,160
It could be because humans tend to respond to those type of storylines.

368
00:30:59,160 --> 00:31:03,760
So at a high level, that's kind of a mapping to understand what is the general storyline

369
00:31:03,760 --> 00:31:06,720
from the knowledge graph.

370
00:31:06,720 --> 00:31:12,800
The second phase after that is, again, in terms of good storytelling, there is always

371
00:31:12,800 --> 00:31:17,040
a tension and resolution.

372
00:31:17,040 --> 00:31:18,040
Happens throughout.

373
00:31:18,040 --> 00:31:23,480
So if you look at good storytelling, it's never a flat affect all the way through because

374
00:31:23,480 --> 00:31:26,040
that doesn't make for an interesting story.

375
00:31:26,040 --> 00:31:28,120
But something always happens, right?

376
00:31:28,120 --> 00:31:32,120
Even if you're writing a story about somebody doing research, it's not like they kept

377
00:31:32,120 --> 00:31:34,880
working night and day and then found a solution.

378
00:31:34,880 --> 00:31:38,320
It's like, no, they worked really hard.

379
00:31:38,320 --> 00:31:43,960
They were close to a breakthrough and something happened and the world came crashing down the

380
00:31:43,960 --> 00:31:50,400
next day like they're, you know, some of the relatives entered into a car crash and they

381
00:31:50,400 --> 00:31:51,400
got depressed.

382
00:31:51,400 --> 00:31:54,000
So there's always has to be something.

383
00:31:54,000 --> 00:32:00,160
You need that volatility in the story just to attract the audience.

384
00:32:00,160 --> 00:32:05,400
So those are things that can be extracted, you know, without necessarily going too deep

385
00:32:05,400 --> 00:32:12,880
into kind of an emotional analysis or sentiment analysis of the script, even like more higher

386
00:32:12,880 --> 00:32:19,400
level things that we can extract, like positive negative interactions, friendly, unfriendly

387
00:32:19,400 --> 00:32:20,400
interactions.

388
00:32:20,400 --> 00:32:24,960
Those are things that we can extract from the script and that can give us a very good

389
00:32:24,960 --> 00:32:28,520
read into how well it would resonate with the audience.

390
00:32:28,520 --> 00:32:33,400
You've helped us understand the problem that you're taking on, providing, since you

391
00:32:33,400 --> 00:32:37,120
create a support for script writers.

392
00:32:37,120 --> 00:32:43,200
In many cases centered around this knowledge graph and you've mentioned, you know, using

393
00:32:43,200 --> 00:32:50,440
techniques like Bayesian inference, but in trying to apply kind of the full breadth of

394
00:32:50,440 --> 00:32:56,960
machine learning, NLP, NLG, NLU, you know, all of this stuff to these types of problems.

395
00:32:56,960 --> 00:33:03,240
I'm curious if you can talk through the, I just, what are, what are some of the

396
00:33:03,240 --> 00:33:08,000
roadblocks that you've run into applying, you know, things that may have been created

397
00:33:08,000 --> 00:33:12,400
in some research lab to, you know, practical tools that you're trying to put in the hands

398
00:33:12,400 --> 00:33:17,080
of script writers and how have you overcome those challenges?

399
00:33:17,080 --> 00:33:23,760
Yeah, I mean, you know, our, we have had to develop a lot of new methods along the way.

400
00:33:23,760 --> 00:33:31,760
So the, one of the big challenges whenever we deal with generative models is a lot of

401
00:33:31,760 --> 00:33:38,480
the research papers. They look really promising because you read it and it looks like this

402
00:33:38,480 --> 00:33:42,120
natural language generation technique really works, but a lot of the times it could just

403
00:33:42,120 --> 00:33:49,240
be really hand-picked answers, hand-picked solutions, and, you know, working with that very

404
00:33:49,240 --> 00:33:51,320
specific kind of data set.

405
00:33:51,320 --> 00:33:57,320
So, you know, LSTM, right, with that was an experiment we did, if we could just generate

406
00:33:57,320 --> 00:34:01,720
a script using LSTM, what it would look like and we ended up with sunscreen.

407
00:34:01,720 --> 00:34:09,080
And in trying to make that useful, we realized a lot of, you know, a lot of gaps, right,

408
00:34:09,080 --> 00:34:15,880
in where academic research is right now versus what we need to create this full solution.

409
00:34:15,880 --> 00:34:21,400
So with LSTMs, we needed to augment that those models in a lot of different ways.

410
00:34:21,400 --> 00:34:28,280
So first, context becomes super important. Also, we modified those models involved active

411
00:34:28,280 --> 00:34:33,280
learning because we wanted to learn preference functions, we wanted the model to kind of

412
00:34:33,280 --> 00:34:43,280
be, wanted to train it more iteratively. Also, we started looking at conditioning on like

413
00:34:43,280 --> 00:34:50,680
the knowledge graph to generate dialogue or generate next responses instead of just

414
00:34:50,680 --> 00:34:56,680
an unsupervised solution, which a lot of these LSTM-like models papers they talk about.

415
00:34:56,680 --> 00:35:03,520
So just from model architecture and coming up with new solutions, we had to take many

416
00:35:03,520 --> 00:35:10,160
steps beyond what was currently shown. And so, a couple of things, one is, you know,

417
00:35:10,160 --> 00:35:18,840
it may not be a great academic paper if you just combine context plus memory models,

418
00:35:18,840 --> 00:35:23,480
plus hierarchical models into one solution and show how great the results are. A lot

419
00:35:23,480 --> 00:35:28,720
of the machine learning papers kind of are publication center around coming up with

420
00:35:28,720 --> 00:35:35,920
a whole new model and showing the mathematics behind it. And that's what ends up, you

421
00:35:35,920 --> 00:35:42,400
know, getting accepted to nips and ICML. So a lot of that research is not necessarily

422
00:35:42,400 --> 00:35:48,720
directly applicable to what we're doing, whereas it's kind of like more on engineering.

423
00:35:48,720 --> 00:35:57,400
So there's a lot of engineering work that goes in, which may not be the most interesting

424
00:35:57,400 --> 00:36:03,040
machine learning advance, like a new model advance, but it's still pushing the results

425
00:36:03,040 --> 00:36:09,520
and the applications a lot along the way. So that's more on the model side. And, you

426
00:36:09,520 --> 00:36:17,720
know, I think something that there's been a lot of work in on the generative side, there's

427
00:36:17,720 --> 00:36:24,720
been a lot of work in in images. So we look at sharp looking images that have been produced

428
00:36:24,720 --> 00:36:31,960
by GANs. And that's why GANs have become such popular models. But that GANs don't work

429
00:36:31,960 --> 00:36:37,280
very effectively when you're looking at natural language generation. There we have found

430
00:36:37,280 --> 00:36:43,440
auto encoder models to be much more effective, partly because the GAN objective function,

431
00:36:43,440 --> 00:36:52,880
it starts pushing it towards defining the posterior distribution very sharply. So that's how

432
00:36:52,880 --> 00:36:57,200
you end up getting very sharp realistic looking images. But when it comes to natural language

433
00:36:57,200 --> 00:37:04,480
generation, it kind of produces the same very predictable sentences over and over. So

434
00:37:04,480 --> 00:37:09,280
that's where something like an auto encoder model works very well. So these are certain

435
00:37:09,280 --> 00:37:16,880
kind of findings that would be interesting for researchers to explore, but that doesn't

436
00:37:16,880 --> 00:37:22,880
really fit into a lot of the current academic models or current models of, you know, what

437
00:37:22,880 --> 00:37:26,560
gets what's a more interesting publication over another.

438
00:37:26,560 --> 00:37:34,440
So you mentioned a few things in there. One is hierarchical LSTMs. What are those and

439
00:37:34,440 --> 00:37:40,720
how do they come into play? So SunSpring, you know, it all came about because we want

440
00:37:40,720 --> 00:37:46,720
to improve models, the model beyond SunSpring. So an LSTM long short to memory model, you

441
00:37:46,720 --> 00:37:55,320
know, it does a better job than an RNN in terms of long term dependencies. So an RNN tends

442
00:37:55,320 --> 00:38:05,520
to kind of forget, you know, what was the word that was like five, six sequences ago,

443
00:38:05,520 --> 00:38:11,160
whereas an LSTM tends to remember. So like within a sentence, you get more coherence,

444
00:38:11,160 --> 00:38:16,480
but there's still limitations. So if you keep producing coherent sentences, that doesn't

445
00:38:16,480 --> 00:38:22,480
make for a story. So let's add more structure to that. So now if you're with a hierarchical

446
00:38:22,480 --> 00:38:30,240
LSTM, you can learn dependencies across sentences, for example. So now your paragraph that you're

447
00:38:30,240 --> 00:38:37,360
produced becomes more coherent, more understandable. Now you can take it a step further. Now you

448
00:38:37,360 --> 00:38:44,600
can define hierarchies, right? It could be paragraphs to sentences to words. It could

449
00:38:44,600 --> 00:38:49,240
be even beyond that. It could be multiple paragraphs to sentences to words. So these hierarchical

450
00:38:49,240 --> 00:38:59,000
LSTMs provide more consistency in the sentences or the words that have been generated.

451
00:38:59,000 --> 00:39:06,000
Are you training them hierarchically as well, meaning independently, or are they, is it

452
00:39:06,000 --> 00:39:13,360
a model that you're training end to end? Yeah, that's a very good question. So when we

453
00:39:13,360 --> 00:39:20,000
tried to train them purely unsupervised, the hierarchical models, you do get consistency

454
00:39:20,000 --> 00:39:25,400
across paragraphs. That's not necessarily the most useful when you're trying to generate

455
00:39:25,400 --> 00:39:33,080
like stories or do something interactive with a story writer or director. So we had to

456
00:39:33,080 --> 00:39:40,360
use a lot of like annotations. So annotations where basically we had human annotators saying

457
00:39:40,360 --> 00:39:49,040
these paragraphs kind of depict the same idea. Or we had encoding saying which we're using

458
00:39:49,040 --> 00:39:55,960
purely hierarchical models, then every paragraph would be sequentially. We'll make the assumption

459
00:39:55,960 --> 00:40:01,400
that every paragraph is sequentially follows a sequential format. That's not necessarily

460
00:40:01,400 --> 00:40:07,480
the case with stories. So we had to tag these stories by paragraph or by even sentences

461
00:40:07,480 --> 00:40:17,400
to say this relates to that paragraph from page 3 section 2. So those are the longer

462
00:40:17,400 --> 00:40:24,800
range dependencies that we could train our model on because we had the annotated data.

463
00:40:24,800 --> 00:40:33,680
It does strike me that a lot of the challenge in what you're doing has to do with how do

464
00:40:33,680 --> 00:40:39,240
you represent these different contexts or these different concepts and contexts for

465
00:40:39,240 --> 00:40:46,880
that matter and the relationships between them and what are the properties of, I guess

466
00:40:46,880 --> 00:40:51,240
you can get arbitrarily detailed with this. Like what are the properties of a given prop

467
00:40:51,240 --> 00:40:57,320
and is there some inconsistency in the way this LSTM is trying to generate someone's

468
00:40:57,320 --> 00:41:03,760
use of a prop or you're probably not trying to go that far. But have you learned any secrets

469
00:41:03,760 --> 00:41:08,080
or tips for you know just dealing with this representation problem generally?

470
00:41:08,080 --> 00:41:14,440
Yeah. So in terms of representation, that's why we found that the knowledge graph is

471
00:41:14,440 --> 00:41:21,320
the more of most efficient structure for learning or encoding these dependencies. It's

472
00:41:21,320 --> 00:41:28,040
the most succinct representation of these dependencies. So now we augmented our model

473
00:41:28,040 --> 00:41:36,440
so that even for the LSTM it's conditioning on all the output is conditioned on the state

474
00:41:36,440 --> 00:41:41,480
within a knowledge graph. So let's say you have dialogues that you're trying to generate

475
00:41:41,480 --> 00:41:47,640
for a character. Now node number 5 is where let's say we need in node in scene 5 is where

476
00:41:47,640 --> 00:41:53,680
we need to generate new dialogue. So we could either go with everything, all the words

477
00:41:53,680 --> 00:42:01,400
that were written before or we represent the state of node 5 and then the LSTM generates

478
00:42:01,400 --> 00:42:09,520
output conditioned of how you've encoded that state in node number 5. So that's a much

479
00:42:09,520 --> 00:42:17,840
much better representation and I think you know the more research needs to be done in looking

480
00:42:17,840 --> 00:42:21,960
at these knowledge graphs and how you can condition on these knowledge graphs to do

481
00:42:21,960 --> 00:42:28,880
a generation. I'm curious if the concept of embeddings comes into play when you're building

482
00:42:28,880 --> 00:42:36,680
these models? Definitely. So you can't just rely on just straight straight up words because

483
00:42:36,680 --> 00:42:42,680
again when we're looking at words embeddings becomes very important. You have analogies,

484
00:42:42,680 --> 00:42:48,440
you have similarities. So we have to learn we base everything on learning a word and embedding

485
00:42:48,440 --> 00:42:54,040
first. And generally I mean that's you know it's the whole natural language generation and

486
00:42:54,040 --> 00:43:01,560
understanding is a very interesting problem in its own right because everything is so context

487
00:43:01,560 --> 00:43:08,080
dependent compared to let's say image recognition. And you know that's why you know you have

488
00:43:08,080 --> 00:43:16,640
a company like Google and Facebook because they have the largest data set of images you

489
00:43:16,640 --> 00:43:23,440
can see the most variation in that data. But that doesn't necessarily work so much when

490
00:43:23,440 --> 00:43:30,840
you're dealing with natural language where you really have to focus on a particular focusing

491
00:43:30,840 --> 00:43:36,680
on a particular vertical gives you much better results on trying to go after a broad problem.

492
00:43:36,680 --> 00:43:42,600
You know there's some of the natural language processing APIs out there cannot even get

493
00:43:42,600 --> 00:43:49,600
anywhere close to what we want to do because they're going after a lot of breath in their

494
00:43:49,600 --> 00:43:55,640
responses. So that's why in terms of like understanding sentences in their context to

495
00:43:55,640 --> 00:44:02,600
given depth is very difficult because context changes kind of so much based on the domain

496
00:44:02,600 --> 00:44:08,000
and the vertical. You also mentioned auto encoders can you share a little bit more about

497
00:44:08,000 --> 00:44:15,480
how you've used those? Yes so auto encoders we've found work very well with natural

498
00:44:15,480 --> 00:44:23,360
language generation. So what an auto encoder does is you know the input layers have some

499
00:44:23,360 --> 00:44:31,080
number of nodes. The hidden layers actually have fewer nodes than or fewer units than the

500
00:44:31,080 --> 00:44:37,720
input layer and the output layer. So what an auto encoder does is it starts with a large

501
00:44:37,720 --> 00:44:44,240
number of input input nodes. So let's say 100 and the output nodes could be another 100.

502
00:44:44,240 --> 00:44:51,640
But the hidden layers would be fewer like 20 nodes. So what that forces the auto encoder

503
00:44:51,640 --> 00:45:00,840
to do because it's using fewer nodes fewer units in the middle. It has to compress the

504
00:45:00,840 --> 00:45:07,480
input and try to generate the output of the same dimensionality. But it has to compress

505
00:45:07,480 --> 00:45:15,360
the data that's coming in. So what that does in terms of language is you know it forces

506
00:45:15,360 --> 00:45:21,760
one way of understanding it. It takes words and forces that into concepts. So let's say

507
00:45:21,760 --> 00:45:29,400
there's you know the input you want to represent the idea that I take walk stale. There are

508
00:45:29,400 --> 00:45:36,840
different ways of saying that like I'm a regular pedestrian. I occasionally walk around

509
00:45:36,840 --> 00:45:44,240
a lot. So the same idea can be expressed in multiple formats. So an auto encoder because

510
00:45:44,240 --> 00:45:50,480
it's compressing the data that's coming in kind of maps things or is forced to map words

511
00:45:50,480 --> 00:45:58,720
into concepts in non supervised way. Are you doing anything where you're taking those

512
00:45:58,720 --> 00:46:05,480
concept vectors from the middle part of the auto encoder and then using those as representations

513
00:46:05,480 --> 00:46:10,160
in your knowledge graph. Is that kind of what you're getting at? Yes. So those kind of

514
00:46:10,160 --> 00:46:16,000
form you know they're basically they can form the states of the knowledge graph. So the

515
00:46:16,000 --> 00:46:21,560
state doesn't necessarily of a knowledge graph doesn't necessarily need to be something

516
00:46:21,560 --> 00:46:28,800
that is human interpretable. It could be you know the middle layers of an auto encoder.

517
00:46:28,800 --> 00:46:37,200
That could inform the state. Oh interesting. It's an interesting case study perhaps on

518
00:46:37,200 --> 00:46:45,480
the you know the practical applications of of AI like all of the different pieces that

519
00:46:45,480 --> 00:46:53,240
you've had to put together to build the solution. Very very interesting. We started off in

520
00:46:53,240 --> 00:46:59,280
something that can be considered fairly niche because these are scripts written for you

521
00:46:59,280 --> 00:47:06,200
know Hollywood. But from business point of view a Hollywood itself is a very big industry.

522
00:47:06,200 --> 00:47:15,640
But even if you look at beyond Hollywood it's the whole content creation industry is massive

523
00:47:15,640 --> 00:47:22,680
in hundreds of billions of dollars. So although it's a very niche problem the economic impact

524
00:47:22,680 --> 00:47:30,360
of it is very substantial you know it's going it's going after one of the biggest markets.

525
00:47:30,360 --> 00:47:38,840
So at this point still there's a lot of you know trying different experiments or taking

526
00:47:38,840 --> 00:47:43,000
different ideas or taking different models and putting these things together. I think

527
00:47:43,000 --> 00:47:50,640
there's fundamentally more work that can be done you know to to build a whole new class

528
00:47:50,640 --> 00:47:58,360
of models that are able to take context you know in different settings able to encode

529
00:47:58,360 --> 00:48:05,240
relationships between entities between people and use that in generation. Now that's that

530
00:48:05,240 --> 00:48:13,160
is a very interesting research problem to go after. And so as a startup trying to bring

531
00:48:13,160 --> 00:48:23,440
an AI product to market do you see part of your contribution is kind of advancing that

532
00:48:23,440 --> 00:48:29,040
research effort or is that outside of scope of what you're trying to do and if someone

533
00:48:29,040 --> 00:48:36,120
does it great you'll use it but you're not in a position to push these research types

534
00:48:36,120 --> 00:48:42,040
of questions. How do you manage or balance that? As a startup we discovered whole new

535
00:48:42,040 --> 00:48:48,440
market instead of applications we started in you know in Hollywood and analyzing scripts

536
00:48:48,440 --> 00:48:54,760
because that data is plentiful like there's a lot of public data available but we found

537
00:48:54,760 --> 00:49:01,200
that a lot of companies fortune 500 fortune 1000 companies have the same need they want

538
00:49:01,200 --> 00:49:06,600
to produce content you know they all are striving to produce better and better content.

539
00:49:06,600 --> 00:49:15,640
So you know you we trained our AI trained our models on the best data out there or we we

540
00:49:15,640 --> 00:49:21,680
cut our teeth on the harder problem which is producing really high quality storytelling

541
00:49:21,680 --> 00:49:26,880
high quality content that's you know what Hollywood is all about and then applying that

542
00:49:26,880 --> 00:49:34,480
to a broader set of content creation that is very specific like a particular company

543
00:49:34,480 --> 00:49:40,200
you know they're they have a certain context in which they want to produce their content.

544
00:49:40,200 --> 00:49:47,720
So that you know our model is basically translate over. So generally I think you know as a startup

545
00:49:47,720 --> 00:49:54,400
we can't go after you know the broad class of problems and spend years dedicated to one

546
00:49:54,400 --> 00:50:05,240
or two core areas. So our goal is to produce products that our end users want at the end

547
00:50:05,240 --> 00:50:11,680
of the day are end users are producers production companies corporations producing content.

548
00:50:11,680 --> 00:50:17,280
So that's who we are catering to but along the way we've discovered a lot of very good

549
00:50:17,280 --> 00:50:22,640
problems and that's one of the reasons we have collaborated with a lot of universities.

550
00:50:22,640 --> 00:50:29,720
So we have strong collaborations going on with with Caltech and you know we've like hired

551
00:50:29,720 --> 00:50:37,960
interns to work on research problems very specifically because you know I think eight like discovering

552
00:50:37,960 --> 00:50:43,360
these solutions kind of help the community overall and I wish more and more people would

553
00:50:43,360 --> 00:50:50,480
look at these problems because you know that's benefiting everybody but also we can kind

554
00:50:50,480 --> 00:50:56,360
of discover or we're always looking at the at the research findings so we'll notice them

555
00:50:56,360 --> 00:51:01,160
before everybody else and we'll you know implement them towards problems where we we have

556
00:51:01,160 --> 00:51:07,080
a path to market. So that's our approach which is you know trying to build the best product

557
00:51:07,080 --> 00:51:14,040
but also keeping a close eye on research out there and also funding and collaborating

558
00:51:14,040 --> 00:51:19,960
with research labs and universities. It's been really interesting kind of chatting about

559
00:51:19,960 --> 00:51:26,760
how your path bringing this to bringing this to market. I think they've been a bunch of

560
00:51:26,760 --> 00:51:32,680
interesting tidbits for me but probably the most so you want is this just all of the different

561
00:51:32,680 --> 00:51:38,040
pieces that you've pulled together to build a solution and when I think about you know what

562
00:51:38,040 --> 00:51:47,240
it means to build kind of a knowledge graph for these scripts it strikes me as a really you

563
00:51:47,240 --> 00:51:53,160
know potentially you know challenging problem like in a lot of ways you know it's not although

564
00:51:53,160 --> 00:52:01,880
the scale is very different in terms of the number of documents that you're trying to incorporate

565
00:52:01,880 --> 00:52:08,600
into this graph the the challenge in a lot of ways doesn't seem all that dissimilar from you

566
00:52:08,600 --> 00:52:13,400
know what a Google's knowledge graph you know in terms of the diversity of concepts that you have

567
00:52:13,400 --> 00:52:18,920
to represent in it and the way you're pulling together all these technologies to support that

568
00:52:18,920 --> 00:52:27,640
is very interesting. Yeah thank you and and you know yes the volume is less but we do have a lot

569
00:52:27,640 --> 00:52:36,200
of complexity in the relationships and the data that we're trying to analyze. So as a result of

570
00:52:36,200 --> 00:52:41,720
that we've had to you know build some really solid proprietary data sets we have to had a we

571
00:52:41,720 --> 00:52:48,040
needed a lot of annotations so fortunately we're you know we offer the product that

572
00:52:48,040 --> 00:52:54,040
products a lot of those annotations and we could train our system so that goes hand in hand

573
00:52:54,040 --> 00:53:00,280
you know landing on you know our products are used by production company and producers that

574
00:53:01,000 --> 00:53:07,000
so we have the best data for this type of problems and then we are looking for different ways to

575
00:53:07,000 --> 00:53:13,720
like use that and enriching the knowledge graph in producing better generative output.

576
00:53:13,720 --> 00:53:17,560
Just out of curiosity what does your technology stack look like?

577
00:53:18,280 --> 00:53:25,480
Most of our models are code and in TensorFlow we use Python there's different for on the product

578
00:53:25,480 --> 00:53:35,320
side you know we use more Node.js and web interfaces and for our knowledge graph and quoting a lot

579
00:53:35,320 --> 00:53:43,640
of that internally I'm a champion of OCaml. A Lisp guy? Well OCaml has related to that right?

580
00:53:43,640 --> 00:53:51,160
Yeah OCaml is a functional programming language like Lisp but OCaml does a few other nice things

581
00:53:51,160 --> 00:53:57,720
like it has formal verification that actually helps I don't think the research community is really

582
00:53:58,600 --> 00:54:04,920
figured out how to make best use of that but if you incorporate formal verification

583
00:54:04,920 --> 00:54:10,920
you can do knowledge graphs pretty well there's just some properties that you can exploit from

584
00:54:10,920 --> 00:54:19,080
formal verification to construct knowledge graphs and also OCaml has object you know you can

585
00:54:19,080 --> 00:54:25,960
encode objects and classes that means you can have a richer you encode richer data sets that

586
00:54:25,960 --> 00:54:34,600
you can't do with you know Haskell and Lisp for sure. So functional programming languages generally

587
00:54:34,600 --> 00:54:43,400
I think are the way to go for programming in AI because you know functions are primitives

588
00:54:43,400 --> 00:54:48,520
and at the end of the day you're you're doing lots and lots of functional operations on pieces

589
00:54:48,520 --> 00:54:54,920
of data. So I think like the whole AI community could do well to switch to you know functional

590
00:54:54,920 --> 00:55:02,600
programming languages from you know Python and all the imperative languages that people use now.

591
00:55:03,560 --> 00:55:08,840
Awesome well Dave thanks so much for taking the time to chat today it's been really interesting.

592
00:55:09,640 --> 00:55:11,240
Thank you very much for having me Sam.

593
00:55:15,240 --> 00:55:21,480
All right everyone that's our show for today for more information on Dave or any of the topics

594
00:55:21,480 --> 00:55:27,400
covered in this episode head over to twimmalai.com slash talk slash 178.

595
00:55:28,840 --> 00:55:33,320
If you're a fan of the podcast we'd like to encourage you to visit your Apple or Google

596
00:55:33,320 --> 00:55:39,720
podcast app and leave us a five star rating and review. Your reviews help inspire us to create

597
00:55:39,720 --> 00:55:45,320
more and better content and they help new listeners find the show. As always thanks so much

598
00:55:45,320 --> 00:55:54,440
for listening and catch you next time.

