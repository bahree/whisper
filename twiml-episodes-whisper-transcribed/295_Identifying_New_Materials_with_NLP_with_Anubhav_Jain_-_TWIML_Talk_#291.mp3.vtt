WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.360
I'm your host Sam Charrington. Hey everyone, Sam again with another quick Twimble

00:35.360 --> 00:41.440
Con update. One of the things that's been especially exciting to see is the number of organizations

00:41.440 --> 00:47.320
sending multiple people in some cases entire teams to Twimble Con to learn about scaling

00:47.320 --> 00:52.280
and operationalizing machine learning. A full third of the companies attending are sending

00:52.280 --> 00:59.400
groups in many cases three four and five people. This is awesome. Seeing so many teams attending

00:59.400 --> 01:03.680
is a great indicator that folks really see the opportunity associated with improving the

01:03.680 --> 01:08.040
efficiency of their data science and machine learning operations and are excited about

01:08.040 --> 01:12.520
the conversations we'll be curating at the event. If you'd like to attend Twimble

01:12.520 --> 01:18.880
Con with your team just reach out to me at Sam at Twimlai.com and let's make it happen.

01:18.880 --> 01:22.520
Of course you're welcome to reach out to me if you want to attend as an individual or

01:22.520 --> 01:31.120
just head over to twimblecon.com slash register to sign up. All right on to today's show.

01:31.120 --> 01:36.800
All right everyone I am on the line with Anubav Jane. Anubav is a staff scientist and

01:36.800 --> 01:42.200
chemist at Lawrence Berkeley National Lab as well as the group lead for the hacking materials

01:42.200 --> 01:45.880
research group. Anubav welcome to this week in machine learning and AI.

01:45.880 --> 01:49.840
Yeah it's good to be here. I've actually learned a lot by listening to your podcast so

01:49.840 --> 01:54.280
it's really exciting to be able to share my work with with your listeners as well.

01:54.280 --> 01:59.680
That is fantastic to hear. So the group you lead is called the hacking materials research

01:59.680 --> 02:05.600
group. That is such a compelling name. Tell us a little bit more about your kind of focus

02:05.600 --> 02:08.000
and charter and what you're trying to accomplish.

02:08.000 --> 02:13.520
Yeah so what we're really trying to do with the research group is to imply computing to

02:13.520 --> 02:18.440
accelerate the process of finding new materials for functional applications. So new chemical

02:18.440 --> 02:23.960
compositions or new crystal structures that have really interesting properties. And these

02:23.960 --> 02:29.720
days it's actually possible to use computer simulations and also data mining to really

02:29.720 --> 02:35.640
change the way that that process is being done. So my group works both on theoretical

02:35.640 --> 02:40.440
methods to predict materials properties. And it also works on data mining and machine

02:40.440 --> 02:43.680
learning techniques to help accelerate materials design.

02:43.680 --> 02:50.440
Awesome and so what led you to combining computing and machine learning and materials.

02:50.440 --> 02:51.440
What was your path?

02:51.440 --> 02:56.240
Yeah so I got into material science because I really liked kind of its central premise

02:56.240 --> 03:00.880
which is that you know everything whether it's an airplane or a computer processor or

03:00.880 --> 03:06.440
a battery. It's all made of materials and the properties of those materials really determine

03:06.440 --> 03:10.640
the fundamental limits on the performance of every device. And you know many people have

03:10.640 --> 03:15.120
pointed out that you know whenever there's the fundamental materials advance we often

03:15.120 --> 03:20.360
name like an entire historical era after it you know the stone age the iron age the age

03:20.360 --> 03:24.840
of plastics or the silicon age. And so you know I've really been interested in you know

03:24.840 --> 03:25.840
how can we actually.

03:25.840 --> 03:29.040
I hadn't heard that before that sounds amazing.

03:29.040 --> 03:34.520
Yeah I mean really underscores the importance of materials and the opportunity is available

03:34.520 --> 03:39.200
now that we're kind of applying machine learning and data science to the field.

03:39.200 --> 03:43.280
Yeah and there's also sort of new applications like quantum computing you might have heard

03:43.280 --> 03:48.040
of or you know new biological technologies that really require materials advancements as

03:48.040 --> 03:49.040
well.

03:49.040 --> 03:54.480
But now you know typically material science has been very data poor because the only way

03:54.480 --> 03:58.840
conventionally to get information about materials was to perform an experiment. That means

03:58.840 --> 04:02.880
you have to synthesize the material put in the machine and it might be like $10,000

04:02.880 --> 04:05.200
per data point that you're collecting.

04:05.200 --> 04:09.800
And so the way that I got into this field is that during my PhD I worked with someone

04:09.800 --> 04:13.480
named Garrett Saider who was at MIT at the time he's now at Lawrence Berkeley Lab like I

04:13.480 --> 04:14.480
am.

04:14.480 --> 04:18.520
But what we were doing was we were using a particular type of computer simulation that's

04:18.520 --> 04:23.280
rooted in quantum mechanics. And then we were using simulations that actually predict

04:23.280 --> 04:27.480
the properties of materials do kind of a virtual measurement in the computer. And we

04:27.480 --> 04:32.320
could do this for maybe like one to $10 and computing costs per material. We could scale

04:32.320 --> 04:37.200
that over computing cluster so it was easy to parallelize. And then it was easy to create

04:37.200 --> 04:41.200
these databases and materials properties based on simulation data.

04:41.200 --> 04:45.600
So now we had a way to actually generate large materials data sets and maybe even two

04:45.600 --> 04:49.920
large materials data sets where you know conventional analyses were difficult to actually

04:49.920 --> 04:52.520
figure out what were the trends in that data.

04:52.520 --> 04:55.480
So I got into data science and machine learning.

04:55.480 --> 04:59.560
So we're generating some of these large databases of simulated materials properties as a way

04:59.560 --> 05:03.440
to kind of figure out what more information we could extract from some of that materials

05:03.440 --> 05:04.440
data.

05:04.440 --> 05:11.320
That's an interesting trend that I talked to folks about kind of across different domains.

05:11.320 --> 05:16.120
The integration of the use of simulated data and machine learning. I think one of the

05:16.120 --> 05:23.760
most recent conversations on this topic was in the area of astronomy if I'm remembering

05:23.760 --> 05:28.960
correctly, talk to me a little bit more about how you are combining that simulated data

05:28.960 --> 05:30.960
and machine learning.

05:30.960 --> 05:36.360
Yeah, so typically the way that it's done now isn't a kind of a tiered screening process

05:36.360 --> 05:37.360
for new materials.

05:37.360 --> 05:42.840
So when you have an idea about a new materials or a new chemical space that might be used

05:42.840 --> 05:47.000
for functional application, you might first do a machine learning prediction and that machine

05:47.000 --> 05:51.840
learning might be trained on simulated data because there usually isn't enough experimental

05:51.840 --> 05:55.800
data to train on. So you first do like a machine learning prediction to see what might

05:55.800 --> 06:00.600
be the interesting materials candidates within this broad chemical space.

06:00.600 --> 06:04.240
Then you might run some simulations on the things that are interesting for machine learning.

06:04.240 --> 06:09.520
The simulations are a little bit more expensive and complicated and time consuming to do.

06:09.520 --> 06:15.040
And then finally, if it passes both of those tests, you can actually do experiments.

06:15.040 --> 06:18.360
You might want to actually target doing some real experiments on that material.

06:18.360 --> 06:24.040
So that's one paradigm in which the techniques are working together.

06:24.040 --> 06:29.960
There's also kind of active learning and adaptive design type frameworks where you do this whole

06:29.960 --> 06:31.360
thing in a loop.

06:31.360 --> 06:35.920
So you do iterative machine learning and based on results of the experiments, you retrain

06:35.920 --> 06:40.760
your machine learning model, run the computations and do this whole thing in a loop as well.

06:40.760 --> 06:45.160
So there's a few different kind of paradigms in which you can mix these different techniques.

06:45.160 --> 06:50.480
Do you apply different types of domain adaptation techniques when you're trying to use the simulated

06:50.480 --> 06:52.600
data in the materials domain?

06:52.600 --> 06:57.360
Is that a technique that's effective in the kind of for-cure doing?

06:57.360 --> 07:02.320
Yeah, so I'm actually unfamiliar with the term domain adaptation technique.

07:02.320 --> 07:03.320
What does that refer to?

07:03.320 --> 07:08.440
I guess, you know, similar in some ways to data augmentation.

07:08.440 --> 07:15.000
It's really take, for example, in autonomous vehicles, you know, one of the things

07:15.000 --> 07:19.840
that you hear people doing is training on, like, you know, different types of simulations

07:19.840 --> 07:25.440
like video games, but the models that are trained in these simulated environments don't

07:25.440 --> 07:32.040
generalize well to the real world, so they'll do different types of things to adapt the

07:32.040 --> 07:38.640
or modify the simulation to help the model generalize or perform.

07:38.640 --> 07:44.720
So, for example, they might, you know, take, you know, daylight scenes and turn them into

07:44.720 --> 07:48.560
nighttime scenes or manipulate them so they seem more realistic.

07:48.560 --> 07:53.960
I'm wondering if there are any kinds of tools or techniques that you apply to make models

07:53.960 --> 07:59.920
trained on simulated data rather more applicable to the real world interactions?

07:59.920 --> 08:07.080
Yeah, so I would say there's two parallel threads, none of which is maybe exactly what you're

08:07.080 --> 08:09.160
mentioning here.

08:09.160 --> 08:12.760
One of them is just kind of rooted in physics and trying to make the models as realistic

08:12.760 --> 08:16.000
as possible by improving the underlying physics of those models.

08:16.000 --> 08:20.080
So, try to just get them to be closer and closer to reality.

08:20.080 --> 08:24.680
The second kind of approach that's taken that's somewhat similar is kind of a multi-fidelity

08:24.680 --> 08:29.920
approach where you have some experimental data, you have a whole bunch of computational

08:29.920 --> 08:36.000
data, and then you maybe use some of the computational predictions as features to help train

08:36.000 --> 08:40.120
your experimental model, your model training on experimental data.

08:40.120 --> 08:45.240
So, I think those sorts of approaches exist, but I haven't seen any sort of, you know, systematic

08:45.240 --> 08:50.560
transformation between computational data and kind of real world results other than those

08:50.560 --> 08:51.560
sorts of examples.

08:51.560 --> 08:53.040
Oh, really interesting stuff.

08:53.040 --> 08:55.960
Now, I probably should have mentioned this earlier.

08:55.960 --> 09:02.120
We're primarily going to be focused not necessarily on some of the materials applications of machine

09:02.120 --> 09:09.520
learning that you're working on, but rather some work that you recently published on applying

09:09.520 --> 09:15.280
natural language processing to capturing information from academic literature.

09:15.280 --> 09:21.280
So, maybe if you can talk a little bit about that work and its motivations and the connection

09:21.280 --> 09:25.680
between it and what you're doing on the material side.

09:25.680 --> 09:33.240
Yeah, so, you know, there are many, many millions, probably 50 to 100 million research articles,

09:33.240 --> 09:37.120
scientific research articles that have been published, and there's probably been trillions

09:37.120 --> 09:41.640
of dollars of investment in actually funding the research studies that typically the only

09:41.640 --> 09:45.400
output of those research studies is a published research article.

09:45.400 --> 09:50.640
So there is a ton of knowledge and a ton of opportunity in terms of extracting information

09:50.640 --> 09:53.640
from published research articles.

09:53.640 --> 09:58.040
And today, you know, it's kind of crazy, but the main way to actually get information

09:58.040 --> 10:02.000
from research articles is to have, you know, researchers read them.

10:02.000 --> 10:04.080
And that process is really, you know, limited.

10:04.080 --> 10:08.720
I mean, as a researcher, I probably read a few dozen articles, like really read a few

10:08.720 --> 10:13.640
dozen articles per year, maybe for some people, it's a few hundred, but there are, you know,

10:13.640 --> 10:19.040
thousands of articles published on any domain in any given year.

10:19.040 --> 10:23.880
So what we were trying to do in our study was to design some kind of a system that would

10:23.880 --> 10:30.000
be able to, let's say, read these articles in some sense and synthesize information about

10:30.000 --> 10:36.520
what those articles were saying, and be able to conceptualize, you know, what was happening

10:36.520 --> 10:40.360
in material science by reading these articles.

10:40.360 --> 10:45.560
And not only be able to have these internal conceptions of what the articles were talking

10:45.560 --> 10:52.560
about or to make predictions that could be testable and help guide researchers in their studies.

10:52.560 --> 10:57.640
And so in this paper, what we did was we actually collected the abstracts of over three million

10:57.640 --> 11:01.160
articles restricted to material science.

11:01.160 --> 11:04.240
We did some data pre-processing on those articles.

11:04.240 --> 11:08.600
And then we actually trained an unsupervised algorithm called Word2Vec.

11:08.600 --> 11:11.240
So this algorithm didn't receive any label data.

11:11.240 --> 11:13.680
It didn't receive any chemical training.

11:13.680 --> 11:19.840
But we found that by applying this algorithm on just the data set of material science articles,

11:19.840 --> 11:24.960
it was able to conceptualize concepts like the periodic table.

11:24.960 --> 11:30.720
And it could also predict what functional, what materials should be studied for functional

11:30.720 --> 11:32.200
applications.

11:32.200 --> 11:37.800
We found that through this procedure, we were able to predict, you know, materials that

11:37.800 --> 11:41.880
should be studied for various applications many years before they were actually reported

11:41.880 --> 11:43.240
in the literature.

11:43.240 --> 11:47.560
So we did this process where we actually kind of virtually went back in time, made some

11:47.560 --> 11:51.040
predictions about what material scientists should be studying, what materials they should

11:51.040 --> 11:53.120
be studying for different applications.

11:53.120 --> 11:57.600
And then we saw, you know, in reality, that they actually study those materials that we

11:57.600 --> 12:00.600
predicted over the next few years.

12:00.600 --> 12:04.320
And we found that, you know, we could actually predict at a very high rate what researchers

12:04.320 --> 12:05.840
would be studying in the future.

12:05.840 --> 12:14.400
Well, if we can pause here and maybe unpack some of that, I think probably many listeners

12:14.400 --> 12:21.400
are generally familiar with Word2Vec and the concept of Word embeddings.

12:21.400 --> 12:26.320
We've covered it quite a bit on the podcast.

12:26.320 --> 12:31.360
But you say that, you know, in creating an embedding model, you're able to, the, or the

12:31.360 --> 12:35.960
model is able to conceptualize the, the periodic table.

12:35.960 --> 12:39.520
What does that mean for a model to be able to conceptualize the periodic table or what

12:39.520 --> 12:41.440
are you trying to express with that?

12:41.440 --> 12:42.440
Yeah.

12:42.440 --> 12:46.360
So, you know, as you mentioned, there's been a lot of research into Word2Vec and people have

12:46.360 --> 12:51.160
already demonstrated many times that it can, you know, understand grammatical relationships.

12:51.160 --> 12:56.720
It can also understand semantic relationships like the concept of gender or capitals, etc.

12:56.720 --> 13:00.720
What we're trying to show with this study is that Word2Vec also captures scientific

13:00.720 --> 13:02.520
relationships.

13:02.520 --> 13:06.280
So it actually captures knowledge that requires some amount of scientific knowledge in order

13:06.280 --> 13:10.680
to, in order to, to express that relationship.

13:10.680 --> 13:15.960
So for the periodic table, specifically, what we did is we, we trained these Word embeddings

13:15.960 --> 13:18.800
and then we looked at Word embeddings of the chemical elements.

13:18.800 --> 13:23.600
So words like helium, words like sodium, words like lithium.

13:23.600 --> 13:27.360
And so these are 200 dimensional vectors that represent these words.

13:27.360 --> 13:31.480
And then we projected those 200 dimensional embeddings into two dimensions, like the

13:31.480 --> 13:33.120
periodic table.

13:33.120 --> 13:38.800
We used T-SNE as the embedding mechanism, sorry, as the projection mechanism.

13:38.800 --> 13:42.080
And then we found that when we actually projected these Word embeddings, which were trained

13:42.080 --> 13:46.520
just on the way that chemists and material scientists mentioned these elements in the

13:46.520 --> 13:51.320
course of their research, we actually got the same sorts of trends that you saw on the

13:51.320 --> 13:52.800
periodic table.

13:52.800 --> 13:57.800
So without really being explicitly trained on what the structure of the periodic table

13:57.800 --> 14:02.440
was, we could recover that structure simply by reading these articles and, you know,

14:02.440 --> 14:06.400
projecting these embeddings down to two dimensions.

14:06.400 --> 14:12.960
And just to interrupt again, when you say recover that structure, meaning if you somehow

14:12.960 --> 14:17.560
visualize this two dimensional embedding space and squint the right way, you can kind

14:17.560 --> 14:23.600
of see the periodic table in there, or you can systematically, you know, further transform

14:23.600 --> 14:28.320
this projection and get to something very close to the periodic table.

14:28.320 --> 14:32.760
Yeah, so we did kind of two types of tests.

14:32.760 --> 14:36.920
The first one is more similar to what you described at first, which is that when you project

14:36.920 --> 14:41.480
the elements in two dimensions, you kind of see these clusters that correspond to the

14:41.480 --> 14:43.520
types of groups in the periodic table.

14:43.520 --> 14:46.440
So things like the alkali metals are all grouped together.

14:46.440 --> 14:49.160
The halogens are all grouped together, et cetera.

14:49.160 --> 14:53.680
And there's also structure within the clusters that you can see that correspond to moving

14:53.680 --> 14:56.680
to certain directions in the periodic table.

14:56.680 --> 15:01.120
But now, you know, you're, I think you've talked on this show before about how T-SNE can

15:01.120 --> 15:04.080
distort things and distort distances, et cetera.

15:04.080 --> 15:08.400
And so in addition to kind of just that T-SNE visualization and qualitatively looking

15:08.400 --> 15:13.880
at it, we did another thing, which is we actually first took these two hundred dimensional

15:13.880 --> 15:19.040
word embeddings and projected them down to 15 dimensions using just PCA.

15:19.040 --> 15:24.280
And then we saw whether certain directions in that 15 dimensional embedding space corresponded

15:24.280 --> 15:26.920
to various directions in the periodic table.

15:26.920 --> 15:31.440
So for example, we saw whether there were certain directions in this 15 dimensional space

15:31.440 --> 15:36.800
that corresponded to increasing the atomic weight in the periodic table or increasing the

15:36.800 --> 15:39.840
electro negativity in the periodic table.

15:39.840 --> 15:44.200
So what we were really doing is to see are there certain directions in this embedding space

15:44.200 --> 15:48.640
that correspond to moving in certain directions in the periodic table?

15:48.640 --> 15:53.040
And we found high correspondence in that as well, things like our squared values of about

15:53.040 --> 15:54.360
point eight or so.

15:54.360 --> 15:58.800
So we also did a more quantitative test of whether the information in the periodic table

15:58.800 --> 16:00.320
was being captured by these embeddings.

16:00.320 --> 16:02.320
Oh, really interesting.

16:02.320 --> 16:10.600
Now, can you take a moment to kind of review or overview T-SNE and PCA for those that aren't

16:10.600 --> 16:15.400
familiar with those terms and kind of how they play out in these experiments?

16:15.400 --> 16:21.440
Yeah, so T-SNE is 10s for T-Stochastic Neighbor embedding.

16:21.440 --> 16:25.920
It's just a way to take a high dimensional space like the 200 dimensional word embeddings

16:25.920 --> 16:29.880
we've been talking about and projected them into a lower dimension in a way that tries

16:29.880 --> 16:35.240
to preserve distances as closely as possible.

16:35.240 --> 16:38.640
And PCA stands for Principal Component Analysis.

16:38.640 --> 16:44.440
And it's a way to try and figure out kind of new directions, so new dimensions along which

16:44.440 --> 16:49.840
to project our 200 dimensional vectors that try to capture as much information as possible

16:49.840 --> 16:53.040
while reducing the number of dimensions down.

16:53.040 --> 16:57.920
So in this case, in order to do, in order to test whether we were quantitatively reproducing

16:57.920 --> 17:02.520
the periodic table, because there's only 100 elements and there's 200 dimensions in our

17:02.520 --> 17:06.560
word embeddings to do the test fairly to see whether we were capturing these directions

17:06.560 --> 17:11.760
properly, we actually reduce it dimensionally down to 15 dimensions to make a fair test.

17:11.760 --> 17:12.760
Awesome.

17:12.760 --> 17:13.760
Awesome.

17:13.760 --> 17:20.720
And so you created this embedding space, you kind of compared it to your intuition and

17:20.720 --> 17:26.400
knowledge of chemistry and physics and found that there is some structure there that

17:26.400 --> 17:31.640
resembles what we know about the way the world works, i.e. the periodic table.

17:31.640 --> 17:41.360
And then with this as a basis, you're using it to try to predict materials that will be

17:41.360 --> 17:44.840
useful in further research and literature.

17:44.840 --> 17:49.440
How do you make that jump from the embedding space to these kinds of predictions?

17:49.440 --> 17:50.440
Yeah.

17:50.440 --> 17:56.280
So when you train these embeddings, you typically give it some kind of a task in which

17:56.280 --> 17:58.080
you train the embeddings on.

17:58.080 --> 18:03.320
So in the case of word to veck, the task that you train the embeddings on, there's multiple

18:03.320 --> 18:06.200
versions, but we use something called skipgram.

18:06.200 --> 18:10.080
And the task there is that given a certain word, that word could be a chemical element

18:10.080 --> 18:14.720
like helium, it could be an application word like thermoelectric, or it could be just any

18:14.720 --> 18:16.120
old word.

18:16.120 --> 18:21.560
Given a word as an input, the skipgram model tries to predict what words will appear next

18:21.560 --> 18:24.440
to that word in your document corpus.

18:24.440 --> 18:29.400
We use a window size of eight, so plus or minus eight words from our target word.

18:29.400 --> 18:33.800
So what we're training these embeddings on, the information that they represent is what

18:33.800 --> 18:34.800
are the neighbors?

18:34.800 --> 18:40.880
What are the types of neighboring words that occur next to each target word?

18:40.880 --> 18:46.240
So now we have basically for every word in our corpus, an embedding that represents what

18:46.240 --> 18:51.240
sorts of words are expected to appear next to that word in a scientific abstract, in

18:51.240 --> 18:53.440
a material science abstract.

18:53.440 --> 18:58.520
Now the way that we make this predictive is, a lot of times when people train word to

18:58.520 --> 19:02.320
veck or similar embedding algorithms, once they get the embeddings, they throw away

19:02.320 --> 19:03.320
the task.

19:03.320 --> 19:05.040
The task is not really important anymore.

19:05.040 --> 19:07.920
You have the embeddings and then you work with those.

19:07.920 --> 19:12.360
What really the lead author of this paper did, his name is Vahey Chattoyan, he decided

19:12.360 --> 19:16.400
not to throw away the task and he decided to use that task directly.

19:16.400 --> 19:20.880
And what he said is, well listen, I want a thermoelectric material, I have a model that

19:20.880 --> 19:25.040
can predict what sorts of words are likely to appear in a scientific abstract next to

19:25.040 --> 19:28.560
the word thermoelectric, that's directly my word to veck model.

19:28.560 --> 19:34.240
So let me use my word to veck model to actually directly predict, in some sense, what words

19:34.240 --> 19:36.920
will appear next to the word thermoelectric.

19:36.920 --> 19:40.000
And then he filtered those down to just chemical composition words.

19:40.000 --> 19:44.880
So which chemical compositions will likely appear next to the word thermoelectric?

19:44.880 --> 19:48.840
And most of those chemical compositions predicted by the model are things that were already

19:48.840 --> 19:54.080
known to be thermoelectric, so things that occurred in our corpus many, many times.

19:54.080 --> 19:58.960
But then when Vahey got down, so let's say the 333rd prediction, he found something new.

19:58.960 --> 20:02.880
He said, okay, here's a chemical composition that was never explicitly mentioned in our

20:02.880 --> 20:10.760
corpus as a thermoelectric, yet it's ranking pretty high in the prediction of likely to

20:10.760 --> 20:12.960
appear next to the word thermoelectric.

20:12.960 --> 20:17.120
So maybe the model is telling us something, it's telling us that there's a high likelihood

20:17.120 --> 20:20.960
that someone's going to publish a research article with this chemical composition next

20:20.960 --> 20:25.320
to the word thermoelectric, even though no one has ever published that article before.

20:25.320 --> 20:31.440
So he essentially turned the word to veck into a way to identify gaps in the research literature

20:31.440 --> 20:34.640
by using these embeddings.

20:34.640 --> 20:40.280
How do you validate your interpretation of a finding like that?

20:40.280 --> 20:51.040
You know, imagining that that 331rd could be a chemical compound that has another name

20:51.040 --> 20:58.840
or another kind of composition and has appeared, but under this other name or maybe it's just

20:58.840 --> 20:59.840
random.

20:59.840 --> 21:07.000
Like, is there, how do you validate that this is actually a function of the model that

21:07.000 --> 21:10.680
you created and not just noise?

21:10.680 --> 21:11.680
Sure.

21:11.680 --> 21:15.920
Yeah, so I should first say that we're pretty confident that the materials that we predict

21:15.920 --> 21:20.640
as new predictions are actually things that haven't been studied before.

21:20.640 --> 21:24.440
Because first of all, we can check our abstract corpus to make sure that that compound has

21:24.440 --> 21:28.920
not appeared next to the word thermoelectric or any other word that would indicate studying

21:28.920 --> 21:31.240
that application before.

21:31.240 --> 21:36.200
So part of it is that is using our corpus to figure out that this has been studied before

21:36.200 --> 21:40.440
and then we also did a lot of manual checking whenever we published a prediction on the paper

21:40.440 --> 21:45.240
just to make sure that this chemical composition that we're predicting is not actually something

21:45.240 --> 21:46.640
that has been studied.

21:46.640 --> 21:50.720
So I think we're pretty confident that it's actually a new prediction, but then whether

21:50.720 --> 21:55.160
it's correct or not is I think a more complicated question.

21:55.160 --> 21:57.840
The best validation would be to actually do the experiments.

21:57.840 --> 22:02.040
So to make the material to characterize it and then measure the thermoelectric properties,

22:02.040 --> 22:03.040
let's say.

22:03.040 --> 22:04.040
Sure.

22:04.040 --> 22:05.040
Yeah, that sounds expensive.

22:05.040 --> 22:06.040
Yeah.

22:06.040 --> 22:10.280
So we're working with collaborators right now on a couple of those top 50 materials that

22:10.280 --> 22:15.400
we predicted in our paper, but it will probably be like six months or so before we actually

22:15.400 --> 22:20.120
find out what the results are just because they have to first synthesize the material,

22:20.120 --> 22:24.480
they have to purify it, they have to characterize it and they have to optimize and dope it and

22:24.480 --> 22:25.480
all that stuff.

22:25.480 --> 22:29.240
So it's going to take a few months to figure out whether the ones that we picked out of

22:29.240 --> 22:31.120
the 50 are actually interesting or not.

22:31.120 --> 22:37.840
I mean, I imagine it's even interesting if the model can come up with plausible candidates,

22:37.840 --> 22:39.880
even if they don't ultimately work.

22:39.880 --> 22:43.320
I mean, that's what scientists do.

22:43.320 --> 22:54.280
I guess I'm trying to get at a more kind of technical validation that it's not just

22:54.280 --> 23:05.960
kind of random, you're finding these plausible predictions more consistently than just any

23:05.960 --> 23:12.840
combinations of molecules that appear that can be made up from the embedding space.

23:12.840 --> 23:14.480
Does that question make sense?

23:14.480 --> 23:15.480
Yeah, yeah.

23:15.480 --> 23:16.480
So, absolutely.

23:16.480 --> 23:20.760
And so in the absence of experiments, we actually did three types of alternate validation.

23:20.760 --> 23:26.880
So I think experiments would be the best, you know, you can't argue with it type validation.

23:26.880 --> 23:30.600
But then in the absence of experiments, we did three other types of validation.

23:30.600 --> 23:35.120
The first type of validation we did is that we had an independent data set of simulated

23:35.120 --> 23:36.800
data on thermal electrics.

23:36.800 --> 23:40.360
So these are based on, you know, physics-based simulations.

23:40.360 --> 23:45.280
So the two data sets between NLP and the simulations, they don't talk to each other.

23:45.280 --> 23:49.320
And then what we did was we looked at correspondence between the ranking of thermal electrics that

23:49.320 --> 23:52.480
we did using natural language processing technique.

23:52.480 --> 23:57.760
And the thermal electric properties as calculated by the simulated data.

23:57.760 --> 24:02.720
And what we found is like pretty remarkable, which is that if you look at, for example,

24:02.720 --> 24:09.720
the top 10 materials in the NLP rankings, there are a lot of them are like the 99th percentile

24:09.720 --> 24:15.280
99.5th percentile, et cetera, of the actual simulated data properties.

24:15.280 --> 24:20.400
So there's a high correspondence between the word-to-vec rankings, the NLP rankings,

24:20.400 --> 24:22.000
and the simulated data.

24:22.000 --> 24:26.560
And that's not only on known materials where you would expect that the word-to-vec would

24:26.560 --> 24:30.320
pick up that gets a known thermal electrics, so I'm going to rank it highly, but also

24:30.320 --> 24:31.560
on unknown materials.

24:31.560 --> 24:37.800
So we have simulated data on the predicted data and the things that we predict to be high

24:37.800 --> 24:42.200
with NLP are also high in the simulations for predictions.

24:42.200 --> 24:49.240
So that's kind of one validation is just consistency between simulations and the NLP.

24:49.240 --> 24:55.160
The second type of test that we did was against experimental data.

24:55.160 --> 25:00.440
And here what we tried to do is to say, because experimental data would be all on known thermal

25:00.440 --> 25:06.080
electric materials, we actually tried to make sure that the quality of the ranking also

25:06.080 --> 25:09.920
corresponded to the quality of the experimental results, so the quality of the ranking and

25:09.920 --> 25:11.400
the experimental results.

25:11.400 --> 25:15.960
So things that had a higher word-to-vec ranking would also have a higher experimental-based

25:15.960 --> 25:17.040
ranking.

25:17.040 --> 25:22.080
And so that second validation also panned out where there was kind of a rank correlation

25:22.080 --> 25:26.480
between our NLP results and between the actual experimental data.

25:26.480 --> 25:31.800
And then finally the third type of validation that we did was basically a forecasting-based

25:31.800 --> 25:33.440
cross-validation.

25:33.440 --> 25:38.920
And what we did is we created these virtual data sets where we asked the question, well,

25:38.920 --> 25:42.920
if we had invented this technique back in the year 2000.

25:42.920 --> 25:47.560
So we threw out all the data of abstracts from past the year 2000, and we trained the

25:47.560 --> 25:53.320
model only in the year 2000, for example, on data up to the year 2000.

25:53.320 --> 25:57.200
And then we made predictions at a particular point in time.

25:57.200 --> 26:01.240
And then we saw what actually happened in the research community in the next 19 years

26:01.240 --> 26:04.000
from 2001 to 2019.

26:04.000 --> 26:08.720
We did those predictions that we made using the 2000 data set actually pat out over the

26:08.720 --> 26:10.320
next 19 years.

26:10.320 --> 26:15.920
And we repeated this process for every year from the year 2000 to 2018 or so.

26:15.920 --> 26:20.320
Where every year we kind of virtually predicted what the algorithm would have predicted.

26:20.320 --> 26:24.080
And then compared that to what was actually observed in the research literature.

26:24.080 --> 26:28.880
And we found that the rate at which the materials that were predicted by the algorithm were

26:28.880 --> 26:32.840
actually reported in literature was much, much higher than if you had just picked random

26:32.840 --> 26:35.840
materials from the data set at the time.

26:35.840 --> 26:39.800
And even more than if you had used some kind of chemical heuristics and simulation-based

26:39.800 --> 26:42.920
heuristics to pick the materials from at the time.

26:42.920 --> 26:48.000
So something like, you know, within the first five years, you're like eight times more

26:48.000 --> 26:51.600
likely to get a thermal electric material using this algorithm versus just picking a

26:51.600 --> 26:53.200
random material.

26:53.200 --> 26:55.280
So that was like a third type of validation we did.

26:55.280 --> 26:59.640
And we found through that type of validation that often you could find new thermal electric

26:59.640 --> 27:04.600
materials or suggest new thermal electric materials five to ten years before they were first

27:04.600 --> 27:05.920
reported in the literature.

27:05.920 --> 27:08.160
Oh, that's super interesting.

27:08.160 --> 27:17.320
So it sounds like you did find that the algorithm was the predictions that the algorithm

27:17.320 --> 27:23.920
came up with tended to be kind of like low hanging fruit in a sense that they were found,

27:23.920 --> 27:26.840
you know, within the fur- they tended to be found within the first five years as opposed

27:26.840 --> 27:29.560
to, you know, something that I guess I wouldn't expect

27:29.560 --> 27:34.040
us of an algorithm to come up with truly novel compositions, you know, things that to

27:34.040 --> 27:38.480
20 years to figure out is that basically in line with what you saw?

27:38.480 --> 27:42.960
Yeah, well, so when you look at the predictions that the algorithm makes, so if you look

27:42.960 --> 27:47.640
at, for example, the predictions from right now that the algorithm makes, some of them,

27:47.640 --> 27:52.520
a lot of them, I would say, are what I would call adjacent to known thermal electrics.

27:52.520 --> 27:56.240
So these are things that, you know, maybe are fairly likely that someone would find in

27:56.240 --> 28:00.400
the next few years anyway, even if it weren't for the algorithm.

28:00.400 --> 28:05.680
But I will say that, you know, a five-year acceleration is actually a pretty big deal.

28:05.680 --> 28:10.560
The typical timescale for kind of materials development is usually quoted around 20 years

28:10.560 --> 28:13.480
based on an article that was published in the 90s.

28:13.480 --> 28:18.560
And so cutting five years off of that is actually a, would be considered a major milestone,

28:18.560 --> 28:19.560
I would say.

28:19.560 --> 28:20.560
Absolutely.

28:20.560 --> 28:25.800
Yeah, so, you know, so a lot of the predictions are, I would say, you know, chemically

28:25.800 --> 28:29.920
adjacent to some of the known thermal electrics and probably would be found in a few years

28:29.920 --> 28:31.440
regardless.

28:31.440 --> 28:36.960
But then there are also a bunch of predictions in there that look, would look strange.

28:36.960 --> 28:41.960
I would say to someone who is interested in thermoelectric materials.

28:41.960 --> 28:47.040
And those materials, I think, are more of the wild card because, you know, they could

28:47.040 --> 28:50.560
be strange and they could just be incorrect.

28:50.560 --> 28:54.240
But they could also be strange and they could also be correct at the same time, in which

28:54.240 --> 28:59.040
case it would be very interesting because it's finding really unconventional materials

28:59.040 --> 29:03.680
that, you know, may or not have been studied if it weren't for the algorithm even after

29:03.680 --> 29:06.640
10 years or even after 20 years.

29:06.640 --> 29:10.560
So, you know, it will really require someone to really, the only way to really test that

29:10.560 --> 29:15.840
out is to do experimental tests on some of them and see whether they pan out or not.

29:15.840 --> 29:18.680
But yeah, there's a mix of things that, you know, maybe would have been found in the

29:18.680 --> 29:24.480
next five years or so and things that probably wouldn't be studied even after quite some

29:24.480 --> 29:25.480
time.

29:25.480 --> 29:30.640
But I would emphasize that even five years is actually considered to be a very big acceleration.

29:30.640 --> 29:34.440
And is the testing that you're currently doing with your collaborators?

29:34.440 --> 29:40.280
Is that focused on either end of the spectrum or do you have folks looking kind of across

29:40.280 --> 29:46.720
the adjacent materials as well as the ones that are, you know, strange looking?

29:46.720 --> 29:52.000
Yeah, so, unfortunately, we don't have any dedicated funding to pay experimentalists to

29:52.000 --> 29:54.520
just look at whatever materials we'd want them to look at.

29:54.520 --> 29:58.400
If we had that, yeah, if we had that, I think we would try to spread it out a little

29:58.400 --> 30:01.240
bit more and do kind of both.

30:01.240 --> 30:04.480
But because we're kind of working, these experimentalists that are collaborating with us, they're

30:04.480 --> 30:06.960
doing it essentially in their spare time.

30:06.960 --> 30:11.200
And so they typically will work on materials that are similar to things that they're already

30:11.200 --> 30:16.400
interested in, so things that are, you know, more simple to make and things that are, you

30:16.400 --> 30:20.280
know, kind of within their comfort zone of synthesis and characterization.

30:20.280 --> 30:24.480
So I would say that the couple that we're working on are more in the adjacent category

30:24.480 --> 30:27.520
than the wild and crazy category.

30:27.520 --> 30:33.240
And is the predictions that the model is making?

30:33.240 --> 30:38.840
Is it predicting structure in addition to components?

30:38.840 --> 30:46.800
For example, I'm thinking if you gave it a prompt, like liquid or a wet, and it came

30:46.800 --> 30:51.280
up with hydrogen and oxygen, you know, that, you know, it's still missing the, you know,

30:51.280 --> 30:54.640
the two in H2O and that configuration.

30:54.640 --> 31:00.840
Is it giving you that or is it, does it leave that bit to the imagination, so to speak?

31:00.840 --> 31:04.600
Yeah, so it's giving you a full chemical composition.

31:04.600 --> 31:08.440
So it would give you the H2O, but it isn't giving you a structure.

31:08.440 --> 31:13.280
So it isn't telling you whether it's ice or water or water vapor, for example.

31:13.280 --> 31:18.160
And that is actually one of the big limitations of the study, you know, in material science,

31:18.160 --> 31:22.160
one of the main things you study is how the arrangement of atoms, not just the chemical

31:22.160 --> 31:25.560
formula, but the arrangement of atoms affect properties.

31:25.560 --> 31:30.240
And so for example, if you think of just carbon, carbon can exist as diamond, it can exist

31:30.240 --> 31:33.280
as graphite, or it could exist as like a nano tube.

31:33.280 --> 31:37.600
And in our model, all those carbons are the same, same carbon.

31:37.600 --> 31:41.840
And you know, you could imagine that a more advanced embedding technique, so some of the

31:41.840 --> 31:47.480
context-sensitive embeddings, something like BERT, for example, might be able to distinguish

31:47.480 --> 31:52.840
between those carbons and provide different predictions for carbon that are synthesized

31:52.840 --> 31:56.080
or that are, have different types of structures within them.

31:56.080 --> 31:59.680
But within our word-to-back model, where every carbon is the same regardless of whether

31:59.680 --> 32:04.440
it's diamond or graphite, it's really just the chemical formula that's being used to

32:04.440 --> 32:05.840
make the prediction.

32:05.840 --> 32:12.200
How is it coming out with the chemical formula for these different compounds?

32:12.200 --> 32:16.440
Yeah, so that's actually another one of the limitations of the current approach, which

32:16.440 --> 32:21.120
is that the current approach can only rank chemical formulas that are observed in the

32:21.120 --> 32:23.760
abstract corpus for an application.

32:23.760 --> 32:27.720
So it's not inventing new chemical formulas, it's only taking the list of, you know, tens

32:27.720 --> 32:32.360
of thousands of chemical formulas that it's detected and raking them for a particular

32:32.360 --> 32:33.360
application.

32:33.360 --> 32:39.920
So maybe we're talking about, you know, things that have thermal electric properties,

32:39.920 --> 32:45.880
but the only reason why the algorithm, the model, knows about this particular compound

32:45.880 --> 32:49.680
is because someone looked at it for something else at some other time.

32:49.680 --> 32:50.920
Yeah, that's right.

32:50.920 --> 32:55.720
And so, you know, one of the associations that it might make, for example, is that the

32:55.720 --> 32:59.800
algorithm has found that there is some overlap between compounds that are studied for photovoltaic

32:59.800 --> 33:02.760
applications and thermal electric applications.

33:02.760 --> 33:07.040
So it might detect, for example, that there's a particular material that has been studied

33:07.040 --> 33:09.080
for photovoltaic applications.

33:09.080 --> 33:12.360
It might have some other keywords that it likes that are also associated with thermal

33:12.360 --> 33:17.320
electric, like Chalkoginite or, you know, Hoysler, except, for example, and decide that,

33:17.320 --> 33:20.680
you know, here's something that has a lot of keywords that tend to associate with thermal

33:20.680 --> 33:23.200
electric, but hasn't been studied before.

33:23.200 --> 33:26.040
So it's making those sorts of connections across the literature.

33:26.040 --> 33:35.320
And so, in order to train this embedding and build out this model, did you hear where

33:35.320 --> 33:41.280
there are things that you needed to kind of invent or innovate on beyond kind of word

33:41.280 --> 33:48.440
to veck as, you know, it's becoming increasingly common, or was it a fairly straightforward application

33:48.440 --> 33:51.280
of word to veck to get to this?

33:51.280 --> 33:55.760
Yeah, so, you know, unfortunately, one of the barriers to doing scientific literature

33:55.760 --> 33:58.880
mining is that the data sets are not easily available.

33:58.880 --> 34:01.480
Is the scientific literature itself?

34:01.480 --> 34:05.840
Yeah, well, because all the abstracts and everything are under published for agreements.

34:05.840 --> 34:09.760
And so, for example, we couldn't publish the data set of abstracts to share with everybody.

34:09.760 --> 34:12.280
We were legally not allowed to.

34:12.280 --> 34:16.040
And so, and so what that implies is that anytime you do a study like this, you have to spend

34:16.040 --> 34:20.640
a lot of time in collecting the data and pre-processing the data and tokenization and all that

34:20.640 --> 34:21.640
stuff.

34:21.640 --> 34:27.840
And just to be clear, so your entire model is based only on abstracts or the full content

34:27.840 --> 34:29.640
of the articles.

34:29.640 --> 34:36.600
Yeah, so what we showed in the paper was based only on abstracts, so no full text there.

34:36.600 --> 34:40.840
And so, in terms of whether it was just straight to word to veck or, you know, something more

34:40.840 --> 34:47.520
custom, I would say all the customization came from the actual pre-processing of the data.

34:47.520 --> 34:52.800
So things like being able to detect chemical formulas and doing the tokenization properly,

34:52.800 --> 34:54.600
things like that.

34:54.600 --> 34:59.440
And being able to collect the data in the first place was a bit of an effort as well.

34:59.440 --> 35:02.600
And so, because we had to spend a lot of time in that process, we didn't have a lot of

35:02.600 --> 35:05.640
time to actually work with the algorithm itself.

35:05.640 --> 35:09.440
We also spent a lot of time verifying whether the results were correct or not.

35:09.440 --> 35:12.040
So that's where most of our time went.

35:12.040 --> 35:16.240
So this was, I would say, a pretty straightforward application of word to veck.

35:16.240 --> 35:17.680
We did test against glove.

35:17.680 --> 35:20.360
We did do some hyperprimer optimization, things like that.

35:20.360 --> 35:25.720
But definitely, I wouldn't say we made any conceptual leaps in NLP during the course of

35:25.720 --> 35:26.720
this study.

35:26.720 --> 35:27.720
Sure.

35:27.720 --> 35:28.720
Yeah.

35:28.720 --> 35:31.960
And that question was not at all to take away from what you did, but rather see if there

35:31.960 --> 35:39.440
was any hidden element that it would be worth exploring further.

35:39.440 --> 35:45.720
I think what I'm hearing is very consistent with what I hear speaking with lots of folks

35:45.720 --> 35:52.000
is actually applying this stuff involves a ton of work setting up the data pipelines.

35:52.000 --> 35:55.160
And in your case, you've been getting access to the data.

35:55.160 --> 35:56.160
Yeah.

35:56.160 --> 36:00.880
And in some sense, it's kind of interesting to know that even one of the simplest word

36:00.880 --> 36:05.440
embedding algorithms is capable of giving good results on this problem.

36:05.440 --> 36:10.680
And so it really kind of begs the question, well, what if we were to use more advanced

36:10.680 --> 36:12.600
techniques on this problem?

36:12.600 --> 36:17.320
Or if we were to expand the data set to the full text, what more might be possible?

36:17.320 --> 36:21.800
I mean, if we're already seeing such good results using simple techniques, there could be

36:21.800 --> 36:26.920
a lot of opportunity here if we were to actually spend even more time to do this a little

36:26.920 --> 36:28.720
bit more carefully.

36:28.720 --> 36:37.000
And so if you were to kind of think about the opportunities that are ahead of you kind

36:37.000 --> 36:43.360
of in this direction, how do you rank them, what are the things you're most excited

36:43.360 --> 36:48.240
about kind of building on and where do you think the true opportunities lie?

36:48.240 --> 36:49.240
Yeah.

36:49.240 --> 36:54.360
So actually, you know, a lot of the motivation for this study was not necessarily to make

36:54.360 --> 36:59.840
predictive models, but just to really improve the process of information retrieval from

36:59.840 --> 37:01.640
the scientific literature.

37:01.640 --> 37:07.360
And I think there's really a lot of opportunity in going from this mechanism where scientists

37:07.360 --> 37:12.480
are just browsing articles one by one for relevant information to really using algorithms

37:12.480 --> 37:17.400
to getting the information that they need just in time from the research literature.

37:17.400 --> 37:22.520
So to kind of use an example or analogy, you know, back in the early days of the web,

37:22.520 --> 37:27.040
if you wanted to look up Planck's constant, you would have to find a bunch of science websites

37:27.040 --> 37:31.080
browse through them and see whether one of them had the value that you wanted.

37:31.080 --> 37:34.560
And today, you just type Planck's constant into Google and it just immediately returns

37:34.560 --> 37:35.560
the answer.

37:35.560 --> 37:37.200
You don't have to browse anything.

37:37.200 --> 37:41.640
And so I think there's a lot of opportunity, just an information retrieval from these

37:41.640 --> 37:43.640
very scientific abstracts.

37:43.640 --> 37:48.800
In terms of the actual predictive models and hypothesis generation, we already touched

37:48.800 --> 37:54.680
upon the ability to make predictions on materials that are not in the data set already.

37:54.680 --> 38:00.040
So how can we actually invent word embeddings for hypothetical chemical compositions?

38:00.040 --> 38:02.680
That's an area that we're looking into right now.

38:02.680 --> 38:07.280
We're also interested, as we spoke about before, of these more context sensitive embeddings

38:07.280 --> 38:12.400
and these more advanced methods that might help us distinguish between terms and materials

38:12.400 --> 38:16.720
that aren't really the same, but are the same currently in our work-to-vec model.

38:16.720 --> 38:20.520
So these are all areas that I think we will be touching upon in the future.

38:20.520 --> 38:25.160
What haven't I asked you about this research that will be worth exploring?

38:25.160 --> 38:31.480
Yeah, you know, I think one thing that I just want to mention is that somewhat ironically,

38:31.480 --> 38:41.440
this paper is behind the paywall, but the publisher, so the publisher, Springer Nature,

38:41.440 --> 38:45.480
they've actually made the full text available to read for free on research gate, so hopefully

38:45.480 --> 38:48.960
we can put a link to that in the show notes so that people that want to read the paper

38:48.960 --> 38:54.560
but don't necessarily have access to it can actually legally read the paper.

38:54.560 --> 38:57.560
So I think that's one of the main things I just wanted to mention about this.

38:57.560 --> 39:02.880
Okay, that's awesome, and we will definitely include that link in the show notes.

39:02.880 --> 39:10.160
On above, thank you so much for, well, A, listening to the podcast and B, jumping on

39:10.160 --> 39:14.720
to share a bit of what you're working on with all of us.

39:14.720 --> 39:15.720
Thanks so much.

39:15.720 --> 39:17.720
Yeah, it's been a pleasure.

39:17.720 --> 39:18.720
Thanks.

39:18.720 --> 39:24.120
All right, everyone, that's our show for today.

39:24.120 --> 39:30.120
For more information on today's show, visit twomolai.com slash shows.

39:30.120 --> 39:35.960
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

39:35.960 --> 39:37.440
Conference.

39:37.440 --> 39:51.400
As always, thanks so much for listening and catch you next time.

