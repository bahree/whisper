1
00:00:00,000 --> 00:00:13,400
Welcome to the Twomo AI Podcast.

2
00:00:13,400 --> 00:00:18,800
I'm your host Sam Charrington.

3
00:00:18,800 --> 00:00:23,720
Hey, what's up everyone?

4
00:00:23,720 --> 00:00:28,880
It's been nearly three years since we launched the Twomo Online Meetup, initially as a way

5
00:00:28,880 --> 00:00:34,040
for listeners to connect with one another and study academic research papers together.

6
00:00:34,040 --> 00:00:39,440
The group really took off in mid-2018 though when I mentioned offhandedly in my interview

7
00:00:39,440 --> 00:00:44,820
with fast.ai's Rachel Thomas that I'd be taking their course and I invited the Twomo

8
00:00:44,820 --> 00:00:47,480
listener community to join me.

9
00:00:47,480 --> 00:00:52,240
That led to the very first Twomo study group and the folks who came together for that group

10
00:00:52,240 --> 00:00:57,960
have become the backbone of a community that's now over 3,000 members strong.

11
00:00:57,960 --> 00:01:02,600
Fast forward 18 months and we've supported each other on our personal learning journeys

12
00:01:02,600 --> 00:01:10,640
and a wide variety of courses from stanfordfast.ai, deeplearning.ai, on Kaggle projects and much

13
00:01:10,640 --> 00:01:11,880
more.

14
00:01:11,880 --> 00:01:16,040
I say all that to say that I am humbled by the role that we've been able to play in helping

15
00:01:16,040 --> 00:01:21,400
folks learn machine learning and I am excited about the new educational programs that we're

16
00:01:21,400 --> 00:01:24,320
bringing to the Twomo community this year.

17
00:01:24,320 --> 00:01:28,440
We recently launched a collaboration to bring you the causal modeling and machine learning

18
00:01:28,440 --> 00:01:32,880
course and study group which I've mentioned here before and today I'd like to share some

19
00:01:32,880 --> 00:01:37,600
new details on the AI enterprise workflow study group that we're launching in just a

20
00:01:37,600 --> 00:01:39,200
few weeks.

21
00:01:39,200 --> 00:01:42,600
If you've been listening for a while you know that I am very excited about the work going

22
00:01:42,600 --> 00:01:47,360
on in ML and AI communities to make developing and deploying machine learning and deep learning

23
00:01:47,360 --> 00:01:51,640
models in the enterprise more accessible and efficient.

24
00:01:51,640 --> 00:01:59,320
In fact we hosted an entire conference TwomoCon AI platforms on this topic just last fall.

25
00:01:59,320 --> 00:02:03,720
Until now there have been very few formal resources that folks could turn to to learn

26
00:02:03,720 --> 00:02:08,400
real world machine learning workflows and deployment strategies.

27
00:02:08,400 --> 00:02:13,760
Our folks at IBM are working to change this though with the AI enterprise workflow specialization

28
00:02:13,760 --> 00:02:17,800
that they recently made available on Coursera.

29
00:02:17,800 --> 00:02:21,920
And I am super excited to share that we've partnered with them to host a study group for

30
00:02:21,920 --> 00:02:27,760
this six course specialization program which I will personally be hosting.

31
00:02:27,760 --> 00:02:31,920
The courses in the sequence teach real world machine learning and an enterprise environment

32
00:02:31,920 --> 00:02:37,360
including applying the scientific process to understanding business use cases and structuring

33
00:02:37,360 --> 00:02:45,200
data collection visualizing and analyzing data and hypothesis testing feature engineering

34
00:02:45,200 --> 00:02:51,360
and identifying and addressing data biases selecting the best models for machine learning

35
00:02:51,360 --> 00:02:59,640
vision and NLP use cases and using ensembles deploying models using microservices and containers

36
00:02:59,640 --> 00:03:06,720
Kubernetes and Apache spark and applying unit testing to ML models and monitoring model

37
00:03:06,720 --> 00:03:09,400
performance in production.

38
00:03:09,400 --> 00:03:13,440
I am really looking forward to working through these courses and I would love for anyone

39
00:03:13,440 --> 00:03:15,920
interested in these topics to join me.

40
00:03:15,920 --> 00:03:20,200
If this sounds interesting and you'd like to learn more, I invite you to join a webinar

41
00:03:20,200 --> 00:03:27,080
that I'm hosting with Ray Lopez, the courses instructor on Saturday, February 15th at 9.30

42
00:03:27,080 --> 00:03:37,680
a.m. Pacific time to register visit twimmelai.com slash AI workflow and now on to the show.

43
00:03:37,680 --> 00:03:44,360
Everyone, I am here in San Diego at TubeCon and I am with Eris Cohen, Eris's Vice President

44
00:03:44,360 --> 00:03:47,160
for Cloud and AI at Melanox.

45
00:03:47,160 --> 00:03:49,400
Eris, welcome to the Twimmelai.com podcast.

46
00:03:49,400 --> 00:03:50,400
Thank you very much.

47
00:03:50,400 --> 00:03:51,400
I'm great to be here.

48
00:03:51,400 --> 00:03:53,920
Awesome to be chatting with you.

49
00:03:53,920 --> 00:03:56,280
Tell us a little bit about your background.

50
00:03:56,280 --> 00:04:01,800
So I am leading the Cloud and AI at Melanox Technologies.

51
00:04:01,800 --> 00:04:05,720
I'm actually with the company for some time now over 19 years.

52
00:04:05,720 --> 00:04:09,040
So I've been with the company from the very, very early days.

53
00:04:09,040 --> 00:04:15,880
I am coming from an engineering background, I'm a computer engineer based out of Israel.

54
00:04:15,880 --> 00:04:23,360
And over the past five years or so, I'm focusing mostly on cloud and containerized infrastructure

55
00:04:23,360 --> 00:04:28,440
for high speed networking and obviously machine learning is a big part of it.

56
00:04:28,440 --> 00:04:34,240
Yeah, now I've been familiar with Melanox for quite a while primarily in the context

57
00:04:34,240 --> 00:04:41,280
of high performance computing and infinite band and stuff like that, but not too long ago

58
00:04:41,280 --> 00:04:50,680
the company was acquired by NVIDIA who obviously has a big stake in deep learning and AI.

59
00:04:50,680 --> 00:04:52,480
What's the idea there?

60
00:04:52,480 --> 00:04:53,480
Yeah.

61
00:04:53,480 --> 00:04:57,320
So Melanox was, you know, when we started, it was a networking company and at the time

62
00:04:57,320 --> 00:05:03,040
we focused on Infinibin, which was the new generation of networking at the time that

63
00:05:03,040 --> 00:05:04,040
was supposed to be.

64
00:05:04,040 --> 00:05:05,040
Did I date myself?

65
00:05:05,040 --> 00:05:06,040
I'm sorry?

66
00:05:06,040 --> 00:05:07,040
Did I date myself?

67
00:05:07,040 --> 00:05:08,040
Yes, definitely.

68
00:05:08,040 --> 00:05:15,600
Well, actually, to be honest, Infinibin is still very much a very live technology.

69
00:05:15,600 --> 00:05:21,160
We just this week we have super computing conference, which is the large, you know, gathering

70
00:05:21,160 --> 00:05:24,400
all the high performance components and then very exactly.

71
00:05:24,400 --> 00:05:31,040
And if you look at the top 500 strongest computers in the world today, a lot of them still

72
00:05:31,040 --> 00:05:36,560
run Infinibin, which is the most efficient, still interconnected, highest performance,

73
00:05:36,560 --> 00:05:37,960
most of cost effective.

74
00:05:37,960 --> 00:05:44,080
So it's 20 years old technology, but it's still the cutting edge, definitely running

75
00:05:44,080 --> 00:05:49,440
in on rates of 200 gigabit per second, which is phenomenal if you think about it.

76
00:05:49,440 --> 00:05:53,040
But Melanox wasn't designed to be a high performance computing company.

77
00:05:53,040 --> 00:05:57,880
It was designed to be a networking company that provides the best interconnect for data

78
00:05:57,880 --> 00:05:58,880
centers in the world.

79
00:05:58,880 --> 00:06:03,400
And it doesn't matter if it is high performance computing, when you're running 10,000 servers

80
00:06:03,400 --> 00:06:09,960
doing some crash simulation of the universe, or you're running your telecom infrastructure,

81
00:06:09,960 --> 00:06:13,000
or you're running your machine learning and big data.

82
00:06:13,000 --> 00:06:19,160
So if you look at the essence of networking in the end of the day, it is similar to many

83
00:06:19,160 --> 00:06:20,320
of those applications.

84
00:06:20,320 --> 00:06:25,920
You need a very low latency high bandwidth, very efficient and offloaded infrastructure.

85
00:06:25,920 --> 00:06:30,640
This is exactly what Melanox does, we do it in a way that it can fit a lot of different

86
00:06:30,640 --> 00:06:32,240
verticals.

87
00:06:32,240 --> 00:06:36,720
And machine learning and big data, and I will bundle them in a way together, even though

88
00:06:36,720 --> 00:06:38,360
they're not necessarily bundled together.

89
00:06:38,360 --> 00:06:46,080
But if you look at data analytics, machine learning is part of the data pipeline, and there's

90
00:06:46,080 --> 00:06:50,560
typically also big data because we usually work on large data sets.

91
00:06:50,560 --> 00:06:55,760
If you look at big data and machine learning, they have many high performance networking

92
00:06:55,760 --> 00:06:59,600
requirements that I will talk probably about in a few minutes.

93
00:06:59,600 --> 00:07:04,520
And this is where a lot of the focus of Melanox is these days.

94
00:07:04,520 --> 00:07:10,400
We're looking at acceleration, accelerating networking and providing more efficient networking.

95
00:07:10,400 --> 00:07:15,960
And if you look at what Nvidia does, they do something very similar in a different domain

96
00:07:15,960 --> 00:07:18,200
they're accelerating compute.

97
00:07:18,200 --> 00:07:25,680
And so it was very natural for a company like Nvidia who is accelerating compute and providing

98
00:07:25,680 --> 00:07:31,200
a much more efficient compute with GPUs to look at a company like Melanox which accelerate

99
00:07:31,200 --> 00:07:32,200
networking.

100
00:07:32,200 --> 00:07:37,840
And together we're forming the two or two and a half pillars of networks or of clouds

101
00:07:37,840 --> 00:07:41,560
basically which is compute, networking, and also storage.

102
00:07:41,560 --> 00:07:47,280
The reason I'm saying two and a half pillars is because there's a lot of networking in storage.

103
00:07:47,280 --> 00:07:53,480
So we're touching, Melanox is touching mostly networking and storage in a sense, the networking

104
00:07:53,480 --> 00:07:55,040
part of the storage.

105
00:07:55,040 --> 00:07:59,080
And we're moving a lot of compute into the networking, we can talk about that.

106
00:07:59,080 --> 00:08:04,440
And Nvidia is focusing a lot about the compute, the engines themselves.

107
00:08:04,440 --> 00:08:05,440
Okay.

108
00:08:05,440 --> 00:08:09,520
So if you've got a talk here at the conference tomorrow on networking optimizations for

109
00:08:09,520 --> 00:08:16,440
multi-node deep learning on Kubernetes, when I think about deep learning and in particular

110
00:08:16,440 --> 00:08:22,360
the training phase of deep learning, I think of that as a primarily a compute bound process,

111
00:08:22,360 --> 00:08:26,400
compute bound exercise, you know, why do we care about networking in that?

112
00:08:26,400 --> 00:08:27,400
Exactly.

113
00:08:27,400 --> 00:08:28,400
This is exactly the point.

114
00:08:28,400 --> 00:08:30,360
It is a compute bound process.

115
00:08:30,360 --> 00:08:37,000
And one of the ways to solve compute bound processes is to add more compute.

116
00:08:37,000 --> 00:08:38,480
What we call scale out.

117
00:08:38,480 --> 00:08:44,800
Now when you add more compute to distribute the workloads so that it can run faster, you're

118
00:08:44,800 --> 00:08:48,000
hitting a networking situation.

119
00:08:48,000 --> 00:08:55,640
If you look at models today, models grew enormously over the past several years.

120
00:08:55,640 --> 00:09:02,160
It's not in several percentages, you know, 100 or 200X, the size six years ago.

121
00:09:02,160 --> 00:09:05,520
And the computation time and complexity is enormous.

122
00:09:05,520 --> 00:09:16,040
We see some models training takes weeks on the state-of-the-art GPUs or multiple GPUs.

123
00:09:16,040 --> 00:09:20,040
And the only way to shorten this time, and it has to be shortened because we need those

124
00:09:20,040 --> 00:09:25,720
learning phases to happen much frequently, then, you know, once every three weeks, is

125
00:09:25,720 --> 00:09:26,720
to scale out.

126
00:09:26,720 --> 00:09:34,120
So basically we add multiple servers, each of them with multiple GPUs, and the expectation

127
00:09:34,120 --> 00:09:39,560
or the plan is that the more servers you add, you reduce the time in a linear manner.

128
00:09:39,560 --> 00:09:41,400
That's kind of our goal, right?

129
00:09:41,400 --> 00:09:46,560
So if it takes me one hour with a single server, adding two servers, I would love to have

130
00:09:46,560 --> 00:09:48,040
it in half an hour.

131
00:09:48,040 --> 00:09:54,000
And if I have 60 servers, it should take a minute, and you can do the math from here, right?

132
00:09:54,000 --> 00:09:58,360
Unfortunately, it's not that easy just adding servers and cutting the time.

133
00:09:58,360 --> 00:10:05,960
What happens is that you have now a distributed problem, moving a problem that runs on a single

134
00:10:05,960 --> 00:10:12,080
GPU to run on multiple GPUs and multiple nodes requires a lot of synchronization, requires

135
00:10:12,080 --> 00:10:16,760
a lot of data that needs to go between the servers in a very efficient way.

136
00:10:16,760 --> 00:10:20,840
And this is exactly where the networking is really making a big difference.

137
00:10:20,840 --> 00:10:21,840
Yeah.

138
00:10:21,840 --> 00:10:26,520
Our listeners probably have heard of things like distributed TensorFlow and Horavod and

139
00:10:26,520 --> 00:10:31,960
other technologies that are more focused on kind of the software aspect of that and the

140
00:10:31,960 --> 00:10:37,600
coordination, the state, the shared state that enables distributed training.

141
00:10:37,600 --> 00:10:41,400
But you're saying that's not that part of it, but not all of it, right?

142
00:10:41,400 --> 00:10:42,400
That's part of it.

143
00:10:42,400 --> 00:10:43,400
Yeah.

144
00:10:43,400 --> 00:10:44,400
So definitely.

145
00:10:44,400 --> 00:10:50,800
I mean, if you look at distributed TensorFlow and things like Horavod, which all they do

146
00:10:50,800 --> 00:10:57,600
is they try to improve the way the workers communicate with each other, you know, the

147
00:10:57,600 --> 00:11:06,480
very kind of basic, most simple approach was to have a kind of a node that is doing all

148
00:11:06,480 --> 00:11:07,480
the coordination.

149
00:11:07,480 --> 00:11:15,760
So, you know, you have X amount of workers and they do training of mini batch, a small set

150
00:11:15,760 --> 00:11:23,840
of the data and then they send the information, the gradient factor to the server that usually

151
00:11:23,840 --> 00:11:25,760
we call it a parameter server.

152
00:11:25,760 --> 00:11:30,160
He would gather all the gradients from all the nodes, we'll do some crashing of the data

153
00:11:30,160 --> 00:11:37,280
and distribute new new weights for the different computes and they would do that over and over

154
00:11:37,280 --> 00:11:39,120
again in iteration.

155
00:11:39,120 --> 00:11:44,360
That is a very naive implementation, obviously, when you look at large scale models, every

156
00:11:44,360 --> 00:11:50,880
node will send tens or more of gigabit per second to a single node that needs later on to,

157
00:11:50,880 --> 00:11:55,080
you know, crash it locally and distribute that doesn't scale, that doesn't work, right?

158
00:11:55,080 --> 00:12:03,200
So Horavod and other solutions try to look at different ways of doing this, what we call

159
00:12:03,200 --> 00:12:08,920
collective operation, collective operation in a sense that you have a number of nodes that

160
00:12:08,920 --> 00:12:12,720
are working in a collective way on a single problem and they need to synchronize each

161
00:12:12,720 --> 00:12:14,600
other, right?

162
00:12:14,600 --> 00:12:19,360
So sending to a single server and get it back is one very simple way and then there are

163
00:12:19,360 --> 00:12:25,080
other ways like rings that each send to these neighbors and so on.

164
00:12:25,080 --> 00:12:31,400
Those are all ways to drive the synchronization in a much more efficient way, but in the end

165
00:12:31,400 --> 00:12:36,360
of the day you actually send and receive data and you send and receive data in large quantities

166
00:12:36,360 --> 00:12:41,080
that needs to be very low latency, you don't want to burden your CPUs with all this data

167
00:12:41,080 --> 00:12:47,160
sending and receiving and a lot of the data that you send is not necessarily originating

168
00:12:47,160 --> 00:12:52,040
in the GPU, in the host memory, it is in the GPU memory, the question is how do you send

169
00:12:52,040 --> 00:12:55,000
data from GPU through the network?

170
00:12:55,000 --> 00:12:59,680
Again, the simple implementation is copy that data to the host memory and then copy it

171
00:12:59,680 --> 00:13:02,840
again to the network buffers and then send it out.

172
00:13:02,840 --> 00:13:06,240
That's a very, very inefficient way to do that.

173
00:13:06,240 --> 00:13:12,440
So it's an efficient but again for a CPU bound, compute bound problem rather, we're not

174
00:13:12,440 --> 00:13:15,000
as worried about latency, why do we?

175
00:13:15,000 --> 00:13:17,840
So think about how does it work?

176
00:13:17,840 --> 00:13:25,360
You do the training on a mini batch, everybody, you have X amount of servers, they all do

177
00:13:25,360 --> 00:13:28,120
this training and then they do the communication phase.

178
00:13:28,120 --> 00:13:35,160
Now if the communication phase is long then and you do those cycles, hundreds of thousands

179
00:13:35,160 --> 00:13:40,880
or millions of times, this adds up significantly adds up.

180
00:13:40,880 --> 00:13:46,200
I can tell you that when we use technologies like RDMA, which is an invest transport service

181
00:13:46,200 --> 00:13:52,080
or GPU direct, which is a way to send and receive data from the GPU, we can improve the

182
00:13:52,080 --> 00:13:58,680
total system efficiency by multiple axis and those systems are very expensive.

183
00:13:58,680 --> 00:14:06,880
If you look at a 10 node GPU system with 8 GPUs each, that's a lot of money.

184
00:14:06,880 --> 00:14:11,440
If you can do two X performance, you just save several million dollars or watch yourself

185
00:14:11,440 --> 00:14:12,440
another system.

186
00:14:12,440 --> 00:14:13,440
Exactly.

187
00:14:13,440 --> 00:14:15,680
And this is only networking.

188
00:14:15,680 --> 00:14:17,920
And this is what people don't realize.

189
00:14:17,920 --> 00:14:22,560
Actually people do realize, if you look at all the big guys, all the hyperscalers, they

190
00:14:22,560 --> 00:14:26,080
all run this technology already for many years.

191
00:14:26,080 --> 00:14:30,640
They're running RDMA and they're running GPU direct and they are now looking at additional

192
00:14:30,640 --> 00:14:36,680
technologies that we bring to move some of the computation into the network, actually,

193
00:14:36,680 --> 00:14:38,000
called sharp.

194
00:14:38,000 --> 00:14:41,280
We just announced it in super computing.

195
00:14:41,280 --> 00:14:42,520
It would be part of nickel.

196
00:14:42,520 --> 00:14:48,800
Nickel is the Nvidia communication library, so it will be available together with nickel.

197
00:14:48,800 --> 00:14:51,880
So we kind of talked about the setup in a while, this is important.

198
00:14:51,880 --> 00:14:55,520
And then we've been rattling off a bunch of different technologies that play a role

199
00:14:55,520 --> 00:14:56,520
in here.

200
00:14:56,520 --> 00:15:01,120
I imagine part of your presentation was kind of walking through that stack or the evolution

201
00:15:01,120 --> 00:15:02,120
of technologies.

202
00:15:02,120 --> 00:15:08,080
It's a good time to drill into what is GPU direct and RDMA and nickel and sharp and all

203
00:15:08,080 --> 00:15:09,080
these things.

204
00:15:09,080 --> 00:15:10,080
Yeah.

205
00:15:10,080 --> 00:15:14,360
So the talk we'll be talking about the challenges that we just discussed.

206
00:15:14,360 --> 00:15:19,560
And very briefly talk about RDMA and GPU direct, there's two key technologies that enable

207
00:15:19,560 --> 00:15:21,360
better networking.

208
00:15:21,360 --> 00:15:25,800
And then it will go and talk about the actual implementation in Kubernetes.

209
00:15:25,800 --> 00:15:28,120
So we are in Cubcon after all.

210
00:15:28,120 --> 00:15:34,480
And one of the things we want to make sure is we can enable advanced platforms like a

211
00:15:34,480 --> 00:15:37,560
Kubernetes class to take advantage of these technologies.

212
00:15:37,560 --> 00:15:39,160
It doesn't come up free.

213
00:15:39,160 --> 00:15:46,600
You know, if you look at Kubernetes, networking in Kubernetes is very naive in a sense.

214
00:15:46,600 --> 00:15:47,600
Still.

215
00:15:47,600 --> 00:15:53,480
And evolving obviously it's a new platform or framework.

216
00:15:53,480 --> 00:15:57,200
And it is evolving very fast now, you know, you could see at the amount of people here

217
00:15:57,200 --> 00:15:58,200
in the conference.

218
00:15:58,200 --> 00:16:06,160
It's amazing to see how much energy and how much enthusiasm there is here.

219
00:16:06,160 --> 00:16:08,600
And we, you know, networking will evolve.

220
00:16:08,600 --> 00:16:14,240
What we are trying to do, melanox together with the partners in the ecosystem and definitely

221
00:16:14,240 --> 00:16:18,360
Nvidia is a big part, by the way, they're part of the talk, they're actually a joint

222
00:16:18,360 --> 00:16:27,080
talk between Nvidia and melanox, is we are trying to make Kubernetes networking to be

223
00:16:27,080 --> 00:16:35,440
able to consume advanced network solutions in a very natural, upstream, standard, transparent

224
00:16:35,440 --> 00:16:36,440
way.

225
00:16:36,440 --> 00:16:41,240
This is really, and the talk is talking about mostly that, obviously we need to go through

226
00:16:41,240 --> 00:16:46,480
the concept of what are we trying to solve and what are the technologies, but we will

227
00:16:46,480 --> 00:16:50,880
try mostly to focus about how do we integrate it into Kubernetes, and then talk a little

228
00:16:50,880 --> 00:16:57,040
bit about that will be the Nvidia side, they're running this at scale in large clusters,

229
00:16:57,040 --> 00:17:02,840
and they will talk about what do you need to do in your data center in order to properly

230
00:17:02,840 --> 00:17:06,240
run this and what's the best practices.

231
00:17:06,240 --> 00:17:07,240
Okay.

232
00:17:07,240 --> 00:17:08,240
So that will be the talk most.

233
00:17:08,240 --> 00:17:09,240
Okay.

234
00:17:09,240 --> 00:17:14,680
So you were kind of running through the technologies and the acronyms that were thrown around

235
00:17:14,680 --> 00:17:17,600
and then we'll dive into the Kubernetes therapies.

236
00:17:17,600 --> 00:17:26,200
So I just throw three different technologies, RDMA and GPU directives that are the ones

237
00:17:26,200 --> 00:17:32,120
that we will talk on the talk tomorrow, and I'll just mention that it will not be part

238
00:17:32,120 --> 00:17:34,240
of the talk, but I'll be happy to explain.

239
00:17:34,240 --> 00:17:39,200
RDMA is remote direct memory access, and this is a technology,

240
00:17:39,200 --> 00:17:44,200
which it's a transport service, if you like, it's the same way or it is.

241
00:17:44,200 --> 00:17:50,200
Yeah, it came from the infiniband days, it is a transport service, very much like TCP

242
00:17:50,200 --> 00:17:57,240
or UDP, I think most of the listeners at least heard about TCP and UDP, but unlike those

243
00:17:57,240 --> 00:18:05,200
TCP and UDP, it was designed in 2000 rather than the 70s of the last decade, and it is

244
00:18:05,200 --> 00:18:06,720
much more advanced.

245
00:18:06,720 --> 00:18:13,680
It has few very important capabilities, first is the notion of read and write versus

246
00:18:13,680 --> 00:18:18,360
only send and receive in TCP or UDP or just send a packet, and somebody on the other side

247
00:18:18,360 --> 00:18:24,320
needs to get it and decide what to do with that, it is a send, receive mechanism.

248
00:18:24,320 --> 00:18:29,880
RDMA allows you not only to do send and receive, but actually write to a specific memory

249
00:18:29,880 --> 00:18:34,800
address on the remote host or read from there, just like a local DMA.

250
00:18:34,800 --> 00:18:40,800
And that's open up a lot of very interesting use cases for writing applications, truly

251
00:18:40,800 --> 00:18:43,080
change how you write your application.

252
00:18:43,080 --> 00:18:50,320
This is one thing, the other thing is something called kernel bypass, in TCP and UDP, an

253
00:18:50,320 --> 00:18:55,800
application that wants to send and receive data will do a socket call, and then it will

254
00:18:55,800 --> 00:19:00,720
go to the Linux kernel or Windows kernel or whatever kernel, and the kernel will do a bunch

255
00:19:00,720 --> 00:19:06,560
of work preparing the packet and send it out, so there's a lot of dependencies on the kernel.

256
00:19:06,560 --> 00:19:12,160
In RDMA, you just from the user space can send and receive data directly, and there's

257
00:19:12,160 --> 00:19:18,720
no basically kernel involvement at all, so it allows you first low-verlo latency.

258
00:19:18,720 --> 00:19:23,920
We're looking at micro-second latency sending and receiving packet versus at least older

259
00:19:23,920 --> 00:19:27,840
of magnitude higher when you look at TCP, minimum.

260
00:19:27,840 --> 00:19:32,880
And then the fact that you don't go to the kernel and the NIC itself, the network adapter,

261
00:19:32,880 --> 00:19:37,800
implements all the transport and how there allows you to move enormous amount of data without

262
00:19:37,800 --> 00:19:43,440
any CPU utilization, which means that if anybody been working on high performance networking,

263
00:19:43,440 --> 00:19:49,560
you know that you need to tweak the kernel and the CPUs like crazy to get the juice out

264
00:19:49,560 --> 00:19:55,800
of the network, but in Finland it's not with RDMA, it's not the case, you just tell it

265
00:19:55,800 --> 00:20:00,680
to send and it will send 100 gigabit without CPU, 200 gig doesn't really matter.

266
00:20:00,680 --> 00:20:05,360
RDMA is available on Finland initially, but now it's a part of Ethernet as well, it's

267
00:20:05,360 --> 00:20:09,400
a standard, it's not a proprietary intercom.

268
00:20:09,400 --> 00:20:13,120
And is available on any kind of enterprise Ethernet NIC?

269
00:20:13,120 --> 00:20:17,600
Yeah, so in the Melanox is definitely one of the leaders in this space we've been doing

270
00:20:17,600 --> 00:20:19,600
that for 20 years.

271
00:20:19,600 --> 00:20:24,920
These days all the network adapters have Ethernet that would have RDMA, I would argue

272
00:20:24,920 --> 00:20:29,400
that Melanox is a far more feature rich and stable in this space.

273
00:20:29,400 --> 00:20:30,400
Nothing less.

274
00:20:30,400 --> 00:20:35,000
Yeah, but you know there's the benefits of experience I guess.

275
00:20:35,000 --> 00:20:41,240
And so my guess then is the GPU direct is RDMA for GPU memory?

276
00:20:41,240 --> 00:20:42,840
Exactly, so exactly right.

277
00:20:42,840 --> 00:20:49,280
So RDMA allows you to write and read directly to memory so the NIC can access the host memory,

278
00:20:49,280 --> 00:20:54,120
there's a lot of relationship in RDMA between the NIC and the host memory and there's a lot

279
00:20:54,120 --> 00:20:59,480
of mechanism that allows it to be very efficient, including like an IOMMU implemented on the

280
00:20:59,480 --> 00:21:00,480
NIC and IOMMU.

281
00:21:00,480 --> 00:21:01,480
IOMMU?

282
00:21:01,480 --> 00:21:08,080
Yeah, I'm sorry for all the heckering, but this is basically a mechanism that makes sure that

283
00:21:08,080 --> 00:21:12,840
you write the data to the right address in the memory and that you allow to do that.

284
00:21:12,840 --> 00:21:17,920
So you don't expect input, output memory, something unit or something.

285
00:21:17,920 --> 00:21:26,400
Yeah, this is typically an Intel component of the server, but the point is that if you think

286
00:21:26,400 --> 00:21:36,240
about it when a remote host writes to your memory of local host, this memory may be in different

287
00:21:36,240 --> 00:21:40,520
physical, there's virtual addresses and physical addresses so you need somebody to make sure

288
00:21:40,520 --> 00:21:45,200
that first you're allowed and you're not bridging in their security breaches and then that

289
00:21:45,200 --> 00:21:50,640
you're writing to the right place because the physical memory kept on shifting and changing.

290
00:21:50,640 --> 00:21:58,560
So there is a mechanism to make sure that the memory is intact properly.

291
00:21:58,560 --> 00:22:05,360
GPU direct is a co-development that NVIDIA and Melanox did together, actually it started

292
00:22:05,360 --> 00:22:11,400
a while back, it started from the high performance computing days when NVIDIA started getting

293
00:22:11,400 --> 00:22:13,240
into this space.

294
00:22:13,240 --> 00:22:20,640
I think it was over 10 years ago, I don't remember exactly, but at least over 10 years ago

295
00:22:20,640 --> 00:22:25,440
and basically what it allows is it allows the NIC and the GPU to be able to communicate

296
00:22:25,440 --> 00:22:31,800
over PCI directly without going through the host memory and so there's no CPU intervention

297
00:22:31,800 --> 00:22:39,040
and there's no copies, there's no other challenges, blockers and so on that we had before.

298
00:22:39,040 --> 00:22:45,440
And that really opens up a huge button like that GPU networking had, all those copies

299
00:22:45,440 --> 00:22:51,160
took a lot of time and they killed performance completely and they synced up a lot of CPU cycles

300
00:22:51,160 --> 00:22:52,400
for that.

301
00:22:52,400 --> 00:22:59,520
So that is GPU direct, it is almost transparent or some because the NIC and the GPU need

302
00:22:59,520 --> 00:23:05,440
to talk there, there's some system requirements that needs to be met on how the PCI layout

303
00:23:05,440 --> 00:23:10,480
is done and where's the NIC and where's the GPU.

304
00:23:10,480 --> 00:23:16,320
It is especially important with the large GPU boxes and typically in large GPU boxes you

305
00:23:16,320 --> 00:23:22,600
will see that there's a GPU in a box like the DGX one, for example and there's a very

306
00:23:22,600 --> 00:23:27,960
interesting PCI structure, it is not a single PCI switch that connects all the GPUs and

307
00:23:27,960 --> 00:23:33,880
the NICs there, there's a topology that and then you need to be very careful on which

308
00:23:33,880 --> 00:23:39,920
NIC talks to with GPU, to enable GPU direct, but other than that it is quite transparent

309
00:23:39,920 --> 00:23:43,840
for the user if they use the right drivers and applications.

310
00:23:43,840 --> 00:23:50,120
The last technology that I mentioned and this is not going to be on the talk is sharp, sharp

311
00:23:50,120 --> 00:23:52,720
here.

312
00:23:52,720 --> 00:23:58,120
I explained earlier about the need to get all the vectors from when you do the, the

313
00:23:58,120 --> 00:24:04,080
needs to be the training, you get all the vector to a single location that does some crunching

314
00:24:04,080 --> 00:24:08,640
and redistribute the new weight vector.

315
00:24:08,640 --> 00:24:14,720
What if you could do that instead of a server or in a ring, do that on the network itself.

316
00:24:14,720 --> 00:24:18,600
So this is what Malanox does, basically we are in our infinivance switches and this

317
00:24:18,600 --> 00:24:21,240
is specifically for infinivance right now.

318
00:24:21,240 --> 00:24:25,720
We added a capability to run compute inside the network.

319
00:24:25,720 --> 00:24:35,800
So doing the, what we call reduction, the vector reduction, we do that actually on the switch

320
00:24:35,800 --> 00:24:37,520
systems themselves.

321
00:24:37,520 --> 00:24:41,160
And we can do that in line rate through a lot of many ports, you know, those switches

322
00:24:41,160 --> 00:24:46,600
has 30, 30 ports or 40 ports depends on the generation.

323
00:24:46,600 --> 00:24:51,960
Switch is not the adapters, the adapter sends the data to the switch, each server sends

324
00:24:51,960 --> 00:24:57,200
the vector of the gradients to the switch and the switch think, let's say that it has,

325
00:24:57,200 --> 00:25:02,960
let's say 30 ports, it gets 30 such streams, the switch will do the reduction, we'll get

326
00:25:02,960 --> 00:25:08,680
all the vectors, we'll do some crunching and we will redistribute that to the servers

327
00:25:08,680 --> 00:25:11,920
to continue the next phase of the training.

328
00:25:11,920 --> 00:25:16,040
And that allows you to open up a lot of bottlenecks.

329
00:25:16,040 --> 00:25:22,920
And is the, the crunching that the switch is doing, is it like how configurable is that,

330
00:25:22,920 --> 00:25:28,840
is it, you know, there's the way that it does it today or the, kind of the way that, you

331
00:25:28,840 --> 00:25:33,560
know, melanox says that it should do it or is it a software defined thing where I can

332
00:25:33,560 --> 00:25:37,640
have a strategy and push it to the switch and it's going to do what I want, is it, to

333
00:25:37,640 --> 00:25:40,480
what, how generalized is the compute available on the switch?

334
00:25:40,480 --> 00:25:41,800
So I think it's a good question.

335
00:25:41,800 --> 00:25:46,600
One of the things that I haven't mentioned is our strategy around software, melanox is

336
00:25:46,600 --> 00:25:54,560
very open source, centric company, on, you know, we do, we believe the best hardware.

337
00:25:54,560 --> 00:26:04,040
And we, the software piece is all upstream first, we, everything we do is upstream, not

338
00:26:04,040 --> 00:26:10,840
private or no, no black box software, we try to open up everything and have standard

339
00:26:10,840 --> 00:26:17,760
APIs as part of this for shop, for example, we have standard APIs that is developed as part

340
00:26:17,760 --> 00:26:19,480
of the RDMA interface.

341
00:26:19,480 --> 00:26:20,480
Okay.

342
00:26:20,480 --> 00:26:24,400
And then from a software perspective, you know, it's just a standard API.

343
00:26:24,400 --> 00:26:30,080
The hardware implementation, obviously there is some flexibility, but that's, I think,

344
00:26:30,080 --> 00:26:35,640
less of the point, the point is that any application that uses a standard open source API,

345
00:26:35,640 --> 00:26:42,000
like at an advantage of any hardware that implements this API capabilities, specifically

346
00:26:42,000 --> 00:26:47,600
for our implementation, it is flexible, but the flexibility is not exposed outside,

347
00:26:47,600 --> 00:26:50,200
it's exposed through those standard APIs.

348
00:26:50,200 --> 00:26:51,200
Okay.

349
00:26:51,200 --> 00:26:52,200
Okay.

350
00:26:52,200 --> 00:26:58,360
So that the idea then would be that somewhere between melanox and NVIDIA, you would identify

351
00:26:58,360 --> 00:27:03,120
what the strategies are of interest, whether it's Horavod or distributed TensorFlow or

352
00:27:03,120 --> 00:27:07,920
something else, and make the module available for the switch.

353
00:27:07,920 --> 00:27:08,920
Yeah.

354
00:27:08,920 --> 00:27:13,920
So the way you consume this is through applications that do distributed training.

355
00:27:13,920 --> 00:27:23,640
Now, obviously the market is quite active with a lot of different projects and so on.

356
00:27:23,640 --> 00:27:30,240
We try to focus on the most dominant frameworks, TensorFlow is definitely one of the frameworks

357
00:27:30,240 --> 00:27:38,960
we see for performance-oriented environments, which is mostly used, and we work mostly with

358
00:27:38,960 --> 00:27:45,160
TensorFlow, not only not limited, we also work with PyTorch and Cafe and others, but definitely

359
00:27:45,160 --> 00:27:52,680
TensorFlow is where we invest more of our time, we're part of the community, the open source

360
00:27:52,680 --> 00:27:55,040
community of TensorFlow.

361
00:27:55,040 --> 00:28:01,760
If you look at the distributed implementation, today the most common configuration, it's

362
00:28:01,760 --> 00:28:06,320
like a Lego, you can put it in multiple ways, but if I try to look at what is common in

363
00:28:06,320 --> 00:28:14,160
the industry today, people will typically use TensorFlow together with Horavod and Nikol,

364
00:28:14,160 --> 00:28:21,040
Nikol is the distributed library of NVIDIA.

365
00:28:21,040 --> 00:28:26,960
We usually look at how do you do the same challenge of distributed workload, but within a server,

366
00:28:26,960 --> 00:28:30,960
right, in a server you have multiple GPUs, you also need to distribute the workload between

367
00:28:30,960 --> 00:28:31,960
those GPUs.

368
00:28:31,960 --> 00:28:36,560
That's kind of a very similar way, and then when they scale out multiple servers, they

369
00:28:36,560 --> 00:28:41,000
use RDMA and GPU direct and all those technologies.

370
00:28:41,000 --> 00:28:42,000
Okay.

371
00:28:42,000 --> 00:28:49,320
We layer those together, Nikol, TensorFlow and then Nikol and then Horavod, and the library

372
00:28:49,320 --> 00:28:56,640
that enables sharp NRDMA and GPU direct is Nikol, okay, and that is something that will

373
00:28:56,640 --> 00:28:57,640
be available very soon.

374
00:28:57,640 --> 00:29:02,000
It's not limited to Nikol, I mean, anybody can use that, it's an open source, anybody

375
00:29:02,000 --> 00:29:06,520
if there's somebody here in the audience that listen and want to enable that for Python,

376
00:29:06,520 --> 00:29:09,240
should we, we definitely can do that, there's no problem.

377
00:29:09,240 --> 00:29:13,880
And so then real quick, the Kubernetes piece of this, like we're talking about hardware

378
00:29:13,880 --> 00:29:18,560
and network adapters, you know, I must, it's part of me that says, you know, this is

379
00:29:18,560 --> 00:29:23,480
of, is in the box, the kind of host operating system, you know, has drivers for all the

380
00:29:23,480 --> 00:29:26,480
stuff, like why do we care about this at the Kubernetes level?

381
00:29:26,480 --> 00:29:28,480
What are the challenges there?

382
00:29:28,480 --> 00:29:29,480
Excellent question.

383
00:29:29,480 --> 00:29:35,000
You know, somebody once said that advanced or very smart silicon is nothing but very

384
00:29:35,000 --> 00:29:39,200
expensive send if you don't have the right software, right?

385
00:29:39,200 --> 00:29:43,680
So everything I said here, if you remember, is talking about how do I connect the application

386
00:29:43,680 --> 00:29:48,360
directly to the hardware almost, right, all the bypasses and all the acceleration.

387
00:29:48,360 --> 00:29:53,320
You're looking at layers that provide direct one activity as much as possible from the

388
00:29:53,320 --> 00:29:56,240
application all the way to the hardware.

389
00:29:56,240 --> 00:30:02,720
And when you go to virtualization or containerization, you actually go the opposite direction, right?

390
00:30:02,720 --> 00:30:08,320
You go and you add software layers, like service, and all the other things you hear as

391
00:30:08,320 --> 00:30:14,560
the ends and so on, that all they want to do is virtualize the network and provide a

392
00:30:14,560 --> 00:30:21,760
layer that completely disconnect the application from physical resources, like networked up.

393
00:30:21,760 --> 00:30:27,600
So you're trying to couple the two to gain optimization, but these things are trying

394
00:30:27,600 --> 00:30:28,960
to decouple the two.

395
00:30:28,960 --> 00:30:32,160
So I think they're not exactly working against each other.

396
00:30:32,160 --> 00:30:35,720
Each of them is looking for a different direction.

397
00:30:35,720 --> 00:30:40,640
And what we try to do is marry all the direction to something which is cohesive and high performance.

398
00:30:40,640 --> 00:30:48,920
So what all those SDN software-defined networks and service meshes and virtualization technologies

399
00:30:48,920 --> 00:30:54,960
are trying to do is they're trying to provide you a layer of standardization so you don't

400
00:30:54,960 --> 00:31:01,200
have to write your application multiple times for different hardware and automations and

401
00:31:01,200 --> 00:31:06,480
all the other goodies, like provision network through code, software-defined, and so on.

402
00:31:06,480 --> 00:31:11,640
This is very important, you do want to have a controller that you set security elements,

403
00:31:11,640 --> 00:31:17,040
you set behavioral, you create networks and ports and so everything in software.

404
00:31:17,040 --> 00:31:22,320
But at the same time, if you add multiple layers of software, you have the flexibility,

405
00:31:22,320 --> 00:31:26,760
you have the features, you don't have the efficiency, you don't have the performance,

406
00:31:26,760 --> 00:31:30,320
which you know, you're kind of shooting yourself in the leg.

407
00:31:30,320 --> 00:31:35,280
And what we are trying to do is we're trying to provide a high performance together with

408
00:31:35,280 --> 00:31:36,480
all the flexibility.

409
00:31:36,480 --> 00:31:42,920
So we're trying to marry software-defined service meshes and all these technologies and still

410
00:31:42,920 --> 00:31:51,040
allow in a transparent manner and without any compromises to run in full, bare metal kind

411
00:31:51,040 --> 00:31:52,040
of performance.

412
00:31:52,040 --> 00:31:57,440
You know, getting the same performance as you would get with a bare metal server in a very

413
00:31:57,440 --> 00:32:00,480
highly virtualized, containerized environment.

414
00:32:00,480 --> 00:32:03,480
This is the challenge that we're trying to address.

415
00:32:03,480 --> 00:32:05,320
And so is there a solution?

416
00:32:05,320 --> 00:32:08,680
Is there a project or something that people can go take a look at?

417
00:32:08,680 --> 00:32:09,680
What is that?

418
00:32:09,680 --> 00:32:10,680
Absolutely.

419
00:32:10,680 --> 00:32:11,680
So it's called Kubernetes.

420
00:32:11,680 --> 00:32:13,280
We're doing that as part of Kubernetes, right?

421
00:32:13,280 --> 00:32:14,280
Okay.

422
00:32:14,280 --> 00:32:15,280
So it's going to get vacant.

423
00:32:15,280 --> 00:32:21,080
In Kubernetes, we are developing a CNI, CNI is a network plugin.

424
00:32:21,080 --> 00:32:24,840
Kubernetes has this concept of CNI's.

425
00:32:24,840 --> 00:32:32,280
And we are today using a CNI for technology called Desarai UV, which is a PCI technology.

426
00:32:32,280 --> 00:32:34,880
I don't think we have a lot of time to go through that.

427
00:32:34,880 --> 00:32:39,920
But that is the direction that we provide the direct connectivity.

428
00:32:39,920 --> 00:32:41,800
It's still not fully complete.

429
00:32:41,800 --> 00:32:46,760
There's more and more things that will go in like SDN offloads and other things that today

430
00:32:46,760 --> 00:32:51,800
are available for, say, solution like VMware or OpenStack and other places.

431
00:32:51,800 --> 00:32:54,280
Kubernetes is still young.

432
00:32:54,280 --> 00:32:55,280
And but we are working.

433
00:32:55,280 --> 00:33:00,000
We're working on this with a lot of partners in the open source community where basically

434
00:33:00,000 --> 00:33:06,400
we want to provide the full flexible secure network with the highest performance possible.

435
00:33:06,400 --> 00:33:07,400
Awesome.

436
00:33:07,400 --> 00:33:10,000
Well, Edas, thanks so much for taking the time to walk through all this stuff.

437
00:33:10,000 --> 00:33:11,000
Really good.

438
00:33:11,000 --> 00:33:12,000
Sure.

439
00:33:12,000 --> 00:33:13,000
My pleasure.

440
00:33:13,000 --> 00:33:14,000
Thank you very much.

441
00:33:14,000 --> 00:33:15,000
Thank you.

442
00:33:15,000 --> 00:33:16,000
All right, everyone.

443
00:33:16,000 --> 00:33:21,280
That's our show for today for more information about today's guests or any of the topics mentioned

444
00:33:21,280 --> 00:33:26,600
in the interview, visit twomelai.com slash shows.

445
00:33:26,600 --> 00:33:31,240
To learn more about the IBM AI Enterprise workflow study group, I'll be leading, visit

446
00:33:31,240 --> 00:33:35,160
twomelai.com slash AI workflow.

447
00:33:35,160 --> 00:33:40,080
Of course, if you like what you hear on the podcast, please subscribe, rate, and review

448
00:33:40,080 --> 00:33:42,800
the show on your favorite pod catcher.

449
00:33:42,800 --> 00:33:59,960
Thanks so much for listening and catch you next time.

