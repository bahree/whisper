WEBVTT

00:00.000 --> 00:16.000
Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting

00:16.000 --> 00:20.840
people doing interesting things in machine learning and artificial intelligence.

00:20.840 --> 00:36.360
I'm your host Sam Charrington. In this episode I'm joined by Claire Galnick, CTO of Turbium

00:36.360 --> 00:43.720
Labs, to discuss our thoughts on the reproducibility crisis currently haunting the scientific landscape.

00:43.720 --> 00:49.800
For a little background, a nature survey in 2016 showed that more than 70% of researchers

00:49.800 --> 00:55.720
have tried and failed to reproduce another scientist experiments, and more than half have failed

00:55.720 --> 01:01.720
to reproduce their own experiments. Claire gives us her take on the situation and how it applies

01:01.720 --> 01:07.400
to data science, along with some great nuggets about the philosophy of data and a few interesting

01:07.400 --> 01:13.560
use cases as well. We also cover her thoughts on Bayesian versus frequentist techniques,

01:13.560 --> 01:19.400
and while we're at it, the VIM versus EMAX debate. No, actually I'm just kidding on that last one,

01:19.400 --> 01:23.400
but this was indeed a very fun conversation that I think you'll enjoy.

01:25.000 --> 01:30.920
Okay, you all know I traveled to a ton of events each year, and the event season is just getting

01:30.920 --> 01:37.320
started for me. On Friday, I'm on my way back to the Bay Area for the scaled ML conference and

01:37.320 --> 01:45.160
video GTC. But the event I'm most excited about is my very own AI summit, the successor to the

01:45.160 --> 01:51.080
awesome future of data summit I did last year. This year's event takes place on April 30th and

01:51.080 --> 01:57.160
May 1st, and is once again being held in Las Vegas in conjunction with the Interrupt ITX conference.

01:58.200 --> 02:02.600
This year's event is much more AI focused and is targeting enterprise line of business and

02:02.600 --> 02:09.640
IT managers and leaders who want to get smart on AI very quickly. Think of it as a two-day no-fluff

02:09.640 --> 02:15.160
technical MBA and machine learning and AI. I'll be presenting an ML and AI bootcamp,

02:15.160 --> 02:20.040
and I'll have experts coming in to present many workshops on topics like computer vision,

02:20.040 --> 02:25.560
natural language processing, conversational applications, machine learning and AI for IOT and

02:25.560 --> 02:33.400
industrial applications, data management for AI, building an AI first culture, and operationalizing

02:33.400 --> 02:38.200
machine learning and AI applications and systems. For more information on the program,

02:38.200 --> 02:43.960
visit twimmolai.com slash AI summit. And now on to the show.

02:49.160 --> 02:54.920
All right, everyone. I am on the line with Claire Galnick. Claire is CTO at Turbium Labs.

02:54.920 --> 02:59.480
Claire, welcome to this week in machine learning and AI. Thank you. I'm happy to be here.

03:00.280 --> 03:05.800
So you've got a background in neuroscience and biomedical engineering,

03:05.800 --> 03:10.040
but you've ended up spending a lot of your time working in data science and machine learning.

03:10.040 --> 03:12.040
Can you tell us a little bit about your path?

03:13.720 --> 03:19.640
Definitely. So my background, my graduate school research was actually in information processing

03:19.640 --> 03:25.320
and neural networks, but not artificial neural networks, not neural networks that data scientists

03:25.320 --> 03:31.080
are usually talking about. And so it was definitely an interesting transition, leaving

03:32.440 --> 03:37.160
leaving graduate school, spending all my time trying to figure out how the brain works,

03:37.160 --> 03:42.280
and then coming into data science and hearing people say, you know, like my algorithm works

03:42.280 --> 03:49.720
just like the brain, and I was like, wow, interesting. You should tell the neuroscientists how that works.

03:49.720 --> 03:54.360
But it was, yeah, it was an interesting transition. It's definitely a different type of problem,

03:54.360 --> 04:01.080
and a different way of using data to make the world better. So in science, you're trying to learn

04:01.080 --> 04:06.200
so that you in particular in biomedical engineering to help cure diseases. And now I work in

04:06.200 --> 04:10.920
cybersecurity. And so you're using data and fighting another hard problem. It's just a very different

04:10.920 --> 04:18.280
type of problem. Can you tell us a little bit more about your graduate work? What was your research

04:18.280 --> 04:27.880
there? Yes. So we studied how your brain processes sensory information. So after, you know,

04:27.880 --> 04:32.280
light touches your retina or something touches your skin or your somatosensory system,

04:33.240 --> 04:37.800
how is that information propagated through neural signals, through spike trains,

04:37.800 --> 04:43.400
and how is it that eventually that turns into perception would be the big question.

04:44.200 --> 04:49.720
And interestingly, with the engineering bent, the question was using this knowledge,

04:49.720 --> 04:56.040
could we replace this sensory signal with artificial signals so that we can mimic sensory input

04:56.040 --> 05:01.720
for people who have lost it? So for example, if you've lost a limb and you no longer have your

05:01.720 --> 05:06.440
sense of touch, could we replace that, for example, was a long-term goal?

05:06.440 --> 05:12.600
Interesting. And now when you're looking at this as a scientist, a neuroscientist,

05:12.600 --> 05:20.120
are you exploring it from at the level of chemicals, or are you looking at it, or were you looking at

05:20.120 --> 05:27.480
it more from a system level, electrical signals, or all of the above? Yeah. So we used a couple of

05:27.480 --> 05:31.960
different signals, but broadly we're operating on the like spike train or neuron level.

05:31.960 --> 05:37.240
And so I like to think of it as a, you know, a non-natural language. What is the language of the

05:37.240 --> 05:41.480
brain? And the best current model for understanding how information is transmitted

05:42.280 --> 05:48.200
in the brain is through neuron spiking. And these patterns of spiking and the different

05:48.200 --> 05:52.760
neurons spiking at different times to represent different patterns that could create different

05:52.760 --> 05:58.760
perceptions. Do you find it in working in data science and talking with data scientists that

05:58.760 --> 06:03.480
there are things that, you know, things that you learned about the brain and how those types

06:03.480 --> 06:09.560
of networks work that you feel would be more helpful, if that would be helpful, if they were more

06:09.560 --> 06:15.640
broadly appreciated within data science? That is an interesting question. The thing that I find

06:15.640 --> 06:23.320
most fascinating is how these two fields interact. One of the things that is true is that we don't

06:23.320 --> 06:29.880
know how the brain works. And often we're using mathematical frameworks. And we force that onto

06:29.880 --> 06:35.880
our model of the brain, as opposed, and we see if it's a good model for predicting, for understanding

06:35.880 --> 06:40.840
the brain activity. And so it's a weird interplay where you would, you would in many cases prefer it

06:40.840 --> 06:46.680
to be more from the brain to feeding the statistical modeling and the network modeling.

06:46.680 --> 06:53.960
But often it goes the other way around. A new statistical estimation technique comes, uh, is developed

06:53.960 --> 06:59.720
and then it is applied backwards on into neuroscience. It is an interesting dynamic because you would,

06:59.720 --> 07:05.000
you would imagine or hope in some cases that it went more the other way. Right, right. Especially

07:05.000 --> 07:10.680
going back to your comment, how people talk about artificial neural networks as being brain inspired

07:10.680 --> 07:16.600
when, uh, it often happens the other way around. Yeah, it can. I definitely see it going the other

07:16.600 --> 07:23.880
way around very often. Awesome. And so you've been recently spending some time thinking about,

07:25.240 --> 07:30.920
kind of going back to your roots in science, the reproducibility crisis that has been

07:31.960 --> 07:38.600
much talked about in that field and what data scientists can take away from that. Maybe start by

07:38.600 --> 07:43.320
talking about that reproducibility crisis for folks that aren't familiar with it. Definitely.

07:43.320 --> 07:48.840
So I would say that the reproducibility crisis is a little bit controversial. But the, the main,

07:48.840 --> 07:55.160
the main observation is that a lot of the literature that has been published, a lot of the

07:55.160 --> 07:59.400
experiments that have been summarized and the results that have been summarized in particular in

07:59.400 --> 08:03.960
the fields that I was in. So in biology and neuroscience and in, and in psychology,

08:05.000 --> 08:10.040
is not reproducible. Meaning that if you were to pick a paper out of the literature and try to

08:10.040 --> 08:15.560
perform the experiment as exactly a set out in the methods, you likely would not achieve the same

08:15.560 --> 08:21.720
results as the authors report. And you say it's, do you say it's controversial? Would you say it's

08:21.720 --> 08:31.000
controversial? That it exists or because it exists? I think it is controversial because it implies

08:31.000 --> 08:38.280
that the methods that scientists are using are not actually objective or perfect. And a lot of

08:38.280 --> 08:43.400
scientists work very hard and their work is very close to their identity. And so the implication

08:43.400 --> 08:47.400
that the body of work that a lot of very hard working and well-meeting scientists have put out

08:47.400 --> 08:55.480
into the world is not reliable as a, as a body of knowledge. It's a lot of people really

08:55.480 --> 08:59.800
core to their identity. And I think the, the way that it's controversial, it comes from, it comes

08:59.800 --> 09:06.920
from that sense of ownership over this body of literature. Is it, is it generally accepted though

09:06.920 --> 09:14.520
that it's a problem or is, is that still up for a debate in, in scientific corners?

09:15.480 --> 09:20.520
I would say that the fact that there is a problem is no longer debated. The scale of the problem

09:20.520 --> 09:27.400
is still up for debate. You know, when I've heard about, when the, when the topic of the reproducibility

09:27.400 --> 09:34.200
crisis comes up, a lot of times, you hear people throwing around like p-values and, and the

09:34.200 --> 09:40.040
implications of that, what's that all about? That's, that's very interesting. So I can talk

09:40.040 --> 09:45.720
from my own experience about how I was taught about hypothesis testing and p-values being a part

09:45.720 --> 09:51.080
of that. And I think it's interesting because I see a lot of parallels to the way that data

09:51.080 --> 09:56.600
scientists taught now. When I was in high school, I learned about hypothesis testing and I thought

09:56.600 --> 10:03.800
it was the coolest technology that anyone could possibly imagine because I so desperately wanted

10:03.800 --> 10:08.920
to be a scientist and I knew that scientists needed to be objective. So I needed this objective

10:08.920 --> 10:14.600
way to take an experiment that I ran and then know whether something was true, whether my experiment

10:14.600 --> 10:21.560
was meaningful. And the hypothesis testing was just so compelling as a tool because it'll

10:21.560 --> 10:25.560
basically allow me to just take this data and run it through an algorithm and then have an answer.

10:25.560 --> 10:33.720
And what is, what is interesting about that is that now these hypothesis tests are the center

10:33.720 --> 10:39.800
of the controversy over over the reproducibility crisis and a lot of people are attributing them.

10:39.800 --> 10:44.920
And I agree that it's a major part of the cause of the reproducibility crisis. And one of the most

10:44.920 --> 10:51.480
interesting things about it is that it's actually a problem of scale that p-values and hypothesis

10:51.480 --> 10:56.600
testing applied across many scientists simultaneously or over and over and over again on the same

10:56.600 --> 11:04.520
data set. That's a process we now call key hacking. It's actually re-analyzing the same data over

11:04.520 --> 11:07.720
and over and over again until you find something that appears to be statistically significant.

11:09.000 --> 11:14.360
And that's now considered one of the major problems and major causes of the reproducibility

11:14.360 --> 11:19.240
crisis. And what's interesting is that really machine learning is exactly that. It's re-analyzing

11:19.240 --> 11:24.440
the same data over and over and over again testing hypotheses in a very, very fast faster than

11:24.440 --> 11:28.920
even any human or any scientist normally would be able to and then trying to pick the one that best

11:28.920 --> 11:33.640
explains your data. And so there's a lot of parallels between these hypothesis testing and machine

11:33.640 --> 11:39.160
learning. So with that in mind, how should it impact the way we approach data science?

11:39.880 --> 11:45.240
That is an excellent question. So one of the things that is true about how machine learning is

11:45.240 --> 11:50.280
taught is you're always taught to cross validate, which means that you you hypothesis generate on

11:50.280 --> 11:57.160
a training set of data. You run all those models and then you test that data once. And that it's

11:57.160 --> 12:02.760
interesting is that testing testing this model once on a new set of data is a lot like making a

12:02.760 --> 12:08.760
very specific prediction and then running that experiment, which is what the scientific method is

12:08.760 --> 12:15.960
and it's best formulation, make a very specific prediction that is unlikely to occur in less

12:15.960 --> 12:20.600
your hypothesis is true and then run the experiment and see if that actually happened. And so in a

12:20.600 --> 12:27.640
lot of ways cross validation is taught and is is meant to solve the problem of machine learning

12:27.640 --> 12:33.800
being essentially pehacking. And what's interesting is when you think about it that way, you realize that

12:33.800 --> 12:40.200
a person, a data scientist who is well-meaning and trying to make a model really, really work

12:40.200 --> 12:43.960
could essentially start pehacking their data by repeating this training and testing,

12:43.960 --> 12:48.760
training and testing, training and testing over and over and over again. Sometimes people call this

12:48.760 --> 12:54.200
overfitting on your test set, but it's also just another formulation of pehacking.

12:54.920 --> 13:01.880
Right. Right. Interesting. I guess what's kind of thrown me for about that description is that

13:01.880 --> 13:06.600
it's also, I think, called data science, right? That's what people are fundamentally doing is

13:07.480 --> 13:14.920
trying to manipulate models and model parameters through an iterative loop to try to come up with

13:14.920 --> 13:21.640
something that works. Yeah. So what's really fascinating is so I became very frustrated with this

13:21.640 --> 13:27.000
when I started researching the reproducibility crisis was the thing that I got me interested,

13:27.000 --> 13:32.120
but it led me into this sort of academic deep dive into, wait, why? But why? But why? And I ended up

13:32.120 --> 13:38.360
eventually studying the philosophy of data and why is it that we believe that we can learn from data

13:38.360 --> 13:46.680
at all? And what I learned is that there's a couple important assumptions that you're making implicitly

13:46.680 --> 13:52.840
when you think that you can learn things from data. One of the most important ones is that if you

13:52.840 --> 13:59.800
think about it, 100% of the data that we have ever collected is about the past. Right. We don't have

13:59.800 --> 14:04.040
data about the future. Right. But most of the time, we're trying to learn something about the future.

14:04.040 --> 14:08.040
Right. We're trying to make predictions about the future. You're assuming some kind of predictive

14:08.040 --> 14:14.280
value or dependence. Exactly. You're making the assumption that something about the past and the

14:14.280 --> 14:20.280
future is shared or that there's a shared set of rules in the past and the future. That means that

14:20.280 --> 14:26.280
you can learn something about the past and use it to predict the future. And one of the things

14:26.280 --> 14:31.640
that's interesting is that if you think what is the counterfactual to that? What is the opposite

14:31.640 --> 14:37.480
of having rules? Potentially, things just happen due to chance. Right. Perhaps things that happened

14:37.480 --> 14:43.240
in the past just happened and there wasn't a rule. So when you choose to use data to try to make

14:43.240 --> 14:48.440
predictions about the future, you're implicitly saying, I believe this system that I am studying

14:48.440 --> 14:53.480
is determined by a set of rules. And I'm just trying to figure out what those rules are.

14:54.120 --> 14:58.440
And what's really cool about, or what's really interesting is if you think about the ways and

14:58.440 --> 15:03.640
machines, the ways and the problems in which machine learning has been the most successful,

15:04.280 --> 15:10.200
they're all in the types of problems that are extremely rule based. Or more often,

15:10.200 --> 15:17.480
we're humans determined the rules. For example, the first example is chess. Right. The very first

15:17.480 --> 15:24.040
example of automated intelligence. Yes, is a entirely rule based game that they're playing.

15:24.600 --> 15:29.000
And one of the great parts about it is that about making this attractable problem and making

15:29.000 --> 15:33.400
p-hacking not a problem is that you know the game you're playing, you know the rules. And if you

15:33.400 --> 15:36.440
someone tried to tell you, you know, oh, I'm going to move my pawn and I'm going to move it,

15:36.440 --> 15:41.240
you know, seven spaces diagonally to the right. You'd say, oh, no, that just breaks the rules.

15:41.240 --> 15:45.640
You can't do that. And because it's a rule based intractable like that is actually

15:45.640 --> 15:50.680
the machine learning ends up being an automated intelligence, ends up being a much more powerful

15:50.680 --> 15:55.320
tool in this type of problem than one in which the rules are not known to exist.

15:56.120 --> 16:01.560
So what's an example of a system where the rules aren't known or known to exist and

16:02.840 --> 16:08.200
you know, machine learning doesn't work as well? That's an excellent question. So I have a couple,

16:08.200 --> 16:13.800
but one of my favorites to talk about is actually talking about moving a problem along a spectrum

16:13.800 --> 16:19.880
between being very rule based and not rule based. It's not quite machine learning but it certainly

16:19.880 --> 16:28.680
is predictive and that is predicting elections. Okay. Elections in the US are rule based or

16:28.680 --> 16:34.200
they have been historically, which means, you know, one person, one vote. The definition of state

16:34.200 --> 16:40.680
boundaries are determined by a governing body and the amount of electoral votes or number of

16:40.680 --> 16:46.840
votes that these states get is determined in advance as a rule. And at the end of the day,

16:46.840 --> 16:53.080
what you end up predicting or measuring and predicting is just a few parameters. Basically,

16:53.080 --> 16:59.800
who will vote and who they'll vote for? And that makes it a very tractable problem for

16:59.800 --> 17:04.840
predictive type analytics, whether it's machine learning or, you know, your Bayesian approaches

17:04.840 --> 17:09.960
or however you want to go about it. An interesting thought experiment is to say, well, what happened?

17:09.960 --> 17:14.760
What would happen? How would we predict elections if those rules didn't apply? For example,

17:14.760 --> 17:19.000
if you didn't know how many votes each person got and you had to infer that as well,

17:19.640 --> 17:23.880
or if you didn't know what the state boundaries were and you had to use data to try to guess

17:23.880 --> 17:27.720
based on previous elections that you've seen as outcomes, what the state boundaries were,

17:28.440 --> 17:33.960
how much harder would this problem be? Or worse, if you're not in a functioning democracy where

17:33.960 --> 17:39.560
rules matter and the rules and votes can just be deleted and removed or added or stuffed arbitrarily

17:39.560 --> 17:45.400
randomly, then what use is your predictive technology? What use is data? I'm figuring out what

17:45.400 --> 17:52.440
the next outcome would be. And so what does it tell you to explore, you know, these alternate scenarios

17:52.440 --> 18:00.760
where the rules don't apply? So my favorite use case, like practical use case of this philosophy

18:00.760 --> 18:06.120
and this understanding is to think about what framing the problem means. So you'll often hear

18:06.120 --> 18:11.400
people say, oh, it's all about how you frame the problem. Often if you ask them, well, what does

18:11.400 --> 18:15.560
that mean? How do you frame the problem? It's hard to articulate what that is. For me,

18:16.280 --> 18:22.680
what understanding about this problem of like needing a rule-based system in order for learning

18:22.680 --> 18:29.480
from data to be a practical solution? You have to think about when you're framing the problem,

18:29.480 --> 18:35.960
you're trying to frame it so that the rules apply. So if you have a choice, if you want to choose

18:37.160 --> 18:41.480
potentially multiple different ways of framing your problem or setting up your model or your

18:41.480 --> 18:47.000
features, you're trying to pick features that are representative of true rules about how the domain

18:47.000 --> 18:54.280
actually works. It's basically an argument for domain expertise. It's not at its surface,

18:54.280 --> 18:58.600
it's basically like understand the system that you're using because that's what you need in order

18:58.600 --> 19:03.480
to make good models. I don't think that that's particularly controversial, but it does suggest

19:03.480 --> 19:09.320
that you can't just add a bunch of data into a model and then press go and get the meaning of

19:09.320 --> 19:14.280
the universe out on the other side. Is there an example from your experience at Turbium where

19:15.160 --> 19:22.600
a given use case, maybe there was an initial temptation to approach it without thinking about

19:22.600 --> 19:29.320
the underlying rules and kind of applying this philosophy and thinking about those rules,

19:29.320 --> 19:34.120
it changed your approach to modeling and your outcome? Yeah, that's an excellent question.

19:35.720 --> 19:40.520
So it's Turbium broadly works in the field of cybersecurity, so speaking a little more broadly

19:40.520 --> 19:48.120
than Turbium specifically, this is a problem that people in cybersecurity come up against all the time,

19:48.120 --> 19:54.600
which is that hackers are not rule-based entities. When they try to get into a system, they're trying

19:54.600 --> 20:01.400
to break the rules. So sometimes you have a choice. You have a choice of trying to learn the rules

20:03.000 --> 20:07.480
or you have a choice to try to set the rules in your system. And so for example,

20:08.200 --> 20:13.320
you could choose to try to learn how a network or a given company's network works and then

20:13.320 --> 20:21.880
try to detect anomalies in order to use machine learning to detect anomalies. And that approach

20:21.880 --> 20:26.600
will work, but it's hard. It's hard because the way the network interacts is changing all the time,

20:26.600 --> 20:31.560
the rules are changing, which computer is talking to, which computer are changing. But another way

20:31.560 --> 20:37.480
to approach the same problem is to set really good policies on your network, essentially permissions

20:37.480 --> 20:44.040
policies, and make those rules extremely formalized and designed by humans. Like this computer can

20:44.040 --> 20:49.240
only talk to this one. Like this is the rule. And then when you start observing anomalies, when you

20:49.240 --> 20:55.160
have a network that is really well designed, in which you have all the minimal access to any

20:55.160 --> 21:01.800
particular resource, then data suddenly starts working a lot better at detecting bad actors on your

21:01.800 --> 21:07.960
network for example. So you have a choice of I can just learn the rules as they exist or I can

21:07.960 --> 21:13.320
create the rules and then data suddenly becomes much more powerful as a tool if you create the rules.

21:13.960 --> 21:19.000
Right. It sounds to me like a combination between the previous point you're making about the

21:19.000 --> 21:27.800
importance of domain knowledge, but also just the utility of constraints and making a problem

21:27.800 --> 21:34.760
manageable, which I also don't think would be necessarily controversial as a point. But it's

21:34.760 --> 21:41.400
interesting to kind of layer it on to this way of thinking about data. You mentioned that you

21:41.400 --> 21:48.360
started researching the philosophy of data. Where did you find research about this or who's out

21:48.360 --> 21:55.480
there philosophizing about data? There's a lot. It's actually fairly interesting. I'll tell you

21:55.480 --> 22:04.680
because some of my favorites. So David Hume wrote about a long time ago about the law of induction

22:04.680 --> 22:09.800
and a lot of these a lot of these are philosophers. And so instead of talking in the concept of

22:09.800 --> 22:14.360
data, they're talking about thought experiments. That's their usual unit of work. And he posed a

22:14.360 --> 22:21.080
famous thought experiment that asked how many times do you have to observe the sun rise to be

22:21.080 --> 22:27.080
certain that it will rise again tomorrow? And this is a really compelling interesting question

22:27.080 --> 22:31.480
for me because the question is essentially how much data can I collect before I'm certain?

22:33.160 --> 22:37.640
And if you think about that question, right, it gives you this deep sense of like, wait, what?

22:37.640 --> 22:43.240
Like does this actually work? So that's one that's one writer I really like. Another one is Karl

22:43.240 --> 22:50.040
Popper. Karl Popper is famous for a lot of things, but one of the things that he did was come up with

22:50.040 --> 22:58.280
the concept of falsifiability. So in science and in data driven endeavors in general, you never

22:58.280 --> 23:03.800
prove anything true. You just demonstrate that things your best idea, your best model is not yet

23:03.800 --> 23:09.880
false. And I was taught this in high school. And I thought it had existed since like, you know,

23:09.880 --> 23:13.560
something like the beginning of time, you know, I thought that this was like, you know,

23:13.560 --> 23:17.560
a happen had come up, you have thousands of years ago as like the nature of knowledge, but it turns

23:17.560 --> 23:22.840
out that this concept of falsifiability that's so central is actually from like the 1960s.

23:22.840 --> 23:29.320
Oh wow. It's much newer than you would think. Then you would think, yeah. And that just gave me

23:29.320 --> 23:33.000
this feeling that like so many of the techniques that we've been using and the way we've been

23:33.000 --> 23:38.040
thinking about data and this entire endeavor to learn about our world using like the systematic

23:38.840 --> 23:44.840
collection of data is like very new in the world of like philosophy and how you how you learn from

23:44.840 --> 23:51.880
data. So it was very fascinating. So those are two, those are two of my favorites, but I'm always

23:51.880 --> 23:59.720
looking for more. Yeah, it's interesting. I mean, humor, obviously, I associate with philosophy,

23:59.720 --> 24:07.560
but never would have connected that question to one of data explicitly, although it's obvious

24:07.560 --> 24:13.080
now that you say it. If anyone else who's listening to this knows of some others, definitely send

24:13.080 --> 24:24.920
them my way. It's super interesting. So reproducibility crisis kind of led you to thinking more broadly

24:24.920 --> 24:33.800
about data and some of the, maybe some of the implicit and explicit assumptions we make about data.

24:35.560 --> 24:41.160
One of the thoughts that I had when you were describing this, you know, the inherent

24:41.160 --> 24:50.200
belief in, in, you know, that there are rules that govern our data is kind of brings me back

24:50.200 --> 24:55.000
to the whole Bayesian versus frequentist thing like the implication is almost that, you know,

24:55.000 --> 25:03.080
there's no such thing as a non Bayesian. Well, yes, the way I like to think of it is that Bayesian,

25:03.080 --> 25:07.640
I think Bayesian statistics as an approach is much stronger than frequentist statistics,

25:07.640 --> 25:14.040
but it doesn't actually solve the problem because in order to make a prior, in order to frame it,

25:14.040 --> 25:18.920
you're still making a lot of assumptions about what matters in the problem. So what's fun to do

25:18.920 --> 25:24.440
as a thought experiment? I think one again, Bayesian is just a much more powerful technique,

25:24.440 --> 25:30.440
but it still requires you to make this like an initial guess on how, on how the world works. And

25:32.120 --> 25:36.040
that's again, that rule-based thing like you have to assume that there are rules. So it's,

25:36.040 --> 25:44.040
it's interesting. Yeah. And so did you, you know, to kind of come back to the implication

25:44.040 --> 25:51.080
for data scientists about this reproducibility crisis? Like, do you have, and when you talk about

25:51.080 --> 25:59.320
this, is there, is there like a prescriptive list at the end that you, you know, do A, B, and C,

25:59.320 --> 26:05.960
and you'll be clear? You'll avoid the fate of these poor, you know, these poor, air-room-producible

26:05.960 --> 26:15.560
scientists? Or is it not that clean? Well, so what I, what I say is data is not magic. And you

26:15.560 --> 26:20.840
just have to be okay with that. At a certain extent, you have to think about data and inference as

26:20.840 --> 26:27.400
a sum of the tools in your toolkit that you can use to approach a problem, but not necessarily

26:27.400 --> 26:36.200
always the answer. The second thing that I say is, this is for, for people who are in the position

26:36.200 --> 26:42.440
of making strategic investment in data products. So for example, V, C, these are even managers,

26:42.440 --> 26:49.240
and you're trying to understand which of these solutions that someone has put in front of me

26:49.240 --> 26:55.400
is most likely to actually generalize and actually work when put out into, you know, either as a

26:55.400 --> 27:00.360
company or as a new model for your software or your platform or whatever it is. And at that

27:00.360 --> 27:06.200
moment, when you have to make a choice, a really important question to ask is how did you make this

27:06.200 --> 27:12.840
model? Like what was it that made it work? And a warning flag for me being in this position all

27:12.840 --> 27:19.640
the time, like having to make a decision about where to invest our company's resources is to hear

27:19.640 --> 27:26.280
a data scientist say, I just ran 17,000 different algorithms in different tasks. And I did a huge

27:26.280 --> 27:31.240
grid service across like all of the perimeter space. And I found the one that performed the best.

27:31.880 --> 27:37.720
Because to me, that sounds like p-hacking. And it has been proven out in my experience that

27:37.720 --> 27:44.520
that's a riskier model to choose to, to put into production. Then alternatively, if someone comes

27:44.520 --> 27:49.000
to you and they say, you know, how did you make this model happen? What was the thing that mattered?

27:49.000 --> 27:53.960
And they come and they say, hey, like I thought really hard about the problem. And when I realized,

27:53.960 --> 27:58.520
I realized that if you look at the problem differently, if you look at it this way as opposed to this

27:58.520 --> 28:05.400
way, then everything made sense. Then my model that didn't work suddenly started working. And to me,

28:05.400 --> 28:11.240
that sort of I reframed the problem. And then things I didn't have to work as hard to find the

28:11.240 --> 28:17.000
right answer. I didn't have to p-hack. I didn't have to really push, you know, beat my data in order

28:17.000 --> 28:22.440
to get an answer. That is a sign I think has of a model that's much more likely to work

28:23.000 --> 28:26.680
when pushed into production. Or when you invest in a model.

28:27.560 --> 28:35.800
Is there an implication there that you're not a fan of deep learning? A lot of the way I think

28:37.560 --> 28:43.800
deep learning, the deep learning camp thinks about the world is like throw a huge amount of data

28:43.800 --> 28:49.880
at this neural network and, you know, let the network figure it out. And we shouldn't have to

28:51.480 --> 28:56.840
you know, in kind of the pure sense, like we shouldn't have to bring a lot of our priori knowledge

28:56.840 --> 29:01.960
into the way that we, you know, build these networks or like, you know, process the data or what

29:01.960 --> 29:05.480
have you. That's the role of the data and the network to figure that stuff out.

29:06.920 --> 29:10.600
You have, you have me pegged at what I call a deep learning skeptic.

29:10.600 --> 29:18.600
I would say that much like any tool, there are specific situations in which I think that could

29:18.600 --> 29:25.000
be a really interesting approach, but that it is not a panacea to all problems. And I don't believe,

29:25.000 --> 29:30.440
based on my understanding of philosophy, based on my understanding of the assumptions that go

29:30.440 --> 29:38.200
into the process of learning from data, that there will ever be a modeling architecture or structure

29:38.200 --> 29:46.520
that avoids this process or makes, that makes this this problem go away. Until like what would

29:46.520 --> 29:51.160
it take, it would take a new system of logic, something different than our current definition of

29:51.160 --> 29:59.160
inference. A solution to Hume's law of induction, so to speak, before whatever new algorithm or

29:59.160 --> 30:04.600
new approach comes out, I would, I would think that it was a revolutionary change to how we

30:04.600 --> 30:11.720
solve, solve these problems. I know it strikes me that you're also saying something about

30:12.840 --> 30:18.280
you know, the role of optimization, like you can, you know, your example with like, you know,

30:18.280 --> 30:24.600
grid searching your, your model to death, so to speak, like is this kind of philosophy? Does

30:24.600 --> 30:30.200
it undergarative belief that, you know, that there's some inflection point where you're 90%

30:30.200 --> 30:37.160
model that's based on fundamental principles, you know, is, you know, way better than your 95%

30:37.160 --> 30:43.960
you know, performing model that is kind of brute forced when you, you know, try to actually put

30:43.960 --> 30:49.160
it in a production. Yeah, so I mean, this is sort of, this is where it gets hard, right? So

30:49.800 --> 30:54.760
what I would say is what you learn when you study philosophy of and learning from data is that

30:54.760 --> 31:03.720
the pursuit of pure objectivity is a fool's errand in some way. But so why, why do you do data

31:03.720 --> 31:08.280
science? Well, you do it. Why do you build these models? It's because they're useful. And what I do

31:08.280 --> 31:16.520
think is true is that there's potentially diminishing returns in the last 90 to 95%. But that's not

31:16.520 --> 31:22.360
true in all cases. In some cases, you're, you know, that's, I always say that you don't make

31:22.360 --> 31:26.600
decisions based on probabilities. You make decisions based on expected value. If you have a

31:26.600 --> 31:34.040
use case where failing is so catastrophically terrible that you can't risk that extra 2%,

31:34.040 --> 31:38.680
then it might be worth your time and effort to try for the hopes that it actually does work. And

31:38.680 --> 31:44.520
it is possible, you know, to stumble upon the answer, even if it's not a rigorous process,

31:45.480 --> 31:50.440
it is possible to find it, right? It just means that it's not a robust way of going about it or

31:50.440 --> 31:57.080
a reproducible way of going about it, which is the nature of the game. So it's really about

31:57.080 --> 32:04.200
understanding, you know, what are you doing? Are you trying to sell like 5% more products?

32:04.200 --> 32:11.000
Are you trying to prevent 5% more crime? Like how bad are your potential failures? And then,

32:11.000 --> 32:15.800
and then deciding whether it's worth it to try to risk it to like, to do this brute force approach.

32:15.800 --> 32:20.920
But if you have the option, like you only do that if you don't have the option to solve it by

32:20.920 --> 32:26.200
understanding something fundamental or rule based about the problems, they should all, that should

32:26.200 --> 32:33.480
always be, in my opinion, your first step in building a model. Now we've talked about the

32:34.600 --> 32:40.040
reproducibility crisis in science and applying that, you know, the lessons learned there,

32:40.040 --> 32:48.200
two data science. You also hear from time to time, you know, talk of reproducibility issues

32:48.200 --> 32:54.120
in data science, meaning in the machine learning research, the difficulty going from a research

32:54.120 --> 33:06.280
paper to a working implementation. Any thoughts on that? Yeah. That was a very heavy yeah.

33:06.280 --> 33:14.840
My feeling about that is that it's probably my guess. I've seen the reports of this. I have not

33:14.840 --> 33:21.640
myself tried to reproduce some of these these core central papers, but my, my guess if I were to

33:21.640 --> 33:27.320
make a guess on what the cause of this is is sort of rooted in the same idea where if it's applied

33:27.320 --> 33:33.480
to the right problem, a problem that has good, a good formulation of rules, it probably works the way

33:33.480 --> 33:37.880
the person presented in the original paper. And then if you take the exact same use and you

33:37.880 --> 33:41.880
tweet that problem even slightly, like sometimes that problem can just be like where you got your data

33:41.880 --> 33:47.240
from. Because the rules no longer apply in quite the same way that they did because of how you

33:47.240 --> 33:52.520
collected it or because of your bias or some amount of data that you're missing for whatever reason.

33:53.720 --> 33:59.080
And that might explain some of the lack of reproducibility. In the same way,

33:59.080 --> 34:06.120
you know, that that the reproducibility crisis in science might be explained in some part by people

34:06.120 --> 34:11.000
not reproducing in exactly the same way, not using exactly the same materials, not doing things with

34:11.000 --> 34:15.880
exactly the same technology. That that some of that variance might be explained. And what that's

34:15.880 --> 34:21.480
really saying is that you were relying on rules that weren't actually core and central to your problem,

34:21.480 --> 34:26.360
but they were artifacts. They were, you know, extra extra things like my technology works this way.

34:26.360 --> 34:30.760
And so because so this worked in this case, as opposed to this other case, it didn't generalize

34:30.760 --> 34:35.000
in the same way as I expected it to. Yeah. Yeah. Yeah. It strikes me that there are a number of

34:35.000 --> 34:42.280
things going on there. I think there's the case that you mentioned where I'm trying to apply,

34:42.840 --> 34:50.600
you know, this paper to my use case. And all of the things that you mentioned apply, it also ties

34:50.600 --> 34:59.800
back to, I should say, the kind of issue that the Ali Rahimi brought up at in his nips presentation,

34:59.800 --> 35:07.240
you may have heard of this where he kind of decried the lack of rigor in the field or at least

35:07.240 --> 35:15.800
certain parts of the field, specifically, I think, talking about deep learning. And you know,

35:15.800 --> 35:20.360
that kind of ties to reproducibility and that, you know, people will publish papers, you know, there's

35:20.360 --> 35:26.440
not a set of kind of fundamental rules for how you got your architecture or your hyper parameters.

35:26.440 --> 35:31.000
You just have them. If you don't share them with anyone, then they can't reproduce what you did

35:31.000 --> 35:35.640
because they're like magic numbers that you just found in your grid search. Yeah.

35:36.600 --> 35:42.440
I think if we could learn anything from how poorly the scientist are handling the reproducible

35:42.440 --> 35:47.640
body crisis on a human level, I would suggest that when we try to solve these problems, we do our

35:47.640 --> 35:55.080
best to not throw stones or name call or point out individuals and talk again about the problem

35:55.080 --> 36:00.920
as like, hey, this is how data works. And we have to know this about the tool that we are using.

36:02.360 --> 36:08.520
Going into it so that it doesn't feel like a direct attack on someone who's trying to make

36:08.520 --> 36:14.120
things work with the tools as they understand them. I think a better solution to this problem is

36:14.120 --> 36:19.640
to raise the awareness of the idea that, hey, like, this doesn't actually work. We all wanted to work,

36:19.640 --> 36:24.280
you know, have data, magic formula, and then we know things. But it doesn't, and it's actually

36:24.280 --> 36:27.720
harder than that. And we all have to be okay with that when we when we approach it.

36:28.680 --> 36:34.920
Awesome. Any closing thoughts to kind of tie things up for us?

36:34.920 --> 36:39.880
No. Which is a totally valid answer.

36:42.200 --> 36:46.200
Yeah, I guess I guess I do have one closing thought is that another really great thought

36:46.200 --> 36:51.720
experiment to look into is the infinite monkeys theorem or infinite monkeys thought experiment.

36:51.720 --> 36:56.680
It's a really good demonstration of of what happens when you scale up inference.

36:56.680 --> 37:00.840
And explain that. I love this thought experiment. So it goes it goes basically like this.

37:00.840 --> 37:05.720
You walk into a room and there's a monkey and the monkey's on a typewriter.

37:06.280 --> 37:10.200
And the monkey's begging around on the typewriter and you look at what the monkey is typing

37:10.200 --> 37:15.400
and you see, hey, look, this monkey has typed all of Shakespeare's hamlet.

37:16.600 --> 37:19.800
And you think, oh my gosh, like, this is so crazy.

37:20.680 --> 37:25.480
Something must be up with this monkey. There has to be a rule to explain why this monkey happened.

37:25.480 --> 37:31.400
Clearly, this monkey came from the alien overlords to like down to earth to rewrite Shakespeare's hamlet.

37:32.600 --> 37:36.200
So you're very surprised by this. And then but then someone tells you, hey,

37:36.200 --> 37:39.160
if you had gone over to that room over on your left or the one over on your right,

37:39.160 --> 37:42.360
you would have seen another monkey. And in fact, there's not only one monkey, but there's actually

37:42.360 --> 37:46.120
millions of monkeys or billions of monkeys. And they've been typing on these typewriters

37:47.400 --> 37:51.160
for as long as anyone can remember. And every day we've been looking and we've been looking and

37:51.160 --> 37:55.880
we've been looking, we've just trying to find that monkey that had typed something that we thought

37:55.880 --> 38:01.400
was compelling that looked like there was evidence of a rule. And we finally found it.

38:01.400 --> 38:06.520
And then you think, oh, well, it's not like it's not thinking about this monkey. It's not the fact

38:06.520 --> 38:10.200
that this data happened or this data was collected, but how we came to find it.

38:11.880 --> 38:15.480
And so one of the things I like to always say is when you're trying to evaluate how

38:15.480 --> 38:19.960
strong your evidence is, you should ask how many monkeys you want to know how many times you had

38:19.960 --> 38:24.680
to look to try to find it. Nice. You're going to have to find a way to work that into the title of

38:24.680 --> 38:33.240
this podcast. How many months? Awesome. Well, Claire, you've definitely given us a lot to think about.

38:33.240 --> 38:38.360
Thank you for taking the time to chat with me. Thank you. It's been great.

38:42.760 --> 38:48.360
All right, everyone. That's our show for today. For more information on Claire or any of the

38:48.360 --> 38:53.800
topics covered in this episode, you'll find the show notes online at twomla.com.

38:54.680 --> 38:59.160
If you're new to the pod and like what you hear or you're a veteran listener and haven't

38:59.160 --> 39:05.080
already done so, head on over to your podcast app of choice and leave us your most gracious

39:05.080 --> 39:09.880
rating and review. It helps new listeners find us, which certainly helps us grow.

39:09.880 --> 39:19.880
Thanks so much for listening and catch you next time.

