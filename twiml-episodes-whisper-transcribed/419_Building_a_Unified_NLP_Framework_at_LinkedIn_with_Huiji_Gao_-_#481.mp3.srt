1
00:00:00,000 --> 00:00:21,000
All right, everyone, I am here with Ouijie Gao. Ouijie is a senior engineering manager for ML and AI at LinkedIn. Ouijie, welcome to the Twoma AI podcast.

2
00:00:21,000 --> 00:00:36,000
Thank you. It's my pleasure to be here. Hey, I'm really looking forward to digging into our conversation. We're going to talk quite a bit about NLP and some of the software and models that your team is building.

3
00:00:36,000 --> 00:00:44,000
But before we do that, I'd love to have you jump in and share a little bit about your background and how you came to work in ML.

4
00:00:44,000 --> 00:00:53,000
Yeah, thank you. I have always been interested in this machine learning and the applications back to my college.

5
00:00:53,000 --> 00:01:07,000
I remember kind of like 15 years ago, I was in the starting the information retrieval and I was using CIS for the conditional random field model to develop a name of the entity coordination.

6
00:01:07,000 --> 00:01:16,000
Then in my PhD study, I was in this machine learning and the data mining lab, starting the recommended systems.

7
00:01:16,000 --> 00:01:27,000
So in my major research topic at that time is how to develop advanced the recommended system with spatial, temple, social and content information.

8
00:01:27,000 --> 00:01:43,000
And in addition to several papers and also a book called the mining human mobility in location based social networks, we also developed several algorithms to help to build a recommendation for the POI for locations.

9
00:01:43,000 --> 00:01:53,000
During my study, I had an internship at LinkedIn and at that time I was working on how to model users' interests in companies to help on job recommendations.

10
00:01:53,000 --> 00:02:01,000
And after that, after my graduation, I joined the LinkedIn as a food time that is about six years ago.

11
00:02:01,000 --> 00:02:09,000
Yeah, I started my journey in ads, in the ads domain, with a CTR prediction and targeting progress.

12
00:02:09,000 --> 00:02:20,000
I was quite amused by the LinkedIn's rich data set. We have a large amount of members with their profiles and members they connect to other members.

13
00:02:20,000 --> 00:02:28,000
They also consume content and they also say search or looking at learning courses.

14
00:02:28,000 --> 00:02:43,000
While different members, they have different lawyers. For example, a member could be a job seeker looking for jobs, could be a recruiter looking for good candidates, and could be a salesman, and the member could also be a content generator or consumer.

15
00:02:43,000 --> 00:03:02,000
So, with all that comprehensive data set at LinkedIn, that was also my first time to get into this largest gear distribution training area to learn how to train a model with distributed training and how to process the data.

16
00:03:02,000 --> 00:03:16,000
And about two years after I stayed in the ads domain, I moved to search progress. So, that time, I was developing LinkedIn algorithms for people recommendation, for people search.

17
00:03:16,000 --> 00:03:20,000
So, the prototype is called people search.

18
00:03:20,000 --> 00:03:32,000
About three years ago, I started to manage a team called the AI algorithm foundation team. And this team is to develop advanced AI technologies.

19
00:03:32,000 --> 00:03:48,000
So, I started to manage a process in learning and personalization to part all the LinkedIn search and recommend systems. In addition to the product impact, we have so far, we have also open source, a set of technologies.

20
00:03:48,000 --> 00:04:08,000
Most recently released open source packages called D-text, D-E-T-X-T. It is a intended in the text understanding framework for people to, for our engineers to easily generate models for linking classification and language generation.

21
00:04:08,000 --> 00:04:25,000
So, it has also received several stars on GitHub. So far, we have about 1,000 stars. We also have published several papers on the data work. And one of them gets the best people award last year in CICM.

22
00:04:25,000 --> 00:04:41,000
Nice, nice. So, let's maybe dig into D-text and start with what are the main motivations for the tool? What were you trying to accomplish when you started down that road?

23
00:04:41,000 --> 00:05:02,000
Yes, I think we started to think about this idea two and a half years ago. As you know that LinkedIn, we have reached data search in all kinds of search and recommend systems with respect to different entities users are interested in.

24
00:05:02,000 --> 00:05:15,000
So, for example, for people, we have people search, people recommendation is called PYMK, people you may know. And for jobs, we have job search and job recommendations called Jingdi, jobs you may be interested in.

25
00:05:15,000 --> 00:05:32,000
For content, we have content search and say content recommendations like feed and ads, we also have other entities like schools, like groups. Now, all these information, it is kind of like represented by a large amount of text data.

26
00:05:32,000 --> 00:05:51,000
And from the members side, we also have member profiles and also members interest, members past behavior by interacting with these text data. So, how to understand this data is crucial for us to improve all these kinds of search and recommend system at LinkedIn.

27
00:05:51,000 --> 00:06:08,000
And at that time, I would say majority of the system is leveraging the energy in terms of the keyword match or skip Grammy embedding that is not considering a lot of contact information among words when we generally embedding.

28
00:06:08,000 --> 00:06:30,000
So, why is there a large amount of applications in search recommendation, so we can name a few for example ranking is one and understand users intent intent classification sentiment analysis, file detection, all the complete machine translation, all these problems, they will need to be part, they can be part and to get better performance back.

29
00:06:30,000 --> 00:06:47,000
So, we are thinking about build a say end to end framework that again targeted on two girls, first is to provide a better semantic understanding with advanced embedding technology.

30
00:06:47,000 --> 00:07:10,000
Second is to provide a better framework that can leverage using embedding is using embedding technology for all kinds of applications, as I mentioned before. So, that is the text, it is offline framework to help our engineers train offline models with these energy technology for ranking classification and the language generation.

31
00:07:10,000 --> 00:07:23,000
And so, what was the landscape for using embeddings for these kinds of ranking problems at LinkedIn before detects.

32
00:07:23,000 --> 00:07:43,000
Yes, before the text, the way is we started with keyword match like let me give example is if you search for which software engineer LinkedIn, this is a query and we look at documents that contains the keyword which software engineer or LinkedIn.

33
00:07:43,000 --> 00:07:57,000
But you see sometimes there are cinemas, for example, software engineer and software developer, they are similar meaning if we rely on the keyword match, then the job with software developer will not detectable.

34
00:07:57,000 --> 00:08:14,000
So, sometimes this is the early stage and then we start to look at different embedding technologies, for example, skitgram, that is also one of the state art, the art embedding technologies.

35
00:08:14,000 --> 00:08:29,000
So, I would say the cause of this kind of embedding technology cannot better capture the context among words, that context between software engineer and LinkedIn, between Huiji and software engineer.

36
00:08:29,000 --> 00:08:45,000
So, there are a lot of new technologies coming out in the past few years, like how to use CN for embedding and how to use STM for embedding, and I would say about two years ago, the most of the promising technology is bread.

37
00:08:45,000 --> 00:08:59,000
It can help to enhance this kind of contextual modeling among any two words in input text sentence and then generate a better embedding to represent this sentence.

38
00:08:59,000 --> 00:09:14,000
So, that is our kind of target when we develop a D text and pretty much also captured the journey when we started this text in text modeling.

39
00:09:14,000 --> 00:09:23,000
And so, is D text based on Bert, what's the underlying model and the relationship between the framework and the model?

40
00:09:23,000 --> 00:09:33,000
Yes, yes. So, D text is based on Bert, and it can also extend to support CN and STM.

41
00:09:33,000 --> 00:09:46,000
So, there are kind of two ways to use this kind of Bert embedding, or other CN embeddings, one way is to extract this embedding as the first step.

42
00:09:46,000 --> 00:09:52,000
And then we use this embedding as features, put them into our current ranking framework.

43
00:09:52,000 --> 00:10:03,000
It could be, say, a tree model, or a linear model like a logistic regression model. This kind of two stage model, and as you can see, it may lose some information.

44
00:10:03,000 --> 00:10:12,000
Because the embedding was generated by a different algorithm, without the context of the other information that will be used in the ranking model.

45
00:10:12,000 --> 00:10:22,000
So, D text is a model that can help generate this kind of ranking and classification model using through end to end process.

46
00:10:22,000 --> 00:10:41,000
We start from the log text data, get the embedding with Bert, or CN and STM, and then together with other existing handcraft features, eventually, they share the same lost function, and they are parameters updated simultaneously.

47
00:10:41,000 --> 00:10:54,000
And so, the talk a little bit about the kind of broader context for using D text, does it?

48
00:10:54,000 --> 00:11:08,000
You mentioned you did some work on distributed systems earlier at LinkedIn, is are you using it in that kind of distributed context, or is it used as a standalone type of system?

49
00:11:08,000 --> 00:11:24,000
Yes, it is in distributed context. And actually, in order to enable that, to ensure we have a good efficiency of training, we actually did a lot of work to parse that.

50
00:11:24,000 --> 00:11:40,000
So, one of the challenges, as you can imagine, that I'm just using Bert as an example, you can imagine that because of the model of this context information among words, you know, it's easier for takes a lot of time to influence.

51
00:11:40,000 --> 00:11:58,000
So, one way we have looked at is to start with a more flexible Bert structure. For example, the original Bert released by Google has a 12 layers version and 24 layers version.

52
00:11:58,000 --> 00:12:13,000
And there are some gap by using this model directly at LinkedIn. First is with this large number of layers that influence and training time itself would be quite time consuming.

53
00:12:13,000 --> 00:12:17,000
And some of them cannot meet our product request.

54
00:12:17,000 --> 00:12:25,000
Second is, you say, can't meet your product requirements is that from latency and inference.

55
00:12:25,000 --> 00:12:35,000
Also from the offline training, say, when you want to a purely trained model, then the training itself needs to meet certain requirements.

56
00:12:35,000 --> 00:12:50,000
If you want to be trained every four hours, but the training itself takes 40 hours, then this cannot be.

57
00:12:50,000 --> 00:13:06,000
So, we have to be trained with the Wikipedia data. And as we know, in LinkedIn, we have very, say, domain specific taxonomy. So, the skills, titles, and the, and also users experience our company names.

58
00:13:06,000 --> 00:13:13,000
And how can we incorporate this information into the Bert model is a challenge.

59
00:13:13,000 --> 00:13:22,000
And we decided to train a Bert model using LinkedIn data. So, we call this Bert model as Libert.

60
00:13:22,000 --> 00:13:28,000
Zero-zero benefit. Why is as we mentioned, it has better semantic dependation on LinkedIn data.

61
00:13:28,000 --> 00:13:40,000
And secondly is because we are training it from scratch. So, we are able to say start with a more flexible structure, like three layers.

62
00:13:40,000 --> 00:13:55,000
Or six layers, or starting with 256 lengths of embedding size. That is indeed what we do. And the one in our production, I think, currently used is a six layer Libert model.

63
00:13:55,000 --> 00:14:06,000
This can help us to significantly shorten our time by protruding such language model and also shorten the time for online inference.

64
00:14:06,000 --> 00:14:27,000
So, sounds like the general idea there is that, you know, because you've got the resources to do it and a very significant data set, you're giving up the advantage of externally pre trained Bert model and maybe doing some fine tuning and just training from scratch on a simplified model.

65
00:14:27,000 --> 00:14:32,000
And you found that that gives you better results.

66
00:14:32,000 --> 00:14:36,000
That's right. Yeah.

67
00:14:36,000 --> 00:14:54,000
Do you, how do you, you know, when you think about the attempts trying to get Bert to work, standard Bert to work versus Libert, how do you compare the two, what performance metrics did you use when you were evaluating the new approach?

68
00:14:54,000 --> 00:15:09,000
So that actually refers to another challenger of using Bert in D text. So in the beginning, the model performance is not good, comparing to say other state of art, ranking model using CNN or STM.

69
00:15:09,000 --> 00:15:29,000
The reason is in the beginning, what we realized is the way to do, to do Bert fine tuning in ranking tasks. So like what we mentioned before is about how to pre-train a model, pre-train a Bert model with data after we pre-train the model.

70
00:15:29,000 --> 00:15:39,000
The typical way to use Bert model is to fine tune it in a specific task. And in, and in this talent scenario, it is how to better use the engine task.

71
00:15:39,000 --> 00:16:03,000
But the challenge is that time is in the original Google, so in original British paper, it lists several ways to use Bert model for like ranking and naming and naming into the combination. And is the way to use Bert model for ranking is we need to concatenate the information from the member side and document side together as one sentence.

72
00:16:03,000 --> 00:16:17,000
So in magic, I'm a user. I'm here to search for something. I have a query here. And in order to use Bert, we need to concatenate this query information with every document information together.

73
00:16:17,000 --> 00:16:38,000
Then we send it to Bert and to get a genetic representation for the whole sentence. Then we decide to how to read and cut this special document. But you see this because the big challenge for online, for online latency because we are not able to do a lot of pre computing until we see a query.

74
00:16:38,000 --> 00:16:52,000
And only after we see a query, we have to start to concatenate this query with every document candidate and then do the scoring, which is usually beyond the seconds or even tens of seconds that is not acceptable by the production.

75
00:16:52,000 --> 00:17:09,000
And the reason why this comes about is because you have, you know, you have to include the user and you have so many users that you can't pre compute.

76
00:17:09,000 --> 00:17:29,000
With the user type of information, this, this forbidden us to pre compute a lot of information, which was used to be able to do before using Bert model. And, and of course, the reason of doing this for for ranking for ranking models because the reason of carbonism together is because

77
00:17:29,000 --> 00:17:46,000
Bert is good at capturing the contact information among different words by concatenism together. It does pose a good opportunity to capture the content information, contact information, but it makes it a challenge to get into product.

78
00:17:46,000 --> 00:18:05,000
So what we do is we tried a lot of different frameworks settings and initially we found this way that is all counted as the current way used in detect. We split the information from member side and the query set to different input fields.

79
00:18:05,000 --> 00:18:21,000
And with this info input fields, we still send them to bridge to get the embeddings. However, we concatenism together after we get an embedding, actually we close them together after we get an embedding.

80
00:18:21,000 --> 00:18:29,000
So to generate the similarities among these different fields as based on the embedding extracted from the bread.

81
00:18:29,000 --> 00:18:39,000
And from this way, we kind of like break the context earlier, but then model this context later after we get the embedding.

82
00:18:39,000 --> 00:18:50,000
And there's a risk because it is uncertain that whether use this framework can still capture the original content information that Bert is supposed to capture.

83
00:18:50,000 --> 00:18:59,000
So we did several experiments and eventually found out this way. So we are using detects indeed it is able to do so.

84
00:18:59,000 --> 00:19:10,000
Now can you can you go through that again what you're what specifically are you doing that's different from the standard bird of concatenating the.

85
00:19:10,000 --> 00:19:27,000
Yes, yes, so in the standard bird it is a concatenated law text. And then getting the embedding then for ranking yeah and indeed text we get the embedding from each text to field before concatenism.

86
00:19:27,000 --> 00:19:31,000
And then the third query and the dog.

87
00:19:31,000 --> 00:19:56,000
Yes, and then we computer the similarities among these different embeddings. So this is kind of like to learn their contacture coupling after we get the embedding and then these similarities becomes new features that we can use later for the ranking model.

88
00:19:56,000 --> 00:20:07,000
Does that allow you to do more pre computation on the document side exactly exactly in this case as you can see when we when we

89
00:20:07,000 --> 00:20:23,000
want to compute the protein embedding it is no longer based on the concatenated sentence. It is based on each individual field. So the query said is computed separately member said is also separated and then after the model is trained we can pre computers is membering

90
00:20:23,000 --> 00:20:35,000
embeddings and then store them in certain store that can be fetched online where query side because it is short. So we are able to do the real time inference with bread that is doable.

91
00:20:35,000 --> 00:20:51,000
And in this case as in can imagine when online a user comes and use a search with a query we use this model to do real time inference for this query is protein embedding and then we fetch the member embedding from the pre computer store.

92
00:20:51,000 --> 00:20:55,000
So that we do the inference for ranking.

93
00:20:55,000 --> 00:21:03,000
So end of the day what kind of performance properties does this have?

94
00:21:03,000 --> 00:21:10,000
Yeah, we don't really match your goal, but we don't have to compare to what you saw with the original bird.

95
00:21:10,000 --> 00:21:28,000
Yeah, so our metrics is based on the different product applications. For example, when we evaluated the people search and the job search we look at the metrics called and DCG that is a kind of like a typical evaluation metrics for search ranking.

96
00:21:28,000 --> 00:21:46,000
And when we work when we use detects for feed and as we look at the CTR a you see so that is all that is kind of like look at how much likely a user would click on a specific content given the impression.

97
00:21:46,000 --> 00:22:14,000
For this is for offline and for online we also have different measures for different product for example search wise we have search sessions and as that is revenue and also online CTR example as we as I can share is when we use this model for people search we absorb the significant lift on the online searching success rate.

98
00:22:14,000 --> 00:22:22,000
And also interested in the inference performance in terms of time, how did that compare from before and after.

99
00:22:22,000 --> 00:22:26,000
Yeah, good point.

100
00:22:26,000 --> 00:22:44,000
So the answer is I'm not able to provide this result because the baseline model is not able to ship into product due to the latency cancer. Yeah, it is not doable. Yeah, so you can even get far enough with the baseline model.

101
00:22:44,000 --> 00:22:58,000
Yeah, yeah, for the counter model the online latency is within 20 milliseconds. Okay, yeah, only one as I can imagine it would be more than 10 seconds something.

102
00:22:58,000 --> 00:23:21,000
Are there other ways that you have optimized it besides from the kind of high level architecture. Yes, we optimize the on the product stack. So even with this kind of a detect structure, even with this decoupling of the user side and the document side.

103
00:23:21,000 --> 00:23:36,000
We still encounter challenges about online scoring because of the large amount of candidates. So like for search system, some of the search systems, they have hundreds of thousands of candidates to rank at real time.

104
00:23:36,000 --> 00:23:54,000
In that case, we actually set up a few ranking stages. So for example, we have first pass ranker, which is using a very simple model to quickly filter out those definitely relevant documents.

105
00:23:54,000 --> 00:24:06,000
And then we use the detects to do a more fine granular to the ranking based on this say, using the model a size of candidates.

106
00:24:06,000 --> 00:24:11,000
And is the pre filtering model, is that also a learn model.

107
00:24:11,000 --> 00:24:28,000
It is a learn model. Yeah, it is a several options to use this model for example, a true model with simple structure, maybe just a few layers and logistic reaction model and the features that could be very small, a few tens to hundreds.

108
00:24:28,000 --> 00:24:43,000
Because the purpose is not to get a better ranking at this stage. It is made to filter out those irrelevant documents. So the next stage of ranking by detects can be can be launched without latency function.

109
00:24:43,000 --> 00:25:05,000
And so what do the what do the features look like for that pre model. Yeah, so the features could be for example, like users users, the region and the matching between the query and the documents in terms keyword match.

110
00:25:05,000 --> 00:25:20,000
And this could also be the user some content information for example, whether this user is is searching the query at a certain time point day of the week, all of the day.

111
00:25:20,000 --> 00:25:28,000
And the users social connections, these are the information can be used. I'm just giving you some examples.

112
00:25:28,000 --> 00:25:47,000
So the first one that you mentioned is, you know, very intuitive, you might want to if the users issuing a query in English, you might want to limit your search to English and sounds like this pre model or this pre filter is one of the mechanisms that you might use to do something like that.

113
00:25:47,000 --> 00:26:13,000
And so tell us a little bit about, you know, maybe in past conversations, I've talked with folks at LinkedIn and the idea of graphical models comes up a lot in the way you built machine learning on the user graph and content does.

114
00:26:13,000 --> 00:26:29,000
In what ways does these models detect them and liberate do they kind of interoperate or, you know, are they related to being used in a graphical context.

115
00:26:29,000 --> 00:26:42,000
Yeah, currently, we are still exploiting this direction. Detector is is not using graphically embedding technology.

116
00:26:42,000 --> 00:26:54,000
But we do see the motivation of using graphically embedding together with text information to generate a better embedding. So let me give you example.

117
00:26:54,000 --> 00:27:11,000
So what we are looking at is we can leverage users into actions, their actions to certain content click like share and their social connection to different people, these are kind of like linking information if you look at this graph.

118
00:27:11,000 --> 00:27:24,000
And there are also noting information that is known properties, for example, when a user is clicking on another job post and in addition to this linking information can be used for graphical neural modeling.

119
00:27:24,000 --> 00:27:40,000
So you can also look at the noting information from member from the job post this text information and be to a joint modeling. Then you can imagine in the end, we can generate a more comprehensive embedding with this huge economic graph at LinkedIn.

120
00:27:40,000 --> 00:27:58,000
The benefit of doing so is maybe in the future, we can eliminate certain filters by doing this kind of modeling together for example, maybe in the retrieval side, we want to first filter out by the social connections.

121
00:27:58,000 --> 00:28:13,000
And then we do a ranking within this specific range of social connections, but if we have this graph in neural embedding with the text information, then we can have better representation, we will not longer need this kind of filters in the beginning.

122
00:28:13,000 --> 00:28:25,000
So this is some ideas still at the early stage to discuss, but we do have some future plan to improve the text on that direction.

123
00:28:25,000 --> 00:28:40,000
Along the same lines is D text, is it tightly coupled to bird or could you swap in another type of model language model in to replace bird.

124
00:28:40,000 --> 00:29:02,000
You can, you can switch the birth model to other models like CNN, like Libert, like STM, and even for the birth model, it is quite compatible to different kinds of birth model, like Google's birth model, Microsoft Tuning model, and the laboratory model developed by Facebook.

125
00:29:02,000 --> 00:29:10,000
These are all different options that we offer to the clients.

126
00:29:10,000 --> 00:29:22,000
And with clients using those different models, is there any intuition you could share around, you know, when they tend to use one versus the other, how do they compare all to one another for your use cases.

127
00:29:22,000 --> 00:29:39,000
Yeah, yeah, so far all these exploration is actually more on the towards the offline set for production wise, because as we, as we do have this product constraint, so far the Libert model is still the first choice, because of the flexible structure three layers six layers.

128
00:29:39,000 --> 00:29:52,000
And the moments wise, I would say for a fair comparison, it may be very hard, because the Libert is trained on LinkedIn data, and the other but model is trained on other domain specific data.

129
00:29:52,000 --> 00:30:04,000
So we do observe that offline wise, there are some performance difference, but it's sometimes very hard to draw conclusion on this kind of set up to see like which model is better or not.

130
00:30:04,000 --> 00:30:12,000
Yeah, are there other models that you hope to customize in the way that you have with Burton Lieber.

131
00:30:12,000 --> 00:30:25,000
Yes, we do, yeah, we do, so because these models, they are also open source, it actually provide us opportunity to customize them further and compare with the Libert then provide to our all the LinkedIn clients.

132
00:30:25,000 --> 00:30:32,000
Yeah, as long as we have enough for say, engineering resource to support that.

133
00:30:32,000 --> 00:30:49,000
Talk a little bit about the practical use of this from engineers perspective, is this a library that they include in their notebooks and and use in that way, is it an online system that they're querying, what's the deployment model.

134
00:30:49,000 --> 00:31:04,000
Yeah, there are two ways to use this one is as you mentioned use it as a Jupyter notebook and this way is more about earlier offline analysis and a quick explore of the results.

135
00:31:04,000 --> 00:31:21,000
So in that way, users can just say, organize their training data set into a format that as required by D text and in D text, we have a lot of parameters, but we also have this to say,

136
00:31:21,000 --> 00:31:36,000
um, starters parameter that's before the parameter, we already set up for most of the use cases and we also have advanced parameters that can for use for like by machine learning experts to better tune performance.

137
00:31:36,000 --> 00:31:57,000
So users can use them in the Jupyter notebook or in our offline training stack, so this can be used to say as a library, you can incorporate into your current text stack and adding it as like a workflow with this parameter set up and you're wrong and get to the model.

138
00:31:57,000 --> 00:32:17,000
And can you give us a sense for how often our users kind of using off the shelf basic parameters versus, you know, the other end of the spectrum, you know, folks that may want to tweak the advanced parameters or even, you know, fine tune.

139
00:32:17,000 --> 00:32:39,000
I'm trying to get a sense for with these types of models and your type of use case, you know, do you find that everybody has to, you know, fine tune quite a bit because they need to, they need to do that to get the level of performance that's acceptable or is it, you know, good enough off the shelf for a lot of people.

140
00:32:39,000 --> 00:32:59,000
That's that's that's that's very good question here. So, the tune, first of all, I think fine tuning it's there for takes some effort by working dealing with these parameters, it will require some of the expertise from our engineers.

141
00:32:59,000 --> 00:33:12,000
But it also depends on the application as we observe, for example, in the sum of the search applications, they can share similar patterns on tuning these parameters.

142
00:33:12,000 --> 00:33:30,000
And some of the other applications say like ads feed, they have very large skill data and also very sparse features, then the tuning may take some time. That's why our current one of our current effort is to develop advanced optimizer for detects.

143
00:33:30,000 --> 00:33:44,000
And this can also be genetically used by other deep learning algorithms. So this optimizer is trying to help our engineers found a better setting of those key parameters say batch size learning rate.

144
00:33:44,000 --> 00:34:04,000
And the model selection iterations and we want to make it like more optimal and the more and then make the model less sensitive to those parameters in terms of the users usage. And that is already ongoing and we actually see some quite promising results by using this advanced optimizer.

145
00:34:04,000 --> 00:34:25,000
One example as I can share is via exporting the name layer wise optimizer for good. Yeah. And so is the optimizer, you know, what can you say about the general approach to optimization, is it, you know, along the lines of a Bayesian optimization or, you know, that's, that's

146
00:34:25,000 --> 00:34:38,000
more or is it more like heuristics, if your problem looks like this, then your parameters should look like that. Oh, yeah. So the optimizer, I'm going to put in here is more like a gradient descent and Adam.

147
00:34:38,000 --> 00:35:01,000
Yeah, kind of optimizers. Got it. So not specifically the hyperparameter. That is, I would say another direction, we are looking at that is a phone example, how to use the common knowledge of these hyperventilizing from different domain to extract patterns.

148
00:35:01,000 --> 00:35:15,000
So we can more efficient today to another workflow in a different domain, these hyperventilization, that's also, I would say, at the early stage of exploring.

149
00:35:15,000 --> 00:35:44,000
But at the lower level is this work on the optimizer is that, do you think of that as kind of, you know, basic research, trying to come up with the next generation optimizer, or is it more, you know, structurally something about the types of problems you're solving, you know, creates an opportunity to specialize the optimizer for those problems.

150
00:35:44,000 --> 00:35:59,000
It is currently, it is more research oriented for this direction. Yeah, okay. Awesome. Awesome. Well, Huiji, thanks so much for taking the time to share a bit about what you are up to.

151
00:35:59,000 --> 00:36:04,000
Thank you, Sam. Yeah, it's a great pleasure to be here. Awesome. Thank you.

152
00:36:04,000 --> 00:36:14,000
Thank you.

