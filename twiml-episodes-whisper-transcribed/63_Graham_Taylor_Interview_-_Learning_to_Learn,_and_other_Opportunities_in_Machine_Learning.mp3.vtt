WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.480
I'm your host Sam Charrington.

00:23.480 --> 00:27.400
What you're about to hear is the third of a series of shows recorded at the Georgian

00:27.400 --> 00:31.200
Partners Portfolio Conference last week in Toronto.

00:31.200 --> 00:36.040
My guess this time is Graham Taylor, professor of engineering at the University of Guelph

00:36.040 --> 00:39.040
who keynoteed day two of the conference.

00:39.040 --> 00:43.200
Graham leads the machine learning research group at Guelph and is affiliated with Toronto's

00:43.200 --> 00:47.520
recently formed Vector Institute for Artificial Intelligence.

00:47.520 --> 00:51.320
Graham and I discussed a number of the most important trends and challenges in artificial

00:51.320 --> 00:56.800
intelligence, including the move from predictive to creative systems, the rise of human in the

00:56.800 --> 01:02.720
loop AI and how modern AI is accelerating with our ability to teach computers how to learn

01:02.720 --> 01:04.040
to learn.

01:04.040 --> 01:08.200
Georgian Partners is a venture capital firm whose investment thesis is that certain tech

01:08.200 --> 01:14.160
trends change every aspect of a software business over time, including business goals, product

01:14.160 --> 01:19.640
plans, people in skills, technology platforms, pricing and packaging.

01:19.640 --> 01:24.160
Georgian invests in those companies best positioned to take advantage of these trends and then

01:24.160 --> 01:28.920
works closely with those companies to develop and execute the strategies necessary to make

01:28.920 --> 01:30.280
it happen.

01:30.280 --> 01:35.280
Applied AI is one of the trends they're investing in as our conversational business and security

01:35.280 --> 01:36.280
first.

01:36.280 --> 01:39.660
Georgian sponsored this series and we thank them for their support.

01:39.660 --> 01:45.400
To learn more about Georgian, visit twimmolai.com slash Georgian where you'll also be able to

01:45.400 --> 01:51.240
download white papers on their principles of applied AI and conversational business.

01:51.240 --> 01:55.720
Before we jump in, if you're in New York City on October 30th and 31st, we hope you'll

01:55.720 --> 02:00.120
join us at the NYU Future Labs AI Summit and Happy Hour.

02:00.120 --> 02:04.040
As you may remember, we attended the inaugural summit back in April.

02:04.040 --> 02:08.440
The fall event features more grade speakers including Karina Cortez head of research at

02:08.440 --> 02:14.320
Google New York, David Venturelli, science operations manager at NASA Ames Quantum AI

02:14.320 --> 02:19.480
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.

02:19.480 --> 02:27.600
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the

02:27.600 --> 02:29.720
code twimmol25.

02:29.720 --> 02:36.120
For details on the Happy Hour, visit our events page at twimmolai.com slash events, and now

02:36.120 --> 02:38.120
onto the show.

02:38.120 --> 02:49.280
Alright everyone, I am here at the Georgian Partners portfolio conference and I've got

02:49.280 --> 02:51.880
the pleasure of being seated with Graham Taylor.

02:51.880 --> 02:57.840
Graham is a professor at the University of Guelph here in Canada, and he did a really

02:57.840 --> 03:04.120
interesting talk today on some of the challenges and opportunities associated with machine learning

03:04.120 --> 03:07.840
in AI and particularly around his research area and deep learning.

03:07.840 --> 03:11.040
And we're here to spend a little bit of time chatting about that.

03:11.040 --> 03:12.720
Graham, welcome to the podcast.

03:12.720 --> 03:13.880
Thanks for having me on the program.

03:13.880 --> 03:17.880
I told you this is my first time doing a podcast, so I'm really excited to being a consumer

03:17.880 --> 03:20.520
of podcasts to actually get back to the podcast community.

03:20.520 --> 03:21.520
So thanks for having me.

03:21.520 --> 03:23.640
Nice, absolutely, absolutely.

03:23.640 --> 03:27.800
Why don't we get started by having you tell us a little bit about your background and

03:27.800 --> 03:32.000
how you got involved in machine learning in AI and what you're up to nowadays?

03:32.000 --> 03:36.360
Sure, so currently I'm working as a professor at the University of Guelph.

03:36.360 --> 03:41.440
I'm also a member of the new Vector Institute for Artificial Intelligence, which has started

03:41.440 --> 03:45.400
up and getting ready to move in in November here in Toronto.

03:45.400 --> 03:52.280
I'm the academic director of a program called Next AI, which is a founder development program

03:52.280 --> 03:55.560
for startup specifically working on AI technologies.

03:55.560 --> 03:59.480
So I wear a number of different hats, but they're all focused on artificial intelligence

03:59.480 --> 04:01.000
and machine learning.

04:01.000 --> 04:04.760
So let me tell you a little bit about how I entered that space.

04:04.760 --> 04:10.400
I often get asked this question of how I got into AI, and fortunately I can point at

04:10.400 --> 04:14.680
one specific point in my life, which really convinced me.

04:14.680 --> 04:19.240
And that was an inspiring professor when I was an undergrad student at the University

04:19.240 --> 04:20.240
of Waterloo.

04:20.240 --> 04:21.640
So I had a course.

04:21.640 --> 04:27.040
I believe the course was called Machine Intelligence, and the way that course was set

04:27.040 --> 04:33.920
up was to actually encourage us all the students to write AI programs to play each other's

04:33.920 --> 04:36.280
AI programs in this game called Abelone.

04:36.280 --> 04:39.360
And it's amazing looking at this effectively.

04:39.360 --> 04:43.960
I would say it's been at least 15 years now since we did this.

04:43.960 --> 04:47.800
But with all the news last week, in terms of this new AlphaGo system by Google Deep

04:47.800 --> 04:52.720
Mind, playing the game of Go, and how it was trained entirely by self-play.

04:52.720 --> 04:55.120
This is exactly what we were trying to do on this assignment.

04:55.120 --> 05:00.480
It was an easier game, but it was really inspirational to build these agents, played against each

05:00.480 --> 05:05.480
other, and our team ended up winning the competition, made us really proud and excited and eager

05:05.480 --> 05:06.480
to do more work.

05:06.480 --> 05:07.480
Nice.

05:07.480 --> 05:09.480
Yeah, so that's what started all off, okay?

05:09.480 --> 05:12.440
And so what's your path been so far?

05:12.440 --> 05:18.800
So from that point, as an undergrad, I got so excited about the potential of AI.

05:18.800 --> 05:23.000
No, I wouldn't say at all I was sort of going to predict what would happen up to this

05:23.000 --> 05:25.640
time and how huge it's growing.

05:25.640 --> 05:27.560
But I just observed being a technical person.

05:27.560 --> 05:32.920
I was really excited about building those tools and wanting to learn more.

05:32.920 --> 05:34.440
So I went to the University of Toronto.

05:34.440 --> 05:36.920
I knew they had a good machine learning program.

05:36.920 --> 05:42.080
They had a number of faculty there who I was aware of, their work, lots of graduates,

05:42.080 --> 05:43.080
students.

05:43.080 --> 05:44.440
It seemed like a great place to be.

05:44.440 --> 05:47.800
It was not too far away from Waterloo and London, Ontario, where I grew up.

05:47.800 --> 05:48.800
Okay.

05:48.800 --> 05:50.800
It seemed like a natural choice.

05:50.800 --> 05:55.720
Now I had no idea how big that group would become and the influence that Toronto machine

05:55.720 --> 05:58.760
learning group would have on deep learning today.

05:58.760 --> 06:03.480
So some people asked me about this and I said, well, I know I kind of stumbled upon this

06:03.480 --> 06:04.480
group.

06:04.480 --> 06:08.400
It seemed the right place to be for all the right reasons, but it ended up being an amazing

06:08.400 --> 06:09.400
time to be there.

06:09.400 --> 06:15.600
This is for me 2004 to 2009, really the start of the deep learning movement.

06:15.600 --> 06:22.280
So a lot of the individuals that are leading the major industrial research labs or even

06:22.280 --> 06:28.360
the nonprofit efforts like OpenAI, for example, they were students in the group at the time.

06:28.360 --> 06:34.840
A lot of the key papers and key ideas that were published and disseminated at that time

06:34.840 --> 06:38.120
have gone on, right, to be sort of the foundations of A&M.

06:38.120 --> 06:41.760
And a lot of it came out of that group and the groups that we were having close collaboration

06:41.760 --> 06:46.600
with, including the Montreal, now they call it Montreal Institute for Learning Algorithms

06:46.600 --> 06:51.240
run by Yasha Bengeo, and then the group at NYU, which at that time was led by you on

06:51.240 --> 06:53.160
the can.

06:53.160 --> 06:56.240
And mentioning NYU, that's where I went immediately after PhD.

06:56.240 --> 06:59.920
We had a good relationship, working relationship between these three labs, Montreal, Toronto

06:59.920 --> 07:00.920
and NYU.

07:00.920 --> 07:05.680
I considered both as postdoc options, but ultimately decided to go to New York for a couple

07:05.680 --> 07:10.600
years and work there as a postdoc, but felt the pull to come home after that.

07:10.600 --> 07:15.920
It was really exciting working in New York with Jan with Rob Fergus, another professor named

07:15.920 --> 07:21.760
Chris Breggler, and that really actually got me working in computer vision more.

07:21.760 --> 07:28.000
And then I came back in 2011, and my heart was really pulling me not just towards coming

07:28.000 --> 07:31.320
back to Canada, but also towards an academic position.

07:31.320 --> 07:36.040
So I had the opportunity to join the faculty at Guelph in 2012, and that's when I started.

07:36.040 --> 07:37.040
It's been five years.

07:37.040 --> 07:38.040
Nice, nice.

07:38.040 --> 07:40.760
And you mentioned a bunch of names there, but you didn't mention that your advisor for

07:40.760 --> 07:42.280
your PhD was Jeff Hinton.

07:42.280 --> 07:43.280
I should, yeah.

07:43.280 --> 07:47.600
I didn't mention a bunch of names, but I didn't credit my PhD advisor, I'm sorry, Jeff,

07:47.600 --> 07:48.600
I'm sorry.

07:48.600 --> 07:53.680
But I've talked to so many people who he's, you know, impacted via advising and other

07:53.680 --> 07:54.680
ways, so.

07:54.680 --> 07:55.680
That's right.

07:55.680 --> 08:00.680
I was co-advised by Jeff and another individual named Sam Royce, who was really influential

08:00.680 --> 08:01.960
in machine learning.

08:01.960 --> 08:08.520
He passed away actually in 2009, right when I started at NYU.

08:08.520 --> 08:15.280
It was tragic to lose him, but I certainly wanted to note his influence on my PhD as well.

08:15.280 --> 08:20.400
It was really amazing having one very senior advisor, Jeff, who really experienced having

08:20.400 --> 08:24.880
worked in the field, but also someone quite junior Sam had started as a faculty, I think,

08:24.880 --> 08:28.160
around 2005 or so.

08:28.160 --> 08:32.960
And he was full of energy and also helped me along the way.

08:32.960 --> 08:33.960
Awesome.

08:33.960 --> 08:34.960
Awesome.

08:34.960 --> 08:35.960
So you did a talk here this morning.

08:35.960 --> 08:38.560
Tell us a little bit about what your talk was about.

08:38.560 --> 08:39.560
Sure.

08:39.560 --> 08:42.240
So I broke my talk up into three parts.

08:42.240 --> 08:45.840
The first part was just introducing myself and telling people a little bit about the work

08:45.840 --> 08:50.280
that we do in Guelph and the types of machine learning problems we're interested in.

08:50.280 --> 08:55.600
The second part of the talk was focused on the challenges and also the opportunities

08:55.600 --> 09:01.600
in AI, and that was more of a technical discussion of what was coming around the corner.

09:01.600 --> 09:06.320
And then the third part of the talk was about some of the barriers, some startups might

09:06.320 --> 09:07.320
be facing.

09:07.320 --> 09:10.800
We're here at the George and Partners portfolio conference, so there's many startups in

09:10.800 --> 09:11.800
the audience.

09:11.800 --> 09:17.000
I've done a lot of work with startups, and I tried to focus on, again, some of the barriers

09:17.000 --> 09:19.640
they might face building companies.

09:19.640 --> 09:24.480
So why don't we start with kind of a rundown of the challenges and opportunities as you

09:24.480 --> 09:25.480
see them?

09:25.480 --> 09:26.480
Sure.

09:26.480 --> 09:32.480
So I started by talking about the technological changes coming towards us, and I think the

09:32.480 --> 09:39.000
first one that I started with was the move from what I would call largely predictive systems

09:39.000 --> 09:40.000
to creative systems.

09:40.000 --> 09:44.680
So when I say predictive systems, I mean these systems that were used to interacting

09:44.680 --> 09:49.040
with on a daily basis, the systems that might give us a temperature forecast tomorrow,

09:49.040 --> 09:53.200
or we might get up our mapping application and would tell us an estimated time to get

09:53.200 --> 09:58.440
it from A to B, or the people on the financial side might be interested in forecasting the

09:58.440 --> 10:01.800
price of a particular financial instrument the next day.

10:01.800 --> 10:06.000
But those types of inputs, they're either a category or they're a number, they're pretty

10:06.000 --> 10:09.480
low dimensional, and there's usually a single right answer.

10:09.480 --> 10:13.280
And so when I talk about the movement towards creative systems, I'm talking about systems

10:13.280 --> 10:18.880
that produce high dimensional output, and where there's no single right answer.

10:18.880 --> 10:25.080
So examples of this sort of more on the creative side would be art creation or poetry or music.

10:25.080 --> 10:31.440
And while these are some of the more culturally flavored activities, which gets some attention,

10:31.440 --> 10:38.080
there's also some real commercial applications such as automatic email reply or conversational

10:38.080 --> 10:44.920
dialogue systems, or I showed a proposed design for a robot that's creating meals and serving

10:44.920 --> 10:46.440
to them to you every day.

10:46.440 --> 10:49.560
And so I talked about some of the challenges in building those kinds of systems.

10:49.560 --> 10:51.360
I thought that one was pretty interesting.

10:51.360 --> 10:55.480
Folks that listen to the podcast will be pretty familiar with the idea of generative

10:55.480 --> 11:02.600
networks and style transfer and creating all the efforts we've seen to create movie

11:02.600 --> 11:05.920
scripts and poetry and all this kind of stuff.

11:05.920 --> 11:11.280
And so the art example has been the front and center for me for a while.

11:11.280 --> 11:18.400
But then when you kind of describe the recipe creation, that's a totally different domain

11:18.400 --> 11:22.680
than one that we hear about all the time, maybe because there are a bunch of different disciplines

11:22.680 --> 11:29.040
that need to come together for us to really explore, you know, or fulfill that, the Jetsons

11:29.040 --> 11:30.040
vision.

11:30.040 --> 11:31.040
Totally.

11:31.040 --> 11:32.040
Yes.

11:32.040 --> 11:37.240
But we're, you know, quickly moving towards an area where a lot of opportunities and value

11:37.240 --> 11:43.280
exist around generative, using neural networks specifically in AI in general for generative

11:43.280 --> 11:44.800
purposes.

11:44.800 --> 11:48.560
What are the key challenges there in your mind in that transition?

11:48.560 --> 11:53.640
I'm particularly interested in this as an engineer working in engineering school.

11:53.640 --> 12:00.360
I build things and I think we're seeing design migrate over from purely human design

12:00.360 --> 12:06.080
to at least at this the next few years being machines and humans working together on design

12:06.080 --> 12:11.760
and building objects, whether they be recipes or they be parts for vehicles or aircraft.

12:11.760 --> 12:18.400
So I think the major challenges in working towards a more algorithmic design would be what

12:18.400 --> 12:25.160
I pointed out this morning, namely the fact that there's no single right answer for design.

12:25.160 --> 12:31.800
You have potentially infinite number of solutions to a problem or designs that would be acceptable.

12:31.800 --> 12:37.400
And this makes it very hard to come up with reward signals or what we would call objectives

12:37.400 --> 12:41.800
for machine learning systems depending on how they're trained, right?

12:41.800 --> 12:48.960
So for us, when we think about a simple task, like image classification, image goes in,

12:48.960 --> 12:50.360
category comes out.

12:50.360 --> 12:53.800
We compare it with the ground truth category, there's usually a single right answer.

12:53.800 --> 12:58.560
For a system going back to this recipe example, how do you sort of, how do you measure the

12:58.560 --> 13:02.040
output of the system that cooks you a meal? I mean, you can, I guess, get some sort of

13:02.040 --> 13:06.760
subjective judgment of the person eating the meal, but it's not really calibrated with

13:06.760 --> 13:07.920
other people.

13:07.920 --> 13:11.240
And it's also just that single reward, these are the types of rewards maybe that are being

13:11.240 --> 13:14.920
used in reinforcement learning systems, they're very weak signals as well.

13:14.920 --> 13:21.280
So it'd be nice to maybe find some sort of medium between this like weak subjective reward

13:21.280 --> 13:26.080
or the explicit guidance that works for supervised learning systems.

13:26.080 --> 13:32.920
The other path for a long way in that, against that challenge, like that particular approach.

13:32.920 --> 13:38.320
Yeah, I think we've seen, as you've mentioned, the listeners of the podcast are going to

13:38.320 --> 13:43.400
be familiar with examples, particularly on the visual side of generative systems.

13:43.400 --> 13:46.920
That's kind of where we're stuck right now is evaluating generative systems, coming

13:46.920 --> 13:51.800
up with quantitative metrics, one to evaluate them, but also maybe as a way of feeding

13:51.800 --> 13:56.040
this kind of quantitative metric back into the system to make them better, right?

13:56.040 --> 13:58.840
How do we improve the objectives to train them?

13:58.840 --> 14:03.320
So then we've also seen a lot of progress really recently on the reinforcement learning

14:03.320 --> 14:09.440
reward side, but we aren't really anywhere, I think, on the sort of merger of those two

14:09.440 --> 14:10.440
systems.

14:10.440 --> 14:12.120
I think there's a lot more to be done.

14:12.120 --> 14:18.840
And particularly, we've seen a lot of nice examples in the generative space of language,

14:18.840 --> 14:23.480
so machine translation systems, conversational dialogue systems, but I think we're still

14:23.480 --> 14:28.880
stuck in coming up with the right kinds of metrics, so do you have, say, an English sentence

14:28.880 --> 14:35.560
going in, a French translation coming out, again, there's so many possible valid translations.

14:35.560 --> 14:43.400
We're still, in most cases, stuck at measuring a single, maybe I would say a canonical example

14:43.400 --> 14:47.640
given in some data set with the output of the system rather than really considering

14:47.640 --> 14:51.560
the space of the potential answers it could give.

14:51.560 --> 14:52.880
Okay.

14:52.880 --> 14:59.640
One of the examples that you use was inbox by Google, which I also use and you went a little

14:59.640 --> 15:00.640
further.

15:00.640 --> 15:05.920
You have a percentage in your mind of the time that you use that for responses, not quite

15:05.920 --> 15:09.560
there yet, but I do use the responses every once in a while.

15:09.560 --> 15:14.600
But you also talked about, you talked about a bunch of concepts, you know, transfer

15:14.600 --> 15:19.400
learning, you know, meta learning, feshut learning, and one of the questions that I had

15:19.400 --> 15:24.480
as you were kind of going through this was, you know, what are the, kind of the mechanisms

15:24.480 --> 15:31.880
and approaches for using, you know, in a large scale system, the feedback that you provide

15:31.880 --> 15:38.560
by selecting, you know, one of these responses in conjunction with the, like, the broader model

15:38.560 --> 15:40.760
that's trained for everybody.

15:40.760 --> 15:44.440
And what is that, what's that problem called, what are the approaches, like, how far are

15:44.440 --> 15:49.240
we along in, you know, developing a body of thinking around that?

15:49.240 --> 15:50.240
Right.

15:50.240 --> 15:55.040
So I think I was referring to this Google inbox client and I'm a big fan of it, and the

15:55.040 --> 15:56.040
user of it.

15:56.040 --> 16:01.440
And like you said, some percentage of the time, the auto email reply feature, it's what

16:01.440 --> 16:04.280
I would call a human and a loop system.

16:04.280 --> 16:08.360
And what I was saying earlier this morning is essentially, I wouldn't want to necessarily

16:08.360 --> 16:11.400
hand over all my email to an automatic reply system.

16:11.400 --> 16:12.400
Sure.

16:12.400 --> 16:16.240
AI is not at that stage where I could stop writing emails and people would just interface

16:16.240 --> 16:22.680
with me through this, this agent, but it's working at the level where it can propose several

16:22.680 --> 16:25.600
candidate replies, and I can still execute judgment over there.

16:25.600 --> 16:27.600
I can decide not to send the email at all.

16:27.600 --> 16:31.600
I can decide to not accept the proposals and write an email myself.

16:31.600 --> 16:36.040
I'm still completely in control, but it's making me more efficient when it, once in a

16:36.040 --> 16:39.440
while, proposes something that I can just click on and it will send.

16:39.440 --> 16:42.840
So I would say this is a system, it's a human and a loop system.

16:42.840 --> 16:47.560
It's where I maintain the judgment over what goes out, and I see this as an effective paradigm

16:47.560 --> 16:51.400
of humans and machines working together over the near term.

16:51.400 --> 17:00.280
But I also really like this idea of the transition from sort of full human control over a particular

17:00.280 --> 17:07.960
task all the way to fully automated performance, but this gray area in between, and a company

17:07.960 --> 17:12.960
that I co-founded in Toronto named Kindred, they're actually exploring this for robotics,

17:12.960 --> 17:18.880
where essentially the company is teaching robots to perform tasks that are very difficult

17:18.880 --> 17:24.000
to automate by allowing a human operator to control one or more robots.

17:24.000 --> 17:28.000
So the robot will be autonomous, but when it gets into trouble, it can be taken over

17:28.000 --> 17:32.560
by a remote operator who sort of gets it out of that, whatever it's stuck to, and the

17:32.560 --> 17:38.480
hope is again to, if this happens enough times, the robot learns about the way that the

17:38.480 --> 17:43.240
human assisted in getting out of that particularly difficult situation so that it becomes more

17:43.240 --> 17:44.880
and more autonomous.

17:44.880 --> 17:51.280
So again, it's not 0% or 100% automation, we're sort of exploring that gray area in between.

17:51.280 --> 17:53.560
So I really like this paradigm.

17:53.560 --> 17:58.600
Because you're using inbox, it's presenting you these possible emails that you might want

17:58.600 --> 18:03.240
to respond with, it gives you three, you choose one.

18:03.240 --> 18:09.800
That's potentially augmenting the set of training, label training data that the system has.

18:09.800 --> 18:16.280
And one way for Google in particular, or someone building a system like this in general,

18:16.280 --> 18:23.360
is to kind of throw that all in and continuously update the model and produce better models

18:23.360 --> 18:26.120
that are trained on more data.

18:26.120 --> 18:33.960
It strikes me that another way for a system like this to operate is that there's a general

18:33.960 --> 18:39.360
component of the model, but then there's a subcomponent of the model that's personalized

18:39.360 --> 18:42.560
to me and the way I respond.

18:42.560 --> 18:46.040
And the question is really, is anyone doing that?

18:46.040 --> 18:47.600
Does that have a name?

18:47.600 --> 18:49.600
Are there architectures for that?

18:49.600 --> 18:51.680
Have you ever come across that?

18:51.680 --> 18:54.400
Well, I would say this fits into the idea of personalization.

18:54.400 --> 18:59.360
And I think it's important for a product like inbox to have some element of personalization.

18:59.360 --> 19:04.040
A colleague actually told me that he doesn't use inbox because it makes him sound like

19:04.040 --> 19:07.280
a California dude.

19:07.280 --> 19:11.760
He said it puts exclamation marks on everything he says and uses terms like awesome exclamation

19:11.760 --> 19:14.680
mark, which he wouldn't say himself.

19:14.680 --> 19:15.840
So interesting.

19:15.840 --> 19:21.360
And he also claims that when I send him emails, he can tell when it's coming from the

19:21.360 --> 19:23.880
Google inbox, got a reply system.

19:23.880 --> 19:28.080
So again, what would fix something like this and maybe make him an adopter is a system

19:28.080 --> 19:31.160
that would adapt to his own style of writing emails.

19:31.160 --> 19:38.320
So it also ties back to the bias element of the conversation in a more subtle way than

19:38.320 --> 19:39.880
we sometimes think about it.

19:39.880 --> 19:40.880
Exactly.

19:40.880 --> 19:42.200
So why is it making him sound like a California dude?

19:42.200 --> 19:46.320
Well, maybe it was a bunch of emails from people in California, right?

19:46.320 --> 19:48.280
So it certainly ties back to that.

19:48.280 --> 19:53.840
I think in terms of how to talk about this and you even raised the idea of having a model

19:53.840 --> 19:58.840
that's been built from a whole lot of data, sort of a master model, and then personalize

19:58.840 --> 20:01.400
models for each of the people and sort of adapting.

20:01.400 --> 20:03.120
I think we do see it.

20:03.120 --> 20:04.920
It's an instance of transfer, right?

20:04.920 --> 20:09.360
So I was talking today about how do you deal with these problems where you have a very limited

20:09.360 --> 20:13.800
amount of labeled data, and I said, well, it's very popular right now to train a model

20:13.800 --> 20:19.360
in a big generic data set and then cut off the top of it and then replace that with something

20:19.360 --> 20:23.480
more specific and then train on a very much smaller set of data.

20:23.480 --> 20:27.640
So you can take something like generic object recognition, a big image net style data

20:27.640 --> 20:32.720
data set and then tackle a task like bird species classification, which is fine grain,

20:32.720 --> 20:34.400
but you have much less data.

20:34.400 --> 20:39.720
But that works because in the big system, there are birds in it, right?

20:39.720 --> 20:44.600
The data set image net has birds, so you can learn about feathers and wings and colors

20:44.600 --> 20:47.120
of birds and beaks and those sorts of features.

20:47.120 --> 20:50.840
That works when there's good match between the two different domains.

20:50.840 --> 20:54.400
And so this is what you're saying in terms of personalization, it can be viewed that way

20:54.400 --> 20:55.400
as well.

20:55.400 --> 20:58.680
You have a large data set of a whole bunch of different speakers and you can learn a

20:58.680 --> 21:05.480
model on that, but then you want to transfer or adapt this system to a particular individual

21:05.480 --> 21:08.040
where you have a smaller subset of data.

21:08.040 --> 21:12.840
And it makes sense, you wouldn't want to necessarily have a model for each individual person

21:12.840 --> 21:17.760
that work in isolation because there's probably not enough data there to generalize well.

21:17.760 --> 21:21.880
In that case, you want to capitalize from the all of the email that Google is holding

21:21.880 --> 21:25.120
in its service with people using Gmail.

21:25.120 --> 21:26.120
Okay.

21:26.120 --> 21:27.120
Okay.

21:27.120 --> 21:29.720
So additional challenges that you were describing.

21:29.720 --> 21:30.720
Oh, yeah.

21:30.720 --> 21:36.160
So I think we had gotten, we'd really only gotten across the sort of the two opportunities

21:36.160 --> 21:37.160
coming across.

21:37.160 --> 21:40.320
I could move into challenges or I could tell you about another couple of things that are

21:40.320 --> 21:42.520
coming across as sort of trends.

21:42.520 --> 21:44.760
So maybe I'll try to finish those off.

21:44.760 --> 21:51.000
The two trends that I hadn't mentioned yet, one was this idea of moving from careful human

21:51.000 --> 21:53.880
construction to learning to learn.

21:53.880 --> 21:57.240
So right now, like these, the systems are like the output of the hard work of graduate

21:57.240 --> 22:02.240
students and the faculty members advise in them and researchers and practitioners.

22:02.240 --> 22:08.200
I mentioned about the migration from feature engineering to architecture engineering, right?

22:08.200 --> 22:12.240
The way people describe deep learning often is that, oh, it's the end of feature engineering.

22:12.240 --> 22:16.600
We no longer have domain experts who craft very specific features.

22:16.600 --> 22:18.360
We can learn all the features with deep learning.

22:18.360 --> 22:19.360
Right.

22:19.360 --> 22:23.600
And I saw you had a picture of Stephen Merritti's article on your slide, which I've talked

22:23.600 --> 22:25.800
about on the podcast a while ago.

22:25.800 --> 22:26.800
Okay.

22:26.800 --> 22:27.800
Fantastic.

22:27.800 --> 22:31.360
So I love that blog post and it really, I think it's totally accurate.

22:31.360 --> 22:33.960
It moves into the world of architecture engineering.

22:33.960 --> 22:39.200
And so one way of getting out of this is essentially having these metal learning style algorithms.

22:39.200 --> 22:43.920
I mentioned a specific example in our lab where we're dealing with multimodal data.

22:43.920 --> 22:49.560
So in this case, we might have video and audio and we had motion capture coming in.

22:49.560 --> 22:53.120
And so we're figuring out how to actually merge those different modalities.

22:53.120 --> 22:57.240
I mean, the nice thing about deep learning models is with multimodal learning, you have

22:57.240 --> 23:03.440
so many opportunities of how to extract representations from the different modalities and how many

23:03.440 --> 23:06.560
levels of representations you should go for each of those.

23:06.560 --> 23:10.560
And when they should be merged together and which modalities should be merged, but there's

23:10.560 --> 23:12.560
all these decisions to be made.

23:12.560 --> 23:16.320
And so you can either have a grad student like we had who we just really skilled at figuring

23:16.320 --> 23:20.040
this all out and spends a year working towards a competition.

23:20.040 --> 23:23.800
But ultimately, we'd like to hand that over to an algorithm that figures that out.

23:23.800 --> 23:25.200
And that's what we've done.

23:25.200 --> 23:28.520
So that's one instance of learning in architecture.

23:28.520 --> 23:32.480
I know Google Brain has been working on this with reinforcement learning.

23:32.480 --> 23:34.520
They worked on learning optimizers.

23:34.520 --> 23:39.240
They're now working on, and other people are also working on learning activation functions.

23:39.240 --> 23:44.040
So really like it's, yeah, hand it over to the algorithm and this metal learning is really

23:44.040 --> 23:45.040
exciting.

23:45.040 --> 23:49.000
There's some great work that was done at Twitter for you go right now, show Google

23:49.000 --> 23:53.960
over to Google Brain on learning an algorithm that's just good at few shot or one shot

23:53.960 --> 23:54.960
learning.

23:54.960 --> 23:56.400
Also an instance of metal learning.

23:56.400 --> 23:59.560
So there's, it's an exciting area.

23:59.560 --> 24:07.320
I think after Steven's article came out, I spent a long time trying to, through my interviews,

24:07.320 --> 24:14.080
trying to understand the process of architecting deep neural networks.

24:14.080 --> 24:18.760
And I guess it took me longer to, you know, retrospectively, it took me longer than

24:18.760 --> 24:22.960
it should have to figure out, you know, the gradient descent by graduate student there

24:22.960 --> 24:28.040
with a couple of different versions of this, and graduate student descent.

24:28.040 --> 24:33.560
And you know, to, at least, you know, it seemed like a year ago or so, like that was the

24:33.560 --> 24:34.560
state of the art.

24:34.560 --> 24:42.360
But since then, you know, you described a number of methods for kind of automating architecture.

24:42.360 --> 24:46.040
One of the ones that you mentioned was a Bayesian based approach and there were some

24:46.040 --> 24:47.040
others.

24:47.040 --> 24:50.240
Can you go into a little bit more detail on the various ones that you mentioned?

24:50.240 --> 24:54.720
Sure, so we explored a couple of different approaches in, in my lab for the multimodal

24:54.720 --> 24:56.360
learning problem.

24:56.360 --> 25:01.000
One approach is Bayesian optimization, which a lot of people in the deep learning field

25:01.000 --> 25:06.760
are familiar with from the point of view is doing model search or hyper parameter optimization.

25:06.760 --> 25:10.120
So these are the decisions that we all need to make about how many layers and how many

25:10.120 --> 25:14.760
units per layer and what kind of activation function and then on the, the learning algorithm

25:14.760 --> 25:17.080
side, how long do we train for?

25:17.080 --> 25:22.440
Should we use atom optimizer or should we use RMS prop or what should our regularization

25:22.440 --> 25:23.440
coefficients be?

25:23.440 --> 25:26.240
There's all these decisions and, and with deep learning, there's more of these decisions

25:26.240 --> 25:28.920
in classical machine learning models.

25:28.920 --> 25:34.320
So people have proposed Bayesian optimization as a, a suitable tool and it, it's actually

25:34.320 --> 25:37.960
been very successful in automating some of the hyper parameter search.

25:37.960 --> 25:43.600
So we, in, in our first example, we just viewed architecture as another hyper parameter

25:43.600 --> 25:48.680
and we proposed essentially a, a search space of potential architectures in which this

25:48.680 --> 25:51.000
modality fusion could happen.

25:51.000 --> 25:55.840
And then we had a, Bayesian optimization algorithm with a, the main technical achievement

25:55.840 --> 26:02.320
was a, a, a, a, a, a, a, a way of assessing similarity between different architectures.

26:02.320 --> 26:06.160
And that was a building block for the Bayesian optimizer to basically search over that

26:06.160 --> 26:08.560
space of potential fusion architectures.

26:08.560 --> 26:09.560
Okay.

26:09.560 --> 26:13.520
And it came up with one that would beat the graduate student descent method.

26:13.520 --> 26:18.400
It in about 30 or so proposals, report different architectures.

26:18.400 --> 26:19.400
Okay.

26:19.400 --> 26:22.920
The downside to that system is when I'm saying 30 different architectures, each of those

26:22.920 --> 26:25.280
had to be trained and evaluated.

26:25.280 --> 26:29.560
And then that result given to the Bayesian optimizer such that it could propose the next

26:29.560 --> 26:30.560
one.

26:30.560 --> 26:34.840
So it's this iterative method in which you're training full architectures to convergence,

26:34.840 --> 26:39.120
you're evaluating them, you're choosing another one going back and evaluating.

26:39.120 --> 26:40.560
And so it gets quite slow.

26:40.560 --> 26:47.320
We explored a second approach in which we do, we view architecture searches to CASTIC

26:47.320 --> 26:48.320
regularization.

26:48.320 --> 26:52.640
It's kind of a meaty thing to say, but it's what you see in methods like dropout where

26:52.640 --> 26:55.560
people just knock out activities randomly.

26:55.560 --> 26:59.200
In neural networks, there's also drop connect where people knock out weights.

26:59.200 --> 27:02.160
And this is done on an example by example basis.

27:02.160 --> 27:06.880
So every time you present a new example to the model, you knock out a different subset

27:06.880 --> 27:10.800
of hidden activities where you knock out a different subset of weights.

27:10.800 --> 27:15.280
So they call this CASTIC regularization and it's been shown to make networks generalize

27:15.280 --> 27:16.280
better.

27:16.280 --> 27:19.440
And it was very popular until some things like batch norm came along and people started

27:19.440 --> 27:20.440
working with that.

27:20.440 --> 27:23.240
But still, it's a general principle for us.

27:23.240 --> 27:29.680
We did this kind of block wise, knocking out certain weights inspired by an approach by

27:29.680 --> 27:32.520
a graduate student at CMU called block out.

27:32.520 --> 27:35.840
And what this student found with this block out is that if you knock out certain blocks

27:35.840 --> 27:40.800
of weights, this can give you very different architectural patterns made through this

27:40.800 --> 27:41.800
weight structure.

27:41.800 --> 27:46.800
So you can have sort of mergers of groups of hidden units or splits or you can just completely

27:46.800 --> 27:51.000
ignore certain features that are being discovered in the network.

27:51.000 --> 27:55.800
And we basically propose a modality aware version of this.

27:55.800 --> 28:02.080
So as this training, explore many different multimodal fusion architectures and then eventually

28:02.080 --> 28:04.600
converge to one that worked pretty well.

28:04.600 --> 28:07.600
So that ended up being more efficient than the basic optimization approach.

28:07.600 --> 28:08.600
Okay.

28:08.600 --> 28:09.600
Okay.

28:09.600 --> 28:10.600
That's pretty technical.

28:10.600 --> 28:11.600
No, that's great.

28:11.600 --> 28:12.600
That's great.

28:12.600 --> 28:13.600
That's great.

28:13.600 --> 28:15.600
And then there was another challenge.

28:15.600 --> 28:16.600
Yes.

28:16.600 --> 28:20.600
So I talked about the idea, which is both an opportunity and a challenge of explainability

28:20.600 --> 28:21.600
in AI.

28:21.600 --> 28:22.600
Right?

28:22.600 --> 28:25.600
And I don't know if you've talked much about explainability yet.

28:25.600 --> 28:27.000
It's a little bit about it.

28:27.000 --> 28:32.000
I don't think you mentioned it in your talk, but I did an interview with Carlos Guestrin

28:32.000 --> 28:37.040
who has a paper called Lime, which seeks to do explainability.

28:37.040 --> 28:43.200
I appreciated the, you were quoting someone else, I believe, and you took issue with, you

28:43.200 --> 28:47.840
know, we often talk about neural networks as black boxes.

28:47.840 --> 28:52.000
And I think you, you, well, you can, yeah, sure, I can talk about that.

28:52.000 --> 28:58.400
It was actually a quote by Cuyengan Cho, a researcher at NYU, and he came to Toronto last

28:58.400 --> 29:02.360
summer very graciously to be part of this next AI program for startups.

29:02.360 --> 29:07.200
So part of that program, we bring in world-class individuals like Cho, and they talk about

29:07.200 --> 29:09.280
various things he was doing in course on NLP.

29:09.280 --> 29:13.720
But he criticized people calling neural networks black boxes and said that they're actually

29:13.720 --> 29:15.200
white boxes.

29:15.200 --> 29:20.640
And that was kind of neat because this, or after my talk, Nikola Paprano talked about

29:20.640 --> 29:23.520
black box versus white box attacks.

29:23.520 --> 29:24.920
And it's the same concept here.

29:24.920 --> 29:29.360
And in some sense, you can open them up, you can look at their parameters, you just happen

29:29.360 --> 29:35.400
to have hundreds of millions of parameters most of the time, and they're just uninterpretable,

29:35.400 --> 29:36.400
right?

29:36.400 --> 29:42.760
So they're not, I mean, if you're accessing them through an online service, like in Paprano's

29:42.760 --> 29:46.680
work, they were trying to attack a method that had been deployed, I think one was on

29:46.680 --> 29:49.960
Metamine services, one was on Amazon, one was on Google.

29:49.960 --> 29:53.520
And if you're interacting through the predictions, yes, it's black box.

29:53.520 --> 29:56.880
But if you're the person evaluating the machine learning system, or maybe you're

29:56.880 --> 29:59.640
your model, it's black box, right?

29:59.640 --> 30:02.080
And generally, still uninterpretable.

30:02.080 --> 30:03.400
It's still uninterpretable.

30:03.400 --> 30:08.720
So that's why we're keen to move to more interpretable systems or explainable systems

30:08.720 --> 30:10.200
in certain setups.

30:10.200 --> 30:15.880
So we've looked at it sort of in the medical space, we've looked at it in the financial

30:15.880 --> 30:20.000
prediction space forecasting, and then we've looked at sort of the classical vision problems

30:20.000 --> 30:24.680
on the benchmark data sets that everybody else benchmarks on.

30:24.680 --> 30:29.120
And yeah, I guess it's like when you teach about software and you're talking about requirements

30:29.120 --> 30:33.680
gathering, and how much money you're going to spend on each stage of the software development

30:33.680 --> 30:39.080
lifecycle, the same thing, assess the risks, like some problems require more careful consideration

30:39.080 --> 30:40.480
of risks, others don't.

30:40.480 --> 30:46.360
And I'd say that the same thing about interpretability and explainability, I mean, let's see what

30:46.360 --> 30:50.920
the application is, who are the users, what are the risks involved.

30:50.920 --> 30:55.880
And in many cases, we likely want to make the system more explainable.

30:55.880 --> 30:57.680
Okay, then opportunities.

30:57.680 --> 31:00.840
Yeah, we've arrived to opportunities.

31:00.840 --> 31:05.120
Yeah, actually, we've gone, sorry, we've gone through a lot of opportunities and I'll

31:05.120 --> 31:06.960
move more into sort of barriers.

31:06.960 --> 31:09.200
Let's say that the last part was some of the barriers.

31:09.200 --> 31:15.600
And when I talked about barriers, it was, I'll just go quickly through them, data.

31:15.600 --> 31:17.800
The next one was talent.

31:17.800 --> 31:21.120
And then the third one I talked about was building trust.

31:21.120 --> 31:24.800
So we can maybe go in and dissect each of these.

31:24.800 --> 31:30.600
And first of all, for data, I think in deep learning, we've seen a tremendous number

31:30.600 --> 31:34.960
of really cool examples of deep learning working in practice, but I would argue that it's

31:34.960 --> 31:37.680
been done in fairly limited set of domains.

31:37.680 --> 31:44.720
The ones I mentioned were the big three vision, speech and audio processing, and then natural

31:44.720 --> 31:45.720
language.

31:45.720 --> 31:46.720
Right.

31:46.720 --> 31:53.440
And these are generally unstructured domains where there's a lot of data, label data in particular.

31:53.440 --> 31:58.840
These are the sorts of applications being pursued by the commercial internet giant threat.

31:58.840 --> 32:03.400
And actually, it's something I've had a struggle with in my lab just motivating some students

32:03.400 --> 32:08.560
to tackle other kinds of problems where benchmark data sets are not available.

32:08.560 --> 32:13.160
So I actually mentioned today, for example, some agricultural applications that we've

32:13.160 --> 32:18.320
worked on, but again, you're rewarded more as a researcher to conduct your experiments

32:18.320 --> 32:23.600
reasonably quickly, get your papers out, and compare to other people in the literature.

32:23.600 --> 32:27.640
And you know, so you download ImageNet, you propose a new architecture, you publish paper

32:27.640 --> 32:28.640
on it.

32:28.640 --> 32:33.360
At the end of the day, if you want to solve a problem that actually, a really important

32:33.360 --> 32:40.280
problem like growing food in an environmentally friendly way and sustainable way, and that

32:40.280 --> 32:45.360
gives decent yields for the farmers, and you want to explore, say, deep learning for

32:45.360 --> 32:51.160
remote sensing in agricultural fields, this involves a crazy amount of data collection.

32:51.160 --> 32:55.680
It takes a lot of work to get in the field, do those flights, say UAV flights, do the

32:55.680 --> 33:00.160
ground truthing, which involves actually collecting samples, say you're looking at soil properties

33:00.160 --> 33:02.960
or nitrogen properties of plants.

33:02.960 --> 33:06.760
So this might take a summer, it might take multiple growing seasons, and you actually

33:06.760 --> 33:11.640
don't see the effects of any interactions until the end of the growing season when you

33:11.640 --> 33:13.720
actually can measure neat yields.

33:13.720 --> 33:20.600
So this is not the same time frame of a lot of the experimentation that happens in machine

33:20.600 --> 33:21.600
learning.

33:21.600 --> 33:26.240
So again, going back to the AlphaGo example, the system is able to play two and a half

33:26.240 --> 33:31.720
million games against itself because it can carry out a game and get the reward in less

33:31.720 --> 33:37.600
than a second, in an agricultural situation you can't get a reward in less than a second,

33:37.600 --> 33:40.720
it's six months, right, or longer.

33:40.720 --> 33:45.120
So anyways, this is a bit of a ramble just saying that we're fairly limited in the ways

33:45.120 --> 33:51.040
that we're applying deep learning right now, and it's a lot about the data, how do you

33:51.040 --> 33:56.680
collect the data, where do you get the data, how hard is it to gather that process it?

33:56.680 --> 34:00.480
And so anyways, there's a quote by Christix and then I gave it at the end, which was

34:00.480 --> 34:04.800
data is really the key ingredient to AI because it's the missing ingredient.

34:04.800 --> 34:09.560
So we publish our algorithms, like there's great algorithms out there, they're available

34:09.560 --> 34:11.080
to people.

34:11.080 --> 34:17.400
Compute power has really grown and it's become cheaper, so we have access to great compute.

34:17.400 --> 34:21.720
So it's really the data that we don't have, that's the missing ingredient for these sorts

34:21.720 --> 34:23.240
of problems I'm talking about.

34:23.240 --> 34:27.760
And it's also for companies, it's the proprietary ingredient.

34:27.760 --> 34:33.360
So people aren't publishing data sets as quickly as they're publishing papers on algorithms.

34:33.360 --> 34:36.600
What's one of the challenges that I see is data.

34:36.600 --> 34:42.160
Well, if you can maybe quickly summarize the other two and leave us with any final thoughts

34:42.160 --> 34:43.160
as we wrap up.

34:43.160 --> 34:44.160
Sure.

34:44.160 --> 34:50.320
So in terms of the other two, one is talent, and I think this is the idea of companies

34:50.320 --> 34:54.400
face with, well, how are we going to fill these positions where we need really skilled

34:54.400 --> 34:59.760
people in machine learning, and whether, you know, questions around, oh, do we need PhDs,

34:59.760 --> 35:01.640
our masters graduate, it's good enough.

35:01.640 --> 35:06.400
Can we take somebody training in a different area and move them into machine learning field?

35:06.400 --> 35:10.600
I think there's a lot of amazing stuff going on here, particularly in, not just Canada,

35:10.600 --> 35:11.600
but Toronto.

35:11.600 --> 35:15.880
There's an announcement last week by the provincial government to fund Vector Institute

35:15.880 --> 35:22.680
with $30 million to work with the Ontario universities to develop professional graduate

35:22.680 --> 35:25.560
programs and AI and machine learning.

35:25.560 --> 35:29.800
And in five years, we're going to be looking at graduating 1,000 students per year.

35:29.800 --> 35:33.760
That will be the goal for steady state in this province.

35:33.760 --> 35:36.880
I think we're addressing that issue with talent right now.

35:36.880 --> 35:42.360
But as I mentioned today, we also have a lot of professors leaving academics, going to

35:42.360 --> 35:48.400
work at industry part time or full time, and we need to work on retaining those individuals.

35:48.400 --> 35:53.800
I think decisions like starting the government supporting AI initiatives, like Vector and

35:53.800 --> 36:00.960
Meela and Amy and Alberta, those are all working toward making this, making academics attractive

36:00.960 --> 36:06.760
in this field versus industry, but I think we need to do more to encourage the people

36:06.760 --> 36:12.440
staying in academics or moving into academics to continue to train the next generation.

36:12.440 --> 36:15.240
And then the final topic was on trust, right, building trust.

36:15.240 --> 36:18.760
And actually, we've already touched on a couple of those issues.

36:18.760 --> 36:21.320
Explain to you as one, bias and fairness.

36:21.320 --> 36:27.600
I tend to like the idea of using technology as actually as Nikola Papernos said in his

36:27.600 --> 36:33.360
talk following mine, these ideas are on differential, privacy preserving algorithms to increase

36:33.360 --> 36:38.880
people's trust in machine learning systems, rolling out technology like fair representations

36:38.880 --> 36:43.160
for removing bias from algorithms that make predictions.

36:43.160 --> 36:47.840
And so I think I feel pretty good about the future of AI, I guess I'll summarize it

36:47.840 --> 36:48.840
that.

36:48.840 --> 36:50.640
It's a great field to be working in.

36:50.640 --> 36:54.320
This Toronto area is a great place to be working on these technologies.

36:54.320 --> 36:56.360
There's a lot more to come.

36:56.360 --> 37:01.440
And I think in terms of the problems, I do see a diversification in the future of the types

37:01.440 --> 37:03.760
of tasks we're solving.

37:03.760 --> 37:07.600
And I think they're at least working with the startups in the next AI program.

37:07.600 --> 37:12.080
I also see a lot of interest, both from the companies building these technologies, but

37:12.080 --> 37:17.560
also from the investment side, and companies that are doing social good as well.

37:17.560 --> 37:22.720
So building both profitable companies, but also solving real important issues.

37:22.720 --> 37:23.720
Right.

37:23.720 --> 37:24.920
And so that's why I look forward to.

37:24.920 --> 37:25.920
Awesome.

37:25.920 --> 37:30.240
Well, Graham, thanks so much for taking the time to sit with me and share all that you

37:30.240 --> 37:33.720
shared about your kind of vision for this and how you see it.

37:33.720 --> 37:34.720
Thanks a lot to my pleasure.

37:34.720 --> 37:35.720
Great to meet you.

37:35.720 --> 37:36.720
Great.

37:36.720 --> 37:37.720
Thank you.

37:37.720 --> 37:43.120
All right, everyone, that's our show for today.

37:43.120 --> 37:47.840
Thanks so much for listening and for your continued feedback and support.

37:47.840 --> 37:52.720
For more information on Graham or any of the topics covered in this episode, head on over

37:52.720 --> 37:56.920
to twimlai.com slash talk slash 62.

37:56.920 --> 38:04.800
To follow along with the Georgian partner series, visit twimlai.com slash GPPC 2017.

38:04.800 --> 38:10.600
Of course, you can send along feedback or questions via Twitter, at twimlai, or at Sam

38:10.600 --> 38:14.600
Charrington, or leave a comment on the show notes page.

38:14.600 --> 38:18.080
Thanks once again to Georgian partners for their sponsorship of the show.

38:18.080 --> 38:23.160
Be sure to check out their white papers, which you can find by visiting twimlai.com slash

38:23.160 --> 38:24.160
Georgian.

38:24.160 --> 38:35.640
Thanks again for listening and catch you next time.

