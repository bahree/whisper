1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:39,480
I'm your host, Sam Charrington, in this episode I'm joined by Missy Cummings, head of Duke University's humans and autonomy lab and professor in the department of mechanical engineering.

4
00:00:39,480 --> 00:00:50,680
In addition to being an accomplished researcher, Missy also became one of the first female fighter pilots in the US Navy following the repeal of the combat exclusion policy in 1993.

5
00:00:50,680 --> 00:01:00,280
We discussed Missy's research into the infrastructural and operational challenges presented by autonomous vehicles, including cars, drones and other unmanned aircraft.

6
00:01:00,280 --> 00:01:06,680
We also covered trust, explainability and interactions between humans and autonomous vehicle systems.

7
00:01:06,680 --> 00:01:13,480
This was an awesome interview and I'm glad to be able to bring it to you and now on to the show.

8
00:01:13,480 --> 00:01:29,080
Alright everyone, I am on the line with Missy Cummings, Missy is a professor in the department of mechanical engineering and material science at Duke University and I'm excited to have Missy online to talk to us about her work in autonomous vehicles.

9
00:01:29,080 --> 00:01:31,880
Missy, welcome to this week in machine learning and AI.

10
00:01:31,880 --> 00:01:33,080
Thanks for having me.

11
00:01:33,080 --> 00:01:41,480
Absolutely. As is the tradition here, why don't we get started by having you introduce yourself to the audience and tell us a little bit about

12
00:01:41,480 --> 00:01:49,480
how you got interested in autonomous vehicles and kind of your path to where you are?

13
00:01:49,480 --> 00:02:07,080
So I think the path to how I got to be not just a professor of mechanical engineering and material science, but I'm also the head of Duke Robotics and the Duke Humans and Autonomy Laboratory, which is how I'm tied into artificial intelligence and machine learning in the first place.

14
00:02:07,080 --> 00:02:15,080
But I think what most people find unusual is that I was one of the United States Navy's first female fighter pilots.

15
00:02:15,080 --> 00:02:16,080
Wow.

16
00:02:16,080 --> 00:02:29,080
So yeah, that kind of dates me too, which means that I was flying fighters in the 90s for the US Navy and it was a very tumultuous time.

17
00:02:29,080 --> 00:02:37,080
You know, it was very much an era of social change, but also combined with technology change.

18
00:02:37,080 --> 00:02:48,080
And so when I was flying fighters, I lost about one friend, Pierre, a month for the three years that I flew the planes.

19
00:02:48,080 --> 00:03:03,080
And it was always because of human air and it was human air driven by a lack of human appreciation for what the computers were doing flying fly by wire aircraft, which were still relatively new back in the 90s.

20
00:03:03,080 --> 00:03:20,080
Seeing so many friends die because of effectively what was extremely poor human computer interaction, eventually when I decided to leave the Navy, I decided to go back to graduate school and I decided to focus my research in this area of human machine interaction.

21
00:03:20,080 --> 00:03:35,080
And so it's been around for a little while, but I decided to kind of jump in and ramp this game up. And so MIT found out that I was on the job market, and this is around 2004.

22
00:03:35,080 --> 00:03:50,080
And so then I was at MIT for a decade, running a lab there in aeronautics department. I had a really great time at MIT and basically turned into a diva.

23
00:03:50,080 --> 00:04:08,080
So at Duke University, Duke University treats me like the diva that I think I am. And so then I moved my lab to Duke. And that's a recent move. And we do focus on human interaction with autonomous systems.

24
00:04:08,080 --> 00:04:23,080
And I do a lot of work in drones, but also driverless cars really unmanned vehicle systems of all types where a human is trying to affect change, typically through some kind of remote control station.

25
00:04:23,080 --> 00:04:35,080
But occasionally you'll find me in power plants and in other areas, medical devices, where autonomous technologies are starting to become on the rise.

26
00:04:35,080 --> 00:04:51,080
One of the areas we can explore to get started with is maybe you can kind of compare and contrast the challenges associated with autonomous vehicles that are flying and those that are driving.

27
00:04:51,080 --> 00:05:09,080
You know, we've are focused to date in this series has been on driverless cars. There's, you know, clearly a lot of energy and excitement around that. And I think it's something that we can all relate to as users of cars.

28
00:05:09,080 --> 00:05:24,080
But I imagine the challenges, you know, there may be some unique challenges associated with with flying vehicles as well.

29
00:05:24,080 --> 00:05:39,080
So it was definitely one of the first researchers into drones. I've also become one of the first researchers researchers to generalize to flying cars. So I actually, yeah, we're doing research for NASA right now on flying cars.

30
00:05:39,080 --> 00:06:01,080
I would say it's surprising to most people to find out that actually it's easier to operate drones than it is driverless cars. And what really drives the fact that drones are truly further along in their technological development life cycle is the fact that we have been flying aircraft by them.

31
00:06:01,080 --> 00:06:15,080
And they've been flying themselves being able to land and take off for over 30 years. And so these are actually really mature technologies and aviation drones have been around in various shapes and forms for a long time.

32
00:06:15,080 --> 00:06:19,080
So we have a lot of operational experience.

33
00:06:19,080 --> 00:06:36,080
And the other big discriminator is the obstacle density field. So even though drones have a third dimension, they generally do not have to contend with the number of obstacles and certainly not in the proximity that driverless cars.

34
00:06:36,080 --> 00:06:49,080
I would say driverless cars, I'm using that necessarily right now as a catch all for driverless and self driving, which are not exactly the same thing, but just for the purposes of this discussion, I'll lump them all together.

35
00:06:49,080 --> 00:07:06,080
And I imagine the variety of foreign objects that a drone is likely to encounter is much more limited than a vehicle.

36
00:07:06,080 --> 00:07:19,080
And with 20 other small Subaru's around it, with a couple of drones around it, and with some people that are walking in and around the airspace, right?

37
00:07:19,080 --> 00:07:31,080
I mean you just don't have mixed, you know, this is a problem of mixed equipage. So there's just not that number of heterogeneous vehicles in the sky that planes have to contend with.

38
00:07:31,080 --> 00:07:41,080
And so the radars don't have to be quite as good. LiDars don't have to be quite as good. In fact, you only see LiDars on aircraft when they get close to the ground like in helicopters.

39
00:07:41,080 --> 00:07:50,080
And so there's just a sensor suite that doesn't really even need to apply to drones as like it does in driverless cars.

40
00:07:50,080 --> 00:08:04,080
So if self flying and self landing planes have been around for 30 years, and you know, that's a largely solved problem or at least a mature problem.

41
00:08:04,080 --> 00:08:10,080
What is the research frontier for autonomous drones?

42
00:08:10,080 --> 00:08:20,080
I think it's a great question and it really all comes down to infrastructure. So how are we going to design the air traffic control for the future?

43
00:08:20,080 --> 00:08:33,080
One project that we're actively working on right now is how to design new dispatch centers because air traffic controllers and dispatchers in today's world are going to be able to effectively command these vehicles directly.

44
00:08:33,080 --> 00:08:43,080
This actually switches what would have been the workload to the pilot one or two pilots inside the cockpit over to the person on the ground who is trying to direct traffic.

45
00:08:43,080 --> 00:08:52,080
And so that has a lot of implications. And certainly for flying cars, there's a whole new set of issues that come in.

46
00:08:52,080 --> 00:09:11,080
And how do you design a cockpit, for example, for a driverless car so that a person can't do anything to override the system and either intentionally or potentially force the cybersecurity implications for both drones but also flying cars are significant.

47
00:09:11,080 --> 00:09:15,080
And so there are lots of big research questions in those areas.

48
00:09:15,080 --> 00:09:20,080
You mentioned some research that you're doing with NASA. Can you elaborate on that?

49
00:09:20,080 --> 00:09:30,080
Right. And if you go to my website, you can read, we have a paper that was put up a few months ago. We have another one coming out any day now that really those issues surrounding it.

50
00:09:30,080 --> 00:09:48,080
So just if you want to know just how close we are to having your own flying car, you can read these papers and it will lay out the complex systems engineering issues that span all the way from what are the problems with the technologies on the aircraft, which include things like electric power.

51
00:09:48,080 --> 00:09:53,080
So that's kind of all the rage right now both in cars and flying cars and even drones.

52
00:09:53,080 --> 00:10:01,080
But unfortunately electric vehicles are really untested in the traditional aerospace settings.

53
00:10:01,080 --> 00:10:07,080
And so particularly for passenger carrying, which demands a much higher degree of safety.

54
00:10:07,080 --> 00:10:14,080
So it's okay to have your battery powered drone, but you know, really do you want to send your kid to school and your battery powered drone.

55
00:10:14,080 --> 00:10:23,080
Right. Right. Well, the small consumer drones, the granted the battery smaller, but the weight of the drone is also much smaller.

56
00:10:23,080 --> 00:10:31,080
And you're we're at getting maybe 15, 20 minutes, a half an hour at the, you know, if you're spending a ton of money on those.

57
00:10:31,080 --> 00:10:43,080
I can't imagine the, you know, a drone that's meant to carry people what the weight is there and how much battery that's going to have to require to move that.

58
00:10:43,080 --> 00:10:46,080
And then what the fly time of something like that might be.

59
00:10:46,080 --> 00:10:48,080
Do you have any examples of that?

60
00:10:48,080 --> 00:10:57,080
Yeah, it's doable. So there, there are couple people who are out there, you know, some companies who are aurora flight sciences, who is recently acquired by Boeing.

61
00:10:57,080 --> 00:11:07,080
They've got, they've had an interesting demos that could potentially carry a single person for, you know, I would say about 20 to 30 minutes flight.

62
00:11:07,080 --> 00:11:13,080
And certainly there are people who are looking at these short flights right now.

63
00:11:13,080 --> 00:11:18,080
But the reality is that these systems are so new and they're so untested.

64
00:11:18,080 --> 00:11:24,080
And then part of the other systems engineering issues that have to be contended with are the regulatory issues.

65
00:11:24,080 --> 00:11:38,080
And, you know, how, so the FAA is a very strong regulatory body with a very, very detailed set of guidelines that you must meet to put new technology in the air.

66
00:11:38,080 --> 00:11:47,080
And if that's the case, and this stuff is untested, then, you know, our testing, it's not that we don't have, we know how to do it, we know how to do flying cars.

67
00:11:47,080 --> 00:12:00,080
And having the testing, the technology meet the test requirements that it would take to certify them as safe and fly particularly to put a person with the Asian background in the cockpit.

68
00:12:00,080 --> 00:12:05,080
We're still far away from making that kind of social revolution.

69
00:12:05,080 --> 00:12:16,080
Yeah, I mean, we've been seeing, there have been YouTube videos going around of, you know, various attempts of self driving cars for many years now.

70
00:12:16,080 --> 00:12:24,080
Hey, probably as long as YouTube has been around, there's been, you know, folks that, you know, claim to be, you know, just right there.

71
00:12:24,080 --> 00:12:27,080
But, you know, there's still a long way to go.

72
00:12:27,080 --> 00:12:36,080
Well, you know, and I think that's a good point, especially when you take into consideration that we still don't have the drone situation worked out.

73
00:12:36,080 --> 00:12:47,080
So commercial drones are only allowed if they're under 55 pounds. True revolution will happen when we can get the FedEx, UPS size aircraft to be drones.

74
00:12:47,080 --> 00:13:03,080
And we know how to do that. We actually know how to do that pretty well, but it's a regulatory framework that's an issue, not a technology one for driverless cars that the game changes because they are still very much basic technology issues that have yet to be solved.

75
00:13:03,080 --> 00:13:21,080
And technologies are not mature. And so that's why driverless cars tend to be a much more difficult problem and their timeline is not nearly as close as many companies would like to believe they are because we simply don't have some of the basics worked out.

76
00:13:21,080 --> 00:13:33,080
So you mentioned that you mentioned the, you know, some research challenges associated with building out this infrastructure.

77
00:13:33,080 --> 00:13:51,080
Maybe share what's what some of the, you know, some of your work in this area is imagining, you know, it's touching on things like cooperative systems and kind of these networks of, you know, kind of a network control plan and all these distributed things.

78
00:13:51,080 --> 00:14:00,080
How to, you know, effectively control these distributed things flying around those the kinds of areas that you're involved in or you tackling a different part of the problem.

79
00:14:00,080 --> 00:14:25,080
Yeah, that's true. So we work a lot. So we have a major research thrust in looking at dispatch centers of the future. And that is true for drones, flying cars, driverless cars. In fact, dispatch centers of the future are a wide open area that I think you're going to see really jump up in terms of applicability for driverless cars of the future because if we're going to operate to vehicles and we will eventually get there.

80
00:14:25,080 --> 00:14:33,080
The idea is what's going to happen if you have a fleet of Google cars with no steering wheel and something happens and one or more cars go down.

81
00:14:33,080 --> 00:14:40,080
How are you going to fix that remotely? You're not obviously the person can't push the car over to the side of the road in most cases.

82
00:14:40,080 --> 00:14:50,080
So you're going to have to have somebody remotely intervened. So we have a lot of work going into trying to figure out how do you know what that set of function allocations is going to look like.

83
00:14:50,080 --> 00:15:12,080
How many dispatchers are you going to need for how many numbers of cars? What kind of contingency operations? Is this going to look like a call center, like an on star center? Is it going to look like a dispatch center where, you know, with a set of remote controls where, you know, people can pick up some kind of Xbox controller and drive are out of a stuck situation, for example.

84
00:15:12,080 --> 00:15:24,080
So there's a lot of infrastructure in terms of remote vehicle management, but I would say the other big area that's related to this that we're very passionate about is explainable AI.

85
00:15:24,080 --> 00:15:33,080
And this is the idea of course that you're going to have all of these systems, both drones and driverless cars have a lot of embedded AI in them.

86
00:15:33,080 --> 00:15:48,080
And they're both going to have to get through various hoops in the certification and regulatory cycle and to do that regulators are going to have to have some sense of what the competency boundaries are for the underlying algorithms.

87
00:15:48,080 --> 00:16:06,080
But, you know, and I'm sure most of your audience is going to appreciate this more than anyone, you know, artificial intelligence, machine learning, deep learning, these are very opaque technologies, particularly as the complexity grows. And my students do not understand what is happening in the algorithms that they're designing.

88
00:16:06,080 --> 00:16:27,080
And so trying to get them to communicate what the limits of these algorithms are to people who are not experts in the field is even more challenging. So we're spending a lot of time looking at how can you design the algorithms themselves to be more explainable to people who range and expertise levels from none to some.

89
00:16:27,080 --> 00:16:46,080
And then how could you communicate that information in such a way, you know, is it a visualization component, is it a what if sensitivity analysis component, how can you communicate what that algorithm can and can't do so that you start to engender some kind of trust and confidence in these algorithms.

90
00:16:46,080 --> 00:17:11,080
Alright, that's a really interesting way to think about explainability when I talk about explainability with folks were typically looking at a kind of an after the fact assessment of a decision or prediction and how the network came to that or, you know, doesn't necessarily need to be a neural network in particular.

91
00:17:11,080 --> 00:17:40,080
But, you know, how the machine learning or AI system came to its conclusion, but you're also using it here to encompass our ability to kind of create some kind of bounds around the, you know, the performance or accuracy or the behaviors that these systems drive for us, you know, which are a lot more, you know, doing so is a lot more complex than, you know, the traditional non probabilistic approaches where we can.

92
00:17:40,080 --> 00:17:49,080
Kind of say, well, you know, it's, you know, this is going to be the range of the input, so this is going to be the range of the output and we're very confident about that.

93
00:17:49,080 --> 00:18:03,080
Right, and that's actually how most complex transportation systems are certified today, basically using deterministic tests, which we know quite well that every single time we have a set of inputs, the outputs match every single time we run that test.

94
00:18:03,080 --> 00:18:10,080
And probabilistic reasoning systems like you'll get an AI embedded systems, it just simply is not the case.

95
00:18:10,080 --> 00:18:19,080
And this is a whole new way of thinking and one of the things that I've been very critical about of the US government is it's it's lack of expertise in this area.

96
00:18:19,080 --> 00:18:24,080
And I think that this is true across the Department of Defense, even the Department of Energy.

97
00:18:24,080 --> 00:18:36,080
And this is a largely a result of the sucking sound that's coming out of Silicon Valley and Detroit, taking up every possible engineer and putting a marvelous car problem.

98
00:18:36,080 --> 00:18:48,080
But, you know, our government agencies simply do not have the expertise needed to understand how these autonomous systems are working, much less how to certify or regulate them.

99
00:18:48,080 --> 00:19:03,080
And so this expertise problem I think is going to grow. And so that's one issue, which of course needs to be addressed not just by universities, but I think companies need to be more invested in how they support education programs.

100
00:19:03,080 --> 00:19:22,080
But that's why explainable AI becomes even more important because we need to recognize that these algorithms are difficult to understand, but and maybe to develop, but that doesn't mean that we can't give people inside into how answers came about.

101
00:19:22,080 --> 00:19:33,080
And I think one of the important focus that we are having this lab is we're not as interested in explaining a necessarily aggregate pattern that comes out of the data.

102
00:19:33,080 --> 00:19:48,080
I mean, that's interesting and that might help you understand generally how the algorithm works, but in safety critical systems like transportation systems, what we really want to know is why did an algorithm choose to ignore some cases, right?

103
00:19:48,080 --> 00:20:02,080
And are those cases the edge cases? So looking at the data that gets rejected by these algorithms in many ways is more important than looking at the aggregate data that comes out.

104
00:20:02,080 --> 00:20:21,080
It strikes me that one way to think about this world that you're describing here is if you think about current processes around commercial aviation, when an accident happens and you know the systems are fairly mature, so these accidents happen very infrequently.

105
00:20:21,080 --> 00:20:35,080
But there's a very rigid, regimented, established set of processes that take place that start with finding the black box, finding this thing that's going to tell us what went wrong.

106
00:20:35,080 --> 00:20:59,080
And in this new world, even if we were recording all of the things that are happening and all of the inputs and outputs, that doesn't tell us exactly what went wrong because there's this probabilistic reasoning that's happening in the middle that as you said may have chosen to ignore certain inputs or prioritize other inputs.

107
00:20:59,080 --> 00:21:14,080
And in order for this current regulatory framework to make sense of this new world, we've got to develop new techniques that give them more to work with, I guess, to put it simply.

108
00:21:14,080 --> 00:21:33,080
Yes, you know, give people more to work with, but more, how do we understand what really amounts to huge data sets with, you know, edge, are they edge cases? Are they noise? What can we learn from that? What should we learn from it?

109
00:21:33,080 --> 00:21:50,080
So, you know, it's kind of a mix of trying to understand the limits of what an algorithm is doing. How does it reason about or not about edge cases, but it also has a huge human understanding component to it too.

110
00:21:50,080 --> 00:22:03,080
When we're talking about looking at thousands, tens of thousands, maybe hundreds of thousands of data points, how's a human going to understand something meaningful about that? It's not like they're not going to do this one factor at a time.

111
00:22:03,080 --> 00:22:27,080
So, you know, it's a combination of high dimensional data visualization and understanding in concert with looking at the assumptions underlying many artificial intelligence algorithms and trying to get people to understand how to question that, particularly when they are trying to determine what the edges of an algorithms envelope would be.

112
00:22:27,080 --> 00:22:50,080
That's maybe been a year or more since there was the big autonomous drone delivery frenzy Amazon, you know, showed their kind of concept videos and then soon after that Domino's pizza showed theirs. Do you have a kind of a crystal ball guess as to when we get there and this stuff becomes a reality.

113
00:22:50,080 --> 00:23:12,080
Well, you know, that's interesting because it depends on where we sit in the regulatory cycle. So, you know, about a year ago, I would have said, well, you know, it's still going to be some time off because the regulatory framework is just still so prohibitive that's unlikely that we're going to make a lot of progress anytime soon.

114
00:23:12,080 --> 00:23:26,080
But you wake up today and the Trump administration has basically decided to wipe out a set of regulations that would have prevented that so say what you will about your political affiliation.

115
00:23:26,080 --> 00:23:44,080
But, you know, there's a new sheriff in town and an announcement that we are now going to be allowed to test drones at night and in beyond line of sight with a relaxed regulatory infrastructure. Now whether or not that actually happens remains to be seen.

116
00:23:44,080 --> 00:23:59,080
But if he makes good on those promises of significantly reducing the regulatory barriers, then what would have taken a few years to do could actually be here in a year or less.

117
00:23:59,080 --> 00:24:14,080
So, and it's but it's an interesting argument because, you know, I'm very much pro technology, but I also know the limits of these systems. And so small drones, for example, for Amazon and Domino's pizza.

118
00:24:14,080 --> 00:24:24,080
One big issue that we're going to have are how these vehicles contend with high wind or even even more than high wind gusty wind situations.

119
00:24:24,080 --> 00:24:34,080
So, there hasn't been a lot of work not nearly as much as there needs to be on the kind of impacts that that could have on these operational outcomes.

120
00:24:34,080 --> 00:24:45,080
But, and I say this lovingly as a person who is from Tennessee, I'm a southern gal. I grew up making biscuits and shooting guns at the same time.

121
00:24:45,080 --> 00:24:56,080
You know, I come from I come from the rust belt and my family is the first group of people who are going to let it Amazon drone, bring it a case of beer.

122
00:24:56,080 --> 00:25:00,080
And as soon as the beer is dropped off, they're going to shoot the drone out of the sky.

123
00:25:00,080 --> 00:25:08,080
And I say, why? And everyone in my family would look at me and go, why not? Right?

124
00:25:08,080 --> 00:25:18,080
So, this is the idea of human behavior around these systems too. So, even if the regulatory infrastructure really loosens up for drone delivery.

125
00:25:18,080 --> 00:25:26,080
And I think it is. And I think that you're going to start to see some big progress in these areas that still does not regulate human behavior.

126
00:25:26,080 --> 00:25:43,080
And so, it will be interesting to me to see how humans will tolerate or not tolerate these systems, whether or not it's intentional, you know, benign malevolence pranksters or intentional malevolence.

127
00:25:43,080 --> 00:25:51,080
But that even being said, you know, Amazon is not going to be able to turn over a large market share to these small delivery drones.

128
00:25:51,080 --> 00:25:58,080
You're not going to look up in the sky and see drones, drones, drones. Really, these things cannot operate in bad weather.

129
00:25:58,080 --> 00:26:12,080
The winds are going to be an issue. And so, I think that you will see some breakthroughs, but you're not going to start getting your tacos and pizza by air like you think you might look way.

130
00:26:12,080 --> 00:26:38,080
You know, thinking about folks trying to shoot drones out of the sky, you know, makes me wonder if, you know, our vision for these delivery drones ends up looking, you know, to what degree the everyday delivery drone might need to have the same capability as something that you might expect in a military type of situation, the ability to evade, you know, attack.

131
00:26:38,080 --> 00:26:58,080
Is that, you know, is that something that you are people looking at that and are you involved, especially given your background in some of the kind of research is happening around the military versions of these and is there anything there that you can share with us.

132
00:26:58,080 --> 00:27:16,080
Yeah, so there are people who are looking at that. I think, you know, most of that work is done in something we called concepts of operations. So I doubt we're going to have drones that come along and like shoot you with a taser if you try to take the pizza and it's not your pizza.

133
00:27:16,080 --> 00:27:34,080
Although that does bring up that could be a nice interesting set of films, Quentin Tarotino films around that. But one of the things we are looking at and we just started a big National Science Foundation effort is in the passive deterrence of drones.

134
00:27:34,080 --> 00:27:52,080
So one of the issues that we are concerned about as drones start to rise in numbers both commercially, but also with recreational drones are in public spaces, we're starting to see drones become increasingly annoying.

135
00:27:52,080 --> 00:28:10,080
I have a daughter, 10 year old daughter who plays soccer and there's always, you know, and it makes parents unhappy to see the pervert drone flying around filming everybody. Now, do we say that? I mean, it could be just somebody filming for fun or filming their own daughter, maybe, but the rest of the parents don't like it.

136
00:28:10,080 --> 00:28:28,080
And in fact, it's illegal. And so one of the things that we're working on at Duke is developing both passive detection technologies to let managers of small outdoor spaces to let them know that a drone is nearby and or operating in their vicinity.

137
00:28:28,080 --> 00:28:57,080
And we're thinking we're working with the bull Durham from the movie, the Durham Bulls. We're working with a small minor lead team to start helping them, you know, they have these same issues as well to figure out are the drones around. And then what can be done to dissuade illegal drones from operating in your area? What can you do from a landscape architecture standpoint? What can you put in the environment? How can you design the lighting systems, for example?

138
00:28:57,080 --> 00:29:10,080
What if you layer in mist? So one of the things that we know about any drone with LiDAR and some computer, their cameras, they just don't work well when there's a light layer of moisture in the air.

139
00:29:10,080 --> 00:29:22,080
So maybe one of the things you could do, especially on a hot summer night in North Carolina is just put some mist out there. And that might be enough to keep the drones who shouldn't be there, keep them away from your area.

140
00:29:22,080 --> 00:29:23,080
Interesting.

141
00:29:23,080 --> 00:29:37,080
Yeah. And so, you know, this is of a big importance right now because we need low cost solutions, not every stadium can get jammers that could potentially jam the radio frequencies that these drones are operating on.

142
00:29:37,080 --> 00:29:52,080
And there's actually a lot of debate about whether or not that's actually illegal from a constitutional standpoint. So there is a lot going on in terms of, you know, trying to look at how people either on the ground or with drones are acting in potentially nefarious.

143
00:29:52,080 --> 00:30:10,080
There's not going to be one great solution. There's no silver bullet here, but as a technologist in the academic setting, we really love it when one new technology comes along because the counter to that technology will come and then the counter to the counter.

144
00:30:10,080 --> 00:30:11,080
Right.

145
00:30:11,080 --> 00:30:13,080
It just keeps us in business. So it's all good.

146
00:30:13,080 --> 00:30:34,080
Right. Right. Yeah, it's interesting. I recently saw a video and I'm sure there are many companies working on this type of thing, but it was a company that had a drone detection technology that you can kind of put up around the perimeter of your space, but also like this jamming gun that forces the drone to land.

147
00:30:34,080 --> 00:30:55,080
I guess there's technology pieces of that. I guess the way this thing operated when the drone lost contact with its, you know, it's, you know, whatever its controller was and lost, lost radio contact, its instincts towards, you know, the way it was programmed was to land immediately.

148
00:30:55,080 --> 00:31:09,080
But it struck me that there's that that might not be the right behavior, at least if these jamming guns become popular, you might want the drone to do something else if it loses contactors.

149
00:31:09,080 --> 00:31:25,080
Like you said, it becomes a bit of a cat and mouse game, not just from a technology perspective, but the way we kind of think about and, you know, direct these drones to behave based on the kinds of things they experience in the environment.

150
00:31:25,080 --> 00:31:35,080
Yes. And then of course there's some derivative effects that people don't think of, you know, assuming that the drone does take the safest option and land itself in these cases.

151
00:31:35,080 --> 00:31:49,080
I mean, that's that's a win-win except that that doesn't always happen and oftentimes people will do things like that and the drone will lose its ability to maintain stability and then they fall out of the sky and then that becomes a problem.

152
00:31:49,080 --> 00:32:12,080
But the idea of, you know, jamming drones and other technologies, this is an interesting issue that we're also seeing in driverless cars. And so there's been some work going on at the University of Texas at Austin to look at how easy it is for people to spoof drones GPS signals, which are also true.

153
00:32:12,080 --> 00:32:20,080
It was a similar problem for driverless cars. GPS is a very vulnerable signal and it can be relatively easily tricked.

154
00:32:20,080 --> 00:32:32,080
But there's a researcher, a friend of mine, Todd Humphries, who works at University of Texas at Austin, who told me that they were trying to do these field tests and they were having trouble doing the field test within the city limits of Austin.

155
00:32:32,080 --> 00:32:47,080
Because so many people in Texas were driving around with GPS jammers in the back of their cars trying to make sure their bosses and wives and whoever did not know where they are, that they had trouble getting a good clean signal.

156
00:32:47,080 --> 00:33:03,080
And so I think that that's an interesting case and, you know, one to be taken seriously that you don't even have to be a person who's trying to do anything bad or necessarily trying to do anything to thwart a person specifically on an individual basis.

157
00:33:03,080 --> 00:33:21,080
But yet we still have some kind of paranoia in our society that then could these people driving around with GPS jammers in the back of their cars, what they then do is that they're going to screw up every other driverless car in the near vicinity from doing what they need to do.

158
00:33:21,080 --> 00:33:41,080
So now we have this trickle down effect of okay, you know, how are we going to make sure that cars driverless cars who are legitimately operating the space that they should be operating in are not going to start behaving erratically, because we've got another set of people who are paranoid about what the technology is doing.

159
00:33:41,080 --> 00:33:42,080
Right. Right.

160
00:33:42,080 --> 00:33:51,080
I'm going to be a business for a long time. I was just going to say that's before we even get into cyber security, which you mentioned earlier.

161
00:33:51,080 --> 00:34:03,080
Yeah, right. And so I think the cyber security issues again, the more we do electronically, the more of these fly by wire aircraft and drones and trains and you name it.

162
00:34:03,080 --> 00:34:13,080
You know, CPS and certainly cyber physical systems, it's definitely a huge growth area, not just in academia, but in the public at large.

163
00:34:13,080 --> 00:34:19,080
So I would expect if you're looking wanting to know what's the good field to go in, that's one of them.

164
00:34:19,080 --> 00:34:34,080
Absolutely. One of the areas that you've done a lot of work in is you alluded to this earlier when you describe your background is kind of the human computer interface and interactions between humans and these systems.

165
00:34:34,080 --> 00:34:45,080
And certainly that's been kind of an underlying thread in our conversation art to so far in terms of the way we design systems for this.

166
00:34:45,080 --> 00:34:55,080
But in terms of that, you know, the humans interacting with the autonomous vehicles themselves, what kinds of things are you looking at there?

167
00:34:55,080 --> 00:35:03,080
Right now, it's funny because I've been doing so much drone research for such a long time that I've become bored with it.

168
00:35:03,080 --> 00:35:09,080
That's why I'm like, Lord, looking at defending against drones now because I've done so much of it. And it's funny.

169
00:35:09,080 --> 00:35:23,080
I have a similar issue going on with driverless cars. I'm so bored with the idea of driver distraction and this idea of how to bring the human back into the loop of the driverless car and very fatalistic about it.

170
00:35:23,080 --> 00:35:29,080
Humans are awful or terrible drivers. You just should not assume that they're going to pay attention when you need them to pay attention.

171
00:35:29,080 --> 00:35:39,080
And so unless the technology can be guaranteed to jump in when it needs to jump in, you know, you cannot rely on the humans to do it.

172
00:35:39,080 --> 00:35:45,080
So I've actually moved my area of research now to start looking at people outside the car.

173
00:35:45,080 --> 00:36:09,080
So we have a big research effort in trying to help people who are pedestrians, bicyclists, other people, vulnerable roadside users, which we would consider to be construction crews, and state troopers, for example, first responders who are on the roadside in some way, shape, or form operating in and around self driving and driverless cars.

174
00:36:09,080 --> 00:36:22,080
So looking at how to do pedestrian alerts, how to alert state troopers that they may have a car coming that may or may not be able to see them under certain atmospheric conditions, weather conditions.

175
00:36:22,080 --> 00:36:33,080
So yeah, we're we're I'm really concerned about the other stakeholders in the system other than the person who is in the car who I can assure you is not paying attention.

176
00:36:33,080 --> 00:36:42,080
So does that involve infrastructure like beacons or things like that or what's the direction that you see.

177
00:36:42,080 --> 00:36:52,080
That's one of the things we're looking at. We're looking at how to put very low cost transmitters on a car so that it can so your cell phone can listen for it.

178
00:36:52,080 --> 00:37:07,080
And so, of course, one of the big things that we see are people are heads down crossing streets. There was just a law passed in Hawaii that, you know, they're going to find you 100 bucks if you're caught crossing the street looking at your cell phone.

179
00:37:07,080 --> 00:37:17,080
Wow, which is always interesting to me. I'm like, first of all, the evidence is quite clear that you can legislate that all you want. That's not going to stop people from doing it.

180
00:37:17,080 --> 00:37:28,080
And secondly, good job that you're going to find them 100 bucks, for example, but they're dead because that's not really a helpful solution.

181
00:37:28,080 --> 00:37:42,080
So what we're trying to do is we're trying to get car your cell phones, which are what you're looking at to note up to listen and detect the car before it gets to you and tells you to stop since you're looking at it.

182
00:37:42,080 --> 00:37:52,080
We're working on very many related projects in that area, but our idea is, you know, to try to use the devices that are distracting you to help you.

183
00:37:52,080 --> 00:38:19,080
And so does that is the underlying phenomenon that's driving that research direction, kind of a feeling that in order for humans to feel comfortable around this technology, though, want to know or be alerted when self-driving cars are around, or if you have a similarly fatalistic view of the self-driving cars themselves that they're not going to be able to not crash into people.

184
00:38:19,080 --> 00:38:29,080
Well, no, actually, I mean, I believe that the cars, you know, enabled with this right sensors and the right sensor mix that works in all weather conditions.

185
00:38:29,080 --> 00:38:36,080
I think I'm much, much more trusting of a technology solution than humans.

186
00:38:36,080 --> 00:38:50,080
The real question, though, is making sure that humans, you know, we're talking about humans responding to technology as, and I think that real issue is one of reliability.

187
00:38:50,080 --> 00:39:03,080
So one of the things that we know for sure is, if you're walking across the road and you're looking down at your cell phone like Google Maps, you know, that's a very common problem because you're trying to figure out where you're going and you're crossing the road.

188
00:39:03,080 --> 00:39:13,080
And all of a sudden, your cell phone both showed a big stop sign and screamed at you to stop because there was a car coming and, you know, and it could detect the car from some distance away.

189
00:39:13,080 --> 00:39:21,080
And you looked up and saw this car bearing down on you and jumped out of the road. I mean, that would be a really good, you would say, this technology is great.

190
00:39:21,080 --> 00:39:36,080
But one of the problems that we have is that we can do that and we've actually shown that we have pretty good reliability. In some cases up to about 90%. But the question is, is that enough because humans are incredibly fickle?

191
00:39:36,080 --> 00:39:55,080
So just because 9 out of 10 times, you have something correctly alerting you, it may be, even if it's wrong, 1 out of 10 times, that's enough to cause you to distrust it. So in fact, that's where our big focus of our research is right now is looking at these issues of trust.

192
00:39:55,080 --> 00:40:07,080
So, where do we draw that line? How infallible does the technology have to be for people to turn it off, for example?

193
00:40:07,080 --> 00:40:27,080
A funny thought jumped into my head when you, as you described this, this human looking at Google Maps and getting alerted when, you know, an object was, you know, in this case, a moving object was potentially about to collide with them, you know, we're almost turning the human into kind of a water-based self-driving vehicle.

194
00:40:27,080 --> 00:40:40,080
It's like it's got navigation, it's got object detection or collision avoidance, you know, that's via the, via computing system and we're just using this, the eye is kind of the computer brain interface.

195
00:40:40,080 --> 00:40:53,080
Well, I don't know if I'd call it the interface, but actually say it's the sensor, right? It's your primary sensor that most of us use in the world to get around, but it's not perfect either.

196
00:40:53,080 --> 00:41:03,080
So, we are running up to the, or coming up to the top of the hour, as we close out, are there any thoughts that you'd like to leave the audience with?

197
00:41:03,080 --> 00:41:17,080
No, I mean, I don't have to tell your audience how important it is that the job that they're doing and how popular it is and, you know, it's amazing as I was walking into work this morning with another computer vision professor.

198
00:41:17,080 --> 00:41:28,080
You know, we were just kind of laughing at how awesome it is to be in the in crowd, you know, we're in a very hot field and it's exciting and it's growing.

199
00:41:28,080 --> 00:41:46,080
But I do want people to really try to take a step back and take a deep breath and, you know, these issues around explainable AI, you know, some AI researchers like it, a lot of researchers, you know, kind of take on this, you know, negative attitude, kind of a condescending attitude.

200
00:41:46,080 --> 00:42:06,080
But in the end, we've got to get this technology to a point that we can have true safety guarantees and that is not to say that's not the same thing as saying that an algorithm is provably safe, which has a different academic implication as it does to a real world connotation.

201
00:42:06,080 --> 00:42:22,080
So I do want people in this field to keep pushing like they're pushing, but also take a step back every now and then and remember that in the end, you've got to communicate these results to a range of people.

202
00:42:22,080 --> 00:42:32,080
And so I believe that the ideas that we're working on around explainable AI need to start coming into everybody's vernacular sooner rather than later.

203
00:42:32,080 --> 00:42:38,080
Right. Right. Well, great. Well, thank you, Missy, so much for spending some time chatting with us.

204
00:42:38,080 --> 00:42:40,080
It was my pleasure.

205
00:42:44,080 --> 00:42:57,080
All right, everyone. That's our show for today. For more information on Missy or any of the topics covered in this episode, you'll find the show notes at twomolei.com slash talk slash 128.

206
00:42:57,080 --> 00:43:02,080
Thanks so much for listening and catch you next time.

