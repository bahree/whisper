All right, everyone, I'm here with Ziyad Asgar. Ziyad is vice president of product management
for Snapdragon technologies and roadmap at Qualcomm. Ziyad is great to see you again and
welcome to the Twomo AI podcast. Thank you for having me, Sam.
Hey, I'm looking forward to digging into our chat. We are going to cover a lot of ground.
We'll be talking about AI in the cloud on edge devices and cars and more. But before we dig in,
I would love to have you introduce yourself to our audience and share a little bit about your
background and how you came to the field of AI. Thank you. You know, it's been really exciting.
I have just great job where I get to work on fancy new technology that really changes the way
people use their devices. And it's been quite interesting because I started with Texas instruments
when they used to be kind of the lead on smartphone and the very early beginnings of what you can call
an operating system and how things have changed. So being fortunate enough to kind of see that whole
smartphone, you know, evolution that has happened right in front of our eyes. And then really the
advantage that I've been able to do at Qualcomm is to take some of these technologies and then to
bring it into even additional fields beyond the smartphone. And you know, what's been really
amazing is that AI is this new technology that is really spanning across multiple product lines,
across multiple technologies. And it's really bringing so many new capabilities that we at least
are super excited. Our customers are very excited. Just the value that we are able to bring
based on that is superb. So very excited about what he is going to enable us to do.
Awesome. And I think our listeners are familiar with Qualcomm. But I'd love to have you maybe
spend a few minutes just contextualizing the company's role in the AI ecosystem. And we hear about
the Snapdragon thing all the time. What exactly is Snapdragon? Sure. So Snapdragon is the name
of our platform that takes the best in-class technologies that we work on from, you know,
camera, graphics, processors, modem, and put it all together into these products that we call
Snapdragon product line. And essentially, it has best in-class capabilities across all of those
vectors. And at the same time, we are relating this wave on 5G right now. And really,
I always say this, but 5G and AI are technologies that are very symbiotic. They go together hand
and hand. 5G makes AI better, AI makes 5G better. So really, a very key focus area for us right now
are those two technologies. And they come together in the form of a Snapdragon product.
We of course make the hardware, the software, all the experiences that go around it to be able to
create these end products that are just amazing and throw all our consumers that use them.
Can you elaborate a bit on the symbiosis between 5G and AI? I think my sense is that
the carriers in some cases have kind of jumped the gun and labeled their 4G plus technologies
as 5G and have kind of disillusioned a lot of us over 5G. I don't know if that's a broadly held
take, but how does 5G and AI complement one another? Yeah, I think that's a great question. But,
you know, you remember, this is pretty much the same thing that we saw with the advent of 4G.
There was a lot. It's from not a drone. And you know, it met very different things and did it
really mean 4G or not? So there is some of that going on, but I think the way you should think
about 5G is millimeter wave brings in the amazing capabilities of unbelievable throughput,
unbelievably low latency. And that is what really changes the game. I think that's superb.
And the way AI and 5G come together is just think about it this way. We are applying AI techniques
to how our modems work. Modems are basically the blocks that allow you to receive the signal
from the air. And by applying AI techniques, we're actually able to make them work more efficiently
to be able to green the signal from most complex of channel conditions. And that's how AI makes 5G
better. But the way 5G makes AI better now is that basically with the advent of 5G, we have
intelligence that's interspersed across different parts of the network. We have it on our mobile
phones. We have it in our connected cameras. We have it in our devices of all sorts. But what 5G
does is that it now bridges the gap between the end nodes, which might be these end products
all the way to the cloud. And in doing so, what it allows you to do is even though the right
place to do AI is on the device for privacy for immediacy reasons. But now with the leverage of 5G,
if there is need for accessing more AI capability, you can basically extend your intelligence into
the cloud, do more work over there, and be able to bring it back to the device very, very fast.
So this is what we're calling kind of the advent of distributed intelligence across the whole network.
And that's what is possible due to 5G. Well, let's maybe start with or continue with the device
and dig into what's new and enabling developers to take full advantage of AI on mobile devices.
Sure. I mean, today many people might not realize it, but there are many use cases that are
already using AI on the device. So for example, the imaging experience has been completely changed,
transformed because of the application of AI, right? For example, now your cameras on your devices
can see into the darkest of rooms. It allows you to be able to do right. I mean, sometimes my
eyes can't see and I take a picture and my phone can see through it. But that's the power of AI,
right? We are basically training those models to be able to take out the image or the picture
from the darkness. Very, very powerful. It's been an incredible revolution. And just how much
of photography has shifted from kind of these physical characteristics like the lenses to software.
And AI has been a big part of that. Absolutely. Right. The fact that you can do face-based
payments, it's driven by AI because of the fact that now you can use AI to basically check
for live-ness. Because if somebody tries to trick that image by putting a picture in front of you,
AI can actually tell you if that's a person in real life or not, right? But the power of AI
really is not just on imaging. We applied it to audio. We applied it to speech. I mean, in the past,
we were able to use this to be able to do machine language translation, right? And to be able to do
that on the device is a very different experience than to do it in the server or the cloud. Because
now what you can do is you can actually have two people that live, you know, in far-flung places
can talk to each other, but your devices are going to make it almost seamless for you to be able
to converse together with each other. Very, very powerful. And then what we have done is we have
applied that to echo and noise cancellation. We applied that to imaging. We applied that to video
now more recently where you can literally take a video and take out the object or the subject of
that video and replace it with a different person. Now, that is, of course, room for a lot of fun
exercises that you might see on social networking as well, where people are saying things or doing
things that they're not really doing. And that's why there's also security element that comes
through. And we talked about that also, which is content authenticity initiative that we have
basically engaged with to make sure that what you are seeing is truly what the camera was able to
capture. But what makes this work on the device is basically best-in-class hardware. And what I
mean by that is not just peak performance, you got to be able to do that AI processing at the lowest
power possible. For it to be able to work in these handheld devices, it has to be able to do it
very low power. Then we need to have pristine software. Software that's able to work at many
different levels of the stack. So we offer what we call Snapdragon neural processing SDK at a higher
level. We offer something that we call hexagon AI direct at a lower level to be able to write very
close to metal to get the most out of the hardware. And then the last piece is really tools.
And methodology or algorithms that make it possible for people to be able to use AI more efficiently.
What I mean by that is, so we've introduced what we call AI model efficiency toolkit. And we've
open sourced a lot of this. But what we do is we work with our corporate R&D team. They develop
certain methodologies to quantize the networks to be able to compress the networks. And by the
way, those techniques are providing so much uplift to the amount of AI processing that we can do
for a given amount of power that we are able to get a lot out of it. So like for example, if you
quantize a 32-bit floating point model and take it to 8-bit integer, you get 4x improvement.
We have shown that by compressing and taking out the redundant parts of a network, we get
3x compression. So many times you can reduce the, you know, the consumption by three times.
And that we do by applying, you know, single-valued decomposition or Bayesian
approach is to be able to do that. We have talked about add-around, you know, recent paper,
which is adaptive rounding. So, you know, if you run to the nearest
number, the accuracy is different than if you are able to do adaptive rounding. So, you know,
those are the kind of technologies and techniques that we are spending a lot of time and effort
on to really be able to make it work across different domains to be able to do more processing
at the lowest power as well. Where did these tools fit in in the kind of the tool chain of the
the mobile developer? Are they typically using the snapdragon tools directly or are they
accessing them via their OS, iOS, Android, whatever the device is? And there are kind of OS
layers that take advantage of the snapdragon tools. Yeah, that's a good question. So,
the way it's done right now is these techniques are open-source. So, technically, anybody can apply
and use them. But they are fitting, of course, very closely into the set snapdragon neural processing
SDK, because that's where you're able to get a lot of the benefit out of it. It's done specific
to our hardware such that we can get all that benefit out. But yeah, there isn't anything that
prevents those techniques to be used by others as well.
And so, digging a little bit deeper, the snapdragon architecture at the hardware level
is constantly evolving. And as an Android user and a phone connoisseur, I'm always kind of waiting
for the latest and greatest. And this time around this year, it's the 888, is that right?
That's right. What does that do for me as an AI user? Sure. So, let me spend a little
time talking about the hardware. So, we are continually updating hardware to get the most out of,
you know, what we are able to do on the smartphone device. But we specifically don't snapdragon 888 is,
we typically take a heterogeneous approach. So, our AI engine is a combination of what you can do
on the CPU, what you can do on the graphics, and then what we call our hexagon processor. And what
we're able to do with the snapdragon 888 is a combined peak performance of almost 26 trillion
operations per second. That's a heck of a lot for a device that fits in the palm of your hand
and can already run the whole day. So, what we have done specifically on 888 is, we typically in
the hexagon processor have three different components. We basically have the scalar part,
the vector part, and then the tensor component. And if you look at a typical model, a neural
model neural network, Sam, what you would notice is that there is an initial part, which basically maps
very well to the scalar component. And then there is a, you know, typically an end portion, which is
like the fully connected layer, which kind of maps very well to the vector part. And then many of
the intermediate layers map very, very well to the tensor processor. So, that's how we're able to
basically put organ together. Now, in the past, some of these sub blocks, they were not all together
into a fused architecture. With the 888, we actually brought them together. In one architecture,
we have wrapped around a much larger shared memory. And by doing that, we are actually able to
reduce power a lot. So, I'll give you an example. One of the problems in AI is that people need to
move a lot of data, which is the weights and activations from where they're stored into the AI block
where they're processed. Now, if you have a larger shared memory, which in this case, it's many
times more than what we basically had on the previous generation part, you're able to bring a lot
of power consumption savings. So, we are able to bring in almost two to three x power consumption
saving in some scenarios by doing that. At the same time, as you context switch between those
different sub blocks, the vector or the tensor component, you're actually able to reduce the
latency significantly. In some cases, 1,000 x more. To really improve the efficiency all around
from a power, from peak performance, from a latency perspective with this new architecture
that we have come out with, we pretty much do like a major upgrade of AI architecture every
other year with minor upgrades in the middle as well. But just because this field is evolving so
fast, we study the different networks from the perspective of what we're seeing or consumers
and our customers do with the device. And based on that, we're continually upgrading different
components to make it run even more efficiently than before. And do these advances kind of come from
the traditional ability to squeeze more transistors onto a given device size? Or are you pulling in
innovations from all the research labs that you're doing, papers, things like that? What's the
balance of the evolution of the product versus incorporating research concepts?
Yeah, definitely. It's coming a lot from the research that our corporate R&D team does.
I mean, just to give you perspective, we have more than a decade of research before we actually
brought AI to a product. And our first product that actually enabled this was in 2015.
So we're really continually changing the architecture because you can get a lot out of that today.
I mean, what has made AI work on the devices is the fact that you can do so much processing
on the device at low power, which you could not do in the past. And there is still, by the way,
a lot more that we can take out with the combination of, like I said, hardware software and the tools.
So definitely, we are also continually looking at new approaches. There are other methodologies
and architectures that people are studying. We keep an eye on those too. But of course,
we're very close to the products. We want to make sure these are techniques that we can commercialize.
So there are other things that our teams are looking at more from a pure R&D perspective that you
will see coming to the front-end in future years. Okay. So we've talked about in the in the setup
to this, we talked about 5G enabling this kind of distributed environment between the mobile devices
and the cloud. Let's maybe jump over to that cloud side of things. I had the opportunity to attend
the cloud AI 100 launch a couple of years ago. This is a new infrastructure hardware that
Qualcomm has been working on. What's new on that front? Yeah, I think it's actually a very
exciting area. So, you know, these advantages, Sam, that I talked about from the perspective of
mobile. By the way, they are as important also on the cloud side. And the reason for that is,
I mean, if you look at, you know, big data center or big platforms, like let's say Facebook,
you will notice that they have publicly said that they have inferences that are happening in
the range of about 200 to 400 trillion inferences per day. That's a pretty massive number. And then
that old trajectory is very steep. So some of the data that people are looking at is the data center
power is doubling every year. So, unless and until people are able to change the architectures used
in the data center, it is really start to push against a wall. So what the cloud AI 100 does is that
it basically takes that advantage that we've created on the mobile side. And really, of course,
in a very different setup, leverages all of our advantages, all of qual comes perigree for best
in class performance for the least power and then brings to the cloud. And we're really seeing
applications on, you know, on the cloud side, we're also seeing applications in what we call,
you know, like smart city like applications. So you can envision, right, that you have,
that's a multiple cameras installed at a junction. And then of course, you can't have as many
people staring at each of those streams. So that data is actually going into a device like the cloud
AI 100, where we are able to make sense of what that video stream means. And if an event has happened
an event like an accident, perhaps at the intersection or something else, right. So those are kind of
the scenarios that we're talking about. And we recently actually published data for ML Commons.
And it actually shows that, you know, cloud AI 100 really leads for, you know, 50 watt and for even
smaller configurations of power for the amount of performance we're able to get out of that platform.
So very exciting, really, to be able to move into this new area. We have multiple partners
that we're working with right now. And Emma comments is a benchmarking effort? That's right. So
it's ML Commons is basically kind of the umbrella, I believe, for ML Perth. And they published a lot
of results very recently for the cloud side of things. And very, very good story for us. I mean,
if you look at something like ResNet 50, if you look at other networks, you'll see the power
of the Cloud AI 100 like product that shows very well on ML Commons. And with Cloud AI 100, is it
taking that core Snapdragon architecture or the architecture that underlies the Snapdragon
and scaling it up to kind of the scale of a card that might go on a server in the cloud for
inference? Or is it a, is it a different architecture? Yeah, I think the way we developed that
IP is that we, we have a root IP where we leave a lot of configurabilities the way to think about
it, right? So for example, the TCM or the memory that you have in the block can be modified.
The amount of processing that you have in the tensor core can be modified or in the vector section
can be modified or the precision that you are making available in those cores can be modified.
The process node comes into play also here and there. So my point is there is a lot of
configurability. The number of cores that you might have, right? So the root IP you can say is
probably very similar, but as you make all those changes, it leads to a very different product,
but it still has those fundamental benefits that we brought in at that research level. And that's
why we think Cloud AI 100 really shines in terms of the performance that you're able to see. So
leveraging all the benefits, but definitely a very different machine than what we have in the
mobile side. And I mentioned this, but I want to make sure that I'm correct. The focus for the
Cloud AI 100 is primarily or maybe even exclusively inference as opposed to training and inference.
Is that correct? That is right. So it's primarily focused on inferences. It's a machine that's
built ground up with the right architecture for inferencing rather than leveraging a CPU or GPU
like architecture because that makes a fundamental difference in the power consumption that you can get.
And you mentioned that this is primarily targeting these large platforms like
Facebook. This isn't a GPU that you're going to just go and install, you know, get one on Amazon
and install it in your server. Am I correct about that? You're absolutely right.
But I think what people are realizing that as the inference need is, you know, really going up
and ramping up, people are realizing that the CPU or GPU architecture cannot scale. It cannot scale
to be able to give you the level of inferencing needed at a given amount of power. And I think
that's where the advantage really comes through for the architecture that we have within the Cloud
AI 100. Very, very strong story for us. And I think what we've also been able to do as we
kind of talk about, I think, is take some of these processors like the Cloud AI 100 and take it
also into other areas. For example, on the right platform that we have launched, which is for
automotive. So it's giving us a lot of flexibility in being able to take that wherever there is a
need for massive inferences at low power, this becomes the platform that we're able to use.
And is it available? Are there partners in the ecosystem that offer it kind of as a service
underlying cloud service that is accessible to developers? Yeah, so it is available today.
We already have partners that we're working with to launch on multiple different product segments.
And yeah, you will be talking about some fairly soon here, but we did announce one very recently.
So this is an engagement that we had done when Gigabyte. And what it does is it actually takes
not just one, but multiple Cloud AI 100s into a server rack. And by the way, we start talking about
tops and that configuration. We talk about pops as in peta. So yeah, very, very strong platform,
which you can scale very well. Awesome. You mentioned the ride platform. The company's been active
in automotive segments for a while. Can you give us a summary of that ride platform and what you're
up to in automotive? Sure, sure. I think again, from an AI perspective, you know, it's really an
amazing area, which we're seeing with automotive. I think it's starting with, of course, just looking
at the very low level at inside the cockpit. So driver monitoring at a very, very low level.
And then going into L1, L2, like autonomy, and then it scales all the way into L4, L5. And what
we're able to do is the ride platform is that we have that ability to be able to scale from say
about, you know, 50 tops, then to 400 tops and all the way to like 700 or 800 tops of capability
that allows you that scaling that continuum of performance. And then you have algorithms
that you're able to scale. I think auto, of course, has very special needs and poses very special
challenges. As you move into electric vehicles, I think power consumption is absolutely pristine
over there as well. The kinds of models and networks are more perhaps focused on, of course,
object detection. But now you have very different kinds of sensors. You have light hours, you have
radars, you have multiple cameras, you have very high bits. So it's not about image quality,
but it's more about cleaning information from that. So I think in that context as well, the
architecture that I explained, which is, you know, this heterogeneous-like approach actually works
extremely well. We're seeing very good traction with multiple customers. And as you know,
the way we've done this is that we started our work on automotive site with
infotainment. And within we've taken a step towards, you know, very simple ADAS and then, of course,
we're continuing to build on that as we move into autonomy. So we feel we have all the building
pieces. We think we're very good engagement with partners today that give us a very good path
to growing that side also. Yeah, that was going to be my next question. You've, you know, clearly
have the research capability on the pure AI side. Are you primarily going after this market with
partners to develop the vehicle specific capabilities? Or are you investing in kind of domain
specific research that's relevant to, you know, these auto and autonomy challenges?
Yeah, I think it's a mix of both, really, because we need to do some fundamental research to be
able to make sure we're creating the right hardware software tools. But now, also developing some
key algorithms that are able to make sure and make use of that hardware. So we build those building
blocks. And then, of course, on top of that, we work with partners who are doing a lot of research,
who have a lot of data based on their current experience. And then we can build on top of that
platform that we are built. But, you know, the strength that we are able to bring to this is we have
that very large investment already done on the mobile side in many cases. And it's very easy for
them us for us to leverage this into these new areas that we are growing in all the different domains
from hardware software, but also from an algorithm's perspective, from tools perspective that I
think gives us a leg up that other people don't have in this space. You mentioned this kind of path
from I forget the low end of that, maybe 25 tops to our operations per second up to, you know,
into the hundreds. And you also mentioned kind of infotainment to driver monitoring, to level one,
to level and autonomy. Like, are those correlated? And how are those correlated? And, you know,
what's the tail that's wagging the dog here? Are there impute requirements in the vehicle
a limitation or is the limitation for the higher levels of autonomy? Is that, you know, currently
on the algorithmic side or integration? How do you just see the challenges that are, you know,
that are taking place in the vehicle platforms today? Yeah, indeed. I think it's the need for
more peak performance, but it's also the need for specific kinds of algorithms. And then
according to of those algorithms to the hardware that you are able to run at the highest performance
level. And I think there are challenges in each of those steps. So you need to be able to
optimize models very quickly. Again, tools come into play over there. You need to be able to get
to that peak performance and be able to sustain it, right? Automotive is a very sustained use case.
It's not a use case where you, you know, go up for a little bit and you got to come down. And then,
of course, you have to have redundancy. You need to have basically a surety and you need to have
multiple different layers of, you know, redundancy built into it. So yes, it's it's algorithms
that have to improve with time, but it is also the enablement, which is what I mean by that is hardware,
software, and then of course, the tools and everything that come along with it. But I think
automotive is very exciting too, because the problems that we are solving are automotive. I think
we'll find it's used in many other places. For example, what I mean by that is we're working a lot
on robotics. We're working a lot on, you know, autonomous. For example, you know, not just cars,
but drones and other things. We demonstrated this some time ago. And I think it's it's really
quite powerful that you can then learn and leverage those things into these adjacent and new areas
that will come with time as well. Let's dig into that a little bit more. I didn't realize this,
but I think you had a role in the Mars chopper. Yeah, I'm quite excited about that actually. Yes,
so the ingenuity chopper that you have on Mars. I mean, this is an engagement with Qualcomm and
NASA. It's really amazing to have a product that we that I worked on actually quite some time ago
that now sits on another planet. And actually, it's just amazing to see that. And actually,
it's using the navigation. It's using, you know, virtual throughout the visual inertial
adornatory. It's using, you know, object detection, many of the capabilities that we have baked
into these products that are being used on that chopper. It's really surreal to some extent to be
able to see, you know, human presence. And for the first time, you know, something that is like
a chopper on a different planet. So we're very excited with this engagement. And to see many of the
tools that we developed being used actually in this experience and, you know, you can see and you
can envision that as we do a lot more on autonomous, for example, route mapping and things of that sort,
you can do a lot more of this stuff in these kind of environments where you don't have direct human
interaction to some extent. Just amazing. I mean, you can envision, you know, very tough environments
even on Earth, where you can leverage a lot of those experiences and learnings that we have gotten
from there. So very exciting. I mean, we were watching it when, you know, the, it was landing
on Mars and we've been tracking pretty much closely ever since. This may be a silly question,
but I'm thinking about like 5G on Mars. Is this a connected application or is this a autonomous
application or the hybrid? I believe there is some link to the to the mother rover that they have
over there, but to great and it can be controlled separately as well. But it would of course be other
more shorter term technologies that we would be using to to have the tool. Otherwise, I'm on
for those base stations on Mars. And so then, you know, we talked about cars, we talked about
choppers. I mean, these are examples of kind of this broader IOT and devices landscape that
enterprises are increasingly interested in. You mentioned smart cities earlier.
Are these, you know, to what degree are you specializing to enable these various applications?
Or is the kind of the core capability, you know, are the developers kind of getting you there
to the specific applications? Sounds like you're doing a little bit of specialization and automotive.
And I'm curious if that is a pattern that extends across to other areas in IOT.
Yeah, I think IOT and some of the fields have usually a very varied sort of application,
right? So you'll find many different applications. So in that case, our plan usually is to enable
some of these things. And then our partners, you know, whether those are ISVs or customers,
they're able to take it from their customizer to their specific needs. For example, very recently
at the China Tech Day that we just had, I think a couple of weeks ago, we actually showed an
application of the robotics platform where you actually have a robot playing ping pong. And that
actually uses the AI technology that we have developed to actually train the players to be able
to play very good table tennis, right? Of course, this is one example of it, but there is a lot more
that's happening on IOT side in also what we call big IOT. So, you know, one example that you can
think of is like smart retail. Now, here is the case where basically, you know, you would have
multiple cameras again with people coming into a store like, you know, a completely autonomous store,
the buy certain things. And you're automatically checked out as you walk out of it. You, of course,
have multiple cameras that are tracking the devices or the products that you're buying.
It's able to object detect, able, it's able to figure out the cost of that one. It's automatically
able to check you out. So, it's quite interesting what we'd be able to do on many of these applications,
but these are still, of course, developing. But there are, there are simpler cases of this that
can be done today. Like in the self-checkout line, there might be concerns of, say, PILFrage or
of errors in people, basically buying certain goods and already, you know, plans like this where you
have, you know, some intelligence on the camera versus some intelligence in the cloud, you're actually
able to cover it quite well. When you've referred to big out of T is the distinction you're making
there, kind of multiple devices needing to collaborate and kind of this distributed AI use case
or, yeah, and if so, what are some examples, some other examples of where you've really seen us
kind of pushed that? I know that I say that kind of knowing that the, at the research level,
you're doing a lot on federated learning and that's an evolving field and I'm really curious
like how far are we in deploying solutions based on that kind of research? That's a good question.
So already when I say big IAT, I mean multiple devices, kind of the way you explained it and then
different levels of intelligence within those devices. So maybe you can do a course AI assessment
and then based on that outcome, you're able to pass on to a central more capable AI that now does
the finer level of assessment, right? So that's kind of an example of big IAT, but at the same time,
I think the experience has changed a lot. Like I always did this example that, you know, I have this
security camera outside and in every time a person passes by, it basically pings me. It's really
quite wasteful, right? I mean, if you have a little bit more intelligence there, you would have
their ability to be able to assert and if that's a car, if that's a person, and if that's a person,
is that the person that is a safe person or not? Right? So it has application of what I call big IAT,
but also as we pack in more AI into connected cameras and other devices, you'll see a lot more
over there. And then I think then the kind of the second part of your question, sorry,
second part of your question was on the related learning and the degree to which that is starting to
see use and industry. Yeah, so I think by the way, federated learning is something that's already
happening on the devices. So if you take the example of the Google keyboard, what you're able to do
there is that it actually takes some learnings from the way you use the keyboard and then actually,
it's able to apply that into in the cloud where basically aggregates a lot of those needs
to get to the best in class experience. So the other people who then download the Google
keyboard is are actually able to leverage and take advantage of that learning. So that continues
to get better over time. We're also doing research on some limited training on the device. I think
it can actually open up some very interesting use cases. Like another set of use case we haven't
talked about is like always on AI. We're doing this on many devices and you know, the idea would be
there is a there's a large engine that can do very complex AI, but there's also a very small
engine that we've introduced on products that uses less than a milliamp of current, a milliamp
year of current and it's always on, which means it can listen, it can take in multiple streams
coming into a product, for example, location stream, other streams to create a contextual picture
where the user is and based on that takes certain actions. So really the options are very wide
in what we are able to do in always on versus active like scenarios as well. And is the idea there,
the next evolution of the wake word like Alexa or Siri? Absolutely, but a lot more than that. See,
that is just taking the microphone input, but now you can also take in the camera stream.
You can take in the location stream, you can take in, you know, video memory,
and all the sensors that you have e-composed, gyro and all, you aggregate all of that and you can
now do some very interesting things already on the device. And have you seen any specific
applications that jump out of you? Yeah, so like there are some very good health and security
applications that already come to mind, right? If you are able to assess with certain degree of
accuracy, for example, that you are the driver of the car, there are certain functions that
automatically should not be available, for example. Or let's say if you know that based on the
timeline that there is a loud noise in the middle of the night, for example, you know that there
is an event that has happened that probably needs to be catered to, right? So there are some very
interesting ideas like that that are coming up. I mean, for example, if your phone is able to
determine that you're in a very noisy environment, it can automatically increase the volume of your
ringer, for example. Right, many convenience factors, but also health and security aspects there.
Yeah, we've talked about use cases like retail and smart cities and
yeah, even the always on AI application brings the question to mind. There's a growing concern
about surveillance and kind of pervasive cameras and things like that. And I'm curious what the
Qualcomm take is on that. Sure. I think definitely privacy comes first, right? We need to ensure that
our private data stays private. And what we are able to do in that regard is we have a very strong
security components that we build into each and every one of our products, stepdragon products.
And that is meant to make sure that the data that is supposed to stay on the device stays on the
device. And that's why I mentioned I think there is a very good benefit of having more AI capability
on the device because now we can do all of that processing on the device. The data doesn't need to go
anywhere. And you're able to actually make sense of it, especially data that has like let's say
your face information or anything that's very, very important from a value perspective should stay
on the device in that sense. So we've talked about a bunch of different areas and you know what's
available today and where things are going, you know, near term, you know, what are you most excited
about kind of looking into the future and the different applications you're seeing arising?
Sure. I think the part that I really get excited about, especially from an AI and its applications
is I think what I call AI for good. And there's a lot of potential there. So there's one initiative
that we had done with the Tata in India where basically you could take a smartphone and put a small
lens behind that. And then you could basically point it at the eye of a person and you'd be able
to glean information from the retina. And then you can actually run an algorithm where there might
be a location where there isn't a doctor close by. And it does a very coarse but a good assessment
of if there is any concern or let's say conditions like diabetic retinopathy, right? If any such
condition is coming in, it can automatically go on, it can tell you to go see a doctor. I think
there are many applications like that where as we start to get more AI processing on the device
which is able to monitor our health, I think there's a lot that we can do there. Make sense of
it just from the amount of information that's already there. If you have a smartwatch, you know,
you have your heart rate, but to be able to make sense of that in combination with all the other
factors that you have. Because for example, if you have a smartphone and a smartwatch,
now you can a certain very well if the person was standing and he's no longer standing for example,
right? There is a lot of such what I call AI for good. And I'm quite excited that it will
make life better. It will make the people's life safer, right? For example, cars.
As you do more this autonomy, I think autonomous cars will turn out to be a lot more safer than
human drivers. And if they can save lives, if they can make the experience, you know, safer for
especially teenagers, people who are just learning how to drive, I think that those are all,
you know, great things that I think if you're a technologist and if you can get something like
this, it's very heartening and it makes you feel good that your technology is going in a place
which makes human's lives better. Awesome. Awesome. Yeah, thanks so much for joining us and sharing
a little bit about what you and Qualcomm are up to. Thank you for having me. Enjoy the discussion
very much. Thank you. Thank you.
