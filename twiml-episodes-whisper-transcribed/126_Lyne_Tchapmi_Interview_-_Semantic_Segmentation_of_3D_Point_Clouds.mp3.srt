1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,440
I'm your host Sam Charrington. I'm recording this intro from San Jose, California, where

4
00:00:34,440 --> 00:00:40,760
I'm attending the NVIDIA GTC conference. NVIDIA CEO Jensen Huang made a bunch of interesting

5
00:00:40,760 --> 00:00:44,760
announcements at his keynote this morning, which I live tweeted from my Twitter account

6
00:00:44,760 --> 00:00:50,440
at Sam Charrington. I'll also be writing about my thoughts from GTC for an upcoming addition

7
00:00:50,440 --> 00:00:55,760
of my newsletter, which I'd encourage you to subscribe to by visiting twimbleai.com

8
00:00:55,760 --> 00:01:02,640
slash newsletter. In this episode, I'm joined by Lin Chapme, PhD student in the Stanford

9
00:01:02,640 --> 00:01:08,760
Computational Vision and Geometry Lab, to discuss her paper SIG Cloud, semantic segmentation

10
00:01:08,760 --> 00:01:15,800
of 3D point clouds. SIG Cloud is an end-to-end framework that performs 3D point level segmentation,

11
00:01:15,800 --> 00:01:20,680
combining the advantages of neural networks, trial linear interpolation, and fully connected

12
00:01:20,680 --> 00:01:26,360
conditional random fields. In our conversation, Lin and I cover the ins and outs of semantic

13
00:01:26,360 --> 00:01:32,080
segmentation, starting from the center data that we're trying to segment to 2D and 3D representations

14
00:01:32,080 --> 00:01:38,240
of that data, and how we go about automatically identifying classes. Along the way, we dig

15
00:01:38,240 --> 00:01:43,640
into some of the details, including how she obtained a more fine-grained labeling of points,

16
00:01:43,640 --> 00:01:49,000
and the transition from point clouds to voxels.

17
00:01:49,000 --> 00:01:54,480
Before we jump into the show, a few quick questions for you. Are you an IT technology or business

18
00:01:54,480 --> 00:01:59,400
leader who needs to get smart on the broad spectrum of machine learning and AI opportunities

19
00:01:59,400 --> 00:02:04,600
in the enterprise? Or perhaps someone in your organization could benefit from a level

20
00:02:04,600 --> 00:02:10,560
up in this area? Or maybe you would benefit from them leveling up? If this sounds like

21
00:02:10,560 --> 00:02:16,400
you or someone you know, you'll probably be interested in my upcoming AI Summit event.

22
00:02:16,400 --> 00:02:22,640
Think of the event as a two-day no-fluff technical MBA in machine learning and AI. You'll

23
00:02:22,640 --> 00:02:27,240
leave with a clear understanding of how machine learning and deep learning work, with no

24
00:02:27,240 --> 00:02:32,480
math required, how to identify machine learning and deep learning opportunities within your

25
00:02:32,480 --> 00:02:37,680
organization, how to understand and take advantage of technologies like computer vision and

26
00:02:37,680 --> 00:02:45,040
natural language processing, how to manage and label data to take advantage of ML&AI,

27
00:02:45,040 --> 00:02:51,240
and how to build an AI first culture and operationalize AI in your business. You'll have an informed

28
00:02:51,240 --> 00:02:56,240
perspective on what's going on across the ML&AI landscape, and you'll be able to engage

29
00:02:56,240 --> 00:03:00,720
confidently in discussions about machine learning and AI with your colleagues, customers,

30
00:03:00,720 --> 00:03:05,840
and partners. I am super excited about this event and the speakers will get a chance

31
00:03:05,840 --> 00:03:12,840
to learn from. For more information, visit twimlai.com slash AI Summit, and feel free to contact

32
00:03:12,840 --> 00:03:16,400
me with your questions. And now on to the show.

33
00:03:16,400 --> 00:03:28,840
All right, everyone. I am here at NIPS and I am with Lynn Chapme, who is a PhD student

34
00:03:28,840 --> 00:03:33,920
at Stanford in the Vision and Learning Lab. Lynn, welcome to the podcast.

35
00:03:33,920 --> 00:03:40,240
Thank you. How did you get involved in machine learning and artificial intelligence?

36
00:03:40,240 --> 00:03:47,160
So I have been passionate about science and mathematics from a very young age, and when

37
00:03:47,160 --> 00:03:54,360
I got to college, I started studying electrical engineering and computer science. I got involved

38
00:03:54,360 --> 00:03:55,360
with...

39
00:03:55,360 --> 00:03:56,360
And where was that?

40
00:03:56,360 --> 00:03:57,360
That was at MIT.

41
00:03:57,360 --> 00:03:58,360
Okay.

42
00:03:58,360 --> 00:04:07,560
So I participated in the Super-UR program there, which is an undergraduate research program.

43
00:04:07,560 --> 00:04:16,400
And as part of that program, I got to work on designing, building, and programming wireless

44
00:04:16,400 --> 00:04:22,880
health monitor that essentially keeps track of cardiovascular signals. It connects to the

45
00:04:22,880 --> 00:04:28,800
ear and connects that data and sends it to a smartphone. And what was that program called?

46
00:04:28,800 --> 00:04:29,800
Super-UR.

47
00:04:29,800 --> 00:04:30,800
Super-UR?

48
00:04:30,800 --> 00:04:33,960
Yes. That was super on the Graduate Research Opportunity program.

49
00:04:33,960 --> 00:04:34,960
Oh, okay.

50
00:04:34,960 --> 00:04:40,960
That's what I stands for. Yes. So it was a pretty exciting project, and I found out that

51
00:04:40,960 --> 00:04:43,960
I was really interested in both the hardware and the software side.

52
00:04:43,960 --> 00:04:44,960
Okay.

53
00:04:44,960 --> 00:04:50,560
As a result, I continued my study. Mostly on the hardware side, I took a bunch of circuit

54
00:04:50,560 --> 00:04:57,760
design courses, and continued doing more research. Eventually, I continued with my masters.

55
00:04:57,760 --> 00:05:01,760
So at MIT, doing a power electronics.

56
00:05:01,760 --> 00:05:02,760
Okay.

57
00:05:02,760 --> 00:05:10,480
So my master's thesis was on designing an efficient power converter for low power circuits that

58
00:05:10,480 --> 00:05:11,480
I use in mobile devices.

59
00:05:11,480 --> 00:05:13,960
So I just had this in small phones.

60
00:05:13,960 --> 00:05:14,960
Okay.

61
00:05:14,960 --> 00:05:21,760
So when I finished my masters, I liked research, so I decided I would apply for a PhD.

62
00:05:21,760 --> 00:05:27,040
I got into Stanford. And when I came in, initially, I thought that I was going to continue

63
00:05:27,040 --> 00:05:29,160
and work in power electronics.

64
00:05:29,160 --> 00:05:30,160
Okay.

65
00:05:30,160 --> 00:05:36,720
But Stanford has a PhD rotation program. So during your first year, you can spend all three

66
00:05:36,720 --> 00:05:38,800
quarters working in three different labs.

67
00:05:38,800 --> 00:05:39,800
Oh, really?

68
00:05:39,800 --> 00:05:43,480
Yes. So with three different professors, and it kind of gives students the opportunity

69
00:05:43,480 --> 00:05:48,480
to explore a little bit, all of the different professors, all of the different projects

70
00:05:48,480 --> 00:05:50,080
that are available.

71
00:05:50,080 --> 00:05:53,200
So I took advantage of the program.

72
00:05:53,200 --> 00:05:58,600
I worked with Professor Juan Rivas from the power electronics lab.

73
00:05:58,600 --> 00:06:03,320
Then I worked with Professor Sebastian Trond, who used to do robotics.

74
00:06:03,320 --> 00:06:08,360
And at the time, actually, was more focused on computer vision things.

75
00:06:08,360 --> 00:06:13,120
And that's when I learned a little bit about computer vision, thought I was interesting,

76
00:06:13,120 --> 00:06:15,360
started taking classes.

77
00:06:15,360 --> 00:06:20,280
And during my third rotation, I worked with Professor Sylvia Savarisa, who is currently

78
00:06:20,280 --> 00:06:21,280
my advisor.

79
00:06:21,280 --> 00:06:22,280
Okay.

80
00:06:22,280 --> 00:06:25,120
In his lab, that's when I learned about deep learning.

81
00:06:25,120 --> 00:06:30,240
I started exploring it a bit more, thought he was interesting.

82
00:06:30,240 --> 00:06:36,600
And I think the most fascinating aspect of it was that just the possibility of using the

83
00:06:36,600 --> 00:06:43,600
techniques and the tools that are available to solve various problems ranging from energy,

84
00:06:43,600 --> 00:06:48,200
finance, construction, medical.

85
00:06:48,200 --> 00:06:50,280
So I thought I was pretty exciting.

86
00:06:50,280 --> 00:06:51,280
Awesome.

87
00:06:51,280 --> 00:06:52,280
Awesome.

88
00:06:52,280 --> 00:06:54,880
Tell me a little bit about the current focus of your research.

89
00:06:54,880 --> 00:06:55,880
What are you working on?

90
00:06:55,880 --> 00:06:59,080
In fact, you've got a paper that you're presenting here at NEPS, right?

91
00:06:59,080 --> 00:07:00,080
Yes.

92
00:07:00,080 --> 00:07:03,520
So I focused on 3D scene understanding.

93
00:07:03,520 --> 00:07:04,520
Okay.

94
00:07:04,520 --> 00:07:13,120
So my goal is to take visual information about the 3D world and try to make sense of it.

95
00:07:13,120 --> 00:07:20,440
And the reason why such information is important is that if you have intelligent systems that

96
00:07:20,440 --> 00:07:25,040
are surrounding human beings and that are able, in order for them to be able to assist

97
00:07:25,040 --> 00:07:28,640
human beings, they need to be able to understand what is going on around us.

98
00:07:28,640 --> 00:07:35,560
And some of the applications of that are self-driving cars and just general assistance within

99
00:07:35,560 --> 00:07:36,560
the home.

100
00:07:36,560 --> 00:07:42,880
Let's say an elderly person who needs help with daily tasks or even a surgeon in the hospital

101
00:07:42,880 --> 00:07:47,320
that needs assistance, maybe for getting tools around and things like that.

102
00:07:47,320 --> 00:07:51,200
So I have been working.

103
00:07:51,200 --> 00:07:55,720
My recent paper is about semantic segmentation of 3D point clouds.

104
00:07:55,720 --> 00:08:02,800
So with sensors such as connect sensors, lighter sensors, you can acquire models of 3D environments.

105
00:08:02,800 --> 00:08:06,160
It could be indoor spaces or outdoor spaces.

106
00:08:06,160 --> 00:08:12,400
And that information is not meaningful in and of itself and using deep learning, we can

107
00:08:12,400 --> 00:08:14,120
make sense of it.

108
00:08:14,120 --> 00:08:20,680
And the goal of the project was essentially to identify all of the different elements that

109
00:08:20,680 --> 00:08:23,680
compose the data, the 3D data.

110
00:08:23,680 --> 00:08:29,560
In indoor spaces you might be interested in identifying the walls, the ceilings, the objects,

111
00:08:29,560 --> 00:08:32,160
the tables, chairs, and all that.

112
00:08:32,160 --> 00:08:39,680
And in outdoor spaces you might be interested in identifying roads, the cars, and buildings

113
00:08:39,680 --> 00:08:41,080
for instance.

114
00:08:41,080 --> 00:08:46,360
So the method that we develop is just before you go into the method to just to make sure

115
00:08:46,360 --> 00:08:47,360
I understand.

116
00:08:47,360 --> 00:08:52,240
I'm imagining we've got a connect sensor in here and it's giving us kind of this 3D point

117
00:08:52,240 --> 00:08:57,400
cloud of everything in this room, but it's just points.

118
00:08:57,400 --> 00:09:06,480
And so the points aren't distinguished from the table, the chairs, et cetera, the people.

119
00:09:06,480 --> 00:09:14,120
And so you're applying deep learning to on top of that raw data set, then categorize

120
00:09:14,120 --> 00:09:18,600
or is it a categorization problem for each of the points into some object and you don't

121
00:09:18,600 --> 00:09:24,560
know the objects that are in the field before you do this, right?

122
00:09:24,560 --> 00:09:29,640
Yes, we don't know the objects that are in the field, but we have a certain set of objects

123
00:09:29,640 --> 00:09:31,200
that we're interested in.

124
00:09:31,200 --> 00:09:39,440
So the thing with deep learning or a little supervised learning is that you need to know

125
00:09:39,440 --> 00:09:42,640
the classes that you are interested in ahead of time.

126
00:09:42,640 --> 00:09:46,920
So they're in order to first perform the annotation because the ground to this needed.

127
00:09:46,920 --> 00:09:51,360
So during the annotation process you essentially choose the classes that you are interested

128
00:09:51,360 --> 00:09:52,360
in.

129
00:09:52,360 --> 00:09:53,360
Okay.

130
00:09:53,360 --> 00:09:55,160
And that's what you use for training.

131
00:09:55,160 --> 00:09:59,680
So although you don't know all the classes that are available in your data set, there

132
00:09:59,680 --> 00:10:04,960
will be a specific set of classes that are used during training and during testing.

133
00:10:04,960 --> 00:10:10,000
And usually there's a default class that is called clutter or other.

134
00:10:10,000 --> 00:10:11,000
Okay.

135
00:10:11,000 --> 00:10:14,920
Which is used to identify anything else that was enabled in the data set.

136
00:10:14,920 --> 00:10:15,920
Okay.

137
00:10:15,920 --> 00:10:22,640
And so when you're identifying the classes that you expect, is this a small number of,

138
00:10:22,640 --> 00:10:26,720
you know, I'm expecting to see, you know, this is a home environment, you know, they're

139
00:10:26,720 --> 00:10:31,160
probably, you know, one of these 10 things or is it like, you know, a thousand classes

140
00:10:31,160 --> 00:10:33,720
all of image net or something like that?

141
00:10:33,720 --> 00:10:41,720
So at the level of 3D data, it's still a very small number of classes, in part because

142
00:10:41,720 --> 00:10:46,280
3D data is much more difficult to annotate.

143
00:10:46,280 --> 00:10:55,040
And so the number of classes range somewhere between 8 to 10 or 20 classes.

144
00:10:55,040 --> 00:10:56,880
So it's not yet in the hundreds.

145
00:10:56,880 --> 00:10:57,880
Okay.

146
00:10:57,880 --> 00:11:00,640
It's a lot more difficult to annotate 3D data.

147
00:11:00,640 --> 00:11:07,920
And the fact that sensors, depending on what modality you annotated as well, the sensor

148
00:11:07,920 --> 00:11:09,800
data could also be very noisy.

149
00:11:09,800 --> 00:11:13,640
So it could be difficult to identify certain classes.

150
00:11:13,640 --> 00:11:19,080
So and that's one of the challenges with 3D data compared to images.

151
00:11:19,080 --> 00:11:21,880
Images are much easier to acquire.

152
00:11:21,880 --> 00:11:27,360
And pretty much everyone today has a cell phone, a camera, so there's a lot of images

153
00:11:27,360 --> 00:11:30,960
that are being created at every second.

154
00:11:30,960 --> 00:11:33,720
And the same, it doesn't apply to 3D data.

155
00:11:33,720 --> 00:11:40,200
You know, everyone has a connect sensor or a lighter that they use on a daily basis essentially.

156
00:11:40,200 --> 00:11:46,200
So 3D data is much less available and then you need those specialized tools to really

157
00:11:46,200 --> 00:11:47,200
annotate them.

158
00:11:47,200 --> 00:11:49,240
So it's a bit more difficult.

159
00:11:49,240 --> 00:11:56,920
And when you say annotate, are you talking about using your model to annotate or are you

160
00:11:56,920 --> 00:12:07,800
talking about the pre-training task of having, you know, creating the training set by annotating

161
00:12:07,800 --> 00:12:08,800
sample data?

162
00:12:08,800 --> 00:12:09,800
Yeah.

163
00:12:09,800 --> 00:12:14,680
So I'm talking about the pre, the task before training, the task of generating the data

164
00:12:14,680 --> 00:12:19,320
I say itself in the annotations that are going to be used for training.

165
00:12:19,320 --> 00:12:25,000
So that is the most challenging part around 3D computer vision.

166
00:12:25,000 --> 00:12:30,400
You know, even for 2D data, right?

167
00:12:30,400 --> 00:12:34,640
My sense is that the folks that are doing annotation at scale have all kind of come up with

168
00:12:34,640 --> 00:12:36,400
their own systems for doing this.

169
00:12:36,400 --> 00:12:40,160
There's not like an off-the-shelf open source annotation toolkit.

170
00:12:40,160 --> 00:12:42,720
I think there should be.

171
00:12:42,720 --> 00:12:45,240
But I haven't really seen anything like that.

172
00:12:45,240 --> 00:12:47,320
There are various ones that are out there.

173
00:12:47,320 --> 00:12:48,320
Are they really?

174
00:12:48,320 --> 00:12:50,520
Yeah, they definitely are.

175
00:12:50,520 --> 00:12:57,000
I think one of the, one that I can think of of the top of my head is called LabelMe.

176
00:12:57,000 --> 00:13:01,240
So it's available to the research community and a lot of people use it.

177
00:13:01,240 --> 00:13:02,240
Oh, okay.

178
00:13:02,240 --> 00:13:03,240
Yeah.

179
00:13:03,240 --> 00:13:10,040
So there definitely are four images and also for 3D data, more recently there was an annotation

180
00:13:10,040 --> 00:13:14,400
tool called scannet that was released to annotate 3D data.

181
00:13:14,400 --> 00:13:16,800
So it makes the task a lot more easier.

182
00:13:16,800 --> 00:13:17,800
Okay.

183
00:13:17,800 --> 00:13:25,400
Essentially, the task is to paint over a bunch of segments in 3D and choose a class for

184
00:13:25,400 --> 00:13:27,080
the segments that I painted over.

185
00:13:27,080 --> 00:13:30,160
So it makes the task a lot more, a lot easier.

186
00:13:30,160 --> 00:13:36,640
But even then, it sounds like there's, you've got all kinds of ambiguity because you're

187
00:13:36,640 --> 00:13:40,360
painting in 2D over this 3D point cloud.

188
00:13:40,360 --> 00:13:41,360
Is that right?

189
00:13:41,360 --> 00:13:43,880
Or are you doing like rotations and all that kind of stuff?

190
00:13:43,880 --> 00:13:44,880
Yeah.

191
00:13:44,880 --> 00:13:46,720
Scannet actually allows you to paint in 3D.

192
00:13:46,720 --> 00:13:47,720
Oh, really?

193
00:13:47,720 --> 00:13:48,720
Okay.

194
00:13:48,720 --> 00:13:53,880
So that is, it's more suitable for 3D data.

195
00:13:53,880 --> 00:14:02,320
The challenge there is that the reconstruction that is used as the basis of the annotation needs

196
00:14:02,320 --> 00:14:07,600
to be a very high quality, which is not always the case because the sensors, the 3D sensors

197
00:14:07,600 --> 00:14:09,600
are noisy.

198
00:14:09,600 --> 00:14:14,720
And what happens is that you're able to annotate the bigger objects that are more visible.

199
00:14:14,720 --> 00:14:15,720
Okay.

200
00:14:15,720 --> 00:14:25,800
So you can easily paint the table, but painting a bottle of water on the table is going to be

201
00:14:25,800 --> 00:14:29,800
a lot harder because points will be missing or that kind of thing.

202
00:14:29,800 --> 00:14:30,800
Exactly.

203
00:14:30,800 --> 00:14:32,760
Whereas on an image, it's much easier to do that.

204
00:14:32,760 --> 00:14:33,760
Right.

205
00:14:33,760 --> 00:14:34,760
Yeah.

206
00:14:34,760 --> 00:14:35,760
Well, interesting.

207
00:14:35,760 --> 00:14:39,200
I didn't realize that, you know, I've had this conversation with a bunch of folks that

208
00:14:39,200 --> 00:14:42,480
are working on this and they all say, no, no, there's nothing really out there.

209
00:14:42,480 --> 00:14:44,240
We had to build it ourselves.

210
00:14:44,240 --> 00:14:50,160
There are some tools, depending on the quality, there are mostly research tools.

211
00:14:50,160 --> 00:14:51,160
Okay.

212
00:14:51,160 --> 00:14:55,800
So they may not be very thoroughly developed if you want to use it for production.

213
00:14:55,800 --> 00:14:57,280
Maybe it's not what you want to use.

214
00:14:57,280 --> 00:14:58,280
Right.

215
00:14:58,280 --> 00:15:01,160
But there are tools that are used by the research community.

216
00:15:01,160 --> 00:15:02,160
Okay.

217
00:15:02,160 --> 00:15:03,160
Yeah.

218
00:15:03,160 --> 00:15:09,760
But I imagine then that, you know, even, yeah, if you're trying to do 3D annotation,

219
00:15:09,760 --> 00:15:16,560
is it, I'm imagining that it's harder to, like, get a mechanical turquer to do 3D labeling,

220
00:15:16,560 --> 00:15:20,440
you know, A because, you know, there's probably some kind of special plug-in that has to

221
00:15:20,440 --> 00:15:26,080
be used if you can even do it over the web, but then B, just the complexity of, like, the

222
00:15:26,080 --> 00:15:29,000
instructions you give and that kind of thing.

223
00:15:29,000 --> 00:15:30,680
It sounds very difficult.

224
00:15:30,680 --> 00:15:36,280
It likely is slightly more difficult than images, but it is doable.

225
00:15:36,280 --> 00:15:37,280
Okay.

226
00:15:37,280 --> 00:15:41,680
So the tool that I mentioned scan it, that's essentially what they did.

227
00:15:41,680 --> 00:15:48,280
They crowdsourced both the data collection process and the annotation process.

228
00:15:48,280 --> 00:15:49,280
Okay.

229
00:15:49,280 --> 00:15:55,320
So they had people, they had turkers, I'm not sure who was turkers, but they had workers

230
00:15:55,320 --> 00:16:00,360
actually annotates using their tool, they gave them instructions and the annotations turned

231
00:16:00,360 --> 00:16:01,360
out pretty decent.

232
00:16:01,360 --> 00:16:02,360
Okay.

233
00:16:02,360 --> 00:16:03,360
Yeah.

234
00:16:03,360 --> 00:16:04,360
Interesting.

235
00:16:04,360 --> 00:16:10,320
We've got the, you know, we've got some training data assembled, you know, tell us about

236
00:16:10,320 --> 00:16:15,680
your process and how you went from that to point segmentation.

237
00:16:15,680 --> 00:16:16,680
Okay.

238
00:16:16,680 --> 00:16:24,240
So essentially for each point in the 3D data in the point cloud, we want to assign it label.

239
00:16:24,240 --> 00:16:30,840
And we wanted to leverage convolutional neural networks since they've been shown to work

240
00:16:30,840 --> 00:16:32,840
really well for images.

241
00:16:32,840 --> 00:16:39,360
The data representation for 3D point clouds is not directly suitable for convolutional

242
00:16:39,360 --> 00:16:40,680
neural networks.

243
00:16:40,680 --> 00:16:51,000
So we have it because think about an image, an image is a 2D array and it has a level

244
00:16:51,000 --> 00:16:54,360
of structure that is suitable for convolutional neural networks.

245
00:16:54,360 --> 00:17:00,520
So CNN's kind of take advantage of that structure in the image through the convolutions.

246
00:17:00,520 --> 00:17:02,400
So you've got an array of pixels?

247
00:17:02,400 --> 00:17:03,400
Yes.

248
00:17:03,400 --> 00:17:07,520
And kind of mapping that to a point cloud at any level of resolution, you could have multiple

249
00:17:07,520 --> 00:17:09,560
points in kind of a box.

250
00:17:09,560 --> 00:17:11,680
So how would you translate it?

251
00:17:11,680 --> 00:17:16,160
So the point clouds, when they actually are collected, they come in as a 2D array, but

252
00:17:16,160 --> 00:17:19,320
it's more, it's not structured.

253
00:17:19,320 --> 00:17:20,320
Okay.

254
00:17:20,320 --> 00:17:25,320
So it's a set of XYZ coordinates with RGB color.

255
00:17:25,320 --> 00:17:26,320
Okay.

256
00:17:26,320 --> 00:17:27,400
And so it's a bit less structured.

257
00:17:27,400 --> 00:17:35,000
So there is no, unless you go to the process of oxidization, which is what we did.

258
00:17:35,000 --> 00:17:36,000
Of what?

259
00:17:36,000 --> 00:17:37,000
Of oxidization.

260
00:17:37,000 --> 00:17:40,200
So essentially turning that representation into a grid.

261
00:17:40,200 --> 00:17:41,200
Okay.

262
00:17:41,200 --> 00:17:48,320
So, and the way to do that essentially is to look at the 3D space and divide the 3D space

263
00:17:48,320 --> 00:17:56,440
into a grid essentially and assign, turn your XYZ RGB representation into a different

264
00:17:56,440 --> 00:17:59,880
representation, which has four channels.

265
00:17:59,880 --> 00:18:04,560
The first channel is occupancy, which is zero or one.

266
00:18:04,560 --> 00:18:08,440
And if it's a one, then it means that that space is occupied.

267
00:18:08,440 --> 00:18:11,480
And zero is not occupied.

268
00:18:11,480 --> 00:18:16,880
And the last three channels are not the RGB color.

269
00:18:16,880 --> 00:18:17,880
The RGB.

270
00:18:17,880 --> 00:18:18,880
Yes.

271
00:18:18,880 --> 00:18:22,720
Which usually is the average of all the points that are within the unit grid cell, which

272
00:18:22,720 --> 00:18:23,920
are a color box.

273
00:18:23,920 --> 00:18:24,920
Okay.

274
00:18:24,920 --> 00:18:25,920
Yeah.

275
00:18:25,920 --> 00:18:29,480
So you have to make that transition from point cloud to voxel.

276
00:18:29,480 --> 00:18:33,600
And the voxel usually has a predefined size in our paper.

277
00:18:33,600 --> 00:18:36,560
We start out with five centimeters.

278
00:18:36,560 --> 00:18:43,640
And there is another difficulty with 3D data, which is the added dimensionality.

279
00:18:43,640 --> 00:18:47,360
So in 2D, you just have two dimensions plus the channels.

280
00:18:47,360 --> 00:18:51,840
In 3D, you have the added depth dimension.

281
00:18:51,840 --> 00:18:58,280
So that kind of scales the problem and makes it a bit more difficult to solve in terms

282
00:18:58,280 --> 00:18:59,280
of memory.

283
00:18:59,280 --> 00:19:03,880
So 3D data would take more space when you're using CNNs.

284
00:19:03,880 --> 00:19:08,360
And for this reason, when we're using traditional convolutional neural networks, we actually

285
00:19:08,360 --> 00:19:11,960
have to down sample our input volume further.

286
00:19:11,960 --> 00:19:17,240
So we start out with the five centimeter voxel and we down sample it through the CNN.

287
00:19:17,240 --> 00:19:21,880
So the CNN would down sample the volume to 20 centimeter voxel.

288
00:19:21,880 --> 00:19:31,080
So at the end, what the CNN outputs is a label over 20 centimeter grid unit, which is

289
00:19:31,080 --> 00:19:32,080
relatively coarse.

290
00:19:32,080 --> 00:19:33,080
Yeah.

291
00:19:33,080 --> 00:19:34,080
Exactly.

292
00:19:34,080 --> 00:19:42,280
So what our paper explored is how can we obtain still average CNN, how can we obtain a

293
00:19:42,280 --> 00:19:47,760
more fine green labeling of our point car, how can we label the individual points which

294
00:19:47,760 --> 00:19:50,600
come from the original sensor.

295
00:19:50,600 --> 00:19:56,600
And to do that, we design a trial and error interpolation layer that a trial and error interpolation

296
00:19:56,600 --> 00:19:57,600
layer.

297
00:19:57,600 --> 00:19:59,120
A trial and error interpolation layer.

298
00:19:59,120 --> 00:20:05,120
So we use it to train during training to train the CNN and to end.

299
00:20:05,120 --> 00:20:12,240
And basically what it does is the CNN outputs a set of scores for each voxel in space.

300
00:20:12,240 --> 00:20:17,640
For each point in space, we look at the eight closest voxel centers.

301
00:20:17,640 --> 00:20:23,480
And we compute the score of the point by weighing the combination of the ways that are coming

302
00:20:23,480 --> 00:20:25,920
from the eight nearest voxels.

303
00:20:25,920 --> 00:20:30,600
So you can think of the closest voxel will be contributing more to the final score at

304
00:20:30,600 --> 00:20:34,400
the point than the voxels that are a bit further away from the point.

305
00:20:34,400 --> 00:20:38,880
And so we use that during training and the alternative to using.

306
00:20:38,880 --> 00:20:44,160
And now is this the eight closest occupied voxels or just the eight closest voxels?

307
00:20:44,160 --> 00:20:47,680
The eight closest occupied voxels, yes.

308
00:20:47,680 --> 00:20:56,400
And so during training, the alternative to doing that is simply doing a nearest neighbor.

309
00:20:56,400 --> 00:21:00,560
So taking the closest voxel and assigning that label to the point.

310
00:21:00,560 --> 00:21:05,040
And during training, we find out that actually using trial and error interpolation layer

311
00:21:05,040 --> 00:21:08,800
performs a lot better than using a nearest neighbor.

312
00:21:08,800 --> 00:21:14,440
So that was the first module that we added to our CNN.

313
00:21:14,440 --> 00:21:20,040
And in the end, we also used a conditional random field to refine the predictions that

314
00:21:20,040 --> 00:21:24,400
are given to us by the CNN and the trial and error interpolation layer.

315
00:21:24,400 --> 00:21:32,120
And so the conditional random field essentially defines an energy function that enforces consistency

316
00:21:32,120 --> 00:21:34,840
between labeling intuitively.

317
00:21:34,840 --> 00:21:41,120
You can think, if you take two points in space, if those points are close to each other,

318
00:21:41,120 --> 00:21:45,440
then they should, the probability that they have the same label is high.

319
00:21:45,440 --> 00:21:51,760
And so the energy function of the CRF enforces that close points will have, are more likely

320
00:21:51,760 --> 00:21:53,960
to have similar labels.

321
00:21:53,960 --> 00:21:59,880
And so without energy function, we are able to refine our predictions.

322
00:21:59,880 --> 00:22:07,520
And we use a nice implementation of conditional random fields that was published in a 2015

323
00:22:07,520 --> 00:22:10,280
paper called CRF as RNN.

324
00:22:10,280 --> 00:22:14,720
So the CRF is implemented as a recurrent neural network.

325
00:22:14,720 --> 00:22:20,640
And the advantage of that implementation is that you can actually combine both the CNN,

326
00:22:20,640 --> 00:22:24,720
the trial and error interpolation layer and the CRF and train them into N.

327
00:22:24,720 --> 00:22:25,720
Okay.

328
00:22:25,720 --> 00:22:30,720
It's a nice module that can be used for semantic segmentation.

329
00:22:30,720 --> 00:22:31,720
Okay.

330
00:22:31,720 --> 00:22:32,720
Yes.

331
00:22:32,720 --> 00:22:33,720
Interesting.

332
00:22:33,720 --> 00:22:39,000
And so what kind of, what kind of results have you seen with that?

333
00:22:39,000 --> 00:22:40,240
We've seen great results.

334
00:22:40,240 --> 00:22:46,440
So we evaluated framework on the Stanford Indoor Space Data Set.

335
00:22:46,440 --> 00:22:52,080
And we performed the state of the art on that data set, which was pointed.

336
00:22:52,080 --> 00:23:01,160
And currently we also have, we also tested on the semantic 3D net data set, which is the

337
00:23:01,160 --> 00:23:05,560
largest outdoor point cloud data set that is available.

338
00:23:05,560 --> 00:23:09,120
And at the time of publication, we're also the state of the art, but I think there is

339
00:23:09,120 --> 00:23:12,520
a better method now because that's what happens.

340
00:23:12,520 --> 00:23:13,520
But yeah.

341
00:23:13,520 --> 00:23:20,520
So currently we are, I guess the second best method performing, the second best performing

342
00:23:20,520 --> 00:23:22,280
method on those data sets.

343
00:23:22,280 --> 00:23:23,280
Okay.

344
00:23:23,280 --> 00:23:30,480
And for these data sets, was the improvement like order of magnitude or was it like huge

345
00:23:30,480 --> 00:23:36,040
or was it incremental improvements relative to the prior state of the art?

346
00:23:36,040 --> 00:23:40,800
So depending on what we saw was that the best improvement that we got was on the largest

347
00:23:40,800 --> 00:23:41,800
data set.

348
00:23:41,800 --> 00:23:42,800
Okay.

349
00:23:42,800 --> 00:23:49,960
So we had about, I cannot remember the actual numbers, but it was more significant on

350
00:23:49,960 --> 00:23:52,160
the larger outdoor point cloud data set.

351
00:23:52,160 --> 00:23:53,160
Okay.

352
00:23:53,160 --> 00:24:01,200
There was also a pretty large gap between on the, on the, on the smaller indoor spaces data

353
00:24:01,200 --> 00:24:02,200
set.

354
00:24:02,200 --> 00:24:06,960
But the data set is also relatively small.

355
00:24:06,960 --> 00:24:11,280
And so the performance, the absolute performance is still a little bit low.

356
00:24:11,280 --> 00:24:17,640
And I think as we get more data, the numbers should get better, yeah.

357
00:24:17,640 --> 00:24:22,000
Now this conversation reminds me of another conversation that I've had here at NIPS.

358
00:24:22,000 --> 00:24:24,440
But it doesn't remind me of it.

359
00:24:24,440 --> 00:24:28,880
It is potentially related to another conversation that I've had here at NIPS.

360
00:24:28,880 --> 00:24:35,120
And the, the idea with the research they were doing is that there are, like here you've

361
00:24:35,120 --> 00:24:42,560
had to, you know, a lot of what your research focus was was kind of fitting or transforming

362
00:24:42,560 --> 00:24:49,880
this point cloud to the, the voxels and potentially losing the relationship between some of the,

363
00:24:49,880 --> 00:24:53,000
the individual points in the cloud.

364
00:24:53,000 --> 00:25:03,000
And they, this was an interview as Joanne Bruna at NYU and Michael Browstein and they are

365
00:25:03,000 --> 00:25:08,400
developing, or they're researching algorithms that take more like a graph approach.

366
00:25:08,400 --> 00:25:09,400
Okay.

367
00:25:09,400 --> 00:25:15,680
And the individual point would be seen as a graph as opposed to points in a Euclidean

368
00:25:15,680 --> 00:25:16,680
space.

369
00:25:16,680 --> 00:25:17,680
Okay.

370
00:25:17,680 --> 00:25:20,640
Have you looked at that kind of approach to doing segmentation?

371
00:25:20,640 --> 00:25:28,040
I have not personally, but one, I think it's a promising approach as well.

372
00:25:28,040 --> 00:25:33,880
We do use some amount of graphical representation in our method, not at the level of the CNN

373
00:25:33,880 --> 00:25:37,160
but more at the final module, which is the conditional random field.

374
00:25:37,160 --> 00:25:38,160
Okay.

375
00:25:38,160 --> 00:25:43,760
So that's back to our initial representation, but I do think that approach also is likely

376
00:25:43,760 --> 00:25:44,760
promising.

377
00:25:44,760 --> 00:25:45,760
Yeah.

378
00:25:45,760 --> 00:25:46,760
Interesting.

379
00:25:46,760 --> 00:25:50,280
I guess that's the great thing about Nips is that you get exposure to all of these different

380
00:25:50,280 --> 00:25:55,240
folks working on kind of related things and can, you know, try to pull together different

381
00:25:55,240 --> 00:25:56,240
ideas.

382
00:25:56,240 --> 00:25:57,240
Yeah.

383
00:25:57,240 --> 00:26:00,480
There's definitely a lot of work that is being done in this field.

384
00:26:00,480 --> 00:26:04,800
People are looking into different ways of processing the data.

385
00:26:04,800 --> 00:26:12,880
So one of the papers that we compared to was Poinets, which essentially proposed to process

386
00:26:12,880 --> 00:26:16,080
the points directly rather than doing voxalization.

387
00:26:16,080 --> 00:26:21,760
So they're definitely alternatives to skipping voxalization.

388
00:26:21,760 --> 00:26:29,120
The framework that we have in a way, it could, the conditional random field could still

389
00:26:29,120 --> 00:26:34,440
apply to some of those methods, but definitely the point representation is flexible and

390
00:26:34,440 --> 00:26:37,640
there are several ways that people could do it.

391
00:26:37,640 --> 00:26:42,440
There, for instance, also, are more efficient ways to do convolution.

392
00:26:42,440 --> 00:26:48,400
There's sparse convolutions that could be done so that could help with efficiency.

393
00:26:48,400 --> 00:26:53,760
The opnet is one of the papers that export that direction essentially.

394
00:26:53,760 --> 00:26:54,760
All of it.

395
00:26:54,760 --> 00:26:55,760
Opnet.

396
00:26:55,760 --> 00:26:56,760
Yes.

397
00:26:56,760 --> 00:26:57,760
Okay.

398
00:26:57,760 --> 00:27:03,120
So it's actually trying to perform the convolution operations on the spaces that are occupied

399
00:27:03,120 --> 00:27:05,360
rather than the entire volume.

400
00:27:05,360 --> 00:27:11,400
So they're definitely various approaches to solve this problem, so interesting.

401
00:27:11,400 --> 00:27:19,280
With the conditional random field and the energy function, actually, that's not an area

402
00:27:19,280 --> 00:27:21,280
that I'm well versed in.

403
00:27:21,280 --> 00:27:26,440
Maybe you can talk a little bit more about that, but I'm specifically wondering is the thing

404
00:27:26,440 --> 00:27:32,240
that you did that was interesting was applying conditional random fields and energy functions

405
00:27:32,240 --> 00:27:36,920
or coming up with a specific energy function that worked well in this case.

406
00:27:36,920 --> 00:27:42,400
So the energy function itself was standard except for the fact that it was applied on

407
00:27:42,400 --> 00:27:46,520
3D points rather than traditionally on images.

408
00:27:46,520 --> 00:27:54,800
So we, one of the novel things was that we applied it on the point at the point level.

409
00:27:54,800 --> 00:27:55,800
Okay.

410
00:27:55,800 --> 00:28:03,920
So the end part of our framework essentially is different in that we're not operating

411
00:28:03,920 --> 00:28:05,320
in voxel space.

412
00:28:05,320 --> 00:28:12,640
So traditionally when you're using the CNN and adding a CRF to it, usually the input does

413
00:28:12,640 --> 00:28:15,680
not change from start to end input space.

414
00:28:15,680 --> 00:28:21,880
We sort of have this interpolation there that helps us to go from a discrete space to a

415
00:28:21,880 --> 00:28:28,520
continuous space and we do further processing within our continuous space with the CRF as

416
00:28:28,520 --> 00:28:31,360
well as the tri-linear interpolation layer.

417
00:28:31,360 --> 00:28:40,080
In terms of the energy potential that we use, we use XYZ coordinates and RGB color as

418
00:28:40,080 --> 00:28:41,320
the features.

419
00:28:41,320 --> 00:28:47,080
So the energy function itself has two different terms, the unit potentials as well as the pair

420
00:28:47,080 --> 00:28:54,120
white potentials. So for the unit potentials, those come directly from the CNN true, the

421
00:28:54,120 --> 00:29:02,040
tri-linear interpolation layer and that sort of represents the initial belief of the system

422
00:29:02,040 --> 00:29:06,240
about the classes of each point.

423
00:29:06,240 --> 00:29:11,440
And then you have the pair white potentials that are also added to the energy function and

424
00:29:11,440 --> 00:29:17,320
those are the potentials that ensure the consistency between the neighborhood of each point.

425
00:29:17,320 --> 00:29:20,000
And is that the piece that you mentioned was kind of more graph-like?

426
00:29:20,000 --> 00:29:21,000
Yes, exactly.

427
00:29:21,000 --> 00:29:22,000
Representation.

428
00:29:22,000 --> 00:29:23,000
Yes, exactly.

429
00:29:23,000 --> 00:29:26,680
So we use a fully connected conditional random field.

430
00:29:26,680 --> 00:29:30,200
So it represents relationships between each point and its neighborhood.

431
00:29:30,200 --> 00:29:34,560
In this case, because it's fully connected, it actually has a connection between each point

432
00:29:34,560 --> 00:29:37,160
and every other point in the point cloud.

433
00:29:37,160 --> 00:29:40,440
So that's the graph representation that we're using.

434
00:29:40,440 --> 00:29:41,440
Yeah.

435
00:29:41,440 --> 00:29:44,160
So what's next for this research?

436
00:29:44,160 --> 00:29:52,560
So this research is actually a small subset of a bigger set of problems that I am exploring.

437
00:29:52,560 --> 00:29:59,760
So I am interested in, as I said earlier, I'm interested in trying to see how we can enable

438
00:29:59,760 --> 00:30:03,480
intelligence systems to assist human beings in their environments.

439
00:30:03,480 --> 00:30:06,480
So the first subset of that is seen understanding.

440
00:30:06,480 --> 00:30:07,480
Okay.

441
00:30:07,480 --> 00:30:08,480
What's on the environment?

442
00:30:08,480 --> 00:30:10,480
Exactly.

443
00:30:10,480 --> 00:30:20,800
You can imagine if you give a robot in order to go to the bedroom and get a book or get

444
00:30:20,800 --> 00:30:26,320
the Harry Potter book from my bookshelf, what does the robot need to do in order to accomplish

445
00:30:26,320 --> 00:30:27,320
that task?

446
00:30:27,320 --> 00:30:30,280
There's various fields that come into that.

447
00:30:30,280 --> 00:30:34,400
There's natural language processing, there's vision, there's robotics.

448
00:30:34,400 --> 00:30:40,800
And so the natural language processing to first understand the command, the visual part,

449
00:30:40,800 --> 00:30:47,200
the perception part to navigate through the room, get to the room, identify the object,

450
00:30:47,200 --> 00:30:52,240
and finally, the robotics part to actually do the interaction and pick up the objects.

451
00:30:52,240 --> 00:31:01,320
So I started working on the scene understanding within in terms of a more detailed scene understanding

452
00:31:01,320 --> 00:31:07,360
identifying objects, but there's also a higher level scene understanding that is involved

453
00:31:07,360 --> 00:31:14,400
in those kinds of assistance, which is how do you identify the different places within

454
00:31:14,400 --> 00:31:15,640
a building.

455
00:31:15,640 --> 00:31:18,080
So how do you know where the bedroom is?

456
00:31:18,080 --> 00:31:22,960
And so when we take those 3D point clouds, that information also is not available.

457
00:31:22,960 --> 00:31:30,000
So I am interested in looking at getting the higher level understanding of scenes as well

458
00:31:30,000 --> 00:31:42,080
and specifically using neural networks to obtain a navigation focused floor map of environments.

459
00:31:42,080 --> 00:31:48,480
So there has been work done in getting floor maps of environments mainly segmenting rooms

460
00:31:48,480 --> 00:31:53,080
between each other, so figuring out the separation between rooms.

461
00:31:53,080 --> 00:31:59,280
But in order for those to be a helpful navigation, we need more information.

462
00:31:59,280 --> 00:32:02,200
We need the identity of rooms.

463
00:32:02,200 --> 00:32:08,320
So we need to know that this is a living room, a bedroom, bathroom, and so on and so forth.

464
00:32:08,320 --> 00:32:14,760
And in addition, potentially, we need to know the location of entrances to each room.

465
00:32:14,760 --> 00:32:19,600
So once you have that kind of information, you cannot plan navigation to go to a specific

466
00:32:19,600 --> 00:32:20,680
room.

467
00:32:20,680 --> 00:32:25,840
So those are the two parts of scene understanding that I'm interested in.

468
00:32:25,840 --> 00:32:31,240
So getting a high level understanding of scenes as well as the more detailed object level

469
00:32:31,240 --> 00:32:33,240
understanding of the scenes.

470
00:32:33,240 --> 00:32:37,640
So for the higher level piece, there's been, as you mentioned, there's been a ton of

471
00:32:37,640 --> 00:32:43,720
work that's happened, a lot of it in the robotics community in terms of mapping environments

472
00:32:43,720 --> 00:32:54,840
and things like that is part of what you're trying to do to be able to identify, identify

473
00:32:54,840 --> 00:33:00,240
room, for example, based on what's in the room, as opposed to being given a specific label

474
00:33:00,240 --> 00:33:01,240
or something like that.

475
00:33:01,240 --> 00:33:05,840
Yeah, the idea is to identify the room based on what is in the room, based on the visual

476
00:33:05,840 --> 00:33:13,760
information, but also at the 3D level, because there is work that can identify a room based

477
00:33:13,760 --> 00:33:16,680
on an image of the room.

478
00:33:16,680 --> 00:33:24,120
But if you are in one location of a building and you want to get to the other location,

479
00:33:24,120 --> 00:33:26,680
that kind of information is not going to be very helpful.

480
00:33:26,680 --> 00:33:33,560
You need to have a map of the environment and know where each room is located.

481
00:33:33,560 --> 00:33:38,720
And so we're doing that more at the 3D level and at a higher perspective added in using

482
00:33:38,720 --> 00:33:39,720
images.

483
00:33:39,720 --> 00:33:40,720
Yeah.

484
00:33:40,720 --> 00:33:47,280
And in practice, do you envision that a system like this would be used dynamically or

485
00:33:47,280 --> 00:33:48,280
statically?

486
00:33:48,280 --> 00:33:54,080
And what I mean is, are we going to have like a mini light on the robot and as it navigates

487
00:33:54,080 --> 00:34:01,600
the environment, it's making determinations about what the room is, or more like in the

488
00:34:01,600 --> 00:34:06,040
future, all buildings will have, you know, they'll come with 3D point clouds or something

489
00:34:06,040 --> 00:34:08,800
on whatever the future version of a floppy disk is.

490
00:34:08,800 --> 00:34:09,800
Okay.

491
00:34:09,800 --> 00:34:15,920
I think there would be an initial map of 3D environments.

492
00:34:15,920 --> 00:34:22,320
And as the robot navigates around the environment, then it can update what the map looks like,

493
00:34:22,320 --> 00:34:24,240
at a more local level.

494
00:34:24,240 --> 00:34:29,360
So if you have an initial state of a room and you pass at a certain point and you see

495
00:34:29,360 --> 00:34:33,800
that something has changed, then you can update that in the internal map that you have.

496
00:34:33,800 --> 00:34:41,800
So it doesn't need to be updated in real time, well, the entire map does not need to be

497
00:34:41,800 --> 00:34:48,040
updated always, but there needs to be an initial starting point that can be updated locally

498
00:34:48,040 --> 00:34:53,360
as the, as more information is gathered over time.

499
00:34:53,360 --> 00:35:01,320
So the, the robot would know the kind of overall 3D structure of the environment, but might

500
00:35:01,320 --> 00:35:06,800
be able to infer that, well, you removed the bed and put a desk, so this is an office

501
00:35:06,800 --> 00:35:08,880
now as opposed to a bedroom.

502
00:35:08,880 --> 00:35:09,880
Exactly.

503
00:35:09,880 --> 00:35:16,360
So I guess for an office to change, so a bedroom there would probably be a lot more than

504
00:35:16,360 --> 00:35:24,000
like one single change, but even imagining that a room, the room state is still the same,

505
00:35:24,000 --> 00:35:27,040
but just a chair was moved to a different location.

506
00:35:27,040 --> 00:35:31,360
That can affect navigation planning, for instance, and that's something that is worth

507
00:35:31,360 --> 00:35:35,120
updating as well.

508
00:35:35,120 --> 00:35:42,200
And then, you know, it coming from a group that's that's largely focused on vision or is

509
00:35:42,200 --> 00:35:47,280
your line of research like, in a sense, swimming against the tide, like I get the sense that

510
00:35:47,280 --> 00:35:52,200
I get the sense that, you know, vision folks think that everything will be solved through

511
00:35:52,200 --> 00:35:57,480
cameras, as opposed to, you know, point clouds and things like that.

512
00:35:57,480 --> 00:36:04,080
Well, I would say, in a way, if you think about getting the cameras, they, in a sense,

513
00:36:04,080 --> 00:36:11,720
are also the cameras that give you 2D data and you can transform it into 3D point cloud.

514
00:36:11,720 --> 00:36:17,560
I think the sort of complementary, I think 3D can come from images.

515
00:36:17,560 --> 00:36:23,120
So in a way, you could say that you're using images, if you're, I guess, working mainly

516
00:36:23,120 --> 00:36:25,160
with RGBD data.

517
00:36:25,160 --> 00:36:32,320
In terms of lighter, I think, I guess there is a use for longer range sensors such as

518
00:36:32,320 --> 00:36:33,320
sliders.

519
00:36:33,320 --> 00:36:37,240
So they can allow you to detect objects that are further apart.

520
00:36:37,240 --> 00:36:44,960
And I, you can also, you can try to use reconstruction algorithms, but they're not always as effective

521
00:36:44,960 --> 00:36:50,960
as the more longer, as longer range sensors are in terms of data collection.

522
00:36:50,960 --> 00:36:58,000
So I think both will have a role to play in terms of solving these problems and maybe

523
00:36:58,000 --> 00:37:02,720
in different applications, images would be better than long-range sensors.

524
00:37:02,720 --> 00:37:03,720
Okay.

525
00:37:03,720 --> 00:37:04,720
Yeah.

526
00:37:04,720 --> 00:37:05,720
Okay.

527
00:37:05,720 --> 00:37:07,200
Paulin, thank you so much.

528
00:37:07,200 --> 00:37:09,120
It's been a really interesting conversation.

529
00:37:09,120 --> 00:37:13,480
Any final words or places that you'd like to point folks to or?

530
00:37:13,480 --> 00:37:19,680
I guess if you want to follow my work, go and check out the website, the project that I

531
00:37:19,680 --> 00:37:23,960
worked on is called setcloud, 3D semantic segmentation.

532
00:37:23,960 --> 00:37:24,960
Okay.

533
00:37:24,960 --> 00:37:25,960
Yeah.

534
00:37:25,960 --> 00:37:26,960
Great.

535
00:37:26,960 --> 00:37:28,960
And we'll include a link to that in the show notes.

536
00:37:28,960 --> 00:37:29,960
All right.

537
00:37:29,960 --> 00:37:30,960
Thank you.

538
00:37:30,960 --> 00:37:31,960
All right.

539
00:37:31,960 --> 00:37:32,960
Thank you.

540
00:37:32,960 --> 00:37:33,960
Thank you.

541
00:37:33,960 --> 00:37:34,960
All right.

542
00:37:34,960 --> 00:37:37,640
All right, everyone, that's our show for today.

543
00:37:37,640 --> 00:37:42,600
For more information on Lynn or any of the topics covered in this episode, you'll find

544
00:37:42,600 --> 00:37:48,200
the show notes at twomolai.com slash talk slash one, two, three.

545
00:37:48,200 --> 00:37:52,920
As you know, we love to hear your questions and feedback on the show, so don't hesitate

546
00:37:52,920 --> 00:37:54,520
to comment there.

547
00:37:54,520 --> 00:38:05,160
Thanks so much for listening and catch you next time.

