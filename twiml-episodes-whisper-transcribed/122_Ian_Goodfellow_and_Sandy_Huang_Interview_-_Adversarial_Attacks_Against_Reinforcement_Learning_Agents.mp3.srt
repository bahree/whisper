1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,560
I'm your host Sam Charrington.

4
00:00:32,560 --> 00:00:37,200
Before we hop into today's interview, I'd like to send a huge shout out to everyone

5
00:00:37,200 --> 00:00:41,360
who participated in the Twimble Online Meetup earlier this week.

6
00:00:41,360 --> 00:00:46,760
In our community segment, we had a very fun and wide-ranging discussion about freezing

7
00:00:46,760 --> 00:00:50,840
your brain, and if you missed that startup's announcement this week, you probably have

8
00:00:50,840 --> 00:00:55,560
no idea what I'm talking about, as well as machine learning in AI in the healthcare

9
00:00:55,560 --> 00:00:58,560
space and more.

10
00:00:58,560 --> 00:01:04,040
Community member Nicholas Teague, who goes by underscore Nick T underscore on Twitter, also

11
00:01:04,040 --> 00:01:08,880
briefly spoke about his essay, A Toddler Learns to Speak, where he explores connections

12
00:01:08,880 --> 00:01:11,680
between different modalities in machine learning.

13
00:01:11,680 --> 00:01:17,320
Finally, a hearty thanks to Sean Devlin, who presented a deep dive on deep reinforcement

14
00:01:17,320 --> 00:01:21,640
learning and Google deep mind seminal paper in the space.

15
00:01:21,640 --> 00:01:27,320
Be on the lookout for the video recording and details on next month's meetup at twimblei.com

16
00:01:27,320 --> 00:01:30,800
slash meetup.

17
00:01:30,800 --> 00:01:36,360
Now you all know I travel to a ton of events each year, and event season is just getting

18
00:01:36,360 --> 00:01:38,000
underway for me.

19
00:01:38,000 --> 00:01:43,320
One of the events I'm most excited about is my very own AI summit, the successor to the

20
00:01:43,320 --> 00:01:47,440
awesome future of data summit event I produced last year.

21
00:01:47,440 --> 00:01:52,240
This year's event takes place April 30th and May 1st and is once again being held in

22
00:01:52,240 --> 00:01:57,800
Las Vegas in conjunction with the interop ITX conference.

23
00:01:57,800 --> 00:02:02,680
This year's event is much more AI focused and is targeting enterprise line of business

24
00:02:02,680 --> 00:02:08,480
and IT managers and leaders who want to get smart on AI very quickly.

25
00:02:08,480 --> 00:02:14,880
Think of it as a two day no fluff technical MBA in machine learning and AI.

26
00:02:14,880 --> 00:02:18,960
I'll be presenting a machine learning and AI bootcamp and I'll have experts coming

27
00:02:18,960 --> 00:02:24,600
in to present many workshops on topics like computer vision, natural language processing

28
00:02:24,600 --> 00:02:31,400
and conversational applications, machine learning and AI for IOT and industrial applications,

29
00:02:31,400 --> 00:02:37,600
data management for AI, building an AI first culture in your organization and operationalizing

30
00:02:37,600 --> 00:02:40,240
machine learning and AI.

31
00:02:40,240 --> 00:02:49,360
For more information on the program, visit twimmolai.com slash AI summit.

32
00:02:49,360 --> 00:02:54,720
In this episode, I'm joined by Ian Goodfellow, staff research scientist at Google Brain

33
00:02:54,720 --> 00:03:01,320
and Sandy Huang, PhD student in the EECS department at UC Berkeley to discuss their work on the

34
00:03:01,320 --> 00:03:05,840
paper, adversarial attacks on neural network policies.

35
00:03:05,840 --> 00:03:10,200
If you're a regular listener here, you've probably heard of adversarial attacks and have

36
00:03:10,200 --> 00:03:15,080
seen examples of deep learning based object detectors that can be fooled into thinking

37
00:03:15,080 --> 00:03:20,520
that, for example, a giraffe is actually a school bus by injecting some imperceptible

38
00:03:20,520 --> 00:03:22,760
noise into an image.

39
00:03:22,760 --> 00:03:28,320
Well, Sandy and Ian's paper sits at the intersection of adversarial attacks and reinforcement

40
00:03:28,320 --> 00:03:29,320
learning.

41
00:03:29,320 --> 00:03:33,440
Another area we've discussed quite a bit on the podcast.

42
00:03:33,440 --> 00:03:37,920
In their paper, they discuss how adversarial attacks can be effective at targeting neural

43
00:03:37,920 --> 00:03:41,200
network policies and reinforcement learning.

44
00:03:41,200 --> 00:03:45,920
Sandy gives us an overview of the paper, including how changing a single pixel can throw off

45
00:03:45,920 --> 00:03:49,440
performance of a model train to play Atari games.

46
00:03:49,440 --> 00:03:55,040
We also cover a lot of interesting topics relating to adversarial attacks in RL individually

47
00:03:55,040 --> 00:04:00,560
and some related areas such as hierarchical reward functions and transfer learning.

48
00:04:00,560 --> 00:04:04,720
This was a great conversation that I'm really excited to bring to you.

49
00:04:04,720 --> 00:04:07,560
And now on to the show.

50
00:04:07,560 --> 00:04:11,680
All right, everyone.

51
00:04:11,680 --> 00:04:15,880
I am on the line with Ian Goodfellow and Sandy Huang.

52
00:04:15,880 --> 00:04:22,880
Ian is a staff research scientist at Google Brain and Sandy is a PhD student at UC Berkeley.

53
00:04:22,880 --> 00:04:25,720
Ian and Sandy, welcome to the podcast.

54
00:04:25,720 --> 00:04:26,720
Thank you.

55
00:04:26,720 --> 00:04:27,720
Thanks.

56
00:04:27,720 --> 00:04:28,720
Fantastic.

57
00:04:28,720 --> 00:04:31,120
I am so excited to have you both on the show.

58
00:04:31,120 --> 00:04:36,680
You recently published a paper called Adversarial Attacks on Neural Network Policies and I'm

59
00:04:36,680 --> 00:04:41,000
really looking forward to digging into some of what you worked on together.

60
00:04:41,000 --> 00:04:45,400
But before we do that, why don't we take a moment to have each of you introduce yourselves

61
00:04:45,400 --> 00:04:46,880
to the audience?

62
00:04:46,880 --> 00:04:47,880
Ian?

63
00:04:47,880 --> 00:04:49,200
Hi, I'm Ian.

64
00:04:49,200 --> 00:04:54,680
I lead a team at Google where we study adversarial machine learning that can be generative adversarial

65
00:04:54,680 --> 00:04:59,800
networks that can also be adversarial examples, which will be the main thing we talk about today.

66
00:04:59,800 --> 00:05:04,000
I've been working on deep learning since about 10 years ago, really.

67
00:05:04,000 --> 00:05:07,160
I got into deep learning when it was a very academic thing.

68
00:05:07,160 --> 00:05:11,640
I spent my PhD studying deep learning and then when I came and did an internship at Google,

69
00:05:11,640 --> 00:05:16,600
I got interested in the security side of machine learning when I helped Christian Zegity

70
00:05:16,600 --> 00:05:19,200
write the first paper on adversarial examples.

71
00:05:19,200 --> 00:05:23,000
And that's my main focus today is making sure that machine learning is secure.

72
00:05:23,000 --> 00:05:28,520
Awesome. And what prompted you to study machine learning for your graduate degree?

73
00:05:28,520 --> 00:05:31,800
I was really interested in figuring out how intelligence works.

74
00:05:31,800 --> 00:05:36,880
As an undergrad, I started out taking a psychology class freshman year and then I decided that

75
00:05:36,880 --> 00:05:39,680
it wasn't quite concrete and technical enough.

76
00:05:39,680 --> 00:05:44,560
So I moved on to cognitive science and then neuroscience and then eventually I decided

77
00:05:44,560 --> 00:05:49,120
that I'd be more likely to figure out how intelligence works by studying machine learning

78
00:05:49,120 --> 00:05:50,120
directly.

79
00:05:50,120 --> 00:05:54,760
It's really, really hard to reverse engineer the human brain partly just because so far

80
00:05:54,760 --> 00:05:58,800
we haven't had the tools to look in and measure the activity of all the neurons.

81
00:05:58,800 --> 00:06:03,080
We can't measure as many neurons simultaneously as we would like to.

82
00:06:03,080 --> 00:06:07,160
During my PhD, I was really interested just in getting machine learning to work, just

83
00:06:07,160 --> 00:06:09,440
making AI start to happen.

84
00:06:09,440 --> 00:06:14,040
And now that that ball has started rolling, I'm a lot more interested in making sure that

85
00:06:14,040 --> 00:06:18,080
there's a good outcome as AI develops further.

86
00:06:18,080 --> 00:06:21,800
Like Google, we have a lot of different teams like the people and AI research group that

87
00:06:21,800 --> 00:06:25,680
study different aspects of how AI relates to society.

88
00:06:25,680 --> 00:06:29,080
I started a group that works on adversarial machine learning because I want to make sure

89
00:06:29,080 --> 00:06:32,640
that systems with machine learning in them are secure.

90
00:06:32,640 --> 00:06:38,120
That people on the outside can't intentionally mess with the machine learning system and

91
00:06:38,120 --> 00:06:41,360
cause it to do what they would like it to do rather than what the designers would like

92
00:06:41,360 --> 00:06:42,360
it to do.

93
00:06:42,360 --> 00:06:43,360
Awesome.

94
00:06:43,360 --> 00:06:44,360
How about you, Sandy?

95
00:06:44,360 --> 00:06:47,000
How'd you get involved in machine learning and AI?

96
00:06:47,000 --> 00:06:52,520
Yeah, I think like Ian, my interest also started when I was at undergrad.

97
00:06:52,520 --> 00:06:58,520
I started off being really interested in computer science, but more from a bio computation side

98
00:06:58,520 --> 00:07:03,440
of things, I was really interested in understanding how DNA works, understanding genetics, that sort

99
00:07:03,440 --> 00:07:04,440
of thing.

100
00:07:04,440 --> 00:07:10,240
But then, as I was thinking more biocomp classes, I realized that I was actually most interested

101
00:07:10,240 --> 00:07:15,280
in the machine learning discoveries that had been made, like machine learning driven discoveries

102
00:07:15,280 --> 00:07:18,360
that were made in biocomputation.

103
00:07:18,360 --> 00:07:23,560
For example, clustering, breast cancer genes to figure out those two different types.

104
00:07:23,560 --> 00:07:29,720
That's when, junior year, I started taking more AI, more machine learning classes, and

105
00:07:29,720 --> 00:07:33,520
that's when I realized that, okay, there's a lot more to learn here, and I wanted to

106
00:07:33,520 --> 00:07:37,960
do a PhD in this area and see what else I can figure out.

107
00:07:37,960 --> 00:07:38,960
Awesome.

108
00:07:38,960 --> 00:07:44,360
And you're advised by both Peter Biel, who's been on the show before and Anka?

109
00:07:44,360 --> 00:07:45,360
Is that right?

110
00:07:45,360 --> 00:07:46,360
Yes.

111
00:07:46,360 --> 00:07:47,360
And Anka, drawing, yep.

112
00:07:47,360 --> 00:07:51,440
And so does that mean that you spend a lot of time thinking about robotic applications

113
00:07:51,440 --> 00:07:53,200
of machine learning and AI?

114
00:07:53,200 --> 00:07:54,840
Yes, exactly.

115
00:07:54,840 --> 00:08:00,040
Recently, I've been thinking a lot about how we can make machine learning systems more

116
00:08:00,040 --> 00:08:06,160
interpretable and more predictable, and so that ties in very closely with helping human

117
00:08:06,160 --> 00:08:10,040
robot interaction be more feasible in the future.

118
00:08:10,040 --> 00:08:11,040
Fantastic.

119
00:08:11,040 --> 00:08:15,000
So who wants to get started by telling me a little bit about the paper that you worked

120
00:08:15,000 --> 00:08:16,000
on together?

121
00:08:16,000 --> 00:08:17,000
Yeah, sure.

122
00:08:17,000 --> 00:08:19,080
I can give a summary.

123
00:08:19,080 --> 00:08:25,200
So the idea here was that there had already been a lot of work that shows that adversarial

124
00:08:25,200 --> 00:08:29,840
examples are really effective at attacking classifiers.

125
00:08:29,840 --> 00:08:35,880
So things like object recognition, if you train something to recognize objects in a scene

126
00:08:35,880 --> 00:08:41,000
in an image, it's pretty straightforward to find a small perturbation that will

127
00:08:41,000 --> 00:08:46,000
get your neural network to output a completely different label than what you had anticipated

128
00:08:46,000 --> 00:08:48,040
and then what the correct label is.

129
00:08:48,040 --> 00:08:54,960
And so we were thinking, we wanted to see if this would apply to neural network policies

130
00:08:54,960 --> 00:08:57,440
that were trained with deep reinforcement learning.

131
00:08:57,440 --> 00:09:02,560
And in particular, we were really interested in, to what extent, these adversarial perturbations

132
00:09:02,560 --> 00:09:07,960
could disrupt the performance of these policies and how transferable they were.

133
00:09:07,960 --> 00:09:13,160
And if you didn't know how a particular policy was trained, for example, which deep reinforcement

134
00:09:13,160 --> 00:09:18,640
learning algorithm was used to train it, could you still attack that policy?

135
00:09:18,640 --> 00:09:22,640
And so that was the overall question that we were trying to answer.

136
00:09:22,640 --> 00:09:30,840
So you referenced the work on adversarial examples for classifiers and these are examples.

137
00:09:30,840 --> 00:09:35,640
Like actually before spouting out some examples, do you do each of you have your own kind of

138
00:09:35,640 --> 00:09:40,160
favorite example of adversarial attacks against classifiers?

139
00:09:40,160 --> 00:09:42,640
Well, a lot of them are pretty similar.

140
00:09:42,640 --> 00:09:47,880
I would say one of my favorite observations is a paper called Delving into Transferable

141
00:09:47,880 --> 00:09:55,120
Adversarial Examples, where the authors found that if they fool several different classifiers

142
00:09:55,120 --> 00:09:59,520
simultaneously, if they actually use an optimizer to search for an adversarial example

143
00:09:59,520 --> 00:10:05,280
that fools very many different classifiers, then that input is extremely likely to fool

144
00:10:05,280 --> 00:10:07,960
another classifier that wasn't involved.

145
00:10:07,960 --> 00:10:12,600
You can design these attacks that will actually fool more or less anything without access

146
00:10:12,600 --> 00:10:16,280
to the target model that you want to fool.

147
00:10:16,280 --> 00:10:25,440
So in that example, if you somehow manage to create an example that visually looks like

148
00:10:25,440 --> 00:10:33,400
an ostrich but is classified as a school bus for multiple classifiers, they've demonstrated

149
00:10:33,400 --> 00:10:37,800
that it's likely to work on some broader number of classifiers.

150
00:10:37,800 --> 00:10:38,800
Exactly.

151
00:10:38,800 --> 00:10:44,160
Like suppose that you're a malicious attacker and you want to fool somebody's computer

152
00:10:44,160 --> 00:10:47,160
vision system, you don't know what they're using.

153
00:10:47,160 --> 00:10:48,160
Right.

154
00:10:48,160 --> 00:10:51,560
Let's say for the sake of argument, they're using VGGnet, but the attacker doesn't know

155
00:10:51,560 --> 00:10:52,560
that.

156
00:10:52,560 --> 00:10:57,560
The attacker could do something like fool inception and fool a resonant with the same input

157
00:10:57,560 --> 00:10:58,560
image.

158
00:10:58,560 --> 00:11:02,320
And if they go ahead and fool those two and a few other models, it's much more likely

159
00:11:02,320 --> 00:11:08,360
that they'll fool VGG, even if they never actually worked on fooling VGG specifically.

160
00:11:08,360 --> 00:11:16,200
Is the fooling specific to the network architecture as opposed to the specific parameters of a given

161
00:11:16,200 --> 00:11:18,200
model?

162
00:11:18,200 --> 00:11:22,000
That's what this paper is able to overcome.

163
00:11:22,000 --> 00:11:26,680
It's able to fool models regardless of their parameters or their architecture.

164
00:11:26,680 --> 00:11:30,920
As long as the models are trying to solve the same task, like recognize school buses

165
00:11:30,920 --> 00:11:36,280
and ostriches, this attack can find a reliable way of fooling pretty much any architecture

166
00:11:36,280 --> 00:11:37,760
that we've tested.

167
00:11:37,760 --> 00:11:42,160
And that's a lot of what Sandy was saying about how in this paper that we just published

168
00:11:42,160 --> 00:11:47,680
on adversarial policies, we wanted to find out if this property of adversarial examples

169
00:11:47,680 --> 00:11:52,280
transferring from one policy to another holds up in the same way that a transfer between

170
00:11:52,280 --> 00:11:54,480
classifiers holds up.

171
00:11:54,480 --> 00:12:00,240
And the context here is policies applied to reinforcement learning.

172
00:12:00,240 --> 00:12:05,400
Can you give us a concrete example of what you've got in mind there?

173
00:12:05,400 --> 00:12:09,760
Sure, maybe Sandy, do you want to talk about the Atari games?

174
00:12:09,760 --> 00:12:10,760
Yeah, sure.

175
00:12:10,760 --> 00:12:15,520
So I think when you think deeper reinforcement learning, the first really impressive example

176
00:12:15,520 --> 00:12:23,440
of this was when DeepMind was able to train DeepQ networks, DeQin, use that to play Atari

177
00:12:23,440 --> 00:12:27,440
games at human level performance.

178
00:12:27,440 --> 00:12:33,400
And so the idea there is that you basically start from a random initialization network

179
00:12:33,400 --> 00:12:35,000
that knows nothing.

180
00:12:35,000 --> 00:12:39,000
And then very slowly over time, just by getting this reward as feedback, this network is

181
00:12:39,000 --> 00:12:43,400
able to learn which actions will help it maximize reward.

182
00:12:43,400 --> 00:12:44,400
What's the next step?

183
00:12:44,400 --> 00:12:50,240
How does that apply or how does adversarial attacks apply in that example?

184
00:12:50,240 --> 00:12:57,240
Yeah, so the kind of adversarial attacks we were looking at, we assume that we've already

185
00:12:57,240 --> 00:13:01,960
got a policy that was trained with deep reinforcement learning.

186
00:13:01,960 --> 00:13:07,040
So it's fully trained and it's able to get really high performance on the game.

187
00:13:07,040 --> 00:13:11,920
So for example, a policy that's trained to play space invaders.

188
00:13:11,920 --> 00:13:17,760
What we can do is compute these small adversarial perturbations in the image of the game.

189
00:13:17,760 --> 00:13:21,240
So we do things like, for example, change a single pixel in the game.

190
00:13:21,240 --> 00:13:26,960
And that's able to significantly decrease the performance of this fully trained policy.

191
00:13:26,960 --> 00:13:29,960
And so the policy itself is fixed at that point.

192
00:13:29,960 --> 00:13:33,280
All we're changing is the input that the policy is given.

193
00:13:33,280 --> 00:13:34,280
Wow.

194
00:13:34,280 --> 00:13:43,000
And so, and did you find that across, like how broadly did you find that this applies,

195
00:13:43,000 --> 00:13:48,840
that you're able to change a single pixel value and dramatically impact the performance

196
00:13:48,840 --> 00:13:50,440
of the model?

197
00:13:50,440 --> 00:13:51,440
Yeah.

198
00:13:51,440 --> 00:13:53,680
So we did look at a few different games.

199
00:13:53,680 --> 00:13:59,240
All the only domain we looked at in this paper was Atari, but we looked at chopper command,

200
00:13:59,240 --> 00:14:02,880
sequests, space invaders, and pong.

201
00:14:02,880 --> 00:14:08,400
And so across all those games, and across three different ways of training these policies,

202
00:14:08,400 --> 00:14:15,280
A3C, TRPO, DQN, adversarial examples are, you can pretty easily find adversarial examples

203
00:14:15,280 --> 00:14:19,680
that will significantly decrease performance, like at least by half.

204
00:14:19,680 --> 00:14:24,360
We looked at a range of different perturbations, there are graphs in the paper that show

205
00:14:24,360 --> 00:14:33,160
this more concretely, but basically no matter which Atari game you're looking at or how

206
00:14:33,160 --> 00:14:38,480
it was trained, it's definitely true that adversarial examples exist.

207
00:14:38,480 --> 00:14:45,160
In some training methods, adversarial examples are more effective at decreasing performance,

208
00:14:45,160 --> 00:14:46,960
but yeah, they're pretty prevalent.

209
00:14:46,960 --> 00:14:55,440
Yeah, it's fascinating that you found this, you know, and I think about it in the context

210
00:14:55,440 --> 00:15:01,280
of reinforcement learning, I guess my initial take is that it seems different from just

211
00:15:01,280 --> 00:15:09,520
looking at an image, but you know, now that you've got me thinking about it, I can see

212
00:15:09,520 --> 00:15:15,560
how because the training methods are so similar, you would expect to have similar occurrences

213
00:15:15,560 --> 00:15:20,560
of adversarial examples in reinforcement learning types of models.

214
00:15:20,560 --> 00:15:25,720
One thing that I find really exciting about Sandy's results is that they help us answer

215
00:15:25,720 --> 00:15:29,240
a question that's almost more philosophical than technical.

216
00:15:29,240 --> 00:15:34,000
Most of the previous work on adversarial examples was about object recognition, looking

217
00:15:34,000 --> 00:15:39,840
at a photo and saying whether that photo is of an ostrich or a school bus.

218
00:15:39,840 --> 00:15:43,920
But a lot of the time we were making up unusual photos.

219
00:15:43,920 --> 00:15:49,200
They weren't photos made by taking a camera and snapping a picture of a school bus in

220
00:15:49,200 --> 00:15:50,200
the real world.

221
00:15:50,200 --> 00:15:56,560
They were made by a computer program and there's a deep philosophical question about how

222
00:15:56,560 --> 00:16:02,160
you can say what the objectively true answer is in such a photo.

223
00:16:02,160 --> 00:16:06,400
We've mostly evaluated our systems based on whether they agree with human judgment, but

224
00:16:06,400 --> 00:16:08,400
maybe the human is making a mistake.

225
00:16:08,400 --> 00:16:13,200
So it's hard to say that what the system does in the end is objectively wrong.

226
00:16:13,200 --> 00:16:24,880
And are you, if I can interrupt, is that critique to the specific examples that have become

227
00:16:24,880 --> 00:16:30,880
popularized of adversarial attacks like the ostrich and school bus?

228
00:16:30,880 --> 00:16:34,800
I mean, I guess it's not clear to me.

229
00:16:34,800 --> 00:16:35,800
Exactly.

230
00:16:35,800 --> 00:16:36,800
Yeah.

231
00:16:36,800 --> 00:16:43,000
I guess one of the strange things about adversarial examples is we're studying how to make computers

232
00:16:43,000 --> 00:16:45,160
make mistakes.

233
00:16:45,160 --> 00:16:51,560
And we happen to have done that mostly on, on kinds of data where we don't know objectively

234
00:16:51,560 --> 00:16:53,880
what a mistake is or isn't.

235
00:16:53,880 --> 00:16:57,920
If you make up an entirely new image, it's hard to say objectively how that image should

236
00:16:57,920 --> 00:16:58,920
be categorized.

237
00:16:58,920 --> 00:17:04,320
But for things like playing Atari games, it's, it's objective how the points are awarded.

238
00:17:04,320 --> 00:17:08,880
For pong, you need to actually make the ball go through the opponent's goal post.

239
00:17:08,880 --> 00:17:12,840
It's not really a question of whether a human observer thought the ball went through

240
00:17:12,840 --> 00:17:13,840
the goal post.

241
00:17:13,840 --> 00:17:18,600
It's just a question of how the pong game physics defined the scoring.

242
00:17:18,600 --> 00:17:25,160
And so in these experiments, we can actually say that the machine learning system is objectively

243
00:17:25,160 --> 00:17:28,280
compromised, that it really is doing worse.

244
00:17:28,280 --> 00:17:31,920
It's not just that it's playing pong differently than a human would play.

245
00:17:31,920 --> 00:17:35,840
It's actually playing pong in a way where it receives fewer points.

246
00:17:35,840 --> 00:17:42,160
Were you able to produce specific failure modes via these attacks or are we only looking

247
00:17:42,160 --> 00:17:48,440
at this from the perspective of subpar performance or reduced scores?

248
00:17:48,440 --> 00:17:57,000
Could you always make, could you by manipulating a single pixel or some number of pixels always

249
00:17:57,000 --> 00:18:06,520
make the pong paddle, the agent playing pong, missed the ball in the upper left corner?

250
00:18:06,520 --> 00:18:10,880
There's some work that came out after ours that focuses on what they call an enchanting

251
00:18:10,880 --> 00:18:11,880
attack.

252
00:18:11,880 --> 00:18:16,400
Where they attract the agent toward a particular state.

253
00:18:16,400 --> 00:18:20,040
We were just trying to reduce the score that the agent receives.

254
00:18:20,040 --> 00:18:24,240
I don't know, Sandy, did you notice any particular failure modes like any specific kind of mistakes

255
00:18:24,240 --> 00:18:26,440
that the agent would do over and over again?

256
00:18:26,440 --> 00:18:32,640
No, I think because when we were computing the adversarial examples, all we were trying

257
00:18:32,640 --> 00:18:38,000
to do is to get the agent to not do what it thought was the best action.

258
00:18:38,000 --> 00:18:42,840
It could take any other action besides the best action, and that would be a successful

259
00:18:42,840 --> 00:18:45,840
adversarial attack.

260
00:18:45,840 --> 00:18:52,680
I didn't see any specific failure modes, although I did see patterns in terms of what particular

261
00:18:52,680 --> 00:18:58,160
adverse profound and so in particular, you mentioned changing one pixel.

262
00:18:58,160 --> 00:19:04,440
In something like chopper command, when you change one pixel in this game, the highest

263
00:19:04,440 --> 00:19:09,360
impact you can get from that is by changing it in this small miniature map of the entire

264
00:19:09,360 --> 00:19:10,360
game.

265
00:19:10,360 --> 00:19:13,280
That's at the bottom of your screen.

266
00:19:13,280 --> 00:19:19,640
When we found adversarial examples for this, actually, most of the ones where we only

267
00:19:19,640 --> 00:19:26,640
changed one or two pixels, those pixels would get changed in that miniature map.

268
00:19:26,640 --> 00:19:30,080
That's the optimal way to fold the agent.

269
00:19:30,080 --> 00:19:35,920
That does shed some light on what the agent has learned to pay attention to, because you

270
00:19:35,920 --> 00:19:40,840
could imagine that maybe if you're just training a deep reinforcement learning agent on images,

271
00:19:40,840 --> 00:19:46,040
maybe it just would ignore this miniature map and not realize it's important.

272
00:19:46,040 --> 00:19:50,480
In that way, adversarial examples are also kind of interesting because they can make

273
00:19:50,480 --> 00:19:54,760
policies more interpretable in terms of what they're paying attention to in the scene.

274
00:19:54,760 --> 00:19:55,760
Okay.

275
00:19:55,760 --> 00:20:02,640
Other things that surprised you in terms of the things you learned in doing this?

276
00:20:02,640 --> 00:20:09,400
I guess one other interesting result that we haven't actually posted yet, but we've

277
00:20:09,400 --> 00:20:14,960
talked about at presentations and stuff, is it's actually possible to have dormant adversarial

278
00:20:14,960 --> 00:20:16,560
examples as well.

279
00:20:16,560 --> 00:20:22,200
We looked at policies that are recurrent, which means they have some sort of memory.

280
00:20:22,200 --> 00:20:27,880
One canonical example is if you have an agent trying to navigate through a maze and you

281
00:20:27,880 --> 00:20:31,920
show the agent, a particular indicator, say it's a certain color at the beginning of

282
00:20:31,920 --> 00:20:35,920
the maze, it has to remember that in order to figure out which goal to go to at the end

283
00:20:35,920 --> 00:20:38,880
of the maze.

284
00:20:38,880 --> 00:20:44,440
If you just did playing adversarial attacks, you could have an agent that just while it's

285
00:20:44,440 --> 00:20:48,760
navigating through this maze starts acting randomly and never reaches the goal at all.

286
00:20:48,760 --> 00:20:55,040
But if you're using, if you compute dormant adversarial examples, which means that you

287
00:20:55,040 --> 00:21:00,080
perturb a particular input, that's given to the agent, but then the agent keeps acting

288
00:21:00,080 --> 00:21:04,360
correctly until some time point in the future.

289
00:21:04,360 --> 00:21:08,800
In this maze, it would be the agent still navigating the maze correctly and then all

290
00:21:08,800 --> 00:21:12,640
of a sudden at the very end actually goes to the wrong goal.

291
00:21:12,640 --> 00:21:19,480
But the key there is that the point at which the adversarial example was introduced is

292
00:21:19,480 --> 00:21:23,800
actually significantly earlier than the point at which the agent makes the mistake, which

293
00:21:23,800 --> 00:21:25,840
is what makes it dormant.

294
00:21:25,840 --> 00:21:29,200
And so these also exist for recurrent policies.

295
00:21:29,200 --> 00:21:33,040
They're a little bit harder to find, but you can find them in very much the same way, just

296
00:21:33,040 --> 00:21:35,400
frame you as an optimization problem.

297
00:21:35,400 --> 00:21:37,760
What does harder to find mean?

298
00:21:37,760 --> 00:21:43,160
Hard to find as in, it takes more computational power to find it.

299
00:21:43,160 --> 00:21:48,600
The problem itself is a little bit, there are more local minima, for example, you'll have

300
00:21:48,600 --> 00:21:53,320
a lot of examples that won't meet all the constraints of your authorization.

301
00:21:53,320 --> 00:21:57,880
You're constrained to basically that you do the right thing for the next, let's say,

302
00:21:57,880 --> 00:22:02,080
10 time steps and the wrong thing on the 11th time step.

303
00:22:02,080 --> 00:22:06,600
You can think of dormant adversarial examples as being a little bit like post hypnotic

304
00:22:06,600 --> 00:22:13,240
suggestion in a cheesy spy movie, where there's a character who has been pre-programmed to

305
00:22:13,240 --> 00:22:18,160
suddenly carry out an assassination, and even that person themselves doesn't know that

306
00:22:18,160 --> 00:22:19,960
they've been programmed in that way.

307
00:22:19,960 --> 00:22:20,960
Kind of the mentoring.

308
00:22:20,960 --> 00:22:21,960
Kind of.

309
00:22:21,960 --> 00:22:22,960
Kind of.

310
00:22:22,960 --> 00:22:23,960
Exactly.

311
00:22:23,960 --> 00:22:27,320
Yeah, that was the movie we were talking about when we first had the idea for this project.

312
00:22:27,320 --> 00:22:28,320
Interesting.

313
00:22:28,320 --> 00:22:35,040
From a security point of view, dormant adversarial examples are more worrisome because they could

314
00:22:35,040 --> 00:22:40,400
be presented to an agent before it enters the area that you've secured.

315
00:22:40,400 --> 00:22:44,240
You could imagine if you have some kind of room where you're careful about what objects

316
00:22:44,240 --> 00:22:45,240
are there.

317
00:22:45,240 --> 00:22:49,960
You could make sure that nothing in that area can confuse your robot, but if your robot

318
00:22:49,960 --> 00:22:55,400
could be confused by something it saw before it came into the secure environment, then you

319
00:22:55,400 --> 00:22:59,680
actually have to secure it at the level of the machine learning software, rather than

320
00:22:59,680 --> 00:23:04,320
securing it by making sure that there's nothing unusual in its physical environment.

321
00:23:04,320 --> 00:23:14,640
Are you aware of any publicized examples of adversarial attacks in the wild?

322
00:23:14,640 --> 00:23:20,680
I've heard people in finance say that they spend a lot of effort obfuscating their trading

323
00:23:20,680 --> 00:23:26,240
algorithms so that their competitors don't reverse engineer their trading algorithm and

324
00:23:26,240 --> 00:23:30,040
fool them into making unprofitable trades.

325
00:23:30,040 --> 00:23:35,680
I don't know of anything very similar to the computer vision, object recognition examples

326
00:23:35,680 --> 00:23:38,120
that we're studying so far.

327
00:23:38,120 --> 00:23:40,600
Do you know of anything like that, Cindy?

328
00:23:40,600 --> 00:23:42,760
I don't know of anything in the wild.

329
00:23:42,760 --> 00:23:50,000
I mean, there have been more examples of real world adversarial examples where you do

330
00:23:50,000 --> 00:23:55,520
something like print out a poster of a stop sign and paste that on top of a real stop

331
00:23:55,520 --> 00:24:04,080
sign, and that's able to fool a classifier from a lot of different angles and distances.

332
00:24:04,080 --> 00:24:10,000
So, I've been fortunate that the real malicious people don't seem to be using these techniques

333
00:24:10,000 --> 00:24:11,000
yet.

334
00:24:11,000 --> 00:24:13,760
I think that people probably well in the future.

335
00:24:13,760 --> 00:24:16,800
At the moment, I think we're protected by a few factors.

336
00:24:16,800 --> 00:24:21,000
One is I think there's a lot of other malicious things you can do that are easier.

337
00:24:21,000 --> 00:24:28,320
And two, if you have the deep learning expertise, there are less risky ways to turn a profit.

338
00:24:28,320 --> 00:24:35,240
Have you come across, I don't know if you would call these adversarial examples, but what's

339
00:24:35,240 --> 00:24:40,640
the analog accidental adversarial examples, like natural adversarial examples, is there

340
00:24:40,640 --> 00:24:41,640
such a thing?

341
00:24:41,640 --> 00:24:48,320
I mean, I think that would just be called like training and test, well, it would be like

342
00:24:48,320 --> 00:24:53,520
training and test distribution shift, but something that you saw it test time that you didn't

343
00:24:53,520 --> 00:24:58,600
train on and you didn't expect to see, which I think does happen all the time.

344
00:24:58,600 --> 00:24:59,600
Sure.

345
00:24:59,600 --> 00:25:00,600
Yeah.

346
00:25:00,600 --> 00:25:01,600
Yeah.

347
00:25:01,600 --> 00:25:05,600
There are optical illusions and things like that that fool humans, even when they're not

348
00:25:05,600 --> 00:25:06,920
really designed to.

349
00:25:06,920 --> 00:25:10,800
I'm sure you've seen silly images posted on Reddit, where the first time you look

350
00:25:10,800 --> 00:25:13,800
at it, you think you see something entirely different.

351
00:25:13,800 --> 00:25:18,960
There's lots of party photos where people standing near each other, it looks like someone's

352
00:25:18,960 --> 00:25:20,960
arms are actually someone else's legs.

353
00:25:20,960 --> 00:25:24,680
So it looks like there's somebody with four legs or something like that.

354
00:25:24,680 --> 00:25:27,920
That kind of thing comes up in machine learning too, just by chance.

355
00:25:27,920 --> 00:25:32,280
So where do you see the work in this paper going?

356
00:25:32,280 --> 00:25:36,600
One thing that I'd be really interested in is going further in the direction of what

357
00:25:36,600 --> 00:25:42,640
you were alluding to earlier about controlling the agent to do complicated behaviors that

358
00:25:42,640 --> 00:25:48,360
are different from what the designer wanted, rather than just doing worse at the main task.

359
00:25:48,360 --> 00:25:52,640
One of the main things making that harder for researchers to do is that we only have access

360
00:25:52,640 --> 00:25:54,560
to one reward function.

361
00:25:54,560 --> 00:25:57,480
We have a reward function that says play pong very well.

362
00:25:57,480 --> 00:26:01,880
And as an attacker, we can choose actions that make that reward go down.

363
00:26:01,880 --> 00:26:06,320
But if we had two different reward functions for the same environment, like if we had a robot

364
00:26:06,320 --> 00:26:10,600
that can cook, and it's been asked to cook scrambled eggs, but we also have a reward

365
00:26:10,600 --> 00:26:12,840
function for making a birthday cake.

366
00:26:12,840 --> 00:26:17,280
It would be interesting to show that we could trick it into making a birthday cake.

367
00:26:17,280 --> 00:26:18,720
It's really easy to break things.

368
00:26:18,720 --> 00:26:22,640
It's harder to create something that wasn't there already.

369
00:26:22,640 --> 00:26:26,800
And so if as an attacker, we could show we have so much control that we're able to actually

370
00:26:26,800 --> 00:26:31,360
create a birthday cake rather than just interfere with making scrambled eggs.

371
00:26:31,360 --> 00:26:36,160
That would be a lot more impressive in terms of the capabilities of the attacker.

372
00:26:36,160 --> 00:26:44,360
In the example that you use, the robot has multiple reward functions corresponding to what

373
00:26:44,360 --> 00:26:49,200
are, you could view as largely different tasks are there.

374
00:26:49,200 --> 00:26:59,240
Are you seeing a move in reinforcement learning towards something other than a single reward

375
00:26:59,240 --> 00:27:02,800
function, something that's more kind of nuanced or complex?

376
00:27:02,800 --> 00:27:06,560
I'm not sure exactly what that means or if the question makes sense, but you're saying

377
00:27:06,560 --> 00:27:12,680
is there a different way to evaluate performance or to give a learning signal to the agent?

378
00:27:12,680 --> 00:27:14,320
I don't work on reinforcement learning as much.

379
00:27:14,320 --> 00:27:17,440
Sandy is probably better qualified to comment on that.

380
00:27:17,440 --> 00:27:23,000
I personally feel like we need to move beyond the paradigm of just maximizing reward for

381
00:27:23,000 --> 00:27:25,080
a lot of different reasons.

382
00:27:25,080 --> 00:27:28,880
One reason that I have is that it's a strange way to communicate with an agent.

383
00:27:28,880 --> 00:27:33,160
Imagine we wanted a reinforcement learning agent to plan a mission to Mars and we give it

384
00:27:33,160 --> 00:27:37,040
a reward of one when it gets there and a reward of zero otherwise.

385
00:27:37,040 --> 00:27:41,360
How would it even know we wanted it to go to Mars until it got there?

386
00:27:41,360 --> 00:27:45,920
You could imagine having two different super intelligent agents and we want one to cure

387
00:27:45,920 --> 00:27:50,480
cancer and one to go to Mars and we just let them both lose on Earth.

388
00:27:50,480 --> 00:27:54,520
How would each one know which task it had been assigned until it finished its task?

389
00:27:54,520 --> 00:27:59,960
Conceivably, one could cure cancer but it was supposed to go to Mars and get no reward

390
00:27:59,960 --> 00:28:02,160
and the other vice versa.

391
00:28:02,160 --> 00:28:06,040
I should say I'm saying all of this is someone who mostly studies classifiers in

392
00:28:06,040 --> 00:28:11,400
generative models, so maybe I'm being very inferred at the reinforcement learning point.

393
00:28:11,400 --> 00:28:15,280
Sandy, do you have any thoughts about the future of reward functions?

394
00:28:15,280 --> 00:28:21,160
Well, I think there is a lot of complexity that can go into a reward signal.

395
00:28:21,160 --> 00:28:29,600
I do sort of agree with Ian that it is a pretty severe constraint on how you can give

396
00:28:29,600 --> 00:28:35,000
information to your agent if all you're giving it is this particular reward but you can

397
00:28:35,000 --> 00:28:41,480
do things like shape the reward or do something more like curriculum learning where you start

398
00:28:41,480 --> 00:28:49,160
off the agent with smaller tasks even if the reward is sparse and over time give it

399
00:28:49,160 --> 00:28:53,760
harder task but it's already learned how to solve the easier smaller tasks so it should

400
00:28:53,760 --> 00:28:57,240
be able to more easily solve more difficult ones.

401
00:28:57,240 --> 00:29:04,920
I think, yeah, I mean there has been a lot of work recently on trying to do things like

402
00:29:04,920 --> 00:29:12,840
transfer learning or meta learning in the context of reinforcement learning and so that's

403
00:29:12,840 --> 00:29:14,880
also a promising direction.

404
00:29:14,880 --> 00:29:16,920
You mentioned curriculum learning.

405
00:29:16,920 --> 00:29:19,760
Can you elaborate on that?

406
00:29:19,760 --> 00:29:27,160
Are you essentially iteratively training with more comprehensive or longer term rewards?

407
00:29:27,160 --> 00:29:29,720
Is it related to transfer learning in that sense?

408
00:29:29,720 --> 00:29:33,920
Yeah, it is related transfer learning in the sense that you're trying to transfer knowledge

409
00:29:33,920 --> 00:29:36,640
from easier tasks to harder tasks.

410
00:29:36,640 --> 00:29:41,840
So there's been some work Carlos Lorenza and Peter's group has done some work where you're

411
00:29:41,840 --> 00:29:46,880
in a setting with sparse reward like Ian was talking about where you get a one if you succeed.

412
00:29:46,880 --> 00:29:53,240
But you can do things like start your robot from starting states that are closer to your

413
00:29:53,240 --> 00:29:54,240
goal.

414
00:29:54,240 --> 00:29:59,000
For example, if you're trying to get a robot to insert a key into a lock and turn it, you

415
00:29:59,000 --> 00:30:03,000
could start it with the key already inside the lock and it just has to turn the key and

416
00:30:03,000 --> 00:30:06,560
then start with the key just a tiny bit outside the lock so it just has to figure out and

417
00:30:06,560 --> 00:30:11,920
start the key and then turn it and so you just slowly you have a set of states where the

418
00:30:11,920 --> 00:30:17,840
robot succeeds from and you slowly expand that set to starting states that are just a

419
00:30:17,840 --> 00:30:21,880
tiny bit more difficult than the ones the robot can already succeed from.

420
00:30:21,880 --> 00:30:27,000
And so that and then at the end you have a robot that can insert this key into the lock

421
00:30:27,000 --> 00:30:31,840
and turn it from any point within a large range of different points.

422
00:30:31,840 --> 00:30:36,520
It's called curriculum learning as an analogy to the way that we teach people in school

423
00:30:36,520 --> 00:30:37,520
schools.

424
00:30:37,520 --> 00:30:41,640
We start out teaching people how to read the alphabet in kindergarten and then build

425
00:30:41,640 --> 00:30:48,200
up to very easy C spot run type books and then gradually to more and more difficult

426
00:30:48,200 --> 00:30:53,080
reading tasks and then once people can read fluently we start teaching them subject matter

427
00:30:53,080 --> 00:30:55,720
that they read in textbooks.

428
00:30:55,720 --> 00:31:02,040
We don't on day one of kindergarten start throwing everyone questions sampled uniformly

429
00:31:02,040 --> 00:31:07,520
from the set of all knowledge we expect them to have by age 18 we don't give anyone questions

430
00:31:07,520 --> 00:31:11,360
from their algebra two class on day one of kindergarten.

431
00:31:11,360 --> 00:31:16,560
We arrange the order of the experiences so that it gets harder and harder as they go through.

432
00:31:16,560 --> 00:31:22,000
It seems really obvious in retrospect but in machine learning we actually usually do

433
00:31:22,000 --> 00:31:26,840
uniformly sample all the experiences that we test the machine learning system on.

434
00:31:26,840 --> 00:31:30,560
So curriculum learning is a pretty big change from what's the standard practice.

435
00:31:30,560 --> 00:31:36,840
And I was thinking ahead a little bit to how the adversarial attacks might apply in

436
00:31:36,840 --> 00:31:45,520
that example and what are the extent to which we've looked at transferability of adversarial

437
00:31:45,520 --> 00:31:51,640
attacks in transfer learning cases in general have either of you looked into that.

438
00:31:51,640 --> 00:31:55,280
I've been a co-author of some work and I've followed a lot of other work with a lot of

439
00:31:55,280 --> 00:32:01,920
interest. In 2013 when Christian Zeggedy wrote one of the first papers on adversarial

440
00:32:01,920 --> 00:32:07,520
examples he found that if you just make adversarial examples for one model and don't do anything

441
00:32:07,520 --> 00:32:12,560
to try to make them transfer they will often fool other models just by chance without

442
00:32:12,560 --> 00:32:15,320
needing to do anything special to cause it to happen.

443
00:32:15,320 --> 00:32:21,440
And then later Nikola Paparno in a paper that we wrote together showed that if you train

444
00:32:21,440 --> 00:32:26,880
one neural network to copy another neural network you can actually train it to copy the

445
00:32:26,880 --> 00:32:32,840
behavior of the target network on unusual inputs that don't correspond to any kind of real

446
00:32:32,840 --> 00:32:36,880
data as well as regular inputs that look like data.

447
00:32:36,880 --> 00:32:40,480
But once you've managed to copy all the decision boundaries of this target network in that

448
00:32:40,480 --> 00:32:45,240
way then you know that adversarial examples for your copy are very likely to fool the target

449
00:32:45,240 --> 00:32:46,240
as well.

450
00:32:46,240 --> 00:32:50,000
You can copy a network like that without actually having access to its parameters or its

451
00:32:50,000 --> 00:32:51,000
architecture.

452
00:32:51,000 --> 00:32:54,760
You can just send inputs to it and see what output it assigns them and then you train

453
00:32:54,760 --> 00:32:58,320
your own model to copy that input to output mapping.

454
00:32:58,320 --> 00:33:04,400
So is that you basically using the model that you're copying or the network that you're

455
00:33:04,400 --> 00:33:08,440
copying is that essentially generating your label data so you've got some inputs you're

456
00:33:08,440 --> 00:33:15,520
sending it to that and then you're training your model on the inputs and labels that

457
00:33:15,520 --> 00:33:17,000
that thing generates.

458
00:33:17,000 --> 00:33:18,000
Exactly.

459
00:33:18,000 --> 00:33:21,560
The attacker doesn't even need to have enough resources to label their own data set.

460
00:33:21,560 --> 00:33:22,560
Right.

461
00:33:22,560 --> 00:33:27,480
And so any specific thoughts on how adversarial attacks might apply in this curriculum learning

462
00:33:27,480 --> 00:33:30,400
type of use case?

463
00:33:30,400 --> 00:33:36,840
One thing that's kind of interesting and related to curriculum learning is a machine learning

464
00:33:36,840 --> 00:33:40,440
security problem called training set poisoning.

465
00:33:40,440 --> 00:33:43,400
It's almost like the opposite of curriculum learning.

466
00:33:43,400 --> 00:33:49,280
That curriculum learning is when a benevolent designer of the system structures the training

467
00:33:49,280 --> 00:33:53,160
set to be really easy for the model to learn from.

468
00:33:53,160 --> 00:33:57,400
Training set poisoning is when an attacker sneaks something into your training set that

469
00:33:57,400 --> 00:33:59,200
you didn't know was there.

470
00:33:59,200 --> 00:34:04,400
And then it can make the machine learning model do something that the attacker chose at

471
00:34:04,400 --> 00:34:08,720
test time based on what it learned from the training set poisoning.

472
00:34:08,720 --> 00:34:11,400
Another really similar idea was introducing it.

473
00:34:11,400 --> 00:34:15,320
Before you move on from there, can you give a specific example of that?

474
00:34:15,320 --> 00:34:16,320
Yeah.

475
00:34:16,320 --> 00:34:22,480
There's a paper from Stanford that came out last year where they showed that they can

476
00:34:22,480 --> 00:34:26,760
introduce a specific picture of a dog that has been altered a little bit just like an

477
00:34:26,760 --> 00:34:28,160
adversarial example.

478
00:34:28,160 --> 00:34:33,040
And if you include that dog in your training set, it will misrecognize lots and lots and

479
00:34:33,040 --> 00:34:36,080
lots of dogs as fish at test time.

480
00:34:36,080 --> 00:34:37,080
Wow.

481
00:34:37,080 --> 00:34:38,080
Okay.

482
00:34:38,080 --> 00:34:39,080
Interesting.

483
00:34:39,080 --> 00:34:40,080
Yeah.

484
00:34:40,080 --> 00:34:41,080
I've not come across that one.

485
00:34:41,080 --> 00:34:45,960
Another similar thing is introducing a paper called Badnets and they call it adding a

486
00:34:45,960 --> 00:34:47,720
backdoor to the network.

487
00:34:47,720 --> 00:34:52,200
The idea is that the person who trains the network might intentionally add something weird

488
00:34:52,200 --> 00:34:57,920
to the training set that then lets the network do something they want later on.

489
00:34:57,920 --> 00:35:04,000
So for example, suppose that I was training a face recognition system that I would give

490
00:35:04,000 --> 00:35:06,560
to other people to use to secure their facility.

491
00:35:06,560 --> 00:35:11,440
If I was a bad person, I might put myself in the training set and tell it to always let

492
00:35:11,440 --> 00:35:12,440
me in.

493
00:35:12,440 --> 00:35:15,960
And then I could go break into their warehouse later without needing to do anything special

494
00:35:15,960 --> 00:35:18,320
to get through security.

495
00:35:18,320 --> 00:35:22,400
Detecting those kinds of backdoors in neural networks is a really interesting research

496
00:35:22,400 --> 00:35:28,000
challenge because when a network does something unusual, it can be hard to tell whether the

497
00:35:28,000 --> 00:35:30,480
network is making a random mistake.

498
00:35:30,480 --> 00:35:33,840
The network is making a mistake that someone built into it as a backdoor.

499
00:35:33,840 --> 00:35:37,240
Or in some cases, the network isn't even making a mistake, it's doing something smarter

500
00:35:37,240 --> 00:35:39,800
than you, the human, have thought of.

501
00:35:39,800 --> 00:35:42,600
And it's actually correcting one of your own mistakes.

502
00:35:42,600 --> 00:35:46,640
So when you disagree with it, you don't actually know a priority who's right or wrong.

503
00:35:46,640 --> 00:35:51,480
Yeah, the thing that this makes me think about is, I guess some of the conversations we've

504
00:35:51,480 --> 00:35:54,400
had as an industry around code reuse.

505
00:35:54,400 --> 00:36:00,800
I forget the specific example, but there was an example about a year ago or so of an NPM

506
00:36:00,800 --> 00:36:06,720
library that I don't think anything malicious happened, but someone either changed it or

507
00:36:06,720 --> 00:36:09,000
unpublished it or something like that.

508
00:36:09,000 --> 00:36:17,560
And because so many people had used this library in their code, it had the potential to disrupt

509
00:36:17,560 --> 00:36:20,320
a whole bunch of applications.

510
00:36:20,320 --> 00:36:26,360
And I think the NPM folks, the node folks came in and did something extraordinary to make

511
00:36:26,360 --> 00:36:30,720
sure this library didn't go away and break all these applications.

512
00:36:30,720 --> 00:36:39,520
And we're seeing a lot of the same, the analogy of code reuse in the machine learning world

513
00:36:39,520 --> 00:36:42,040
is like reusing these data sets.

514
00:36:42,040 --> 00:36:50,200
But I don't know that we have any real standards for certifying data sets as being untempered

515
00:36:50,200 --> 00:36:51,200
with.

516
00:36:51,200 --> 00:37:00,360
And the idea that you can introduce back doors or make neural networks misbehave and

517
00:37:00,360 --> 00:37:08,360
really bad ways by manipulating the training data sets suggests that we need these kinds

518
00:37:08,360 --> 00:37:09,360
of standards.

519
00:37:09,360 --> 00:37:12,880
And even a lot of people create their training data sets by crawling the web.

520
00:37:12,880 --> 00:37:20,560
And we know that you can kind of poison web search results by creating linking architectures

521
00:37:20,560 --> 00:37:28,640
and things like that so that certain things could make certain results become more popular.

522
00:37:28,640 --> 00:37:32,160
And that could have a downstream impact on these models as well.

523
00:37:32,160 --> 00:37:33,160
Exactly, yeah.

524
00:37:33,160 --> 00:37:37,240
Sandy, do you have any war stories from training sets at Berkeley?

525
00:37:37,240 --> 00:37:42,160
No, I guess in the, well, in the context of reinforcement learning, you don't really

526
00:37:42,160 --> 00:37:48,440
have the training set like you do in supervised learning, you have your simulator.

527
00:37:48,440 --> 00:37:52,960
And so I think, I mean, something similar does apply.

528
00:37:52,960 --> 00:38:01,200
You could have a simulator that somehow gets the agent to learn some correlation that actually

529
00:38:01,200 --> 00:38:07,440
impairs it when it's launched on, unlike a test simulator.

530
00:38:07,440 --> 00:38:12,080
I mean, I think this is sort of related to reward hacking.

531
00:38:12,080 --> 00:38:16,920
If there is something that the agent can exploit in your simulator, a lot of times if you

532
00:38:16,920 --> 00:38:22,200
are training it with deep reinforcement learning, it will find that and it will exploit that.

533
00:38:22,200 --> 00:38:26,120
But the difference is in the context of reward hacking, it's pretty obvious when your agent

534
00:38:26,120 --> 00:38:31,080
has done that if you just watch a rollout of the agent, you can usually detect that, okay,

535
00:38:31,080 --> 00:38:33,800
it's not actually doing what I want it to do.

536
00:38:33,800 --> 00:38:39,560
And just to make sure folks are familiar with the term reward hacking, you know, these

537
00:38:39,560 --> 00:38:46,040
are examples in the case of video games where the one that I remember was kind of this

538
00:38:46,040 --> 00:38:50,440
agent that's a boat that figures out that if it swirls around in a circle that racks

539
00:38:50,440 --> 00:38:54,840
up a whole bunch of points, even though it's not making progress towards its end goal,

540
00:38:54,840 --> 00:38:57,560
or what you might want it to end goal to be.

541
00:38:57,560 --> 00:38:58,560
Yeah.

542
00:38:58,560 --> 00:38:59,560
Exactly.

543
00:38:59,560 --> 00:39:01,120
That's a really popular example.

544
00:39:01,120 --> 00:39:05,840
And the problem there is that you told the agent to maximize score.

545
00:39:05,840 --> 00:39:10,640
And by swirling around, it's able to get all these points by getting these things in

546
00:39:10,640 --> 00:39:12,840
the environment that appear periodically.

547
00:39:12,840 --> 00:39:14,840
And so that's what's doing while swirling around.

548
00:39:14,840 --> 00:39:19,000
But really, you wanted the agent to win the game, but you couldn't really specify that

549
00:39:19,000 --> 00:39:21,160
particular reward function.

550
00:39:21,160 --> 00:39:25,160
So that's why you gave it, just told it to max line points.

551
00:39:25,160 --> 00:39:29,840
But that goes along with the discussion we were having earlier about how the reward function

552
00:39:29,840 --> 00:39:31,680
is really important.

553
00:39:31,680 --> 00:39:35,520
And you need to be really careful in selecting your reward function and make sure you're

554
00:39:35,520 --> 00:39:37,840
actually telling the agent what you want it to do.

555
00:39:37,840 --> 00:39:38,840
Yeah.

556
00:39:38,840 --> 00:39:42,440
And I think the question that I was, the way I was trying to ask the question previously

557
00:39:42,440 --> 00:39:51,160
was, is there a notion at all of either hierarchical reward functions or multiple reward functions

558
00:39:51,160 --> 00:40:00,560
that you try to optimize simultaneously, or is that just beyond the frontier of complexity

559
00:40:00,560 --> 00:40:01,960
for us right now?

560
00:40:01,960 --> 00:40:07,160
There was a really interesting recent result from DeepMind with a system called Impala.

561
00:40:07,160 --> 00:40:12,000
They showed that they could train on several different tasks at the same time and actually

562
00:40:12,000 --> 00:40:18,920
do better on task A because they had also studied task B, C, and T. That was actually pretty

563
00:40:18,920 --> 00:40:20,920
hard to get even that much working.

564
00:40:20,920 --> 00:40:28,040
And so would you expect it to be easier or harder to, I'm assuming in this case, there are

565
00:40:28,040 --> 00:40:32,280
separate rewards functions for each of these tasks.

566
00:40:32,280 --> 00:40:37,560
But I'm envisioning at least conceptually that you can have a single task with multiple

567
00:40:37,560 --> 00:40:40,440
reward functions.

568
00:40:40,440 --> 00:40:43,720
Would you expect that to be easier or harder than what they did?

569
00:40:43,720 --> 00:40:45,680
I would expect that to be easier.

570
00:40:45,680 --> 00:40:50,600
It's mostly that for most of the simulators we have, there's really one thing that you

571
00:40:50,600 --> 00:40:51,600
want to do.

572
00:40:51,600 --> 00:40:56,320
In the pung simulator, you want to play pung by knocking them all through the opponent's

573
00:40:56,320 --> 00:40:57,320
goal post.

574
00:40:57,320 --> 00:40:58,320
Yeah.

575
00:40:58,320 --> 00:41:02,800
I guess I'm thinking of something, maybe the simple example of the kind of thing I'm

576
00:41:02,800 --> 00:41:07,200
thinking of is the whole explorer exploit.

577
00:41:07,200 --> 00:41:14,320
Maybe there's, you know, you have a game that, you know, it's like a map based or world

578
00:41:14,320 --> 00:41:22,520
based game and you want, you know, one reward function to be when the game.

579
00:41:22,520 --> 00:41:29,280
But you also want to encourage your agent to explore the world.

580
00:41:29,280 --> 00:41:33,880
And so you might have, at least I'm envisioning, you'd have one reward function that correlates

581
00:41:33,880 --> 00:41:39,440
to the amount of the world that's been explored and another that correlates to winning the

582
00:41:39,440 --> 00:41:40,440
game.

583
00:41:40,440 --> 00:41:44,520
And, you know, what does it mean to kind of maximize both of those?

584
00:41:44,520 --> 00:41:45,520
Yeah.

585
00:41:45,520 --> 00:41:50,560
I think that that reminds me of hierarchical reinforcement learning, which is definitely

586
00:41:50,560 --> 00:41:53,800
a topic that is being studied.

587
00:41:53,800 --> 00:41:58,800
So in that sense, you have sort of different levels of agents.

588
00:41:58,800 --> 00:42:05,120
You have a planner that essentially decides, okay, what do I want to do?

589
00:42:05,120 --> 00:42:07,160
What is most important to do at this point in the game?

590
00:42:07,160 --> 00:42:08,160
Do I explore?

591
00:42:08,160 --> 00:42:09,160
Do I exploit?

592
00:42:09,160 --> 00:42:10,160
Do I do something else?

593
00:42:10,160 --> 00:42:17,880
And then that, they pass down that, I guess, that sub-reward to a policy that's been

594
00:42:17,880 --> 00:42:22,840
trained to, for example, explore really efficiently.

595
00:42:22,840 --> 00:42:26,440
And so you can learn this whole thing into end.

596
00:42:26,440 --> 00:42:29,360
I mean, DeepMine also had a paper on this last year.

597
00:42:29,360 --> 00:42:33,760
I forget what the, what the new hood it is, but yeah, where they're able to show that

598
00:42:33,760 --> 00:42:40,040
it is possible to train a giant policy that does hierarchical reinforcement learning and

599
00:42:40,040 --> 00:42:41,760
passes down these sub-rewards.

600
00:42:41,760 --> 00:42:47,400
I think this is really important for complex tasks, like trying to win a game of starcraft,

601
00:42:47,400 --> 00:42:52,360
for example, it might be, it might take a lot longer to train this end to end compared

602
00:42:52,360 --> 00:42:55,520
to if you're doing it in a hierarchical way.

603
00:42:55,520 --> 00:43:01,760
You know, we've talked about examples of, you know, fooling classifiers or fooling, you

604
00:43:01,760 --> 00:43:09,560
know, reinforcement-trained agents, which could be robots.

605
00:43:09,560 --> 00:43:14,880
But I'm wondering, given the focus of your work is on robotics, I'm wondering if there

606
00:43:14,880 --> 00:43:22,760
are any more subtle examples that you've come across or things that, you know, are areas

607
00:43:22,760 --> 00:43:32,320
of concern for the application of adversarial examples or adversarial training in the

608
00:43:32,320 --> 00:43:34,360
context of robotics.

609
00:43:34,360 --> 00:43:42,440
Yeah, I think one of the key problems is that adversarial examples make policies and

610
00:43:42,440 --> 00:43:49,520
they make robots less predictable, and it's harder to anticipate their behavior.

611
00:43:49,520 --> 00:43:58,160
And so if you are a human trying to interact with a robot or riding in a car, you have this

612
00:43:58,160 --> 00:44:04,000
mental model of how you think this robot is going to behave in the next, like, one, two

613
00:44:04,000 --> 00:44:05,000
seconds.

614
00:44:05,000 --> 00:44:09,040
And so the dangerous thing about adversarial examples is that that basically breaks your

615
00:44:09,040 --> 00:44:14,320
model and puts you in a position where you're not sure how to respond as a human, and so

616
00:44:14,320 --> 00:44:16,680
that's really dangerous.

617
00:44:16,680 --> 00:44:23,600
I think the fact that it does seem possible to introduce adversarial examples, whether

618
00:44:23,600 --> 00:44:31,240
you train this policy with reinforcement learning or with supervised learning, does seem,

619
00:44:31,240 --> 00:44:36,440
I mean, that's pretty, pretty scary.

620
00:44:36,440 --> 00:44:41,360
Although it is hard to get adversarial examples in the real world, and like Ian said, there's

621
00:44:41,360 --> 00:44:46,520
a lot of other ways in which robots can misbehave that don't depend on adversarial

622
00:44:46,520 --> 00:44:47,520
examples.

623
00:44:47,520 --> 00:44:55,600
And so the whole challenge of getting robots unpredictable and interpretable robots is

624
00:44:55,600 --> 00:45:00,280
much bigger than just trying to solve the problem of adversarial examples.

625
00:45:00,280 --> 00:45:06,520
What I'm really excited about is that adversarial examples give us a way of studying how robust

626
00:45:06,520 --> 00:45:11,080
a robotic system is in very concrete mathematical terms.

627
00:45:11,080 --> 00:45:16,480
We specify a model where we say all the things that an attacker can do, and then we try

628
00:45:16,480 --> 00:45:22,200
to prove that our policy will still work, even in the last case where the attacker chooses

629
00:45:22,200 --> 00:45:26,480
the thing that is the most likely to interfere with with the robot's abilities.

630
00:45:26,480 --> 00:45:30,160
So far we're not able to defend against that kind of attack, but in the future when our

631
00:45:30,160 --> 00:45:35,560
defense algorithms get better, if we're able to perform well in the worst case, it should

632
00:45:35,560 --> 00:45:40,720
also mean that we're always able to guarantee good performance in the average case, that if

633
00:45:40,720 --> 00:45:47,480
we're able to resist actual tempering, we can also be robust to things that interfere

634
00:45:47,480 --> 00:45:49,160
with robot policies right now.

635
00:45:49,160 --> 00:45:54,240
Like when you train a policy on one robot body and then run it on another robot body that

636
00:45:54,240 --> 00:45:58,960
is slightly different due to mechanical imperfections, that can be enough to interfere

637
00:45:58,960 --> 00:46:02,000
with that policy.

638
00:46:02,000 --> 00:46:06,960
That's actually a good segue to what I think will be our last question here.

639
00:46:06,960 --> 00:46:18,280
A lot of the work on the supervised learning camp of adversarial examples has been on

640
00:46:18,280 --> 00:46:25,160
architectures or methodologies for creating robustness in the networks to these kinds of

641
00:46:25,160 --> 00:46:27,360
attacks.

642
00:46:27,360 --> 00:46:34,360
Have you done or seen anything in the reinforcement learning world along those lines yet?

643
00:46:34,360 --> 00:46:38,960
Some of the follow-up work on our first paper actually used some of the techniques from

644
00:46:38,960 --> 00:46:42,120
classifiers to increase the robustness.

645
00:46:42,120 --> 00:46:48,960
So there was some work from CMU last year, it was called robust adversarial reinforcement

646
00:46:48,960 --> 00:46:49,960
learning.

647
00:46:49,960 --> 00:46:55,240
They were trying to, well, their definition of adversarial attacks was more physical,

648
00:46:55,240 --> 00:47:00,640
so you're training this locomotion agent in Mujoko, and the adversary can apply these

649
00:47:00,640 --> 00:47:03,240
forces to the agent.

650
00:47:03,240 --> 00:47:08,320
And you train the adversary actually in parallel with the policy that you're trying to train

651
00:47:08,320 --> 00:47:13,840
to get this agent to walk or make forward progress.

652
00:47:13,840 --> 00:47:21,000
And what they were able to find is that by training this agent to be able to walk despite

653
00:47:21,000 --> 00:47:25,800
these forces applied to it by the adversary, they're able to get an agent that was more

654
00:47:25,800 --> 00:47:32,360
robust in terms of being able to locomot across many different parameters of the environment

655
00:47:32,360 --> 00:47:36,760
in terms of friction or mass, different body parts of the agent, things like that.

656
00:47:36,760 --> 00:47:40,120
And Mujoko, that's a simulator, that's right.

657
00:47:40,120 --> 00:47:41,360
Yeah, that's a simulator.

658
00:47:41,360 --> 00:47:45,960
So that's where you have things like the half-cheetah and the humanoid on the swimmer,

659
00:47:45,960 --> 00:47:46,960
yeah.

660
00:47:46,960 --> 00:47:47,960
Great, great.

661
00:47:47,960 --> 00:47:53,440
Well, any final words from either of you, any parting thoughts or things that you'd

662
00:47:53,440 --> 00:47:57,960
like to point folks to if they're interested in learning more about this stuff?

663
00:47:57,960 --> 00:48:02,880
You could summarize a lot of what we talked about today as Goodheart's law in action.

664
00:48:02,880 --> 00:48:10,400
Goodheart's law is an idea that came from economics that says, once you use some value as a metric

665
00:48:10,400 --> 00:48:14,920
that you make it your target to optimize, it's no longer a good metric.

666
00:48:14,920 --> 00:48:20,640
And we see that happen with both adversarial examples and with reward hacking.

667
00:48:20,640 --> 00:48:25,640
If we use the output of a classifier as something that we're going to optimize, we find

668
00:48:25,640 --> 00:48:30,560
an adversarial example instead of a good input from a particular target class.

669
00:48:30,560 --> 00:48:36,160
And similarly, if a reinforcement learning agent optimizes its reward function too well,

670
00:48:36,160 --> 00:48:41,240
it can find ways of obtaining rewards that are serious and not doing what we actually

671
00:48:41,240 --> 00:48:43,160
hoped that our agent could do.

672
00:48:43,160 --> 00:48:45,560
Yeah, I think that's a great summary.

673
00:48:45,560 --> 00:48:51,480
Are there any specific implications of thinking of this stuff in terms of Goodheart's law?

674
00:48:51,480 --> 00:48:56,600
I guess one thing is just that it lets us see that there are many different things that

675
00:48:56,600 --> 00:49:01,040
all fall in the same category that you could think of reward hacking as a kind of adversarial

676
00:49:01,040 --> 00:49:02,800
example or vice versa.

677
00:49:02,800 --> 00:49:05,840
And you can see that solutions to one might help with the other.

678
00:49:05,840 --> 00:49:06,840
Got it.

679
00:49:06,840 --> 00:49:07,840
Awesome.

680
00:49:07,840 --> 00:49:12,360
Well, Ian Sandi, thank you both so much for taking the time to chat with us about this

681
00:49:12,360 --> 00:49:15,320
stuff is really interesting and important work.

682
00:49:15,320 --> 00:49:16,320
Thank you.

683
00:49:16,320 --> 00:49:17,320
Oh, thank you.

684
00:49:17,320 --> 00:49:18,320
Thank you.

685
00:49:18,320 --> 00:49:23,640
All right, everyone, that's our show for today.

686
00:49:23,640 --> 00:49:29,160
For more information on Ian, Sandi, or any of the topics covered in this episode, you'll

687
00:49:29,160 --> 00:49:35,120
find this show notes at twemolei.com slash talk slash 1-1-9.

688
00:49:35,120 --> 00:49:39,520
If you have any questions for Ian or Sandi, please post them there and we'll make sure

689
00:49:39,520 --> 00:49:41,400
to bring them to their attention.

690
00:49:41,400 --> 00:49:45,480
If you're new to the podcast and you like what you hear or you're a veteran listener

691
00:49:45,480 --> 00:49:48,280
and haven't already done so, please take a moment

692
00:49:48,280 --> 00:49:53,200
to head on over to your podcast app of choice and leave us your most gracious rating and

693
00:49:53,200 --> 00:49:54,360
review.

694
00:49:54,360 --> 00:49:57,800
It helps new listeners find us, which helps us grow.

695
00:49:57,800 --> 00:50:01,560
Thanks in advance and thanks so much for listening.

696
00:50:01,560 --> 00:50:27,040
Catch you next time.

