WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:36.680
This week's shows are drawn from some of the great conversations I had at the recent Nvidia

00:36.680 --> 00:40.960
GPU technology conference and they're brought to you by Dell.

00:40.960 --> 00:44.840
If you caught my tweets from GTC, you may already know that one of the announcements this

00:44.840 --> 00:49.320
year was a new reference architecture for data science work sessions powered by high

00:49.320 --> 00:54.440
NGPUs and accelerated software such as Nvidia's Rabbids.

00:54.440 --> 00:58.960
Dell was among the key partners showcased during the launch and offers a line of workstations

00:58.960 --> 01:03.040
designed for modern machine learning and AI workloads.

01:03.040 --> 01:07.720
To learn more about Dell precision workstations and some of the ways they're being used by customers

01:07.720 --> 01:12.320
in industries like media and entertainment, engineering and manufacturing, healthcare

01:12.320 --> 01:23.520
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.

01:23.520 --> 01:26.440
Alright everyone, I am on the line with Paul Moller.

01:26.440 --> 01:31.680
Paul is a senior data scientist and technical product manager for machine learning at Nvidia.

01:31.680 --> 01:35.080
Paul, welcome to this week in machine learning and AI.

01:35.080 --> 01:36.400
Thanks for having me.

01:36.400 --> 01:37.400
Absolutely.

01:37.400 --> 01:42.960
I'm looking forward to jumping into our conversation which will be focused on what Nvidia is doing

01:42.960 --> 01:47.800
around rapids and QML and all of the interesting stuff in that area.

01:47.800 --> 01:51.160
But before we do that, you were a philosophy major.

01:51.160 --> 01:55.760
How did you make your way to working in machine learning?

01:55.760 --> 02:02.320
I was a philosophy major and two years before I was set to graduate, I added economics as

02:02.320 --> 02:09.040
a major because I read the economist magazine and thought that it was a fascinating collection

02:09.040 --> 02:13.640
of a bunch of different articles about different aspects of the world.

02:13.640 --> 02:19.440
So I figured if that's what economists read, I wanted to be an economist.

02:19.440 --> 02:27.200
I went on to do a master's degree in economics where I mostly focused on quantitative methods.

02:27.200 --> 02:34.360
In an earlier life, I went out to Washington, D.C. where I worked as an economist.

02:34.360 --> 02:42.040
I began my career at the World Bank, serving on health and human welfare issues in sub-Saharan

02:42.040 --> 02:43.520
Africa.

02:43.520 --> 02:49.200
And then I worked for the office of the chief economist at Fannie Mae.

02:49.200 --> 02:54.480
Now one day, while waiting for the bus to get home from Fannie Mae, there was an article

02:54.480 --> 02:59.800
from the New York Times about a gentleman from SUNY Buffalo that had written an algorithm

02:59.800 --> 03:03.040
that was offering notes on screenplays.

03:03.040 --> 03:09.560
And I always had like a hobby interest in the different creative arts aspects of our

03:09.560 --> 03:10.560
culture.

03:10.560 --> 03:16.680
And when I saw that somebody had written essentially a big block of math and code that was telling

03:16.680 --> 03:23.440
writers how to write their screenplays better, I immediately decided that I needed to switch

03:23.440 --> 03:30.160
into data science or big data, which was the more popular term at the time, because that

03:30.160 --> 03:34.240
was where some of the most interesting things in the world were going on.

03:34.240 --> 03:35.760
That's a great story.

03:35.760 --> 03:42.120
You remember the moment, literally, the bus ride that triggered the, that set you off

03:42.120 --> 03:43.880
down this path.

03:43.880 --> 03:48.680
Yeah, it was raining at the time.

03:48.680 --> 03:49.680
Awesome.

03:49.680 --> 03:51.480
And so, what do you do now?

03:51.480 --> 03:54.200
I've been doing for the last year.

03:54.200 --> 04:00.320
I had been at a couple of startups, and I joined in the video to work on what we've been

04:00.320 --> 04:05.240
calling the Rapids project or the Rapids ecosystem.

04:05.240 --> 04:13.000
Now what that began as is that our director of engineering, who I'd worked with previously

04:13.000 --> 04:21.440
at Accenture, for years had been saying that we see this great acceleration in neuro-acceleration

04:21.440 --> 04:26.040
neural network methods as a result of getting them on GPUs.

04:26.040 --> 04:31.640
But you are not seeing any of those advantages for more of the bread and butter data science

04:31.640 --> 04:37.200
that happens at a lot of places like Fannie Mae or the World Bank, where they may have large

04:37.200 --> 04:41.920
data sets and questions they want to address through machine learning, but we aren't talking

04:41.920 --> 04:49.120
about convolutional neural networks to understand images.

04:49.120 --> 04:56.520
So, you know, a lot of the time when I was working at a data scientist at a couple of startups,

04:56.520 --> 05:03.360
I like to joke that I was really a bar trivia champion, because while I was waiting for

05:03.360 --> 05:09.200
my code to finish running and spit out my result, I had plenty of time to read all the news

05:09.200 --> 05:13.280
of the world on the internet.

05:13.280 --> 05:19.040
And I guess, you know, unfortunately, for me and my compatriots in data science, what

05:19.040 --> 05:25.880
we've done with QML and QDF in particular is, you know, if somebody knows pandas or they

05:25.880 --> 05:33.080
know the Pi data ecosystem, they can immediately jump right in and start seeing just crazy

05:33.080 --> 05:39.880
speed ups, like, you know, 50X, like, sometimes more than that on doing their end-to-end workflows.

05:39.880 --> 05:46.640
And that includes, you know, reading from a disk to GPU memory, doing all your data

05:46.640 --> 05:53.400
munging and merging and variable creation through actually executing your algorithm and, you

05:53.400 --> 05:56.080
know, making inferences.

05:56.080 --> 06:02.560
So the idea was that since all of this, well, I mean, some of it is not obviously tractable

06:02.560 --> 06:10.520
to GPUs, we are able to process strings in the latest iteration of QDF, which to me seems

06:10.520 --> 06:16.480
like a miracle, but it really, I like to joke that it's kind of like before, you know,

06:16.480 --> 06:22.640
the team that I work with had delivered these big pieces of QDF, it's like I could drive

06:22.640 --> 06:25.680
a car, and now suddenly I can fly a plane.

06:25.680 --> 06:32.840
And I don't need to be an expert in CUDA or parallel algorithms or anything except the

06:32.840 --> 06:35.280
tools that I've worked with most of my career.

06:35.280 --> 06:38.320
Now, let's take a step back, maybe.

06:38.320 --> 06:45.080
When I introduce you, I mentioned QML, you've mentioned QDF, mentioned Rapids.

06:45.080 --> 06:52.720
Can you kind of paint a picture of the broader ecosystem of software and libraries and

06:52.720 --> 07:00.280
tools that comprise our makeup, Rapids, and, you know, how they all fit together?

07:00.280 --> 07:01.280
Yeah.

07:01.280 --> 07:06.920
So Rapids is the overall name of the project, and that's made up of smaller sub libraries

07:06.920 --> 07:12.200
that all start with CUDA, because that's inherited from CUDA.

07:12.200 --> 07:14.640
Rapids is built on Nvidia CUDA.

07:14.640 --> 07:22.440
And CUDA is the, for anyone who's not familiar with the underlying library or API for

07:22.440 --> 07:24.680
doing things on Nvidia GPUs.

07:24.680 --> 07:25.680
Right.

07:25.680 --> 07:30.520
It's the general purpose computing library for Nvidia GPUs.

07:30.520 --> 07:31.520
Okay.

07:31.520 --> 07:40.160
And if you think about the more broad, pie data ecosystem, I think a lot of people do

07:40.160 --> 07:45.680
a lot of their initial data cleaning and exploration in pandas.

07:45.680 --> 07:52.400
And so that's what CUDA is meant to replace for people that are moving their workload

07:52.400 --> 07:54.080
on to GPUs.

07:54.080 --> 08:04.280
And so the API is very, very close, and you're able to, in some cases, just change the import

08:04.280 --> 08:09.560
statements at the top of your program, and it will just work.

08:09.560 --> 08:16.520
And so pandas has this kind of core abstraction of a data frame, and so CUDA is just a kind

08:16.520 --> 08:19.080
of, you can think of it as a CUDA-powered data frame.

08:19.080 --> 08:20.080
Yeah.

08:20.080 --> 08:21.680
I think that's the best way to think about it.

08:21.680 --> 08:22.680
Yeah.

08:22.680 --> 08:32.920
QML is our machine learning toolkit, and we aspire to, one day have almost all the functionality

08:32.920 --> 08:36.200
that exists in Scikit Learn.

08:36.200 --> 08:40.640
Scikit Learn is an eminent package built by some of the world's greatest developers.

08:40.640 --> 08:42.840
So we've got a ways to go there.

08:42.840 --> 08:49.080
But we've been rapidly adding algorithms in the last release, for example.

08:49.080 --> 08:58.120
We have stochastic gradient descent regression, ordinary linear regression, ridge regression,

08:58.120 --> 09:04.200
principle components analysis, and some other things like Kalman filtering.

09:04.200 --> 09:12.160
What we're trying to do is start with the things that are the real workhorses of day-to-day

09:12.160 --> 09:17.680
machine learning in business and other parts of industry.

09:17.680 --> 09:20.360
And it's been exciting to watch the package grow.

09:20.360 --> 09:30.000
In fact, when we launched back in October, we had, I think, four algorithms in QML, and

09:30.000 --> 09:32.280
we've doubled that over the last couple of months.

09:32.280 --> 09:40.120
It was very exciting to present at the GPU developer's conference in San Jose, California,

09:40.120 --> 09:46.280
a couple of weeks ago, to the wider community, all the things that we had been able to deliver

09:46.280 --> 09:48.480
in such a short amount of time.

09:48.480 --> 09:52.360
You mentioned that you kind of aspired to Scikit Learn.

09:52.360 --> 09:58.040
Does that mean that QML replaces Scikit Learn?

09:58.040 --> 10:05.560
It sounds like it does, for folks that are trying to take advantage of the GPU.

10:05.560 --> 10:11.440
And was there an opportunity to rather than replacing Scikit Learn kind of fit in underneath

10:11.440 --> 10:19.200
it so folks can that use that API or that have existing work that uses Scikit Learn could

10:19.200 --> 10:25.680
take advantage of the GPU acceleration without having to rewrite their apps?

10:25.680 --> 10:33.680
I mean, at least with the algorithms we've delivered, we've tried to keep the API one-to-one.

10:33.680 --> 10:38.120
For any of your listeners, I would encourage them to take a look at the API and just see

10:38.120 --> 10:39.920
how close it is to Scikit Learn.

10:39.920 --> 10:46.560
I'd also like to add that we've partnered with Enrea, the French research institution

10:46.560 --> 10:54.840
that does a lot of work on Scikit Learn and over the next few months and few years we're

10:54.840 --> 10:58.240
going to be building that collaboration with them.

10:58.240 --> 11:04.640
I don't think we'll ever replace Scikit Learn because there are still problems where I

11:04.640 --> 11:11.600
don't think it's big enough or the use case is right to necessarily go to full GPUs.

11:11.600 --> 11:18.840
So I think of certain analyses I did as an economist which would look like machine learning

11:18.840 --> 11:26.720
but we're maybe a few thousand rows and this was much more traditional frequentist statistics.

11:26.720 --> 11:32.080
I think that there's always going to be a lot of that work being done and I think that

11:32.080 --> 11:37.320
with any data science work it's about finding the right tool for the job.

11:37.320 --> 11:48.000
But I will tell you when I was testing out our code earlier in the summer, our demo workflow

11:48.000 --> 11:55.880
involved reading in, I think around like a gigabyte of CSV data to pandas and on my MacBook

11:55.880 --> 12:04.800
Pro it took like five and a half minutes and on a single GPU it took like 15 or 20 seconds.

12:04.800 --> 12:15.320
When you talk about data analysis is an iterative interactive process and the faster you can

12:15.320 --> 12:21.160
move the more fluid your conversation with the data will feel to you as a user.

12:21.160 --> 12:26.400
There won't be the long wait times to see results or see if you made a coding error in

12:26.400 --> 12:27.400
my case.

12:27.400 --> 12:31.360
That's an opportunity to become a bar trivia master.

12:31.360 --> 12:34.960
Yeah, it's a shock.

12:34.960 --> 12:39.920
So before we dig into that because that's an interesting point there, we're kind of

12:39.920 --> 12:41.640
talking about the landscape.

12:41.640 --> 12:44.360
You mentioned Kudieff, Kuh-A-Mal.

12:44.360 --> 12:48.680
Are there other major pieces that we should be keeping track of in this conversation?

12:48.680 --> 13:00.920
Yeah, we are working on a graph analytics package called Kuh-Graph and yeah, our minds

13:00.920 --> 13:05.640
are so fixated on accelerating the algorithms, we're totally out of bandwidth for fancier

13:05.640 --> 13:06.640
names.

13:06.640 --> 13:12.160
But everyone knows that Jensen does all the naming it in videos, why would anyone else

13:12.160 --> 13:16.000
spend any time thinking about that?

13:16.000 --> 13:21.040
Kuh-Graph is embarrassing in a sense that we compare it to a graph analytics package

13:21.040 --> 13:26.560
in Python and it's one of those things where you see the numbers, you just really want

13:26.560 --> 13:31.320
to double check them, like 10,000 time speed ups over network X for certain algorithms.

13:31.320 --> 13:37.280
Well, that was my reaction to the loading the data frame.

13:37.280 --> 13:40.440
And I still want to get through the kind of broad landscape before we dig deep into

13:40.440 --> 13:44.680
that, but that's the first place I'm going to come back to once we do.

13:44.680 --> 13:51.520
So you've got a graph analytics piece in Kuh-Graph, any other major components here?

13:51.520 --> 13:55.800
Some of the things that began as major components are now under the hood.

13:55.800 --> 14:00.600
So we put a bunch of effort into building a string reader.

14:00.600 --> 14:07.320
So you could directly parse data sets with strings, it's a very common thing that happens

14:07.320 --> 14:13.080
in data science, GPUs do not like strings.

14:13.080 --> 14:21.240
But now you can do things like just easily create your dummy variables from strings on

14:21.240 --> 14:26.920
the GPU, which sounds kind of ho-hum, but it's actually a pretty major achievement.

14:26.920 --> 14:29.000
It's just part of the whole speeding things up.

14:29.000 --> 14:34.160
I don't think our case would be as compelling if we said that it could only be numerical

14:34.160 --> 14:37.840
data in the Kuh-Graph data frame.

14:37.840 --> 14:40.680
That simply will not work for many use cases.

14:40.680 --> 14:51.200
So we also have a package called Kuh-Cross filter, it's written as Kuh-X filter, and we're

14:51.200 --> 14:58.640
going to be building out some GPU-accelerated visualizations.

14:58.640 --> 15:07.240
So if you think of the workflow from munging to analysis to insight through visualizations,

15:07.240 --> 15:11.480
we want to be able to offer every piece of that puzzle.

15:11.480 --> 15:17.600
The other thing, we're heavily using a software called DASC.

15:17.600 --> 15:24.240
And DASC is a package that handles distributed computing.

15:24.240 --> 15:30.720
It has been used to scale out pandas, for example, to multiple cores.

15:30.720 --> 15:37.960
And we were lucky enough that the creator of DASC has joined our team, and is helping

15:37.960 --> 15:45.240
us use that as a way to distribute workloads when we're talking about moving beyond single

15:45.240 --> 15:47.280
node of GPUs.

15:47.280 --> 15:54.280
Let's maybe go back to the initial example you gave of loading the pandas data frame.

15:54.280 --> 15:56.640
We're loading the data frame.

15:56.640 --> 15:58.040
You said it was a terabyte?

15:58.040 --> 15:59.640
It was a gigabyte.

15:59.640 --> 16:05.000
It's pretty easy to choke pandas, and I'm sure a lot of your listeners have experienced

16:05.000 --> 16:07.160
this before.

16:07.160 --> 16:16.560
The workflow was Fannie Mae makes loan-delinquency data going back, I think, 16 years available

16:16.560 --> 16:18.200
for free.

16:18.200 --> 16:26.440
And this is the whole payment history for a subset of all the loans that Fannie Mae has

16:26.440 --> 16:27.840
acquired.

16:27.840 --> 16:35.600
And as a demo workflow, what we wanted to do is read in however many quarters of data

16:35.600 --> 16:45.160
we could fit, or were relevant, and then apply XGBoost to predict default, which reminds

16:45.160 --> 16:49.160
me of another sort of under-the-hood improvement that we've made.

16:49.160 --> 16:56.720
It's not really under-the-hood, but we have made contributions to the DMLC XGBoost library,

16:56.720 --> 16:59.560
and we'll continue to do so.

16:59.560 --> 17:06.960
That has appeared in a lot of our early presentations and webinars.

17:06.960 --> 17:12.800
I think XGBoost is almost like magic, and it's a good, broad workhorse for the first thing

17:12.800 --> 17:14.200
that we were going to introduce.

17:14.200 --> 17:24.000
But we are working with the community to make certain changes to XGBoost that make it more

17:24.000 --> 17:29.920
amenable for the rapid ecosystem, but then giving those back to the community.

17:29.920 --> 17:41.200
So pardon that sidebar, but before any real data processing had happened, just bringing

17:41.200 --> 17:49.400
in this dense, large data set, at a rule of thumb, we couldn't do more than a couple of

17:49.400 --> 17:50.720
quarters of data.

17:50.720 --> 17:56.240
And then you would really see the time to load and go through the data preparation and

17:56.240 --> 18:00.760
execute the algorithm increase substantially.

18:00.760 --> 18:10.000
So I think that the largest we could do on pandas quarter-wise was like two or three quarters

18:10.000 --> 18:11.000
of data.

18:11.000 --> 18:12.000
They were small.

18:12.000 --> 18:15.040
So I think the biggest we tried was about one and a half gigabytes.

18:15.040 --> 18:21.840
And that's where you saw those really kind of frustrating load times of more than five

18:21.840 --> 18:22.840
minutes.

18:22.840 --> 18:31.800
And so what's happening here in these two different scenarios is on the panda side, you've got a gig

18:31.800 --> 18:37.840
and a half on both sides, you've got a gig and a half of information on a disk, probably

18:37.840 --> 18:45.000
a CSV file, perhaps, and you're loading it into a data frame on the panda side.

18:45.000 --> 18:48.080
It's a data frame that's located in RAM.

18:48.080 --> 18:54.760
And on the rapid side, it's a QDF frame that's located on the GPU itself.

18:54.760 --> 18:55.760
Is that right?

18:55.760 --> 18:56.760
Yep.

18:56.760 --> 18:57.760
In GPU memory.

18:57.760 --> 18:58.760
Okay.

18:58.760 --> 19:05.040
And that's where the kind of the five minute versus 15 seconds differentiation come from.

19:05.040 --> 19:06.040
Yeah.

19:06.040 --> 19:11.080
And this is where this really is more for like our hardcore C guys, but there's some

19:11.080 --> 19:15.320
problems with pandas that have been known for a while.

19:15.320 --> 19:16.320
It's great.

19:16.320 --> 19:23.840
I can't thank West McKinney and the community enough for open sourcing it because it's put

19:23.840 --> 19:25.840
food on my plate for five or six years.

19:25.840 --> 19:30.600
But it's also single threaded on the CPU.

19:30.600 --> 19:37.720
So even in the world of CPUs, you started to see people look for things like desk to help

19:37.720 --> 19:39.320
better leverage.

19:39.320 --> 19:44.000
And the multiple cores of CPUs you might have on your MacBook Pro.

19:44.000 --> 19:48.760
I think it's, you know, I think it's kind of funny, you know, every, every data science

19:48.760 --> 19:53.200
gig I've had, they've given me like a shiny MacBook Pro and I mostly work in Jupyter

19:53.200 --> 19:54.200
notebooks.

19:54.200 --> 20:00.000
And most of that stuff is only taking advantage of a single core of the processor.

20:00.000 --> 20:01.000
Right.

20:01.000 --> 20:02.000
Right.

20:02.000 --> 20:07.800
I mean, I, I don't want to exaggerate too much, but it's almost like you still want

20:07.800 --> 20:09.800
the next MacBook Pro.

20:09.800 --> 20:10.800
Yeah.

20:10.800 --> 20:11.800
Well, wait and see.

20:11.800 --> 20:13.000
I want to see what they do with that touch bar.

20:13.000 --> 20:21.320
I got some thoughts on the touch bar, but so let's not even go there.

20:21.320 --> 20:27.720
But you know, and so the other thing is that when we, when we move past the, you know,

20:27.720 --> 20:36.920
the massive parallelism that you can get from using GPUs, when you get to the side,

20:36.920 --> 20:37.920
I know better.

20:37.920 --> 20:44.400
This is all for the most part, matrix algebra and GPUs love matrix algebra.

20:44.400 --> 20:53.280
They are designs to do it and, you know, in our algorithms like doing, um, Ridge regression,

20:53.280 --> 21:00.720
which can take a some time to run on, uh, in the conventional, pydated ecosystem, um,

21:00.720 --> 21:08.800
I gave a, uh, tutorial at the GPU, uh, developers, uh, GPU technology convention, I was just

21:08.800 --> 21:13.720
like, we're going to just do a hyper parameter search and run through like a thousand Ridge

21:13.720 --> 21:19.720
regressions on this Black Friday data set because it's just fast enough that we can kind

21:19.720 --> 21:23.800
of brute force hyper parameter search on certain data set sizes.

21:23.800 --> 21:29.320
And, uh, do you have comparative results for that particular scenario?

21:29.320 --> 21:31.680
We aren't baking off every algorithm these days.

21:31.680 --> 21:32.760
Ridge regression is fast.

21:32.760 --> 21:33.760
I don't know.

21:33.760 --> 21:38.840
Like doing a Ridge regression, I think like 800,000 rows took me like, uh, less than a second

21:38.840 --> 21:39.960
couple of seconds.

21:39.960 --> 21:44.240
I remember being fast enough where I could do a live demo and run through a hundred iterations

21:44.240 --> 21:45.240
of it.

21:45.240 --> 21:48.600
Normally doing two or three would have taken quite some time.

21:48.600 --> 21:55.440
Let's jump into the, uh, to this, this part of it, the QML library.

21:55.440 --> 22:02.080
Can you maybe talk us through the, uh, technical underpinnings of this?

22:02.080 --> 22:06.480
I mean, is it as simple as, hey, these things love matrix multiplication and we're just

22:06.480 --> 22:12.880
doing matrix multiplication using kuda and it's just faster or they're kind of interesting

22:12.880 --> 22:19.280
nuances to the way some of these algorithms work that, uh, might be worth chatting about.

22:19.280 --> 22:24.720
Um, well, to start with a little bit about the, uh, architecture of QML.

22:24.720 --> 22:32.000
Um, so QML is built on top of what we're calling ML prams.

22:32.000 --> 22:40.840
Um, these are, uh, primitive functions that are composed of even lower level math libraries

22:40.840 --> 22:46.480
or various things that have been developed at Nvidia for certain linear algebra purposes.

22:46.480 --> 22:53.640
And so, um, we take these primitives and they are, uh, delivered in C++.

22:53.640 --> 23:01.880
So then when we need something new, um, like, uh, I have a colleague working on, um, doing

23:01.880 --> 23:05.480
massively parallel arena regressions.

23:05.480 --> 23:12.580
And so when he began working on that, we already had, uh, Kalman filter primitive and an

23:12.580 --> 23:14.040
OLS primitive.

23:14.040 --> 23:21.000
And so the amount of new work that he needed to begin composing a prototype was dramatically

23:21.000 --> 23:22.400
reduced.

23:22.400 --> 23:27.840
One day in the future, I actually want to see these ML prams wrapped in Python.

23:27.840 --> 23:33.360
So, um, different, you know, machine learning researchers or graduate students that aren't

23:33.360 --> 23:39.920
experts in parallel programming would be able to, um, mock up the new algorithms they're

23:39.920 --> 23:44.680
inventing and be able to, uh, leverage the advantages of the GPU.

23:44.680 --> 23:47.880
Yeah, that sounds like a, uh, that sounds like a no brainer.

23:47.880 --> 23:52.680
The ML Prams open source or they locked up in a binary or something.

23:52.680 --> 23:57.520
I mean, even if they're in a binary, they want to be able to, yeah, that's a mix.

23:57.520 --> 24:01.040
I mean, so some of the stuff is, uh, Nvidia proprietary stuff and we've tried to wrap it

24:01.040 --> 24:04.680
in a binary, but other ones are more open source.

24:04.680 --> 24:07.360
And it's a, it's a discussion that we're going to continue to have.

24:07.360 --> 24:09.720
But wherever that ends up, oh, sorry, go ahead.

24:09.720 --> 24:17.440
Is it a well documented, uh, you know, set of primitives or is it kind of internal, nobody

24:17.440 --> 24:21.880
really knows about them outside of the company?

24:21.880 --> 24:26.080
It's something that, you know, um, I've tried to mention publicly, whatever we speak about

24:26.080 --> 24:27.080
it.

24:27.080 --> 24:28.560
It's not super well documented right now.

24:28.560 --> 24:33.840
So you'd need to, uh, you need to be able to go into, uh, our GitHub folder and look

24:33.840 --> 24:39.920
at the primitives folder and be able to read a little bit of C C plus plus got it.

24:39.920 --> 24:40.920
Okay.

24:40.920 --> 24:47.080
Um, but, uh, yeah, I hopefully, uh, want to, you know, wrap these in Python and introduce

24:47.080 --> 24:50.640
them to the greater development community to see what else they can do with them.

24:50.640 --> 24:54.680
Um, now you've mentioned, are there, are there some algorithms that are more or less

24:54.680 --> 24:55.680
tractable to that?

24:55.680 --> 25:03.400
I mean, right now we're working on building a lot of, uh, different solvers for more exotic

25:03.400 --> 25:12.040
kinds of, um, regressions and one of the challenges in developing those has been, um, and this

25:12.040 --> 25:17.760
really is where, you know, I'm, uh, you know, all of some of the, the thinking of my colleagues,

25:17.760 --> 25:22.760
they're essentially sequential, uh, algorithms, the way that they originally designed, right?

25:22.760 --> 25:28.400
If you look at the most basic version of like, uh, a gradient descent, you, you know, start

25:28.400 --> 25:32.960
some place and you keep taking little steps until you're satisfied and then you stop.

25:32.960 --> 25:40.240
Now that's, um, uh, sequential operation, um, when we've been doing some early inspections

25:40.240 --> 25:47.240
on different solvers, um, this morning, in fact, um, a colleague told me that he was disappointed

25:47.240 --> 25:52.560
that we were only getting a 3x speed up because he was still trying to think around how to

25:52.560 --> 25:55.400
make the algorithm less sequential.

25:55.400 --> 26:01.000
So there will be things just by the way that the underlying math works that aren't going

26:01.000 --> 26:08.680
to necessarily be another 10,000 x speed up, but a two or three x speed up, I think is still

26:08.680 --> 26:10.040
pretty great.

26:10.040 --> 26:16.800
And the other, you know, really heavy intellectual work that, um, is going on right now, um, that

26:16.800 --> 26:23.360
we hope to wrap up by the end of the summer or the fall is going to be on, um, multi-node,

26:23.360 --> 26:24.680
multi-GPU algorithms.

26:24.680 --> 26:26.720
Um, that's using DASC.

26:26.720 --> 26:34.960
Uh, in some cases, it'll use DASC and we're currently working our way through what, um,

26:34.960 --> 26:40.880
other kind of, uh, communications layers could be helpful in trying to, you know, block

26:40.880 --> 26:47.200
up this data and distribute it across, uh, a cluster of GPUs in a way that creates, uh,

26:47.200 --> 26:49.440
you know, a wow moment for the user.

26:49.440 --> 26:50.440
Mm-hmm.

26:50.440 --> 26:55.040
I know that's a little marketing, but that's what I'm, I'm looking at, I'm looking at what,

26:55.040 --> 26:58.320
you know, we're doing with QML, like that's what I'd really like.

26:58.320 --> 27:03.200
You know, we've, we've been lucky so far that, um, we have gotten some wows with what

27:03.200 --> 27:10.840
we deliver, but the, the underlying, uh, you know, algebra and algorithms of breaking

27:10.840 --> 27:18.560
some of these things into parallel jobs is, uh, very far from trivial.

27:18.560 --> 27:25.560
You mentioned that, uh, some of the things that you're doing are allowing you to load

27:25.560 --> 27:29.200
your strings onto the GPU.

27:29.200 --> 27:40.160
Are you able to utilize the GPU for kind of, uh, heavy, heavy kind of NLP types of algorithms?

27:40.160 --> 27:45.160
I guess for a lot of those, you're kind of numericalizing the textual data anyway, but,

27:45.160 --> 27:48.480
are there any limitations there, one way or the other?

27:48.480 --> 27:54.400
There are some limitations, and we do want to do ML, uh, NLP, um, we're not quite there

27:54.400 --> 27:55.400
yet.

27:55.400 --> 28:01.520
So, um, we're on a, uh, implementation of word to veck, um, in terms of, uh, preparing

28:01.520 --> 28:08.720
or understanding your data, we have a lot of, um, ordinary string functions like, uh, token

28:08.720 --> 28:17.440
izers, um, we have a regular expressions engine, so you can, um, search for regular expressions

28:17.440 --> 28:22.120
on strings and substrings and use that to create variables.

28:22.120 --> 28:26.160
I think probably closer towards the end of the year would be the soonest I expect us

28:26.160 --> 28:29.840
that we're going to deliver that, but it's certainly something that we're interested

28:29.840 --> 28:33.000
in and are currently working on.

28:33.000 --> 28:40.120
And in just in terms of like the, the less, the kind of the pre-processing to NLP is,

28:40.120 --> 28:41.440
is where we started.

28:41.440 --> 28:47.720
Um, and so, uh, now that we have, you know, released and continue to iterate on our, our

28:47.720 --> 28:54.120
string manipulations package, that's going to lay the foundation for, um, NLP practitioners

28:54.120 --> 28:58.840
to be able to work in a way that they're used to and have algorithms like word to veck,

28:58.840 --> 29:06.560
um, or LDA or other things like that, um, open and available to them.

29:06.560 --> 29:16.640
One of the things I'm curious about is in the Clomel library are all of these algorithms

29:16.640 --> 29:21.440
and everything you're doing with that library only dependent on the GPU.

29:21.440 --> 29:29.120
In other words, are you doing all of the compute in the GPU, uh, or are you using the, the

29:29.120 --> 29:35.920
CPU, you know, as needed or, or where appropriate and, you know, and I'm wondering more, you

29:35.920 --> 29:40.440
know, more broadly is it kind of an all or nothing kind of thing or is, is the focus

29:40.440 --> 29:46.200
really on doing a given operation in the best place for that operation?

29:46.200 --> 29:53.960
I think it's doing, uh, a given operation, um, where it's best suited, but, um, we're

29:53.960 --> 30:00.520
really just looking at things that are, are suited to the GPU, uh, that's actually,

30:00.520 --> 30:08.280
so doing everything where it best, where it's best suited, but everything is best suited

30:08.280 --> 30:16.040
in the GPU.

30:16.040 --> 30:21.600
Our work has been focused on, um, you know, once the, especially on the Clomel side, once

30:21.600 --> 30:25.120
the data is in GPU memory, we don't want to move it around.

30:25.120 --> 30:31.600
Um, that's kind of one of the big advantages that we have by doing int and data science.

30:31.600 --> 30:34.120
We're dramatically cutting down on these read rights.

30:34.120 --> 30:40.120
So like the data will come in and it'll sit as a coup df data frame and I can immediately

30:40.120 --> 30:45.200
pass it into my algorithm and it all stays in one place.

30:45.200 --> 30:51.960
So, um, we're able to cut down some of the overhead by thinking really hard about, um,

30:51.960 --> 30:57.560
reducing, uh, the copies that happen in the course of doing this and all of those.

30:57.560 --> 31:04.640
I imagine that there's a bit of a dichotomy or decision point around, you know, do you

31:04.640 --> 31:11.600
kind of optimize around these NN workflows and then everyone who, you know, wants to do

31:11.600 --> 31:16.840
anything that utilizes the GPU and takes advantage of what you're doing needs to wait

31:16.840 --> 31:23.320
for you to build their algorithm, you know, or, you know, is there some way, you know,

31:23.320 --> 31:28.800
if I want to do something that, you know, that QML offers an optimized version of, but

31:28.800 --> 31:34.760
I also need to do stuff that, uh, for which there's not a QML optimized version, do I

31:34.760 --> 31:40.800
need to load, you know, do the five minute pen to load in the RAM and the load into the

31:40.800 --> 31:48.360
GPU and kind of go back and forth each time or, you know, is there some kind of scenario

31:48.360 --> 31:57.600
where I can load into the GPU and keep the data there, but also do CPU based operations

31:57.600 --> 31:58.600
against it.

31:58.600 --> 32:02.200
I don't even know if technically that makes any sense, you know, or is feasible.

32:02.200 --> 32:07.000
We have a bunch of different formats you can export data from QDF.

32:07.000 --> 32:12.280
So if you had an algorithm that we haven't built at, um, and I'd say anybody is welcome

32:12.280 --> 32:13.280
to join in.

32:13.280 --> 32:14.480
This is an open source project.

32:14.480 --> 32:18.960
We'd love to have, uh, any help we can get building these algorithms out, but if you have

32:18.960 --> 32:23.120
something that you need to do for work today and you're like, could you have sounds like,

32:23.120 --> 32:28.400
like a great way to speed up the pandas part of my workflow, but I am going to do a bunch

32:28.400 --> 32:32.920
of algorithms, for example, um, I want to do a bunch of Bayesian stuff.

32:32.920 --> 32:40.520
Um, we can export data from the GPU data frame into a pandas data frame.

32:40.520 --> 32:46.560
If that's what we'd like to work with it, um, into an empty array, um, we support the

32:46.560 --> 32:54.360
arrow data format, um, and we also just introduced support for DL pack, which plays nicely with

32:54.360 --> 32:57.800
a handful of the deep learning packages like PyTorch.

32:57.800 --> 33:04.440
So, um, while we're working as hard as we can to add more algorithms to it for the user

33:04.440 --> 33:11.400
community, um, you can, uh, pick and choose what's, what's most useful to you at this time.

33:11.400 --> 33:16.240
I'd also like to add that, um, QML can, uh, take in Numpy arrays.

33:16.240 --> 33:22.480
So these packages were designs to be closely used together, but, um, we know that that's,

33:22.480 --> 33:28.480
uh, not going to cover all users and, um, that's not, uh, necessarily a requirement.

33:28.480 --> 33:34.680
The other piece of this is QGraph, and I gather that's, uh, a newer, kind of more emerging

33:34.680 --> 33:37.000
part of the rapid ecosystem.

33:37.000 --> 33:39.000
Uh, yeah it is.

33:39.000 --> 33:43.720
Um, I don't have the deepest knowledge of, uh, QGraph on the team.

33:43.720 --> 33:49.840
Uh, I'm not, uh, really a, uh, graph guy, but, um, I know that their benchmarks have been

33:49.840 --> 33:55.520
fabulous and we're hoping to make more graph algorithms available to people that heavily

33:55.520 --> 33:58.840
rely on graph theory in their, uh, day to day work.

33:58.840 --> 34:06.080
And then maybe switching gears a little bit beyond the work that's happening, uh, in

34:06.080 --> 34:07.080
rapids.

34:07.080 --> 34:15.320
Uh, one of the things that was mentioned at this recent GTC was some, uh, announcements,

34:15.320 --> 34:21.400
uh, and partnerships around, uh, creating a reference architecture for data science

34:21.400 --> 34:22.400
workstations.

34:22.400 --> 34:24.080
What can you tell us about that initiative?

34:24.080 --> 34:25.080
Yeah.

34:25.080 --> 34:32.040
The reference architecture for, um, data science workstations is, uh, very new, but, um,

34:32.040 --> 34:37.520
what I think is exciting about it is that, uh, for, for people that are able to get an

34:37.520 --> 34:42.720
Nvidia data science workstation, um, we are going to have the, uh, the software that's

34:42.720 --> 34:49.200
going to be heavily based on rapids laid out, um, ideally so once you get the data science

34:49.200 --> 34:55.040
workstation, um, it's loaded with the software that you need to have that, that reference

34:55.040 --> 35:01.760
architecture will also refer to what we think the best, uh, hardware lay out is, um, and

35:01.760 --> 35:09.040
we're just trying to, in another way, make GPU data science more, um, accessible to people.

35:09.040 --> 35:14.760
Um, sometimes, uh, and it's a common project in data science circles to try to build your

35:14.760 --> 35:16.240
own deep learning rig.

35:16.240 --> 35:20.240
I think that's a great exercise, but it's not for everybody.

35:20.240 --> 35:25.560
And I've been in some very, you know, um, serious corporate environments where IT is not

35:25.560 --> 35:29.400
going to let you bring in the computer that you built to start working on their proprietary

35:29.400 --> 35:30.400
data.

35:30.400 --> 35:31.400
Right.

35:31.400 --> 35:32.400
Right.

35:32.400 --> 35:39.920
And the, the data science workstation initiative is, um, really about making it as easy

35:39.920 --> 35:45.040
as possible for an organization that wants to dive into GPU data science to get started.

35:45.040 --> 35:46.040
Cool.

35:46.040 --> 35:53.680
Any parting thoughts from you on rapids or a QML or advice for folks who haven't really,

35:53.680 --> 35:58.320
you know, been exposed to, uh, what Nvidia is doing on the software side that want to

35:58.320 --> 36:00.480
explore more?

36:00.480 --> 36:03.840
I would just really encourage, uh, everybody to take a look.

36:03.840 --> 36:12.360
If you go to, um, rapids.ai, that's, uh, kind of a portal, uh, landing page that will

36:12.360 --> 36:16.840
do somebody to everything that, um, I haven't covered here as well, some of the things that

36:16.840 --> 36:21.000
I have, um, links to documentation, links to GitHub.

36:21.000 --> 36:28.640
We've begun, uh, Google group, uh, we encourage everybody that, uh, touches rapids and find

36:28.640 --> 36:34.280
something that they don't like or that doesn't work to file a ticket.

36:34.280 --> 36:39.120
You can see, um, our roadmaps and our current work on GitHub.

36:39.120 --> 36:44.040
And we really want, um, the community to be involved like, uh, you know, as I think about

36:44.040 --> 36:49.840
the machine learning algorithms that I'm going to road map next for the team to develop

36:49.840 --> 36:55.120
on, a lot of that has been informed by customer and community feedback.

36:55.120 --> 36:59.360
And it's going to continue to be informed by customer and community feedback.

36:59.360 --> 37:06.680
And so, um, I would just, you know, uh, ask anybody that, uh, is interested in taking

37:06.680 --> 37:11.760
a look, um, you know, uh, please try to get involved with this because that's, that's

37:11.760 --> 37:14.200
really what's going to measure the success of our project.

37:14.200 --> 37:19.560
Uh, there's a real open source project and, um, we've done a great job of building community

37:19.560 --> 37:20.560
so far.

37:20.560 --> 37:26.640
We've got lots of stars and forks, but we want to see more of those and, uh, we're always

37:26.640 --> 37:29.080
happy to see issues opened on GitHub.

37:29.080 --> 37:30.080
Awesome.

37:30.080 --> 37:34.720
Well, Paul, thanks for taking the time to share with us what you're up to.

37:34.720 --> 37:37.720
Well, uh, thanks for having me.

37:37.720 --> 37:43.520
All right, everyone, that's our show for today.

37:43.520 --> 37:50.040
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com

37:50.040 --> 37:52.040
slash GTC19.

37:52.040 --> 37:55.760
Thanks again to Dell for sponsoring this series.

37:55.760 --> 38:00.520
Be sure to check them out at delemc.com slash precision.

38:00.520 --> 38:04.040
As always, thanks so much for listening and catch you next time.

