1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:37,840
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.

5
00:00:37,840 --> 00:00:42,840
And last week we kicked off the second volume of our listener favorite AI platform series,

6
00:00:42,840 --> 00:00:47,200
sharing more stories of teams working to scale and industrialize data science and machine

7
00:00:47,200 --> 00:00:49,800
learning at their companies.

8
00:00:49,800 --> 00:00:54,160
We've been teasing that there's more to come and today I am super excited to announce

9
00:00:54,160 --> 00:00:59,360
the launch of our inaugural conference, Twimblecon AI platforms.

10
00:00:59,360 --> 00:01:04,600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

11
00:01:04,600 --> 00:01:09,400
necessary to scale the delivery of machine learning and AI in the enterprise.

12
00:01:09,400 --> 00:01:14,840
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

13
00:01:14,840 --> 00:01:18,640
we're creating our Twimblecon events to build on that tradition.

14
00:01:18,640 --> 00:01:24,240
The event will feature two full days of community oriented discussions, live podcast interviews

15
00:01:24,240 --> 00:01:30,440
and practical presentations by great presenters sharing concrete examples from their own experiences.

16
00:01:30,440 --> 00:01:34,640
By creating a space where data science, machine learning, platform engineering and ML ops

17
00:01:34,640 --> 00:01:39,640
practitioners and leaders can share, learn and connect, the event aspires to help see

18
00:01:39,640 --> 00:01:44,800
the development of an informed and sustainable community of technologists that is well equipped

19
00:01:44,800 --> 00:01:48,560
to meet the current and future needs of their organizations.

20
00:01:48,560 --> 00:01:52,880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

21
00:01:52,880 --> 00:01:58,120
learning and deep learning models into production, how to apply ML ops and DevOps to your machine

22
00:01:58,120 --> 00:02:03,040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

23
00:02:03,040 --> 00:02:07,520
support for data management, experiment management and model deployment.

24
00:02:07,520 --> 00:02:11,520
The latest approaches, platforms and tools for accelerating and scaling the delivery

25
00:02:11,520 --> 00:02:16,760
of ML and DL and the enterprise, platform deployment stories from leading companies like

26
00:02:16,760 --> 00:02:23,480
Google, Facebook, Airbnb, as well as traditional enterprises like Comcast and Shell, an organizational

27
00:02:23,480 --> 00:02:27,000
and cultural best practices for success.

28
00:02:27,000 --> 00:02:31,600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

29
00:02:31,600 --> 00:02:33,600
love to meet you there.

30
00:02:33,600 --> 00:02:39,520
EarlyBurt Registration is open today at twimblecon.com and we're offering the first 10 listeners

31
00:02:39,520 --> 00:02:45,720
who register the amazing opportunity to get their ticket for 75% off using the discount

32
00:02:45,720 --> 00:02:48,160
code TwimbleFirst.

33
00:02:48,160 --> 00:02:54,920
Again, the conference site is twimblecon.com and the code is TwimbleFirst.

34
00:02:54,920 --> 00:03:00,080
I am really grateful to our friends over at Sigopt who stepped up to support this project

35
00:03:00,080 --> 00:03:01,880
in a big way.

36
00:03:01,880 --> 00:03:07,000
In addition to supporting our AI Platforms podcast series and next ebook, they've made

37
00:03:07,000 --> 00:03:12,960
a huge commitment to this community by signing on as the first founding sponsor for the event.

38
00:03:12,960 --> 00:03:17,760
App Software is used by enterprise teams to standardize and scale machine learning experimentation

39
00:03:17,760 --> 00:03:23,200
and optimization across any combination of modeling frameworks, libraries, computing

40
00:03:23,200 --> 00:03:25,760
infrastructure and environment.

41
00:03:25,760 --> 00:03:31,480
Teams like Two Sigma, who will hear from later in this podcast series, rely on Sigopt Software

42
00:03:31,480 --> 00:03:36,520
to realize better modeling results much faster than previously possible.

43
00:03:36,520 --> 00:03:41,440
Of course, to fully grasp its potential, it's best to try it yourself and this is why

44
00:03:41,440 --> 00:03:46,760
Sigopt is offering you an exclusive opportunity to try their product on some of your toughest

45
00:03:46,760 --> 00:03:49,600
modeling problems for free.

46
00:03:49,600 --> 00:03:55,760
To learn about and take advantage of this offer, visit twimblei.com slash Sigopt.

47
00:03:55,760 --> 00:04:00,400
And now on to the show.

48
00:04:00,400 --> 00:04:03,960
All right, everyone.

49
00:04:03,960 --> 00:04:06,040
I'm on the line with Kelly Revoir.

50
00:04:06,040 --> 00:04:11,000
Kelly is an engineering manager at Stripe working on machine learning infrastructure.

51
00:04:11,000 --> 00:04:13,840
Kelly, welcome to this week in machine learning and AI.

52
00:04:13,840 --> 00:04:14,840
Thanks for having me.

53
00:04:14,840 --> 00:04:16,680
I'm really excited to chat.

54
00:04:16,680 --> 00:04:22,280
We got in touch with you kind of occasioned by a talk you're giving at strata, which

55
00:04:22,280 --> 00:04:25,400
is actually happening as we speak.

56
00:04:25,400 --> 00:04:32,160
I'm not physically in SF for it this time, but your talk, which is going to be later today,

57
00:04:32,160 --> 00:04:37,440
is on scaling model training from flexible training APIs to resource management with

58
00:04:37,440 --> 00:04:43,280
Kubernetes, and of course, machine learning infrastructure and AI platforms is a very popular

59
00:04:43,280 --> 00:04:45,560
topic here on the podcast.

60
00:04:45,560 --> 00:04:52,760
And so I'm looking forward to digging into the way Stripe is platforming its machine learning,

61
00:04:52,760 --> 00:04:54,080
processes and operations.

62
00:04:54,080 --> 00:04:58,040
But before we do that, I'd love to hear a little bit about your background and how you

63
00:04:58,040 --> 00:05:00,360
got started working in this space.

64
00:05:00,360 --> 00:05:02,160
Yeah, sounds great.

65
00:05:02,160 --> 00:05:05,960
Maybe I'll say a little bit about what I do now and then kind of work backwards from

66
00:05:05,960 --> 00:05:06,960
that.

67
00:05:06,960 --> 00:05:12,600
So right now, I'm an engineering manager at Stripe, and I work with our data infrastructure

68
00:05:12,600 --> 00:05:19,040
group, which is seven teams kind of at the lowest level things like our production databases

69
00:05:19,040 --> 00:05:23,960
or things like Elasticsearch clusters and then kind of working up through like batch

70
00:05:23,960 --> 00:05:30,840
and streaming platforms, core, like ETL, data pipelines and libraries, and also machine learning

71
00:05:30,840 --> 00:05:31,840
infrastructure.

72
00:05:31,840 --> 00:05:38,120
I've been at Stripe for very close to six years now from when the company was about 50

73
00:05:38,120 --> 00:05:42,840
people and have basically worked on a bunch of different things in sort of like risk,

74
00:05:42,840 --> 00:05:51,560
data and machine learning, both as an engineer and engineering manager and also initially

75
00:05:51,560 --> 00:05:56,360
more on kind of like the application side and then over time moving over to the infrastructure

76
00:05:56,360 --> 00:05:59,360
side.

77
00:05:59,360 --> 00:06:05,320
By training, I'm like a kind of research scientist person, so I studied physics and electrical

78
00:06:05,320 --> 00:06:11,520
engineering in school, did my PhD at Stanford working on nanopotonics and then did a short

79
00:06:11,520 --> 00:06:14,360
postdoc at HP labs on nanopotonics.

80
00:06:14,360 --> 00:06:15,360
Is that a nanopotonics?

81
00:06:15,360 --> 00:06:16,360
Yeah.

82
00:06:16,360 --> 00:06:21,600
I think you had like I get an odds count on recently, which is not too far away, so maybe

83
00:06:21,600 --> 00:06:23,600
that gives you a little bit of an idea.

84
00:06:23,600 --> 00:06:27,240
And then yeah, it was at HP labs for a year, so working on sort of similar things and

85
00:06:27,240 --> 00:06:29,520
also some 3D imaging.

86
00:06:29,520 --> 00:06:34,240
And I guess I like to call what I did, although I don't know that anyone else calls it that sort

87
00:06:34,240 --> 00:06:40,440
of like full stack science where like you have an idea and then you do some theory or modeling

88
00:06:40,440 --> 00:06:44,120
or simulation and then you use that to design a device and then you actually go in the

89
00:06:44,120 --> 00:06:47,720
clean room and like make the device and then you actually go in the optics lab and like

90
00:06:47,720 --> 00:06:51,480
you know shoot a bunch of lasers at your device and measure it and then you sort of like

91
00:06:51,480 --> 00:06:54,800
process the data and compare it to your theory and simulation.

92
00:06:54,800 --> 00:07:00,560
And I was like I found like kind of the two ends the most, like sort of the magical moment

93
00:07:00,560 --> 00:07:05,600
where like you know the data that you collected like matches what you thought was going to

94
00:07:05,600 --> 00:07:08,120
happen from your modeling.

95
00:07:08,120 --> 00:07:11,960
And I kind of decided that I wanted to do more of that and a little less of like fabrication

96
00:07:11,960 --> 00:07:12,960
or material science.

97
00:07:12,960 --> 00:07:16,920
And I was kind of sitting in Silicon Valley and started looking around and like stripe

98
00:07:16,920 --> 00:07:22,280
was super exciting in terms of its mission, like having interesting data and just like having

99
00:07:22,280 --> 00:07:23,280
amazing people.

100
00:07:23,280 --> 00:07:29,120
Awesome, awesome, stripe sounds really interesting but shooting lasers at stuff also sounds really,

101
00:07:29,120 --> 00:07:30,120
really cool.

102
00:07:30,120 --> 00:07:33,880
Yeah, people get really excited when you tell them that.

103
00:07:33,880 --> 00:07:36,040
So that was fun for a while.

104
00:07:36,040 --> 00:07:42,440
Maybe tell us a little bit about stripes, kind of machine learning journey from an infrastructure

105
00:07:42,440 --> 00:07:44,440
perspective.

106
00:07:44,440 --> 00:07:51,120
You know how did it, it sounds like you're doing a bunch of interesting things both from

107
00:07:51,120 --> 00:07:57,440
a training perspective, from a data management perspective, inference, but how did it evolve?

108
00:07:57,440 --> 00:08:02,440
Yeah, I think one thing that's interesting about machine learning at stripe, like I think

109
00:08:02,440 --> 00:08:07,880
a lot of places you talk to machine learning kind of like started out as being for some,

110
00:08:07,880 --> 00:08:13,080
some kind of like offline analytics more like you know internal business questions like

111
00:08:13,080 --> 00:08:16,760
maybe like you're trying to calculate long term value of your users.

112
00:08:16,760 --> 00:08:20,880
And we do stuff like that now, but we actually started like our kind of core uses have always

113
00:08:20,880 --> 00:08:27,000
been very much on kind of the production side, like our kind of most business critical

114
00:08:27,000 --> 00:08:32,440
and first machine learning, machine learning use cases were things like scoring transactions

115
00:08:32,440 --> 00:08:36,840
in the charge flow to evaluate whether they're fraudulent or not.

116
00:08:36,840 --> 00:08:44,240
We're doing kind of like internal risk management of like making sure our users are selling

117
00:08:44,240 --> 00:08:48,520
things that we can support from our terms of service or that they're kind of like good

118
00:08:48,520 --> 00:08:51,200
users that we want to support.

119
00:08:51,200 --> 00:08:55,920
And so we started out from having kind of a lot of these more like production requirements

120
00:08:55,920 --> 00:08:59,120
of it needs to be this fast and it needs to be this reliable and I think our machine

121
00:08:59,120 --> 00:09:04,440
learning platform kind of like evolved from that side, where you know initially we had

122
00:09:04,440 --> 00:09:08,240
kind of like one machine learning team and then even just having a couple of applications

123
00:09:08,240 --> 00:09:13,920
we started seeing like here are some commonalities like everyone needs to be able to score models

124
00:09:13,920 --> 00:09:18,880
or you know even like having some notion of shared features could be really valuable

125
00:09:18,880 --> 00:09:21,240
across just a couple of applications.

126
00:09:21,240 --> 00:09:25,880
And then as we split our machine learning team one piece of that became machine learning

127
00:09:25,880 --> 00:09:30,240
infrastructure, which we've developed since then and you know it's really important

128
00:09:30,240 --> 00:09:33,760
for that team to work both with the teams doing the business applications, which now

129
00:09:33,760 --> 00:09:38,800
include a bunch of other things in our user facing products like radar and billing as

130
00:09:38,800 --> 00:09:43,640
well as internally and also you know it's important for the machine learning infrastructure

131
00:09:43,640 --> 00:09:48,240
to build on the rest of your data infrastructure and really the rest of all of your infrastructure

132
00:09:48,240 --> 00:09:52,760
and we've worked really closely with like our orchestration team on you know as you said

133
00:09:52,760 --> 00:09:56,280
and chatting about my talk like getting training to run on Kubernetes.

134
00:09:56,280 --> 00:10:01,080
Yeah man that's maybe an interesting place to start.

135
00:10:01,080 --> 00:10:07,160
You kind of alluded to the interfaces between machine learning infrastructure as a team

136
00:10:07,160 --> 00:10:15,880
and you know data infrastructure and you know just infrastructure how do they how do they

137
00:10:15,880 --> 00:10:23,400
connect you know maybe even organizationally and how do they tend to work with them up

138
00:10:23,400 --> 00:10:24,400
with one another.

139
00:10:24,400 --> 00:10:30,520
For example you know in you know training on Kubernetes you know where is the line

140
00:10:30,520 --> 00:10:37,240
between what the ML infrastructure team is doing and you know what it's requiring of some

141
00:10:37,240 --> 00:10:40,680
you know broader technology infrastructure group.

142
00:10:40,680 --> 00:10:44,440
Yeah I think the Kubernetes case is really interesting and it's one that's been super

143
00:10:44,440 --> 00:10:50,880
successful for us so I guess maybe like a year or two ago we we didn't really focused

144
00:10:50,880 --> 00:10:54,920
on the kind of scoring like real-time inference part of models because that's the hardest

145
00:10:54,920 --> 00:10:58,280
and we'd sort of left people on their own it's like well you figure out how to train a

146
00:10:58,280 --> 00:11:02,840
model and then you know if you manage to do that we'll help you score it and we realized

147
00:11:02,840 --> 00:11:08,040
that that wasn't like great right so we started thinking you know what can we do and at first

148
00:11:08,040 --> 00:11:12,280
we built some CLI tools to kind of like wrap the Python people were doing but then we wanted

149
00:11:12,280 --> 00:11:16,360
to kind of do more so eventually we built an API and then a big hassle had been the resource

150
00:11:16,360 --> 00:11:21,000
management and we just kind of wanted to like abstract that all the way and as it happened

151
00:11:21,000 --> 00:11:25,800
at that time our orchestration team had gotten like really interested in Kubernetes and I think

152
00:11:25,800 --> 00:11:30,600
they wrote a blog post like maybe a year and a half ago they had kind of just moved our first

153
00:11:30,600 --> 00:11:35,400
application to Kubernetes which was some of our con jobs that we use in our financial infrastructure

154
00:11:35,400 --> 00:11:40,600
and so we ended up collaborating this was kind of like a great next step of a second application

155
00:11:40,600 --> 00:11:45,320
they could work on and you know we had some details we had to work out we had to figure out like

156
00:11:45,320 --> 00:11:50,520
how do we package up all of our Python code and to you know some Docker file we can deploy and

157
00:11:50,520 --> 00:11:55,560
it was really useful to be able to work with them on that but I think we have found really

158
00:11:55,560 --> 00:12:00,040
good interfaces and working with them where you know we wrote a client for the communities API but

159
00:12:00,040 --> 00:12:05,160
it's like anytime we need help or anytime there's management of the Kubernetes cluster they take

160
00:12:05,160 --> 00:12:09,880
care of all of that so it's kind of given us this flexibility where we can define different

161
00:12:09,880 --> 00:12:14,280
instance and resource types and swap them out really easily if we need CPUs or GPUs or we need

162
00:12:14,280 --> 00:12:19,160
to like expand the cluster but we as machine learning infrastructure kind of like don't have to

163
00:12:19,160 --> 00:12:23,480
deal with managing Kubernetes or updating it we have this amazing team of people who are like

164
00:12:23,480 --> 00:12:29,960
totally focused on that restraint so your talk at strata was focused on this area

165
00:12:31,320 --> 00:12:36,520
what was kind of the flow of your talk what were the main points that you are that you're

166
00:12:36,520 --> 00:12:43,080
planning to go through with the audience there yeah great question so we we kind of think about

167
00:12:43,080 --> 00:12:48,920
this in two pieces and you know maybe that's because that's how we actually did it so one piece

168
00:12:48,920 --> 00:12:52,840
was the resource management that I talked about was you know getting getting things to

169
00:12:52,840 --> 00:12:57,640
around on Kubernetes that was actually kind of like the second piece for us the first piece was

170
00:12:58,760 --> 00:13:02,840
figuring out sort of like how should the user interact with things and like where should we give

171
00:13:02,840 --> 00:13:08,600
them flexibility and where should we constrain things and so we ended up building what we call

172
00:13:08,600 --> 00:13:14,040
internally rail yard which is like a model training API and it goes with there's sort of two

173
00:13:14,040 --> 00:13:18,840
pieces there's like what you put in the API request and then there's what we call a workflow

174
00:13:18,840 --> 00:13:23,720
and the API request is a little bit more constrained like you have to say you're meta data for whose

175
00:13:23,720 --> 00:13:29,080
training so we can track it you have to tell us like where your data is like how you're doing things

176
00:13:29,080 --> 00:13:34,360
like hold out just kind of basic things that you'll always need but then we have this workflow piece

177
00:13:34,360 --> 00:13:39,640
that people can write like kind of like whatever Python they want as long as they define a

178
00:13:39,640 --> 00:13:45,720
train method in it that will hand us back like the fitted model and we definitely have found that

179
00:13:45,720 --> 00:13:51,080
like initially we were very focused on binary classifiers for things like fraud but people have done

180
00:13:51,080 --> 00:13:56,120
things like word embeddings we have people doing time series forecasting we're using like

181
00:13:57,320 --> 00:14:02,280
things like psychic learn actually views fast text pytorch profit so this has worked pretty

182
00:14:02,280 --> 00:14:06,680
well in terms of like providing enough flexibility that people can do things that we actually didn't

183
00:14:06,680 --> 00:14:12,520
anticipate originally but it's constrained enough that we can run it and sort of track what's going

184
00:14:12,520 --> 00:14:18,120
on and and give them what they need and be able to automate the things we need to automate do you

185
00:14:18,120 --> 00:14:24,680
think of your users as more kind of the data science type of user or machine learning engineer type

186
00:14:24,680 --> 00:14:31,240
of user or is there a mix of those two types of backgrounds yeah it's a mix which has been really

187
00:14:31,240 --> 00:14:36,040
interesting and I think back to what I said earlier like because we initially focused on

188
00:14:36,600 --> 00:14:41,640
these kind of critical production use cases we started out where the teams users were really

189
00:14:41,640 --> 00:14:45,720
pretty much all machine learning engineers and very highly skilled machine learning engineers

190
00:14:45,720 --> 00:14:50,280
like people who are excellent programmers and you know they know stats in ML and they're

191
00:14:50,280 --> 00:14:56,120
kind of like the unicorns to hire and over time we've been able to broaden that and I think

192
00:14:56,120 --> 00:15:01,880
having things like you know this tooling has made that possible like in our user survey right

193
00:15:01,880 --> 00:15:07,080
after we first shipped even just the kind of like API workflow piece and we were actually just

194
00:15:07,080 --> 00:15:11,640
like running it on some boxes aside car process we hadn't even done Kubernetes yet but a lot of the

195
00:15:11,640 --> 00:15:15,800
feedback we got was like oh this new person started on my team and I just like pointed them to the

196
00:15:15,800 --> 00:15:19,800
directory where the workflows are and I like didn't have to think about how to split all these

197
00:15:19,800 --> 00:15:23,880
things out because like you know you just kind of pointed me in the right direction and I could

198
00:15:23,880 --> 00:15:28,520
point them in the right direction so I think that having having these kind of like common ways of

199
00:15:28,520 --> 00:15:34,120
doing things has been a way to broaden our user set and as our data science team which is more

200
00:15:34,120 --> 00:15:38,840
internally focused has grown they've been able to kind of like start picking up increasingly

201
00:15:39,480 --> 00:15:44,840
large pieces of what we built for the ML engineers as well and we've been like excited to see

202
00:15:44,840 --> 00:15:53,320
that and work with them and so the interface then is kind of Python code and our is the platform

203
00:15:54,200 --> 00:16:00,680
containerizing that code or is the user expected to do it or is it integrated into some kind of

204
00:16:00,680 --> 00:16:06,280
workflow like they check it in and then it becomes available you know to the platform via

205
00:16:07,880 --> 00:16:16,120
check-in or CICD type of process yeah so we still have the experimental flow where people can

206
00:16:16,120 --> 00:16:20,440
like kind of try things out but when you're ready to productionize your workflow basically what you

207
00:16:20,440 --> 00:16:27,080
do is you get your code reviewed you merge it we use we ended up using google subpar library because

208
00:16:27,080 --> 00:16:32,600
it works really well with basil which we use for a lot of our build tooling what are the those two

209
00:16:33,480 --> 00:16:41,080
yeah so subpar is a google library that helps us like package Python code into like a self-contained

210
00:16:41,080 --> 00:16:45,640
executable both the source code and any dependencies like if you're running PyTorch and you need some

211
00:16:45,640 --> 00:16:51,320
CUDA stuff okay and it works kind of out of the box with basil which is the open source version

212
00:16:51,320 --> 00:16:57,560
of Google's build system which we have started to use that stripe a few years ago and have expanded

213
00:16:57,560 --> 00:17:03,160
since it's really nice for like speed reproducibility and working with multiple languages

214
00:17:04,360 --> 00:17:08,840
so this is where our ML info team kind of worked with our orchestration team to figure out the

215
00:17:08,840 --> 00:17:13,560
details here to be able to kind of like package up all this Python code and have it so that

216
00:17:13,560 --> 00:17:18,360
basically almost like a service deploy you can kind of like have it turn into a Docker image that

217
00:17:18,360 --> 00:17:25,080
you can deploy to like Amazon's ECR and then Kubernetes will kind of like know how to pull that down

218
00:17:25,080 --> 00:17:30,280
and be able to run it so the ML engineer or the data scientist doesn't really have to think about

219
00:17:30,280 --> 00:17:34,440
any of that it just kind of works as part of the you know you get your PR emerged and you

220
00:17:34,440 --> 00:17:39,960
deploy something if you need to change the workflow but earlier on in the process when you're

221
00:17:39,960 --> 00:17:49,400
experimenting the currency is a you know some Python code what kind of tooling have you built up

222
00:17:49,400 --> 00:17:56,920
around experiment management and automatically tracking various experiment parameters or hyper

223
00:17:56,920 --> 00:18:02,920
parameters hyper parameter optimization that kind of thing are you doing all that or is that

224
00:18:02,920 --> 00:18:10,760
all on the user to do yeah that's a really good question so one of the things that we added

225
00:18:10,760 --> 00:18:15,320
in our API for training as we found it was really useful to have this like custom prams field

226
00:18:16,760 --> 00:18:20,840
especially because we eventually people ended up and you know we have some shared services

227
00:18:20,840 --> 00:18:25,800
to support this like sort of a retraining service that can automate your training requests

228
00:18:26,360 --> 00:18:30,760
okay and so one of the things that people from the beginning use the custom

229
00:18:30,760 --> 00:18:35,240
programs for was hyper parameter optimization optimization we are kind of working toward building

230
00:18:35,240 --> 00:18:41,160
that out as a first-class thing like we now have like evaluation workflows that can be integrated

231
00:18:41,160 --> 00:18:44,840
with all of this as well and that's kind of like the first step you need for hyper parameter

232
00:18:44,840 --> 00:18:48,920
optimization if you want to do it as a service like what are you optimizing if you don't know what

233
00:18:48,920 --> 00:18:53,480
metrics are looking at right and so that's something we hope to do like over the next you know

234
00:18:53,480 --> 00:18:57,720
three to six months is to make that like a little bit more of first-class support and you mentioned

235
00:18:57,720 --> 00:19:06,840
this this directory of workflows elaborate on that a little bit yeah so one of the nice things is

236
00:19:06,840 --> 00:19:11,480
you know when you're writing your workflow if you put it in the right place then our our

237
00:19:11,480 --> 00:19:16,120
scholars service really are we'll know where to find it but one of the side benefits has also

238
00:19:16,120 --> 00:19:21,400
just been that there is one place where people's workflows are and so that that's been kind of

239
00:19:21,400 --> 00:19:25,480
like a nice place for people to get started and see like you know what models are other people

240
00:19:25,480 --> 00:19:31,960
using or like what preprocessing or kind of what other things are they doing or what what types

241
00:19:31,960 --> 00:19:37,960
of parameters like estimator parameters are they looking at changing to just kind of you know have

242
00:19:37,960 --> 00:19:44,360
that be like a little bit more available to our users are internal users and the workflow

243
00:19:45,000 --> 00:19:54,280
element of this is it is it graph-based is it something like airflow how's that implemented

244
00:19:54,280 --> 00:20:00,440
yeah so in this case by workflow all I mean is just like Python code that you know you give it

245
00:20:00,440 --> 00:20:07,000
like we're actually really hard our API passes to it like what are your features or what are your

246
00:20:07,000 --> 00:20:14,920
labels and then you are Python code returns like here is the fitted pipeline or model and like usually

247
00:20:14,920 --> 00:20:22,920
something like the evaluation data set that we can pass back we have had so we've people have

248
00:20:22,920 --> 00:20:28,360
kind of built us and users like interesting things on top of having a training API

249
00:20:28,360 --> 00:20:33,080
okay so some of our users built out actually the folks working on radar for our product built out

250
00:20:33,080 --> 00:20:38,520
like an auto retraining service that we've since kind of taken over and generalized where they

251
00:20:38,520 --> 00:20:44,200
schedule like nightly retraining of all the tens and hundreds of models and you know that's

252
00:20:44,200 --> 00:20:49,400
integrated to be able to even like if the evaluation looks better like potentially automatically

253
00:20:49,400 --> 00:20:55,960
deploy them we do also have people who have put like training models via our service into like

254
00:20:55,960 --> 00:21:01,160
air flow decks if they have you know some some slightly more complicated set of things that they

255
00:21:01,160 --> 00:21:06,760
want to run so we're definitely seeing that as well and you've mentioned radar a couple of times

256
00:21:06,760 --> 00:21:14,040
is that a product that's right we're an internal project yeah radar is our like user facing fraud

257
00:21:14,040 --> 00:21:20,920
product it runs on all of our machine learning infrastructure and you know every charge that

258
00:21:20,920 --> 00:21:25,800
goes through stripe within usually a hundred milliseconds or so we've kind of like done a bunch

259
00:21:25,800 --> 00:21:31,240
of real-time feature generation and evaluated like kind of all of the models that are appropriate

260
00:21:31,800 --> 00:21:37,240
and in addition to sort of the machine learning piece there's also a product piece for it

261
00:21:37,240 --> 00:21:42,600
where users can get more visibility into what our ml has done they can kind of like write their own

262
00:21:42,600 --> 00:21:48,440
rules and like set block thresholds on them and there's there's sort of like a manual review

263
00:21:48,440 --> 00:21:53,080
functionality so they're kind of some more product pieces that are complimentary to the underlying

264
00:21:53,080 --> 00:21:59,240
machine learning just trying to complete the picture here you've got these workflows which are

265
00:21:59,240 --> 00:22:06,520
essentially Python they expose a train entry point and you you mentioned this directory of

266
00:22:06,520 --> 00:22:11,640
workflows is that like a directory like on a server somewhere with just like dot py files or is

267
00:22:11,640 --> 00:22:18,600
that are they do you require that they be versioned and are you kind of managing those versions

268
00:22:19,240 --> 00:22:24,360
yeah so that that's just like actually like in a code basically so that's like yeah the workflows

269
00:22:24,360 --> 00:22:29,800
live together in code as part of kind of our training API it's like when you submit here's my

270
00:22:29,800 --> 00:22:35,000
training request which has you know here's my data here's my metadata this is the workflow

271
00:22:35,000 --> 00:22:41,000
I want you to run we give you back a job ID which then you can check the status of you can check

272
00:22:41,000 --> 00:22:47,240
the result the result will have things in it like what was the getcha and so that's like something

273
00:22:47,240 --> 00:22:55,240
that we can track as well got it so you're submitting the job with the code itself as opposed to

274
00:22:55,240 --> 00:23:03,000
a getcha so I guess it depends a little bit which workflow you're running through like in the case

275
00:23:03,000 --> 00:23:07,800
where you're running on Kubernetes you've merged your code to master and then we kind of

276
00:23:07,800 --> 00:23:12,520
package up all this code and deploy the Docker image and then from there you can kind of make

277
00:23:12,520 --> 00:23:18,840
requests to our service which will run the job on Kubernetes so at that point your code it's

278
00:23:18,840 --> 00:23:22,520
you know whatever's on master for the workflow plus whatever you've put in the request

279
00:23:23,240 --> 00:23:27,720
let's talk about where the the data comes from for training and what kind of

280
00:23:29,320 --> 00:23:34,280
you know platform support your offering folks yeah that's a really interesting question

281
00:23:34,280 --> 00:23:42,680
kind of within the framework of like what do you need for a like really RDPI request we support

282
00:23:42,680 --> 00:23:50,360
two different types of data sources one is more for experimentation which is like you can kind of

283
00:23:50,360 --> 00:23:55,800
tell us how to make this equal to query the data warehouse and that's kind of nice for experimentation

284
00:23:55,800 --> 00:24:01,400
but not so nice for production what pretty much everyone uses for production is the other data

285
00:24:01,400 --> 00:24:08,520
source we support which is parquet from s3 so it's like you tell us you know where to find that

286
00:24:08,520 --> 00:24:15,240
and what your future names are and usually that's generated by our features framework that we call

287
00:24:15,240 --> 00:24:23,080
semblance which is basically like a DSL that helps you know gives you a lot of ways to write complex

288
00:24:23,080 --> 00:24:28,680
features like think have things like counters be able to do things like joins do a lot of transformations

289
00:24:28,680 --> 00:24:36,120
and then you know the ML infrastructure team figures out like how to run that code in batch if you

290
00:24:36,120 --> 00:24:42,760
are doing training or like there's a way to run it in real time basically and kind of like a

291
00:24:42,760 --> 00:24:47,640
Kafka consumer setup but you only have to write your code feature code like once

292
00:24:48,520 --> 00:24:53,800
is it the user that's only writing a feature code once are you going after kind of sharing

293
00:24:53,800 --> 00:25:02,200
features across the user based to what extent are you seeing shared features yeah that's like a

294
00:25:02,200 --> 00:25:08,040
really excellent question um yeah so the user writes their code once and like also I think having

295
00:25:08,040 --> 00:25:11,800
a framework similar to the training workflows where people can see what other people have done

296
00:25:11,800 --> 00:25:19,080
has been really powerful um so we do have people who are like definitely kind of sharing features

297
00:25:19,080 --> 00:25:22,840
across applications and there's there's a little bit of a tradeoff like it's like a huge amount

298
00:25:22,840 --> 00:25:27,480
of leverage if you don't have to rewrite some complicated business logic um you do have to manage

299
00:25:27,480 --> 00:25:33,000
a little bit of making sure that um you know everything is versioned and that you're paying attention

300
00:25:33,000 --> 00:25:37,800
to like not deprecate something someone else is using and that you're not like just like changing

301
00:25:37,800 --> 00:25:43,000
a definition in place that you are kind of like creating a new version every time you are changing

302
00:25:43,000 --> 00:25:46,840
something right so there's a little bit more management there and hopefully over time we can

303
00:25:46,840 --> 00:25:51,560
improve our tooling around that but I think it's you know even even since before we had a feature

304
00:25:51,560 --> 00:25:56,040
framework like being able to kind of share some of that stuff has been like hugely valuable for us

305
00:25:57,320 --> 00:26:07,400
is the features framework is that a set of APIs or is that uh kind of a runtime uh thing like

306
00:26:07,400 --> 00:26:13,080
what what exactly is it yeah there's kind of two pieces so um which is basically sort of what you

307
00:26:13,080 --> 00:26:18,520
said like you know one is more like the API uh um like what are what are the things we you know

308
00:26:18,520 --> 00:26:23,960
let users express and one thing we tried to do there is actually constrain not a little bit

309
00:26:23,960 --> 00:26:28,680
so we like you have to use events for everything and we don't really let you express notions of time

310
00:26:28,680 --> 00:26:33,720
so you kind of can't mess up that time machine of like what was the state of the features

311
00:26:33,720 --> 00:26:37,560
at some time in the past where you want to be training your model we kind of like take care of that

312
00:26:37,560 --> 00:26:44,520
for you um so that's kind of one piece and then yeah we kind of compile that into like an AST

313
00:26:44,520 --> 00:26:48,760
and then we use that to essentially write like a compiler to be able to run it on different

314
00:26:48,760 --> 00:26:54,280
backends um and then we can kind of like you know write tests and try and check um at the framework

315
00:26:54,280 --> 00:26:58,600
level that that things are going to be as close as possible to the same across those different

316
00:26:58,600 --> 00:27:04,200
backends so back end could be um something for training where you're going to materialize like

317
00:27:04,200 --> 00:27:08,600
what was the value of the features at each point in time in the past that you want as inputs to

318
00:27:08,600 --> 00:27:13,080
training your model um or another back end could be like I mentioned we have kind of this

319
00:27:13,080 --> 00:27:18,840
Kafka consumer based back end that we use like for example um for radar to be able to like evaluate

320
00:27:18,840 --> 00:27:25,480
these features like as a charge is happening uh and so to what extent you find that limitation of

321
00:27:25,480 --> 00:27:31,800
everything being event based gets in the way of what folks want to do yeah that that's a really

322
00:27:31,800 --> 00:27:37,480
good question to um it's definitely was originally a little bit of a paradigm shift for people

323
00:27:37,480 --> 00:27:43,880
they're like oh I just want to use this thing from the database right um but we found that actually

324
00:27:43,880 --> 00:27:48,840
it's worked out pretty well and that especially when you have users who are ML engineers like they

325
00:27:48,840 --> 00:27:54,200
do really understand the value of like why you want to have things be event based and like the

326
00:27:54,200 --> 00:27:59,720
sort of gotchas that that helps prevent um because I think everyone has their story about how you

327
00:27:59,720 --> 00:28:04,840
were just looking something up in the database but then you know the value changed uh and you didn't

328
00:28:04,840 --> 00:28:09,240
realize it so it's kind of like you're leaking future information into your training data and

329
00:28:09,240 --> 00:28:15,800
then your model is not going to do as well as you thought it did so like I think moving to a more

330
00:28:15,800 --> 00:28:21,160
event based world and I mean I think in general shape has also kind of been doing more streaming work

331
00:28:21,160 --> 00:28:27,400
and um more having like good support also as as uh at the infrastructure level with Kafka has been

332
00:28:27,400 --> 00:28:36,200
really helpful with that and so does that mean that the models that they're building need to be

333
00:28:36,200 --> 00:28:43,400
aware of kind of this streaming paradigm during training uh or do they get a static data set

334
00:28:43,400 --> 00:28:48,440
to train? Yeah so basically um you can kind of use our future's framework to just generate like

335
00:28:48,440 --> 00:28:54,360
par k and s3 that has materialized like all of the information you want of what was the value of

336
00:28:54,360 --> 00:28:59,800
each of the features that you want at all the points in time that you want and then yeah your input

337
00:28:59,800 --> 00:29:05,400
to the training API is like please use this par k from s3 um we could make it a little more seamless

338
00:29:05,400 --> 00:29:11,960
than that but that's worked pretty well and par k is just like a serialized like a file format

339
00:29:11,960 --> 00:29:17,480
yeah it's pretty efficient you know I think it's used in a lot of kind of big data uses um you can

340
00:29:17,480 --> 00:29:21,160
also do things like predicate pushdown and we have like a way in the training API to kind of

341
00:29:21,160 --> 00:29:27,640
specify some filters there um to just kind of like save save some effort uh use a predicate pushdown

342
00:29:27,640 --> 00:29:32,120
yeah so if you know you only need certain columns or something like you know you can you can load it

343
00:29:32,120 --> 00:29:36,920
a little bit more efficiently and not have to carry around a lot of extra data got it okay the other

344
00:29:36,920 --> 00:29:43,800
interesting thing that you talked about in the context of the this event base framework is the

345
00:29:43,800 --> 00:29:51,560
whole um you know time machine is the way you said it kind of alluding to the the point in time

346
00:29:51,560 --> 00:29:59,960
correctness of uh you know feature snapshot can you elaborate a little bit on um did you

347
00:29:59,960 --> 00:30:07,240
did you start there or did you evolve to that that seems to be in my conversations kind of uh

348
00:30:08,200 --> 00:30:12,360
I don't know maybe you'd like one of the the cutting edges or bleeding edges that people are

349
00:30:12,360 --> 00:30:17,880
trying to deal with as they scale up these um these data management systems for features

350
00:30:19,000 --> 00:30:24,920
yeah for this particular project um in this version we started there straight previously

351
00:30:24,920 --> 00:30:29,720
had kind of looked at something a little bit related a couple years before um and in a lot of ways

352
00:30:29,720 --> 00:30:34,200
we kind of learned from that so we ended up with something that was more more powerful and sort

353
00:30:34,200 --> 00:30:40,200
of solved some of these issues at the platform level um we did you know at that point we had been

354
00:30:40,200 --> 00:30:44,120
running machine learning applications in production for a few years so I think everyone has

355
00:30:44,120 --> 00:30:50,040
their horror stories right of like all the things that can go wrong um especially kind of

356
00:30:50,040 --> 00:30:53,640
out of correctness level and like everyone has their story about like re-implementing features

357
00:30:53,640 --> 00:30:57,880
in different languages which we we did for a while too and kind of like all the things that can

358
00:30:57,880 --> 00:31:04,040
go wrong there so um yeah I think we we really tried to learn from both like what are all the

359
00:31:04,040 --> 00:31:09,560
things we'd seen go well or go wrong in individual applications and then also from kind of like

360
00:31:09,560 --> 00:31:14,600
our previous attempts um at some of this type of thing like what what was good and you know what

361
00:31:14,600 --> 00:31:20,840
could still be better and out of kerosene what do you use for data warehouse and are there multiple

362
00:31:20,840 --> 00:31:27,560
or is it is there just one um we've used a combination of redshift and presto um over the past

363
00:31:27,560 --> 00:31:33,720
couple of years um yeah they have a little bit of sort of like different abilities and strengths

364
00:31:34,280 --> 00:31:39,320
and those are those are things that people like to use to experiment with machine learning although

365
00:31:39,320 --> 00:31:43,400
like you know we generally don't use them in our production flows because we kind of prefer the

366
00:31:43,400 --> 00:31:51,320
event based model and so is the event based model parallel or orthogonal to to redshift or

367
00:31:51,320 --> 00:31:57,560
presto is there or is it a front end to either of these two systems yeah I guess we have we

368
00:31:57,560 --> 00:32:02,920
actually have a front end that we've built for redshift and presto um you know separately from

369
00:32:02,920 --> 00:32:08,200
from machine learning that's really nice and lets people like um you know to the extent they have

370
00:32:08,200 --> 00:32:15,400
permissions to do so like explore tables or put annotations on tables um we haven't integrated

371
00:32:15,400 --> 00:32:20,760
our in general I would say we could do some work on our UIs for for ML stuff we've definitely

372
00:32:20,760 --> 00:32:24,920
focused more on the back end and in front API side although we do have some things like our

373
00:32:24,920 --> 00:32:30,200
auto retraining service has a UI where you can see like um what's the status of my job like was

374
00:32:30,200 --> 00:32:35,480
it you know did it finish um did it produce a model that was better than the previous model

375
00:32:35,480 --> 00:32:42,920
I think I'm just trying to wrap my head around the event based model here you know as an example

376
00:32:42,920 --> 00:32:50,120
of a question that's coming to mind uh in an event based world are you regenerating the features

377
00:32:50,840 --> 00:32:55,880
you know every time and if you've got you know some complex feature that involves a lot of

378
00:32:55,880 --> 00:33:00,680
transformation or you have the backfill of ton of data like what does that even mean in an event

379
00:33:00,680 --> 00:33:06,680
based world where I think of like you have events and they go away uh is there some kind of store

380
00:33:06,680 --> 00:33:13,000
for all that that isn't redshift or presto um well whenever we say event you know we're publishing

381
00:33:13,000 --> 00:33:18,680
something to Kafka and then we're archiving it to s3 okay then that persists like you know as long

382
00:33:18,680 --> 00:33:26,680
as we want it to um in some cases basically forever um and so that is available we do do end up

383
00:33:26,680 --> 00:33:32,920
doing a decent amount of backfilling of kind of like you know you define the transform features you

384
00:33:32,920 --> 00:33:37,640
want but then you need you know you need to run that back over all the data you'll need for your

385
00:33:37,640 --> 00:33:41,880
training set that's something that we've actually done a lot of from the beginning partly because of

386
00:33:41,880 --> 00:33:47,160
our applications like when you're looking at fraud um you know the way you find out if you were

387
00:33:47,160 --> 00:33:53,400
right or not is that like in some time period usually within 90 days but sometimes longer than that

388
00:33:53,400 --> 00:33:59,480
the cardholder decides um whether they're going to dispute something as fraudulent or not um and

389
00:33:59,480 --> 00:34:03,560
that's compared to like you know if you're doing ads or trying to get clicks like you kind of get

390
00:34:03,560 --> 00:34:10,360
the result right away um and we you know so I think we've always like been interested in kind of

391
00:34:10,360 --> 00:34:14,600
like being able to backfill so that is you know you can log things forward but then it's like you'll

392
00:34:14,600 --> 00:34:18,120
probably have to wait a little bit of time before you have enough of a data set that you can train

393
00:34:18,120 --> 00:34:24,120
on it cool so we talked about the data uh side of things we talked about training and experiments

394
00:34:24,840 --> 00:34:30,760
how about inference yeah that's that's a really great question and that's that's kind of like the

395
00:34:30,760 --> 00:34:37,400
first thing that we built infrastructure support for um at first a decent number of years ago like

396
00:34:37,400 --> 00:34:43,960
I think even before things like TensorFlow were really popular and so we have um like our own

397
00:34:43,960 --> 00:34:51,320
scholar service that we use to do our production real-time inference um and you know we started out

398
00:34:51,320 --> 00:34:55,560
especially because we have like mostly transactional data we don't know a lot of things like images

399
00:34:55,560 --> 00:35:00,600
at least as our most critical applications at this point um a lot of our early models and even

400
00:35:00,600 --> 00:35:04,360
still today like most of our production models are kind of like tree based models like initially

401
00:35:04,360 --> 00:35:10,040
things like random forest and now things more like xg boost and so you know we've kind of like um

402
00:35:10,040 --> 00:35:15,800
um we have the serialization for that built in to our training workflows and um we've optimized

403
00:35:15,800 --> 00:35:20,360
that to run pretty efficiently in our scholar inference service and then we've built some kind of

404
00:35:20,360 --> 00:35:27,000
nice layers on top of that um for things like model composition kind of what we call meta models where

405
00:35:27,000 --> 00:35:32,280
you know you can kind of like take your machine learning model and um kind of like almost like

406
00:35:32,280 --> 00:35:38,840
within the model sort of compose something like add a threshold to it um or like for radar we train

407
00:35:38,840 --> 00:35:44,040
you know some array of like in some cases user specific models along with like maybe more of

408
00:35:44,040 --> 00:35:49,720
some global models and so you can kind of incorporate in the framework of a model um doing that

409
00:35:49,720 --> 00:35:54,520
dispatch for your kind of like if it matches these conditions that score with these models otherwise

410
00:35:54,520 --> 00:35:59,960
score with this model and like here's how you combine it um and then the way that interfaces with

411
00:35:59,960 --> 00:36:06,200
your application is that each application has uh what we call a tag and basically the tag points

412
00:36:06,200 --> 00:36:10,840
to the model identifier which is kind of like immutable and then whenever you have a new model

413
00:36:10,840 --> 00:36:16,040
you're ready to ship you just like update what does that tag point to um and then you know

414
00:36:16,040 --> 00:36:20,840
print in production you're just saying like score the model for this tag I think that's pretty

415
00:36:20,840 --> 00:36:24,360
similar to like you know if you read about uber's Michelangelo and things like that sometimes

416
00:36:24,360 --> 00:36:30,440
we're like oh we all came up with the same thing yeah I think that like a lot of people have kind

417
00:36:30,440 --> 00:36:34,600
of come up with some of these these ways of doing things that just kind of make sense yeah it

418
00:36:34,600 --> 00:36:40,840
also sounds a little bit like uh some of what uh seldom is trying to capture in the kubernetes

419
00:36:40,840 --> 00:36:48,760
environment um uh which I guess brings me to is the inference running in kubernetes or is that

420
00:36:48,760 --> 00:36:53,320
a separate infrastructure it's not right now but I think that's mostly like a matter of time

421
00:36:53,320 --> 00:36:59,160
and prioritization um like the first thing we move to kubernetes was uh the training piece because

422
00:36:59,160 --> 00:37:03,880
the workflow management piece was so powerful or sorry the resource management piece was so powerful

423
00:37:03,880 --> 00:37:09,720
like being able to swap out CPU GPU high memory we've moved some of our the sort of real-time

424
00:37:09,720 --> 00:37:14,600
feature evaluation to kubernetes which has um been really great and made it like a lot less

425
00:37:14,600 --> 00:37:19,480
toil to kind of deploy new feature versions at some point we will probably also move the

426
00:37:19,480 --> 00:37:23,080
inference service to kubernetes we just kind of haven't gotten there yet because it is still some

427
00:37:23,080 --> 00:37:31,640
work to do that and is the uh the inferences happening on AWS as well and are you using kind of

428
00:37:31,640 --> 00:37:41,080
standard CPU instances or are you doing anything fancy there yeah so um we ran on cloud for pretty

429
00:37:41,080 --> 00:37:48,280
much everything and um definitely use a lot of AWS for the real-time inference of the most sensitive

430
00:37:48,280 --> 00:37:55,480
like production use cases um we're definitely mostly using um CPU and we've done a lot of optimization

431
00:37:55,480 --> 00:38:00,440
work um so that has worked pretty well for us um I think we do have some folks who've kind of

432
00:38:00,440 --> 00:38:07,320
experimented a little bit with like hourly or batch scoring um using some other things so

433
00:38:07,320 --> 00:38:11,400
I think that's something that we're definitely thinking about as we have more people productionizing

434
00:38:12,360 --> 00:38:16,280
kind of like more complex types of models where you know we might want something different

435
00:38:16,280 --> 00:38:23,320
you mentioned a lot of optimization that you've done is that uh on a model by model basis or are

436
00:38:23,320 --> 00:38:30,840
there uh platform uh things that you've done that help optimize across the various models that

437
00:38:30,840 --> 00:38:36,360
you're deploying uh for inference yeah definitely a lot of things at the platform level like I think

438
00:38:36,360 --> 00:38:41,880
the first models that we ever ever scored in our inference service were serialized with YAML

439
00:38:42,680 --> 00:38:47,960
and they were like really huge and um they caused a lot of garbage when we tried to load them

440
00:38:47,960 --> 00:38:54,840
and so like we did some work there for kind of tree-based models um to be able to load things

441
00:38:54,840 --> 00:38:59,720
from disk to memory really quickly and like not producing much garbage um so that that kind of

442
00:38:59,720 --> 00:39:04,280
thing are things that we did especially kind of like in the earlier days what are you using for

443
00:39:04,280 --> 00:39:11,800
querying the models are you doing rest or grpc or uh something altogether different

444
00:39:11,800 --> 00:39:16,840
yeah we use rest right now I think grpc is like something that we're interested in um but we haven't

445
00:39:16,840 --> 00:39:25,560
done yet is all of your all of the inference done via um kind of via rest and like kind of

446
00:39:25,560 --> 00:39:34,200
microservice style or do you also do uh more I guess embedded types of uh inference for like where

447
00:39:34,200 --> 00:39:40,040
you need have super low latency requirements does rest kind of meet the need across the application

448
00:39:40,040 --> 00:39:45,640
portfolio yeah um even for our most critical applications like shield things have worked pretty

449
00:39:45,640 --> 00:39:49,880
out one other thing our orchestration team has done that's worked really well for us is um

450
00:39:49,880 --> 00:39:55,560
migrating a lot of things to envoy um so we've seen some some things where like we didn't understand

451
00:39:55,560 --> 00:39:59,720
why there was some delay like in what we measured for how long things trick versus like what it

452
00:39:59,720 --> 00:40:04,680
took to the user there's just kind of one away as we move to envoy and what is envoy

453
00:40:05,320 --> 00:40:10,600
envoy is like a service service networking mesh that was developed by lift um and is kind of like

454
00:40:10,600 --> 00:40:16,120
an open source open source library and so it handles a lot of it can handle a lot of things like

455
00:40:16,120 --> 00:40:23,000
service to service communication the inference environment is it doing absent of Kubernetes all the

456
00:40:23,000 --> 00:40:29,160
things that you'd expect Kubernetes to do in terms of like auto scaling and um you know load

457
00:40:29,160 --> 00:40:39,480
balancing across the different service instances or is that stuff all done um statically um we take

458
00:40:39,480 --> 00:40:45,800
care of the routing um ourselves and we also at this point have kind of like charted our inference

459
00:40:45,800 --> 00:40:51,160
service so not all models are stored on every host so that you know we don't need hosts with like

460
00:40:51,160 --> 00:40:58,760
infinite memory and so that we take care of ourselves um the scaling we is not fully automated

461
00:40:58,760 --> 00:41:03,480
at this point we do we have kind of like quality of service so we have like multiple kind of clusters

462
00:41:03,480 --> 00:41:08,280
of machines and we tear a little bit by like you know how sensitive your application is and what

463
00:41:08,280 --> 00:41:13,080
you need from it um so that we can be a little bit more relaxed with people who are developing and

464
00:41:13,080 --> 00:41:18,840
want to test and not have that like potentially have any impact on more critical applications um but we

465
00:41:18,840 --> 00:41:22,840
haven't done like totally automated scaling that's something we kind of still look at a little bit

466
00:41:22,840 --> 00:41:30,520
ourselves uh so if you were kind of just starting down this journey uh without having done all the

467
00:41:30,520 --> 00:41:35,000
the things that that you've done it's right where do you think you would start if you just

468
00:41:35,000 --> 00:41:40,840
you know you're you're at an organization that's kind of increasingly invested in or investing in

469
00:41:40,840 --> 00:41:47,880
machine learning and you know needs to try to you know gain some efficiencies. Yeah I mean I

470
00:41:47,880 --> 00:41:52,040
think if you're just starting out like it's good to think about like what are your requirements

471
00:41:52,040 --> 00:41:57,080
rate um and you know if you're just trying to iterate quickly it's like do the simplistic

472
00:41:57,080 --> 00:42:03,000
thing possible right so you know if you can do things in batch like great do things in batch um

473
00:42:03,000 --> 00:42:09,880
I think a lot of there are a lot of both open source libraries as well as manage solutions um like

474
00:42:09,880 --> 00:42:14,680
on all the different cloud providers so I think you know I don't know you know if you're only one

475
00:42:14,680 --> 00:42:19,000
person then I think that those could make a lot of sense also for people starting out because I

476
00:42:19,000 --> 00:42:23,240
think one of the interesting things with machine learning applications is that um it takes a little

477
00:42:23,240 --> 00:42:28,200
bit of work like usually there's sort of this threshold of like your modeling has to be good enough

478
00:42:28,200 --> 00:42:32,600
for this to be like a useful thing for you to do like for fried detection that's like if we can't

479
00:42:32,600 --> 00:42:37,080
catch any fraud with our models then like you know we probably shouldn't have like a fraud detection

480
00:42:37,080 --> 00:42:42,680
product um so I think it is useful to kind of have like a quick iteration cycle to find out like

481
00:42:42,680 --> 00:42:46,680
is this a viable thing that you even want to pursue and if you have an infrastructure team they

482
00:42:46,680 --> 00:42:51,480
can kind of like help um lower the bar for that but I think there are other ways to do that

483
00:42:51,480 --> 00:42:55,640
especially as you know there's been like this Cambrian explosion in the ecosystem of different

484
00:42:55,640 --> 00:43:00,760
open source platforms as well as different managed solutions. Yeah how do you how do you think

485
00:43:00,760 --> 00:43:05,320
in an organization knows when they should have an infrastructure team?

486
00:43:06,440 --> 00:43:13,400
ML in particular. Yeah I think that's a really interesting question um I guess uh in our case I

487
00:43:13,400 --> 00:43:18,280
think um you know the person who originally founded the machine learning infrastructure team

488
00:43:19,000 --> 00:43:24,520
um had worked in this area before at Twitter and kind of had a sense of like this is going to be

489
00:43:24,520 --> 00:43:28,360
a thing that we're really going to want to invest in given how important it is for a business

490
00:43:28,360 --> 00:43:32,760
and also that if you don't kind of like dedicate some folks to it it's easy for them to kind of

491
00:43:32,760 --> 00:43:37,080
get sucked up in other things like if you just have data infrastructure that's undifferentiated

492
00:43:37,880 --> 00:43:42,440
so I think it's a really interesting question there probably is this business piece right of like

493
00:43:42,440 --> 00:43:48,280
what are your ML applications like how critical are they to your business and like how difficult

494
00:43:48,280 --> 00:43:52,760
are your infrastructure requirements for them as well I think a lot of companies develop their

495
00:43:52,760 --> 00:43:57,560
ML infrastructure like starting out with things like making the notebook experience really great

496
00:43:57,560 --> 00:44:01,400
because they want to support like a lot of data scientists who are doing a lot of analysis

497
00:44:01,400 --> 00:44:05,320
and so that's like a little bit of a different arc from from the one that we've been on and I think

498
00:44:05,320 --> 00:44:11,560
that's like actually a pretty business dependent thing. Okay awesome awesome well Kelly thanks so

499
00:44:11,560 --> 00:44:17,880
much for taking the time to chat with me about this really interesting uh story and I've enjoyed

500
00:44:17,880 --> 00:44:26,840
learning about it. Cool um thanks so much for chatting really enjoyed it. All right everyone that's our show

501
00:44:26,840 --> 00:44:33,240
for today for more information about today's guest or to follow along with AI platform volume 2

502
00:44:33,240 --> 00:44:41,480
visit twimmelai.com slash AI platforms 2 make sure you visit twimmelcon.com for more information

503
00:44:41,480 --> 00:44:47,960
order register for twimmelcon AI platforms thanks again to sig up for their sponsorship of this

504
00:44:47,960 --> 00:44:52,680
series to check out what they're up to and take advantage of their exclusive offer for twimmel

505
00:44:52,680 --> 00:45:00,200
listeners visit twimmelai.com slash sig opt as always thanks so much for listening and catch you next

506
00:45:00,200 --> 00:45:11,400
time

