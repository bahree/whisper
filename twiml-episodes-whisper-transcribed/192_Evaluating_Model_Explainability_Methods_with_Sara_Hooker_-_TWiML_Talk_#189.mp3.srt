1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,760
I'm your host Sam Charrington.

4
00:00:31,760 --> 00:00:37,560
A few weeks ago, machine learning and AI researchers, practitioners and students from Africa and around

5
00:00:37,560 --> 00:00:42,720
the world met in Cape Town for the second annual deep learning in Daba, an event that aims

6
00:00:42,720 --> 00:00:47,160
to expand African participation and contribution in the field.

7
00:00:47,160 --> 00:00:50,960
While I wasn't able to make it to Cape Town myself, I did have a chance to speak with some

8
00:00:50,960 --> 00:00:55,080
of the event's awesome speakers and I'm excited to present to you our deep learning in Daba

9
00:00:55,080 --> 00:00:56,880
series.

10
00:00:56,880 --> 00:01:02,200
In this, the first episode of the series were joined by Sarah Hooker, AI resident at Google

11
00:01:02,200 --> 00:01:03,200
Brain.

12
00:01:03,200 --> 00:01:07,040
I had the pleasure of speaking with Sarah in the run up to the endaba about her work on

13
00:01:07,040 --> 00:01:09,960
interpretability and deep neural networks.

14
00:01:09,960 --> 00:01:14,360
We discussed what interpretability means and when it's important and explore some nuances

15
00:01:14,360 --> 00:01:19,080
like the distinction between interpreting model decisions versus model function.

16
00:01:19,080 --> 00:01:24,400
We also dig into her paper evaluating feature importance estimates and look at the relationship

17
00:01:24,400 --> 00:01:29,040
between this work and interpretability approaches like Lime.

18
00:01:29,040 --> 00:01:33,240
We also talk a bit about Google, in particular the relationship between Brain and the rest

19
00:01:33,240 --> 00:01:38,120
of the Google AI landscape and the significance of the recently announced Google AI Lab in

20
00:01:38,120 --> 00:01:42,240
a Cragana, being led by friend of the show Mustafa Sisay.

21
00:01:42,240 --> 00:01:46,480
And of course, we chat a bit about the endaba as well.

22
00:01:46,480 --> 00:01:50,280
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their

23
00:01:50,280 --> 00:01:54,240
support of the podcast and their sponsorship of this series.

24
00:01:54,240 --> 00:01:58,360
In this podcast, you heard Sarah talk about the AI residency program she's in at Google.

25
00:01:58,360 --> 00:02:03,840
Well, just yesterday, they opened up applications for the 2019 program.

26
00:02:03,840 --> 00:02:07,960
The Google AI residency is a one year machine learning research training program with the

27
00:02:07,960 --> 00:02:13,080
goal of helping individuals become successful machine learning researchers.

28
00:02:13,080 --> 00:02:17,640
The program seeks residents from a very diverse set of educational and professional backgrounds

29
00:02:17,640 --> 00:02:19,480
from all over the world.

30
00:02:19,480 --> 00:02:23,960
So if you think this is something that sounds interesting, you should definitely apply.

31
00:02:23,960 --> 00:02:29,280
Find out more about the program at g.co slash AI residency.

32
00:02:29,280 --> 00:02:31,920
And now on to the show.

33
00:02:31,920 --> 00:02:34,560
All right, everyone.

34
00:02:34,560 --> 00:02:36,520
I am on the line with Sarah Hooker.

35
00:02:36,520 --> 00:02:39,560
Sarah is an AI resident at Google Brain.

36
00:02:39,560 --> 00:02:42,360
Sarah, welcome to this week in machine learning and AI.

37
00:02:42,360 --> 00:02:43,360
Hi, Sam.

38
00:02:43,360 --> 00:02:46,440
I am so excited for this conversation.

39
00:02:46,440 --> 00:02:54,280
Let's start with a little bit of background and you've got a somewhat non traditional

40
00:02:54,280 --> 00:02:57,760
background for a Google brain researcher.

41
00:02:57,760 --> 00:03:00,000
You taught yourself machine learning.

42
00:03:00,000 --> 00:03:02,000
Tell me a little bit about that.

43
00:03:02,000 --> 00:03:07,840
Yeah, it's interesting with these with these type of questions of how did you get into

44
00:03:07,840 --> 00:03:08,840
research.

45
00:03:08,840 --> 00:03:12,960
And you have these very common reference points that I shared.

46
00:03:12,960 --> 00:03:16,040
I went to this school and then I did this PhD.

47
00:03:16,040 --> 00:03:22,720
And a lot of my career has been driven by things which are less compactly described,

48
00:03:22,720 --> 00:03:31,400
which is they've been much driven by curiosity and like moderate obsession with certain questions.

49
00:03:31,400 --> 00:03:38,800
But I think probably one of the pivotal moments was I started a nonprofit four years ago.

50
00:03:38,800 --> 00:03:44,400
And it was cool delta at what is cool delta analytics and it works with other nonprofits

51
00:03:44,400 --> 00:03:49,480
all over the world and also teaches machine learning.

52
00:03:49,480 --> 00:03:54,600
But at the time when I started it, it was my background originally academically as an

53
00:03:54,600 --> 00:03:55,600
economics.

54
00:03:55,600 --> 00:04:00,200
And I was very excited to just do economic modeling for nonprofits.

55
00:04:00,200 --> 00:04:05,840
And then the composition of volunteers that we had because we were in the Bay Area was

56
00:04:05,840 --> 00:04:13,600
kind of eclectic mix of economists and engineers and machine learning researchers.

57
00:04:13,600 --> 00:04:17,000
And that was kind of a turning point for me because in some ways the tasks that we were

58
00:04:17,000 --> 00:04:18,440
looking at were really exciting.

59
00:04:18,440 --> 00:04:25,440
And we were working with nonprofits that were doing education programs using pre-smart

60
00:04:25,440 --> 00:04:27,960
technology in Nairobi.

61
00:04:27,960 --> 00:04:34,840
We also worked with fascinating problems where it was detecting illegal chainsaw activity

62
00:04:34,840 --> 00:04:37,920
and rainforests using audio.

63
00:04:37,920 --> 00:04:42,840
And it both gave me this idea that machine learning is just so exciting and so interesting.

64
00:04:42,840 --> 00:04:47,760
But also I was working with people who were a lot better than me.

65
00:04:47,760 --> 00:04:56,320
And so I think that's also was fairly critical in just my own sense of measuring my technical

66
00:04:56,320 --> 00:05:01,960
progress and feeling like, wow, I really want to learn both the tools and the framework

67
00:05:01,960 --> 00:05:05,320
to be able to solve some of these problems.

68
00:05:05,320 --> 00:05:12,960
The second part really was I've spent two years working deploying algorithms.

69
00:05:12,960 --> 00:05:18,040
So I joined an online education company called Udemy.

70
00:05:18,040 --> 00:05:23,280
And there I was working on recommendation and spam detection.

71
00:05:23,280 --> 00:05:29,760
And that was fascinating because a lot of it was how do we deploy models which is very

72
00:05:29,760 --> 00:05:35,360
different from only focusing on developing models mainly because you're willing to give

73
00:05:35,360 --> 00:05:41,880
up quite a bit of accuracy in order to deploy successfully and have a robust deployment pipeline.

74
00:05:41,880 --> 00:05:47,600
So for me, kind of at the end of those two experiences and Delta has really been in parallel

75
00:05:47,600 --> 00:05:53,680
this whole time, for me brain was really a question of, do I enjoy research?

76
00:05:53,680 --> 00:05:58,480
Because I really enjoyed all these very domain specific questions.

77
00:05:58,480 --> 00:06:05,000
And I want to see, firstly, does that curiosity translate to a research framework and is

78
00:06:05,000 --> 00:06:08,520
it satisfying in the same way?

79
00:06:08,520 --> 00:06:14,200
And the residency has been, in particular, doing research at brain has been a very good

80
00:06:14,200 --> 00:06:18,040
way to answer that question.

81
00:06:18,040 --> 00:06:23,120
So that kind of brings me to today a lot of what I've done over the last years being

82
00:06:23,120 --> 00:06:28,080
doing research on interpretability and now model compression of brain.

83
00:06:28,080 --> 00:06:32,640
Have you arrived at an answer in terms of how you feel about research?

84
00:06:32,640 --> 00:06:41,720
And I'm curious from someone who, the perspective of someone who is new to research from an applied

85
00:06:41,720 --> 00:06:47,440
background and not to mention a self-taught background like, how has that transition been

86
00:06:47,440 --> 00:06:48,440
for you?

87
00:06:48,440 --> 00:06:50,520
How has that experience been for you?

88
00:06:50,520 --> 00:06:52,200
What do you like about research?

89
00:06:52,200 --> 00:06:54,360
What do you prefer about application?

90
00:06:54,360 --> 00:06:56,960
Yeah, that's an excellent question.

91
00:06:56,960 --> 00:07:04,320
And it's important because I think that there's few of my colleagues who have perhaps experienced

92
00:07:04,320 --> 00:07:05,320
both.

93
00:07:05,320 --> 00:07:12,640
And so, although it's a sample size of one, maybe useful in some ways for other people

94
00:07:12,640 --> 00:07:17,080
thinking about whether they want to do research or stay in applied, I think there's different

95
00:07:17,080 --> 00:07:18,680
pros and cons.

96
00:07:18,680 --> 00:07:28,280
So with my applied work, a lot of what is important about the skills and the thought process

97
00:07:28,280 --> 00:07:33,160
that applied work in parts is that you can't really abstract any part of the pipeline.

98
00:07:33,160 --> 00:07:40,680
So you have data preparation and your data is normally very specific to your use case.

99
00:07:40,680 --> 00:07:45,320
And so often you spend a lot of time getting usable data.

100
00:07:45,320 --> 00:07:50,920
And many times, for example, in the Delta projects, the cost of incremental data collection

101
00:07:50,920 --> 00:07:55,720
is fairly high unless you're using a mobile app or something that automatically generates

102
00:07:55,720 --> 00:07:57,200
new data.

103
00:07:57,200 --> 00:08:02,640
So understanding the distribution of your data is much, much more important.

104
00:08:02,640 --> 00:08:08,040
And then you also have the deployment phase and applied, which again, imposes very severe

105
00:08:08,040 --> 00:08:13,480
constraints on what you can actually do because you need to justify every additional step

106
00:08:13,480 --> 00:08:16,320
of complexity that you add to a model.

107
00:08:16,320 --> 00:08:22,120
On the other hand, I think research is this very kind of particular way of having a discourse

108
00:08:22,120 --> 00:08:23,120
by the problem.

109
00:08:23,120 --> 00:08:25,320
And it's often prickly.

110
00:08:25,320 --> 00:08:33,400
And I mean that in the sense that you're often using a very clearly stated set of assumptions

111
00:08:33,400 --> 00:08:40,360
about how you want to navigate a given task.

112
00:08:40,360 --> 00:08:48,280
But you are advancing solutions that should have a contribution, even if a time is marginal

113
00:08:48,280 --> 00:08:53,040
to a large set of problems, and not just a specific problem.

114
00:08:53,040 --> 00:08:57,080
But why I raised the difference earlier, you have to think about your data pipeline,

115
00:08:57,080 --> 00:09:01,200
you have to think about your deployments, what applied is that research or at least the

116
00:09:01,200 --> 00:09:05,040
current body of machine learning research largely abstracts those two.

117
00:09:05,040 --> 00:09:08,880
So we work with three datasets.

118
00:09:08,880 --> 00:09:14,800
And I'm sure most researchers listening know which ones, but evidence, sci-fi, and

119
00:09:14,800 --> 00:09:15,800
image net.

120
00:09:15,800 --> 00:09:17,760
And we take that as given.

121
00:09:17,760 --> 00:09:21,000
And we're not too concerned about deployment.

122
00:09:21,000 --> 00:09:25,840
We don't really think about this very interesting new research that is focusing on the interaction

123
00:09:25,840 --> 00:09:27,840
between the model and the hardware.

124
00:09:27,840 --> 00:09:31,320
But for the large part, we focus on the representation.

125
00:09:31,320 --> 00:09:37,560
And so large part of our time is very much thinking about how can we learn better representations

126
00:09:37,560 --> 00:09:44,200
given this data, and not really fusing too much about the details of how it translates.

127
00:09:44,200 --> 00:09:50,560
This is good and bad, it really, and maybe here's, and probably, I sense in plighting

128
00:09:50,560 --> 00:09:55,240
your questions also for me how abrupt the transition was.

129
00:09:55,240 --> 00:10:00,200
And like, what were challenges, personally, and what did I enjoy?

130
00:10:00,200 --> 00:10:05,640
I think that doing this and doing the residency is a very particular form of entering research

131
00:10:05,640 --> 00:10:13,320
because you have a lot of contact with very senior established researchers in the field,

132
00:10:13,320 --> 00:10:16,240
probably even more so than you would have in a PhD program.

133
00:10:16,240 --> 00:10:22,120
So to a large degree, if anything, I would say the fact that I really enjoy research

134
00:10:22,120 --> 00:10:29,560
and the fact that I think this way of discourse is valuable is probably also because I've

135
00:10:29,560 --> 00:10:33,520
had a very exciting set of collaborations here.

136
00:10:33,520 --> 00:10:37,280
And a lot of what determines how you think about a problem is also who you work with.

137
00:10:37,280 --> 00:10:40,800
And so that has been very charged and very exciting.

138
00:10:40,800 --> 00:10:48,920
I will say I do miss the feedback loop that comes from deploying a model because a lot

139
00:10:48,920 --> 00:10:57,480
of where how we measure progress in a task and research are perhaps metrics that might

140
00:10:57,480 --> 00:11:04,040
not actually might, and I say this with a note of hesitation in my voice because I think

141
00:11:04,040 --> 00:11:10,640
it's an active debate, but we use metrics like accuracy or for interpretability, it's

142
00:11:10,640 --> 00:11:13,920
even more complex, what metrics do we even use.

143
00:11:13,920 --> 00:11:21,200
But these are metrics where you may actually have a very high, may have a successful representation

144
00:11:21,200 --> 00:11:25,720
that inches up interpretable accuracy for these given data sets, but it's unclear

145
00:11:25,720 --> 00:11:31,280
like if you were to actually translate that to real world tasks, whether that would be

146
00:11:31,280 --> 00:11:32,800
a successful or not.

147
00:11:32,800 --> 00:11:39,040
And I think I'll point there because I'm curious like whether anything I said I should elaborate

148
00:11:39,040 --> 00:11:40,040
on.

149
00:11:40,040 --> 00:11:41,240
And I do a few things out there.

150
00:11:41,240 --> 00:11:46,080
Well, what's interesting about what you said was that you're probably at, you're doing

151
00:11:46,080 --> 00:11:52,720
research at one of the places that, you know, along the spectrum of pure and applied

152
00:11:52,720 --> 00:11:59,520
research is probably a lot closer to the applied side than most.

153
00:11:59,520 --> 00:12:05,280
And in fact, we see all the time how research that's happening at Google Brain ends up

154
00:12:05,280 --> 00:12:13,760
in products like Duplex and the Google Cloud products, I'm wondering how that fits into

155
00:12:13,760 --> 00:12:14,760
the equation.

156
00:12:14,760 --> 00:12:23,880
And I guess what's interesting is that you still characterize it as fairly disconnected

157
00:12:23,880 --> 00:12:28,360
from application, whereas I tend to think of it as fairly close.

158
00:12:28,360 --> 00:12:32,200
And I'm wondering if you can provide some context for that.

159
00:12:32,200 --> 00:12:33,200
Yeah.

160
00:12:33,200 --> 00:12:37,040
So I think it's much to do with incentive structures.

161
00:12:37,040 --> 00:12:44,840
So brain is exciting for researchers, because there's no incentive in how we, in our iteration

162
00:12:44,840 --> 00:12:48,720
cycle of ideas, that obliges us to think about product at all.

163
00:12:48,720 --> 00:12:56,360
In fact, like that's what's so charged about the atmosphere is that we don't have to,

164
00:12:56,360 --> 00:13:02,000
at any time, even if we don't deploy to a Google product or to any product, our research

165
00:13:02,000 --> 00:13:06,320
can still be considered successful and a contribution.

166
00:13:06,320 --> 00:13:12,560
The larger part of how brain, specifically, is orientated as around contributing to

167
00:13:12,560 --> 00:13:15,080
the wider research discourse.

168
00:13:15,080 --> 00:13:22,520
So that's what I meant by it's removed is that largely our framework is the same as academia.

169
00:13:22,520 --> 00:13:30,120
We start with an idea, and then a lot of times the combination of ideas is a successful

170
00:13:30,120 --> 00:13:36,280
contribution to a conference or it's open-sourcing code, so things that enrich the larger community.

171
00:13:36,280 --> 00:13:40,160
That being said, I will say one way in which perhaps this is a little bit different from

172
00:13:40,160 --> 00:13:49,360
what we imagine academia to be, is that because of, because it's an industry lab, there's

173
00:13:49,360 --> 00:13:55,400
much more room to do empirical experiments, which I think is quite exciting, and when

174
00:13:55,400 --> 00:14:02,160
I say empirical experiments, what I mean is that oftentimes that, previously, I would

175
00:14:02,160 --> 00:14:10,000
say, you can see the evolution in what data sets are considered the benchmark for just

176
00:14:10,000 --> 00:14:14,640
to find a certain hypothesis, or just to find a certain theoretical approach.

177
00:14:14,640 --> 00:14:21,360
We kind of moved from amnesty and sufficient to now, probably most people would not consider

178
00:14:21,360 --> 00:14:26,280
amnesty sufficient as evidence for your hypothesis is correct.

179
00:14:26,280 --> 00:14:32,880
And now you have cypher, and I would say at places like Google, these research labs,

180
00:14:32,880 --> 00:14:37,720
it is possible to do large scale experiments for data sets like ImageNet and do a lot

181
00:14:37,720 --> 00:14:45,800
of empirical and important empirical work to corroborate hypothesis, or perhaps to say

182
00:14:45,800 --> 00:14:51,160
that certain hypotheses that were previously held may not be correct given the data.

183
00:14:51,160 --> 00:14:56,720
So that is exciting, and that's one way in which the conversation is different.

184
00:14:56,720 --> 00:15:01,440
That being said, I'm sure there's aspects to a culture of academic lab that foster new

185
00:15:01,440 --> 00:15:07,560
directions of thinking, that perhaps because we can iterate so quickly with experiments

186
00:15:07,560 --> 00:15:13,360
here, we may not inform that in the way that we talk about these things.

187
00:15:13,360 --> 00:15:17,840
That I'm not sure about, because this is the only lab I've known, but I can imagine that

188
00:15:17,840 --> 00:15:20,600
would be a counterpoint in someone would say.

189
00:15:20,600 --> 00:15:27,120
It does surprise me a bit to hear that you are working day-to-day with the same data sets

190
00:15:27,120 --> 00:15:28,520
that everyone else is.

191
00:15:28,520 --> 00:15:36,240
I would have imagined that you had some super ImageNet plus plus or cypher plus plus that

192
00:15:36,240 --> 00:15:43,280
you're using, and when you're ready to publish, you dump things down for the outside world.

193
00:15:43,280 --> 00:15:49,640
I can only speak for my experience, my immediate colleagues, but there's no ImageNet plus

194
00:15:49,640 --> 00:15:55,720
that we just scurry around with behind the scenes.

195
00:15:55,720 --> 00:16:03,080
And perhaps that's because of essentially the cost of switching in between data sets.

196
00:16:03,080 --> 00:16:04,760
So this is another interesting insight.

197
00:16:04,760 --> 00:16:11,840
This is a lot of the reason why we, this is my opinion, but I sense a lot of the reason

198
00:16:11,840 --> 00:16:17,040
why we're using these same data sets is that often, during an idea and implementing

199
00:16:17,040 --> 00:16:24,200
these data sets, when you try and translate it to perhaps a more realistic data set or

200
00:16:24,200 --> 00:16:29,080
more complex, you may, it's not that you would get different results, it's just that like

201
00:16:29,080 --> 00:16:33,240
the implementation and how you go about it may be fairly different.

202
00:16:33,240 --> 00:16:38,960
And so there's given limited time resources and given that everyone else is articulating

203
00:16:38,960 --> 00:16:42,040
progress in terms of these three data sets.

204
00:16:42,040 --> 00:16:47,480
If you are a researcher, you also want to be talking in the framework of these publicly

205
00:16:47,480 --> 00:16:52,520
available data sets, because research is a lot about how do you measure progress in

206
00:16:52,520 --> 00:16:58,680
a task, and how do you articulate convincingly that you've made a contribution, and that

207
00:16:58,680 --> 00:17:04,640
largely involves referencing prior work in the same area that has used these same data

208
00:17:04,640 --> 00:17:05,640
sets.

209
00:17:05,640 --> 00:17:10,640
And then you say, given that this is a commonly understood reference point, this is what

210
00:17:10,640 --> 00:17:15,440
I advance is for the progress, that kind of restricts how much you can jump back and

211
00:17:15,440 --> 00:17:16,440
forth.

212
00:17:16,440 --> 00:17:21,040
But maybe, maybe what, maybe some researchers are using ImageNet for suspects, but I just

213
00:17:21,040 --> 00:17:25,200
haven't been kept in the loop.

214
00:17:25,200 --> 00:17:33,680
So in some ways, I'm thinking it's a bit of a negative critique on the state of our methods

215
00:17:33,680 --> 00:17:41,640
and research in general, and perhaps even application, that we're so tied to specific

216
00:17:41,640 --> 00:17:42,640
data sets.

217
00:17:42,640 --> 00:17:49,480
You know, certainly having a data set as kind of a lingua franca for comparing results

218
00:17:49,480 --> 00:17:51,320
and all that makes sense.

219
00:17:51,320 --> 00:17:58,480
But there's also a degree to which our methods and our research results are tied to these

220
00:17:58,480 --> 00:18:02,080
specific data sets, kind of the classic overfit on ImageNet.

221
00:18:02,080 --> 00:18:04,640
Do you see that as well in your work?

222
00:18:04,640 --> 00:18:05,640
Yes.

223
00:18:05,640 --> 00:18:09,400
Sam, yeah, you're bringing up an important point.

224
00:18:09,400 --> 00:18:16,680
And this is one that I often, I think, is a very active thread of discussion at brain.

225
00:18:16,680 --> 00:18:23,320
I sense people, well, I sense I've heard two different perspectives on this.

226
00:18:23,320 --> 00:18:29,480
There's a general consensus that, as you described, overfitting the data sets that we have.

227
00:18:29,480 --> 00:18:35,000
And I would say that a few researchers would dispute that entirely or would at least acknowledge

228
00:18:35,000 --> 00:18:41,640
that we've really centered a lot of attention on these three data sets.

229
00:18:41,640 --> 00:18:42,640
You can even go further.

230
00:18:42,640 --> 00:18:47,440
Like, both the fields that I've worked in interpretability and model compression, a lot

231
00:18:47,440 --> 00:18:49,920
of the focus has been on computer vision.

232
00:18:49,920 --> 00:18:57,800
In fact, I would venture that very few papers have talked about different tasks or different

233
00:18:57,800 --> 00:18:58,800
architectures.

234
00:18:58,800 --> 00:19:04,480
Even though there's an urgent need for interpretability beyond just computer vision models.

235
00:19:04,480 --> 00:19:07,840
And I kind of said at the beginning, there's two perspectives to this.

236
00:19:07,840 --> 00:19:14,920
The other perspective is that research is a very particular way of talking about a problem.

237
00:19:14,920 --> 00:19:25,040
And the framework of how discussion occurs in research community has to be, is by design

238
00:19:25,040 --> 00:19:35,560
fairly narrow, mainly because it's a very precise way of advancing a scientific hypothesis

239
00:19:35,560 --> 00:19:37,160
and a contribution.

240
00:19:37,160 --> 00:19:44,080
So it's unclear to me that if someone did deviate from this norm and showed up with a new

241
00:19:44,080 --> 00:19:48,520
paper on a new data set and said, this is a huge improvement.

242
00:19:48,520 --> 00:19:52,920
I think either they would have to benchmark previous methods on a new data set, which

243
00:19:52,920 --> 00:19:59,040
is a large technical contribution, depending on the field.

244
00:19:59,040 --> 00:20:02,640
All I would be left with doubt is to what their contribution actually is.

245
00:20:02,640 --> 00:20:07,280
And so that kind of captures the dilemma that a lot of researchers feel is that they acknowledge

246
00:20:07,280 --> 00:20:10,920
it, but it's unclear how to proceed.

247
00:20:10,920 --> 00:20:18,080
So your particular research, or at least the research that you've spent the most time

248
00:20:18,080 --> 00:20:21,680
on thus far, is around interpretability.

249
00:20:21,680 --> 00:20:25,760
And you're starting to do some model compression work now, but tell us specifically about the

250
00:20:25,760 --> 00:20:29,840
interpretability work that you've been publishing on.

251
00:20:29,840 --> 00:20:36,680
Maybe I'll start by just introducing interpretability and how it's commonly thought of within

252
00:20:36,680 --> 00:20:38,760
research.

253
00:20:38,760 --> 00:20:44,080
And then that will provide some context for the work that I've been doing.

254
00:20:44,080 --> 00:20:46,480
Interpreterability is a very interesting problem.

255
00:20:46,480 --> 00:20:53,040
interpretability broadly, when I ask, for example, Sam, if I asked you, do we want models

256
00:20:53,040 --> 00:20:54,040
to be interpretable?

257
00:20:54,040 --> 00:20:57,160
What would your answer be?

258
00:20:57,160 --> 00:20:59,920
Do we want models to be interpretable?

259
00:20:59,920 --> 00:21:03,440
I think sometimes if it doesn't cost too much.

260
00:21:03,440 --> 00:21:04,440
Excellent.

261
00:21:04,440 --> 00:21:05,440
Interesting.

262
00:21:05,440 --> 00:21:06,440
Yeah.

263
00:21:06,440 --> 00:21:13,200
You captured one of the key misconceptions that exists in the field, which is that all models

264
00:21:13,200 --> 00:21:14,200
must be interpretable.

265
00:21:14,200 --> 00:21:18,280
In fact, there's like new answers there, which is what you're describing.

266
00:21:18,280 --> 00:21:19,280
Yeah.

267
00:21:19,280 --> 00:21:23,680
The starting point when I ask people those questions is that it's a firm yes.

268
00:21:23,680 --> 00:21:28,360
So I like that you've hedged.

269
00:21:28,360 --> 00:21:32,800
And I think that's because we instinctively think of interpretability as this desirable

270
00:21:32,800 --> 00:21:37,240
property, kind of like fairness or bias.

271
00:21:37,240 --> 00:21:42,680
And it's interesting because then the next question is, well, what do you think is an

272
00:21:42,680 --> 00:21:44,000
interpretable model?

273
00:21:44,000 --> 00:21:50,800
And that's generally where there's uncertainty on the part of the person who's answering.

274
00:21:50,800 --> 00:21:57,040
And maybe here I'll say is that interpretability within research has really focused on this

275
00:21:57,040 --> 00:22:03,360
idea for these deep neural networks, where we can't articulate in a compact way what the

276
00:22:03,360 --> 00:22:10,280
function that the model learns is, can we arrive at a methodology to try and explain

277
00:22:10,280 --> 00:22:17,040
the model prediction or perhaps even the model function, which means I can the model function,

278
00:22:17,040 --> 00:22:23,320
like can we understand how the model maps every input to every output.

279
00:22:23,320 --> 00:22:28,440
And you illustrated one of the key misconceptions is that the degree to which we want to do this

280
00:22:28,440 --> 00:22:35,040
and where we really want to invest effort may of fact depend depend upon the task.

281
00:22:35,040 --> 00:22:37,760
So you talked about the burden, well, the burden can be thought of in a few different

282
00:22:37,760 --> 00:22:45,040
ways, but one way is this idea that for some tasks, like if we can't incorrectly explain

283
00:22:45,040 --> 00:22:51,320
how the model rises the decision, the cost on human welfare may be intolerable.

284
00:22:51,320 --> 00:22:57,640
Like examples of this could be, for example, healthcare, where we're using a deep neural

285
00:22:57,640 --> 00:23:00,160
network to revert diagnosis.

286
00:23:00,160 --> 00:23:04,720
And then the doctor has to try and explain that diagnosis to a patient.

287
00:23:04,720 --> 00:23:09,240
If there's an incorrect explanation that's given to the doctor, you can understand it

288
00:23:09,240 --> 00:23:15,000
has perhaps, like we do, as a society, we wouldn't be willing to tolerate that.

289
00:23:15,000 --> 00:23:21,480
However, there may be other tasks where either because the actual cost to humans is seen

290
00:23:21,480 --> 00:23:28,160
to be, I hate to say the words low impact, but at least there's not a significant notions

291
00:23:28,160 --> 00:23:34,320
of decreasing welfare involved, or the task has enough empirical evidence where we're

292
00:23:34,320 --> 00:23:37,960
just more confident in like the overall behavior of the model.

293
00:23:37,960 --> 00:23:41,920
Those are tasks which are currently said we don't need to focus as much as interpretability

294
00:23:41,920 --> 00:23:47,160
because we have a, we have reassurance in different ways that this model is working

295
00:23:47,160 --> 00:23:48,880
the way it is.

296
00:23:48,880 --> 00:23:56,280
The other key challenge about interpretability is that it's very unclear when we've actually

297
00:23:56,280 --> 00:23:59,520
arrived.

298
00:23:59,520 --> 00:24:05,560
And this is really the tricky part is that when do we say this model is interpretable?

299
00:24:05,560 --> 00:24:08,440
We're happy, we signed off, job done.

300
00:24:08,440 --> 00:24:13,600
In fact, it's both hard to measure progress on the task, but interesting enough, the

301
00:24:13,600 --> 00:24:17,200
finish line may look very different for different people.

302
00:24:17,200 --> 00:24:22,680
The burden we carry of delivering a satisfying interpretable explanation may be different

303
00:24:22,680 --> 00:24:26,360
depending on even what our downstream task is.

304
00:24:26,360 --> 00:24:34,520
Like a good example of this is if you're sitting on a plane, and your plane is just delayed,

305
00:24:34,520 --> 00:24:42,480
and the pilot comes on in and says, oh, we have an arrow with one of the engines on the

306
00:24:42,480 --> 00:24:48,360
plane, and then they say that, we're fixing it, we have ground staff.

307
00:24:48,360 --> 00:24:53,360
So as a passenger, you're just sitting in the cheap seats, and like you actually don't

308
00:24:53,360 --> 00:24:58,880
know much beyond the technical error that's being fixed.

309
00:24:58,880 --> 00:25:00,560
The pilot probably knows a lot more.

310
00:25:00,560 --> 00:25:04,040
They've probably been talking to the ground staff, and they have an understanding of at least

311
00:25:04,040 --> 00:25:06,720
like what part of the plane and what would it influence.

312
00:25:06,720 --> 00:25:10,920
And then the ground staff probably has the most technical level of conversation because

313
00:25:10,920 --> 00:25:14,400
they're precisely locating a certain sensor.

314
00:25:14,400 --> 00:25:19,920
And so when the ground staff tells the pilot that it's fixed, that probably involves some

315
00:25:19,920 --> 00:25:23,240
level of technical detail, so the pilot's confident.

316
00:25:23,240 --> 00:25:29,240
When the pilot tells the passengers, it's normally, okay, sold, we're taking off.

317
00:25:29,240 --> 00:25:35,400
And so I think that in a compact way kind of describes that we may need different levels

318
00:25:35,400 --> 00:25:38,400
of explanations, and that is okay.

319
00:25:38,400 --> 00:25:43,360
In fact, like if the pilot were to tell you every single technical detail where the ground

320
00:25:43,360 --> 00:25:49,800
staff told them, we may feel slightly overwhelmed, or at least we would feel like,

321
00:25:49,800 --> 00:25:53,480
oh wow, this is really serious, because they're telling a lot of different job game that

322
00:25:53,480 --> 00:25:58,640
I don't know, and I'm feeling very, very disorientated.

323
00:25:58,640 --> 00:26:06,080
And a lot of interpretability is trying to understand how do we both create a meaningful

324
00:26:06,080 --> 00:26:11,880
explanation, and meaningful here really means given the domain, and given who the person

325
00:26:11,880 --> 00:26:15,120
is, and what they have to do with that information.

326
00:26:15,120 --> 00:26:21,600
But the topic of my research has been that it also should be reliable, and by reliable,

327
00:26:21,600 --> 00:26:31,920
I mean that in creating a meaningful explanation, we shouldn't communicate information that

328
00:26:31,920 --> 00:26:35,600
is not an accurate reflection of what the model has learned.

329
00:26:35,600 --> 00:26:42,000
And this is a delicate area because in some ways the key problem is that with deep neural

330
00:26:42,000 --> 00:26:44,920
networks, we don't know the ground truth to begin with.

331
00:26:44,920 --> 00:26:49,440
So it's hard to say this explanation is better than this one, because we don't know the

332
00:26:49,440 --> 00:26:51,480
true explanation.

333
00:26:51,480 --> 00:26:57,440
And so a lot of what I've worked on for the last year is how do we create frameworks to

334
00:26:57,440 --> 00:27:03,360
measure progress on this task, even in spite of the fact that there's no ground truth?

335
00:27:03,360 --> 00:27:10,920
And setting the stage for us, you pointed out the distinction between, I think you refer

336
00:27:10,920 --> 00:27:17,560
to it as interpreting model function versus model decision.

337
00:27:17,560 --> 00:27:23,880
But you also use the term interpretability and explainability interchangeably, or at

338
00:27:23,880 --> 00:27:25,440
least that was the impression that I got.

339
00:27:25,440 --> 00:27:30,400
I tend to think of like interpretability as that functional, you know, what is really

340
00:27:30,400 --> 00:27:36,000
happening in the model and explainability is, you know, given the model is more or less

341
00:27:36,000 --> 00:27:39,520
black boxy, how do we make some sense of what it's telling us?

342
00:27:39,520 --> 00:27:44,200
Is there a distinction there for you at all, or do you use them interchangeably, but,

343
00:27:44,200 --> 00:27:48,040
you know, in light of acknowledging the two different tasks?

344
00:27:48,040 --> 00:27:56,760
Yeah, really what I meant when I talked about explanations is this idea that a lot of interpretability

345
00:27:56,760 --> 00:28:02,960
so in interpretive research is often explanations is considered a subset.

346
00:28:02,960 --> 00:28:06,280
Explanations are just trying to explain a single example.

347
00:28:06,280 --> 00:28:11,920
So that's how I use that word there is that for an explanation, you have a given input

348
00:28:11,920 --> 00:28:17,360
and you're trying to say for this input, why did the model arrive at this prediction?

349
00:28:17,360 --> 00:28:21,800
Whereas when you're trying to understand the function as a whole, you're trying to understand

350
00:28:21,800 --> 00:28:29,600
perhaps what was most important to the function of the model learned over at least a larger

351
00:28:29,600 --> 00:28:34,560
subset of examples, if not the entire data set.

352
00:28:34,560 --> 00:28:41,800
Most interpretability work for deep neural networks is focused on the single example explanation.

353
00:28:41,800 --> 00:28:49,000
So there often it's an image and you're trying to say, why did the model arrive at this

354
00:28:49,000 --> 00:28:50,880
prediction for this image?

355
00:28:50,880 --> 00:28:56,040
That is being the largest part of the discourse so far within deep neural networks.

356
00:28:56,040 --> 00:29:04,360
And so your work is almost at a meta level, it's, you know, there's a bunch of research

357
00:29:04,360 --> 00:29:11,240
that is happening and needs to happen in interpretability, but how do we compare these

358
00:29:11,240 --> 00:29:17,560
different results without really knowing what the model is doing in the first place?

359
00:29:17,560 --> 00:29:22,200
Yeah, I love how you capture that very succinctly.

360
00:29:22,200 --> 00:29:25,840
Yeah, that's exactly the challenge.

361
00:29:25,840 --> 00:29:29,840
We now have a rich set of methods.

362
00:29:29,840 --> 00:29:37,880
And the question that has to be answered is, okay, I have all these different methodologies

363
00:29:37,880 --> 00:29:41,640
for arriving an explanation, which one do I use?

364
00:29:41,640 --> 00:29:48,200
And so my work is focused on methods that estimate feature importance in a network, and

365
00:29:48,200 --> 00:29:50,440
they estimate imperfection importance.

366
00:29:50,440 --> 00:30:00,160
So the estimators that I have evaluated with my co-authors in both pieces of work that

367
00:30:00,160 --> 00:30:07,400
I did over the last year have asked in this given input image, these estimators will say

368
00:30:07,400 --> 00:30:13,200
this pixel will essentially arrive at a ranking of these and the most important pixels for

369
00:30:13,200 --> 00:30:15,120
the model prediction.

370
00:30:15,120 --> 00:30:23,240
So imagine an image of an ostrich and these estimators will arrive at a different estimate

371
00:30:23,240 --> 00:30:29,760
for how important the pixel of the ostrich knows is to the prediction of the model at the

372
00:30:29,760 --> 00:30:32,160
other end.

373
00:30:32,160 --> 00:30:40,520
And the first piece of work that I did was it essentially started with a premise that

374
00:30:40,520 --> 00:30:46,920
one definition of reliability is that if the model is not affected by a transformation

375
00:30:46,920 --> 00:30:54,320
of the input, then these estimators should not be affected because if the model is unchanged,

376
00:30:54,320 --> 00:31:00,400
we want these estimators to reflect the model.

377
00:31:00,400 --> 00:31:10,200
And to carry out this test, we establish a very narrow ground truth where we created a

378
00:31:10,200 --> 00:31:12,960
transformation to the input, which was a mean shift.

379
00:31:12,960 --> 00:31:17,640
And then by construction, we ensure that the model was not affected.

380
00:31:17,640 --> 00:31:23,000
And so the gradients of the model did not change, the weights of the model will unchanged,

381
00:31:23,000 --> 00:31:27,200
the model was impervious to this change in the input.

382
00:31:27,200 --> 00:31:30,520
But then we showed that many of these estimators changed.

383
00:31:30,520 --> 00:31:33,320
What specifically do we mean by estimator here?

384
00:31:33,320 --> 00:31:37,480
So estimators, I actually use them to change the way with methods.

385
00:31:37,480 --> 00:31:40,400
So think of it as just these set of methods.

386
00:31:40,400 --> 00:31:45,880
And I use them as estimators because I think it's useful to think of some of these tasks

387
00:31:45,880 --> 00:31:49,760
as trying to, because there's no ground truth, really that model is trying to estimate

388
00:31:49,760 --> 00:31:51,640
what's important.

389
00:31:51,640 --> 00:31:55,600
And we're just trying to evaluate whether that estimate was good or not, whether it was

390
00:31:55,600 --> 00:31:57,920
accurate or reliable.

391
00:31:57,920 --> 00:32:06,240
When I think of the description you gave of the method, it calls to mind the Lyme method

392
00:32:06,240 --> 00:32:13,920
of explainability, which also seeks to perturb or inject noise into the inputs and use that

393
00:32:13,920 --> 00:32:20,160
to determine what parts of the input are most relevant.

394
00:32:20,160 --> 00:32:26,720
Can you maybe, for those that are familiar with Lyme, talk about how the two ideas relate?

395
00:32:26,720 --> 00:32:29,720
They relate, it's the same idea.

396
00:32:29,720 --> 00:32:30,720
Yeah.

397
00:32:30,720 --> 00:32:31,720
Okay.

398
00:32:31,720 --> 00:32:34,000
That was an excellent way to anchor the conversation.

399
00:32:34,000 --> 00:32:41,040
Yeah, so when I talk about methods or estimators, Lyme is a key example of an estimate that many

400
00:32:41,040 --> 00:32:47,440
people are familiar with, and other examples of methods or estimators can be things like

401
00:32:47,440 --> 00:32:48,800
just taking the gradient.

402
00:32:48,800 --> 00:32:55,880
So you end up with a gradient heat map or it could be a guide of backdrop or this integrated

403
00:32:55,880 --> 00:33:03,560
gradients, which is another fantastic example of an estimator that is trying to weigh importance

404
00:33:03,560 --> 00:33:05,800
of these input pixels.

405
00:33:05,800 --> 00:33:11,320
But yes, I'm really glad that you interjected there because it's one in the same.

406
00:33:11,320 --> 00:33:16,600
But so Lyme is one of these estimators and it's doing noise injection, but it sounds

407
00:33:16,600 --> 00:33:22,120
like you're essentially taking that same approach, but applying it again at kind of this meta

408
00:33:22,120 --> 00:33:26,120
level to evaluate the estimators.

409
00:33:26,120 --> 00:33:30,280
Well, maybe not quite.

410
00:33:30,280 --> 00:33:37,960
So, Lyme is one of the estimators that could be evaluated, for example.

411
00:33:37,960 --> 00:33:44,000
And I think perhaps if you think about it, so we have something like Lyme, we have these

412
00:33:44,000 --> 00:33:47,960
other methods that are estimating importance.

413
00:33:47,960 --> 00:33:57,760
And in both, in my line of research, essentially what I'm asking is all of the estimates correct

414
00:33:57,760 --> 00:34:09,520
or not, Lyme is very intuitive because I think it's exciting because it gives a clear explanation

415
00:34:09,520 --> 00:34:13,360
of what the model arrived at for that prediction.

416
00:34:13,360 --> 00:34:17,760
But the question I ask is that how do we actually know that what is exciting to us as humans

417
00:34:17,760 --> 00:34:21,160
is also a reliable reflection of the model?

418
00:34:21,160 --> 00:34:27,440
And to do that, the first piece of research I described essentially takes these methods

419
00:34:27,440 --> 00:34:33,880
and says, we've applied a transformation to the input images, which is a means shift.

420
00:34:33,880 --> 00:34:35,640
And the model has not changed.

421
00:34:35,640 --> 00:34:37,240
Do the rankings change?

422
00:34:37,240 --> 00:34:45,720
And if they change, then that is considered a failure in this test case because they

423
00:34:45,720 --> 00:34:49,120
should simply reflect what the model thinks is important.

424
00:34:49,120 --> 00:34:54,520
And if they diverge from that, then it's unreliable.

425
00:34:54,520 --> 00:35:00,400
The second research that I did was one step further, and again, this idea of how do we

426
00:35:00,400 --> 00:35:06,840
measure progress on this task of reliably estimating importance?

427
00:35:06,840 --> 00:35:15,600
And what we did there was we said, OK, these different methods all rank the pixels.

428
00:35:15,600 --> 00:35:20,240
Let's remove the pixels that they rank as most important a fraction.

429
00:35:20,240 --> 00:35:27,440
And then give those examples of these modified inputs back to the model.

430
00:35:27,440 --> 00:35:32,920
And the presumption is that if the methods actually identified what was important, if they

431
00:35:32,920 --> 00:35:38,200
actually rank these pixels, then it should really damage the model if you're training

432
00:35:38,200 --> 00:35:40,120
it again.

433
00:35:40,120 --> 00:35:44,680
And we compared it to a random removal of pixels.

434
00:35:44,680 --> 00:35:50,520
So really what they were asking, if you randomly remove pixels, essentially you're randomly

435
00:35:50,520 --> 00:35:53,120
assigning importance to the inputs.

436
00:35:53,120 --> 00:35:59,000
And if these methods don't damage the model more than just a random removal, then in some

437
00:35:59,000 --> 00:36:03,320
ways, they're less able to accurately rank than just random removal.

438
00:36:03,320 --> 00:36:05,600
So that was a very interesting piece of research.

439
00:36:05,600 --> 00:36:11,320
And it's kind of again, again, this idea that in the absence of any ground truth, what

440
00:36:11,320 --> 00:36:17,520
we need to do in this field is kind of stay precisely these desirable properties and

441
00:36:17,520 --> 00:36:22,880
come articulate frameworks in which to measure these, because then at least we haven't

442
00:36:22,880 --> 00:36:31,760
established, we've articulated a way of saying we are measuring it in this sense and given

443
00:36:31,760 --> 00:36:37,320
this framework, this method appears to be more accurate or reliable.

444
00:36:37,320 --> 00:36:46,360
And so was the end result of these two research efforts, a kind of a ranking of these different

445
00:36:46,360 --> 00:36:47,920
estimators.

446
00:36:47,920 --> 00:36:52,120
And if so, what did you tend to find?

447
00:36:52,120 --> 00:36:59,720
But also is ranking these estimators a one-dimensional thing or are there multiple ways that we should

448
00:36:59,720 --> 00:37:02,000
be comparing these different methods?

449
00:37:02,000 --> 00:37:03,000
Yeah.

450
00:37:03,000 --> 00:37:11,360
And you raise an important point, so to kind of share the results, on both we found very

451
00:37:11,360 --> 00:37:12,360
interesting results.

452
00:37:12,360 --> 00:37:20,040
And in fact, kind of an advance, at least for me, I felt taught me new things about how

453
00:37:20,040 --> 00:37:24,600
the model actually arrives at feature importance.

454
00:37:24,600 --> 00:37:32,840
So I'm both, it was a little bit of a grumpy picture.

455
00:37:32,840 --> 00:37:41,920
We found that in the most recent work, which is a paper called remove and retrain where

456
00:37:41,920 --> 00:37:47,320
you essentially are moving the most important and retraining the model, we found that the

457
00:37:47,320 --> 00:37:51,120
estimates that we consider, and we only consider a small subset.

458
00:37:51,120 --> 00:37:56,720
So I'll caveat with that, they didn't perform better than a random assignment of importance.

459
00:37:56,720 --> 00:38:06,600
But we did find this, which is fascinating, which is that if you ensemble, so if you take

460
00:38:06,600 --> 00:38:14,120
for the same image, you take various noisy estimates, which means that you add noise to

461
00:38:14,120 --> 00:38:18,360
the estimate and then do that repeatedly.

462
00:38:18,360 --> 00:38:20,920
And you combine with squaring.

463
00:38:20,920 --> 00:38:25,040
So these are two separate transformations applied to the estimate.

464
00:38:25,040 --> 00:38:32,800
So imagine this for Lyme, like essentially you'd be taking noisy Lyme estimates, squaring.

465
00:38:32,800 --> 00:38:39,040
We found that that far outperformed a random estimate of importance.

466
00:38:39,040 --> 00:38:44,880
And so that's kind of interesting, aha moment, because it's not quite aha moment, because

467
00:38:44,880 --> 00:38:49,040
it's not the theoretical justification yet for why that is the case.

468
00:38:49,040 --> 00:38:54,480
But it suggests firstly, the importance of empirical research, which is an option you

469
00:38:54,480 --> 00:39:01,800
discover on interesting things that motivate research in a new direction, but also tells

470
00:39:01,800 --> 00:39:09,480
us that we, in some ways, how we're considering these estimates now, there's very exciting

471
00:39:09,480 --> 00:39:15,120
things that we can do that will improve progress on a task.

472
00:39:15,120 --> 00:39:20,800
And then you ask, like your second question is critical, because you say, is this one

473
00:39:20,800 --> 00:39:21,800
dimension?

474
00:39:21,800 --> 00:39:22,800
Absolutely.

475
00:39:22,800 --> 00:39:28,960
Like, this work should be considered one dimension, and in fact, like, there's almost,

476
00:39:28,960 --> 00:39:34,560
even if we were to narrow it to reliability and accuracy, which is really what this

477
00:39:34,560 --> 00:39:38,880
work is trying to get at, there could be other dimensions for measuring this.

478
00:39:38,880 --> 00:39:44,280
Because essentially what we've done is we've just stated, this is our way of technically

479
00:39:44,280 --> 00:39:51,600
defining these properties, and given our definition, this is what we think are the best methods.

480
00:39:51,600 --> 00:39:58,400
Someone else would come along and say, there's another way to define technical reliability,

481
00:39:58,400 --> 00:40:04,040
and given this, this is what we find, that's the nature of this field.

482
00:40:04,040 --> 00:40:05,440
And I would encourage that.

483
00:40:05,440 --> 00:40:09,960
I think it's important for researchers and for the community as a whole to think about,

484
00:40:09,960 --> 00:40:20,040
how do we measure, how do we ensure that we are actually delivering an explanation that

485
00:40:20,040 --> 00:40:24,600
people can trust that is a reflection of the model?

486
00:40:24,600 --> 00:40:30,320
I will say one more thought on that, and then I'll pause, which is that you can also

487
00:40:30,320 --> 00:40:36,040
think about a whole other set of soluble properties that are on the being meaningful

488
00:40:36,040 --> 00:40:37,640
to human side.

489
00:40:37,640 --> 00:40:44,640
So often how that's measured now in the body of research is that user studies are conducted,

490
00:40:44,640 --> 00:40:49,720
and things like, do you trust this, are asked?

491
00:40:49,720 --> 00:41:00,320
And that in itself, I sense, could also benefit from a more, well, both larger scale treatments,

492
00:41:00,320 --> 00:41:05,760
but also much more nuance, like, is for this task, for this person, does this make sense?

493
00:41:05,760 --> 00:41:10,600
Because, inevitably, interpretability is going to evolve on a few different levels.

494
00:41:10,600 --> 00:41:17,080
Like, based on our perception as a society of what do we need for us to trust, technical

495
00:41:17,080 --> 00:41:23,800
implementations of deep neural networks, as well as in a job-specific way, which is that,

496
00:41:23,800 --> 00:41:27,840
you know, a doctor is probably going to always want an explanation of every example.

497
00:41:27,840 --> 00:41:32,280
It's always going to be at the single example level.

498
00:41:32,280 --> 00:41:37,680
But as a researcher, for example, what's more important to me, I would suggest as an interpretability

499
00:41:37,680 --> 00:41:42,520
tool, is that I want to get a sense of the distribution, and I want to understand, out

500
00:41:42,520 --> 00:41:51,000
of the distribution of data, what data points are perhaps more, either, problematic, and

501
00:41:51,000 --> 00:41:52,920
you can define problematic in a few ways.

502
00:41:52,920 --> 00:41:58,360
But if I have a better way to understand what is different in a data set, that's helpful

503
00:41:58,360 --> 00:42:02,080
for me in understanding, am I comfortable to deploy this model?

504
00:42:02,080 --> 00:42:06,640
So all these vantage points will require different desirable properties, but I'll stop

505
00:42:06,640 --> 00:42:07,640
them.

506
00:42:07,640 --> 00:42:15,240
One question that I did have was in the evaluating feature importance estimates paper,

507
00:42:15,240 --> 00:42:21,680
you look at, as you mentioned, a subset of these different estimator types, gradient

508
00:42:21,680 --> 00:42:28,160
heat map, integrated gradients and some others, is lime a subset of one of these?

509
00:42:28,160 --> 00:42:34,040
Like are you using a generic term of these that you include that lime falls under, or

510
00:42:34,040 --> 00:42:37,960
did you just not look at lime particularly?

511
00:42:37,960 --> 00:42:44,480
We did not look at lime, and I would be really excited to see the results online.

512
00:42:44,480 --> 00:42:51,640
The constraint on these papers is always this tension between, we have limited time and

513
00:42:51,640 --> 00:42:56,800
certain amount of computational resources, but partly, I think what this motivates more

514
00:42:56,800 --> 00:43:02,960
than anything is open sourcing code, which is something we're doing for this paper.

515
00:43:02,960 --> 00:43:08,440
And that's important because really if this is going to be a sustainable benchmark, researchers

516
00:43:08,440 --> 00:43:15,160
have to be able to self-serve and also be able to take what we did and apply to that method.

517
00:43:15,160 --> 00:43:20,480
But I would be really interested to see the results of this for lime, mainly because lime

518
00:43:20,480 --> 00:43:27,920
imposes contiguous feature importance by that. I mean that let me frame this using a

519
00:43:27,920 --> 00:43:29,040
counting example.

520
00:43:29,040 --> 00:43:34,200
So gradient heat maps, for example, don't require that feature importance are connected

521
00:43:34,200 --> 00:43:35,200
set of pixels.

522
00:43:35,200 --> 00:43:44,680
You just take the gradient of the pre-soft mix activation for the model prediction with

523
00:43:44,680 --> 00:43:47,320
respect to the input and then you rank.

524
00:43:47,320 --> 00:43:54,400
But that tends to result in very diffuse attribution, meaning that importance when you rank it

525
00:43:54,400 --> 00:44:02,640
doesn't concentrate exclusively on a set of connected pixels, whereas lime imposes this

526
00:44:02,640 --> 00:44:11,640
constraint by its methodology that importance is restricted to the connected pixels.

527
00:44:11,640 --> 00:44:18,600
Humans tend to, and I say this hesitantly because this is not a research conclusion, this

528
00:44:18,600 --> 00:44:25,640
is hypothesis, but I sense humans tend to like connected pixels as more what they perceive

529
00:44:25,640 --> 00:44:27,520
to be interpretable.

530
00:44:27,520 --> 00:44:33,760
And so I would be excited to at least benchmark lime on another example of a method that

531
00:44:33,760 --> 00:44:39,360
does assist feature importance in this way using remove and retrain, because it may have

532
00:44:39,360 --> 00:44:42,240
very different results.

533
00:44:42,240 --> 00:44:48,600
Or at least tell us something about how connected importance versus this very few importance

534
00:44:48,600 --> 00:44:55,000
actually is related to the reliability of the method.

535
00:44:55,000 --> 00:44:56,000
Interesting.

536
00:44:56,000 --> 00:45:04,480
It strikes me that from that perspective, the idea that humans prefer these connected

537
00:45:04,480 --> 00:45:13,280
areas versus diffuse areas, almost says that humans want explainability that makes sense

538
00:45:13,280 --> 00:45:18,800
to them more than interpretability that might be more functionally accurate.

539
00:45:18,800 --> 00:45:29,160
Yeah, I hesitate to say, we're just talking here, we're just throwing stuff out there.

540
00:45:29,160 --> 00:45:37,160
But I will say the most, how researchers articulate this in papers is quite funny.

541
00:45:37,160 --> 00:45:41,400
So this comes up, no one quite gets at this explicitly, because again, it's not something

542
00:45:41,400 --> 00:45:46,760
that's exactly testable, and even in my current research, I don't benchmark any of these

543
00:45:46,760 --> 00:45:53,640
connected methods, so I really hesitate to venture, but what I will talk about this frustration

544
00:45:53,640 --> 00:45:58,280
with diffuse methods, because that's kind of a coherent pattern in these papers.

545
00:45:58,280 --> 00:46:07,800
Researches will say that the method is perceived to be visually noisy, and that's interesting,

546
00:46:07,800 --> 00:46:08,800
right?

547
00:46:08,800 --> 00:46:14,160
Because that actually says nothing about whether the method is accurate or not for gradients.

548
00:46:14,160 --> 00:46:19,360
It just speaks to our reluctance to, either it's hard to discern what the explanation

549
00:46:19,360 --> 00:46:24,640
is saying, what's actually important, this image of going to gradients, and that might

550
00:46:24,640 --> 00:46:35,720
be because as humans, we may not know what to do with the complexity, again, here I'm

551
00:46:35,720 --> 00:46:42,160
using another word that is very subjective, like visual noise, the complexity of the explanation.

552
00:46:42,160 --> 00:46:49,680
Whereas if something is connected and placed in a specific area, at least I sense it may

553
00:46:49,680 --> 00:46:53,800
be more reassuring, but again, I think we're both throwing things out there, and that's

554
00:46:53,800 --> 00:46:59,920
what makes this domain so challenging, is that a lot of it is subjective, a lot of it

555
00:46:59,920 --> 00:47:08,280
is inherently like, my preference may differ in fact from your preference, and that's

556
00:47:08,280 --> 00:47:09,800
why there's no clear finish line.

557
00:47:09,800 --> 00:47:16,760
And did you find that the specific approach to ensembling, you mentioned squaring the estimate,

558
00:47:16,760 --> 00:47:22,680
I'm not sure, well, you can elaborate on that, but the question is, did it apply equally

559
00:47:22,680 --> 00:47:28,280
to all of the methods you looked at, gradient, heat map, integrated gradients, smooth gradients,

560
00:47:28,280 --> 00:47:37,240
grid gradient, etc., or are there specific formulas or formulations that apply to individual

561
00:47:37,240 --> 00:47:38,240
methods?

562
00:47:38,240 --> 00:47:42,800
Well, this is the cool thing, is that it benefited all the estimators, all the methods

563
00:47:42,800 --> 00:47:47,680
that we considered, and I apologize for, I keep on interchangeably using those words

564
00:47:47,680 --> 00:47:57,640
so yeah, it really dramatically, I would say, improve the accuracy of the three estimators

565
00:47:57,640 --> 00:48:06,480
that we considered, and what, what it, when I say it, like it was firstly adding noise

566
00:48:06,480 --> 00:48:13,720
to the images, so for a single image, you would create a set of 15 noisy images, and then

567
00:48:13,720 --> 00:48:20,040
you would arrive at 15 different predictions, and given those 15 different predictions,

568
00:48:20,040 --> 00:48:25,280
you would take the estimate for the methods you're considering, and you would square it,

569
00:48:25,280 --> 00:48:29,240
and then you would average those, so you left with one estimate, and again, from these

570
00:48:29,240 --> 00:48:35,560
15 different images, and again, an estimate, in this case, is,

571
00:48:35,560 --> 00:48:46,120
lie, example, but the actual estimate is a, like a per pixel probability, for example,

572
00:48:46,120 --> 00:48:49,880
or something on a per pixel basis, right?

573
00:48:49,880 --> 00:48:55,400
It's on a per pixel basis, it's often, in the case of gradients, it's not capped at a

574
00:48:55,400 --> 00:49:00,760
certain, it's just the magnitude can be as important, so it's not probability, but other

575
00:49:00,760 --> 00:49:06,920
methods will cap it, so it's cumulative, so the sum of all the estimates and importance

576
00:49:06,920 --> 00:49:13,560
should sum to one, that's a property called completeness, but yes, so you're correct,

577
00:49:13,560 --> 00:49:20,080
it's on a per pixel level, so you're averaging across, however many noisy estimates you

578
00:49:20,080 --> 00:49:24,760
have to arrive at a single estimate for a given picture.

579
00:49:24,760 --> 00:49:30,440
So that's your interpretability research, if you have a few more minutes, I'm curious

580
00:49:30,440 --> 00:49:38,280
about some of the model compression work that you're doing, and that ties into the event

581
00:49:38,280 --> 00:49:42,560
that's kind of brought us together, which is the deep learning in Dava, which is coming

582
00:49:42,560 --> 00:49:50,760
up in South Africa, and you, you know, we're well beyond our typical background segment

583
00:49:50,760 --> 00:49:53,760
here, but you grew up in...

584
00:49:53,760 --> 00:50:01,240
I'm at a similar reaction when you said that, I'm like, oh wow, this is introducing

585
00:50:01,240 --> 00:50:13,920
some context later in the game, but it's, yeah, I grew up in Africa, and mostly in Southern

586
00:50:13,920 --> 00:50:21,840
Africa, so I grew up in Mozambique, South Africa, in the Soutu, Swaziland, Kenya, my family

587
00:50:21,840 --> 00:50:30,160
just moved to West Africa, they just moved to Monrovia, Liberia, so I, then Dava, so

588
00:50:30,160 --> 00:50:38,520
then Dava's coming up, you mentioned, but also Google is opening this AI lab in Acra,

589
00:50:38,520 --> 00:50:44,560
and both are very exciting, because so much of it, I think, is quite exciting personally

590
00:50:44,560 --> 00:50:52,240
for me, as a way to, well, in Dava, I would say, is very much directly doing this, and

591
00:50:52,240 --> 00:50:57,960
Dava, the motivation is, let's build technical capacity, and that's the most important

592
00:50:57,960 --> 00:51:03,320
determinant, I kind of mentioned this when I talked about what made this year possible,

593
00:51:03,320 --> 00:51:09,080
it's contact with very experienced researchers, and just the ability to collaborate, and I

594
00:51:09,080 --> 00:51:13,840
think that Dava is really in the same vein, like you're bringing these researchers from

595
00:51:13,840 --> 00:51:19,240
all over the world, and all these students, and also practitioners from all over Africa

596
00:51:19,240 --> 00:51:23,920
to the same place, and wow, is that exciting?

597
00:51:23,920 --> 00:51:29,120
I don't need to, I'll be too exuberant about it, but I think, I recently went to Data

598
00:51:29,120 --> 00:51:42,760
Science Africa in Kenya, and the energy level is insane, mainly because we, right now,

599
00:51:42,760 --> 00:51:49,480
a technical talent, I would say, is very correlated with geographic location, so places like San

600
00:51:49,480 --> 00:51:57,240
Francisco, New York, Paris, a handful of other geographies have a lot of, just, I'd

601
00:51:57,240 --> 00:52:03,080
say, experience, and as soon as you leave one of these cities, you realize it's a clip

602
00:52:03,080 --> 00:52:09,520
hanger, like the ability to have a community to learn and grow in the places where I grew

603
00:52:09,520 --> 00:52:16,520
up is extremely limited, and so for students who attend these gatherings, it's just really,

604
00:52:16,520 --> 00:52:21,960
they're so excited because this is a way for them to connect, many times what they've

605
00:52:21,960 --> 00:52:28,920
been studying by themselves online, with people who are actually doing this and practicing

606
00:52:28,920 --> 00:52:34,400
in the field or doing research, so that's why it's so exciting, I forget the second part

607
00:52:34,400 --> 00:52:40,880
of your question, but maybe, yeah, maybe on points there.

608
00:52:40,880 --> 00:52:49,680
Well, that provides some interesting context for your move to the Accra Office and your

609
00:52:49,680 --> 00:52:54,600
work in model compression, I think that was the tie-in that I wanted you to elaborate

610
00:52:54,600 --> 00:52:55,600
on.

611
00:52:55,600 --> 00:53:06,920
Well, the Accra Office, so most of us, these days, is leading the Google AI Accra Office,

612
00:53:06,920 --> 00:53:07,920
and he's amazing.

613
00:53:07,920 --> 00:53:15,920
He's one of my, both mentors and collaborators here at Brain, and the Google Accra Office

614
00:53:15,920 --> 00:53:22,160
is like any other Brain Office, so the goal is to go and have, attract the best researchers

615
00:53:22,160 --> 00:53:28,480
and do research, so very much in the same vein of a lot of the things I've described today.

616
00:53:28,480 --> 00:53:33,880
I think what most of us, I sense would agree with, is that this is other ideas, that by

617
00:53:33,880 --> 00:53:43,680
placing in researchers in different geographies and in different environments, you have often

618
00:53:43,680 --> 00:53:50,800
very novel approaches to ideas, and that is both because the resource constraints that

619
00:53:50,800 --> 00:53:58,240
you find may be different, as well as because simply a lot of researchers who you work with

620
00:53:58,240 --> 00:54:04,240
and your day-to-day conversations, and you may find novel directions because of that.

621
00:54:04,240 --> 00:54:10,520
The other hope for this Accra Office is that having researchers there may provide important

622
00:54:10,520 --> 00:54:17,120
externalities for the ecosystem as a whole, and Mr. Fah has also really championed this

623
00:54:17,120 --> 00:54:22,840
master's program, which will, is also starting this year, and that's supported by Facebook

624
00:54:22,840 --> 00:54:29,520
and Google, I believe, but how that relates to monocompression, it doesn't necessarily

625
00:54:29,520 --> 00:54:35,800
have to, but it happens to, for my current research, which is also with Mr. Fah and Eric

626
00:54:35,800 --> 00:54:44,920
and another Collaborate Trevor at Brain, and there, what I'm interested in, is this idea

627
00:54:44,920 --> 00:54:51,440
that really, the starting point for this research is, why do we need such large models?

628
00:54:51,440 --> 00:54:59,000
Do you know on networks that are notorious for the amount of parameters that need it,

629
00:54:59,000 --> 00:55:05,960
and also the tendency for the number of parameters to grow year-over-year in the body of research?

630
00:55:05,960 --> 00:55:10,680
But I think it's interesting if we impose different constraints, like, for example, if we

631
00:55:10,680 --> 00:55:16,560
actually think about deployment in very resource-constrained environments, such as mobile phones, which

632
00:55:16,560 --> 00:55:24,000
a lot of parts of Africa have jumped directly from neither having a laptop or mobile phone

633
00:55:24,000 --> 00:55:26,240
to just jumping to a mobile phone.

634
00:55:26,240 --> 00:55:34,320
So if we think about that, that imposes very different resource boundaries than what

635
00:55:34,320 --> 00:55:41,480
we're currently used to, and often, like, thinking about that drives interesting ways of attacking

636
00:55:41,480 --> 00:55:42,480
the same problem.

637
00:55:42,480 --> 00:55:48,080
There's a great example of this, and how engineers at Google, and I believe what's that Facebook

638
00:55:48,080 --> 00:55:55,320
have tackled, how do they make engineering products, which are better suited to low bandwidth

639
00:55:55,320 --> 00:55:57,120
or limited connectivity environments?

640
00:55:57,120 --> 00:56:02,400
And the solution was one of the solutions that we experienced and is highly visible as

641
00:56:02,400 --> 00:56:07,640
this idea that there's an entirely separate internet connection that you can connect to

642
00:56:07,640 --> 00:56:13,800
that will give you the experience of being in an impaired bandwidth environment.

643
00:56:13,800 --> 00:56:19,960
And this was actually like a starling catalyst for a lot of engineering innovation around

644
00:56:19,960 --> 00:56:22,080
these problems.

645
00:56:22,080 --> 00:56:30,600
Again, like, this is my interest in model compression, I sense could have been equally

646
00:56:30,600 --> 00:56:38,480
pursued in Manavue at Google Brain headquarters or in the new office in Accra.

647
00:56:38,480 --> 00:56:46,000
But it's exciting to sense, like, energize, like, what I'm thinking about by also connecting

648
00:56:46,000 --> 00:56:52,800
with people who are experiencing the pains of trying to deploy models.

649
00:56:52,800 --> 00:56:57,240
And that's one of the most frequent questions that you get from students when you go

650
00:56:57,240 --> 00:57:02,480
to teach in a place like Nairobi, is that a lot of people have an idea, have tried to

651
00:57:02,480 --> 00:57:08,320
implement it, and now trying to deploy it using something like T.F. Light and now experiencing

652
00:57:08,320 --> 00:57:10,520
severe pain points.

653
00:57:10,520 --> 00:57:13,840
That's exciting for me at least.

654
00:57:13,840 --> 00:57:19,440
My research direction is specifically on this question of how do we effectively remove

655
00:57:19,440 --> 00:57:23,560
weights from a model while preserving accuracy?

656
00:57:23,560 --> 00:57:28,720
There's a few different threads to how people tackle this problem.

657
00:57:28,720 --> 00:57:33,880
You may have to rein me in again if we get started, so I'll pause.

658
00:57:33,880 --> 00:57:37,400
I don't know if there were any immediate questions.

659
00:57:37,400 --> 00:57:44,040
It sounds like from our earlier chat that you can't go into a ton of detail about your research

660
00:57:44,040 --> 00:57:48,560
because it hasn't been published yet, but if you could give us an overview of kind of

661
00:57:48,560 --> 00:57:55,880
the landscape that you're playing in as we wrap up, that would be great.

662
00:57:55,880 --> 00:58:03,040
I'll mention probably four key directions that this problem has been thought about, and

663
00:58:03,040 --> 00:58:07,960
the other defining characteristic of this field of research is that I sense it's been

664
00:58:07,960 --> 00:58:08,960
underserved.

665
00:58:08,960 --> 00:58:17,760
There's being very little, it's a rather new field of research.

666
00:58:17,760 --> 00:58:23,120
But some of those solutions are actually very old.

667
00:58:23,120 --> 00:58:31,960
I know that was an odd caveat, but I'll give you more context for what I mean.

668
00:58:31,960 --> 00:58:36,760
The key approaches to this problem are that you have things like pruning, and pruning

669
00:58:36,760 --> 00:58:42,000
says, let's reduce the number of weights in a network.

670
00:58:42,000 --> 00:58:47,200
You either do that over the course of training by setting certain weights to zero or regularizing

671
00:58:47,200 --> 00:58:52,080
the certain weights become very close to zero, or at the end of training by saying, this

672
00:58:52,080 --> 00:58:58,760
is our model, and now we're going to try and arrive at a much smaller model.

673
00:58:58,760 --> 00:59:06,960
The metric that's been optimized for is a normally level of sparsity given a certain degree

674
00:59:06,960 --> 00:59:08,480
of accuracy.

675
00:59:08,480 --> 00:59:13,600
The second approach, which has been enormously successful, and in some ways has taught

676
00:59:13,600 --> 00:59:18,720
us a lot about deep neural networks in general, is quantization.

677
00:59:18,720 --> 00:59:24,080
Quantization refers to this idea of you have a certain level of precision and the weight

678
00:59:24,080 --> 00:59:32,400
of themselves, and you can essentially take a floating point weight, and you can reduce

679
00:59:32,400 --> 00:59:38,400
the number of bits, and then you can still have a remarkable level of accuracy.

680
00:59:38,400 --> 00:59:46,960
So this takes, like most often, a trained model, and then changes the representation of

681
00:59:46,960 --> 00:59:48,760
the weights.

682
00:59:48,760 --> 00:59:56,040
And this has huge implications for memory, so often you're able to really improve memory

683
00:59:56,040 --> 01:00:02,280
of the models, the memory needed to store the models by simply changing how the weights

684
01:00:02,280 --> 01:00:03,280
represented.

685
01:00:03,280 --> 01:00:13,000
This is third direction, which is model distillation, so this is also interesting enough in present

686
01:00:13,000 --> 01:00:19,400
as an interpretability direction, but you have this teacher model, which is your massive

687
01:00:19,400 --> 01:00:25,800
deep neural network, and then you're trying to train a student network to have the same

688
01:00:25,800 --> 01:00:34,240
accuracy as a teacher network with a fewer amount of parameters, and all these are quite

689
01:00:34,240 --> 01:00:35,240
exciting.

690
01:00:35,240 --> 01:00:41,480
The fourth, which I think is probably the most underserved, is this idea of trying to

691
01:00:41,480 --> 01:00:48,240
do things that are optimized for the actual hardware.

692
01:00:48,240 --> 01:00:53,760
And this is tricky, because this is a nice loop back to how we began this conversation,

693
01:00:53,760 --> 01:01:01,520
but it's hard to pursue a research that is optimized to hardware, because hardware tends

694
01:01:01,520 --> 01:01:03,680
to be non-standard.

695
01:01:03,680 --> 01:01:12,360
And so think about a TPU, which is made by Google, is one of the first hardware that's directly

696
01:01:12,360 --> 01:01:16,040
has in mind deep learning, and then a GPU made by Nvidia.

697
01:01:16,040 --> 01:01:18,640
But even when you think about cell phones, it's non-standard.

698
01:01:18,640 --> 01:01:23,560
And so this research, in some ways, the constraint is that you want to make it generalizable enough

699
01:01:23,560 --> 01:01:29,760
but you still want to make significant inroads into how it's being deployed.

700
01:01:29,760 --> 01:01:38,240
But I started this kind of framework by saying, this odd disconnect, I said, oh, in some

701
01:01:38,240 --> 01:01:42,760
ways this is a very new field of research, and other ways it's not, I'm thinking specifically

702
01:01:42,760 --> 01:01:43,760
of pruning.

703
01:01:43,760 --> 01:01:47,520
So pruning has actually been around since the 1990s.

704
01:01:47,520 --> 01:01:52,680
So pruning was the first field I mentioned where you're trying to remove weights and arrive

705
01:01:52,680 --> 01:01:54,560
in a smaller model.

706
01:01:54,560 --> 01:01:57,640
And there was the first paper came out in the 1990s.

707
01:01:57,640 --> 01:02:04,640
It was initially called double back prop, but the current proposal in the form of optimal

708
01:02:04,640 --> 01:02:08,000
brain damage, which I think is a great name for a paper.

709
01:02:08,000 --> 01:02:14,240
It certainly has a ring to it, but he proposed one of the first methods.

710
01:02:14,240 --> 01:02:18,400
And so some of these approaches have been around for a while.

711
01:02:18,400 --> 01:02:23,440
It's just that there's a new level of attention, both because there's a sense that now the

712
01:02:23,440 --> 01:02:26,520
resource constraint is firm.

713
01:02:26,520 --> 01:02:31,720
There's a lot of discussion around Moore's Law and how we can't quite get the hardware

714
01:02:31,720 --> 01:02:34,760
to just catch up with whatever researchers want to do.

715
01:02:34,760 --> 01:02:41,720
And then, instead, we must think of interesting ways for researchers to meet the hardware.

716
01:02:41,720 --> 01:02:49,560
And that's not an easy feat, because hardware and research has tended to be very siloed

717
01:02:49,560 --> 01:02:51,400
in both directions.

718
01:02:51,400 --> 01:02:56,040
So that's what makes this particularly, like, perhaps like a very interesting time for

719
01:02:56,040 --> 01:02:59,440
a lot of the enthusiasm around the subfield.

720
01:02:59,440 --> 01:03:05,640
I think we could launch into another hour long conversation about this topic.

721
01:03:05,640 --> 01:03:11,680
I didn't want you to have to ring me in, so.

722
01:03:11,680 --> 01:03:16,960
But we will have to find a time and place to reconvene on that one.

723
01:03:16,960 --> 01:03:21,920
But Sarah, thank you so much for taking the time to chat with us.

724
01:03:21,920 --> 01:03:28,520
It's been a pleasure to have you on the show and to learn about what you're doing across

725
01:03:28,520 --> 01:03:30,760
all the various things that you're working on.

726
01:03:30,760 --> 01:03:31,920
Thank you so much.

727
01:03:31,920 --> 01:03:35,200
It was really fun chatting with you, Sam.

728
01:03:35,200 --> 01:03:42,000
And oh, I think Sam had mentioned offline, which I'll repeat here for social peer pressure,

729
01:03:42,000 --> 01:03:46,600
but he mentioned that he would come visit the Ghana Acro office.

730
01:03:46,600 --> 01:03:49,160
So I plan to hold you to that.

731
01:03:49,160 --> 01:03:53,880
Let's do it, for sure.

732
01:03:53,880 --> 01:04:02,760
And you mentioned earlier, Mustafa Sisay, who is heading up that office for folks who didn't

733
01:04:02,760 --> 01:04:03,760
catch it.

734
01:04:03,760 --> 01:04:10,760
I interviewed him when he was at Facebook AI Research, and we will drop a link to that

735
01:04:10,760 --> 01:04:12,560
show in the show notes.

736
01:04:12,560 --> 01:04:14,920
But once again, Sarah, thanks so much.

737
01:04:14,920 --> 01:04:19,680
Yeah, thank you.

738
01:04:19,680 --> 01:04:20,680
All right, everyone.

739
01:04:20,680 --> 01:04:22,360
That's our show for today.

740
01:04:22,360 --> 01:04:28,240
For more information on Sarah or any of the topics covered in this show, visit twimlai.com

741
01:04:28,240 --> 01:04:31,360
slash talk slash 189.

742
01:04:31,360 --> 01:04:36,920
To follow the entire deep learning and double podcast series, visit twimlai.com slash

743
01:04:36,920 --> 01:04:39,320
endaba 2018.

744
01:04:39,320 --> 01:04:42,400
Thanks again to Google for their sponsorship of this series.

745
01:04:42,400 --> 01:04:49,520
Be sure to check out the 2019 AI residency program at g.co slash AI residency.

746
01:04:49,520 --> 01:05:01,280
As always, thanks so much for listening, and catch you next time.

