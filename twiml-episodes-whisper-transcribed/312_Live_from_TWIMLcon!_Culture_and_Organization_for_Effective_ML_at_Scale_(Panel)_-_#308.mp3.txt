Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
What's up everyone?
This is Amari, producer of the Twimal AI Podcast.
What about now?
You're probably wondering, where's Sam?
Well on the heels of such an amazing event, we decided it was time for him to take a break.
So he's currently on vacation.
But as they say, the show must go on.
Today we continue our Twimal Con coverage with the first of our panel discussions, culture
and organization for effective machine learning at scale.
In this discussion, moderator Maribel Lopez, founder and principal analyst of Lopez
research, is joined by panelists Jennifer Prinky, founder and CEO of Electio, Eric Colson,
emeritus chief algorithms officer of stitchfix, and partist Norzad, data science manager
at Twitter.
Before we move on, I'd like to extend a huge thank you to everyone that joined us at
Twimal Con earlier this month.
We had an amazing time and we can't wait to see you all again next year at Twimal Con
2020.
And now on to the show.
Without further ado, I would like to welcome up the moderator of our first panel, Maribel
Lopez, founder of Lopez research.
Maribel?
Hi, I'm Maribel Lopez, so I'm the founder of Lopez research and also the founder of
data for betterment with this nonprofit organization that helps companies figure out how to move
forward in the land of AI and giving them information about career change and other things
in that area.
And I could think of no better thing to do than to be here today talking about culture
and organization and how to do things like effectively have machine learning at scale.
So without further ado, I'm going to jump right in it.
One of the things that we're seeing with organizations is that there's a tremendous amount
of talk about artificial intelligence in organizations.
Not necessarily a lot of companies or executives or a line of business people actually really
understanding what that means, how to do it.
Sometimes it's like I'm just going to go out and buy something with machine learning
in it and see how that works out.
So one of the challenges I think that all of you in the organization are facing is how
do we get the different groups in an organization to effectively work together?
So if we look at this, if you have a data science team, data engineering, how do you actually
work with line of business managers, how do you work with the C-suite so that you can
actually get to delivering business value with AI technologies and tools?
So I thought that might be a good first place to start as we're talking about culture
and organization because technically you probably can't decide what kind of organization
you want to switch it to, centralize or decentralize, but you will have to try to make
this effective and efficient and delivering business value.
So maybe we can start there, maybe we will start with Jennifer.
You want to give us a few pointer tips or how you've seen this work in terms of talking
to the different groups within an organization.
Yeah, no, I saw as you stated before completely agree that this is certainly probably a
bigger challenge than the technology itself, right?
So the first thing I would say is like first of all, I have very good news is that there
is a very easy way to solve the way you communicate with the C-suite or product management and
that is by educating the people you're going to communicate with.
And the easiest way to do this is to go all the way to an MBA program and teach them and
basically prepare the next generation of leaders and product managers to be ready to interact
with people who are going to handle machine learning research and machine learning product,
right?
So we're basically in survival mode right now, it means that we have to figure out a way
to make it until this next generation of leaders becomes, comes in place and these are actually
the people who are either reporting to or working with, right?
I mean, so in the meantime, obviously, there are different segments and so to me personally
like one of the biggest challenges I always had was like, define what product management
means for machine learning, but that's a long, long story probably.
Okay, we're going to circle back to defining what project management means in machine learning
in a second.
Eric, maybe have some thoughts on this?
Well, sure.
I think one thing you can do to align yourself with the C-suite is you can become the C-suite.
So at StitchFix, we do have a special arrangement, the chief algorithms officer, an officer
of the company, up their peers with the CFO and CTO and so forth.
So that does help change things easier to get alignment as peers rather than somewhere
buried down the org structure.
With officer representation, what happens is you become accountable, you're not a supportive
team to other folks, instead you have your own goals, revenue goals or other metrics,
right?
That you are accountable for not supporting another team.
And so that changes things dramatically on how the company embraces data science and
the other thing that happens with officer representation is you get influence.
You can influence the way companies work and you can do away with the notion of big ideas
and instead companies have hypotheses that are put to the test and it, and once the company
learns after a short period of time that they're wrong a lot, then they set the change of
the way they do even engineering where they're not going to build a certain feature, they're
going to build a general platform to try 100 features because Lord knows the first one's
going to be wrong.
And you need to iterate and do things a little more generally.
So that's one way to go about it is if you get the actual officer representation and
it's, I'll be honest, it's a much easier way to do it than to try to manage it from down
below.
So if you're fortunate enough to be in that situation, it actually is something I highly recommend.
Yeah, I know part of you actually are in a company that has a lot of data behind it and
a lot of data science behind it, but there's also a lot with the marketing organization
and sales.
So how have you looked at approaching this problem in your organization?
Absolutely.
The way I approached working with sales and marketing is kind of the same way as I think
about data science working with these various product teams.
Nowadays, you have continuous integration and you have online products, products in the
cloud.
So products are being launched almost every day and product is changing every day.
And because of that, the way data science is working with the engineering teams needs
to be fully integrated, looking at every feature, launch, every experiment, every kind of change
in order to make sure that the data reflects that and we're accounting for it for in building
our models, updating our models and so on.
And so is the same with sales and marketing because in the past, there were various versions
of, let's say, a product, Windows 95 and the specifications were already made clear.
There was enough time for sales marketing to understand what the market would be like,
but also be able to fully understand the product to go and sell.
Now that's changing.
The product is changing so fast in order for sales to know what to sell.
They need to be as integrated within the kind of product as much as possible to know exactly.
Okay, the real estate on the app is going to be one-tenth of what it used to be so we
need to communicate that to our partners.
Or the targeting algorithm will change and so we're actually going to bring in 10 times
more traffic to your product and so sales could kind of be in the loop for that to be able
to communicate that to the partners and that's how we can like marketing and sales up for
success.
That makes sense.
Having that data and insight to know what's going to happen next so that they can plan
and make changes to their models important.
Now we had a session earlier where Libas was talking about some other things that they
did so that's a longstanding established company.
Jennifer, I know that you actually have done some work with Walmart in the past.
Have you seen any differences in terms of how to think about that?
They're a company that's had a lot of data that's been working with data for a long time.
Did you see a change happen over the course of the time that you worked there in terms
of how they worked with it?
Also when I joined Walmart, one of the big change I had to provide to the company is like,
I think you were talking about integration earlier.
Something I would also like to state here is that I think one of the biggest challenge
like a machine learning team is going to have is lack of integration with engineering.
If you do have that integration, it puts you in a position where you are fully responsible
for what you're building.
It's almost like you do some machine learning research.
You can create a product with it and prove to the C suite and the rest of the organization
that this is actually going to provide money for the company.
Before we talked about Walmart, I would like to state something else that's actually
pretty interesting.
I was lucky to have both like a machine learning at Atlassian and I managed a machine learning
at a smaller startup figure eight.
In both cases, we have the different structures.
For Atlassian, for instance, I was really lucky to have the opportunity to build the
entire team from scratch and basically I brought in like a basically 50% of the team was
engineering.
I mean, so it meant that the algorithms we were building, we were actually able to create
and fully prototype on our own and it was much easier to push this production.
We had other challenges related to the internal organization within Atlassian where you had
to push that into specific products.
But at least there was a prototype that could exist.
By a position when I was a figure eight, my team was 100% machine learning scientist
and this is something I had no control of because the team was already established when
I joined.
Unfortunately, basically we were constantly frustrated that things would not go to production
because there was no engineering that could actually bring these things to production.
That brings me back to Walmart.
I think one of the challenges like companies like a few years ago did not necessarily have
this understanding that a machine learning team is not an engineering team.
They don't think the same way, they don't function the same way and a machine learning
scientist or data scientist is not trained to write production level code and push this
to production.
And so basically this means you have to rethink the organization.
At Walmart, that was a huge challenge because when the machine learning teams started building
new algorithms, for instance, and in particular when we know that because we are machine learning
people, we know that models need to be retrieved on a regular basis.
But the C-suite, except if the C-suite is an engineering scientist, does not know that
you would be surprised the number of people, you created this algorithm, why do you have
to retrain it?
And this is not something that's, and so this is where machine learning life cycle management
is something you have to push onto the organization if you want to be successful.
I actually think this point of machine learning life cycle management is a huge issue and
largely under-addressed in our communities.
Eric, do you have any concepts or thoughts around either a full stack or life cycle management?
Well, yeah, I have strong opinions on the full stack thing, so the team at StitchFix we
built to avoid what we call handoffs.
We don't like when a research scientist wants to do some modeling, so they partner with
somebody called an ETL developer to get them the data.
And then once they find the model, they have a train, they hand it off to a machine learning
engineer to implement it.
And then they also might employ an inference engineer to measure it, right?
So all that is fine and dandy, but it requires handoffs between that.
And that slows you down.
Handoffs are appropriate when you know what it is you're building.
Say if you're manufacturing something, you've got the requirements are crystal clear
down the millimeter of precision, then you can specialize and do those handoffs.
But when it needs to be iterated on, you're not clear what it is you're building or learning,
then I really prefer to have it in as few hands as possible, ideally, even sometimes
one, one full stack data scientist that can do all those parts through the ETL, through
the modeling, implement it him or herself, and set up the AB test appropriately to measure
it.
That's ideal because that person can move as quickly as possible with no handoffs.
So and again, the benefit of the golden data science is to learn something to implement
some new algorithmic capability, preferably that makes a lot of money, but to learn it,
to figure out how to do it.
It's not to do the fine tuning, not to do something more efficient or just a skim, a few pennies
off of something.
It's usually to do something big and profound and something that was new to the company,
a new capability.
And that type of thing, usually with data products, you can't design it up front.
You need to learn as you go.
So you have to design the team to enable them to learn as they go, so they don't get caught
up in so many handoffs.
So that's the thought on the full stack data scientist, as we call it, at Citrix.
Yeah, so what I like about this is it highlights that there are different ways to do it, but
you have to be very specific about understanding what are the different levels that you need,
and who is going to take care of those different levels.
And I think if you're lucky enough and you can find full stack people, that works great.
If you don't have full stack people, you need to know how that you're going to break this
up so that the right people are doing what they're skilled at, but that they're coordinated
enough to make that happen, part of any thought on your side.
I guess the only thing I would add would be around the fact that the data is always changing
because, as we spoke before, the product is always changing.
It's also the fact that sometimes company strategy might be changing, and so you'd want
to update your objective functions accordingly, and another thing is the data could change
because the, you know, your product is growing so fast in additional markets, and suddenly
you have users using the product in a completely different way if you're lucky.
And so all of those various factors kind of change, you know, your product completely.
And so you'd want to be, have people providing always on support to be able to make those
changes in updates as soon as possible.
I'm going to circle back to lifecycle management and maybe we can go down this way and just
talk a little bit about what you think is or isn't happening there and what you think
needs to happen there.
So Eric, why don't we kick off with you?
Sure.
For lifecycle management, so the process I just described earlier is about an initial
implementation of some new capability, and that's what I think goes fastest without
the handoffs, without the specialization you have more of a general full stack data scientist.
That said, once it's in production, supporting it is hard, and so you'll probably need some
help.
And in fact, that's something that's often unintuitive to folks either from the business
side or engineering side is they think that resources will roll off a project once it's
implemented.
And that doesn't happen.
You increase the number of resources you put.
So if something is successful, a new recommendation engine or something, and it's live now, it's
not like you deploy those people to go somewhere else.
No, they double down on it.
They have their best ideas come after implementation, and they can get better, better and better algorithms
in there by testing them against each other and making successive changes to them.
So you end up usually increasing, not decreasing after implementation.
And then the support stuff is problematic.
It becomes more of a burden than actually building the thing.
As you mentioned, data changes a lot, and it could wreak havoc on your stuff.
A new engineering feature goes out and isn't compatible with your algorithm.
Things need to be adjusted and changed pretty constantly.
And that's when you, so you, again, it might take one single person to implement the
capability, but you need to start staffing up that's team to support it.
As that person eventually wants to take a vacation now and then, right, and doesn't want
to be left supporting from wherever they go on vacation.
And so you need to add resources later, and that's something that needs to be kind of explained
and anticipated.
But I wouldn't, we do kind of a tough thing, we wait till it's successful and then start
adding the resources.
So it's always a scramble, because a lot of things do fail, and you don't want to set up
people, hey, you're going to support that thing once it's in production, but it never
gets to production.
So you end up in this game of catch-up by design, we consider it as the lesser of the
two evils.
But you do have to plan as best you can that, okay, if these things are successful, you
got to get ready to staff up to help support.
All right, no, I mean, so going back to the example with Walmart, right?
I mean, so actually when I was at Walmart, and we had more and more models coming out
to production, so I actually started serving my team for, basically, ask them, like, for
every, like, a new model that's coming in how much more work do you have?
So after I served my entire team, turns out that when we were, like, basically, like in periods
where the models had to be retrained frequently, for example, close to Black Friday, or this
type of periods, or close to Christmas, people were spending, like, almost 75% of their
time retraining models, relaunching things, because there was no pipeline to do this
automatically.
Actually, like, one of my pushbacks on management was, like, we need, like, a way to systematize
the way that you actually, like, deploy and retrain the models specifically in an environment
where you need to retrain models every day, like, it's very typical for e-commerce or
similar spaces, I'm sure it's true for social media as well, right?
And yeah, and so eventually, what I saw as being a problem, so I almost had the reverse
problem at some point where it's almost like, you have to think about, like, what really
life cycle management means, like, when you come back to this later, right?
But finally, I convinced the company, like, we need a systematic way to retrain this,
like, using technology such as airflow to, you know, like, keep things going even after
you move forward.
Then the management team almost had the sense that the model is taking care of itself,
right?
I mean, so it's going to be retrained fully automatically, and so this is where I think
people are missing the point, like, in life cycle management, you have life cycle management.
So cycles, and this is actually kind of important because, to me, cycle means feedback, right?
And so there is a huge missed opportunity when you try to fully automate this, to take
the feedback of the model you've trained and basically understand the failures, the
reason why, you know, like, you didn't reach 100% accuracy and feed that information
back into the system, right?
So I think, like, it goes back to, like, maintaining in the long term, right?
I mean, the model and, like, fixing the failures you had at first and so forth and so on.
And so I think it's also important to see life cycle management as being an opportunity
to get your model in a better place, not just keeping it up to par or up to speed with
the current data.
Definitely.
Yeah.
100% agree with that just because so many things are changing around the model, whether
it be, like, sometimes policy, there are policy changes and so the way you're labeling
your data will be, will change and you will need to then think about your model differently
in that kind of space.
Again, the way that you will be, your objective functions might change and the user's
might change as we talked about in a different geography and things like that.
And I also wanted to, like, reiterate something that Eric was talking about around kind of
headcount and the fact that if something is successful, you will need more or not less
people to kind of work on that project.
And so if something is a high priority, a lot of times you would hear, could we just borrow
or two data scientists to kind of think about or opportunity size this project?
And it's usually, well, if this is going to be a priority, we need to, like, think about
long-term data science support for this data product.
Yeah.
I think the one thing that you've heard a lot that you should really take away from this
is that one, because of some of the marketing messaging that's gone out around AI, people
do think it's very automated, and that once you get something that it does just take care
of itself.
And that is something that you really have to get ahead of, because this concept of
resources for whether it's new software or tooling so that you can create a more automated
retraining pipeline flow or whether or not it's actual more headcount, this is going
to be a thing in the culture and organization discussion that you definitely have to get
ahead of.
And we're talking to one bank that did not realize that they were, you know, they spent
a long time going through their model, and it was in the chat bot section.
And they had to retrain it every day for a very long time, because they didn't think
that, well, people don't speak in terms such as activate my card.
You know, something a bank would say, not something a human would typically say, you might
say, turn on my card or some other variant of that.
And those types of, you know, feedback loops and interesting learning meant that it takes
a very long time, a lot longer than people think, which means also that programmatically,
you probably have a pipeline, and your pipeline is going to take a lot longer to get through
than your management originally thought.
We have about three minutes left, and I was wondering if there was one question in the
audience that anybody would like to ask if so, when to raise a hand stand up or anything.
Yeah, great talk about educating the whole company about how building ML machine and
AI product is.
I hear a lot, some solution is to hire different people into the team, so the team actually
have different kind of knowledge they can help each other out.
I think there's another dimension of what's your take is there's a whole company.
You can't solve all the problem.
Like you need data.
Are you going to build the entire data lake yourself?
Are you going to build the entire computational layer for yourself as well?
How would you educate almost like it takes a whole village to build it?
But you're right.
There are certain parts of certain capabilities that the data science team is totally autonomous
for, but most things you're partnering with somebody, they're something with marketing
or merchandising other teams, and it does take a village.
You need a lot of it, and a lot of times integration with engineering, but because data science
works differently, right, much more iterative and much more uncertain inherent uncertainty
in what it is we're building and what we'll find and what becomes significant, you have
to plan for that iteration, and since it's the data science team that works differently
from the rest, what we've done is we've made it a combat that we build the APIs that engineering
will integrate with.
They call the APIs, and that abstracts us, right, we're now behind an API that they're
going to call, so we can change things and test things as much as we can and we don't
need to coordinate those, right, because they're just calling the same API before.
So the little tricks like that really help to decouple teams, and yet still work together.
We have to be aligned on the goals of what we're doing, and then when we work with marketing
and merchants, for example, there's no APIs for buying merchandise or for making new
creatives.
So those are more of a formal relationship that you have, and you want to blend both of
their experiences, you get some of the domain expertise from marketers and merchants,
what they know, and then also get what the data is telling you, which are sometimes very
complementary or different or even contradictory, and both together are usually much better
than anyone on their own.
Any other comments?
No, so I was going to say, so you're actually like a similar comment to what you just
said, right?
I mean, I just want to build on top of that, so Atlassian, we had like a similar kind
of model, so we actually like the data science-lash machine learning team was under the platform
department, and so basically we're like a team that would service the other teams.
And so similarly to what you said, like we had like either APIs, and we're like basically
in charge of building a model, right?
So I think something that's very important to note here, and I think like a not specific
to Atlassian or any company, right?
There is like really like machine learning research, and there is machine learning product,
right?
So for instance, like you, I can imagine a company in which you have like the machine learning
team, the deep, deep down machine learning team that creates, let's say, an OCR model,
and then there is a product or an applied machine learning team that actually applies this
model to create cool new AI products, right?
And these don't need to be the same team, right?
I mean, that's almost the way that I see that it was actually pretty successful to do
that at Atlassian because we would empower the rest of the organization of integrating
AI in their own products, even though they were not AI engineers or machine learning scientists
themselves.
Closing common partners?
I think one aspect of being able to get by and to get more support from engineering on
let's say issues of data quality or building data pipelines was to show kind of the impact
and the fact that they could move faster if, you know, through that collaboration, they
would be able to ship with more confidence and kind of showing that over a couple of
screens.
So slow them down, but speeds them up.
Exactly.
Excellent.
So with that, thank you for your time and attention.
We're finished.
All right, everyone, that's our show for today.
To learn more about today's show or any of our panelists, visit twemalai.com slash shows.
Head over to twemalcon.com slash news to check out Twemalcon shorts.
The series of short interviews recorded straight from the Twemalcon community hall.
Thanks.
