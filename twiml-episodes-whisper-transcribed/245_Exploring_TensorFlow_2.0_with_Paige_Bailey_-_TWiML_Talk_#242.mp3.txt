Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A few weeks ago I had a chance to attend the TensorFlow Developer Summit on the Google
Cloud campus in Sunnyvale, California.
I had a great time at the summit, meaning a bunch of Twimble listeners and friends of the
show and learning about all the new features released as part of the TensorFlow 2.0 Alpha.
This week on the show, we'll be bringing you a series of conversations from the event,
which we'll be kicking off with today's interview with Paige Bailey, a TensorFlow Developer
Advocate at Google.
Paige and I sat down to talk through the latest TensorFlow updates and we cover a ton of
ground, including the evolution of the TensorFlow APIs and the role of Eager mode, TF.Kiris
and TF.Function, the introduction of TensorFlow for Swift and its inclusion in the new fast.ai
course, new updates to TFX or TensorFlow Extended, Google's end-to-end machine learning platform,
and the emphasis on community collaboration with TensorFlow 2.0 and a bunch more.
Now Dev Summit attendees received an awesome gift box from Google that included some really
fun toys that will appeal to the hackers and builders in our audience, including the
new Coral Edge TPU device and the Spark Fun Edge Development Board powered by TensorFlow.
If you're a geek like me and get excited about toys like this, I'm excited to share that
we've got our hands on a few of these for you, our dedicated listeners.
To enter to win one, just visit twimmelai.com slash TF giveaway and let us know what you
would do with this kit if you got your hands on it.
Of course, we'd like to send a huge thanks to the TensorFlow team for helping us bring
you this podcast series and giveaway.
With all the great announcements coming out of the TensorFlow Dev Summit, including the
2.0 Alpha, you should definitely check out the latest and greatest over at TensorFlow.org,
where you can also download and start building with the framework.
And now on to the show.
All right, everyone.
I am here with Page Bailey.
Page is a developer advocate for TensorFlow with Google, and we're here at the TensorFlow
Developer Summit.
Page, welcome to this week in machine learning and AI.
Excellent.
I'm delighted to be here.
I'm delighted to have you on the show and looking forward to diving into all that's new and
interesting with TensorFlow.
But before we do that, a little about your background, how did you get started working
and all this stuff?
Yeah.
So I got started working in machine learning and data science before data science was
really a term, I guess.
My background in undergrad was very geophysics applied math, I did research in planetary
science, doing data analysis on large amounts of data from NASA equipment, so I got to
work at places like the laboratory for atmospheric and space physics and Southwest Research Institute
doing lunar ultraviolet research.
And that today, doing statistical analysis on multivariate data would be called data science.
But back then, it was just called research.
So that was when the bug first started.
And then I started doing machine learning for work when I worked at Chevron for predictive
modeling in the oil and gas industry.
So I guess in terms of data science things, I've been doing it for about 10 years for machine
learning about five to six.
Did you jump from Chevron to Google?
No.
So that was an interesting sort of career progression, I guess.
I had done planetary science research as an undergrad for my first two internships and
then my third was with Chevron.
And they had me on a really cool project I got to create a database and do some scripting,
some geospatial analysis using Python.
And after it, they said, hey, do you want to come work for us?
And that sounded like a cool job opportunity, especially since they were offering to pay
for grad school.
Because I went to go work there for about five years.
And after Chevron, it was time to kind of leave and to explore new opportunities.
And that was Microsoft.
So I worked for Microsoft doing machine learning developer advocacy and then as a senior software
engineer in the office of the Azure CTO.
For those that aren't familiar, what exactly is a developer advocate and how does that translate
into how you spend your day-to-day?
Absolutely.
That is a great question.
And developer advocacy means different things to different companies.
Here at Google, developer advocacy is very focused on improving the experience for our end
users and for basically anybody who would go about using our tool.
And that can include everything from improving documentation to user experience research,
to building tutorials and quick starts, to creating curriculum materials like the Udacity
and Deep Learning AI courses that we just released, to also engaging with the community via
social media, but also through things like special interest groups and making sure that
if there is a problem with the tool, it's surfaced, fed back to the engineering team and we can
build a plan to resolve whatever that issue happens to be.
So if anything is frustrating, so if maybe a higher level API isn't intuitive, it might
require making a pull request and resolving that or say we're missing a symbol in our
documentation, then making a pull request and fixing that in the docs.
So it's really a variety of things, but as I mentioned, it's all very focused on communication,
improving the developer experience, education, and also very much engineering tasks.
And if you're a person who enjoys all of the above, then maybe developer advocacy is
right for you.
Awesome.
Awesome.
So, there was a bunch of interesting TensorFlow news yesterday.
Yes, there was so much.
It felt like a fire hole was almost and that's coming from so many new, it was all about
to happen.
So many new products.
Yep.
Yeah, so I'm really interested in kind of digging into them, one of the things that I've
mentioned this to a couple of times, maybe yesterday, one of the things that's been a little
bit difficult for me to parse is an outsider is like, what exactly is new and all of these
things that are kind of new?
Like, eager mode was highlighted very extensively yesterday, but that was announced last year.
There were other things that I'm like, I know I've seen this before, there was a paper
that was a couple of years ago, you know, so for me, you know, probably the best thing
to do is to have you kind of walk through from your perspective, you know, what were the
highlights and what was new and then we can kind of dig in from there.
Right.
So, it is difficult to kind of place what's actually new and then what's just been kind
of bundled and packaged, right?
So the biggest announcement yesterday by far was the alpha release of TensorFlow 2.0.
And TensorFlow 2.0 is a new version of TensorFlow focused on developer productivity, ease of
use, carous as the recommended higher level API, an eager execution by default, which enables
you to do things like better debugging, but also the pathonic experience that you, especially
folks coming from scikit-learn and the data science community would expect.
So TensorFlow 2.0 is, I mentioned it's eager execution by default, and you're correct
in that eager execution was announced last year, but in order to enable it, you had to
do something called like tf.enable eager execution at the top of your script.
So you had to kind of specify that at the beginning and you had the option of sticking
in graph mode if you didn't enable eager execution, tf2.0 is eager by default, which means
that you don't have to have that line, but you're thrown directly into this experience
where you don't have to create a static graph, you don't have to have sessions, you don't
have to create the thing before you can interact with it, before you can actually start using
it.
It feels much more pathonic.
So that's kind of a new thing in the eager execution by default.
Another very, very new thing is that TensorFlow 2.0 was created entirely in collaboration with
the community through the RFC process.
So every change that was made to the API had to be proposed by an engineer or proposed
by someone endorsed by the entire community, discussed, and before it was incorporated.
So if you have any curiosity about these RFCs, you can go and take a look at it on github.com
slash TensorFlow slash community.
We have a list of all of them, and if you care about details, they're all important.
We also have a community of special interest groups, everything from TensorBoard, which
is a visualization tool, to build, which is sort of building TensorFlow for various Linux
distributions, and then also with Nvidia graphics card strivers and the like.
We have a special interest group for testing.
So if you're trying out TensorFlow 2.0 and you're running into Snags, or if you need migration
support, there's that option for you as well.
And then others are on networking and data ingestion and lots and lots of different ways
to get involved.
And really, as I mentioned, the focus on TF2.0 is community, sort of building something
that people would love to use, as opposed to the first TensorFlow release, which kind
of felt like, hey, here's some code, like go forth and prosper, okay?
And not exactly the most straightforward sort of code, so that's the general idea.
A lot of the kind of top line takeaway is around kind of APIs and developer experience.
Eager by default, you can kind of think of that as, okay, so there's one line of code
less that I have to write.
But is it deeper than that in terms of the underlying architecture and the way that TensorFlow
supports that?
That's a great question.
And absolutely.
So Carmel, during the keynote yesterday, she had this great slide, where, so if you're
a carous developer, if you have experience using TF.carous, you can create a neural network
with about 10 lines of code.
And it's very straightforward.
It's intuitive.
It's easy to understand.
And again, feels very private.
She showed a slide saying, here's what your model would look like if you're using TensorFlow
like 113 or TensorFlow 112 or whatever version.
And it's 10 lines of carous.
And she said, and here's what it would look like in TensorFlow 2.0.
It's the same code.
The same code expressed both ways.
But the reality is that TF2.0 is doing a lot of things under the hood to improve model
performance and to take that same carous code and make it scalable in a way that it wasn't
experienced before.
So even though the code looks exactly the same, there are a lot of optimizations under
the hood that makes it a bit more performant, especially with distributed architecture,
so with distribution strategies.
One of the things that was presented yesterday was there was a fair amount of detail that
was gone into around TF.function.
Is that totally new?
Yeah, TF.function is new.
TF.function is basically a way for you to take any sort of Python function, rapid
and define it as a core TensorFlow operation.
So you could have anything from adding two numbers, so you could have a function that would
take, you would define something that would add A and B. And if you preface it with an
at TF function, it would define it as a core TensorFlow operation that you could then
use in your graph.
That is pretty new.
People have had a lot of positive feedback about autograph and TF function both when using
TensorFlow 2.0, and it makes it a lot easier to take things that traditionally would have
been very difficult to do with static computation, static graphs and sort of use them in a friendly
dynamic way.
Okay.
And what is autograph?
Autograph helps you write complicated graph code using normal Python.
So it automatically transforms your code into the equivalent TensorFlow graph code, and
it supports a lot of the Python language.
So that is that's a kind of one-liner explanation of autograph.
2.0 introduces some significant changes to the API, but that's not the only kind of developer
facing developer experience kind of announcement.
One of the ones that's gotten a lot of conversation going is around the Swift for TensorFlow
and curious what your thought, do you deduce, do you know Swift?
I have to play with that at all.
So I have fiddled with it, but only since I got to Google, I hadn't really experimented
with Swift before kind of, well, I get to sit next to Chris Latner, which is Chris
Latner and Brennan and the entire Swift team.
So that's kind of rad.
And the idea behind Swift for TensorFlow is that if you're a Python developer, Python
is great in terms of user experience.
As a user interface, as a programming language, it's a great way to build out products.
But it also has a lot of limitations and drawbacks.
So for example, gradient tape is something that causes a great deal of frustration whenever
you're using TensorFlow.
gradient tape?
gradient tape.
What is that?
It's how you deal with variables as you're creating your graph.
And so as a Python developer, it's often very frustrating to try to manage all of these
different aspects.
But with a lower level tool like Swift, a language that's a little bit closer to C++,
you get afforded a lot more control over what you're creating.
And that's kind of the idea.
And I think that's what captured the imagination of Jeremy Howard.
So I'm sure you saw that in Alzheimer's as well.
He's using Swift for TensorFlow for his latest iteration of the FAST AI curriculum, which
we're all very excited about.
And I'm very curious to see how that progresses.
Another really cool thing about Swift is, or Swift for TensorFlow rather, is that you
can use Python within Swift just within port Python.
And there is a great demonstration from Brennan and from Chris yesterday where you could create
something like a plot of performance or accuracy over time with Matplotlib, just as simply
as you would with Python, except all you have to do is practice it with LAT for your
variable.
So I think Swift for TensorFlow is a really interesting space.
They're still very new, so they're still developing their story and they're still developing
the product.
But I'm curious to see how it progresses this next year.
And they certainly have a vibrant community.
If you have any, I mentioned the special interest groups a little bit before, but they also
have these really active mailing lists where people are asking questions and sharing products
that they've created and sort of ideas that they have about changing designs of API and
technical things.
And the Swift for TensorFlow user group and mailing list is consistently one of the most
active.
So if you have curiosity about it, absolutely join and send your questions to swiftittensorflow.org.
Awesome.
Awesome.
Yeah.
People keep asking me like what I think about it and like, I don't know, I've never seen
Swift outside of a slide on the presentation today.
Yeah.
It's very, when I first, because it was announced last year, Swift for TensorFlow, but in
a very sort of low key way, right?
Okay.
I missed that.
Yeah.
It was a presentation.
It was also the best non-leak of TensorFlow history.
I think like nobody anticipated that it was coming, but so it was announced last year.
And all of the progress that you've seen since has been made since that initial announcement.
But yeah.
Like usually you would think Swift and think, you know, iOS or, you know, something.
That's exactly what I think.
Yeah.
Yeah.
You know, I tell people, I'm kind of excited about it because Jeremy's excited.
Part two of the new course is starting up soon and sounds like he and Chris will be collaborating
on a couple of lessons in that course to kind of highlight.
Yeah.
Well, not just kind of highlight the Swift for TensorFlow, but also this may be a little
bit of kind of inside baseball or whatever, but like the way that the way that he builds
the courses is basically by building out the framework, right?
And that becomes the process of building out the framework becomes the course.
And so it sounds like what he and Chris will be doing in this course is like starting
to build out the fast.ai Swift version as kind of part of this course is really fascinating
kind of approach for multiple reasons.
I took a look at his blog post about kind of why he was so excited about Swift and one
of the reasons that that he's real playing about is like, this is the ground floor opportunity
like Swift, you know, TensorFlow or rather Python, you know, they're all of these challenges
associated with like the barrier between Python and C when you have to get really deep
and not being able to debug all the way through and not be able to trace variables all
the way through that kind of thing.
And needing to kind of shim out underneath Python to get performance and one of the reasons
why he's excited about Swift is because it is high performance.
And you know, to the extent that over time kind of the entire stat gets built in a high
performance language, you don't have these barriers that you have to figure out.
Absolutely.
Like it's a fascinating space and also so one of the things that shocked me the most
I guess about Swift was how intuitive it was to understand as a language.
Like if you look at the Swift code used to generate a neural network, it feels very
similar to Keras.
It's not because I took some people as well as classes in college and I am not a fan
and will not be shy about saying that.
Like I love the idea of it conceptually as a language, but it's something that's very
it doesn't feel natural for me to use.
Swift feels like a great deal more natural.
And one of the challenges I think that they're going to face is the that Swift as you mentioned
hasn't traditionally been a data sciencey language.
So they don't have all of the vast libraries of tools that Python has.
So things like matplotlib and psychic learning for traditional machine learning tasks and
the like.
But it's really interesting and that so we're doing Google Summer of Code for the very
first time this year.
And five of our projects or five of our proposed projects are Swift for TensorFlow related.
Summer of Code for those that don't know as a program where open source projects can get
matched with like college students or students in general maybe and get some funding to pay
them to work on the project.
Absolutely.
So the Google Summer of Code is one of my favorite things about Google.
And it was also when I applied for it a long time ago when I was still doing the grad school
thing.
I applied to a project called AIMA and got a response back from Peter Norvig who was
apparently the person who who was sponsoring the project and that was my first email from
a Googler.
And it was Peter Norvig.
Wow.
Yeah.
It was amazing.
But it's it's so enchanting in that open source projects can apply to be part of this
GSOC program if they're selected that they can have a partnership of mentors who are
directly working on the project and students.
The mentors work collaboratively with the students, doesn't matter where they're located.
And it's there's certainly encouraged to be remote.
And the students are kind of guided through making their first substantial open source
contribution and also paid for it, which you know as a student that's kind of like a
dream come true, right?
Like you get to work on open source, you get to be paid and potentially you get to be
mentored by like in the Swifferents for TensorFlow case.
Like you would be mentored by Chris Ladner or yeah, or the AIMA case you would be mentored
by Peter Norvig or for the rest of our for the rest of our project opportunities, right?
We've got autograph and TF function and sort of TensorFlow data sets, you're mentored
directly by the often the people, the engineers who've created those products.
So at least in TensorFlow's case and in the other projects that are sponsored, it's really
a phenomenal opportunity.
And I'm excited to see what the GSOC students build out for Swiffer TensorFlow.
Interesting.
You mentioned something earlier that kind of tied to a question that I had in kind of seeing
the Swiffer TensorFlow announcement, how would a Swiffer TensorFlow kind of play or not
play with a carous?
Yes.
So Swiffer TensorFlow, like you can use carous within it, but for the most part it's like
you can use any Python within Swift.
Yes, but it's recommended that you that you would program using Swift.
But again, the Swift syntax looks very, very similar to carous syntax.
And we can, if folks are interested, we can share some of the Colab notebooks that like
links to it that were shared yesterday, right?
But yeah, I would love to see that.
And another really cool thing about Swift for TensorFlow is that it works in Jupyter
notebooks, it works in Google Colab.
So really the notebook experience, if you're a data scientist, feels very familiar as
well.
And the big kind of announcements in addition to kind of the under the cover stuff was kind
of a rationalization of the APIs, it sounds like it's more than just, hey, curious is the
default and more than even, I don't know the right way to ask this, but we're getting
organized.
Is that what it is?
I think so, or at least that's what it feels like.
So the original, I started using TensorFlow, like not for real substantial work, but just
for, you know, personal projects and prototyping things in 2015 when it was released.
And it was, oh man, like it was, it was not fun, like it was, it was not good at all.
Like TensorFlow, the original version, like you, you had to write so much boilerplate in
order to get something to work.
And then since the initial release, it's just, it had grown just sort of exponentially.
We had this module called Contrib, TF Contrib that was kind of a generalized catch-all for
things that folks wanted to add.
So if you were a grad student and you had created, you know, a collection of loss functions,
you would, you would add those in Contrib or we had TF dot Contrib dot GAN, which was
really sort of beloved, you know, GAN, GAN creation tooling for like a straightforward
path to creating GANs using TensorFlow.
And we'll still have it just migrated to its own repo.
But the, but the sad reality of Contrib was that none of those contributions had a support
plan.
None of them had a defined owner, so like a proxy maintainer.
And a lot of them may be functioned in an earlier version of TensorFlow, but failed to work
later.
And there was no clear way to understand what worked, what didn't, what was a duplicate,
and like how would you migrate from what, like a symbol to the core API.
So that really wasn't defined.
We also had a lot of mathematical operations and sort of more traditional statistics capabilities
that were in various places in the API, but not really kind of structured and grouped together.
And it was, gosh, I don't want to, I don't want to count how many symbols there were,
but there were certainly thousands.
There were thousands of symbols without any clear structure or consistent aiming conventions
and a lot of duplication and a lot of sort of things that kind of failed to work together
nicely.
So now we've taken a more modularized approach.
So all of the statistics and mathematical capabilities have been kind of bracketed
off and turned into something called TF probability.
So if you don't want to use all of the things in TensorFlow, like you don't really care
about, you don't really care about, you know, Keras, or you don't care about some other
portion of the API, you can just use TF probability and you can download it as a
pit package or estimators, you can just download estimators.
So all of these things, we've removed duplication, we've kind of taken all of the symbols that
were just kind of dispersed to the winds and grouped them together in a way that seems
more logical and also applied a lot of sort of well thought out renamed to make things
more consistent and to make the sort of calls feel more intuitive.
But that's kind of the idea is that instead of one monolithic application that you have
to download in its entirety in order to use a single bit of functionality, you just take
the pieces that fit for you in your organization and that helps a lot with the build and deployment
process as well.
I definitely saw that in the discussion yesterday and one of the analogies that kind of popped
up in my head is like Hadoop and not all the negative associations with Hadoop but like
people think of Hadoop as, you know, they think of it as this one thing, MapReduce, right?
When really it was like a storage system and MapReduce and like this whole ecosystem
of stuff.
All of the Apache products.
Yeah.
All of the Apache stuff, right?
And TensorFlow, like I think we're transitioning from the point where you would think of TensorFlow
as this monolithic thing to like, you got an ecosystem, right?
With the probability stuff, which we can talk maybe a little bit more about the TFX stuff,
which I'm really excited about, which is almost its own ecosystem.
So that I think that's an interesting kind of transition and one that, I mean, it's kind
of a, there's a natural point in a large open-source project where you start to see that kind
of thing happen.
Yeah, and it's really interesting to, at least from a developer advocacy perspective,
to see the different communities that build up for each one of those tools.
So for example, we mentioned Swift for TensorFlow and usually the folks who gravitate towards
that camp are very interested in things like algorithmic fusion, they're very interested
in compilers and, you know, sort of, performance, and then you have folks in the TensorFlow
JS community that are, so TensorFlow JS is JavaScript in the browser that are very focused
on creating these sort of artistic experiences often.
Like you would have code pens to do neural drum machines or you would have, you know, really
interesting sort of, you know, like puppy ears placed on people heads that, you know,
would follow you around as you, as you look in your, your phone camera or your browser,
or, you know, even things like, sort of in a more product-centric way, like if you're
a developer who wanted to include a text box in your application, automatically, including
something like sentiment analysis within it.
So as a person is typing, yeah, it would be like, hey, man, you know, your tone's kind of,
your tone's kind of negative there, maybe you should make it a little bit happier or
maybe you should be a little bit, like, change this word to be this other word and that
should be fine.
So those sorts of tools and then TensorFlow Core, you know, you get a lot of academic researchers
sure, but you also get businesses who are really interested in productizing machine learning
and deep learning and then they also are interested in TFX and, yeah, it's really cool to see.
Yeah.
Well, let's maybe talk a little bit about TFX.
Yeah.
So that was one of the things that I was most excited about.
TFX is TensorFlow Extended.
Yes.
And there's a workshop for it today, is it?
And there's a workshop for it today that I will be popping in on.
I'm excited about that.
So my, I've mentioned this to a couple of people.
Yes, I, I've, you know, we've done a series of podcasts on the show about machine learning
infrastructure and I've been writing ebooks about machine learning infrastructure and it's
a space that I'm really excited about and I've looked at TFX in the past and it was hard
to really like wrap my head around it, it's just like this kind of random collection of
tools.
Yeah.
But what was announced yesterday seemed like the beginning of making it more coherent and
kind of like, when you read the TFX like the papers, like it's clearly Google has this
kind of very elaborate sophisticated infrastructure internally.
And I think looking at that and then seeing what was previously available made it even
more visible.
It's like, that's all you're giving me is like, do you have a melody?
Yeah.
And certainly.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
So maybe kind of what you, it's your, you know, what's the kind of the top line on TFX
or what are you excited about from that perspective?
I'm so glad that you mentioned TFX because TFX is one of the reasons why I wanted to
come work at Google in the first place.
Like I read the paper and I think I mentioned a little bit before I worked as a data scientist
in the, in the energy industry for a long time and the hard bit like it was, it was always
delightful to create the models, you know, it was cool to have this like crunchy engineering
challenge and to take data and to figure out how to solve it in a way that, in a way that
it sort of made sense and that would, and that would be performant and explainable.
But then the hard part was always, okay, cool.
Now how do I deploy it?
Like how do I get it placed?
How do I have this bottle placed into a format that somebody can use in a software application?
And then once it's released, how do I make sure that it's doing its job and that it's
kept up to date?
And how do I make sure that the incoming data stays consistent with the data that was used
to train the model initially?
And if I want to do online training, how would I go about doing that?
So all of these like really interesting questions where the model was just like this tiny piece
of this big, big overall, like sort of continuous integration, continuous deployment strategy,
like DevOps for data science.
And the, and TFX, the paper sort of gave, it sort of elaborated on a strategy that Google
has been using successfully over the past 10 years to do that effectively.
Because most businesses they create custom glue code, they, they don't really, they might
have a component of DevOps for their machine learning models.
But a lot of it might be manual, so not necessarily an example would be if your model falls
below a given accuracy for a stent of time.
How would you?
Yeah, you know, like, or it's like, and if you're at the other, the other piece, right,
is that data scientists, they're probably creating their model in Python or R. In order
to get it integrated with a software application, that model, at least where I worked previously,
it would have to be refactored into something like Java or C++, which means that you're,
you know, usually your accuracy goes down, like, it just, it's, it's not a good situation.
And then if your model needs to be updated, how are you going to take that C++ and Java,
give it back to your data science team and be like, well, fix it now.
You know, there's, there's no straightforward path to do that.
With TensorFlow, you know, it's, it's the same framework for everything.
So the data scientists can create it.
It can be deployed by your, you know, your DevOps team. It can be maintained by them as
well.
And then if it needs to be modified, it's the same TensorFlow code that your data scientists
know how to use.
So TFX, you're right in that when it was announced last year, they only had like two little
components of this, of this sort of toolbox of things, the date, everything from data validation,
which allows you to do those data checks that I was mentioning before, transformation.
So TFT, TensorFlow transform, which allows you to do pre-processing on data pipelines,
to make sure that an in full past data processing, to make sure that your statistical distributions
for each feature are similar as they're coming through your pipeline.
Model analysis, which gives you insight into how your model is performing over time.
There's even a GUI to show you sort of metrics associated with that.
Of course, they're serving, and then there's also sort of additional tools that I believe
Clemens mentioned yesterday, collaboration with Cubeflow, but all of these sort of products
that bundled up together, alleviate the need for you and your company to have to write
Blue Code and to have to refactor models.
It gives you the tools to deploy models in a maintainable way.
Yeah, there's also an interesting bit in there about a metadata repository where you're
able to track all of the configs and parameters associated with various data linux and data
problems.
Right.
So you have your experiments, and then when you have a model that you're pushing out to
production, all that information, and there were some use cases presented to your point
that shows how you can use that information to go kind of work backward from a model decision
to what training data impacted that model decision and what the experiments were that kind
of drove to those sets of model parameters and stuff.
Absolutely.
And that's huge in terms of model explainability.
So if you're working in a high impact industry, again, like the oil industry, you had to be
able to explain why you made a decision just in case it was the wrong one, and then also
reproducibility.
So if somebody wants to take your results and try to replicate them on their own data,
or try a different model and see if the performance is, if the performance varies, then having
all of that metadata associated with the input is huge.
Anything else that we've not talked about yet?
You left excited about yesterday or while you knew about it already, but it was still
exciting to hear.
So I'm really, it's not a secret.
Like I am most excited about the community focus for TensorFlow, and I really love, I really
love that 2.0 has been pushing for community involvement and like not making any changes
unless everybody who's using the tool is, you know, is informed and has had the opportunity
to give feedback.
So the original release of TensorFlow felt very much like, you know, delivering tablets
down from a mountain kind of almost.
It's, you know, the code was released and over time, like community could get involved,
but it was really only through this thing called Contrib.
And for the most part, the engineering was done by the engineering team at Google.
And since then, it's, it's developed into more of a partnership between individuals in
the community, but also companies who are using TensorFlow extensively.
And you know, students or application developers or, you know, not just deep learning researchers,
but really people coming from all walks of life who want to incorporate machine learning
into their tools.
I have crunchy problems that machine learning can help solve.
So that is, that is my favorite thing is that TensorFlow 2.0 is all about community.
You mentioned the RFC process and that's a, that's a new process since that's a new,
the RFC process is a new process that was begun, I believe, halfway through last year.
So maybe July, August of last year, okay.
And again, if you, if you want to make a change to the API, anybody can do it.
So if you have an idea that you think would be solid, you just propose it.
The engineering team, both at Google and then also externally reviews it and delivers commentary.
And if everybody agrees, then development work can start.
Awesome.
Well, Paige, thanks so much for taking the time to chat with me.
Thank you so much for having me.
This was fun.
Absolutely.
Absolutely.
All right, everyone, that's our show for today.
For more information on Paige or any of the topics covered in this episode, visit twimlai.com
slash talk slash 242.
Thanks again to the TensorFlow team for their sponsorship of this series.
Make sure you check out the tensorflow 2.0 alpha at tensorflow.org and enter our TensorFlow
Edge giveaway at twimlai.com slash TF giveaway.
As always, thanks so much for listening and catch you next time.
