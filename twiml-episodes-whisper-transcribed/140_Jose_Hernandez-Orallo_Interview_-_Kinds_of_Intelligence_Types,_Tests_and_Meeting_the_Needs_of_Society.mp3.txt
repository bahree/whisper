Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Next Tuesday, May 15, the Twimble online meetup is back.
Our main event will be a presentation by Santosh GSK on the paper YOLO9000 Better Faster
Stronger by Joseph Redman and Ali Farhaddi.
We'll also discuss the landscape of object detection, the current state of algorithms
in that space, and the challenges ahead.
If you aren't already signed up, head over to twimbleai.com slash meetup to register.
See you there.
And event season continues.
Today in tomorrow I'll be at Figure 8 Train AI Conference in San Francisco.
I'll be podcasting all day from the event, so if you're attending the conference, stop
by and pick up a sticker.
It's not too late to use our Twimbleai discount code if you'd like to attend.
In this episode, I'm joined by Jose Hernandez Arayo, professor and the Department of Information
Systems in Computing at the Polytechnic University of Valencia, and fellow at the Leverhume Center
for the Future of Intelligence, working on the Kinds of Intelligence Project.
Jose and I caught up at Nips last year after the Kinds of Intelligence Symposium that he
helped organize there.
In our conversation, we discussed the three main themes of the symposium, which were
understanding and identifying the main types of intelligence, including non-human intelligence,
developing better ways to test and measure these intelligence, and understanding how and
where research efforts should focus to best benefit society.
Alright, let's do it.
Alright, everyone, I am here at Nips in Long Beach, California, and I have the pleasure of
being seated with Jose Hernandez Arayo, who is a professor at the University of Polytechnica
of Valencia, and is currently on sabbatical and visiting at the Center for the Future
of Intelligence at Cambridge, UK.
Jose, welcome to this week in Machine Learning and AI.
Thank you for having me here.
Absolutely.
Absolutely.
So, why don't we get started by having you tell us a little bit about your background
and how you got involved in the Machine Learning world?
Well, it's a long story, probably, that many people in the area would share.
As a child, I had my first computers, and at some times I thought, how can I make this
computer think?
And at the time I had, well, probably it's not very common here in the States, but in
Europe there was this ZX, which is kind of a Commodore, the early computer, the early
computer, like the Sinclair.
Yeah, Sinclair, yeah.
I had that one with 48 Ks, memory.
And at some point I started to play with some kind of basic ideas of logic, because you
have that at high school and, okay, this is about thought.
And then, well, even I just played a little bit with some kind of probabilistic versions
of that, even now it's completely naive from the point of view of, well, you know, the
things are much more complicated than that.
They will try as a teenager and all of that.
And then, well, because of all of that, I started the computer science, but at some point
when I was, what I completed that, I felt that what I really liked was AI, that of course
I was from the beginning, but I didn't see that much AI, just one subject during a five
year degree, and I was like, can it be that you do computer science, no machine learning
at the time.
It was a wind, it was what was this, this the 90s, right?
So, and I decided I have to do a PhD one, and at some time it was an MSC and a PhD on
something different, more AI.
It wasn't that easy to find something.
It was all about these expert systems, and I said, this is not exactly AI, for me, so
this is, and at some point, I did a PhD on something related to, I started with deduction
I still, I thought the reason it was a lot about logic and all of that, and at some point
to learn, this is more about inductive inference about learning.
And then I moved a little bit to machine learning, at some point, then when I started as
an academic, at some point, I was really concerned, and that was from the beginning, about
how to certify at some point that the system has some capabilities, or how you would say
that the system is able to do something, so that's an kind of evaluation of these systems.
And of course, at that time, people already knew about the during tests, and I was also
interested in psychometric tests.
Okay.
About 20 years ago, so it's a long time.
And then I, well, because you, at that time, machine learning started to flourishing with
applications, the terms data mining became fashionable at that time, and I also worked on building
different kinds of classifiers, I was involved in ICML for some periods, in terms of trying
to submit papers, so that's basically a traditional, let's say, a career in trying to get into
the field of machine learnings in the 2000, and then at some point, about a few years
ago, I recorded some of these ideas out, because I was working on evaluation of classifiers,
okay, so things like ROC curves, and how to evaluate a system for a range of context rather
so related to cost-sensitive learning, and things like that, and I said, okay, but some
of these ideas could be linked to the evaluation of AI as well, and how to evaluate some other
more general systems.
And at some point, and I started to work on this area that I call AI evaluation, and
I saw that there was a lot of things to do there, it's perhaps not an area where people
prefer talking about building things, but I like to evaluate what you do is, well, you
are on the other side, like you are not an engineer, but I thought that this was really
important, and whenever I talk to people, well, we need to know where we are, where we
are going, so it's important we have very good measurement instruments, and at the moment,
we don't have very good instruments in AI, because it's evolving so far, and we are also
focused on particular challenges and tasks that we are not really sure whether our systems
are really progressing.
We have that feeling, and of course, there's a kind of objectivity there that there
were systems that are much better now, because we are able to solve more, but at some point
I said that there's a gap here, and I tried to recover some of the old ideas with new
ideas, tried to see what people were doing in terms of evaluating, not only machine learning,
but AI in general, also robotics, and I started to work on this area, and then about
one year ago I wrote a book about this, I've been involved in the organization of several
events around AI evaluation, all of these things, and now I'm here at the last year, at
this center for the future intelligence in Cambridge, where they are doing this idea of evaluation
goes well with some of the general objectives of the center, and basically I'm still really
enthusiastic about machine learning and AI, and see what's going on.
Awesome, what's the title of your book?
Well, the title is the measure of all minds, evaluating natural and artificial intelligence,
so it covers AI evaluation, but also it puts that in the context of how natural intelligence
is evaluated, animals, non-human animals, and humans, a little bit of animal commission,
how tests that you can use, for instance, for development in children, or tests that
you would use to test, or maybe IQ tests, what's the relation between IQ tests, and what
we are doing in AI, and then the first answer is perhaps nothing, but there's something
that you scratch a little bit on the surface, you find connections, and these interesting
questions, some of these connections are developed in the book, for instance.
I'd love to dig into that a little bit, in a little bit more detail, but first I'd like
you to describe the symposium that you helped organize here at Nips, called the Kinds
of Intelligence, right?
Yes, so it was around a project that we have at the center of the future of intelligence
in Cambridge, UK, this project is called Kinds of Intelligence, or at some point there
was a suggestion to propose a symposium on some of the ideas of how to characterize the
different kinds of intelligence that we know at the moment, not only human intelligence,
but also non-human animal intelligence, natural intelligence, and how to locate AI in this
landscape of intelligence.
So that was the original idea of the symposium, and then at some point we developed that idea
into three strands, the first strand was to understand this space, and to see where AI is
just represented as a subset of human intelligence, or something completely different that is
taking us to different places, so trying to analyze this landscape a little bit from different
perspectives, and we wanted to have different perspectives from animal commission and from
human commission as well, for human intelligence and development.
And the second strand was about how to test all of this, and of course I had some influence
in having this strand in the symposium, I was okay, now we have this landscape, but
how can we locate where we are, so we are moving in some direction, but can we say okay,
what are the dimensions of this landscape, and how can we certify that the system is
moving in that direction.
And the third one was okay, we are able to answer all these questions, which of course
sounds very abstract and challenging, the third question is now that we are able to understand
this landscape where we are, the question is where we want to go, and there was a third
strand of this symposium about what are the priorities for society, and whether these
priorities, I wouldn't say the low hanging fruits because some of the recent challenges
are really, really challenging, but sometimes we are motivated by things that are doable
in with current technology, rather than perhaps aiming at the really important problems,
because you are not going to have a success in a matter of one, two years, so it's more
like a long-term project that perhaps academia, or even government, can be interested,
they have to ask companies, but this is changing because the tech giants are also interested
now in long-term goals, and also some areas that we might reach at some point in the future,
even in the near future, that are perhaps dangerous or unethical, but if we don't know where
we are, it is very difficult also to assess the progress in the direction that we want
to take.
So we have that, and in the end, yesterday we had a fantastic lineup of speakers.
I saw that.
Yeah, from different areas.
So it seemed very interdisciplinary, very diverse in terms of the viewpoints that folks came
from.
Yeah, we have people from, for animal cognition, from human intelligence, we have people
from AI, for different perspectives from AI, people perhaps more in favor about more orthodox
approach to AI, where you would like to learn from a lot of example of more in alignment
with the, with nips, people are more contrarian to this view, where you're okay, what about
the learning more human-like, or more with a few examples, or more hypothesis-driven rather
than data-driven?
And we have a very interesting discussion in the second session about this, the different
perspectives.
Also, we had people from the first session come in and ask questions to the people
of the second session.
It was very active.
The third session was a little bit about this society, some of the risks, and some of
the things that people are talking that we are going to see in the future, like at some
point the discussion, it was, also indeed, it was at the right moment also to talk about
at some point things like corporations having a lot of power, and there was a moment where
there was this link about AI in the future, could resemble some of the corporations that
we have at the moment, or that we have had in the past two centuries, and they have a
lot of power.
So that perhaps we can link some of the risks to some things that we have already seen,
so not everything that is coming is new in terms of the effects on society.
Interesting.
Maybe we can take these three strands in turn and spend a little bit of time kind of
having you characterize the landscape before diving a little bit deeper into the measurement
in your research in that area, and then we'll finish up with some of the directional stuff
that you discussed.
So what does that landscape look like?
How do you characterize the way folks are thinking about the kinds of intelligence?
Yeah, that's a really challenging problem, because the first thing is we disagree on our
notion of intelligence.
Some people even say that the term intelligence should be eliminated from our discourse,
and we should only focus on, or even just use the word learning and forget about intelligence.
Some people, because we have these two different views of intelligence, I would call one extreme
is we negate that there's such a thing as intelligence, and some of these people, you
can also have some of these people from, even from machine learning, saying we have the
no freelance theorem, so even if you design a system to solve this problem, there's some
other problem for which there are other systems that will be optimal for these problems,
but not for the first problem and the other way around.
So this idea of just having more general systems or more intelligent systems is nonsense.
Well, I would argue against that, but then you have the other extreme.
It seems like being intelligent is very different from being optimal at everything, which is
kind of what this no freelance theorem seems to argue against.
Yeah, well, I think that that's assuming that our world is random in a way.
Do you have this block uniformity as an assumption, which is, well, the special case is that
the data you receive is random, so it just says, if that's a case, well, the theorem
is a theorem, that's a proof, and you say, okay, so any machine learning algorithm will
be equally good or equally bad at, but the thing is the assumption that there's assuming
something very, very strong, and then basically that our university is completely chaotic,
and it's not like, wow, there are patterns, and there are patterns not because they're
laws of physics, patterns because what we receive goes through humans, animals, devices,
so not just to explain this a little bit more technical, when you have a theory machine
or any machine, it doesn't have to be a theory machine, but just any machine and you put
random inputs, what you get as an output is not random at all, right, right.
So what we see in our world, because it's just going through all of these filters and
these filters are machines in a way that it's an animal or it's a thermostat or something
like that, what you get as a result has patterns, and then you can explore these patterns,
and if that's the case, then the assumptions for the North Korean land students don't hold
in that scenario, but well, that will be like sometimes it gets a little bit philosophical
about that question, but that's one extreme, so you have like infinitely many intelligences,
and this links a little bit with to psychology where you say, okay, there's not such a thing
as intelligence, and you will have just many of the multiple intelligences theory where
you will be good at this, but you're not good at that. The other extreme is there's only one
single intelligence, and we hear that occasionally, especially for some of these discussions
about superintelligence, and people think of intelligence, something monolithic,
and even some people try to assimilate intelligence in humans with IQ values, which is of course
a simplification, and in psychomedic, people will say, no, no, no, no, this is just an indicator
that each useful predicts this and that, but you can't assimilate that to intelligence,
not people in. So the idea you have to infinitely many intelligences is just one,
perhaps the virtue here is to think that intelligence is something with structure,
so you can think of perhaps something like a journal intelligence, but is related to some other
abilities, until you go down, you can see this as in a hierarchical well from the top where you
would have something like a general ability like that or more, and you go to some kind of skills,
until you reach the bottom when you have a very specific ability, sorry, a very specific task,
so you go from very specific, as you aggregate tasks into, you put the, say,
task skills generally, you have these, you can have many levels as you want,
and that gives you a structure, so when we were going back to your question about the
landscape of intelligence, there's one way of looking at this landscape of intelligence,
so we can just look at the, at this landscape at the bottom, and we will have no structure,
so there's no interest in this landscape, because we will have, okay, I'm able to solve this task,
this task, this task, and I'm also able to solve these tasks, and these tasks are important
commercially or whatever, that's interesting, but that's not, this is not going to give us a hint
about what if I give you this new task, can you tell us something about your system solving these tasks?
I mean, a lot of the research that we see today, it's kind of predicated on an assumption that if we,
you know, build towards very task-based intelligence that will help us understand and get towards
a general intelligence, I'm thinking of things like AlphaGo, right? That's, yeah, maybe task is too
limited for what it's doing, but it's solving a very specific problem, but the only reason why
they're doing that is because of this hope that it'll get us towards a higher level, more general
intelligence. Yeah, I think that the program, the program is, they're there having research in
deep mind with AlphaGo, AlphaGo 0, and Alpha0, we had that talk yesterday, Damage gave an excellent
talk yesterday, and people were talking about AlphaStar at the end of the end of the session.
He made a very good point in one of the questions about, okay, what we're interested in, perhaps
we have to put a lot of bias to solve Alpha, or well, there was a discussion about
quality bias or knowledge, or to solve AlphaGo, then we, and human knowledge, we can remove that
and call it AlphaGo 0, still there's some knowledge or something, you have to put the rules of the
game, for instance, but then it's a first stage that you can even make this general enough to cover
some other games, and then you have this Alpha0, which is, so this line of progress that we see
is really interesting because you're going for a very specific task, right? Using a lot of knowledge,
which is very difficult to add up to just a small change of that, and you need a lot of effort
to succeed for that task, to a system where you can have, even if you have to put still some
knowledge, the rules and that, the system is able to do much more independently, much more
autonomous ways, and then you have a third step in this progress where you have a system that
does more general, still the system is only able to solve some kind of ball games if you give the
rules, but that sounds amazing in terms of just 10 years ago, that you would have just one single
system, just give the rules of any ball game, and perhaps we don't know because you need to do all
of the experiments, but perhaps for a wide range of programs, you have a system that is much better
than humans for all of them. This goes in that direction of bottom up, which is a very interesting,
so you're getting more general. How general you can get, how interesting is to have something
like a very general system without, by as I told it, was one of the topics for discussion
yesterday. I think that in some case we have this discussion about the kinds of tasks we are
interested in, and some people say we are interested in those tasks and humans are well at,
a good at, but perhaps there are some other tasks that people are very bad at, and they are really,
really interesting as complementary to what we are doing, so it's not easy to map this space,
but I think that's perhaps a very important challenge, and it's, I wouldn't say that we are
playing with fire, but we are doing a lot of great things with a really understanding where we
are, and perhaps just train to analyze the connection between tasks. What the tasks have in common,
because we say, okay, test and go have something in common, but can we extrapolate what alpha
star is able to do with robotic navigation? It was a completely different system. You can,
you can be used some of the ideas, but of course, this system is not meant to solve this kind of
task, it's not generally not, but the human, even if it is not that good, is not optimal
for any of the stacks, and so we go to this idea of intelligence being second best at everything,
you just have to be the best, and we move away as well from this idea that your child is a genius
at something, but very bad at all the rest, so intelligence is something in the middle, you are
not the best for everything, but also it's not this idea, you are very good at just one thing,
because you wouldn't call that intelligence, that's the narrow approach to AI, so I think this
is basically the interesting part to categorize what it is, what we have in the middle,
this kind of structure of intelligence and relation between tasks, and how can we,
how we can extrapolate from what the system is able to do to other tasks, and it's a question of
applicability in machine learning and AI, sometimes what about, I have this task, can I use your
technique, and you have all these discussions you will need to train, it's not only that you have
to train all the hyper parameters, sometimes you have to change the architecture completely,
and we have all these discussions about general IT in AI, machine learning, and so this landscape
tries to give some kind of conceptualization about this, of course, I don't think there's going
to be one way of looking at this landscape, but if there are different proposals in some areas,
at least there's some, you can say, okay, what about a number of examples that you need,
a training course, you can put some dimensions that you can plot some systems according to
these dimensions, okay, I'm here, and I want to go there, and humans are present at that corner,
and we know where we are, but we need to do more of this, and that was one of the motivations of
the symposium, and also some of the projects that we have in the CFI as well.
And so where does measurement fit in, and how does that led you to a specific set of research
that you work on? Yeah, I think measurement is everywhere in science and technology.
Sure. So for me, it was a kind of a surprise to say, okay, how much we progress in AI,
especially in machine learning, and we don't have very good measurement tools. Of course,
we have tasks, and we can say, okay, accuracy, or some other metrics.
Task performance, for example. Yeah, with different, but where is that lacking?
Yeah, the thing is that the question about measurement here is measurement for which tasks,
and that's the main question. Just for one single task, you can put some kind of a metric of
performance, and this is going to work well. And there's no objection if even you have a utility
function, or even just some cost function money associated with a task. That's perfectly okay,
for instance, self-driving car. I think that's complex to find a good metric, but it's clear.
You can say, okay, these routes are more frequent than others, accidents, spinny things like that.
You can put all of these in a formula and say, yeah, I want to maximize this. Yeah.
But for some other systems we want them to be more general, we don't have a, we want to say,
we want a system that has very good variable abilities. This is too abstract.
We don't really know how to do that. Well, so we can have a good translator.
Even for machine translation, there's a strong debate about the evaluate metrics,
whether what you get is readable, or you get the idea, or what kind of mistakes are worth.
Because we are entering an area where we have to talk about meaning and interpretations,
human interpretations, about whether I think that this is a good translation. Of course,
you can always say, okay, I can just perform a lot of translations and have a human access
these translations from zero to ten, or something like that. And you can have a metric.
But in that case, you are not sure what you are really evaluating.
In contracts where the, for instance, the self-driving car, which is kind of objective in a way.
So we find that even in robotics, there were some discussions, for instance, in the,
there were, there have been several European Union projects on robotics. And one of the big meetings
that was someone who said, okay, we are progressing. And can we really measure that?
You say you saw this task, but we still have this task without being solved. And the system
solved this task, but not the other task. So we have more, because we solve more tasks,
because we have more robots. So a more specialization, but not really because we have one robot
that is able to solve. And in terms of efficiency and economy, what we want in machine learning is
to have systems that are able to solve a range of tasks without a lot of tweaking and tuning
and changing architectures, because that takes a lot of effort from teams. And we want to have
systems that are easier to develop. They require, for instance, less data, all of these things.
So having metrics around all of this is going to give us a better assessment of whether we are
really progressing. And especially at conferences, or when we have one of these breakthroughs in AI,
really understand whether this is a great breakthrough. For instance, when I saw Alpha, Alpha go,
I was impressed like everybody had one. Come on, this is, wow, this is so good. And
but that was, for instance, the other day, I was more impressed about this system just learning
go test, and this Japanese go, this Japanese test as well. And I was really, I think for me,
this is even more important than what we saw two years ago. Right. And well, this is my view.
And at some point, we would like to have some kind of metrics where we can say, okay,
can we say something about what these systems are able to do and put that some kind of
even quantitative assessment. So talking more in terms of skills and abilities rather than
specific tasks. So that's basically the interest in measurement. And now all of these new trends
about evaluation using video games. I think this is really, really interesting. But again,
grab that work. Is that like related to the reinforcement learning type of work? Yeah, we are having many
platforms. For instance, we have Microsoft research, launch, my graph, which is my more platform.
We have good AI release these, well, now it's integrated in the AI universe. Open AI universe.
Open AI universe. Open AI universe. And then you have deep mind where there are also
the evaluation platforms and also some competitions using video games. And we also have these
the Atari games. And so we in a way, we see many evaluation platforms where we have a range of
tasks as different video games. And we see that our systems have been evaluated according to how
well they behave for this range of games. It seems like relative to some of the,
you know, relative to more kind of higher level or conceptual definitions of intelligence,
performance on a video game is more akin to evaluating a self-driving car. What's my score?
How fast did I do it? How many times did I die? Things like that. What makes these video game
platforms interesting for the types of measurement that you want to get to? Yeah, that's a very good
question. But I think that in a way, a self-driving car is, even if it, perhaps, it requires
more technology, more different technologies than, for instance, a video game. A video game is a
mini-world in a way. It's an environment. Of course, the self-driving car is an environment,
but the thing is that for video games, if you just define one video games, I see no difference,
even, of course, that the self-driving car is much more complex, much more into it,
in terms of the technology that you need to solve it. But the good thing of video games,
you usually include 20, 50, 100 video games, and then you have a range of tasks. And then we have
a range of tasks, different tasks, even if they're still, they are 2D or 3D, and there are some kind
of a shared input output for these platforms. There's the kind of a... So you're speaking to
being able to kind of demonstrate generalizability. Yeah, that's the idea. So you tried these games
to be as diverse as possible, in order to show that you're able to learn these tasks.
Then if your system is good at all the tasks, or just kind of, or even if it's not optimal at any of
them, I think you have a kind of a general performance for these range of tasks. And the more general,
you make these pool of tasks, you go up in this skill from bottom task specific performance to
kind of skills, or maybe to towards you go up in this direction of more general intelligence.
And that's what you can claim that even if you take 100 games, Atari Games, that's very specific
subset of all the possible tasks that you might have. And that's true. I think you can explore these
first steps from bottom up from very specific tasks to more general. And you can also analyze all
of these ideas about why is it that this system is good at these tasks, but not at those set?
Because then we'll know these tasks are more about abstract thinking or planning. So we see
that for us how we're reinforcement learning techniques, even if we are using deep learning with
them. They are not very good at generalizing for these, so they are not solving these tasks.
Well, so we can learn much more than we've just focused on one single problem that in the
end, we put a lot of effort, we'll ace at that problem, that perhaps how much of that effort
is extrapolable to other problems, that's basically what we want. We want this generalization
ability in machine learning. Have you published any research that specifically looks at
applying measurement to these video game scenarios? I've taken two different approaches. There's
the idealistic approach, which is basically going from first principles, which is basically
in my book and some related publications. How can we define a set of tasks that, by definition,
are necessary to show that the system has this ability? That will be great, because of course
you can say, okay, why 100 games? Why don't you use 1000? Why are these 100 games sufficient
for this? You can decompose the games to these set of map them to these set of tasks and
determine this game framework's ability to even assess intelligence at all. Is that one of the
things? Yes, there is. We can generate these tasks, rather than we use some of these platforms
generating games. If you generate a game just randomly, you get something that is completely
meaningless. Usually, and it's not fun to play. You have a lot of random noise. You have to
pull some structure into an environment to make it appealing for humans and meaningful,
or even for machine learning. The thing is, how can we generate these tasks in a principle way,
so that we ensure that a key concept here is that you can define the notion of difficulty of this
task, so you can have a scale of difficulty as well. You can relate these tasks in terms of
that difficulty and what they have in common. The problem about this approach is that you can only
generate, at least in principle or easily, you can generate some kind of very abstract environments,
a very simple environment. That's a very good way, but perhaps it might take more efforts and more
ideas just to make it work. In practice, and the other one, which is summarizing some papers,
for instance, last year at the European conference on AI, we had a paper on that,
and it's in some other machine learning conferences. It's how to use, for instance,
one technique from psychology, from psychology, with this item response theory,
where you can characterize from a range of tasks, for instance, you take a competition.
This is the video game playing competition. You take that, you take the results from last year,
and you analyze the, you have, for instance, a 40 task, and you have a 30 competitors,
and you analyze, you just try to understand not only the performance of each of them,
but you try to extract latent variables by analyzing these result matrix,
and you try to analyze, okay, can I add to the difficulty of this task,
or the relations between these tasks? Can I analyze the relation between the participants as well?
And at the end, you can get, for some of these, and you can have something like the difficulty
of the task, and then you can realize these tasks, according to this population of AI systems,
or machine learning systems, this is the most difficult task, but not because a score is 101,
the other is 80, but just how challenging it is for the systems, and you can also determine
the ability of the agents. Of course, this is simplification because you can extract two latent
variables that you could do 10 latent variables to be, but these latent variables explain the
behavior of these systems for this set of tasks, and it gives you information, for instance,
the next competition, which tasks are completely, you get some other preferences, you can extract
a third parameter, which is discrimination, you say, okay, these tasks are completely useless,
because, or sometimes you have some kind of a negative discrimination, this task is performed
well by the, by the bad methods, and the other way around, they say, okay, so what's the point
of having this task here? And that's, that's something inherited from psychomedic, we have a question
that you say, okay, good students are typically bad at that question, you remove the question,
if that doesn't make sense, there's something, there's a catch there or something, because people
get confused, because otherwise you wouldn't get an explanation. So all of the things help us
to understand a little bit better, what is going on, especially when we have competitions,
when we have benchmarks, and we've also applied that to the Atari Games results,
and see what's happening with some of the games and the difficulty of the games, and we can have
a better understanding of the abilities on the problems of current technology. Of course,
this is, this is not for first principle, this is experimental, but at least gives us more
understanding of what's going on that just looking at the results and the winner of a competition.
So it's a set of methods for essentially decomposing these results into kind of generating the tasks
themselves, or inferring the underlying task and their difficulty from the, the results,
is that the right way to think about it? It's a question of trying to describe the tasks,
and also the participants, which is kind of a latent variable that you put into the models and
that you estimate through the results, and you say, okay, and these latent variables, you can call
them difficulty, discrimination, and on the side of the techniques, you can call that ability,
these factors of variables help you understand what's going on. Is there something that you do in
some other, in some other scenarios? It's nothing really new for it's, but it gives you more
understanding just the, just looking at the matrix, or just a scale that you get from the performance.
For many of these games, it doesn't make sense to make it an average of the results.
Some other, in many papers, you have this comparison with humans, which is okay.
For instance, you have 50 Atari games, and you have one technique, one machine learning technique,
and you have the results for each of them, and then you have the humans, the average human.
Basically, we have use, or we are on Amazon Turk, or whatever they have, and they have
this average score, and you say, we are superhuman on task one, we are, we are subhuman on task two,
well, that doesn't say too much to me in a way, because well, we are not able to solve this
task in terms of the human level, but we are not comparing also the number of games that we
are given to humans and the number of games that we agree with. So it's not a fair comparison
in the first place. It doesn't allow us, either to say, we are, there are 50 games, we are
above human on 40 of them, and we are below human on 10 of them. Perhaps as an assistant is 39
11, and it's much better because the difference are much better. So we have a lot of discussion
about how to integrate things that are not commensurate in a way, when we put many, many tasks
together. Okay. And all of these questions appear again and again, especially benchmarks,
competitions about people typically would like to have one kind of a single score or something like
that. And looking at all these specific scores is you don't get a lot of meaning, but you just
simplify over, simplify to a single average score, something like that, and you probably, you miss
a lot of information as well. So something in the middle that you can summarize the behavioral
of these systems for this set of tasks that help us to understand whether we are going in the right
direction, we are still very far from solving the problem that we want to solve. Okay.
And all of these things. Interesting. So how about quickly kind of a quick overview of the
you know, the future directions and priorities? Where did that conversation end up?
Yeah, it was a very open discussion yesterday. When you set up a session with what society needs,
it's like, well, everyone has a say here and we have many questions as well. And we had a panel
for 40, 50 minutes. We was a very long panel. A lot of things we touched upon many, many topics.
But there was this idea of having more human-like AI rather than complementary or not to what
humans are able to do. I think that there's no agreement there. Because computer science and AI,
one of the main goals have been automation of tasks. So of course it's good that the machine's
able to do things that we can't do. That's great. But we are also interested in some tasks that
can be automated and perhaps we can do some other things. But of course you have a lot of
implications about that. And that these idealists view that we want humans and machines to be
complementary ethics. It's very idealistic. And there's a kind of that. But perhaps we can focus
on these things rather than just so human-like intelligence is important. Another way finding
things that machines can do that we can't that is not really human-like. This is also a very
good direction. There was also some more political questions about domination, power,
for instance, corporations and organizations having a lot of power. There was a discussion
not really in terms of AI companies and all that, which is a different way that we didn't touch
them. But about this idea that in the future intelligence has changed our planet.
And having more of that intelligence or different kinds of ways of extending the intelligence
that we know of at the moment is going to transform everything. So we have a debate on all of
these things that might be transformed. And also ethical things about there was this discussion
about buyers in machine learning. So we had a little bit of everything with about 40 minutes
of discussion. Oh, wow. Do you know if the symposium was recorded? Will people be able to find
it afterwards? Yeah, we've been told that it has been recorded. It was recorded. There was a
camera there. So well, probably it will take a while to have it online. But we hope that everything
will be online soon. Okay, great. It sounds like a really wide-ranging and interesting set of
questions that it raised. And I hope to get a chance to take a look at it. Thank you Jose
for spending some time to chat with us. Any final thoughts or ways that folks can catch up with you
or find out more about what you're doing? Well, we are having some workshops next year. But
just if you Google my name or you will find some of the things that we are working on and some of
the projects in the past years and now with the center of the future intelligence. There are many
things, many interesting things going on there. Okay, awesome. Well, thanks so much. That's for
having me here and it's been a pleasure. All right, everyone. That's our show for today.
For more information on Jose or any of the topics covered in this episode, head on over to
twimmaleye.com slash talk slash 137. Of course, thanks so much for listening and catch you next time.
