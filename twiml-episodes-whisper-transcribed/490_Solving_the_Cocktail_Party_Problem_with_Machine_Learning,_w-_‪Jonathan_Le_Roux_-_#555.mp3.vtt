WEBVTT

00:00.000 --> 00:16.000
Hey, everyone. I am here with Jonathan LaRou. Jonathan is a senior principal research scientist

00:16.000 --> 00:21.920
and Mitsubishi Electric Research Laboratories on Merle. Jonathan, welcome to the Twimal AI

00:21.920 --> 00:25.040
podcast. Hi, thanks for having me.

00:25.040 --> 00:33.960
So we'll be digging into your work in the speech and audio field broadly. And to get us

00:33.960 --> 00:38.680
cut up, I'd love to have you share a little bit about your background and how you came to

00:38.680 --> 00:41.200
work on applying ML to those problems.

00:41.200 --> 00:47.580
Sure, yeah. So my background actually started with mathematics. I studied math, the pure

00:47.580 --> 00:55.420
math for a while until my master's and then did a master's with Cedric Villaini, who later

00:55.420 --> 01:01.940
got the fields metal. And I was supposed to do my PhD with him. But I wanted to do a gap

01:01.940 --> 01:07.060
here in China. So I need to, because I love, I studied Chinese and I like languages.

01:07.060 --> 01:11.020
So after spending a year in China and I got back, he said, I'm going to Stanford. He

01:11.020 --> 01:14.780
met as well as spending over here somewhere else. So I decided to go to Japan because I met

01:14.780 --> 01:21.140
friends there in China, Japanese friends there. And I started studying Japanese and then

01:21.140 --> 01:28.460
I realized that maybe math, pure math wasn't actually so much my thing. And I fought, I looked

01:28.460 --> 01:34.460
around what I could do and I thought that math and language is kind of mixed well. And

01:34.460 --> 01:40.180
I also, I was really into music and, you know, I said playing with in a band with friends

01:40.180 --> 01:44.940
and I thought like kind of the intersection of all his interests was the speech and audio

01:44.940 --> 01:50.300
field. And I got introduced to a professor at University of Tokyo, who was in there.

01:50.300 --> 01:56.060
And so I was able to do my PhD with him and with a professor in Paris. So that's kind of

01:56.060 --> 02:00.500
how I got into the field of speech and audio. And I'm really happy I did.

02:00.500 --> 02:06.180
That's awesome. That's awesome. And a lot of your work is focused on this classical problem

02:06.180 --> 02:12.700
in the field called the cocktail party problem. Can you tell us a little bit about the background

02:12.700 --> 02:18.420
of that problem and, you know, some of the ways that you've kind of built your research

02:18.420 --> 02:19.900
program around it?

02:19.900 --> 02:27.740
Sure. So yeah, I worked and my core of my work is centered around a social separation, audio

02:27.740 --> 02:34.020
social separation. And in the field, the kind of the holy grail is the so-called cocktail

02:34.020 --> 02:41.220
party problem, which was kind of stated in that name by Cherry in 1953. And it's kind

02:41.220 --> 02:46.500
of, if you imagine yourself in a cocktail party and we humans have this amazing ability

02:46.500 --> 02:52.980
that we are able to entertain a conversation and kind of to focus on pretty much any given

02:52.980 --> 02:58.260
sort of sound that we wanted to focus on. And despite all the background noises and the

02:58.260 --> 03:03.580
reverberation and all these interferences that are happening in this very cluttered

03:03.580 --> 03:12.460
acoustic scene. And so what we've been interested in in my group over the years has been to

03:12.460 --> 03:20.340
kind of tackle this problem, to chop at it. And it's first by working on speech enhancement,

03:20.340 --> 03:28.340
which is the separation of speech from noise. And then later on we worked on speech separation,

03:28.340 --> 03:37.420
which is separation of speech from speech. And this was kind of a big milestone because speech

03:37.420 --> 03:40.860
and noise have very different characteristics. And so you can kind of use this at your

03:40.860 --> 03:45.100
advantage to separate them from each other. You can use machine learning to learn these

03:45.100 --> 03:51.900
characteristics and then kind of classify one and one from the other. When it comes to

03:51.900 --> 03:58.180
speech from speech, then it wasn't clear really how you would handle this. I mean,

03:58.180 --> 04:04.980
because they have, by definition, the same kind of characteristics. So the main challenge was

04:04.980 --> 04:12.100
how to deal with that problem. And so that's kind of where we started developing methods to do

04:12.100 --> 04:20.580
this efficiently using deep learning. Nice. And is the ability to separate speech from noise or

04:20.580 --> 04:29.300
speech is that a necessary prerequisite for a machine to be able to attend to a particular speech

04:29.300 --> 04:36.100
signal strikes me that we don't necessarily separate we attend to. It's a good question. I mean,

04:36.100 --> 04:41.540
it's actually a very good question, whether we humans actually effectively separate in the brain or

04:41.540 --> 04:51.300
not. And I think there is some some hints that we are actually to some extent separating the

04:51.300 --> 04:56.820
signals. Okay. And maybe we're not separating all the signals. It's more like an attention mechanism

04:56.820 --> 05:02.020
where, you know, like the higher part of the brain kind of guide the lower parts of the brain to

05:02.820 --> 05:08.660
decide which features which parts of the sound to focus on and to cancel other ones. So we're not

05:08.660 --> 05:15.060
completely separating a scene as a machine maybe potentially could do. But there is some hints

05:15.060 --> 05:22.020
that in neuroscience that's kind of kind of part of the process. It's not necessary. You're right.

05:22.020 --> 05:28.740
Like you could totally design a speech recognition or somebody's speech recognition algorithm that

05:28.740 --> 05:35.140
does not explicitly separate before recognizing noisy speech or even multi speaker speech. And

05:35.140 --> 05:40.260
that's actually something that we did do. And we're not the only ones. So we,

05:42.420 --> 05:48.020
basically, even nowadays, I would say the state of the art in like kind of single channel,

05:48.740 --> 05:54.580
noisy speech recognition does not necessarily involve enhancement. It's sometimes surprising.

05:54.580 --> 06:01.380
Even though it sounds much, much better, it's often a better strategy to just include more noise

06:01.380 --> 06:06.980
in your training data and to let the neural network that does the transcription, you know,

06:07.860 --> 06:12.740
be robust to noise inherently by the version of training instead of like, you know,

06:13.460 --> 06:17.940
thinking that you're doing the network of favor by removing the noise when you're actually

06:17.940 --> 06:24.740
probably introducing some artifacts that are bothering the system. So it's not always the best

06:24.740 --> 06:31.380
strategy. But it, I mean, going forward, I think as the enhancement algorithms get better and

06:31.380 --> 06:37.700
better, it's likely that it will become probably probably the state of the art to include some form

06:37.700 --> 06:43.940
of enhancement. Got it. Got it. Maybe a good way to talk through your work in the field is to start

06:43.940 --> 06:52.660
with a recent paper. And as you kind of describe what you're doing there, you can talk through some

06:52.660 --> 07:00.500
of the work that's led up to it. And so that recent paper is a new formulation of the cocktail

07:00.500 --> 07:06.180
party problem that you call the cocktail fork problem. What does that mean? Yeah, so that's,

07:06.180 --> 07:13.620
we tried to be a little bit cute there. And so basically this project started with myself,

07:13.620 --> 07:18.500
not being an English native speaker and trying to listen to movies and not always being able to

07:18.500 --> 07:24.020
catch a dialogue with all the special effects and the music. And I was like, and I really like

07:24.020 --> 07:29.380
to be able to just, you know, separate the dialogue and enhance it. And some some smart TVs,

07:29.380 --> 07:33.860
you know, claim they can do this and they can do some extent, but they mostly rely on, you know,

07:33.860 --> 07:41.140
some form of equalization. They don't actually separate the sources. And so we thought, well,

07:41.140 --> 07:45.860
you know, we have all these separation technology, they can separate speech and speech. And now

07:45.860 --> 07:52.900
nowadays we also can separate, you know, multiple types of sounds from each other, like for like a car,

07:52.900 --> 07:59.940
from a, from a belt, for example. And it was kind of a natural thing to try to see, well, can we

07:59.940 --> 08:06.580
just, you know, formulate this problem as to separating complex acoustic scene into like speech,

08:06.580 --> 08:13.540
music and sound effects or sound events. And that's basically what we did. Like we,

08:13.540 --> 08:21.860
it's like carefully crafted a dataset that of that uses these free types of sources and try to

08:21.860 --> 08:30.820
replicate with as much realism as we could the mixing process of like real soundtracks, like movie

08:30.820 --> 08:36.420
or TV show soundtracks. So in terms of loudness or in terms of amount of overlap. And to be

08:36.420 --> 08:43.380
able to training set basically. And then we trained our kind of somewhat standard algorithms

08:45.060 --> 08:50.100
for separation. And we treat them a little bit in order to account for the specificity of a task.

08:51.300 --> 08:58.100
So that we called it the cocktail fork problem because there's free outputs. And cocktail

08:58.100 --> 09:06.340
forks happen to have three outputs, free branches. So if it wasn't a nice hint at the cocktail party

09:06.340 --> 09:10.020
problem to, to call it the cocktail fork problem because it's a, it's a tiny, it's a tiny,

09:10.020 --> 09:19.300
easy version of the cocktail party problem. And so the, given that you started with a

09:19.300 --> 09:29.860
kind of an inorganic training set, you, you created your training set from his component pieces,

09:30.500 --> 09:34.820
you know, thus bypassing one of the, you know, might otherwise be the hardest part of,

09:34.820 --> 09:42.980
of training a model here. I imagine that the, on the back end, the generalization was a big

09:42.980 --> 09:49.540
challenge in building the model. It turned out that it actually generalizes pretty well. So we,

09:50.420 --> 09:55.540
the demos that we put online, we're trained on, only on this synthetic data set. And, and we,

09:55.540 --> 10:01.060
we, we show examples that we got just from taking YouTube videos like movie trailers or TV shows

10:01.060 --> 10:07.780
and applying the network pretty much as, and exactly as it was. And it, it, it does make mistakes.

10:07.780 --> 10:12.820
But it does generalize pretty well. And actually an interesting thing is, at first, we,

10:13.540 --> 10:18.180
we trained the network at 16 kilohertz something rate. So we, we downsample the signals. So,

10:18.900 --> 10:25.860
because it was lighter weight easier and faster to train and to try things. And interestingly,

10:25.860 --> 10:33.540
we, we first used a wrong version of the speech data set, but was downsampled with a slightly

10:33.540 --> 10:38.740
too harsh band pass filter, like low pass filter. And it was, so it was leading out a bit of,

10:38.740 --> 10:45.380
of frequency, of the higher frequencies of speech. And when we applied it to, to these real videos,

10:45.380 --> 10:49.060
we, there was a lot of bleed from the speed, the high frequencies of speech, a very

10:49.060 --> 10:54.180
squeaky speech in the other sources like, and we're like, what, what's going on? And, and actually,

10:54.180 --> 10:59.380
we realized that actually we, the version of the speech data we had was not full band. And,

10:59.380 --> 11:03.700
and when we, we trained our models on, on a better quality data that it just went away.

11:04.740 --> 11:08.500
So kind of interesting that like, if you, if you listened to it, you wouldn't notice,

11:09.300 --> 11:14.820
but the network definitely was not able to learn how to separate the high frequencies for speech.

11:15.460 --> 11:21.220
Oh, wow. It did the model. Is the model language agnostic?

11:22.420 --> 11:25.940
So we only trained it on English, but we found that these,

11:25.940 --> 11:32.180
these speech separation models are pretty language agnostic. A few years back in 2017,

11:32.180 --> 11:40.660
we had a, we did a, a demo of speech separation. And that was trained on 30 hours of English,

11:40.660 --> 11:47.860
you know, world-sweet journal, red sentences speech. And we were, this was a live demo in Japan.

11:47.860 --> 11:54.340
And, and we were separating, you know, English from Japanese for, and Japanese from French,

11:54.340 --> 11:59.700
right? Like, people who visited the demo tried every crazy combination. And it worked pretty well.

11:59.700 --> 12:03.140
I mean, it's really, it's not picking up on the language content. It's really the acoustics.

12:04.180 --> 12:11.460
And when it's, when it's separating, how clean is the separation of the language? If you,

12:11.460 --> 12:16.260
you listen to the separated language, do you hear artifacts of whatever might be going on in

12:16.260 --> 12:22.500
the background or... So it depends how you trained. Yeah, back in the back then, we only used

12:22.500 --> 12:30.020
clean speech for training. And actually, it is still a challenge in the field to, to expand,

12:30.020 --> 12:36.580
like to extend these methods to, to work well on noisy and especially, especially in reverberant

12:36.580 --> 12:44.020
conditions. So like the, the data set that we put out in 2016, when we first came up with this

12:44.020 --> 12:48.180
method called deep clustering, that kind of, kind of revived the field of speech separation. Because

12:48.180 --> 12:52.660
the first one that uses deep, use deep learning, you know, way that it could separate unknown

12:52.660 --> 12:57.380
speakers. You didn't have to train on a specific pair of speakers. It could be anybody. And

12:58.340 --> 13:03.380
we created a data set to do this based on the Warsaw Journal. And this is still used to this day,

13:03.380 --> 13:11.060
but it's kind of completely beaten up, like the performance on that clean data set has become

13:11.060 --> 13:18.500
so amazing that it's, it's pointless. It's like super clean over 20 dB. And so there's no,

13:19.380 --> 13:25.540
I think we solved that task, not we, but the field. The community has solved that task. And

13:25.540 --> 13:31.140
the challenge now is to moving to more challenging scenarios with noise and especially with reverberation.

13:31.140 --> 13:37.460
So we put out a couple data sets to encourage for a community to, to move on to that. We call them

13:37.460 --> 13:46.820
WAM, the Wall Street Journal, WSJ hipster ambient mixtures. And it's like, like the band WAM.

13:48.580 --> 13:54.340
And it's, people are now using this, this data set as well, to try, try the algorithm in a

13:54.340 --> 13:59.060
more challenging scenario. The performance is, of course, much worse in reverberant and noisy

13:59.060 --> 14:03.620
conditions. And when you say reverberant and noisy, you're talking about when the training,

14:03.620 --> 14:10.580
the speech training data has noise. Both for training and the tests. So typically, like if you

14:10.580 --> 14:16.980
don't train on data that has noise and reverberation, that could be a completely fail. But even if

14:16.980 --> 14:21.780
you include noise and reverberation in your training data, the performance is clearly not as good

14:21.780 --> 14:27.140
as in clean conditions, because the noise, but especially the reverberation really smears the

14:27.140 --> 14:32.180
characteristics of the speech, everything that is normally in a spectrogram. So in a time frequency

14:32.180 --> 14:37.380
representation of your signal, where you show the energy at each time in frequency, speech is very,

14:37.380 --> 14:41.140
very clean. You can see the patterns. But as soon as you introduce reverberation, everything

14:41.140 --> 14:50.180
gets smeared. And so it becomes much harder to first to, for the network to identify the structure.

14:50.180 --> 14:56.180
And then there's more overlap between, because everything gets like mushed. So there's more overlap

14:56.180 --> 15:02.340
between the different sound sources. So you imagine like, let's say imagine like on a spectrogram,

15:02.340 --> 15:08.900
typically you have stripes when you have harmonic speech tends to be, there's a lot of harmonic parts

15:08.900 --> 15:13.700
in speech and these show up as stripes on a spectrogram. So you imagine stripes and you have two

15:13.700 --> 15:20.900
sets of stripes. If they are very clean, you can probably identify them and then make a mask that

15:20.900 --> 15:25.700
cancel one of them and only keeps one. But if both stripes are kind of smeared and become

15:25.700 --> 15:30.420
like kind of mushed, then there's a lot of portion but overlap and it's also harder to see the

15:30.420 --> 15:37.540
stripes in the first place. So it makes it makes it difficult both to identify structure and it's

15:37.540 --> 15:41.620
it's, it's, it is actually, there actually is more overlap between the structures.

15:42.340 --> 15:52.820
So when you, so the way you've set up the, the cocktail fork problem and maybe this is general

15:52.820 --> 16:01.460
to my question is general to other cocktail party problem applications. You've got speech and you've

16:01.460 --> 16:08.420
got, you know, other, you know, background things. It could be other speech or it could be other,

16:09.780 --> 16:16.020
you know, the movie soundtrack and sound effects and the like. And then you pass it through this

16:16.020 --> 16:24.900
model and you have a speech stream that comes out on the other end. And I, I guess independent of

16:24.900 --> 16:33.460
whether you are working with clean speech or noisy speech on the back end, I'm curious how clean

16:33.460 --> 16:47.940
the output speeches or I'm trying to, I have a, a picture in my head of something that is doing

16:50.260 --> 16:55.140
I don't even know how to describe it like you, some other application that's similar to this kind

16:55.140 --> 17:00.340
of separation, but I don't remember where it's from. But like you get the speech, but there's also kind

17:00.340 --> 17:05.860
of this warbling noise in the background that there's a, in, in, in like kind of more,

17:06.740 --> 17:13.860
more conventional, I would say, separation methods. There's this artifact that's kind of like

17:15.460 --> 17:19.700
that are kind of added to the speech. That's something that's typical of deep learning

17:19.700 --> 17:26.580
based methods. We don't observe that much actually. Interestingly, yes, there's still artifacts,

17:26.580 --> 17:32.980
I mean, but they don't sound musical mode is not such an issue in deep learning based methods.

17:32.980 --> 17:40.100
And I'm not sure I have a good insight for why. But yeah, it's sort of like these methods like

17:40.100 --> 17:44.740
introduce sort of a spread out noise across the structure, but doesn't really have a structure,

17:44.740 --> 17:50.660
but then that kind of creates that kind of musical noise. I would say the artifacts in deep learning

17:50.660 --> 17:57.380
based methods are, I mean, most of the time it's really failure to separate like, or you get,

17:58.100 --> 18:02.340
when you're doing speech separation in an era that we very often get is that the speaker,

18:03.540 --> 18:06.900
if you listen to each, because you have two speakers and you separate them,

18:07.860 --> 18:12.180
the speakers actually separate that at any time instant, they're pretty well separated, but,

18:12.180 --> 18:17.460
but halfway through, the other than makes a mistake in how it's stitched thing and then it switches

18:17.460 --> 18:23.780
the speakers. And so if you listen with headphones, you're like, you suddenly have the speakers

18:23.780 --> 18:30.420
to switch sides, they're separated, and that can happen multiple times. So this kind of speaker

18:30.420 --> 18:37.620
permutation problem is one of the issues that we have typical issues that these systems have.

18:38.740 --> 18:42.180
And so people have tried to come up with methods to alleviate that.

18:42.180 --> 18:51.300
But yeah, that's a, invest to a lot of artifacts. It tends to work pretty well,

18:51.300 --> 18:57.060
except when it completely fails. That's a pretty typical thing in deep learning, I would say.

18:57.860 --> 19:06.420
And you've talked a little bit about the spectrograph representation of speech.

19:06.420 --> 19:12.340
Do these models tend to work like in the frequency domain or a time domain, or like,

19:12.340 --> 19:18.100
how do you think about the way they're working? So there are multiple approaches in speech

19:18.100 --> 19:23.940
separation. Historically, a lot of them were based in the time frequency domain. So what we do

19:23.940 --> 19:30.260
is we take, you know, short snippets of signals, we call frames. And so we just put a window

19:31.140 --> 19:35.060
a little bit of signal and do a four-year transform to analyze a frequency content,

19:35.060 --> 19:41.700
of that's more window. And so that gives us a vector. And then we have a bunch of these vectors,

19:41.700 --> 19:50.100
and this is what makes the spectrogram. And historically, people only mostly treated the,

19:50.820 --> 19:54.900
these numbers are all complex numbers, by the way. The free transform gives you a complex number

19:54.900 --> 20:02.420
at each time frequency. And historically, people have looked mainly at the magnitude at the length

20:02.420 --> 20:11.060
of that vector, another of that complex number. And the phase part, the reangle of the complex

20:11.060 --> 20:14.980
number was kind of left as it is because it was thought to be pretty difficult tomorrow.

20:15.940 --> 20:22.340
And if you look at a picture of a magnitude spectrogram, so where you don't consider afraid,

20:22.340 --> 20:26.660
the phase looks very random, but the magnitude looks very nice with these stripes that I was

20:26.660 --> 20:30.420
mentioning. So it feels like you can do something about it. You can do machine learning on it.

20:30.420 --> 20:36.500
And that's what people have been doing. And still to this day, there's a lot of methods that only

20:36.500 --> 20:43.700
handle the magnitude part, like for example, with some transform, like a frequency transform,

20:43.700 --> 20:49.060
like a, what we call mail transform, to gather some frequencies according to some filters.

20:49.060 --> 20:55.380
And that's what is used in, in speech recognition, for example. But, so we started with that.

20:55.380 --> 20:58.980
And for many years, most of the methods were based in the time frequency domain,

20:58.980 --> 21:07.060
nowadays, the state of your methods, either deal, now include the phase, they include both

21:07.940 --> 21:13.540
the model of a full complex number in the time frequency domain, or they ditch, they do not

21:14.180 --> 21:19.460
explicitly use a time frequency transform. They go straight from the time, the waveform,

21:19.460 --> 21:27.060
the time domain signal to the desired time domain signal. Inplicitly, they are kind of learning

21:27.060 --> 21:36.180
the time frequency transform. So instead of using the usual Fourier transform, they use a filter bank,

21:36.180 --> 21:42.180
a bank of filters, and they let the network figure out what these filters should be. So it's very

21:42.180 --> 21:47.300
similar. And it's implicitly based, still based in some form of time frequency representation,

21:47.300 --> 21:52.020
but it's fully learned. So we see both types of methods. Can you talk a little bit about the

21:52.020 --> 22:00.260
the model and architecture that you used in the cocktail fork problem? Sure. So that more

22:00.260 --> 22:06.100
actually, we took from kind of state of the art model from music separation called Trust and Mix.

22:07.460 --> 22:16.100
It was introduced by Sony. And, and it's based on a recurrent neural network,

22:16.100 --> 22:25.620
a bi-directional long short term RNN, so BLCM. The its characteristic, what kind of makes it a

22:25.620 --> 22:32.340
bit different is that in their model, you start from a single, start from the mixture. And then

22:32.340 --> 22:40.260
you have multiple branches that have different, different transforms, different layers

22:40.260 --> 22:48.980
that go through that transformers signal in ways that the network can decide. And then they

22:48.980 --> 22:55.700
average this transform. And this goes through some BLSTM layers. And then they average again,

22:56.340 --> 23:04.100
the outputs. And it's kind of this averaging that's between various branches that is the

23:04.100 --> 23:14.020
special sauce. And so we took this. And the only thing that we changed is that we thought that

23:14.020 --> 23:20.180
in the case of a cocktail fork problem, the signals that we deal with are so speech,

23:20.180 --> 23:26.820
sound effects, and music. And they have kind of different dynamics. They, like some very,

23:26.820 --> 23:31.860
especially with sound effects, you may have very short, impulsive sounds like bangs or claps,

23:31.860 --> 23:42.980
or you may have long harmonic sounds like sirens, for example. And the way you do your time

23:42.980 --> 23:48.660
frequency transform kind of dictates what characteristics of the signal you can focus on.

23:49.460 --> 23:57.220
So if you use a very impulsive sound, you want a very short window in order to be able to

23:57.220 --> 24:03.700
really analyze the frequency content of that very short, impulsive sound. If you have a long

24:03.700 --> 24:09.940
sinusoidal, you want a longer window in order to get a more fine-grained, like to narrow down

24:09.940 --> 24:15.860
the frequency, exact frequency. Otherwise, it's going to be more like a blurry representation.

24:15.860 --> 24:22.260
So the longer the window, the more fine-grained frequency resolution you get, the worse time resolution

24:22.260 --> 24:27.940
you get. Everything gets more blurred in the time direction. And vice versa. So depending on the

24:27.940 --> 24:35.060
characteristic of signal, your choice of window length is going to impact what you can see in the

24:35.060 --> 24:44.180
signal. So what we decided to do is that we already had several branches in the Sony

24:44.180 --> 24:50.980
cross end mix algorithm. And we decided to specialize each branch to have a different window length.

24:50.980 --> 24:56.500
So to make it able to focus on different characters, to kind of nudge it, to focus on different

24:56.500 --> 25:03.460
characteristics of the signal. So that was the main twist on that architecture that we had. And it

25:03.460 --> 25:06.900
did bring a small bit of significant improvement on performance.

25:07.700 --> 25:13.060
And does that window length end up being a hyperparameter that you have to tune for each of your

25:13.060 --> 25:23.780
sources? So yes, in the sense that we manually tried various combinations and looked at which one

25:23.780 --> 25:30.820
performed the best on a development set. Yes. We just tried some variety within a reasonable

25:32.020 --> 25:39.060
set of parameters. There's not many things to try. Got it. You kind of characterize this as like a

25:39.060 --> 25:47.620
toy problem or a subset of the cocktail party problem. Going back to the broader problem,

25:48.340 --> 25:55.460
how do you characterize where we are with regard to solving that problem with machine learning

25:55.460 --> 26:01.540
or deep learning? And what are some of the big research directions and challenges that folks are

26:01.540 --> 26:07.860
working on? Yeah. I would say it's a smaller version, but it's not the list. It's pretty,

26:07.860 --> 26:13.140
I would say it's an important application that is kind of surprising to me that nobody actually

26:13.140 --> 26:22.580
looked at this earlier, because it sounds like it should be useful for analyzing YouTube videos,

26:22.580 --> 26:26.580
for example. If you want to know what's going on in a YouTube video and maybe sometimes somebody

26:26.580 --> 26:31.220
speaking, but you really want to listen more to the sound events, then you would like to separate

26:31.220 --> 26:35.540
the speech from a sound events. And most, and sometimes you have music in the background where you

26:35.540 --> 26:43.220
don't care about. So we think of it as an important pre-processing step that we plan to use

26:43.220 --> 26:51.540
ourselves in our projects for this kind of multimodal content analysis. But to answer your question,

26:52.580 --> 27:00.420
I think the field is progressing very fast in the capacity that we have to separate general scenes.

27:00.420 --> 27:08.900
I'm very excited about the progress that's been made. And you know, there's a lot of work that's

27:08.900 --> 27:20.580
been done on general sound separation sound events. I feel like what is maybe what the challenges

27:20.580 --> 27:30.740
are, are the first, the realism of data, like how realistic can you, can you get, can you make

27:30.740 --> 27:38.020
your data? And because it's still very important currently for these algorithms to train in a

27:38.020 --> 27:44.420
supervised manner. And we're only seeing, starting seeing methods that can, that can be trained

27:44.420 --> 27:51.140
without super, like supervised data. So what I mean by this is the typical way to train these

27:51.140 --> 27:57.780
methods is to get a bunch of sounds, mix them together, and this is your input, and then you ask

27:57.780 --> 28:03.540
the algorithm to come up with the separated, isolated sources, which you already have,

28:04.100 --> 28:09.300
because you mix them together. So that's easy. The downside of that is that that mixture may not be

28:09.300 --> 28:14.340
very realistic. So, you know, we make some efforts to do it, but it's never going to be a real thing.

28:15.540 --> 28:22.340
And so recently some, some groups have developed unsupervised methods where you only have the

28:22.340 --> 28:28.020
mixture. And you don't have, you can't, you can't ask the algorithm to be as close as possible

28:28.020 --> 28:32.980
to the isolated source, because they don't exist. You don't have them. And so one of the methods,

28:32.980 --> 28:37.860
for example, was developed by my former colleague, John Hershey, and his colleagues at Google,

28:37.860 --> 28:47.540
Scott Wisdom, is called mix it. So what they do is they take two mixtures and they mix them together.

28:48.500 --> 28:55.060
And so they have the original mixtures. And what they have the algorithm do is very, very simple

28:55.060 --> 29:02.900
bit of a clever is they have, they ask the algorithm to separate all the sources in this mixture

29:02.900 --> 29:11.460
of mixture. And you have many, you have many sources. And then they judge how well this happened

29:11.460 --> 29:18.500
by picking the best combination that comes back to each of the mixtures. And because the algorithm

29:18.500 --> 29:22.820
doesn't know which sources mix with each other, it has to separate all of them in order to be

29:22.820 --> 29:27.460
able to reconstruct the mixtures. So that's very simple and works pretty well.

29:27.460 --> 29:33.300
So I think these kind of methods and another word that we also did

29:34.420 --> 29:43.620
called weekly supervised source separation, where we trained a system where the reference

29:43.620 --> 29:50.260
signal, the only labeling we had was wherever a type of a sound of a particular type was present

29:50.260 --> 29:55.460
or not in the mixture. So imagine that you want separate dogs from cats from bells from cars.

29:55.460 --> 30:04.180
And your data is mixtures of these sound classes. And in each mixture, which sources are present?

30:04.180 --> 30:10.660
Like is there a dog? Yes, maybe somewhere as a dog and somewhere as a car. But that's it. That's

30:10.660 --> 30:18.660
all you know. And so we devised an algorithm to train the separation under that constraint. And

30:18.660 --> 30:25.620
the idea is that you have a separator come up with separate sources and then you use a classifier

30:25.620 --> 30:31.860
that looks at each source and should tell you there's a dog in there when there should be a dog.

30:31.860 --> 30:39.540
And if it detects a cat, then you did something wrong. So that's kind of one of those methods that

30:39.540 --> 30:45.060
try to reduce the amount of labeling that is needed to train these methods. And that's very

30:45.060 --> 30:52.260
important because if you go in the field and you record something, you may be able to ask someone

30:52.260 --> 30:58.100
to tell you which sources are present, but it's impossible to get the actual real isolated sources.

30:59.620 --> 31:04.900
So these methods that kind of reduce the amount of labeling that's needed to train separation

31:04.900 --> 31:12.260
systems are pretty important to going forward. And I would say maybe yeah, I was going to suggest

31:12.260 --> 31:20.260
another challenge in the separation. Before we move on to the next challenge, are there methods

31:23.380 --> 31:28.340
that approach it from weak supervision from the perspective of, hey, I've got

31:29.700 --> 31:35.380
set of training data that is, you know, it's speech and isolation. I know it's speech and

31:35.380 --> 31:40.820
isolation. I have sound effects, you know, to you know, put in the context of this cocktail fork

31:40.820 --> 31:48.020
problem, sound effects and isolation, music and isolation, but not providing the mixture.

31:52.020 --> 31:56.260
Well, yeah, I think there are such like based on like auto encoders, for example,

31:57.140 --> 32:02.500
there are methods that try to do that. Now the question is like, if you have these sounds, why not

32:02.500 --> 32:13.460
mix them? So that's that's maybe one of a downside of this. But yeah, I mean like in a sense,

32:13.460 --> 32:19.540
if you're able to train auto encoders on each source type, then it's pretty modular. Like if you

32:19.540 --> 32:24.020
want to, if you have a speech autoencoder, for example, this this network is only able to reconstruct

32:24.020 --> 32:31.300
speech. And then you have a music autoencoder, then you can combine them to kind of separate speech

32:31.300 --> 32:37.940
from music. And so if they are these methods, typically, they don't work as well as the best,

32:39.620 --> 32:47.540
the artets of methods that are not based on like generating the output. I think your implicit

32:47.540 --> 32:55.780
responses that a bad mixture is going to be better than no mixture at all. I mean, I'm pretty

32:55.780 --> 33:00.660
sure that the auto encoders are going to go further, but at this point, yes, they don't work as well,

33:01.540 --> 33:06.740
as as this kind of more training based on mixtures.

33:08.580 --> 33:14.100
Of course, yeah, that's and there's another type of seniority that is interesting is if you

33:14.100 --> 33:23.300
actually have example of speech and noise, and you have some time to remember the exact

33:23.300 --> 33:28.740
scenario. So you have example of speech and noise, and you have example of speech, but you don't

33:28.740 --> 33:36.260
have example of noise and isolation. Then you can still kind of do some tricks to train methods

33:36.260 --> 33:40.340
to to separate the speech from the noise. So you don't have, you have a mixture of speech and

33:40.340 --> 33:45.860
noise, but you don't have the corresponding isolate around truth. But you're sort of what speech

33:45.860 --> 33:51.300
sound like. So you can then come up with some tricks to to use these data to train a method,

33:51.300 --> 33:55.860
a separation system. So you're about to mention the next challenge.

33:55.860 --> 34:00.660
I mean, something that is kind of what I'm excited about that we started a little bit working

34:00.660 --> 34:09.460
on still and we need to go further, I think is this idea of hierarchy in in sounds. So we published

34:09.460 --> 34:15.940
one paper on hierarchical source separation, where we try to argue that, you know, source

34:15.940 --> 34:23.940
separation sometimes is actually a bit of an ear pose problem. Because let's say you're in a bar

34:23.940 --> 34:29.860
and you're talking with people and there's a band playing in the background and you say,

34:29.860 --> 34:38.740
well, okay, separate that scene. What does it exactly mean? Like if you're saying, if your problem

34:38.740 --> 34:43.540
is separated separate speech from noise, that's pretty well posed or speech from speech that's

34:43.540 --> 34:49.620
pretty well posed. But as soon as you get into more complex scenarios, you ask to separate the

34:49.620 --> 34:54.900
scene, do you want to put all the people talking together in one group? Do you want to put the band

34:54.900 --> 35:00.980
as one coherent source or do you want each instrument in the band? You want each person separately?

35:00.980 --> 35:08.020
So it's kind of a hierarchical structure of the sound within a scene. And this hasn't been

35:08.020 --> 35:16.740
I could even envision trying and pose some kind of spatial hierarchy, like the people that may

35:16.740 --> 35:21.780
be talking to one another because they're closer than more distant people. Exactly. Yeah, that's

35:21.780 --> 35:28.660
another type of hierarchy. Exactly. So there's many different cues that can be that we can use

35:28.660 --> 35:35.140
when we're in that scene to kind of make sense of it. But this hasn't been, I mean, the spatial

35:35.140 --> 35:42.340
information have been used with microphone array methods, but maybe not exactly in that, in that

35:42.340 --> 35:50.340
with that angle, if I may say so. And so we had a, we tried to argue that paper, but like

35:50.340 --> 35:55.460
people should start trying to think about social separation as a hierarchical, we have a hierarchical

35:55.460 --> 36:04.420
angle. And the first thing that we did in that topic was to consider a music source separation

36:05.380 --> 36:12.420
with a hierarchy of instruments. And so a slightly different idea of hierarchy, but basically,

36:12.420 --> 36:24.820
we were saying that if a user wants to create a system to separate a particular instrument

36:24.820 --> 36:32.980
in a music mixture, and you allow the user to give you an example of what they want.

36:34.500 --> 36:39.700
And so they give you a sound snippet of what they want. And so let's say you have a rock band

36:39.700 --> 36:46.740
and they give you an acoustic guitar snippet. What do they mean? Do they want the acoustic guitar

36:46.740 --> 36:53.460
in that recording? If there is one or nothing else, do they want any guitar electric acoustic?

36:53.460 --> 37:00.500
Do they want any instrument that is kind of harmonic? So it's it's not exactly what they mean. So

37:02.100 --> 37:08.260
or so we thought, well, why not consider that hierarchy? Like so when you the user

37:08.900 --> 37:15.140
queries with guitar or acoustic guitar, you you're going to have a network estimate all these

37:15.140 --> 37:21.300
all these levels of hierarchy. So is there an acoustic guitar if no silence? If there is there any

37:21.300 --> 37:28.020
guitar, then you you you output the mixture of all the guitars in that recording. And then

37:28.020 --> 37:31.780
above is all the kind of harmonic instruments and then the drums on the other side.

37:33.060 --> 37:41.700
And we found that if you train an network to do this, it can actually help it train better with

37:41.700 --> 37:48.660
less data at the fine grain level. So it was able. So for example, if you you don't have a lot of

37:48.660 --> 37:55.700
tracks with acoustic guitar, you have a lot of tracks with electric guitar. The you can network

37:55.700 --> 38:02.980
and leverage that and kind of learn the structure and can improve its performance on acoustic guitars

38:02.980 --> 38:09.940
by learning to separate together the other guitars during train. And do you think is that an

38:09.940 --> 38:17.940
instance of multitask learning? Is that part of why that's working? I think that's it's

38:17.940 --> 38:24.820
some form of multitask learning. Yes. But it's in that case, it's it's a bit more specific in

38:24.820 --> 38:32.180
that the tasks are really related to each other. So we actually impose constraint that the the

38:32.180 --> 38:38.100
output at the upper level should include the output at the lower level. So for example, if you

38:38.100 --> 38:47.620
have electric guitar and acoustic guitar outputs, the their parent is we impose and let's say you use

38:47.620 --> 38:55.220
you do the separation by using a mask. So a mask is what we typically use I mean very often use

38:55.220 --> 39:01.300
for separation. It's numbers typically between zero and one that we apply we multiply at each

39:01.300 --> 39:08.020
time and frequency and we say to zero if we want to basically turn turn there this part of this

39:08.020 --> 39:13.860
time frequency energy off and we put it to one if we want all of it. And so you can apply a mask to

39:13.860 --> 39:20.820
shut off the things you don't want and keep the others. So one way to impose a hierarchy go

39:20.820 --> 39:25.700
consternate say well I have two masks for the electric and acoustic guitar and I'm going to impose

39:25.700 --> 39:31.860
that the apparent mask is at least as large as the max of the two. And so that kind of

39:31.860 --> 39:35.700
imposes a hierarchy course structure and this regularization helps the network train a little bit

39:35.700 --> 39:42.980
there. Nice. Any other challenges come to mind? Or areas that you're excited about?

39:43.940 --> 39:50.580
Let's see I mean I'm also excited on the audiovisual aspects. So not only Sanone but the

39:50.580 --> 39:57.300
what I mean in my team what we're really trying to do is to make sense of a complex audiovisual

39:57.300 --> 40:04.260
scene. And so using using vision with our partners in the computer vision group at Merl and

40:04.260 --> 40:12.020
trying to type this with sound. So we did some work on trying to use vision as an auxiliary

40:12.020 --> 40:18.420
feature to improve sound source separation and in particular try to model how objects interact

40:18.420 --> 40:24.100
with each other to create sound in a scene like think of a bang a stick banging on a pot.

40:25.540 --> 40:34.900
And so this kind of visual cues can help you separate better. So yeah I'm very excited about

40:34.900 --> 40:41.460
trying to I mean our ultimate goal in my team is what we call total transcription.

40:42.500 --> 40:47.780
So it's like if you have a complex acoustic scene is to completely transcribe what's going on.

40:47.780 --> 40:52.580
So if somebody speaks you transcribe the speech. You may also be interested in the emotion and how

40:52.580 --> 40:58.660
they say it. And if there's music you you could separate the music potentially transcribe it as

40:58.660 --> 41:04.260
well. If you have sound events you could you know try to detect them, localize them in 3D space as

41:04.260 --> 41:09.300
well. So that's kind of that more holistic approach that I'm pretty excited about.

41:09.860 --> 41:16.100
Nice. Well Jonathan thanks so much for sharing with us a bit about what you're working on.

41:16.100 --> 41:18.100
Thank you for having me.

