1
00:00:00,000 --> 00:00:17,480
Hello everyone, I am back here at NYU Future Labs at the AI Nexus Accelerator speaking

2
00:00:17,480 --> 00:00:23,480
with Mika Perra and Hamid Zaheri from Cambrian Intelligence.

3
00:00:23,480 --> 00:00:25,480
All right guys, say hi.

4
00:00:25,480 --> 00:00:26,480
Hi.

5
00:00:26,480 --> 00:00:27,880
Great to be here.

6
00:00:27,880 --> 00:00:29,880
Hi everyone.

7
00:00:29,880 --> 00:00:32,880
So tell us about Cambrian Intelligence, what are you guys up to?

8
00:00:32,880 --> 00:00:40,160
Yes, the Cambrian Intelligence, the word Cambrian comes from Cambrian Explosion that happened

9
00:00:40,160 --> 00:00:42,240
some 60 million years ago.

10
00:00:42,240 --> 00:00:49,640
So we are developing robot always made of AI modules.

11
00:00:49,640 --> 00:00:59,520
And that can be used to do various tasks and solve to enable robotics in every industry

12
00:00:59,520 --> 00:01:00,520
possible.

13
00:01:00,520 --> 00:01:01,520
Okay.

14
00:01:01,520 --> 00:01:09,040
And with regards to the analogy to the Cambrian Explosion, what's being exploded?

15
00:01:09,040 --> 00:01:10,800
Like what have we seen proliferate here?

16
00:01:10,800 --> 00:01:12,960
Is it the intelligence itself?

17
00:01:12,960 --> 00:01:13,960
Is it robotics?

18
00:01:13,960 --> 00:01:14,960
Is it?

19
00:01:14,960 --> 00:01:20,360
Yeah, so we believe that sticking this narrow AI applications into modules, we can achieve

20
00:01:20,360 --> 00:01:25,680
much, much more and we can more rapidly develop different applications for robotics.

21
00:01:25,680 --> 00:01:31,640
So basically, what we are doing is making robots more intelligent.

22
00:01:31,640 --> 00:01:38,440
And so maybe tell us a little bit about your personal backgrounds, how did you get to

23
00:01:38,440 --> 00:01:39,440
where you are?

24
00:01:39,440 --> 00:01:40,440
Yeah, sure.

25
00:01:40,440 --> 00:01:47,400
So I have a background in aerospace and from shoot guard in embedded systems and robotics.

26
00:01:47,400 --> 00:01:48,400
Okay.

27
00:01:48,400 --> 00:01:56,000
And I moved to London, then there are MedMika, Mika has a background in cognitive science

28
00:01:56,000 --> 00:01:58,880
and deep learning.

29
00:01:58,880 --> 00:02:06,240
And there we figured that we both share passion for robotics and for artificial intelligence.

30
00:02:06,240 --> 00:02:09,120
And we decided to start this company together.

31
00:02:09,120 --> 00:02:10,120
Oh, nice.

32
00:02:10,120 --> 00:02:13,080
And how long have you been doing it?

33
00:02:13,080 --> 00:02:15,080
A little bit over all that year.

34
00:02:15,080 --> 00:02:16,080
Okay.

35
00:02:16,080 --> 00:02:20,160
If you've been at it for around a year, where are you at as a company?

36
00:02:20,160 --> 00:02:21,160
Yeah.

37
00:02:21,160 --> 00:02:29,800
So right now we are working for pilots for a large auto manufacturer and like to implement

38
00:02:29,800 --> 00:02:36,840
the first application out of our operating system on industrial robots.

39
00:02:36,840 --> 00:02:43,480
So we had started on developing that and we planned to release the platform more pro-dera

40
00:02:43,480 --> 00:02:45,400
late this year.

41
00:02:45,400 --> 00:02:51,760
What specific problem are you going after that an auto maker would have?

42
00:02:51,760 --> 00:02:57,960
So right now robots are programmed point by point to accomplish different tasks.

43
00:02:57,960 --> 00:02:59,760
The robot is blind.

44
00:02:59,760 --> 00:03:06,960
You put some specific parts into some specific locations with the accuracy of sub-millimeter.

45
00:03:06,960 --> 00:03:13,040
And the robot is programmed to just go there and pick it apart and place it some place.

46
00:03:13,040 --> 00:03:17,640
It's already defined for a robot, but robot has no knowledge about the environment or the

47
00:03:17,640 --> 00:03:20,600
task that it's trying to accomplish.

48
00:03:20,600 --> 00:03:23,760
But we're doing, we're trying to make that robot more intelligent.

49
00:03:23,760 --> 00:03:31,080
So the robot can have understanding of the environment and can accomplish the task by self-learning.

50
00:03:31,080 --> 00:03:36,720
Then there won't be any need for programming the robot.

51
00:03:36,720 --> 00:03:41,840
We're basically changing the abstraction level of robot programming from low level programming

52
00:03:41,840 --> 00:03:49,040
of the robot to be decode to high level, just specifying the task and the specific location

53
00:03:49,040 --> 00:03:52,800
and the initial location, the target location and the robot can figure out on its own how

54
00:03:52,800 --> 00:03:55,600
to accomplish the task.

55
00:03:55,600 --> 00:04:03,640
So I've seen a bunch of videos from a variety of different sources actually with showing

56
00:04:03,640 --> 00:04:10,880
robots trying to do these pick and place types of operations and there are a number of challenges

57
00:04:10,880 --> 00:04:18,880
including, you mentioned the millimeter resolution with what you need to program the robot.

58
00:04:18,880 --> 00:04:24,560
What are some of the other challenges that you see folks having with the traditional approach

59
00:04:24,560 --> 00:04:26,920
to programming these robots?

60
00:04:26,920 --> 00:04:33,440
A beauty program in this robot is really time-consuming and it's not just the programming itself,

61
00:04:33,440 --> 00:04:39,280
but the robot changes the behavior with environmental factors like even life and conditions or

62
00:04:39,280 --> 00:04:43,280
temperature of the environment and so on can affect how the robot behaves.

63
00:04:43,280 --> 00:04:49,760
So every time these conditions are changing they need to reprogram the robot.

64
00:04:49,760 --> 00:04:56,200
So our approach is to add that level of intelligent to the robot that the robot can adapt to the

65
00:04:56,200 --> 00:05:06,000
new changes of the environment and it basically can learn how to optimize for different tasks.

66
00:05:06,000 --> 00:05:12,080
Current challenges, it includes localizing the part with the accuracy that we mentioned,

67
00:05:13,200 --> 00:05:18,240
extracting the pose of the part, grasping the part from the right location and with the right timing

68
00:05:18,240 --> 00:05:25,520
and so on. And these are the challenges we want to remove from the programming and let the robot

69
00:05:25,520 --> 00:05:29,040
to learn.

70
00:05:29,040 --> 00:05:31,600
And why the auto industry?

71
00:05:31,600 --> 00:05:41,600
So other industry, currently 2.3 million industrial robots out there operating as we speak and 40

72
00:05:41,600 --> 00:05:50,800
percent of those in car industry, automotive industry and kind of out of car industry,

73
00:05:50,800 --> 00:05:55,360
about 70 percent operating the body shop which is where they make the chassis of the car.

74
00:05:56,240 --> 00:06:00,240
So these are the pictures and videos you see with the car kind of going down the line

75
00:06:00,240 --> 00:06:06,720
starting from the frame and then putting all the, and those are massive, massive robots that are

76
00:06:06,720 --> 00:06:10,320
open. Yeah and they require like massive holes to operate on.

77
00:06:11,600 --> 00:06:17,920
And it's a big, big task to configure like when you can imagine when a new car model comes in

78
00:06:17,920 --> 00:06:24,240
with all the different variants and how many parts the car is made of. So you can imagine the

79
00:06:24,240 --> 00:06:29,600
task of programming all this in which all robot excels down at the production line.

80
00:06:29,600 --> 00:06:33,760
So it's massive task. Do you have a sense for how long that usually takes

81
00:06:33,760 --> 00:06:39,680
for when a new, you know, once I've completed design on a new car to complete the,

82
00:06:40,640 --> 00:06:44,160
you know, the programming of the line to get it to market?

83
00:06:44,160 --> 00:06:50,640
Yeah so typically they practice the production like they have 100 cars, 200 cars and so on and

84
00:06:50,640 --> 00:06:58,960
often even with the production run practice like you typically change the part in dimension of form

85
00:06:58,960 --> 00:07:06,400
to fit in better, but it's like you're talking in days of like very, very hard ramp up production.

86
00:07:10,480 --> 00:07:14,640
I've also seen where, you know, there are companies that are trying to,

87
00:07:17,440 --> 00:07:22,160
there are companies that are trying to solve this robot programming problem by,

88
00:07:23,040 --> 00:07:28,160
as opposed to, you know, the traditional programming approach basically have the robot like,

89
00:07:28,160 --> 00:07:33,200
you know, shadow a human or you kind of move the robot the way you want the move to move the robot

90
00:07:33,200 --> 00:07:39,040
to, you manually manipulate the robot in order to teach it. It doesn't sound like that's what

91
00:07:39,040 --> 00:07:46,560
you guys are doing. That's part of our platform. Okay so tell us more about, you dig into the platform

92
00:07:46,560 --> 00:07:52,720
a little bit and how, how a company would use it. So the basic idea of the platform is to enable

93
00:07:52,720 --> 00:07:58,800
people to have natural interaction with the robot. If there are tasks that the robot can self-learn,

94
00:07:59,920 --> 00:08:05,680
the robot can do it using a platform if there's a necessity for the human to have a simple

95
00:08:05,680 --> 00:08:11,280
interaction with the robot to show some specific tasks. Okay. Still possible using our platform.

96
00:08:12,720 --> 00:08:17,440
Basically we are trying to remove the need for the human to have expertise in the robotics

97
00:08:17,440 --> 00:08:22,000
in order to interact with the robots. We want normal human without that kind of expertise to be

98
00:08:22,000 --> 00:08:27,200
able to interact with the robot and be able to teach it simple tasks or even more communicate in

99
00:08:27,200 --> 00:08:34,400
the future hopefully. So kind of like we see that all like teaching by demonstration or even like

100
00:08:35,600 --> 00:08:42,080
teaching by scripting or like even voice commands like we see this like blend in in our platform

101
00:08:42,080 --> 00:08:47,520
and some point in the future. So you can have any industrial, any robotic arm on your table and you

102
00:08:47,520 --> 00:08:54,240
can ask it to grab your coffee cup and then it's able to do that. So that's that's where we are

103
00:08:54,240 --> 00:09:02,320
aiming at. And now today the robots aren't even typically or to what degree do the robots

104
00:09:02,320 --> 00:09:10,080
typically already have visual sensors or is that something that that has to be outfitted in

105
00:09:10,080 --> 00:09:17,840
order for a company to use your approach? Yeah so we kind of for industrial robots in the kind

106
00:09:17,840 --> 00:09:24,320
of so we're going to retrofit robot industry robots with a specific set of sensors so it can

107
00:09:24,320 --> 00:09:29,520
sense our own it's environment. Okay. We're not trying anything to manufacture any specific

108
00:09:29,520 --> 00:09:35,520
sensor we're trying to use off the shelf sensors and not really expensive ones. Okay. And

109
00:09:35,520 --> 00:09:41,680
what kinds of sensors primarily visual or are there other types of sensors that go into?

110
00:09:41,680 --> 00:09:47,120
We started by visual sensors but they're going to be a sense of fusion using different inputs

111
00:09:47,120 --> 00:09:56,240
to enhance the performance. Okay. I had a conversation with someone actually from one of the

112
00:09:56,240 --> 00:10:02,240
large auto manufacturers where they mentioned that one of the challenges that they were having in

113
00:10:02,240 --> 00:10:12,480
this space was that the robotics manufacturers are increasingly trying to you know they see one

114
00:10:12,480 --> 00:10:18,240
of the big they see the data that the robots are producing in terms of the sensors that they

115
00:10:18,240 --> 00:10:24,240
already put on the data and and they're operating data as kind of proprietary and they're trying

116
00:10:24,240 --> 00:10:31,520
to sell that now to the automakers and are you finding have you come across anything like that

117
00:10:31,520 --> 00:10:41,840
where the the manufacturers are you know having issues automating because the robotics suppliers

118
00:10:41,840 --> 00:10:47,200
are making it more difficult for them. Yeah of course like many especially big manufacturers

119
00:10:47,200 --> 00:10:54,080
like they are very sensitive about the data like what they have inside the inside this factory

120
00:10:54,080 --> 00:11:02,640
so they don't obviously want it to go away anywhere. What when you go when you're working with a client

121
00:11:02,640 --> 00:11:09,520
like this large auto manufacturer what challenges do you run into as they seek to use the your product?

122
00:11:12,080 --> 00:11:19,600
Yes obviously like with a for quite small company like us startup face and then

123
00:11:19,600 --> 00:11:26,560
scaling up to a multiple factories around the world so that's like that's something like where

124
00:11:26,560 --> 00:11:32,560
you need to ramp up your operation force and so on and so but we are not like we are iterating there

125
00:11:32,560 --> 00:11:39,840
so in order to gain the trust of those big manufacturers they definitely need certain level of

126
00:11:39,840 --> 00:11:50,800
safety and precision. That should be proven over time and they need to trust the product

127
00:11:50,800 --> 00:11:57,840
and make sure it works before they can deploy manufacturing. So what's their experience with the

128
00:11:57,840 --> 00:12:06,320
product when they you know you guys come in and get them set up with I imagine a small use case

129
00:12:06,320 --> 00:12:10,000
to start to build that trust. What does it look like from their perspective?

130
00:12:11,280 --> 00:12:19,520
Yes at the moment we are still so like our product is not yet in operation so yeah so it's going

131
00:12:19,520 --> 00:12:25,600
there. What do you envision the user experience to be when you get a new customer onboard?

132
00:12:25,600 --> 00:12:31,680
Yes so the user experience will be very simple so like for them like instead of now programming

133
00:12:31,680 --> 00:12:40,400
the robot they can just upload the cat model of that specific part they want to be creeped and

134
00:12:40,400 --> 00:12:46,240
then they specify the end location where it needs to go and so it's that simple.

135
00:12:46,240 --> 00:13:01,280
And you mentioned so you as opposed to having to explicitly tell the robot you know give it

136
00:13:01,280 --> 00:13:07,360
coordinates of a pick location you're giving them what instead or are you giving them coordinates

137
00:13:07,360 --> 00:13:12,960
but just not the dimensions of the item or yeah yeah so the problem we are solving is not

138
00:13:12,960 --> 00:13:20,640
them so typically that's where the part needs to go for example most of the operations are

139
00:13:20,640 --> 00:13:27,440
welding these parts or gluing these parts together. So the end point is typically could be

140
00:13:28,400 --> 00:13:36,640
pretty determined but where do you grasp like that part if you now they need to manufacture

141
00:13:36,640 --> 00:13:44,240
custom manufacturer chicks so that they hold stack of parts very very accurately and with every

142
00:13:44,240 --> 00:13:48,960
new part if you have new model or something like you need to custom manufacturer a new chick and

143
00:13:48,960 --> 00:13:55,680
this is obviously a pick so we can just like stack paths arbitrarily and so they still leave the

144
00:13:55,680 --> 00:14:02,160
creeping location is specified still the robot can figure out from exactly that location but

145
00:14:02,160 --> 00:14:07,280
the parts doesn't have to be in that exact orientation for the robot in order to find given if

146
00:14:07,280 --> 00:14:12,480
they have some randomness in their orientation the robot can still extract the pose of the object

147
00:14:12,480 --> 00:14:18,560
and recognize that part and the creeping location and grasp from there. So there are two

148
00:14:20,320 --> 00:14:25,200
two problems in there one is kind of the bucket of parts problem right so how do I know what

149
00:14:25,200 --> 00:14:32,800
the orientation is but then there's also are you also enabling them to use general grippers as

150
00:14:32,800 --> 00:14:38,080
opposed to custom made grippers for a given part or for the specific task of manufacturing they

151
00:14:38,080 --> 00:14:46,240
usually have their own specific tools or groupers the systems to be integrated with that but in the

152
00:14:46,240 --> 00:14:54,720
future the robot can learn to use different kind of tools. So maybe let's dive into the technology

153
00:14:54,720 --> 00:15:00,640
a little bit you know what machine learning and AI techniques are you using to make all this work?

154
00:15:00,640 --> 00:15:06,480
Sure so in the robot industry at the moment you don't have a kind of common operating system

155
00:15:06,480 --> 00:15:12,960
that the robot can use every manufacturer to make their own custom programs it's really low level

156
00:15:12,960 --> 00:15:22,800
programming we're trying to make it a bit more general something that imagined operating system

157
00:15:22,800 --> 00:15:27,360
that has a low level interface that can interface different kind of robots and that are high level

158
00:15:27,360 --> 00:15:34,080
modules like recognizing the parts grasping the parts or collision awareness or any intelligent

159
00:15:34,080 --> 00:15:38,800
that you need in the robot behavior that are abstracted from the low level communication and they

160
00:15:38,800 --> 00:15:47,680
can grow and get better over time and this makes our operating system. And so how would you characterize

161
00:15:47,680 --> 00:15:58,240
where you are with the product so far? So far we've integrated with three different robot

162
00:15:58,240 --> 00:16:08,000
manufacturers I can name it it's Kuka, Universal Robots and Stubli and we're implementing some

163
00:16:08,000 --> 00:16:14,080
basic AI modules but later we're going to open up this platform for people to add more skills

164
00:16:14,080 --> 00:16:18,960
to the robot and expand the platform over time. So what are some examples of the AI modules

165
00:16:18,960 --> 00:16:25,760
that you've implemented? So right now we are working on the very accurate localization so that's

166
00:16:25,760 --> 00:16:33,760
the key value adding module in our product at the moment and then we've added our recognition

167
00:16:34,560 --> 00:16:43,600
and then simple robot controllers. So what goes into localization? What are the challenges there

168
00:16:43,600 --> 00:16:50,640
and how are you attacking those? Yeah so basic is kind of like where the uncertainty comes from

169
00:16:50,640 --> 00:16:57,520
is like what you can receive the resolution from your sensors and then kind of how you can fit

170
00:16:57,520 --> 00:17:07,520
that into your model and kind of get accurate enough prediction over time and like over multiple

171
00:17:07,520 --> 00:17:13,600
samples like and what do you do and about like going back to the modules like you can think

172
00:17:14,400 --> 00:17:19,600
different kind of modules like you can think of like safety modules where you can detect people

173
00:17:19,600 --> 00:17:26,480
if people somehow enter this cell where the robot is operating so you can have these sorts of

174
00:17:26,480 --> 00:17:32,000
all you can have force feedback where you can recognize different forces when you're installing

175
00:17:32,000 --> 00:17:42,480
some components and so on and so are you are you building everything from the ground up from scratch

176
00:17:42,480 --> 00:17:51,440
or are you using various open source libraries for the analytics parts? It's a combination of both

177
00:17:52,800 --> 00:17:59,280
we need some certain tools we cannot develop everything from scratch but there is a certain level

178
00:17:59,280 --> 00:18:06,480
of modification that we need to make to the existing tools to make them usable for that precision

179
00:18:06,480 --> 00:18:11,120
required for manufacturing and that's the work that we need to accomplish. Anything that we might

180
00:18:11,120 --> 00:18:18,560
know in terms of tools are you running on top of yeah on the deep learning side we use TensorFlow

181
00:18:18,560 --> 00:18:25,200
so that's great yeah we also use on the robotic side kind of like low level commands we use

182
00:18:25,200 --> 00:18:34,320
ROS so in the robotic research there's this platform called ROS a robot operating system and it has

183
00:18:35,120 --> 00:18:41,360
kind of distributed architecture no it can run in parallel and they're coming to come and get

184
00:18:41,360 --> 00:18:47,760
into the master mode and so on we're using the idea the whole platform like what has been developed

185
00:18:47,760 --> 00:18:55,200
for research is not suitable for manufacturing in terms of safety and so on but the idea is

186
00:18:55,200 --> 00:19:01,280
great so we're trying to adapt those ideas but implement our own version that makes it suitable

187
00:19:01,280 --> 00:19:09,440
for precision required for manufacturing. Okay that was great great anything else you'd like to

188
00:19:09,440 --> 00:19:19,360
share about what you guys are up to? Yeah we'll share like in the bit later this year just follow us

189
00:19:19,360 --> 00:19:28,720
on Twitter and check out website and you'll hear more about us. Okay awesome how can folks what is

190
00:19:28,720 --> 00:19:40,800
your Twitter and working folks find you online? Yeah it's called Keynes or C4-1 NT. C4-1 NT. Okay and

191
00:19:40,800 --> 00:19:48,160
that's the Twitter or is that the website? Yeah and the website is C-A-I-N-T that's O-O

192
00:19:48,160 --> 00:19:54,240
okay got it awesome well thanks so much for being on the show it's great to hear about what you

193
00:19:54,240 --> 00:20:05,360
guys are up to. Thanks for having me. Yeah thanks for having us.

