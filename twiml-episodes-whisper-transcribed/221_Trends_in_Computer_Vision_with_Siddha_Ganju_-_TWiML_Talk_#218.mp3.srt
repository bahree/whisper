1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

4
00:00:35,920 --> 00:00:40,640
to present to you our first ever AI rewind series.

5
00:00:40,640 --> 00:00:45,040
In this series I interview friends of the show for their perspectives on the key developments

6
00:00:45,040 --> 00:00:49,720
of 2018 as well as a look ahead at the year to come.

7
00:00:49,720 --> 00:00:54,320
We'll cover a few key categories this year, namely computer vision, natural language

8
00:00:54,320 --> 00:00:59,200
processing, deep learning, machine learning and reinforcement learning.

9
00:00:59,200 --> 00:01:03,840
Of course we realize that there are many more possible categories than these, that there's

10
00:01:03,840 --> 00:01:08,800
a ton of overlap between these topics and that no single interview could hope to cover

11
00:01:08,800 --> 00:01:12,240
everything important in any of these areas.

12
00:01:12,240 --> 00:01:17,120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

13
00:01:17,120 --> 00:01:26,000
by commenting on the series page at twimbleai.com slash rewind 18.

14
00:01:26,000 --> 00:01:31,240
In this episode of our AI rewind series, we're excited to have Sida Gondu back on the show.

15
00:01:31,240 --> 00:01:36,480
Sida, who is now an autonomous vehicle solution architect at Nvidia, shares her thoughts on

16
00:01:36,480 --> 00:01:40,040
trends in computer vision in 2018 and beyond.

17
00:01:40,040 --> 00:01:45,320
We cover her favorite CV papers of the year in areas such as neural architecture search,

18
00:01:45,320 --> 00:01:50,600
training from simulation, application of computer vision to augmented reality and more,

19
00:01:50,600 --> 00:01:54,160
as well as a bevy of tools and open source projects.

20
00:01:54,160 --> 00:01:55,160
Enjoy.

21
00:01:55,160 --> 00:02:00,640
Alright everyone, I've got Sida Gondu on the line.

22
00:02:00,640 --> 00:02:07,160
Sida is a solution architect working on autonomous vehicles at Nvidia.

23
00:02:07,160 --> 00:02:11,120
Sida, welcome back to this week in machine learning and AI.

24
00:02:11,120 --> 00:02:12,120
Thank you.

25
00:02:12,120 --> 00:02:15,280
It's been a great year so far and thank you for having me once again.

26
00:02:15,280 --> 00:02:16,280
Absolutely.

27
00:02:16,280 --> 00:02:17,280
Absolutely.

28
00:02:17,280 --> 00:02:24,200
So for folks that want to go back to your original podcast on the show, it was twimble talk

29
00:02:24,200 --> 00:02:25,520
number 95.

30
00:02:25,520 --> 00:02:31,720
At the time you are, you were at deep vision, but you've since moved over to Nvidia.

31
00:02:31,720 --> 00:02:32,720
Yeah.

32
00:02:32,720 --> 00:02:35,280
What will you be working on at Nvidia?

33
00:02:35,280 --> 00:02:42,520
So at Nvidia, I'm focusing on a particular part of the entire self-driving stack, which

34
00:02:42,520 --> 00:02:48,520
is called simulation and resimulation, which is the part that does the testing and the

35
00:02:48,520 --> 00:02:51,640
verification of the entire perception stack.

36
00:02:51,640 --> 00:02:57,520
So how do we know that the computer vision, the lidar, the radar networks that we have

37
00:02:57,520 --> 00:03:02,760
trained, how do we know that they actually work well in real life because real life testing

38
00:03:02,760 --> 00:03:07,800
for autonomous vehicles is very hard to do because you can't test out the millions and

39
00:03:07,800 --> 00:03:15,320
millions of possibilities on actual roads with real test drivers inside the vehicles.

40
00:03:15,320 --> 00:03:20,920
So simulation is a develop a friendly way to do that.

41
00:03:20,920 --> 00:03:24,880
And it's also, you know, you can squeeze millions and millions of miles.

42
00:03:24,880 --> 00:03:29,200
You can add tens and thousands of scenarios to simulate and test.

43
00:03:29,200 --> 00:03:33,480
So it's really a nice solution to testing.

44
00:03:33,480 --> 00:03:40,000
And so you're joining us to represent computer vision in our Year in Review and Prediction

45
00:03:40,000 --> 00:03:41,720
series.

46
00:03:41,720 --> 00:03:43,320
So let's jump right in.

47
00:03:43,320 --> 00:03:51,000
So you took the time to kind of prepare your thoughts on some recent, some papers this

48
00:03:51,000 --> 00:03:57,480
year that struck you as particularly noteworthy in the computer vision space.

49
00:03:57,480 --> 00:03:58,760
What do you want to start?

50
00:03:58,760 --> 00:04:03,440
So before I start, I think I want to say that, you know, I'm only human and I'm probably

51
00:04:03,440 --> 00:04:09,000
missing tons of game-changing papers that came out this year and, you know, through this

52
00:04:09,000 --> 00:04:14,360
discussion, we're probably just scratching the surface of this year's CV research.

53
00:04:14,360 --> 00:04:19,680
Now that being said, I'm going to start with a couple of papers that are providing different

54
00:04:19,680 --> 00:04:25,040
solutions to problems rather than improving on existing tasks.

55
00:04:25,040 --> 00:04:31,880
So the very first paper that I really, really liked was called learning transferable architectures

56
00:04:31,880 --> 00:04:34,360
for scalable image recognition.

57
00:04:34,360 --> 00:04:36,720
And this is by Google Brain.

58
00:04:36,720 --> 00:04:43,240
Now this is being projected as the future of deep learning and the main reason behind

59
00:04:43,240 --> 00:04:49,880
that is because it introduces something called neural architecture search or NAS.

60
00:04:49,880 --> 00:04:57,120
So you know, Sam, you've been doing these podcasts definitely more than 95 podcasts by

61
00:04:57,120 --> 00:04:58,120
now.

62
00:04:58,120 --> 00:05:05,760
And I'm sure during your discussions, a lot of people have talked about how designing

63
00:05:05,760 --> 00:05:09,440
a network architecture is a big pain point.

64
00:05:09,440 --> 00:05:13,600
And especially for those people who are just entering the field.

65
00:05:13,600 --> 00:05:17,080
So neural architecture search is really the beacon of light for them.

66
00:05:17,080 --> 00:05:22,600
It's a network that searches for the best model structure instead of you manually designing

67
00:05:22,600 --> 00:05:24,560
the network architecture.

68
00:05:24,560 --> 00:05:30,600
Now internally, the search for this ultimate model is based on a reward function that rewards

69
00:05:30,600 --> 00:05:33,360
the model for performing well on the data set.

70
00:05:33,360 --> 00:05:39,400
It has been validated on and at the same time, it's using the same metrics to validate

71
00:05:39,400 --> 00:05:43,080
and test the ultimate network.

72
00:05:43,080 --> 00:05:48,600
So I really like this paper because the details that such architectures achieve better accuracy

73
00:05:48,600 --> 00:05:51,200
than manually designed models.

74
00:05:51,200 --> 00:05:57,320
And searching also allows the network itself, you know, a more brute force type coverage

75
00:05:57,320 --> 00:06:00,480
over the entire service space.

76
00:06:00,480 --> 00:06:06,160
Now we can only imagine the huge benefits that a good NAS algorithm can give, rather than

77
00:06:06,160 --> 00:06:11,520
hand designing a specific network for the millions and millions of specific applications

78
00:06:11,520 --> 00:06:13,920
that we might want to develop.

79
00:06:13,920 --> 00:06:19,000
So this is actually one of its biggest advantage that it is generalizable.

80
00:06:19,000 --> 00:06:24,760
So a well-designed NAS algorithm is supposed to be flexible enough to find a good network

81
00:06:24,760 --> 00:06:27,840
for any specific task.

82
00:06:27,840 --> 00:06:33,400
And within the paper, they've outlined both detection and classification.

83
00:06:33,400 --> 00:06:39,760
So on ImageNet, they reported a 1.2% improvement in top 1% accuracy.

84
00:06:39,760 --> 00:06:46,640
And this is compared to what I guess we can now call the best human invented architecture.

85
00:06:46,640 --> 00:06:53,360
And the coolest thing is that this architecture has 9 billion fewer flops.

86
00:06:53,360 --> 00:06:58,760
Can you imagine the improvement in speed in real-life production systems that we can get

87
00:06:58,760 --> 00:06:59,760
from this?

88
00:06:59,760 --> 00:07:05,800
I mean, NAS is bringing a whole new definition of real time.

89
00:07:05,800 --> 00:07:12,440
And on C for 10, I think NAS net achieves a 2.4% error rate, which is the new state of

90
00:07:12,440 --> 00:07:14,160
the art.

91
00:07:14,160 --> 00:07:20,720
And on the detection side of things, the features learned by NAS net from ImageNet classification

92
00:07:20,720 --> 00:07:27,040
combined with foster R C and N gives a new state of the art, which has a 4% improvement

93
00:07:27,040 --> 00:07:32,080
on the previous state of the art paper.

94
00:07:32,080 --> 00:07:39,600
So NAS is something really exciting, and I expect a lot of people entering the field to really

95
00:07:39,600 --> 00:07:45,760
go and test it out because it gives them a huge advantage of not wasting hours and hours

96
00:07:45,760 --> 00:07:51,200
trying to think what is the ideal architecture for their specific application.

97
00:07:51,200 --> 00:07:56,920
The paper talk at all about the training time to achieve that kind of performance relative

98
00:07:56,920 --> 00:08:03,360
to the training time for a single static architecture.

99
00:08:03,360 --> 00:08:11,360
So when we talk about a single static architecture, it's not like we fix a single architecture and

100
00:08:11,360 --> 00:08:16,360
we stick to it throughout the experimentation pipeline.

101
00:08:16,360 --> 00:08:24,280
As a deep learning engineer, I often start with one architecture and go back and forth

102
00:08:24,280 --> 00:08:29,200
on every single aspect of that architecture to get to the final architecture that is put

103
00:08:29,200 --> 00:08:30,520
into production.

104
00:08:30,520 --> 00:08:37,400
So I think comparing the times between what NAS will do versus what a single architecture

105
00:08:37,400 --> 00:08:41,240
does isn't really an apples to apples comparison.

106
00:08:41,240 --> 00:08:46,440
And that's kind of what I was getting at, the training time to train a single resident

107
00:08:46,440 --> 00:08:52,320
or something like that on ImageNet while it's decreasing can still be significant for

108
00:08:52,320 --> 00:08:58,480
some types of problems and so I'm imagining that kind of searching through solutions like

109
00:08:58,480 --> 00:09:02,440
that is going to be even more complex.

110
00:09:02,440 --> 00:09:06,880
But you're saying that the comparison isn't really, you know, against, you know, just

111
00:09:06,880 --> 00:09:13,760
training, but also the time that the, you know, that a human would typically spend in coming

112
00:09:13,760 --> 00:09:15,760
up with the architecture.

113
00:09:15,760 --> 00:09:16,760
Absolutely.

114
00:09:16,760 --> 00:09:17,760
Yeah.

115
00:09:17,760 --> 00:09:23,120
So that was the learning transferable architectures paper, what else do you have for us?

116
00:09:23,120 --> 00:09:24,120
Mm-hmm.

117
00:09:24,120 --> 00:09:25,120
Yeah.

118
00:09:25,120 --> 00:09:32,000
So the next one is from Nvidia and it's called training deep networks with synthetic

119
00:09:32,000 --> 00:09:37,080
data bridging the reality gap by domain randomization.

120
00:09:37,080 --> 00:09:44,720
So through this paper, Nvidia tries to solve an age old problem, which is can we pre-train

121
00:09:44,720 --> 00:09:52,400
with synthetic data for tasks that demand either expert knowledge or labels that were difficult

122
00:09:52,400 --> 00:09:59,120
to specify manually or images that were difficult to capture in large quantities with, you

123
00:09:59,120 --> 00:10:03,680
know, a huge variety in many such unanswered questions.

124
00:10:03,680 --> 00:10:12,840
So Nvidia built a plugin tool for Unreal Engine 4 UE4 that simulates images for training

125
00:10:12,840 --> 00:10:18,680
data and uses that to train convolutional neural networks.

126
00:10:18,680 --> 00:10:27,440
Now the biggest plus point is that all the factors spanning the main object to structures,

127
00:10:27,440 --> 00:10:35,560
viewpoints, all these factors that the training data depends on are randomized and automated,

128
00:10:35,560 --> 00:10:40,800
making it so much more easier and useful to develop a data set.

129
00:10:40,800 --> 00:10:47,800
So when we talk about the number, variety, texture of objects, the background, then for

130
00:10:47,800 --> 00:10:55,080
destructors, a huge variation in their number, types, colors, scales.

131
00:10:55,080 --> 00:10:59,960
For the camera, you can change the camera location view point, be it the virtual camera with

132
00:10:59,960 --> 00:11:06,040
respect to the scene or the angle of the camera with respect to the scene or the number

133
00:11:06,040 --> 00:11:09,120
and the location of point lights.

134
00:11:09,120 --> 00:11:15,520
An interesting thing that they reported was that with additional fine tuning and real data,

135
00:11:15,520 --> 00:11:20,680
this network yields better performance than using real data alone.

136
00:11:20,680 --> 00:11:27,720
So this result really opens up the possibility of using inexpensive synthetic data for training

137
00:11:27,720 --> 00:11:29,720
neural networks.

138
00:11:29,720 --> 00:11:36,760
I mean, imagine there are millions and millions of applications that don't have concentrated

139
00:11:36,760 --> 00:11:43,280
data collection efforts or maybe collecting data is just too difficult for certain types

140
00:11:43,280 --> 00:11:45,160
of applications.

141
00:11:45,160 --> 00:11:51,600
So such an example of using synthetic data for training would be a game changer in these

142
00:11:51,600 --> 00:11:52,600
cases.

143
00:11:52,600 --> 00:12:00,240
Now, I remember hearing about papers that showed that synthetic data wasn't all that

144
00:12:00,240 --> 00:12:07,840
effective at training agents that can perform in the real world, but results like this are

145
00:12:07,840 --> 00:12:10,360
changing that is that.

146
00:12:10,360 --> 00:12:20,280
It has 2018 been kind of a key year in our ability to incorporate synthetic data into training

147
00:12:20,280 --> 00:12:27,880
or is this paper building on successes that we've kind of seen in recent years?

148
00:12:27,880 --> 00:12:35,720
I think in 2018, we had a lot of papers coming out on simulation or simulating data or synthetic

149
00:12:35,720 --> 00:12:42,560
data and different papers write out different techniques to how do you develop the data?

150
00:12:42,560 --> 00:12:49,920
How do you, for example, change the surface or the material of an object?

151
00:12:49,920 --> 00:12:52,080
How do you make sure it looks different from the background?

152
00:12:52,080 --> 00:12:57,840
So there are a lot of papers in this synthetic data field coming up and a lot of them

153
00:12:57,840 --> 00:12:59,240
achieve good results.

154
00:12:59,240 --> 00:13:04,280
And I was talking about this one because they've made it so much more easier and useful

155
00:13:04,280 --> 00:13:06,360
because they have a plug-in tool.

156
00:13:06,360 --> 00:13:11,640
You know, they've got so many options to change from, for example, the object, the background,

157
00:13:11,640 --> 00:13:14,400
the distractors, the viewpoints.

158
00:13:14,400 --> 00:13:22,320
It's, you know, if you can imagine a Photoshop kind of a generator that, you know, that can

159
00:13:22,320 --> 00:13:28,360
spit out image frames based on how you're coding it up.

160
00:13:28,360 --> 00:13:30,000
It's really similar to that.

161
00:13:30,000 --> 00:13:31,920
So what's next on your list?

162
00:13:31,920 --> 00:13:38,560
So this is a really fun but extremely amazing piece of research and it comes from University

163
00:13:38,560 --> 00:13:44,640
of Washington that makes for more commonplace AI applications and it's called soccer on

164
00:13:44,640 --> 00:13:46,600
your tabletop.

165
00:13:46,600 --> 00:13:52,120
So they've developed a system that takes, as input, a YouTube video of a soccer game and

166
00:13:52,120 --> 00:13:58,040
the system outputs a dynamic 3D reconstruction of the game that can be viewed interactively

167
00:13:58,040 --> 00:14:02,800
on your tabletop with an augmented reality device.

168
00:14:02,800 --> 00:14:09,320
So you know how people right now watch matches on their mobile devices, tablets, laptops,

169
00:14:09,320 --> 00:14:12,080
television sets and so on.

170
00:14:12,080 --> 00:14:16,280
Imagine watching it on your dining table, your work table, your kitchen counter literally

171
00:14:16,280 --> 00:14:20,000
everywhere or anywhere for that motto.

172
00:14:20,000 --> 00:14:22,280
So this system is really multimodal.

173
00:14:22,280 --> 00:14:29,040
They are combining different types of information, for example, bounding boxes, poses, trajectories,

174
00:14:29,040 --> 00:14:35,200
all extracted from the player to segment them and these 3D segments are projected onto

175
00:14:35,200 --> 00:14:40,400
any plane which becomes the virtual soccer field.

176
00:14:40,400 --> 00:14:48,640
And they've released an example video of this and on YouTube and it's really cool and

177
00:14:48,640 --> 00:14:51,760
I encourage all of you to check it out.

178
00:14:51,760 --> 00:14:53,400
It's called soccer on your tabletop.

179
00:14:53,400 --> 00:14:58,360
Oh, that sounds incredible, I haven't made that I haven't come across that video.

180
00:14:58,360 --> 00:15:00,800
Yeah, you said definitely check it out now.

181
00:15:00,800 --> 00:15:07,920
So the next one, let's talk a little bit about vision and language coupled together.

182
00:15:07,920 --> 00:15:13,600
So before we talk about the papers, I think it's worth mentioning the second edition of

183
00:15:13,600 --> 00:15:21,280
the VQA Challenge dataset, VQA 2.0, which actually was released in 2017 but it's worth talking

184
00:15:21,280 --> 00:15:28,040
about it because it was a much more balanced dataset and it reduces the language prices

185
00:15:28,040 --> 00:15:33,560
over VQA 1.0 and it's double the size of VQA 1.0.

186
00:15:33,560 --> 00:15:41,200
So the very first paper in VQA is learning to count objects in natural images for visual

187
00:15:41,200 --> 00:15:42,880
question answering.

188
00:15:42,880 --> 00:15:46,880
This is by Jan Zang and others.

189
00:15:46,880 --> 00:15:56,320
So this paper focuses on developing a counting solution for VQA and they use mostly attention

190
00:15:56,320 --> 00:15:57,640
for that.

191
00:15:57,640 --> 00:16:02,200
They've proposed a differentiable counting component which explicitly counts the number

192
00:16:02,200 --> 00:16:10,520
of objects based on a hand design architecture using a graphical object proposals and non-maximum

193
00:16:10,520 --> 00:16:12,360
suppression.

194
00:16:12,360 --> 00:16:17,520
So it's basically just applying non-maximum suppression on object proposals and counting

195
00:16:17,520 --> 00:16:21,720
the number of objects that are within the image.

196
00:16:21,720 --> 00:16:24,560
What is non-maximum suppression?

197
00:16:24,560 --> 00:16:29,880
So it's an algorithm for mostly use for detection.

198
00:16:29,880 --> 00:16:38,040
So this method improves the baseline by about 6.6% on the counting questions.

199
00:16:38,040 --> 00:16:45,440
And I don't remember the statistics in VQA 2.0, but in the VQA 1.0, the counting was

200
00:16:45,440 --> 00:16:51,560
a very big problem and it was extremely biased because most questions, for most counting

201
00:16:51,560 --> 00:16:54,840
questions, the answers was usually two.

202
00:16:54,840 --> 00:16:58,560
So boosting a baseline by 5% is huge.

203
00:16:58,560 --> 00:17:01,080
So I really like this paper for that.

204
00:17:01,080 --> 00:17:08,040
And so with this paper, is the idea that this model that they've developed for counting

205
00:17:08,040 --> 00:17:15,760
would be used as kind of like a submodel for a broader VQA system?

206
00:17:15,760 --> 00:17:16,760
Yes.

207
00:17:16,760 --> 00:17:18,360
So I was actually coming on to that.

208
00:17:18,360 --> 00:17:24,800
The next paper, which is bilinear attention networks for visual question answering, they

209
00:17:24,800 --> 00:17:32,680
have integrated the counting module that we just talked about from Zang, and along with

210
00:17:32,680 --> 00:17:41,360
that, they've introduced other techniques to improve their VQA accuracy.

211
00:17:41,360 --> 00:17:46,240
And this paper is called bilinear attention networks for visual question answering.

212
00:17:46,240 --> 00:17:50,640
It's by Jean Joachim from SEAL National University.

213
00:17:50,640 --> 00:17:53,360
They are using the counting module.

214
00:17:53,360 --> 00:18:00,720
They are also using bilinear attention, which is the interaction between the word and the

215
00:18:00,720 --> 00:18:02,680
visual concepts.

216
00:18:02,680 --> 00:18:10,000
They also have a low rank bilinear pooling and they have residual learning with attention

217
00:18:10,000 --> 00:18:13,880
mechanism for incremental inference.

218
00:18:13,880 --> 00:18:16,160
That sounds a lot.

219
00:18:16,160 --> 00:18:23,320
And I think I'm not sure, but I think this was among the top VQA 2.0 winners.

220
00:18:23,320 --> 00:18:31,240
And then one paper that is one model that is right for the right reasons is women also

221
00:18:31,240 --> 00:18:35,320
snowboard overcoming bias in captioning models.

222
00:18:35,320 --> 00:18:41,120
So they've introduced a new equalizer model that encourages equal gender probability when

223
00:18:41,120 --> 00:18:47,480
gender evidence is occluded in a scene and confident predictions when gender evidence

224
00:18:47,480 --> 00:18:48,880
is present.

225
00:18:48,880 --> 00:18:54,400
So they're basically forcing the resulting model to look at the person rather than use

226
00:18:54,400 --> 00:18:59,080
contextual cues to make a gender specific prediction.

227
00:18:59,080 --> 00:19:05,320
And they've also introduced two kinds of losses, which is the appearance confusion loss and

228
00:19:05,320 --> 00:19:07,480
the confident loss.

229
00:19:07,480 --> 00:19:12,040
Both of them are generalizable and they can be added to any description model in order

230
00:19:12,040 --> 00:19:16,960
to mitigate the impacts of any unwanted bias in the description dataset.

231
00:19:16,960 --> 00:19:21,880
And when we talk about their results, this research gives two things.

232
00:19:21,880 --> 00:19:30,640
They get a lower error than existing work when describing images with people that mentions

233
00:19:30,640 --> 00:19:37,280
the gender and it much more closely matches the ground truth ratio of sentences that include

234
00:19:37,280 --> 00:19:41,000
women to sentences, including men.

235
00:19:41,000 --> 00:19:48,200
And the second achievement is when we visualize the results, we see that the model is actually

236
00:19:48,200 --> 00:19:53,640
looking at the people when predicting their gender, which is really important because a lot

237
00:19:53,640 --> 00:20:02,000
of visualization results, you know, they focus on, they don't necessarily focus on the

238
00:20:02,000 --> 00:20:04,600
people when they are predicting the gender.

239
00:20:04,600 --> 00:20:10,400
So this was a really interesting find by this paper, women also snowboard overcoming

240
00:20:10,400 --> 00:20:12,080
bias and captioning models.

241
00:20:12,080 --> 00:20:14,680
That one sounds really interesting.

242
00:20:14,680 --> 00:20:21,560
The next one is weekly supervised photo-enhanced for digital cameras.

243
00:20:21,560 --> 00:20:24,960
So I think Sam, you enjoy photography rate.

244
00:20:24,960 --> 00:20:30,360
I do enjoy photography, especially digital photography.

245
00:20:30,360 --> 00:20:37,720
Yeah, so this one, I hope you find interesting because they've trained a generator adversarial

246
00:20:37,720 --> 00:20:43,680
network to improve the asphetic quality of standard or normal looking photos.

247
00:20:43,680 --> 00:20:49,680
So let's say I take really bad photos and I put it in this generator adversarial networks

248
00:20:49,680 --> 00:20:55,040
and it comes out looking completely professional looking, you know, applying through the thirds,

249
00:20:55,040 --> 00:21:01,160
improving lightning, having enhancements that you get from image editing software.

250
00:21:01,160 --> 00:21:03,920
And there are two cool parts about this.

251
00:21:03,920 --> 00:21:12,000
The first one is that they're using GANs, so you don't need a pair of good looking and

252
00:21:12,000 --> 00:21:13,880
bad looking images.

253
00:21:13,880 --> 00:21:19,640
You only need a set of good looking images and a set of bad looking images.

254
00:21:19,640 --> 00:21:25,640
And the second part is that because it's weekly supervised, the pair of input and visual

255
00:21:25,640 --> 00:21:28,520
enhanced images isn't necessary.

256
00:21:28,520 --> 00:21:32,480
So there are these two really cool things about this paper on GANs.

257
00:21:32,480 --> 00:21:38,760
And if you look at the results, they are, you know, GANs have become so photorealistic

258
00:21:38,760 --> 00:21:45,200
that it's just, you know, it's crazy to imagine that GANs are once putting out what that

259
00:21:45,200 --> 00:21:51,200
was, you know, not as aesthetically pleasing, but now when you look at these images, it's

260
00:21:51,200 --> 00:21:52,200
just amazing.

261
00:21:52,200 --> 00:22:00,480
Yeah, the original images that you see with GANs are like these kind of grotesque approximations

262
00:22:00,480 --> 00:22:06,480
and now the celebrity work that Nvidia did.

263
00:22:06,480 --> 00:22:11,840
I think that was earlier this year or maybe late last year.

264
00:22:11,840 --> 00:22:19,280
And there's one that's been going around even more recently, the photorealistic images.

265
00:22:19,280 --> 00:22:24,920
I mean, it is getting quite incredible what they're able to do.

266
00:22:24,920 --> 00:22:26,880
Absolutely.

267
00:22:26,880 --> 00:22:33,560
So another people that I really like is called, this is a really long name.

268
00:22:33,560 --> 00:22:34,640
So bear with me.

269
00:22:34,640 --> 00:22:43,200
So it's called efficient interactive annotation of segmentation data sets with polygon RNN.

270
00:22:43,200 --> 00:22:47,080
So this is an interactive annotation tool.

271
00:22:47,080 --> 00:22:52,960
Now if you think about labeling of data sets and we talk about segmentation data, we know

272
00:22:52,960 --> 00:22:58,920
that class labeling needs labeling of each and every pixel in the image.

273
00:22:58,920 --> 00:23:04,480
Now this is quite literally forever if you're talking about millions and millions of

274
00:23:04,480 --> 00:23:05,880
images.

275
00:23:05,880 --> 00:23:11,680
We know that deep neural networks work well when they can feast on a large and fully annotated

276
00:23:11,680 --> 00:23:13,360
data set.

277
00:23:13,360 --> 00:23:18,160
So this paper is really the economical bridge between these two worlds because with polygon

278
00:23:18,160 --> 00:23:26,360
RNN, you can set rough polygon points around each object in the image that you want to annotate.

279
00:23:26,360 --> 00:23:31,640
And then the network will automatically generate the segmentation annotation.

280
00:23:31,640 --> 00:23:36,480
And a big advantage is that a method generalizes well.

281
00:23:36,480 --> 00:23:41,640
So it can be used to create quick and easy annotations for segmentation tasks.

282
00:23:41,640 --> 00:23:48,520
So it's the idea here that typically for image segmentation, we want this to be on a pixel

283
00:23:48,520 --> 00:23:53,760
by pixel basis so that we can kind of train the network very accurately.

284
00:23:53,760 --> 00:23:59,760
But what this is doing is allowing someone who's doing segmentation to just do it on a polygon

285
00:23:59,760 --> 00:24:00,760
basis.

286
00:24:00,760 --> 00:24:09,360
And then the network will map that to what will find more the more accurate pixel based segmentation.

287
00:24:09,360 --> 00:24:13,080
Yes, that is the idea.

288
00:24:13,080 --> 00:24:20,960
But it makes the process of annotations much faster because you don't have to go pixel by

289
00:24:20,960 --> 00:24:21,960
pixel, right?

290
00:24:21,960 --> 00:24:23,480
You only have to give a rough polygon.

291
00:24:23,480 --> 00:24:31,680
And then the system will adapt the polygon to the actual, the edges of the object.

292
00:24:31,680 --> 00:24:32,680
Mm-hmm.

293
00:24:32,680 --> 00:24:33,680
Yeah.

294
00:24:33,680 --> 00:24:36,280
And do you have a sense for this how close do you need to get?

295
00:24:36,280 --> 00:24:39,360
We still need to get within a few pixels.

296
00:24:39,360 --> 00:24:45,360
Is it meant to be kind of a fine tuning mechanism or can you kind of very roughly put a polygon

297
00:24:45,360 --> 00:24:50,720
around, say, a person, a picture of a street scene?

298
00:24:50,720 --> 00:24:52,960
You can make it pretty rough.

299
00:24:52,960 --> 00:24:58,320
I mean, in their examples, they've shown a real imagery and those examples are pretty

300
00:24:58,320 --> 00:24:59,320
neat.

301
00:24:59,320 --> 00:25:00,320
Oh, interesting.

302
00:25:00,320 --> 00:25:01,320
Cool.

303
00:25:01,320 --> 00:25:02,320
Yeah.

304
00:25:02,320 --> 00:25:05,840
I've got one more that I really, actually two more that I really like.

305
00:25:05,840 --> 00:25:06,840
Okay.

306
00:25:06,840 --> 00:25:07,840
Okay.

307
00:25:07,840 --> 00:25:13,640
So this one is super slow mo, high quality estimation of multiple intermediate frames for

308
00:25:13,640 --> 00:25:16,080
video interpolation.

309
00:25:16,080 --> 00:25:21,480
And this comes from University of Massachusetts at Amherst.

310
00:25:21,480 --> 00:25:25,960
So you're given two consecutive frames.

311
00:25:25,960 --> 00:25:32,560
And they apply a method of video interpolation that aims at generating intermediate frames

312
00:25:32,560 --> 00:25:38,080
that are both spatially and temporally coherent sequences.

313
00:25:38,080 --> 00:25:42,560
So internally, it's utilizing optical flow and convolutional neural networks between

314
00:25:42,560 --> 00:25:48,240
frames to interpolate video frames input at 30 frames per second.

315
00:25:48,240 --> 00:25:52,720
And it produces crisp looking results to 40 frames per second.

316
00:25:52,720 --> 00:25:59,120
And they've shown examples of like a bullet going through an egg.

317
00:25:59,120 --> 00:26:04,760
And you know, to a human eye, you don't really notice much.

318
00:26:04,760 --> 00:26:09,440
But when you look at it in super slow modes, it's really breathtaking.

319
00:26:09,440 --> 00:26:16,400
I mean, you can see how the cracks on the egg shell propagate.

320
00:26:16,400 --> 00:26:17,400
And that's really cool.

321
00:26:17,400 --> 00:26:18,400
Oh, wow.

322
00:26:18,400 --> 00:26:19,400
Yeah.

323
00:26:19,400 --> 00:26:20,400
Wow.

324
00:26:20,400 --> 00:26:23,040
And they're doing this from optical flow and CNN's.

325
00:26:23,040 --> 00:26:24,040
Right.

326
00:26:24,040 --> 00:26:25,040
Right.

327
00:26:25,040 --> 00:26:31,640
Against slow-mo data, or is this interpolation happening without specific training in that

328
00:26:31,640 --> 00:26:32,640
way?

329
00:26:32,640 --> 00:26:34,360
It's got training.

330
00:26:34,360 --> 00:26:35,360
OK.

331
00:26:35,360 --> 00:26:43,000
So the network kind of learns what the effect of slow-mo is from slow-mo training data and

332
00:26:43,000 --> 00:26:48,960
then can apply that effect to non-slomo video.

333
00:26:48,960 --> 00:26:49,960
Yeah.

334
00:26:49,960 --> 00:26:50,960
OK.

335
00:26:50,960 --> 00:26:51,960
Very interesting.

336
00:26:51,960 --> 00:26:57,680
And then the GAN-based photo-enhanced, you can start to see how some of these techniques

337
00:26:57,680 --> 00:27:01,280
can work their way into our everyday devices.

338
00:27:01,280 --> 00:27:02,280
Absolutely.

339
00:27:02,280 --> 00:27:11,360
And that's actually a really good point because handheld devices now have GPUs in them.

340
00:27:11,360 --> 00:27:17,920
You know, they have separate accelerators that accelerate CNNs.

341
00:27:17,920 --> 00:27:24,520
So you know, I guess even the mobile industry is expecting developers to use these techniques

342
00:27:24,520 --> 00:27:29,400
because they're putting these heavy GPUs inside these devices.

343
00:27:29,400 --> 00:27:32,160
And the salt started, I think, in 2015.

344
00:27:32,160 --> 00:27:37,240
So if you go around benchmarking all these devices, you notice that in 2015, there is

345
00:27:37,240 --> 00:27:42,840
a sudden jump in hardware acceleration.

346
00:27:42,840 --> 00:27:48,120
And that is when these GPUs were introduced into mobile phones.

347
00:27:48,120 --> 00:27:49,120
OK.

348
00:27:49,120 --> 00:27:51,680
So I'm really excited for all the new applications that are about to come.

349
00:27:51,680 --> 00:27:52,680
Yeah.

350
00:27:52,680 --> 00:27:53,680
Same here.

351
00:27:53,680 --> 00:27:54,680
Yeah.

352
00:27:54,680 --> 00:27:55,680
So you have one more paper?

353
00:27:55,680 --> 00:27:56,680
Yes.

354
00:27:56,680 --> 00:28:02,880
Now, this is called, who let the dogs out, modeling dog behavior from visual data, and

355
00:28:02,880 --> 00:28:05,440
this comes again from the University of Washington.

356
00:28:05,440 --> 00:28:06,440
OK.

357
00:28:06,440 --> 00:28:12,800
So I remember when this paper came out, I found it through a YouTube recommendation.

358
00:28:12,800 --> 00:28:20,200
And a dog was walking around with all these sensors, GoPro and Arduino attached and a number

359
00:28:20,200 --> 00:28:26,200
of sensors on the dog's limbs, and they'd linked an archive paper with it.

360
00:28:26,200 --> 00:28:34,440
And I was really fascinated by what is this dog doing on archive paper.

361
00:28:34,440 --> 00:28:40,080
And then I read it and it's very interesting because when we talk about visually intelligent

362
00:28:40,080 --> 00:28:41,480
agents.

363
00:28:41,480 --> 00:28:48,240
We tend to break that into smaller, more approachable subproblems, like classification, detection

364
00:28:48,240 --> 00:28:49,560
and planning.

365
00:28:49,560 --> 00:28:54,240
But this paper conquers it as a one big problem.

366
00:28:54,240 --> 00:28:59,400
So it takes input images and it produces planning actions.

367
00:28:59,400 --> 00:29:05,360
And again, the data collection is from a dog using a GoPro, Arduino, and a number of

368
00:29:05,360 --> 00:29:08,080
sensors on the dog's limbs.

369
00:29:08,080 --> 00:29:14,280
So they've got feature extractors from CNNs, which they used to get image features from

370
00:29:14,280 --> 00:29:16,240
the video frames, of course.

371
00:29:16,240 --> 00:29:20,960
And then all of this is passed to a set of LSTMs along with a sensor data.

372
00:29:20,960 --> 00:29:25,200
And the system learns and predicts dog's actions.

373
00:29:25,200 --> 00:29:29,440
And in the paper, they've exemplified three particular tasks.

374
00:29:29,440 --> 00:29:31,840
One is acting like a dog.

375
00:29:31,840 --> 00:29:40,320
So you have a previously seen sequence of images and you want to predict what will be the

376
00:29:40,320 --> 00:29:42,720
future movement of the dog.

377
00:29:42,720 --> 00:29:49,840
The second task is planning like a dog where you have a sequence of a source and a destination

378
00:29:49,840 --> 00:29:51,560
locations.

379
00:29:51,560 --> 00:29:58,200
And the goal is to find a sequence of actions that take the dog from the initial, the source

380
00:29:58,200 --> 00:30:01,440
location and the destination location.

381
00:30:01,440 --> 00:30:05,400
The third and the final task is learning from a dog.

382
00:30:05,400 --> 00:30:10,040
So can we learn, learn representations for a third task?

383
00:30:10,040 --> 00:30:17,520
For example, if I'm a dog, can I try to understand if the surface in front of me is walkable?

384
00:30:17,520 --> 00:30:18,840
So stuff like that.

385
00:30:18,840 --> 00:30:26,640
And I think this paper is really interesting because it's like an end-to-end solution,

386
00:30:26,640 --> 00:30:35,040
not it's not breaking up different tasks like planning detection or classification.

387
00:30:35,040 --> 00:30:37,960
It's really treating it as one big intelligent agent.

388
00:30:37,960 --> 00:30:42,160
And I haven't seen those examples in a really long time.

389
00:30:42,160 --> 00:30:46,280
So I think that's why this paper stands out so much.

390
00:30:46,280 --> 00:30:51,880
Thinking of applications of this kind of paper and a thing that jumps out at me is when

391
00:30:51,880 --> 00:30:58,400
you think about these Boston Dynamics robots, how they might, this kind of training might

392
00:30:58,400 --> 00:31:02,160
help them get to something that's more intelligent.

393
00:31:02,160 --> 00:31:08,200
So the Boston Dynamics robots are really, you know, they've got four limbs and like an upper

394
00:31:08,200 --> 00:31:12,360
body part and then a sense of network of cameras and so on.

395
00:31:12,360 --> 00:31:17,640
But when I think about this paper, I think that it's not only applicable to like the Boston

396
00:31:17,640 --> 00:31:28,600
Dynamics robots, but generally all robots because planning, action, learning, you know, coupling

397
00:31:28,600 --> 00:31:35,600
with understanding of the outside world, all these are, you know, very essential parts

398
00:31:35,600 --> 00:31:39,880
of the visually intelligent agent.

399
00:31:39,880 --> 00:31:44,720
And you know, it's not, it's not just a single robot that can benefit from it.

400
00:31:44,720 --> 00:31:51,440
I mean, you know, this system could be easily applicable in, in phones, for example, you

401
00:31:51,440 --> 00:31:58,000
know, for helping blind people or for the deaf community in certain cases.

402
00:31:58,000 --> 00:32:03,600
So it's, it's really exciting to see how end-to-end applications are developing and coming

403
00:32:03,600 --> 00:32:05,880
into real life.

404
00:32:05,880 --> 00:32:12,200
Definitely some interesting papers from, from 2018 and, you know, particularly with

405
00:32:12,200 --> 00:32:18,520
your caveat, this is just the few that came to mind as, particularly meaningful contributions

406
00:32:18,520 --> 00:32:25,200
and, you know, related to the, the kind of scope of, of the elements of the field that

407
00:32:25,200 --> 00:32:27,520
are of most interest to you.

408
00:32:27,520 --> 00:32:28,520
Yeah.

409
00:32:28,520 --> 00:32:30,400
And I think I've already said this.

410
00:32:30,400 --> 00:32:37,400
I don't know how many times, but the speed of publications is just immense.

411
00:32:37,400 --> 00:32:40,960
And the good thing is that it's all quality work that's coming out.

412
00:32:40,960 --> 00:32:46,160
So keeping up with all of this is so much more challenging than it used to be before.

413
00:32:46,160 --> 00:32:54,280
One of the things that we wanted to talk about was kind of your perspective on the different

414
00:32:54,280 --> 00:32:57,880
kind of research accomplishments of 2018.

415
00:32:57,880 --> 00:32:59,280
But that's been difficult for you.

416
00:32:59,280 --> 00:33:03,640
You said there's so many interesting papers in 2018.

417
00:33:03,640 --> 00:33:08,600
So here's an idea, you know, we know that there are different stages in the AI pipeline.

418
00:33:08,600 --> 00:33:12,720
Like get a collection, labeling, training, and so on.

419
00:33:12,720 --> 00:33:17,360
And then there are some new entries, like modern reproducibility, phoenix, and biasing.

420
00:33:17,360 --> 00:33:23,200
And so why don't we sort of use these stages as a demarcation?

421
00:33:23,200 --> 00:33:27,920
And in each talk about papers and tools and new research that have come up.

422
00:33:27,920 --> 00:33:29,440
Oh, that sounds great.

423
00:33:29,440 --> 00:33:33,800
And again, I think like you've been doing through all the Tremelai AI talks, we will try

424
00:33:33,800 --> 00:33:38,800
to link all the tools and use cases so people can read about them a little more.

425
00:33:38,800 --> 00:33:44,080
You know, if you consider data as the new oil, then labeled data really becomes the new

426
00:33:44,080 --> 00:33:45,240
gold.

427
00:33:45,240 --> 00:33:49,040
And that brings us to a first category, which is data collection and labeling.

428
00:33:49,040 --> 00:33:59,440
So AWS released something called AWS Deep Racer, which is a small scale race car that

429
00:33:59,440 --> 00:34:04,920
gives you an interesting way to get started with reinforcement learning.

430
00:34:04,920 --> 00:34:10,200
So as we all know, reinforcement learning takes a very different approach to training models

431
00:34:10,200 --> 00:34:18,040
than other machine learning techniques, because it learns complex behaviors without requiring

432
00:34:18,040 --> 00:34:20,520
any labeled training data.

433
00:34:20,520 --> 00:34:25,680
And at the same time, you can make short-term decisions while optimizing for a longer-term

434
00:34:25,680 --> 00:34:32,720
goal. And especially with AWS Deep Racer, you can get hands-on reinforcement learning.

435
00:34:32,720 --> 00:34:38,720
You can do a lot of experimentation with cloud-based 3D-racing simulators.

436
00:34:38,720 --> 00:34:43,840
You can raise your friends all while tapping your toes into autonomous driving.

437
00:34:43,840 --> 00:34:51,080
I think AWS Deep Racer really comes out as a top-new tool that Amazon has released this

438
00:34:51,080 --> 00:34:52,080
year.

439
00:34:52,080 --> 00:34:57,840
Yeah, I was at re-invent last week when they announced it.

440
00:34:57,840 --> 00:35:06,960
And they also announced a new extension to their SageMaker tool that's focused on reinforcement

441
00:35:06,960 --> 00:35:07,960
learning.

442
00:35:07,960 --> 00:35:12,640
And I'm really glad you brought up RL in this context.

443
00:35:12,640 --> 00:35:19,480
I think when I think about the work that OpenAI and Deep Mind are doing around reinforcement

444
00:35:19,480 --> 00:35:28,120
learning, for those organizations, I think, a lot of their motivation is that we kind

445
00:35:28,120 --> 00:35:33,520
of, as humans kind of learn in a reinforcement learning way, we don't have labeled training

446
00:35:33,520 --> 00:35:34,520
data.

447
00:35:34,520 --> 00:35:38,720
We kind of explore our worlds and learn.

448
00:35:38,720 --> 00:35:47,040
And so they are aggressively pursuing reinforcement learning as a stepping stone to AGI.

449
00:35:47,040 --> 00:35:56,120
And I think for most of us, the practical implications of RL or that kind of gets us,

450
00:35:56,120 --> 00:36:01,520
it sidesteps this need that we usually have for label training data.

451
00:36:01,520 --> 00:36:06,600
And in the near future, that's going to be its big contribution.

452
00:36:06,600 --> 00:36:14,400
So I'm particularly excited that AWS, which has so much weight in the space, is starting

453
00:36:14,400 --> 00:36:19,880
to kind of shine the light on reinforcement learning and get people, you know, starting

454
00:36:19,880 --> 00:36:22,560
down the path to experimenting with it.

455
00:36:22,560 --> 00:36:23,560
Yeah.

456
00:36:23,560 --> 00:36:30,400
And now that you've mentioned, you know, going into the future, how will we have labels

457
00:36:30,400 --> 00:36:31,400
available?

458
00:36:31,400 --> 00:36:38,520
There's work done by the Microsoft machine learning team and the Apache Spark community.

459
00:36:38,520 --> 00:36:45,960
And they actually worked on creating a deep distributed object detector that works without

460
00:36:45,960 --> 00:36:50,440
any human labels or human generated labels.

461
00:36:50,440 --> 00:36:56,040
Now, you know, if anyone who has ever trained their own detector knows how incredibly

462
00:36:56,040 --> 00:37:01,440
painful it is to get the bounding boxes and the labels right, but now thanks to a technology

463
00:37:01,440 --> 00:37:05,560
called Lime, we can work without any data.

464
00:37:05,560 --> 00:37:10,760
And Lime is local interpretable model agnostic explanations.

465
00:37:10,760 --> 00:37:17,240
And this was built by Marco Ribeiro and a team from University of Washington.

466
00:37:17,240 --> 00:37:22,640
And it helps in understanding the classification of any image classifier.

467
00:37:22,640 --> 00:37:27,320
So it's really telling us where the classifier is looking.

468
00:37:27,320 --> 00:37:28,800
And then here's the coolest thing ever.

469
00:37:28,800 --> 00:37:32,640
It makes no assumptions about the kind of model.

470
00:37:32,640 --> 00:37:37,720
So you can use it for your own secret model, random published models that you downloaded

471
00:37:37,720 --> 00:37:44,320
from the internet or even a very patient human classifier, which means that it has extremely

472
00:37:44,320 --> 00:37:50,960
wide applicability, not just across models, but also across domains.

473
00:37:50,960 --> 00:37:58,480
So Lime was originally published a few years ago, I think, but you're saying that folks

474
00:37:58,480 --> 00:38:08,440
are now using it to beyond its initial intent of model explainability, but to allow you

475
00:38:08,440 --> 00:38:12,320
to create classifiers without label training data.

476
00:38:12,320 --> 00:38:13,320
Yeah.

477
00:38:13,320 --> 00:38:14,920
So I was actually coming to that.

478
00:38:14,920 --> 00:38:21,400
So Lime has a huge drawback that it is extremely computationally intensive.

479
00:38:21,400 --> 00:38:28,200
So what the Microsoft machine learning and the Apache Spark community did was they

480
00:38:28,200 --> 00:38:33,520
made this available in a distributed implementation and SparkML.

481
00:38:33,520 --> 00:38:37,320
And that's how you can make it more real time.

482
00:38:37,320 --> 00:38:45,800
And because of that, you can create image classifiers for classification or detection from

483
00:38:45,800 --> 00:38:47,880
bounding boxes.

484
00:38:47,880 --> 00:38:53,760
And they experimented with this on a real life use case for the conservation of snow leopards

485
00:38:53,760 --> 00:38:55,000
in Krigaston.

486
00:38:55,000 --> 00:38:56,960
So that was pretty interesting.

487
00:38:56,960 --> 00:39:03,880
And so can you give me a sense for how the use case works or what the flow is in working

488
00:39:03,880 --> 00:39:05,280
with this tool?

489
00:39:05,280 --> 00:39:06,280
Yeah.

490
00:39:06,280 --> 00:39:12,280
So it's basically you have all this data, which is not labeled.

491
00:39:12,280 --> 00:39:20,080
So you have plain basic images, which can be, you know, without the actual object, which

492
00:39:20,080 --> 00:39:25,640
is a snow leopard in this case, or it can include the snow leopard.

493
00:39:25,640 --> 00:39:28,880
And you run Lime over all these images.

494
00:39:28,880 --> 00:39:33,760
And then you ask simple questions like, what is the most common object that you're seeing

495
00:39:33,760 --> 00:39:35,960
over all the data?

496
00:39:35,960 --> 00:39:40,480
Where is the object located in most of these images?

497
00:39:40,480 --> 00:39:47,080
And that's how the system really learns what is most common, what it should be looking

498
00:39:47,080 --> 00:39:48,240
for.

499
00:39:48,240 --> 00:39:55,160
And once you have these questions answered, you can use that sort of like a label.

500
00:39:55,160 --> 00:39:58,920
And without any human actually working on labeling.

501
00:39:58,920 --> 00:40:02,920
And then you can generate the entire system of object detection with that.

502
00:40:02,920 --> 00:40:04,080
Oh, interesting.

503
00:40:04,080 --> 00:40:05,080
Yeah.

504
00:40:05,080 --> 00:40:07,320
And then again, all of this is real time.

505
00:40:07,320 --> 00:40:08,320
It's distributed.

506
00:40:08,320 --> 00:40:10,440
So it's pretty quick.

507
00:40:10,440 --> 00:40:17,880
And if I remember correctly, they had a statistic in their published work, which was that

508
00:40:17,880 --> 00:40:24,560
it takes, if it takes one hour for you to evaluate your model on a particular data set,

509
00:40:24,560 --> 00:40:32,040
then it would take 50 days worth of computation to convert these predictions to interpretations.

510
00:40:32,040 --> 00:40:33,840
And now that doesn't sound attractive at all.

511
00:40:33,840 --> 00:40:39,360
And that is where the improvement is coming by making it a more distributed implementation

512
00:40:39,360 --> 00:40:41,520
and packaging it in SparkML.

513
00:40:41,520 --> 00:40:42,520
Okay.

514
00:40:42,520 --> 00:40:43,520
Very cool.

515
00:40:43,520 --> 00:40:44,520
Yeah.

516
00:40:44,520 --> 00:40:54,360
And then a Christmas miracle is, when we talk about labeling, is two companies, digital

517
00:40:54,360 --> 00:40:59,040
divide data by Jeremy Hawkinsstein and Samir Reyna.

518
00:40:59,040 --> 00:41:00,440
And I met it.

519
00:41:00,440 --> 00:41:05,760
Now both these are nonprofit companies that provide labeling solutions.

520
00:41:05,760 --> 00:41:10,600
But you know, when I'm talking about it right now, it probably sounds like the million

521
00:41:10,600 --> 00:41:13,400
other labeling companies out there.

522
00:41:13,400 --> 00:41:16,160
But here is really where the heart is.

523
00:41:16,160 --> 00:41:20,880
They train and employ people who have no other sources of employment.

524
00:41:20,880 --> 00:41:24,840
And so the most part don't have access to higher education.

525
00:41:24,840 --> 00:41:28,880
These people learn by doing and they earn enough money to support themselves and eventually

526
00:41:28,880 --> 00:41:37,360
hone enough skills to go for certifications or undergraduate cases.

527
00:41:37,360 --> 00:41:43,320
And I think digital data divide mentioned that they have uplifted more than 3,000 individuals,

528
00:41:43,320 --> 00:41:46,200
their families and communities all around the world.

529
00:41:46,200 --> 00:41:47,520
That's pretty interesting.

530
00:41:47,520 --> 00:41:48,520
Oh yeah.

531
00:41:48,520 --> 00:41:54,520
And we talk about labeling in terms of how it's really impacting the world.

532
00:41:54,520 --> 00:41:55,520
It's interesting.

533
00:41:55,520 --> 00:42:00,280
We talk a lot about AI for social good.

534
00:42:00,280 --> 00:42:04,360
And it's usually the, you know, we're thinking about the AI itself.

535
00:42:04,360 --> 00:42:10,360
But now this is an example of how even the process of creating AI can be beneficial to

536
00:42:10,360 --> 00:42:11,360
communities.

537
00:42:11,360 --> 00:42:12,360
Yeah.

538
00:42:12,360 --> 00:42:14,680
I mean, imagine what these people can achieve in the future.

539
00:42:14,680 --> 00:42:16,400
It's just limitless.

540
00:42:16,400 --> 00:42:24,360
And then there is another tool called Prodigy, which is helping solve a really big issue.

541
00:42:24,360 --> 00:42:31,800
Now we all know that the AI workflow doesn't really follow what fault development strategy.

542
00:42:31,800 --> 00:42:37,000
It's more like a chicken and egg problem because you can't really start experimentation

543
00:42:37,000 --> 00:42:41,640
until you have at least the first batch of annotations.

544
00:42:41,640 --> 00:42:47,160
And the annotation team can start until they receive the annotation manuals and to produce

545
00:42:47,160 --> 00:42:52,720
the annotation manuals, you need to know what statistical models you need based on the

546
00:42:52,720 --> 00:42:54,600
features you're trying to build.

547
00:42:54,600 --> 00:43:00,360
So all in all machine learning is an inherently uncertain technology, whereas the waterfall

548
00:43:00,360 --> 00:43:05,200
annotation process really relies on accurate and upfront planning.

549
00:43:05,200 --> 00:43:11,440
So what Prodigy does, honestly, living up to its name, it solves this problem by letting

550
00:43:11,440 --> 00:43:16,640
data scientists conduct their own annotations for rapid prototyping.

551
00:43:16,640 --> 00:43:22,480
It puts the model in the loop so it can actively participate in the training process and learns

552
00:43:22,480 --> 00:43:25,640
as you go in an active learning kind of a setup.

553
00:43:25,640 --> 00:43:31,600
It has AB evaluations and a whole suite of tools to help you prototype much faster, bringing

554
00:43:31,600 --> 00:43:36,880
in the much needed increased agility.

555
00:43:36,880 --> 00:43:42,240
And what's really awesome about Prodigy is that it's written by the authors of Spacey,

556
00:43:42,240 --> 00:43:45,160
which is a really cool NLP library.

557
00:43:45,160 --> 00:43:47,760
I've heard quite a bit about Spacey.

558
00:43:47,760 --> 00:43:54,360
And so what's the user experience of working with Prodigy?

559
00:43:54,360 --> 00:43:57,160
Prodigy is extremely efficient.

560
00:43:57,160 --> 00:43:59,760
It's got, you know, you can use it via command line.

561
00:43:59,760 --> 00:44:01,760
It's also got a beautiful UI support.

562
00:44:01,760 --> 00:44:09,200
It helps in both annotation and training, and I've tested it out for detection problems.

563
00:44:09,200 --> 00:44:16,160
And it's really easy to, you know, get like 10, 20 images to find what object you want

564
00:44:16,160 --> 00:44:23,640
to detect, give a few bounding boxes and see how the model predicts the bounding boxes

565
00:44:23,640 --> 00:44:29,920
on the next couple of set of images and then use those as a bigger data set for training.

566
00:44:29,920 --> 00:44:32,600
Huh, interesting.

567
00:44:32,600 --> 00:44:40,160
That sounds like it would take a lot of the early effort out of kind of experimentation.

568
00:44:40,160 --> 00:44:47,680
I remember going through the fast.ai course and, you know, this whole process of like searching

569
00:44:47,680 --> 00:44:55,160
for images on, you know, like Google images and, you know, labeling them and putting them

570
00:44:55,160 --> 00:45:01,800
in the right, you know, the right folders and all this stuff can be pretty time consuming.

571
00:45:01,800 --> 00:45:03,200
Yeah, that's true.

572
00:45:03,200 --> 00:45:10,000
And what's even better is because it's got an active learning setup, it can figure out

573
00:45:10,000 --> 00:45:15,960
which of the following images is something that, you know, if it's included in the training

574
00:45:15,960 --> 00:45:21,600
data set, it will perform better rather than, you know, adding images that don't really

575
00:45:21,600 --> 00:45:23,600
improve the accuracy.

576
00:45:23,600 --> 00:45:30,560
On that note, AWS announced something in the same vein at re-invent SageMaker Ground

577
00:45:30,560 --> 00:45:35,840
Truth, which uses active learning in the same way.

578
00:45:35,840 --> 00:45:45,160
It basically is a labeling pipeline that incorporates human and loop laborers, which can

579
00:45:45,160 --> 00:45:52,480
be mechanical Turk or some of their partners, maybe some of the companies that you previously

580
00:45:52,480 --> 00:45:54,560
mentioned are included in that.

581
00:45:54,560 --> 00:46:02,440
They also use active learning to try to make the labeling process more efficient by only

582
00:46:02,440 --> 00:46:11,920
labeling the images that will, actually, I don't even recall if it's specific to vision

583
00:46:11,920 --> 00:46:19,840
or if it's broader, but only labeling the kind of the data that will add the most value.

584
00:46:19,840 --> 00:46:26,000
That's true, it's definitely available in AWS now.

585
00:46:26,000 --> 00:46:34,000
It's also been, if I'm not wrong, available in Google Cloud from their AutoML team.

586
00:46:34,000 --> 00:46:38,640
And internally, they use Crowdflower or what is now known as Figure 8.

587
00:46:38,640 --> 00:46:43,480
And that also employs a method of active learning, if I remember correctly, and hopefully

588
00:46:43,480 --> 00:46:44,480
I am.

589
00:46:44,480 --> 00:46:51,600
But yeah, so active learning for labeling has been a top priority, I think, for companies

590
00:46:51,600 --> 00:46:52,600
this year.

591
00:46:52,600 --> 00:46:55,080
At least that's what it seems like to an onlooker.

592
00:46:55,080 --> 00:46:56,080
I agree.

593
00:46:56,080 --> 00:46:57,080
Yeah.

594
00:46:57,080 --> 00:47:04,560
Yeah, and then a really interesting trend that's up in coming is gamification of labeling,

595
00:47:04,560 --> 00:47:09,080
which keeps the interest going in human and the loop labeling.

596
00:47:09,080 --> 00:47:15,880
So what you do is, you know, you put these certain objects that you're looking to detect

597
00:47:15,880 --> 00:47:23,280
or classify at really weird places that you wouldn't expect.

598
00:47:23,280 --> 00:47:30,720
And we know that convolutional neural networks, they depend not only on the object, but also

599
00:47:30,720 --> 00:47:35,000
on the usual surroundings of that object.

600
00:47:35,000 --> 00:47:42,080
So it kind of, you know, focuses the network to try to concentrate on the object itself

601
00:47:42,080 --> 00:47:50,160
rather than its surroundings, because you are getting so much variation in the surroundings.

602
00:47:50,160 --> 00:47:55,800
Michael Arthur, I know that actually talks about this in his TEDx talk, and it's really

603
00:47:55,800 --> 00:48:02,560
interesting to see how companies are incorporating gamification in their annotation cycles.

604
00:48:02,560 --> 00:48:12,360
And so is this gamification for human laborers or gamification somehow trying to gain the

605
00:48:12,360 --> 00:48:13,800
neural networks themselves?

606
00:48:13,800 --> 00:48:23,160
So it's kind of both, because you are looking for these certain images that, you know,

607
00:48:23,160 --> 00:48:31,880
you know, will not work as well, because the object isn't in their natural environment.

608
00:48:31,880 --> 00:48:32,880
So that's one.

609
00:48:32,880 --> 00:48:39,560
And then second, it also keeps the interest of the human laborers really ongoing in the

610
00:48:39,560 --> 00:48:45,480
system, because, you know, they are so familiar with the system, they know how to break

611
00:48:45,480 --> 00:48:49,960
it, what are the kind of images that will really break the system, because they are the

612
00:48:49,960 --> 00:48:56,760
ones who've gone through the entire data distribution that the model really depends on.

613
00:48:56,760 --> 00:48:58,360
So it's a bit of both.

614
00:48:58,360 --> 00:49:07,720
It's interesting, it makes me think of the humans and the network in some kind of adversarial

615
00:49:07,720 --> 00:49:11,640
relationship, you know, separated by distance and time.

616
00:49:11,640 --> 00:49:15,960
Yeah, I mean, I actually think it did the same way.

617
00:49:15,960 --> 00:49:16,960
Interesting.

618
00:49:16,960 --> 00:49:22,160
I think we should move on to your next category to make sure we can get everything in.

619
00:49:22,160 --> 00:49:23,160
Okay.

620
00:49:23,160 --> 00:49:24,160
Awesome.

621
00:49:24,160 --> 00:49:32,160
And the category would be training and training, I think there have been a lot of frameworks

622
00:49:32,160 --> 00:49:42,760
which have come out, which help to do hyperparameterization, which help in, you know, maintaining

623
00:49:42,760 --> 00:49:48,840
like a GitHub equivalent of the entire training cycle.

624
00:49:48,840 --> 00:49:49,840
Okay.

625
00:49:49,840 --> 00:49:59,280
And then moving on to an open source experimentation, optimization, library, which is called hyper

626
00:49:59,280 --> 00:50:00,280
OPT.

627
00:50:00,280 --> 00:50:08,800
It's a distributed asynchronous hyperparameterization, optimization, library and Python.

628
00:50:08,800 --> 00:50:15,960
And it does both serial and parallel optimizations over certain spaces, which can be real value,

629
00:50:15,960 --> 00:50:18,760
discrete and conditional dimensions.

630
00:50:18,760 --> 00:50:27,640
It hyperass is actually a very simple, convenient wrapper around hyper OPT, foster prototyping with

631
00:50:27,640 --> 00:50:28,640
Keras models.

632
00:50:28,640 --> 00:50:34,400
And I think hyperass is something that's, that I feel, you know, it can really be a game

633
00:50:34,400 --> 00:50:40,800
changer because a lot of people when they start in deep learning, they don't, you know,

634
00:50:40,800 --> 00:50:45,400
they don't really learn about how do you actually tune the hyperparameters?

635
00:50:45,400 --> 00:50:49,080
What is the initial value at which you want to start?

636
00:50:49,080 --> 00:50:54,640
But with hyperass, you know, you can define an entire hyperparameter range and then internally

637
00:50:54,640 --> 00:51:00,480
it will learn which is the best value for each particular parameter.

638
00:51:00,480 --> 00:51:06,400
And this has really the only difference that, or the only additional thing that you would

639
00:51:06,400 --> 00:51:12,000
have to do apart from just defining your model as you already do in Keras.

640
00:51:12,000 --> 00:51:17,960
And then you can also run multiple models, training, testing in parallel while you're

641
00:51:17,960 --> 00:51:19,680
using MongoDB as a backend.

642
00:51:19,680 --> 00:51:26,440
So hyperass has become really cool, I think, for companies, you know, that are really

643
00:51:26,440 --> 00:51:29,520
looking for a small-scale solution.

644
00:51:29,520 --> 00:51:30,520
Interesting.

645
00:51:30,520 --> 00:51:39,720
Yet one of the other things that I've repeatedly seen as a best practice in this experiment

646
00:51:39,720 --> 00:51:45,800
management category is automating hyperparameter optimization.

647
00:51:45,800 --> 00:51:52,400
I think a lot of people think of that, or historically, a thought of hyperparameter optimization

648
00:51:52,400 --> 00:51:57,840
as this, you know, the cherry on the cake or the icing on the cake or something like the

649
00:51:57,840 --> 00:52:04,440
thing that you do last to kind of eke, you know, a few more percentage points, or, you

650
00:52:04,440 --> 00:52:08,800
know, a few more points of accuracy or performance out of your models.

651
00:52:08,800 --> 00:52:19,760
But all of the folks that I'm seeing doing this bake it very deeply into their experimentation

652
00:52:19,760 --> 00:52:28,080
pipelines so that you're really automating this optimization throughout the, you know,

653
00:52:28,080 --> 00:52:33,040
the whole experimentation cycle as opposed to kind of just at the very end.

654
00:52:33,040 --> 00:52:45,160
And if you parameterize your models correctly, the hyperparameter optimization can help you

655
00:52:45,160 --> 00:52:50,680
kind of define and evolve your architectures as well.

656
00:52:50,680 --> 00:52:59,080
So another product out in this base is Sig Opt, who is a sponsor of this Agile Machine

657
00:52:59,080 --> 00:53:05,160
Learning Platforms eBook that I referred to earlier.

658
00:53:05,160 --> 00:53:13,280
And they've also been, they've presented at the, the Twoma Online Meetup, I think that

659
00:53:13,280 --> 00:53:18,880
might have been last year, about some of the things you're doing, but I definitely expect

660
00:53:18,880 --> 00:53:25,880
to see a lot more folks working with the, with hyperparameter optimization and kind of

661
00:53:25,880 --> 00:53:30,800
building it deeply into their, their pipelines.

662
00:53:30,800 --> 00:53:31,800
Yeah.

663
00:53:31,800 --> 00:53:38,200
And another thing, interesting to note here is that, you know, 2018 has been a year of

664
00:53:38,200 --> 00:53:46,680
neural architecture search and combining that with Hyperass is just, you know, taking

665
00:53:46,680 --> 00:53:51,880
training and optimization to a whole new level, because you're not only optimizing the

666
00:53:51,880 --> 00:53:57,160
parameters, you're also optimizing the entire architecture, which is, I mean, architecture

667
00:53:57,160 --> 00:54:02,600
definition is always something that, you know, it's, it's really difficult to get right.

668
00:54:02,600 --> 00:54:06,680
And you never know, is this exactly right?

669
00:54:06,680 --> 00:54:11,480
Or, you know, if I remove some layers, will it still be the same?

670
00:54:11,480 --> 00:54:15,600
So that's, you know, that's a problem that I don't know if there's any solution for

671
00:54:15,600 --> 00:54:20,840
it, but neural architecture search is definitely amazing.

672
00:54:20,840 --> 00:54:25,440
I mean, it's absolutely stunning how they've been able to achieve it.

673
00:54:25,440 --> 00:54:27,680
And it's results are amazing.

674
00:54:27,680 --> 00:54:28,680
Yeah.

675
00:54:28,680 --> 00:54:33,680
There was a period of time on the podcast where I would, I would ask every guest in this

676
00:54:33,680 --> 00:54:36,160
space, like, how do you come up with architectures?

677
00:54:36,160 --> 00:54:37,160
Where do they come from?

678
00:54:37,160 --> 00:54:40,360
I probably asked you this.

679
00:54:40,360 --> 00:54:47,600
And the conclusion that I came to, you know, a year ago, a couple of years ago, is that,

680
00:54:47,600 --> 00:54:51,680
you know, it's a little bit of black magic, like, you know, people just try a bunch of

681
00:54:51,680 --> 00:54:54,120
things and see what works.

682
00:54:54,120 --> 00:54:57,840
And I think what's interesting about what we're seeing now with the neural architecture

683
00:54:57,840 --> 00:55:05,120
search is that, you know, we're bringing a little bit more of science and methodology

684
00:55:05,120 --> 00:55:06,120
back to it.

685
00:55:06,120 --> 00:55:07,120
Yeah.

686
00:55:07,120 --> 00:55:08,120
Yeah.

687
00:55:08,120 --> 00:55:11,440
It's, it's a little more of, you know, the software engineering techniques that we've

688
00:55:11,440 --> 00:55:17,480
all been learning and incorporating those into the AI development cycle.

689
00:55:17,480 --> 00:55:18,480
Right.

690
00:55:18,480 --> 00:55:19,480
Right.

691
00:55:19,480 --> 00:55:20,480
Yeah.

692
00:55:20,480 --> 00:55:25,760
And I think another tool that's worth mentioning is Rapids from Nvidia.

693
00:55:25,760 --> 00:55:33,160
So, you know, we talk about how training has been accelerated by GPUs, how inference,

694
00:55:33,160 --> 00:55:38,400
for inference, you have a different kind of GPUs or different kind of hardware on which

695
00:55:38,400 --> 00:55:40,360
you want to do inference.

696
00:55:40,360 --> 00:55:49,600
But Nvidia's Rapids is really a suite of software libraries that allow the execution of the

697
00:55:49,600 --> 00:55:54,520
entire end-to-end data science pipeline entirely on GPUs.

698
00:55:54,520 --> 00:56:01,400
So it can be data processing, training, testing, loading, anything.

699
00:56:01,400 --> 00:56:07,240
Everything is inbuilt onto GPUs and of course, internally it's using CUDA primitives for

700
00:56:07,240 --> 00:56:13,760
low-level compute optimization and it exposes the GPU parallelism that we all love.

701
00:56:13,760 --> 00:56:17,240
And it's got really high bandwidth memory speed.

702
00:56:17,240 --> 00:56:23,080
It exposes very easily usable Python interfaces.

703
00:56:23,080 --> 00:56:28,080
And because it's been focusing on a lot on data preparation, which I know is a really,

704
00:56:28,080 --> 00:56:30,760
you know, I mean, I've been struggling with data preparation.

705
00:56:30,760 --> 00:56:37,200
How do you load such heavy data when you know that this data said won't fit into memory?

706
00:56:37,200 --> 00:56:42,640
So you know, these kind of problems that people face every day, Nvidia Rapids has actually

707
00:56:42,640 --> 00:56:48,640
a solution to it because it's introduced the data frame API that integrates with existing

708
00:56:48,640 --> 00:56:54,880
machine learning algorithms for the complete end-to-end pipeline acceleration.

709
00:56:54,880 --> 00:57:04,160
And you know, you forego the serialization costs which you would have to endure otherwise.

710
00:57:04,160 --> 00:57:10,040
I came across the Rapids announcement, which was relatively recently, but never quite

711
00:57:10,040 --> 00:57:14,360
had a chance to dig into it and see what they're doing, I think.

712
00:57:14,360 --> 00:57:21,960
I don't know if it was Rapids or something else that one of the big points about it was

713
00:57:21,960 --> 00:57:30,880
that, you know, we kind of think about GPUs as, you know, having primary, you know, being

714
00:57:30,880 --> 00:57:37,600
primarily useful for neural networks, but they went ahead and created GPU-accelerated versions

715
00:57:37,600 --> 00:57:44,640
of a bunch of traditional machine learning operations like, yeah, okay, yeah, yeah.

716
00:57:44,640 --> 00:57:49,120
And then, you know, it's also available for multi-node, multi-GPU deployment.

717
00:57:49,120 --> 00:57:55,240
So, you know, if your work is scaling up, Rapids is the best, you know, sort of software

718
00:57:55,240 --> 00:58:02,000
libraries that you can use to scale up down as your requirement changes.

719
00:58:02,000 --> 00:58:07,120
You know, when I think about when I started off learning about machine learning and day

720
00:58:07,120 --> 00:58:13,000
planning, we, I think we didn't have anything like this.

721
00:58:13,000 --> 00:58:19,600
And, you know, today I feel like, you know, people have so many sources to learn from.

722
00:58:19,600 --> 00:58:28,200
And, you know, be it fast.ai, Kaggle or deep learning.ai or even Siraj Ravel School

723
00:58:28,200 --> 00:58:29,200
of AI.

724
00:58:29,200 --> 00:58:35,680
There's just so much to learn and so much to do that I feel 2019 is going to be a year

725
00:58:35,680 --> 00:58:41,280
of, you know, every single day, there's going to be something new because it seems like

726
00:58:41,280 --> 00:58:43,280
that sometimes doesn't it?

727
00:58:43,280 --> 00:58:49,200
Yeah, I mean, sometimes it feels like, you know, we're on an episode of, um, um,

728
00:58:49,200 --> 00:58:55,680
Silicon Valley because every single day something is coming up and, you know, one day it's the

729
00:58:55,680 --> 00:58:59,560
state of the art in the next day, there's another state of the art.

730
00:58:59,560 --> 00:59:03,640
And then we have testing and deployment.

731
00:59:03,640 --> 00:59:08,720
And I honestly feel that, you know, as an AI, as a member of the AI community, testing

732
00:59:08,720 --> 00:59:16,120
and deployment have become really, um, like even though there's a lot of focus on training,

733
00:59:16,120 --> 00:59:21,640
and deployment have become even more important these days because, you know, that's where

734
00:59:21,640 --> 00:59:24,160
things are actually in production.

735
00:59:24,160 --> 00:59:30,960
So for example, if you talk about Onyx, Onyx came a couple years ago when Facebook and,

736
00:59:30,960 --> 00:59:36,760
um, Microsoft said, you know, why do you want to limit yourself to only one framework?

737
00:59:36,760 --> 00:59:40,240
And we want to promote model interoperability.

738
00:59:40,240 --> 00:59:52,080
So Onyx came through and now if you look at the benchmarks for Onyx.js, you know, TensorFlow.js

739
00:59:52,080 --> 00:59:58,440
and Keras.js are not even understanding when it comes to how ridiculously fast Onyx.js

740
00:59:58,440 --> 01:00:00,440
is.

741
01:00:00,440 --> 01:00:06,640
And, you know, all this comes through years of research and experimentation when we realize

742
01:00:06,640 --> 01:00:11,160
that, you know, we want to build smaller models when the model size is smaller.

743
01:00:11,160 --> 01:00:13,960
You can have smaller image sizes.

744
01:00:13,960 --> 01:00:21,280
You can use the inherent hardware acceleration in JavaScript and, you know, literally within

745
01:00:21,280 --> 01:00:29,280
the snap of a finger, you can have an entire batch of images, tested and predictions made

746
01:00:29,280 --> 01:00:31,280
on.

747
01:00:31,280 --> 01:00:35,800
And now the way to actually talking about browsers, I have to mention the amazing work

748
01:00:35,800 --> 01:00:43,960
by Osramos, who is using OpenPose to track eyes to help navigate through a browser.

749
01:00:43,960 --> 01:00:48,400
And of course, there's, there is a limit to how much is possible.

750
01:00:48,400 --> 01:00:54,720
But imagine the benefits that people with impairments can read from this.

751
01:00:54,720 --> 01:01:04,680
And, um, you know, the inspiration behind Osramos working on this is also really, um, encouraging.

752
01:01:04,680 --> 01:01:13,760
And, you know, he was a veteran who sadly lost his home and, um, you know, he was seeing

753
01:01:13,760 --> 01:01:23,080
people around him who weren't really capable, um, physically to, you know, use their systems

754
01:01:23,080 --> 01:01:26,000
or laptops or, um, desktops.

755
01:01:26,000 --> 01:01:31,840
So he decided to use an eye tracking software to actually help people navigate browsers

756
01:01:31,840 --> 01:01:32,840
and so on.

757
01:01:32,840 --> 01:01:37,840
I believe he's on patron and, you know, he's doing a lot of cool and interesting work.

758
01:01:37,840 --> 01:01:40,840
Uh, so maybe switching gears a little bit.

759
01:01:40,840 --> 01:01:48,840
Uh, I'm curious what you foresee for 2019 in computer vision.

760
01:01:48,840 --> 01:01:58,320
Um, a lot of people are expecting GANs, you know, to really be in, um, videos, for example.

761
01:01:58,320 --> 01:02:02,960
I mean, this is more evolutionary rather than revolutionary because when you look at the

762
01:02:02,960 --> 01:02:07,640
current state of the art, GANs have achieved almost photorealistic results and it's just

763
01:02:07,640 --> 01:02:08,960
amazing.

764
01:02:08,960 --> 01:02:12,800
And I can't wait to see what's next with GANs and hopefully it's going to be something

765
01:02:12,800 --> 01:02:13,800
in videos.

766
01:02:13,800 --> 01:02:14,800
Mm hmm.

767
01:02:14,800 --> 01:02:21,720
And then, um, in robotic perception, because we know robots are really good at doing only

768
01:02:21,720 --> 01:02:27,120
one thing right now, but when you talk about general understanding, they don't really

769
01:02:27,120 --> 01:02:28,680
work as well.

770
01:02:28,680 --> 01:02:34,160
So, you know, this is their visual understanding, scene understanding, sort of like a multimodal

771
01:02:34,160 --> 01:02:41,400
input comes in and it would be really interesting to see how, um, perception systems or, you

772
01:02:41,400 --> 01:02:48,040
know, how the, uh, human AI interactivity, um, takes off.

773
01:02:48,040 --> 01:02:53,600
And then I think utilizing AI in other fields, for example, now so it's fronted development

774
01:02:53,600 --> 01:02:54,600
lab.

775
01:02:54,600 --> 01:03:00,360
It's amazing here this year, um, I'm looking forward to see what happens next year in

776
01:03:00,360 --> 01:03:06,240
their work in astrobiology, detecting meteors, asteroids and so on.

777
01:03:06,240 --> 01:03:12,080
And then in healthcare, AI is really useful and in particular, computer vision is really

778
01:03:12,080 --> 01:03:19,560
useful for both, you know, learning about healthcare and for patients as well.

779
01:03:19,560 --> 01:03:25,160
So when you're learning about healthcare, you know, for example, you can learn or you can

780
01:03:25,160 --> 01:03:31,600
visualize how the heart is pumping the blood, you know, you can visualize all the different

781
01:03:31,600 --> 01:03:39,360
valves in the heart and see how the pulmonary, um, veins in the pulmonary arteries are sending

782
01:03:39,360 --> 01:03:43,240
blood to the lungs and getting it back to the heart.

783
01:03:43,240 --> 01:03:48,920
You know, that's, that's really something that is absolutely revolutionary and, you know,

784
01:03:48,920 --> 01:03:54,320
imagine how the entire education system would change with just, um, you know, these simple

785
01:03:54,320 --> 01:03:55,720
applications.

786
01:03:55,720 --> 01:03:56,720
Right.

787
01:03:56,720 --> 01:04:02,360
And then, oh, I have to mention this, um, most of my friends have actually stopped buying

788
01:04:02,360 --> 01:04:07,680
cars in our instead of leasing and they're waiting for 2021 to come by to experience self-driving

789
01:04:07,680 --> 01:04:08,680
cars.

790
01:04:08,680 --> 01:04:13,560
I imagine, you know, you're calling a lift or an Uber or taxi and it's self-driving.

791
01:04:13,560 --> 01:04:15,360
I mean, that would be so cool.

792
01:04:15,360 --> 01:04:19,640
Yeah, it's, uh, it's, it's funny.

793
01:04:19,640 --> 01:04:24,920
I was having a conversation with a friend about kind of this idea that, you know, we may

794
01:04:24,920 --> 01:04:32,320
be the last generation for whom, you know, buying a car is a real thing and increasingly,

795
01:04:32,320 --> 01:04:37,280
you know, we're kind of entering this domain of, you know, more transportation as a service,

796
01:04:37,280 --> 01:04:43,760
whether it's, you know, summoning a vehicle from someone else's fleet or, um, you know,

797
01:04:43,760 --> 01:04:50,920
leasing a self-driving vehicle, you know, you know, maybe one per family and you've, you've,

798
01:04:50,920 --> 01:04:56,240
the vehicles kind of always doing something for someone, uh, as opposed to, you know, most

799
01:04:56,240 --> 01:05:02,240
cars sitting around, you know, waiting for you to finish work or finish whatever you're

800
01:05:02,240 --> 01:05:03,240
doing.

801
01:05:03,240 --> 01:05:04,240
Yeah.

802
01:05:04,240 --> 01:05:08,160
I mean, it's really interesting, you know, I mean, no one really knows what the future is

803
01:05:08,160 --> 01:05:13,400
going to be like, but it's really interesting to, you know, um, think about it and I think

804
01:05:13,400 --> 01:05:17,320
it becomes even more exciting when you know you're actually working on it and, you know,

805
01:05:17,320 --> 01:05:24,200
you see the perception that different people have regarding it and you realize, um, oh,

806
01:05:24,200 --> 01:05:28,560
you know, we can do this to improve the system or, you know, this feature would be a really

807
01:05:28,560 --> 01:05:31,040
cool thing that people might even love.

808
01:05:31,040 --> 01:05:32,040
Uh-huh.

809
01:05:32,040 --> 01:05:33,040
Yeah.

810
01:05:33,040 --> 01:05:39,800
So, um, another thing I think that would be really exciting is development around zero

811
01:05:39,800 --> 01:05:47,520
short learning because we all realize that academic data sets are really good, but real

812
01:05:47,520 --> 01:05:53,240
life deployment is very, very difficult and some sort of practical application would be

813
01:05:53,240 --> 01:05:56,840
really interesting around zero short learning.

814
01:05:56,840 --> 01:06:03,280
And, um, a couple more things around reproducibility and finis and biasing.

815
01:06:03,280 --> 01:06:09,000
Um, so the number one thing I think is going to take off and it's really the need of

816
01:06:09,000 --> 01:06:15,680
the R is model cards, which is a revolutionary piece of research that comes from Margaret

817
01:06:15,680 --> 01:06:17,640
Mitchell and a team at Google.

818
01:06:17,640 --> 01:06:24,000
So model cards are like short book reports for training machine learning models that

819
01:06:24,000 --> 01:06:31,520
provide benchmarked evaluations across different kind of conditions like cultural demographic

820
01:06:31,520 --> 01:06:37,760
or different kind of intersectional groups, which are all relevant to the intended application

821
01:06:37,760 --> 01:06:38,760
domain.

822
01:06:38,760 --> 01:06:43,800
So, they also disclose the context in which the models are intended to be used, details

823
01:06:43,800 --> 01:06:50,040
of how the performance of the model is evaluated and other relevant information.

824
01:06:50,040 --> 01:06:55,960
So you know, in two years when, you know, people will be freely communicating through models,

825
01:06:55,960 --> 01:07:01,240
it will seem insane that it's not standard to report how well machine learning models

826
01:07:01,240 --> 01:07:06,000
work when they're actually made available for general public use.

827
01:07:06,000 --> 01:07:09,920
And I can see model cards really taking off in this aspect.

828
01:07:09,920 --> 01:07:18,880
It's interesting, over the past year or so, uh, many of the large AI, many of the research

829
01:07:18,880 --> 01:07:24,280
arms of the large, uh, the companies that are, you know, biggest in AI I'm thinking specifically

830
01:07:24,280 --> 01:07:31,840
about, you know, Microsoft research with the data, data sheets for data sets work that

831
01:07:31,840 --> 01:07:42,160
Timnick, Timnick Gebruh worked on, IBM research did an extension of that concept and it's,

832
01:07:42,160 --> 01:07:46,680
which sounds very similar to the model cards work you're describing, but none of these

833
01:07:46,680 --> 01:07:52,240
companies are publishing any of these, you know, data sheets or model cards for their

834
01:07:52,240 --> 01:07:54,440
commercial projects yet.

835
01:07:54,440 --> 01:07:59,880
That's definitely something I hope to see them do kind of putting their money where their

836
01:07:59,880 --> 01:08:04,520
mouth is so to speak in, you know, 2019 or 2020.

837
01:08:04,520 --> 01:08:13,120
Yeah, and especially given the fact that, you know, we have these, like, a model exchange

838
01:08:13,120 --> 01:08:19,000
service that has really come up this year, for example, TensorFlow has TensorFlow Hub.

839
01:08:19,000 --> 01:08:26,000
So you have, you know, some piece of reproducible code, sorry, reusable code that, you know,

840
01:08:26,000 --> 01:08:33,040
you use in most applications, for example, if I want to write code to load a large piece

841
01:08:33,040 --> 01:08:37,200
of data and I don't know how to do it, I would look it up, find something on GitHub or

842
01:08:37,200 --> 01:08:42,280
Stack Overflow and reuse that code with the appropriate references.

843
01:08:42,280 --> 01:08:48,080
And TensorFlow Hub really makes this much more easier because they've set up a library

844
01:08:48,080 --> 01:08:54,400
that has these small snippets of reusable parts with machine learning code.

845
01:08:54,400 --> 01:09:00,440
It's published, you know, it's easy to discover things that, you know, are efficient and,

846
01:09:00,440 --> 01:09:05,640
you know, just really use the best out that's available out there rather than, you know,

847
01:09:05,640 --> 01:09:09,920
spending hours of your time actually trying to figure it out yourself.

848
01:09:09,920 --> 01:09:14,760
And then similar to this is Model Depot, which is a place where you can find and share

849
01:09:14,760 --> 01:09:19,520
optimized, pre-trained machine learning models that are perfect for your development needs,

850
01:09:19,520 --> 01:09:22,400
similar to what Model Zoo was before.

851
01:09:22,400 --> 01:09:33,520
Okay. Yeah. Interesting. So we're running low on time, any final thoughts or predictions

852
01:09:33,520 --> 01:09:40,920
before we close out? I guess, you know, I'm looking forward to the day

853
01:09:40,920 --> 01:09:47,360
when we will thank AI for taking away our work of holism and then let us get on with the

854
01:09:47,360 --> 01:09:52,800
job of being human. I mean, that would be pretty cool. Yeah. Yes. You know, we talked a lot

855
01:09:52,800 --> 01:10:00,400
about different tools, different research projects that have come up in different parts

856
01:10:00,400 --> 01:10:06,960
of the AI pipeline, how these are applicable in real life examples.

857
01:10:06,960 --> 01:10:13,600
So I mean, it's been a pretty eventful year and I can't wait for 2019 and, you know,

858
01:10:13,600 --> 01:10:18,400
be the downpour of publications. Come on one by one.

859
01:10:18,400 --> 01:10:25,040
All right, Cedar. Well, thanks so much for once again, kind of spending the time to kind

860
01:10:25,040 --> 01:10:27,520
of go through this stuff with us. Thank you.

861
01:10:30,480 --> 01:10:36,480
All right, everyone. That's our show for today for more information on CIDA or any of the topics

862
01:10:36,480 --> 01:10:43,920
covered in this episode, visit twimlai.com slash talk slash 218. You can also follow along with

863
01:10:43,920 --> 01:10:53,840
our AI rewind 2018 series at twimlai.com slash rewind 18. As always, thanks so much for listening

864
01:10:53,840 --> 01:11:08,720
and catch you next time. Happy holidays.

