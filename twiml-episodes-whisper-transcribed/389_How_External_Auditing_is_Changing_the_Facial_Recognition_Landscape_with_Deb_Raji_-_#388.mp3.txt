All right, everyone. I am on the line with Deb Rajee. Deb is a technology fellow at the AI
now Institute at New York University. Deb, welcome to the Twimal AI podcast.
Hey, thanks for having me. It is great. Yeah, absolutely. I'm really looking forward to this
conversation. We've certainly got a lot to catch up on. Yeah. There's a lot been going on.
It's been exciting couple of weeks. Yeah. It's been an exciting few weeks, absolutely, in the
the field that you work in. But let's maybe start from the beginning and have you share a little
bit about how you came into artificial intelligence and where it all started.
Awesome. Yeah. So I kind of, I guess it all started in university. So I'm Canadian for reference.
And I kind of entered on a whim the engineering program at the University of Toronto.
And that was where I actually learned how to code. So my first semester of my first year was
like my first coding class. And I hadn't encountered that before coming in. So it was just like
interesting, awesome experience. And I kind of just kept doing it and kept getting involved.
And my degree was in robotics engineering. So I got a lot of exposure to the computer vision
space and built some skills there as well. So I spent between my third year and my fourth year,
I had the opportunity to do this, to take a year off and do a year-long internship.
So I did that internship at Clarify, which is this computer vision company in New York,
at New York City. And I was on their applied machine learning team there. And that's really where
I like learned about the machine learning research community. It was like the first time I went
to Neurips, which is the big machine learning conference. And it was sort of the first time that I
kind of identified the issue of bias and the ethical concerns involved in facial recognition.
In the computer vision as a field in general. So while working on models there, I kind of
began to notice that even the research data sets and even some of the data sets we were using as
part of our engineering processes did not have, you know, faces that look like me. Like I was,
I was very aware of the lack of representation that was there. And it got to a point where I kind
of just started complaining about it. And people were like, we don't know what to do. And,
you know, like it's already so hard to collect data at all. How do we think about bias? What do we,
like, you know, we don't understand this problem. And then it became very clear that it was just kind
of this understudied phenomenon. And that was when I started kind of scourging the internet,
trying to identify anyone that was doing the same thing or had noticed the same problem. And that's
how I kind of landed on Joy Blemwini's TED Talk, which she had given, I think probably a year or
so before, I should have given this TED Talk on her experience with attempting to use open-source
facial recognition software and having the technology not identify her face because she was,
you know, darker skinned and having to use a white mask in order to be identified. And that was
pretty much what her TED Talk was about at the time. And I was like, okay, cool. So when I was like,
what does that even mean? Having to use a white mask to...
So she had to use a white mask to have the facial recognition system like identify her or...
Oh, meaning identify that there was a face there?
Identify that there was a face there. Oh, wow. Yeah. So it was pretty, yeah. This was like,
that was sort of the extent of the articulation of the problem at the time. And that was what prompted
her to start the algorithmic justice league project. So I kind of reached out to her like very recently
after I started at clarifying. I was like, hey, you know, I'm noticing this thing. You give a talk
about this thing. Like, can we talk? And it was a very extra email. Like, I was like, already like,
really deep in the woods of like, here's these data sets that we use in computer vision. And like,
here's like the very tiny percentage. Like, the data sets we use are, you know, 80 to like,
95% lighter skinned subjects. So it was like really bad. So I was like, these are the stats.
Like, I've been trying to like get more info. Can you help? And I think she was just kind of like,
wow, this person like cares. So happy to respond. And her response was something like, yeah,
let's talk in a month or something like that. And I was like, okay. So yeah, like a couple months
later, we actually started talking and we started, you know, collaborating and working on some
stuff. So at the time, she was working on her thesis, which was around gender shades. So I helped
her with that. And then, you know, as gender shades sort of, so gender shades came out February
that year. So as gender shades kind of began kind of gaining steam. And like, we understood that it
was a problem that other people could also recognize and empathize with. I kind of was like, okay,
cool. Like, you have enough support that like, I can kind of work with you full time over a summer.
Or it was ended up being a summer and a fall. But we ended up working together on this sort of
follow-up study trying to identify what, what about gender shades made it an effective audit to sort
of characterize and communicate these problems in a way that pressured these companies to sort of
feel cornered to take action. So that was a lot of the follow-up work I did with Joy. And a lot
of my subsequent work is thinking about, you know, how do we actually capture some of these,
like, limitations or these failures at these models experience? And how do we communicate it
to the public, but also to other researchers, to other engineers in a way that actually makes
that limitation super clear. And like, raises concern in a way that prompts people to take action.
So that was a lot of what my journey is. And that's a lot of the work I'm doing today. So, you know,
following that, I started working with Timnett and Meg at Google and we worked on the model
cards project, which was a way of sort of documenting and communicating audit results and ended
up sort of becoming part of the engineering process at Google for machine learning models.
And then, you know, following that, like, the National Institute of Standards and Technology is
sort of taking up some of the findings in our work and the terminology in our work.
So that was, it sort of become a thing, you know, and it's really sort of stem from this desire to,
like, identify the problem in a consistent way and communicate in a consistent way. So that's
kind of the ongoing work I'm doing today at AINow and wherever I end up in the future.
That's awesome. That's awesome. And so you think of the broad area as you've referred to audits
on several occasions. What all is kind of captured in that terminology?
Yeah. So the reason I mentioned, I talk about sort of auditing and, you know,
some of the work I've done with Google will, like, refer to it as, like, internal auditing.
Is this, you know, anchor to the idea that, so especially the work that I do with Joy,
we look at models that are already out there that, like, someone already decided was, like,
sufficient to deploy. It had already passed whatever deployment conditions were already there,
and the person had already sort of, like, thrown it over the fence. So we look at, like,
models that are already built and deployed, and then we try to understand, you know,
how they actually operate within society. How do they actually operate within a deployed context?
So, you know, for gender shades, for example, building a test set where we identify different
populations that could potentially be affected by such a product, represent that, like,
each of these subgroups within a test set, and then evaluate for each of these subgroups and
discuss the results for each of these subgroups. And the disparities between these subgroups
is us trying to sort of simulate discussion around, you know, within society, how can we
anticipate this model that's already, this product that's already out there? How can we
anticipate its performance on these subgroups that we've decided that we want to look at that we
care about? So that's why it's framed as an audit versus just, kind of, like, an assessment or
an evaluation. It's kind of these quantitative tests to see, like, when you've deployed this thing,
and it's already out there, is it actually good enough for these specific groups, these specific
populations that we've decided that we care about, and we want to see, we want to observe the
performance on. And this is a lot of a lot of the innovation of gender shades, too, was not just
looking at subgroups along, you know, one axis of race or gender, but looking at that intersection
of, for this darker female subgroup that we've decided to, to, you know, study the performance
of this deployed system on, you know, how well does that, that model work for this, the subgroup
that's at the intersection of different identities. So that was also kind of an interesting
difference between how gender shades worked versus how other, kind of, assessments had worked
in the past. And then the other sort of element of it, which I alluded to earlier, was this idea of
a, of a user representative test set of, I identify all these different populations that matter,
and it's not about, you know, the fact that, let's say, like, 10% of the population in
Kansas is, like, darker skin. So 10% of my test set is darker skin. It's like, no, there are
darker skin people. So it needs to work for them. So they're going to be equally represented in
the test set so that, you know, their performance matters just as much as the performance of any
other type of user that I care about. Yeah. So we kind of implemented these strategies to really
look at, or allude to understanding better how these models perform, you know, in society,
once they're deployed, once they're already out there. Okay. So we've kind of talked about,
we've talked about gender shades, but we haven't really said what it was, it's this,
this audit, but in particular, you developed this audit set or test set, and then you deployed it
against some of the public facial recognition technologies that were offered by
several of the cloud vendors. And it was, I think there were two different iterations of it,
or releases of it with different, different vendor communities. Yeah. And that was on purpose.
So the first, the first audit was IBM, Facebook Plus, and Microsoft. And it was, so the name of the
test set, by the way, is the pilot, the pilot, the pilot parliament's benchmark, which is like
PPB for short, and the test set where, you know, and if there's a great paper called diversity
and faces where they actually just sort of PPB in comparison to all the other test sets in
facial recognition at the time, or up to date, which is sort of, you can see that PPB is balanced
for gender and also balanced for skin type. So you have like a set of darker images that are
sort of equivalent to the number of lighter images. And in other benchmarks in this space,
you can see that the proportion is, you know, highly skewed towards lighter images and highly skewed
male. So it was sort of the first benchmark that encompassed sort of this balance and enabled this
intersectional testing, which was really sort of the key differentiator between other, like just sort
of the typical facial recognition evaluation process. And then with respect to the companies that
we were looking at, we kind of picked these very specific targets. So the first iteration was
Microsoft, IBM and Facebook Plus, these were, you know, huge vendors in the space. And they were selling
sort of off the shelf facial recognition APIs, so like application program interfaces. So they
would sell pretty much the access to the models, their facial recognition models. And if I'm an,
you know, and I'm an app developer, I can just take that model and send my images to the model
to get a certain set of predictions. So, you know, these models are being integrated in all kinds
of applications through developer clients. So we knew it was like very impactful technology. So
what we did was we evaluated the performance of these different models on our test set and
observed sort of how well the models performed on these different subgroups. So darker female,
lighter female, darker male and lighter male. And the result of that initial gender shake study
was that there was a, you know, almost 30% disparity between the darker female and the lighter male subgroup,
which was really surprising. Like you would never, you know, from my time working on a machine
learning team, I know that you would never sort of deploy anything that's, you know, if it had a
60% accuracy, right, you'd never deploy that system. So it was really surprising to see that
on the darker female subgroup, it was performing at like 60% accuracy. So yeah, that was sort of
the initial shock of the first study. And then the follow-up study was to say like, well, after
that was revealed, there was a very public, it was a very public situation. So a lot of the companies
released statements saying that they acknowledged the issues, a lot of them reproduced the results
and committed to doing better. So, and all of them re-released. So within seven months, all of them
had released new, so they had released new models, they'd retrained models and redeployed them.
So we in the follow-up study sort of tested, well, how well, how much did these original
audited companies actually improve their performance on the benchmark that we've, you know, designed?
But also, you know, the companies that were not evaluated, that were not audited, did they,
in any way, get affected by this? And the response was that, you know, the companies that were
directly audited did make that improvement, but the companies that were not audited, including
Amazon, which is sort of, you know, one of the big players in the space and at the time was
selling their technology to facial recognition. All those companies, including Amazon, did not
still have that disparity of, you know, up to 30 percent between the darker female subgroup
and the lighter male subgroup. So they were still demonstrating that bias that we had initially
identified, which was really alarming, especially for Amazon at the time.
Alarming, but also I think it says a lot of interesting things, right? It says that, you know, with,
you know, with some investment, you can change it, right? But you have to care. But it also says that
external pressure is what makes you care, not just that, I mean, you've got to believe that Amazon
and everyone else that's selling products in the space knew about the original data study and,
you know, could have taken the steps proactively to address the issues, but based on the results of
the second study, that didn't appear to be the case. Yeah, for sure. It also just reveals that if
they're not audited for it, it's very easy for them to ignore. Yeah, like exactly what you said
about external pressure, but also kind of like targeted, targeted pressure to very specific,
to specific companies. Yeah, if they hadn't sort of been called out by name, the probability of
them doing better is not is very low. And you know, in a study after that, even another paper that
we've very, very recently put out called staving face, we look at the different tasks. So, you know,
gender shades is looking at the gender classification tasks. So how well does it do on the binary task of
identifying if it's a male or female? But there's other tasks, you know, there's like a small
detection task, there's the actual face detection, you know, there's age detection, things like that.
And we also evaluated for some of these other tasks using sort of a more a benchmark with a lot
more metadata. And what we found was that the companies that were initially audited, not only did
they, they only, they all only improved on gender classification. So they had, they still had
like large disparities for age classification, for example. So even Microsoft that had been audited
a couple years ago, now they have like, you know, very small disparities between their performance
on darker females and lighter males for gender classification as a task. But for age classification,
for example, they still have like a 30% disparity between the groups. So yeah, you have to be very
specific about which populations and which subgroups you're, you're looking at your evaluating
performance for, but you also have to be very specific about the task and very specific about the
target. And all of this is really more or less a case for like a regulatory regime where like
everyone has to sort of restrict their use of facial recognition in specific ways or get assessed
kind of universally across the industry in very specific ways for very specific tasks that we are
worried about as a society. And also, you know, for very specific populations that we are concerned
about as a society. So yeah, it just kind of reveals how specific you have to be with respect to
how you design these audits. Yeah, I was going to add you answered the question that I was going to
ask, which is around like, well, I maybe you didn't, right? I think what, you know, what you saw is
that folks can kind of engineer systems for the test, like engineer their system for the benchmark.
And it sounds like an, to some extent, what you're saying is that that's what we need to happen
is that we establish the benchmarks broadly and we, you know, through regulation or some other
measure encourage the companies that are offering these technologies to engineer for these tests.
And I guess there's part of me that says, you know, should it be something else? Should we show them
like the error of their ways? And they say, oh, we should have a diverse team and we should kind
of go off and think about all of these things. And yeah, you know, maybe the, you know, can the,
can the test ever be exhaustive? Yeah, that's a great question. I don't think I answered that question.
So that's a great question, because that was something that like plagued me for a long time,
too, where I was like, because the other thing too is not even just with respect to the task,
because I think with respect to the task, and this is what the National Institute of Standards and
Technology is really into is, you know, they identify that the, that the, the, the facial sort of
identification task, which is the ability for me to like identify you as Sam, you know, identify
myself as Deb, or the facial verification task. If there's two images of me to be able to say
that they're the same image, like they see those as the most important sort of pertinent tasks
with respect to facial recognition. So they will sort of focus on that. And they're like, we don't
actually care about these other things happening. So we've already identified the tasks, and they're,
and then they'll say like, oh, you know, these are the groups that we care about. So we've already
identified these groups. But then we, we rose the question. And this was a lot of our, our third
paper, I call it sort of like like an existential crisis paper, because it's kind of, it talks about
some of these like, who exists that we need to talk about. One of which is this idea of, you know,
looking at the intersection of, you know, skin type and gender is just one way to look at it.
There's other contexts in which it's really important to look at age. And how, you know,
how do you identify the intersection of age and gender and and skin type? Like that just gives
you an infinite number of permutations. And the way that we ended up sort of gaining some level
of like peace is to sort of, I guess, reflect on this as limitations of the audit structure of
of an audit in general to say that there are certain things that you can learn from an audit
and certain things that will actually be very difficult to learn from an audit and actually
require different types of evaluations such as pilots, for example, piloting the technology
or even like overall restriction. So this is one of the cases for the idea of a moratorium where
we understand that there's concerns with this technology and it's not just the racial bias,
right? So, you know, when the technology doesn't work in the case of what we've identified
through gender shades, you know, when the technology is less
performing on darker skin individuals, for example, that puts them at higher risk. You know,
if I get misidentified as a criminal at a higher rate than others, that's because of,
you know, the technology not being as functional for me as it is for another person. But then there's
also the situation of, you know, especially in cases where it's difficult to properly assess the
functionality of the system, you know, maybe we should just restrict the use of the system in general.
Maybe we should just take it out of the market as we figure out and we learn these things that we
have these sort of more nuanced conversations and discuss the other sort of facets of concern as
we discuss other issues such as privacy and transparency that exist that we need to have honest
conversations about in addition to like the complexities of that bias situation. So I think that's
sort of been my approach to it is that there are actually very clear limits to these kinds of
audits. We need to be very aware of. There's a lot that we can learn from them, but there's also
limits to what a standard can tell us and what, you know, NIST can actually do and can actually
provide with respect to insight into how these technology, how the technology operates in deployment.
So we need to be very careful about that. We need to do a lot more thinking about that. And while
we're thinking about that, maybe this technology that's like very clearly immature in certain ways
needs to be taken off the market. Well, I definitely want to dig into the kind of the broader
implications of facial recognition technology and the question of moratorium or not. But before we do
that, I want to continue to kind of pull on some threads around your research and the auditing
and you alluded to this in your last statement. But I thought that the savings face paper was
really interesting in that it was essentially saying, you know, this is our third, you know,
auditing paper, you know, our third go at auditing. And it's still hard and we still mess up and
it's probably not enough. And you need to be really careful even, you know, not just trying to
feel the technology, but just auditing it. Yeah. Can you talk a little bit more about what you
found there? Yeah. So that's saving face paper. I said it's sort of like an existential crisis
paper because following gender shades and actionable auditing, we were sort of seeing these gender
shades like audits appearing everywhere. Like everyone was kind of just building their own version
of PPB, which is the benchmark for sort of any kind of task or any kind of situation.
And people were like you were like you mentioned earlier sort of building to the test. So they
were saying, okay, we're going to improve performance on on PPB or whatever PPB shadow we created
for ourselves. And that is going to be sort of the bar. And once we're over that bar, then that
is something sort of significant. And that whole paper was us saying like hold up something like
an audit like gender shades is a demonstration of, you know, some of these clear oversights with
respect to testing. So the fact that, you know, some of these really huge companies, they're deployed
products, you know, failed. So these are, you know, companies that, you know, they have many people
on their teams and no one on the team had sort of tested for this. Previously, that was the
demonstration was to show that lack of oversight in that negligence. So an audit like gender
shades is really a demonstration of that negligence and to point to the fact that there are,
there are very clear gaps in the way that we currently evaluate and assess this technology
before we deploy it. And it's not necessarily the bar. It's not necessarily a high bar to cross.
So it's kind of like if you triple over the bar, that's embarrassing and you should be ashamed of
yourself. But if you like, you know, if you pass the bar, it doesn't actually mean anything.
There's still, that doesn't necessarily mean that there's no more work to do. So the saving
face paper was us to say, it was us saying, you know, just because Microsoft is now doing well
on PPB, it doesn't mean that it's doing well with respect to, you know, age classification. Like
I mentioned, they still have huge disparities there. But that doesn't mean that, you know,
just because IBM is doing okay on PPB now, it doesn't mean that they haven't thought a lot about,
you know, the privacy concerns that came up with their diversity and faces paper and flicker,
you know, just because we're having conversations around, you know, some of these audits being
sort of conditions for use. So, you know, we're seeing a lot of policy conversations of
gender-shaped-out audit being sort of the condition, you know, you have to pass gender-shades in
order for your model to be able to be deployed. And we're like, there are a lot of things that
need to happen before a system, like a facial recognition system gets deployed. For one thing,
like community consultation, so community participation in that decision-making process,
like transparency around the performance of the system, you know, there's, you know, privacy
concerns being addressed. So, there's so many other things involved. It's not the barred across.
So, that was the main takeaway from that work was us saying, you know, there's so many other
elements to this. And, you know, those considerations can be integrated into the way that we assess
our systems, right? So, you know, NIST could, you know, take on these sort of more qualitative aspects
of, you know, reflecting on the assessment of these systems and facilitating some of these
processes required for any kind of deployment. But, you know, we need to recognize that whole,
you know, this is a whole can of worms that is much more complicated than a lot of us actually
understand at the moment. And we need to, you know, while we're having these more nuanced
conversations, take the product off the market or at least, you know, support that moratorium stance
of pressing pause, as we all have this like deeper conversation. The other thing too, that I was
at least personally realizing at the time, and I think Timnit and Joy were kind of going through
this as well, where we were noticing that there were situations where the issue was not that facial
recognition wasn't working. So, it wasn't that it wasn't even that like, you know, the data was
an encrypted property or the privacy within, or the data was a managed property. It wasn't even that,
you know, it wasn't it wasn't working for different subgroups, but it was just that it was being
like actively weaponized by like this authority figure. So, when you think about facial recognition
as a technology, I like to remind people that a face is the equivalent of a fingerprint. It's
an identifiable biometrics. So, you know, I have your face. I can do a lot of things with that,
except, you know, we are so careful with our fingerprint data. There are so many standards around,
you know, how to store that information, how to how much you can centralize that data about,
you know, how many people, but when it comes to faces, there's, you know, no rules. So, people
have these, you know, immense repositories of people's, you know, identifiable biometrics,
all in this sort of like centralized location that can be controlled by this authority figure.
This is very dystopian. I apologize in the current times to like bring up this image, but
I think we've seen, I think we've seen like the Clearview example come to light earlier this year
where I'm sure you're familiar with that. Do you want to share a little bit about, you know,
kind of what you know about that one? Yeah, so Clearview AI was, you know, a group that actually,
you know, was really, it was really great reporting by the New York Times to actually identify
that group and really expose them because they were, they were sort of intent on being sort of
this covert under the radar stealth company for a very long time. But what they did was they looked
they kind of collected social media face data. So they were, they sort of did something that I'm
reflecting on as sort of digital surveillance where they would, you know, if you upload your face
to Instagram, Facebook, they were collecting all of this information and using that.
Like swapping all public pictures, they can get there. Yeah, yeah.
I mean, it was pretty egregious what they were. Yeah, like a cartoon villain type plot. Like,
really awful. Yeah. And they were, and the worst part is that they were actually cooperating
with law enforcement in different ways and pitching to law enforcement and different
government agencies. So they were using that information to like identify either, they were
using it to identify you online or to identify you through, you know, surveillance camera footage
and other sort of sort of terrifying modes of surveillance. So it was kind of a situation where,
you know, they would give that power to any authority figure that could easily abuse it,
easily weaponize it against you. Yeah, I would agree that the law enforcement examples were the
worst in terms of kind of mass potential harm. But there was also a total lack of of governance
where like from what I remember from the New York Times article, like board members would say,
hey, can you find this person for me or something like that? And they would. Yeah. It was, yeah,
there was also a story that I had heard of. I'm not sure how like, true this is someone like,
try to identify like employees trying to identify like personal like ex-girlfriends and stuff. And
it's very hard. I remember reading that as well. Yeah. Yeah. Yeah. I was going to say there are,
like there's, there has been reports in the past that kind of hinted at this kind of technology
where, you know, ice would just show up at people's houses and people wouldn't understand how they
found them. And then it was later revealed that there they were matching sort of facial recognition
data of like, you know, we know that you look like this based off of whatever mugshot or whatever
information we have from your visa or whatever and identifying your Facebook profile or identifying
your your trace online that way. So yeah, it really is this dangerous technology that like empowers
some of these institutional bullies to kind of just barge into people's lives and like really
affect them. So it has these like kind of whimsical situations, but also just like very important
kind of dire consequences as well. And we keep getting pulled into kind of these broader questions.
I'm still very curious about the the auditing thing. One of the questions that occurs for me is
I, well, I'm curious if you if you explored, I'm not sure the kind of the taxonomy of different
stand types of standards, but when I think of standards like the ISO 9001, they're like process
standards as opposed to like checklist standards. I don't know what the proper names for these are,
but you know, strikes me that, you know, the analogy that comes to mind is like the Volkswagen
gaming the emissions standards. Like they, you know, they had the car set up so that when they
learned that they were in, you know, being tested like it, it switched the engine to
more environmentally friendly mode. But then the regular mode was, you know, just doing what it
was doing through. Yeah. And in this case, there's, I don't think there's like, well, is there an
objective state of, you know, a facial, a good facial recognition system or a facial recognition,
separate from that question of the use of facial recognition and should exist and all that kind
of stuff, there's not really an objective, all encompassing measure of, you know, goodness from
diversity perspective, like there's all different things that you might want the system to be
capable of and you have to engineer them. They're not emergent qualities. You have to engineer them
in. Yeah. And so maybe the, you know, have you looked at this idea of like being around the process
as opposed to, you know, checking off boxes at the end of the process? Oh, yeah. Yeah. This is
like a huge part of my current work. So yeah, just to like, because you said a lot of interesting
things in there, like one thing around standards is that you're, you're totally right. I'm kind of
curious as to, because not a lot of people are reading standards. I'm like, I don't know. I'm curious
as to, you know, which standards are you reading, Sam, but in my, yeah, I've been, I've been looking
a lot at this question of standards in facial recognition mostly because it comes up so much in
policy. So a lot of policy bills will sort of either reference the National Institute of Standards
and Technology in the US, which is sort of the governing body around establishing some of these,
like metrics or these, these bars that need to be jumped over by industry players in order for
them to be considered, you know, as potential vendors within, you know, the sort of
space of working with government agencies. So the National Institute of Standards and Technology
is really the key kind of industry indicator of the performance of your system. If you're kind of
hoping to, you know, work with different governments or different official bodies,
and they very recently, like literally within the last year, you know, citing our paper too,
which was very exciting, they only very recently started evaluating performance across different
demographic subgroups. And that just happened literally last year for the first time. So they,
before that, they hadn't incorporated that into their understanding of, you know, assessment
and evaluation. And I think like with the saving face paper, we were actually challenging them to
go even further and to say, there's all of these other considerations within the process of
how official recognition system, quote unquote, works. You know, can you really say it works if it's
violating the privacy of millions of people and there, and there's no consideration for that,
there's no privacy policy included or incorporated or reflected on. Can you say it works if there's
no method of, you know, transparent communication around its deployment and its use case and there's
no clear evidence of like ethical consideration? Like, is that a system that makes sense to even
consider to use? So yeah, some of those questions, some of those more holistic questions,
they're currently like not even in that kind of space of assessment or evaluation. Like at the
moment, I think as far as it goes is kind of thinking about like, how easy is it to use this and
incorporate this into an application? You know, that's kind of the, they're looking at it at a very
sort of product level. So that's like missed, but ISO and IEEE and other groups have actually
thought about facial recognition. Wef is also sort of proposed some ideas around sort of assessing
facial recognition technology. Yeah, so they actually, they sort of put together like a working
group and they put it together white paper on facial recognition assessment and there's other
sort of think tanks that are attempting to kind of build frameworks for the evaluation or the
assessment of this kind of technology because it's kind of this contentious, controversial tool.
And people are trying to identify all the axes of concern and understand what you have to think
about with respect to will it ever be okay to use this tool? So that's why I think our voices are
in there. Yeah. I think the distinction I was trying to get at was one is a set of standards
around the output of the the process. The other is a set of standards around the process itself.
Like, you know, I can envision a standard that says that, you know, a compliant facial recognition
system has to be developed in a company where there's, you know, some kind of mathematical
review board project, you know, process and there's, you know, some percent of diversity on the
team or something, some metric of diversity on the team that's working on it and it needs to,
you know, be the database, the training data set has to have some set of qualities as opposed
to the output, you know, this process through which it's developed. Yeah. So this was, this was
something that we brought up in our saving face papers as well as a huge where a lot of the
current standards, even the ones that are focused on privacy, like the ISO standard is very,
you know, into this idea of like, hey, this is an identifiable biometric. It goes under all of
these things that we have where identifiable biometrics and it can only be stored in this way
and it needs to be encrypted in this way and they'll check the output. They won't check any process
that you, like, they won't, like, the privacy policy of data collection, like, the way that you
collect data. If that was completely unethical, like, they, they just care that the data is encrypted
at the end, right? So that's the definition of privacy and it's very removed from these processes
like you mentioned that if they were mandated would allow for kind of richer measures to be in
place and richer guardrails to be in place. I worked on a paper with colleagues at Google
of called closing the AI accountability gap. I had a funer name for it, but they all refused to
let me. Are you bringing that up because you want to say the name? No, I'm not going to, I'm not
going to, so I wanted, you know, I wrote it with a colleague named Andrew Smart. And I wanted to
name the framework, the smarter framework because his name is smart. So I thought it was really funny.
No one else thought it was funny. So we ended up naming it the smacked air framework, which is kind of,
you know, it doesn't ring off the tongue as well, but Andrew Smart, I think, just didn't want that
attention to like smart of the smarter framework was a little bit too much for him, but effectively,
yeah, with that framework, we talk about sort of these, some of these procedural considerations,
like that framework is pretty much us trying to say like, you know, and us using the approach of
documentation to try to really identify all of these decisions that engineers make. So for
example, you know, there might be someone that would have a facial recognition system where they
would say, oh, this, you know, this system has no bias because we've evaluated it on like a
gender-shaped style audit. You know, we have these different subgroups and according to whatever,
you know, whatever taxonomy or whatever labels that we have, you know, the performance of the
model is equal for group A and group B. So there's no issue. However, you know, with, you know,
some of these, you know, with some of these process-oriented audits that we did at Google,
we saw that, you know, perhaps the way that the data was collected was incredibly unethical,
and that's where a lot of the issues arose. Or perhaps the way that the labels were set up.
So the taxonomy of the labels and the way that, you know, because when you create a computer vision
system, you actually set up the targets for the system, implicitly or explicitly. So you actually
define, you know, this is the objective of what I want my model to do. And here are, you know,
a set, you know, if I wanted to predict between a cat and a dog, I actually give it that label of
cat and dog, and I select the images that represent cat and the images that represent dog. And that
process at the moment is sort of seen very callously as like, oh, this is, you know, I scrape whatever
I get from the internet, and that's what I use. But there's sort of this realization that even some
of these very subtle engineering decisions that we don't like to admit to ourselves are actually
very important. And there's ways that you can articulate, you know, goals for the model that are
implicitly discriminatory, whether or not it performs well on different subgroups. You know,
so that's some of those procedural sort of elements or some of those engineering decisions,
even outside of some of the governance structures that you've mentioned around, has there been an
ethical review word that looked through this? Or some of the questions that you were mentioning
around diversity of the people involved, right? So or even consultation with the community or with
the public. So some of those even some of those governance issues separate from those in the
engineering process, even there's a last there's a loss of accountability. Like one of the most
sort of surprising things for me is how little we understand, especially as, you know, a machine
learning engineer, sort of a typical machine learning engineer, there's not a lot of accountability
currently around, you know, data providence where I get my data from. So I can create a data set
coming from anywhere and there's no sort of accountability with respect to where that data is
collected from and what that actually represents in which world view that's coming from and which,
you know, all these politics to that data source. And there's a great project that actually
happened at AI now called excavating AI. And I've been talking a lot about it because I think it
does a good job discussing sort of the politics of like, you know, which labels that you pick and,
you know, where your data sources are coming from and the ethics of that as part of, you know,
as an integral part of the ethics of the entire development system of the model. Yeah. So that
work was us trying to get people to write some of that stuff down. So at minimum, we can start talking
about it. Yeah. So you've the risk of going on another. It's all good tangent divergence rabbit hole.
You mentioned the decisions that machine learning engineers make, accountability in the engineering
process, reminding me of the recent kind of thread with Jan LaCoon where he essentially, at
least the part that I'm referring to kind of absolve research of any responsibility for bias in AI
and said it's, you know, related to the things that that you just talked about, you know,
engineering process and discipline and, and the like, does that mean that you agree with his take on?
It was bait. I didn't realize I came to be baited like that. No. That was great. No. I think there's
responsibility on all sides. So one of the reasons why I, I personally gravitate towards engineering
decision making and accountability with respect to engineering decision making is because
through documentation is because I understand that sometimes actually engineers do not understand
their sense of responsibility and do not have the resources to support them in communicating about
it. But a lot of the things that we've worked on with respect to, you know, you know, action
closing the AI accountability gap, but also in earlier work, the earlier work on model cards
and, you know, related work in like data sets, data sheets for data sets and other projects.
All of that has been widely used by the research community before anyone else. Like the research
community, especially applied machine learning work, including computer vision and natural
language processing, you know, a lot of that work involves the engineering of these models.
You know, and I recently like rage tweeted about this where it is so strange to me that
you on the concept that because he literally works at an industry lab where they works, they,
a lot of the work that fair does is, you know, a step towards productization and a lot of the
models that they build, especially some of these larger models, all of these industry labs,
a lot of the models that they build are models that other companies and other groups build off of.
So they will sort of build these quote unquote general models, curate the data set required
to train these large immense models that are then kind of fine tuned by different groups using
that model for different purposes. But that idea of building this general model for whatever
purpose is a lot of control. It's a huge engineering decision. It's applied science effectively.
So it's so strange to me that he thinks the best that research process is separate from
sort of the engineering step and the applications of that, yeah, of that work.
The other thing too is, you know, there's a whole separate set of issues connected to research,
I think. The problems that you choose to work on in research, the way that you test systems
and research really sets the precedent for the field in a way that I'm not sure he acknowledges
with his response. He kind of downplays. So, you know, with a specific example that I think
we're thinking of, it was this model that depixelized faces. So, you know, take
face faces and attempt to reconstruct it. And the my main issue with that work or one of the big
issues with that work, there's many concerns. But one issue is that they didn't seem to test
for people of color. There were multiple examples of people depixelizing the faces of people of color
and then those people being reconstructed to look Caucasian. And it was just very clear that because
it's not mentioned at all in the paper and it's not mentioned by the creators until it kind of
blew up on Twitter. You know, the lack of testing for people of color continues to be this
theme of, it's sort of procedural negligence where it's like, why how could you not,
like, people of color exist, you know, how could you not evaluate or assess for this particular
group that is definitely going to be part of the people, the group of people affected by this
tool. So, I think him downplaying the severity of that oversight was kind of is one of the main
reasons I don't agree with him. But also, I think there's, there's multiple layers of issues there.
Yeah. I don't fully get the, I don't fully get why it's so important to some people to
distinguish between algorithms being biased and data sets being biased. Why is that the hill
that we have to die on some hill? No, but I was going to say, like, I think that it comes from a
place of like really not wanting to making sure that people understand that there's not a quick fix
to this. So, you know, we heard for a long time, data is all we need, you know, we just need more
data, we need more diverse data. In which case, I do not have to think about, you know, diversity or
ethics at this moment because I'm just creating a prototype and the next iteration, I can think
about diversity and I can think about bias and I can think about data because this is just a data
issue. But there's been a lot of great work. And I think you just kind of triggered the entire
sort of fairness, machine learning fairness community. It was fascinating to watch.
And we'll link for the, the tweet and threads. I think that would be, you know,
as well as my pixelated. There's one of you. There's one of me. Yeah, Robert Ness did it for me
and gave me hair. Oh, gosh. Someone said it. Someone said the output had me looking like Steven
Seagal. No, actually, no, I said that. Someone says some other someone's TV that actually Steven
Seagal. Oh, my gosh. Yeah. And it's just so silly to me that like, yeah, that it's sort of
something that's discovered on Twitter. In the same way that it was silly to me that, you know,
gender shades is something that is happening so recently. And it's this new discovery of like,
you know, when you test on people of color or women of color, some of these things don't work.
I think that's for me that continues to be sort of this recurring frustration is like, how can
you not like see this group of people that clearly exist and not evaluate before it's on them?
It's a silly example. Yeah, it sounds like you're saying that this, this, it's not the algorithm,
it's the data set is essentially, you know, either, you know, saying it's someone else's problem
or kind of kicking the can down the road. That's why it keeps coming up. Yeah, because people
had designed algorithms. They actually do influence some of these outputs. They do influence like
the fairness of some of these outcomes. And with respect to this particular work, there was
an interesting thread that didn't get as much traction, but there's another thread of
someone that had tried alternate approaches to the problem. And he had actually gotten sort of more
faithful representations reconstructed. And it was because like so his, his represent,
when he reconstructed his faces, they were not all Caucasian looking, they were all white looking.
And he had taken just a different algorithmic approach to the problem.
Well, they'm underlying data set the face HQ data set, but actually cared to. Yeah, or just even
took a different approach and got different results, right? Like it wasn't even, it was more of that
heat. It was not fixing the problem. It was just, he was approaching the problem, understanding
that like black people exist. And we should test for that as well. And if it doesn't work for this
group, and you know, if your black faces end up looking like white people, that is not a functional
product that you can use for black people. Like no black person can use that product. And I think
that for, like because that was already at the forefront of the discussion, some of these like
counter proposals of these other algorithmic methods that were that kind of preserved the black
faces to put it lightly or even other people are to their Asian faces as well that it wasn't
working for Hispanic faces. Yeah, so people that had kind of come in with this use case in mind.
We're sort of discussing and exploring alternative approaches that I kind of saw some of their
preliminary results. And I was like, yeah, that like didn't completely morph the face into like,
you know, Adam Sandler, whoever. So I think like, yeah, there's definitely a role of algorithmic
approaches. And this is like a well study thing. You know, there's, there's so much literature on
this and a lot of people bombarded y'all looking with that literature last night. It's crazy
because I literally feel like this happened a week ago, but I was like, no, that all happened
last night. But yeah, it was kind of good to see some of these papers come out too because so many
people have so many misconceptions about how bias works. Like we often get the responses like,
oh, isn't it just the data? It's like, no, there's so many layers to this. I think it was a good
prompt for people to reopen the discussion of like just how complex that question of bias is
and just how hard fairness is as a problem as a problem space. And how many different factors
kind of like lead to it. Yeah. I said I wanted to come back to the topic of the broader topic of
facial recognition and we probably should do that if we're going to get to it. You know, tell me,
I'm just curious kind of your perspective on that kind of from the beginning. Like does it
start with, I'm imagining it starts with the potential for harm and what those harms are. And
maybe you can kind of talk through what you've seen. Yeah, so just like in terms of broader issues
of facial recognition. Right. And you know, if the question that, you know, we want to put on the
table or talk through right now is like, should facial recognition be on the market? Oh, I see,
yeah. Right. You know, where does the answer to that question start? Yeah. So I think like a lot
of the work that a lot of the work that gender shades did I think was break the myth that facial
recognition worked because I think for a long time the debate was, oh, this system already works,
you know, do we want it to be enabled within society as this surveillance tool as the system of
control, as the system that can be weaponized, you know, debates around what does it mean to be safe,
what does it mean to have a tool that promotes or discourages safety? What does it mean to have
different authority figures in charge of this tool? What does it mean to allow or restrict specific
use cases? So something like gender shades just broke the myth that it worked in the first place.
And I think that was an important myth to break because this is very immature technology. So,
you know, currently, there's a lot of hype around people, you know more than anyone. There's a lot
of hype. There's a lot of hype around machine learning. There's a lot of hype. Yeah, I did a
little bit, a little bit. And it's really important to have some of these audits come in to say like,
wait, actually, it doesn't work for this group of people. Wait, actually, it's really biased
in discriminatory in this particular way. And it opens up the conversation for future reflections
of, wait, not only is it biased, but also there's these privacy issues. Oh, not only is it biased
and are there privacy issues, but there's also very specifically concerning use cases that we need
to pay attention to. And maybe, you know, the benefit is not actually worth the harm. So like, when
we start having these, we start with this place of weight, it doesn't actually work and it's
an immature technology. And we move towards a place of like, oh, wait, there's actually, even if it
did work, there's all of these other concerns. Like I said, when it doesn't work, there's issues,
but there's also issues when it does work. So I'm starting with that place of like, wait, this
is not this magical, you know, functional thing. Just breaks like the rose colored glasses kind
of come off and people are much more comfortable questioning other aspects of the technology. And I
think where we are today is that, you know, Amazon, Microsoft and Facebook, or that Facebook Plus,
Amazon, Microsoft and IBM have all kind of very recently come out publicly to say, there are
clear limitations of this technology. We have been confronted with the facts. But also, you know,
we understand this concern and we now resonate with it in the, in the current context of sort of,
you know, racial injustice that we're seeing in this, in this country. So we understand that
the risks are here and we understand that, you know, this technology is immature and not really
ready for market. So I think that's why that idea of a moratorium of like, let's take it off of
the market while we have these conversations around regulation. So like, you know, proper restriction.
But also disclosure, like, you know, if any kind of agency is using facial recognition,
how can we empower sort of members of society and the community to like be part of that process
and to understand when facial recognition is being used on them? Because right now,
we don't know the extent to which, you know, Georgetown's 2017 work looking at sort of the
prevalence of facial recognition to use in the US, you know, by police, by law enforcement specifically,
we don't even know about immigration and other groups. So, you know, that was so shocking to so
many people that no one had any idea because it's a technology that someone can use on you without
you having any clue. So, you know, can we actually enforce disclosure? Can we actually enforce,
you know, some of this community participation? But also, you know, can we reconsider what it means
for the technology to work? Is it just accuracy or do we actually have to understand that it has
to work for different subgroups or do we actually have to start having conversations around,
like you mentioned, process and privacy and all these other complex things? I think the work of
exposing, you know, the bias is sort of like a good way to just expose the complexity of the
technology itself and break that like myth of just like a perfectly functional kind of tool.
And then once that myth is sort of broken, then people understand that like, oh, this is a
Pandora's box. This is a very complex system. There's so many dimensions of concern here.
We need to be way more careful than we are currently being around about it. And as a result,
you know, maybe it's not ready for it's not ready to be so widely used, you know, right now it's
used in a lot of places. So, maybe we should just be more careful and should just pull it off the
market as we have these deeper, longer conversations about sort of the complexity of what it is.
Yeah. So, that's sort of how I approach kind of the facial recognition issues in general. Yeah.
Okay. Now, if I parse those recent announcements and remember them correctly, IBM's announcement
was the broadest of the ones that I remember seeing. They said that they were going to stop
developing facial recognition technology for, I don't know if there was a time frame associated
with that or if it was indefinite. Yeah. Amazon, on the other hand, it was a fairly restricted
moratorium on the sale of facial recognition to law enforcement agencies. Yeah.
And I don't recall Microsoft's the scope of their announcement.
Your announcement. It was very vague. It was very vague on purpose, I think.
Yeah, I'm also trying to figure out what Microsoft's announcement was about.
Having read it several times. Yeah. And watch the video. I'm still trying to figure out what
Brad Smith was saying, but yeah, but they all kind of made broad sweeping statements in different
ways. I think most shocking was Amazon because they had been so stubborn, especially with us,
they had sort of, yeah, they had sort of, you know, one of our second paper came out and we had
kind of called out Amazon and shattered their rose colored glasses to like, you know, how functional
their situation was. We had to face a lot of, a lot of, I guess, like, aggressiveness coming from
them. But they posted a blog post. A fairly flawed blog post. Yeah.
Very quick kind of like off the cuff, like, yeah, like, I feel like it was just him being angry
and just wrote a couple paragraphs and put it out. But also, and subsequent interviews too,
you know, I read a lot of articles that coded him. And this is something that came up in
the documentary that we recently, there's a recent documentary that I'll give me just this
week sort of released, coded bias. And there's a scene of like me, Joanne Timnett, who's the co-author
for Gender Shade. And we were sort of sitting there talking about this blog post. And I remember
one of the things that struck me about that conversation that always remembers like us, we were like,
we worked for like months, we wrote this paper, it passed peer review, all of these things happened
in order to validate our results. We had like, you know, so many supplementaries that we like,
because we wanted to make sure that our results were sort of validated and could stand up. And
this guy writes like a overnight, like, blog post with zero citations. And like, you know, in the
press articles, they're quoting these things as if they're like equivalent sort of rebuttals. And
we're like, what? But do you have a sense for? So yeah, that was that was my frustration with Amazon.
But they, I think you have a sense for what drove, you know, them to respond in that way. I mean,
IBM and Microsoft and others were kind of confronted with the same realities. And I said, okay,
yeah, this is probably pretty bad. Let me, you know, let's do something about that. But Amazon
resisted that. Yeah, I suspect, I suspect this happened for a couple of reasons. One being that
ACLU, so I'm not, you know, I'm not privita exactly what's going on internally at Amazon. But
HTMLU had a couple months before the summer before our paper came out, released a couple reports
that they had found of Amazon attempting to pitch at the time. They were in the process of
trying to pitch their technology to ICE to, you know, different intelligence agencies to different
law enforcement agencies. Also, you know, Amazon through their ring product. So ring is sort of the
smart doorbell product. Sports are available in storebell where they'll sort of monitor your
porch. They were trying, they were thinking of the idea of implementing sort of facial recognition
and Amazon recognition to help process the footage from these ring products. And they had a lot of
partnerships with, you know, thousands to the order of over 3000, I think police police departments
at the time. So for them, it was sort of at the cost of this very like promising economic
opportunity. We had kind of just like clip their wings a little bit by revealing the fact that
their technology fell short for for people of color, especially, you know, darker darker skin
women. So by just demonstrating and questioning the functionality of that product, we really sort
of threw the whole, threw the whole product into sort of this period of like people really truly
questioning and revisiting like does this thing actually work? Oh, but also privacy. Oh, but also
surveillance. And like it kind of just was like the, it's always the tip of the iceberg to this
larger conversation. And I think they understood that because that had, that's what had happened
for Microsoft and for IBM. So I think that was why they were super defensive initially.
But thankfully, you know, the research community really came out to support us. They wrote this
public letter. There was a lot of press around that letter where they had kind of just refuted all
the like things that Matt Woodson said in his blog post, but also, you know, other statements
by Amazon later on. And it kind of ended up in a place where they conceded that we needed policy.
They sort of kind of made this appeal of like, okay, so regulate us, which is a similar appeal
to what Microsoft had said. And the difference between kind of those earlier appeals, you know,
right at the publications of our paper, there's always this response of, oh, we support regulation
in facial recognition. You know, this is what Amazon said. This is what IBM said in St.
Microsoft early on. And the difference between that stance and the current stance is that I think
currently, they now understand or they're now sort of pressured to understand that, you know,
you can't just say that you support facial recognition policy and that you acknowledge the
concerns without while also having the product on the market. So, you know, if you say that you
care about the concerns that have been brought up around your technology and that you support the
development of policy and regulation for facial recognition, you can't keep selling, you know,
that technology at the same time. So, you know, the recent announcements around we are no longer
sort of selling this technology, we're no longer allowing it to be used. It's a step forward to
say that, you know, we recognize that until there is some greater understanding and that work has
been done, you know, we really should not keep promoting this technology or keep deploying it.
Yeah. Yeah, their response for a long time was, I don't even, I'm trying to remember if they
were saying it was in their terms of service, but or, you know, in general, they were saying that,
you know, no law enforcement agencies were making decisions based on this
recognition law and it was, you know, people were making the decisions. And I'm curious,
you know, what you've seen in terms of kind of the failure mode of that, you know, rationale
and, you know, or, you know, specific examples of, you know, have there been well-publicized
examples of, you know, the use of facial recognition technology and policing for harm.
Oh, for harm? You know, that resulted in harm, like the current state of that.
Yeah, I think most cases resulted some harm. There's a great, there's a great report from Georgetown
law. I think Georgetown law has done a great job tracking the use of facial recognition in law
enforcement. And honestly, just exposing some of the malpractice that happens in that space.
I think one of the, you know, more striking examples is the way that law enforcement and this
was reported on multiple occasions, you know, law enforcement will try to use facial recognition
in order to identify as suspects in video footage, right? So they might, you know, have a crime
happen. I get some footage of the video scene and then attempt to match, you know, faces in their
mugshot database with, you know, whatever they can identify from the footage. Or, you know,
the more kind of scary situations, they'll take, you know, sketch descriptions of a face and try to
match that with mugshots in their database. You know, or other sort of iterations of, you know,
insane, you know, there was one reported case from Georgetown of them photoshopping like a celebrity
face kind of because they were told that the suspect looked like, you know, this particular
celebrity. So they're like, okay, let's see like in our mugshot database, if we can try to match,
you know, this celebrity face, like an arbitrary celebrity space, you know, to, you know,
somewhat in our database. And, you know, they, they definitely, and this is a huge element of it as
well, you know, the idea of how these, how the technology is actually being used is another layer
of concern to another dimension to think about because Amazon in their rebuttal to us always talks
about the idea of thresholds, you know, we want us, you know, we tell our clients to use a 99,
so certainly with a 95% threshold. And then when the press got worse, they were like, oh,
actually, we met a 99% threshold, a 99% threshold for all groups. And then if you do that,
then there's no more bias. And then there was a great reporter at Gizmodo that actually went to
investigate and talk to one of their police clients. And the police client was like, what is,
what is a threshold? We don't know what that is. And mind you, the default is 80%. This is like
the confidence threshold to make a prediction. So, yeah, there's so many ways in which, you know,
this technology is being incredibly misused by, you know, different police clients in ways that
are problematic that just it's not built to be used as. And then there's sort of the other case
of the technology being explicitly weaponized. So one situation that I think of often is the
Atlantic Tower sort of Plaza is, you know, a rent-controlled rent-stabilized building in Brooklyn. And
the residents of that building recently protested the landlords installation or applications
to install facial recognition in the building. And if you like sort of study the details of that case
and discuss with the tenants, it becomes clear that their concern is not just around sort of the
discriminatory performance of these systems or even, you know, the level of privacy. But the fact
that their landlord is, they know that their landlord is trying to evict certain people because
it's rent-controlled. So he can kind of raise the rent if he kind of gets rid of certain people
and brings a new tenants. But also he kind of has a history with these tenants of, you know,
harassing them in different ways and over-modeling them and attempting to track their movements
and kind of jeopardize, kind of paradoxically jeopardize their safety by virtue of installing
all of this surveillance tech. So they understood that the technology was not for their own safety,
but for the sake of this landlord to be able to kind of weaponize its use to monitor them,
to control their actions, to threaten them and, you know, threaten their safety and their
security, their home security. So it was an interesting situation of like not even the functionality
of facial recognition being part of the conversation, but just the conversation of like, wait,
this is a tool where, you know, one person has a lot of identifiable biometric information about
you and, you know, this authority figure can choose to use that for good or they can choose to
manipulate that and really, you know, weaponize that against you. So yeah, there's sort of those
situations that come up with respect to how facial recognition is used is like the client that
doesn't understand how to use it and messes up in a way that hurts people. The technology not even
working and being sort of mature to actually do its job and then sort of the situation of
the concern being around the authority figure not being very trustworthy and we're in a current
sort of state of society where not a lot of people trust the police right now. There's a lot of
questioning of, you know, the police authority and sort of the validity of some of these,
some of these groups that, some of these authority figures that actually currently have a lot of
the access to facial recognition today. So people are really beginning to question, you know,
what does it mean for us to build these tools and put it in the hands of, you know, certain authority
figures that we're now questioning that we now don't trust this easily. So yeah, lots of very
complicated questions with respect to, you know, how it ends up hurting people in the end.
Yeah, it makes me think of some of the work that Aviva Barhan is doing around trying to shift
the frame of reference from, you know, where is this possibly working to, you know, who is
this possibly harming and using that as the, using that as kind of the core question that we're
asking. Yeah, no, I totally, I totally agree with that shift and I actually, I've been like telling a
lot of people about, you know, because there's this whole community in AI and the machine learning
community about like AI for social good, right? So it's a space where there's sort of this active
imagination or imagining of like positive use cases and it's kind of this exploration of
different positive applications of AI. And I've been sort of pushing my idea, which, you know, it
might get some pickup soon of this idea of AI for social bad, where like, like we need to actually
understand, you know, how it hurts people and we need to like taxonomize, you know, the harms
that come up, we need to, we need to really reflect on some of these downstream consequences and
like build a vocabulary for, you know, all the ways things can go wrong and all the things impact
society. Yeah. Yeah, I root that. I think, you know, part of my question earlier around the
examples is trying to get at that. And, you know, when I think about, um, I guess I, you know,
I haven't, I haven't seen a kind of broadly publicized like pro-publica compass version of
facial recognition and law enforcement, you know, I mean, and I think that that, you know,
we've seen kind of the, the implication I can't tell you how many times I've seen pro-publica
compass, you know, images and slides and things like that. You know, I think we need those kinds of
examples so that people understand what the, what the challenges are. Yeah, applications are,
yeah. I would, I would really recommend to anyone that's interested in that to check out George
Town Law's work on this because I think they're probably the closest to identifying, you know, the
pro-publica, um, the pro-publica article was really compelling because they had against kind of selected
a very specific target and they were able to recreate the situation using very specific examples
and, and pull everything together in a beautiful story. Um, and I think the George Town Law
work with respect to looking at some of these, um, real world applications raises up important
questions and maybe what we need is to anchor that to a narrative that we can resonate with, like,
you know, very specific example or very specific tool that's being used, a very specific police
department. Um, but I would encourage anyone that's, that's curious to understand how facial recognition
is being used in law enforcement to check out that work. Um, but if they're kind of looking for
that story, you're right, that's something that, like, it still work to do with respect to telling
that narrative of how facial recognition kind of interfaces with some of these real world, um,
uh, uh, uh, institutes and structures. The other thing I'm going to say is like, it's not just law
enforcement and this is something that is hard for people to understand because it's never communicated
to us when facial recognition is used on us. But, um, you know, for example, higher view is a company
that uses facial recognition as part of their scoring system. You know, they'll evaluate or assess
for a different emotional cues to different questions and have that be part of your scoring system
to get a particular job. So like in a recorded interview, such as this one, um, or, uh, you know,
there's a lot of cases that I personally encounter an immigration of, you know, different, uh, you know,
a lot of, a lot of the impetus for facial recognition becoming a field was because, you know, um,
in like 1996, there was a push by the government in the US to, to literally funnel millions of
dollars, uh, to sort of, uh, propel the, the community forward, like the research community and kind
of build a research community. And they're incentive at the time, you know, a lot of the sponsors for
that initial effort was Homeland Security, different intelligence agencies. So, um, it's a huge
part of the kind of processes for immigration for verifying identification of, and I think
probably a lot of people as part of the immigration pipeline, you know, when you're at an airport
to match a passport to your face, they go through a process of verification through facial
recognition. Um, and like that's already kind of been rolled out for a while now. So there's a lot
of these interesting, um, use cases that like we actually experienced, but we're kind of
not necessarily registering as like, oh, this is actually facial recognition happening to me at
the airport to verify my identity right now. Um, but yeah, like immigration is another space where
this is very prevalent. Um, there's also, you know, situations and education of like monitoring
people and, um, uh, kind of student security systems. So yeah, there's definitely a lot of,
uh, a lot more applications and it's a lot more prevalent than people assume. Um, and the other
thing I will kind of mention, uh, and it's connected to immigration and connected to law enforcement,
but the idea of sort of digital surveillance of, you know, I post my face on my Twitter profile,
and I also post it on, you know, Facebook or whatever. And because of that, people can kind of
connect these different accounts in different ways or find me, even if I change my name completely,
even if I change my hair completely and change everything about me, because I can't change my,
like, actual facial structure. Uh, yeah. So some of that digital surveillance is something that I
think was very much exposed through clear view of like, oh, wait, uh, you know, when I put this
information out there, um, it's actually very traceable and it can kind of be used against me. Yeah.
Yeah, I think there's still this, um, you know, there's still the, the answer to this, well,
you know, I don't care, you know, the government knows who I am. I haven't done anything wrong.
And I think, you know, my hope is that through, you know, what's happening now in, in terms of,
you know, the kind of increased awareness of the, um, you know, for example, police brutality
that we're seeing in this country. And, you know, maybe we can connect that to, you know,
Abibas, algorithmic injustice and relational ethics and, and, you know, kind of minimizing
potential harm to undervoiced or, you know, communities without voices. And, um,
I'm not sure where I'm going with that. I think I'm expressing frustration. Like, how do you,
how do you, I think that there's, you know, and even, even personally, like, I think, you know,
I thought about it when I got global entry, but I still did it. And I'm like, yeah, yeah, you
know, it's like, it's a convenience in a lot of cases, but, you know, how do we parse through,
like, what's the cost of that convenience and for who and at the, at what level, um, and there's
just so many complex questions and issues here. Like, when I talk to sort of, um, my friends
outside of the space about facial recognition, they'll be like, oh, like this, the thing that,
like, let's me have a Snapchat filter. Like, I want that. Like, I want to be able to have,
you know, facial recognition, identify, you know, my, my, my, um, my key features so that I can
sort of, like, the, like, face landmarks I can put on, like, you know, the buddy ears or whatever.
Like, that's important to me. And I'm like, okay, no one's threatening your Snapchat filter
here. Calm down. And, you know, if anything, if you're someone that sees yourself as this, like,
citizen that, like, you know, doesn't necessarily, um, that isn't guilty of anything. Like, if anything,
that is sort of a reason to care more because, you know, especially if you're a person of color,
I think that, um, there's definitely risk, uh, with respect to just being in these systems,
which a lot of us are already, you know, a vast majority of Americans already in these systems,
already embedded in these systems. Um, you know, some of the test sets, um, coming in from
Homeland Security are coming from like visa images and images that you, uh, you know, and there's
also, you know, recent reports of DMV, you know, driver license images sort of being shared with
ISM being integrated into these mugshot databases, right? So there's no way to tell, you know,
when someone takes their picture, where that picture will land, especially if it's a government
agency taking that picture. And I think that that is something that we should sort of be concerned
about on an individual basis, because, um, especially if you're innocent, because they can kind of
duplicate you in these processes and these, um, these, uh, these, these systems that, you know,
you really have no business sort of, uh, being pulled into. And it, and it encompasses your
life, or could it has the potential to future for future inconveniences that could really, uh,
disrupt your way of life right now. So on an individual basis, I do think there's enough
reason for concern. But if you're like fully like, I have, you know, I, I have enough money for
bail type person, like, I, like, even if I get like misidentified, I can protect myself on any,
because there are people that are like that. Um, and then it's at that moment that you kind of
appeal to, um, okay, well, think about those that do not have that privilege and people that are
sort of increasingly vulnerable because of this technology. And I think that's why, um, a lot of
the conversations around restriction, you know, I appreciate the companies for publicly sort of
denouncing the use of the technology in a way that, um, um, you know, has been sort of well
received by the public and sort of well understood by the public. But I, I don't think I'm going to
depend on these companies to go all the way. Like, I don't think, I don't think they're going to
shoot themselves so in the foot that, um, you know, it actually achieves some of the protections
that need to happen for the sake of some of these marginalized groups and communities
most at risk. Um, I think that, um, you know, at best or at worst, what we can expect from these
companies is to, you know, uh, perhaps protect the majority of their, their clients or their users,
because they're, their users and the concerns of their, well, that's how we got here in the first
place. Yeah. Um, like, that's like a best, like, the people that would buy, you know, like,
whatever product they're selling, like, maybe they'll think about them. But for them to think about
people that, you know, might never have interacted with Amazon in any way, um, are completely out
of their scope of concern. You know, how do we actually support and protect them? And that's when
we start thinking about how important regulation is and how important it is to restrict its use, um,
in very particularly sort of like, uh, predatory cases against some of these very vulnerable groups.
So I think that's the impetus for that direction is like, it really shouldn't depend on companies
sort of when, the other thing too, is like, a lot of the companies that are most notorious.
It sounds awful, but like, you know, uh, Palantir, NEC, like, you know, some of these names,
people don't even know because they're, they're not consumer-facing. Even if Palantir was to,
to make a statement, I'm not sure how many, um, you know, how many people like, you know,
within my family or people that aren't in this space would like recognize that name, NEC,
a lot of people don't understand what that company is. Um, but they're really a large,
a large part of this market. They're actually a lot of the players and a lot of these small
startups too that I don't even, you know, I haven't even been able to identify as vendors because
they're so, it's such an opaque system. It's such an opaque process. Um, you know, how can we actually
force like, uh, government agencies to disclose and identify some of these groups so that we can
begin to like question the functionality of their systems and audit them. Um, but also, you know,
how can we protect the people that are currently affected by these companies that will very likely
not change because facial recognition is their main product. Um, unlike with IBM, um, uh,
Microsoft and Amazon, where they have other sources of revenue. Um, so, you know, how do we,
you know, rather than focusing on convincing those companies to change, how can we actually
push the regulation that can protect everybody that can protect everyone, especially those at risk?
So yeah, I think that there's still a lot of incentive, even though there's sort of
potential for personal apathy, you know, uh, there's still a lot of incentive to work on this
and to think about sort of these broader implications and, uh, push for regulation specifically.
Yeah. Cool. Well, Deb, thanks so much for taking the time. Yeah, I know, with you covered a lot of
ground. You look like you're like, every time I talk to someone about this, you're just like,
what just happened? Don't talk about facial recognition at the end of anybody. Awesome. Don't have to hear
about it, but okay. I think I'm looking at the, uh, I'm looking at the kind of recording time
right here. And I'm like, we could go for another hour. That's very complicated.
And end up in a similar place, actually. Yeah. Unless we're rolling up sleeves and like, you know,
coming up with a textonomy or something like that. And it's moving so quickly too, right? Like,
things are happening every day, something happened last week, something. How many of the things
that we talked about just happened this week? Like, and we didn't, yeah, I was going to ask you about
the, uh, did you see the, was it one of the Springer publications? Yeah, I was trying to detect
criminality out of off of facial images. Yeah. That was just a couple of days ago. I saw,
at least that's when I saw the tweet. Yeah, I've been telling people like, I wonder what's going to happen
tomorrow? Like, the summer that I was trying to write the saving face paper, we, we wrote a section
on sort of a policy developments and facial recognition. And we were keeping track of all the state
developments, all the bills and all the federal bills coming out literally every week. I would have
to rewrite the section just because it was just like constant stream of like bills coming out. Yeah,
it's crazy. The, the activity happening and because Amazon set this arbitrary one-year deadline,
people are going into hyperdrive and things are moving even quicker. So we'll see what happens,
but definitely an active space. There's a lot to talk about. Yeah. Awesome. Well, thanks so much, Deb.
Yeah, for sure. Thanks for having me. Okay. All right. Take care.
