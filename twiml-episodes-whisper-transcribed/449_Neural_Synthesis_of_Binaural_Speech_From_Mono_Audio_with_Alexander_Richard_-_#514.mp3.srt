1
00:00:00,000 --> 00:00:15,960
All right, everyone. I am here with Alexander Richard. Alexander is a research

2
00:00:15,960 --> 00:00:20,920
scientist with Facebook reality labs. Alexander, welcome to the Twomol AI

3
00:00:20,920 --> 00:00:25,760
podcast. Hi, thanks for having me. Absolutely. Really looking forward to our

4
00:00:25,760 --> 00:00:30,080
conversation. Let's get started by having you share a little bit about your

5
00:00:30,080 --> 00:00:35,120
background. How'd you come to work in machine learning? That is basically just a

6
00:00:35,120 --> 00:00:39,280
chain of coincidences. I was never planning to do it. I was planning to study

7
00:00:39,280 --> 00:00:42,920
music, but you know, in order to have a good career. Yeah, but in order to have a

8
00:00:42,920 --> 00:00:46,400
good career, you know, you need to be extraordinary, which I am not, that's

9
00:00:46,400 --> 00:00:51,000
basic. So it was like, that was my backup plan. And like in the first semester at

10
00:00:51,000 --> 00:00:54,640
university, it was in Germany at university, where they had a big institute for

11
00:00:54,640 --> 00:00:58,240
speech recognition. And there was before smartphones were big, before there was

12
00:00:58,240 --> 00:01:02,000
Alexa, before there was Siri. And they had like this open house one day, and you

13
00:01:02,000 --> 00:01:05,960
know, free food, free drinks, as a poor student, of course, you go there. And they

14
00:01:05,960 --> 00:01:09,040
had this demo where you could talk into a microphone at the words that you said

15
00:01:09,040 --> 00:01:13,160
would just magically appear on the screen. And it was like, wow, I was fleshed by

16
00:01:13,160 --> 00:01:17,080
that. That was amazing. So that was probably the point when I decided I really

17
00:01:17,080 --> 00:01:20,680
want to go into machine learning and this exactly what I want to do. So I

18
00:01:20,680 --> 00:01:25,240
basically stayed at this institute until I got my masters for like four or five

19
00:01:25,240 --> 00:01:32,520
years and then changed to computer vision for the PhD and randomly ran to some

20
00:01:32,520 --> 00:01:36,840
guy I cited a lot because he was doing the PhD in the same topic that I did. And

21
00:01:36,840 --> 00:01:40,000
he was like, you know, I graduated, I'm at Facebook now. We're looking for interns.

22
00:01:40,000 --> 00:01:43,840
It's a slightly different field than what you're doing. But don't you want to

23
00:01:43,840 --> 00:01:48,200
join us? I'm never going to leave Europe or Germany. But well, for an internship,

24
00:01:48,200 --> 00:01:54,760
why not? So I came to Pittsburgh. I joined this lab and it was, again, an

25
00:01:54,760 --> 00:01:58,920
amazing experience, like the first time that I could really see that with my

26
00:01:58,920 --> 00:02:03,560
education, I can work on something that can potentially change the way we

27
00:02:03,560 --> 00:02:08,960
communicate and really have a big impact on people's lives. And there was so

28
00:02:08,960 --> 00:02:12,120
exciting that I completely made up my mind. And half a year later was like, okay,

29
00:02:12,120 --> 00:02:16,520
I graduate and I want to go back to Facebook. There was a great project, a great

30
00:02:16,520 --> 00:02:20,240
time and you work with great people. So that's how I ended up here. Super random.

31
00:02:20,240 --> 00:02:24,360
Awesome. Well, tell us a little bit about Facebook reality labs. I'm assuming

32
00:02:24,360 --> 00:02:30,080
there's some kind of CMU connection given. I mean, there are a lot of

33
00:02:30,080 --> 00:02:34,160
industry labs in Pittsburgh, probably due to CMU. Our director, yes,

34
00:02:34,160 --> 00:02:39,200
I used to be a CMU professor. Okay. I think that's the origin of it all. And

35
00:02:39,200 --> 00:02:45,400
that's also the origin how Facebook reality labs came to the city. Yeah, I mean,

36
00:02:45,400 --> 00:02:51,160
the lab is all about social talent presence, right? So our mission, so to

37
00:02:51,160 --> 00:02:57,120
say, is, well, if we would have succeeded already, then we wouldn't have this

38
00:02:57,120 --> 00:03:01,480
conversation over video conferencing, right? You wouldn't be a small rectangle in

39
00:03:01,480 --> 00:03:04,960
my big room. I wouldn't be a small rectangle in your big room. But we would

40
00:03:04,960 --> 00:03:08,080
both be wearing a virtual reality headset. And we would stand right next to each

41
00:03:08,080 --> 00:03:12,400
other and having this conversation in 3D in virtual reality in a shared space.

42
00:03:12,400 --> 00:03:16,320
And that is really the promise and the mission of this lab that we try to accomplish,

43
00:03:16,320 --> 00:03:20,560
which I believe is super exciting. And as I said, which is like this one thing that

44
00:03:20,560 --> 00:03:24,200
can really be a big leap forward in the way how we connect over the

45
00:03:24,200 --> 00:03:28,680
distance, which in my view, makes it so exciting to be part of that. Nice. And as

46
00:03:28,680 --> 00:03:34,600
the lab, what's the relationship between the Facebook reality lab and

47
00:03:34,600 --> 00:03:41,840
the Oculus? Yeah, I mean, it emerged from Oculus. It used to be Oculus research,

48
00:03:41,840 --> 00:03:46,360
actually, at the time when I interned there. And then transitioned into Facebook

49
00:03:46,360 --> 00:03:50,160
reality lab, which is now the big branch, which is looking into VR and AR

50
00:03:50,160 --> 00:03:56,960
at Facebook. Okay. Awesome. Awesome. So tell us a little bit about your

51
00:03:56,960 --> 00:04:02,680
research interests. We're going to be talking in particular about one of your

52
00:04:02,680 --> 00:04:07,960
papers, Neural Synthesis of Binaural Speech from Mano Audio, which was an

53
00:04:07,960 --> 00:04:13,920
ICLR best paper award winner. But you know, more broadly, I'd love to hear

54
00:04:13,920 --> 00:04:19,280
your general research interests. Right. I'm particularly interested in

55
00:04:19,280 --> 00:04:23,760
anything audio visual. And particularly, you know, in generative modeling from

56
00:04:23,760 --> 00:04:28,240
audio and visual input. And that is like, I believe one field that has been

57
00:04:28,240 --> 00:04:33,240
largely forgotten in early years of computer vision, that the visual modality

58
00:04:33,240 --> 00:04:36,240
always just gives you very limited information, particularly when you think

59
00:04:36,240 --> 00:04:40,080
about a webcam, where the resolution is low, or when you think about a virtual

60
00:04:40,080 --> 00:04:44,840
reality headset, where you just have very limited sensory data. And audio is

61
00:04:44,840 --> 00:04:48,400
like one of these modalities that gives you a lot of cues that can fill in

62
00:04:48,400 --> 00:04:52,440
the gaps. So it holds like a big promise on improving whatever we developed

63
00:04:52,440 --> 00:04:56,600
over the last years in computer vision. And really filling in what visual

64
00:04:56,600 --> 00:05:01,520
sensors in specific circumstances cannot give you. And I found that super

65
00:05:01,520 --> 00:05:04,880
exciting. So I was really starting to focus looking particularly onto how can

66
00:05:04,880 --> 00:05:09,200
we combine audio and vision in a generative modeling setting where we really

67
00:05:09,200 --> 00:05:13,600
want to generate something like realistic looking faces in the end. And you can

68
00:05:13,600 --> 00:05:18,200
imagine that this is difficult if in this specific application of virtual

69
00:05:18,200 --> 00:05:22,240
reality, if you wear a headset, your face is partly occluded, you just cannot

70
00:05:22,240 --> 00:05:27,240
observe anything. Your data is always lossy. And occluding audio is a much

71
00:05:27,240 --> 00:05:32,240
hard task or put in other words, it's much easier to get the complete audio

72
00:05:32,240 --> 00:05:35,240
information that is surrounding you than to get the complete visual

73
00:05:35,240 --> 00:05:41,240
information. Yeah, can you can you give an example that you think really

74
00:05:41,240 --> 00:05:48,240
illustrates the, you know, the ultimate power of combining audio and video?

75
00:05:48,240 --> 00:05:52,440
Yes, I think we should talk about two things there. One is clearly the

76
00:05:52,440 --> 00:05:57,040
sensing side. So what is the input, right? And let us take the example of a virtual

77
00:05:57,040 --> 00:06:00,440
reality headset. We might have a camera that is kind of facing your mouth, but

78
00:06:00,440 --> 00:06:04,440
it's very hard to really get good illumination of the interior of your mouth

79
00:06:04,440 --> 00:06:08,440
and have some accurate tongue modeling. It is very hard to get all the mouth

80
00:06:08,440 --> 00:06:12,040
all the mouthclosures correct. Like I have a beard, right? There is a camera

81
00:06:12,040 --> 00:06:17,440
facing towards me with a skewed angle. It is extremely hard to see every detail

82
00:06:17,440 --> 00:06:21,640
of my lips. But with audio, we get these details. I can't produce the P sound

83
00:06:21,640 --> 00:06:25,240
without closing my lips. I can't use the M sound without closing my lips.

84
00:06:25,240 --> 00:06:29,240
All these correlations are clearly some that we need to explore. That is like

85
00:06:29,240 --> 00:06:33,040
on the sensing side, but then we also work obviously on the on the output

86
00:06:33,040 --> 00:06:37,840
side where it comes to putting things into virtual reality. And we have

87
00:06:37,840 --> 00:06:40,840
this correlations between audio and vision there as well, right? I mean,

88
00:06:40,840 --> 00:06:44,240
I just have a single microphone here, but if we had multiple microphones

89
00:06:44,240 --> 00:06:47,640
and I would be snapping my finger, you should hear this from two different

90
00:06:47,640 --> 00:06:51,640
directions. And that needs to be some audio visual correspondence, which

91
00:06:51,640 --> 00:06:55,040
is frequently forgotten. It's freaking like, yeah, okay, we generate something

92
00:06:55,040 --> 00:06:59,040
where I am talking and I can do things with my hand, but we have mono audio

93
00:06:59,040 --> 00:07:02,240
and the systems. And that seems like leaving out half of the signal that

94
00:07:02,240 --> 00:07:09,040
is there. Yeah, I think, you know, thinking about video games that are

95
00:07:09,040 --> 00:07:14,440
kind of purely, you know, generated, you know, in many cases, like, you know,

96
00:07:14,440 --> 00:07:19,640
it's as much as much more like a movie production than like a generated

97
00:07:19,640 --> 00:07:24,040
something that a model generates. You know, I used to hearing kind of stereo

98
00:07:24,040 --> 00:07:30,440
sounds and, but I'm imagining that, you know, that's a lot more difficult

99
00:07:30,440 --> 00:07:37,040
when you're creating generative scenes from a model of some sort.

100
00:07:37,040 --> 00:07:41,840
Yeah, absolutely. I think one of the big problems here is that in video games

101
00:07:41,840 --> 00:07:44,840
it's usually fine if you have something that is just plausible, meaning

102
00:07:44,840 --> 00:07:48,040
the sound is coming from the right direction and kind of fits the environment

103
00:07:48,040 --> 00:07:51,640
that's all good. Same for the visual part, right? You have a synthetic

104
00:07:51,640 --> 00:07:55,440
video character that doesn't necessarily authentically look like you.

105
00:07:55,440 --> 00:08:00,440
So plausibility is fine in video games, but for the mission that we are pursuing

106
00:08:00,440 --> 00:08:03,640
plausibility is not enough. We really want to have accuracy. We really

107
00:08:03,640 --> 00:08:07,640
want to represent yourself in virtual reality that your loved ones, your

108
00:08:07,640 --> 00:08:10,840
friends, people who know you are close to you can really recognize you and

109
00:08:10,840 --> 00:08:14,640
recognize all the subtleties in your voice, in your facial experience.

110
00:08:14,640 --> 00:08:17,840
But all the expressions, this micro expressions that you do, right?

111
00:08:17,840 --> 00:08:21,040
That needs to be there if you want to have a real social experience.

112
00:08:21,040 --> 00:08:26,040
And that is the big difference to movies and games where you can just post

113
00:08:26,040 --> 00:08:28,640
process and just need to produce something that is plausible.

114
00:08:28,640 --> 00:08:36,040
Right. Right. Yeah. For a long time now, the kind of holy grill and ARVR is

115
00:08:36,040 --> 00:08:39,440
this idea of the uncanny valley, like there's something, there's just

116
00:08:39,440 --> 00:08:43,840
something wrong. I think for a long time it's been pretty basic things,

117
00:08:43,840 --> 00:08:49,640
you know, the resolution of the visor isn't high enough. I think you're

118
00:08:49,640 --> 00:08:54,640
pointing to, you know, maybe we've overcome some of those challenges.

119
00:08:54,640 --> 00:09:00,040
And now the uncanny valley frontier is a lot more subtle. Is that part of

120
00:09:00,040 --> 00:09:03,940
it? Absolutely. Absolutely. It is. Whenever we have social

121
00:09:03,940 --> 00:09:07,440
interaction, there is so much that we are trained as humans, right?

122
00:09:07,440 --> 00:09:11,440
So I have like decades of training talking to people. You have a lot of

123
00:09:11,440 --> 00:09:15,040
training talking to people and we really know how a social conversation

124
00:09:15,040 --> 00:09:18,840
between humans works and what these subtleties are and how to interpret

125
00:09:18,840 --> 00:09:22,240
them. Well, let's say our mind knows, but we do not know this actively.

126
00:09:22,240 --> 00:09:26,840
Yeah. So it's very hard to quantify. So how do we teach a machine to do

127
00:09:26,840 --> 00:09:30,440
this to do something we cannot even quantify ourselves? And that is the

128
00:09:30,440 --> 00:09:33,160
massive challenge now to go beyond the uncanny valley with the

129
00:09:33,160 --> 00:09:37,640
subtleties of communication. And our approach there is to argue that

130
00:09:37,640 --> 00:09:41,440
everything needs to be metric, meaning whatever we can measure in reality

131
00:09:41,440 --> 00:09:44,640
should be transferred like that into virtual reality. If we do that

132
00:09:44,640 --> 00:09:47,840
right, then automatically we will have all these subtle social cues and

133
00:09:47,840 --> 00:09:51,440
virtual reality as well. However, if we would resort to a solution that

134
00:09:51,440 --> 00:09:54,640
is only plausible, we might transfer something into virtual reality

135
00:09:54,640 --> 00:09:57,440
that substantially changes the meaning of what you are saying.

136
00:09:57,440 --> 00:10:00,840
Like, I don't know, it's my smile, a generally happy smile, or am I

137
00:10:00,840 --> 00:10:03,840
being sarcastic? This is very important for our conversation.

138
00:10:03,840 --> 00:10:07,040
But if we misinterpret that because we generate something that is

139
00:10:07,040 --> 00:10:11,440
just plausible, but not accurate, not metric, then we have a problem

140
00:10:11,440 --> 00:10:13,240
because we changed the meaning of conversation.

141
00:10:14,240 --> 00:10:18,640
I'd love to have you dig a little bit into that comment a bit more

142
00:10:18,640 --> 00:10:23,440
about metrics and having everything that you can measure in a

143
00:10:23,440 --> 00:10:27,440
physical world, be measurable in virtual reality. The thing that

144
00:10:28,640 --> 00:10:32,840
the thought that it prompts is that there are an infinite many things

145
00:10:32,840 --> 00:10:37,840
that we could measure in the physical world and to treat all of

146
00:10:37,840 --> 00:10:43,440
those as discrete metrics in the virtual world. And then, you

147
00:10:43,440 --> 00:10:46,840
know, try to train a model, for example, that's looking at, you

148
00:10:46,840 --> 00:10:52,240
know, so many metrics. You know, that's a stark contrast to the

149
00:10:52,240 --> 00:10:55,840
way things that the way things have generally been trending,

150
00:10:55,840 --> 00:11:00,440
which is, you know, let's just take pixels and try to, you know,

151
00:11:00,440 --> 00:11:04,440
focus on pixels and then the networks, if we can throw enough

152
00:11:04,440 --> 00:11:07,640
compute at them, they'll figure everything out. Can you elaborate on

153
00:11:07,640 --> 00:11:09,040
kind of the way you think about that?

154
00:11:09,040 --> 00:11:13,240
I mean, absolutely. So when we talk about training a network,

155
00:11:13,240 --> 00:11:16,640
we still want to optimize a metric loss. Like in standard L2 loss,

156
00:11:16,640 --> 00:11:19,240
what everyone else is doing in the vision community as well,

157
00:11:19,240 --> 00:11:21,640
when you have generative models, it's, you see that frequently

158
00:11:21,640 --> 00:11:24,240
when you have supervision, you optimize your L2 loss against

159
00:11:24,240 --> 00:11:29,040
your supervised signal. So we do not really try to reinvent that.

160
00:11:29,040 --> 00:11:31,640
But what we are saying, when we say we want to be metric,

161
00:11:31,640 --> 00:11:35,440
means our systems have to learn from the best possible measurements,

162
00:11:35,440 --> 00:11:38,640
meaning if I only give you data where you see like a snippet of

163
00:11:38,640 --> 00:11:43,440
my mouth and maybe images of my eyes and a mono audio signal,

164
00:11:43,440 --> 00:11:45,840
something we can maybe get from a headset, from a virtual reality

165
00:11:45,840 --> 00:11:50,040
headset, we cannot expect that we can make a realistic representation

166
00:11:50,040 --> 00:11:53,640
of that in virtual reality, if we do not have better measurements,

167
00:11:53,640 --> 00:11:57,040
meaning it all starts with accurate measurements, highly accurate

168
00:11:57,040 --> 00:11:59,640
measurements. So the research that we do is like you would come to

169
00:11:59,640 --> 00:12:03,240
Pittsburgh, you get a 3D scan of your face, and that is prior

170
00:12:03,240 --> 00:12:06,440
information that is accurate measurements about how your face moves.

171
00:12:06,440 --> 00:12:09,440
And then the next step is to take this metric information

172
00:12:09,440 --> 00:12:14,240
of this priors that the models have learned and take the VR headset

173
00:12:14,240 --> 00:12:18,240
input. And you know, we can take these parts that are visible and

174
00:12:18,240 --> 00:12:21,440
transfer them into virtual reality. But there are parts that are

175
00:12:21,440 --> 00:12:24,440
not visible that we cannot measure while you're wearing a headset.

176
00:12:24,440 --> 00:12:28,240
And for this, we need the strong priors. And the strong priors

177
00:12:28,240 --> 00:12:31,440
is what we get from very accurate measurements in the sense that

178
00:12:31,440 --> 00:12:35,840
we need a kind of set of people where we have high fidelity,

179
00:12:35,840 --> 00:12:39,440
3D scans of facial motion, facial expressions, so that we can

180
00:12:39,440 --> 00:12:42,440
fill in the gaps. So, you know, whenever information is not

181
00:12:42,440 --> 00:12:45,840
present, how can we get the closest to reality? Well, we need

182
00:12:45,840 --> 00:12:51,440
some kind of prior in whatever way. So you mentioned, you know,

183
00:12:51,440 --> 00:12:55,740
going to the lab and getting scanned the paper that we'll be

184
00:12:55,740 --> 00:12:59,740
talking about falls under the context of this broader effort

185
00:12:59,740 --> 00:13:02,840
called codec avatars. Is that what that is? That scanning

186
00:13:02,840 --> 00:13:06,440
process? And exactly. It's also a little bit more about the

187
00:13:06,440 --> 00:13:10,940
broader effort. Absolutely. The idea of codec avatars is that

188
00:13:10,940 --> 00:13:15,340
you can have an avatar of yourself in virtual reality that

189
00:13:15,340 --> 00:13:18,340
looks like you, sounds like you, moves like you, of course,

190
00:13:18,340 --> 00:13:21,440
not autonomously, but driven by you while you wear the headset

191
00:13:21,440 --> 00:13:26,340
with this lossy sensory input. And this whole problem looks like

192
00:13:26,340 --> 00:13:29,440
you were not talking about the forget the name of the apple

193
00:13:29,440 --> 00:13:34,640
avatars cartoons. We're talking. No, no, we are realistic

194
00:13:34,640 --> 00:13:38,940
avatars. Exactly. Ideally, indistinguishable from reality,

195
00:13:38,940 --> 00:13:41,540
of course, this is a very high bar that we said for ourselves

196
00:13:41,540 --> 00:13:44,340
there, but we are talking about photo realistic avatars.

197
00:13:44,940 --> 00:13:48,040
Because again, the same point that I made before, if we really

198
00:13:48,040 --> 00:13:50,640
want to transfer and transmit all these subtleties of

199
00:13:50,640 --> 00:13:53,940
communication, a comic style avatars just not enough. There is

200
00:13:53,940 --> 00:13:57,340
not this very personalized specific smirk that you might

201
00:13:57,340 --> 00:14:00,940
have when you smile and your very specific facial expression.

202
00:14:00,940 --> 00:14:04,440
So we need something that is photo realistic. And well, the

203
00:14:04,440 --> 00:14:07,140
way we approach this is essentially, well, it all starts with

204
00:14:07,140 --> 00:14:10,540
like accurate data measurements. So we need enough facial

205
00:14:10,540 --> 00:14:14,640
data to be able to make a high quality 3D reconstruction

206
00:14:14,640 --> 00:14:17,900
in VR. We usually talk codec avatars, since for like

207
00:14:17,900 --> 00:14:21,440
encoder decoder. So when we talk about the decoder side, that

208
00:14:21,440 --> 00:14:25,440
is what would generate your highly realistic 3D representation

209
00:14:25,440 --> 00:14:29,840
in virtual reality. And that is what we tried to learn from lots

210
00:14:29,840 --> 00:14:33,040
of data where we have lots of cameras capturing your face.

211
00:14:33,040 --> 00:14:36,440
And then we are able to reconstruct your face in 3D. Of course,

212
00:14:36,440 --> 00:14:38,740
while you are being captured, you cannot do every kind of

213
00:14:38,740 --> 00:14:42,340
expression that you can do in reality. But what we can ask you

214
00:14:42,340 --> 00:14:46,240
to do is some peak expressions, some example expressions. And

215
00:14:46,240 --> 00:14:49,340
we know that neural networks are magnificent interpolation

216
00:14:49,340 --> 00:14:51,840
machines. So we feed this data into neural networks. And then

217
00:14:51,840 --> 00:14:54,540
if you have some facial expression that falls in between two

218
00:14:54,540 --> 00:14:58,240
different peaks, we can generate a very accurate and faithful

219
00:14:58,240 --> 00:15:01,340
representation of that specific expression just from what

220
00:15:01,340 --> 00:15:03,940
the network has learned from the peak expressions that you

221
00:15:03,940 --> 00:15:08,340
can be doing. So this is the decoder side of codec avatars. On

222
00:15:08,340 --> 00:15:11,140
the encoder side, we are talking more about how can we take the

223
00:15:11,140 --> 00:15:14,040
sensory input from the headset, which is obviously lossy, we

224
00:15:14,040 --> 00:15:17,340
cannot place hundreds of cameras around here, right? How can

225
00:15:17,340 --> 00:15:21,540
we take this very limited data and map this to a representation

226
00:15:21,740 --> 00:15:25,240
where the decoder can interpret enough to reconstruct your

227
00:15:25,240 --> 00:15:29,440
face how it looks like. And as you can imagine, this is a very

228
00:15:29,440 --> 00:15:31,540
challenging problem that we have on the visual and on the

229
00:15:31,540 --> 00:15:35,740
audio side, because the data that we can collect for generating

230
00:15:35,740 --> 00:15:38,840
this 3D representation, well, we can have multiple cameras

231
00:15:38,840 --> 00:15:41,740
et cetera, of your face, but we can never have paired data

232
00:15:41,740 --> 00:15:44,740
with whenever you're wearing your headset, because your face

233
00:15:44,740 --> 00:15:47,440
is occluded by the headset, your sound might be different

234
00:15:47,440 --> 00:15:50,140
because you know, you're in a massive capture stage that is

235
00:15:50,340 --> 00:15:53,040
loud because there is a sea and stuff like that for visual

236
00:15:53,040 --> 00:15:55,340
captures, but then for audio captures, you might be in another

237
00:15:55,340 --> 00:15:58,540
room, I have high ceilings that might be reverberation. So all

238
00:15:58,540 --> 00:16:01,040
these differences in the domains that we have on the input

239
00:16:01,040 --> 00:16:03,640
side and on what we want to generate, this poses the big

240
00:16:03,640 --> 00:16:06,640
problem of how can we actually connect these two parts, the

241
00:16:06,640 --> 00:16:10,340
encoder and the decoder. And that is one of the big challenges

242
00:16:10,340 --> 00:16:14,140
that's probably a bit deep if we want to dig into the details

243
00:16:14,140 --> 00:16:16,940
here for codec avatars in general, but I'm happy to answer any

244
00:16:16,940 --> 00:16:20,140
questions in specific if you want to dig deeper on that.

245
00:16:21,740 --> 00:16:26,240
Since you're inviting us to elaborate a little bit on the

246
00:16:26,240 --> 00:16:30,440
challenges there, what makes the connection between these two

247
00:16:30,440 --> 00:16:33,740
the key source of challenge in this problem domain?

248
00:16:34,240 --> 00:16:38,440
Right. So what people have shown is that, you know, you have

249
00:16:38,440 --> 00:16:41,740
variational auto encoder, you have generative for zero networks.

250
00:16:41,840 --> 00:16:45,840
So if you learn a representation of data that you captured,

251
00:16:45,840 --> 00:16:49,340
we have shown that we can generate faithful and highly accurate

252
00:16:49,340 --> 00:16:52,240
representations of, for example, your face or even of sound,

253
00:16:52,240 --> 00:16:54,640
if you look at what's going on in the Texas Beach community,

254
00:16:54,840 --> 00:16:57,740
that is extremely realistic and works extremely well.

255
00:16:57,740 --> 00:17:02,040
What is a bit lacking is the question if you have a noisy

256
00:17:02,040 --> 00:17:05,040
input for audio that could be speech with like a lot of background

257
00:17:05,040 --> 00:17:08,240
noise with kids running around with car driving with an AC

258
00:17:08,240 --> 00:17:11,140
running for vision that is you wear the headset, you have

259
00:17:11,640 --> 00:17:15,040
difficult illumination, you have varying background because

260
00:17:15,040 --> 00:17:18,640
you move around. And the question is, how can we use this

261
00:17:18,640 --> 00:17:21,540
information to really condition our models that can generate

262
00:17:21,540 --> 00:17:25,240
this faithful representations and still match exactly what we

263
00:17:25,240 --> 00:17:31,240
have seen on the sensory input? And yeah, so one way we approach

264
00:17:31,240 --> 00:17:35,440
this is by essentially looking at the renders in VR.

265
00:17:35,440 --> 00:17:38,940
So if you think about the whole pipeline, you have headset

266
00:17:38,940 --> 00:17:43,540
inputs, we want to decode this to some numeric representation

267
00:17:43,540 --> 00:17:46,740
that can be transmitted from the transmitted to the receiver.

268
00:17:46,940 --> 00:17:49,940
And then on the receiver side, we want to have this big network

269
00:17:49,940 --> 00:17:52,640
that creates this code and generates your face out of that.

270
00:17:52,640 --> 00:17:55,540
The question is, how do we match the code? And yeah, for that

271
00:17:55,540 --> 00:17:59,340
we basically apply some standards, domain transfer techniques

272
00:17:59,340 --> 00:18:04,340
it's based on guns based on some other techniques, it is

273
00:18:04,340 --> 00:18:08,540
always difficult to evaluate this because a major problem is

274
00:18:08,540 --> 00:18:10,940
that we do not have correspondences. So how do we really

275
00:18:10,940 --> 00:18:14,440
know that what we represent is absolutely faithful and 100

276
00:18:14,440 --> 00:18:16,940
percent faithful. And yeah, that is where one of the big

277
00:18:16,940 --> 00:18:19,140
research challenges lies for the future.

278
00:18:19,140 --> 00:18:22,940
And is the idea that you've got the headset and the headset

279
00:18:22,940 --> 00:18:28,940
has cameras looking in at the eyes? And is there a, is there

280
00:18:28,940 --> 00:18:32,940
a research foundation that comes out of, I don't know where

281
00:18:32,940 --> 00:18:37,340
we'll come out of, you know, biology, psychology, neuropsychology

282
00:18:37,340 --> 00:18:41,440
that says that the eyes are robust enough to tell you

283
00:18:41,440 --> 00:18:43,440
everything that might be happening with the face?

284
00:18:44,140 --> 00:18:47,940
We try to stay away from these kind of priors because

285
00:18:47,940 --> 00:18:50,540
as I said, there are several things in communication that we

286
00:18:50,540 --> 00:18:53,140
just cannot quantify and even psychology on neuroscience

287
00:18:53,140 --> 00:18:55,840
cannot really tell us which parts of the face are important

288
00:18:55,840 --> 00:18:58,740
for which parts of nonverbal communication specifically.

289
00:18:58,940 --> 00:19:03,240
So what we try is really to get the most amount of data we

290
00:19:03,240 --> 00:19:06,740
can get from a headset, whatever restricts us in terms of

291
00:19:06,740 --> 00:19:10,940
hardware, but that is our approach to this and not relying on

292
00:19:10,940 --> 00:19:13,540
something where we introduce human knowledge, which in the

293
00:19:13,540 --> 00:19:16,940
end might be wrong or might be biased. And it's just not

294
00:19:16,940 --> 00:19:20,140
data driven anymore in that case, but there's an assumption

295
00:19:20,140 --> 00:19:24,740
that there is that relationship, the eyes, you know, we're

296
00:19:24,740 --> 00:19:27,340
going to look at the eyes and we're going to try to predict

297
00:19:27,340 --> 00:19:31,040
the face and with enough data, we hope that that relationship

298
00:19:31,040 --> 00:19:31,540
holds.

299
00:19:32,140 --> 00:19:34,740
Absolutely. I mean, if you just look at the anatomy of your

300
00:19:34,740 --> 00:19:36,940
face, you see that some parts of your face, just if more

301
00:19:36,940 --> 00:19:40,340
muscles can expose much more motion and that is like areas

302
00:19:40,340 --> 00:19:43,940
around the lips, which are certainly, you know, harder or like

303
00:19:43,940 --> 00:19:47,240
more important to actually observe than your ears, which barely

304
00:19:47,240 --> 00:19:50,040
move at all and which are very easy to just plug in from

305
00:19:50,040 --> 00:19:51,940
prior information that we have from you.

306
00:19:52,440 --> 00:19:59,040
Yeah. Yeah. Awesome. Awesome. So tell us about the specific

307
00:19:59,040 --> 00:20:02,620
paper, the neural synthesis of the neural speech from

308
00:20:02,620 --> 00:20:07,140
Mono audio. What's the kind of what's the setting and

309
00:20:07,140 --> 00:20:12,740
motivation? Yeah. Let's draw a quick line to computer vision

310
00:20:12,740 --> 00:20:16,940
or computer graphics. 3D neural rendering is big in that field

311
00:20:16,940 --> 00:20:20,040
for years and has led to massive improvements. And for some

312
00:20:20,040 --> 00:20:23,340
reason, audio was kind of lacking behind that, right? So when

313
00:20:23,340 --> 00:20:28,440
you look at like, how do you do specialist audio in computer

314
00:20:28,440 --> 00:20:32,040
games and movies, it is all breaking down to linear time and

315
00:20:32,040 --> 00:20:35,340
variant systems. You have some filters that you measure in an

316
00:20:35,340 --> 00:20:40,240
idealized anicoic setup. And then you just like stack a bunch

317
00:20:40,240 --> 00:20:42,940
of linear filters on top of each other and get some plausible

318
00:20:42,940 --> 00:20:48,240
reconstruction. And we wondered why this is the case and why we

319
00:20:48,240 --> 00:20:50,840
do not follow the same route that computer graphics is going

320
00:20:50,840 --> 00:20:54,240
where you have some 3D renderers that you can train and to end

321
00:20:54,240 --> 00:20:57,940
from data. Because that is ultimately what we want, right? If

322
00:20:57,940 --> 00:21:01,240
we want to represent reality, we do not want to have a linear

323
00:21:01,240 --> 00:21:04,040
function that is as close as possible. But we want to have

324
00:21:04,040 --> 00:21:06,540
data that we can measure and that we can optimize against.

325
00:21:06,540 --> 00:21:10,240
And that was basically the premise we started this work with.

326
00:21:10,240 --> 00:21:15,040
And it was kind of interesting to see, first of all, to see

327
00:21:15,040 --> 00:21:18,040
how much better traditional signal processing and audio

328
00:21:18,040 --> 00:21:22,540
processing is compared to how good traditional computer

329
00:21:22,540 --> 00:21:25,840
graphics methods were, right? So if you look at a traditional

330
00:21:25,840 --> 00:21:28,440
render of a computer graphics method, traditional face renders,

331
00:21:28,440 --> 00:21:31,740
you had facial models that were crude approximations and we're

332
00:21:31,740 --> 00:21:34,640
really looking on Kenny. And if you look at results like

333
00:21:34,640 --> 00:21:37,540
recent progress in neuro rendering, that looks super realistic.

334
00:21:37,840 --> 00:21:40,840
In audio, it seems that this gap was much smaller probably

335
00:21:40,840 --> 00:21:43,540
because audio is well understood on a mathematical way has

336
00:21:43,540 --> 00:21:47,140
less uncertainty in it inherently. But we were still figuring

337
00:21:47,140 --> 00:21:52,640
that there is there is the stack of linear transformations

338
00:21:52,640 --> 00:21:55,540
that you do in signal processing that can gradually introduce

339
00:21:55,540 --> 00:21:57,840
more and more errors. And you always make assumptions, you always

340
00:21:57,840 --> 00:22:01,240
make an assumption that your room behaves in a very specific

341
00:22:01,240 --> 00:22:04,040
way. You make assumptions that your ears behave in a very

342
00:22:04,040 --> 00:22:06,940
specific way, in the way, you know, how they modify sound.

343
00:22:07,240 --> 00:22:10,940
And that is not really true. Let me tell you about a funny

344
00:22:10,940 --> 00:22:14,340
experiment we did initially. We used by neural microphones

345
00:22:14,340 --> 00:22:16,840
that we stuck into the ears of a colleague of mine. And he was

346
00:22:16,840 --> 00:22:20,040
just walking around and recording sound. And I was putting on

347
00:22:20,040 --> 00:22:22,640
my headphone and listening to that recording. And because our

348
00:22:22,640 --> 00:22:25,940
two ear shapes were so different, to me, it always sounded as

349
00:22:25,940 --> 00:22:28,840
it was coming from behind my head. And that was clearly not

350
00:22:28,840 --> 00:22:32,140
the case in reality. So there is clearly something where this

351
00:22:32,140 --> 00:22:34,940
traditional signal processing based approaches reach their

352
00:22:34,940 --> 00:22:37,940
limits. And if we have a data driven approach, we can really

353
00:22:37,940 --> 00:22:41,540
learn something from evidence from data and do not have to make

354
00:22:41,540 --> 00:22:45,240
modeling assumptions. And we could show in this paper that we

355
00:22:45,240 --> 00:22:48,940
can really improve the quality of audio in this case. And I

356
00:22:48,940 --> 00:22:51,740
think it's a very exciting direction to think about 3D

357
00:22:51,740 --> 00:22:55,040
modeling of audio in the way that we think about 3D modeling

358
00:22:55,040 --> 00:23:02,740
of graphics. And so when you talk about improving the quality

359
00:23:02,740 --> 00:23:07,940
of audio, you know, drill down into the specific metric that

360
00:23:07,940 --> 00:23:09,340
you're looking at there.

361
00:23:10,140 --> 00:23:14,640
Yeah, also super difficult. We of course went went down the

362
00:23:14,640 --> 00:23:17,100
first way that you always do in computer vision. You

363
00:23:17,100 --> 00:23:19,540
optimize an else who lost on the raw waveform. Why shouldn't

364
00:23:19,540 --> 00:23:22,340
it work? If you reach zero loss, you are perfect. So it's all

365
00:23:22,340 --> 00:23:25,340
good. The problem is in reality, of course, you never reach a

366
00:23:25,340 --> 00:23:28,040
zero loss. And then the question is, what kind of errors do

367
00:23:28,040 --> 00:23:31,340
you have? And it seems that particularly in audio, this

368
00:23:31,340 --> 00:23:34,340
discrepancy between the value of your L2 loss and the

369
00:23:34,340 --> 00:23:37,640
perceptual quality of the sound that you generate is pretty

370
00:23:37,640 --> 00:23:42,140
massive. So you can easily imagine that more so than a video,

371
00:23:42,140 --> 00:23:45,140
you can easily imagine that you have like tiny deviations

372
00:23:45,140 --> 00:23:48,140
that just imagine you have a sine wave in audio. And you

373
00:23:48,140 --> 00:23:50,740
have like a tiny flickering, you always say pretty close, but

374
00:23:50,740 --> 00:23:53,240
you have like a high frequency signal that flickers. All of

375
00:23:53,240 --> 00:23:55,540
a sudden, you have the super annoying high frequency noise

376
00:23:55,540 --> 00:23:59,440
in your signal that we perceptually find really disturbing

377
00:23:59,440 --> 00:24:02,540
and can immediately say, well, this is not real. So that was

378
00:24:02,740 --> 00:24:06,140
really challenging. And we were diving deeper into this

379
00:24:06,140 --> 00:24:09,140
looking into the loss functions and how to train audio. And

380
00:24:09,140 --> 00:24:12,340
one of the observations is that the L2 loss has a very

381
00:24:12,340 --> 00:24:16,140
undesirable property audio is basically a composition of

382
00:24:16,640 --> 00:24:19,540
amplitude information and face information amplitude, meaning

383
00:24:19,540 --> 00:24:23,340
how high are the peaks of the sine waves and face information

384
00:24:23,340 --> 00:24:25,840
meaning, where does the sine wave start? How much is it

385
00:24:25,840 --> 00:24:29,340
shifted, right? And then we can add up potentially infinitely

386
00:24:29,340 --> 00:24:31,940
many sine waves. And this is the audio signal that you can

387
00:24:31,940 --> 00:24:35,440
hear, essentially a four year decomposition. And what we

388
00:24:35,440 --> 00:24:39,040
figured out is that the L2 loss, you can actually show the L

389
00:24:39,040 --> 00:24:42,240
2 loss optimizes the amplitudes aggressively in the

390
00:24:42,240 --> 00:24:45,440
beginning of the training process of a neural network, but

391
00:24:45,440 --> 00:24:47,940
completely neglects the face information. That means you

392
00:24:47,940 --> 00:24:51,140
try to fit amplitudes of signals where you shouldn't even

393
00:24:51,140 --> 00:24:53,940
fit the amplitude because if you just would shift the signals

394
00:24:53,940 --> 00:24:56,940
accordingly, it would solve all your problems. The L2 loss is

395
00:24:56,940 --> 00:25:01,140
not doing this inherently, it is keeping, it is trying to

396
00:25:01,140 --> 00:25:03,440
match the amplitudes, but it's not trying to match the face

397
00:25:03,440 --> 00:25:06,140
and it's not trying to match the face at a very late time

398
00:25:06,140 --> 00:25:11,240
in training, which turns out to be a problem. So the very simple

399
00:25:11,240 --> 00:25:14,240
solution on the loss side is to explicitly optimize for this

400
00:25:14,240 --> 00:25:18,440
face information. However, face information is highly

401
00:25:18,440 --> 00:25:21,740
difficult because as soon as you have noise and in every real

402
00:25:21,740 --> 00:25:24,240
life measurement, you do have noise as soon as you have

403
00:25:24,240 --> 00:25:29,640
noise, this is not really reliable data. So we went a step

404
00:25:29,640 --> 00:25:34,040
further and tried to implement something about noise in both

405
00:25:34,040 --> 00:25:37,340
the amplitude direction and the face direction. Yeah,

406
00:25:37,340 --> 00:25:40,640
absolutely. I mean, we tried to record audio in a room and

407
00:25:40,640 --> 00:25:43,040
we tried to get something that is as clean as possible, but

408
00:25:43,040 --> 00:25:45,740
you might have electrical noise because of your setup. You

409
00:25:45,740 --> 00:25:49,340
might have some airflow that is, I don't know, going into the

410
00:25:49,340 --> 00:25:52,340
microphones and that is that is air pressure, right, which is

411
00:25:52,340 --> 00:25:56,140
interpreted as sound. So all these subtleties really corrupt

412
00:25:56,140 --> 00:25:59,140
your data constantly when you have audio, and you have to

413
00:25:59,140 --> 00:26:04,240
account for that. So yeah, we kind of shot for a solution that

414
00:26:04,240 --> 00:26:08,040
is on the modeling side. And I believe that is something that

415
00:26:08,040 --> 00:26:10,840
has been highly successful in all of deep learning that you

416
00:26:10,840 --> 00:26:15,040
take some component of traditional processing and incorporated

417
00:26:15,040 --> 00:26:18,040
in your networks, the most prominent success stories, certainly

418
00:26:18,040 --> 00:26:20,140
the convolution operation, right, bringing convolution

419
00:26:20,140 --> 00:26:24,340
into your networks has boost computer vision. In audio

420
00:26:24,340 --> 00:26:27,640
dynamic time, warping is a very old traditional technique

421
00:26:27,640 --> 00:26:31,640
where you want to align signals in some way. And we essentially

422
00:26:31,640 --> 00:26:34,040
incorporate that as a neural network layer, so fully

423
00:26:34,040 --> 00:26:36,640
differentiable neural network layer to solve this face

424
00:26:36,640 --> 00:26:39,840
problem because this time warping allows you to shift

425
00:26:39,840 --> 00:26:43,040
components of the audio signal left or right depending on where

426
00:26:43,040 --> 00:26:46,040
you want to have them. And yeah, it turned out that once

427
00:26:46,040 --> 00:26:50,240
more incorporating one of these traditional methods can have

428
00:26:50,240 --> 00:26:53,040
a big impact on how well neural networks perform.

429
00:26:54,240 --> 00:27:00,440
And so in the problem that you've formulated, what are the

430
00:27:00,640 --> 00:27:04,240
the inputs is it, you know, clearly there's a mono audio

431
00:27:04,540 --> 00:27:09,740
and the output is going to be some stereo audio. But are

432
00:27:09,740 --> 00:27:15,440
you also, you know, you localized as their location inputs

433
00:27:15,440 --> 00:27:18,740
like how what are the very inputs? Yeah, let's let's talk

434
00:27:18,740 --> 00:27:20,640
about this very specific application where we want to

435
00:27:20,640 --> 00:27:23,440
generate by a neural sound, meaning that is spatialized where

436
00:27:23,440 --> 00:27:25,940
it can hear where it's coming from, but that also sounds as if

437
00:27:25,940 --> 00:27:28,340
you perceive it with your ears, you know, all the transformations

438
00:27:28,340 --> 00:27:31,240
that your ears do to the sound. The input clearly, as you said,

439
00:27:31,240 --> 00:27:34,940
is mono audio. I might have a microphone very close to my lips

440
00:27:35,140 --> 00:27:37,940
and I just speak into that and that is all I need as input

441
00:27:37,940 --> 00:27:41,140
signal, but I also need to know is the relative position

442
00:27:41,140 --> 00:27:43,640
between you and me. If we are in a virtual environment and you

443
00:27:43,640 --> 00:27:46,440
are standing two meters apart to the right of me, your

444
00:27:46,440 --> 00:27:48,840
voice will sound different as if you're like super close to my

445
00:27:48,840 --> 00:27:51,940
face and on the left of me. So that is like the second input.

446
00:27:51,940 --> 00:27:55,040
Where are we in the virtual space in relation to each other?

447
00:27:56,040 --> 00:27:59,040
And given this input, we want to modify your mono signal

448
00:27:59,440 --> 00:28:02,940
based on these positions so that you receive a stereo

449
00:28:02,940 --> 00:28:06,540
signal on your headphones that is, you know, simulating this

450
00:28:06,540 --> 00:28:08,840
effect of you hear it with your own ears, meaning the

451
00:28:08,840 --> 00:28:11,840
spatialization is correct and the transformations, the signal

452
00:28:11,840 --> 00:28:14,840
undergoes with your ears is correct as well. So that is

453
00:28:14,840 --> 00:28:18,540
essentially the problem setting of this particular problem.

454
00:28:19,240 --> 00:28:23,540
What we were restricting ourselves to is one set of ears

455
00:28:23,540 --> 00:28:26,540
because you can imagine ears are like fingerprints like

456
00:28:26,540 --> 00:28:30,640
everyone has a different ear shape and it's a really unsolved

457
00:28:30,640 --> 00:28:33,440
problem how to generalize to different ear shapes. So we use

458
00:28:33,440 --> 00:28:36,240
like a kind of medium ear shape, a generic ear shape that

459
00:28:36,240 --> 00:28:40,240
works kind of well for everyone. But apart from that, it's

460
00:28:40,240 --> 00:28:42,640
really this full setting of having mono signal and just the

461
00:28:42,640 --> 00:28:45,440
positions and getting a complete manner of signal synthesized

462
00:28:45,440 --> 00:28:46,240
on your headphones.

463
00:28:46,440 --> 00:28:52,240
Okay. And what is the the data set that you use for the

464
00:28:52,240 --> 00:28:55,140
modeling? How did you collect it? What challenges did you find

465
00:28:55,140 --> 00:28:55,440
there?

466
00:28:56,340 --> 00:29:00,240
Yeah, interesting question. Glad you asked for that. It was a

467
00:29:00,240 --> 00:29:04,840
long process to actually get a successful capture. We started

468
00:29:04,840 --> 00:29:09,340
trying something where you can buy mannequins with silicon

469
00:29:09,340 --> 00:29:13,240
ears that behave similar to you flesh of similar properties.

470
00:29:13,440 --> 00:29:17,040
And we started with that and placed just such a mannequin

471
00:29:17,040 --> 00:29:20,140
into a room and then had a tracking system that we can put

472
00:29:20,140 --> 00:29:22,540
markers on top, basically a motion tracking system,

473
00:29:22,540 --> 00:29:24,640
markers on top of the mannequin, markers on top of the

474
00:29:24,640 --> 00:29:27,440
participant who is walking around and talking to the mannequin.

475
00:29:27,640 --> 00:29:30,440
There were ears and microphones in the ears of the mannequin

476
00:29:30,640 --> 00:29:34,540
and we get sound so far so good. We have tracked everything

477
00:29:34,540 --> 00:29:36,940
so we get all the information we need, right? The position

478
00:29:36,940 --> 00:29:40,440
between mannequin and speaker and also the binarital sound.

479
00:29:42,440 --> 00:29:46,940
The initial round of this was completely useless and without

480
00:29:46,940 --> 00:29:50,040
any success. We had an AC running in that room and if you

481
00:29:50,040 --> 00:29:52,240
spend like, I don't know, two hours in the room, it gets

482
00:29:52,240 --> 00:29:56,040
hot at the AC turns on and off and on and off. And then you

483
00:29:56,040 --> 00:29:59,240
could hear footsteps of people and you could hear people

484
00:29:59,240 --> 00:30:02,140
breathing as they came too close to the microphone. So you

485
00:30:02,140 --> 00:30:04,640
can imagine all these things, all these lessons that you

486
00:30:04,640 --> 00:30:06,640
that you learned when you do a data collection that you never

487
00:30:06,640 --> 00:30:07,640
did before.

488
00:30:09,140 --> 00:30:12,640
We can look at that as some kind of regularization or

489
00:30:13,640 --> 00:30:15,640
the main adaptation or something.

490
00:30:15,640 --> 00:30:18,240
That is true. That is true. But with the premise that we want

491
00:30:18,240 --> 00:30:21,640
to learn metric information that we really want to get out

492
00:30:21,640 --> 00:30:24,340
with reality sounds like regularization is bad, right?

493
00:30:24,340 --> 00:30:27,040
Regularization is just like another prior that we impose

494
00:30:27,040 --> 00:30:30,240
another assumption that we make on the signal and ideally we

495
00:30:30,240 --> 00:30:32,340
want to learn from a signal that is as clean as possible.

496
00:30:32,340 --> 00:30:35,240
So it was really worth going all the way of making a room

497
00:30:35,240 --> 00:30:38,240
acoustically treated that it's silent enough and that you

498
00:30:38,240 --> 00:30:43,840
really just have the default room reverberations and these

499
00:30:43,840 --> 00:30:47,440
audio effects that you have when you talk in a room, but no

500
00:30:47,440 --> 00:30:48,440
noise floor anymore.

501
00:30:49,440 --> 00:30:53,040
We basically went down all this way, installed some

502
00:30:53,040 --> 00:30:56,240
acoustic paneling and improved the capture setup

503
00:30:56,240 --> 00:30:59,040
significantly, which in the end gave us successful data.

504
00:30:59,040 --> 00:31:03,040
And it should also be mentioned that like, if you compare

505
00:31:03,040 --> 00:31:06,040
to how this has been solved before, you need to build an

506
00:31:06,040 --> 00:31:08,040
unequake chamber and then you would record just the

507
00:31:08,040 --> 00:31:11,040
transformations that are here that your ear are doing in an

508
00:31:11,040 --> 00:31:13,040
unequake chamber because you do not have room reverberations

509
00:31:13,040 --> 00:31:14,040
there, right?

510
00:31:14,040 --> 00:31:17,040
So you just capture that as a bunch of fixed spatial

511
00:31:17,040 --> 00:31:19,040
positions and you have a model for that.

512
00:31:19,040 --> 00:31:22,040
And then you have another model where you measure room in

513
00:31:22,040 --> 00:31:24,040
pulse responses at different places in a room.

514
00:31:24,040 --> 00:31:26,040
And then again, you try to stack this together.

515
00:31:26,040 --> 00:31:28,040
That is what traditional processing is doing.

516
00:31:28,040 --> 00:31:32,040
And one of the big disadvantages there was you always measure

517
00:31:32,040 --> 00:31:34,040
as discrete spaces in time.

518
00:31:34,040 --> 00:31:37,040
And if you can imagine you walk around, this is how sound

519
00:31:37,040 --> 00:31:38,040
changes, right?

520
00:31:38,040 --> 00:31:39,040
Doppler effect.

521
00:31:39,040 --> 00:31:42,040
If I get closer to some sound source or closer to a

522
00:31:42,040 --> 00:31:46,040
microphone, the sound waves basically compress and you get

523
00:31:46,040 --> 00:31:48,040
like these, yeah, these motion effects.

524
00:31:48,040 --> 00:31:50,040
And you can never capture this in a static setup.

525
00:31:50,040 --> 00:31:53,040
So we were really shooting for something where we have this

526
00:31:53,040 --> 00:31:56,040
dynamic setup where we track a person in real time and we

527
00:31:56,040 --> 00:31:59,040
have moving trajectories of sound that we can model.

528
00:31:59,040 --> 00:32:03,040
And I think, yeah, that is probably one of the reasons that

529
00:32:03,040 --> 00:32:05,040
the system in the end turned out to work so well.

530
00:32:05,040 --> 00:32:08,040
That this is like the first time that you have this realistic

531
00:32:08,040 --> 00:32:11,040
scenario of moving sound sources.

532
00:32:11,040 --> 00:32:16,040
Now, I can even with that said, I can imagine a pretty broad

533
00:32:16,040 --> 00:32:18,040
spectrum of complexity.

534
00:32:18,040 --> 00:32:23,040
Like I can imagine looking at the relative position of the

535
00:32:23,040 --> 00:32:26,040
participant in the mannequin in 2D space.

536
00:32:26,040 --> 00:32:29,040
Can imagine then looking at them in 3D space.

537
00:32:29,040 --> 00:32:33,040
You know, I can imagine then, you know, extending that to

538
00:32:33,040 --> 00:32:37,040
include like radio direction for each other part.

539
00:32:37,040 --> 00:32:40,040
Like how did you manage that complexity factor?

540
00:32:40,040 --> 00:32:41,040
Absolutely.

541
00:32:41,040 --> 00:32:44,040
I mean, we tried the simplest thing which is restricting

542
00:32:44,040 --> 00:32:47,040
ourselves to a kind of donut of social interaction.

543
00:32:47,040 --> 00:32:50,040
People usually tend to feel uncomfortable if I can very

544
00:32:50,040 --> 00:32:53,040
close them and conversations.

545
00:32:53,040 --> 00:32:56,040
Or if I'm very far, I wouldn't have a conversation but I

546
00:32:56,040 --> 00:32:57,040
would move close efforts.

547
00:32:57,040 --> 00:33:00,040
So we restricted ourself to the circle around all like basically

548
00:33:00,040 --> 00:33:03,040
this donut around people where social interactions typically

549
00:33:03,040 --> 00:33:04,040
happen.

550
00:33:04,040 --> 00:33:07,040
So we had a restricted input space which can be helped.

551
00:33:07,040 --> 00:33:10,040
One thing that we completely misjudged is a mannequin is

552
00:33:10,040 --> 00:33:11,040
always standing still.

553
00:33:11,040 --> 00:33:14,040
If you have a user in virtual reality, putting on a virtual

554
00:33:14,040 --> 00:33:16,040
reality has seen something here and something.

555
00:33:16,040 --> 00:33:19,040
First thing that you do is look around in all the directions.

556
00:33:19,040 --> 00:33:22,040
So we tried to imply our first model and it was all

557
00:33:22,040 --> 00:33:25,040
breaking because this was all movement that we didn't

558
00:33:25,040 --> 00:33:28,040
see and that we didn't account for in this simplified setting.

559
00:33:28,040 --> 00:33:31,040
So we actually moved forward from this donut allowed

560
00:33:31,040 --> 00:33:33,040
near field captures where people get really close to the

561
00:33:33,040 --> 00:33:36,040
mannequin where people move around to talk from the top

562
00:33:36,040 --> 00:33:40,040
and from below into the ears of the mannequin to cover

563
00:33:40,040 --> 00:33:42,040
much denser spatial coverage.

564
00:33:42,040 --> 00:33:45,040
And then in the end, there are other effects that you have

565
00:33:45,040 --> 00:33:48,040
to have to think of not just positions but even speech

566
00:33:48,040 --> 00:33:49,040
direction.

567
00:33:49,040 --> 00:33:53,040
I have a head which is modifying the way I sound.

568
00:33:53,040 --> 00:33:56,040
So if I look in this direction, I sound different to you as

569
00:33:56,040 --> 00:33:58,040
if I look straight to your face.

570
00:33:58,040 --> 00:34:00,040
And these were like effects that we didn't model at all.

571
00:34:00,040 --> 00:34:03,040
Well, yeah, this is neglectable but it is honestly not.

572
00:34:03,040 --> 00:34:06,040
If you want to learn from the raw waveforms and that is such

573
00:34:06,040 --> 00:34:09,040
sensitive data, then this is not neglectable and you really

574
00:34:09,040 --> 00:34:13,040
need all these additional information that you can get.

575
00:34:13,040 --> 00:34:17,040
So talk a little bit about the method or approach

576
00:34:17,040 --> 00:34:20,040
that you came up with for solving the problem.

577
00:34:20,040 --> 00:34:21,040
Yeah, absolutely.

578
00:34:21,040 --> 00:34:22,040
I'd love to.

579
00:34:22,040 --> 00:34:28,040
So the massive advancement in deep learning for audio is

580
00:34:28,040 --> 00:34:32,040
certainly wave net, which was this 2016 paper by DeepMind.

581
00:34:32,040 --> 00:34:36,040
And it has ever since revolutionized text to speech and

582
00:34:36,040 --> 00:34:40,040
vocoding, meaning transferring mouse pectograms of frequency

583
00:34:40,040 --> 00:34:43,040
information into actual waveforms of speech.

584
00:34:43,040 --> 00:34:46,040
So this is the logical starting point whenever you do

585
00:34:46,040 --> 00:34:49,040
something where you have generative audio and you want to

586
00:34:49,040 --> 00:34:50,040
generate a raw waveform.

587
00:34:50,040 --> 00:34:53,040
And that is where we also started from a standard wave net.

588
00:34:53,040 --> 00:34:57,040
And we just used a conditioning on this position information

589
00:34:57,040 --> 00:34:59,040
between transmitter and receiver.

590
00:34:59,040 --> 00:35:00,040
We were hoping that this would do the job.

591
00:35:00,040 --> 00:35:03,040
This wouldn't be like a exciting research inside,

592
00:35:03,040 --> 00:35:05,040
but in terms of getting the job done that would have been

593
00:35:05,040 --> 00:35:06,040
amazing.

594
00:35:06,040 --> 00:35:09,040
But as you can expect, you get a lot of distortions.

595
00:35:09,040 --> 00:35:12,040
You get into a lot of problems for the reasons that

596
00:35:12,040 --> 00:35:15,040
I told you before that, you know, it is hard to match

597
00:35:15,040 --> 00:35:16,040
the face.

598
00:35:16,040 --> 00:35:19,040
Also, this original wave net architecture has been designed

599
00:35:19,040 --> 00:35:21,040
to produce something that is plausible, but not necessarily

600
00:35:21,040 --> 00:35:23,040
matrically accurate.

601
00:35:23,040 --> 00:35:25,040
So we were facing all these problems.

602
00:35:25,040 --> 00:35:27,040
And from there on, extending the architecture.

603
00:35:27,040 --> 00:35:31,040
One of the major components is that, well, if you have

604
00:35:31,040 --> 00:35:34,040
a BINORAL audio, this sound takes some time to travel

605
00:35:34,040 --> 00:35:36,040
from me to you.

606
00:35:36,040 --> 00:35:39,040
And we can clearly geometrically map that.

607
00:35:39,040 --> 00:35:41,040
But this is never correct because what does it mean

608
00:35:41,040 --> 00:35:42,040
geometrically?

609
00:35:42,040 --> 00:35:44,040
If I look in this direction, it travels in a slightly

610
00:35:44,040 --> 00:35:45,040
different way.

611
00:35:45,040 --> 00:35:47,040
If you look away, sound has to travel around your head.

612
00:35:47,040 --> 00:35:50,040
So just going by distances is not enough.

613
00:35:50,040 --> 00:35:53,040
But what we do want to do is having some warping that

614
00:35:53,040 --> 00:35:57,040
tells us, OK, we are at time T when I emit the sound.

615
00:35:57,040 --> 00:36:01,040
What is time T prime when this specific part of sound

616
00:36:01,040 --> 00:36:03,040
arrives at your ears?

617
00:36:03,040 --> 00:36:06,040
And this is what we implemented as a form of neural

618
00:36:06,040 --> 00:36:07,040
time warping.

619
00:36:07,040 --> 00:36:10,040
Basically taking dynamic time warping, this very traditional

620
00:36:10,040 --> 00:36:13,040
approach to align two time sequences, which is not

621
00:36:13,040 --> 00:36:16,040
differentiable and putting it into a differentiable setting

622
00:36:16,040 --> 00:36:19,040
while still maintaining physical properties.

623
00:36:19,040 --> 00:36:22,040
Meaning, sound has to be causal, right?

624
00:36:22,040 --> 00:36:24,040
I mean, if I say something, it cannot arrive at your ears

625
00:36:24,040 --> 00:36:26,040
before I actually said it.

626
00:36:26,040 --> 00:36:27,040
It has to be monotonous.

627
00:36:27,040 --> 00:36:29,040
So what I say first arrives at your ears first.

628
00:36:29,040 --> 00:36:32,040
Unless I move faster than speed of sound, which unfortunately

629
00:36:32,040 --> 00:36:33,040
I can't.

630
00:36:33,040 --> 00:36:35,040
And we have to incorporate these physical properties

631
00:36:35,040 --> 00:36:36,040
into the model.

632
00:36:36,040 --> 00:36:37,040
And this is what we essentially did.

633
00:36:37,040 --> 00:36:40,040
We designed this differentiable version of dynamic

634
00:36:40,040 --> 00:36:43,040
time warping as a neural network layer.

635
00:36:43,040 --> 00:36:46,040
And this enables us to maintain all the physical properties

636
00:36:46,040 --> 00:36:49,040
of sound, but at the same time still aligning

637
00:36:49,040 --> 00:36:51,040
exactly between the transmitter and the receiver.

638
00:36:51,040 --> 00:36:54,040
And from that point, if you have an exact alignment,

639
00:36:54,040 --> 00:36:57,040
standard deep learning approaches, convolutional

640
00:36:57,040 --> 00:37:01,040
time domain convolutional architectures, do a very good job

641
00:37:01,040 --> 00:37:03,040
in changing the amplitudes of the signal.

642
00:37:03,040 --> 00:37:06,040
But the face of the signal has already been accounted for

643
00:37:06,040 --> 00:37:08,040
to a large amount because you already

644
00:37:08,040 --> 00:37:11,040
want everything in the time domain on the right position.

645
00:37:11,040 --> 00:37:14,040
So you solve one of the major problems, one of the major

646
00:37:14,040 --> 00:37:17,040
issues that L2 lost a struggle with doing.

647
00:37:17,040 --> 00:37:21,040
And if you start from there, essentially, was a piece of cake.

648
00:37:21,040 --> 00:37:25,040
And so what assumptions did you need to make to make that

649
00:37:25,040 --> 00:37:28,040
differentiable?

650
00:37:28,040 --> 00:37:31,040
Not too many assumptions, actually, because you can formulate

651
00:37:31,040 --> 00:37:35,040
most of these, most of these components in a quite straightforward

652
00:37:35,040 --> 00:37:36,040
way.

653
00:37:36,040 --> 00:37:39,040
One assumption is clear that you do not move faster than

654
00:37:39,040 --> 00:37:41,040
speed of sound, but for any kind of human.

655
00:37:41,040 --> 00:37:42,040
It's reasonable.

656
00:37:42,040 --> 00:37:44,040
Opposition that is absolutely reasonable.

657
00:37:44,040 --> 00:37:46,040
Absolutely.

658
00:37:46,040 --> 00:37:50,040
One maybe slightly bigger limitation or assumption that

659
00:37:50,040 --> 00:37:53,040
we had to make in this whole architecture and this whole

660
00:37:53,040 --> 00:37:57,040
network is that the acoustic properties of your environment

661
00:37:57,040 --> 00:37:58,040
are the same.

662
00:37:58,040 --> 00:38:01,040
So what we cannot model is changing acoustic properties

663
00:38:01,040 --> 00:38:04,040
because, I don't know, you stand super close to a wall

664
00:38:04,040 --> 00:38:07,040
and you would have different diffractions.

665
00:38:07,040 --> 00:38:10,040
That would require you to really have a prior on rooms and to

666
00:38:10,040 --> 00:38:13,040
really have a good representation of the 3D scene you are in.

667
00:38:13,040 --> 00:38:16,040
So what the model learns is a kind of global representation

668
00:38:16,040 --> 00:38:18,040
of the room acoustics you are in.

669
00:38:18,040 --> 00:38:22,040
But it does not learn that you are standing in the corner of

670
00:38:22,040 --> 00:38:24,040
the room and it sounds significantly different.

671
00:38:24,040 --> 00:38:27,040
This is something we need to solve if we want to bring this

672
00:38:27,040 --> 00:38:30,040
technology into virtual reality and want to have a realistic

673
00:38:30,040 --> 00:38:31,040
impression.

674
00:38:31,040 --> 00:38:34,040
But yeah, in this initial work, this is something we had to

675
00:38:34,040 --> 00:38:37,040
abstract from and really focus on only like the global

676
00:38:37,040 --> 00:38:40,040
appearance of sound and getting the spatialization and

677
00:38:40,040 --> 00:38:42,040
banalization correct.

678
00:38:42,040 --> 00:38:47,040
I'm trying to remember if we've talked about this already

679
00:38:47,040 --> 00:38:52,040
in a slightly different context, but the relationship

680
00:38:52,040 --> 00:38:57,040
between the metrics that you're training on and like

681
00:38:57,040 --> 00:38:59,040
perceptual metrics.

682
00:38:59,040 --> 00:39:02,040
You know, from everything we've said thus far,

683
00:39:02,040 --> 00:39:05,040
you know, you've got a room that's kind of a fixed

684
00:39:05,040 --> 00:39:08,040
size and fixed geometry.

685
00:39:08,040 --> 00:39:12,040
You've got the mannequin ears and you've talked at length

686
00:39:12,040 --> 00:39:16,040
about how sensitive the models are to those specifics.

687
00:39:16,040 --> 00:39:19,040
It seems like you can do very well on your metrics,

688
00:39:19,040 --> 00:39:21,040
but still from a perception perspective.

689
00:39:21,040 --> 00:39:25,040
And maybe this is more generalization than from person

690
00:39:25,040 --> 00:39:29,040
to person than perception, still have issues.

691
00:39:29,040 --> 00:39:31,040
Can you talk a little bit about that?

692
00:39:31,040 --> 00:39:33,040
Yeah, that's a very good question.

693
00:39:33,040 --> 00:39:35,040
We need to make a distinction here where we have good

694
00:39:35,040 --> 00:39:37,040
generalization properties and where we don't.

695
00:39:37,040 --> 00:39:39,040
Because in the setup, you always have a transmitter and

696
00:39:39,040 --> 00:39:40,040
a receiver.

697
00:39:40,040 --> 00:39:42,040
That is fundamentally different from vision where we kind

698
00:39:42,040 --> 00:39:44,040
of assume if you render an image, we all see it in the same

699
00:39:44,040 --> 00:39:45,040
way.

700
00:39:45,040 --> 00:39:47,040
We do not care that maybe your eyes are slightly different

701
00:39:47,040 --> 00:39:48,040
than my eyes.

702
00:39:48,040 --> 00:39:50,040
For audio, we do care because our ears are fundamentally

703
00:39:50,040 --> 00:39:51,040
different.

704
00:39:51,040 --> 00:39:54,040
And we generalize really well on the transmitter side.

705
00:39:54,040 --> 00:39:57,040
Meaning it doesn't really matter which kind of person is

706
00:39:57,040 --> 00:39:58,040
speaking.

707
00:39:58,040 --> 00:40:00,040
We can train on a very small amount of people.

708
00:40:00,040 --> 00:40:01,040
A few dozen is enough.

709
00:40:01,040 --> 00:40:05,040
And generalize to any kind of arbitrary voice and get

710
00:40:05,040 --> 00:40:07,040
an accurate representation and reconstruction of this

711
00:40:07,040 --> 00:40:09,040
arbitrary transmitter voice.

712
00:40:09,040 --> 00:40:12,040
When it comes to the receiver, of course, the generalization

713
00:40:12,040 --> 00:40:15,040
is well, we have one fixed ear pair.

714
00:40:15,040 --> 00:40:18,040
So if you have an extraordinary ear shape,

715
00:40:18,040 --> 00:40:21,040
you might have something that is typically called front back

716
00:40:21,040 --> 00:40:23,040
confusion that you cannot tell if the sound is coming

717
00:40:23,040 --> 00:40:25,040
from the front or from the back of you.

718
00:40:25,040 --> 00:40:28,040
And these are issues we can really only solve if we can

719
00:40:28,040 --> 00:40:30,040
generalize towards ears.

720
00:40:30,040 --> 00:40:33,040
Which again gives us this close connection to codec

721
00:40:33,040 --> 00:40:37,040
avatars because if we do have a realistic representation

722
00:40:37,040 --> 00:40:40,040
of your head and of your ear shape of your, you know,

723
00:40:40,040 --> 00:40:43,040
of your head geometry in the end, what we hope is that we

724
00:40:43,040 --> 00:40:47,040
can personalize the receiver side based on this ear

725
00:40:47,040 --> 00:40:50,040
geometries that we know from codec avatars.

726
00:40:50,040 --> 00:40:54,040
And how about the room geometry?

727
00:40:54,040 --> 00:40:59,040
Does that generalize well across different types of

728
00:40:59,040 --> 00:41:00,040
environments?

729
00:41:00,040 --> 00:41:01,040
Yes.

730
00:41:01,040 --> 00:41:03,040
Quite frankly, no, it does not.

731
00:41:03,040 --> 00:41:07,040
If you record audio in this kind of setting,

732
00:41:07,040 --> 00:41:10,040
or you were a virtual reality headset, your microphone

733
00:41:10,040 --> 00:41:12,040
would be super close to your mouth.

734
00:41:12,040 --> 00:41:15,040
So you do not really pick up a lot of these room acoustics

735
00:41:15,040 --> 00:41:16,040
anyway.

736
00:41:16,040 --> 00:41:18,040
And the signal you have is almost unequally.

737
00:41:18,040 --> 00:41:21,040
It doesn't really have a lot of the information of the room

738
00:41:21,040 --> 00:41:24,040
you are in if it's very close to your mouth anyway.

739
00:41:24,040 --> 00:41:26,040
So from that point of view, that is fine.

740
00:41:26,040 --> 00:41:28,040
The question is, how can we, you know,

741
00:41:28,040 --> 00:41:31,040
transfer the system into different virtual rooms?

742
00:41:31,040 --> 00:41:35,040
At the moment, the way it sounds is the sound of the room

743
00:41:35,040 --> 00:41:37,040
where we collected the data.

744
00:41:37,040 --> 00:41:40,040
This might not necessarily be the only room in virtual reality

745
00:41:40,040 --> 00:41:41,040
you want to be.

746
00:41:41,040 --> 00:41:44,040
And honestly, you probably want to be in a variety of rooms,

747
00:41:44,040 --> 00:41:47,040
which means we have to put some work into disentangling

748
00:41:47,040 --> 00:41:49,040
this part, disentangling the binarialization part

749
00:41:49,040 --> 00:41:51,040
from the room acoustics part.

750
00:41:51,040 --> 00:41:54,040
And one way to go there is to have a better capture setup

751
00:41:54,040 --> 00:41:58,040
where the room where you record all this specialized audio data,

752
00:41:58,040 --> 00:42:01,040
all this binarial audio data is quasi-anicoic.

753
00:42:01,040 --> 00:42:04,040
So basically doesn't really have room responses at all.

754
00:42:04,040 --> 00:42:07,040
And if you have that, and then if you have at the other time,

755
00:42:07,040 --> 00:42:09,040
captures in multiple rooms where you can learn

756
00:42:09,040 --> 00:42:11,040
characteristics of the room responses,

757
00:42:11,040 --> 00:42:13,040
you can disentangle these two effects

758
00:42:13,040 --> 00:42:15,040
and model them sequentially.

759
00:42:15,040 --> 00:42:18,040
But again, in a data driven way, this is very hard

760
00:42:18,040 --> 00:42:19,040
because then the question is again,

761
00:42:19,040 --> 00:42:22,040
how do we get correspondences between the data we capture

762
00:42:22,040 --> 00:42:27,040
in rooms and the data we capture in this binarial capture stage?

763
00:42:27,040 --> 00:42:29,040
It's something we are looking into,

764
00:42:29,040 --> 00:42:33,040
but we do not have a solution to that yet.

765
00:42:33,040 --> 00:42:34,040
Okay.

766
00:42:34,040 --> 00:42:38,040
So we've talked about the setup.

767
00:42:38,040 --> 00:42:40,040
We've talked about the approach.

768
00:42:40,040 --> 00:42:44,040
We've talked about metrics and evaluation a bit.

769
00:42:44,040 --> 00:42:48,040
We've also talked about this big picture

770
00:42:48,040 --> 00:42:51,040
that you're trying to get to.

771
00:42:51,040 --> 00:42:54,040
What are the next steps from where you are

772
00:42:54,040 --> 00:42:56,040
to get you towards the bigger picture?

773
00:42:56,040 --> 00:42:59,040
Where does this research direction take you?

774
00:42:59,040 --> 00:43:01,040
Oh, yeah, absolutely.

775
00:43:01,040 --> 00:43:06,040
As I said initially, my interest is working on audio visual topics

776
00:43:06,040 --> 00:43:09,040
and this binarialization, the sound binarialization paper

777
00:43:09,040 --> 00:43:13,040
is basically an approach to do 3D rendering of sound.

778
00:43:13,040 --> 00:43:16,040
So where's the visual component here, right?

779
00:43:16,040 --> 00:43:19,040
And again, if we talk about codec avatars, encoders,

780
00:43:19,040 --> 00:43:21,040
and decoders, there are these two elements

781
00:43:21,040 --> 00:43:23,040
where we need to fuse the vision in.

782
00:43:23,040 --> 00:43:26,040
On the decoder side very clearly, we talked about ear shapes.

783
00:43:26,040 --> 00:43:28,040
So we need to make sure that the visual information

784
00:43:28,040 --> 00:43:32,040
can influence the way you perceive the sound based on your ear geometry,

785
00:43:32,040 --> 00:43:34,040
based on your hatch geometry.

786
00:43:34,040 --> 00:43:36,040
And on the side of the encoder data,

787
00:43:36,040 --> 00:43:41,040
we talked about how we kind of rely on a quasi-anechoic mono input, right?

788
00:43:41,040 --> 00:43:44,040
But what if you are in a noisy environment?

789
00:43:44,040 --> 00:43:46,040
What if there are kids running around?

790
00:43:46,040 --> 00:43:48,040
We don't want to binarialize that sound, right?

791
00:43:48,040 --> 00:43:50,040
If I'm in VR and your dog, your kids,

792
00:43:50,040 --> 00:43:52,040
whatever is making noise in the background,

793
00:43:52,040 --> 00:43:55,040
it's not present in the virtual environment here.

794
00:43:55,040 --> 00:43:57,040
And so we need to get rid of that.

795
00:43:57,040 --> 00:44:00,040
So that is like another big direction where we have to think about

796
00:44:00,040 --> 00:44:03,040
how can we take the sensory data from the headset,

797
00:44:03,040 --> 00:44:06,040
use the audio sensors, the microphone that we have,

798
00:44:06,040 --> 00:44:08,040
but also use the visual sensors to make sure

799
00:44:08,040 --> 00:44:11,040
that only what you actually say,

800
00:44:11,040 --> 00:44:13,040
only your actual speech is transferred into virtual reality.

801
00:44:13,040 --> 00:44:15,040
And we get rid of all of these other noises

802
00:44:15,040 --> 00:44:17,040
and all of these other background signals

803
00:44:17,040 --> 00:44:20,040
that we are not interested in, you know,

804
00:44:20,040 --> 00:44:22,040
transferring into virtual reality.

805
00:44:22,040 --> 00:44:26,040
Well, Alexander, thanks so much for taking the time to share with us

806
00:44:26,040 --> 00:44:28,040
a bit about what you're up to.

807
00:44:28,040 --> 00:44:30,040
Congrats on the best paper award.

808
00:44:30,040 --> 00:44:33,040
I guess I should ask you, you know, what is your...

809
00:44:33,040 --> 00:44:36,040
What do you think were the factors that

810
00:44:36,040 --> 00:44:43,040
led to the judges picking out this paper for that award?

811
00:44:43,040 --> 00:44:45,040
I mean, the judges are probably the best people

812
00:44:45,040 --> 00:44:48,040
to talk to for that question, but if you ask me,

813
00:44:48,040 --> 00:44:53,040
I believe it's probably this novelty of the field,

814
00:44:53,040 --> 00:44:55,040
you know, moving for audio,

815
00:44:55,040 --> 00:44:57,040
which is still predominantly signal processing

816
00:44:57,040 --> 00:44:59,040
and linear time invariant systems

817
00:44:59,040 --> 00:45:01,040
to what data-driven deep approaches,

818
00:45:01,040 --> 00:45:03,040
having something that is, you know,

819
00:45:03,040 --> 00:45:06,040
a 3D rendering equivalent for audio.

820
00:45:06,040 --> 00:45:08,040
And I think this is just like something

821
00:45:08,040 --> 00:45:09,040
that has not been done before.

822
00:45:09,040 --> 00:45:13,040
And it is probably kind of unclear why audio

823
00:45:13,040 --> 00:45:17,040
is like lagging behind the computer graphics community

824
00:45:17,040 --> 00:45:18,040
in that sense.

825
00:45:18,040 --> 00:45:20,040
And I believe that is probably the big impact of the paper

826
00:45:20,040 --> 00:45:23,040
that we are able to make this step into 3D audio rendering

827
00:45:23,040 --> 00:45:26,040
that is data-driven and deep.

828
00:45:26,040 --> 00:45:28,040
That's awesome. Well, congrats again on that.

829
00:45:28,040 --> 00:45:32,040
I'm looking forward to the follow-on, I think.

830
00:45:32,040 --> 00:45:35,040
A lot of interest in the community now

831
00:45:35,040 --> 00:45:38,040
about kind of multi-modal, multi-channel,

832
00:45:38,040 --> 00:45:41,040
combining audio, video, and other modalities.

833
00:45:41,040 --> 00:45:43,040
Oh, absolutely. Sure.

834
00:45:43,040 --> 00:45:46,040
Yeah, I mean, we are always happy to host interns

835
00:45:46,040 --> 00:45:49,040
and have people over who are interested in this work.

836
00:45:49,040 --> 00:45:53,040
So super happy if someone here is interested

837
00:45:53,040 --> 00:45:55,040
in this direction of audio-visual.

838
00:45:55,040 --> 00:45:57,040
We are always looking for interested.

839
00:45:57,040 --> 00:45:58,040
Mission reach out to you.

840
00:45:58,040 --> 00:46:00,040
Oh, of course.

841
00:46:00,040 --> 00:46:03,040
To our recruiters, whatever you want.

842
00:46:03,040 --> 00:46:04,040
No, it's always welcome.

843
00:46:04,040 --> 00:46:06,040
I feel it's a community that needs to grow.

844
00:46:06,040 --> 00:46:08,040
I feel the community really looking at this multi-modal

845
00:46:08,040 --> 00:46:11,040
problems, this audio vision problems, is too small.

846
00:46:11,040 --> 00:46:13,040
I skimmed over the nearest publications

847
00:46:13,040 --> 00:46:16,040
and there were like nine papers that had sound in that title,

848
00:46:16,040 --> 00:46:18,040
which seems to be too low for a conference

849
00:46:18,040 --> 00:46:19,040
of the size of the nearest.

850
00:46:19,040 --> 00:46:21,040
So I really feel it's a field that needs to grow.

851
00:46:21,040 --> 00:46:23,040
Yeah, absolutely.

852
00:46:23,040 --> 00:46:26,040
Well, once again, Alexander, thank you so much for joining us.

853
00:46:26,040 --> 00:46:30,040
Thank you for having me.

