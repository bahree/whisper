All right, it looks like it is live, and let me just get the turn of volume off there.
There is a delay if you, I don't recommend necessarily that you pull it up, but we'll
be checking here to make sure everything's hunky-dory.
But there is a delay, and it's very confusing if you have the current and the 22nd delay
audio going.
So if you do want to see it in 22nd delay, be sure to mute it.
But we are live.
Everyone, welcome to the first Twinmo Live, where we'll be talking about OpenAI's recent
GPT-2 language model release and announcement and controversy and exploring the host of
issues that it raises.
I am Sam Charrington, your host for the discussion, and for those of you who may have stumbled
across this and are not familiar with this week a machine learning in AI, it is a podcast
that I launched coming up on three years ago that's really dedicated to informing and
educating people about machine learning and artificial intelligence, and I've been
fortunate to have very many wonderful guests, including several of our panelists today.
You can find the podcast easily at twinmolei.com.
So before we dive into our discussion, I'd like to give our panelists an opportunity to
introduce themselves.
Anima, why don't you get us started?
Hi, I'm Anima Anand Kumar, Director of Machine Learning Research at NVIDIA, as well as
a professor at Caltech.
So thank you, Sam, for doing this, I think you're always looking at the pulse of the community
and this is a topic that has garnered a lot of recent interest and thought process.
So thanks for doing this and I'm happy to be part of this.
I do want to make a clarification that these are my personal comments.
I'm a very well coincide with my employers, but I'm not assigned to be speaking on their
behalf in this particular instance.
Awesome, thank you.
Amanda?
Hi, I'm Amanda Askel.
I'm a policy research scientist at OpenAI.
My background is actually in ethics and since working on all areas really still like policy
here.
Awesome.
Rob?
Yeah, my name is Miles Rendage and I'm also on the policy team at OpenAI.
My background is more in social science and tech policy and I have a particular interest
in malicious uses of AI and was involved in a report last year on the topic, so it's
part of my interest here.
Awesome.
Rob?
Hey everyone, I'm Rob Monroe.
I'm a VP of Product at Lilt.
Lilt makes technology that combines human and machine translation.
My background is mixed, I've been a founder and executive of a number of AI startups.
In larger companies, I ran product for AWS's first natural language processing and translation
services.
And before I moved here to the US to get a PhD and then I'll be at Stanford, I was working
in post-conflict development in Sierra Leone and Liberia for the United Nations and I've
continued to work in disaster response, both for Man and Made and Natural Disaster System.
Great.
Great.
And Stephen.
Hi, I'm Stephen Marady, most commonly known on the internet as Smarity and I'm an independent
AI researcher but the primary reason I'm interesting here is that I focus on language modeling
as my research area and I've held steadily out results on some of those same results
as OpenAI's model and two of the data sets that they get steadily out results on are
mine.
Awesome.
Awesome.
So let's dive right in and given your focus on this area, Stephen, you would be a great
person to kind of provide some context for us.
What is a language model and how are they used?
Why are they important and what's the kind of context, the technical context in which
this announcement was made?
Right.
So language modeling for anyone who hasn't run across it yet.
If you have your phone out and you're doing your normal typing, the predictive keyboard
is essentially the time you'll run into your language models immersed.
So the aim is just to guess the next word in your sequence but then you can of course
go for some more complicated steps along rather than guessing words.
You could guess characters or turquins but yeah, the underlying technology is literally
just guess the next turquin in the sequence.
What of the sequence might be?
And so you've most likely run into it with your phone but it's also used in speech recognition
to disambiguate some words so the word recognition itself could be ref the ignition of a cough
theoretically or it could be speech recognition.
So it's used there.
It's also used in a number of other situations in similar contexts or abstract summarization
and so on.
But most recently the kind of really interesting step has been that these incredibly complex
language models if you run them over enough data can basically capture a bunch of sub-tasks
that you don't ask it to capture by just guessing the next word but it might end up learning
that anyway.
So things like counting in some models we never tell them how a model is supposed to count
but it ends up doing that to be able to guess text and you can then take this model and
slot it into a more complex system where it kind of this knowledge that it's already gotten
just to be able to guess the next word in the sequence well ends up transferring to these
other tasks.
So things like sentiment analysis or question answering or translation.
Awesome and so how are there some standard tests it sounds like that are used to kind
of assess the performance of language models.
What are some of these tests?
Yeah so the absolute standard is something called complexity which is basically how confused
is a language model when you tell it what the next token is.
So if you're guessing and you say New York and you're going to guess the next word and
you say city if I said New York state you know maybe I'm not that confused by it because
I was thinking that as well but usually it's city versus if you came up with New York static
or something like that some completely unexpected word then you'll see the complexities spike.
So basically the only aim for language modeling is to minimize how confused the model is
it having seen like a given sequence and so that's kind of the metric that open AI focus
on and kind of all these previous papers have as well.
Okay.
And so Miles and Amanda this work with GPT2 is as the two indicates the second in a series
of research into language models can you talk a little bit about the background of this
project as well as the type of model specifically that it represents namely transformer models.
Yeah so I mean we've been interested in sort of unsupervised learning of useful representations
of text for a while in the sentiment neuron paper or blog post I believe a year or two
ago was an example of sort of early interest at open AI in the first GPT paper as well.
The main difference in terms of you know what GPT2 versus previous transformer based language
models is scale so it's not the biggest language model that's ever been produced but as far
as we know it's the best performing model along various metrics including the quantitative
ones as well as sort of qualitative assessment of the quality of long text production.
And specifically in terms of like size you know the range is you know from millions to
billions of parameters and the one that we have chosen not to release is 1.5 billion.
So there have been bigger language models previously but what's interesting here is that
there's a very diverse data set being used to produce it and make use of this larger capacity
of the model and the GPT sort of model is easier to sample from than say birch or other
sort of recent efforts to sort of push language modeling beyond this scale.
Okay and the specific type of models a transformer model what is that represent?
I think probably Samarity would be a better person to answer that.
Okay so the transformer model many of the audience might have heard of like recurrent neural
network so RNNs and LSTM is that type of thing.
The idea with that would be imagine you could only see one word on a page at a time and
you only had one button which was to go to the next word.
So that's the way the LSTM or recurrent neural network ends up looking at text and trying
to guess the next word.
And the problem with that is I don't know about the rest of the panelists but my memory
is terrible so about 10 words in I'll have forgotten what everything was behind that.
The idea with the transformer network is instead of having this you know step along one at
a time.
You say okay I have 100 words and I'm trying to guess the next word.
The word at the very end can basically talk to all of the words previous to it and try
and pull in some of their knowledge based on whether or not you know I should be essentially
talking to you.
So if I'm about to say if I the last word is president I might look back the last 100
characters 100 words and I might find some other exact instances of president so a good
idea there would be just to grab the next word along or you might do something more complicated
where multiple words kind of have to chat to each other.
And so that's the idea of this attention phase where words can look around at other words
based on how you know interesting they are to the back of the word and you go through
multiple stages of this and hopefully at the end all of kind of the relevant knowledge
from this sequence will be captured in the very word at the end and that word can go
through.
Cool.
I was going to say in Obama or Trump or Nixon or something like that.
Awesome.
Awesome.
So a big part of the controversy I suppose with the release of this model was you know
not so much the research itself and the technical details but kind of the way the model was
released.
Robin wondering if you can maybe provide some context from your perspective just reflecting
on the release and the firestorm that it created at least in Twitter.
I don't want to over amplify it.
Twitter can be a bit of an echo chamber as we all know.
But what's your take on you know some of the things that were maybe controversial about
the announcement?
Yeah.
Happy to.
And Maul's Amanda correct me if I'm characterizing up an AI wrong.
So I believe that open AI decided not to make this model public which is something that's
been standard recently in the research community.
And the reason behind this was because the potential negative use cases outweigh the positive
ones.
So you could get bad actors who could use a model like this to for example generate fake
news which stylistically sounded very much like a real person.
And so it could be used for things like election meddling or generally creating discontent
on the internet both by individual trolls or potentially state sponsored.
So is that is that is that correct of an AI folk?
Yeah.
I mean one thing I'd clarify is that you know we did not claim nor are we confident that
the out that the negative uses of GPT-2 would outweigh the risks but rather that we're
not confident that they wouldn't and that you know this is sort of what seems to us like
a sort of precautionary approach in this context of given the sort of you know your reversibility
of release.
You want to add anything to that?
Yeah.
Now this sounds correct.
So I think it's easy to think that you have to have like really high confidence that
what you're releasing is going to have negative consequences before you decide to at
least do a partial release.
I think our thought was that caution early is a good plan and then to try and get feedback
on this approach so it might be that you know one criticism might be that this is kind
of like too preemptive or too early and I think it's just that the costs of starting
to think about these things early are generally lower than the costs of thinking about them
too late when you are fairly confident that the misuse risk is high and so this was like
some of the kind of reasoning that went behind this and then as Miles said you know deciding
to do a partial release is reversible whereas deciding to do a full release is not reversible
so exercising caution can mean initially doing a partial release and that was what you decided
to do.
Because yeah I guess that's a very thorough distinction not yet knowing rather than being
confident that it was necessarily bad.
Yeah and so this is like certainly a decision process that I've had to be go through many
times in the past.
So in disaster response in Chinabaspring in particular thinking about what kinds of data
were being collected and how for example if you take a tweet of someone reporting a blocked
road they don't know why it's blocked but then all of a sudden you recontextualize that
and your reporting that there are rebel movements in an area and now that the rebels know
that you've reported them.
So if you're very careful about taking other people's data and releasing that and certainly
you know build in a model on open data like overnight I did and then we release in that
accounts as recontextualized as in.
Similarly I've seen very full more propaganda from a cost radio through to social media
used in in Sierra Leone Liberia country studies to live in and I've worked in a lecture
monitoring in during their elections there and even when there weren't state sponsored
actors made decisions like this as well as responding to earthquake in Haiti in 2010
and a centralized data which machine learning scientists can build on.
We deliberately admitted all the data which were reported on a company minus children
who are alone and for this very reason we believed that the potential negative use cases
was something we couldn't protect well for and so this wasn't something that we wanted
to make available.
So I appreciate that decision process.
To characterize that the alchrist I think that there's two aspects to it one at least
me at least I didn't see the context in how this decision was made it felt a little bit
buried in the paper and it probably deserved more space in the article.
Hope I'm preaching to the converted because we've got open AI's ethics people here not
there other machine learning people and I think the other which is less than me to speak
about I call myself a practicing researcher now is that the paper very proudly reported
new state of doubt results for a model that wasn't then immediately available to the research
community to replicate.
Yeah, I can add one point on sort of the norm that you mentioned around openness I think
it's important to sort of be clear that it was not the case that before this everyone
always release all their state of the art models all the time it's rather that it was
rarely or you know it was rarely the case that people would explicitly use sort of misuse
of the model of the sorts and of the source that we're worried about as a justification
as opposed to profit or sort of you know keeping things underrapener to have a big announcement
or something like that so it's the motivation for non-release that I think is distinctive
as opposed to that we didn't publish everything.
Was there a big recent paper in language modeling where they didn't release?
So I'm thinking of the big recent ones kind of an international order being laser from
Facebook, Bert from Google and Elmo from Alan Institute.
I believe they all release their models where other others are not aware of.
Yeah, so I was speaking more about AI generally it might be the case that there's more of a
tendency towards publication in the language model literature specifically.
Right, right.
And so part of the issue that this raises is around reproducibility and I'm sure we'll
come back to that in the conversation.
Anima you've also raised some issues around the kind of the way it's been handled from
just a reporting and media relations perspective.
Can you elaborate on those?
Yeah, I mean, you know, I certainly understand that it's important to think about malicious
use cases and all the impact of releasing a certain technology, right.
So I'm appreciative of the thought process that goes into the risk analysis but as Miles
pointed out that's never been the reason for not releasing a language model before.
So that's, you know, suddenly like where the new risk is coming up that didn't come up
with the earlier models which are nearly as good if not as good.
And also the claim that this is much better than the previous models is not even something
the research community has verified or can easily verify without access to the models.
So there is the issue of like truly understanding what the capabilities of the most recent
model are and having independent researchers evaluate that effectiveness.
And before all that's done, it felt like the media was at the center stage of all this,
right.
The access was only given to journalists and in a very limited way to write articles for
the public.
So before it reached the research community before there was any chance to evaluate its
technical capabilities, you know, there was these huge like kind of media blitz on, you
know, the terminator coming, it's gotten so dangerous that AI needs to be locked up in
a wall to these kind of articles, you know, promoted a lot of fear mongering that's already
been present in some of these general media articles.
And that's where this distortion of the scientific facts and the current capabilities that
I have big issues with.
Okay.
And another issue that's been raised along the lines of the reproducibility concern has
been one about open source and the model's source code being open.
Steven, is that one that you have a daughter?
Yeah, so the idea that the model being open, that's been a pretty popular idea for many
of the AI research labs.
As Miles mentioned, it isn't the default.
There are many kind of papers that don't publish their models or don't publish that occurred.
But you know, one of the main ideas behind kind of this entire field is the open nature
of our work.
We've published the papers on the archive.
There's no payment to get the paper.
The techniques are generally very well learned by everyone involved and there are free toolkits
or frameworks.
And to this stage, you know, Google's collaboratory or what have you, they can give you free
GPUs.
This is a great idea because, you know, basically anyone can get involved.
If someone would like to, they could take open AI's language model or, you know, one
from Google and video and test and see how it works, potentially use it for the application.
So open, reproducible research, it kind of hits all those lines down, whether or not
someone can take the model and improve it, whether they can take it to use it for an
interesting application, whether they can just explore the current capabilities as, you
know, maybe a researcher trying to understand the latest advances.
Yeah.
And at some point, I'll come back to kind of my also perspective on, well, basically
like I love the kind of discussion and the open AI model itself.
I'm, you know, always interested in language modeling research, but I feel like one issue
was that everything kind of came together.
It was a new language model.
It was discussions about responsible disclosure, how journalists react to AI research and
publication and then how the general media consumes it.
I think that was one of the main things kind of powering this confused firestorm on Twitter
and potentially in media.
Yeah, Amanda and Miles, you're shaking your heads.
We agree, we agree.
Thoughts from, from you.
You're muted.
That was my fault.
Yeah.
Like, there's a lot of points being raised.
I think one thing, I just want to go back to a point that was made earlier, where there's
this kind of question of like, well, it's like, you know, we've seen lots of really impressive
language models before, the way that this was presented was like, as if this is some
kind of like changing kind, you know, what's this new risk that's arising that wasn't
arising before?
And I think on this, one thing that's worth noting is just that if machine learning is
an area where you see incremental progress, one concern you might have is the point at
which it makes sense to do something like partial disclosure of research is always going
to kind of look like the wrong point, because it's going to be this sudden shift from
full disclosure of models in the case of like language models to like a partial release,
like what we did here.
But if it's the case that you always just have these like incremental improvements, and
this is like an example of it, there might not be some huge shift.
It might not be that you saw some like completely new potential misuse.
It's just that at some point you have to make the decision.
So I guess I want to say like it doesn't, we weren't intending to imply that it was like
something that it wasn't.
I think we were very explicit in the blog post and in the paper about like the nature
of the research and where it sat in relation to other research, but it's hard to, you
know, convey that well also deciding that you're going to do a slightly different
release type from what's happened before.
Yeah, I mean, we did try and, you know, be clear that we were talking both about, you
know, GPT-2 as well as language models in general, but I think we could have been a lot
clear about sort of what are the threats of GPT-2 raw versus GPT-2 fine-tune versus,
you know, GPT-3 or, you know, someone reproducing it.
So, and like what are the sort of, you know, domains or skill levels required for these
different things?
So this is something that we plan to be a lot more transparent about in the future about
like why would we do this and what are the trade-offs involved and sort of, you know, what
was, what were the options we considered and why did we not do the things that people
are saying, yeah, we should do now.
Nima, you're reacting to this.
I think I'm kind of worried when it's, you know, when Amanda said that, you know, just
because it's incremental, doesn't mean we'll, you know, that at some point we should stop
releasing or only do this partial release, right?
And that's what I'm worried about if the community is moving towards, away from openness
and to close setting just because one day we suddenly feel there is a threat and even
if there is, it's not going to help, right?
Because there's already so much available in the open and it's so easy to, you know,
go look at these ideas and including the blog post and the paper from OpenAI and reproduce
this.
I think it was Stephen who commented on Twitter about the kind of resources it takes for
a bad actor to truly reproduce if they wanted to and it's not a lot, right?
So, you know, so it's not really stopping the bad actors and these malicious use cases
because of this, you know, partial release and holding back this full scale model, but
what it's, who it's truly hurting are the academic researchers.
You know, the students, the junior researchers with the least access to the resources, you
know, the marginalized communities, maybe, you know, people across the world where there
is less compute infrastructure.
I mean, they cannot easily reproduce the resources.
That will, it'll take them a lot more to go and reproduce and then do for the research.
So it's hurting the research community a lot more and almost doing nothing to stop the
malicious use cases, in my view.
I'd like to interject with a question from a user on YouTube, G23, who asks, it would
be possible and this is asking maybe a bit of a theoretical, but would it be possible
to establish some kind of partnership program so that researchers kind of vetted researchers
could get access to this to this work without fully making it open.
Is that something that OpenAI has considered?
Yeah, so absolutely, and you know, there's an email address on the blog, the original
blog post where you can suggest both, you know, specific use cases that you're interested
in as well as ideas around, you know, alleviating these trade-offs in terms of access versus
limiting this use.
To comment briefly on Anima's point around openness, I totally agree there are tons of benefits
of openness and it's been, you know, the benefits of openness, if anything, have become
more acute to us through this process because it's frustrating, you know, making claims
that people are saying, you're unsubstantiated and trying to sort of, you know, persuade
people that, you know, we're not making this up that there are actually these capabilities.
So it's super, you know, frustrating to have to deal with that trade-off and, you know,
that we, it's quite possible that we made the wrong choice that, you know, we should
have been thinking more, I mean, we did think, to some extent, we should have been thinking
more about sort of the low, you know, low compute, you know, asserts of actors, you know,
people in developing countries and so forth who, you know, could only get access to this
through a pre-train model.
But it's, I think, it's also quite plausible and I hope it's the case we might the right
call and having a sort of reading period to have this conversation and start thinking
more critically about defenses and coordination around these topics will actually be an
up benefit.
I mean, I think we're thinking a lot about these considerations, so things like are there
ways that we can give access to this kind of work to academics who want to work on it,
for example, also are there ways of interacting like across industry or like, you know, people
suggested a kind of partnership? We're interested in exploring all of those ideas, and I think
one of the purposes of sort of starting a conversation here was to get a lot of that out on the table
and not to say something like, oh, we want to just take action on our own and decide to close things
off. The goal was really to start a conversation around this and get feedback. Not to say something
like, yes, we're just like, we think that we can like prevent misuse by simply closing up research
or something like that. That's like not the intention. So just like earlier points that people
read. Yeah, one of the things that it confused me a little bit about the conversation that as I
was following it on Twitter was there seemed to be, and it came up in this conversation as well,
some suggestion that the models themselves weren't particularly novel, and I guess part of the
issues that we can't really know, but Jeremy Howard, for example, seemed to suggest that the models
were, you know, the code is out there to do what OpenAI did. They just did it at a scale that
no one has done before. I wonder if anyone has a take on that. I suddenly do it if I can jump in.
Much of the model is really quite the same as OpenAI's previous release of GBT,
and the main thing I kind of refer to is scale it till it breaks models, where you just take
an existing model and you ask that, you know, interesting question, because this is something
you can ask with machine learning, if you just keep scaling the model up and keep throwing in more
data, does the behavior of the model change substantially? And so that's really the question that
the OpenAI team were asking, not necessarily like, you know, we have a new WhizBang model underneath
the surface. But because of that, that also does, you know, raise some interesting questions,
along with the fact that OpenAI released the code immediately, because in terms of kind of
responsible disclosure for this, anyone can kind of reproduce the research, either with their
existing code or with previously released code. And I think Anima mentioned, I kind of crunched
numbers. It's about $43,000, but it's suddenly not cheap, but for, you know, a state actor or
someone else like that, or a substantially large company, it's suddenly within, you know, their
ability to do so. But the model itself hasn't really strongly changed. And so it's more a question
of, I guess, capabilities when you scale models up to this size. Rob, how about you, any thoughts
on that? Yeah, I mean, I actually like to go back to the question of limited release as well.
So most research institutions have some form of IRB, all in the U.S. too, so ethical review
boards. Who do exactly this? And so when I was doing my PhD, some of the data I looked at
was health care messages in the general language of Malawi. And because they contained PII,
I had to go through a review process, both here in the USA and then also in Malawi to get approval
to use this data and, you know, promise to treat it carefully, delete it when I know long
it needed it. So a lot of these processes are already in place. People in other scientific
disciplines, especially biological and medical ones, routinely have to go through this process.
And yeah, I think that already exists for a lot of AI researchers. And that kind of takes me
into the point that I wanted to make because, yeah, to Steven's point, these models continually get
better with more and more data. But we don't have more and more data for most of the world's languages.
So I think the way that OpenAI differ from all the other language models that have been released
recently is that it really only looked at English. It did some look at novel translation
between English and French. But when you look at Bert, you know, a month ago, I went for that
Facebook. It's a few weeks ago. Bert and laser from Facebook had a hundred different languages.
So English, you know, it's only constitutes 5% of the world's conversations daily. It's the
most privileged language in the world. And it's the language for which it's most easy for us to
fight fake news right now because we have AI that can identify fake versus real news. We have
teams of people at the different social media companies doing this. And so for me, rather than fake
news or killer robots or other things that your employer might be worried about, OpenAI, it's
inclusion in AI, which I think is the biggest ethical problem that we're facing right now.
And if these models are any working at a scale that we have for English, then even the software
component, the algorithms don't matter. They're not going to be able to be used for 99% of the world's
languages. So I'm really curious that, you know, if this model kind of model won't even work for
the majority of the world's languages, where if I didn't fake news is the hardest right now,
because that data simply doesn't exist. Why, why, why are we particularly concerned then about
the OpenAI English-only model compared to others? Miles, you have a thought on that?
Yeah, I mean, yeah, so in terms of the sort of biased question and
representiveness around the language, I think that it's definitely something we've considered
in addition to other sort of more subtle or non-obvious risks. And you know, certainly we
forward grounded the malicious use risks of sort of people deliberately using this, but that's
also something we need to consider in terms of, you know, what is the consequence of releasing
these models and, you know, sort of bias around sex and race is another thing that we've considered
as a reason for caution. So, you know, that's not to say that we wouldn't, if we had, if we were,
if we had no other concerns besides, you know, the English, you know, bias, would we so release it?
I don't know, that's an interesting question. I think one of these, I think one of these
correlates, too. So, obviously, like, language and race correlate strongly, but in some cases,
the more closely intertwined. So, your race through a lot of Latin America is determined more
by the language, you speak, than your actual biological ethnicity. And there is a huge
gender bias there, too. So, in a lot of the world, you're more likely to have more education
and be taught a privileged language if you're raised male than you are female. So, these really
do correlate strongly with each other. And also, not to single you out. Even though these other
models have been released in more languages, that's missing from their valuations as well.
So, I think their, the first model to really get a lot of publicity in the machine learning
community was the alumni model out of the Allen Institute. So, they wanted best paper award,
I think two years ago, maybe last year, at one of the big competition linguistics conferences.
They've evaluated NDD recognition, so identifying the names of people, places, and locations.
They evaluated a multilingual data set, which was both English and German,
but you feel they're paper, then you have one set of results. And then you say this,
you just have to infer that they know all the German data and only evaluated the English.
And then the birth paper at a Google did exactly the same. They reported a new set of
the art on this data set, which is called a multilingual data set, but reported only to English
results. And English and German are basically as close to related as any two languages can be.
They're like from the same language family, they're a lot of cognates.
Enough played around with the Elmo model, and it doesn't get into a near state of the art
for this German data set. Bird gets a little bit better, but again, not state of the art.
And so I worry a little bit then, you know, to what extent have those researchers, or ones at OpenAI,
given that the imperative of always having state of the art, have they tried this in other languages,
maybe something as close related to English as German, they didn't get state of the art results,
and as a result, they brushed it under the carpet rather than sharing a really important negative result.
Steven, what do you have some thoughts on the language issue?
Yeah, so one of the kind of proof of concepts that's in the OpenAI paper, but we've also seen
similar strands of research across the community, is kind of twofold. One is that unsupervised
language models substantially help translation. That's kind of an obvious. But in this situation,
the OpenAI team were actually purposely stripping out than just retaining just English.
And for one of those reasons is that the data sets they were comparing against primarily English.
But they ended up kind of accidentally leaving, I think it was 10 megabytes or so of
French in there. And these were kind of like, I wish I knew more French, but like Bonjour,
means hello in English, like as a sentence. And the language model, it does,
a reason I mentioned this as a proof of concept, they obviously tried to strip out as much
of the languages they could, but it ended up with some remaining in there. But even from that small
amount, translating from French to English did reasonably well. And there's reasons for that,
you know, the language model itself has just been learning what English looks like. And so from
even a few examples of French, I can say, well, frequently these go across. But the next stage up,
which is kind of the broader community, there are many efforts to have unsupervised translation
between languages. And I think you made reference Rob to laser beforehand. And the beautiful thing
about this is that by helping, say, translate from English to German, which are very similar,
but have, you know, at least a few, I guess, rules in terms of changing around the orders of things,
or, you know, different ways in which words combine. You can take those same kind of learnings
for this model and transfer it to a very resource-low language, and still have that transition across.
Now, it is a completely fair point that it hasn't worked. The opening I team for their language model,
here hasn't applied it to, you know, further languages. But one thing which I kind of personally
have some, this is almost unrelated experience to me, I released a language model sometime back
called the AWDLSTM. The fast.ai team took it and then have it as a kind of underlying basis,
now it's been immensely modified, the underlying basis originally for the language model ULM fit.
And the fast.ai community have then ported this to dozens of different languages. And kind of
the really fun thing for me is I was mainly focused on English. I should probably expand my language
modeling vocabulary, even if I don't know the languages myself. But the code that I wrote because
it was general and your machine learning does this, transfer very well across these other languages.
And I'm aware of at least seeing that the transform model has been able to do this quite successfully
in the past. So I'd expect, naively, the opening I model to have the same sorts of advantages.
That past, definitely true. So the transform model has been a lot more successful across
languages than the R&N LSTM based methods. And then it actually comes down to the reason that
Stephen introduced initially. And that R&N LSTM based model is really only looking at one word at
a time, enough to pass that memory all the way through, rather than being smarter about finding
long distance relationships. And so English is a complete outlier in terms of how important word
order is. It's more common in most languages, that the subjective verb in the object,
the subjective verb by the suffixes or the prefixes. They go in those words and the words can be
in any order. And so this is, one of the things that is a little bit problematic about a lot of
these results is that testing only on English, which is it's not in the middle, it's an outlier,
how important word order is and then standardized bellings are and then lack of suffixes.
It doesn't really tell you about how it's going to do more broadly.
It's certainly the machine translation community and that's what I'm addressing my company right
now. The transformer based methods are really blowing the R&N based methods out of the water.
But that's not so clear in a lot of the language models recently, even if they've been released
in multiple languages, only been evaluated in English. And so I think it's representing a real gap
in the knowledge that people like me and industry can take from the research community.
So a question I've got for Anima is really about kind of the true capabilities of these
types of models. I think looking at the sample that was released in the blog post,
with this model, you can provide a prompt and the output is conditioned on this prompt. And so
the prompt was something about scientists discover unicorns and there's a rather long and
rather coherent text about the backstory of the scientific discovery. It was rather impressive to
me. Are you equally impressed? Where do you think this fits in the broad scheme of
capability of these types of models? I think that particular example,
I think you could argue that it furthers the whole AI boogie man terminator thing. It's
particularly unexpected or at least I found it particularly unexpected. You know, do you think
and we can we'll ask open AI as well like and I think actually to be fair in the blog post,
I said this was I think they said this was an example that was selected out of 10 or something
with that prompt. But to what extent are these types of examples cherry picked? You know,
what does it say about kind of where we are in this, you know, the path towards, you know,
some AI outcome that we don't fear? The thing that we're talking about here that we fear and that
we're kind of not disclosing because we fear, you know, how close are we? And you know, and a
short answer is, right, I can't really tell without knowing all the details about the model,
right, and having to the model. I mean, any researcher would I think comment that as like a
one-line answer, right? But more, I guess importantly, the issue is like, as you said, not one
example. Like, you know, the question of like not just like how well it's doing on some cherry
picked examples, but also what the failure modes were. Like, what did the others look like?
Or was it completely incoherent or was it like diverse enough? Was it doing the same thing
over and over again? I mean, these are all questions we ask for when we try to evaluate the models,
right? I mean, we can look at quantitative measures like for complexity, but that's, you know,
like not enough by itself, right? Like, you know, so that's, I mean, there is no easy way to
evaluate unsupervised learning, right? That's a general philosophical question. Like, what does it mean
to have done unsupervised learning well? Because with supervised learning, we have a notion of accuracy,
okay, you get 100% accuracy on your unseen test dataset, then you're, you know, really amazing,
right? But even there are the limitations because you may want to go beyond the test dataset and
their further issues. Whereas with unsupervised learning, the question is, what is a good model?
Like, you know, when, when do we say that this, you know, the answer that you obtained that you're
happy with? Was it because that it was coherent enough? Or did you want it to have certain
factual reasoning? Did you want it to go through a certain logical set of steps?
Right? What would mean it to be impressive? Right? One from a human evaluation perspective,
that other from a more quantitative perspective, it is hard to tell and that's why this is
an open research topic that the community thinks a lot about on how to evaluate language models.
And that's the reason we need the research community to be very much embedded in discussing
this model and getting access to it. Miles, a reaction of that? Yeah, I mean, I totally agree that
it's much easier to evaluate their capabilities with more access and it's a very acute trade-off
that we're trying to navigate. I mean, one point that Anima raised was that, and you know,
very much on point, is that it was not obvious what the specific sort of threats we were concerned
about were and, you know, and more generally where to draw those lines. So, and you know, this is
still something we're thinking through and planning to share more about our process around sort of
threat modeling and evaluating these capabilities, but just to get some intuition. You know, first of all,
you know, this is not just about GPT-2, but also language models in general. So, all of this should
be sort of taken with the grain of salt that we don't know exactly where the biggest threats are
and how quickly things will develop from here. But roughly, we have, you know, a few sort of
tiers of, you know, sources of information that we draw on. We look at how things are being used
in the wild, like what is actually the situation with fake news and, you know, where the bottlenecks in
terms of text reduction and so forth. So, that's one set of perspectives. It's like, what is the
role of, you know, text in society and what are the defenses against mass produced or misleading
text. And then there's sort of in-house analysis, you know, through, you know, both sort of
formally doing science as well as informally, you know, allowing people access to the model,
including non-experts within the opening eye organization. So, that's where some of the
samples came from for the blog posts were sort of non-expert users playing out with an interactive
version of the system as opposed to, you know, like Alec Radford and Jeff, we were trying to come up with
the most impressive possible example. So, but it's, I, we agree that, you know, from the text,
it's not obvious that that's the case. Yeah. So, in terms of threat modeling, you know, it's
important to think about, you know, what can we do in-house? What can we do with a given level of skill
as well as, you know, what would fine-tune variations of the system? You know, we gave the example
of Amazon Reviews as one example where we've looked at fine-tuning and we're able to realize that
it was quite tractable, but we're still thinking through, you know, what is the sort of suite of,
you know, questions you should ask about powerful language models.
Awesome. So, Anima, a question for you from Connor on YouTube, maybe pushing back a little bit,
do you see any limits with respect to the types of models or with respect to releasing models?
Are there any societal considerations that an AI scientist should make in creating
or responsibilities that they bear in releasing their models? Where would you draw the line?
I mean, certainly, I think every scientist should think about societal impacts, right?
In, you know, in any discipline, I think we should all be mindful of the impact we have
through the deployment of technologies we release. But at the same time, we need to ask ourselves,
if I'm limiting a certain technology, what are the trade-offs? Like, you know, there's both the
benefits and the risks in releasing a technology and we need to do that trade-off. And in the machine
learning field, which has been very open until now, most of it is in the open, right? And even if
it's not, it doesn't take a whole lot of resources to get to those capabilities. And so locking
it up seems counterproductive to me at this stage, especially in the context of language models
and similar research where so much of it, very similar frameworks, you know, the open source code,
they're all available. It doesn't take a lot to reproduce that by bad actors. On the other hand,
it can limit access to people in the marginalized communities, people with low, you know,
with limited access to resources. So that's why I see in this setting the equation to be more
tilted towards release. Okay. So a question or response from Miles or Amanda, which of you is that?
Oh, yeah. In this case, I think one thing that's just worth noting is like, we are like very
sensitive to these issues. Like, I agree that it's important that there's like equity among researchers
and that one of the downsides of like not releasing anything, even just doing a partial release,
is that researchers don't have access to it. You have issues like, you know, it makes it harder
to replicate or at least there's a delay in replication. I think it would be interesting to me,
I guess I have two thoughts. One is that we might not even disagree about roughly the weight
of all of the considerations here because I think our position was one of kind of caution
where it's just like, the question isn't something like, do you think this is the exact right moment
to do a partial release? But rather something like, are you like basically how certain are you that
isn't or how certain are you or how confident are you that you're actually on the right side of
this scale? And I think that's like the kind of questions that we're asking. And in part,
also I think the thing that this really highlights, you know, when we start bringing up these pros
and cons is the need for like greater discussion of this in the ML community. So in some ways, when
there isn't a kind of framework or there isn't a kind of agreed upon set of norms, so there isn't
a partnership on this. Each actor is having to kind of think about these issues themselves.
And for our part, we relate, well, that means that you have to take on a lot of caution. And it
is important to be cautious, even if you're considering all of the negative consequences of that.
So I think it's great that this discussion is happening, but it's also worth noting that this could
be made easier if we did potentially have some of these mechanisms to like really help people think
through when to release watch release and the pros and cons. Yeah, and how to raise risks in a way
that isn't seen or isn't actually, you know, alarmist. I mean, you know, so I think one perspective
that I think, you know, is worth considering is that the AI community does not have all the answers,
and it's not the only one that needs to know what the technology's coming down in the pipeline are.
And you know, that was sort of where we were coming from with the outreach to journalists,
prior to the launch, and it's possible that, you know, we could have done more, you know,
increase the ratio of researcher, you know, outreach to, you know, non-researcher outreach,
but, you know, the basic ideas that this is not, you know, just an open AI thing or not an AI
community thing, but, you know, a more general question of sort of how do we handle these powerful
technologies that seem to be coming, even if they're not, you know, totally there. Yeah.
Can you can you comment on the the approach you took to evaluating the kind of the ethical field
in the release, the level of kind of rigor or detail, did you identify, you know, did you have
a specific, you know, kind of persona or threat that you are most concerned about, whether it's
one that's been stated or unstated, or where you kind of reacting, responding, or anticipating
just the broad threat. How, you know, how did you kind of pursue this?
Yeah, so I think, you know, there are a couple of different lenses, and we're still not
sure what the right lens is. There's sort of, you know, GPT-2 itself, and, you know, there's
a fine-tuned version. We, you know, part of where we were coming from was seeing what the zero-shot
version of GPT-2 was, and that sort of gave us some heuristic evidence that even more powerful
capabilities would be possible with fine-tuning or larger models. So, you know, some of it was sort
of, you know, anecdotal experiences of people interacting with the model, like Alec and Jeff
just sort of seeing impressive things, and sort of, you know, a growing number of people interacting
with the model, seeing how easy it was to get, you know, human-ish-looking outputs, not necessarily,
you know, semantically accurate or factually correct to, you know, one of the points that Anima
raised earlier, you know, what is the relevant threshold? We thought that it was, you know,
we still don't know exactly what the right threshold is, but something about sort of human passable,
human-ish-text across a very wide range of demands for very little human-input.
This is, this is testable. You know, there's obviously a lot of fake news out there right now,
and typical bad actor does this in a really simple way. It's, you know, like templates,
we get to drop in certain words and it generates variations of those sentences. Just, you know,
like a hundred lines of code, but it's powerful. You have like ten sentence variants and ten
sentences. You can create billions of different unique paragraphs. And so, you know, it's testable
to create a system like that today, and then have humans say, you know, which is, you know,
which is the more likely to be real. And so that's, and that's standard, right? Like how am I
compared to the state of the R? And the paper had this. So the paper had a bunch of benchmarks
against existing technologies to show they're better than researchers. But the ethical component
or at least everything I've seen so far has been purely qualitative. And so as the ethics people
in open AI, do you think you could convince that the scientists to drop one academic benchmark
and run some new studies which would really demonstrate what the negative impact might be compared
to what's already out there? I mean, I think we're kind of heading in that direction in terms of
quantifying the risks of models and things like bias, you know, in the context of language is
something that we're starting to be more aware of. And I could, you know, I could imagine, you know,
sort of like misuse potential, you know, label for, you know, different sizes of models or something.
But I think we're still in a more sort of pre-conceptual framework phase in the sense that like we,
we know a few considerations, we know a few sort of specific threats, we have some ideas for
how to evaluate them, but ultimately, you know, we don't, we don't have in-house experts on,
you know, everything related to fake news, everything related to cybersecurity, et cetera. So this
is very much, you know, a conversation that we welcome input on. So specific ideas for testing the
model, specific ideas for sort of threat modeling are super welcome.
I think it's a little interesting that Westine have this discussion about text, which at least
I might have an optimistic view, but I feel like the technology is still getting there in terms
of the potential, you know, strongest misuse possibilities. And then, you know, a lot of the time
misinformation online isn't going to be about writing a long-form article. It seems reasonable.
It's about writing a very short snippet that is terrible in a bunch of ways and spreading it
everywhere. But this type of discussion, you know, we as a community should have really started
properly having like, we shouldn't be caught in the dark by this as even a question,
because, you know, deepfakes has been substantially, you know, to many people quite destructive
to their lives. And for the same sort of things that powered that, the code that we released,
the pre-trained models, which have been released, which were then built upon.
Like, I feel like we as a community should have had a better response to that. Like, that should
have been a awakening moment. And this should be a potential like later, okay, you know, now we can
stop considering text modeling through this lens before it's time. I don't know if anyone else
has strong thoughts on that. Do we know where the right forum is for that conversation? I'm not
sure that it's Twitter. I mean, Twitter is a part of it, but, you know, is it standard bodies?
Is it kind of for at the conferences like Nourips and others? Or is it some, you know, structure that,
is it, you know, regulatory? Is it some structure that hasn't been created? Are there things that we can
learn from other, you know, technologies that have both beneficial uses and potential for weaponization
that, you know, what have they done? I mean, I think I would be careful in using terms like
weaponization, right? Because a lot of, you know, like discussions on, especially on Twitter,
as you said, it's not the best medium, like tends to like devolve all the way into a nuclear weapons.
And, you know, those, you know, it's cases of malicious use, right? And this is where there is a
lot of, you know, what we saw that I was most disappointed about the whole episode was the media
distortion in, you know, wide reaching to the public in so many different countries. The message was,
this is so dangerous that this is now locked up, right? That's what the public took away.
And that's disappointing because I think that's a severe distortion from these much more nuanced
conversation we are having today, right? This is what we need to be doing. We need to have the
dialogue within the community and also with the public. I think there is still a big gap of what
the think of as AI or even intelligence, you know, they are not able to truly evaluate
how intelligent or how capable the current AI systems are. And I think that's a severe deficiency
if we cannot close that gap between the research community and the general public. And that's
what worries me the most that this conversation, you know, we need to present it in a much more
balanced form to the public and to the media. I also want to mention like one thing that I
always think in my mind, if we think way back when Facebook released a paper that was essentially
about two robots bothering. And somehow like I was in Australia and I heard on like the nightly
news, Facebook AI has like, you know, taken over something they had to shut down the experiment.
It's so dangerous they had to shut it down. Exactly. But that's a thing, right? Like we have very
few chances of reaching this global audience and we need to be very careful about what ends up stuck
in their heads. Because we have the potential to put the right piece of information in someone's
head. So I know, you know, maybe to be wary about receiving in the future, receiving that email
from grandma, which seems entirely syntactically correct, but she's asking about Bitcoin and has
an account already to feed it, right? Like maybe that's the one piece of information we can
somehow impose a worry about in the future. But you know, in the case of the Facebook AI story,
you know, I feel like the information that has been locked in people's head is really quite wrong.
And, you know, Facebook's, they took the, you know, they took it on themselves. They read a new
blog purse that explained just how it got distorted within the media. But I can, you know, imagine
very strongly that people who saw that television station news segment in Australia are almost never
going to read that blog purse. And that's I think what I'm more worried about fake news about AI
than AI generating the fake news. So maybe to start to wrap things up, miles in Amanda with the
benefit of hindsight, you know, how much you approach this differently? Well, yeah, I'll give Amanda
a second to think about that. And while I give you a comment, which is that in terms of venues for
keeping the conversation going, we'll be hosting and dinner at I clear for where both policy people
and technical people from opening I will be happy to discuss them talk about the future of language
models, generally, not just to PT too. Yeah, so I think that like ideally being able to have a kind
of wide range of inputs from the ML community prior to like making a decision like this is going
to be very helpful. In the case of like media attention, there's a sense in which it's like a
little bit harder to kind of navigate or control because like the way that something is going to be
told like as we've kind of heard is like like a little bit more delicate, although I really sympathize
with this, you know, like if we could do the not necessarily just how you do things differently,
but like like how you wish things would go, you know, we were quite positive in the blog post as
well, like we talked about all of the positive ways that this could be used. And obviously like in
general, like the thing that gets reported is quite negative. And I think that's like really
unfortunate because like, you know, people who are doing ML research are doing it because they want
to see, you know, excellent uses of it in the world. And you know, I think that it wasn't our intention
to say something like this research is all bad and you should be afraid of it. Rather just like,
hey, we need to start having a conversation about this. So I think yeah, I would like to see more
of that. And I think that it's correct that essentially what I would like to see more of is the
kind of nuance discussion that we're having here, where we're sensitive to all of the pros and
cons both of like doing open research and like the potential misuse of that research. So yeah,
I think like creating forums where people can do that is like the thing that I would really like
to see going forward. Yeah, I mean, I agree with all that. And just in general, both open AI and
the rest of the AI community needs to find ways to smooth this conversation out over time. So it
doesn't happen all of it, you know, in one, you know, Twitter storm and sort of, you know, find
whether it's sort of, you know, recurring workshops at conferences or whatever to sort of institutional
conferences. And how about the rest of you, Steven, what, what would you like to see done differently
and what do you hope to see grow out of this this scenario? Yeah, well, I mean, you know, I love
language modeling. It's always exciting for me when I'm about. So it's a good thing for me. What
I'm going to be interested in is, you know, at some point when I can play with this model,
play with other models, you almost like one of the biggest issues we haven't filled is, you know,
urban fitting. You find ways that these machine learning models learn to cheat in subtle and
strange ways. And one of the, you know, craziest examples of that is in visual question answering.
So you give the machine learning model an image and then you ask it a question to answer.
For some time, these visual question answering systems did worse than just looking at the question
without ever looking at the image. And the field didn't quite realize it just because they didn't
run the right experiments. So that's the thing that I think I would enjoy playing with. There's
the modern version of that for text, which is for a question answering data sets called squad.
You know, many people are really excited about how intelligent it was, how many of the questions
it got correct. But then a group went through and kind of methodically looked at each instance and
was like, oh, with a, you know, a few dozen lines of code and red jerks' regular expressions
basically ways to capture some patterns. You can answer all of these. So it's a question of,
okay, this language model is obviously quite good. But exactly how good is it? And, you know,
what interesting, strange methods of cheating might it be using that's able to trick all of us
at a glance? Rob? Yeah. So I wrote what I said earlier. And then to speak to Anima's point about
making sure the right message gets out there, I felt like for me, the release wasn't most offensive
in terms of what wasn't and what wasn't public. I think the biggest shortcoming was that it was
only an image that it was not diverse, that it was the most privileged language, which correlate
to almost every other privileged demographic. And that to me was the bigger ethical concern than
anything to do with what actually got released in terms of the language model. Anima? Yeah. I mean,
I think, you know, putting more thought process into, you know, collaborating with academia and
research community in general, right? And making sure that especially the, you know, researchers with
less compute resources are not at this advantage. That's something, you know, non-profit like
OpenAI would have a very big role to play, right? To remove the barriers and to truly democratize AI.
I think thinking also on that angle while quantifying the risks and coming up with a more
quantitative analysis of risks and also maybe incentive mechanisms like how to better
deploy AI and better release it to the community. These are all things we can, you know,
for the research and come up with some best practices for the community and also best practices
in terms of how we talk about this to the general media and how this gets reported. That's something
we as a community need to work more about. Yeah. And, you know, I love the that you're hosting the
dinner at. I clear, but I'd also love to see, you know, OpenAI kind of roll up its leaves and
help figure out where the right place is to have this conversation more formally. And, you know,
who the right people are to bring to the table a number of the folks that I've commented on YouTube
and Twitter about, you know, security researchers have dealt with this kind of issue for a long time.
How do we get them into the fold? You know, folks have been doing threat modeling. How do we get them
into the folds? There's a lot of work that has to happen to, you know, create, you know, that space
and use it to further the conversation. And, yeah, I'd love to see more happening there. But for
tonight, I'm glad to be part of the, you know, getting this beyond 280 characters a pop.
And thank all of you for taking the time to jump on and talk about this really important issue.
Thank you. Thanks a lot for doing this. All right. Good night, everyone.
Good night, good night. Thanks, everyone, for joining via YouTube live.
Yeah. And before we go, actually, we'll put it in the description when we post the video,
but for folks who aren't following all of you on Twitter, why don't we do a quick
Twitter handle a roll call?
Um, I'm sorry. Rob, uh, WWE Rob World Wide Rob. Smarity is Smarity. Smarity.
Miles underscore Brundage. Yeah, Miles underscore Brundage. I'm just Amanda Askell.
Oh, one word. Awesome. Awesome. Well, thanks. Thanks again, everyone. Good night.
All right. Bye. Thanks.
