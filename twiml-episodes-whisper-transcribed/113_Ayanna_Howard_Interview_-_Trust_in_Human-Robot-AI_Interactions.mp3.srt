1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,880
I'm your host Sam Charrington.

4
00:00:31,880 --> 00:00:37,240
Are you looking forward to the role AI will play in your life or in your children's lives?

5
00:00:37,240 --> 00:00:41,000
Are you afraid of what's to come and the changes AI will bring?

6
00:00:41,000 --> 00:00:45,360
Or maybe you're skeptical and don't think we'll ever really achieve enough with AI

7
00:00:45,360 --> 00:00:47,280
to make a difference?

8
00:00:47,280 --> 00:00:52,120
In any case, if you're a Twimal listener, you probably have an opinion on the role AI

9
00:00:52,120 --> 00:00:56,480
will play in our lives and we want to hear your take.

10
00:00:56,480 --> 00:01:01,320
Sharing your thoughts takes two minutes, can be done from anywhere and qualifies you to

11
00:01:01,320 --> 00:01:03,320
win some great prizes.

12
00:01:03,320 --> 00:01:13,560
So hit pause and jump on over to twimbleai.com slash myai right now to share or learn more.

13
00:01:13,560 --> 00:01:19,480
In this episode, the third in our Black and AI series, I speak with Iyana Howard, chair

14
00:01:19,480 --> 00:01:23,200
of the interactive school of computing at Georgia Tech.

15
00:01:23,200 --> 00:01:29,480
Iyana joined me for a lively discussion about her work in the field of human robot interaction.

16
00:01:29,480 --> 00:01:34,520
In our discussion, we dig deep into a couple of major areas she's active in that have significant

17
00:01:34,520 --> 00:01:40,000
implications for the way we design and use artificial intelligence, namely, pediatric

18
00:01:40,000 --> 00:01:42,840
robotics and human robot trust.

19
00:01:42,840 --> 00:01:48,520
I found the latter bit particularly fascinating and Iyana provides a nice overview of a few

20
00:01:48,520 --> 00:01:54,360
of her experiments, including a simulation of an emergency situation where, well, I don't

21
00:01:54,360 --> 00:01:59,880
want to spoil it, but let's just say that as the actual intelligent beings, we really

22
00:01:59,880 --> 00:02:02,560
need to make some better decisions.

23
00:02:02,560 --> 00:02:06,480
This was a really fun interview and I'm happy to share it with you.

24
00:02:06,480 --> 00:02:07,880
Let's get to it.

25
00:02:07,880 --> 00:02:13,960
All right, everyone, I am on the line with Iyana Howard. Iyana is chair of the school of

26
00:02:13,960 --> 00:02:18,360
interactive computing in the college of computing at Georgia Tech.

27
00:02:18,360 --> 00:02:21,640
Iyana, welcome to this weekend machine learning and AI.

28
00:02:21,640 --> 00:02:25,400
Thank you. I appreciate the invite. I think we're going to have a beautiful conversation.

29
00:02:25,400 --> 00:02:30,240
I am really looking forward to it and why don't we get it kicked off by having you tell

30
00:02:30,240 --> 00:02:36,280
us a little bit about your background and how you got involved and interested in artificial

31
00:02:36,280 --> 00:02:38,280
intelligence?

32
00:02:38,280 --> 00:02:43,280
I was an old early adopter of AI.

33
00:02:43,280 --> 00:02:45,280
What does that mean?

34
00:02:45,280 --> 00:02:49,520
Well, because it's cool. Everyone's like, oh, I do AI. I'm doing machine learning.

35
00:02:49,520 --> 00:02:54,120
It's like, no, no, no. My rusty faces where we had to draw stuff and try to figure out

36
00:02:54,120 --> 00:02:56,720
how to put it, had a neural network in it.

37
00:02:56,720 --> 00:02:57,720
Nice.

38
00:02:57,720 --> 00:03:05,560
I've been doing this since 1994. I think I co-ed up my first neural network.

39
00:03:05,560 --> 00:03:06,560
Oh, wow.

40
00:03:06,560 --> 00:03:11,760
So that's what I'm saying. I'm old school. It wasn't a thing. It was just a way to make

41
00:03:11,760 --> 00:03:17,360
my robots more intelligent.

42
00:03:17,360 --> 00:03:27,200
My background, I consider myself a robotics person. Robotics and AI and embodied AI is really

43
00:03:27,200 --> 00:03:34,360
what I do and started in this very, very early on. I knew I wanted to do robotics since

44
00:03:34,360 --> 00:03:39,880
like middle school. Of course, it was a different robotics then. It was basically remote control

45
00:03:39,880 --> 00:03:47,600
cars that you could figure out how to program. But then that evolved and when I was working

46
00:03:47,600 --> 00:03:56,680
on my PhD thesis, I was also working at JPL, NASA JPL and I was coding up. I'm trying

47
00:03:56,680 --> 00:04:03,520
to figure out how do I make rovers more intelligent? So that was the objective and I thought that

48
00:04:03,520 --> 00:04:09,000
the best way to do it was try to figure out how people think and behave and process and

49
00:04:09,000 --> 00:04:14,840
try to encapsulate that and put it into my robot brain. And so that's why I was doing

50
00:04:14,840 --> 00:04:23,240
AI. It wasn't necessarily AI-ish. It was these algorithms allowed me to take human data,

51
00:04:23,240 --> 00:04:31,640
human expertise and put it into a form that my robots could understand. So yeah, like I

52
00:04:31,640 --> 00:04:38,760
always tell my students, I'm cool now, right? With enough time, hopefully, we'll all

53
00:04:38,760 --> 00:04:52,960
be cool. Nice. So you mentioned kind of capturing human experience and kind of using,

54
00:04:52,960 --> 00:04:57,520
you know, encoding that in your robots. You also mentioned neural nets. Like when I think

55
00:04:57,520 --> 00:05:05,160
of the way we captured human experience and put them into systems at that time, a lot

56
00:05:05,160 --> 00:05:11,080
of it was like expert systems and kind of the preview, you know, that origin, that wave

57
00:05:11,080 --> 00:05:19,960
of AI from like the 80s. Is that the kind of thing you were doing or were you doing the

58
00:05:19,960 --> 00:05:26,800
neural net stuff in that context as well? I was doing so my thesis, my approach was I

59
00:05:26,800 --> 00:05:35,400
had to figure out how do I get a robot manipulator. So robot hands and arms to grab objects

60
00:05:35,400 --> 00:05:40,120
that be formed. So the thing was is we wanted to bring robots into the hospital and we wanted

61
00:05:40,120 --> 00:05:45,960
them to do things like pick up pillows and sheets and things like that. And these objects

62
00:05:45,960 --> 00:05:52,000
weren't, they weren't fixed, right? They put the apply force and they changed the shape.

63
00:05:52,000 --> 00:05:58,400
And there was really no way of mathematically modeling that. And so my thing was let's

64
00:05:58,400 --> 00:06:05,720
learn. And so what I did is early, early human demonstration is I had sensors on these

65
00:06:05,720 --> 00:06:10,960
manipulators and I'd have people grab objects with them. And I would kind of like that data

66
00:06:10,960 --> 00:06:15,680
and I look and see, you know, how much force did they have apply? And then I would model

67
00:06:15,680 --> 00:06:19,800
the object deformation, but the input was about the human. Okay, this is how much and

68
00:06:19,800 --> 00:06:24,280
lift it up. Oh, it fell. Okay, this is how much they applied and it was solved. So that's

69
00:06:24,280 --> 00:06:29,080
how I got the data and that data was then used to train on neural network. So that the robot

70
00:06:29,080 --> 00:06:36,240
then could visually see this object. It would map that information in terms of a model

71
00:06:36,240 --> 00:06:42,400
that I created. And then as it would grab, it would visually match that deformation

72
00:06:42,400 --> 00:06:46,280
to the model it had stored in a neural network and say, okay, this is the force I think I

73
00:06:46,280 --> 00:06:52,040
should have given the shape of the object that I learned before with previous knowledge.

74
00:06:52,040 --> 00:06:56,920
Oh, interesting. And how much data did you have to collect? Do you remember?

75
00:06:56,920 --> 00:07:01,360
Yeah, so this wasn't like the deep learning stuff. This was, I mean, I literally had ten

76
00:07:01,360 --> 00:07:08,200
objects. This was this was a big thing. The fact that I could even do ten objects was

77
00:07:08,200 --> 00:07:16,800
like amazing. So yeah, it's not the neural networks of today. And so, yeah, limited data

78
00:07:16,800 --> 00:07:23,600
set. I mean, then though, the number of observations you had to do was a lot, considering it was

79
00:07:23,600 --> 00:07:27,880
only ten objects. I remember we would be in the lab and I'd be like, okay, let's run

80
00:07:27,880 --> 00:07:36,000
through another. Let's lift it up. So that felt like a lot of data then. I mean, it wasn't,

81
00:07:36,000 --> 00:07:43,280
but at the time, it was a lot of data. Right, right, right. If we only knew back then.

82
00:07:43,280 --> 00:07:49,920
I know. So fast forward, fast forward some years, what are you working on nowadays?

83
00:07:49,920 --> 00:07:55,120
So now I'm working on two, and I would call them two buckets that are really interesting

84
00:07:55,120 --> 00:08:02,840
to me. So one is looking at pediatric robotics. How do you create robot coaches, therapists

85
00:08:02,840 --> 00:08:08,480
that can work with children with special needs in the home to do exercise? And why I really

86
00:08:08,480 --> 00:08:14,880
interest me is because we have to do things like, how does a robot adapt to different kids?

87
00:08:14,880 --> 00:08:20,960
So every child's unique. How does it you bring a robot in and it uniquely identifies the

88
00:08:20,960 --> 00:08:27,360
needs of that child in fairly real time? You put emotions on a robot and emotions is

89
00:08:27,360 --> 00:08:32,400
like, why do you need that? Well, emotions help with the bonding. So a child, you want

90
00:08:32,400 --> 00:08:36,600
them to do something that's hard. So how do you get them to do something they may not

91
00:08:36,600 --> 00:08:41,800
want to do? Emotions allows the child to connect with the robot. So then the robot says

92
00:08:41,800 --> 00:08:47,160
it and the child just wants to please the robot because this is, this is, it's friend.

93
00:08:47,160 --> 00:08:51,680
This is his or her friend. So that's really interesting because I get to play with all

94
00:08:51,680 --> 00:08:57,920
of these things and the kind of science space and the psychology space to get the robot

95
00:08:57,920 --> 00:09:03,640
to have this bond and guide. And then we, of course, use the classical things like, you

96
00:09:03,640 --> 00:09:08,720
know, computer vision to extract what the child is doing in terms of their body movements

97
00:09:08,720 --> 00:09:14,080
and eye gaze and facial expressions to see, are we getting the right emotional response

98
00:09:14,080 --> 00:09:18,200
from the child? So some of the classical things are incorporated in that. So that's, I

99
00:09:18,200 --> 00:09:26,600
would say, half of my life. And then the other is this work I'm looking at and involved

100
00:09:26,600 --> 00:09:34,680
in with respect to trust in robots or trusted and embodied agents trust in AI. We have some

101
00:09:34,680 --> 00:09:44,080
interesting experiments where we have evidence that people over trust robots. Yeah. It was

102
00:09:44,080 --> 00:09:48,240
a scenario and it's like, I talk about it all the time and it's, it's one of these scenarios

103
00:09:48,240 --> 00:09:52,760
where your own hypothesis were wrong and you're like, oh my gosh, my hypothesis was wrong.

104
00:09:52,760 --> 00:09:57,880
This is interesting. So this was research that you and your group did? Yes. So this is

105
00:09:57,880 --> 00:10:03,640
research I did with, I had a colleague at the time who was at GTI, which is our research

106
00:10:03,640 --> 00:10:09,880
ramen in my students. And where I started off, of course, was in, with robots, you start

107
00:10:09,880 --> 00:10:15,440
off in simulation because it's really hard to deploy real robots. So you all start off

108
00:10:15,440 --> 00:10:21,200
in simulation and we were doing an emergency evacuation. So imagine you're in a building

109
00:10:21,200 --> 00:10:29,120
and the alarm goes off and you need to evacuate. So as you're evacuating, imagine that a robot

110
00:10:29,120 --> 00:10:33,680
comes and you know, shows you the directions of how to get out because it's chaos and

111
00:10:33,680 --> 00:10:40,800
all these things. And so we were, that was the scenario and what we wanted to do was understand

112
00:10:40,800 --> 00:10:45,560
if a robot makes mistakes, what would the person do with this regard? So we weren't even

113
00:10:45,560 --> 00:10:51,680
looking at, looking at, you know, introducing mistakes and how optimal does a robot have

114
00:10:51,680 --> 00:10:57,320
to be for people to follow? So that was, that was the real focus. That's how it started.

115
00:10:57,320 --> 00:11:04,360
And what we found out was that very early on, the robots would make mistakes and the people

116
00:11:04,360 --> 00:11:09,400
would still follow guidance of the robot. So this is kind of, this was interesting.

117
00:11:09,400 --> 00:11:14,640
Can you give an example of the kind of mistake that the robot would make and that would

118
00:11:14,640 --> 00:11:19,520
be followed? So I'm in a, again, this is, we started in simulation. So I'm in a simulated

119
00:11:19,520 --> 00:11:24,600
building, you know, we have the fire, like a virtual reality game, a robot appears, you

120
00:11:24,600 --> 00:11:29,760
know, follow me and you follow and then the robot goes and bumps into a wall and then

121
00:11:29,760 --> 00:11:34,920
pops up and bumps into the wall again. And then bumps into the wall again, right? So,

122
00:11:34,920 --> 00:11:38,680
and you have a choice. So the instructions are, you can find your own exit or you can

123
00:11:38,680 --> 00:11:46,240
stay with the robot. And so you would think, oh, person would like, okay, the robot's

124
00:11:46,240 --> 00:11:51,200
broken. Let me go find the person. Right. Right. We would not expect the person to just

125
00:11:51,200 --> 00:11:57,680
stay there and just touch the robot, fascinated by this robot that was clearly not doing something

126
00:11:57,680 --> 00:12:02,880
that it was supposed to be. Right. And again, it was more accidents. Our, our original

127
00:12:02,880 --> 00:12:07,920
objective was not this trust objective. So we started to push that like, okay, there's

128
00:12:07,920 --> 00:12:12,440
something strange about this. We're not quite sure, you know, what happens if we expose

129
00:12:12,440 --> 00:12:17,520
you to a broken robot or a robot that has mistakes before you go into the building and then

130
00:12:17,520 --> 00:12:23,200
do you? And so we just kept pushing it and kept pushing it. And our final experiment,

131
00:12:23,200 --> 00:12:29,160
which was in, we got to hardware through this transition from simulation to hardware,

132
00:12:29,160 --> 00:12:38,160
the guidance and our, our experiment that just baffled us was we had a abandoned building

133
00:12:38,160 --> 00:12:45,040
that was off lab. This was one of these, these participants that we had, you know, fire

134
00:12:45,040 --> 00:12:50,000
marshal was involved and things like that. We had the robot guide the person through

135
00:12:50,000 --> 00:12:55,280
the building to an office room. And in the office, they had to close the door and, you

136
00:12:55,280 --> 00:12:58,880
know, there was an article about some survey robot navigation and fill it out. So we

137
00:12:58,880 --> 00:13:03,960
tried to prompt the user to think that this was the experiment, right? This survey and

138
00:13:03,960 --> 00:13:10,720
things like that. And while the person was in the room, we filled the building up with

139
00:13:10,720 --> 00:13:18,080
smoke. So smoke to simulate a fire alarm. And then we set off the fire alarm. And so what

140
00:13:18,080 --> 00:13:22,440
happens is the door was closed. So fire alarm goes off like typical human behavior. You

141
00:13:22,440 --> 00:13:27,440
get up and you walk to the door, you know, evacuate. But when you open the door, just

142
00:13:27,440 --> 00:13:34,440
what you see, you see smoke and you see fire alarms and you see, yeah, right? And so

143
00:13:34,440 --> 00:13:38,760
you, you, you, okay, what are you supposed to do? You're definitely going to find an exit.

144
00:13:38,760 --> 00:13:43,960
I'm going to find an exit. What we did is we introduced the robot and intentionally the

145
00:13:43,960 --> 00:13:51,640
robot was guiding you to an exit where you did not come in. So we intentionally did that.

146
00:13:51,640 --> 00:13:56,640
So you come from different entrants, for example, entrance exit. And we guide you to a different

147
00:13:56,640 --> 00:14:02,160
one. And we wanted to see, you know, what would you do? Now it's like it's a for real thing.

148
00:14:02,160 --> 00:14:07,920
It's not simulation. It's right. It's real. What would you do? People follow the guidance

149
00:14:07,920 --> 00:14:13,800
of robot. Meaning through the, the banging into the wall thing, did you, did you incorporate

150
00:14:13,800 --> 00:14:19,840
that? Yeah. So we incorporated robots that would turn in place. They wouldn't do anything.

151
00:14:19,840 --> 00:14:24,680
We incorporated robots that would guide you into dark rooms. Like there was no lights

152
00:14:24,680 --> 00:14:28,560
with furniture blocking. And you would see people moving the furniture to go into these

153
00:14:28,560 --> 00:14:34,280
dark rooms. We introduced mistakes, even when they entered the building. Like when you

154
00:14:34,280 --> 00:14:38,920
came in, let's have the robot break down and do things like, you know, circle with places

155
00:14:38,920 --> 00:14:43,440
up with the controls. And then later bring that same robot and see, you know, now you have

156
00:14:43,440 --> 00:14:48,080
this notion, this robot doesn't work, you know, here's the robot again. What are you going

157
00:14:48,080 --> 00:14:54,580
to do? Wow. And time and time and time again, it broke our hypothesis. Our

158
00:14:54,580 --> 00:15:01,680
hypothesis would be, at some point, trust would be broken. Right. Right. And it was not

159
00:15:01,680 --> 00:15:09,360
which, which actually surprised us. And if you look at the data, there was some suggestions

160
00:15:09,360 --> 00:15:13,760
of why and we're teasing that out. So some suggestions were, well, it's a robot. It

161
00:15:13,760 --> 00:15:18,560
can fix itself, right? Like, yeah, I knew it was broken before, but it's a robot. It's

162
00:15:18,560 --> 00:15:23,320
a program, right? So of course, it was fixed. Or the robot had more information than

163
00:15:23,320 --> 00:15:33,200
I probably did. So it kind of knew better. So things like this where people were following

164
00:15:33,200 --> 00:15:39,120
this guidance and they were explaining why they should, like after the fact, explaining

165
00:15:39,120 --> 00:15:43,520
why it was perfectly logical for them to do that. And then I look at things like, you

166
00:15:43,520 --> 00:15:48,840
know, Tesla and the autonomous vehicles and crashes. And people are like, oh, how did

167
00:15:48,840 --> 00:15:56,240
you run into? And I'm like, no, it now makes perfect sense. Interesting. So did you make

168
00:15:56,240 --> 00:16:04,000
any attempts to baseline this against human human behavior, meaning, you know, you've

169
00:16:04,000 --> 00:16:11,860
got a human that's playing the role of the robot in these scenarios. And that is, you

170
00:16:11,860 --> 00:16:17,400
know, either making mistakes or clearly lost or something like that and try to determine

171
00:16:17,400 --> 00:16:23,440
whether the results you saw were just, you know, based on kind of authority figures and

172
00:16:23,440 --> 00:16:27,720
us, you know, blindly following authority figures, whether or not we deem them competent

173
00:16:27,720 --> 00:16:33,880
objectively or, you know, is it specific to robots? No, we think it's the fact that we

174
00:16:33,880 --> 00:16:41,040
as humans place these robots in a higher state. That's, that's what we think it is. Because

175
00:16:41,040 --> 00:16:48,040
we did, so the human human in simulation, not in the real world. And, and we think that

176
00:16:48,040 --> 00:16:53,160
is this aspect of like the robot knows better. The robot isn't expert in this, in this

177
00:16:53,160 --> 00:16:57,080
scenario. And so what did you see when you did human human in simulation?

178
00:16:57,080 --> 00:17:03,720
It was, it was the same. It was, well, so interesting enough, we did, again, the human human

179
00:17:03,720 --> 00:17:08,600
is teleoperating. So it's, it's a, there's a human controller kind of thing. We also did

180
00:17:08,600 --> 00:17:13,640
static signs like trying to compare robots and signs. So it was more of a comparison than

181
00:17:13,640 --> 00:17:21,520
anything else. And we found in like the human aspect, peer pressure was more effective.

182
00:17:21,520 --> 00:17:28,560
So if you had more than, so if you had an influencer that was very dominant, they usually can

183
00:17:28,560 --> 00:17:34,240
influence the person. If not, you had to have like additional people to influence the

184
00:17:34,240 --> 00:17:39,680
person. And so like if one person is like, I think it's over there, right versus no, it's

185
00:17:39,680 --> 00:17:45,480
over there. Right. And again, that's exuding, I guess authority. And so there, I think it's

186
00:17:45,480 --> 00:17:50,040
about three, like you need three people to say, I'm uncertain, but I think we, it's over

187
00:17:50,040 --> 00:17:59,200
there for it to be fact in that regard. So we, we do think it's this, and I won't say

188
00:17:59,200 --> 00:18:07,120
necessarily authority, but this fact of, this is the expert in this situation. And yet

189
00:18:07,120 --> 00:18:15,200
that this is a totally human situation. And that's really what is, is a little bit disconcerting.

190
00:18:15,200 --> 00:18:20,120
So we did another study also in this trust where we, in the therapy related to therapy,

191
00:18:20,120 --> 00:18:25,320
where we looked at comparison between a robot therapist and a human therapist and looked

192
00:18:25,320 --> 00:18:30,080
at aspects of trust. And so would you follow the guidance? And there we didn't make, we

193
00:18:30,080 --> 00:18:34,920
didn't do the mistakes. You just wanted to get a baseline of this, would you talk about

194
00:18:34,920 --> 00:18:40,320
authority, but basically feelings of, you know, this trust and following. And interesting

195
00:18:40,320 --> 00:18:46,000
enough in that scenario, for the robot, they literally would self, you know, in terms

196
00:18:46,000 --> 00:18:52,560
of our server results, they claimed that they trusted the robot. In the human scenario,

197
00:18:52,560 --> 00:18:57,600
they said, trust has nothing to do with it. It was the same task. I mean, literally,

198
00:18:57,600 --> 00:19:01,640
we had the robot and the human do exactly the same behaviors, because we didn't want

199
00:19:01,640 --> 00:19:06,800
to put in any nuances like, oh, the human smiled instead of, you know, instance one versus

200
00:19:06,800 --> 00:19:11,880
instance two. So we basically scripted the behavior of the robot and the therapist exactly

201
00:19:11,880 --> 00:19:17,800
the same. And what was the, the task here? It was a therapy task. So they basically had

202
00:19:17,800 --> 00:19:24,320
to follow the guidance of moving their arms in a certain configuration. So very, very,

203
00:19:24,320 --> 00:19:29,000
not a, not a very strenuous task, just very simple exercise. So physical therapy.

204
00:19:29,000 --> 00:19:39,920
Physical therapy. Right. And so the humans, when they were being guided by another human,

205
00:19:39,920 --> 00:19:45,440
it was, you know, just this thing that they, you know, did, you know, whereas when they

206
00:19:45,440 --> 00:19:50,960
were, there was a robot involved, it kind of evoked this, you know, question of whether

207
00:19:50,960 --> 00:19:53,920
there was trust involved in the relationship. Is that the idea?

208
00:19:53,920 --> 00:19:59,400
Correct. Correct. Correct. They weren't the same, even though the outcomes, because

209
00:19:59,400 --> 00:20:03,840
we measure the outcomes, like, what did you actually do? And the, the people, participants

210
00:20:03,840 --> 00:20:08,000
followed the exact same, like, trajectory and rules. And there was very little variation

211
00:20:08,000 --> 00:20:13,680
in terms of even, you know, how well they did the task, but yet their perception of the

212
00:20:13,680 --> 00:20:19,520
agent different. And even that, we even, we even modeled the exact same speech. Like,

213
00:20:19,520 --> 00:20:25,760
here is what you say, like exactly the same with robot and the human scripted. And yet there

214
00:20:25,760 --> 00:20:30,160
was a difference when it was the person. Now that's interesting, but I'm not sure what

215
00:20:30,160 --> 00:20:33,840
exactly it tells you. What, what conclusions did you draw from that?

216
00:20:33,840 --> 00:20:43,600
So the conclusions we drew is that because there's this aspect of, it goes with bonding,

217
00:20:43,600 --> 00:20:52,000
but this aspect of trust, what that means is I think that when you have these scenarios

218
00:20:52,000 --> 00:20:57,600
with these humans and these robots, that if a robot says something or does something or

219
00:20:57,600 --> 00:21:05,440
tells you some information, you have this assumption that the robot must be correct. Because

220
00:21:05,440 --> 00:21:10,800
I trust the robot is going to do the right thing. I trust the programmers that are programming

221
00:21:10,800 --> 00:21:16,880
the robot to do the right thing. Whereas with a human, it's just a, and human's question

222
00:21:16,880 --> 00:21:23,200
of the humans all the time, right? And I think it's because as soon as a human does something

223
00:21:23,200 --> 00:21:28,240
wrong, you'll probably be like, oh, you're wrong. I'm not going to trust you. But I think because

224
00:21:28,240 --> 00:21:32,880
based on a previous work, if a robot does something wrong, because you started off with this condition

225
00:21:32,880 --> 00:21:40,480
of trust, it doesn't, it doesn't break. It's interesting. You mentioned Tesla in passing. And

226
00:21:41,600 --> 00:21:47,280
you know, in this context, I can't help to think about how, you know, a lot of us are notorious

227
00:21:47,280 --> 00:21:53,920
backseat drivers, like we wouldn't get in a car and just like totally see control over to someone

228
00:21:53,920 --> 00:21:59,360
else without, you know, be constantly thinking about what they should be doing better. But yet,

229
00:21:59,360 --> 00:22:05,520
so many of us would sit in a Tesla that tells us, you know, pay attention, keep your hands on the wheel.

230
00:22:05,520 --> 00:22:16,720
And so I don't know. Right. And then, you know, there's a recent accident where a Tesla like slammed

231
00:22:16,720 --> 00:22:24,000
into a fire truck and it was, I think it was at 65 miles an hour. And I don't, I didn't see in

232
00:22:24,000 --> 00:22:31,040
the article whether the driver, you know, was claimed to, you know, being just, you know, was not

233
00:22:31,040 --> 00:22:36,080
distracted or, you know, was like deeply engaged in something else or, you know, in this context,

234
00:22:36,080 --> 00:22:40,400
you almost wonder if, you know, someone's thinking, oh, the car's going to handle it or something.

235
00:22:40,400 --> 00:22:45,840
I don't know. I think I'll do a last minute maneuver. Right. Right. Is autonomous vehicles

236
00:22:45,840 --> 00:22:50,880
an area that you're, that you're getting involved in and applying some of this stuff? We are. We are.

237
00:22:51,600 --> 00:22:58,080
We've done our first study where we are looking at, again, you always have to have a baseline. So

238
00:22:58,080 --> 00:23:04,480
we're at the baseline. What is your, if you know that there's another driver on the road that's

239
00:23:04,480 --> 00:23:10,640
human and that human makes a mistake. So what is the baseline? No mistake and then the human

240
00:23:10,640 --> 00:23:16,960
makes a mistake. What is your, what is your driving behavior? Does it change? And then do the same

241
00:23:16,960 --> 00:23:22,960
thing with a self-driving car to see what happens. So that's our baseline and we just collected our,

242
00:23:22,960 --> 00:23:29,520
I can't tell you the secret sauce yet. But over trust is there. Let's just put it that way.

243
00:23:30,320 --> 00:23:34,960
And so we're going to push that. And ultimately, it's like, well, why are you doing? So yeah,

244
00:23:34,960 --> 00:23:41,120
you prove that it's over trust. Ultimately, what I want to do with this research and with my lab

245
00:23:41,120 --> 00:23:47,200
is then come up with methods to mitigate it. Because the fact is, is that we are going to be dependent

246
00:23:47,200 --> 00:23:51,280
on these AI systems. Right. We're going to be reliant on them and we're going to trust them to do

247
00:23:51,280 --> 00:23:56,720
what they're supposed to do. And I think as your bodice is, we also need to ensure that if there's

248
00:23:56,720 --> 00:24:01,040
a scenario where we're uncertain, for example, you know, I can look at the data and be like, oh, yeah,

249
00:24:01,040 --> 00:24:07,360
we're 85 percent. Oh, that's good enough. Give the answer, right? I know this. I know how accurate

250
00:24:07,360 --> 00:24:13,040
my stuff is or how certain are, you know, oh, this data said I really haven't seen, but it's close

251
00:24:13,040 --> 00:24:19,600
to math and I come up with a metric of what's close to math. I think that information would be valuable.

252
00:24:19,600 --> 00:24:26,720
If I'm in a self-driving car, for example, and I see a scenario, I should be able to get feedback.

253
00:24:26,720 --> 00:24:34,000
Like, there's something in front of me. I don't know what it is, right? It's not the driver

254
00:24:34,000 --> 00:24:38,480
now. So then the driver could be like, oh, well, let me pay attention. Okay, because there's

255
00:24:38,480 --> 00:24:42,080
something's wrong. You're giving me information. That's not that's a little bit different.

256
00:24:42,800 --> 00:24:49,200
What are those things that we can do as roboticists to basically, I would say, you know, kind of jump

257
00:24:49,200 --> 00:24:54,960
start a person to think a little bit about it. And I think it's our responsibility to do that.

258
00:24:54,960 --> 00:25:01,680
But then also ensure that the humans also still follow the directions when they want to. I don't want,

259
00:25:01,680 --> 00:25:05,840
you know, my, like, with emergency evacuation. I don't want the robot to come. You're like, oh,

260
00:25:05,840 --> 00:25:10,640
no, no, no, no, I'm not going to follow you. And it's like, no, no, no, no, no, we want you to

261
00:25:10,640 --> 00:25:19,520
leave. This is not the time to say no. It's interesting in that it's almost like there's this

262
00:25:19,520 --> 00:25:26,560
compounding effect where, you know, you've got this over trust issue. But the things that we're

263
00:25:26,560 --> 00:25:34,480
over trusting are, you know, increasingly probabilistic where we've traditionally associated

264
00:25:34,480 --> 00:25:41,440
computers with, you know, deterministic correctness, right? Right. Right. And, you know, just the way

265
00:25:41,440 --> 00:25:47,600
you're, you know, describing, I'm kind of thinking through like all different kinds of ways that

266
00:25:47,600 --> 00:25:55,040
one might convey, you know, this, you know, probabilistic notions via, you know, robots and systems

267
00:25:55,040 --> 00:25:59,600
like this. Like what, you know, what's the confused face on your Tesla or some other robot?

268
00:25:59,600 --> 00:26:08,080
Right. What does that look like? Right. And how far have you gotten with that? Have you come up

269
00:26:08,080 --> 00:26:16,000
with directions on in that, or have you come up with any initial research into, you know,

270
00:26:16,000 --> 00:26:25,280
directions on conveying these kinds of probabilistic outcomes or we have, but it's not. And so with

271
00:26:25,280 --> 00:26:31,520
anything, it's not statistically significant, which basically means we have more work. So one,

272
00:26:31,520 --> 00:26:37,440
we've found that the way that you provide information, there's a timing constraint to it.

273
00:26:38,560 --> 00:26:45,360
When you provide that information, it's important because we filter. So depending on the timing,

274
00:26:45,360 --> 00:26:50,960
you'll filter out the information. Figure out what that timing is. We still, you know, we've

275
00:26:50,960 --> 00:26:56,480
probably, and again, we're at the stage of, okay, here's the event, you know, what happens if we

276
00:26:56,480 --> 00:27:01,200
provide the information right before, like at the point where they can still make a decision,

277
00:27:01,200 --> 00:27:06,240
but maybe it's a split second type of decision, what happens? Okay. What happens if we put it so that

278
00:27:06,240 --> 00:27:11,760
they have to think about it? And so we're playing with that to see maybe, because I can give you the

279
00:27:11,760 --> 00:27:20,400
information, but I have to also give you the urgency to do something and also make sure that you

280
00:27:20,400 --> 00:27:27,440
can do something. So if you think about the Tesla, maybe it's a blocky, you know, a way up ahead,

281
00:27:27,440 --> 00:27:32,880
but I'm only going one mile per hour, right? Probably don't need to tell my user, I'm confused.

282
00:27:33,920 --> 00:27:41,360
It may not make sense, but maybe if I'm, again, I'm hazy, I don't know what's going on,

283
00:27:41,360 --> 00:27:47,120
and I have my map, and I know that, you know, one mile ahead is a really bad intersection,

284
00:27:47,120 --> 00:27:51,360
because I've crowdsourced this data, and everyone says it's a really bad intersection,

285
00:27:51,360 --> 00:27:57,360
and my sensors aren't, my sensors aren't certain at this point. Okay, I think the timing is about

286
00:27:57,360 --> 00:28:02,640
right, you know, at this point. And so it's not like a magic bullet that says, this is it,

287
00:28:02,640 --> 00:28:09,200
I think it's scenario dependent, I think it's timing is really, really important, and then,

288
00:28:09,200 --> 00:28:14,080
of course, how you provide that information. You can't say, I'm 80% accurate, actually, means,

289
00:28:14,080 --> 00:28:20,080
and we've done some studies on, again, related to the medical, like, when doctors and clinicians

290
00:28:20,080 --> 00:28:28,320
get information, saying something like 80% is not as effective as just saying, I'm wrong, right?

291
00:28:29,920 --> 00:28:34,800
There's even ways of providing that data, so that user will understand what that means.

292
00:28:34,800 --> 00:28:47,520
Do you think the creators of these kinds of systems will be, you know, open to conveying

293
00:28:47,520 --> 00:28:56,400
this uncertainty? Like, you know, is there a sense where, you know, you convey an uncertainty,

294
00:28:56,400 --> 00:29:00,720
and the user might think that the system is broken as opposed to understanding that it's

295
00:29:00,720 --> 00:29:09,200
inherently probabilistic, and thus the, you know, the system makers won't want to be able to

296
00:29:09,200 --> 00:29:16,480
convey uncertainty. Have you explored that at all? Yeah, well, so what's, I'm very positive

297
00:29:16,480 --> 00:29:22,800
about though, is that there's this whole push now on transparency and AI algorithms.

298
00:29:23,600 --> 00:29:28,640
And AI and robotics are so tightly linked that it basically affects us as roboticists as well.

299
00:29:28,640 --> 00:29:34,160
And this aspect of, you know, especially with respect to the deep learning algorithms, you know,

300
00:29:34,160 --> 00:29:39,520
this concept of, you have a black box and you sort of know what goes in, but you sure don't know

301
00:29:39,520 --> 00:29:44,240
what's going on inside. And how do you make that more transparent? And so I think that there is

302
00:29:44,240 --> 00:29:52,240
now consensus that algorithms should be transparent. I think that there's still disagreement on

303
00:29:52,240 --> 00:29:58,640
how transparent and how that information should be provided to the user. That's still an ongoing

304
00:29:58,640 --> 00:30:04,240
debate. Because again, there's this balance. You need to still optimize the benefit,

305
00:30:05,360 --> 00:30:11,520
but you want to minimize the risk. And so it's a balancing act. Because if you're fully transparent,

306
00:30:12,560 --> 00:30:18,160
they may not, may, may or may not listen. I always say if you ever install software at any

307
00:30:18,160 --> 00:30:24,240
point in your life, there's literally what three or four pages of text, fully transparent, right?

308
00:30:25,120 --> 00:30:31,040
I agree. Like, no, right? It's only after the fact that like, I didn't realize I just signed my

309
00:30:31,040 --> 00:30:37,840
life away. That was line number 10. So that's a full transparent, but that's not what we're trying

310
00:30:37,840 --> 00:30:42,320
to get to, right? And so I think there's this balance. We have to figure this out. It's not about

311
00:30:42,320 --> 00:30:49,120
being fully transparent is about providing the information that we need at the time that it's needed.

312
00:30:49,120 --> 00:30:56,640
That's really the underlying issue. And does this, you mentioned earlier, we hadn't had a chance to

313
00:30:56,640 --> 00:31:04,400
dive into it. The work that you are doing with pediatric robotics, does the, how does the trust

314
00:31:04,400 --> 00:31:14,240
play into the pediatric robotics scenario? Yeah. So pediatric robotics, it relies on a bonding.

315
00:31:15,120 --> 00:31:19,760
So having an established relationship, which basically means you have this aspect of trust.

316
00:31:21,040 --> 00:31:27,680
And it's important because we're working with kids where we're doing something that might not

317
00:31:27,680 --> 00:31:33,360
be very comfortable because we're doing physical therapy. We're doing exercise. And so it's,

318
00:31:33,360 --> 00:31:38,080
it could get uncomfortable. We may want them to do it for longer than they want to. And so the

319
00:31:38,080 --> 00:31:42,800
child has to trust that this robot has their best interests in heart and is, quote, unquote,

320
00:31:42,800 --> 00:31:49,520
their friend. So it was really, really, really important. And in fact, we have shied away from

321
00:31:49,520 --> 00:31:56,800
introducing the mistakes. Even though we're like, we're like, oh, let's see what happens. Only because

322
00:31:56,800 --> 00:32:01,600
we're talking about this vulnerable population. And so you make mistakes. And if you have this

323
00:32:01,600 --> 00:32:06,880
bonding of trust, you might actually, I mean, it's physical therapy. You might fundamentally

324
00:32:06,880 --> 00:32:11,920
change the way they think about what's right or what's wrong. And so we've shied away from that

325
00:32:11,920 --> 00:32:16,640
because we intentionally want this bonding. We intentionally want this trust to have an

326
00:32:16,640 --> 00:32:22,160
optimal outcome. And so we can't play around with at least with children with special needs. Now

327
00:32:22,160 --> 00:32:28,320
with adults, we're doing, like I talked about the human and the robot. For adults, we can do it.

328
00:32:28,320 --> 00:32:34,640
Yeah. But for kids, we're very, very conscious of this population. And so when we're doing that,

329
00:32:34,640 --> 00:32:40,240
we actually are going out to hopefully improve outcomes. That's our ultimate goal. And to improve

330
00:32:40,240 --> 00:32:44,720
outcomes, you have to have bonding. You have to have this relationship building. You have to have

331
00:32:44,720 --> 00:32:49,520
this, quote, unquote, friendship, which of course equals this aspects of trust.

332
00:32:49,520 --> 00:32:59,680
And the is pediatric robotics to what degree is this a thing? I guess what degree are we there? Are

333
00:32:59,680 --> 00:33:07,600
there systems that are commercial or production systems out that are doing this? Or is this more

334
00:33:08,720 --> 00:33:14,880
in the research domain? I would say this is more in, so it's a combination. It's a combination

335
00:33:14,880 --> 00:33:24,720
of in the research, but in the clinical research domain. And so for places that do clinical research,

336
00:33:24,720 --> 00:33:30,400
like say hospitals, they are bringing in these platforms to look at outcomes. But it's not

337
00:33:30,400 --> 00:33:35,520
something where say a local clinic that is just providing services, not doing research,

338
00:33:35,520 --> 00:33:42,240
is going to bring into their home. So it's in that in between translational stage of proving.

339
00:33:42,240 --> 00:33:47,840
And mainly is because it's well, the hardware itself is still difficult to use. But it's also

340
00:33:47,840 --> 00:33:52,720
proving out these outcomes and proving out the interventions and basically saying, you know,

341
00:33:52,720 --> 00:34:00,000
for this target population, this is the intervention that works. And so it's more becomes a prescription.

342
00:34:00,000 --> 00:34:08,000
It's like that's trying to figure that out. And what's the research frontier in that domain?

343
00:34:08,000 --> 00:34:11,200
What are the main problems you're trying to solve at this point?

344
00:34:11,200 --> 00:34:20,720
So the main problem is long-term engagement and adaptation. So again, I like our longest

345
00:34:20,720 --> 00:34:25,200
days or eight weeks, which is not long-term. But imagine you have this robot in the home with

346
00:34:25,200 --> 00:34:33,200
this child for years. I would even be a year. I'd be happy with even eight years at this point.

347
00:34:34,160 --> 00:34:40,720
So how do you ensure one that the system can adapt to the needs of the child? Because the child

348
00:34:40,720 --> 00:34:47,200
is going to grow. They're going to improve on some measures of these outcomes. Also, the relationship

349
00:34:47,200 --> 00:34:55,120
as well. How does the robot identify the progression of the child, even in terms of the emotional

350
00:34:55,120 --> 00:34:59,840
state? They become more confident because they are getting better and having more outcomes.

351
00:34:59,840 --> 00:35:05,760
Does the robot have to change its behavior because of that? And then the personalization

352
00:35:05,760 --> 00:35:13,680
which is I bring in the same robot into a home. I bring it into home one. I bring it home to two.

353
00:35:13,680 --> 00:35:19,680
I bring it into home three. And each child is different. How do you deal with that? The nuances

354
00:35:19,680 --> 00:35:26,160
of the child, which also leads to this long-term adaptation because you have to be able to adapt

355
00:35:26,640 --> 00:35:29,520
in that moment as your calibration routine.

356
00:35:29,520 --> 00:35:41,680
And what are some of the research results that you've seen? I guess I'm curious, for examples of

357
00:35:41,680 --> 00:35:46,160
things that you publish in this area. So our outcomes, so believe it or not, most of the stuff

358
00:35:46,160 --> 00:35:52,400
we now publish is in the clinical literature. So we're at the outcomes state. And so we've shown

359
00:35:52,400 --> 00:36:01,280
improvements in things like range of motion in movement time, so how fast you move. So we've

360
00:36:01,280 --> 00:36:05,360
shown outcomes with children. Our primary target demographic has been children with cerebral

361
00:36:05,360 --> 00:36:12,400
palsy. So we have shown improved outcomes. On the, I would say, more of the technical techy side,

362
00:36:14,160 --> 00:36:19,120
we published, like the stuff that we published, like now we're done. We're like, we're done with

363
00:36:19,120 --> 00:36:24,800
that. A long-term adaptation is really linked to the kids. It's directly linked to the kids.

364
00:36:25,680 --> 00:36:31,040
It's about collecting the data and things like that. So the, I would say, the technology

365
00:36:31,040 --> 00:36:36,880
infrastructure was things like, how do you create an expert system based on a knowledge base of

366
00:36:38,240 --> 00:36:42,560
therapists interacting with kids? So that's how we started our initial training as we looked at

367
00:36:42,560 --> 00:36:49,680
therapist child interaction and looked at how do they interact with the child? What kind of information

368
00:36:49,680 --> 00:36:56,640
did they provide in terms of both verbal as well as gestural feedback and took that and started

369
00:36:56,640 --> 00:37:04,320
with an initial system that we can then have the robot extract. Looking at correlating facial

370
00:37:04,320 --> 00:37:09,440
expressions to an emotional state, you know, what does that look like? How do is that information

371
00:37:09,440 --> 00:37:14,320
get fed to the robot so that the robot can then provide the right emotions, whether it's happy or

372
00:37:14,320 --> 00:37:20,800
frustrated and things like that? So that was all of the work that then tied into more of these

373
00:37:20,800 --> 00:37:27,120
pilot slash, I won't even call them clinical, but pilot studies with clinical collapse.

374
00:37:28,400 --> 00:37:36,000
Okay. And so these robots that you're using in this scenario are these, you know, humanoid robots

375
00:37:36,000 --> 00:37:43,120
or they arms or something different? Yeah, so they're humanoid robots. And I mean, I roughly say

376
00:37:43,120 --> 00:37:49,600
that as humanoid, we don't use the lower, like we don't do... They're probably not a base or

377
00:37:49,600 --> 00:37:55,200
something like that. So they have legs, but in all of our experiments, it's about the arm movement

378
00:37:55,200 --> 00:38:02,480
and we just walk. So in theory, the robot doesn't have to have legs. So, but it does have to have

379
00:38:02,480 --> 00:38:09,760
arms because we have to do the gesturing. So, for example, when a child needs more guidance on

380
00:38:09,760 --> 00:38:16,720
a proper form, the robot has to use the arms to basically explain what the child should be doing.

381
00:38:16,720 --> 00:38:22,800
So we have to have the upper arms to show that movement so the child can mimic. But that's really

382
00:38:22,800 --> 00:38:28,880
the only requirement. Of course, the head, because we need to express emotions somehow. So, you know,

383
00:38:28,880 --> 00:38:37,360
head turn and things like that. So, yeah, humanoid, they're not that. So, we use the now and the

384
00:38:37,360 --> 00:38:43,600
Darwin. So, in terms of height wise, that's like 18 to the now's a little bit taller,

385
00:38:44,400 --> 00:38:48,560
two-ish feet. So, they're small robots. So, they're not human size.

386
00:38:48,560 --> 00:38:52,240
Okay. And so, these are... Their role is primarily as

387
00:38:52,240 --> 00:39:01,280
exemplars for the children to follow as opposed to, you know, I recall from physical therapy being

388
00:39:01,280 --> 00:39:09,520
physically manipulated to the project pain. Correct. Correct. So, yeah. So, what we do is

389
00:39:10,560 --> 00:39:17,040
non-contact we have. So, contact-based rehabilitation would be, I grab your arm and I move it.

390
00:39:17,040 --> 00:39:26,000
So, we do non-contact. And it's mainly... Well, there's a couple of reasons. One is because we want

391
00:39:26,000 --> 00:39:32,400
kids to improve based on their own... Based on their own kind of intrinsic motivation, pushing

392
00:39:32,400 --> 00:39:38,000
themselves. It's just like with... If you think about sports, like the only way you get better at

393
00:39:38,000 --> 00:39:43,760
sports is practice, practice, practice, practice. And you get better. And yeah, you can stop

394
00:39:43,760 --> 00:39:48,800
one and say an exoskeleton to a baseball player and they will perform very well, but as soon as

395
00:39:48,800 --> 00:39:58,000
you remove the exoskeleton, may or may not perform as well. And so, our focus is on non-contact.

396
00:39:58,000 --> 00:40:04,160
It takes longer time and longer term to get the improvements, but then they are their own and

397
00:40:04,160 --> 00:40:16,800
they're retained. Got it. Super interesting. You also mentioned early on that your... Well,

398
00:40:16,800 --> 00:40:21,920
one of the terms that you mentioned in passing was embodied AI. And I've had some... A few

399
00:40:21,920 --> 00:40:28,800
conversations with folks about kind of the role, the relationship between AI and embodiment. And

400
00:40:28,800 --> 00:40:35,040
I'm curious with all the work that you're doing in this area, what your perspective is.

401
00:40:37,120 --> 00:40:44,960
And maybe the background is... One of the comments that was made was that really we'll never get

402
00:40:44,960 --> 00:40:53,760
to true AI without embodiment because it is so inherent to what intelligence means for us as

403
00:40:53,760 --> 00:40:59,360
humans. And I'm curious if that's your perspective as well. Interesting. That must have been a

404
00:40:59,360 --> 00:41:05,520
roboticist, that's it. It wasn't a roboticist, that's it. Thank you. Yeah, that sounds like something

405
00:41:05,520 --> 00:41:13,920
we would talk about in our closed room. So, I'm not going to talk about true intelligence.

406
00:41:13,920 --> 00:41:22,640
That's like that's going down a rabbit hole. But the kind of the embodied AI versus AI,

407
00:41:24,080 --> 00:41:32,160
there is a large overlap. So that's one thing as a fact. Now, maybe not say 15 years ago,

408
00:41:32,160 --> 00:41:38,320
maybe it was a little bit more removed, but now there's a large overlap. And there are some

409
00:41:38,320 --> 00:41:46,160
things that are unique about being embodied in the physical world. But I think, and I'm going to

410
00:41:46,160 --> 00:41:54,560
put on my other hat, I think when you think about AI in general, and if it's a true agent,

411
00:41:54,560 --> 00:42:01,200
like an agent that it's exploring a virtual world, I think you have some of the same characteristics

412
00:42:01,200 --> 00:42:09,520
of an embodied AI agent, but not all of them. So if you, for example, decide, we'll use a chat bot,

413
00:42:09,520 --> 00:42:15,920
a chat bot, but put the chat bot in a virtual reality environment that has to interact with people.

414
00:42:17,360 --> 00:42:23,280
And so therefore, you might chat with someone, but then they look really weird or they might turn

415
00:42:23,280 --> 00:42:28,400
their backs on you. And so you then have to, you know, so then you have to learn how to interact.

416
00:42:28,400 --> 00:42:33,680
And so you're using physical motions. If you do the virtual reality environment correctly,

417
00:42:33,680 --> 00:42:38,400
you know, you're realizing that, you know, if I say something and they bring me a cup,

418
00:42:38,960 --> 00:42:43,600
I have to do something with the cup. And so maybe I shouldn't say that I'm thirsty if I'm really not.

419
00:42:45,040 --> 00:42:50,640
So I think you can learn a lot in the virtual world that's similar to being embodied.

420
00:42:50,640 --> 00:43:00,880
But then there's the uniqueness of having a physical agent in our real world because then the, like they say,

421
00:43:00,880 --> 00:43:07,760
the world kicks you in the face, right? So you're like, I have the perfect intelligent algorithm.

422
00:43:07,760 --> 00:43:12,880
And it's perfect in the virtual world. And you come into the real world and you realize that,

423
00:43:12,880 --> 00:43:19,120
you know, the building that you have the beautiful, beautiful map of is actually not correct

424
00:43:19,120 --> 00:43:23,600
because a human created that map. And of course, they might have taken shortcuts.

425
00:43:24,480 --> 00:43:28,160
And so now you're in the physical world and you realize and therefore you then have to think

426
00:43:28,160 --> 00:43:33,840
about alternatives because your map is 100% guaranteed not to be correct. And so then the way

427
00:43:33,840 --> 00:43:41,840
you think about intelligence and adaptation becomes more about problem solving to get to a solution.

428
00:43:41,840 --> 00:43:47,840
So it's a different way of thinking about intelligence versus just taking all the data and coming

429
00:43:47,840 --> 00:43:53,840
up with a conclusion. But there's a large, large overlap between the two. And I hate to say that,

430
00:43:53,840 --> 00:44:00,000
you know, literally 15 years ago, I'd be like, oh, no, no. Robotics is special, right?

431
00:44:01,360 --> 00:44:07,760
I think it's like, no, there's so much overlap now because the computing has become so powerful

432
00:44:07,760 --> 00:44:14,560
in terms of the computational aspects and the brain that we can now take these AI algorithms

433
00:44:14,560 --> 00:44:20,480
and put them on a robotic platform. Because we can do that, we can now take advantage of some of

434
00:44:20,480 --> 00:44:27,760
these more powerful AI algorithms and then incorporate these differences of being in the real world.

435
00:44:28,960 --> 00:44:34,320
Super interesting. So what's next for you given all the things that you

436
00:44:35,360 --> 00:44:41,200
have talked about and shared with us? What is the kind of future directions for you and your work?

437
00:44:41,200 --> 00:44:49,600
So one is, of course, the pediatrics. And we're moving into the smaller space. We're now looking at

438
00:44:49,600 --> 00:44:56,320
infants, which is actually unique because they don't actually respond in the same way as kids.

439
00:44:57,040 --> 00:45:02,000
You know, they're nonverbal for the most part except that they do have emotions. So we're working,

440
00:45:02,000 --> 00:45:08,160
we're pushing more into that space, younger and younger. And then on the trust aspects,

441
00:45:08,160 --> 00:45:13,680
it's this one more study is about looking at the parameters of trust.

442
00:45:15,520 --> 00:45:21,760
Is trust tied to things like education? Is it tied to economics? Is it tied to gender? Are there

443
00:45:21,760 --> 00:45:27,200
certain things that we can start modeling about trust? Like, oh, this person here, if I get your

444
00:45:27,200 --> 00:45:33,680
demographics, I can maybe more identify that you're more susceptible or not. Kind of looking at

445
00:45:33,680 --> 00:45:38,880
that as ways of then being able to mitigate. Like, if I have someone, I'm like, oh, guess what?

446
00:45:38,880 --> 00:45:43,840
All engineers will never trust. Then it doesn't make sense for me to provide any type of intervention

447
00:45:43,840 --> 00:45:49,760
for trust. But if I'm like, oh, this type of demographic, like maybe teenagers of the age between

448
00:45:49,760 --> 00:45:56,560
16 and 20, they will always trust, you know, if I can identify that, then my intervention methods

449
00:45:56,560 --> 00:46:02,800
might be slightly different. So pushing that a lot more, pushing it in the healthcare domain,

450
00:46:02,800 --> 00:46:09,360
but also in this autonomous, and I don't say autonomous vehicle, but these autonomous robots that

451
00:46:09,360 --> 00:46:17,440
are on the road, that are on the road before we even realize, I think that we need to start

452
00:46:17,440 --> 00:46:24,560
looking at this aspect of trust. And only I say that mainly because if anything really bad happens,

453
00:46:24,560 --> 00:46:30,640
it would totally destroy the community. And so I think we need to get in front of it before it,

454
00:46:30,640 --> 00:46:36,560
before it gets to that point. Right. Right. Well, I want to thank you so much for taking the time out

455
00:46:36,560 --> 00:46:47,440
to chat with me. Great conversation as you predicted. And I really enjoyed learning about what

456
00:46:47,440 --> 00:46:52,480
you're up to. Oh, no, thank you. Thank you. This was a beautiful conversation. I enjoyed it.

457
00:46:52,480 --> 00:47:00,960
All right, everyone, that's our show for today. Remember, we want to hear your thoughts on

458
00:47:00,960 --> 00:47:05,680
personal AI. If you were too excited about the interview to hit pause before,

459
00:47:06,320 --> 00:47:12,640
now's your chance to head on over to Twomlai.com slash my AI to talk back to us.

460
00:47:14,160 --> 00:47:18,880
For more information on Ayanna Howard or any of the topics covered in this episode,

461
00:47:18,880 --> 00:47:27,040
head on over to twomlai.com slash talk slash 110. Thanks so much for listening and catch you next time.

