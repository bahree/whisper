1
00:00:00,000 --> 00:00:15,760
Alright everyone, I'm here with Eric Rice. Eric is an associate professor at the University

2
00:00:15,760 --> 00:00:20,840
of Southern California and co-director of the USC Center for Artificial Intelligence

3
00:00:20,840 --> 00:00:24,600
in Society. Eric, welcome to the Twimal AI podcast.

4
00:00:24,600 --> 00:00:28,640
Well, thank you very much. It's a pleasure to be here. Hey, I'm really looking forward

5
00:00:28,640 --> 00:00:33,800
to digging into our conversation. Before we do, I'd love to have you share a little bit

6
00:00:33,800 --> 00:00:39,720
about your background. You are a social scientist, but you work extensively with the AI community.

7
00:00:39,720 --> 00:00:45,520
Tell us a little bit about how that came to be. Oh, God. Yeah, sure. As if that's a quick

8
00:00:45,520 --> 00:00:51,080
story, but I'll try to make it quick one. So yes, I am a social scientist. I'm a sociologist

9
00:00:51,080 --> 00:00:56,600
by training, and I'm a professor in the Susanne DeVorque Peck School of Social Work at USC.

10
00:00:56,600 --> 00:01:04,480
So I've been an applied social scientist working in social work for about 12 years. And

11
00:01:04,480 --> 00:01:12,400
I started working with computer scientists back in 2014. And my initial collaboration

12
00:01:12,400 --> 00:01:19,880
was with Mill and Tombay, who I know you've had on this podcast in the past. And really,

13
00:01:19,880 --> 00:01:26,760
he, he and I met at a kind of a meet and greet for faculty across schools. And I think

14
00:01:26,760 --> 00:01:35,200
the, and when he and I started talking, we realized that surprisingly, we shared an interest

15
00:01:35,200 --> 00:01:41,280
in social network influence problems, which I didn't realize the computer scientists were

16
00:01:41,280 --> 00:01:45,000
interested in it at all. But then it turns out that they are, especially in the context

17
00:01:45,000 --> 00:01:50,080
of, you know, Twitter networks and, and even in sort of modeling other sorts of networks

18
00:01:50,080 --> 00:01:55,440
because the interesting computational problems that go along with it. And I was a social network

19
00:01:55,440 --> 00:02:01,400
researcher since the 90s, really, when I was originally trained in sociology. And so

20
00:02:01,400 --> 00:02:08,080
I've been work, I had been working on some projects around social influence in youth

21
00:02:08,080 --> 00:02:12,800
experiencing homelessness and HIV prevention. And so when he and I started talking, we got

22
00:02:12,800 --> 00:02:17,680
all excited about this very odd, you know, a connection that we, that we didn't expect

23
00:02:17,680 --> 00:02:25,080
to have. And then when we, when we decided to start meeting together, things quickly seemed

24
00:02:25,080 --> 00:02:30,440
very promising. And so we started playing around and it became a paper. And then a paper

25
00:02:30,440 --> 00:02:34,840
became a grant. And then a grant became a center. And, you know, the next thing I know,

26
00:02:34,840 --> 00:02:38,760
I wake up and, you know, people expect me to say intelligent things about AI. And I

27
00:02:38,760 --> 00:02:42,760
am not a computer scientist, right? So I mean, I, I hang out. I've, I've got a lot of friends

28
00:02:42,760 --> 00:02:46,520
that are computer scientists, but I am not one. I mean, the last time I took a computer science

29
00:02:46,520 --> 00:02:52,640
class, it was 1994. And I was learning Pascal, right? So just to give you a sense of like what

30
00:02:52,640 --> 00:02:56,840
a dinosaur, you know, my computer science is person. That's awesome. Pascal's a great

31
00:02:56,840 --> 00:03:02,160
language. Sure. I don't think I could code a word in it anymore. But, you know, yeah,

32
00:03:02,160 --> 00:03:06,360
it was, it was cool when I was in college. Yeah, but it's, but, but, but, but, you know,

33
00:03:06,360 --> 00:03:09,880
I was a math nerd. You know, I, I mean, when I first started college, I thought I was going

34
00:03:09,880 --> 00:03:16,800
to be, I started off as a math major. And I took, sort of, as much math as the, um, folks

35
00:03:16,800 --> 00:03:22,440
that do economics and physics do just because a, I liked it. And it was like, and it was,

36
00:03:22,440 --> 00:03:27,240
math was an easy A for me. So it's like several, a couple years of math as electives was what

37
00:03:27,240 --> 00:03:30,920
I did, even though I was a sociology major, just because it was, I liked it. But I was really

38
00:03:30,920 --> 00:03:38,760
much more drawn, I think intellectually to social science and issues of inequality, uh, and

39
00:03:38,760 --> 00:03:42,560
poverty. I mean, I was living in, I was at the University of Chicago. I was an undergraduate

40
00:03:42,560 --> 00:03:48,280
in the south side of Chicago in the early 90s. And it was a really, you know, race relations

41
00:03:48,280 --> 00:03:53,680
were really tense. Poverty was really extreme. And it was, it was, and University of Chicago

42
00:03:53,680 --> 00:03:59,200
has a great kind of history of doing sociology. And so it was really hard not to get kind

43
00:03:59,200 --> 00:04:03,160
of drawn into this. And then as time went on, I realized that what I was really interested

44
00:04:03,160 --> 00:04:07,480
in was these very applied version of social sciences, which is what you get in social work

45
00:04:07,480 --> 00:04:12,000
where people are really interested in intervention work. And then all of that is sort of to say

46
00:04:12,000 --> 00:04:17,280
that, like, I become the domain expert, right? I mean, so I, I hang out with folks and talk

47
00:04:17,280 --> 00:04:22,160
to them about social problems. And then we try to come up with, uh, algorithmic solutions

48
00:04:22,160 --> 00:04:27,320
to, to, to make a difference is kind of like a high level, uh, of where, of where I'm at.

49
00:04:27,320 --> 00:04:34,560
Awesome. Awesome. And you recently participated in a workshop at ICLR, uh, where I clear,

50
00:04:34,560 --> 00:04:41,080
uh, on responsible AI. Uh, tell me a little bit about, uh, that workshop, the focus there

51
00:04:41,080 --> 00:04:50,120
and, sure, yeah. I've been very fortunate that, um, the computer science community has

52
00:04:50,120 --> 00:04:56,560
been very gracious and welcoming of me. I mean, I, I'm, I'm surprised pleasantly when

53
00:04:56,560 --> 00:05:01,120
I hear people talk about the value of working with social scientists and the value of social

54
00:05:01,120 --> 00:05:05,840
science, like social work. And I sort of, I was in, like, wow, this is weird that people,

55
00:05:05,840 --> 00:05:09,960
you know, people in this, in this room would say such a thing. But it's, it's, um, but

56
00:05:09,960 --> 00:05:15,080
it's really gratifying. And so I was, uh, this responsible AI workshop was really cool.

57
00:05:15,080 --> 00:05:20,120
There were, um, I don't know, about a dozen, um, people that they asked to give half an hour

58
00:05:20,120 --> 00:05:25,360
talks. And then they had some panel discussions where we all got to, in groups of about six,

59
00:05:25,360 --> 00:05:30,240
yeah, kind of riff on ideas with one another. And, you know, as usual for me in a computer

60
00:05:30,240 --> 00:05:35,040
science convening, I'm, you know, the one social scientist in the room. But, um, so I say

61
00:05:35,040 --> 00:05:40,640
probably, I don't know, maybe high level interesting things, but maybe in the details, like,

62
00:05:40,640 --> 00:05:45,680
dumb things, but I try my best, you know, and, um, and, and one of the things that I was talking

63
00:05:45,680 --> 00:05:50,080
about for myself when they asked me to give a talk was really, you know, what have I learned

64
00:05:50,080 --> 00:05:55,040
as a social scientist about doing interdisciplinary work? Because that's really been the

65
00:05:55,040 --> 00:06:01,920
focus since the beginning with Milland, um, Tom Bay, uh, back in 2014. And now in the center

66
00:06:01,920 --> 00:06:05,840
that I run at USC and the collaborations that I have with people like Phoebe Vianos and

67
00:06:05,840 --> 00:06:11,040
B. Stratelkina, um, you know, we still are doing all these interdisciplinary work. And one of

68
00:06:11,040 --> 00:06:16,160
the things I was talking about was some lessons learned for me. And, and I think I'll share this

69
00:06:16,160 --> 00:06:20,880
with, with, with your, with your audience, because I think that, you know, since most folks are,

70
00:06:20,880 --> 00:06:27,760
as you said, practitioners in the AI realm, um, that, you know, what I found in working collaboratively

71
00:06:27,760 --> 00:06:32,480
is that the, the, there's a few lessons that are valuable. I mean, the first is that you actually

72
00:06:32,480 --> 00:06:38,960
have to, what I say, well, say, like, collaborate for real. And that means don't just bring in domain

73
00:06:38,960 --> 00:06:44,080
experts and as like a token or if you're in social science, don't just bring in a machine learning

74
00:06:44,080 --> 00:06:48,640
person as a token person and say, hey, run these models. It's like, if you, but if you get people

75
00:06:48,640 --> 00:06:56,320
together to genuinely work on problems from the get go, the solutions and the, the con, the

76
00:06:56,320 --> 00:07:01,440
contours of the problem are much more detailed and the solutions are much more thoughtful and

77
00:07:01,440 --> 00:07:05,920
innovative and it's, it's much better work. Um, the other thing is you have to learn how to

78
00:07:05,920 --> 00:07:15,360
communicate, which is actually really difficult because you, um, in the social sciences and in

79
00:07:15,360 --> 00:07:20,720
computer science, there's elaborate professional languages that are very arcane and specific to those

80
00:07:20,720 --> 00:07:28,720
disciplines and they're very jargon-filled and sometimes the same words mean different things

81
00:07:28,720 --> 00:07:32,800
and different words mean the same things like, for example, you know, social scientists we talk

82
00:07:32,800 --> 00:07:37,600
about variables in the way that a computer scientist would talk about features of a data set.

83
00:07:37,600 --> 00:07:41,600
But when you talk about a variable, you're usually talking about something that's in the level of

84
00:07:41,600 --> 00:07:46,800
sort of like a, an algorithmic level model, whereas that's what we talk about when we're talking

85
00:07:46,800 --> 00:07:51,440
about features of a data set. And likewise, like a model, I mean, good lord. Like models mean like

86
00:07:51,440 --> 00:07:54,800
six different things because there's like theoretical models, but then theory means something

87
00:07:54,800 --> 00:07:58,560
different in social science than it means in computer science and so you can talk past each

88
00:07:58,560 --> 00:08:02,960
other really quickly, even though you think you're speaking the same language and it takes time

89
00:08:02,960 --> 00:08:09,440
to, to learn to communicate effectively. And then the example of this just prior to starting the

90
00:08:09,440 --> 00:08:16,720
interview, we were out, I was describing the, our audience as practitioners and to you that

91
00:08:16,720 --> 00:08:21,600
was social scientists. Do you mean that meant social workers? That meant practicing social workers,

92
00:08:21,600 --> 00:08:26,160
like not social scientists who would be running right from us, but people that actually work with,

93
00:08:26,160 --> 00:08:30,560
you know, a homeless individual and try to get them a house, you know, that's a practitioner,

94
00:08:30,560 --> 00:08:33,840
right? And so it's, it's, it's funny because yeah, we're using the same word to mean two

95
00:08:33,840 --> 00:08:38,560
totally different things. I mean, great example of that. Yeah, no, exactly. And it's, it happens

96
00:08:38,560 --> 00:08:45,920
all the time, all the time. And it's not insurmountable, it just takes time. And then, and then the

97
00:08:45,920 --> 00:08:52,560
last thing is really that you have to iterate, which is part, which is, you know, part of the processes

98
00:08:52,560 --> 00:09:00,880
that, you know, as you work collaboratively in these interdisciplinary spaces, the initial solutions

99
00:09:00,880 --> 00:09:05,680
may not actually encompass all of the depth of the problem and that you don't really even realize

100
00:09:05,680 --> 00:09:12,080
that you've left things out until you get to the next step down the road. And then you realize

101
00:09:12,080 --> 00:09:17,840
that while you've solved one aspect of the problem, new aspects of the problem are emerging and

102
00:09:17,840 --> 00:09:24,240
becoming visible to you simply by having solved that first part of the problem. But yeah, so anyway,

103
00:09:24,240 --> 00:09:29,360
we're, you know, it's, it's, it's these, these, these lessons learned about interdisciplinary work

104
00:09:29,360 --> 00:09:33,520
are things that, you know, I kind of came across over having done several projects over the years

105
00:09:33,520 --> 00:09:37,840
with several different computer scientists. And, and it's really, it's, it's really fun work.

106
00:09:37,840 --> 00:09:44,160
I mean, that's the other thing that, that I, I, I hope that I shared with the, the ICLR

107
00:09:45,600 --> 00:09:50,640
responsible AI is not only that, you know, we can be responsible and part of being responsible is

108
00:09:50,640 --> 00:09:59,360
doing this work in this collaborative interdisciplinary way, but also that it's a, it's a, it can be a very

109
00:09:59,360 --> 00:10:06,240
joyful process, you know, it's, it's, it's fun to, I mean, not that working on problems like HIV

110
00:10:06,240 --> 00:10:13,040
and homelessness, like those aren't fun things to talk about, but working with other people that

111
00:10:13,040 --> 00:10:18,400
are dedicated to making the world a better place and who want to solve problems and be thoughtful

112
00:10:18,400 --> 00:10:24,640
about it can be a very joyful and, you know, processed, you know, even though the issues that you're

113
00:10:24,640 --> 00:10:32,400
working on can sometimes be really heavy. I'd love to maybe carry those three lessons through

114
00:10:32,400 --> 00:10:38,000
as a frame for talking about some of the projects that you've worked on and some of the collaborations.

115
00:10:39,920 --> 00:10:46,240
One that you've already mentioned is this work with homelessness or this collaboration with

116
00:10:46,240 --> 00:10:54,320
Melentombe. You know, we talked quite extensively with him about it, but, you know, maybe before we

117
00:10:54,320 --> 00:10:59,920
jump into, you know, something new, maybe the kind of a fresh perspective from your side of the

118
00:10:59,920 --> 00:11:10,400
collaboration. Sure, sure. So when we start, like I was saying a couple minutes ago,

119
00:11:10,400 --> 00:11:16,960
part of this started because we realized that we had this surprising shared intellectual interest

120
00:11:16,960 --> 00:11:26,960
in how networks can be essentially mobilized to, with influence, maximization as sort of the

121
00:11:26,960 --> 00:11:35,920
basis for this. And so can you pick a set of, you know, nodes within this network that could be

122
00:11:35,920 --> 00:11:40,960
trained, this is in the context of the way that I think about, that could be trained to disseminate

123
00:11:40,960 --> 00:11:45,680
messages that would improve health and well-being in a community. In this case, we were really interested

124
00:11:45,680 --> 00:11:52,480
in youth who are experiencing homelessness and preventing HIV because almost one in 10 youth

125
00:11:52,480 --> 00:12:02,720
who are experiencing homelessness have HIV, which is compared to 0.0.2% of the housed population.

126
00:12:02,720 --> 00:12:08,880
So we're talking, you know, orders of magnitude more HIV risk. And that's because, you know,

127
00:12:08,880 --> 00:12:15,760
these young people are, you know, they're from very risky backgrounds where they're, you know,

128
00:12:15,760 --> 00:12:21,680
they're coming from very abusive households, lots of substance abuse in those families,

129
00:12:21,680 --> 00:12:27,680
substance abuse amongst the young people themselves. They get involved in exchange sex in the

130
00:12:27,680 --> 00:12:35,200
impartus survive. And also they're just having, you know, you know, sex in sexual relationships

131
00:12:35,200 --> 00:12:42,000
with people that are also in risky environments. And so it's just a, it's a dangerous

132
00:12:42,880 --> 00:12:50,080
from a sexual health standpoint life to live. But, you know, HIV is very preventable if people

133
00:12:50,080 --> 00:12:57,120
use condoms, if people get tested for HIV regularly, if you can inform people about, you know,

134
00:12:57,120 --> 00:13:01,760
some myths and realities about HIV testing. And so that's the sort of the public health goal.

135
00:13:01,760 --> 00:13:07,520
But then the challenge is that youth who are experiencing homelessness are really transient,

136
00:13:07,520 --> 00:13:13,840
their relationships come and go very quickly. They themselves are very mobile as a population. So

137
00:13:13,840 --> 00:13:18,160
they're kind of hanging out in a specific part of a city for a while. Then they move to another

138
00:13:18,160 --> 00:13:23,280
part of the city, maybe even to another city. So you want to, when you want to spread

139
00:13:23,280 --> 00:13:28,640
health information in that community, you have to do it, and you have to do it quickly and

140
00:13:28,640 --> 00:13:33,680
efficiently. So therefore the engineering process becomes really compelling because can we

141
00:13:34,640 --> 00:13:41,120
implement a version of these studies where we can, where we can pick the ideal set or nearly

142
00:13:41,120 --> 00:13:47,120
ideal set of young people to be that we would work with intensively on a short-term basis.

143
00:13:47,120 --> 00:13:51,120
And then they work in their communities to advocate for the health of their, of their friends.

144
00:13:51,120 --> 00:14:00,800
And so that project was really exciting. We had some graduate students in computer science

145
00:14:00,800 --> 00:14:05,440
who worked on the algorithms. I had some graduate students in social work who worked on developing

146
00:14:05,440 --> 00:14:12,960
the actual HIV prevention, intervention, you know, training and how to work with the young people

147
00:14:12,960 --> 00:14:21,440
face to face. And we had some successful papers that won awards, that students won on the computer

148
00:14:21,440 --> 00:14:28,160
science side, projects that got funded and had a lot of visibility on the social work side

149
00:14:28,160 --> 00:14:33,360
for my students. And we were lucky enough to get a grant funded by the state of California,

150
00:14:33,360 --> 00:14:39,680
California HIV research project. They gave us almost a million dollars to do a large-scale study,

151
00:14:39,680 --> 00:14:49,360
and we actually did an experimental study where we had 714 homeless youth over the course of two

152
00:14:49,360 --> 00:14:56,880
years that we enrolled in this program and watched, you know, whether or not we could use the AI-driven

153
00:14:56,880 --> 00:15:04,240
influence maximization peer leader selection process to improve the impact of this intervention.

154
00:15:04,240 --> 00:15:08,240
And we did. And it was, it was really gratifying. I mean, I remember the first time that I ran the

155
00:15:08,240 --> 00:15:14,560
data after the project had, we had gotten some initial, the initial data back from the field.

156
00:15:15,440 --> 00:15:19,680
And I just sat there shaking my head, oh my god, it actually worked. I mean, it's one thing as

157
00:15:19,680 --> 00:15:25,600
a social scientist to see these simulation models where, you know, you can see that, you know,

158
00:15:25,600 --> 00:15:32,320
from a, from a mathematical basis that this ought to be more efficient and it ought to have a

159
00:15:32,320 --> 00:15:39,120
greater impact. It's another thing to actually see the numbers of real human beings who were

160
00:15:39,120 --> 00:15:45,120
impacted be greater when you use those algorithms versus when you didn't. And it was just unbelievably

161
00:15:45,120 --> 00:15:50,720
exciting to have that happen. And so, you know, and that really took in terms of like those three

162
00:15:50,720 --> 00:15:55,920
lessons. I mean, you know, it was the social science team, the computer science team, but also the

163
00:15:55,920 --> 00:16:05,440
community. So, you know, the young people themselves and, and three big community agencies that

164
00:16:05,440 --> 00:16:09,920
work with homeless youth and Los Angeles worked with us very closely on that project. And it was

165
00:16:09,920 --> 00:16:15,920
the collaborations there, the long process of learning to talk to one another and really iterating.

166
00:16:15,920 --> 00:16:22,640
I mean, it was the, the final, the final algorithm that we tested with 700 people was, I think,

167
00:16:22,640 --> 00:16:30,400
the fourth or fifth iteration of the, the influence maximization algorithm. So it was not, you know,

168
00:16:30,400 --> 00:16:36,400
we had some of them were computational solutions that were discarded for improved computational

169
00:16:36,400 --> 00:16:40,000
solutions. And some of them were computational solutions that we tested and then said, well,

170
00:16:40,000 --> 00:16:44,880
now we've learned a new problem. We need to fix that as well. So here's a, here's a computational,

171
00:16:44,880 --> 00:16:50,320
new computational solution to an even more complex field problem. So it was, it was really interesting

172
00:16:50,320 --> 00:16:57,360
and gratifying work. And, you know, and I think the one, yeah, so, so I mean, that was, that was,

173
00:16:57,360 --> 00:17:02,480
that was kind of how it started. And that's the work that I did with, with, with Millen, and then,

174
00:17:04,000 --> 00:17:10,400
you know, and that was really the project that was the, the center piece really of him and my

175
00:17:10,400 --> 00:17:20,080
creating the center at USC back in 2016 and the fall, because we had just gotten funded

176
00:17:20,080 --> 00:17:26,640
to, for that study, we had just gotten, you know, I think the second paper that we'd written with

177
00:17:26,640 --> 00:17:33,440
some students in computer science had one on award working on this project. We had some other

178
00:17:34,000 --> 00:17:38,400
people that were really getting excited about the work that we were doing and we decided to

179
00:17:38,400 --> 00:17:43,680
create this center with some other faculty and just bring people are, you know, our students and

180
00:17:43,680 --> 00:17:48,560
faculty together to start working in these interdisciplinary projects. And, and, you know,

181
00:17:48,560 --> 00:17:53,760
and that's really what I've been doing for the last five years, you know, is, is, is now just doing

182
00:17:53,760 --> 00:18:01,840
social science, computer science, hybrid projects. And, and, and kind of talking about this

183
00:18:01,840 --> 00:18:08,720
idea of learning to communicate in different languages, it's, what extent did that extend to

184
00:18:09,600 --> 00:18:17,120
the different ways that you might approach assessment and measurement in a project like that as a

185
00:18:17,120 --> 00:18:22,400
social scientist, you know, versus, you know, Millen's approach and the computer science approach

186
00:18:22,400 --> 00:18:28,480
of kind of assessing the algorithmic performance of that, that specific model.

187
00:18:29,520 --> 00:18:37,280
Yeah, now that's an interesting question. I think, so computer scientists often get data

188
00:18:38,080 --> 00:18:45,360
that is relatively robust data. So, for example, you know, if you're getting, you know,

189
00:18:45,360 --> 00:18:52,320
click through rates from web pages like that data, it's pretty solid data. It's not messy

190
00:18:53,440 --> 00:19:00,080
data. In the social sciences, we often deal with very, very, very messy data because it's collected

191
00:19:00,080 --> 00:19:08,240
by human beings from the self-reports of human beings. And, you know, if you're interested in

192
00:19:08,240 --> 00:19:13,200
say studying, you know, youth who are experiencing homelessness over time, you've got to find those

193
00:19:13,200 --> 00:19:18,800
people again, right? And, you know, you've got to find those people again, you've got to create

194
00:19:18,800 --> 00:19:24,640
situations in which they feel like they can tell you the truth. You've got to create, you know, data

195
00:19:24,640 --> 00:19:33,280
collection techniques that are in language that those people understand, right? So, I mean, it's like,

196
00:19:33,280 --> 00:19:38,560
you know, you as an academic may understand a sentence that's got double negatives,

197
00:19:38,560 --> 00:19:45,200
but when you are talking to a 18-year-old who is a high school dropout, a double negative

198
00:19:45,200 --> 00:19:50,000
sentence may be completely, you know, confusing. I mean, it may not always be, but certainly,

199
00:19:50,000 --> 00:19:55,040
you know, you have to deal with with this as a reality. And, and then there's also, there's also

200
00:19:55,040 --> 00:20:01,200
messiness that I've experienced in working with computer scientists and social scientists together

201
00:20:01,200 --> 00:20:06,880
when we work with community partners and we're trying to use their existing administrative data.

202
00:20:06,880 --> 00:20:11,440
So, sometimes people get really excited about the fact that social service agencies

203
00:20:12,320 --> 00:20:18,960
collect a lot of data on their interactions with their clients over time. And that's a really

204
00:20:18,960 --> 00:20:25,440
exciting thing, but the challenge there is that the data is usually entered by social workers

205
00:20:25,440 --> 00:20:33,280
who are overworked, who may not necessarily prioritize data entry rigor over, you know,

206
00:20:33,280 --> 00:20:37,120
helping somebody find a house, right? And so, it's like, oh, at the end of the day, oh, yeah,

207
00:20:37,120 --> 00:20:40,320
right, I got to enter these things and then I'm trying to remember what it is that I did over the

208
00:20:40,320 --> 00:20:44,640
course of the day and, you know, and, you know, oh, you know, I'll do it tomorrow kind of thing,

209
00:20:44,640 --> 00:20:50,000
you know, and so things get forgotten, things get, get entered incorrectly, you know. And so,

210
00:20:50,000 --> 00:20:54,800
there's a lot of uncertainty, which is actually why it's been for me. One of the things that's

211
00:20:54,800 --> 00:21:00,480
been really fun is that as a social scientist, the way that we used to have to deal with uncertainty,

212
00:21:00,480 --> 00:21:04,320
or the way I used to have to deal with uncertainty is, okay, I'm going to do some basic statistical

213
00:21:04,320 --> 00:21:08,160
modeling and there's just going to be a lot of error. And at the end of the day, I'm going to have

214
00:21:08,160 --> 00:21:12,880
to talk about it at the end of my paper, what I understand as the complexity of the sources of

215
00:21:12,880 --> 00:21:17,360
these errors, like some of the errors, like some of the things I just described to you. But with

216
00:21:17,360 --> 00:21:22,560
computer scientists, sometimes they talk about, you know, robustness, right? So it's like, okay,

217
00:21:22,560 --> 00:21:30,320
well, if we can, if we can actually kind of delineate what it is, the world of uncertainty

218
00:21:30,320 --> 00:21:36,480
is, we can actually try to design some algorithms that are kind of best solutions in spite of the

219
00:21:36,480 --> 00:21:41,760
presence of this uncertainty or error, which as a, which as a social scientist was mind-blowing,

220
00:21:41,760 --> 00:21:45,680
when I first started talking to them, I was like, you, but it's really kind of the difference

221
00:21:45,680 --> 00:21:53,120
between treating data in sort of this very 20th century kind of way. It's like data as is versus

222
00:21:53,120 --> 00:21:59,600
treating data in a much more probabilistic fuzzy kind of way, which has become much more the,

223
00:21:59,600 --> 00:22:06,240
the realm of data science and computer science. And so it's really exciting, but still as a social

224
00:22:06,240 --> 00:22:11,120
scientist, I think one of the things that I can do is I can help the data science folks to understand

225
00:22:12,640 --> 00:22:17,920
what is, what are the sources of messiness in this data that we need to be paying attention to?

226
00:22:17,920 --> 00:22:22,960
You know, because you know, and there are things that, you know, I may know from sort of years

227
00:22:22,960 --> 00:22:30,080
of collaborating with community partners about the nature of these data that could be helpful,

228
00:22:30,080 --> 00:22:33,760
which isn't to say that you couldn't be a very thoughtful data scientist and make a good connection

229
00:22:33,760 --> 00:22:38,320
with a community agency and really talk to them in great detail about how they collected their

230
00:22:38,320 --> 00:22:42,960
data and what it all means and these sorts of things. But there's sort of a, I don't know,

231
00:22:42,960 --> 00:22:48,000
I think the three, the kind of this triangle, so to speak, of computer science, social science,

232
00:22:48,000 --> 00:22:56,000
and community collaborations really is a nice combination of partnerships where there's some,

233
00:22:56,000 --> 00:22:59,840
there's some real value added by everybody who comes to the table there.

234
00:23:04,880 --> 00:23:10,240
Let's maybe talk about some of the other collaborations and projects you've worked on.

235
00:23:10,240 --> 00:23:16,800
Sure, sure, no, I'd love to. So one of the ones that I'm, so first of all, I guess it's one

236
00:23:16,800 --> 00:23:23,040
thing to say is that, you know, as personally as an academic, most of my work has been around

237
00:23:24,480 --> 00:23:29,920
homelessness over the years. And so while HIV prevention is certainly one of the issues that

238
00:23:29,920 --> 00:23:34,240
people face, I've also done a lot of work around housing and housing intervention.

239
00:23:34,240 --> 00:23:38,720
And, and, and, and then my center, and I'll come back to the housing intervention in a minute,

240
00:23:38,720 --> 00:23:44,560
but the center itself, we've got several areas that we focus in on. So we're, we're interested in

241
00:23:44,560 --> 00:23:52,320
what we think of as, as sort of robust resilient communities, which is in a sense like planning for

242
00:23:52,320 --> 00:23:58,080
disasters and really the, the, the, the impact on human populations of global climate change is a big

243
00:23:58,080 --> 00:24:03,360
part of this. There's also some, some work that we've done on, on, on conservation as well. But

244
00:24:03,360 --> 00:24:08,400
then most of them are a little bit more, you know, hardcore social work driven. So it's, you know,

245
00:24:08,400 --> 00:24:12,960
health and mental health. We're, we're really interested in homelessness is a lot of the work

246
00:24:12,960 --> 00:24:20,480
that we do. We have a specific focus on suicide prevention. And we also have a big focus on

247
00:24:21,040 --> 00:24:26,960
substance abuse interventions as well, because of the, some of the other social scientist

248
00:24:26,960 --> 00:24:32,320
collaborators that, that are like, I have a colleague in front of Jordan Davis, who, who's an expert

249
00:24:32,320 --> 00:24:37,920
in substance abuse interventions. And he is, you know, he's, he's working in that area. And those

250
00:24:37,920 --> 00:24:46,720
are part of our portfolio. And so the, the two projects that I've been working on, other than the

251
00:24:46,720 --> 00:24:50,880
one that I just described with Millen, the most intensely over the last years, it is, our last

252
00:24:50,880 --> 00:24:58,320
couple of years has been one in suicide prevention and one in, in housing allocation. And so the,

253
00:24:58,320 --> 00:25:06,480
the suicide prevention project is working really on trying to use some machine learning predictive

254
00:25:06,480 --> 00:25:14,240
analytics to understand network level interactions between people. I'm really interested in social

255
00:25:14,240 --> 00:25:18,560
networks, which is going to come up over and over again. And trying to understand like really who it

256
00:25:18,560 --> 00:25:23,840
is that people turn to in times of need when their mental health is really, when they're really

257
00:25:23,840 --> 00:25:30,000
suffering and trying to understand if we can do some data mining of network data to try to understand

258
00:25:30,000 --> 00:25:38,480
better who might be people that could be eventually intervention allies, kind of in the same way

259
00:25:38,480 --> 00:25:44,320
that we thought about the intervention allies in the, in the HIV prevention study. So, you know,

260
00:25:44,320 --> 00:25:49,600
the HIV prevention study had a lot of social science work before I got involved in data mining

261
00:25:49,600 --> 00:25:56,400
types type of work that led to the, the, eventually the, the models that we created for interventions.

262
00:25:56,400 --> 00:25:59,200
And so likewise, we're doing stuff with suicide prevention. And we're working with a couple

263
00:25:59,200 --> 00:26:04,880
different populations, there are three actually. One is college students, another is homeless youth,

264
00:26:04,880 --> 00:26:09,680
again, homeless youth. I'm all about homeless adolescents. And then the third is actually working

265
00:26:09,680 --> 00:26:15,920
with folks in the army. And so we actually have a study that we, that we just finished doing the

266
00:26:15,920 --> 00:26:26,080
field work on where we, we, we, we looked at a, a battalion who had experienced a very high profile

267
00:26:26,080 --> 00:26:34,000
member of their community had died by suicide. And then we were allowed to come in and interview

268
00:26:34,000 --> 00:26:39,920
about 250 of these soldiers who had been impacted by this suicide about how they were talking with

269
00:26:39,920 --> 00:26:45,440
one another. You know, and so we've got, I don't know, something like, you know, 250 humans and maybe

270
00:26:45,440 --> 00:26:50,400
2,000 connections and the ways that they're communicating about ideas. And it's, it's really,

271
00:26:50,400 --> 00:26:55,680
we've just barely gotten into it. But it's, it should be really, hopefully really impactful to,

272
00:26:55,680 --> 00:27:00,960
to help the, you know, the folks in the army as well. And so it's like, there's a variety of

273
00:27:00,960 --> 00:27:07,120
populations that are impacted by suicide. You know, and unfortunately actually since COVID has happened,

274
00:27:07,840 --> 00:27:11,680
you know, suicide is up in the United States. And actually, I think across the world, although I

275
00:27:11,680 --> 00:27:15,360
don't know the data very well outside of the United States. And, and, and I think that, you know,

276
00:27:15,360 --> 00:27:20,400
so it's, it's timely to be, you know, thinking about these issues. But then the other project that

277
00:27:20,400 --> 00:27:27,440
we're jumping in to mention, because I remember going down this path and the conversation with

278
00:27:27,440 --> 00:27:32,400
Millen, where at time he said, network, I was thinking, oh, okay, so we're mining social network

279
00:27:32,400 --> 00:27:38,640
information for, you know, relationships. But in fact, that's not typically the case in your work.

280
00:27:38,640 --> 00:27:44,880
It's more about using social work techniques to try to understand the implicit networks

281
00:27:44,880 --> 00:27:49,360
among the participants in your studies. Is that right? You know, that's a really good thing to,

282
00:27:49,360 --> 00:27:55,200
to clarify, because I think in the most people when you say networks these days, think about

283
00:27:55,200 --> 00:28:00,400
Twitter networks or Instagram networks or Facebook networks, right? And what, what, what I'm talking

284
00:28:00,400 --> 00:28:07,040
about usually is, and at least in the context of homeless youth was face to face networks of

285
00:28:07,040 --> 00:28:13,120
conversation networks that people have that are these really, I guess you might think of as

286
00:28:13,680 --> 00:28:18,480
implicit networks that, you know, you can observe people talking to each other, right? But there's no,

287
00:28:18,480 --> 00:28:23,040
there's no, I friended you, right? On the streets, like, like, I'm talking to you, because you're

288
00:28:23,040 --> 00:28:28,080
my friend, but we don't, we don't like, you know, there's no formal linkage through a, through a

289
00:28:28,080 --> 00:28:34,000
software, right? And I think the same is true with folks that we're talking about in the army. So

290
00:28:34,000 --> 00:28:37,760
you've got in, in a sense, you know, when you're looking at people that are in a battalion,

291
00:28:37,760 --> 00:28:42,720
in a formal sense, they're all connected, right? Like these are, you know, these are platoons that

292
00:28:42,720 --> 00:28:46,160
are within, you know, there's like these nested structures and there's these formal networks,

293
00:28:46,160 --> 00:28:52,080
but then there's also informal networks, right? So not everybody that I work with, do I necessarily

294
00:28:52,080 --> 00:28:59,280
trust with the, you know, my deepest, darkest fears and secrets, that's a subset of people. And

295
00:28:59,280 --> 00:29:04,720
when you're talking about people that are, you know, sort of like on base, getting ready to gear

296
00:29:04,720 --> 00:29:09,920
up for deployment, which is who the folks that we were talking to were, you know, they're also talking

297
00:29:09,920 --> 00:29:14,640
to their spouses, if their spouse lives on base with them, their kids, if their kids live on base

298
00:29:14,640 --> 00:29:19,040
with them, their girlfriends, if they've, or boyfriends, if they've got a girlfriend or boyfriend that's,

299
00:29:19,040 --> 00:29:24,080
you know, in the, in the community, or frankly, friends from high school, or girlfriends or

300
00:29:24,080 --> 00:29:29,600
boyfriends that aren't on base and aren't even in, you know, the nearby, the, the base, but they're,

301
00:29:29,600 --> 00:29:33,440
you know, whatever, 3,000 miles away in California or something like this. And so,

302
00:29:34,320 --> 00:29:41,200
but some of those connections are Facebook, Instagram, Twitter, you know, Snapchat, whatever.

303
00:29:41,200 --> 00:29:45,200
But, but most of the time when I'm thinking about networks, I'm thinking about them in a more

304
00:29:45,200 --> 00:29:50,240
kind of, you know, abstract social work sense, which is the sort of sense of, like, you know,

305
00:29:50,240 --> 00:29:56,880
we're all networked together as human beings. And, but, but how we define those networks is, is,

306
00:29:57,840 --> 00:30:04,800
can be very important. And, and rarely do I do work where we are, say, mining Twitter networks

307
00:30:04,800 --> 00:30:07,840
or something like this. I mean, I've got some colleagues that do that work. And it's really cool.

308
00:30:07,840 --> 00:30:11,680
There's some really interesting questions to answer there. But most of the questions that I've

309
00:30:11,680 --> 00:30:17,040
been interested in over the years because of my focus on, especially people who are experiencing

310
00:30:17,040 --> 00:30:20,880
homelessness who have limited access to technology, right? I mean, it's like most of the people

311
00:30:20,880 --> 00:30:26,240
that I've worked with over the years, like they have a cell phone. But, you know, oftentimes,

312
00:30:26,240 --> 00:30:32,400
they don't have any, their data plan is, is, is, doesn't work. And so it's kind of like a Wi-Fi device.

313
00:30:32,400 --> 00:30:37,760
And so they're like finding a, you know, it's like, so yeah, they're using social media, but they're,

314
00:30:37,760 --> 00:30:41,760
but the homeless youth that I'm looking at are, they're using it on like a daily basis or a weekly

315
00:30:41,760 --> 00:30:47,120
basis, not an hourly basis, the way that you would think about like a young adult who, you know,

316
00:30:47,120 --> 00:30:52,480
has a, you know, typical 20-year-old in America is like glued to their phone. Whereas,

317
00:30:52,480 --> 00:30:55,120
a homeless youth would love to be glued to their phone. They just don't have the resources to

318
00:30:55,120 --> 00:31:03,520
be glued to their phone, you know. And so in terms of the, this applying machine learning to the

319
00:31:03,520 --> 00:31:10,000
social networks to try to identify and prevent suicide, what's the general approach and

320
00:31:10,640 --> 00:31:14,800
where does machine learning come into play there? Sure, sure. So, so, so what we're, what we're doing

321
00:31:14,800 --> 00:31:22,880
is that we've collected through some surveys. We ask people to talk to us, or to, to, to delineate

322
00:31:22,880 --> 00:31:28,320
in these surveys, the 10 or 20 people that they talk to the most frequently. And then we ask them

323
00:31:28,320 --> 00:31:34,160
in a, a battery of, you know, I think it's about 30 or 40 questions about who these people are,

324
00:31:34,160 --> 00:31:38,800
the frequency of which you talk to them, what are the means by which you talk to them, what do you

325
00:31:38,800 --> 00:31:45,120
talk to them about, what roles do they occupy in your life, you know, things like, you know,

326
00:31:45,120 --> 00:31:49,200
if you needed to borrow, you know, a thousand dollars, is there anyone on this network that you

327
00:31:49,200 --> 00:31:56,240
could do that from? When you are feeling, you know, if, if you were feeling yourself suicidal,

328
00:31:56,240 --> 00:31:59,280
who would you go into? Is there anyone in this network space that you would talk to?

329
00:32:00,400 --> 00:32:06,720
Who did you know before you joined the service? Who, who on here is a family member?

330
00:32:07,360 --> 00:32:12,000
Is there anyone on here that's a romantic partner? How frequently do you talk to each of these

331
00:32:12,000 --> 00:32:16,080
people, these sorts of things? And then, and then that becomes a data set that has

332
00:32:16,080 --> 00:32:23,040
thousands of diads. So thousands of relationships that have a lot of information, a lot of features

333
00:32:23,040 --> 00:32:27,840
about those relationships. And then rather than approaching it in kind of the traditional social

334
00:32:27,840 --> 00:32:33,360
science way, which is to say, well, we a priori hypothesize that these four or five factors are the

335
00:32:33,360 --> 00:32:37,200
things that are important. And so we're going to look at some statistical models and see what,

336
00:32:37,200 --> 00:32:42,560
and see if those four or five things are statistically significantly associated with, say, the,

337
00:32:42,560 --> 00:32:48,400
the ability to communicate about suicidal thoughts to a particular person as an outcome, you know,

338
00:32:48,400 --> 00:32:51,360
here we're going to, we're going to do some data mining, right? So we'll just do some predictive

339
00:32:51,360 --> 00:32:55,200
analytics, right? Like let's just throw in all the features that we've got in the data set,

340
00:32:55,200 --> 00:32:59,040
you know, and, and see what, and see what shakes out, you know, and, and so then it's, you know,

341
00:32:59,040 --> 00:33:03,200
and with, it was only a couple of thousand people, you know, you can, you know, yeah, I mean,

342
00:33:03,200 --> 00:33:06,400
convolutional neural networks probably aren't going to work, right? But, but certainly you can do

343
00:33:06,400 --> 00:33:12,480
things like decision trees and, and, and random forests and, you know, maybe even some, you know, SVMs

344
00:33:12,480 --> 00:33:18,720
or whatever. But, but even those, those contributions thinking about letting the data speak for itself

345
00:33:19,280 --> 00:33:26,720
is, is, is, is not the way that traditionally social scientists have, have, have done things. And,

346
00:33:26,720 --> 00:33:33,440
and, and there are some interesting ways in which, when you're, when you're stuck in, when you're

347
00:33:33,440 --> 00:33:38,720
stuck in the world of linear models, like, logistic regressions, like, like traditional statistical

348
00:33:38,720 --> 00:33:45,600
modeling, social scientists come up with very linear answers to relationships between variables.

349
00:33:46,400 --> 00:33:50,320
The nice thing about the way the computer scientists have, and data scientists have started to

350
00:33:50,320 --> 00:33:55,120
look at things is that sometimes non-linear combinations of variables are really, are really

351
00:33:55,120 --> 00:34:02,160
what's happening in the world. Like, the, and, and that is a very interesting new way of thinking

352
00:34:02,160 --> 00:34:07,680
about things that, that social sciences are, I think, benefiting from in recent, and recent years. But,

353
00:34:08,880 --> 00:34:12,080
yeah, so it's, it's, so that's kind of how we would, how would use it. And then, and then the idea

354
00:34:12,080 --> 00:34:19,360
would be, especially if we could use a, if the models are ones that have more transparency to,

355
00:34:19,360 --> 00:34:23,200
to how they work and what the, and what the underlying features are, then we can know what are the

356
00:34:23,200 --> 00:34:27,120
important features to track over time. And then as we move forward towards thinking about intervention

357
00:34:27,120 --> 00:34:32,400
models, you could think about creating small assessments that could then help you very quickly

358
00:34:32,400 --> 00:34:38,960
identify people who could be targets of intervention down the road. So, who are the types of people

359
00:34:38,960 --> 00:34:44,400
that, that, homeless youth need to talk, need to be having their, in their networks, cultivate

360
00:34:44,400 --> 00:34:49,760
in their networks to talk about suicide, or who are the people in, in these, you know, these,

361
00:34:49,760 --> 00:34:54,560
these, in networks of soldiers that people need to talk to, and trying to identify who those people

362
00:34:54,560 --> 00:34:59,840
might be. I don't know if that, that makes sense, but that's kind of the, the, the thrust of the ideas.

363
00:35:00,800 --> 00:35:07,680
It sounds like the initial phase of the research is to understand these communities and build a

364
00:35:07,680 --> 00:35:16,560
model. And then later on, you start to experiment with, or apply the model to determining which

365
00:35:16,560 --> 00:35:21,120
interventions are likely to be the, the most successful. And they're kind of distinct phases.

366
00:35:21,120 --> 00:35:25,360
It sounds like. Yeah, yeah. I mean, and this is interesting because I think this is something that

367
00:35:25,360 --> 00:35:32,400
both engineers and social workers think about a lot. Like there's, there's the, trying to understand

368
00:35:32,400 --> 00:35:38,560
the world phase of things, which oftentimes is sort of prediction, right? And then there's the,

369
00:35:38,560 --> 00:35:43,440
what do we do, phase of things, which is, which oftentimes, you know, I hear computer scientists say

370
00:35:43,440 --> 00:35:48,960
prescription is the, is the phrase I've heard like Eric Horowitz say from Microsoft research say,

371
00:35:48,960 --> 00:35:52,640
like here's prescription, prediction, then prescription. And the way that, you know, social scientists

372
00:35:52,640 --> 00:36:00,640
think about is oftentimes when you're looking at like trying to intervene with risky communities,

373
00:36:00,640 --> 00:36:04,720
you usually think about the first phase of the research is trying to understand what are the

374
00:36:04,720 --> 00:36:11,280
risk and protective factors that you could then leverage in your intervention phase of things.

375
00:36:11,280 --> 00:36:16,000
And that really basically boils down to prediction and then prescription. And then so it's like,

376
00:36:16,000 --> 00:36:23,120
again, language difference, right? Well, we're talking about two very analogous, you know, phases

377
00:36:23,120 --> 00:36:32,880
of work. And it's been interesting to me how much social work and engineering share in world

378
00:36:32,880 --> 00:36:38,320
views, even though the techniques are very different. Because in both engineering and this,

379
00:36:38,320 --> 00:36:46,080
I think, you know, when you end and in social work, the idea is to do not necessarily to do science

380
00:36:46,080 --> 00:36:53,280
for the sake of science, but it's to do science for the sake of creating solutions to problems.

381
00:36:53,280 --> 00:36:58,560
And in the case of social work, it's social problems. In the case of engineering, it can be,

382
00:36:58,560 --> 00:37:04,480
you know, physical problems as well as social problems. But now my experience with this sort of AI

383
00:37:04,480 --> 00:37:10,400
for social good, you know, universe is that there's a lot of computer scientists that want to work

384
00:37:10,400 --> 00:37:16,560
on social problems. And so, you know, we're, you know, trying, at least I'm trying my best to sort of,

385
00:37:16,560 --> 00:37:20,240
you know, work with those folks and see what else, you know, what new things can come out of those

386
00:37:20,240 --> 00:37:26,480
collaborations. Great, great. I want to make sure we talk about a third project that you mentioned.

387
00:37:26,480 --> 00:37:32,800
Oh, yeah. It sounds like, well, it's relating to systemic racism. And I believe it's housing

388
00:37:32,800 --> 00:37:39,120
specific. Yeah, thank you. It's like, the project, I'm probably most, I'm most invested in right

389
00:37:39,120 --> 00:37:44,400
now. And I forgot to talk about Phoebe Viandos, who's my partner, and this would kill me.

390
00:37:44,960 --> 00:37:49,280
So, yeah, so, so, so I've got this wonderful project that we're working on.

391
00:37:50,480 --> 00:37:54,480
We've got a partnership with the LA Housing Service Authority. I also have a partnership with

392
00:37:54,480 --> 00:38:00,480
some people at UCLA in the, in the, in the Semmel Institute of Nurse Psychiatry as well as the,

393
00:38:00,480 --> 00:38:05,440
as the, as well as the, the California Policy Lab, then people in, in the, you know,

394
00:38:05,440 --> 00:38:09,440
the School of Engineering at USC. So, there's this large group of people who've got community

395
00:38:09,440 --> 00:38:17,600
partners. And what we are doing there is we are, the problem that we're trying to solve is,

396
00:38:18,240 --> 00:38:24,480
how do we identify people who are in the greatest need for housing resources and make sure that

397
00:38:24,480 --> 00:38:31,280
they get the best interventions possible. And the challenge with this is that in a lot of communities

398
00:38:31,280 --> 00:38:36,240
like Los Angeles and many other big cities across the country, there are more people who experience

399
00:38:36,240 --> 00:38:40,960
homelessness than there are resources to go around, right? So LA, for example, every night has

400
00:38:40,960 --> 00:38:45,840
about 60,000 people who are either living in an emergency shelter around the streets.

401
00:38:46,880 --> 00:38:51,520
LA Housing Service Authority placed the most human beings of any community into housing

402
00:38:51,520 --> 00:38:56,720
last year, relative to anyone. It was 20,000 people, right? So you can see there's a huge disconnect

403
00:38:56,720 --> 00:39:02,240
between, you know, the available resources and, and, and what they're. So the challenge then is

404
00:39:02,240 --> 00:39:07,520
who do you serve? And so one of the thrusts has been to serve people that are considered to be the

405
00:39:07,520 --> 00:39:13,600
most vulnerable. And then, so then, okay, that sounds great. Like we don't want people to die on the

406
00:39:13,600 --> 00:39:17,120
streets. So we're going to try to, we're going to try to make sure that we get people who need

407
00:39:17,120 --> 00:39:22,560
these resources the most into housing. But then the challenge becomes, is this fair? And you can

408
00:39:22,560 --> 00:39:26,720
kind of think about this in a few different ways. One is, are the resources allocated fairly,

409
00:39:26,720 --> 00:39:32,720
meaning, you know, if there's 40% of the homeless population in LA that's black, which is what,

410
00:39:32,720 --> 00:39:37,600
which is what we have, are 40% of the resources going to black people. And what we find is the answer

411
00:39:37,600 --> 00:39:45,920
to that is yes. But then, if you ask the question, are, are black people succeeding in those housing

412
00:39:45,920 --> 00:39:53,280
allocation as it, at the same rates? What we're seeing is that you're the likelihood that people

413
00:39:53,280 --> 00:39:59,760
return back to homelessness after getting a housing intervention seems to be higher for black

414
00:39:59,760 --> 00:40:06,560
and Latinx folks than it is for white folks. So then the question is, okay, so we're trying to do

415
00:40:06,560 --> 00:40:13,760
this and we're trying to do this fairly. But can we, can we, can we really do this in a way that

416
00:40:13,760 --> 00:40:22,640
is genuinely mitigating a history in the United States of housing inequality that, that, especially

417
00:40:22,640 --> 00:40:28,480
bullet bot communities experience, but also the Latinx community is experiencing. And, and so what

418
00:40:28,480 --> 00:40:33,840
we're trying to do is we're trying to, and the way that this is done is that in the, in, in most

419
00:40:33,840 --> 00:40:40,400
communities in the US, they use these triage tools. It's like a survey instrument where we ask about,

420
00:40:40,400 --> 00:40:46,400
you know, about 40, 40 to 50 questions about terrible things that happened to you in your life.

421
00:40:46,400 --> 00:40:52,320
Like, have you been, have you been arrested? Do you have a substance abuse problem? Do you have health

422
00:40:52,320 --> 00:40:59,280
issues like HIV? And on and on and on. Have you been abused? And then the more of those risk

423
00:40:59,280 --> 00:41:04,000
factors that you have, the more vulnerable you are and the more likely you are then to be served by

424
00:41:04,000 --> 00:41:08,720
these, you know, the, the scarce housing resources. But there are a couple of different housing

425
00:41:08,720 --> 00:41:14,320
resources that most communities have. Some of them are short-term rental subsidies and others

426
00:41:14,320 --> 00:41:21,040
are more long-term programs that have extensive social services and social workers that are attached

427
00:41:21,040 --> 00:41:26,400
to them. And so one of the things that Phoebe Vianos and I have been working on and this is

428
00:41:26,400 --> 00:41:33,120
really her area of expertise is really these resource allocation problems. So given that we have

429
00:41:33,120 --> 00:41:38,720
this information about about, in this case with, with LA, we've got tens of thousands of people

430
00:41:38,720 --> 00:41:44,480
every year that are housed. And we've got this history of information about these risk factors.

431
00:41:44,480 --> 00:41:48,480
And then we have a history of information about, did you get a rental subsidy or did you get these

432
00:41:48,480 --> 00:41:54,400
more these the what we call permanent supportive housing, which is this more robust form of housing.

433
00:41:55,280 --> 00:42:00,720
Then we can look at what are the features of people in the past who've done well in each

434
00:42:00,720 --> 00:42:07,760
of those interventions. And then we can see whether or not we can allocate resources

435
00:42:07,760 --> 00:42:11,840
differentially. Because the current system basically says the highest risk people get the

436
00:42:11,840 --> 00:42:16,160
get the most intensive resources, the medium risk people get the get the get the interventions,

437
00:42:16,160 --> 00:42:20,400
which is kind of, which is better than nothing. It's it's thoughtful, but we can do better.

438
00:42:20,400 --> 00:42:28,960
Right. So and so the what we find, especially with the context of people who have experienced

439
00:42:28,960 --> 00:42:36,080
a lot of, you know, systemic racism and inequality, you know, in in the housing market is that

440
00:42:36,640 --> 00:42:42,240
when we reallocate resources such that, well, one of the issues is that when you have,

441
00:42:43,520 --> 00:42:48,800
you have to do fair machine learning, right. So you don't just want to, if you're, if you're,

442
00:42:48,800 --> 00:42:55,840
if your machine learning is based on a majority white population, but it's, but it's inaccurately

443
00:42:55,840 --> 00:42:59,840
then characterizing the black population and the Latinx population, you may actually

444
00:43:00,560 --> 00:43:06,960
misallocate people because they don't fit the, the, the, the, the, the, the, the modal categories,

445
00:43:06,960 --> 00:43:10,480
right. So there's those issues that come into play. And then there's issues about just

446
00:43:10,480 --> 00:43:15,680
allocating resources differently, depending on what people's histories within these systems

447
00:43:15,680 --> 00:43:24,480
have been. And so what we're trying to do now is, is, is redesign the, the use of these survey

448
00:43:24,480 --> 00:43:29,600
assessments that are these vulnerability assessments to try to serve the communities better. And at

449
00:43:29,600 --> 00:43:36,800
least our preliminary, preliminarily, the, the computational experiments that we've been working

450
00:43:36,800 --> 00:43:41,600
on would suggest that you can do a lot better than, than what we've done in the past by both

451
00:43:41,600 --> 00:43:48,720
improving the, the, the, the decreasing the bias in the machine learning processes and then

452
00:43:48,720 --> 00:43:57,200
reallocating resources to, to, you know, to match people in a more thoughtful way than just kind of

453
00:43:57,840 --> 00:44:02,720
really risky gets really good. Medium risky gets pretty good, you know, that you can do better than

454
00:44:02,720 --> 00:44:08,160
that. So, so that's, that's the other project that I'm really invested in. And, and I might have

455
00:44:08,160 --> 00:44:12,640
explained it in a much quicker way than, than the other ones, which is probably unfair to my

456
00:44:12,640 --> 00:44:17,520
commitment to it. But, but I also feel like I kind of rambled on about all of these other ones. So,

457
00:44:17,520 --> 00:44:20,320
but yeah, it's, it's, it's, but it's really exciting. I mean, one thing that's really

458
00:44:20,320 --> 00:44:25,600
exciting about that project is that because we have this partnership with the LA Housing Service

459
00:44:25,600 --> 00:44:33,520
Authority, if we can come up with a viable, some viable solutions, you know, we could potentially

460
00:44:33,520 --> 00:44:40,400
help thousands of people be allocated to more appropriate resources, which would increase the

461
00:44:40,400 --> 00:44:45,520
likelihood that people would succeed in those resources, which would mean potentially that over

462
00:44:45,520 --> 00:44:50,080
time, you know, thousands of people will not return to homelessness that would have returned to

463
00:44:50,080 --> 00:44:56,000
homelessness. And that's, that seems like a really big deal. And, and, and so the social impact of

464
00:44:56,000 --> 00:45:03,120
this could be, could be quite, could be quite large if we, if we do, if we do a good job. And,

465
00:45:03,120 --> 00:45:09,520
and it's exciting to, to get to work on a project like that, frankly. And what stage are you at,

466
00:45:09,520 --> 00:45:17,120
with that? Are you just getting started? So, so we, we are, we've been working on this for a

467
00:45:17,120 --> 00:45:22,480
couple of years. We have a couple of more years in the current grant project that we're working on.

468
00:45:22,480 --> 00:45:27,360
I don't know that will be done entirely by the time, you know, that project is done. It may be one

469
00:45:27,360 --> 00:45:31,760
of these things that, you know, it kind of becomes, you know, something that we work on for, for

470
00:45:31,760 --> 00:45:36,160
wildbies, as I said before, iteration is part of the game, right? So, I, but we will, you know,

471
00:45:36,160 --> 00:45:40,800
my, my, there, there's going to be things in the next, you know, year or so that we will be pilot

472
00:45:40,800 --> 00:45:46,720
testing with, with, with, with in Los Angeles to see how things work. But, you know, kind of larger

473
00:45:46,720 --> 00:45:50,160
scale implementation may take longer. I mean, there's also a huge, I mean, the reality is that

474
00:45:50,160 --> 00:45:54,720
there's a huge political process that's involved in all of this as well, because, you know, housing

475
00:45:56,160 --> 00:46:02,880
is not something that just, there's a, you know, the federal government provides a lot of this

476
00:46:02,880 --> 00:46:08,560
money. Local communities have a, a huge vested interest in how these resources get done. So,

477
00:46:08,560 --> 00:46:14,240
even if we come up with a genius engineering, so engineering slash social work solution to this,

478
00:46:14,240 --> 00:46:18,480
the community then has to still say, like, hey, we think this is important. And part of how we're

479
00:46:18,480 --> 00:46:24,560
doing that is that we have a really heavily involved community process, like we've got a lot of

480
00:46:25,280 --> 00:46:30,480
people that we meet with regularly who themselves have been home, lived through homelessness,

481
00:46:30,480 --> 00:46:36,240
or, and, or who are providers of homelessness resources. We work closely with the LA Housing

482
00:46:36,240 --> 00:46:41,600
Service Authority. We work closely with the departmental health. And we, we talk to all of these

483
00:46:41,600 --> 00:46:47,360
people every step of the way and really get them to help tell us where, you know, where our blind

484
00:46:47,360 --> 00:46:52,400
spots are and how to, and how to pivot. Because, like I said before, you know, social scientists may

485
00:46:52,400 --> 00:46:57,760
contribute a lot to the dialogue, but, but also the, the boots on the ground people who live these,

486
00:46:57,760 --> 00:47:03,040
who live with this, you know, these issues and, and, and with trying to implement solutions also have

487
00:47:03,760 --> 00:47:08,480
insights that, that are valuable. And so, we all come together and we're trying the best that we can.

488
00:47:08,480 --> 00:47:13,920
And, and hopefully that'll mean that people will be more eager to, to, to try these solutions out. But

489
00:47:13,920 --> 00:47:18,720
it's a, it's not, it's not a foregone conclusion that just a good engineering solution gets implemented.

490
00:47:19,360 --> 00:47:22,560
But hopefully this one will, and I think that there's a lot of reason to think that it will,

491
00:47:22,560 --> 00:47:24,880
because of the community participation that we have.

492
00:47:24,880 --> 00:47:27,280
Awesome. Awesome. Yeah.

493
00:47:27,280 --> 00:47:29,760
Well, Eric, thanks so much for joining us today.

494
00:47:29,760 --> 00:47:32,560
Hey, thank you for working on this very cool stuff.

495
00:47:32,560 --> 00:47:37,280
Well, I really appreciate it and thanks for inviting me and, and, and, and pleasure to meet you and,

496
00:47:37,280 --> 00:47:55,440
and, and good luck. Thanks, Eric. All right. Thanks.

