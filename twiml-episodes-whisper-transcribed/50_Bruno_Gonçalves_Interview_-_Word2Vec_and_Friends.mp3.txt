Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A big thanks to everyone who joined us last week for our second Twimble online meetup.
Led by Nicola Cucherova, we discuss the paper Learning Long-Term Dependencies with Gradient Descent
is Difficult by Yasuo Benjio and Company.
I had a great time and learned a ton.
For those who weren't able to attend live, we have the video posted for you on Twimbleai.com
slash meetup and if you're interested in joining us next month, please head on over to that site
to get signed up. We're also accepting presenters so feel free to shoot me a note with your ideas.
Next up, the time has come for the Artificial Intelligence Conference brought to you by O'Reilly
and Intel Nirvana. I'll be in San Francisco the entire week next week and we have a ton of great
interviews lined up. So this should be another awesome series. If you haven't had a chance to
check out our series from the New York event, you can find it at twimbleai.com slash O'Reilly AI
and Y. Also, if you'll be at the conference, please send me a shout out on Twitter or via email.
I'd love to connect. See you then. Okay, about our show. This week I'm bringing you an interview
with Bruno Gunn's office, a more Sloan Data Science Fellow at NYU. As you're hearing the interview,
Bruno is a longtime listener of the podcast. We were able to connect at the NY AI conference back in
June after I noted on a previous show that I was interested in learning more about Word Tevek.
Bruno graciously agreed to come on the show and walk us through an overview of Word embeddings,
Word Tevek, and a bunch of related ideas. He provided a great overview of not only Word Tevek,
but related natural language processing concepts such as skipgram, continuous bag of words,
no Tevek, TF IDF, and much more. By the time you hear this, it'll be too late to catch it live,
but Bruno is doing a half day tutorial on Word Tevek and friends on Monday at the AI conference.
If you'd like to see it, I'm sure it'll be available via the O'Reilly Safari website and now on to the show.
All right everyone, so I am here on location at the O'Reilly AI conference and I have the
pleasure of being here with Bruno Gunn's office who is apparently a longtime listener of the podcast
and reached out to me after hearing me note that I wanted to learn more about Word Tevek and embeddings
and made it happen. So we're here to talk about that. Welcome, Bruno. Thank you. It's a pleasure
to be here. So I've been listening for a very long time and I'm very happy to be here and be able
to participate. Awesome. Awesome. I was just asking you, you mentioned that you've been listening since
before the interview format and I don't know, I'm still stuck on like the transition from the
from the news format to the interview format. How was that for you? Like what's your take on kind
of the before and after? No, I like the new interview format actually. But this might be a
personal buy as well. When it was news, a lot of the news I'd already seen or listened somewhere
else. So it wasn't as new. In this way, since you're interviewing people that I don't necessarily
listen to it and then no personally, it's good to have a fresh view from them. That's great.
That's great. Like I said, I was, you know, and I guess I still am hung up on the fact that I
switched the format. But I've been told by people that things along the same lines that the news
is essentially commoditized and it's hard to, you know, beat out tech crunch or whatever that people
count on for their news. But the folks really seem to be enjoying the interview format. And so,
you know, again, it's great to have you on for an interview instead of talking about the news.
So tell us a little bit about kind of your background and what you're up to. So I'm,
I can say, I'm originally a physicist, but I've always been involved in optimization problems,
originally in spin glasses, soft optimization and data mining. Spin glasses. Spin glasses. So it's
this, it actually has interesting connections with neural networks and how they work. But
okay, basically a disorder magnetic system. Okay. Put it very, very simply. But the mathematical
structure is very rich and it leads itself to exploring different types of optimization problems.
Okay. From there, I evolved very quickly. And this was because I noticed that in spin glasses,
a lot of the behavior you see has to do with the way that the different elements are connected.
I moved very quickly towards networks and studying basically connections between components of
a system. Okay. More deeply. And when you're working with network and complex networks in general,
what you do a lot is basically data mining. You're, you're crawling websites, you're parsing a
patchy logs to look for connection between a URLs. For example, you're, you're basically doing
in a sense applied graph theory. Okay. But applied to real data. So the transition towards data
came very early and very naturally, even before, I mean, people were talking about data science.
This was back in maybe 2006, 2007. Okay. And then more recently, I've been moving more towards
more data science, intensive aspects and hence my interest in the work to vac and that type of
algorithms. Okay. You know, I think the best way to do this is to just jump in and have you
kind of walk us through work to vac, but the kind of the foundational thinking that led to work to
vac and embeddings. As I mentioned the other day, it's, it's an area that I've been meaning to dive
into and I'm glad you're, you're here to tell us about it. So the original idea is actually as
many things doing this field comes from outside the field. So it's comes from linguistics. Okay.
And it's expressed more or less very in a very general way by James Firth, if I'm not mistaken,
around the 1953. Okay. And basically what he said is you, you'll know the meaning of a world by
the company it keeps. So if you don't understand what a world means, you kind of look at the context
and the context gives clues about what the world says. Okay. Intense to say. So we're to vectorize
to take advantage of this by saying that the embedding of a world is defined by the context in
which it appears. So words that appear in the same context are more somewhat equivalent. So they
will have vectors that are close to each other. While words that tend to appear in very different
contexts will be very different and have very different vectors or vector representations in this
case. And so how do you get to the word vectors? I guess my impression is that you're, it's all
relative to a specific corpus, right? There's not like a, you know, we haven't done word to veck on
everything like some grand unification of word to veck. Is that the right way to think about it?
And then you're doing some math on that corpus to get you to the vectors. Can you talk a little
bit about that process? Yes. So basically what it does, it's a very simple neural network. So
it just says one hidden layer. The activation function is linear. So it just passes through.
It's very simple. What the actual wording embedding that are basically the weight on the
hidden layer. Depending on which model can be skip grammar, can be continuous bag of words. It
can be the weight on the leading to the hidden layer. It can be the weights outside heading out
of the hidden layer. But in practice, it's the same mathematically. And so the skip
ram refer to one of those and continuous bag of words refer to the other skip grammar is
leading in the weight's leading in. And yes, so the way you look at what first was saying that
you know a word by the company it keeps is if you have the word, you can kind of guess the
context in which it appears, right? Or if you have the context, you can kind of guess what word
would fit in the middle. So the skip ram and the continuous bag of words basically look at these
two approaches. In one case, your input is the word and you're trying to guess in the output
layer what the context is. On the other case, you have the context words as input and you're trying
to guess what is the word that might go in the middle. And here, just to clarify, the context is
usually defined by a window of words before that word and the words after that word. So if you're
looking at word, I, so your context would be, for instance, I minus one, I minus two and I plus
one, I plus two, like two words before, two words after or five words before five words after.
Okay. And so that windows what's used to compute the vector for that representation for
a given word. Yes. So that window will give you what is the context and the context will define
what is the word. Okay. Now, like you're saying, people haven't done word-to-vec in all the text in
the world. So every time you run this, you will get different vectors. Right. Also, if you run
twice in the same corpus, the vectors will be different. The reason for this is you're initializing
all the weights or the vectors randomly. Right. And then you're adjusting them. Okay. So you
will end up with something different. However, if you do it well enough and it converges and you get
something that this reasonable, the vectors will be different, but they'll basically be a rotation.
So you can get vectors trained into corpus and align them so that they match. Because what the word
-to-vec algorithm and similar algorithms are trying to do is learn the relations between vectors.
So you're basically, you're trying to define the differences of vectors, but not the actual vectors.
Right. So there's some distance metric or something. So if you rotate the distances all remain
the same. Right. So there's still valid. Okay. And it's because the distances are preserved.
Right. It's the the distance that preserves the semantic meaning that gives you the semantic
relations. Right. And the idea is very simple. So if the same word, or rather if two words,
appearing in the same context, they have to be defined by similar vectors. And this also means
that if the relation between these pair of words and relation between these pair of words is similar,
the difference between them in terms of vectors will also be similar. Okay. So this is why you can do
word arithmetic and say the vector for Italy minus the vector for Rome has to be equal to the
vector for France minus the vector for Paris. Right. Right. So if you have three of these,
you can calculate the other one. Right. And basically, it's one of the ways they use to calculate
or to measure how good the embeddings are. They use this called analogies. Right. So
Okay. Paris is to France's Rome is to and they'll give you Italy. If you look for the vector that's
closest to that difference. Okay. Interesting. So one of the questions that, well, two questions
come off from me. One is if the context is defined by this narrow window, you said it's usually
used two words before two words after is that example of two words in the in the official implementation.
I think the default is set to five, but it depends. You can vary it. It's an input parameter. And
also in practice, if you're going to the details, when you're you're pre-processing the corpus,
you will sometimes remove some words because they're too common, kind of like what you do with
stop words. Right. And that effectively changes the size of the window. Right. Because you do this
before you calculate the context. So it's almost as if sometimes your windows are a bit bigger.
Okay. And that is because you remove the word. Right. So you're catching more information.
Okay. So the question is, do folks experiment with a lot with bigger windows or is it possible
to do this on the entire corpus and get, I guess in my mind, what I'm hearing is the word
relationships are only relative to this very small window. And wouldn't we have, wouldn't the
vectors capture more information if we were somehow creating them based on bigger windows or the
entire corpus? Is that the right way to think about it? Yes, and no. So if you make the window too
big, you're basically including information that's not relevant for the word. Right. You can have
a very long sentence. The word at the end of the sentence doesn't necessarily have anything to do
with the word at the beginning of the sentence. So you're trying to keep nearby words that help you
define what that word means. So adjectives verbs that directly relate to what the concept is.
So what the word is in particular. Right. I guess I'm thinking about it in the context of
like running TF IDF on a Wikipedia article. Right. If I'm looking at a Wikipedia article that's
talking about neural networks and CNNs and RNNs and, you know, artificial neural networks, all
these, these are all related terms. And I'd want to capture that relatedness. But they may be in,
you know, very different. They may be far from one another spatially in my document. But I still
want to capture that context. Can I just not use word to effect for that? You can. So basically what
you're doing is you're scanning through the entire document. Right. Right. And and embedding the
that you learn for each word as to do with all the contexts it appears in. So it's not just one.
So I can give the example of artificial neural network. Right. Artificial will appear next to
neuron in this context. But in a maybe a few paragraphs later it will appear next to intelligence
or to approach. Right. So that means that artificial will be defined by all of these contexts.
Mm-hmm. And do look very differently than the word maybe like convolutional that always appears
next to neural network. Right. And it doesn't appear in other contexts. Right. So it does take
the whole information of the of the corpus into account. But if, for example, I'm running it on
a Wikipedia article on neural networks. And there's one section on convolutional neural network. And
then another section on RNNs and another section on LSTMs. Since those individual terms are separated,
by these sections, will it capture those relationships? Yes. Just the neural network part or...
Yes. So one thing maybe I should be clear. So when I say corpus in this case, I mean,
all of Wikipedia. I don't mean one page of Wikipedia. Okay. So these are very large corpus. Got it.
And the reason why you use a very large corpus is because you want to learn what is the meaning of
the word in a sense. Okay. So this is what you're trying to capture. It's not necessarily what does
this word mean in this document? Got it. So it's more of a it's a more generic thing. Okay. And this
is why actually people have started publishing high quality embeddings. Google has published some
Facebook as far as the trend on billions of words or rather corporate that are billions of words
long because they're trying to capture what is a exact meaning. So you can use, for instance,
these vectors. One very simple application is, for example, for queries and biguation. Because
you know the word, you know, what the vector is, you can look around that word to see what
are words that are related to that one. And maybe you can show results that use those other words.
Okay. You can use the vectors and this translation invariance distance preservation as a way of
mapping the word, for instance, to a verb version of the word or a noun version of the word
or a past tense version. So you can use all of these relations in a sense, all of these linear algebra,
as a way of getting more knowledge about what the text is. Okay. Interesting. So the other question
I had was the length of the vector. How do you know what the right dimensionality is for these
vectors? As far as I know, there is no well-defined way of measuring what is the optimal size.
Okay. In practice, people tend to use dimensions between about a hundred and five hundred.
Okay. Which is relatively small for corporate or other dictionaries. So the number of individual
words of the order of hundreds of thousands. In a sense, what one of the things you're also doing
is you're doing very much a dimensionality reduction. You're mapping this from this very large
high dimensional space, which dimension is a word into this very small space in a way that
is still preserving the meaning of the semantic value of the words. Okay. Okay. Do you recall the
show that I did with Francisco Weber? Maybe. It was from a cortical that I only talked about
the kind of neural representations. Yes. I actually remember that I wanted to look into that
more carefully. Okay. It's explicitly mentioned that this is actually related with this type of
vector representations, because yeah, it seems that each word is actually represented in different parts
of the brain at the same time. Right. So it's something I'm curious to look more deeply into it.
I tried looking at their website, but I couldn't get the technical details to it. Okay. I was curious
if you had looked into that at all. And if you were familiar with that model and and it's
thought you might have on. I don't know the details. I found some things online, but it seemed to be
more marketing oriented. So there wasn't like the scientific articles behind it. Okay. So embeddings
has been around since the 50s word to back now is a did you say 1950s something? So 95s is the
is the idea is this idea that the meaning of the word is coming from the context appears. So this
got the distribution of the hypothesis in linguistics. Okay. We're embedding some so I think they've
been around for maybe 10 years, maybe maybe a little bit more. Okay. In practice or like in
wide adoption, they became very popular and get a lot of attention with word to back when
we were published in 2013. Right. But I might as me call off if I'm not mispronouncing his name horribly.
Okay. Okay. And so, you know, we're a few years into word to back and now there are a bunch of
other to Vex. Right. Have you looked into any of those as well? Yes. So there's some that are
very interesting. There's one for instance that is DNA to back and you try to find embeddings for
sequences of DNA. Okay. And see how that might relate to protein structure and
genome organization, which I find fascinating. There is one by Eurel Laskovek in Stanford
where it's called note to Vex and it tries to use this note like back in a graph where it's
basically trying to do that to find embeddings for nodes and graphs as a way of measuring
relations between nodes. And basically the way it does it since it doesn't have a sequence
of words. It doesn't have a sequence of nodes. So it basically runs a random walk process on the
net on the node. Okay. And that generates the sequence. And then based on that sequence,
then you can treat each node as if it was a word. It appears next to other nodes because there's
some how it's the neighborhood. And that can and from there you can define. And it's from that
is able to capture some of the structure of the network. I wouldn't. So the idea is that you're
I guess I would have thought that that a graph is its own kind of representation of all this
information. It is and it isn't in a sense right. The point is it's not necessarily easy and this
is a one on problem to compare graphs. So I'll give you a graph and tell you the nodes here
are words. I give you another graph and I tell you the nodes here are people. So you can't just
match the labels directly. Right. It's very hard to see if the structure of the graph is the
same. It's actually the total permutations and all the things. So if I'm not mistaken, I think
that can be complete problem. Okay. So people have been trying this idea of graph embeddings
for a while. Okay. When you try to map the graph into some set of points that does not depend
on the details of the graph or the labels that you give to nodes or anything like that. Okay.
Or any permutations you do. Hmm. Interesting. So I've been fascinated by basically how
versatile this idea of world to back is. Yeah. Are there others? I'm trying to I'm
meta-loss for I know I've seen it. It seems like a dozen of these different X to X of late.
But it sounds like anytime you have a space with some complex structure, this is a tool that you
can use to allow you to compare both individual points within that space to other spaces. Is that
a general way to think about it? More. It's more whenever you have a sequence of tokens. Let's say
you can use it as a way of finding a representation of these tokens that then you can use to find
the relationships. Okay. In this sequence. Right. So it works very well for text because you have
sequences of words. Hmm. Like in the case of DNA2VAC, it works very well because you have a sequence
of nucleotides in DNA. Hmm. For node2VAC, you have a sequence of nodes that generated by these
random processes. This random walk process is on the graph. And they actually tried different
definitions of the random walk, different rules. And so that that's somehow able to capture
different aspects of the of the network structure. Hmm. Interesting. And the sequences have to have
I guess the the relationships between the tokens and the sequences are defined by the corpus.
I guess the I was thinking of I wonder if you can do like transaction to VEC and like do word
of embedding on the sequence of transactions and use that for fraud detection or something like that.
I have actually saw something like this yesterday on medium. I didn't I didn't really because
I didn't have time. But there was literally I don't remember the details. It was really something
like stock to VEC that somebody published on medium yesterday. It's on my reading list,
but I have not read it yet. Interesting. Interesting. I don't know how they're doing it. I just saw
the text. Okay. But notionally, there's there's something in there somewhere. Yes. So in principle,
every time you have a sequence, you can do something like this. Yeah. I've also been getting more
into learning about blockchain and how that all works. Yes. I'm wondering now if there are some
applications to that as well. Possibly. I have not seen anything with blockchain using this.
I've looked in detail at blockchain a couple of years ago. Those interested in basically these
transaction networks. It might be possible to do something. I said, I haven't seen it yet.
Interesting. Maybe an idea for some of your listeners. Yeah. Yeah. What are some of the most
interesting or what are some of the interesting applications you've seen of word to VEC and friends?
I've fascinated by basically how much power these word vectors have. Because they are capturing
these semantic relations, means that you can use them for disambiguation, query expansion,
analogies, like the original metric that they used. So basically, they're a very powerful tool to
map text into a vector representation or to an numerical representation. The then is very powerful
in how you can manipulate it. Because everybody knows computers have a hard time understanding text,
but they're very good at understanding vectors. So here you're mapping text to vectors. So you're
making computers' lives much easier. So people have used this for machine translation. Because you
can translate vectors into different languages. Then you align them. Because of relationships
have to be the same mostly. And then you can use them basically too much. So you find the word in
embedding in one language. You find what is the most similar word, not similar vector in the
other language. And you can find the translation. Also, an idea that came up in the Francisco
Webber conversation. So yeah, we both need to dig into that one and try to figure out what they're
doing. That's different than regular embeddings. I mean, and then there's that variation. There's
also glove, glove, glove, glove, glove of vectors that tries to do a different formulation.
Another very interesting application, which is actually the reason why I started getting
interested in word-to-vec in particular is this paper I saw, which is actually tracking linguistic
change over time. So they train using word-to-vec. They train word vectors using Google and Gramps
data for different decades. And then they align the vectors. And then they look for the words that
are changing over time. Oh, that's interesting. So you can track basically how the meaning of a word
is evolving. Is there a metric for measuring the, I guess, like the dispersion of the vectors
in a given embedding? Like I'm wondering if in that example, where you train an embedding on
this engram data, can you look at something analogous to like the standard deviation of the words
or like the spread of the degree to which the not really I don't think? I mean, if you look at the
entire set of vectors that made itself, I don't think so. Because what you're doing is you're
starting with a bunch of random vectors. Then you're slightly adjusting them. So that specific
pairs have specific relations between them. And that specific groups have specific relations
between them. So they're more or less at the same distance from each other. But all of these
groups and all of these words can be arbitrarily rotated with respect to each other. So I suspect
that if you just get even very high quality embeddings and you just start measuring distances between
them, it will look random. Because you're putting in too much noise. The distance between, I don't know,
bed and blue might, doesn't necessarily mean any material. But then more generally, there's not
a set of statistics or metrics or things that are relevant at the aggregate level for the embedding.
Not that I know of now. Usually what people do is they look for specific relations between
specific words. So they do this analogy problem. Yeah. Yeah. Because that's kind of how you check
that what you're capturing is what you think it is. They're actually capturing the semantic
meaning of the words and the semantic relations between between words. Why do we end up using
neural nets to do embeddings that seem like pretty basic math that you can do like more
deterministically? Yes. So I think that's one of the one of the motivations for people to do
to invest in glove and glove is the reaction for global vectors. And this is I want to say it's
coming from Stanford. Okay. And basically they try to do it basically in a deterministic
way, in an optimization process, defining an optimization process by specifically saying,
I want factors that where the relationships are given in this specific way. Right. The reason
I think one of the motivations for using neural networks for this is because now you have very powerful
tools to optimize and train neural networks in large scales. So you can use all of that machinery.
Because when you actually look at the mathematics and the network structure it's actually using
it's actually very simple. So it's an extremely simple neural network. Right. And where it's mostly
vector multiplications and then a soft max at the end with some exponentials. So it's nothing
particularly sophisticated. If you look at networks like ImageNet or AlexNet or something that have
dozens of layers, they're very complex. Right. So you have all the technology to develop for
these very complex things. This being applied to something that's very simple, so it makes it
very efficient. So single layer, the matrix multiplications are just applying your weights coming in
and coming out and then remind us what softmax is. So softmax is just an optimized way basically
to calculate what is the maximum value of a vector. In a very simplified way, while at the same
time basically turning a vector of numbers into something that's normalized. So the only thing you
do is for each element of the vector, just take the exponential of that value and then you divide by
the sum of all the exponentials. That's a softmax. And what that means is basically makes any value that
is slightly larger than the others will become much larger. So it's easier to capture the maximum
value. And then of course, there's many optimizations on top of that. That's the general idea.
Because you don't necessarily want to calculate all the exponentials for the entire vector.
Because these vectors are basically one hot team bedding of the words in the in the corpus.
So this can be 100,000 or a million words. Okay. So you don't necessarily want to have to do that
at every iteration. So then there's hierarchical softmax. There's all sorts of tricks to train
optimizations on the more like computational tricks. But the concept is very simple. And then there's
of course optimizations that people apply to it to make it more efficient and more robust.
Okay. And so what's the relationship between the one hot encoded, 10,000 dimensionality vectors
and 100, 500 vectors. So like I said before, the computers have a hard time understanding text,
but they're very good at understanding numbers. Yep. So what you do is you map in the beginning,
the first approximation is you map words to numbers. So just have a dictionary. This is word number
one. This is word number two. That's your one hot encoding. And that's one hot encoding.
So it's just the way you represent the world so that then you can manipulate them
numerically to so that you can calculate these vectors. So you I'm imagining that you're very
first step. Then your your input layers and your, well, it's just one layer, but your inputs,
you're sending in the inputs to the hidden network. And then it's the one hot encoded. So basically
corpus, you have the input values here. And these are, let's say in the case of skipgram,
this is just the context word that you have in them. And this would be if your dictionary is
10,000 words, this would be a one hot encoding, 10,000 dimensional vector. Right. This gets fed
into the hidden layer in the center, which is the dimension of the embedding, say 300.
And then from this hidden layer, you're trying to calculate the context. And the context can be
depending on the window size, can be, let's say, 10 times the size of the dictionary.
Okay. So it'd be 10 times 10,000, so the 100,000, because they're trying to predict 10 words,
5 words before 5 words after. Okay. So then you've got your, so dimensionality 10,000 or so vector
coming in your network is your, the dimensionality of your hidden vector, that's your hidden layer.
And then the output side is 100,000, it's like 10 times 10,000. If your window size is 5,
you're trying to predict all the context from the single word, then you're trying to go from 10,000
to 300, to 100,000. But you're not really using the 100,000, you're just using that as kind of
an optimization to get at the weights for the 300. And that's what you use. Yes. So basically,
you can think of over to back actually as unsupervised learning problem, because you're feeding it,
you're feeding it, the inputs and the outputs, so that you can learn something about the system.
So you're not really in practice, you're never really trying to predict the output. In the set,
I guess you could use that if you're trying to generate, use a generate text without
anything unusual. Okay. So you're trying to basically see what the network is learning from the
text that you're giving it, and you're giving it the word and the context, and you say, okay,
figure this out, figure out what is the right representation that from this input can generate
this output. Okay. And then in the end, what you're interested in is actually this internal
representation, this wording beddings, inspectors. Awesome. Awesome. This has been super helpful.
Like, I feel like I finally understand it. It's easier to explain with pictures and drawings,
but yeah, for folks that are not in the room, we're not moving our hands around and drawing on,
you know, layer in the air and stuff like that. I can send you a link to my slides. Actually,
I was actually just going to ask, like, if someone wants to learn more, what's the best way for them
to learn more? I mean, all of these papers are online, you can find them easily. I'm preparing
this tutorial for O'Reilly AI in San Francisco in September. Okay. I will post all the slides and
all the code in my GitHub. Okay. Right now, there is already the slides for a shortened version that
I presented a couple of weeks ago. So I can, I can share that with you. And then eventually,
after O'Reilly AI, I will update it with all the, the newest stuff. Okay. Great. Yeah. So,
send that over and we'll get that in the show notes. Excellent. Yeah. Great. Thanks so much for
stopping by. Thank you for inviting to the pleasure. Awesome. All right, everyone. That's our show
for today. Thank you so much for listening. And of course, for your ongoing feedback and support.
For the notes for this episode, head on over to twimolei.com slash talk slash 48.
If you like this episode or have been a listener for a while and haven't done so yet,
please, please, please take a moment to jump on over to iTunes and leave us a five-star review.
We love reading these and it lets others know that the podcast is worth tuning into.
One last note, you've probably heard me mention Strange Loop. A great technical conference held
each year right here in St. Louis. We're a bit over a week away from that conference,
so I encourage you to check them out. It's thesestrangeloop.com. I'll be there,
let me know if you'll be there too. For more info on any of these events, check out the show notes.
Thanks again for listening and catch you next time.
