All right, everyone. Welcome to another episode of AI Rewind 2021. Today, we are joined by Zachary
Lipton, an assistant professor in the machine learning department and operations group at Carnegie
Mellon University to talk through all things machine learning and deep learning. Zach last joined
us on the show for the 2019 edition of Rewind and I'm super excited to have him back once again,
Zach. Welcome back to the Twimal AI podcast. Well, thanks for having me, Sam. Great to see you again.
It is great to see you again. I think the last time we physically had the opportunity to hang out
was also 2019 in Vancouver. I think that's probably a story shared by a lot of folks in our field,
like that was the last opportunity that folks had to hang out in person. Right. How have the last
couple of years been for you? Oh, man, it's been a good full. You know, I'm not going to pretend it's
all been smooth, but I mean, some things are nice. Like, my students are great and I think how
been, I think it's not very easy for everyone like some people, some people got sick, some people
lost, some people didn't get to see their family for a couple of years. I feel like it's a
weird thing where people managed to be startlingly productive or at least, you know, maybe I don't
want to shame anyone who doesn't feel productive. I feel like from just my feeling in my life,
I feel like people are, I maybe it's just kind of like CMU culture design. I feel people have been
really, yeah, like locked in and researched, but I think there's a kind of like emotional
wear and tear of just not seeing anyone, like, especially like some folks are like living by
themselves, you know, and then like when they're quarantined, it's not seen in another human for
six months. And I and for others of us just like catching up now. So it's been, it's been a little
bit wild, but you know, interesting on the research side and you know, we got a puppy. So like
interesting personally. Nice, nice. So as I mentioned in the lead up, we are here to review the
year in ML and deep learning. This is the third rewind that we'll publish. The first couple were
an NLP in computer vision or computer vision and NLP in the order that they were published. And
so far a couple of key themes have emerged. One, which was common in those first couple of
episodes is this idea that as John Bahan and put it, NLP eating machine learning, kind of like
in the same way we would say, you know, AI eating the software or what have you. The idea that
computer vision is adopting transformers and things like that. You have echoed one of the other
observations that John made in that NLP conversation and it is that particular point is kind of a
slowing down of the field and a little bit of a respite from that kind of breakneck pace of
of change that we were experiencing for a while. So maybe that is a place for you to jump in and
riff for a bit. Yeah, I'm happy to riff. You know, like just you know, maybe I may be like
choking a contrarian or something. I'll start by, you know, pushing back. I want to think it's
kind of interesting of like, you know, that phrase of like NLP eating ML is kind of cute because
it's sort of, well, among other things, right? Like in some sense, there's like the
deline for for the long as time for the last seven years. It's sort of been machine learning
eating NLP and that like if you look at the seven people going into like sort of NLP oriented
like Radpar, there's a point where like NLP sat really close to like, you know, there were like
NLP and computational linguistics like sort of two sides of a coin and they sat not so far from
their like philosophers of linguistics, whatever. And now you have a moment for the last however many
years where the the median person in NLP knows absolutely nothing about language. It's nothing
interesting to say about language that couldn't just as easily and it's hard to say nobody or that
there isn't anyone with something interest, interesting observations or interesting experiments
that are that are kind of hitting off both sides, but say that like the center of gravity of the
field has moved to this way that almost there there's almost no L in NLP, you know, it's just like
it's just sort of like, you know, a set of tools where if if the like commercial demand was more
on music than on NLP, you would use almost, you know, conceivably like the same set of mosques,
all they care is just like a sequence of tokens, like a very generic sort of approach. And so in
some sense, it's sort of just been like deep learning eating NLP has been the story for a while,
and I think that like this version of NLP eating ML is, well, I guess one, they don't really mean
NLP, but really more it's like other application areas. I think, you know, they don't really mean
NLP as much as the thing that eight NLP, which is Transformers.
Right, whatever is like that like new organism that displaced NLP is now coming across,
but right, it's it's more like, you know, there was like a discipline of computer vision where
you had people that like the typical person who was in there knew something about like the physics
of light and optics and like was sort of like doing this sort of like, you know, that that was the
angle. They were like a real expert on like the the modality of vision and the person NLP knew
something about language. And I think they both got eight by deep learning in such a way that,
you know, over the last seven years, ideas that would hit on one side could very easily
poured across to the other. And from most of that history, I think it's hard to say precisely why
if there's some reason or if it's just sort of the the order in which things happen, like because
like the those breakthrough image net results are really caught people's attention where we're
envisioned first. But I think from most of that history, it's been very one dimensional,
like very one directional. I think things going from, you know, mostly in the direction of
computer vision to NLP. And I think if anything, this is not really NLP eating vision, but it's just
notable that maybe one of the bigger things happening in vision is crossing in the other direction,
like contrary to the pattern. But yeah, you know, broadly on the like things slowing down pattern,
I think I've been I've been noting this and writing about this for I don't know, maybe like four
years now, but I think there's those definitely a moment where like if we were to look through
the history and say like 2012 image net 2014, like sequence to sequence models, 2015, Alpha Go,
2016, 2017 big advances and like the kind of like perceptual quality of like generative models,
2017, transformers, 2018, Bert. There was a kind of change that, you know, I don't think these
are necessarily all profound in a sense of like some big intellectual move, but they are like
qualitative changes in capabilities. They it's like there's a big move in a sense of like what
set of problems do I think are best tackled with these tools and what sort of performance can I
expect from them? And a big difference in the sense of if I'm a practitioner in the field and
somebody hits me with a typical like industry problem, what's my go-to tool? And if we look at like
2021 and now we're saying, well, if someone hits you with a classification task, what are you
going to do? If I'm just going to use a resonant from 2014 and 2015, you know, someone hits you
with an LP task, like basically fine tuning Bert or a Bert like, you know, very Roberta Alberto,
you know, whatever it's like, you know, right, like there's this moment where, you know,
like I think that in some sense, I think it's okay in that like research is not now need to like
start looking somewhere else other than just like what if I tweaked the architect here a little
bit? Like there's a and as I was telling you like when we were ripping before, is that I think
a research is there is some aspect that people like grow up it around in the dark looking for,
looking for a way in, it's almost like they're like swinging out a pinnata with a blindfold on
and trying to find like where's where's where's there an angle that like where's there something big?
And I think you want to have a lot of researchers in that mindset of like I'm looking for the blind
spot, like I'm looking for the big prize that other people aren't looking for. And look,
every now and then someone gets, every now and then someone really lands a mark and the pinnata
rips open and a bunch of candy falls on the floor. And then everybody rushes on and there's some
period of time where nobody's worried about like nobody even knows where the bat is, everybody's
just picking candy off the floor. And you know, I think where we saw that period of like people
finding all this, you know, every, it wasn't like you didn't need a big intellectual breakthrough
to have a big, to have an impactful breakthrough in all those years. And I think we're getting to
the point where like, you know, there is some amount of stagnation because like most of the good
candies can take up and people are like looking at the old grimy like moldy stuff that's like,
you know, maybe there's still something in there. It's like it's like the like the sloppy seconds
on the the research, you know, pinnata. With that analogy in mind, you know, where should research
be swinging the bat? Do you have, is that a, is that it, you know, it's a crystal ball kind of question,
but what is your intuition tell you where opportunities might be?
Well, I always look for, you know, what, what is it that we actually care about? When people
are selling a story, an aspirational story about, you know, I'm not like a mathematician first,
I'm not just like, what's a hard problem? Like, which is all for that reason, you know, I got into it
too late, you know, I kind of back into it from like, what's the, like, what's the dream?
And if you look at like the dream people are selling people, even people with like existing
companies right now, the claim they're making, like if you looked at like IBM Watson, which
went up in flames, if you look at the claims they were making, like, what are we going to do for you?
It's like, we're going to, you're going to make better decisions, you're going to provide
personalized health care, you're going to help people to, you know, have better health outcomes
and otherwise would have without our AI. If you look at this kind of stuff, you know,
what are people selling? What are people hoping to actually achieve? And then you look at like,
what are people actually doing? And if I like, I always kind of look back and forth between those
and say, like, what's, what's like the missing part? Like, if you actually want to realize the dream
of what, you know, people have to seem to want, what's, what's gone? Like, what's, what's not even
being addressed in a, in a mature way? And so I think one, one thing that sort of jumps out
is that everybody's sort of premise, you know, everyone says like these things are good,
it's always based on some notion of like accuracy or the return of an RL system as evaluated on
some fixed static environment. And, and then you look at what people actually do and it's like,
they're taking some model, trained in some context on some set of data and deploying a crop in
some different environment, which is changing in unpredictable ways. And where the whole environment
is not just changing like in a kind of benign or passive way, often it's changing in direct
response. So like, you know, like think about Google search, right? You deploy Google every single
time they tweak their algorithm. What's the first thing that happens? And it's like all the message
boards light up and all the SEO goons are like, oh, like SEO change the algorithm. Now you need to add
this keyword. You need to do this. And I think that ML doesn't address that kind of stuff. I might
say ML doesn't. I don't mean nothing that we aspire to in a hell, but I mean like the, the main
thing, you know, the main thing that practitioners do, the toolkit, the mature one, the like, you
know, I know how to use PyTorch and train, you know, ResNet and ResNet and whatever, whatever.
That world, it's like completely set in an environment of like, I train a model, evaluate on a,
so like, I ID holdout set. Or even if you evaluate on some kind of challenge set, it's not like
with any coherent principle for why you should expect this model to do well on that challenge set,
or why you should think the performance on that challenge that's representative of what you should
encounter in the world. So I think performing in a dynamic world, making decisions and not just
predictions, right? Because everybody's sort of saying, ultimately, if you think you're going to
make money off of this, or you think you're going to affect some kind of like, societally
beneficial outcome by using AI, if you think you're going to do anything, then ultimately it's like
what you're, the claim at some some point, what you're hoping to do is guide some kind of decision
or automate some kind of decision, right? You're actually hoping to have an outcome, not just
to like be a passive observer to the world and make accurate predictions about what would happen,
were you not to take any action at all? And, and this kind of setting of like actually providing
guidance for what you should do in the world is, is we are, you know, it's, it's a thing like,
yeah, there are people working on causal inference. There are people who are trying to bring
reinforcement learning closer to the real world by, you know, maybe incorporating some ideas
and causal inference like to think about confounding that might exist in the data to be able to
build models and off policy kind of way, you know, so that you're not just saying I'm just going
to deploy some randomly acting system in an important application and have it stuck for two
million years until it learns. But they're relatively mature and then they get relatively
little attention. You're going to look at like, you know, what are people, you know, what is,
you know, I'm not going to beat up on our buddies in the press, we say, what is like, you know,
Cade going to write a big article about in the New York Times, it's not typically like the,
the, the, the, the slog of like scientific advances and making robust machine learning or,
um, you know, off policy RL or something like this, it's, it's, uh, you know, there's a big
neural network that, you know, is nine trillion parameters and a billion dollar investment from
Microsoft and this kind of, and, and so I think right, um, decision making, uh, robustness and
dynamic environments and, and actually addressing certain societal desert or audit, you know,
people have sort of noticed the problems that arise in terms of the, um, ways sort of,
like AI system can affect whether it's like, um, on ethical sort of outcomes if you plug them
and night evenly into certain decision making systems, but the sort of like,
field of actually like developing systems that could, in some coherent way,
align with societal desert is quite primitive, right? So there's like a recognition that there's
a problem, but we're, we're very early stages on getting towards solutions. So I think that,
to me, like, these are the areas I think are, are more interesting, um, you know, in that like,
like, if you can get, if you can squeeze half a point out on like, you know, all the NLP benchmarks by
like making a slight variation on birth, you'll get a lot of citations that everyone will use it,
and they should, and it is useful, but it's, I feel like not, it's, it's like a direct, it's a,
a slight change in degree, it's not a change in kind. And so I think, you know, when you,
when I look at it the field, I think the, the luxury of being an academia, the reason of being an
academia is to think that like, I don't have to just think, how do I do epsilon better than someone
at the same crop we're all doing tomorrow? But like, what, what actually is something that,
you know, addresses, you know, some problem that nobody's even engaging with intellectually right
now? It sounds like your answer then to the where to swing the bat is, um, and getting closer to,
you know, real world problems that that folks are having. And you mentioned a lot of different
elements, um, you know, I heard some aspects of domain generalization than there. I heard, um,
you know, aspects of even like user interface, like how you're presenting the information,
heard aspects of fairness in there, but broadly, it sounds like you're kind of also calling
into the question kind of the simplification that often happens in research of problems that
removes them from, you know, all of the constraints and fuzziness of the real world. Yeah, and look,
it's tricky, right? Because everybody's, everybody thinks they're doing that? Well, it's more like
everybody's got to choose the focus on something and, and to focus on the thing they want to focus
on, they got to compromise on something else. Um, and, and it's not that like one thing is right
or wrong. Like, I don't think it's wrong if there's people out there building bigger language
model. I don't think that's fundamentally wrong. Um, I think you got to be like, um, you know,
maybe like you got to use a brain to think about how, how, how, what kind of claims I can make
about these things or, or how should they be used in the real world? But I think like, look,
it's interesting to turn that knob and say, what if I make this big or what, you know, what
happens? Um, there, there, there's plenty of work to be done. Um, you know, like, like one
cutting on like trade off that you often have is that, um, you know, you have like, if I want to
get close to what real data looks like, you know, and I want to get close to things I can actually
do in a bunch of domain, often like all that's available is, you know, uh, data is, um, like,
there's, there's a way that like predictive modeling and like the status quo, you know, is closer
to the real world and that it, it touches real data and it gets within its like narrow aspiration
of like, just predict well on like, IID data, like under a naive assumption of how the world doesn't
change, it's able to do that on really complex, high dimensional real world data. Um, on the other
hand, what you give up when you focus only on just that problem is any kind of consideration of,
you know, I think people just only think about how do I get better predict, you know, building
predictive models, um, is, you know, like, they're getting close to like the dealing with real data,
but they're asking a very narrow set of questions about it, which is like, how do I get higher accuracy?
Then on the other side, you know, you have folks say, for example, trying to get at like, um, fundamental
questions about, um, what sort of like causal query is I can make. And very often in order to flesh
out those questions, now they're taking on like a more ambitious set of like, kinds of queries
that I can ask, but in order to make progress, like understanding the fundamental form of those
questions is often starts with, well, I got to analyze like fundamentally one of these questions
even answerable from the data sets that I have. And in order to like maybe get some of that
analysis to go through, I have to make some simplifying assumptions about the form of the data,
like, I assume that the whole world is linear and it's not high dimensional and it's not whatever.
So, you know, you have plenty of people who are, who are doing, you know, work that's like really
more like ambitious and expansive on the front of like the kinds of questions I can ask,
but they're making really simplifying assumptions, uh, in terms of like the source of data I have
and the number of variables I have and, uh, not worried about that, but I mean, that's to compromise,
they make it as other folks building predictive models and trying to get close to like, do something
that works on real data, but be naive about the kind of questions you can ask and, you know,
not worry too much about just how limited is like what you could do with those predictions or
their power to guide decisions in the real world. And, and then I think once you have that kind of
tension of like, okay, everybody's looking at something and not looking at something else,
you know, I think the question you have is like a research community is like, are you overleveraged
somewhere? You know, I think oftentimes people, there's like a naive form of a criticism,
which is like, oh, this thing sucks and this thing is good, but like there's a more mature version
of it, which is like, we're way overleveraged on this thing and paying way too much attention
to this thing and neglecting these other things. So it's like, you know, like more matter of like
moving the needle. It's not that like nobody should be building a bit of bigger language model,
or or tinkering with architectures, but it's sort of like, okay, like we're at a point where
we're not getting nearly as much juice per squeeze doing that. Why do we have 99% of the community
engaged in this? Why do we have so many papers that are being submitted that most of which know,
you know, are not actually contributing anything either as an idea or as a result? And so,
yeah, I think we're sitting in like some funny terrain like that. So were there notable
papers or research advances that you think kind of poked at some of these issues that you're
raising or you think are swinging the bat in the right direction? Yeah, I think there's
this very weird climate now, which is like, I think for a lot of these questions, you have sort of
like a growing recognition that there are problems, but then you have like a subset of people that
are just kind of like taking advantage of the way in which the like peer review system is like
overtaxed and scattered and like sort of just using the language of those problems, but not
actually addressing them. And I think you see this in all of these and you see this in the
fairness literature. I think you see it in the robustness literature. I mean, you see in the
causality, like in that people submitting papers that sound like they're addressing causal problems,
they're not actually people just saying this model's robust in a way where it's like, by the way,
you can never just say a model is robust. Like if you state nothing about the ways in which the
environment is allowed to change and, you know, there's no such thing as like general robustness,
right? Because I can pose two different assumptions about the world where in one of them when the
environment changes, you know, this is what I should be doing. And the other one I want to change
is that's what I'm should be doing. I have no way of discerning which world I'm in, right? Like
a class would be like, do I live in the labelship assumption? Like if I make that assumption,
that like that distribution of categories is changing, but the class conditional literature,
what does it, what does, you know, COVID versus not COVID look like is what's not changing,
but the prevalence is changing. First of all, I assume that like the covariate distribution is
changing, but that the label, the conditional, the probability of the label given the input is
different. I might have no way of discerning whether I'm in world A or world B, but, you know,
one thing is the right, like the robust model in this setting at, you know, should do this,
and the robust model in that setting should do that. So you have like a set of people that are
just kind of doing the deep learning thing, which, you know, like prediction lets you get away
with it, like let me throw spaghetti at the wall, because I get to evaluate on the whole
doubt data, and like how well I'm doing is identified. So I don't have to be able to state in
terms of any principle. If you get to like a causal effect, you don't get to observe the causal
effect. So it's like, if what you're doing doesn't actually identify the causal effect, you could
call it a causal, you could, you could, you could just like use the language of causality,
and it'd be quite in favor of not actually like addressing causality in any kind of sound way,
and fool a reviewer, but not necessarily be doing it. And so, you know, I think the thing that,
like a lot of these other problems are kind of more foundational, like they're not problems where,
like we know how to evaluate systems, let's have people try stuff and whatever their problems were.
So I'll give you an example, like in, you know, the distribution shift world, I mean,
there's, I think a handful of things people are doing that are a little bit more interesting
or sound or actually giving a path forward. There, there's a group at Stanford, some of Percy's
students, Shiori and Pongway among down, put together this really expansive benchmark called
Wilds, and it's a, like a collection across a whole lot of different application domains of a whole
lot of different settings where you have some kind of subpopulation shift or some other kind of
distribution shift, and it provides like a sort of unified resource for a whole bunch of settings.
Again, like you still need to have some kind of, you know, you can't just like use the data set
and say, oh, I tried this thing in this one domain and it generalize well to these two others,
therefore it's robust, but at least it gives you some, like unified resource for asking a question,
you know, like if you compare to the world where basically people are just saying, like I have
pictures of eminus images and then eminus images on funky backgrounds, I think it's like a
big advance towards like a nice sanity check and putting people in touch with the sorts of
problems that are arising in the real world. There's one formulation of these domain
adaptation settings is that there's a version called domain generalization and here it's like
sort of saying I have like a bunch of different environments that I collected data from and now I
have, you know, I want to generalize well to target environments, possibly using the fact that like
I can look at the different source environments that I've had and they're actually marked out of
different environments. I could try to see something like what's stable versus unstable across
environments. And there've been some interesting papers. So by the way before we met, I ping some
of my students be like, what do you think are some of the interesting papers so I want to give some
credit to my students who are now like the extension of my memory. So my students are appointed
out. There's a lot of interesting work where you have a whole lot of methods that are proposed
but it turns out that if you set up a really rigorous baseline and listen papers, some from CMU
from my friend Alon Rosenfeld and is advisor under SESCI, some from David Lopez Paz at Fair,
but where they've shown things that like for a lot of these setups, it's really, really hard
to be like really stupid baselines, like just dump the data together and just do ERM on it,
like just train on all the data together and don't use the environmental labels in any sophisticated
way. You know, in our own lab, my students are, I've been making a lot of progress on these
distribution shift problems and we have some results that we've been excited about, like among
other things, working out when you're presented with, you know, you've trained on data from,
you've seen, you have some classes you've seen before and then suddenly at test time, you have
some data that shows up from some additional class that like you never saw before. Can you actually
on the fly, look at, you know, this previously seen data from from some classes and now additional
data from some from some unknown class and identify like, oh, I can I say exactly precisely what
fraction of the new data is from some previously unseen class and even develop a classifier that
can now start predicting it so to say, oh, I think these samples have this probability of
belonging to that class. So you'd imagine that like in the context of like a model monitoring pipeline,
you'd eventually like live in that world where if the world changes in some way, the model could
go back to you and say, hey, I think with high probability, like, you know, at least 20% of your
new data actually belongs to some new class that you've never seen before and here are some examples
that I think belong to that class and then you could sort of, you know, take some kind of corrective
action if you think that the model's wrong. So that's on the robustness side. On the causality side
and I got some of these tips from my student, Shun Senua, some of the work that we've been talking
about and going over, I think there's so causality research is really exciting because it actually
gets to the question we care about, which is like, what would happen if I did this versus what
would have happened if I were just a passive observer watching the decisions get made as they
always are and and causality gives like a philosophically coherent way for answering those kinds of
questions. But the danger is that those answers are almost always predicated on some pretty strong
assumptions that like our, you know, I can learn this like the parameters of my causal model,
but the structure of the causal model is given like a priori and I know it exactly and there's
no one observed confounding that, you know, can make all my results invalid. And so there's a lot
of interesting things happening among them. There's some folks like Carlos Chinelli who just started
a faculty job in the physics department at UW have been doing a lot of interesting work on
sensitivity analysis. So if there's measurement error or if there's some, some omitted variable bias
or something, just how, you know, like frameworks for being able to say just how much would there
have to be for me to change my causal conclusion, right? So getting towards like, I'm not just saying,
oh, if I'm nailing all these ridiculously precise assumptions about how the world is,
then this is the answer to your public query, but saying like, you know, this is how far off those
assumptions would have to be for like me to like have to totally change my mind. Eric Chuchin Chuchin
is a researcher at Wharton's statistician who does a lot of exciting work in this area and
Johnson to hit me to a paper that he's doing which addresses a specific problem of, you know,
people often make this assumption that there's no unobserved confounding. And, you know, that is
such a strong assumption because it's like, even if you have the right confounder, if you just
measure it in a slightly noisy way, then there's unobserved confounding. And so he's gotten to
this formulation we call proximal causal learning. And it's like, you can allow that, okay, I have
some proxies for the underlying confounders, but they're not perfect proxies. And what can I do
in that situation? And finally, one thing on the causal inference side that I've been
really excited about is, you know, a whole lot of machine learning just sort of takes
this stance, which is like, I've got this set of variables and I've got a collection of examples.
It's like something like, you know, my data looks something like a table. Now it could be kind
of complicated because if it's like texts, the different documents could be different length,
but it's sort of the typical formulation that people work with don't usually allow for
the setting where it's like, oh, I have a collection of a bunch of different data sets and I observe
this thing in this data set and this other thing in that data set. But I feel like a lot of real
world decision making is actually governed by that kind of process. I think we've all gotten a
little bit of a crash course in this from just watching like the like COVID response, like
unfold in the public eye. And it's like, oh, I've got this data from the CDC, but it has these
features, but it doesn't, you know, it tells you how many reported cases, but it doesn't tell you
how many tests are run, you know, but oh, I have this other data from the manufacturers of diagnostic
equipment and that data actually tells me what fraction of tests are positive, not just what number
of tests are positive. And I have this other data from, you know, the local municipalities.
And so you get to these questions where it's like, if I have some question where, you know,
I think very often we have queries, especially in economics, this comes up to call like data fusion
type problems, but where like the answer can't come like directly, I have no one data set that
can necessarily answer my query, but I have a whole bunch of different data sets and it's possible
that if I combine them intelligently, I could sort of like try and relate to the answer to the
question that I have. We talk about that on the infrastructure side as well. Is there kind of
terminology evolving on the machine learning side for thinking about problems like this?
For some reason, it also calls to my graphical kinds of things in that you want, you'd imagine
some kind of connectedness in the data and the way they represented to one another.
Well, you know, in the econ world, they call these like data fusion problems and
someone who's done some really interesting work on that from like the AIML side is a researcher
named Elias Ferenboim. So he's a professor at Columbia and he was a Udapuril grad student and
now he's a prophet, he's doing a bunch of, I think a lot of the, you know, super exciting work
in this area. And, you know, he's gotten to these sort of questions where, like is this paper from,
I don't know if it was technically 2020, but I read it in 21, so we can call it 2021.
But on an algorithm, what he calls like the general, general identifiability problem.
So it's not just saying like, oh, I've got this one data set, is this thing, you know, can I answer
my causal query? But it's like, oh, I've got this collection of data sets. And in this data set,
these are variables are observed. And this other set, this other variables observed. And maybe this
data set was collected by someone doing a particular kind of experiment on one of the variables.
And this other, you know, it's like, I might have different data sets from different experiments.
They're not even necessarily just different views of like the same data. One of them, someone was
intervening in some way. But it's, if you kind of have this collection and data sets and some
underlying causal structure, now can you tell me precisely, how can I combine all these data sets
to answer the question that you have? Or, or, or I guess like, you know, with causal questions,
always the first step is, is it possible to identify, you know, the, the answer to the question
that you have based on the data that's available? And then, you know, if you can, well, give me,
give me the formula such that if I plug in the data, different data sets, I could, you know,
it would give me that, that estimate. So it's like, is it estimable? And if so, like, how do I
produce such an estimate? Yeah, so there's, you know, I, these are general, exciting areas. There's
also a lot of work happening now in causal discovery. So this is a really ambitious problem.
Because, so in causal inference, you basically say, I know the structure of the causal
graph. I know which variables potentially cause which other variables. But I just don't know the
functions that determine, you know, so it's like x, you know, x and y together influence z.
I don't know what is the function by which the values of x and y determine z. But I know that like
z listens to x and y. Like if I were to intervene on x that could potentially change the value of z,
whereas if I were to intervene on z, it wouldn't change the value of x, right? And so if you have this
kind of structure, causal inference says, well, how do I like figure out, you know, basically,
what are those functions that I can then answer a causal query? But there's like a sort of,
like in that by itself, it's super hard. And, you know, we always never agree on causal effects
because it's like, well, if you assume the graph looks like this, and it's slightly different,
or, you know, than all bets are off, causal discovery basically says, what if I don't even know
the graph that for you, or I have some partial knowledge of the graph, but I don't actually know
fully which arrows, you know, go from which variables to which other variables. So in that case,
you know, you, you ask this question of like, well, what, when is it possible to recover the graph?
And in general, you can only recover the graph up to like something called an equivalence class.
But now there's a whole bunch of other papers and I don't have all the links that I can send them
to you offline, but things really start asking questions that says, okay, like, well, if I'm able to
use causal discovery to get the graph up to like equivalence, well, now I can ask
question like, what sort of experiments should I run in what order to as efficiently as possible,
resolve any lingering ambiguities? So like, right, just the observational data might at least tell
me something like certain variables aren't connected to other variables, and I'm able to
orient for some edges in the graph, like which direction do they point, but others I can't,
you know, but then, you know, the hope of causal discovery is to be able to additionally do that.
So that's one, you know, exciting thing. And my student Charlton who's been working a lot in that
area, yes, we have a paper that gets at this question of if you get to make these decisions about
kind of like I was telling you before, if you have different data sets and by combining them,
you can answer a question, but not necessarily by using one of them alone, or even if you can
combine them to answer the question, there's still an unresolved question of how much data should
I collect from this source versus that source in order to like as efficiently as possible pin down
the causal effect? We've been working on that problem of basically, how do I like, you know,
imagine you're working at a company and you have some third-party data provider that charges you
for, you know, I participate this much per thousand examples, right? Like, you know, how would I make
the decision sequentially of like, okay, based on what I know now, which data source should I query
next and for how many samples? And okay, now I update my beliefs, I make a subsequent decision,
which I think this decision process is always going on in the background, right? Like if you're
a company that's buying data from people or going out and actively, you know, doing some kind of
monitoring effort data collections, you're making decisions on the fly about, oh, I want to collect
data from here. Oh, now I know something I didn't know before. This is going to guide my decision
of what to collect next, but we don't usually formally model that process. We usually sort of assume
the data is already there and then focus on how do you estimate, you know, something given that the
data is there. So, yeah, those are, you know, some, some are so excited about that direction.
And I think on the fairness side, I think one way that things are maturing is that people have been
posing these questions in ways that are maybe, I don't know if you're familiar with this, a
philosopher Charles Mills who passed away recently. Yeah, so he's, he's, this is great, like,
moral and political philosopher. And he writes about, sort of like ideal approach to theorizing
about questions of justice. And in, and I think it, you know, and my student, I had a post-doc
in graduate, Cena Fuzzle, who was now a professor at Northeastern, but we wrote a paper recent,
like a couple of years ago, just, just making a connection between what's going on in ML and,
and this sort of framing of ideal versus non-ideal theorizing about justice that comes from,
among other folks, Charles Mills. But, you know, he has this point that, you know, when you start
posing like a question about equity or question about justice as a sort of, like, technical problem,
and you, you make up a toy model, there's this danger that, you know, you, you sort of highlight
as, like, salient and relevant, those parts of the problem that are captured by, like, your toy model.
And you relegate as, like, not even of, you know, academic consideration, everything that doesn't
show up in your model, right? So I think that, and the danger here is that, like, if the things that
you're just, like, completely forgetting about are actually, like, the, everything that really
matters, that you wind up in a situation where you could do a lot of academic tinkering and you could
even develop, like, elegant mathematical theories, but they have almost nothing to say about the
underlying question of justice that you care about. And I think this is sort of the situation that
we've been in to some degree, and it's not to implicate everyone, but it's, say, like, the, the main
thing, right? Is that we've been posing these questions of equity in the form of, like, say,
I have a data set, say, I have a particular feature, let me just sort of start enumerating different
things that should be equal. And I'm saying, oh, it's not possible to make them all equal simultaneously.
So let's either just, like, naively pick one and then flesh out an algorithm for it, or just kind
of, like, pying about how fairness is impossible. And I think, like, what gets lost in that whole
kind of discussion is that it's almost all of it. It's, like, picking for granted, just,
I've got a data set. There's a bunch of anonymous features. I don't really say anything about
what they actually mean, or what real world processes they correspond to, or how disparities arise,
and how that consideration really bears on what is sort of the appropriate response from a
standpoint of, like, affecting justice. Like, when is it, you know, we don't look at every single,
like, we don't look at, I don't know, you don't look at, like, the major league baseball, for example,
and say, like, oh, I said, I noticed there were more players from some country that, you know,
than from some other countries, you know, relatives that are, you know, and say, okay, let me just
equalize it instead of quote a system. But it's also because I don't, you don't believe that, like,
I don't know, say some country that, like, you know, excels in basal, like Puerto Rico,
so you don't think they've been giving an unfair advantage in getting to the major leagues
or something. And so, like, this whole backstory of, like, what actually are the,
what actually do these variables mean, and to the extent that there are disparities reflected
in the data, like, where do they come from, and how do they correspond to some political,
some coherent political stance or theory that sort of makes the straight line from that,
to who has responsible to remediate, who has a responsibility to remediate it. These are,
like, fundamentally, the concerns that we sort of always have when we speak, I think,
in the law or in a broader sense about questions of justice. That's some, for some reason,
I think there's something I think maybe just about the fact that it's a new field or something
like that, but that for some reason, I've been just kind of completely sidelined or completely
as a strong word, but by, like, the main branch of fairness research. And I think there is a
number of people who are, who are doing interesting work here to try to actually ask the critical
questions. And I think, like, Lily, who, and, um, is to call their houseman, uh, or two people who,
I think, just have been, like, kind of, asking the right kinds of questions, um, for, for a while,
and, and kind of framing that critique in a way that I think is what's so rare. It's like,
both really understands what's happening in, like, the sort of fair ML world, and also really
understands the sort of context and, um, like, the people actually understand ethics and actually
understand, like, legal principles of justice and our, um, I think, able to speak from some
degree of authority to sort of what's missing in a way we're opposing those questions and tackling
them. Before, before you jump into their work, the things that come to mind for me are this idea of, um,
you know, techno-solutionism being part of the problem, like, we're trying to, you know, throw
technology at the problems that technology is, is creating for us. Yeah, we talked about that
a couple of years ago. We did. We did. Uh, and also there's kind of a nod in, in the way you talked
about the problem of fairness to causality, and, you know, when we all got really excited about
causality a couple of years ago, you know, and, uh, I think it was that same NURPS, uh, it was,
you know, where everyone left excited about causality. Like, it was supposed to be the savior of
fairness, and it was, you know, applying causal modeling to machine learning more broadly was going
the, you know, give us, you know, transparency, give us fairness, give us, uh, interpretability,
and, you know, break open all of the black boxes and all of that. Like, where is that?
You know, and I think I just, I couldn't more highly recommend, um,
issa and lilies work here that I think, you know, there's a, there's a handful worse, you know,
there's some work by Elias Barron-Boyam and by Ilya Spitzer that has, and, and before some earlier
work by like Matt Kuzner that sort of posed different, um, notions or of that, that are, like,
within a causal framing kind of like coming out of like pearls, causal modeling. So some notions of
like, you know, the earliest versions of that say something like, well, there's a lot of different
versions. If someone wants to say something like, okay, it's not just a question of is race or,
or gender or whatever is considered as protected attribute. Does it turn out to be correlated with
some outcome? We want to ask some question of like, is it what causes the outcome?
Um, and there, there's a way that like these questions have been paused, and it's actually,
you know, the causal framing is not unique to machine learning. It's actually something that, um,
I think like the legal scholarship itself often expresses things in causal terms, right?
You know, um, and before them, you know, I think like economists, for example, like, you know,
there's a famous experiments, right, where people say, well, recent, like, the resume experiment,
that I think, Sindel Millenathan and some others ran, or they, they, they randomized names to be,
uh, tip more, more likely sort of, um, uh, you know, like black Americans sounding as more likely
to be white Americans sounding as, you know, and then they, they, they send the resumes to people
and they measure the response. And it's, it's an interesting experiment. It's certainly like,
uh, uh, valuable research. And the fact that there is, in certain contexts, a difference in
the response race, like, you know, does jump out as, um, you know, problematic, right? On the other
hand, if you sent them out and there was no difference in response race, um, should be conclude in
the other direction, like, oh, there's, there's nothing wrong, right? And, and I think the answer is,
you know, obviously, different people will have different opinions and, and, and the answer might
be answered different in different contexts. But I think that there's a lot of context so much,
I think many of us or most of us would say that that's not necessarily the case, right?
Um, for example, if, uh, right, like if you, if you, if you, if you, like, if you're like, oh,
you know, like, because, you know, it took, what does it mean to just change the name, right?
Like, I could change your, I could change your name, but, um, and that one make a difference,
but if I change what college you went to from, say, like, HBCU to a, um, you know, say some,
some, some other school, and that made a difference, even if your name by itself conditioned on
everything else didn't make a difference. So, so there's this notion that's baked into a lot
of literature that tries to pose questions about, um, discrimination through a causal lens that sort
of tends to adopt a rather like narrow notion of what could constitute discrimination as like the
direct effect of some attribute, like the direct effect of gender, the direct effect of race on a
decision. And the problem is that, well, what about like all of these sort of potentially
indirect effects that could still be, you know, if I were to make, if someone were to make a
decision based on some factor that is super correlated with race and also irrelevant to the decision
otherwise, well, like, would you say that that's not discrimination? Um, and so there's this,
you know, and then there's some work by Elias Barron-Moham and Ily Schbitzer, which is,
I think a sort of step at least conceptually in a, in a more interesting direction, whereas
what they try to do is sort of, you know, if you have like a causal model over all the variables,
you could say something like, well, let me disentangle the, like, the, how the effect of some attribute
of interest, whether it's race or gender, comes to influence some outcome of interest along all the
different sort of plausible causal paths that it might take. And I could sort of attribute,
to what extent does this, you know, influencing the outcome via that variable versus via this,
via this path versus via this other path. Um, now keep in mind though, like, that sort of,
I think what's cool about it is it's like a thinking tool. Like in practice, do we actually expect
that we would have a causal model that captures all the variables of interest and actually says
exactly, we would know precisely which variables influence, like every variable that goes from
somebody's gender, you know, to whether or not they got hired, you know, we'd be able to,
like, we're going to trace like over what scale, like over the scope of someone's entire life,
we're going to, you know, build into our graph, every opportunity, right, every, every, every,
every decision, every opportunity that someone was given or not given on that account,
like, that we're going to have a graph that is so rich as to capture all of that, you know,
it seems unlikely, but at least it gives you maybe like a thinking tool as like, okay, it's it,
you know, at least I can conceptualize and step back and think about the fact that there are these.
But among other things, it outsources the normative work, right? At the end of the day,
presumably that the reason to disambiguate these different pathways is to say
someone believes that, you know, they usually put as like in the terms of like some paths
are permissible, like maybe they run through like unambiguous qualifications for the
position being hired for, versus other paths are sort of like impermissible paths because all
they're really doing is telegraphing information, but they're not actually influencing,
you know, they're not actually, say, relevant to the job qualifications or whatever the context is,
but there's still outsourcing the normative work of someone, well, someone has to go and say
which paths are permissible and which paths are impermissible, and Lily has a really sharp
critique. She also has a nice set of blog posts that are called like disparate causes, I believe,
on this blog phenomenal world, but it goes into this problem kind of critically, and among other
things, you know, getting at this question of what we call like a direct effect or an indirect
effect is partly an artifact of the representation that we have, and there are some causal questions
where for any kind of process that we describe, there's multiple different valid causal representations
conceivably, right, because you can always like zoom into a via this variable in this variable
in an edge between, you know, I can always zoom into it and say, oh, like it's not just that like,
you know, someone's college influences their internship, it's actually that college influences
this subtle decision that's made by some recruiter, which influences this, which influences that,
and so you can always zoom into it and sort of bring more into focus on whether or not someone
would like look, you know, now like a very generic question, like what is the average
treatment effect? Maybe as long as you had, whether you had a very sort of granular or very sort of
um, you know, coarse representation of some process, if they're both valid, you'll have the same
answer for a question like that, but this question about like what are the pathways taken
is sort of like, um, and are they permissible or not? Is sort of an artifact partly of
at what resolution do you zoom into this process and do you capture it? And something might look
okay, you know, if you zoom way out and you subsume a whole lot of mediators into just like an arrow,
but if you zoom in closely and you knew more about how that process took place, then maybe you
would say, oh, this isn't kosher. Um, so I think, you know, at a high level, um, I think that like
causality gives us like a set of thinking tools for thinking critically about some of these
problems, and they are maybe in some way like a partial step in the right direction.
Um, but at the same time, you know, I don't think it's like a magic bullet that like sort of addresses
all questions of, of, of fairness or justice or discrimination, and I think that often,
you know, they're, they're, that the, the sort of like model that was sufficiently rich to be able
to, you know, even if you believe that they were, like you couldn't, you wouldn't actually have,
you know, uh, you wouldn't actually be able to like produce the causal models that you could
fully resolve those questions. And I think what, one nice point that Lily makes, um, and I think
I might have been, uh, um, in joint work with, with Issa, but, but I remember one of the points is
is that, you know, I think that there's a lot of times like a danger is that it could be a little
bit of a distraction, you know, like you said, there's like heroic amount of, you know, I have to know
every single variable and every single thing and estimate every single relation. Uh, and before I
can make any kind of conclusion about whether there's discrimination, um, that there might not
actually be necessary and it's not in general what we do. I think there are situations where we can
size up, um, at, at a bird's eye view that there's, there's some fundamental like inequity in
society and, and, and conclude that we all, that we, we have some responsibility to do something about
it and that that doesn't need to be contingent upon saying that like I've exactly estimated every
single possible, you know, uh, causal functional on the pathway of every single factor that, you
know, plays any role in, uh, you know, on the path to some decision that's made about someone in
their life that that might set like a, um, at the end of the day, like too high a bar that, you know,
you kind of, um, that, that I think that I think we're, we're able to recognize cases of discrimination
and plenty of cases where, where we're not able to do this kind of like, you know, um, hurtfully and
like numerical feed. Let's maybe shift gears and talk a little bit about, uh, use cases or
application areas that have made notable progress in 2021. Um, anything come to mind there?
You know, well, look, um, one, one obvious one and, you know, as much as I might, you know,
be kind of called on as a contrarian, but like, uh, well, one more, I think I'd give some credit is
I think, um, alpha folds from deep mind, um, like I'm not a protein folding expert, but I know
some people that are not just like gullible deep learning boosters, um, who do work in the area
and as far as I can tell, it's like actually a pretty significant, um, leap forward, um, that,
you know, it's work that, you know, could very well have, you know, one like a significant,
you know, science prize, you know, uh, like that level of accomplishment, um, and, you know,
that's a little bit, you know, hearsay and that like, I'm not, I'm not, uh, an expert in,
in protein folding, but as far as I understand it, it really is, uh, a legitimate significant
contribution. And I think an area where, you know, maybe deep learning wasn't quite as,
you know, inlined as a essential tool. So that that's certainly, um, a use case. I think you're
starting to see, um, a lot of the use cases that were maybe obvious ones, um, but not necessarily,
um, you know, for example, like radiology, it's sort of like an obvious initial thing because
and harkening back to like our earlier conversation about the difference between prediction and
decision, part of why radiology is like people see it as like this big target is that, um, you
know, there are certain roles of the radiologists where they really are involved in decision
making and, uh, recognition is part of a weird, it's like an interventional radiology,
but there are also our lots of people who literally are looking at images and making classifications
and medical imaging is a case like a diagnostic kind of imaging, right? And I think that's a case where
we've known since the moment, you know, the kind of big image recognition results started
hitting and say 2012, 2013 that radiology was a potential target and he had some maybe
overly, uh, optimistic statements from like Jeff had been like, if you're, you know, if I were,
if you're a medical school now, do not specialize in radiology, it hasn't quite gotten to that point,
it hasn't taken the radiologist out of the loop, but I've been, I've been chatting with a lot of
radiologists recently and I've been surprised to find, well, sort of two things, one, on one side,
that some of the systems really are quite good and you actually have some systems being
deployed already, actually, like, actually piping information into patient records.
And at the same time that I think some of the problems that we discussed earlier about what can
go wrong are happening on the ground and you do have a situation where, for example,
systems that work well on one set of equipment are not performing well on, you know, some new scanner,
which, you know, you know, it's efficiently similar to all the other scanners that a human
radiologist would have no problem. And these are not adversarial examples, nobody's out there
designing a scanner, there's not like, there's like a radiologist on the, I'm going to, I'm going
to build a scanner that just fucks up all the previous, fucks all the deep learnings, so that way
we can like keep our dogs. So I think you sort of have, you know, both, uh, a moment of the
technology actually kind of like making landfall. But I think you also have some, some moment of
like the rubber hitting the road and people, people sort of seeing up front some, you know, up close,
some, some of the ways in which the technology is brittle and dangerous. Yeah. And I think that
largely, if this might take an unsexy story because it's like what's like the big sexy application,
I think largely this story now. And I think that overall the biggest like, if I were to like,
take a, like a bird's eye view of the economy and just like, I'm just watching like, what is AI
doing? You know, I think, right, the story of like 2014, 2015 is like these new use cases,
like fundamental new things popping up, like things we weren't doing with deep learning suddenly,
like machine translation, people swapping out the old guts and sticking into deep learning
systems. And, um, suddenly like every single mobile phone having the capacity to run some kind
of small deep learning model because it's being used for recognizing objects in the cameras and
doing the face recognition that unlocks your phone and all of that. I think the bigger story of
the last couple of years has been more on the side of, um, more on the side of like, deployment
and like diffusion and like maturity of the, like, uh, like operations around ML. Like, I
notice more and more companies that like their pain point isn't that they need someone who could
train a model. Their pain point is they need an ML ops president. They need someone who,
who can actually keep the crap running day in, day out that someone who knows, you know,
it's like, there's some specialized, it's not, it's like a pure like ML researcher, like someone
like me even, like, I don't have this skill set. I haven't spent my life in like, you know,
there's, there's a real serious discipline and like keeping software working day in, day out.
Like the people, it's, it's amazing what we could do when Eastern companies that like have a
software that, you know, product that like 400 million people use every day and they like go
seven years without a single, you know, hour of downtime. You know, it's absolutely bonkers how
difficult that is. And machine learning has rose in a whole, you know, I think researchers don't
have that. But machine learning grows like a weird set of complications because all kinds of ways
that things go wrong, even if there aren't software bugs. And so they need to understand something
about enough about statistics. They have some sense of what could go wrong and ways that you need
to model things that aren't software glitches. They're like, the world changing glitches.
There's like the world is the bug. Even if, even if everything's coded precisely and need to be
able to interface back and forth between like software developers, ML engineers and researchers.
So I think like the maturity of ML ops and also just broadly like the use of ML not just in,
you know, I think there was a moment right when there was Google, Amazon, Facebook, Microsoft.
And I don't know if you ever read this, you know, but like I wrote this like satire that just
because like when everyone was making a big deal about like, oh, whatever professor left to go to
whatever company and just their salary and they were kind of writing about it, like, you know,
almost like like football players getting traded or something. And so I wrote this stupid post
that was just like announcing that I had been hired as like the intergalactic head of the
machine learning by Johnson and Johnson or something. And I'd be, you know, for some, you know,
astronomical sum. And it was just a stupid joke. But the final is like a year later,
I forget where I was and I met someone and they worked at like Johnson and Johnson AI research.
Right. And I think that this is part of like what's going on now is that like there was a moment in
time. I'm sure like I'm making this up. I researched it before though, you know, but you know,
this is what you come here to speak from academic authority to make up crap on your podcast.
I'm sure there was a moment in time where like there were only a small number of like elite tech
firms that were using like modern SQL databases. You know, when it was fresh, I think it was at IBM,
when it was developed, but there's probably a moment. It was just like a really hot,
fundamentally new technology that really changed its business operations at places. And there were
like a handful of like super technical firms that knew how to do it. And now it's like the most
boring technical firm in the world uses SQL. Right. And I think that this is a huge part of what's
happening in AI if you were to size up like commercial environment. So I think there are exciting
startups that are using this technology in new ways. There are, you know, interesting things going
on at like the sexy tech companies. But I think there's a lot of like, you know, there's no company
that you go to, you know, whether it's like, you know, I'm sure if you went to like a waste management
company, like they're finding, you know, they're using AI for something or forecasting demand or
trying to figure out how to route their trucks or something. And I think that like this sort of
just general like progression of AI from like a luxury good to a commodity is like an essential
part of what's going on. And like the fact that like, yeah, like every company has, you know,
this is becoming their concern. And I think I think part and parcel of that is the way that the
tooling is getting better and better and better. A whole lot of companies, you know, like what are
they offering? It's like things that make it that like the stuff that everyone's already been doing
for a while that anyone could do it, right? And it's easy to track and, you know, it's easy to
organize. Like, you know, I think this movement of like AI from a concern of like what's the new
model to like what is like a stable workflow that we can adopt such that a company that can't
spend half a million dollars for engineer can still use this technology like successfully and
profitably. I think that's a major part of the story of like the commercial application of AI
right now. And it's kind of like, it's a pretty unsexy story. Maybe I've just like, oh, this is
becoming right. But I think this is what happens to everything, right? Like, I don't know,
I'm like, sorry, if you're a AI researcher, but if you're an MLOPS, it's pretty cool.
Oh, yeah, absolutely. I don't really cool stuff happening in that field. And there's like a lot more
jobs at every company in the world together than there is at like whatever it is, like Apple
Microsoft, Amazon, Facebook, whatever. So I think that moved. Before we run out of time,
I'd love to have you kind of dust off the crystal ball a little bit more and kind of share some
of your predictions for the upcoming year, years. We've talked a little bit about where you'd
swing the bat from a research perspective, but how do you think 2022? With the backdrop of
kind of the, I don't know if you'd call it a cooling or slowing or boring a vacation or whatever
you'd want to call it. Are there innovations that are, you know, you kind of see the silhouette
emerging from the shadows and you think something's there? It's not. The funny thing about that
is it's not like it's all cooling or it's all heating up. It's like the coolest or weirdest
interesting thing about it is that whenever you sum up something like a complex
phenomena with a single number, you lose a lot of information. I think it's like more like
follow the Roman Empire, right? It's like like the like Rome's all right. Like Rome is still partying
and like the borders are still expanding, but you also have like, you know, like you have like cities
being lost and whole country is going off the map and it's like, I think that's happening,
right? Like you have like like Uber AI shutting down. I research you have like hiring
freezes at major companies. The big leader is like having major hiring freezes not offering
the kinds of salaries in 2022 that they were offering in 2018 to like, it's kind of like well-known
researchers. And at the same time, you have like whole companies where like the shockwave hasn't
even hit them yet. And they're like first getting into the like like major health systems starting
to adopt like, you know, deep learning and I think that yeah, there's that going on. If I had to
predict what's going to happen, I'm going to double down on decision-making. So, you know, I think
it's like that I'm already seeing a lot of is, you know, like, you know, you could think of like
like two things came, a few things came together that made AI so hot, which is like one was suddenly
the fact that like the existence of easily queriable, well-organized curated data at every single firm
in the world, you know, the fact that like health companies are using electronic health records,
every company being basically an internet company, everyone having a digital trace of all their
customer interactions. Now, you know, we can get to a separate like normative point about whether
we want to live in that world or like whether we're irked by the surveillance state or from a
standpoint, like an economic standpoint, that happened together with like advances in both the
tooling and algorithms around statistical modeling. And so the question became we have this data,
we have statistical tools, how do we do this like analytics on the data, right? But there's
another side which is like, how do we guide, how do we use the data to guide actions? And I think
people, I think one one thing that is underutilized by most firms and I think only a small number of
people are really sophisticated about it is, is really focusing on this like the decision problem.
And part of that is, part of that is, you know, offline causal inference, which is some of the
stuff we were talking about, like how can I use some some causal background knowledge together with
the data that I have to infer a causal effect and use that to guide decisions. But a huge part of
that is experimentation. And I think that this is a huge thing that not enough companies do that
you're going to start seeing, you know, become, you know, obviously like Amazon, you know, has,
has the what they call like web labs, you know, where the, you know, Google, like, you know, does
randomized control trials for, you know, which shade of green the G should be in Google or something.
But I think most companies grossly underutilized experimentation, like really methodical
experiments, because, you know, that plays into the data fixer. Online experiments in particular?
Well, online, not necessarily in the sense, I mean, I think online is part of, you know, in a sense of
like, you know, doing like reinforcement learning or having a policy that's adaptive as you're getting
the results, but even experimenting at all, right? Like just like we've been guiding, if you look
at how we guide personalized decisions, it's often in the context of I just take passively collected
traces of people's data. I do some kind of latent factor analysis or whatever to build a
recommended system versus actually I'm going to randomize choices and try to estimate the sort of
like, you know, potentially like heterogeneous treatment effects of how different people will
respond differently to different things, but actually to estimate the effects on, you know,
whether it's people's behavior or whatever, you know, I don't mean this is sort of like a sound like
I'm, I'm advising that we like really nearly experiment on people without thinking about the
considerations or which decisions or which experiments are potentially like of ethical import,
and obviously there's a lot that needs to, there's a lot of considerations that need to go into
how you do that and doing it right. But yeah, I think that, you know, the reckoning we're seeing,
I think is over and over again, it's like people claiming the AI is going to, you know,
going to personalize this, personalize that, it's going to lead you to make all these different decisions
in better ways, and then people find like, oh, I just now newly trained a predictive model came up
with some heuristic for how to operationalize that its decision, and something didn't go as planned.
And I think that people actually getting more into this world of both using offline, you know,
kind of causal inference on observational data, but also actually experimenting in the real world,
and, you know, developing more mature processes for saying, how do I, how do I test hypotheses?
How do I see what the impacts are of, you know, different actions that I have? I think that that's
going to become more and more and more important, and you're going to start seeing the like hiring
focus, you know, and just like where teams start moving, you know, towards those kinds of
problems, and again, I don't think this is like overnight, you're going to go from people like
hiring 90% which, you know, deep learning, you know, like, like pie towards jockey to like 90%
hiring, you know, experts in like, you know, bounded algorithms and causal inference,
but I do think that there is a, there's a shift here, I'm seeing it at every level, I'm seeing it
in what, what looks, you know, interesting among new students, what looks interesting among
our folks hitting the hiring market, I think that this sort of intersection of like, you know,
CS, operations, research, economics, and bringing to bear, like, you know, tools of predictive
modeling that we've gotten, but also more sophisticated processes of experimentation and
estimating causal effects, and principles of just guiding, you know, intelligent decision-making.
I see there's like a, I think there's like a growing up process happening there, and you know,
I think the other thing though is, well, one thing I add on, and this is not like a specific
prediction, but a meta prediction, is, you know, like the internet, like web 2.0, web 3.0,
whatever the hell we're doing, like very little, there's a lot of like new, there's a lot of new
stuff that we're seeing, and like the way companies are behaving in the way they're interacting with
people, that isn't technologically new, right? There's a lot of stuff that like you could have done
from the late 1990s, the tooling wasn't in there, which restricted how many people could develop it,
but it was something else, it was something about like, you know, there was a capability that came,
and a few people have figured out some very like, some early players that figured out like, you know,
how to conquer e-commerce, like Amazon, whatever, but it took a long time before you got to Uber,
right? And so there are certain innovations there that were like, you know, it's like there were
a bunch of pieces that need to fit together, like a certain understanding of like markets,
or a certain understanding of like usage patterns of phones with the technological capability that
had been there all along, and I think that like, there's a kind of like innovation in deployment
that doesn't actually correspond, like I think when people have been stoked about ML recently,
right? It's been like, oh, BERT is like good at classifying texts, or you know, like, you know,
seek to seek, you know, LSTMs, and then Transformers are good at, you know, this one thing.
There's like single-purpose models, but like, I'll give you an example, so I'm an advisor for
a company, so it's full COI. I'm an advisor for a company called a bridge AI, and a bridge is
a company that like is sitting between like doctors and the patients, and sitting in this like
interaction where patients, it turns out patients are recording their visits on their cell phones,
and they're doing this already. Sometimes sort of viciously, sometimes the doctors can
send, and may or may not be wired up, and depending upon like what your particular state, it's like,
you know, two-party sends, so their idea is like, let's inline this as a normal part of the
doctor-patient interaction. Like, let's have permission, let's have both parties get in,
they'll agree to record the conversation, they'll pull out a bridge, and then they record it, and
there's all kinds of different things you could do, right? Like, you can help the doctor to draft
the summary of the visit, you can help the patient to understand, like, oh, like, you know, don't
forget, like you mentioned that you would be starting this new medication, have you picked it up,
or have you, you know, called in that prescription, or did you schedule out this follow-up?
So there's like a million different places to plug in models, and any one of them by itself may
or may not be like, you know, a single purpose, like, major innovation, but the ways that you can
mix and match these, like, okay, I've got the conversation, I've got to send it to an ASR model,
like, in fact, the text of the text, I need to flag out, like, well, what are the interest,
what are the relevant or salient parts of the conversation? How do I then take that, turn it into,
like, an interface feature that, you know, provide some value or a mixing's useful to the patient?
And I think that, like, a lot of things, like, when Alexa works really well, right?
You know, or Google Home, or any of these things, why don't work really well? It's usually not
because there's one model that's, like, magnificent. It's, like, the magic is in the clever way that they
stitch together, you know, some astute observations about what are the common interaction patterns
together with, like, what are the right little places you can patch in machine learning,
and the right ways that you can, like, patch in some intelligent heuristics and rules around it,
such that, like, you know, you have, like, an end-to-end product that feels like it's magic, right?
Um, you know, Shazam even is a little bit like that, but when these things are like, there's a few,
like, like, a little heuristics, where if you start thinking, how do I decompose this into, like,
something that works? It's like, you can make a pipeline where every single step of it's kind of
simple, but, like, the end of the end result is something where it feels a little bit magical, right?
And I think that, like, this is going to be a, I think, a major part of this is, like, I think
we've been looking at, like, people who are really good at building single-purpose models,
then turning that into a big start-up, and, or trying to turn, parlay that into a start-up,
and I think there will be some amount of, like, the single-purpose models are mature,
and they'll get a little bit better, but what's maybe under-explored a bit are ways that you mix
and match models together with cool interaction patterns, and, you know, some clever understanding
of, like, what people want, and, you know, what data's available, et cetera, to build, like, kind of,
user experiences that maybe under the hood are invoking, like, seven different models, and
seven different contact, and, but it's, like, kind of hidden from the user in a clever way, where it
just feels like you're having, like, it adds up to a new capability that no one model,
or piece of software, like, by itself would provide. And so, I do think that there is some
element of this, like, you know, like, we've built a bunch of cool Legos, and we haven't given
people that many years, so, like, you know, instead of, like, you know, some innovation comes
from, like, I designed a new Lego piece, but, like, I think a lot of innovation will come from
people that are, you know, you know, don't have to be, like, have, like, off-the-chart skills at
building Legos, but they're really, they have a kind of design sentence for, you know, what are
cool ways to put them together? I think that's a, it's a natural consequence of the, the broader
maturity conversation we've had, we've been having, right? The Lego pieces are, you know, that
that we've come up with every Lego piece that's ever going to be created, and there aren't some
cool ones to come, but all the basic pieces required to build really cool stuff is in place, and
now it's all about, how do you put them together? Yeah, and even more so than the pieces themselves,
the tools to easily put them in place, you've got your hugging faces, you've got your ML ops tools,
like, it's a great time to be a builder. Right, yeah, it also, like, it takes some of that work
away that it allows you to focus, right? I think music's like that a little bit, right? Like,
you know, there's this way, like, when you're learning an instrument, and you're like, I got to
practice articulation, I got to practice rudiment, I got to practice scales, I got to do that, and you,
you know, you're sitting here, you're going to do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do,
and then you're playing this kind of shit over and over and over again when you're like 10,
11, 12, 13, 14 years old, but you get to some point where like, maybe you still practice like that
one hour a day, but when you go to play, you're not even thinking at that level at all, like,
it's not, and I think there is some element of that, of like, people using machine learning recently
have been thinking like, just like, how do I get the data and train a single model? I think,
once you have a lot of these contexts where maybe you don't even need to train a model,
maybe there's an off-the-shelf model that's sufficiently good at this task and it works better
than anything you could train, even if you're applying it sort of on slightly different
domain-shifted data, right? Then you start getting at this point where,
right? Like the difference between like a great artist and like a super boring artist isn't
that like the great artist is better at scales, you know? It's not, it's at a point, you know,
right? So it's not like, oh, like, like, mild Davis played like better, you know,
played like cleaner scales than, you know, like, I don't know.
He's hated the artist staying on key or something like that. It's not like that.
Yeah. So, I think there is, you know, a lot of, a lot of iteration that did be had on that side.
Yeah. Awesome. Awesome. Well, Zach, it has been wonderful catching up. Let's make sure it's not
two years, until the next time. Yeah. Right. Who knows what pandemic will be in full swing
by that time. Awesome. Well, thanks so much for helping us reflect on 2021 and the ML&D
LDM and catch you next time. Yeah, thanks for having me sound. Great to see you.
