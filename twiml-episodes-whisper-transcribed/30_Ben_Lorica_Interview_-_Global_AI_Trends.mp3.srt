1
00:00:30,000 --> 00:00:33,480
Hello and welcome to another episode of Twimble Talk,

2
00:00:33,480 --> 00:00:36,080
the podcast where I interview interesting people,

3
00:00:36,080 --> 00:00:38,400
doing interesting things in machine learning

4
00:00:38,400 --> 00:00:40,720
and artificial intelligence.

5
00:00:40,720 --> 00:00:43,440
I'm your host, Sam Charrington.

6
00:00:43,440 --> 00:00:45,840
So as you know, a couple of weeks ago,

7
00:00:45,840 --> 00:00:48,760
I announced the first anniversary of the podcast

8
00:00:48,760 --> 00:00:51,320
and to celebrate it our first anniversary

9
00:00:51,320 --> 00:00:54,240
listener appreciation contest.

10
00:00:54,240 --> 00:00:56,640
I wanna start this show by thanking everyone

11
00:00:56,640 --> 00:00:58,920
who participated in the contest.

12
00:00:58,920 --> 00:01:01,680
You all have come out in droves over the past couple of weeks

13
00:01:01,680 --> 00:01:04,480
to shower the podcast with love and for that,

14
00:01:04,480 --> 00:01:07,480
we are humbled and forever grateful.

15
00:01:07,480 --> 00:01:10,200
Overall, we receive nearly a hundred entries

16
00:01:10,200 --> 00:01:12,960
via our website and iTunes from listeners

17
00:01:12,960 --> 00:01:14,800
from 16 countries.

18
00:01:14,800 --> 00:01:16,560
Your stories have been awesome

19
00:01:16,560 --> 00:01:18,440
and we're most proud that we can help

20
00:01:18,440 --> 00:01:21,000
light that AI fire for you every week.

21
00:01:21,000 --> 00:01:23,560
Of course, everyone who entered will receive

22
00:01:23,560 --> 00:01:25,640
a couple of Twimble and AI stickers,

23
00:01:25,640 --> 00:01:28,000
one for you and one for a friend.

24
00:01:28,000 --> 00:01:30,280
Those will be on their way soon.

25
00:01:30,280 --> 00:01:32,960
But now, the moment you've all been waiting for,

26
00:01:32,960 --> 00:01:35,680
we have our winners for the first anniversary

27
00:01:35,680 --> 00:01:38,560
listener appreciation contest.

28
00:01:38,560 --> 00:01:41,720
Just a reminder, our grand prize winner receives

29
00:01:41,720 --> 00:01:44,720
a bronze pass for the O'Reilly AI conference

30
00:01:44,720 --> 00:01:46,680
at the end of this month.

31
00:01:46,680 --> 00:01:50,360
And our runner-up winner gets, well, hey Google,

32
00:01:50,360 --> 00:01:52,400
what's the second prize?

33
00:01:52,400 --> 00:01:55,360
The second prize in Twimble's first anniversary listener

34
00:01:55,360 --> 00:01:57,440
appreciation contest is a brand spanking

35
00:01:57,440 --> 00:02:00,200
your Google home, like me.

36
00:02:00,200 --> 00:02:02,560
All right, without further ado,

37
00:02:02,560 --> 00:02:05,760
our second prize winner is anchor Patel.

38
00:02:05,760 --> 00:02:08,760
Anchor wrote in on the show notes page with this.

39
00:02:08,760 --> 00:02:12,320
Twimble, congratulations on your first anniversary.

40
00:02:12,320 --> 00:02:15,040
I have loved listening to the show.

41
00:02:15,040 --> 00:02:16,840
I remember the news and machine learning

42
00:02:16,840 --> 00:02:20,160
from the first few months, what a sheer amount of information.

43
00:02:20,160 --> 00:02:23,640
But the more recent interviews format is excellent.

44
00:02:23,640 --> 00:02:26,480
I particularly love the ones where you interview folks

45
00:02:26,480 --> 00:02:30,520
at AI and ML conferences, very insightful indeed.

46
00:02:30,520 --> 00:02:34,320
Thanks for contributing to the broader ML community.

47
00:02:34,320 --> 00:02:36,920
Well, thank you, anchor, for being a loyal listener

48
00:02:36,920 --> 00:02:40,680
and will be in touch with you about that Google home.

49
00:02:40,680 --> 00:02:45,800
And now, our grand prize winner is Mason Grimshaw.

50
00:02:45,800 --> 00:02:49,360
Mason also wrote in via the show notes page.

51
00:02:49,360 --> 00:02:51,200
Here's what he said.

52
00:02:51,200 --> 00:02:53,320
The show is fantastic.

53
00:02:53,320 --> 00:02:56,840
I started listening right when you switched to the interview

54
00:02:56,840 --> 00:02:58,880
format, and I've definitely noticed your improvement

55
00:02:58,880 --> 00:03:01,600
as a journalist and the improvement of the show.

56
00:03:01,600 --> 00:03:03,080
Great job.

57
00:03:03,080 --> 00:03:06,400
I go to school in Boston, so the show keeps me company

58
00:03:06,400 --> 00:03:08,920
on my 20 minute walk to school every morning,

59
00:03:08,920 --> 00:03:12,560
and I couldn't ask for a more interesting companion.

60
00:03:12,560 --> 00:03:14,280
I'm studying analytics.

61
00:03:14,280 --> 00:03:17,840
And while I may not directly use AI in my profession,

62
00:03:17,840 --> 00:03:20,280
I certainly use it as a hobby.

63
00:03:20,280 --> 00:03:23,040
I really enjoyed Carlos Gastron when he came on

64
00:03:23,040 --> 00:03:26,280
to talk about the line paper and the Danny Lang discussion

65
00:03:26,280 --> 00:03:28,040
about video games.

66
00:03:28,040 --> 00:03:31,560
Well, Mason, we hope to see you in New York in a few weeks.

67
00:03:31,560 --> 00:03:35,520
Congratulations, and thank you for being a loyal listener.

68
00:03:35,520 --> 00:03:38,320
Although there can only be one first prize winner,

69
00:03:38,320 --> 00:03:40,440
we'd like to give everyone the opportunity

70
00:03:40,440 --> 00:03:43,440
to attend the O'Reilly AI conference with us.

71
00:03:43,440 --> 00:03:48,000
With the code PC Twimmel, that's PC-TW-I-M-L,

72
00:03:48,000 --> 00:03:51,240
all of our listeners will get 20% off of registration fees

73
00:03:51,240 --> 00:03:54,240
and purchasing passes for the conference.

74
00:03:54,240 --> 00:03:56,760
Please let us know if you're planning to attend the event.

75
00:03:56,760 --> 00:03:58,040
We're looking forward to meeting up

76
00:03:58,040 --> 00:04:00,360
with Twimmel listeners there.

77
00:04:00,360 --> 00:04:03,360
Okay, to continue with the O'Reilly AI theme,

78
00:04:03,360 --> 00:04:05,560
this week I've got a very special guest.

79
00:04:05,560 --> 00:04:08,840
I've invited my friend Ben Lorca onto the show.

80
00:04:08,840 --> 00:04:11,760
Ben is chief data scientist for O'Reilly media

81
00:04:11,760 --> 00:04:14,040
and program director for strata data

82
00:04:14,040 --> 00:04:16,560
and the O'Reilly AI conference.

83
00:04:16,560 --> 00:04:18,680
Ben has worked on analytics and machine learning

84
00:04:18,680 --> 00:04:20,920
in the finance and retail industries

85
00:04:20,920 --> 00:04:24,440
and serves as an advisor for nearly a dozen startups.

86
00:04:24,440 --> 00:04:25,960
In his role at O'Reilly,

87
00:04:25,960 --> 00:04:27,520
he's responsible for the content

88
00:04:27,520 --> 00:04:31,440
for seven major conferences around the world this year.

89
00:04:31,440 --> 00:04:33,560
In the show, we discuss all of that.

90
00:04:33,560 --> 00:04:35,720
Touching on how publishers can take advantage

91
00:04:35,720 --> 00:04:37,760
of machine learning and data mining,

92
00:04:37,760 --> 00:04:40,240
how the role of data scientist is evolving

93
00:04:40,240 --> 00:04:42,000
and the emergence of the machine learning.

94
00:04:42,000 --> 00:04:43,800
Hey, everyone, I am on the line

95
00:04:43,800 --> 00:04:45,320
with Ben Lorca.

96
00:04:45,320 --> 00:04:49,240
Ben is the chief data scientist at O'Reilly media

97
00:04:49,240 --> 00:04:51,360
and he's also responsible for content

98
00:04:51,360 --> 00:04:53,640
for both the strata data conference

99
00:04:53,640 --> 00:04:56,360
as well as the O'Reilly AI conference.

100
00:04:57,320 --> 00:04:58,800
Hey, Ben, how are you doing?

101
00:04:58,800 --> 00:05:00,960
Great, great to be here, Sam.

102
00:05:00,960 --> 00:05:02,360
Awesome, awesome.

103
00:05:02,360 --> 00:05:04,520
So every time I talk to you,

104
00:05:04,520 --> 00:05:06,920
you are just getting off of a plane

105
00:05:06,920 --> 00:05:09,880
and it sounds like that is the case this time as well.

106
00:05:09,880 --> 00:05:14,080
Yeah, so we had an outstanding strata data conference

107
00:05:14,080 --> 00:05:15,240
in London.

108
00:05:15,240 --> 00:05:20,240
Yeah, basically many, many people from many different parts

109
00:05:21,800 --> 00:05:24,360
of Europe came and spoke and attended.

110
00:05:24,360 --> 00:05:26,480
And this year in particular,

111
00:05:26,480 --> 00:05:30,840
we had concerted effort to focus on deep learning

112
00:05:30,840 --> 00:05:32,560
on the machine learning site.

113
00:05:32,560 --> 00:05:37,400
But the staple topics of strata were still remain popular

114
00:05:37,400 --> 00:05:40,680
particularly on architecting big data application.

115
00:05:40,680 --> 00:05:42,320
Okay, awesome, awesome.

116
00:05:42,320 --> 00:05:44,080
Well, I thought we'd start this conversation

117
00:05:44,080 --> 00:05:45,960
by having you spend a little bit of time

118
00:05:45,960 --> 00:05:47,600
talking about your background

119
00:05:47,600 --> 00:05:50,920
and how you got started with data

120
00:05:50,920 --> 00:05:55,920
and how you kind of your path leading up to O'Reilly.

121
00:05:57,520 --> 00:06:00,600
Sure, so I have a PhD in math

122
00:06:00,600 --> 00:06:05,600
and focused on non-linear partial differential equations

123
00:06:05,720 --> 00:06:09,920
and towards the end of graduate school,

124
00:06:09,920 --> 00:06:13,200
I became interested in specific set

125
00:06:13,200 --> 00:06:16,760
of differential equations called stochastic differential equations

126
00:06:16,760 --> 00:06:21,760
which turn out to be at least theoretically important

127
00:06:22,240 --> 00:06:24,680
for quantitative finance.

128
00:06:25,680 --> 00:06:27,480
So I had some interest already

129
00:06:27,480 --> 00:06:31,120
in kind of the industrial applications

130
00:06:31,120 --> 00:06:33,440
of what I was doing,

131
00:06:33,440 --> 00:06:37,280
but I definitely was on the academic track.

132
00:06:37,280 --> 00:06:38,880
So after grad school,

133
00:06:38,880 --> 00:06:43,000
I was an academic for five years.

134
00:06:43,000 --> 00:06:45,760
But then at some point I decided

135
00:06:46,680 --> 00:06:50,160
I was actually much more interested in industry.

136
00:06:50,160 --> 00:06:53,520
And at the risk of dating myself,

137
00:06:53,520 --> 00:06:56,640
at least at the time when I was contemplating

138
00:06:56,640 --> 00:06:58,320
moving to industry,

139
00:06:58,320 --> 00:07:01,040
there was no data science track.

140
00:07:01,040 --> 00:07:06,040
So the exit strategy was becoming a quant

141
00:07:07,080 --> 00:07:08,920
and so that's what I did.

142
00:07:08,920 --> 00:07:11,160
I did that for about two and a half years

143
00:07:11,160 --> 00:07:13,360
in a small hedge fund,

144
00:07:13,360 --> 00:07:18,720
designing, trading models, risk management, portfolio management,

145
00:07:18,720 --> 00:07:19,560
and things like that.

146
00:07:19,560 --> 00:07:22,160
And one of the first things I learned, of course,

147
00:07:22,160 --> 00:07:26,920
is the stuff I thought was going to be super important,

148
00:07:26,920 --> 00:07:28,920
the stochastic PDEs,

149
00:07:28,920 --> 00:07:31,480
while good to know,

150
00:07:31,480 --> 00:07:37,680
not exactly what I needed to do for my job.

151
00:07:37,680 --> 00:07:42,680
So at that time, actually the term machine learning,

152
00:07:43,720 --> 00:07:46,320
I would say was kind of nascent.

153
00:07:46,320 --> 00:07:49,680
The people were still using the term data mining.

154
00:07:51,520 --> 00:07:54,000
And so that's what I did at this.

155
00:07:54,000 --> 00:07:57,680
I applied basically statistical techniques

156
00:07:57,680 --> 00:08:02,640
and machine learning to financial time series.

157
00:08:02,640 --> 00:08:05,720
But then at some point I realized my interest

158
00:08:05,720 --> 00:08:09,160
actually were much more on the tech side,

159
00:08:09,160 --> 00:08:14,160
the programming and building software applications

160
00:08:14,760 --> 00:08:19,600
for analyzing these time series.

161
00:08:19,600 --> 00:08:24,240
And so then I ended up moving to Silicon Valley

162
00:08:24,240 --> 00:08:26,600
around, actually believe it or not,

163
00:08:26,600 --> 00:08:29,320
at the peak of the nascent at the time,

164
00:08:29,320 --> 00:08:31,280
which was around March, 2000.

165
00:08:32,200 --> 00:08:34,640
And so then I was here,

166
00:08:34,640 --> 00:08:39,240
I was here just in time for that first bust.

167
00:08:41,160 --> 00:08:43,080
And then the first bust, yeah.

168
00:08:43,080 --> 00:08:45,240
I was there around the same time.

169
00:08:45,240 --> 00:08:46,320
Yeah, yeah, yeah.

170
00:08:46,320 --> 00:08:47,560
And so,

171
00:08:47,560 --> 00:08:51,440
Do you keep track of what's going on

172
00:08:51,440 --> 00:08:56,440
in the quantitative side of the financial markets

173
00:08:56,960 --> 00:09:00,000
and how folks or how the technology has evolved

174
00:09:00,000 --> 00:09:01,640
since you worked in that space?

175
00:09:01,640 --> 00:09:06,160
I'm not that close to, I still have friends.

176
00:09:06,160 --> 00:09:11,160
And obviously my work leading these two big conferences

177
00:09:11,160 --> 00:09:13,280
that you described, try to data

178
00:09:13,280 --> 00:09:16,320
in the O'Reilly Artificial Intelligence Conference

179
00:09:16,320 --> 00:09:21,320
brings me in contact with the people working in finance.

180
00:09:21,800 --> 00:09:24,880
So I keep track of it that way.

181
00:09:26,480 --> 00:09:28,480
Yeah, so to some extent I do,

182
00:09:28,480 --> 00:09:33,280
but I'm not immersed in the latest techniques

183
00:09:33,280 --> 00:09:33,960
that they're using.

184
00:09:33,960 --> 00:09:37,640
Although I would say that they are actually moving more

185
00:09:37,640 --> 00:09:42,640
towards our world of using machine learning,

186
00:09:43,120 --> 00:09:46,920
alternative data sources, big data,

187
00:09:46,920 --> 00:09:51,920
and some of these more bleeding edge machine learning

188
00:09:52,840 --> 00:09:54,240
techniques, including deep learning.

189
00:09:54,240 --> 00:09:57,120
So to the extent that they're actually showing up

190
00:09:57,120 --> 00:10:00,640
at the events I organize, that's how I keep track.

191
00:10:04,080 --> 00:10:05,080
Okay, okay.

192
00:10:05,080 --> 00:10:06,080
So I interrupted you.

193
00:10:06,080 --> 00:10:07,440
You, you, oh yeah.

194
00:10:07,440 --> 00:10:10,120
So then at some point to the technical side of things.

195
00:10:10,120 --> 00:10:12,080
I got into the technical side of things.

196
00:10:12,080 --> 00:10:16,000
And when I first moved to tech,

197
00:10:16,000 --> 00:10:20,640
I think you know, this will surprise a lot of your listeners,

198
00:10:20,640 --> 00:10:25,400
but you know, one of the big users of data in tech

199
00:10:25,400 --> 00:10:29,480
are the marketing and sales people.

200
00:10:29,480 --> 00:10:33,800
So that's how I kind of moved into tech was basically

201
00:10:33,800 --> 00:10:38,080
talk what I learned in in finance

202
00:10:38,080 --> 00:10:41,640
and started applying it in marketing and sales applications.

203
00:10:41,640 --> 00:10:44,760
So did a couple of startups,

204
00:10:44,760 --> 00:10:48,400
joined a couple of startups that didn't really take off

205
00:10:48,400 --> 00:10:51,880
and then eventually at some point ended up at O'Reilly

206
00:10:51,880 --> 00:10:56,640
as a data scientist working on the types of data that we have,

207
00:10:56,640 --> 00:10:58,680
which is a lot of it is sales data,

208
00:10:58,680 --> 00:11:01,880
but a lot of unstructured and semi structured tech.

209
00:11:01,880 --> 00:11:06,640
So I was, I was definitely one of the first people

210
00:11:06,640 --> 00:11:11,480
doing a lot of these techs mining and machine,

211
00:11:11,480 --> 00:11:13,920
machine learning for techs from the early days.

212
00:11:13,920 --> 00:11:17,040
I mean, I may have, I may have been one of the first people

213
00:11:17,040 --> 00:11:19,920
to actually use this topic models LDA

214
00:11:19,920 --> 00:11:25,520
that David Bly and Drew Aing and Mike Jordan wrote about

215
00:11:25,520 --> 00:11:28,720
for an industrial consulting project.

216
00:11:29,840 --> 00:11:33,560
So, oh wow, can you tell us a little bit more about that?

217
00:11:33,560 --> 00:11:36,800
Oh, I mean, so it's with a well known tech company, right?

218
00:11:36,800 --> 00:11:41,800
So hired us at O'Reilly to analyze our tech sources,

219
00:11:43,360 --> 00:11:46,480
unstructured tech sources, which include job posting,

220
00:11:46,480 --> 00:11:48,960
all the job posting to the US.

221
00:11:48,960 --> 00:11:50,640
Okay, and things like that.

222
00:11:50,640 --> 00:11:55,400
And to just get them, give them strategic advice.

223
00:11:55,400 --> 00:12:00,160
So I thought, I thought using LDA and topic models

224
00:12:00,160 --> 00:12:04,280
would provide some quantitative basis

225
00:12:04,280 --> 00:12:05,920
for the advice we were giving.

226
00:12:05,920 --> 00:12:07,800
And from what I understand,

227
00:12:07,800 --> 00:12:10,800
it did change the direction of this major,

228
00:12:10,800 --> 00:12:15,280
major tech company, tech company that everyone knows about.

229
00:12:15,280 --> 00:12:19,560
Mm, so I guess one thing that I've been meeting

230
00:12:19,560 --> 00:12:22,080
to ask you for a while now actually is,

231
00:12:22,080 --> 00:12:25,440
you've now got five strata conferences, right?

232
00:12:25,440 --> 00:12:27,480
And the two AI conferences,

233
00:12:27,480 --> 00:12:31,440
and I think you're involved in all of those, is that right?

234
00:12:31,440 --> 00:12:35,320
Yeah, so I'm responsible for the program for all of those.

235
00:12:35,320 --> 00:12:39,280
So basically everything that we have at these conferences,

236
00:12:39,280 --> 00:12:43,240
we have two day trainings, we have tutorials,

237
00:12:43,240 --> 00:12:46,480
we have sessions, and then we have keynotes.

238
00:12:46,480 --> 00:12:50,640
So basically, I'm ultimately the person responsible

239
00:12:50,640 --> 00:12:55,640
for the lineup for all of these conferences.

240
00:12:55,640 --> 00:12:57,520
Okay, and so my question then is,

241
00:12:57,520 --> 00:13:02,320
do you actually have any time for doing data science

242
00:13:02,320 --> 00:13:03,600
at O'Reilly nowadays?

243
00:13:03,600 --> 00:13:07,600
Or are you, how could you possibly

244
00:13:07,600 --> 00:13:09,520
with all of these conferences?

245
00:13:09,520 --> 00:13:14,520
You know, I would say that it's become less and less, right?

246
00:13:15,320 --> 00:13:19,880
So I think as we kept adding more conferences,

247
00:13:19,880 --> 00:13:22,680
and as you know, many of these conferences

248
00:13:22,680 --> 00:13:26,520
are spread out across geographic regions, right?

249
00:13:26,520 --> 00:13:31,280
So we have conferences in the US,

250
00:13:31,280 --> 00:13:33,840
but we have conferences in Europe and Asia.

251
00:13:34,720 --> 00:13:39,400
And so yeah, I've found myself with less and less time.

252
00:13:39,400 --> 00:13:41,280
So to be honest.

253
00:13:41,280 --> 00:13:45,880
So now I'm much more less of a practitioner,

254
00:13:45,880 --> 00:13:50,160
more of basically watcher from afar.

255
00:13:50,160 --> 00:13:54,640
But I think that my background and my ability

256
00:13:54,640 --> 00:13:59,120
to read the original papers and talk to the researchers,

257
00:13:59,120 --> 00:14:02,240
many of which whom I've known for many years,

258
00:14:02,240 --> 00:14:05,520
I think I still have kind of some feel,

259
00:14:05,520 --> 00:14:08,800
but maybe not as much of the hands-on feel

260
00:14:08,800 --> 00:14:10,320
that I would like.

261
00:14:10,320 --> 00:14:13,480
But I would say that the trade-off for that though

262
00:14:13,480 --> 00:14:18,160
is I've gained a much more global perspective, right?

263
00:14:18,160 --> 00:14:22,880
So I don't know to what extent you've organized events,

264
00:14:22,880 --> 00:14:27,640
but you know, I mean, for us,

265
00:14:27,640 --> 00:14:31,480
particularly since we organize events across the world,

266
00:14:31,480 --> 00:14:35,360
you can't really just take your set of speakers

267
00:14:35,360 --> 00:14:36,520
from California.

268
00:14:36,520 --> 00:14:40,960
And take them to somewhere else, right?

269
00:14:40,960 --> 00:14:45,040
So you really have to know the local scene,

270
00:14:45,040 --> 00:14:49,280
the local companies, the local communities.

271
00:14:49,280 --> 00:14:54,280
And so to me, I kind of found that very kind of rewarding,

272
00:14:55,280 --> 00:15:00,280
so that I know the data scene in Southeast Asia,

273
00:15:00,480 --> 00:15:03,520
in China, in Europe, and things like that.

274
00:15:03,520 --> 00:15:08,240
So one thing that if we could maybe spend some time

275
00:15:08,240 --> 00:15:11,080
talking through a little bit of the kinds of problems

276
00:15:11,080 --> 00:15:14,840
that you tend to see at O'Reilly with regard

277
00:15:14,840 --> 00:15:18,920
to data science and machine learning in AI

278
00:15:18,920 --> 00:15:21,840
and the types of approaches and techniques

279
00:15:21,840 --> 00:15:24,600
that you are using to solve them,

280
00:15:24,600 --> 00:15:27,520
I think listeners would enjoy hearing a little bit

281
00:15:27,520 --> 00:15:29,200
at that detail.

282
00:15:29,200 --> 00:15:33,680
I think at this, like many companies,

283
00:15:33,680 --> 00:15:35,600
this is not going to be a surprise.

284
00:15:37,080 --> 00:15:40,920
O'Reilly is an older company, it's not a startup.

285
00:15:40,920 --> 00:15:45,920
So we do have many, many different systems,

286
00:15:49,120 --> 00:15:53,880
some are kind of the new bleeding edge open source system,

287
00:15:53,880 --> 00:15:58,800
some are older proprietary systems, right?

288
00:15:58,800 --> 00:16:00,760
And so I actually, to believe it or not,

289
00:16:00,760 --> 00:16:03,000
one of our main problems is still

290
00:16:03,000 --> 00:16:05,960
to this day data integration, you know?

291
00:16:05,960 --> 00:16:10,960
I mean, just because we do have many, many systems,

292
00:16:11,040 --> 00:16:14,760
just getting the data all together in one place

293
00:16:14,760 --> 00:16:17,280
is it remains a challenge.

294
00:16:17,280 --> 00:16:19,840
And then beyond that, I think,

295
00:16:20,800 --> 00:16:25,800
luckily we do have a team dedicated to the data engineering part.

296
00:16:25,800 --> 00:16:30,800
If it remains a, for us, it remains a work in progress

297
00:16:30,800 --> 00:16:35,800
because we also keep adding systems that we're using.

298
00:16:36,160 --> 00:16:40,000
You know, there's many, many software as a service,

299
00:16:40,000 --> 00:16:41,600
DCs, right?

300
00:16:41,600 --> 00:16:44,240
So different parts of the company starts using,

301
00:16:44,240 --> 00:16:45,720
start using different things.

302
00:16:45,720 --> 00:16:48,680
So that's one problem, the other problem is still,

303
00:16:48,680 --> 00:16:53,680
I think a lot of our data is still unstructured, right?

304
00:16:53,680 --> 00:16:56,160
So I mean, I guess there's some structure.

305
00:16:56,160 --> 00:16:58,720
I mean, so if you think about books,

306
00:16:58,720 --> 00:17:00,240
there's some structure there, right?

307
00:17:00,240 --> 00:17:03,040
So, but we also now,

308
00:17:03,040 --> 00:17:06,040
I don't know to what extent you're following

309
00:17:06,040 --> 00:17:07,800
our Safari platform,

310
00:17:08,880 --> 00:17:12,600
which is increasingly relying on, for example, video, right?

311
00:17:12,600 --> 00:17:15,840
So, okay.

312
00:17:15,840 --> 00:17:20,840
So there's a lot of data that we rely on

313
00:17:20,840 --> 00:17:23,000
that remains unstructured.

314
00:17:23,000 --> 00:17:28,000
And one of the, one of the challenges for us,

315
00:17:28,000 --> 00:17:31,200
too, as a kind of a media company

316
00:17:31,200 --> 00:17:36,200
that is building a learning platform for training

317
00:17:38,080 --> 00:17:42,360
is to take all of these many, many data sources,

318
00:17:42,360 --> 00:17:45,160
both structured and unstructured and organized.

319
00:17:45,160 --> 00:17:50,160
So it turns out actually that, you know, search

320
00:17:53,040 --> 00:17:57,600
and a nice human curated taxonomy

321
00:17:57,600 --> 00:18:00,480
is still kind of,

322
00:18:00,480 --> 00:18:03,320
remain the basic problems for companies like us, right?

323
00:18:03,320 --> 00:18:07,880
So let's say, for example, you wanted to learn something

324
00:18:07,880 --> 00:18:11,880
in a new field of machine learning, right?

325
00:18:11,880 --> 00:18:14,560
So we may have thousands and thousands of sources

326
00:18:14,560 --> 00:18:18,160
because our Safari platform doesn't just rely on our content,

327
00:18:18,160 --> 00:18:20,760
it relies on our content partners as well.

328
00:18:21,880 --> 00:18:25,200
So we will have to organize, you can do a search,

329
00:18:25,200 --> 00:18:28,200
so that's one way for you to probably navigate

330
00:18:28,200 --> 00:18:29,960
our Safari platform.

331
00:18:29,960 --> 00:18:33,680
But increasingly, we've, we're finding that

332
00:18:33,680 --> 00:18:36,840
people want kind of curated content, right?

333
00:18:36,840 --> 00:18:39,880
So how do I learn about this new topic?

334
00:18:39,880 --> 00:18:43,960
Well, we'll use kind of the combination

335
00:18:43,960 --> 00:18:47,680
of humans and machines to organize a learning path for you

336
00:18:47,680 --> 00:18:52,680
or a resource center inside Safari.

337
00:18:53,360 --> 00:18:58,360
So I think that whole taxonomy creation

338
00:19:01,320 --> 00:19:05,080
and grappling with data integration

339
00:19:05,080 --> 00:19:10,080
and unstructured data, those are our main challenges.

340
00:19:10,080 --> 00:19:15,080
And have you invested much in personalization using

341
00:19:16,760 --> 00:19:19,720
some of the machine learning and AI techniques

342
00:19:19,720 --> 00:19:21,880
that folks are using for that kind of stuff?

343
00:19:21,880 --> 00:19:26,880
I think I would say we're still in the early phases

344
00:19:26,920 --> 00:19:28,640
work in progress.

345
00:19:28,640 --> 00:19:30,560
Yeah, that's a good point to be honest

346
00:19:30,560 --> 00:19:34,160
because people learn differently, right?

347
00:19:34,160 --> 00:19:38,720
So each individual learn differently from another.

348
00:19:38,720 --> 00:19:42,360
And so to the extent that we are,

349
00:19:42,360 --> 00:19:45,160
at least on our online division,

350
00:19:45,160 --> 00:19:49,720
we are trying to build the best learning platform.

351
00:19:49,720 --> 00:19:52,040
So a lot of that will increasingly

352
00:19:52,040 --> 00:19:54,160
have to rely on personalization.

353
00:19:54,160 --> 00:19:58,000
But I would say we're still in the early, early stages.

354
00:19:59,480 --> 00:20:03,800
And do you have a particular vision in mind

355
00:20:03,800 --> 00:20:08,800
for how the technology is put to use,

356
00:20:09,680 --> 00:20:14,120
to enable a certain kind of experience?

357
00:20:14,120 --> 00:20:16,960
Like do you have a sense for what that experience

358
00:20:16,960 --> 00:20:18,680
looks and feels like and how it's different

359
00:20:18,680 --> 00:20:21,040
from what someone might experience today

360
00:20:21,040 --> 00:20:25,000
and then what the supporting pieces might need to be?

361
00:20:25,000 --> 00:20:27,680
Yeah, in our case, because for example,

362
00:20:27,680 --> 00:20:32,680
many Safari users use them through their companies.

363
00:20:32,680 --> 00:20:34,560
Let's say you work for a large company

364
00:20:34,560 --> 00:20:39,560
that has a subscription to Safari.

365
00:20:40,560 --> 00:20:41,560
Right.

366
00:20:41,560 --> 00:20:44,960
So it will be kind of a combination of you

367
00:20:44,960 --> 00:20:48,480
us serving you content personalized to you,

368
00:20:48,480 --> 00:20:51,360
but us also serving you content

369
00:20:51,360 --> 00:20:56,520
that kind of reflects what your particular organization

370
00:20:56,520 --> 00:21:01,520
is emphasizing or the rest of your team members

371
00:21:01,520 --> 00:21:03,680
are learning about.

372
00:21:03,680 --> 00:21:07,160
So, yeah.

373
00:21:07,160 --> 00:21:08,760
And then the other thing Sam actually,

374
00:21:08,760 --> 00:21:10,840
I should mention is that inside Safari,

375
00:21:10,840 --> 00:21:15,920
we now have live online training on many of the topics

376
00:21:15,920 --> 00:21:18,000
that your listeners might be interested in,

377
00:21:18,000 --> 00:21:23,000
including big data, infrastructure and architecture,

378
00:21:24,560 --> 00:21:28,440
machine learning, data science,

379
00:21:28,440 --> 00:21:32,360
and increasingly we're finding actually

380
00:21:32,360 --> 00:21:37,200
there's a lot of demand for content

381
00:21:37,200 --> 00:21:41,320
that I would describe as much more non-technical.

382
00:21:41,320 --> 00:21:44,440
So a lot of people are grappling with.

383
00:21:44,440 --> 00:21:46,760
They read about a specific topic.

384
00:21:48,040 --> 00:21:51,640
They may not need to implement it right away,

385
00:21:51,640 --> 00:21:55,160
but they need to know at a high level what it is about

386
00:21:55,160 --> 00:21:59,400
and should they be bringing that into their company.

387
00:21:59,400 --> 00:22:04,400
And if so, what are some of the steps they should do

388
00:22:05,480 --> 00:22:08,840
to integrate such and such technology

389
00:22:08,840 --> 00:22:11,520
or technique into their existing products?

390
00:22:15,040 --> 00:22:20,040
Well, so you talked a little bit about the kind of exposure

391
00:22:20,360 --> 00:22:23,880
you get in terms of in your roles

392
00:22:23,880 --> 00:22:25,800
with the different conferences.

393
00:22:26,800 --> 00:22:30,920
I'm interested in kind of taking your temperature on,

394
00:22:30,920 --> 00:22:33,080
you know, the various trends you're seeing out there

395
00:22:33,080 --> 00:22:36,560
and what you're finding most interesting.

396
00:22:36,560 --> 00:22:37,560
Ah, so good.

397
00:22:37,560 --> 00:22:41,040
So I just gave a keynote about this in Israel yesterday.

398
00:22:41,040 --> 00:22:41,880
Oh, really?

399
00:22:41,880 --> 00:22:42,880
Oh, awesome.

400
00:22:42,880 --> 00:22:47,880
Yeah, so I would say on the machine learning side,

401
00:22:49,720 --> 00:22:52,560
last year I kind of told people that this year

402
00:22:52,560 --> 00:22:56,800
I think deep learning will become a machine learning technique

403
00:22:56,800 --> 00:22:59,640
that the people in the data science community

404
00:22:59,640 --> 00:23:00,800
will start using.

405
00:23:02,600 --> 00:23:05,200
So as you know, deep learning is a lot more associated

406
00:23:05,200 --> 00:23:06,800
with the other conference that I run,

407
00:23:06,800 --> 00:23:10,360
which is the AI conference, where they're grappling

408
00:23:10,360 --> 00:23:15,360
with data from images and video and audio,

409
00:23:17,480 --> 00:23:20,640
also computer vision and speech technologies

410
00:23:20,640 --> 00:23:21,920
and things like that.

411
00:23:21,920 --> 00:23:25,160
So what we're seeing is there's a lot of hunger

412
00:23:25,160 --> 00:23:28,640
in the data science community to see

413
00:23:28,640 --> 00:23:32,600
if they can take deep learning and use it

414
00:23:32,600 --> 00:23:36,480
to replace some other existing machine learning technique

415
00:23:36,480 --> 00:23:37,840
that they're using.

416
00:23:37,840 --> 00:23:40,360
So for example, some people are looking at it

417
00:23:40,360 --> 00:23:43,640
in terms of recommender systems.

418
00:23:43,640 --> 00:23:45,880
Some people are looking at it to replace

419
00:23:45,880 --> 00:23:48,600
how they do search rankings and things like that.

420
00:23:48,600 --> 00:23:52,080
So that's one trend.

421
00:23:52,080 --> 00:23:55,720
And so on the data science side,

422
00:23:55,720 --> 00:23:58,160
the other thing we're kind of seeing,

423
00:23:58,160 --> 00:24:02,880
and this might at this point be much more of a day area thing

424
00:24:02,880 --> 00:24:08,200
is that people are starting to talk about a role

425
00:24:08,200 --> 00:24:13,040
that is a hybrid between the classic data scientists

426
00:24:13,040 --> 00:24:14,440
and the data engineer.

427
00:24:14,440 --> 00:24:19,440
So a lot of people use the term machine learning engineer.

428
00:24:19,440 --> 00:24:24,440
So what it is is basically someone who is maybe

429
00:24:24,440 --> 00:24:27,440
a little stronger on the software engineering side.

430
00:24:27,440 --> 00:24:32,240
So they write code with the express intention

431
00:24:32,240 --> 00:24:36,160
that this might or this will be deployed into production.

432
00:24:36,160 --> 00:24:38,920
So it's not one off, sloppy.

433
00:24:39,920 --> 00:24:42,680
And then they also tend to think much more holistically.

434
00:24:42,680 --> 00:24:46,400
So if we're going to deploy this into production,

435
00:24:46,400 --> 00:24:49,240
what is our logging infrastructure,

436
00:24:49,240 --> 00:24:52,560
what is our AB testing infrastructure and so on.

437
00:24:52,560 --> 00:24:56,800
And then, yeah.

438
00:24:56,800 --> 00:25:01,800
So then the emphasis is on production

439
00:25:03,680 --> 00:25:05,960
and less on prototypes.

440
00:25:05,960 --> 00:25:06,960
Okay.

441
00:25:06,960 --> 00:25:11,960
And it's interesting how the role, you know,

442
00:25:12,960 --> 00:25:16,000
these roles just keep being redefined, right?

443
00:25:16,000 --> 00:25:18,040
I think a few years ago,

444
00:25:19,040 --> 00:25:21,960
the big conversation was that we saw,

445
00:25:21,960 --> 00:25:23,880
we actually thought of the data scientists

446
00:25:23,880 --> 00:25:25,600
as this monolithic person, right?

447
00:25:25,600 --> 00:25:27,360
They needed to know how to do everything, right?

448
00:25:27,360 --> 00:25:30,880
They needed to be, you know, statistically savvy,

449
00:25:32,000 --> 00:25:34,560
you know, and know the math behind, you know,

450
00:25:34,560 --> 00:25:37,080
the analytics and the machine learning stuff,

451
00:25:37,080 --> 00:25:41,240
they needed to, you know, understand how to get data

452
00:25:41,240 --> 00:25:43,560
from all these systems because, you know,

453
00:25:43,560 --> 00:25:45,920
their reality was that they spent, you know,

454
00:25:45,920 --> 00:25:47,480
80% of their time or more,

455
00:25:47,480 --> 00:25:49,560
just kind of shuffling data around.

456
00:25:50,080 --> 00:25:52,680
And then we started to get the, you know,

457
00:25:52,680 --> 00:25:55,480
the role started to split off a little bit.

458
00:25:55,480 --> 00:25:59,400
And you started to see, you know, data engineers,

459
00:25:59,400 --> 00:26:02,320
you know, being thought of as separate from, you know,

460
00:26:02,320 --> 00:26:03,560
machine learning people.

461
00:26:03,560 --> 00:26:05,840
And in some places, you'd pair those, you know,

462
00:26:05,840 --> 00:26:10,080
those two with, or I'm sorry, data engineers being,

463
00:26:10,080 --> 00:26:11,960
you know, separate from your data scientists.

464
00:26:11,960 --> 00:26:14,320
And in some places, you'd pair those two with, you know,

465
00:26:14,320 --> 00:26:17,840
your real professional programmers, software engineers.

466
00:26:17,840 --> 00:26:20,040
You know, now it sounds like you're saying,

467
00:26:20,040 --> 00:26:21,920
you know, we kind of came up with a new name.

468
00:26:21,920 --> 00:26:24,640
And it's, you know, again, it's this kind of unicorn

469
00:26:24,640 --> 00:26:26,560
that's supposed to know how to do everything.

470
00:26:27,560 --> 00:26:28,240
Am I reading it?

471
00:26:28,240 --> 00:26:29,400
Am I reading that right?

472
00:26:29,400 --> 00:26:33,720
I would say, no, I mean, I slightly, yeah,

473
00:26:33,720 --> 00:26:36,920
but I think the original term of data scientists

474
00:26:36,920 --> 00:26:40,000
was exactly what you described, which was the unicorn.

475
00:26:40,000 --> 00:26:44,400
This is not someone who has a PhD in machine learning

476
00:26:44,400 --> 00:26:45,320
necessarily, right?

477
00:26:45,320 --> 00:26:47,160
So maybe the background here is much more

478
00:26:47,160 --> 00:26:49,760
of us on the engineering side.

479
00:26:49,760 --> 00:26:54,800
And then they learn it enough machine learning

480
00:26:54,800 --> 00:26:59,800
to know how to basically build machine learning

481
00:26:59,840 --> 00:27:00,880
enabled products.

482
00:27:00,880 --> 00:27:01,880
Got it, got it.

483
00:27:01,880 --> 00:27:04,960
And part of that is that, you know,

484
00:27:04,960 --> 00:27:09,440
the emphasis on production means also knowing

485
00:27:09,440 --> 00:27:13,120
what to do with these models once they hit production, right?

486
00:27:13,120 --> 00:27:18,120
So how to tell when a model gets stale, you know,

487
00:27:18,120 --> 00:27:21,400
and things like that, and when do we need to retrain it?

488
00:27:21,400 --> 00:27:25,240
So, but I would say the background might be much more

489
00:27:25,240 --> 00:27:29,800
on the engineering side and then learn enough machine learning

490
00:27:29,800 --> 00:27:34,800
to start being able to move much more fast, right?

491
00:27:36,440 --> 00:27:39,960
And there might still be data scientists in the organization

492
00:27:39,960 --> 00:27:44,960
to kind of build the prototypes,

493
00:27:45,960 --> 00:27:50,880
but increasingly, I think at least a simpler machine learning

494
00:27:50,880 --> 00:27:54,760
things, maybe this machine learning engineer can take on.

495
00:27:54,760 --> 00:27:58,200
And in fact, actually, if you talk to companies

496
00:27:58,200 --> 00:28:01,400
that use a lot of deep learning,

497
00:28:01,400 --> 00:28:04,600
they even have a much more specific role,

498
00:28:04,600 --> 00:28:06,760
which they call the deep learning engineer, right?

499
00:28:08,360 --> 00:28:10,920
Okay, so then other trends.

500
00:28:10,920 --> 00:28:15,440
So on the data, so the other thing that I pointed,

501
00:28:15,440 --> 00:28:17,960
I've been pointing out to people is this notion

502
00:28:17,960 --> 00:28:22,320
that we've had a lot of progress in machine learning, right?

503
00:28:22,320 --> 00:28:26,280
So you can just read online publications

504
00:28:26,280 --> 00:28:28,440
and there's all sorts of papers being released,

505
00:28:28,440 --> 00:28:30,520
lots of interesting developments, right?

506
00:28:30,520 --> 00:28:33,440
So, and that's great, right?

507
00:28:33,440 --> 00:28:37,360
So, but then I've been, what I've been telling people,

508
00:28:37,360 --> 00:28:41,320
there's an imagine a scenario where nothing happens

509
00:28:41,320 --> 00:28:44,600
on the research front for the next five to 10 years,

510
00:28:44,600 --> 00:28:45,880
or next five years, right?

511
00:28:45,880 --> 00:28:50,880
So, I mean, so my position is that there's still

512
00:28:52,080 --> 00:28:54,560
so much low-hanging fruit in many companies,

513
00:28:54,560 --> 00:28:59,560
including us or Riley, that you can just take what we know now,

514
00:29:00,560 --> 00:29:02,680
you can take what we know now and implement it,

515
00:29:02,680 --> 00:29:05,640
implement it, and we're going to be fine.

516
00:29:06,720 --> 00:29:08,160
We're still going to be fine, right?

517
00:29:08,160 --> 00:29:10,800
So, like I said earlier, we're still grappling

518
00:29:10,800 --> 00:29:12,920
with data integration, right?

519
00:29:12,920 --> 00:29:14,800
Yeah, and I don't think that you guys

520
00:29:14,800 --> 00:29:18,880
are necessarily unique among enterprises.

521
00:29:18,880 --> 00:29:23,880
I think there are some large sophisticated enterprises

522
00:29:24,160 --> 00:29:28,320
that are kind of at the front edge of this thing,

523
00:29:28,320 --> 00:29:31,080
but a lot of folks are really just in the stage

524
00:29:31,080 --> 00:29:33,880
of trying to figure out where it fits

525
00:29:33,880 --> 00:29:37,600
and how to best apply it and where they can extract

526
00:29:37,600 --> 00:29:42,600
the most value and how to put together the teams

527
00:29:42,600 --> 00:29:47,600
to be able to do it because of the talent shortages

528
00:29:48,200 --> 00:29:50,080
as you're well aware.

529
00:29:50,080 --> 00:29:53,080
Yeah, and then to be honest, there's been a lot of progress.

530
00:29:53,080 --> 00:29:57,400
So, now we have to take a lot of those ideas and use them.

531
00:29:59,040 --> 00:30:02,920
And to a large extent, actually, I think that

532
00:30:02,920 --> 00:30:07,920
while we have been fascinated with kind of horizontal platforms,

533
00:30:08,480 --> 00:30:11,800
I think a lot of the interesting applications

534
00:30:11,800 --> 00:30:13,800
of these machine learning models

535
00:30:13,800 --> 00:30:17,320
will be in kind of verticalized applications, right?

536
00:30:17,320 --> 00:30:21,320
So, and I think we'll increasingly see companies

537
00:30:24,080 --> 00:30:28,600
specialized in inserving these industries.

538
00:30:28,600 --> 00:30:32,760
And interestingly, actually, there's a intersection

539
00:30:32,760 --> 00:30:36,760
with the AI community in the sense that

540
00:30:36,760 --> 00:30:41,760
while we read a lot about the general AI,

541
00:30:43,600 --> 00:30:47,880
actually the way the DC community and investors,

542
00:30:47,880 --> 00:30:50,200
at least when you talk to them, have been investing

543
00:30:50,200 --> 00:30:54,720
is they're investing in focused applications, right?

544
00:30:54,720 --> 00:30:59,720
So, whatever that might be, security, drug discovery

545
00:31:00,120 --> 00:31:01,200
and things like that.

546
00:31:01,200 --> 00:31:06,200
So, the other thing that I've been talking to people

547
00:31:06,960 --> 00:31:10,080
about is training data, right?

548
00:31:10,080 --> 00:31:14,200
So, we sometimes forget that a lot of the developments

549
00:31:14,200 --> 00:31:17,360
in deep learning really relied on the existence

550
00:31:17,360 --> 00:31:20,000
of large labeled training data sets.

551
00:31:20,000 --> 00:31:22,920
And to the extent that if you survey companies,

552
00:31:22,920 --> 00:31:25,920
I think that still remains the bottleneck, right?

553
00:31:25,920 --> 00:31:27,800
So, it's not the model.

554
00:31:27,800 --> 00:31:31,520
It's not delivering great models.

555
00:31:31,520 --> 00:31:34,400
It's just coming up with training data.

556
00:31:34,400 --> 00:31:38,160
And there's a lot of interesting things happening, right?

557
00:31:38,160 --> 00:31:39,680
So, in the deep learning community,

558
00:31:39,680 --> 00:31:43,280
you have these generative models

559
00:31:43,280 --> 00:31:48,280
to mostly around generative adversarial networks, right?

560
00:31:49,160 --> 00:31:52,240
But then also in the data science community,

561
00:31:52,240 --> 00:31:54,600
there's interesting work, for example,

562
00:31:54,600 --> 00:31:59,600
by my friend Chris Ray at Stanford, who had a system

563
00:32:00,200 --> 00:32:04,840
called deep dive, and now the next generation is snorkel,

564
00:32:04,840 --> 00:32:08,160
where they basically are able to take

565
00:32:09,840 --> 00:32:11,280
noisier data sources.

566
00:32:11,280 --> 00:32:14,240
So, they start with less labeled data,

567
00:32:14,240 --> 00:32:17,520
supplemented with noisier data sources,

568
00:32:17,520 --> 00:32:20,120
and then they're able to build much more accurate models.

569
00:32:20,120 --> 00:32:22,240
So, I think there's a lot of,

570
00:32:22,240 --> 00:32:24,720
we sometimes forget the importance of data.

571
00:32:24,720 --> 00:32:25,560
Yeah.

572
00:32:25,560 --> 00:32:30,560
And so, I think there'll be still a lot of interesting research

573
00:32:30,560 --> 00:32:35,560
in how we get to training data sets much more efficiently.

574
00:32:35,560 --> 00:32:37,840
And then on the machine learning side,

575
00:32:37,840 --> 00:32:41,520
the other thing that I've been talking a lot about recently

576
00:32:41,520 --> 00:32:45,200
is real time or live data.

577
00:32:45,200 --> 00:32:49,600
So, here I owe my inspiration to the rice lab,

578
00:32:49,600 --> 00:32:52,560
the successor to the amp lab.

579
00:32:52,560 --> 00:32:54,560
So, the amp lab, as people know,

580
00:32:54,560 --> 00:32:57,520
is developed that originated Apache Mesos,

581
00:32:57,520 --> 00:32:59,520
Apache Spark in Alaxia.

582
00:33:00,480 --> 00:33:02,320
So, the new lab, rice,

583
00:33:02,320 --> 00:33:06,720
stands for real time, intelligent, secure execution.

584
00:33:06,720 --> 00:33:10,720
So, live data is basically,

585
00:33:10,720 --> 00:33:14,800
basically think about an agent interacting with an environment,

586
00:33:14,800 --> 00:33:17,680
right, so a user interacting with a website,

587
00:33:17,680 --> 00:33:22,400
a robot navigating its environment, self-driving car,

588
00:33:22,400 --> 00:33:25,840
a player playing a computer assistant player,

589
00:33:25,840 --> 00:33:28,560
playing a computer game,

590
00:33:28,560 --> 00:33:31,240
like an Atari game or go.

591
00:33:31,240 --> 00:33:33,800
So, there you have an environment,

592
00:33:33,800 --> 00:33:36,800
you have an agent interacting with an environment.

593
00:33:36,800 --> 00:33:40,520
So, in the classic reinforcement learning sense,

594
00:33:40,520 --> 00:33:42,600
you're trying to learn a policy, right?

595
00:33:42,600 --> 00:33:44,360
So, given a state of the environment,

596
00:33:44,360 --> 00:33:47,560
what action should, what action should I take?

597
00:33:47,560 --> 00:33:49,960
But, you know, if you actually take a step back

598
00:33:49,960 --> 00:33:53,960
and kind of look at the flow of data

599
00:33:53,960 --> 00:33:56,000
in these types of applications,

600
00:33:56,000 --> 00:33:59,640
the first part looks a little bit like what we've been dealing

601
00:33:59,640 --> 00:34:01,320
with in recent years.

602
00:34:01,320 --> 00:34:04,240
So, it might look like a streaming application.

603
00:34:05,240 --> 00:34:08,440
So, you have data ingestion and things like that,

604
00:34:08,440 --> 00:34:10,280
stream processing and things like that.

605
00:34:10,280 --> 00:34:13,240
But the machine learning part is slightly different, right?

606
00:34:13,240 --> 00:34:15,520
So, in the reinforcement learning sense,

607
00:34:15,520 --> 00:34:17,880
you're trying to learn this policy,

608
00:34:17,880 --> 00:34:20,520
you have to run a lot of simulations,

609
00:34:20,520 --> 00:34:23,400
you have heterogeneous computer graphs,

610
00:34:23,400 --> 00:34:27,000
and if it's truly a live application,

611
00:34:27,000 --> 00:34:31,120
you need to have merely second latency, right?

612
00:34:31,120 --> 00:34:35,280
So, then it turns out the existing frameworks

613
00:34:35,280 --> 00:34:39,200
we have are not able to do machine learning

614
00:34:39,200 --> 00:34:42,640
in these really live dynamic environments.

615
00:34:42,640 --> 00:34:46,280
Yeah, so the classic tools that we've been using

616
00:34:46,280 --> 00:34:50,480
aren't able to do the machine learning

617
00:34:50,480 --> 00:34:51,840
you need in these environments,

618
00:34:51,840 --> 00:34:56,280
which I think increasingly will be much more common, right?

619
00:34:56,280 --> 00:35:00,400
So, as the tools get better,

620
00:35:00,400 --> 00:35:03,360
the use cases will make up much more clear.

621
00:35:03,360 --> 00:35:06,560
So, the people at Rice Lab,

622
00:35:06,560 --> 00:35:08,800
took a look and kind of surveyed,

623
00:35:08,800 --> 00:35:09,640
so what's out there,

624
00:35:09,640 --> 00:35:12,800
and then they realized the computational framework

625
00:35:12,800 --> 00:35:15,960
for these types of applications don't exist, right?

626
00:35:15,960 --> 00:35:20,960
So, then they ended up building one called Ray,

627
00:35:21,480 --> 00:35:23,520
which is out in Alpha,

628
00:35:23,520 --> 00:35:27,360
and Ashley, we're gonna do a tutorial

629
00:35:27,360 --> 00:35:31,800
on reinforcement learning using Ray

630
00:35:31,800 --> 00:35:35,000
at our AI conference in San Francisco.

631
00:35:35,000 --> 00:35:38,080
But the other thing I wanted to emphasize here

632
00:35:38,080 --> 00:35:43,520
is that the interesting thing to me

633
00:35:43,520 --> 00:35:46,880
is kind of machine learning in this live environment, right?

634
00:35:46,880 --> 00:35:47,880
Sorry.

635
00:35:47,880 --> 00:35:49,640
It turns out right now that people

636
00:35:49,640 --> 00:35:51,240
are using reinforcement learning,

637
00:35:51,240 --> 00:35:55,200
but there's other techniques that might emerge.

638
00:35:55,200 --> 00:35:58,240
And the interesting thing about Ray

639
00:35:58,240 --> 00:36:02,080
is that you can use it for reinforcement learning

640
00:36:02,080 --> 00:36:04,240
or other approaches.

641
00:36:04,240 --> 00:36:06,840
So, for example, the OpenAI folks

642
00:36:06,840 --> 00:36:08,560
recently published a paper

643
00:36:08,560 --> 00:36:11,360
where they use evolutionary algorithms, right?

644
00:36:11,360 --> 00:36:14,560
So to solve some of the things

645
00:36:14,560 --> 00:36:17,120
that you would do with reinforcement learning.

646
00:36:17,120 --> 00:36:21,400
But the interesting thing is that Rice Lab people

647
00:36:21,400 --> 00:36:25,200
took that paper and basically they just implemented

648
00:36:25,200 --> 00:36:29,800
this evolutionary algorithm in Ray, no problem.

649
00:36:29,800 --> 00:36:33,080
So, like I said,

650
00:36:33,080 --> 00:36:37,920
I think that the tools are kind of a work in progress

651
00:36:37,920 --> 00:36:40,960
and that includes Ray and as the tools get better,

652
00:36:40,960 --> 00:36:45,040
then people will start using these tools

653
00:36:45,040 --> 00:36:48,360
and then the use cases will be much more clear.

654
00:36:48,360 --> 00:36:53,200
But basically think about any kind of dynamic setting

655
00:36:53,200 --> 00:36:56,800
where you want to be able to take advantage

656
00:36:56,800 --> 00:36:58,160
of machine learning.

657
00:36:58,160 --> 00:36:59,680
And right now I would say Sam,

658
00:36:59,680 --> 00:37:03,320
that my other conference, the AI conference is definitely

659
00:37:03,320 --> 00:37:07,400
much more aware and interested in focus

660
00:37:07,400 --> 00:37:09,400
on these types of applications.

661
00:37:09,400 --> 00:37:11,800
So, for example, we're already seeing

662
00:37:11,800 --> 00:37:14,960
that reinforcement learning content in tutorials

663
00:37:14,960 --> 00:37:17,600
are very popular at that conference.

664
00:37:17,600 --> 00:37:19,520
But I think as the tools get better,

665
00:37:19,520 --> 00:37:23,200
maybe we'll start seeing the data science community

666
00:37:23,200 --> 00:37:25,000
start using them to write.

667
00:37:25,000 --> 00:37:29,000
So, for example, look back three years ago,

668
00:37:29,000 --> 00:37:31,240
deep learning would have been inaccessible

669
00:37:31,240 --> 00:37:33,040
to the data science community.

670
00:37:33,040 --> 00:37:36,840
We didn't write the tools, like TensorFlow, MxNet,

671
00:37:36,840 --> 00:37:39,440
BigDL, and PyTorch, and all these things.

672
00:37:39,440 --> 00:37:41,600
But as the tools got better,

673
00:37:41,600 --> 00:37:43,560
people start kicking the tires, right?

674
00:37:43,560 --> 00:37:45,920
So, go ahead, Sam.

675
00:37:45,920 --> 00:37:49,960
I was just going to drill into your comment

676
00:37:49,960 --> 00:37:52,040
about reinforcement learning.

677
00:37:52,040 --> 00:37:55,480
You know, typically the literature in talking

678
00:37:55,480 --> 00:37:58,840
about reinforcement learning is looking at kind of an agent

679
00:37:58,840 --> 00:38:05,320
exploring an environment and trying to maximize some policy.

680
00:38:05,320 --> 00:38:09,920
So, the off-sighted example is like an agent

681
00:38:09,920 --> 00:38:12,440
being trained to play Atari video games,

682
00:38:12,440 --> 00:38:15,160
like breakout and maximize their score.

683
00:38:16,960 --> 00:38:18,720
Do you have a sense for how this translates

684
00:38:18,720 --> 00:38:23,720
into the real-time streaming machine learning example

685
00:38:23,720 --> 00:38:28,720
that, or even industrial enterprise type scenarios?

686
00:38:31,120 --> 00:38:36,120
You know, I think that the use cases still need to be worked out

687
00:38:37,520 --> 00:38:41,320
by you can imagine a personalization

688
00:38:41,320 --> 00:38:43,160
on the website, maybe, right?

689
00:38:43,160 --> 00:38:46,720
So, where you're interacting with a website,

690
00:38:46,720 --> 00:38:48,720
much like you're interacting with a game.

691
00:38:48,720 --> 00:38:49,720
Right.

692
00:38:49,720 --> 00:38:54,720
I mean, it's on the AI side, you're gonna see applications

693
00:38:55,680 --> 00:38:59,800
in autonomous vehicles and drones.

694
00:38:59,800 --> 00:39:00,640
Sure.

695
00:39:02,480 --> 00:39:04,560
Maybe in inventory management,

696
00:39:04,560 --> 00:39:08,760
if you really need real-time inventory management,

697
00:39:08,760 --> 00:39:13,400
definitely the finance people might be interested in this

698
00:39:13,400 --> 00:39:17,120
from trading strategies or portfolio design.

699
00:39:17,120 --> 00:39:22,120
And then resource allocation, if you imagine the scenario

700
00:39:22,600 --> 00:39:27,600
where resource allocation with live data becomes prevalent.

701
00:39:30,280 --> 00:39:35,280
But definitely robots are in any kind of robotic environment,

702
00:39:35,280 --> 00:39:38,560
so a robotic application is already using it.

703
00:39:38,560 --> 00:39:43,560
But I think I imagine a place, a scenario where

704
00:39:43,560 --> 00:39:48,560
if the tools get better, the people will probably

705
00:39:51,240 --> 00:39:53,520
find the right use cases.

706
00:39:53,520 --> 00:39:55,080
Sure, sure.

707
00:39:55,080 --> 00:39:57,080
Yeah, one of the other things that I find interesting

708
00:39:57,080 --> 00:40:00,800
about the whole real-time machine learning scenario,

709
00:40:00,800 --> 00:40:03,560
and let me know if you have come across this

710
00:40:03,560 --> 00:40:04,680
or have any thoughts on this,

711
00:40:04,680 --> 00:40:06,320
but there seems to be,

712
00:40:07,760 --> 00:40:10,560
there seems to be in those kinds of environments,

713
00:40:10,560 --> 00:40:15,080
a merging of traditionally your training and inference

714
00:40:15,080 --> 00:40:16,760
are two totally different things.

715
00:40:16,760 --> 00:40:19,520
And when we're talking about kind of real-time streaming data

716
00:40:19,520 --> 00:40:22,400
more and more, I see people wanting to do things

717
00:40:22,400 --> 00:40:24,400
like active learning where they're,

718
00:40:25,840 --> 00:40:30,840
the learning is real-time in addition to the inference,

719
00:40:30,840 --> 00:40:35,640
which folks more easily do real-time now.

720
00:40:35,640 --> 00:40:37,320
Is that what you're seeing as well?

721
00:40:37,320 --> 00:40:41,360
Uh, yes, to some extent.

722
00:40:41,360 --> 00:40:46,360
I mean, I think that the use cases that I'm interested in

723
00:40:47,880 --> 00:40:50,720
tend to still separate the training inference.

724
00:40:50,720 --> 00:40:53,720
I mean, the use cases that I'm much more familiar with

725
00:40:53,720 --> 00:40:58,240
tend to still separate the training inference.

726
00:40:58,240 --> 00:41:01,360
I think there are people who are kind of pushing

727
00:41:01,360 --> 00:41:06,280
the envelope towards much more of this online learning scenario.

728
00:41:06,280 --> 00:41:08,840
But then, but then now we start getting into the scenario

729
00:41:08,840 --> 00:41:10,040
I just described, right?

730
00:41:10,040 --> 00:41:13,720
So we're just kind of learning really with live data

731
00:41:13,720 --> 00:41:17,960
and interacting with an environment, right?

732
00:41:17,960 --> 00:41:21,200
So where you have,

733
00:41:21,200 --> 00:41:24,560
where simulations and explorations,

734
00:41:24,560 --> 00:41:28,040
the ability to do those at large scale at very low latency

735
00:41:29,560 --> 00:41:31,720
come into play and those scenarios

736
00:41:31,720 --> 00:41:33,400
are really quite different.

737
00:41:33,400 --> 00:41:34,880
Yeah.

738
00:41:34,880 --> 00:41:37,080
And actually, this is a good tag way

739
00:41:37,080 --> 00:41:38,720
to the last thing I wanted to emphasize,

740
00:41:38,720 --> 00:41:42,000
which is compute, right?

741
00:41:42,000 --> 00:41:45,840
So you were in the age of big models,

742
00:41:45,840 --> 00:41:47,680
which is deep learning, big data,

743
00:41:47,680 --> 00:41:52,920
which is the training data and live data and big compute,

744
00:41:52,920 --> 00:41:54,320
right? So as you mentioned,

745
00:41:57,480 --> 00:42:00,480
in this scenario, we need everything, right?

746
00:42:00,480 --> 00:42:03,160
So we need scale, throughput, latency,

747
00:42:03,160 --> 00:42:05,600
all of that with low power consumption.

748
00:42:05,600 --> 00:42:08,040
So I think there's a lot of interesting things

749
00:42:08,040 --> 00:42:12,560
happening there like what is the future infrastructure

750
00:42:12,560 --> 00:42:13,840
for machine learning, right?

751
00:42:13,840 --> 00:42:18,840
So I think those are still very active areas.

752
00:42:19,560 --> 00:42:23,240
A lot of things happening at a rapid clip, right?

753
00:42:23,240 --> 00:42:28,240
So both from the GPU side, the CPU side,

754
00:42:28,240 --> 00:42:33,240
the CPU side, FPGAs and ASICs and all of that, right?

755
00:42:33,240 --> 00:42:38,240
So, yeah, so I think sometimes people forget

756
00:42:38,240 --> 00:42:41,480
that to make all of this work,

757
00:42:41,480 --> 00:42:42,960
you still need hardware, right?

758
00:42:45,960 --> 00:42:50,840
And hardware, there's a lot of trade offs

759
00:42:50,840 --> 00:42:51,920
when you get to hardware.

760
00:42:51,920 --> 00:42:54,880
Yeah, yeah, and unfortunately, I think for a lot of people

761
00:42:54,880 --> 00:42:57,640
working in the space, you can't forget it enough, right?

762
00:42:57,640 --> 00:43:02,640
I think over time, the level of abstraction

763
00:43:02,880 --> 00:43:06,480
is gonna have to raise where people can just have

764
00:43:06,480 --> 00:43:09,200
the full flexibility to do the things that they wanna do

765
00:43:09,200 --> 00:43:14,200
without having to think about how they can figure their jobs

766
00:43:14,560 --> 00:43:19,120
to run on GPUs or distributed or what have you.

767
00:43:19,120 --> 00:43:22,480
But still, I think a lot of thoughts still has to happen.

768
00:43:22,480 --> 00:43:26,160
More thought than it should be, right?

769
00:43:26,160 --> 00:43:30,240
Yeah, but I think, I think we're getting to the point

770
00:43:30,240 --> 00:43:32,440
where you got these tools for hardware

771
00:43:32,440 --> 00:43:37,440
and software acceleration and then the software libraries.

772
00:43:38,120 --> 00:43:43,120
So I think that for most practitioners,

773
00:43:43,640 --> 00:43:45,280
the only time they'll think about it

774
00:43:45,280 --> 00:43:47,080
is when they look at their bill, right?

775
00:43:47,080 --> 00:43:51,280
So, it's nice.

776
00:43:51,280 --> 00:43:54,560
At least for most practitioners, right?

777
00:43:54,560 --> 00:43:57,560
But then for the bleeding edge researchers

778
00:43:57,560 --> 00:43:59,920
who have to worry really at the low level,

779
00:43:59,920 --> 00:44:03,440
like at the level of interconnects and things like that,

780
00:44:03,440 --> 00:44:06,600
because they're trying to break or set the record

781
00:44:06,600 --> 00:44:09,560
for speech recognition a lot of that still matters.

782
00:44:09,560 --> 00:44:13,280
But for regular people, it's just the cost,

783
00:44:13,280 --> 00:44:16,560
I think, is what they're gonna end up.

784
00:44:16,560 --> 00:44:18,960
That's how they're gonna know what they're using, right?

785
00:44:18,960 --> 00:44:25,960
So the other thing that, or another thing that I often enjoy

786
00:44:26,080 --> 00:44:30,480
chatting with you about is the interesting startups

787
00:44:30,480 --> 00:44:33,480
that are doing interesting things,

788
00:44:33,480 --> 00:44:36,360
both in Silicon Valley and around the world.

789
00:44:36,360 --> 00:44:39,240
What, anything come to mind there,

790
00:44:39,240 --> 00:44:43,840
or I'm particularly interested in ones that we don't hear

791
00:44:43,840 --> 00:44:48,840
about all the time that may be in other parts of the world.

792
00:44:51,920 --> 00:44:55,720
You know, there's a bunch of startups in China

793
00:44:57,000 --> 00:45:02,000
that I'm not sure people here have heard of,

794
00:45:04,720 --> 00:45:09,720
but just generally around applying deep learning

795
00:45:09,720 --> 00:45:13,720
to whatever vertical, right?

796
00:45:13,720 --> 00:45:17,080
So manufacturing, drones.

797
00:45:18,920 --> 00:45:23,520
And to some extent, there's similar applications

798
00:45:23,520 --> 00:45:26,120
that we would see here, but for that market, right?

799
00:45:26,120 --> 00:45:31,120
So for speech recognition and intelligent chatbots

800
00:45:34,080 --> 00:45:35,080
and things like that.

801
00:45:35,080 --> 00:45:40,080
But I would say, actually, so if you look at the AI,

802
00:45:42,040 --> 00:45:46,720
so the one country that I think is really interesting

803
00:45:46,720 --> 00:45:50,560
in terms of its excitement and fascination for AI

804
00:45:50,560 --> 00:45:51,600
is China, right?

805
00:45:51,600 --> 00:45:55,880
So just organizing a conference there

806
00:45:55,880 --> 00:46:00,880
and people just are dying for content in this area.

807
00:46:00,880 --> 00:46:05,880
So in terms of startups, I would say,

808
00:46:06,200 --> 00:46:07,600
I don't know how you feel about it,

809
00:46:07,600 --> 00:46:10,240
but I'm really interested in the startups

810
00:46:10,240 --> 00:46:12,400
that are much more focused.

811
00:46:12,400 --> 00:46:16,240
Yeah, because you know, I think the whole,

812
00:46:16,240 --> 00:46:17,520
if you're gonna do a platform,

813
00:46:17,520 --> 00:46:21,480
it's gonna be tough to compete with these cloud providers,

814
00:46:21,480 --> 00:46:24,320
right, so unless you have a platform

815
00:46:24,320 --> 00:46:26,720
that's focused on a vertical maybe, right?

816
00:46:26,720 --> 00:46:31,320
But, you know, I mean, Amazon, Google, Microsoft,

817
00:46:31,320 --> 00:46:34,400
they have pretty impressive tools for doing,

818
00:46:35,280 --> 00:46:40,080
for doing almost the end to end, right?

819
00:46:40,080 --> 00:46:43,640
So for building these applications.

820
00:46:43,640 --> 00:46:46,240
But if you were very, very focused,

821
00:46:46,240 --> 00:46:48,600
I think that's where you can excel.

822
00:46:49,600 --> 00:46:54,600
So you might be a focused startup on drug discovery

823
00:46:54,600 --> 00:46:58,600
or even actually you can take an area like computer vision, right?

824
00:46:58,600 --> 00:47:01,600
So I happen to advise a startup called Matroid

825
00:47:01,600 --> 00:47:07,600
that's trying to be kind of a computer-fishing enabler

826
00:47:08,600 --> 00:47:10,600
for many companies.

827
00:47:11,600 --> 00:47:13,600
So then they can, they can,

828
00:47:13,600 --> 00:47:17,600
they can, they can take kind of much more of the product approach

829
00:47:17,600 --> 00:47:20,600
in terms of how do companies use this easily

830
00:47:20,600 --> 00:47:22,600
to solve problems, right?

831
00:47:22,600 --> 00:47:27,600
So whatever, whatever, it might be summarizing surveillance cameras

832
00:47:27,600 --> 00:47:29,600
and things like that.

833
00:47:29,600 --> 00:47:33,600
I mean, I guess you can build all of these things yourself

834
00:47:33,600 --> 00:47:36,600
using existing tools or the cloud,

835
00:47:36,600 --> 00:47:39,600
but they already have a product

836
00:47:39,600 --> 00:47:43,600
that your analysts can use, non-programmers, right?

837
00:47:44,600 --> 00:47:49,600
So I'm quite interested in companies like that.

838
00:47:49,600 --> 00:47:52,600
I've been trying to kind of get a,

839
00:47:52,600 --> 00:47:54,600
as you mentioned earlier,

840
00:47:54,600 --> 00:47:57,600
as to whether I pay attention to finance.

841
00:47:57,600 --> 00:47:59,600
I've been recently trying to figure out

842
00:47:59,600 --> 00:48:02,600
what's happening in finance on some of these technologies

843
00:48:02,600 --> 00:48:04,600
and I haven't.

844
00:48:04,600 --> 00:48:07,600
I don't have a quick answer, right?

845
00:48:07,600 --> 00:48:11,600
So it seems like there's a mix of hype and reality.

846
00:48:12,600 --> 00:48:17,600
But finance is kind of also a peculiar industry

847
00:48:17,600 --> 00:48:20,600
in the sense that maybe the most interesting things

848
00:48:20,600 --> 00:48:23,600
are happening in companies who don't want to talk to you.

849
00:48:23,600 --> 00:48:24,600
Yeah, yeah.

850
00:48:24,600 --> 00:48:25,600
Finance can be like.

851
00:48:25,600 --> 00:48:26,600
I know what I mean.

852
00:48:26,600 --> 00:48:29,600
So, so recently, for example, I tried to,

853
00:48:29,600 --> 00:48:32,600
I had one of my editors,

854
00:48:32,600 --> 00:48:38,600
I introduced him to a bunch of companies, right?

855
00:48:38,600 --> 00:48:43,600
So in finance and to try to get a handle on what's happening.

856
00:48:43,600 --> 00:48:45,600
It's just hard.

857
00:48:45,600 --> 00:48:48,600
The people that we think are doing interesting things

858
00:48:48,600 --> 00:48:49,600
don't want to talk.

859
00:48:49,600 --> 00:48:50,600
So.

860
00:48:50,600 --> 00:48:51,600
Yeah.

861
00:48:51,600 --> 00:48:52,600
Yeah.

862
00:48:54,600 --> 00:48:55,600
What else?

863
00:48:55,600 --> 00:48:56,600
So I think,

864
00:48:56,600 --> 00:48:57,600
I think,

865
00:48:57,600 --> 00:48:59,600
like I said, data,

866
00:48:59,600 --> 00:49:03,600
is still going to be an important thing.

867
00:49:03,600 --> 00:49:06,600
So I think people who have interesting data

868
00:49:06,600 --> 00:49:11,600
who are able to take public data and make it usable.

869
00:49:11,600 --> 00:49:12,600
Right?

870
00:49:12,600 --> 00:49:15,600
I guess Chris Ray and Mike Afarella

871
00:49:15,600 --> 00:49:18,600
have this notion of dark data.

872
00:49:18,600 --> 00:49:19,600
Right?

873
00:49:19,600 --> 00:49:20,600
So,

874
00:49:20,600 --> 00:49:23,600
taking data that's very unstructured

875
00:49:23,600 --> 00:49:24,600
in making,

876
00:49:24,600 --> 00:49:26,600
in infusing it with structure

877
00:49:26,600 --> 00:49:29,600
so that you can use it for applications.

878
00:49:29,600 --> 00:49:30,600
Of course,

879
00:49:30,600 --> 00:49:32,600
I think there's still a lot of,

880
00:49:32,600 --> 00:49:36,600
there's still a lot of competitive advantage

881
00:49:36,600 --> 00:49:40,600
to people who have good data.

882
00:49:40,600 --> 00:49:42,600
So what's next?

883
00:49:42,600 --> 00:49:45,600
Well, the AI conference is next literally.

884
00:49:45,600 --> 00:49:48,600
It's coming up at the end of June.

885
00:49:48,600 --> 00:49:50,600
And I mentioned to you

886
00:49:50,600 --> 00:49:54,600
that this podcast is going to be published

887
00:49:54,600 --> 00:49:57,600
at the same time we're announcing a winner

888
00:49:57,600 --> 00:49:59,600
of a giveaway,

889
00:49:59,600 --> 00:50:02,600
a ticket giveaway for the AI conference.

890
00:50:02,600 --> 00:50:05,600
And one question that I had for you was,

891
00:50:05,600 --> 00:50:07,600
you know, I attended the first one.

892
00:50:07,600 --> 00:50:08,600
It was a great event.

893
00:50:08,600 --> 00:50:10,600
I had lots of great conversations there.

894
00:50:10,600 --> 00:50:11,600
I heard great talks.

895
00:50:11,600 --> 00:50:12,600
Yeah.

896
00:50:12,600 --> 00:50:13,600
What's going to be different

897
00:50:13,600 --> 00:50:15,600
about the second event?

898
00:50:15,600 --> 00:50:16,600
So a few things.

899
00:50:16,600 --> 00:50:17,600
One,

900
00:50:17,600 --> 00:50:19,600
the first event was a 2D event.

901
00:50:19,600 --> 00:50:21,600
We did not have training

902
00:50:21,600 --> 00:50:23,600
or tutorials.

903
00:50:23,600 --> 00:50:24,600
So at this event,

904
00:50:24,600 --> 00:50:25,600
we will have both.

905
00:50:25,600 --> 00:50:26,600
So for example,

906
00:50:26,600 --> 00:50:28,600
we have 2D training

907
00:50:28,600 --> 00:50:33,600
on deep learning with TensorFlow.

908
00:50:33,600 --> 00:50:35,600
And then a bunch of other trainings.

909
00:50:35,600 --> 00:50:40,600
And one that stands out is 2D training on,

910
00:50:40,600 --> 00:50:43,600
on NLP with deep learning

911
00:50:43,600 --> 00:50:45,600
with my friend Delip Brown.

912
00:50:45,600 --> 00:50:46,600
Along.

913
00:50:46,600 --> 00:50:48,600
And Delip is also,

914
00:50:48,600 --> 00:50:50,600
Delip is also the organizer of

915
00:50:50,600 --> 00:50:54,600
a fake, fake news challenge.

916
00:50:54,600 --> 00:50:56,600
And so the winners of which

917
00:50:56,600 --> 00:50:59,600
will present at the conference.

918
00:50:59,600 --> 00:51:01,600
Is this generating on the tutorial side?

919
00:51:01,600 --> 00:51:02,600
Detecting.

920
00:51:02,600 --> 00:51:03,600
Yeah.

921
00:51:03,600 --> 00:51:05,600
Yeah, yeah.

922
00:51:05,600 --> 00:51:07,600
And on the tutorial side,

923
00:51:07,600 --> 00:51:10,600
we have a bunch of interesting tutorials

924
00:51:10,600 --> 00:51:13,600
from reinforcement learning.

925
00:51:13,600 --> 00:51:16,600
In this particular edition of the conference,

926
00:51:16,600 --> 00:51:20,600
we're actually going to offer tutorials on a variety of

927
00:51:20,600 --> 00:51:21,600
deep learning frameworks.

928
00:51:21,600 --> 00:51:22,600
Right.

929
00:51:22,600 --> 00:51:23,600
So not just TensorFlow.

930
00:51:23,600 --> 00:51:24,600
We have.

931
00:51:24,600 --> 00:51:25,600
Okay.

932
00:51:25,600 --> 00:51:28,600
Big D L and MX net.

933
00:51:28,600 --> 00:51:29,600
Okay.

934
00:51:29,600 --> 00:51:31,600
We're also trying not to be a deep,

935
00:51:31,600 --> 00:51:36,600
we're trying to be the industry gathering place for AI

936
00:51:36,600 --> 00:51:39,600
where you can learn about many,

937
00:51:39,600 --> 00:51:41,600
many different techniques in AI

938
00:51:41,600 --> 00:51:45,600
and how to use it in your organization or your company.

939
00:51:45,600 --> 00:51:46,600
Okay.

940
00:51:46,600 --> 00:51:47,600
So to that,

941
00:51:47,600 --> 00:51:48,600
and we're also,

942
00:51:48,600 --> 00:51:51,600
we're also going to offer trainings in non-deep learning techniques

943
00:51:51,600 --> 00:51:53,600
like probabilistic programming.

944
00:51:53,600 --> 00:51:54,600
Uh-huh.

945
00:51:54,600 --> 00:51:56,600
And what else?

946
00:51:56,600 --> 00:51:57,600
So the other actually,

947
00:51:57,600 --> 00:52:00,600
so reinforcement learning is a popular tutorial.

948
00:52:00,600 --> 00:52:01,600
It's emerging.

949
00:52:01,600 --> 00:52:02,600
As I mentioned earlier,

950
00:52:02,600 --> 00:52:05,600
and the other popular tutorials have

951
00:52:05,600 --> 00:52:08,600
are once aimed at the non-technical audience, right?

952
00:52:08,600 --> 00:52:09,600
So how do I bring,

953
00:52:09,600 --> 00:52:11,600
how do I manage an AI project?

954
00:52:11,600 --> 00:52:14,600
How do I bring AI back into my company?

955
00:52:14,600 --> 00:52:15,600
Right.

956
00:52:15,600 --> 00:52:16,600
Um,

957
00:52:16,600 --> 00:52:20,600
and then all the keynotes are going to be great.

958
00:52:20,600 --> 00:52:21,600
Right.

959
00:52:21,600 --> 00:52:22,600
So we have,

960
00:52:22,600 --> 00:52:23,600
uh,

961
00:52:23,600 --> 00:52:25,600
David Farucci,

962
00:52:25,600 --> 00:52:26,600
Dave Farucci,

963
00:52:26,600 --> 00:52:28,600
who led the IBM team,

964
00:52:28,600 --> 00:52:30,600
uh, that one jeopardy,

965
00:52:30,600 --> 00:52:31,600
the quiz show.

966
00:52:31,600 --> 00:52:32,600
Okay.

967
00:52:32,600 --> 00:52:33,600
So he hasn't spoken in many,

968
00:52:33,600 --> 00:52:34,600
in many years,

969
00:52:34,600 --> 00:52:36,600
but he has a new research outset

970
00:52:36,600 --> 00:52:38,600
called elemental cognition.

971
00:52:38,600 --> 00:52:39,600
Mm-hmm.

972
00:52:39,600 --> 00:52:41,600
So he's going to give a keynote about what they're up to,

973
00:52:41,600 --> 00:52:42,600
which is basically,

974
00:52:42,600 --> 00:52:43,600
they're trying,

975
00:52:43,600 --> 00:52:45,600
they're taking one of the grand challenges

976
00:52:45,600 --> 00:52:48,600
of AI natural language understanding

977
00:52:48,600 --> 00:52:49,600
and, and basically,

978
00:52:49,600 --> 00:52:50,600
uh,

979
00:52:50,600 --> 00:52:52,600
trying to,

980
00:52:52,600 --> 00:52:53,600
uh,

981
00:52:53,600 --> 00:52:54,600
come up with a system

982
00:52:54,600 --> 00:52:55,600
that can,

983
00:52:55,600 --> 00:52:56,600
uh,

984
00:52:56,600 --> 00:52:57,600
do that well.

985
00:52:57,600 --> 00:52:58,600
Um,

986
00:52:58,600 --> 00:52:59,600
um,

987
00:52:59,600 --> 00:53:01,600
and then, uh,

988
00:53:01,600 --> 00:53:02,600
besides Dave giving a talk,

989
00:53:02,600 --> 00:53:03,600
uh,

990
00:53:03,600 --> 00:53:04,600
one of his colleagues

991
00:53:04,600 --> 00:53:06,600
will do a 40-minute session,

992
00:53:06,600 --> 00:53:07,600
deep dive,

993
00:53:07,600 --> 00:53:08,600
uh,

994
00:53:08,600 --> 00:53:09,600
what, uh,

995
00:53:09,600 --> 00:53:11,600
the technology and techniques

996
00:53:11,600 --> 00:53:12,600
elemental cognition,

997
00:53:12,600 --> 00:53:13,600
uh,

998
00:53:13,600 --> 00:53:14,600
is doing to,

999
00:53:14,600 --> 00:53:15,600
to correct natural language understanding.

1000
00:53:15,600 --> 00:53:16,600
Okay.

1001
00:53:16,600 --> 00:53:17,600
Uh,

1002
00:53:17,600 --> 00:53:18,600
Josh,

1003
00:53:18,600 --> 00:53:18,600
uh,

1004
00:53:18,600 --> 00:53:20,600
Josh Tanenbaum of MIT,

1005
00:53:20,600 --> 00:53:22,600
uh,

1006
00:53:22,600 --> 00:53:23,600
you know,

1007
00:53:23,600 --> 00:53:25,600
I've long been fascinated by,

1008
00:53:25,600 --> 00:53:26,600
uh, what they do.

1009
00:53:26,600 --> 00:53:27,600
So basically,

1010
00:53:27,600 --> 00:53:28,600
they're trying to develop,

1011
00:53:28,600 --> 00:53:29,600
uh,

1012
00:53:29,600 --> 00:53:31,600
techniques

1013
00:53:31,600 --> 00:53:32,600
that make,

1014
00:53:32,600 --> 00:53:34,600
that help machines learn

1015
00:53:34,600 --> 00:53:35,600
and think like people.

1016
00:53:35,600 --> 00:53:36,600
So one,

1017
00:53:36,600 --> 00:53:38,600
I think one of the things that,

1018
00:53:38,600 --> 00:53:39,600
uh,

1019
00:53:39,600 --> 00:53:40,600
deep learning is great at,

1020
00:53:40,600 --> 00:53:41,600
is,

1021
00:53:41,600 --> 00:53:42,600
uh,

1022
00:53:42,600 --> 00:53:42,600
uh,

1023
00:53:42,600 --> 00:53:43,600
perception

1024
00:53:43,600 --> 00:53:44,600
and large scale,

1025
00:53:44,600 --> 00:53:45,600
uh,

1026
00:53:45,600 --> 00:53:46,600
and pattern recognition.

1027
00:53:46,600 --> 00:53:48,600
But it's still,

1028
00:53:48,600 --> 00:53:51,600
it's still relies on a lot of data.

1029
00:53:51,600 --> 00:53:52,600
And so,

1030
00:53:52,600 --> 00:53:53,600
Josh and his crew

1031
00:53:53,600 --> 00:53:54,600
are trying to come up

1032
00:53:54,600 --> 00:53:55,600
with alternative methods

1033
00:53:55,600 --> 00:53:57,600
for maybe taking deep learning

1034
00:53:57,600 --> 00:53:58,600
and infusing it

1035
00:53:58,600 --> 00:54:00,600
with startup knowledge,

1036
00:54:00,600 --> 00:54:01,600
uh,

1037
00:54:01,600 --> 00:54:02,600
making it,

1038
00:54:02,600 --> 00:54:03,600
uh,

1039
00:54:03,600 --> 00:54:04,600
much more efficient,

1040
00:54:04,600 --> 00:54:05,600
and much more similar

1041
00:54:05,600 --> 00:54:06,600
to how people think.

1042
00:54:06,600 --> 00:54:07,600
Okay.

1043
00:54:07,600 --> 00:54:08,600
And then,

1044
00:54:08,600 --> 00:54:08,600
uh,

1045
00:54:08,600 --> 00:54:10,600
I don't know if you followed recently,

1046
00:54:10,600 --> 00:54:12,600
but a group at Carnegie Mellon,

1047
00:54:12,600 --> 00:54:13,600
uh,

1048
00:54:13,600 --> 00:54:15,600
led by Thomas Sandholm,

1049
00:54:15,600 --> 00:54:16,600
one,

1050
00:54:16,600 --> 00:54:17,600
uh,

1051
00:54:17,600 --> 00:54:18,600
uh,

1052
00:54:18,600 --> 00:54:19,600
I'm not a poker player,

1053
00:54:19,600 --> 00:54:20,600
but,

1054
00:54:20,600 --> 00:54:21,600
uh,

1055
00:54:21,600 --> 00:54:22,600
one of these poker tournaments,

1056
00:54:22,600 --> 00:54:23,600
uh,

1057
00:54:23,600 --> 00:54:25,600
where they beat out a bunch of,

1058
00:54:25,600 --> 00:54:26,600
uh,

1059
00:54:26,600 --> 00:54:27,600
human,

1060
00:54:27,600 --> 00:54:28,600
human top human players.

1061
00:54:28,600 --> 00:54:29,600
Okay.

1062
00:54:29,600 --> 00:54:30,600
Uh,

1063
00:54:30,600 --> 00:54:31,600
so similar in,

1064
00:54:31,600 --> 00:54:32,600
you can think of this achievement

1065
00:54:32,600 --> 00:54:33,600
that's basically almost

1066
00:54:33,600 --> 00:54:34,600
at the scale of AlphaGo.

1067
00:54:34,600 --> 00:54:35,600
Right.

1068
00:54:35,600 --> 00:54:36,600
Right.

1069
00:54:36,600 --> 00:54:37,600
People don't,

1070
00:54:37,600 --> 00:54:38,600
uh,

1071
00:54:38,600 --> 00:54:39,600
aren't as aware of it.

1072
00:54:39,600 --> 00:54:40,600
So he's giving

1073
00:54:40,600 --> 00:54:41,600
a key note about this.

1074
00:54:41,600 --> 00:54:42,600
A lot.

1075
00:54:42,600 --> 00:54:43,600
About,

1076
00:54:43,600 --> 00:54:44,600
uh,

1077
00:54:44,600 --> 00:54:45,600
how they,

1078
00:54:45,600 --> 00:54:46,600
uh,

1079
00:54:46,600 --> 00:54:47,600
won the tournament.

1080
00:54:47,600 --> 00:54:48,600
Sounds like a great line up.

1081
00:54:48,600 --> 00:54:49,600
And so,

1082
00:54:49,600 --> 00:54:50,600
yeah,

1083
00:54:50,600 --> 00:54:51,600
and just like the previous conference,

1084
00:54:51,600 --> 00:54:53,600
we have,

1085
00:54:53,600 --> 00:54:55,600
sessions on many of the techniques

1086
00:54:55,600 --> 00:54:57,600
that the people are interested in,

1087
00:54:57,600 --> 00:54:58,600
but much more,

1088
00:54:58,600 --> 00:54:59,600
our focus is,

1089
00:54:59,600 --> 00:55:00,600
uh,

1090
00:55:00,600 --> 00:55:01,600
you know,

1091
00:55:01,600 --> 00:55:03,600
we're also going to try to provide

1092
00:55:03,600 --> 00:55:05,600
a track for people who are interested in,

1093
00:55:05,600 --> 00:55:06,600
uh,

1094
00:55:06,600 --> 00:55:08,600
how to bring these ideas

1095
00:55:08,600 --> 00:55:09,600
and,

1096
00:55:09,600 --> 00:55:10,600
uh,

1097
00:55:10,600 --> 00:55:11,600
technologies and methods

1098
00:55:11,600 --> 00:55:13,600
back into their organizations

1099
00:55:13,600 --> 00:55:14,600
and,

1100
00:55:14,600 --> 00:55:15,600
uh,

1101
00:55:15,600 --> 00:55:17,600
implement them into their products.

1102
00:55:17,600 --> 00:55:18,600
Okay.

1103
00:55:18,600 --> 00:55:19,600
But, uh,

1104
00:55:19,600 --> 00:55:20,600
we also try to,

1105
00:55:20,600 --> 00:55:21,600
uh,

1106
00:55:21,600 --> 00:55:23,600
I invited a bunch of my academic friends,

1107
00:55:23,600 --> 00:55:24,600
uh,

1108
00:55:24,600 --> 00:55:26,600
are going to be speaking at the conference about,

1109
00:55:26,600 --> 00:55:27,600
uh,

1110
00:55:27,600 --> 00:55:28,600
really cool things that,

1111
00:55:28,600 --> 00:55:29,600
uh,

1112
00:55:29,600 --> 00:55:30,600
industry people

1113
00:55:30,600 --> 00:55:31,600
will find interesting,

1114
00:55:31,600 --> 00:55:32,600
and maybe,

1115
00:55:32,600 --> 00:55:34,600
kind of spark a conversation

1116
00:55:34,600 --> 00:55:35,600
and,

1117
00:55:35,600 --> 00:55:36,600
and see how,

1118
00:55:36,600 --> 00:55:37,600
uh,

1119
00:55:37,600 --> 00:55:38,600
we can,

1120
00:55:38,600 --> 00:55:39,600
uh,

1121
00:55:39,600 --> 00:55:40,600
you know,

1122
00:55:40,600 --> 00:55:42,600
be a true gathering place for industry,

1123
00:55:42,600 --> 00:55:43,600
uh,

1124
00:55:43,600 --> 00:55:44,600
interested in,

1125
00:55:44,600 --> 00:55:45,600
uh,

1126
00:55:45,600 --> 00:55:46,600
building AI products.

1127
00:55:46,600 --> 00:55:47,600
Awesome.

1128
00:55:47,600 --> 00:55:48,600
Uh,

1129
00:55:48,600 --> 00:55:49,600
it sounds like it's going to be a great time,

1130
00:55:49,600 --> 00:55:50,600
and I'm,

1131
00:55:50,600 --> 00:55:52,600
certainly looking forward to it.

1132
00:55:52,600 --> 00:55:53,600
Um,

1133
00:55:53,600 --> 00:55:54,600
and it'll be great to,

1134
00:55:54,600 --> 00:55:55,600
you know,

1135
00:55:55,600 --> 00:55:56,600
catch up with you in person once again.

1136
00:55:56,600 --> 00:55:57,600
Cool.

1137
00:55:57,600 --> 00:55:58,600
Yeah, yeah, yeah.

1138
00:55:58,600 --> 00:55:59,600
And, uh,

1139
00:55:59,600 --> 00:56:00,600
at the risk of,

1140
00:56:00,600 --> 00:56:01,600
uh,

1141
00:56:01,600 --> 00:56:02,600
being, uh,

1142
00:56:02,600 --> 00:56:03,600
putting another plugin,

1143
00:56:03,600 --> 00:56:04,600
but, uh,

1144
00:56:04,600 --> 00:56:06,600
we also have an AI conference in San Francisco,

1145
00:56:06,600 --> 00:56:07,600
in September.

1146
00:56:07,600 --> 00:56:08,600
Absolutely.

1147
00:56:08,600 --> 00:56:09,600
Uh,

1148
00:56:09,600 --> 00:56:10,600
and we're still,

1149
00:56:10,600 --> 00:56:11,600
uh,

1150
00:56:11,600 --> 00:56:12,600
uh,

1151
00:56:12,600 --> 00:56:13,600
where I would say 80%

1152
00:56:13,600 --> 00:56:14,600
there,

1153
00:56:14,600 --> 00:56:15,600
as far as completing the lineup,

1154
00:56:15,600 --> 00:56:17,600
but it's already looking great.

1155
00:56:17,600 --> 00:56:18,600
And,

1156
00:56:18,600 --> 00:56:19,600
I'm sure you're,

1157
00:56:19,600 --> 00:56:20,600
you'll be there too, right?

1158
00:56:20,600 --> 00:56:21,600
Of course.

1159
00:56:21,600 --> 00:56:22,600
Yep.

1160
00:56:22,600 --> 00:56:23,600
Looking forward to it.

1161
00:56:23,600 --> 00:56:24,600
Um, is,

1162
00:56:24,600 --> 00:56:25,600
is there,

1163
00:56:25,600 --> 00:56:26,600
uh,

1164
00:56:26,600 --> 00:56:27,600
CFP still open for that,

1165
00:56:27,600 --> 00:56:28,600
or has that been closed out?

1166
00:56:28,600 --> 00:56:29,600
That's been closed out for,

1167
00:56:29,600 --> 00:56:30,600
okay.

1168
00:56:30,600 --> 00:56:31,600
San Francisco.

1169
00:56:31,600 --> 00:56:32,600
Okay.

1170
00:56:32,600 --> 00:56:33,600
All right.

1171
00:56:33,600 --> 00:56:34,600
Great.

1172
00:56:34,600 --> 00:56:35,600
Well, Ben,

1173
00:56:35,600 --> 00:56:36,600
thanks so much for,

1174
00:56:36,600 --> 00:56:37,600
uh,

1175
00:56:37,600 --> 00:56:39,600
taking the time to be on the podcast.

1176
00:56:39,600 --> 00:56:41,600
It was wonderful having you on,

1177
00:56:41,600 --> 00:56:42,600
and again,

1178
00:56:42,600 --> 00:56:44,600
looking forward to seeing you in a few weeks.

1179
00:56:44,600 --> 00:56:45,600
Thank you, sir.

1180
00:56:45,600 --> 00:56:46,600
Thanks.

1181
00:56:46,600 --> 00:56:47,600
Bye-bye.

1182
00:56:47,600 --> 00:56:48,600
Yeah.

1183
00:56:48,600 --> 00:56:49,600
Yeah.

1184
00:56:49,600 --> 00:56:50,600
Yeah.

1185
00:56:50,600 --> 00:56:51,600
Yeah.

1186
00:56:51,600 --> 00:56:52,600
Yeah.

1187
00:56:52,600 --> 00:56:53,600
Yeah.

1188
00:56:53,600 --> 00:56:54,600
Yeah.

1189
00:56:54,600 --> 00:56:55,600
Yeah.

1190
00:56:55,600 --> 00:56:56,600
Yeah.

1191
00:56:56,600 --> 00:56:57,600
Yeah.

1192
00:56:57,600 --> 00:56:58,600
Yeah.

1193
00:56:58,600 --> 00:56:59,600
Yeah.

1194
00:56:59,600 --> 00:57:00,600
Yeah.

1195
00:57:00,600 --> 00:57:01,600
Yeah.

1196
00:57:01,600 --> 00:57:02,600
Yeah.

1197
00:57:02,600 --> 00:57:03,600
Yeah.

1198
00:57:03,600 --> 00:57:04,600
Yeah.

1199
00:57:04,600 --> 00:57:05,600
All right, everyone.

1200
00:57:05,600 --> 00:57:07,600
That's our show for today.

1201
00:57:07,600 --> 00:57:08,600
We love, love, love,

1202
00:57:08,600 --> 00:57:11,600
caring from listeners about the show.

1203
00:57:11,600 --> 00:57:13,600
You can leave your questions and comments

1204
00:57:13,600 --> 00:57:14,600
over on the show notes page

1205
00:57:14,600 --> 00:57:16,600
at twimmalai.com slash talks,

1206
00:57:16,600 --> 00:57:17,600
slash 26,

1207
00:57:17,600 --> 00:57:19,600
where you'll find links to Ben

1208
00:57:19,600 --> 00:57:22,600
and the various resources we mentioned in the show.

1209
00:57:22,600 --> 00:57:23,600
And as always,

1210
00:57:23,600 --> 00:57:25,600
our quote contest continues.

1211
00:57:25,600 --> 00:57:27,600
Just drop us your favorite quote

1212
00:57:27,600 --> 00:57:28,600
on the show notes page

1213
00:57:28,600 --> 00:57:31,600
or your social media network of choice

1214
00:57:31,600 --> 00:57:33,600
and we'll send you a laptop sticker.

1215
00:57:33,600 --> 00:57:34,600
Once again,

1216
00:57:34,600 --> 00:57:36,600
thanks so much for listening

1217
00:57:36,600 --> 00:57:47,600
and catch you next time.

