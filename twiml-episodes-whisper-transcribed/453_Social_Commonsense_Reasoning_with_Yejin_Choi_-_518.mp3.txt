All right, everyone. I am on the line with Yejin Choi. Yejin is a professor at the University of Washington. Yejin, welcome to the Twomo AI podcast.
I'm excited to be here. Thanks for having me. I'm really looking forward to digging into our conversation. I'd love to have you start by sharing a little bit about your background and how you can work in the field of AI.
I primarily work in the area of natural language processing, but like any other subfields of AI, now the boundaries become looser and looser. I'm excited to work on the boundaries between language and vision, language and perception, and also thinking a lot about the connection between AI and human intelligence and what are the fundamental differences in that in terms of knowledge and reasoning.
Let's go a little bit deeper into that. Talk us through some of the ways that you take on those topics in your research portfolio. What are some of the main projects you're working on and the things that you're exploring?
Right, so currently I'm the most excited about the notion of common sense knowledge and reasoning. This was in fact the only dream of AI field. In the 70s and 80s people love to think about it and try to develop formalism for it.
It turns out it's really trivial for humans, but really difficult even for the smartest the people to really think about how to define it formally so that machines can execute it as a program.
So for a long time, scientists assumed that it's a doomed direction because it's just too hard. So AI didn't really thought about common sense for a long time, and then it's only in recent years, some of us got excited to think about it again, which is in part powered by the recent advancements of neural models.
And that that is able to understand large amount of data.
You talked a little bit about the difficulty of or you reference the difficulty of defining common sense reasoning. How do you define it?
Right, so I take a broader definition such that it's everyday knowledge, practical knowledge that most people share in order to function safely and reasonably in our everyday lives.
For example, in general, it's okay to keep your closet door open for a while, but not as okay to keep your fridge door open for a while.
But of course, I can add additional context to make this inference diffisible. So, you know, if the refrigerator is new unplugged from the wall, there's nothing inside than who cares.
So, and again, the closet, you know, if there are rats around, you might want to close the door.
So, it's almost like rules of a thumb that we live by, but the hardness of it is that it's very contextual.
And depending on your cultural backgrounds and everything, the judgments could change.
And yet, it seems that there are this common ground that a lot of people agree with. And so that's the my definite working definition of common sense.
Okay, you described it as rules of thumb is how important is the notion of rules in that.
And I'm thinking about that relative to, you know, we spend a lot of time thinking about deep learning and deep neural networks. And in some ways, there may be more pattern based than rule based.
Is there a notion of fundamental foundational patterns as being akin to common sense, like, you know, the low layers of a neural network learning textures and things like that. Is that kind of common sense in a sense?
Yeah, absolutely. And that's a really sharp question. I appreciate the question. In fact, when I say rules, I use the word very differently from how more formal approaches in AI mean by rules.
So what I mean by rules is really through language. I can describe my rules through language, but you can immediately imagine that these rules are just natural language rules. So there's always a different interpretation you could do.
These are just generally true statement, but it may or may not be true. And it also includes just a declarative knowledge about the, for example, sky is, you know, usually on top of my head.
It might change depending on my location, of course.
Usually it's blue, unless it's red in the evening. And so we have a lot of this knowledge about, you know, I would be surprised if a sky was suddenly pink, because that's less expected.
So there's this expectation about how the physical world works, how the social world works and some of this.
A lot of this is what as a human, we just have this memory of how the world works, but the machines today, not as much.
If you ask a GPT three, for example, how many eyes a horse has in my say three, primarily because people usually don't talk about how many eyes a horse had when they see a horse or ride a horse, they never mention it. It's too obvious.
So, yeah, it's very lucid. It's, in my work, it's all in natural language or in large part, it's in natural language, except when I'm grounding it with vision.
So what is the, how would you articulate the kind of current state of the world or research frontier in terms of common sense, common sense reasoning.
How far along are we.
Yeah, I mean, it's a bit of a moonshot research, I should mention still.
But I think we are seeing the most exciting results I would dare say than ever before, because in my group, we have some research demo running online.
It's a model called the comet. It means common sense transformers built on top of a transformer neural language model.
And it's able to make common sense the inferences about random situations that you might encounter in lives. And it's fascinating about that system is that it has learned from symbolic knowledge graph of some common sense rules, not all because there's no way we can enumerate them all.
And I don't think humans actually really learned from enumerated rules, but what's exciting about current neural language models is that they can generalize out of that symbolic knowledge graph so that it can reason about new events that was never taught directly before.
Sometimes that reasoning is very surprisingly good. It's not perfect yet, but it's a running system that demonstrates the new capability that we've never seen before.
Can you give us a concrete example of the specific task and the kinds of results you're seeing.
It's actually the best to describe the three examples. So if somebody is repelling somebody's attack, you know, it's not very contextualized event. I might say person X repels person wise attack.
Usually we might think, oh, X might be strong person to fight back, because I might just run away. And maybe why did something wrong? Probably I detected beforehand. So we can reason about preconditions post the conditions and causes and effects of their motivations and their mental states probably access on happy about the situation.
And if X successfully repels wise attack, why might either attack back or run away.
Unlikely, why brings a gift and suddenly celebrate, right? So there are uncertainties about the causes and effects and what might happen before and after.
However, we have a reasonable expectation about what are the likely things to happen. So this particular event was in our symbolic knowledge graph, but what's not in the graph that somebody gave me as an adversarial problem to challenge my model was.
What if somebody repelled someone's attack in a chess game? Now, that's a very different situation where it's not about.
Fistified situation anymore. It's really intellectual fight. And our model was able to somehow make an analogy between the two very different situations. And now the model inference is all about having maybe chess game beforehand.
Maybe the person is very intelligent to smart clever person and things like that. So the generalization here is quite exciting.
So in this example, you mentioned that something was in your knowledge graph. What specifically is in the knowledge graph?
So given any center event like a person X repell is a person wise attack in our original atomic knowledge graph, we had nine different inference types, which is generally about people's mental states before and after what they might want to do before and after preconditions and post conditions and things like that.
In our later atomic knowledge graphs, we expanded that to include 24 different inference types.
In the original version, it was very human centric activities that we had as a knowledge graph. But now we also include object centric knowledge as well as event centric knowledge as well.
So that's what's in the graph. And the graph has at the moment 1.3 million if then rose in natural language. So if this happens, then something else might be true, likely to be true.
And are the if then relationships are the conditions, the if conditions and the eventual outcomes represented is that an atomic unit within this graph or are do you have a bunch of conditions and a bunch of outcomes and the graph links those.
Yeah, that's a great question. They are all open free text short natural language phrases. And they are stored as if they're in a graph structure, but in fact, we are going with natural language as much as possible.
There's no logical forms or anything. Okay. Okay. Interesting, interesting, interesting. And hearing you describe the scenario and the example.
I couldn't help but think about a relationship between kind of this common sense reasoning task and in a sense, like storytelling creative storytelling, like you pose the scenario and then you're like, you know, what if this happened?
And maybe this happened and this happened and this happened and all the things that we might expect, you know, an author to do to build a story.
What is that something that you explore at all? What do you think about that relationship?
Absolutely. Yeah, I'm amazed by your question. So in fact, you know, counterfactual reasoning. What if something else happened?
That's really essential part of our reasoning capabilities. And when I think about human reasoning, it's really fascinating because we can also do a lot of the times what I call what researchers call as purse.
Obductive reasoning that was originally proposed by purse long, long time ago. So he was a philosopher and this is obductive reasoning or obduction is about reasoning about the best explanations given partial observations and that that's really important aspect of storytelling and story understanding.
So, you know, normally when we watch a movie, we might be sitting for two hours that might describe a story that happened over, you know, a decade, for example.
We do not need to see all the minor details of everybody involved in the scene because we can connect the dots we can reason about what's not said because in our mind, it's obvious.
That sort of reasoning is obductive reasoning in that we sort of fill in the blank all the time. Sometimes, you know, like another example is you come home, suddenly windows are broken in your house.
They were okay in the morning, then you might infer that maybe thief came in. You don't know for sure, but this almost like pops in your mind right away as opposed to you considering all the other million things that may have happened.
It's almost like we instantaneously have this ability to reason about what's the best likely explanation to a scenario.
And basically what come at the commonsense transformers try to do is like a smaller unit of that sort of a reach reasoning people do.
And so I also work on obductive reasoning as well as counterfactual reasoning as well as a story telling.
And in all of this, I tried to think about this smaller units of reasoning in the form of comments basic more lower level commonsense inferences.
Talk a little bit more about comment. How do you apply transformers to these types of problems?
That's a great question. So now we are entering this knowledge integration when we have this knowledge, how do we integrate for reasoning? The more I work on this areas, I realize that the distinction between knowledge and reasoning, this is in a continuum as opposed to them being two entirely different things in the following sense.
A lot of the times our reasoning is almost memorized knowledge. We don't always recompute the whole thing, but we anything we computed often enough, we already stored in the background the knowledge.
So it's a combination of the two, the more the model memorized, the more likely that it performs better in even for new situations because it's able to draw a lot of knowledge, perhaps it's similar to how humans who read a lot of books probably not guaranteed to be the best in an exam or you know, writing a book, whatever, but probably doesn't hurt it only helps.
So that's that. And when we actually in a technical sense, integrating knowledge, it can be done in many different ways. Sometimes we just provide it as additional textual input for any given QA problems.
Some other times we can integrate that at the continuous level or neural level at the neural representation level. And even for that, there are many different ways of doing it. We could do it more different integration with many layers of transformers or we could do just at a shallow level at the input output layers.
People are trying different versions. That's what we have tried so far.
And so how do you, in your case, how do you prepare your data to train a transformer model around these types of problems?
So didn't talk about that at all. Comed is basically starting off from off the shelf neural language models that have already read a lot of raw data. And then we compile atomic knowledge graph as if it's just additional string of a text.
So the model, the comment is reading atomic knowledge graph as if it's just natural language sentences. And we found that by doing so, the model is able to draw or connect to the dots between the implicit knowledge and the raw text with the more explicit knowledge in the knowledge graph.
And when you say that comment is reading that, do you, is this a training time, or is this your conditioning text when you're doing an inference against your model?
Yeah, that's a during training time. So that during inference, it can operate in a similar way. So given some context, it can then reason about whatever commonsense relationship inference types that we want to look at.
And the, I guess I'm trying to think through how, how do you get to something that has, I guess I'm trying to get to the relationship between, you know, some kind of formal reason, not formal reasoning in the sense of academic formal reasoning.
So it's something, you know, something that is, you know, where there's some actual reasoning happening as opposed to a language model spitting out text based on stuff that it saw that was based on these relationships.
Yeah, yeah, that's a great question. And it's a very, yeah, it's a question that, okay, now I'm going to say something that some people might disagree with.
So I think a lot of academics have this romantic viewpoints about reasoning, you know, it must be something that I cannot easily do, but, you know, computers can do in a very accurate precise manner and it should.
And we should be able to somehow come up with some logical formalism that really spells out all this inductive deductive reasoning that we did in the process.
I think that was actually the reason why in 70s and 80s, the commonsense research didn't go very well because we researchers back then were a bit too hung up on having to come up with this formal language logical forms where some sort of a logical formula that can describe the sort of things we can reason through language.
So I came to this conclusion in recent years that actually language is the best medium for reasoning. In fact, even mathematicians, they cannot do very much of a proof without access to natural language.
If you ask those people who do this all the time, they actually think also a lot through language and, you know, they have to explain what their formulas are supposed to mean through languages, even if they have equations and, you know, logical forms that they define they really cannot do very much without language.
But when we think about how humans learn and how we argue with each other to share our reasoning about an issue.
Everything is a thorough language and the moment we try to invent some other formalism, there's a loss of information or loss of expressiveness.
That loss is so significant so that it may be okay if, you know, I want to only focus on integrating some equations or differentiating some equations then, you know, it's okay to lose the power of natural language.
But if I want to do social commonsense reasoning, physical commonsense reasoning that are that I can describe in natural language and all of it, then we are getting into trouble when we try to translate that content down to some mathematical formalism because we never been able to invent such a language that's equivalent to the power of language.
The part of the problem is that natural language is a bizarre thing. It's just oftentimes ambiguous, which is also why, you know, there's a major discrepancy in the way how people might, I mean, even with the science.
People interpret whether climate change is happening or not, whether the vaccine works or not, one would expect that this, I mean, it's a scientific result. So everybody must be able to interpret it in the same way, but actually not.
So that's a part of the challenge and it seems that we just have to embrace it. So when there's an ambiguity instead of trying to invent a language that does not have an ambiguity, it might be that the agent, AI agent has to be able to work with it.
So it's a very experimental research direction, I would say, but well, empirically, it's been the most promising of all.
I think what I hear you saying, or at least the conclusion that I come to based on what you're saying is that the kind of going back to, you know, using the transformers and language models as part of these common sense reasoning tasks is that the
way you evaluate whether common sense reasoning is happening is if the result that you get is reasonable and not based on some other kind of formal structure, you just have to evaluate the results as opposed to applying some kind of formalism or analysis to the method.
Yeah, that's why I agree with them like it seems like that leaves a lot to ask, but is that essentially kind of the direction you're going.
Kind of, I mean, probably yes, in the sense that I mean, so you know, this is a general question against the deep learning all together though in that, like, can it drive a car by actually learning how to drive a car versus or can it translate language from one language to another?
By actually understanding any of those languages or it's just learning the surface pattern matching and so right now everything surface pattern matching sure, but I think the difference is like.
The in the language translation the task is language translation and we can you know we're only evaluating whether the network is translating the language here you're saying that you know the task is or the objective is reasoning and so how do we evaluate if the system is doing reasoning as opposed to you know producing.
You know doing some task.
It's like pretending to be reasoning but so long as the common sense inferences that it spit out looks common sensical then we're happy and the model performance keeps increasing when judged by humans on events that it's never seen before so we usually only evaluate on events that are brand new because what's the point of.
Memorizing what was in the training data so when we do do that it's been so the first time comment came out in the world was two years ago so every year we see some considerable improvement and it's been only two years machine translation people worked on it for more than a decade so.
Let's not lose hope to soon after two years but machine is not really translating either I mean you know this is a philosophical argument but and I want to redefine reasoning a little bit better because there's an intuitive reasoning like you know that's never guaranteed to be correct and that's
not necessarily what common sense inference is all about because you know when you reason oh maybe somebody broke into the house and stole anything how do you know for sure you cannot know so the correct real correct inductive or deductive reasoning.
Should not allow you to make any judgments whatsoever but then like how do you leave at all like how do you understand story because story understanding is all about abductive reasoning as opposed to inductive or deductive reasoning when people think about the correctness of reasoning oftentimes they're equating that with the induction and deduction which
feels more intellectually reasoning like because a we are more exposed to it and obduction people don't talk about it very much because it's a monster that's hard to really attack for a long time in AI but the truth is every
day human inference is obductive reasoning and that's really what's critical lacking with AI without which AI cannot really have this robustness that humans have like when we encounter
previous lessons in situations we tend to be pretty good at doing the right thing even driving you know we learn to drive really fast we don't have to do all these like tail cases like on seeing previous
zones in cases and you know what do you do with this human that human of human of all sorts you know wearing crazy outfit and we don't need to see any of that and I think there's something really fundamentally lacking in currently which is too
test specific or even data specific not even solving the task of solving only the data set because it's learning all this without learning the proper conceptual knowledge about how the world works so I'm trying to get to that more basic knowledge but it turns out it's
really broad spectral of knowledge that we know about the world and when we want to go for the broad coverage language becomes very important as a medium for reasoning medium for understanding between machine and humans when humans want to check whether machine really know
something or not languages the best communication medium with the machine and so going beyond or digging into the common sense reasoning a little bit more you you pursue research into a couple of
I guess sub areas there one is physical common sense reasoning the other social common sense reasoning can you describe those efforts oh yeah so yeah I think both are very important to so physical common sense reasoning requires perhaps more of the physical
grounding as well perhaps with vision and robotics and so for that I also work on grounded learning of language or grounding with computer vision signals and trying to learn the mapping between the two
and then for social common sense reasoning I think that's really important for AI to actually understand humans like how humans interact with each other and then be polite to be reasonable be fair and now we're entering this
social norms and the moral norms as well because that boundary is not that distinct some of our social common sense knowledge sort of like you know for example it's rude to turn on blender at 5 a.m. if you're living with a roommate
unless the roommate wakes up at 4 but in general 5 a.m. but it's not as morally bad compared to maybe stealing your roommates money I mean you know probably like there are many things that are immoral and there's like you know relative strength of their immoral or moral
implications and again all of these are best to describe the through language and so I worked on paper I think was last year published the last year so it's called the social chemistry or social
one-on-one and it has a lot of annotated rules of a thumbs of social norms moral norms ethical norms and in terms of ethics theory it turns out this line of research that philosophers do there's something called descriptive ethics and this is when people
ask or researchers ask people what would you do if your roommate ran blender at 5 a.m. you know probably don't kill the roommate just because of that so this is like asking in a concrete situation what would they do and we built that symbol
knowledge graph and then again we did something similar to comment which is to build social norm transformers to see whether that can reason about previous ones in situations and again we were seeing very promising results there and I think that should really be integrated into
ideally for example dialogue systems in the future because currently the dialogue agents can be agreeable with anything if even if you say I'm going to kill myself it might just cheer you up to do that or if you say something really
questionable in terms of ethical judgments it might just be agreeable but all of that should change and for that we need a model for social norms moral norms as well.
I'm thinking about you know examples like this day chatbot that learned all kinds of racist things and the like and it sounds like what you're saying is that this is potentially a way for us to model kind of societal norms and and you know I don't know filter is the right word but kind of whatever other
models we build kind of use something like this in conjunction to give the model a sense of appropriateness or whatever the right absolutely.
It's a little bit similar to I mean like when we think about what GPT 2 and 3 know well they read everything including read it and everything including fake news and everything so of course then it's
going to spit out that kind of a language and I don't think the reason why it's a saying all this biased stuff racism and everything is because it's not larger in you know it's not
large enough it's not the scale that will fix this bias or morality issues or ethical issues we just need to teach them what is correct and wrong in the way that we teach our children we you know
tell children we shouldn't do this that simple light we shouldn't do that that's unethical we're willing to teach all of this to humans I think we should more than
willing to teach this to machines in the declarative form so that the machine knows what's right and wrong.
And so at the same time the you know jump side of me that there's incredible challenges with generating ground truth that you might need to train a model when you know humans we don't
agree on in absolute terms you know as to you know for example whether turning on your blender at 5 a.m. is you know better or worse than you know leaving a dish unclean in the sink overnight or
something like that yeah there's no clear hierarchy for most if not many if not most of these scenarios how do you address that kind of ambiguity in your work.
So that ambiguity is what excites me the most because I realize that humans are weird complex beings who are somehow able to navigate through all that and you know even though
you and I may not agree which is worse between the two at least you and I agree that probably not everybody will agree with my decision so we have this
sense of whether you know don't kill people okay this one is probably easy for everybody to agree with but even so you know of course there's always some counter example somebody can come up with in some
culture maybe totally fine to kill your daughter if the door was raped by some other man so I disagree with it but there's such a culture that does exist even in actual human
societies so it's a challenge but the way that I address that me and my students are dressed that in my in our work is by incorporating all of that by asking a lot of
people if you ask a lot of people their disagreement naturally arise in their own notations and also we can even ask them directly do you
think this is what everyone who agree or is this more discretionary question and so then we get that kind of information and then the AI then learns that for this
situation the answers are more like distributed across the different labels whereas for this one it's very skewed onto one thing again that you're
doing it do you think as humans we are good at knowing when other people may disagree with our world view isn't I mean it's not perfect it's not perfect we never are perfect by the way you
must are not perfect you must are confused all the time I mean climate change even yeah yeah exactly yeah you recently did a keynote at the Stanford
HAI foundational models workshop I'd like to have you tell us a little bit about your talk there but before we do tell us a little bit
about the foundational models workshop what was the goal of that that session yeah so that workshop was quite impressive it giving a lot of
attention to basically pre-trained neural models I think they focused a lot on language models in terms of the speakers who were
speaking at that technical session but I think they meant more broadly any neural models trained on large amount of data that
solves as almost like a foundation for other downstream applications and so my talk title was David and the Goliath the art of the leader boarding in the era of
extreme scale neural models because this becomes a bit of a I you know as the neural models scale becomes extreme I would say I
do get a lot of extreme questions from students or journalists asking me these are actual questions ask me whether we will achieve AGI by scaling things up maybe
nothing matters but scaling things up so another extreme question that I often get is what are we supposed to do in a academy I should do is just
all you know go to these companies where people are scaling things up can we do anything impactful from academia so the talk was counter-argument against it because you know
time and again it's not always the case that the winner stays winner forever the giants the you know like there's always new innovations coming from the younger generation
I find it really fascinating why is that the case and you know new startup companies becoming those successful companies and this happens all the time it's not the case that the same company always win all the games
and keep innovating so I do think that there are a lot of things beyond the scale but one counter-argument easy counter-argument that I like to make is that
you know we can't reach to the moon by making the tallest building in the world one inch taller at a time we just can't we have to have a different entirely different game plan so similarly I don't think the scale alone will do it though
scaling laws are real in that it's like denial is almost you you cannot deny it's like it's there it's real
it's a few tile to deny it and however we can be more efficient we can sometimes even win larger models by incorporating different richer learning signals that can include even
grounded learning in the 3D environment or grounded learning with images and videos and then we can also do better knowledge integration symbolic knowledge integration and so that was part of my talk as well and last and
that but list algorithms used to be at the core of computer science when I was a PhD students it felt in some sense more intellectually beautiful because there were just more equations and more algorithms in our papers and I think that can come
back it's just that you cannot use the same kinds of algorithms we need to develop better inference time algorithms for neural networks so that you can do reasoning that neural networks are very bad that so you know when talking to you earlier I was talking mostly about just a
spinning out one word at a time which doesn't feel like hard reasoning or even correct reasoning but in our other work we also do more of this constraint algorithmic inference or reasoning where given logical constraints I have to
go with the algorithm can then do this hard constraints as a faction that might be hard for humans might be easy for humans depending on the scale of it but we found that if we do that we can beat larger scale neural models with the smaller scale
neural models with the power of algorithms logical constraints logical reasoning so I think there are a lot of under explored things there but you know one could always argue back that well even the larger models can benefit from the neural the logical algorithms and that's true
you know if if something is already so good it's just that when something is so good so big so that it's like a GPT three big then most people cannot actually download the model and use it or most people don't even have access to it so there's a I think real needs for making smaller models more powerful at the
moment and so the talk was about that as well.
Does does reasoning in the context that we've talked about it do you think that it requires ultimately some kind of symbolic or algorithmic component or is it possible to do it wholly statistically?
I believe in combination of both I think a statistical or neural approaches can handle some aspects of reasoning really really well but I don't think it can handle you know where's two plus three that's five you know like the
actual real symbolic reasoning is much harder and on a related note I do not believe the neural network in a sequence like sequential just a neural method without actual integration of mathematical concepts can
learn to do mathematics correctly so like you know how many days in a week seven days so let's meet a week a day after this week on you know instead of this Tuesday and like when we do like for example two days before Christmas we know what I refer to
by that neural net or may not be able to and all these like symbolic simple symbolic reasoning is really brittle right now with neural models and I think there has to be some sort of rule symbolic integration into neural network going forward.
And do you how effective are the ways that you know you or others in industry are doing that now you know where where where are they most brittle you know where are they strongest like what are the most interesting things in that integration
at those integration points yeah so the sort of things that I personally tried so we call our algorithm as neural logic decoding because it's like combination of neural stuff with logic constraints we take conjunctive normal forms as the logical forms as the basis of logical forms and then go from there and we developed discrete search algorithm in some sense the classical search that was like text to book AI text to book you know
in all the chapters it's all about search a star search and so it's a search algorithm and that's one way to handle it but in doing so I realized the following which is that the evaluation benchmarks really drive where people focus in terms of research so if the evaluation benchmarks do not require such a logical reasoning in order to do well on the leaderboard then why would the people really bother working on it as much as you know
really bother working on it as much so I think a part of the reason why that's lighting is because existing leaderboards have been more in order to do well on the existing leaderboards oftentimes you have to just focus on scaling things up
and then try to learn superious patterns better instead of doing things more correctly under the hood so as we introduce harder task that actually require true reasoning more correct reasoning I think we can see more research endeavor into that people do work on it it's just that
those are not necessarily the winning recipe for some of the existing or popular benchmarks right now and you do foresee time in which that becomes the case or what do you think
yeah so I think so because I think they will realize that okay they solve the some NLP leaderboards without really solving NLP and so then people start to making new benchmarks
and it turns out it's much harder to make really good benchmarks without exam riders biases what I mean by that is that whenever I was so like the field really
really prefer the multiple choice questions or categorical questions as an exam problems for AI because it's easy to grade you know you don't have to
otherwise we don't know how to auto grade open text answers reliable the downside of it is that machine is are really good at learning solving problems correctly for wrong reasons
just it's almost a clever hunts horse situation where the horse ostensibly can do arithematics well horse can now it's just that it learned how to read the body language of the owner
correctly so there's that going on and I personally believe that there's a fundamental limit in multiple choice questions because there's always exam riders bias sneaking in
and the fact that machine can solve a multiple choice question does not mean that it can actually do inference in real life it should be able to do more like generative
reasoning what I call as a generative reasoning which is you should be able to come up with your own answer like you know house window broke what happened
and then you know you should be able to come up with your own answer as opposed to on Oracle suddenly falls from the sky and provide with the four questions for answers to choose from one of which guaranteed to be true
this is how AI is currently operating with a lot of NOP task but I don't think that's how it should be
you also do some work on language generation and conversational AI may be going back to our exchange about story story generation can you talk a little bit about that work
oh yeah so common sense actually that's a it's a bit of a journey in the past a few years because I start so a while ago several no a few years ago for what it
here so we want the Amazon inaugural Alexa challenge we meaning most students wonderful students I was tagging along at UW University of Washington so but the winning you know at the beginning of this
competition we thought oh we're going to do neural models and reinforcement to learning on tab and do some magic we quickly dropped that idea because we realized that
doesn't work reliably for actual real world the conversational systems you never know what it's going to say we cannot make a single mist I mean you cannot make ethical mistakes that's just not acceptable
but also in terms of controlling the content we found that neural networks are just it's like a child that you never know what it might what they might do so
we decided that it should be more like traditional dialogue system where there's like system diagrams and things are more controlled and templated
and that's how we won the competition back then but somehow it was not until the satisfactory because the reason why we couldn't do that
back in a more neural way is in part because neural network really liked this knowledge about people and the social commonsense knowledge and reasoning capabilities
so that's why we then started to working investing a lot into social commonsense knowledge and as we work on social commonsense knowledge why not also work on physical commonsense knowledge
but so I also like thinking a lot about story writing and you know whether GPT3 can actually write one or not and it's quite interesting when you think about how AI art works these days in that
the paintings and drawings are pretty good it's pretty passable as you know maybe in part because you don't need to worry about this long term coherence
that you have to worry about for language storytelling but with the storytelling I don't I don't know when or if at all
this largest neural models will actually write a good novel it's very human human only capabilities in my mind that the ability to tell a story
because there's so much of inferences and implications and connotations and everything you have to reason about and so it's a rosy grand goal in my mind to do it right
but for now I like thinking about the components the backbone of the story like a structure of a story in terms of the bear backbone like there are usually characters
and relationships between them and things evolve over time therefore we have to somehow represent that memory into some structure and then try to be coherent with it
and these are all fascinating research questions and you know I sometimes address some bits of it in papers but I think we still have a long way to go to make it work
and so the Alexa challenge task was what?
Oh the grand challenge back then was to have a coherent conversation with the humans for 20 minutes and at the beginning I thought even one minute will be hard what 20 minutes
and so we ended up making a system that sometimes it survived the 10 minutes but not 20 it was just impossible and the Amazon was quite clever in scoping that system
designed though in that it was supposed to be more about contentful conversation about some news or events or movies and music and things like that as opposed to random chitchat
it was okay to intervene conversation with a bit of a randomness but you know it was supposed to be more like contentful so that way it was more factually grounded
and people learned something from that conversation something useful something fun and you know and trivia and things like that to keep them entertained
awesome awesome well Asian it has been wonderful chatting with you about your work around common sense reasoning thanks so much for
actually before we do I I'll cut this out I'd love to have you share a little bit of about kind of what's next and where you see your your broad research portfolio going
out of the the areas that you see as being most promising yeah so broadly I'm interested in reducing the gap between AI and human intelligence
especially in the way that AI is not able to learn concepts properly and AI is not able to abstract away conceptual knowledge from its experiences with the raw data
and the fact that AI is not able to learn interactively with the world making hypothesis of its own and then asking for information that it needs on its own
I mean imaging human learning like AI today which is just like a force to to read the lots of the text in a particular order that doesn't make any sense
you you're not able to go back and reread any confusing part or ask a question about that part so I think there's a lot to be done
about the learning paradigm really different learning paradigm so that more meaningful and correct conceptual knowledge will arise from that
learning experience I don't think what my lab has tried with common sense transformers is necessarily the right way to do it
I think it's more like a way a hybrid hybrid version to make the best use of a current deep learning models but not necessarily ideal model
because we're still spoon feeding too much into neural language models and so yeah that's what I would be the most excited to think about going forward
awesome awesome well Eugene thank you so much for taking the time to share a bit about what you're working on very interesting stuff
thank you for having me thank you thank you
