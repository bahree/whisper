WEBVTT

00:00.000 --> 00:04.640
Hey, this is Sam Charrington, host of the Twoma AI Podcast, and today I'm coming to you live

00:04.640 --> 00:10.000
from the Future Frequency Podcast Studio at the AWS re-invent conference in Las Vegas.

00:10.000 --> 00:15.680
Today I'm joined by Kumar Chalapilla. Kumar is general manager for machine learning and AI

00:15.680 --> 00:20.800
services at AWS. Before we get going, please be sure to take a moment to hit that subscribe button

00:20.800 --> 00:24.640
wherever you're listening to today's show. And if you want to check us out here at the studio,

00:24.640 --> 00:29.360
you can bounce over to YouTube if that's not where you're listening now. Kumar,

00:29.360 --> 00:34.720
welcome to the podcast. Thanks for having me, Sam. I'm really excited to chat with you.

00:34.720 --> 00:39.760
You're part of the team that announced one of the big machine learning announcements here at

00:39.760 --> 00:45.120
re-invent this year. Support for geospatial machine learning. We'll be digging into that,

00:45.120 --> 00:49.200
but before we do, I'd love to have you share a little bit about your background and how you

00:49.200 --> 00:54.560
came into the field. Definitely, yeah. I've been in the machine learning field for almost 25

00:54.560 --> 01:00.560
years now. I have a PhD in machine learning from the late 90s. Back then, I was on the research side,

01:00.560 --> 01:04.960
worked at Microsoft Research for the first five years after that. I'm probably best known

01:04.960 --> 01:12.160
for my work on training deep neural nets on GPUs from long before deep learning became a thing.

01:13.040 --> 01:16.960
But more recently, I've been at AWS for two and a half years. Currently,

01:16.960 --> 01:24.240
general manager for several machine learning and AI services, quite a few of them SageMaker related.

01:24.240 --> 01:29.360
A few years before I came to AWS, I worked on self-driving. Last four years before I started,

01:29.360 --> 01:35.440
worked at Uber ATG, which is now part of Aurora, and also Lift Level 5, which is now part of Toyota.

01:35.440 --> 01:39.920
Okay, super excited to be here. So you have to stay close to the exciting spaces.

01:39.920 --> 01:44.160
I've kind of chased, I mean, it's interesting. People say, if you want to do the best physics,

01:44.160 --> 01:48.160
you go to the place where the largest colliders. If you want to do machine learning,

01:48.160 --> 01:54.240
you follow the data and wherever companies have the best to latest data, they have the best problems

01:54.240 --> 01:58.960
for applying machine learning. And my career has kind of followed that. With AWS, it's amazing.

01:58.960 --> 02:03.680
Now, rather than building actual solutions and applications, I actually help customers build

02:03.680 --> 02:08.400
their solutions and democratize machine learning. And SageMaker is a great way to do that.

02:08.960 --> 02:13.520
You mentioned that you cover several services. Can you talk about the ones that you're responsible for?

02:13.520 --> 02:19.920
Definitely. Today, we announced geospatial support in SageMaker. Super excited about that.

02:20.720 --> 02:26.960
I also manage the human-in-the-loop services in SageMaker. For example, SageMaker Ground Truth

02:26.960 --> 02:31.520
is one where folks who want a label data, label data is important for machine learning.

02:31.520 --> 02:35.440
80% of the models built there are built using supervised learning techniques,

02:35.440 --> 02:41.200
which require large amounts of label data. So AWS offers through SageMaker a data-labeling service.

02:41.200 --> 02:49.280
I also own some of the DevOps and Code Guru services that allow AI and ML techniques to be

02:49.280 --> 02:54.400
applied to improve developer operations, things like increasing availability of your services.

02:54.400 --> 03:01.040
Amazon DevOps Guru is the service there. I'm also the GM for Amazon Code Guru,

03:01.040 --> 03:06.000
which helps you use machine learning and automated reasoning to find bugs in your code,

03:06.000 --> 03:11.440
improve your code quality, and also identify security gaps and recommend fixes and so on.

03:11.440 --> 03:15.280
Those are some of the examples. Is there a method to the madness? It seems like a very broad scope

03:15.920 --> 03:21.200
in terms of the diversity of services. Yeah, so I think the way machine learning works is

03:21.200 --> 03:28.480
the applications are persona-based. So I am one of the GMs who actually I'm the GM that owns

03:29.200 --> 03:33.440
services for software engineers and security engineers when they need machine learning and AI.

03:33.440 --> 03:38.960
So you'll see DevOps Guru, you'll see Code Guru and services in that suite. The other one also

03:38.960 --> 03:44.400
human in the loop. Mechanical Turk is a very well-known service, so that's part of my portfolio.

03:44.400 --> 03:50.480
It's the legacy crowd platform for tasking, and then we also have augmented AI,

03:50.480 --> 03:57.200
which allows you to combine a human with the AI service. And that way even if the AI is 80 or 90%

03:57.200 --> 04:02.080
there, but your customers need 95 or 98%, you can have that synergy between the AI that does

04:02.080 --> 04:06.320
most of the heavy lifting, but then you can back off to a human who can then treat like a labeling

04:06.320 --> 04:10.400
task or even a human task so that you get the quality you need without having to wait for the AI

04:10.400 --> 04:14.720
to mature to meet your customer's quality needs. So I think the human in the loop is the other group.

04:15.280 --> 04:19.200
Do you especially something that I think mapping and geospatial is the other area that is kind

04:19.200 --> 04:25.200
of sister areas to that? Well, let's dig into geospatial. I've been in and around that community

04:25.200 --> 04:31.440
for a little bit, not deeply in, but I was at the company that I was at last did a lot of work

04:31.440 --> 04:40.560
with Esri, and we would go to the geomant conferences. And there has been attempts to, you know,

04:40.560 --> 04:46.480
we've been, that community has struggled with the amount of data that has been collected from

04:46.480 --> 04:52.640
satellites. And as the center, as the center sensors, multiplying, get richer,

04:54.400 --> 04:58.480
that those sensors have presented, and that data have presented lots of challenges.

04:58.480 --> 05:05.440
So I'm curious, like why now? Why is now the time that AWS is investing in geospatial?

05:05.440 --> 05:11.360
Yeah, so whenever we look at building an AWS service, whether it's for machine learning and AI,

05:11.360 --> 05:16.000
you look at where is the undifferentiated heavy lifting? What is stopping the latest GIS

05:16.000 --> 05:21.120
expert or the data scientists who want to work with geospatial data? They really want to solve the

05:21.120 --> 05:26.320
ML or the analytics or the AI problem, but in order to get to that problem, a lot of them have to

05:26.320 --> 05:32.480
walk through how do I manage my data? How do I process it? How do I get access to it? So on the one

05:32.480 --> 05:38.720
side, geospatial data is now become ubiquitous. It's very easy to access high resolution,

05:38.720 --> 05:44.480
yet even sub meter level, like 50 centimeter per pixel data is easily accessible. But if you want

05:44.480 --> 05:48.880
to access that, there's a large friction to getting access to that data. There's months of

05:48.880 --> 05:52.480
negotiation, getting a license, getting that data, and most of them think of it as I need to

05:52.480 --> 05:56.320
download the data and then work with it. So with SageMaker geospatial, one of the things we want

05:56.320 --> 06:00.640
to take away was, can you with one click in a Jupyter notebook or SageMaker studio,

06:00.640 --> 06:05.280
can you just get access to the data? And only the data you need, like San Francisco is 50 square

06:05.280 --> 06:10.720
miles of data, not that hard, but with a single click in an area of interest, can you get access to

06:10.720 --> 06:16.480
all of San Francisco data? And now you can run your fancy computer vision models or your downstream

06:17.280 --> 06:21.600
notebook preprocessing or combine it with other data point of interest data. Maybe you have

06:21.600 --> 06:26.880
customers who have given you location data, photos with that long information that you want to

06:26.880 --> 06:31.040
co-late and understand who your closest people are or what are the closest landmarks,

06:31.040 --> 06:37.040
bring point of interest data. So that ability to bring that data very easily into a notebook

06:37.040 --> 06:41.120
experience, or if you want to push something to production, you want to build a pipeline,

06:41.120 --> 06:46.240
and you want to make that like a production pipeline on AWS or AWS SageMaker, how do you make

06:46.240 --> 06:53.040
that easy? So timing-wise, one is unstructured data is growing a lot. It's becoming very

06:53.760 --> 06:59.040
easy to access the data, and we just want to unlock that. And just to get a sense of the scale,

06:59.040 --> 07:03.840
right, it's not even linear scale, this is exponential. Just two years ago, the numbers satellites

07:03.840 --> 07:09.200
in the sky that were doing things like capturing imagery or whether it would be high

07:09.200 --> 07:13.600
perspective or even sorry imagery of the surface of the earth was just less than a thousand.

07:13.600 --> 07:19.280
Now the zero or early next year, it'll be 10,000, 8,000 or 9,000. In another 10 years,

07:19.280 --> 07:24.480
it'll be at 100,000 satellites. Wow. And so, and these are their multipurpose.

07:24.480 --> 07:28.880
Cameras are getting so good that within two years, the cameras on the satellite go obsolete.

07:28.880 --> 07:33.120
Yeah. So companies are not even trying to keep them up there every two years, and the satellites

07:33.120 --> 07:37.120
are like little shoe boxes. They're not these monsters that they used to send out, they cost millions

07:37.120 --> 07:41.040
of dollars. Now they cost tens of thousands, maybe a hundred thousand dollars to get a little

07:41.040 --> 07:44.640
solid up there. And in two years, that thing will slowly come down and when they won't even

07:44.640 --> 07:47.840
hit the ground, it'll just burn out in the atmosphere. Yeah. Right. And so you, you have this,

07:48.480 --> 07:53.520
this capability of sensors everywhere, right? On the internet, we talk about the IoT of

07:53.520 --> 07:59.600
revolution, but even in space, that that capability is coming, how do you bring all that data

07:59.600 --> 08:04.560
in a place where customers can work with it? And you won't believe the scale of data so large that

08:04.560 --> 08:09.200
if somebody wants to work with California, data, just all of California, all of sudden it's like a

08:09.200 --> 08:16.480
10 gigabyte to 10 terabyte image. We're like, I can work with four megabytes or five megabytes.

08:16.480 --> 08:20.640
My iPhone pictures are a few megabytes. I can work with that. You say, oh, we have recognition

08:20.640 --> 08:26.160
in AWS. We can run it through. You're looking at a few like 20 megapixels already state of the art.

08:26.160 --> 08:31.680
Yeah. I'm talking about 20 giga to 20 terapixel images. And we're like, oh, but then if it's computer

08:31.680 --> 08:36.000
vision, you have a deep learning model, you just scan it all the way across and we're like, okay,

08:36.000 --> 08:40.240
if you can do that with a single click or single API call behind the scenes, AWS can say we'll

08:40.240 --> 08:45.280
take care of slicing and dicing it, tiling it, running all your inferences there and then bringing

08:45.280 --> 08:49.600
it back. Or you want to label the same way you do zoom in pan on your favorite mapping app.

08:50.480 --> 08:54.400
The whole world is there, but as you zoom into your neighborhood, and so how do you bring that

08:54.400 --> 08:59.360
capability in an API that makes it easy for people to, so we kind of try to hide all of that and

08:59.360 --> 09:04.800
make it easy for data scientists and GIS experts to work with that. Awesome. Awesome. Not 10, 10, 12

09:04.800 --> 09:10.720
years ago in the space, when you wanted satellite data, there were really two major providers that you

09:10.720 --> 09:18.080
were engaging in these negotiations with. Has that aspect of it diversified as well? Absolutely.

09:18.080 --> 09:24.320
I think the number of startups that are doing satellites today is enormous. I mean,

09:25.040 --> 09:29.520
there's black sky, there's IC, there's, and they have one or two satellites they start with.

09:29.520 --> 09:33.440
And their purpose built satellites. Previously, these things used to like Macsars,

09:33.440 --> 09:38.160
a well-known company, their satellites are millions of dollars. Today, you need to invest,

09:38.160 --> 09:41.760
and that satellite has to serve for 20, 30 years for you to get value out of that. And launching

09:41.760 --> 09:46.080
that satellite is very expensive. Putting anything in geostationary orbit or even low-earth orbit

09:46.080 --> 09:52.000
is very expensive. And so those are very hard to do. That has become easier. The number of

09:52.000 --> 09:57.360
satellite launches, like if you look at SpaceX, just the ability to move payload into near-earth

09:57.360 --> 10:03.920
or geostationary orbit is becoming very accessible. The other side is the sensor suites.

10:04.560 --> 10:09.280
There are satellites built just for detecting methane. NASA is going to launch one,

10:09.280 --> 10:15.040
methane-sat. GHG-sat is a greenhouse gas that companies name it out. GHG-sat. Their whole

10:15.040 --> 10:19.680
purpose is I will set up sensors there for you so that at a certain resolution, I can tell you

10:19.680 --> 10:26.560
how much methane there is. And it's like for sustainability purposes, measuring and sensing is way

10:26.560 --> 10:31.200
better than predicting and modeling and all that. And once you have that, then you can empower a

10:31.200 --> 10:36.640
lot of applications that want that. Hyper spectral is another very big one. The soil vegetation,

10:36.640 --> 10:41.040
they react differently to visible light versus other spectra. So if you want to know moisture

10:41.040 --> 10:45.280
in the soil, you don't use light because that's not that useful. You can detect water versus land,

10:45.280 --> 10:50.640
but if you want to know whether your soil is dry or what kinds of, I mean, how much nitrogen

10:50.640 --> 10:55.040
there is, those kinds of things you can actually detect with hyper spectral. And so their purpose

10:55.040 --> 10:58.480
is to build satellites for ag tech purpose. Of course, clearly surveillance is another one,

10:58.480 --> 11:07.040
people want resolutions. And so to me, even simple things like supply chain, commercial

11:07.040 --> 11:10.800
applications where we know last two years, everybody talks about supply chain, but we're like,

11:10.800 --> 11:15.600
okay, where are you getting information from? Is there, if I could go to the LA port and count

11:15.600 --> 11:18.960
how many containerships are waiting to get in? And they've been sitting there for three days.

11:18.960 --> 11:24.960
So for years now, like counting cars in Walmart to determine, you know, start, you know, try to predict stock

11:24.960 --> 11:28.000
prices, yeah, stock prices thing like things like that. And the beauty of containers, there's only 24,000

11:28.000 --> 11:32.640
containers in the whole world, right? And they're somewhere between East Asia and America and Europe,

11:32.640 --> 11:38.240
they're moving around. They're huge. They're like size of a, you know, a big bus. And even at a half

11:38.240 --> 11:43.360
a meter pixel resolution, or even like at a meter pixel resolution, they show up as nice

11:43.360 --> 11:47.760
tens of pixels by tens of pixels in size. And then they move, but at the satellite images,

11:47.760 --> 11:53.200
they move relatively slowly. So you can detect and track all of them. And you can count them.

11:53.200 --> 11:57.600
Yeah. Some of the resolutions are high enough that you can read the numbers off of these.

11:58.400 --> 12:02.000
And so you can, you don't even have to worry about tracking them over time. You can just kind of

12:02.000 --> 12:06.800
know how many there are. So, and putting that in the hands of somebody who is a commercial,

12:08.240 --> 12:14.320
you know, either of somebody who's predicting the economy, impact of this, or even predicting

12:14.320 --> 12:18.720
the performance of stocks or businesses, you know, financial industries, insurance companies want

12:18.720 --> 12:25.440
to know. Super powerful. So was there something akin to a satellite data marketplace that a

12:25.440 --> 12:31.040
developer can just go to at the beginning of this process, identify what offerings are available,

12:31.040 --> 12:36.800
and use that as the starting place for building models. Yeah. So SageMaker Geospatial currently

12:36.800 --> 12:41.120
offers a catalog. So you can go in there, you can drop down. So for example, Planet Labs,

12:41.120 --> 12:46.960
for Square, they're data sets that we currently support. We also support Sentinel 2 and Landsat

12:46.960 --> 12:52.480
from Amazon Open Data. So Amazon Open Data is also a larger repository of data sets that are

12:52.480 --> 12:56.640
publicly available. They're free for anybody to download and use are using within SageMaker.

12:57.200 --> 13:03.040
So we're starting with the catalog. We want to grow that over time. SageMaker Geospatial is in

13:04.000 --> 13:09.600
public preview right now. And so it'll go GA soon. So we'll offer a wide variety of sources.

13:09.600 --> 13:18.160
Both commercial and open. Yes, both commercial and open. So Amazon Data Exchange has an open

13:18.160 --> 13:23.760
data set catalog. That's pretty rich. For example, NASA first brings all of their Landsat and Sentinel

13:23.760 --> 13:29.200
data to AWS. And we help them process it and we host it. And everybody else who wants that data

13:29.200 --> 13:35.120
downloads it from AWS will continue to do that for more open source data sets. We have commercial

13:35.120 --> 13:39.440
companies like For Square that offer a point of interest data here in Azure Data sets and they're

13:39.440 --> 13:44.880
also available through Amazon Location Service APIs. For Square is a name that I have not heard in a

13:44.880 --> 13:48.640
long time. What are they doing in this space? They are very good. They're well known for their point

13:48.640 --> 13:54.640
of interest data. And I mean, you probably remember the check-ins. Yeah, check-ins 10 years ago.

13:55.440 --> 14:01.760
It's still, I mean, I think of check-ins as like human curated, highly, what I would call it,

14:01.760 --> 14:06.800
curated data sets versus, I mean, I worked with Uber. Somebody said that this is really a bar as

14:06.800 --> 14:11.200
opposed to, you know, some. Yeah, and they check in, right? And you can even have

14:11.200 --> 14:16.880
there's a currency. Plus, it's just one dot. So on the back end, it's so easy. Like, I worked at Uber

14:16.880 --> 14:21.120
and I worked at Lyfton. When you build these right-sharing apps, you know that for safety and for

14:21.120 --> 14:26.960
predicting ETAs and so on, the what the driver app and the writer app talk to the services in the

14:26.960 --> 14:30.880
cloud all the time so that you know where the driver is and you know how far you are from your

14:30.880 --> 14:36.320
destination, all that stuff. Now, they don't, they collect so much data, but if you ask them,

14:36.320 --> 14:39.840
where was the person picked up? Where was the person dropped off? You have to trust whatever

14:39.840 --> 14:44.880
they chose. If I say, I want to be dropped off in Mountain View and I want to be picked up

14:44.880 --> 14:48.640
at the airport, you trust that that's what things happen. But you know how humans are. We're like,

14:48.640 --> 14:54.080
oh, I've got picked up four blocks away or I asked them to drop me off a little bit earlier because,

14:54.080 --> 14:59.200
you know, a lot. Exactly. And so, whereas here when a person says, I checked in, you don't need

14:59.200 --> 15:03.600
that whole stretch of data. You can get really high fidelity data. That's one of the benefits.

15:03.600 --> 15:08.560
Okay. So I think any time humans curate data, it becomes very powerful. It's almost like label data.

15:09.360 --> 15:14.480
I mean, it's still, there's noise and humans do make mistakes. But I think that data is still

15:14.480 --> 15:18.880
crowdsourced through check-ins. Yeah, there's, and of course, and they've kind of grown beyond that.

15:18.880 --> 15:25.520
I believe today, they also aggregate data from multiple sources just to kind of have a complete

15:25.520 --> 15:31.040
data set. Yeah. They're best known for their places data. They also do foot traffic data.

15:31.040 --> 15:36.320
So they know movement of people. So I think they're their places API and the foot traffic data is

15:36.320 --> 15:41.920
pretty good. Okay. Interesting. Interesting. So we've talked about use cases and ag,

15:42.880 --> 15:52.000
financial services, finance, surveillance. Are there, was there any one of those that was

15:52.000 --> 15:57.840
kind of a standout driver for developing this and offering this service?

15:59.360 --> 16:04.640
We, I mean, there's many verticals, right? We are in public preview. So we want to start with things

16:04.640 --> 16:10.560
that we were passionate about and that customer is really cared about. I think AgTech is one where

16:10.560 --> 16:16.640
we've sort of started with AgTech. We also think automotive is important. For example, BMW is using

16:16.640 --> 16:21.680
SageMaker Geospatial to decide where to put things like their charging stations. They have

16:21.680 --> 16:26.880
information about where their drivers and their customers are driving their cars. Things like

16:26.880 --> 16:31.920
today they're gas cars. And so you know where all the gas stations are. And they want to then

16:31.920 --> 16:37.920
figure out if I were to understand the driving profile and the driving behavior of my customers,

16:37.920 --> 16:43.120
I can offer an EV. And I can also decide whether where to put the EVs among the customers I'm

16:43.120 --> 16:49.280
going after so that they don't never have to go too far to get their car charged. So automotive

16:49.280 --> 16:57.040
is one. The third one is insurance. So a lot of our things around predicting and monitoring

16:57.760 --> 17:03.680
forest fires, floods. So insurance companies want to know what is the likelihood of

17:04.480 --> 17:11.120
the impact to a property or impact to an asset that they're evaluating from one or more of

17:11.120 --> 17:16.640
these geospatial signals that you're looking for. So we're talking about folks developing

17:16.640 --> 17:21.600
applications around this type of data. Let's dig into some of the challenges that they tend to

17:22.160 --> 17:27.360
run into. You've talked a little bit already about this idea of taking away the heavy lifting

17:27.360 --> 17:34.400
and you kind of flew by. Things that came back to mind to be like tiling or the rectification,

17:34.400 --> 17:39.440
all these things that you need to do with this data. Talk a little bit about the pain points

17:39.440 --> 17:45.440
that you're trying to solve for customers. Yeah, so the first one is I mentioned data access.

17:45.440 --> 17:50.160
I think a lot of the data scientists really want to be in the notebook and be able to access the

17:50.160 --> 17:55.920
data as if it was from a database or a native frame which they're used to. And so that's one.

17:55.920 --> 18:01.520
I think the intrinsic thing we want to make very easy is create an experience where they can bring

18:01.520 --> 18:06.640
all their data sources in one place so that they can work with the SageMaker Studio or Jupiter

18:06.640 --> 18:12.960
notebook style like environment. The second one is joins. It is surprisingly difficult

18:12.960 --> 18:19.840
to join geospatial data because think of something like GPS traces. You take a right share

18:19.840 --> 18:24.640
from you get picked up at the airport, you get dropped off at home. You think that GPS sensor

18:24.640 --> 18:30.240
on your phone is so good that it will give you all the right locations along the way. It doesn't.

18:30.240 --> 18:35.680
There's noise there is. If you're lucky, it's less than 10 meters. Even the best GPS traces are,

18:35.680 --> 18:39.040
you know, they're pretty noisy. So the first thing people want to do is clean up the data.

18:39.840 --> 18:44.960
So we offer a map matching service which allows you to take a GPS and say, oh I would like you to

18:44.960 --> 18:49.760
match it with this version of an OSM map. And you can go from several thousand data points to a

18:49.760 --> 18:53.680
few dozen data points which are the intersections and the roads you were at. And that can be a

18:53.680 --> 18:57.840
pre-processing engine that will make it very easy for you to later on reason about things.

18:58.640 --> 19:04.880
So that's sort of one direction we are trying to make things easy. The ability to even post

19:04.880 --> 19:10.000
process, people don't want to work with pixels. They start with pixels because that's where the

19:10.000 --> 19:14.320
data is. Most of the data providers will give you pixels. And you mentioned some of the low level

19:14.320 --> 19:20.160
single processing things like orthorectification. So further upstream. Yeah, so we want to enable

19:20.160 --> 19:26.400
the cloud removal is one. Yeah, very simple. Everybody wants it. And you know, and the question is,

19:27.360 --> 19:30.800
I need just need to know where I shouldn't care because I can't see the things

19:30.800 --> 19:36.320
because of cloud cover, right? But there's deeper and deeper things. The technology for working

19:36.320 --> 19:40.160
with geospatial data today, I come from a signal and image processing background before I got

19:40.160 --> 19:46.240
an machine learning my masters was in signal and image processing. And the technology, I mean

19:46.240 --> 19:52.480
computer versions come so far in terms of like phones, languages and text. But that technology

19:52.480 --> 19:59.200
hasn't reached the geospatial space. So you can apply object detection semantic segmentation.

19:59.200 --> 20:05.440
But it's still new. There's no image net for geospatial. Image net for for image classification

20:05.440 --> 20:09.840
is 10 years old. Alex net is 10 years old. It's amazing. It's the 10th anniversary. And that was one

20:09.840 --> 20:14.960
of the pivotal things that brought deep learning and computer vision to the masses. So I think the

20:14.960 --> 20:19.680
geospatial revolution of that kind is happening now. Maybe it's already two years in, but you know,

20:19.680 --> 20:25.040
there's still some ground. And I want to make that easy for people who are bringing the best deep learning

20:25.040 --> 20:30.160
models and apply it to this data. So I'll give you a simple example. The traditional way of doing

20:30.720 --> 20:34.720
geospatial data processing was more around you had vector data, which is like, let's say a road

20:34.720 --> 20:39.520
network. And you have pixel data, which is satellite imagery. And then you have point of interest

20:39.520 --> 20:45.360
data, you know, high-faltower, Statue of Liberty, your favorite restaurant, whatever. And you would

20:45.360 --> 20:51.520
write traditional algorithms, computer science software, and some of them maybe AI where you

20:51.520 --> 20:55.840
write heuristics and rules and algorithms to search and so on. And that's how you'd operate.

20:55.840 --> 21:00.000
Very hard to create a machine, a machine that says, if you give me more data and more labels,

21:00.000 --> 21:04.560
I can get you a better system. With things like ETA prediction, when will my Uber ride arrive,

21:04.560 --> 21:08.240
or when I start taking the Uber ride, how many minutes will it take me to get to my destination?

21:08.240 --> 21:11.920
All those things are predictive models, right? Clearly the distance and the locations matter,

21:11.920 --> 21:15.600
but there's a lot of data that's coming behind it. In order to make those better and better,

21:15.600 --> 21:22.000
you need this data flywheel. Better data, better labels, a better ML model should be iterated

21:22.000 --> 21:26.640
in a flywheel, since get you a better and better model. That is hard to do with traditional

21:26.640 --> 21:33.200
computer science heuristics or even single processing. So I think by bringing these data sets,

21:33.200 --> 21:40.080
there's also a persona change. GIS is the traditional expert. That field is very deep. I have

21:40.080 --> 21:44.960
a lot of respect for that. And there's so many human vagaries that are there on the road network.

21:44.960 --> 21:50.320
Just complexity of that is mind-boggling. Whereas in the future, I think people will just use

21:50.320 --> 21:53.920
large amounts of data and just like language models. They're going to score it around. You don't

21:53.920 --> 21:58.480
need to know grammar and other things to do like breakdown sentences. You'll just train a huge

21:58.480 --> 22:03.840
neural net that's going to train on insane amount of data and just do things for you. And so

22:03.840 --> 22:09.600
bringing that computer vision technology to these spaces will make it very easy. So traditional

22:09.600 --> 22:15.200
things like object detection, semantic segmentation, and the other beautiful thing I like about

22:15.200 --> 22:20.080
geospatial, which is not the same as computer vision, is you can take pictures of people all day

22:20.080 --> 22:25.040
for 10 years or 100 years. People are going to change. Cool. Is there going to change? The

22:25.040 --> 22:30.400
technology will change. Surface of the earth does not change that fast, right? And there's like

22:30.400 --> 22:35.200
planets that... For cars, if you're trying to identify vehicles, top down. There's a finite number

22:35.200 --> 22:39.440
of them. Yeah. And the surface of the earth is finite, right? Things change, but they don't change

22:39.440 --> 22:46.240
that fast. So with every pass of this data of the surface of the earth, you're almost like getting

22:46.240 --> 22:51.040
higher and higher richer information. Now there's some change. One or two percent changes every

22:51.040 --> 22:55.920
while you want to keep up to the date after date with the changes. But it's almost like watching

22:55.920 --> 22:59.280
something that over time you're getting more and more information about. So it's going to get

22:59.280 --> 23:04.080
very, very good. And so I think when you bring computer vision to that, I think the computer

23:04.080 --> 23:09.600
vision models working on geospatial will be way more accurate. And they can bring a lot of value

23:10.400 --> 23:14.880
because things don't change as much and things don't change as fast. We've talked about these

23:14.880 --> 23:21.040
three different types of data, satellite mapping and point of interest. Does someone need to be

23:21.040 --> 23:27.680
building an application that uses all three to be interested in the SafeMaker geospatial ML? Or

23:28.480 --> 23:32.000
if you're just building something around mapping, for example, is it so relevant?

23:32.000 --> 23:37.520
Yeah. You don't need to. In some cases, you can. There's nothing that limits you.

23:37.520 --> 23:41.920
You don't need to. You don't need to, you don't need to combine all three for the application.

23:41.920 --> 23:46.960
I'll give you an example. Let's say, simple thing, let's say you're a mapping company.

23:47.760 --> 23:52.560
You want to find where the roads have changed. You're looking for changes, but you're not looking

23:52.560 --> 23:56.800
for any kind of change, not where the vegetation's growing. You just want to wear the new roads paved

23:56.800 --> 24:01.120
and maybe my map has some roads that are no longer around or with COVID, they've shot them down

24:01.120 --> 24:06.240
or somewhere. So you need to have a deep role. You can build a deep role in that that extracts roads.

24:07.280 --> 24:12.160
And it's just a, it's a bunch of pixels that it lights up as a mask. You can compare that

24:12.160 --> 24:18.880
against your road network because you can always render a vector data set on an image and then you

24:18.880 --> 24:22.560
can look for differences. And then you can have a human in the loop that just looks at the

24:22.560 --> 24:26.640
differences and goes, oh yeah, that's a new highway. We didn't see that. Oh, that old one. Okay,

24:26.640 --> 24:30.560
so you can update your map using that information. Now here, I gave you an example of combining this

24:30.560 --> 24:36.240
two, but you don't have to. A lot of the, let's say you're an ag tech, you have farms. There's no

24:36.240 --> 24:41.200
people use maps today to navigate. You may not even be close to a road. And so there's no such thing

24:42.560 --> 24:47.680
as a road network for farms, but you still have, you have farms that are not going to ask the

24:47.680 --> 24:53.120
question is do I have to care about the satellite imagery component for this to be useful or if

24:53.120 --> 24:58.640
I'm just doing stuff based on mapping, is it helping me? Yes. Yeah. So let's take the example of

24:58.640 --> 25:04.720
GPS traces and road networks. Okay. You have a data scientist who's sitting on 10 million GPS

25:04.720 --> 25:11.360
traces from phones, people, cars, whatever your GPS. They want to clean that data and they want

25:11.360 --> 25:20.640
to understand what is one of the most visited places. And for that, you, you can use map matching to

25:20.640 --> 25:26.400
clean up your data and get it on a registered vector map. Okay. And then you can do post processing

25:26.400 --> 25:30.080
there and that you can use SageMegra.juice patient for doesn't have to use pixels. Okay.

25:30.800 --> 25:35.680
Now, you'll be surprised if you ask a data scientist today to build an ML model, do something.

25:35.680 --> 25:39.520
The first thing they'll do is they'll render that vector graphic into an image,

25:39.520 --> 25:45.120
pass it into a commercial and build a deep net because that technology is like the big hammer.

25:45.120 --> 25:49.040
That's the tool on the toolbox. Yeah. If you ask them to create custom features, they're like

25:50.480 --> 25:55.600
the hand edited hand features is still, but you can still do random forests with XG boost.

25:55.600 --> 26:00.080
Maybe the more veteran machine learning engineer will do it, but the fresh birds are like,

26:00.080 --> 26:04.240
how can I fit this into a deep net? Because then I don't have to think. Yeah. And it's

26:04.240 --> 26:08.560
so most of the time just worked. It's a big hammer, right? Awesome. Awesome.

26:10.080 --> 26:14.240
So we're talking about challenges. We talk about data access and making that easier. We talked

26:14.240 --> 26:25.760
about your study and talk about off-the-shelf models, so helping folks with common tasks. What are

26:25.760 --> 26:33.680
the tasks that you're currently able to help folks with? Yeah. So SageMegra is built for builders.

26:34.640 --> 26:39.200
SageMegra.juice patient will allow a data scientist who really wants to carve out their special

26:39.200 --> 26:45.200
geospatial ML model from scratch. We'll support that. But there are customers, if you go down to

26:45.200 --> 26:49.040
some of the GIS type of customers or customers who are like, I just want to consume something that's

26:49.040 --> 26:57.760
post processed. They're one of the things we're sort of releasing is vegetation indices. You want

26:57.760 --> 27:03.920
to know, I talked about land classification today. You just want to know what parts have certain

27:03.920 --> 27:10.320
properties for land use. It's like classifying. It's a semantic segment. It's like a productive

27:10.320 --> 27:16.320
land. Exactly. If somebody's studying vegetation growth over time, they can then run this model and

27:16.320 --> 27:21.200
just look at the number of pixels and the map of the pixels over time and say, oh, are the forest

27:21.200 --> 27:26.320
shrinking? Are the forest growing? Are the farmland? What is the state of things? And they want to

27:26.320 --> 27:31.600
operate at the output layer. So you can think of it as another image map, but it's post processed.

27:31.600 --> 27:35.280
You don't have to go back to the raw pixels. And the raw pixels come in multispectrum. There's

27:35.280 --> 27:40.400
multiple bands. You may not want to be at that level. So that's one that people are interested in.

27:40.400 --> 27:48.720
So do you think of things like that as pre-processing steps or transformations? Or do you think of

27:48.720 --> 27:53.360
them as pre-trained models that you're offering folks? I try to think of them as pre-trained models,

27:53.360 --> 27:58.160
because in most cases, some of them will be like, I'm happy with it. I'll use it as is. Other

27:58.160 --> 28:03.120
things are like, oh, it's kind of there, but for my data or for my use case, I want to tune it

28:03.120 --> 28:10.240
a bit more. Or can I get a notebook or a solution that I can just then extend from there? So we're

28:10.240 --> 28:17.280
still in that SageMaker middle layer in the AIML stack that we operate. They're builders, but they

28:17.280 --> 28:21.520
aren't your ML engineer or even like, I'm going to spend three months building infrastructure

28:21.520 --> 28:27.120
type of engineer. They're more like, can I quickly get to a model to see how I can build an

28:27.120 --> 28:32.560
application and solve a problem? So a lot of analytics workloads also come in that model.

28:32.560 --> 28:38.080
So my feeling is that the way to think about it is layered. At the very low level, you want to

28:38.080 --> 28:41.680
give them all the power. So they want to work on pixels, we'll give them pixels and the low level

28:41.680 --> 28:48.000
primitives. If they want to work on higher levels over time, there's some standard building blocks

28:48.000 --> 28:52.160
that are there. And we want to make those available so that if people are okay with something

28:52.160 --> 28:57.040
state-of-the-art or even, you know, standard in the industry, not research made me, then they

28:57.040 --> 29:02.640
can just build on top of it. So that way, they don't need to rebuild things that they're okay or

29:02.640 --> 29:07.680
happy with and then build on top of them. So then we went from data access to this catalog of

29:07.680 --> 29:12.480
pre-trained models, but I think jumped over the primitives step. What are some of the primitives

29:12.480 --> 29:17.360
that you provide to make it easier for folks that, you know, want to roll up their sleeves, but,

29:17.360 --> 29:24.960
you know, also are lazy. Like, good engineers are. So we are, the APIs we're providing at the

29:24.960 --> 29:30.560
really low level are closer to computer vision APIs where you can work with pixels, you can work

29:30.560 --> 29:36.400
with vector data in like point-of-interest data or points and polygons. So think of them as

29:37.200 --> 29:42.240
a road network would just be a polygon with renewable things. And we're also making it easy for

29:42.240 --> 29:48.320
them to access data from Amazon location services through here or through Esri and so on. So that

29:48.320 --> 29:53.760
way, they can build a holistic end-to-end app on AWS using both SageMegade.us spatial and Amazon

29:53.760 --> 29:59.440
location services. So that way, anything that you can bring into the notebook or into your pipeline,

29:59.440 --> 30:02.320
your machine learning pipeline, you can combine and mix and match those.

30:05.600 --> 30:10.160
So two examples, maybe, to be very concrete. One is the map matching when I mentioned,

30:10.160 --> 30:15.920
which is more of an algorithm, but we also have reverse geo-quiting. Your phone might release

30:15.920 --> 30:21.840
a lot long for you. You're like, oh, can you tell me if it's an address, if they checked in,

30:21.840 --> 30:24.960
or you know it's a location, what's the closest location, what's the closest intersection,

30:24.960 --> 30:29.200
what's the closest street, those reverse geo-quiting, that's another primitive we offer.

30:31.200 --> 30:38.160
Did you announce, you mentioned BMW as a customer, what other customer examples were announced?

30:38.160 --> 30:41.280
I think I would go with that one. I think automotive is very good.

30:43.680 --> 30:46.320
Is there one use case at BMW or are there multiple?

30:46.320 --> 30:51.440
There were three use cases that we were doing. One was building driver profiles by understanding

30:51.440 --> 30:58.080
their driving behaviors, and they were using map matching as one of the ways in which they could

30:58.080 --> 31:06.000
understand. Things like, you know, are you an aggressive driver or are you a driver who likes

31:06.000 --> 31:11.360
luxury, and so based on that, and their driving styles, they can recommend. Also, if you're close

31:11.360 --> 31:16.960
to an area where EVs are becoming more in the vogue, then you may be open to that.

31:18.320 --> 31:23.920
So that was one use case. They also wanted to figure out where, like, site selection is one,

31:23.920 --> 31:27.760
where a lot of companies make a decision on where should, I mean, the example they give you.

31:27.760 --> 31:34.160
Superchargers. Yeah, superchargers is now, it's sort of interesting, the site selection is exactly

31:34.160 --> 31:38.400
a supercharger location solving problem. But also, the example they give you, where should I open

31:38.400 --> 31:42.720
my next Starbucks? There's so many Starbucks everywhere. I need to find. But the beauty of that is,

31:42.720 --> 31:50.480
that's a pure non-pixel, non-image type of data set where you want population statistics.

31:50.480 --> 31:54.320
You want to know traffic statistics. Anytime you think about opening a business,

31:54.320 --> 31:57.680
like location, location, location, right? Yeah. If you're going to be selling coffee,

31:57.680 --> 32:01.200
you want to know that there's a lot of foot traffic around that space. You want to know what the

32:01.200 --> 32:07.440
Evan flow is. And you want to be away from other services. If you're going to open like a luxury

32:08.400 --> 32:14.080
store for shopping, you want to know the incomes of that zip code, that neighborhood, and the

32:14.080 --> 32:20.880
flow of things. So opening it, so site selection has that. McDonald's does that too. They need to

32:20.880 --> 32:26.320
know where to open the next McDonald's. And they keep doing this, the reclaim locations,

32:26.320 --> 32:31.600
the ad-new locations, just to optimize their whole collection of stores that they have.

32:31.600 --> 32:36.320
I basically remember a conversation with someone talking about the site selection use case,

32:36.320 --> 32:42.320
and they were starting to explore at the time, this was several years ago, reinforcement learning.

32:43.840 --> 32:50.080
And simulation, as a purchase, do you have you come across overlaps between what folks are

32:50.080 --> 32:56.960
doing and what you're doing in geospatial and wanting to apply RL and simulation? So there's

32:58.560 --> 33:04.560
on the simulation is good for, I think there's a set of services that do 3D reconstruction

33:04.560 --> 33:08.720
of the world, the digital twins. I think there's some convergence going there. Even in computer vision,

33:09.520 --> 33:14.720
I spoke a lot about machine learning version and computer vision, but there's also the reconstruction

33:14.720 --> 33:20.000
part of computer vision, which is like slam algorithms to rebuild 3D models of the world. There's

33:20.000 --> 33:25.120
computer vision algorithms and they're more geometric. They're less about this flywheel of data,

33:25.120 --> 33:28.560
machine learning, and I don't care what's happening under the, just get me the best performance you

33:28.560 --> 33:34.800
can. These are like generative. So you actually have people who are building just cities to the degree

33:34.800 --> 33:39.840
or you have a assembly line and they want to rebuild the assembly line to like within centimeters

33:39.840 --> 33:44.400
of accuracy. Once you have that, then you can do simulations. So some of the customers want to

33:44.400 --> 33:51.280
combine the two, but a lot of the work is in how do you generate that 3D world? And then once you

33:51.280 --> 33:56.720
generate the 3D world using some type of geospatial data processing or even geospatial machine learning,

33:56.720 --> 34:00.080
you can, for example, one of the things that people want to do is they want to know the building

34:00.080 --> 34:05.120
extent, like give me the bounding box for the building and how big is the building and so on. And

34:05.120 --> 34:10.560
they want to reconstruct the building from not just satellite imagery but drone and even in some

34:10.560 --> 34:15.440
cases, light our mapping data. But once you build it, then you're going to ask hard questions like

34:15.440 --> 34:19.840
where should I put my next bridge or where should my next, next highway come because I'm seeing all

34:19.840 --> 34:24.560
this traffic congestion. So then they can do simulations. And a lot of those simulations now scale

34:24.560 --> 34:30.880
to like tens, hundreds, even millions of agents that can move. And then you can ask the kinds of

34:30.880 --> 34:36.080
questions that are generative and then think about what problems you want to solve.

34:36.080 --> 34:43.600
Do you see this interacting with some of the emerging work around diffusion models and generative

34:43.600 --> 34:51.200
models, large models? Possibly. So like I said, it is still early days for geospatial machine

34:51.200 --> 34:55.360
learning. And that's one of the things I'm super excited about, which is the tools and the technology

34:55.360 --> 35:00.080
and the knowledge in the minds of these data scientists is pretty deep. But they've been applying

35:00.080 --> 35:06.880
to like traditional camera from smartphones and even self-driving is more advanced.

35:06.880 --> 35:14.640
Bringing that to geospatial will be phenomenal. And to me, that large language models,

35:14.640 --> 35:21.600
generative models for geospatial will be pretty cool. It's still early that they're able to generate

35:22.800 --> 35:28.960
people's faces, like this person does not exist. They're able to generate art of different kinds

35:28.960 --> 35:33.040
because they have huge collections of those that have been labeled or semi labeled and they've

35:33.040 --> 35:37.680
been able to train. There's a ton of geospatial data but nobody's thought about, hey, can I do

35:37.680 --> 35:44.000
super resolution on these images? Or like I said, cloud cover. You remove the clouds. Now you've

35:44.000 --> 35:51.760
got holes. I would love to see a generative model do that. Or if you see the evolution of a

35:51.760 --> 35:56.960
forest over time, deforestation, you want to predict three, four years from now, how will that

35:56.960 --> 36:01.440
look? So there's a lot of generative capabilities. Plus people just love synthetic stuff. So if you like

36:01.440 --> 36:07.360
to visualize how this area would look if you were built out a certain way. So I think there's room

36:07.360 --> 36:14.480
for that. It's still early. Speaking of synthetic, does synthetic data play into geospatial much at

36:14.480 --> 36:22.800
all? I think we were talking a little bit of, there's geospatial and there's 3D reconstruction

36:22.800 --> 36:28.240
or I think there's simulation there. Yeah, and so I mean, video games are another good example

36:28.240 --> 36:33.280
of like the technologies that they're bringing to these 3D reconstruction, the worlds, even the

36:33.280 --> 36:37.920
meta worse if you want to bring that in, is that you can sample from the real world and then build

36:37.920 --> 36:41.920
it. It's much easier to just replicate something and algorithm can do a ton of that and then you

36:41.920 --> 36:47.680
can have a human artist come in and finish it up versus synthesizing from scratch. So there's

36:47.680 --> 36:53.040
some synergies there. Overall, people go back and forth. Matting is one area where they kind of

36:53.040 --> 36:58.800
go back and forth where when you have sensors like LIDAR combined with aerial and drone, you can

36:58.800 --> 37:07.360
then reconstruct a region with a lot of the computer vision and ML technology, then go back and

37:07.360 --> 37:12.560
make it into wireframe and put it into all these fancy 3D models and then run your simulations.

37:12.560 --> 37:21.440
Going back to the kind of target user, do you see this primarily as making the folks that are

37:21.440 --> 37:28.320
currently doing this more effective by eliminating happy lifting or do you see it more as a

37:28.320 --> 37:34.800
kind of democratization play by enabling a broader community of use of folks to work with

37:34.800 --> 37:41.440
geospatial data because now they have better tools. Very good question. So the place where we're

37:41.440 --> 37:47.680
starting is to make the lives of data scientists who want to work with geospatial data easy. I think

37:47.680 --> 37:52.000
that's what AWS does best. That's what SageMaker is really good at and we want to bring that to the

37:52.000 --> 38:00.400
geospatial community. Now the second one in my vision is there's geospatial experts, the GIS

38:00.400 --> 38:05.920
experts, they're slowly moving to the cloud. That's already happening and they're picking up cloud

38:05.920 --> 38:11.680
skills. You have the modern day data scientist who is really well versed in language vision and

38:11.680 --> 38:15.680
those types of technologies, but they haven't spent much time with geospatial. My vision is that

38:15.680 --> 38:21.520
that community will merge when it comes to geospatial. There's a lot of domain expertise that GIS

38:21.520 --> 38:27.040
folks have which might or might not be needed when the future data scientists get to it, but there

38:27.040 --> 38:31.840
are a lot of tricks in the toolbox for a data scientist that would greatly benefit the GIS

38:31.840 --> 38:38.160
experts. And so the merging or the unification or the synergy, synergistic combination of those

38:38.160 --> 38:43.680
two is my vision for how I want to see where we want to go. So today we're starting with how do we

38:43.680 --> 38:48.320
make it easy to access data, which is very hard. You'll be surprised. 80% of work for data scientists

38:48.320 --> 38:53.120
where animal engineer does is just data munging. Getting access to data, not being happy with the

38:53.120 --> 38:57.280
quality of the data, cleaning the data, labeling the data, and then once you get the flywheel

38:57.280 --> 39:03.440
set up, then it's still the actual amount of time you train and evaluate models, even though

39:03.440 --> 39:09.440
that's sort of that part has been hardened quite a bit. Whereas for geospatial, the early parts

39:09.440 --> 39:13.520
are still, there's a lot of work to do. So my sense is in the early days, I want to make it super

39:13.520 --> 39:19.200
easy to access data and work with data in a notebook environment for a data scientist. But then I

39:19.200 --> 39:24.880
think I see in two to five years a convergence between the GIS community and the data science community.

39:24.880 --> 39:29.440
I would love for every data scientist to know how to work with geospatial data and I would love

39:29.440 --> 39:35.040
for every GIS expert to learn how to use a notebook and use cloud and just scale out and bring

39:35.040 --> 39:40.000
them the best of deep learning to their problems. Awesome. Awesome. Well, Kumar, thanks so much for

39:40.000 --> 39:44.320
taking the time to chat. It's great learning about what you're doing with the geospatial I know.

39:44.320 --> 39:55.760
Thanks, I really appreciate that conversation. Thanks for the opportunity.

