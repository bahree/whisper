1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,120
I'm your host Sam Charrington.

4
00:00:34,120 --> 00:00:39,920
As we approach Twimblecon AI platforms, I'd like to let you all in on our first major announcement

5
00:00:39,920 --> 00:00:41,760
from the conference.

6
00:00:41,760 --> 00:00:46,600
Now you all love this podcast for great guests and interviews and we're bringing that concept

7
00:00:46,600 --> 00:00:48,760
right to the Twimblecon stage.

8
00:00:48,760 --> 00:00:54,720
I am super excited to announce that Andrew Eng will be joining me on stage at Twimblecon

9
00:00:54,720 --> 00:00:57,720
for a live keynote interview.

10
00:00:57,720 --> 00:01:02,320
Many of you know Andrew from his work at Stanford, Coursera or his many other efforts in the

11
00:01:02,320 --> 00:01:06,800
industry including recently founding deeplearning.ai.

12
00:01:06,800 --> 00:01:11,480
Andrew and his work have been super impactful on my life and career and I know that's the

13
00:01:11,480 --> 00:01:13,920
case for many of you as well.

14
00:01:13,920 --> 00:01:18,880
In our conversation we'll be discussing the state of AI in the enterprise, the barriers

15
00:01:18,880 --> 00:01:24,000
to using deep learning and production and how to overcome them, his views on tooling and

16
00:01:24,000 --> 00:01:30,640
platforms for efficient AI delivery and other topics from his recently published AI Transformation

17
00:01:30,640 --> 00:01:32,440
Playbook.

18
00:01:32,440 --> 00:01:36,920
Be on the lookout for more great speaker announcements rolling out over the course of the next few

19
00:01:36,920 --> 00:01:37,920
weeks.

20
00:01:37,920 --> 00:01:39,920
You don't want to miss this event.

21
00:01:39,920 --> 00:01:47,120
Get your tickets now at twimblecon.com slash register.

22
00:01:47,120 --> 00:01:50,280
Alright everyone, I am on the line with Stephen O'Diibo.

23
00:01:50,280 --> 00:01:55,640
Stephen is the founder, CEO and chief software architect at Retina AI.

24
00:01:55,640 --> 00:01:58,840
Stephen, welcome to this week in machine learning and AI.

25
00:01:58,840 --> 00:01:59,840
Thanks so much Sam.

26
00:01:59,840 --> 00:02:02,680
It's such an honor to be on the show here with you.

27
00:02:02,680 --> 00:02:04,760
It's great to have you on the show.

28
00:02:04,760 --> 00:02:13,200
So you are an MD, a medical doctor, in addition to having a background in math and science

29
00:02:13,200 --> 00:02:15,160
or math and computer science.

30
00:02:15,160 --> 00:02:20,440
Can you elaborate a bit on your background and how you kind of made your way into working

31
00:02:20,440 --> 00:02:22,000
on artificial intelligence?

32
00:02:22,000 --> 00:02:23,000
Yeah, most certainly.

33
00:02:23,000 --> 00:02:28,520
You know, as you said, I'm a physician, I'm an ophthalmologist, a Retina specialist,

34
00:02:28,520 --> 00:02:33,040
computer scientist, mathematician and a full stack AI engineer.

35
00:02:33,040 --> 00:02:39,160
And you know, it all, it's sort of been a journey that I've really enjoyed along the

36
00:02:39,160 --> 00:02:40,160
way.

37
00:02:40,160 --> 00:02:43,720
There wasn't sort of a grand plan at the beginning to sort of do all of this.

38
00:02:43,720 --> 00:02:48,680
But yeah, I started out at University of Alabama in undergrad and I was a math major, I really

39
00:02:48,680 --> 00:02:49,880
enjoyed that.

40
00:02:49,880 --> 00:02:55,120
But the entire way I was a pre-med student, so I was med school bound.

41
00:02:55,120 --> 00:02:58,640
And the program I was in was an NSF funded program called the Fast Track Program.

42
00:02:58,640 --> 00:03:01,720
So I got a master's degree in the course of that.

43
00:03:01,720 --> 00:03:08,120
And it was, it was a phenomenal place, you know, great mentorship.

44
00:03:08,120 --> 00:03:15,520
And it was, and I would say a little bit of a, however, it was a pure math, which it's

45
00:03:15,520 --> 00:03:17,120
a, however, it's also a great advantage.

46
00:03:17,120 --> 00:03:20,480
But I found that I had some work to do when I graduated.

47
00:03:20,480 --> 00:03:26,880
So we were, in the course of the program, focusing on abstract algebra, topology and things

48
00:03:26,880 --> 00:03:27,880
like that.

49
00:03:27,880 --> 00:03:32,880
And I remember probably halfway through grad school, through the master's part of it,

50
00:03:32,880 --> 00:03:36,320
looking at a friend of mine and saying, you know, where, where the heck did three go and

51
00:03:36,320 --> 00:03:37,320
where's 15?

52
00:03:37,320 --> 00:03:38,320
You know, there were no more numbers.

53
00:03:38,320 --> 00:03:43,920
It was all, whatever happened to 71, you know, I haven't seen that number in so long.

54
00:03:43,920 --> 00:03:48,360
You know, it was all symbols, you know, you know, the wheels and the complex and so on.

55
00:03:48,360 --> 00:03:54,400
But from there, I went to medical school, went to Duke and was in a MDPHD program, started

56
00:03:54,400 --> 00:03:55,400
out.

57
00:03:55,400 --> 00:03:58,880
And I went to the biochemistry department, worked with Bob Lefkowitz, who ended up with

58
00:03:58,880 --> 00:04:02,120
the Nobel Prize in chemistry in 2012.

59
00:04:02,120 --> 00:04:04,640
And you know, I enjoyed the, by the time of the lab, it was phenomenal.

60
00:04:04,640 --> 00:04:06,160
I was two years there.

61
00:04:06,160 --> 00:04:11,240
But somewhere in that course, I had sort of a, an awakening during the grad school part

62
00:04:11,240 --> 00:04:13,320
of my med school journey.

63
00:04:13,320 --> 00:04:20,520
And, and it, I, I, it became clear that I sort of wanted to continue what had started

64
00:04:20,520 --> 00:04:22,920
at in Alabama with the math.

65
00:04:22,920 --> 00:04:27,840
And it, it was clear to me that it wasn't going to happen with the traditional pathway

66
00:04:27,840 --> 00:04:29,880
in the way biology is done.

67
00:04:29,880 --> 00:04:33,960
Biology is done sort of like looking for a needle in a haystack, you know what I mean?

68
00:04:33,960 --> 00:04:35,440
In what sense?

69
00:04:35,440 --> 00:04:37,440
It's less systematic.

70
00:04:37,440 --> 00:04:39,880
I think some of that is going to change.

71
00:04:39,880 --> 00:04:49,240
It's the notion of there are certain people who, by virtue of their position or insights

72
00:04:49,240 --> 00:04:54,280
and prior success, judgment, whatever you want to say are, are able to pick the problems

73
00:04:54,280 --> 00:04:56,120
that are interested.

74
00:04:56,120 --> 00:05:02,840
And there's a lot of arbitrariness to that process and, and craft the story around it.

75
00:05:02,840 --> 00:05:09,600
And something, something about that felt in, in complete, you know, to me, I, I, I wanted

76
00:05:09,600 --> 00:05:15,920
a, I wanted, I, now looking back, I think I wanted the era of big data where we could

77
00:05:15,920 --> 00:05:20,320
more systematically search for things, and we could more systematically draw conclusions

78
00:05:20,320 --> 00:05:24,520
and have more faith in the conclusions that we draw in science.

79
00:05:24,520 --> 00:05:30,520
Does that speak broadly to kind of the difference between the way we approach science generally

80
00:05:30,520 --> 00:05:33,560
and more of an engineering type of an approach?

81
00:05:33,560 --> 00:05:38,920
It's, it's not even science, old science procedure because the engineering thing has changed

82
00:05:38,920 --> 00:05:40,120
too, right?

83
00:05:40,120 --> 00:05:46,360
In the last five years, for example, in, in machine learning, there was the era of feature

84
00:05:46,360 --> 00:05:53,200
engineering, which was more along the lines of the way biology was done is you manually

85
00:05:53,200 --> 00:06:01,400
look for features and manually craft them even, even design them explicitly yourself versus

86
00:06:01,400 --> 00:06:07,160
using an optimization type approach where you're letting the data tell its own story and

87
00:06:07,160 --> 00:06:12,520
you're sort of sitting back as a judge, as opposed to you're the script writer.

88
00:06:12,520 --> 00:06:14,560
Oh, got it.

89
00:06:14,560 --> 00:06:16,120
Interesting analogy there.

90
00:06:16,120 --> 00:06:20,160
So anyway, I, you know, I had, I had that, which coincided with the spiritual awakening

91
00:06:20,160 --> 00:06:25,000
as well, and I decided, you know, I have to go back to, I, so I decided to go to the

92
00:06:25,000 --> 00:06:29,680
computer science department, and you're not at that point, and I switched and, um, and

93
00:06:29,680 --> 00:06:35,960
I started to re ignite the math there with numerical analysis, and I, I had worked to

94
00:06:35,960 --> 00:06:40,480
do because my flavor of math was pure and abstract.

95
00:06:40,480 --> 00:06:44,720
And here I was realizing that, um, if I was going to impact the world, you have to sort

96
00:06:44,720 --> 00:06:47,160
of go through the computer.

97
00:06:47,160 --> 00:06:48,800
And that means hard numbers.

98
00:06:48,800 --> 00:06:56,720
That means matrices, you know, that means matrix matrix multiply, right?

99
00:06:56,720 --> 00:07:00,520
So, you know, that was a blast, you know, I would say that was probably the hardest part

100
00:07:00,520 --> 00:07:06,360
of my journey was, was getting, getting into, back into the game in terms of math computer,

101
00:07:06,360 --> 00:07:09,840
because I was having to really pick up new skills as well, you know, and that was a

102
00:07:09,840 --> 00:07:14,320
Duke also, you know, the computer science department, I was going to get a PhD there in computer

103
00:07:14,320 --> 00:07:15,320
science.

104
00:07:15,320 --> 00:07:18,760
I, I, um, passed the qualifying exams, et cetera, et cetera.

105
00:07:18,760 --> 00:07:23,960
And I acquired all my skills, had to fall in out with my advisor, um, and, and then

106
00:07:23,960 --> 00:07:24,960
I moved on.

107
00:07:24,960 --> 00:07:27,400
At that point, I got the master's, no problem.

108
00:07:27,400 --> 00:07:31,720
And, uh, and went back to medical school, uh, and I, but then I had just one more year

109
00:07:31,720 --> 00:07:37,920
to finish med school, uh, finish that and, and then stayed at Duke for my intranier, uh,

110
00:07:37,920 --> 00:07:42,200
and, uh, and by then I was back in the game, you know, clinically went back, went to Howard

111
00:07:42,200 --> 00:07:45,720
for my ophthalmology residency, uh, which was a lot of fun.

112
00:07:45,720 --> 00:07:47,240
At that point, I was already committed, right?

113
00:07:47,240 --> 00:07:50,680
I was already, I already knew that I have to keep doing this math thing.

114
00:07:50,680 --> 00:07:52,480
You know, I've paid enough of a price.

115
00:07:52,480 --> 00:08:00,280
It was, it was never gonna, I was never gonna let it go, uh, and so even during the course

116
00:08:00,280 --> 00:08:04,920
of my residency, I wrote a book on quantum mechanics, for instance, uh, and then, uh, went

117
00:08:04,920 --> 00:08:08,040
to the University of Michigan for a fellowship in retina.

118
00:08:08,040 --> 00:08:12,560
And I, I loved Michigan, you know, I visited an arbor, uh, it was a really intellectual

119
00:08:12,560 --> 00:08:13,960
mecca was the way that I saw it.

120
00:08:13,960 --> 00:08:17,160
And there was something really pure about the place in terms of what people at least

121
00:08:17,160 --> 00:08:24,040
aspired to, um, sort of a, they, they really, they really took, I felt that they really

122
00:08:24,040 --> 00:08:31,680
took, um, um, academic pursuit as a primary gain, as opposed to something for a secondary

123
00:08:31,680 --> 00:08:36,080
gain, for recognition or money or whatever, um, at the end of the day, we're all human,

124
00:08:36,080 --> 00:08:38,600
you know, and there's always that going on, right?

125
00:08:38,600 --> 00:08:42,000
There's always the politics and there's always stuff, you know, but I felt Michigan stood

126
00:08:42,000 --> 00:08:47,040
out in terms of its, uh, it's, uh, the genuineness of its commitment.

127
00:08:47,040 --> 00:08:50,320
And so I, it was my first choice, you know, and I could, coming down to Howard, I could

128
00:08:50,320 --> 00:08:51,640
have gone anywhere in the country.

129
00:08:51,640 --> 00:08:56,120
Um, so I went to Michigan as my first choice for fellowship, um, and it was really good.

130
00:08:56,120 --> 00:09:02,880
I wanted to stay in the academy at that point, you know, but, um, the irony was during

131
00:09:02,880 --> 00:09:06,600
the course of my time there, it started to dawn on me that if I'm really gonna be able

132
00:09:06,600 --> 00:09:11,360
to make a difference from really gonna be able to use computer, science, math, I didn't

133
00:09:11,360 --> 00:09:14,680
know exactly what I wanted to do at that point, but it dawned on me that if I really was

134
00:09:14,680 --> 00:09:20,440
going to do that, it was sort of a real epiphany, a surprise that I, I needed to sort of craft

135
00:09:20,440 --> 00:09:21,440
my own career.

136
00:09:21,440 --> 00:09:25,320
I really had to go into private practice to become a true academic.

137
00:09:25,320 --> 00:09:29,120
I couldn't do it at the university because there were these programs that were set up,

138
00:09:29,120 --> 00:09:33,800
you know, people said you have to fit into this box, right?

139
00:09:33,800 --> 00:09:38,400
And there was not a, a sense of, uh, and I, you know, that's just the way it is, you

140
00:09:38,400 --> 00:09:41,480
know, there was not a sense of, you know, go do whatever you want.

141
00:09:41,480 --> 00:09:42,480
Nobody says that.

142
00:09:42,480 --> 00:09:43,480
Right.

143
00:09:43,480 --> 00:09:44,480
Nobody wants to do that.

144
00:09:44,480 --> 00:09:46,760
And I, I get it, you know, so I picked the job.

145
00:09:46,760 --> 00:09:47,760
I went to Iowa.

146
00:09:47,760 --> 00:09:52,560
I worked three days out of the week, um, and it being private practice, you pay your, you

147
00:09:52,560 --> 00:09:53,560
pay your own salary.

148
00:09:53,560 --> 00:09:56,320
You wouldn't, you, and so it was no problem.

149
00:09:56,320 --> 00:09:59,800
And the rest of the time I did academics on my own.

150
00:09:59,800 --> 00:10:03,320
And there I wrote a book on a finite group theory, started getting more into the peer

151
00:10:03,320 --> 00:10:09,640
and theoretical part, I've always sort of oscillated between, um, the, uh, applied concrete

152
00:10:09,640 --> 00:10:19,800
real and the celestial ecclesiastic, the clouds, you know, I've always had a gun between

153
00:10:19,800 --> 00:10:20,800
the two.

154
00:10:20,800 --> 00:10:22,120
And so I was all the way in the clouds.

155
00:10:22,120 --> 00:10:26,400
You know, when I was in Iowa and I wrote that book on finite group theory, um, and then

156
00:10:26,400 --> 00:10:31,420
at that point, my brother, uh, David, David O'Dybo, who is co-founder of, uh, and a

157
00:10:31,420 --> 00:10:35,920
little AI, you know, he was ranked 70th in the world, uh, in Kaggle.

158
00:10:35,920 --> 00:10:40,160
Um, and so he started, he started telling me because I had left grad school at that point,

159
00:10:40,160 --> 00:10:43,600
you know, and he started telling me that there's this thing called deep learning.

160
00:10:43,600 --> 00:10:44,600
Hmm.

161
00:10:44,600 --> 00:10:52,800
I, I hear I was who I was basically a guy, a PhD holder in effect in computer science,

162
00:10:52,800 --> 00:10:58,880
who finished and left the computer world in 2010, and I had never heard the word deep

163
00:10:58,880 --> 00:10:59,880
learning.

164
00:10:59,880 --> 00:11:00,880
Right.

165
00:11:00,880 --> 00:11:01,880
That's a duke for that matter.

166
00:11:01,880 --> 00:11:02,880
Right.

167
00:11:02,880 --> 00:11:06,440
Um, there was no such thing, you know, the whole thing about AI, so on and so forth.

168
00:11:06,440 --> 00:11:11,880
There was nothing like that machine learning was the most boring class you can think about.

169
00:11:11,880 --> 00:11:13,880
It was horrible.

170
00:11:13,880 --> 00:11:20,440
I made it with like, uh, you know, Mark of chains and so on and things that are now so exciting,

171
00:11:20,440 --> 00:11:26,240
the most exciting things in science were really, you know, we're really in the cover.

172
00:11:26,240 --> 00:11:31,200
I mean, it's very deep inside of textbooks, they were gathering dust.

173
00:11:31,200 --> 00:11:35,360
And so anyway, my brother told me that this thing is so exciting what he's doing because

174
00:11:35,360 --> 00:11:38,880
he was in grad school at the time, you know, at the University of Alabama, I had left and

175
00:11:38,880 --> 00:11:42,880
then he had, he had been working, uh, he's a year older than I am and he had gone back

176
00:11:42,880 --> 00:11:44,720
to grad school and he said he's doing deep learning.

177
00:11:44,720 --> 00:11:45,720
It's so exciting.

178
00:11:45,720 --> 00:11:48,720
He said he's doing these things and he kept talking about it and you know, eventually

179
00:11:48,720 --> 00:11:51,800
one day at, uh, Christmas, uh, I was over it.

180
00:11:51,800 --> 00:11:57,360
We were spending the Christmas at their place, um, in, in Birmingham and, um, you kept

181
00:11:57,360 --> 00:11:58,560
talking, I said, okay, let me take a look.

182
00:11:58,560 --> 00:11:59,560
What is this?

183
00:11:59,560 --> 00:12:01,840
And he said something to do with images and image classification.

184
00:12:01,840 --> 00:12:04,480
And I said, you know, that's what I do in my work.

185
00:12:04,480 --> 00:12:10,000
Like I basically look at a picture, depending on what the picture tells me I go, uh, and

186
00:12:10,000 --> 00:12:15,440
I decide whether I'm going to stick a needle in somebody's eyeball, um, and administer

187
00:12:15,440 --> 00:12:20,960
injections, uh, or I determined whether I'm going to do laser surgery on that person,

188
00:12:20,960 --> 00:12:22,840
all based off of the picture.

189
00:12:22,840 --> 00:12:23,840
And so he said, really?

190
00:12:23,840 --> 00:12:28,240
I said, yeah, I said, it's all the picture, the picture guides the treatment.

191
00:12:28,240 --> 00:12:30,520
And, um, and so he said, no kidding.

192
00:12:30,520 --> 00:12:37,080
So, so you have a much more, uh, the, the, the stakes involved in image classification

193
00:12:37,080 --> 00:12:40,440
are perhaps much more tangible for you.

194
00:12:40,440 --> 00:12:41,440
Absolutely.

195
00:12:41,440 --> 00:12:42,440
Absolutely.

196
00:12:42,440 --> 00:12:46,520
And none of this was abstract at all because I'll do 40 injections in one day.

197
00:12:46,520 --> 00:12:50,920
You know, sometimes see 50 patients a day and I'm literally running from room to room.

198
00:12:50,920 --> 00:12:56,880
Um, um, and so I, I, so at that point, I'll study for my boards, the, uh, the biology

199
00:12:56,880 --> 00:12:58,440
boards, which that's a whole different story.

200
00:12:58,440 --> 00:13:01,680
We could do a whole another, uh, say a session on that, Sam.

201
00:13:01,680 --> 00:13:06,240
You know, it's, you know, they, you know, they, they put you in a hotel room and they

202
00:13:06,240 --> 00:13:07,240
ask you these questions.

203
00:13:07,240 --> 00:13:10,520
Literally, there's, there's like a bed there at somebody's hotel.

204
00:13:10,520 --> 00:13:14,960
You walk in and, uh, they're asking you questions about on the 10 different areas of the

205
00:13:14,960 --> 00:13:15,960
eye.

206
00:13:15,960 --> 00:13:19,000
And, and so off the body is, is, is it's own thing in that way.

207
00:13:19,000 --> 00:13:20,840
Um, so anyway, I'll study for this exam.

208
00:13:20,840 --> 00:13:27,280
That everybody dreads, um, and I, I, I, so I said, okay, let me finish, let me knock

209
00:13:27,280 --> 00:13:28,880
the boards out of the way.

210
00:13:28,880 --> 00:13:30,560
And then I'll take a look at this thing.

211
00:13:30,560 --> 00:13:36,680
So, uh, finished my boards in 2016, November, uh, and I started studying, I started looking,

212
00:13:36,680 --> 00:13:41,640
I said, okay, I'm going to look at deep learning, uh, and took a trip to Nigeria, um, which

213
00:13:41,640 --> 00:13:42,640
was already planned.

214
00:13:42,640 --> 00:13:46,760
You know, it was Christmas and, um, and that was when I first opened the book, I looked

215
00:13:46,760 --> 00:13:47,760
at it.

216
00:13:47,760 --> 00:13:48,960
So let me look at this deep learning thing.

217
00:13:48,960 --> 00:13:51,400
And then I thought, okay, very interesting.

218
00:13:51,400 --> 00:13:52,640
Do you remember the book?

219
00:13:52,640 --> 00:13:53,640
Well, yeah.

220
00:13:53,640 --> 00:13:54,640
Okay.

221
00:13:54,640 --> 00:13:55,640
Well, you know, it's actually not a book.

222
00:13:55,640 --> 00:13:57,800
It was, uh, um, nobody reads books anymore.

223
00:13:57,800 --> 00:14:02,600
I was, I was curious if it was one of the kind of the classic books, but, um, okay.

224
00:14:02,600 --> 00:14:03,600
Got it.

225
00:14:03,600 --> 00:14:04,600
Got it.

226
00:14:04,600 --> 00:14:05,600
We're speaking metaphorically.

227
00:14:05,600 --> 00:14:06,600
What's the fine metaphorically?

228
00:14:06,600 --> 00:14:07,600
Got it.

229
00:14:07,600 --> 00:14:08,600
Yeah.

230
00:14:08,600 --> 00:14:14,440
It was, it was, the book was, so the metaphoric book was the CS 231 course by, uh, Carpathy.

231
00:14:14,440 --> 00:14:15,440
Ah, yeah.

232
00:14:15,440 --> 00:14:16,440
That was the book.

233
00:14:16,440 --> 00:14:17,440
Got it.

234
00:14:17,440 --> 00:14:20,200
So I looked at the book and I said, holy smokes, this is good stuff.

235
00:14:20,200 --> 00:14:25,440
Uh, and then next thing I, I pulled up a media article with, um, uh, some Keras code in

236
00:14:25,440 --> 00:14:32,120
it, and the Python, uh, they actually had a tutorial on Python on the 231 course.

237
00:14:32,120 --> 00:14:37,520
And so I pulled out that 231, that tutorial, you know, I have a, I have a grad degree in

238
00:14:37,520 --> 00:14:38,520
computer science.

239
00:14:38,520 --> 00:14:40,200
It took me a week to pick up Python and so on.

240
00:14:40,200 --> 00:14:45,000
Um, and then I, I ran the M-ness training in the Keras.

241
00:14:45,000 --> 00:14:49,800
It was basically like copy pasted it into, uh, into the Jupiter notebook.

242
00:14:49,800 --> 00:14:54,200
And I saw the thing, you know, numbers running and, you know, I was like, wow, this is

243
00:14:54,200 --> 00:14:55,200
amazing.

244
00:14:55,200 --> 00:14:57,080
You know, this is fascinating.

245
00:14:57,080 --> 00:15:01,160
And so I wired up actual data, actual image and data to it.

246
00:15:01,160 --> 00:15:05,200
And when I saw the accuracy that was coming out, meaning clinical imagery data, clinical

247
00:15:05,200 --> 00:15:07,080
or like data and M-ness data.

248
00:15:07,080 --> 00:15:12,000
No, after I did the M-ness thing, okay, then I actually took clinical data, actual, you

249
00:15:12,000 --> 00:15:18,720
know, actual patients, you know, data, uh, and I ran that, uh, an image classifier on

250
00:15:18,720 --> 00:15:19,800
that.

251
00:15:19,800 --> 00:15:24,480
And based on the accuracy that was coming out, I was, my jaw dropped, you know, Sam,

252
00:15:24,480 --> 00:15:27,960
I was like, wow, okay, this is a game changer.

253
00:15:27,960 --> 00:15:32,320
And I'll never forget that night in, in that point, it dawned on me that, uh, there's

254
00:15:32,320 --> 00:15:35,240
about to be a revolution that really everything is going to change.

255
00:15:35,240 --> 00:15:41,000
And so, um, my brother and I got to talk in, I launched out the company, retina AI, you

256
00:15:41,000 --> 00:15:45,440
know, he helped me with it, um, and, uh, he, he turned out he was involved with so much

257
00:15:45,440 --> 00:15:47,760
at the time, you know, he was doing his Kaggle thing.

258
00:15:47,760 --> 00:15:49,480
He was ranked 70th in the world on Kaggle.

259
00:15:49,480 --> 00:15:51,040
He was a part of two other startups.

260
00:15:51,040 --> 00:15:54,040
Uh, he was completely his PhD and so he had a lot going on.

261
00:15:54,040 --> 00:15:57,280
So he said, I should carry on on my own, you know, with it.

262
00:15:57,280 --> 00:16:01,000
And you know, that, that turned out to be a good fit, you know, it was, it was challenging

263
00:16:01,000 --> 00:16:05,400
for me, but I had no backup, Sam, um, so you can imagine that's really where I became

264
00:16:05,400 --> 00:16:10,960
a full stack AI engineer, um, cause I had, I had left my job, you know, where, where

265
00:16:10,960 --> 00:16:16,560
you know, in full disclosure, where I was probably making five or 10 times what, what I,

266
00:16:16,560 --> 00:16:25,680
I thought I would ever make in my life, um, and I left that job and, uh, I, I, I left

267
00:16:25,680 --> 00:16:27,080
it because I thought I had a backup.

268
00:16:27,080 --> 00:16:30,440
I thought I had, there was a person who's going to be the CTO who's going to do all the

269
00:16:30,440 --> 00:16:39,600
actual hard work, you know, and I'm just going to be like, you know, a talk ahead, you

270
00:16:39,600 --> 00:16:40,600
know, no problem.

271
00:16:40,600 --> 00:16:43,920
I enjoy that, you know, running around, you know, talk about what we're doing, you know,

272
00:16:43,920 --> 00:16:47,880
connect with folks and, and kind of get things moving along and, you know, talk to investors

273
00:16:47,880 --> 00:16:51,480
and that type of thing and go give some clinical talks.

274
00:16:51,480 --> 00:16:56,840
Um, but here I was, you know, and I had to build a product and I had to build, I put out

275
00:16:56,840 --> 00:16:59,840
an MVP, you know, and I had to, to do all of that.

276
00:16:59,840 --> 00:17:03,440
And so I, I had to pick up a bunch of things along the way.

277
00:17:03,440 --> 00:17:09,640
I, so we built a mobile app and I had to, I had to pick up a Swift and build the iOS and

278
00:17:09,640 --> 00:17:15,240
then, you know, pick up Kotlin and, you know, and Java, uh, script and, um, you know, put

279
00:17:15,240 --> 00:17:19,960
out the Android version of this and I had to learn how to host models in the web, um,

280
00:17:19,960 --> 00:17:24,440
in the cloud, uh, how to serve and, you know, how I had to get into some details with

281
00:17:24,440 --> 00:17:29,280
serverless stuff as well as restful APIs and the like it and, uh, dock, uh, containerization

282
00:17:29,280 --> 00:17:34,240
and all the, all the needy greedy stuff that I quite honestly didn't think it was even,

283
00:17:34,240 --> 00:17:39,040
I didn't even conceive that it was even possible for a physician to ever be doing anything

284
00:17:39,040 --> 00:17:40,040
like that.

285
00:17:40,040 --> 00:17:45,080
Um, but I was, I was sort of forced into it and, you know, I'm really thankful for that.

286
00:17:45,080 --> 00:17:49,280
Um, but anyway, so our company is retina AI Health Incorporated.

287
00:17:49,280 --> 00:17:55,040
Um, we, I, I, whether that storm last year and at the end of the year started, uh, talking

288
00:17:55,040 --> 00:18:00,080
to investors, got a few angels together, all physicians who, you know, they wrote checks

289
00:18:00,080 --> 00:18:04,840
enough that we're going to be, we're going to be around this year for sure.

290
00:18:04,840 --> 00:18:06,400
And, and, you know, next year as well.

291
00:18:06,400 --> 00:18:12,080
And, you know, maybe forever, maybe, you know, sounds like you've definitely made the transition

292
00:18:12,080 --> 00:18:13,280
to startup life.

293
00:18:13,280 --> 00:18:14,280
Yes.

294
00:18:14,280 --> 00:18:15,280
Yeah.

295
00:18:15,280 --> 00:18:17,080
So that's sort of the short, that's a short story.

296
00:18:17,080 --> 00:18:24,160
And just in quick summary of what the company is, um, we are, we are using machine learning

297
00:18:24,160 --> 00:18:31,000
to build autonomous systems that will, that diagnose retinal diseases as well as diagnosed

298
00:18:31,000 --> 00:18:33,400
systemic diseases from pictures of the retina.

299
00:18:33,400 --> 00:18:38,560
For example, cardiovascular risk, uh, somebody who's that guy who's playing golf and looks

300
00:18:38,560 --> 00:18:40,400
really healthy and is 55.

301
00:18:40,400 --> 00:18:43,640
And suddenly you find out that this person dropped dead of a heart attack, a massive heart

302
00:18:43,640 --> 00:18:45,800
attack and has a young family.

303
00:18:45,800 --> 00:18:51,560
Um, it turns out that we, we are coming closer to be able to accurately find out who that

304
00:18:51,560 --> 00:18:54,000
person is by just looking in their eyes.

305
00:18:54,000 --> 00:18:55,680
And saying, you really need to get to a heart doctor.

306
00:18:55,680 --> 00:18:57,040
You really need to change your diet.

307
00:18:57,040 --> 00:19:01,560
You really need to be on anti cholesterol medicines, beta blockers and the like, um, to,

308
00:19:01,560 --> 00:19:03,000
to prevent something like that.

309
00:19:03,000 --> 00:19:04,120
So it's a big market.

310
00:19:04,120 --> 00:19:06,000
It's a big space and it's really exciting.

311
00:19:06,000 --> 00:19:13,320
Can you maybe contextualize what retina AI is doing relative to some of the other activities

312
00:19:13,320 --> 00:19:14,320
in the space?

313
00:19:14,320 --> 00:19:19,600
Uh, as you can imagine, you are paying even more attention to it than I am.

314
00:19:19,600 --> 00:19:25,640
And there are announcements happening constantly about folks using these particular types

315
00:19:25,640 --> 00:19:33,560
of images to make various predictions, uh, deep mind in UK had a tie up with the, uh,

316
00:19:33,560 --> 00:19:37,760
the NHS there around using retinal images.

317
00:19:37,760 --> 00:19:45,160
I did an interview with, uh, a Google developer who was doing some work around some of what

318
00:19:45,160 --> 00:19:49,520
you were just talking about predicting cardiovascular risk factors based on these retinal

319
00:19:49,520 --> 00:19:51,880
funness images.

320
00:19:51,880 --> 00:19:56,400
Can you maybe contextualize, uh, maybe talk a little bit about kind of the broad landscape

321
00:19:56,400 --> 00:20:02,360
of, you know, what's happening in this space and, and how, uh, what you're doing fits into

322
00:20:02,360 --> 00:20:03,360
that.

323
00:20:03,360 --> 00:20:04,360
Yeah.

324
00:20:04,360 --> 00:20:05,360
Great question, Sam.

325
00:20:05,360 --> 00:20:11,800
Um, you know, it's, uh, it's early days and, um, there's, uh, the problem is so big

326
00:20:11,800 --> 00:20:18,920
and the potential for impact, um, is so compelling that, um, both large companies as well as

327
00:20:18,920 --> 00:20:24,440
small startups are interested, uh, Google has, uh, interest in this area, you know, as

328
00:20:24,440 --> 00:20:30,880
you said, you know, a deep mind, uh, whereas working with the, is working with the NHS and,

329
00:20:30,880 --> 00:20:37,320
um, they, you know, recently put out an image classifier with, uh, I think they said 50

330
00:20:37,320 --> 00:20:42,320
different retinal diseases and that, um, sort of thin, you know, that's, uh, it's for

331
00:20:42,320 --> 00:20:47,040
us a standard problem, you know, we have a similar classifier with even more diseases,

332
00:20:47,040 --> 00:20:50,880
um, that, that, uh, we're about to roll out here.

333
00:20:50,880 --> 00:20:56,560
Microsoft isn't interested in this as well, uh, and, you know, has a platform that they're

334
00:20:56,560 --> 00:21:01,800
working on and, and Google is also working with Arvin and I Hospital in India, you know,

335
00:21:01,800 --> 00:21:08,120
uh, looking at the issue of diabetic retinopathy screening, uh, there are, um, startups, some

336
00:21:08,120 --> 00:21:12,880
that are large and have raised significant amount of capital, such as IDX, uh, which is

337
00:21:12,880 --> 00:21:19,960
based out of Iowa City, uh, Michael Abramoff's company that received FDA approval for a

338
00:21:19,960 --> 00:21:24,760
device to do autonomous screening for diabetes, um, and diabetic retinopathy.

339
00:21:24,760 --> 00:21:30,040
So there are a number of people involved interested because it's such a large space in terms

340
00:21:30,040 --> 00:21:36,520
of, um, both the market as well as the humanitarian potential for, for a real positive impact.

341
00:21:36,520 --> 00:21:42,040
The diabetes is a big focus for a lot of these companies as well as for us.

342
00:21:42,040 --> 00:21:49,160
It, uh, for instance, the US has about nine or 10% of our population, uh, is diabetic.

343
00:21:49,160 --> 00:21:52,560
And you know, that, that number is, uh, over 35 million people.

344
00:21:52,560 --> 00:21:56,120
And when you talk about prediabetes, there's over 86 million people.

345
00:21:56,120 --> 00:22:01,280
That's people who, if not diagnosed and treated, um, will develop diabetes within five years.

346
00:22:01,280 --> 00:22:03,280
That's 86 million people.

347
00:22:03,280 --> 00:22:06,160
And on a worldwide scale, it's a lot larger.

348
00:22:06,160 --> 00:22:11,560
It's half a billion people, you know, with diabetes, um, and about one and a half million

349
00:22:11,560 --> 00:22:15,320
people die every year, you know, from the disease.

350
00:22:15,320 --> 00:22:21,040
So the real issue here is that in terms of human labor physicians, it takes so long to train

351
00:22:21,040 --> 00:22:27,280
a doctor, uh, and that's, there's no hope there for us to use human power to train physicians

352
00:22:27,280 --> 00:22:31,280
to be able to diagnose this and to be able to, the world's population is growing much faster

353
00:22:31,280 --> 00:22:35,040
than we're turning out physicians, um, and so the problem is worsening.

354
00:22:35,040 --> 00:22:42,000
So AI is compelling and is actually necessary, um, to be able to address the, the problem.

355
00:22:42,000 --> 00:22:46,000
And so a lot of people are working on in this area.

356
00:22:46,000 --> 00:22:48,320
And we're, we, there, there's competition.

357
00:22:48,320 --> 00:22:50,080
There's also collaboration.

358
00:22:50,080 --> 00:22:56,120
It's all very good, um, for instance, at the National Medical, um, associations, uh, annual

359
00:22:56,120 --> 00:22:58,920
meeting, which is going to be in Hawaii this year.

360
00:22:58,920 --> 00:23:05,000
I'm going to be chairing a panel, um, on, on this very, on, on more broadly, innovations

361
00:23:05,000 --> 00:23:09,160
in ophthalmology, but I've geared it and focused it towards diabetic retinopathy.

362
00:23:09,160 --> 00:23:14,360
And so we have the, uh, technical lead from Google brain team, uh, Dale Webster working

363
00:23:14,360 --> 00:23:15,520
on this diabetic retinopathy.

364
00:23:15,520 --> 00:23:20,120
He's going to be there, um, on the panel and, uh, and that's, uh, Dale Webster.

365
00:23:20,120 --> 00:23:25,240
And then Anushra Trivedi is, she works in Brad Smith, the president of Microsoft's office

366
00:23:25,240 --> 00:23:27,360
in healthcare AI.

367
00:23:27,360 --> 00:23:28,440
And she's the lead on that.

368
00:23:28,440 --> 00:23:29,640
And so I invited her.

369
00:23:29,640 --> 00:23:31,120
She's going to be there as well.

370
00:23:31,120 --> 00:23:33,800
Uh, she's also working on diabetic retinopathy.

371
00:23:33,800 --> 00:23:37,080
And then Michael Abramoff, who's the founder of IDX, he's going to be on the panel as

372
00:23:37,080 --> 00:23:38,080
well.

373
00:23:38,080 --> 00:23:42,200
Um, and so where we collaborate, we're, we're looking at it, we're thinking about it.

374
00:23:42,200 --> 00:23:47,400
The truth of the matter is it's such an enormous problem, um, that, uh, you know, it's great

375
00:23:47,400 --> 00:23:52,360
to share ideas and see how everybody can move forward together.

376
00:23:52,360 --> 00:23:57,720
And so can you maybe talking a little bit more detail about your approach and some of

377
00:23:57,720 --> 00:24:03,760
the research you've published in this space and how it, uh, you know, is, is everyone

378
00:24:03,760 --> 00:24:10,400
kind of doing the same thing, applied to different data sets or folks taking, you know, either

379
00:24:10,400 --> 00:24:15,400
dramatically different or, you know, different in novel ways, uh, types of approaches, like

380
00:24:15,400 --> 00:24:18,520
what distinguishes the different things that are happening in the space.

381
00:24:18,520 --> 00:24:19,520
Right.

382
00:24:19,520 --> 00:24:21,560
It's great, great question, Sam.

383
00:24:21,560 --> 00:24:24,840
So yeah, there's a lot of commonality, a lot of similarity.

384
00:24:24,840 --> 00:24:31,040
Um, whenever one goes to these, um, machine learning conferences, you know, there's no, in

385
00:24:31,040 --> 00:24:32,960
terms of the actual techniques that are being used.

386
00:24:32,960 --> 00:24:35,520
There's nothing that's enormously novel, right?

387
00:24:35,520 --> 00:24:38,120
Everybody, you know, there's only, there's only Python in there.

388
00:24:38,120 --> 00:24:43,360
There's only Keras and, you know, PyTorch and, uh, um, there's, you know, and there's

389
00:24:43,360 --> 00:24:47,920
not that many tools, um, that we're using to execute the job.

390
00:24:47,920 --> 00:24:54,880
Approaches do differ, uh, and, uh, access to data, you know, uh, differs, uh, and that's,

391
00:24:54,880 --> 00:24:59,640
sometimes it's not always, more data is not always necessarily a good thing, which is

392
00:24:59,640 --> 00:25:04,520
a separate topic, um, it depends on the type of data that one has access to.

393
00:25:04,520 --> 00:25:08,800
And then one's understanding and knowledge of the specific domain.

394
00:25:08,800 --> 00:25:13,560
And so I think that's where we stand out, um, because of my experience as, uh, a physician

395
00:25:13,560 --> 00:25:17,440
and a thomadist, I'm a retina specialist, also a full stackier engineer.

396
00:25:17,440 --> 00:25:22,200
And I built the entire prototype of our first prototype on my own and to end, um, I think

397
00:25:22,200 --> 00:25:25,200
what that does for us is it gives us a very unique perspective.

398
00:25:25,200 --> 00:25:27,080
We can innovate very quickly.

399
00:25:27,080 --> 00:25:32,320
For example, one of that, the papers that I just published two weeks ago was, uh, using

400
00:25:32,320 --> 00:25:39,680
a generative adversarial network, um, to generate, uh, artificial data for data augmentation.

401
00:25:39,680 --> 00:25:44,720
And, um, I generated a certain type of scan called an OCT of the retina.

402
00:25:44,720 --> 00:25:48,000
And I passed it along to a few of my colleagues who are also retina specialists.

403
00:25:48,000 --> 00:25:54,200
And, uh, I was impressed to see that, um, at least half of people, uh, didn't get all

404
00:25:54,200 --> 00:25:55,200
of them right.

405
00:25:55,200 --> 00:26:01,440
These are experts, you know, who look at these, these images all day, um, so there are certain

406
00:26:01,440 --> 00:26:02,440
differences.

407
00:26:02,440 --> 00:26:07,760
It's the smaller startups who have more integrated domain knowledge can move quicker and can

408
00:26:07,760 --> 00:26:11,640
innovate, you know, uh, quicker, uh, in that space.

409
00:26:11,640 --> 00:26:14,560
And so everybody's going to have to work together and that's what we're doing.

410
00:26:14,560 --> 00:26:19,680
We currently have, um, we're currently in the Google Cloud, uh, platform, you know, startup

411
00:26:19,680 --> 00:26:25,440
program, so we, we get some, some, some benefits from, you know, having access to, to cloud

412
00:26:25,440 --> 00:26:26,440
infrastructure.

413
00:26:26,440 --> 00:26:31,320
And, you know, that helps us, uh, and we're currently also looking to, to further some

414
00:26:31,320 --> 00:26:35,800
ties with some of the other larger companies to see how everybody can move forward together.

415
00:26:35,800 --> 00:26:43,000
So that's, that's a really interesting perspective on the way these markets, uh, will evolve.

416
00:26:43,000 --> 00:26:47,680
And I want to dive into, uh, the, the GANs, uh, work that you've done.

417
00:26:47,680 --> 00:26:52,200
But, but before we do that, let's linger here for a second, uh, what strikes me as interesting

418
00:26:52,200 --> 00:26:58,560
is there is a period of time, maybe, you know, three, four years ago when everyone kind

419
00:26:58,560 --> 00:27:05,960
of believed that, you know, the, that, that AI was going to be totally driven by access

420
00:27:05,960 --> 00:27:10,520
to data and it was going to be a handful of large companies that would kind of lock up

421
00:27:10,520 --> 00:27:16,000
all of the data and, you know, thus build all of the best models and everyone else would

422
00:27:16,000 --> 00:27:17,640
be frozen out.

423
00:27:17,640 --> 00:27:20,240
And I, I think that, that's changing, right?

424
00:27:20,240 --> 00:27:24,200
That perception is changing and you touched on some of the reasons why.

425
00:27:24,200 --> 00:27:27,440
Can you elaborate on, you know, what you see there?

426
00:27:27,440 --> 00:27:28,440
Yeah.

427
00:27:28,440 --> 00:27:32,680
Um, exactly, you know, people, it was just, it was just 24 months ago.

428
00:27:32,680 --> 00:27:35,880
People were saying, oh, wow, you know, throwing the towel, don't worry about it.

429
00:27:35,880 --> 00:27:39,080
You know, Google's interested, they've, they've looked at it and so it's over.

430
00:27:39,080 --> 00:27:42,880
Um, and, you know, I, we always said, you know, it was, as people were saying that,

431
00:27:42,880 --> 00:27:50,360
that I was leaving my job, you know, because I, I absolutely didn't believe it.

432
00:27:50,360 --> 00:27:56,320
You know, I, and so one thing that I tell people is data is everywhere.

433
00:27:56,320 --> 00:27:59,320
And data has always been everywhere.

434
00:27:59,320 --> 00:28:04,400
And it's, it's kind of like oil, like natural gas, it's, it really is everywhere.

435
00:28:04,400 --> 00:28:08,600
You know, you dig deep enough anywhere, you'll, you'll strike oil somewhere, um, but

436
00:28:08,600 --> 00:28:15,400
it's really difficult to set up the rigs to know how to, to find the oil and how to, you

437
00:28:15,400 --> 00:28:22,040
know, extract it and, and, uh, convert it into something useful into gas and to fuel

438
00:28:22,040 --> 00:28:26,920
into natural gas for, for one's vehicle, for instance, just to jump in there.

439
00:28:26,920 --> 00:28:34,760
I'm not sure that's the best analogy for your position in the sense that actual exploration

440
00:28:34,760 --> 00:28:39,320
and extraction of oil is hugely resource intensive.

441
00:28:39,320 --> 00:28:43,440
And it's kind of the domain of the big oil companies as opposed to, well, I don't know

442
00:28:43,440 --> 00:28:44,440
this market.

443
00:28:44,440 --> 00:28:49,760
So I may be totally wrong, but I'm assuming that it's, you know, that is kind of one of

444
00:28:49,760 --> 00:28:54,840
these things where it's kind of dominated by, you know, huge energy companies and state

445
00:28:54,840 --> 00:28:56,800
on players and things like that.

446
00:28:56,800 --> 00:28:57,800
Right, right.

447
00:28:57,800 --> 00:28:58,800
Yeah.

448
00:28:58,800 --> 00:28:59,800
Yeah.

449
00:28:59,800 --> 00:29:03,400
That's, that's one part that the analogy doesn't work because it's actually the opposite

450
00:29:03,400 --> 00:29:11,400
in the case of, you know, in this era where, you know, a kid in Nigeria or Ghana who has

451
00:29:11,400 --> 00:29:16,640
access to the internet, you know, and has a laptop, you know, can train a machine learning

452
00:29:16,640 --> 00:29:17,640
algorithm.

453
00:29:17,640 --> 00:29:18,640
Right.

454
00:29:18,640 --> 00:29:26,000
Um, so it's other than that part, you know, it's, um, so yeah, the point there is that

455
00:29:26,000 --> 00:29:32,120
no end to data, having domain expertise, which is what these oil companies have, um, having

456
00:29:32,120 --> 00:29:37,400
domain expertise and knowing what to do with data, that's actually what's very expensive.

457
00:29:37,400 --> 00:29:43,000
And in, in, in this case, it's, uh, the analogy does work where expensive in a more general

458
00:29:43,000 --> 00:29:47,160
sense of the word, uh, it's hard to come by.

459
00:29:47,160 --> 00:29:52,760
It's so expensive that Google can't afford it, right, um, in the sense that, and that's

460
00:29:52,760 --> 00:29:57,000
part of the, the strategy they recognize that and so a lot of these big companies have

461
00:29:57,000 --> 00:30:04,280
venture arms that look to partner or acquire, uh, smaller startups, um, because they, that's,

462
00:30:04,280 --> 00:30:10,880
that's where the rubber hits the road is in the domain expertise coupled with the engineering

463
00:30:10,880 --> 00:30:17,840
know how, uh, allows people to find meaningful insights inside of data.

464
00:30:17,840 --> 00:30:23,920
And so another comment that you made was that more data is not necessarily a good thing

465
00:30:23,920 --> 00:30:29,400
and it goes against kind of the common wisdom as well in this space.

466
00:30:29,400 --> 00:30:30,920
Can you elaborate on that?

467
00:30:30,920 --> 00:30:32,280
It totally does.

468
00:30:32,280 --> 00:30:33,880
Yes, yes.

469
00:30:33,880 --> 00:30:37,120
So more data is not always a good thing.

470
00:30:37,120 --> 00:30:42,760
And for example, one of the reasons why, you know, uh, generative adversarial networks

471
00:30:42,760 --> 00:30:48,360
have been thought of as, you know, more powerful than preceding, uh, generative models is because

472
00:30:48,360 --> 00:30:56,880
they can somehow find somehow find, um, different pockets where there's, uh, an up variation inside

473
00:30:56,880 --> 00:31:01,200
of, uh, a probability distribution of a certain data type.

474
00:31:01,200 --> 00:31:06,280
Um, but, but they did that's, that's, that's only so true to a certain extent.

475
00:31:06,280 --> 00:31:12,200
Otherwise, what, what one might be getting is sort of an averaging or blurring of, of

476
00:31:12,200 --> 00:31:15,200
a certain part of the data.

477
00:31:15,200 --> 00:31:22,680
Um, that's the one example where knowing how to, uh, pick the data, understanding what

478
00:31:22,680 --> 00:31:29,760
the implications in the various pockets of variation within a data sample mean in the

479
00:31:29,760 --> 00:31:33,400
real world, um, is priceless.

480
00:31:33,400 --> 00:31:39,480
One example of where this played out is with the RSNA Kaggle competition in which, um,

481
00:31:39,480 --> 00:31:45,400
the top 10, uh, finalists winners were all radiologists who have somehow along the way acquired

482
00:31:45,400 --> 00:31:50,800
ML skills, uh, and there's, there's no substitute, substitute for that.

483
00:31:50,800 --> 00:31:53,440
Um, and that's sort of always going to be the case.

484
00:31:53,440 --> 00:32:00,680
Another example is no one went to this halt training of an algorithm, um, with your standard

485
00:32:00,680 --> 00:32:05,440
supervised learning type problem, uh, you can sort of go off of your loss function.

486
00:32:05,440 --> 00:32:09,040
You can say that the losses dropped, the looser and threshold we're going to stop, we're

487
00:32:09,040 --> 00:32:10,040
doing great.

488
00:32:10,040 --> 00:32:13,720
Um, that process is governed by the data, of course.

489
00:32:13,720 --> 00:32:19,160
So assuming that you had good data, you, you can safely trust your halt point.

490
00:32:19,160 --> 00:32:26,200
But in the case of, say, again, the ultimate arbiter is still the human visual cortex.

491
00:32:26,200 --> 00:32:33,960
It's still a human being who understands what, um, a dog or a cat should look like because

492
00:32:33,960 --> 00:32:37,880
you've got these two competing algorithms that are fighting with each other and the loss

493
00:32:37,880 --> 00:32:40,760
itself is clearly now insufficient.

494
00:32:40,760 --> 00:32:44,840
And so you can't, one can't stop training again when the loss drops below a certain point

495
00:32:44,840 --> 00:32:50,720
because the adversariality, the push and pull that the discriminator and generator are

496
00:32:50,720 --> 00:32:53,720
doing to each other continues past a certain point.

497
00:32:53,720 --> 00:32:59,480
So you don't really know when to stop, uh, and there's no real quantitative approach

498
00:32:59,480 --> 00:33:05,640
that we currently have for knowing when to stop training, uh, these gain algorithms.

499
00:33:05,640 --> 00:33:10,320
And so because adversarial examples are another way that, that make that very apparent,

500
00:33:10,320 --> 00:33:13,520
that you really ultimately still need the domain expertise.

501
00:33:13,520 --> 00:33:18,720
It's an enormous value, um, in today in ML and I think we're all going to, I think people

502
00:33:18,720 --> 00:33:24,440
are starting to get that, um, people, the, the initial big hype and excitement that

503
00:33:24,440 --> 00:33:28,400
computers are going to take off or start into, to dampen, uh, and people are starting

504
00:33:28,400 --> 00:33:34,800
to realize that, um, the only real way forward is going to be, uh, in any, is going to be

505
00:33:34,800 --> 00:33:39,400
domain specific and it's going to be with really integrated interdisciplinary teams and

506
00:33:39,400 --> 00:33:47,400
ideally, people who have, um, uh, detailed, uh, expert level knowledge of both the ML side

507
00:33:47,400 --> 00:33:53,200
as well as whatever other field that they're looking to apply ML to be at agriculture,

508
00:33:53,200 --> 00:33:56,200
you know, be a transportation, be it healthcare.

509
00:33:56,200 --> 00:34:02,240
Tell me a little bit about your experience getting up to speed with, with GANs ultimately

510
00:34:02,240 --> 00:34:05,160
resulting in, uh, this paper you published.

511
00:34:05,160 --> 00:34:06,160
Oh, yeah.

512
00:34:06,160 --> 00:34:11,760
It's, uh, yeah, you know, I basically, I, um, I, I, I thought, you know, well, this, this

513
00:34:11,760 --> 00:34:12,760
could be interesting.

514
00:34:12,760 --> 00:34:14,600
It started with a theoretical question.

515
00:34:14,600 --> 00:34:18,840
I was, you know, I was given a talk and somebody asked me in the audience and I was actually

516
00:34:18,840 --> 00:34:23,640
given talking Nigeria where I went for a data science, uh, Nigeria they invited me in

517
00:34:23,640 --> 00:34:24,840
January just last month.

518
00:34:24,840 --> 00:34:27,560
Oh, two months ago, we're already in March here.

519
00:34:27,560 --> 00:34:33,280
Um, and, uh, and somebody, uh, somebody brought up, uh, whether GANs could be used for

520
00:34:33,280 --> 00:34:40,240
augmentation of data and, you know, and my initial inkling, I hadn't really worked with

521
00:34:40,240 --> 00:34:44,880
GANs at that point, um, only really a month ago.

522
00:34:44,880 --> 00:34:49,760
And, uh, my inkling was to say, no, I don't see it.

523
00:34:49,760 --> 00:34:56,920
I, I don't see a person pulling themselves up by their own bootstraps that it's your,

524
00:34:56,920 --> 00:35:05,320
and your GAN model, your generative model, what it's able to generate is necessarily constrained

525
00:35:05,320 --> 00:35:11,160
or bounded in terms of its accuracy in terms of its fidelity to the actual native data set

526
00:35:11,160 --> 00:35:18,600
is bounded by these initial sample from that data distribution, which is your training

527
00:35:18,600 --> 00:35:20,920
data for the discriminator.

528
00:35:20,920 --> 00:35:26,040
That was my response, you know, and, you know, and, uh, but I, I kept thinking about it

529
00:35:26,040 --> 00:35:30,160
though, you know, that, was that, was that correct?

530
00:35:30,160 --> 00:35:36,480
You know, is that true, um, and, and, uh, it's, it is, you know, I, I'm still there.

531
00:35:36,480 --> 00:35:38,800
And so I said, okay, let me, let me play with this thing.

532
00:35:38,800 --> 00:35:43,080
Let me take a look at it and actually look at the details of how it works and what it

533
00:35:43,080 --> 00:35:44,080
does.

534
00:35:44,080 --> 00:35:50,320
Uh, and so I, I, I read up on it and, you know, trained up again model, uh, and, you

535
00:35:50,320 --> 00:35:54,600
know, generating some data and I'm getting some really insights from, from that, I've

536
00:35:54,600 --> 00:35:57,480
only been doing that over the last couple of weeks here.

537
00:35:57,480 --> 00:36:01,720
And uh, been learning a number of things about how data works and what data is, what the

538
00:36:01,720 --> 00:36:04,120
distributions are and so on and so forth.

539
00:36:04,120 --> 00:36:11,800
And that only, um, strengthens my conviction that for us to go forward in ML, um, we're

540
00:36:11,800 --> 00:36:18,800
going to have to understand the domain very well because they're, it's necessarily a heuristic

541
00:36:18,800 --> 00:36:20,400
field.

542
00:36:20,400 --> 00:36:27,880
And so, um, yes, we can come up with theories that are true to some extent.

543
00:36:27,880 --> 00:36:32,960
But ultimately, only, only so much as they can somehow encapsulate the general properties

544
00:36:32,960 --> 00:36:33,960
of data.

545
00:36:33,960 --> 00:36:39,440
For instance, um, there's the, uh, in signals processing world, right?

546
00:36:39,440 --> 00:36:41,800
Where one is looking at doing a sampling.

547
00:36:41,800 --> 00:36:49,640
If your signal has a certain character to it, you know, if it's band limited, um, then

548
00:36:49,640 --> 00:36:55,640
you know that if you sample a certain way with a certain frequency, you can perfectly

549
00:36:55,640 --> 00:36:59,720
reconstruct that data set, you know, that type of idea, you know, the Shannon Nyquist

550
00:36:59,720 --> 00:37:05,960
sampling theorem, you know, that type of idea, um, can provide some guidance about what

551
00:37:05,960 --> 00:37:08,520
we can do within ML.

552
00:37:08,520 --> 00:37:14,360
But it's completely clear that, you know, Google, the Googles of this world would be completely

553
00:37:14,360 --> 00:37:19,840
outmatched by small teams of people, could be even two or three people who have expertise

554
00:37:19,840 --> 00:37:23,040
in their area and are working on those problems.

555
00:37:23,040 --> 00:37:25,440
And that's a very exciting time in history.

556
00:37:25,440 --> 00:37:33,800
You mentioned this process of working with gans, you know, beyond reinforcing the, the

557
00:37:33,800 --> 00:37:41,360
value of domain expertise kind of led to some insights around some of the core machine

558
00:37:41,360 --> 00:37:45,320
learning problems that you're working on, can you elaborate on those a bit?

559
00:37:45,320 --> 00:37:46,640
Right, right.

560
00:37:46,640 --> 00:37:52,040
So I'm actually, I'm trying to, I want to write it up, you know, put it on the archive.

561
00:37:52,040 --> 00:37:59,560
Some of these same type of ideas are, some of the paradoxes are that more data is not

562
00:37:59,560 --> 00:38:00,560
necessarily better.

563
00:38:00,560 --> 00:38:03,000
You know, that's kind of a big one.

564
00:38:03,000 --> 00:38:05,400
It's what type of data, right?

565
00:38:05,400 --> 00:38:07,800
And what problem are you trying to solve?

566
00:38:07,800 --> 00:38:10,320
Sometimes you actually need less data of a certain type.

567
00:38:10,320 --> 00:38:14,840
Sometimes when you have more data, you're diluted something, depending on what you're trying

568
00:38:14,840 --> 00:38:15,840
to accomplish.

569
00:38:15,840 --> 00:38:22,160
And how do you know what's the, like, this is probably what the thing that you're, that

570
00:38:22,160 --> 00:38:28,280
you need to write up, or that you're kind of moving towards writing up, but like, my

571
00:38:28,280 --> 00:38:33,880
sense is that there's, you know, a set of kind of disciplines behind what you're saying

572
00:38:33,880 --> 00:38:36,280
that aren't really talked about a lot.

573
00:38:36,280 --> 00:38:37,280
Right, right.

574
00:38:37,280 --> 00:38:43,560
And this field is so nascent, you know, this field of ML and data is wide open.

575
00:38:43,560 --> 00:38:44,560
Yeah.

576
00:38:44,560 --> 00:38:45,560
Yeah.

577
00:38:45,560 --> 00:38:46,360
It's totally wide open.

578
00:38:46,360 --> 00:38:51,440
And I think there have been certain mantras that have been just, you know, just regurgitated

579
00:38:51,440 --> 00:38:58,360
verbatim, but I think that, you know, the more we look at it, the more we honestly look

580
00:38:58,360 --> 00:39:04,360
at it, right, as opposed to trying to force machine learning to fit a certain model, to

581
00:39:04,360 --> 00:39:08,480
fit a certain way of thinking.

582
00:39:08,480 --> 00:39:13,200
I think the more we honestly look at it, the more we'll be able to get deeper into what,

583
00:39:13,200 --> 00:39:15,360
what the character of this thing is.

584
00:39:15,360 --> 00:39:20,000
It comes down to specificity, you know, what are we trying to do, you know, and how we

585
00:39:20,000 --> 00:39:24,960
trying to get there in a data set, you know, in a data distribution, for instance, there

586
00:39:24,960 --> 00:39:32,200
would be certain features, certain characteristics within certain neighborhoods of the distribution,

587
00:39:32,200 --> 00:39:34,680
which would be very different from other neighborhoods.

588
00:39:34,680 --> 00:39:40,520
And so if you're trying to capture the essence of a certain neighborhood of the data distribution,

589
00:39:40,520 --> 00:39:45,920
you're better off sampling from that area than coming up with, you know, saying I have

590
00:39:45,920 --> 00:39:52,440
10 million images, you know, of this broad, ill-defined, you know, data class.

591
00:39:52,440 --> 00:39:58,080
When in reality, they're truly subfamilies, multiple subfamilies within that whole area.

592
00:39:58,080 --> 00:40:00,200
So one has to really understand what they're doing.

593
00:40:00,200 --> 00:40:05,640
One has to really understand what the landscape of that data type is, be it language, be it

594
00:40:05,640 --> 00:40:11,000
image and be it a particular type of image and then guide that.

595
00:40:11,000 --> 00:40:15,080
So the human visual cortex and the human experience will remain the eye.

596
00:40:15,080 --> 00:40:21,120
In my view, will remain the ultimate arbiter for at least, you know, for the next decade.

597
00:40:21,120 --> 00:40:22,800
Who knows what's going to happen after that?

598
00:40:22,800 --> 00:40:26,600
Well, Stephen, it was great chatting with you about all this stuff.

599
00:40:26,600 --> 00:40:32,480
I'm looking forward to following along as you start to publish some of your insights

600
00:40:32,480 --> 00:40:41,040
into, you know, GANS and this whole dynamic around data and domain expertise and machine

601
00:40:41,040 --> 00:40:42,040
learning.

602
00:40:42,040 --> 00:40:43,440
It's all really interesting stuff.

603
00:40:43,440 --> 00:40:44,840
Well, thank you so much, Sam.

604
00:40:44,840 --> 00:40:48,360
It's really been an honor to be on your excellent show.

605
00:40:48,360 --> 00:40:52,920
And I'm looking forward to going through your entire archive.

606
00:40:52,920 --> 00:40:57,160
Yeah, I am. It's really good quality, high quality stuff.

607
00:40:57,160 --> 00:41:00,040
Thank you for your contribution to the community.

608
00:41:00,040 --> 00:41:01,040
Thank you.

609
00:41:01,040 --> 00:41:02,040
Thanks, Stephen.

610
00:41:02,040 --> 00:41:03,440
You're most welcome.

611
00:41:03,440 --> 00:41:07,520
All right, everyone.

612
00:41:07,520 --> 00:41:09,400
That's our show for today.

613
00:41:09,400 --> 00:41:15,440
For more information on today's show, visit twomolai.com slash shows.

614
00:41:15,440 --> 00:41:21,360
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platform's

615
00:41:21,360 --> 00:41:22,720
conference.

616
00:41:22,720 --> 00:41:26,400
As always, thanks so much for listening and catch you next time.

