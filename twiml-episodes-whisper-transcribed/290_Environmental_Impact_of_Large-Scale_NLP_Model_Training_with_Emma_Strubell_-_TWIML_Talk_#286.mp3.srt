1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,200
I'm your host, Sam Charrington, hey what's up everyone, we've got an amazing Twimble

4
00:00:34,200 --> 00:00:38,600
Con AI platform's event shaping up for you and I wanted to share a bit about it.

5
00:00:38,600 --> 00:00:41,920
The event is going to be anchored by what we're calling keynote interviews.

6
00:00:41,920 --> 00:00:46,800
These are live podcasts recorded right on the main stage and I'll be joined by some incredible

7
00:00:46,800 --> 00:00:52,680
guests, here are three that I'm really excited about.

8
00:00:52,680 --> 00:00:57,880
One, Andrew Eng, I could not be happier to have Andrew kicking off this event with me.

9
00:00:57,880 --> 00:01:02,640
Andrew is one of my AI heroes and no one has done more to bring new practitioners into machine

10
00:01:02,640 --> 00:01:05,240
learning and deep learning than he has.

11
00:01:05,240 --> 00:01:08,480
Andrew is going to be sharing a bit about what he's learned helping many businesses get

12
00:01:08,480 --> 00:01:12,640
productive with machine learning and AI and also speak with us about where he sees the

13
00:01:12,640 --> 00:01:13,960
field going.

14
00:01:13,960 --> 00:01:18,800
Number two, Hussein Mahana, Hussein is the head of AI and ML at Cruz, the self-driving

15
00:01:18,800 --> 00:01:20,160
car company.

16
00:01:20,160 --> 00:01:25,080
Before Cruz, Hussein helped build Google's cloud ML platform and Facebook's internal

17
00:01:25,080 --> 00:01:27,520
FB learner platform before that.

18
00:01:27,520 --> 00:01:31,480
Hussein is going to be sharing some of the lessons he's learned building ML platforms

19
00:01:31,480 --> 00:01:35,440
from scratch at some of the most advanced companies in the space.

20
00:01:35,440 --> 00:01:37,720
And number three, Fran Bell.

21
00:01:37,720 --> 00:01:41,560
Fran leads a team responsible for data science platforms at Uber.

22
00:01:41,560 --> 00:01:46,560
These are use case focused ML platform supporting application areas like forecasting, anomaly

23
00:01:46,560 --> 00:01:51,560
detection, NLP and conversational AI segmentation and more.

24
00:01:51,560 --> 00:01:56,440
Her platform sit on top of Uber's Michelangelo, putting here on a unique position to speak

25
00:01:56,440 --> 00:02:01,800
with us about how low level and higher level platforms come together to support data scientists

26
00:02:01,800 --> 00:02:04,040
and developer productivity.

27
00:02:04,040 --> 00:02:07,720
Beyond keynote interviews like this, we've got a bunch of outstanding speakers lined up

28
00:02:07,720 --> 00:02:12,200
to share their successes and failures, helping their organizations build and production

29
00:02:12,200 --> 00:02:15,120
lies, ML and deep learning models.

30
00:02:15,120 --> 00:02:21,280
If this lineup sounds interesting to you, visit twimmocon.com today to register.

31
00:02:21,280 --> 00:02:26,800
Use the coupon code great content through July 31st for an additional 10% off of our

32
00:02:26,800 --> 00:02:28,880
special early bird pricing.

33
00:02:28,880 --> 00:02:30,440
Hope to see you at Twimmocon.

34
00:02:30,440 --> 00:02:34,880
Alright everyone, I am on the line with Emma Strubel.

35
00:02:34,880 --> 00:02:38,120
Emma is a PhD student at UMass Amherst.

36
00:02:38,120 --> 00:02:40,720
Emma, welcome to this weekend machine learning and AI.

37
00:02:40,720 --> 00:02:41,720
Thanks a lot.

38
00:02:41,720 --> 00:02:43,720
Thanks for having me.

39
00:02:43,720 --> 00:02:47,400
Excited to talk to you and I'm particularly excited to have an opportunity to talk to you

40
00:02:47,400 --> 00:02:48,400
now.

41
00:02:48,400 --> 00:02:52,360
Just a few days before you'll be defending your thesis.

42
00:02:52,360 --> 00:03:00,120
So the best of luck on that, I'm sure by the time this, by the time this shows errors,

43
00:03:00,120 --> 00:03:05,040
you will, actually, it sounds like you'll be hiking someplace and on your way to being

44
00:03:05,040 --> 00:03:11,200
a visiting scientist at Facebook AI research for a year before you start as an assistant

45
00:03:11,200 --> 00:03:15,160
professor in the Language Technologies Institute at Carnegie Mellon.

46
00:03:15,160 --> 00:03:17,840
So congratulations on all that.

47
00:03:17,840 --> 00:03:18,840
First of all.

48
00:03:18,840 --> 00:03:19,840
Thank you so much.

49
00:03:19,840 --> 00:03:22,320
Yeah, it's very, very, lots of exciting changes.

50
00:03:22,320 --> 00:03:24,560
Yeah, absolutely, absolutely.

51
00:03:24,560 --> 00:03:31,040
So how did you get to this place where you have so much opportunity, like, and focused

52
00:03:31,040 --> 00:03:32,040
on AI?

53
00:03:32,040 --> 00:03:34,680
Like, what tell us a little bit about your path and your story?

54
00:03:34,680 --> 00:03:37,480
Yeah, so it was not at all on purpose.

55
00:03:37,480 --> 00:03:40,640
And actually, I have thoughts about that, which we could talk about if you want.

56
00:03:40,640 --> 00:03:41,640
Okay.

57
00:03:41,640 --> 00:03:44,040
Thoughts are always a good place to start.

58
00:03:44,040 --> 00:03:48,200
Yeah, I didn't even know that, like, sort of grad school or, like, getting a PhD was

59
00:03:48,200 --> 00:03:50,720
really a thing that I could do until undergrad.

60
00:03:50,720 --> 00:03:54,120
And I had professors who kind of, like, maybe pushed me in that direction.

61
00:03:54,120 --> 00:03:56,880
I got involved in some undergraduate research.

62
00:03:56,880 --> 00:04:01,280
But actually, when I applied to graduate school, I intended to study, sort of, like, computational

63
00:04:01,280 --> 00:04:02,280
biology.

64
00:04:02,280 --> 00:04:05,360
So I was actually interested in studying and, like, understanding, like, computational

65
00:04:05,360 --> 00:04:07,640
power of biological systems.

66
00:04:07,640 --> 00:04:12,360
But my relationship with my initial advisor did not work out.

67
00:04:12,360 --> 00:04:17,400
And I was, like, really fortunate to just be introduced to Andrew McComb, who's my current

68
00:04:17,400 --> 00:04:21,600
PhD advisor by someone else I knew in my PhD program.

69
00:04:21,600 --> 00:04:25,440
And then I started working with him, and I actually just, like, found that I really enjoyed

70
00:04:25,440 --> 00:04:26,440
the work.

71
00:04:26,440 --> 00:04:29,360
So what department are you in the CS department, or?

72
00:04:29,360 --> 00:04:32,880
Yeah, it's technically the College of Information and Computer Sciences.

73
00:04:32,880 --> 00:04:36,560
When I joined, it was actually the computer science department, but we've grown a lot.

74
00:04:36,560 --> 00:04:41,120
And that was also my, yeah, my undergraduate degree was in computer science and, like,

75
00:04:41,120 --> 00:04:42,120
a minor in math.

76
00:04:42,120 --> 00:04:45,480
And I also took some biology classes and stuff like this, because I do, I like biology

77
00:04:45,480 --> 00:04:46,480
a lot.

78
00:04:46,480 --> 00:04:47,480
Okay.

79
00:04:47,480 --> 00:04:48,480
Cool.

80
00:04:48,480 --> 00:04:49,480
Also at Amherst?

81
00:04:49,480 --> 00:04:50,480
Nope.

82
00:04:50,480 --> 00:04:51,480
I actually went to the University of Maine.

83
00:04:51,480 --> 00:04:52,480
Okay.

84
00:04:52,480 --> 00:04:53,480
So, yeah.

85
00:04:53,480 --> 00:04:58,480
How do you kind of describe your, your primary research interest nowadays?

86
00:04:58,480 --> 00:05:03,680
I'd say, like, the overall goal of my research is I want to build, like, I want to bring

87
00:05:03,680 --> 00:05:08,440
state-of-art NLP systems to, like, actual practitioners who actually want to use them.

88
00:05:08,440 --> 00:05:11,360
And I actually think there's, like, kind of, like, a large disconnect here between, you

89
00:05:11,360 --> 00:05:14,720
know, people who are doing, like, state-of-art NLP research and sort of, like, chasing these,

90
00:05:14,720 --> 00:05:17,520
like, high accuracies on these benchmark data sets.

91
00:05:17,520 --> 00:05:21,360
And then, but if you take these models that get really high accuracy on some benchmark

92
00:05:21,360 --> 00:05:28,000
data set, like, and then run them on your, like, actual data, like, your actual documents

93
00:05:28,000 --> 00:05:31,600
or whatever, like, that accuracy tends to go way down.

94
00:05:31,600 --> 00:05:37,000
And then similarly, I think people, like, the most accurate systems tend to be, like,

95
00:05:37,000 --> 00:05:42,360
also the slowest, oftentimes, sort of, like, the biggest models that take the most computation.

96
00:05:42,360 --> 00:05:47,240
So, I guess, like, two angles of my work have been, you know, developing machine learning

97
00:05:47,240 --> 00:05:48,840
models that are as efficient as possible.

98
00:05:48,840 --> 00:05:52,520
So trying to get that state-of-art accuracy, but requiring, like, much less computation.

99
00:05:52,520 --> 00:05:56,880
And also developing models that are going to be robust so that we can train them on, like,

100
00:05:56,880 --> 00:05:58,280
the annotated data that we have.

101
00:05:58,280 --> 00:06:02,520
But hopefully, those same models will also perform well on, like, data from many different

102
00:06:02,520 --> 00:06:03,520
sources.

103
00:06:03,520 --> 00:06:07,520
And when you think about practitioners, are you thinking about any particular domain

104
00:06:07,520 --> 00:06:10,400
or are you, kind of, tackling it broadly?

105
00:06:10,400 --> 00:06:15,520
Yeah, I mean, it's generally, like, I hope that my research will apply to practitioners,

106
00:06:15,520 --> 00:06:16,920
like, kind of, from any domain.

107
00:06:16,920 --> 00:06:17,920
That's the hope.

108
00:06:17,920 --> 00:06:18,920
I do.

109
00:06:18,920 --> 00:06:23,680
So specifically, I've worked with collaborators in material science, which has been really

110
00:06:23,680 --> 00:06:24,680
cool.

111
00:06:24,680 --> 00:06:30,680
So they're interested in being able to extract, like, recipes for new materials, essentially,

112
00:06:30,680 --> 00:06:31,680
from the literature.

113
00:06:31,680 --> 00:06:35,720
And so I've been working with them to try to develop models that can do that.

114
00:06:35,720 --> 00:06:37,720
And it's been really, really interesting.

115
00:06:37,720 --> 00:06:38,720
Oh, interesting.

116
00:06:38,720 --> 00:06:45,160
That use case is the literature has the recipes in it, but it's in English, if that's what

117
00:06:45,160 --> 00:06:49,640
you, if that's the language you're studying in B, if that's what you can call what's written

118
00:06:49,640 --> 00:06:54,040
in material science, academic, journal, particles, and you're trying to-

119
00:06:54,040 --> 00:06:55,040
Yeah, exactly.

120
00:06:55,040 --> 00:06:56,040
I can't understand.

121
00:06:56,040 --> 00:06:57,040
Exactly.

122
00:06:57,040 --> 00:06:58,040
Exactly.

123
00:06:58,040 --> 00:07:02,240
And so you're trying to capture from there some representation of what the recipe is.

124
00:07:02,240 --> 00:07:03,240
Yeah, exactly.

125
00:07:03,240 --> 00:07:07,840
Going from some of the unstructured, it is English, yeah, research, article text, to, like,

126
00:07:07,840 --> 00:07:11,680
some structured representation, that's, like, these are the material precursors, then,

127
00:07:11,680 --> 00:07:16,600
you know, you heat them for this amount of time, and then, like, drive them and add this

128
00:07:16,600 --> 00:07:19,440
other thing in, and then you get this result that has these properties.

129
00:07:19,440 --> 00:07:22,720
So then, once you have that information, you can kind of, like, analyze it at a large

130
00:07:22,720 --> 00:07:27,640
scale to, like, poss- like, our dream goal would be to, like, be able to generate new material

131
00:07:27,640 --> 00:07:31,480
recipes, like, given some target material generating the recipe, because, basically, it's

132
00:07:31,480 --> 00:07:36,360
done through, like, trial and error research in, like, a lab right now, so it's a very slow

133
00:07:36,360 --> 00:07:38,200
and error-prone process.

134
00:07:38,200 --> 00:07:39,200
Awesome.

135
00:07:39,200 --> 00:07:42,680
It's a fun application, because it has, you know, this is, like, how we're going to

136
00:07:42,680 --> 00:07:45,640
develop, like, sustainable energy, right?

137
00:07:45,640 --> 00:07:49,560
These are, like, the materials that are used for, like, solar cells and stuff like this.

138
00:07:49,560 --> 00:07:52,160
So it's a fun project.

139
00:07:52,160 --> 00:07:56,480
You kind of describe this problem of, you know, having these state-of-the-art results

140
00:07:56,480 --> 00:08:04,400
in academic literature and practitioners not being able to use them, and you're primarily

141
00:08:04,400 --> 00:08:13,560
focused on, it sounds like, kind of, the general-generalized ability of the results, but it also

142
00:08:13,560 --> 00:08:18,800
kind of speaks to, like, a whole, you know, repeatability in machine-learning research

143
00:08:18,800 --> 00:08:20,200
kind of issue, as well.

144
00:08:20,200 --> 00:08:21,840
Are you tackling that?

145
00:08:21,840 --> 00:08:26,520
I'm not yet, but I actually have been thinking about doing something like that, like, sort

146
00:08:26,520 --> 00:08:31,800
of similar to this energy and policy considerations paper, which, I guess, we haven't necessarily

147
00:08:31,800 --> 00:08:33,320
talked about yet.

148
00:08:33,320 --> 00:08:34,320
But we will.

149
00:08:34,320 --> 00:08:35,320
Yeah.

150
00:08:35,320 --> 00:08:36,320
But we will.

151
00:08:36,320 --> 00:08:37,320
Yeah, yeah.

152
00:08:37,320 --> 00:08:38,320
So that's something I haven't tackled yet.

153
00:08:38,320 --> 00:08:42,320
But I actually was thinking about, like, sort of, like, doing an analysis of, like, I guess

154
00:08:42,320 --> 00:08:46,480
I was thinking about statistical significance in particular, which is kind of, like, a related

155
00:08:46,480 --> 00:08:47,480
issue.

156
00:08:47,480 --> 00:08:51,520
So, like, an NLP, we don't usually measure significance of our results.

157
00:08:51,520 --> 00:08:54,720
And there's kind of, like, issues with people reporting the max number out of many random

158
00:08:54,720 --> 00:08:55,720
seeds.

159
00:08:55,720 --> 00:08:57,320
And then it's hard to repeat these experiments.

160
00:08:57,320 --> 00:08:58,320
Yeah.

161
00:08:58,320 --> 00:09:01,320
But, yeah, I do think that's, like, a really interesting, interesting thing, yeah.

162
00:09:01,320 --> 00:09:07,280
Okay, well, since you spilled the beans, you introduced the topic that we're going to be

163
00:09:07,280 --> 00:09:08,280
talking about this.

164
00:09:08,280 --> 00:09:09,280
Sorry.

165
00:09:09,280 --> 00:09:10,280
No, no, no, I'm just teasing.

166
00:09:10,280 --> 00:09:11,280
I'm just teasing.

167
00:09:11,280 --> 00:09:14,600
So this is a paper that you, it's a pretty recent paper, at least according to the, the

168
00:09:14,600 --> 00:09:18,360
most recent one that's up on archive, was this month, right?

169
00:09:18,360 --> 00:09:19,360
Yeah, yeah.

170
00:09:19,360 --> 00:09:21,760
I actually haven't presented it at the conference yet.

171
00:09:21,760 --> 00:09:26,240
So, like, the speed of research is this.

172
00:09:26,240 --> 00:09:27,240
Yep.

173
00:09:27,240 --> 00:09:32,680
So the paper is called Energy and Policy Considerations for Deep Learning in NLP.

174
00:09:32,680 --> 00:09:35,240
And what conference are you presenting it at?

175
00:09:35,240 --> 00:09:36,240
It'll be A.C.L.

176
00:09:36,240 --> 00:09:42,280
So it's the, uh, annual conference of the Association for Computational Linguistics is, like, one

177
00:09:42,280 --> 00:09:47,080
of the top, like, international NLP conferences, um, it's in Florence in July.

178
00:09:47,080 --> 00:09:48,080
Oh, nice.

179
00:09:48,080 --> 00:09:49,680
That's a good one to go, too.

180
00:09:49,680 --> 00:09:50,680
Yeah.

181
00:09:50,680 --> 00:09:51,680
Well, I think it's, like, peak tourist season.

182
00:09:51,680 --> 00:09:55,680
So, like, I don't know if it's, like, totally the best time to go there, but, okay.

183
00:09:55,680 --> 00:09:56,680
Okay.

184
00:09:56,680 --> 00:09:57,680
Yeah.

185
00:09:57,680 --> 00:10:02,640
Um, so, uh, why don't you tell us a little bit about this paper?

186
00:10:02,640 --> 00:10:04,960
What were your goals in writing it?

187
00:10:04,960 --> 00:10:06,480
Yeah, absolutely.

188
00:10:06,480 --> 00:10:12,480
Um, so I talked about how, like, one of the focuses of my research has been developing

189
00:10:12,480 --> 00:10:13,480
these efficient models.

190
00:10:13,480 --> 00:10:18,280
And so the motivation for a lot of that research is, um, basically that we want, you know,

191
00:10:18,280 --> 00:10:20,880
that we even want machine learning models to be efficient and that we want them to be

192
00:10:20,880 --> 00:10:25,200
efficient because we care about the cost in terms of money, but also in terms of, like,

193
00:10:25,200 --> 00:10:28,800
the cost of the environment, because just bigger models require more energy, which has

194
00:10:28,800 --> 00:10:31,520
more, like, CO2 output, basically.

195
00:10:31,520 --> 00:10:35,400
So that was, it has been our, like, motivation and a few of the papers that are written.

196
00:10:35,400 --> 00:10:40,080
Um, and, but then I realized that no one is really, like, uh, quantified this, especially

197
00:10:40,080 --> 00:10:41,080
not an NLP.

198
00:10:41,080 --> 00:10:43,960
So there's been, like, but, yeah, I don't know of any work actually really quantifying

199
00:10:43,960 --> 00:10:47,280
this in terms of, like, the carbon footprint of training these NLP models.

200
00:10:47,280 --> 00:10:53,560
Um, and at the same time, there's been this, um, surge recently in NLP, um, or this,

201
00:10:53,560 --> 00:10:58,560
I guess, trend of training, like, bigger and bigger models on more and more data and,

202
00:10:58,560 --> 00:11:03,080
um, these models are performing really well, like, getting really high accuracies, um,

203
00:11:03,080 --> 00:11:06,880
but they're just, like, the computational resources required to train them are, like,

204
00:11:06,880 --> 00:11:11,560
enormous, like, such that, you know, a researcher like me who's not currently at, like, a big

205
00:11:11,560 --> 00:11:15,800
company, like, Facebook or Google, like, cannot afford and just, and I don't even have

206
00:11:15,800 --> 00:11:18,800
access to, like, the hardware to train these models.

207
00:11:18,800 --> 00:11:25,520
So we're talking about, like, language models, like GPT-2 or, okay, Google just a couple

208
00:11:25,520 --> 00:11:26,520
of days ago.

209
00:11:26,520 --> 00:11:32,640
Uh, I think we're in the Excel net, uh, and some folks did, uh, kind of back at the

210
00:11:32,640 --> 00:11:40,600
envelope analysis that, uh, showed concluded that it costs $245,000 to train that model.

211
00:11:40,600 --> 00:11:41,600
Yeah, exactly.

212
00:11:41,600 --> 00:11:42,600
Yeah.

213
00:11:42,600 --> 00:11:43,600
That's not accessible.

214
00:11:43,600 --> 00:11:44,600
Yeah, no, not really.

215
00:11:44,600 --> 00:11:50,520
I love, like, it's a pretty large portion of, like, an NSF grant, but, uh, yeah, yeah.

216
00:11:50,520 --> 00:11:53,640
So basically we were inspired by, yeah, the fact that we didn't see people quantifying

217
00:11:53,640 --> 00:11:57,640
this and, like, I think there's been sort of, like, an exponential growth in the actual,

218
00:11:57,640 --> 00:12:00,400
like, energy and computation requirements of models recently.

219
00:12:00,400 --> 00:12:03,760
Um, so, yeah, we just wanted to, to quantify that.

220
00:12:03,760 --> 00:12:07,640
And then also, I guess, talk about some conclusions based on our, our results.

221
00:12:07,640 --> 00:12:15,000
Yeah, and so one of the things that, uh, that you do is kind of compare the estimated

222
00:12:15,000 --> 00:12:21,120
emissions for training some of these models with kind of the things that we usually associate

223
00:12:21,120 --> 00:12:27,480
with, you know, environmental damage or at least emissions, like, air travel and, you

224
00:12:27,480 --> 00:12:29,920
know, car travel, things like that.

225
00:12:29,920 --> 00:12:30,920
What did you find there?

226
00:12:30,920 --> 00:12:31,920
Yeah.

227
00:12:31,920 --> 00:12:42,040
There's like a number of numbers, um, I guess one thing is training one of these models

228
00:12:42,040 --> 00:12:43,200
in this particular way.

229
00:12:43,200 --> 00:12:46,000
That's like very sort of computationally intensive.

230
00:12:46,000 --> 00:12:52,600
Um, so this is like training, uh, a machine translation model using this new technique called

231
00:12:52,600 --> 00:12:57,920
neural architecture search required basically multiple, let's see, what's it three, four?

232
00:12:57,920 --> 00:13:01,880
I'm not going to do math in my head, but multiple times the entire carbon footprint

233
00:13:01,880 --> 00:13:08,440
of like a car in its lifetime, including fuel and manufacturing, um, which is ridiculous.

234
00:13:08,440 --> 00:13:11,960
And that's like, you know, that's even more carbon footprint than like the average American

235
00:13:11,960 --> 00:13:12,960
life.

236
00:13:12,960 --> 00:13:13,960
I guess.

237
00:13:13,960 --> 00:13:17,440
So I should say one of the, one of the things we did is we, um, we analyzed like the total

238
00:13:17,440 --> 00:13:22,880
carbon footprint of, um, basically my last paper, like the research and development required

239
00:13:22,880 --> 00:13:27,000
to, you know, develop the, the last model that I published.

240
00:13:27,000 --> 00:13:28,000
Okay.

241
00:13:28,000 --> 00:13:32,680
It's like just like the carbon footprint of that, well, the work that I did, like easily,

242
00:13:32,680 --> 00:13:38,160
like doubled or tripled, uh, my personal carbon footprint last year, which I found, like

243
00:13:38,160 --> 00:13:39,160
super alarming.

244
00:13:39,160 --> 00:13:40,160
Okay.

245
00:13:40,160 --> 00:13:41,160
Wow.

246
00:13:41,160 --> 00:13:42,160
Yeah.

247
00:13:42,160 --> 00:13:43,160
Yeah.

248
00:13:43,160 --> 00:13:44,160
Huh.

249
00:13:44,160 --> 00:13:48,960
Were those the two models or scenarios that you were primarily, primarily looking at

250
00:13:48,960 --> 00:13:57,160
that your, uh, NLP pipeline and this kind of transformer, neural architecture search model?

251
00:13:57,160 --> 00:13:58,160
Yeah.

252
00:13:58,160 --> 00:14:04,120
I mean, I'd say we looked at a variety of these, um, like these popular pre-trained language

253
00:14:04,120 --> 00:14:05,120
models.

254
00:14:05,120 --> 00:14:10,800
Um, so we looked at, uh, like LMO for GPT-2, um, and then yeah, we were interested in looking

255
00:14:10,800 --> 00:14:16,200
at neural architecture search in particular because, uh, I think like the, yeah, I mean,

256
00:14:16,200 --> 00:14:20,600
it's a very, uh, computation, it's a very computation-hungry approach.

257
00:14:20,600 --> 00:14:27,600
And I think like the, uh, like the resulting benefit is relatively small, um, and so I

258
00:14:27,600 --> 00:14:30,960
had a feeling it would be kind of like a drastic number, so I wanted to compute it and kind

259
00:14:30,960 --> 00:14:35,640
of maybe encourage people to be a little bit more responsible about, uh, like sort of like

260
00:14:35,640 --> 00:14:37,400
these brute force techniques.

261
00:14:37,400 --> 00:14:38,400
Yeah.

262
00:14:38,400 --> 00:14:43,880
I've had a number of conversations with folks and seeing like blog posts, uh, where folks

263
00:14:43,880 --> 00:14:52,720
try to capture, uh, in this case, the case I'm thinking of the, like incremental cost

264
00:14:52,720 --> 00:15:00,800
of training a model, you know, per incremental percentage accuracy benefit or business benefit

265
00:15:00,800 --> 00:15:07,800
and like try to encourage people to think about that, you know, as they're devoting resources

266
00:15:07,800 --> 00:15:12,920
to building out these models, um, yeah.

267
00:15:12,920 --> 00:15:19,400
And, you know, it's not something that I hear people talking about a lot, um, and I haven't

268
00:15:19,400 --> 00:15:28,040
come across any particularly like rigorous way of doing it or, you know, frameworks for

269
00:15:28,040 --> 00:15:30,000
doing it or anything like that.

270
00:15:30,000 --> 00:15:36,160
And I wonder in your research, did you, did, were you somehow able to like, you know, really

271
00:15:36,160 --> 00:15:43,680
get into that kind of the incremental carbon emissions per incremental benefit and accuracy

272
00:15:43,680 --> 00:15:44,680
or something like that?

273
00:15:44,680 --> 00:15:47,520
Did you compare that across different techniques?

274
00:15:47,520 --> 00:15:49,360
That's a great question.

275
00:15:49,360 --> 00:15:53,200
We didn't, but, um, people should totally do that.

276
00:15:53,200 --> 00:15:55,200
But I'm out of here.

277
00:15:55,200 --> 00:15:59,200
I'm not really happy to do it too.

278
00:15:59,200 --> 00:16:01,880
I just had to, you know, buy the time.

279
00:16:01,880 --> 00:16:02,880
Yeah.

280
00:16:02,880 --> 00:16:08,600
So, like, take away as from this paper that we write about is, um, basically that authors

281
00:16:08,600 --> 00:16:11,920
should report numbers that make it easier to do that comparison.

282
00:16:11,920 --> 00:16:16,880
And so I think there's some pretty straightforward, um, things that people could compute for like

283
00:16:16,880 --> 00:16:21,440
any machine learning model that would like allow us to compare sort of like, like how

284
00:16:21,440 --> 00:16:25,200
much computation is required basically to get that accuracy.

285
00:16:25,200 --> 00:16:28,240
And then it would be like super easy to just, yeah, compute those numbers.

286
00:16:28,240 --> 00:16:33,840
So if people could compute, um, or report just like gigaflops to convergence, that's like

287
00:16:33,840 --> 00:16:34,840
a pretty easy thing.

288
00:16:34,840 --> 00:16:37,680
Like, uh, there's like a couple papers where they've reported that like when they're trying

289
00:16:37,680 --> 00:16:41,600
to compare and show their model is like better, um, I think if that was just like a standard

290
00:16:41,600 --> 00:16:45,360
thing we reported alongside, you know, accuracy or whatever of their metric, I think that would

291
00:16:45,360 --> 00:16:50,480
be like really, um, it's like not that hard and, um, it would just really allow us to make

292
00:16:50,480 --> 00:16:57,560
that comparison directly, um, and similarly like something that's a little bit less straightforward

293
00:16:57,560 --> 00:17:02,440
but that would be really helpful in a number of ways is reporting, um, sort of like sensitivity

294
00:17:02,440 --> 00:17:07,640
to hyper parameters, um, because like a lot, yeah, a lot of this energy use actually comes

295
00:17:07,640 --> 00:17:12,280
from sort of like how wasteful we are in terms of, um, you know, tuning these neural network

296
00:17:12,280 --> 00:17:17,400
models that, you know, notoriously require like a lot of finessing and tuning.

297
00:17:17,400 --> 00:17:20,520
And that tends to be like if you want to train out a new data set, you know, you have to

298
00:17:20,520 --> 00:17:23,880
do some kind of tuning, but we don't, we do like a really bad job.

299
00:17:23,880 --> 00:17:28,520
This goes back to what you mentioned about reproducibility. I think we do a really bad job of like

300
00:17:28,520 --> 00:17:33,160
actually being really precise about how much of that tuning happens, um, and then how

301
00:17:33,160 --> 00:17:37,160
much is required like there's like, you know, there's a lot that goes into development of a model

302
00:17:37,160 --> 00:17:42,520
and develop it as a new technique. When you think about this, are you thinking about it like

303
00:17:42,520 --> 00:17:48,920
from a pipeline perspective and hey, you know, this is an iterative process and we're, you know,

304
00:17:48,920 --> 00:17:55,400
kind of playing with all these hyperparameters, uh, to actually get to a model or are you thinking

305
00:17:55,400 --> 00:18:01,800
about it like, I don't know if this actually makes any sense, but like somehow dropout is more

306
00:18:01,800 --> 00:18:08,440
expensive than, you know, something else from a, you know, technique perspective during training.

307
00:18:08,440 --> 00:18:12,600
Oh, that's super interesting. No, I wasn't thinking about that, but dropout is like more,

308
00:18:12,600 --> 00:18:15,800
probably more expensive than like some other technique just because it's like way less

309
00:18:15,800 --> 00:18:19,880
simple efficient, like if you train with dropout versus I thought there's other like more direct

310
00:18:19,880 --> 00:18:24,280
regularization approaches. I was not talking about that, but if you want to collaborate on,

311
00:18:26,280 --> 00:18:31,960
something else for someone else to figure out. Yeah. That's, that's great. Yeah, people should

312
00:18:31,960 --> 00:18:36,280
like listen to your podcast for like cool research ideas. Yeah, no, I was thinking more just

313
00:18:36,280 --> 00:18:40,600
in terms of like if you want to take some like published model and then apply it to your data

314
00:18:40,600 --> 00:18:45,400
to sort of like a new domain, I think it's really hard to tell like how much tuning is actually

315
00:18:45,400 --> 00:18:49,720
going to require it and people just don't report that. Yeah. So it's both kind of like not super

316
00:18:49,720 --> 00:18:54,840
reproduced. You know, you'll say, oh, we did a good search of these things, but like, I think

317
00:18:54,840 --> 00:19:01,240
it's not, it's not actually that straightforward always. You know, characterizing the cost of just

318
00:19:02,040 --> 00:19:09,720
inefficiencies in the way we do this stuff like, you know, imagining there's, you know, some set of

319
00:19:09,720 --> 00:19:16,840
techniques that, you know, people still do all the time that are like far from, you know, best

320
00:19:16,840 --> 00:19:23,160
practice or state of the art that, you know, probably just like throw, you know, tons of CO2

321
00:19:23,160 --> 00:19:27,160
emissions out there for not. Like, have you given any thoughts on anything like that?

322
00:19:27,800 --> 00:19:33,400
That's interesting. I mean, this is not exactly what you said, but something that came to mind

323
00:19:33,400 --> 00:19:40,680
is sort of, I think people are very eager to use like deep learning when they can definitely get

324
00:19:40,680 --> 00:19:47,400
away with like just like a single like logistic regression. So that comes to mind as like something

325
00:19:47,400 --> 00:19:56,680
where, yeah, like at the call, yeah, people want their technology to sound cool and to use the

326
00:19:56,680 --> 00:20:03,240
cool stuff. But yeah, I think for, you know, for some, yeah, for many tasks, you don't really need like a

327
00:20:03,240 --> 00:20:08,120
big deep fancy model. You can just do like classification, like a, yeah, shallow classification.

328
00:20:08,120 --> 00:20:13,560
It'll work fine. Right. Right. Yeah. I mean, all of these, all of these questions are kind of

329
00:20:14,360 --> 00:20:21,720
interrelated. And I guess that goes back to the fundamental contribution of this work, which is,

330
00:20:21,720 --> 00:20:26,600
hey, we really need to be thinking about how much it costs us to get this incremental, you know,

331
00:20:26,600 --> 00:20:30,680
percentage of, you know, performance by whatever metric you're using.

332
00:20:31,320 --> 00:20:35,880
Absolutely. Like something like neural architecture search, where it's kind of,

333
00:20:36,840 --> 00:20:42,280
I guess it's going back to the reproducibility thing. Like, you know, take GPGP2, right? A bunch of

334
00:20:42,280 --> 00:20:49,400
people have reported like trying to reproduce GPGP2 and OpenAI hasn't said everything that you

335
00:20:49,400 --> 00:20:56,120
need to do to get the exact kind of results that they got. Like, do you, you know, if you're

336
00:20:56,120 --> 00:21:02,760
trying to benchmark these things, do you even know like how close you are to the actual

337
00:21:04,120 --> 00:21:10,360
cost to produce their model? Does that make any sense? Yeah. Yeah. That's an interesting question.

338
00:21:10,360 --> 00:21:16,200
I think if I understand what you're saying correctly, it's kind of like, uh, do we actually know

339
00:21:16,200 --> 00:21:23,000
what goes into training? Like, are estimates even reasonable? Like, did we, uh, yeah? Well, I guess,

340
00:21:23,000 --> 00:21:28,520
you know, so part of the question, so maybe taking a step back, like to train the, the neural

341
00:21:28,520 --> 00:21:37,000
architecture search based model did presumably you actually implemented that as opposed to

342
00:21:38,040 --> 00:21:45,640
determine the cost analytically or no? Yeah, no. So yeah, I will describe to you our methodology.

343
00:21:45,640 --> 00:21:53,720
Okay. With the exception of GPGP2 actually, we, we basically took so all the models we analyzed

344
00:21:53,720 --> 00:22:02,280
had open source code. We took that code. We like just had it perform training for like up to a day

345
00:22:02,280 --> 00:22:10,920
and then we sampled like the actual energy draw of the GPU or GPUs and the CPUs and the main memory.

346
00:22:10,920 --> 00:22:15,480
So making the assumption that like the main energy draw from the computer is going to be from

347
00:22:15,480 --> 00:22:20,600
like the, these copy, the computational hardware essentially. And then we extrapolated that

348
00:22:20,600 --> 00:22:27,880
based on, uh, the amount of time and the hardware, um, as reported in the paper that they

349
00:22:27,880 --> 00:22:32,600
used to train the model. Oh, okay. Yeah. So we didn't, because it would have taken, I mean,

350
00:22:32,600 --> 00:22:38,360
as I said, at least two hundred and forty five thousand dollars. Yeah. Right.

351
00:22:38,360 --> 00:22:41,960
So like, I'm like, I think the entire thing, but I figured this is like a reasonable, uh,

352
00:22:41,960 --> 00:22:46,600
way to estimate it. And I'm happy to talk more about like how we then convert that to carbon

353
00:22:46,600 --> 00:22:49,880
and stuff like this because it's a little bit like, you just have to make a lot of assumptions

354
00:22:49,880 --> 00:22:53,720
along the way. But we figured it's like, we tried to make reasonable assumptions and it's like,

355
00:22:53,720 --> 00:23:00,200
we think our estimates are like ballpark reasonable. Did you run it for the day on the exact

356
00:23:00,200 --> 00:23:05,080
hardware that they said they used? If they said they, if they reported the hardware that they used

357
00:23:05,080 --> 00:23:10,520
or did you have to extrapolate to that as well? Yeah. We had to extrapolate to that as well.

358
00:23:10,520 --> 00:23:15,080
But we did make up effort to use hardware that's like fairly similar. So like,

359
00:23:16,200 --> 00:23:22,040
so the GPUs that we used basically have it have the same. So like one of the metrics like

360
00:23:22,040 --> 00:23:27,080
that's reported for a GPU is sort of like, it's maximum power draw. And so the maximum power draw,

361
00:23:27,080 --> 00:23:31,800
the GPUs that we were using, uh, is the same as the maximum power draw, the GPUs they were using.

362
00:23:31,800 --> 00:23:36,760
So yeah, I think it's like probably the, yeah, power use is similar. And they're,

363
00:23:36,760 --> 00:23:40,680
they're not like incredibly different. They're like, you know, similar age GPUs and stuff like

364
00:23:40,680 --> 00:23:45,640
this. But yeah, it wasn't in most cases. It was not the exact same hardware. And then in here,

365
00:23:45,640 --> 00:23:53,640
you talk a little bit about the ultimate source of the energy, like whether it's renewable versus

366
00:23:53,640 --> 00:24:03,480
gas, coal, etc. Like how did that, how were you able to fit that into this model? Do the various,

367
00:24:03,480 --> 00:24:06,680
like data center providers, cloud providers, do they report on this stuff?

368
00:24:08,360 --> 00:24:15,400
Yeah. So they're not super transparent about it. Surprise. Yeah. And so these numbers, we got

369
00:24:15,400 --> 00:24:22,280
these numbers from, well, so for the cloud providers, we got the numbers from a 2017 white paper

370
00:24:22,280 --> 00:24:26,040
that Greenpeace did. And it seemed like they surveyed the companies and got these numbers

371
00:24:26,040 --> 00:24:35,400
somehow from them. Yeah. And so, and then so in our computation, we used basically the mapping

372
00:24:35,400 --> 00:24:44,680
from like kilowatt hours of energy to carbon produced or like CO2 produced as provided by the

373
00:24:44,680 --> 00:24:51,560
US EPA, like that's like the average for power consumed in the United States. Okay. And so if you

374
00:24:51,560 --> 00:24:58,520
look at the breakdown of AWS's energy, and so like obviously like where the mapping from like

375
00:24:58,520 --> 00:25:02,440
electricity, the carbon footprint is very dependent on like these sources. So if it's from

376
00:25:02,440 --> 00:25:07,960
renewable sources, it maps to zero. And then if it's from nuclear, you know, maps to less than from

377
00:25:07,960 --> 00:25:15,160
gas or coal, like much less. So yeah. So if you look at the actual breakdown of energy resources

378
00:25:15,160 --> 00:25:21,880
as of 2017 of Amazon AWS, it like pretty closely mirrors the breakdown from the US. And AWS is

379
00:25:21,880 --> 00:25:28,120
like the largest cloud provider. So for that reason, we thought that it was a like a pretty like

380
00:25:28,120 --> 00:25:33,880
reasonable estimate for mapping. So since coming out with this work, people from some of these

381
00:25:33,880 --> 00:25:38,200
companies have contacted me and let me know that these numbers are out of date and they're doing

382
00:25:38,200 --> 00:25:44,840
better now. So yeah, we're considering, yeah, we're considering doing an update like in the fall

383
00:25:45,800 --> 00:25:50,120
for something with more up to date numbers. And of course, and something that we don't like address

384
00:25:51,320 --> 00:25:56,360
a ton in the paper is that a lot of companies are also moving more towards actually many of them.

385
00:25:56,360 --> 00:26:02,840
I know like Google at least is like 100% renewable in terms of like I don't want to say renewable,

386
00:26:02,840 --> 00:26:10,040
but they're yeah, basically they buy carbon offsets to make up for like energy in their cloud

387
00:26:10,040 --> 00:26:15,240
that does not confirm renewable sources. So that's good. That's better than nothing, but it's also not

388
00:26:15,240 --> 00:26:21,800
the same as like using actual renewable energy. Yeah, but they are relative to the other providers

389
00:26:21,800 --> 00:26:32,520
given this 2017 data kicking, but on the renewable 56% for them relative to 32 for Microsoft and

390
00:26:32,520 --> 00:26:39,880
2017 for AWS. Yeah, definitely. It seems like they really care about that. And also like TPUs are

391
00:26:39,880 --> 00:26:44,360
actually, so all the numbers that we were able to compute were for GPU just because we don't,

392
00:26:44,360 --> 00:26:51,080
we aren't able to get the power draw of TPUs. But I do think TPUs are much more, I mean because

393
00:26:51,080 --> 00:26:54,840
they're better at doing this computation, they just like don't take as much time to do the same

394
00:26:54,840 --> 00:26:59,880
amount of training. So also a technology developed by Google that I think, yeah, it's just like much

395
00:26:59,880 --> 00:27:04,600
more energy efficient than GPUs. So that's nice. Although it's a little sad that it's like,

396
00:27:04,600 --> 00:27:08,920
you know, this, I guess they're all proprietary technologies, but GPUs feel, I mean,

397
00:27:08,920 --> 00:27:12,600
sorry, TPUs feel a little bit more proprietary than GPUs because I can't like buy one.

398
00:27:14,120 --> 00:27:19,720
I'm like pro energy usage. Right, right, right. So you, so you could not

399
00:27:20,600 --> 00:27:26,360
include Excel net in this paper. You just don't have enough information because it's so

400
00:27:26,360 --> 00:27:31,640
all the data they provide it was TPU based. You'd have to do a lot more extrapolation than even

401
00:27:31,640 --> 00:27:38,120
you've done here. Yeah, exactly. So for like the BERT model, for example, this was also originally

402
00:27:38,120 --> 00:27:45,640
trained, you know, at Google on only TPUs. But since then, like Nvidia, like retrained it on GPUs.

403
00:27:45,640 --> 00:27:50,840
So we were able to use like those numbers. But yeah, for, yeah, exactly for Excel net, it's like

404
00:27:50,840 --> 00:27:56,920
unclear. Yeah, exactly what the footprint would be. I guess if I made, if I made like a follow-up

405
00:27:56,920 --> 00:28:01,080
update, make like a blog post or something, I would try to get some, hopefully if it's not

406
00:28:01,080 --> 00:28:06,120
too proprietary, try to get some like energy draw information for TPUs so we can do that estimate.

407
00:28:06,120 --> 00:28:09,080
Right. Because I think it'd be really interesting to actually see like how they compare it

408
00:28:09,080 --> 00:28:20,360
concretely. This work is very specific to NLP. Have you, do you have any specific thoughts on

409
00:28:20,360 --> 00:28:25,160
like how it applies to, I mean, I guess the methodology is pretty general. And what you're

410
00:28:25,160 --> 00:28:30,600
really saying here is that we should be doing this. And that's another exercise for someone

411
00:28:30,600 --> 00:28:38,760
else is to apply to computer vision models and the like. Yeah, yeah. So there is some other people

412
00:28:38,760 --> 00:28:45,320
have done work comparing, let's see, in computer vision in particular, you know, it's like really

413
00:28:45,320 --> 00:28:49,880
a popular area. Let's see, what were they looking at? So I don't know if anyone doing the mapping

414
00:28:49,880 --> 00:28:54,120
to like carbon footprint, but there are people who have compared to like the energy use, like

415
00:28:54,120 --> 00:28:59,000
the energy draw required by the GPU for using sort of like different neural network layers in,

416
00:28:59,720 --> 00:29:04,360
you know, like these big image classification networks and stuff like this are like, you know,

417
00:29:04,360 --> 00:29:08,760
different batch sizes and stuff like this. Yeah. So there's some work they didn't do, like you could

418
00:29:08,760 --> 00:29:13,400
take, I think their numbers and then do like that extra step with mapping the carbon footprint.

419
00:29:13,400 --> 00:29:18,360
I do think like the NLP models, especially these like, you know, notoriously enormous

420
00:29:18,360 --> 00:29:26,200
pre-trained language models are just like some of the most like energy, hungry models in machine

421
00:29:26,200 --> 00:29:30,920
learning. I would say like the other other area that's probably it was just like using insane

422
00:29:30,920 --> 00:29:38,760
amounts of energy would be the work at like open AI on doing like training, you know, training

423
00:29:38,760 --> 00:29:44,360
reinforcement learning models to do like various games. I think that's going to be like probably,

424
00:29:44,360 --> 00:29:48,040
I mean, I think the technique for training this is like kind of similar to neural architecture search

425
00:29:48,040 --> 00:29:53,000
and that you're actually training like a ton of different models. And you kind of early in our

426
00:29:53,000 --> 00:30:03,480
conversation, there's a hint of judgment that, you know, the incremental gains that we're seeing,

427
00:30:03,480 --> 00:30:11,160
you know, going from, you know, one model to the next Elmo, the birth to GPT-2 to excel in that,

428
00:30:11,160 --> 00:30:18,120
like, doesn't necessarily justify the increase in training costs. Like, did you try to quantify

429
00:30:18,120 --> 00:30:23,720
that specifically? Yeah, so I think the question you asked earlier, I'm kind of would have quantified

430
00:30:23,720 --> 00:30:29,160
that like we had just computed like a ratio. Yeah, yeah. Yeah, like the difference in, you know,

431
00:30:29,160 --> 00:30:33,400
compute required versus like the increase in accuracy, we totally showed on that like I think

432
00:30:33,400 --> 00:30:37,640
that would be nice because a lot of these models do evaluate on sort of like some of the same metrics.

433
00:30:37,640 --> 00:30:42,600
Yeah, so we didn't really quantify that except, yeah, that we noted for the neural architecture

434
00:30:42,600 --> 00:30:49,960
search that they get an increase of like 0.1 blue score, which is really not a lot on like English

435
00:30:49,960 --> 00:30:56,520
and German machine translation. Yeah, at the cost of, we say like at least $150,000 in on-demand

436
00:30:56,520 --> 00:31:02,520
compute time. Yeah, I think, I mean, I think yeah, that sort of that approach in particular,

437
00:31:02,520 --> 00:31:08,920
is just a very brute force approach. I do have like a little bit of judgment about that. I mean,

438
00:31:08,920 --> 00:31:15,880
I think like the gains. So I didn't make that up. Yeah, it's true. Elmo versus Bert versus

439
00:31:15,880 --> 00:31:21,000
Excel now. You know, I do think that they are, like, there are actually pretty substantial differences

440
00:31:21,000 --> 00:31:25,320
in accuracy between those models on, you know, these like standard NLP benchmarks. And I think

441
00:31:25,320 --> 00:31:29,880
that that's that is important to do. Something with the neural architecture search, I feel like maybe

442
00:31:29,880 --> 00:31:37,640
there's like like some more like well thought out research that we could be doing to like not just

443
00:31:37,640 --> 00:31:43,640
do a brute force search on architectures to find like really small increase on like a single

444
00:31:43,640 --> 00:31:49,800
language pair. Is there a kind of argument or devil's advocate thing here that says like really

445
00:31:49,800 --> 00:31:58,440
the innovation and all these models is in their use in transfer learning. And if you think about

446
00:31:58,440 --> 00:32:07,960
that, the cost to train these models is amortized across lots and lots of users. And similarly,

447
00:32:07,960 --> 00:32:14,520
the incremental performance gains accrue across lots and lots of users. And so it's worth it.

448
00:32:15,480 --> 00:32:20,680
Yeah, totally. I mean, I am definitely not sick. You know, I'm an NLP researcher. One of them

449
00:32:20,680 --> 00:32:25,480
closely analyzed is like my model. Yeah. And I'm just gonna like stop doing research or something.

450
00:32:25,480 --> 00:32:29,480
Like I'm gonna continue working on these models. Right. Right. And I do think like Elmo and

451
00:32:29,480 --> 00:32:35,000
Bird. And like I guess now ex on that like, yeah, I mean, the aggregate increases on these core

452
00:32:35,000 --> 00:32:40,040
NLP tasks resulting from these models is like awesome. It's like really cool to be like in this field

453
00:32:40,040 --> 00:32:44,280
at this time. Yeah. So I like by no means saying we shouldn't be doing this research and we shouldn't

454
00:32:44,280 --> 00:32:49,160
be using these models. And so I totally agree with you that. Yeah, that's like a great sort of

455
00:32:49,160 --> 00:32:53,800
like counter argument that especially these models in particular, you know, they're sort of like

456
00:32:53,800 --> 00:32:59,240
pre-trained on this large amount of like data. They don't require like explicit supervision. So

457
00:32:59,240 --> 00:33:02,200
you can like kind of easily collect this large amount of data and then just train on it.

458
00:33:03,800 --> 00:33:07,560
And then these models are like really applicable in a lot of different. Yeah, I can sort of

459
00:33:07,560 --> 00:33:12,440
provide them as input in a lot of different tasks. Yeah, I mean, that's totally true. I still think

460
00:33:12,440 --> 00:33:19,480
it's important to characterize the costs. Especially, yeah. So like another one of our conclusions is

461
00:33:19,480 --> 00:33:25,240
sort of like in terms of like access to computation resources. So I do think if you want to use one

462
00:33:25,240 --> 00:33:30,520
of these models in a new domain. So they tend to be trained on like web crawls and like news,

463
00:33:30,520 --> 00:33:36,360
corpora. Yeah, that kind of thing. Yeah, yeah. And so I still think like the so I'm sure if you

464
00:33:36,360 --> 00:33:40,680
initialize some model, let's say that you wanted to run on these like material science journal

465
00:33:40,680 --> 00:33:46,040
articles or like whatever. So I'm like, yeah, research articles in a very specific domain. I think if

466
00:33:46,040 --> 00:33:50,760
you, you know, if you initialize your model with one of these models, like probably it will help,

467
00:33:50,760 --> 00:33:54,120
but what you really want to do is train one of these models on a corpus of that data,

468
00:33:54,840 --> 00:33:59,240
which is actually something that we did in our research. And are you making a distinction between

469
00:33:59,240 --> 00:34:05,400
training and tuning? Yeah, so we fine tune. So that's true. You don't need to, yeah, given the model,

470
00:34:05,400 --> 00:34:09,080
it's already learned a lot of sort of stuff about, let's see if you pair about English. It's learned

471
00:34:09,080 --> 00:34:14,440
a lot of stuff about, you know, English that is sort of that domain independent. I guess it

472
00:34:14,440 --> 00:34:18,840
probably is domain independent. Yeah, so that's a good point. You only need to fine tune it,

473
00:34:18,840 --> 00:34:23,880
but it still takes like a lot to fine tune it basically in, yeah. Yeah.

474
00:34:25,000 --> 00:34:31,000
Oh, yeah, so sorry, what I was saying is, yeah, basically like in terms of like funding and stuff

475
00:34:31,000 --> 00:34:37,240
like this, yeah, just developing and using these models can still be pretty expensive for academic

476
00:34:37,240 --> 00:34:42,280
researchers or even, you know, just, you know, smaller companies and smaller groups who want to

477
00:34:42,280 --> 00:34:47,880
use this technology, but just don't have a ton of money or ton of resources. So it would be nice

478
00:34:47,880 --> 00:34:54,520
to have like public resources or something so that people have more access. Any other takeaways

479
00:34:54,520 --> 00:35:03,080
or conclusions that are worth noting or exploring in this paper? Yeah, so there's one other thing

480
00:35:03,080 --> 00:35:08,360
that we talked about in the paper, which I guess I maybe touched on a little bit earlier,

481
00:35:08,360 --> 00:35:14,600
but like I think that we already have, so like when I analyze like the model that I trained,

482
00:35:15,400 --> 00:35:18,920
a lot of the cost was just like, yeah, doing this like tuning and development and stuff,

483
00:35:18,920 --> 00:35:22,840
basically doing these like just grid search as a hyper parameters, so like just like considering

484
00:35:22,840 --> 00:35:28,200
like every pair of, or every like combination of like K hyper parameters, which is like a really

485
00:35:28,200 --> 00:35:33,400
dumb approach. That's really like wasteful and we have techniques, you know, like Bayesian

486
00:35:33,400 --> 00:35:38,280
hyper parameter search and other stuff that are that are better, but at least in my experience,

487
00:35:38,280 --> 00:35:42,040
like most people are not using them. So I know Google has like an internal tool that does this,

488
00:35:42,040 --> 00:35:45,800
so people do do it at Google, but the other tool is like not a good source, for example.

489
00:35:46,680 --> 00:35:52,120
So I think like another thing that would be great is if these big like deep learning tool cuts

490
00:35:52,120 --> 00:35:57,880
that like everyone is using like TensorFlow and PyTorch, etc, you know, made more of an effort to build

491
00:35:57,880 --> 00:36:04,280
in these more energy efficient approaches that already exist, so it's like easier for people to do

492
00:36:04,280 --> 00:36:10,680
more efficient modeling. That would be great. Nice. Shout out to our friends at Sigopt that does this

493
00:36:10,680 --> 00:36:15,880
for Bayesian optimization, but it's I think another take on it, like they, you know, go in and are

494
00:36:15,880 --> 00:36:21,320
talking to people about like performance and, you know, making better models, and this is a whole

495
00:36:21,320 --> 00:36:29,960
different take on it and looking at the efficiency of doing it. Yeah, absolutely. I think, yeah, that's,

496
00:36:29,960 --> 00:36:36,280
I mean, for me, that's like a big gain of, yeah, like Bayesian searches. Yeah, just like being able

497
00:36:36,280 --> 00:36:41,000
to actually search the space intelligently. Right. Well, I guess yeah, in terms of energy use,

498
00:36:41,000 --> 00:36:45,720
rather than just like the accuracy. I think it's often like easier to show that, at least in my

499
00:36:45,720 --> 00:36:51,080
research, something I've enjoyed is like rather than trying to like make some number higher, I tend

500
00:36:51,080 --> 00:36:54,840
to be just trying to like match some number, but like make the model more efficient. And I think

501
00:36:54,840 --> 00:37:01,640
it's actually a lot easier. Yeah, because like everyone's trying to make the number higher. It's

502
00:37:01,640 --> 00:37:06,440
at a certain point. It can be challenging. Cool. Well, Emma, thanks so much for taking the time to

503
00:37:06,440 --> 00:37:11,960
share what you're working on. It's very cool stuff and an important paper for sure. Yeah, thanks

504
00:37:11,960 --> 00:37:17,000
so much for inviting me. Yeah, I'm really glad because I think like, yeah, it's a really important

505
00:37:17,000 --> 00:37:21,640
thing for people to think about. So I'm grateful that you invited me on the podcast and talk about

506
00:37:21,640 --> 00:37:25,640
this so that we can get some more visibility and more people thinking about this. Yeah, absolutely.

507
00:37:25,640 --> 00:37:29,640
Oh, I guess one more thing that I'd like to say is that there's been kind of like a lot of

508
00:37:29,640 --> 00:37:34,600
discussion about this paper online and we want to like encourage that discussion and provide,

509
00:37:34,600 --> 00:37:38,840
you know, like a place for that discussion to happen. So we were going to upload the paper on

510
00:37:38,840 --> 00:37:42,920
OpenReview, which is like a forum where you have like a paper and then you can have like discussion

511
00:37:42,920 --> 00:37:47,640
by people. So I'm playing on doing that like today or tomorrow. So maybe we can think or something.

512
00:37:47,640 --> 00:37:52,440
But yeah, hopefully if people want to like discuss more, hoping to provide a forum for that to happen.

513
00:37:52,440 --> 00:37:56,200
Okay. Well, yeah, shoot us the like and we'll include it in the show notes page.

514
00:37:56,200 --> 00:37:59,000
Awesome. Thank you so much. Thanks Emma. Thanks.

515
00:38:03,000 --> 00:38:08,040
All right, everyone. That's our show for today. For more information on today's show,

516
00:38:08,040 --> 00:38:15,960
visit twomolai.com slash shows. Make sure you head over to twomolcon.com to learn more about

517
00:38:15,960 --> 00:38:39,880
the twomolcon AI platform's conference. As always, thanks so much for listening and catch you next time.

