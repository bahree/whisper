1
00:00:00,000 --> 00:00:23,000
All right, everyone. I am here with Daniella Rousse. Daniella is director of CSAIL and Deputy Dean of Research at MIT. Daniella, welcome to the Twomo AI podcast.

2
00:00:23,000 --> 00:00:26,000
Thank you so much. It's great to be here.

3
00:00:26,000 --> 00:00:34,000
I'm looking forward to our conversation. We're going to dig into your work and the area of robotics.

4
00:00:34,000 --> 00:00:43,000
But before we do, I'd love to start by having you share a little bit about your background and how you, you know, what spurred your interest in that field.

5
00:00:43,000 --> 00:00:53,000
I went into robotics because I wanted to work on something that brought together the world of mathematics with the world of physical things.

6
00:00:53,000 --> 00:01:00,000
And I was really interested in the science and the engineering of autonomy.

7
00:01:00,000 --> 00:01:07,000
And by this, I mean understanding the mathematical and the biological foundations of systems.

8
00:01:07,000 --> 00:01:12,000
And I was interested in building bodies and building brains for these machines.

9
00:01:12,000 --> 00:01:20,000
Now, this has been a long standing quest or a fantasy, if you like, that goes back to my childhood.

10
00:01:20,000 --> 00:01:33,000
I grew up in Romania at the time of great austerity. And in my spare time, I read a lot of books and reflected and fantasized about superpowers.

11
00:01:33,000 --> 00:01:42,000
I grew up in a family of scientists. And as a school student, I had really broad interest.

12
00:01:42,000 --> 00:01:51,000
But the constraints of growing up in Romania really put me on a STEM track.

13
00:01:51,000 --> 00:02:01,000
And so at that time, it was standard practice for all school students to spend a week each month working in a factory.

14
00:02:01,000 --> 00:02:10,000
The Romanian government believed that this would help us get some skills and prepare us to become part of the proletariat.

15
00:02:10,000 --> 00:02:19,000
And so for some time, one week each month, I worked in a factory that made spare parts for locomotives.

16
00:02:19,000 --> 00:02:25,000
And so at that time, I was a teenager and that work just did not seem that useful.

17
00:02:25,000 --> 00:02:35,000
But as I look back, I can now see how that experience has actually contributed to my growth in robotics.

18
00:02:35,000 --> 00:02:42,000
Because I learned to use all sorts of machines like the late, I even made screws from scratch.

19
00:02:42,000 --> 00:02:47,000
And so I began to understand the physical aspects of things.

20
00:02:47,000 --> 00:02:56,000
And as the math we learned in school became more and more abstract, I realized that I wanted to do something STEM-y, yes.

21
00:02:56,000 --> 00:02:59,000
But also something with a physical component.

22
00:02:59,000 --> 00:03:16,000
And so now here I am, living my dream, fantasizing about machines with superpowers and building robots really that bring together the world of computation with a world of mechanisms and materials.

23
00:03:16,000 --> 00:03:30,000
And this is really a great place to be, it's a great path towards having the background to develop the science and engineering of autonomy.

24
00:03:30,000 --> 00:03:33,000
That's awesome.

25
00:03:33,000 --> 00:03:51,000
I mentioned that you are the director of CSEL, you know, for those not familiar with it, it is a storied kind of research institution at MIT. But can you maybe give us a brief overview of the scope of that research center?

26
00:03:51,000 --> 00:04:08,000
CSEL is such an amazing place and CSEL has an extraordinary history. CSEL stands for Computer Science and Artificial Intelligence Laboratory. And there are two parts to our name, the CS part and the AI part.

27
00:04:08,000 --> 00:04:12,000
And the AI part goes back to 1956.

28
00:04:12,000 --> 00:04:34,000
When Marvin Minsky was a young assistant professor and one summer he decided to gather his friends, they went to the woods of New Hampshire where they spent a month walking, talking, no doubt drinking red wine and thinking deeply about what the next big question in sciences.

29
00:04:34,000 --> 00:04:56,000
And when they returned from that trip, they coined the term artificial intelligence and they announced the importance of developing a new science, the science and engineering of intelligence of making machines with human-like characteristics in how they move, how they perceive the world, how they play games, even how they learn.

30
00:04:56,000 --> 00:05:15,000
So our community at MIT has been on this quest since the very beginnings in 1956. The other part of our name, CS, goes back to 1963 when the big dream was to build a machine that two people could use at the same time.

31
00:05:15,000 --> 00:05:39,000
The machines were as big as the rooms we live in as our offices. And at the time, the project was called Project Mac, where Mac stood for machine-aided cognition, the idea that we're building machines as tools and these tools are meant to help people with cognitive tasks.

32
00:05:39,000 --> 00:05:52,000
Ever since our researchers have developed the foundations of computing and along the way they invented so many things that we now take for granted.

33
00:05:52,000 --> 00:06:16,000
So did you know, for instance, that besides the time sharing machine and the password and the optical mouse, our researchers also developed the first computer to display graphics and the first object-oriented programming language clue, which is the foundation of the entire software industry.

34
00:06:16,000 --> 00:06:28,000
And cryptography, the technology that allows all of us to buy things online. And we take that for granted.

35
00:06:28,000 --> 00:06:49,000
We have had extraordinary contributions to what makes computing the way it is now. And as we look into the future, our researchers really aim to invent the future of computing and figure out how to make the world better through computing.

36
00:06:49,000 --> 00:07:03,000
I'm curious how you define robots. You know, it's a simple question, but I hear lots of different answers.

37
00:07:03,000 --> 00:07:24,000
And I'm curious, you know, I suspect given what you said about your background and the early experiences you had with physicality that physicality might have play a role in that definition, but it doesn't for everyone. And I'm curious what robots are for you.

38
00:07:24,000 --> 00:07:35,000
So the definition I share with my students is is the following a robot is a programmable mechanical device that can exert forces.

39
00:07:35,000 --> 00:07:43,000
And so this captures the physicality of the machine and also the fact that the machine interact with the physical world.

40
00:07:43,000 --> 00:08:03,000
And if I might go to a slightly more detailed definition, I would add that the program and the robot has sensors through which it perceives the world and actuators through which it impacts or affects the world.

41
00:08:03,000 --> 00:08:30,000
And of course, you know, one of the things that we want to talk about is the interplay between, you know, the robots and intelligence AI. And I'm curious what you're, I'm curious like you're kind of broad take on, you know, the field of AI for robotics.

42
00:08:30,000 --> 00:08:52,000
Now, let me say that we are living through extraordinary times for AI for machine learning and for robotics together these technologies and these fields of study combined to open up so many possibilities for the future.

43
00:08:52,000 --> 00:09:21,000
And I actually differentiate between the three because I think of robotics as as the science that puts computation in motion. I see AI as the science that allows us to reason and make decisions and machine learning cuts across both and at the moment machine learning in computer science is about making predictions on data.

44
00:09:21,000 --> 00:09:41,000
And finding patterns in data and giving suggestions to people on what is possible. And together these fields combined to open so many opportunities with these with these advancements, we can better engineer medicines.

45
00:09:41,000 --> 00:10:04,000
We can better diagnose monitor and treat disease. We can ensure that there are no traffic fatalities. We can develop systems that provide customized education to everyone. Essentially, we can imagine a future where machines take on the routine tasks.

46
00:10:04,000 --> 00:10:26,000
And leave people with time and space to focus on what is creative and what is interesting and important to them. We are really moving towards this phase where we can imagine a future with these extraordinary tools that are helping people with cognitive and physical tasks.

47
00:10:26,000 --> 00:10:39,000
And there is so much good news in in this field and and really it's so fantastic to be in the middle of of this development.

48
00:10:39,000 --> 00:11:02,000
Do you remember when Mickey Mouse summons the broomstick in the sorceress apprentice. This was a favorite cartoon of mine. Now just imagine that today you don't need magic to make that happen. You need robotics and AI. And that's extraordinary.

49
00:11:02,000 --> 00:11:26,000
Well, you did a great job answering an admittedly very vague question. I realized after I asked it that I was kind of fishing around for one of the common themes that I hear when I speak to folks that are kind of working at this confluence of robotics and AI is this belief that

50
00:11:26,000 --> 00:11:45,000
we never really will be able to get to intelligence without the kind of embodiment that robotics provides. And I guess I was curious if, you know, if you if you espouse a belief like that or if you think that that that's true or

51
00:11:45,000 --> 00:11:55,000
you know what you think the role of embodiment and robotics is to advancing intelligence generally.

52
00:11:55,000 --> 00:12:19,000
That's a great question. And I would like to emphasize the importance of the body in studying intelligence. In fact, every every machine, whether it's a natural system or an engineered system, every machine that has a body and the brain kind of combines the body and the brain to reach

53
00:12:19,000 --> 00:12:36,000
all the capabilities and all the intelligence that the machine is capable of. And so from a from a scientific point of view, it's very important to study both the body and the brain and the connection between the two.

54
00:12:36,000 --> 00:12:52,000
From an engineering point of view, that is also important working on the body and of the brain because for any kind of task we'd like to create a machine for we need a body that is capable of executing that task.

55
00:12:52,000 --> 00:13:01,000
And then we need a brain that's capable of controlling the body to do the task. So we really need both.

56
00:13:01,000 --> 00:13:22,000
And thinking about this relationship between the body and the brain and in particular, the human body, you know, it's composed of lots of distributed systems that are working, you know, independently into some degree or another.

57
00:13:22,000 --> 00:13:35,000
You know, for example, kind of endocrine system and all these nervous system lots of different elements are coming together to create, as you put it, this, this intelligence.

58
00:13:35,000 --> 00:14:04,000
And wondering, yeah, I believe some of your work is on like distributed robotics and more broadly, kind of the move toward in in machine learning of late has been kind of these giant end to end models that are kind of all encompassing as opposed to, you know, maybe what the, you know, the human body might suggest is lots of independent.

59
00:14:04,000 --> 00:14:12,000
And lots of independent systems that are somehow collectively creating intelligence.

60
00:14:12,000 --> 00:14:18,000
And I think you work on some area aspects of that. Can you comment on that?

61
00:14:18,000 --> 00:14:23,000
Well, Sam, that's a lot of questions packaged in one. So let me.

62
00:14:23,000 --> 00:14:44,000
Let me talk about this in stages. So first of all, you're asking about complex systems. And this is a really important question, because, because we have already in some sense mastered how to do simple tasks with machines.

63
00:14:44,000 --> 00:15:02,000
So the question is how to do more complicated tasks. And whenever we ask a question like this, there is a scientific question, which is, how does the natural world do it? What do we know about all the natural processes that lead to those kinds of capabilities.

64
00:15:02,000 --> 00:15:15,000
And then there is the engineering task, which is about how to build it, how to how to engineer how to create a system that has those desired capabilities.

65
00:15:15,000 --> 00:15:23,000
And so the study of complex systems is super important in answering the engineering question.

66
00:15:23,000 --> 00:15:32,000
And in working on complex systems where we have multiple machines that have to coordinate with one another interact.

67
00:15:32,000 --> 00:15:44,000
We achieve new engineering capabilities, but we also get to potential hypothesis about how the natural world achieves certain things.

68
00:15:44,000 --> 00:16:00,000
And because so many questions about the natural world remain mysterious. We are still studying how ants coordinate. We're still studying how colonies of bees work. We're still studying schools of fish.

69
00:16:00,000 --> 00:16:26,000
And despite many years of studies, we still don't have the final theories if you want. And then if we want to move to studying the human mind, where that remains a huge big mysterious question, the study of human intelligence is one of the most important open problems that is facing us.

70
00:16:26,000 --> 00:16:32,000
And studying intelligence means understanding ourselves. It means understanding life.

71
00:16:32,000 --> 00:16:45,000
And it also means identifying new ideas for for building engineered systems. And this is extraordinarily important right now.

72
00:16:45,000 --> 00:17:09,000
Because I'm moving into your question about machine learning. And what I want to emphasize is that it is important to know that today's greatest advances in machine learning are due to decades old ideas that are enhanced by vast amounts of data and computation.

73
00:17:09,000 --> 00:17:20,000
Without new technical ideas and funding to back them, more and more people are going to be blowing the same field and the results will be increasingly incremental.

74
00:17:20,000 --> 00:17:31,000
We really need major breakthroughs and insights if we are going to manage the major technical challenges that are facing this field.

75
00:17:31,000 --> 00:17:44,000
And we also need computation infrastructure that ensures progress. In other words, we need an infrastructure that will deliver us data and computation like we get water and energy today.

76
00:17:44,000 --> 00:18:02,000
And in studying intelligence in looking towards the natural world to identify new ideas for how to how to imagine machine learning systems. We can we can do better.

77
00:18:02,000 --> 00:18:22,000
So in my work, we have been looking at small organisms in the natural world, they're called nematodes. We have been looking at how biologists mapped their neural system.

78
00:18:22,000 --> 00:18:39,000
And we took that as inspiration to create a new model for machine learning. We call neural circuit policies. And so here is how neural circuit policies works in a regular deep neural network.

79
00:18:39,000 --> 00:18:52,000
We have lots and lots of neurons. And each neuron does something very simple. Each neuron performs what is called the thresholding action. It's a step function.

80
00:18:52,000 --> 00:19:10,000
And so interestingly, small organisms have neurons that do way more than that. If you look at how their computation is can be described, the chemistry can be described.

81
00:19:10,000 --> 00:19:35,000
And then we conclude that in fact, the neurons compute differential equations, which are much more complex than these simple step functions. So at the basis of our neural circuit policies, we have a new kind of neuron that is able to to compute a differential equation whose time.

82
00:19:35,000 --> 00:19:48,000
This time constant is actually varying. So we call them liquid time, because we are allowing time to vary. So liquid time differential equations.

83
00:19:48,000 --> 00:20:05,000
And we have an architecture that is inspired by by the nematodes and architecture where we have specialized nodes, some nodes are motor nodes, some nodes are sensory nodes, some nodes are inter neurons.

84
00:20:05,000 --> 00:20:32,000
So in the architecture, we can significantly reduce the size of the networks. In fact, in our self driving project, we have shown that it takes about 100,000 neurons and a half a million parameters to do end to end learning how to drive from human from humans.

85
00:20:32,000 --> 00:20:45,000
That means you have a camera, you look at how a human steers and that's the size of the network that is needed in order to in order to learn to drive in order to follow the road.

86
00:20:45,000 --> 00:21:04,000
Well, in our neural circuit policies network, we only need 19 nodes. And this is an extraordinary dimensionality reduction, because now with 19 neurons, we can get an understanding and the visualization of what the network is doing.

87
00:21:04,000 --> 00:21:24,000
Whereas in the in the huge network model, we have no idea how the network reaches its its conclusions. And this is this is really important because because today's state of the art deep learning models are black boxes.

88
00:21:24,000 --> 00:21:48,000
There is no way for users of the system to truly learn anything based on the systems working. And it is difficult to detect behavior that is abnormal from a safety point of view. And as a result, it is difficult to anticipate failure modes that are tied to rare inputs that could potentially lead to catastrophic consequences.

89
00:21:48,000 --> 00:22:03,000
If you have more compact models, then you can understand what is happening inside them. If you have more compact models, you have a lot of other benefits. The models are more efficient, they are faster.

90
00:22:03,000 --> 00:22:20,000
And importantly, they have a much much better environmental footprint. So we're talking a lot about climate change. We've had a really strange summer this year.

91
00:22:20,000 --> 00:22:35,000
And this is an area where it is important to consider the role of AI and machine learning systems, because these systems consume enormous amounts of energy.

92
00:22:35,000 --> 00:22:54,000
In fact, the researchers at the University of Massachusetts at Amherst estimated that training a large deep learning model produces 626,000 pounds of carbon dioxide. This is equal to the lifetime emission of five cars.

93
00:22:54,000 --> 00:23:06,000
So just think about how many people are training deep learning models today and think about what each of those training sessions contributes.

94
00:23:06,000 --> 00:23:19,000
And also, if you think about really big models, like the GPT-3, it costs $4.6 million in energy costs to train GPT-3.

95
00:23:19,000 --> 00:23:35,000
The more pervasive these methods become, the more of these models will be needed. And this is something that we need to take seriously, because we can develop different models.

96
00:23:35,000 --> 00:23:49,000
We can develop smaller models and simpler models that are more understandable, that are more efficient, and that can drastically reduce the carbon footprint of machine learning.

97
00:23:49,000 --> 00:24:09,000
This isn't really a hypothetical. This is an area where at TSAIL, we have researchers that are making progress towards building pruning and compression of machine learning. We are looking at neurosurkit policies, another method to shrink the size.

98
00:24:09,000 --> 00:24:22,000
And I think this is a very important line of work, because it will allow us to understand more about what is really going on in the models.

99
00:24:22,000 --> 00:24:28,000
And it will allow us to do that with an eye towards sustainability.

100
00:24:28,000 --> 00:24:39,000
You described the neuron that's at the heart of the neural circuit policies work as focused on differential equation over this liquid time concept.

101
00:24:39,000 --> 00:24:47,000
Is it related to the work that some folks are doing around spiking neural nets?

102
00:24:47,000 --> 00:25:01,000
Well, I mean, it's related at some level, and it's quite different at other levels. It's really an attempt to think about machine learning models in a different way.

103
00:25:01,000 --> 00:25:17,000
The ways in which these models are different is threefold. First, the basic neuron model is different. Second, the neurons are not identical. They have different roles.

104
00:25:17,000 --> 00:25:46,000
And third, we use inspiration from the natural world to create the actual architecture of the system. And so if you consider all these three dimensions, I will say that the work is starting a new kind of a new line of investigation, a new kind of model that we think can have really great potential.

105
00:25:46,000 --> 00:25:56,000
At the moment, we are mostly demonstrating the utility of this model in the context of time series type application.

106
00:25:56,000 --> 00:26:17,000
So your data is video or it's a series of data points like maybe in finance or weather. We have not yet shown the impact of this idea on discrete systems like like single images or text.

107
00:26:17,000 --> 00:26:22,000
But this is coming. This is part of our ongoing work.

108
00:26:22,000 --> 00:26:36,000
And did you mention that you have built self-driving systems out of out of these neurons out of these architectures?

109
00:26:36,000 --> 00:26:56,000
Yes, we have a very exciting team that's working on autonomy for cars. We are pursuing a number of areas in autonomy. One is end to end learning for driving.

110
00:26:56,000 --> 00:27:18,000
And so that means given some small amount of video data, how can you learn from a human how to steer in a variety of circumstances, whether you're alone on a road or whether there is your there are other cars that are moving in the same direction as you or whether you have incoming traffic.

111
00:27:18,000 --> 00:27:29,000
And so we're looking at all of these different situations and we're showing that it is possible to do and to end learning in a state of the art models with deep learning.

112
00:27:29,000 --> 00:27:41,000
But with neural circuit policies, we have simpler and more compact models and and our compact models also have interesting additional properties.

113
00:27:41,000 --> 00:28:00,000
They are they seem to capture causality. So what that means is that the focus of attention of the model is in the in the subsection of the image space that that is important for the task.

114
00:28:00,000 --> 00:28:23,000
For instance, if you look at driving, we want to really focus clean me on the horizon and on the side of the road. Now, if you compare the focus of attention of a neural circuit policy solution versus some other state of the art solutions, what we see is that in fact in in a more traditional approaches.

115
00:28:23,000 --> 00:28:39,000
The focus of attention is very noisy. It's all over the image and in fact it's it's quite well established by now that many machine learning solutions use context in order to make a decision.

116
00:28:39,000 --> 00:28:50,000
And with neural circuit policies, you can look at the at the focus at the action, not at the context to to get your decision.

117
00:28:50,000 --> 00:29:07,000
And so in your work thus far, what what are the kind of primary tasks that you've benchmarked the neural circuit policies approach against and what kind of results have you seen.

118
00:29:07,000 --> 00:29:25,000
Yeah, so so we've used neural circuit policies for a variety of navigation tasks on the ground and in the air. And we have shown that these NTPs are really compact.

119
00:29:25,000 --> 00:29:43,000
They have causal correlations. They are more efficient than than the existing networks. And moreover, they can be represented with just a few hundred kilo bits.

120
00:29:43,000 --> 00:30:01,000
They can run these models on a Raspberry Pi. And that is really extraordinary, especially for people who want to use machine learning for embedded applications that in addition to to this NTP approach to making models smaller.

121
00:30:01,000 --> 00:30:16,000
We are also pursuing a number of pruning and compression approaches that allow us to take an already trained model and reduce it, reduce it significantly.

122
00:30:16,000 --> 00:30:31,000
We show in our work that we can eliminate up to 90% of the parameters and still get approximately the same performance from the network. So that's again really intriguing and exciting.

123
00:30:31,000 --> 00:30:41,000
If you can do that, why not just why not just use these approaches as a backend to any kind of machine learning model.

124
00:30:41,000 --> 00:31:04,000
So you can train your model. And then you have a kind of a compilation process where you feed the result into into this engine that optimizes everything keeps what is important removes what is not not important and you end up with a much more efficient system with a lower environmental footprint.

125
00:31:04,000 --> 00:31:23,000
We will ever get to a kind of cross architecture compilation, where as opposed to pruning and that type of thing, which is relatively incremental, you might compile into neural circuit policies from a traditional type of model.

126
00:31:23,000 --> 00:31:37,000
That's the big dream. We don't have this compiler yet, but we would sure want to do that. In fact, I would sure want to have an engine where you give me where you give me your model.

127
00:31:37,000 --> 00:31:59,000
And I can either shrink it or I can reshuffle it so that it becomes an NCP and this kind of, but this kind of automated generation of small models is an interesting, but remains a big challenge.

128
00:31:59,000 --> 00:32:16,000
We talked about robotics being kind of this integration of brain and body NCP is some of your work on kind of the brain side of things you're also active and working on the body side of things in topics like soft robotics.

129
00:32:16,000 --> 00:32:20,000
Can you talk a little bit about your work in that area.

130
00:32:20,000 --> 00:32:36,000
This is a very exciting part of my field. And I want to share with you that the first industrial robot was a robotic arm that was invented in 1961.

131
00:32:36,000 --> 00:32:51,000
So some number of years back, some decades back, but not a long time back. And so since then, we have really grown the field of industrial automation.

132
00:32:51,000 --> 00:33:09,000
We've built these extraordinary machines that perform so much better than humans, but these machines remain isolated from humans on the factory floor because they're large and and heavy and dangerous to be around.

133
00:33:09,000 --> 00:33:31,000
And so natural question is what would it take to to bring humans and machines closer together to make it safe for people to be around machines. And to me, the one answer to this question is making better bodies, making bodies that are more like our bodies that are safer to be around.

134
00:33:31,000 --> 00:33:47,000
And so that's how I so that's how I became interested in the field of soft robotics, where soft really refers to the body of the robot and softness is measured by

135
00:33:47,000 --> 00:34:02,000
a metric called Young's modulus. And if you compare Young's modulus of the materials, we use in traditional robotics. So these are metals and hard plastics.

136
00:34:02,000 --> 00:34:17,000
So if you compare that against the tissue and the Young's modulus of elements from the natural world, there is a huge difference orders of magnitude different.

137
00:34:17,000 --> 00:34:29,000
And so the question is, can we make machines that have the same Young's modulus as my body and your body, because then those machines would be safer to be around.

138
00:34:29,000 --> 00:34:47,000
So the first thing observation is that the field of industrial robotics, or maybe if we look at the last 60 years of robotics, we see that most of the robots we invented are either inspired by the human form.

139
00:34:47,000 --> 00:34:58,000
Or the arms or the human, humanoids or the boxes on wheels. And, and that kind of limits what these systems can do.

140
00:34:58,000 --> 00:35:06,000
These systems are also made of metals and hard plastics. So can we reimagine what the robot is.

141
00:35:06,000 --> 00:35:20,000
So that the robot can be the robot can be inspired by by the natural world with its form diversity or by the built environment with its form, form diversity.

142
00:35:20,000 --> 00:35:44,000
And the robot is made out of any materials that are available to us. We can make robots out of silicone, out of paper. We can even make robots out of food and ice. So how can we reimagine what a robot is to consider a wide spectrum of shapes and a wide spectrum of materials.

143
00:35:44,000 --> 00:35:54,000
And create a very carefully optimized machines that can truly help people with physical tasks.

144
00:35:54,000 --> 00:36:01,000
What are some examples of the software bots projects that you forked on?

145
00:36:01,000 --> 00:36:18,000
So one of our exciting projects is called Sophie, where Sophie stands for soft fish. So early on, maybe about five years ago or so.

146
00:36:18,000 --> 00:36:36,000
We built a fish robot whose tail was soft and moved by on jolation. And that was accomplished by designing the tail in a certain way and then by pumping water in and out of the tail.

147
00:36:36,000 --> 00:36:49,000
So we took this robot to coral reefs and we found that the robot moved quietly and naturally. So it could really kind of penetrate schools of fish without disturbing them.

148
00:36:49,000 --> 00:36:58,000
It did not appear to be a foreign element in that environment, the way a thruster based robot would.

149
00:36:58,000 --> 00:37:23,000
And so we got really excited about this idea of a soft robotic fish being kind of an underwater observatory for people to really give us to really extend our reach into a world that is typically hard for us to reach.

150
00:37:23,000 --> 00:37:35,000
We've also then developed a number of robotic arms and robotic hands. I've been very inspired by the elephant trunk in this work.

151
00:37:35,000 --> 00:37:39,000
So the elephant trunk is a really extraordinary thing.

152
00:37:39,000 --> 00:37:58,000
The elephant trunk can be a super delicate manipulator. The elephant can pick a potato chip from the from the table without breaking it or the elephant can get a banana from your hands without hurting you.

153
00:37:58,000 --> 00:38:16,000
And that trunk can also be used to fend off an intruder. So it's so interesting to think about a system that can be both delicate and strong.

154
00:38:16,000 --> 00:38:38,000
We have been looking at this issue, namely how do we build soft robots that are both delicate and strong. How do we then create a kind of a sensory skin for these robots so that the robots can see the world through touch.

155
00:38:38,000 --> 00:38:49,000
How can we how can we demonstrate the use of these robots in a variety of tasks that people care about we have shown that we saw fingers.

156
00:38:49,000 --> 00:39:03,000
We can get very compliant hands that are capable of handling your groceries. They can pick up your milk, but then they can also pick up your grapes and broccoli.

157
00:39:03,000 --> 00:39:12,000
And picking up grapes and the broccoli is very hard because we don't really have accurate models.

158
00:39:12,000 --> 00:39:22,000
An industrial or a traditional robot or an industrial robot would really need to know exactly what the body looks like and where it's located.

159
00:39:22,000 --> 00:39:35,000
We can calculate very precisely where to place the fingers so that it gets a solid grasp. If you want to imagine that, just try to use your fingernails to pick up something.

160
00:39:35,000 --> 00:39:48,000
That's how that's how industrial robots work. Our hands are very compliant. So our hands are such that they can wrap around the object we try to grab.

161
00:39:48,000 --> 00:39:58,000
Actually, I don't really need to know exactly what this what the shape of my phone is nor precisely where it is located.

162
00:39:58,000 --> 00:40:16,000
So with soft materials and soft robots, we can begin to mimic this kind of compliance and robustness that we have in the natural world that we have with our hands, but that we have not achieved in industrial manipulators.

163
00:40:16,000 --> 00:40:24,000
So that's another example. Oh, let me tell you one of my favorite examples. So we made.

164
00:40:24,000 --> 00:40:32,000
We made an origami. We made a mini surgeon. That's what we call it. It's a mini surgeon.

165
00:40:32,000 --> 00:40:42,000
It's a mini surgeon. It's essentially a little soft robot that we can package inside inside ice.

166
00:40:42,000 --> 00:40:49,000
And the size of the ice is the same as as a regular pill as regular medicine.

167
00:40:49,000 --> 00:41:00,000
And so you division is like this. You swallow this this robot. By the way, this robot is made out of sausage casing. So it's safe to swallow.

168
00:41:00,000 --> 00:41:09,000
So you swallow this robot when the robot gets into your stomach, the ice melts that allows the robot to deploy.

169
00:41:09,000 --> 00:41:15,000
And now you have the ability to do incision free procedures.

170
00:41:15,000 --> 00:41:26,000
You can remove foreign objects like button batteries that are swallowed too often by people every day.

171
00:41:26,000 --> 00:41:44,000
And batteries are very dangerous because within about an hour, they will puncture the tissue. And so that means that they become lodged inside the tissue and they require, they require surgery in order to be removed.

172
00:41:44,000 --> 00:42:13,000
But you can also collect tissue samples and you can patch a wound. You can deliver medicine at a precise location inside the body. And so I'm very excited about the possibility of using new shapes for robots, new materials like sausage casing to open up a future for surgeries without incisions, without pain, without the risk of infections.

173
00:42:13,000 --> 00:42:19,000
It's hard to imagine sausage casings doing all that.

174
00:42:19,000 --> 00:42:28,000
We can watch some movies if you like. So far, these robots have been shown to be effective in lab experiments.

175
00:42:28,000 --> 00:42:43,000
And the next phase is to go to in vivo experiments before we can proceed with the work. But our role is to show that the vision can be realized.

176
00:42:43,000 --> 00:43:01,000
And what's the, can you talk a little bit about the technology behind it in the sense of is it, it's not sausage casings encapsulating something that's, you know, more traditional structure, imagining is it.

177
00:43:01,000 --> 00:43:16,000
No, no, so yeah, yeah, I like to go property of the sausage casing that makes it, you know, an alley or.

178
00:43:16,000 --> 00:43:32,000
It's kind of compressed like an accordion the robot also has a tiny magnet embedded in it. And the operation is imagined to take place inside a controllable magnetic field.

179
00:43:32,000 --> 00:43:45,000
So what that means is that when you swallow the robot, you can locate it because of the magnet. And then you can manipulate the magnetic field to drive the robot as you as you wish.

180
00:43:45,000 --> 00:43:53,000
And so in some sense, the control of the robot is external through this magnetic field.

181
00:43:53,000 --> 00:44:08,000
And, but this is not an alien idea for medicine because we have MRI machines that are in use all the time at most hospitals around the world.

182
00:44:08,000 --> 00:44:23,000
Awesome, awesome. Yeah, one more thing I'd love to have you share a bit about and that is some of your work on adaptive control and the autonomous vehicle domain.

183
00:44:23,000 --> 00:44:38,000
We spoke a little bit about this before and your it kind of grows out of your interest in human and machine collaboration. Can you can you give us an overview of that work.

184
00:44:38,000 --> 00:44:52,000
Let me let me say the following first. So I believe that advancing the science and engineering of autonomy requires that we spend some effort thinking about the body of the robot.

185
00:44:52,000 --> 00:45:03,000
We spend efforts thinking about the brain of the robot, but also we spend efforts thinking about the interaction between robots with each other and with people.

186
00:45:03,000 --> 00:45:15,000
And one interesting case study for this interaction. And in fact, for the connection between interaction body and brain is in the context of autonomous driving.

187
00:45:15,000 --> 00:45:42,000
So by now, I am sure that your audience knows about many efforts to create the to create robot taxi. Well, my own belief is that we're really far from robot taxi. The technology is ready for slow moving safe speed vehicles, low speed vehicles in simpler environments.

188
00:45:42,000 --> 00:45:53,000
We can deploy the technology into products. In these cases, but we still have big challenges. Some some of the challenges include.

189
00:45:53,000 --> 00:46:12,000
Dealing with fast speeds. Some of the challenges include dealing with a great deal of interaction and dynamics in the environment. And some of the challenges include dealing with weather, sensors don't work well in rain and in snow.

190
00:46:12,000 --> 00:46:34,000
And some of the challenges have to do with how you blend a robot car with human driven cars. There have been some deployments in cities where where the robots got really confused by the language that people naturally understand as they drive.

191
00:46:34,000 --> 00:46:46,000
So if you're in an intersection, you can have a kind of a silent communication with an incoming car and you know if you can go or not. But robots don't understand that language.

192
00:46:46,000 --> 00:47:09,000
And so one exciting project has been developing an adaptive algorithm for robot cars that is able to learn the personality, the personalities of the drivers around the ego robot car and adapt the controller to that.

193
00:47:09,000 --> 00:47:19,000
So the car can the robot car can observe you a little bit and figure out whether you are an egoistic driver, a pro social driver or an altruistic driver.

194
00:47:19,000 --> 00:47:30,000
If you are an egoistic driver and the robot wants to make a left hand turn, the robot better let you go because otherwise you'd have a dangerous situation.

195
00:47:30,000 --> 00:47:45,000
But if you are a pro social or an altruistic driver and you want the robot to go, then then the robot has to understand that and hurry up and make that turn otherwise the robot would confuse traffic.

196
00:47:45,000 --> 00:48:01,000
And just think about if you think about when you are learning to drive and you are at team intersections trying to figure out when is the right time to make an unprotected left turn. That is hard even for humans.

197
00:48:01,000 --> 00:48:18,000
So we have some results in this space. And what we are doing is we are leveraging something called social value orientation. This is a metric developed by the social psychology community.

198
00:48:18,000 --> 00:48:38,000
And it's a really cool metric because it maps human personalities onto a circle. In other words, it associates your person, your observed personality with an angle that measures the reward to self versus rewards to others.

199
00:48:38,000 --> 00:49:00,000
So now when we have mathematics that we can associate with personalities, we can embed those mathematics in our cost functions in our learning functions and we can drive controllers to capture the right response in an adaptive way.

200
00:49:00,000 --> 00:49:18,000
And I'm curious about how you collect ground truth for training models around the social value orientation. Do you have observers kind of looking at video and grading other drivers on the screen or is there something else happening.

201
00:49:18,000 --> 00:49:32,000
Well, we are, we have done a lot of simulation and we have inserted so we have used publicly available data sets like NG sim and we have inserted our own control vehicles in those in the data.

202
00:49:32,000 --> 00:49:35,000
And we have observed what happens.

203
00:49:35,000 --> 00:49:45,000
Well, Daniela, thanks so much for taking the time to share a bit about what you're up to. Super interesting stuff and wonderful to chat with you.

204
00:49:45,000 --> 00:49:51,000
Thank you so much for having me Sam and for your interest in my work.

