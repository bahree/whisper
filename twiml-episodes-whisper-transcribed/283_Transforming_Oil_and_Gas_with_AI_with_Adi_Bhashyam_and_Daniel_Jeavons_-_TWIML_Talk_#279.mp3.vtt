WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.940
I'm your host Sam Charrington.

00:31.940 --> 00:36.660
You are invited to join us for the very first Twimblecon conference which will focus on

00:36.660 --> 00:41.220
the tools, technologies and practices necessary to scale the delivery of machine learning

00:41.220 --> 00:43.740
and AI in the enterprise.

00:43.740 --> 00:48.600
The event will be held October 1st and 2nd in San Francisco and early bird registration

00:48.600 --> 01:01.720
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

01:01.720 --> 01:07.120
Before we continue, a big thanks to the folks at C3 for their sponsorship of today's show

01:07.120 --> 01:11.520
which was recorded at their C3 Transform Conference earlier this year.

01:11.520 --> 01:17.720
C3's AI suite helps enterprises rapidly develop, deploy and operate AI at scale and offers

01:17.720 --> 01:23.720
adaptable AI powered applications for predictive maintenance, fraud detection, customer engagement

01:23.720 --> 01:25.800
and many more use cases.

01:25.800 --> 01:30.840
Take a look at what they're up to at c3.ai and if you enjoyed this conversation be sure

01:30.840 --> 01:35.920
to let them know on Twitter at c3 underscore ai and thank them for their support of the

01:35.920 --> 01:36.920
show.

01:36.920 --> 01:39.200
Alright let's do it.

01:39.200 --> 01:47.440
Alright everyone I am here at the C3 Transform Conference in San Francisco and I've

01:47.440 --> 01:53.440
got the amazing pleasure to be seated with Dan Gevins, a veteran of the podcast and

01:53.440 --> 02:00.520
Adi Bashaam, Dan is the general manager of data science at Shell and Adi is the vice

02:00.520 --> 02:05.600
president of Ford Deployed Solutions, alliances and strategy at C3.

02:05.600 --> 02:13.520
So Dan was on the show twimblecon number 202 back in November which was rolled out as

02:13.520 --> 02:16.400
part of our AI platform series.

02:16.400 --> 02:21.840
So Dan I'm going to refer folks back to that show to get your full background but before

02:21.840 --> 02:28.640
we dive into the heart of the topic Adi tell us a little bit about your background.

02:28.640 --> 02:29.640
Sure happy to.

02:29.640 --> 02:36.280
So I have had a few roles at C3 but I've been at C3 now for about four years.

02:36.280 --> 02:41.640
I currently lead forward deployed solutions that think of that as sales consulting, sales

02:41.640 --> 02:42.640
engineering.

02:42.640 --> 02:45.400
I also lead other alliances and strategy arc.

02:45.400 --> 02:51.480
I joined C3 in the product organization and had to get my hands dirty both verifying

02:51.480 --> 02:55.640
machine learning algorithms deployed correctly and also writing product spec, actually build

02:55.640 --> 03:01.080
applications that use those algorithms to actually solve business problems.

03:01.080 --> 03:04.520
Prior to C3 I was at McKinsey and company.

03:04.520 --> 03:11.320
I was there for a total of about 12 years which in consulting years feels like 50.

03:11.320 --> 03:16.880
And in that time and most recently before I left I was largely leading our predictive

03:16.880 --> 03:22.360
analytics and big data solutions focused on B2C subscription businesses, things like

03:22.360 --> 03:25.880
telecom and other SaaS businesses.

03:25.880 --> 03:31.520
I have an MBA from the Kellogg school at Northwestern University just not the Chicago and I'm

03:31.520 --> 03:35.880
a computer science and I have a computer science degree from IIT Bombay.

03:35.880 --> 03:40.120
So I have my graduate work was an electrical engineering at Northwestern.

03:40.120 --> 03:45.680
I lived in the basement of tech for I live mostly in Jacob's center, but yes.

03:45.680 --> 03:46.680
Nice.

03:46.680 --> 03:50.800
Dan I just had an opportunity to catch part of your presentation here or actually all

03:50.800 --> 03:57.360
of your presentation here at the conference and one thing that really struck me was at

03:57.360 --> 04:02.560
least from my perspective based on our last conversation how much has matured around

04:02.560 --> 04:07.120
AI platforms at Shell in just four months.

04:07.120 --> 04:10.800
So you had a slide that says Shell.ai and like a platform or three different we didn't

04:10.800 --> 04:13.440
talk about any of that stuff.

04:13.440 --> 04:16.560
Talk about the kind of evolution over.

04:16.560 --> 04:20.960
So it's kind of funny because obviously you get to talk about more as it matures right

04:20.960 --> 04:22.960
so that's that's part of it.

04:22.960 --> 04:25.480
Obviously you didn't come up with it and do it in form.

04:25.480 --> 04:26.480
Exactly.

04:26.480 --> 04:27.480
Exactly.

04:27.480 --> 04:30.760
No I think we were looking for a banner to bring this all together.

04:30.760 --> 04:36.080
So I talked a lot about in the presentation the real importance that this is not just

04:36.080 --> 04:42.000
a platform play, it's also a play around how do you create a narrative and a culture

04:42.000 --> 04:46.720
and a way of working that starts to drive transformation right across the business.

04:46.720 --> 04:47.720
And that's the vision really.

04:47.720 --> 04:49.920
So we're doing that under this banner of Shell.ai.

04:49.920 --> 04:55.400
If you want to check it out we have a website now, Shell.ai is real, you can just google

04:55.400 --> 04:58.120
it and you'll see a lot of the things we're doing.

04:58.120 --> 05:01.200
We've got some little videos out there talking about some of the work that we're doing

05:01.200 --> 05:05.000
and we're going to put more and more out there over the coming months.

05:05.000 --> 05:10.560
And it's becoming a bit of a narrative within Shell and also something we're using quite

05:10.560 --> 05:13.760
prolifically now with the external world as well.

05:13.760 --> 05:16.800
And just out of curiosity why externalize that?

05:16.800 --> 05:21.200
I'm assuming you're not going to have external users of Shell.ai, I'm not going to go create

05:21.200 --> 05:24.440
an account and put in my credit card and run some TensorFlow.

05:24.440 --> 05:25.440
Yeah, that's right.

05:25.440 --> 05:26.440
Exactly right.

05:26.440 --> 05:28.080
It's not going to be that sort of thing.

05:28.080 --> 05:30.680
I think there will be, so there's two reasons.

05:30.680 --> 05:35.640
And increasingly we recognize within Shell that this whole journey is about partnership.

05:35.640 --> 05:39.720
It's about partnership with companies like C3.

05:39.720 --> 05:44.840
It's about partnership with the innovative tech startups right here in San Francisco

05:44.840 --> 05:47.360
of which I'll be meeting a few this week.

05:47.360 --> 05:53.000
It's about also trying to attract the top talent that want to work on really tough problems

05:53.000 --> 05:55.280
in the energy industry.

05:55.280 --> 05:59.200
And the final thing is it's also about partnership with some of the people we want to work with

05:59.200 --> 06:01.840
and because a lot of our assets are ventures there.

06:01.840 --> 06:03.840
It's not just Shell, it's other companies involved.

06:03.840 --> 06:06.480
So we need to be able to say to them, this is what we're doing.

06:06.480 --> 06:07.480
Now let's work together.

06:07.480 --> 06:12.640
So our spirit is not one of creating a product that we want to sell, but it's much more

06:12.640 --> 06:16.200
something which we say let's work together on this because that's the whole spirit of

06:16.200 --> 06:17.200
digital.

06:17.200 --> 06:22.880
So let's maybe dig in a little bit into this platform that you've created.

06:22.880 --> 06:26.680
You described kind of three specific elements.

06:26.680 --> 06:29.520
Do you walk us through the elements and the roles that they play?

06:29.520 --> 06:30.520
Yeah, absolutely.

06:30.520 --> 06:37.720
So if you think at the raw end, we've got a whole raft of advanced citizen data scientists.

06:37.720 --> 06:38.720
It's a horrible term.

06:38.720 --> 06:40.960
I don't like it, but it kind of characterizes what they are.

06:40.960 --> 06:46.120
These are people who are engineers there, subsurface specialists in Shell.

06:46.120 --> 06:50.520
They've typically got some sort of technical background and they're quite comfortable with

06:50.520 --> 06:51.520
algorithms.

06:51.520 --> 06:55.760
So the question is how can I make it really easy for those folks to use some standard

06:55.760 --> 07:00.680
tools that are available on the market to come in and rapidly prototype something.

07:00.680 --> 07:04.760
So we work with companies like Ultrix and Databricks to make it really easy for those people

07:04.760 --> 07:09.840
to come in and also other companies like MathWorks, for example, and the MATLAB product, which

07:09.840 --> 07:12.160
actually has come on quite a long way in the last few years.

07:12.160 --> 07:17.680
So we create that environment largely focused on R and Python to allow them to prototype

07:17.680 --> 07:19.400
these things quickly.

07:19.400 --> 07:22.800
We've then got kind of a group of people who also need a platform where they want to

07:22.800 --> 07:24.280
train deep learning.

07:24.280 --> 07:27.880
So we're applying deep learning in spaces like Autonomous Well Drilling.

07:27.880 --> 07:33.040
We talked about that the last time, deep learning on seismic fault detection.

07:33.040 --> 07:37.520
We're also looking at deep learning applications in a number of other areas, so things like machine

07:37.520 --> 07:40.080
vision or natural language processing.

07:40.080 --> 07:43.560
And those guys tend to want to work in the latest open source technologies.

07:43.560 --> 07:47.080
So they might be comfortable using a Databricks and a Spark, but they also need things like

07:47.080 --> 07:49.640
Cubeflow and Kubernetes.

07:49.640 --> 07:53.080
They want to be able to manage their data versionings.

07:53.080 --> 07:55.560
So we work with companies like Packidum for that.

07:55.560 --> 07:59.720
So we're looking at those sort of technologies and bringing those together into an integrated

07:59.720 --> 08:06.560
hole to allow our real specialist data scientists to train things at scale and to make sure that

08:06.560 --> 08:11.160
they're able to deal with the big data volumes in an predominantly cloud-based environment.

08:11.160 --> 08:13.520
But then you've got to take that to the masses somehow, right?

08:13.520 --> 08:17.880
So you've got to be able to deploy that easily and effectively.

08:17.880 --> 08:20.720
And so we're working with the cloud vendors, but we're also working with companies like

08:20.720 --> 08:26.400
C3 to help really take those to enterprise grade software solutions that you can then

08:26.400 --> 08:30.600
deploy out to your business users and that they can get the benefits from the applications

08:30.600 --> 08:33.960
that we're building in the context of their day-to-day work.

08:33.960 --> 08:39.240
Clearly, as an organization, you're not afraid of rolling up your sleeves and integrating

08:39.240 --> 08:41.280
pieces together.

08:41.280 --> 08:46.680
A lot of the other companies in fact that we featured in our platform series, the likes

08:46.680 --> 08:49.760
of Facebook and Airbnb, they're building platforms.

08:49.760 --> 08:54.520
They kind of rolled up their sleeves, glued a bunch of stuff together or welded, in

08:54.520 --> 08:58.080
the case maybe, you're not afraid to do that.

08:58.080 --> 09:06.200
Why rely on a partner like C3 to provide more of an off-the-shelf kind of solution as

09:06.200 --> 09:12.400
opposed to just going that extra step and doing the welding or gluing?

09:12.400 --> 09:13.400
It's a great question.

09:13.400 --> 09:18.200
I think we thought about it, so just being completely honest and Adi and I had some

09:18.200 --> 09:23.080
tough conversations around this, but I think the real thing for me was, if you look at

09:23.080 --> 09:27.480
my data science heritage as an organization, we trace our origins, my team right back

09:27.480 --> 09:32.320
to the 1970s, where Shell was the industry leader in things like scenario planning and

09:32.320 --> 09:37.720
we found it from the first statistics groups in the industry, the issue is that we don't

09:37.720 --> 09:40.440
have that same heritage in software engineering.

09:40.440 --> 09:44.280
We were really looking for a partner that's going to help us move forwards with not just

09:44.280 --> 09:48.680
the stitching together of being able to make it work, but actually being able to make

09:48.680 --> 09:53.480
it work at scale in a production-ready product that's going to allow us to deploy this right

09:53.480 --> 09:55.960
across our business very, very fast.

09:55.960 --> 10:00.240
That's something that we weren't, we didn't feel we were fully equipped to do and that's

10:00.240 --> 10:04.160
also where C3 have been helping us in that whole journey.

10:04.160 --> 10:09.200
And I think the other thing that's worth saying is we liked the reusability of that type

10:09.200 --> 10:10.200
system.

10:10.200 --> 10:17.080
We know that a lot of the setups that you have in the sort of, I guess the cloud space,

10:17.080 --> 10:21.000
they don't really think about a common data model to knit it all together.

10:21.000 --> 10:26.000
Having that ability to create reusable data assets that can then be built on for other

10:26.000 --> 10:31.040
software development is actually pretty powerful, particularly when you start thinking about

10:31.040 --> 10:35.240
a world in which you've got a lot of common data in common areas that is then reusable

10:35.240 --> 10:36.720
for multiple use cases.

10:36.720 --> 10:42.680
Now, I think we've talked about in my previous interviews here a bit about the data model

10:42.680 --> 10:48.360
and the notion of virtual data lakes and the like, but from your perspective as helping

10:48.360 --> 10:54.800
to support Shell, how have you been able to kind of bring these ideas to bear to support

10:54.800 --> 11:00.920
their use cases and what are some of the things that you've learned in that process?

11:00.920 --> 11:02.600
It's a great question.

11:02.600 --> 11:08.440
I think, let me answer that question to very distinct ways.

11:08.440 --> 11:16.640
In the work we are doing with the Shell COE team, the center of excellence team, the core

11:16.640 --> 11:26.680
value of the C3 AI suite or the platform is really around very high or rapid extensibility

11:26.680 --> 11:29.040
and reusability.

11:29.040 --> 11:33.960
And hopefully, and I think we're doing this, though there's no easy peer to peer or

11:33.960 --> 11:38.520
like to like comparison, significantly reducing the amount of lines of code you actually do

11:38.520 --> 11:42.000
write when you build an app.

11:42.000 --> 11:51.000
So to give you an illustration of that with Dan's team, what we were presented with was

11:51.000 --> 11:57.480
a fully working solution that they had prototyped that had scaled to about an order of 28 to 30

11:57.480 --> 12:01.360
units, in this case valves, right?

12:01.360 --> 12:07.920
All drawn on a historical data set with some data feeds from one location.

12:07.920 --> 12:16.120
What we did with it as part of a very intense month in the month of May last year was replicate

12:16.120 --> 12:28.240
that historic data set 2,000 times, thus creating something like 400,000 unique valves that

12:28.240 --> 12:32.720
we had to deal with, not 20, but 400,000.

12:32.720 --> 12:38.880
And then train automatically in the platform a unique machine learning model for each valve,

12:38.880 --> 12:44.800
discovering along the way the features that affected the performance of that valve under

12:44.800 --> 12:50.160
the normal operating conditions, as well as under anomalous operating conditions.

12:50.160 --> 12:55.600
The work I did to do that was entirely baked into the platform, really what I wrote was

12:55.600 --> 13:01.360
a parallelization script that went and trained 400,000 models, right?

13:01.360 --> 13:08.400
Turns out in doing so I spun up something like 150 workers, elastically in the cloud, used

13:08.400 --> 13:14.200
it to train these models, persisted those model weights back into the platform as objects

13:14.200 --> 13:18.920
with metadata, callable as an API, and callable on demand.

13:18.920 --> 13:26.000
And having seen that, I think the question naturally became why not use this now to scale,

13:26.000 --> 13:27.000
right?

13:27.000 --> 13:29.680
And that's kind of where we ended the proof of technology and said, okay, now let's put

13:29.680 --> 13:32.800
this into a production type application.

13:32.800 --> 13:38.160
Where we are in that journey is like Dan described a little earlier in our, in his keynote

13:38.160 --> 13:48.680
at C3 transform, we have fully scaled out from 20 valves to about 400 valves from one location.

13:48.680 --> 13:55.680
We have also built out the infrastructure to talk to three distinct data sources from

13:55.680 --> 14:02.600
Shell, along with what we call canonicals that talk to those systems seamlessly irrespective

14:02.600 --> 14:05.400
of what data is being sent to us.

14:05.400 --> 14:11.800
But similarly, I have trained not just the model itself, but I've written algorithms

14:11.800 --> 14:20.720
or programming logic that trains the model automatically for any one of those assets

14:20.720 --> 14:25.480
and persists both the model and all associated metadata with it and makes it available

14:25.480 --> 14:27.480
to the end user, right?

14:27.480 --> 14:31.880
When you start solving those problems, what happened naturally in the course of this

14:31.880 --> 14:35.800
is one project, the valves project got started earlier.

14:35.800 --> 14:40.400
They had more teething problems and they just kind of had to solve issues along the way.

14:40.400 --> 14:46.080
The second project, the compressor project started about order of two months after valves.

14:46.080 --> 14:51.680
And when they started, they realized from their teammate sitting kind of two desks down

14:51.680 --> 14:56.560
that that teammate had already solved the problem of ingesting all sensitive data.

14:56.560 --> 15:01.280
That teammate had already solved the problem of what a model looks like in Shell with associated

15:01.280 --> 15:03.440
metadata fields.

15:03.440 --> 15:09.520
Had already solved the problem of transforming data from the raw data feed into the variants

15:09.520 --> 15:14.760
that I need to use to support the asset hierarchy within Shell.

15:14.760 --> 15:15.760
Done problem.

15:15.760 --> 15:22.160
The other piece that got better in the second time we did it was the understanding of what

15:22.160 --> 15:24.000
the app needs to do, right?

15:24.000 --> 15:28.480
And this is to dance point around actually building cloud-based software applications, right?

15:28.480 --> 15:31.480
What's the specification of what problem we're solving?

15:31.480 --> 15:35.160
What should each screen look like?

15:35.160 --> 15:41.600
Having done that, invariably because everything in C3 is an object that we call a type, had

15:41.600 --> 15:47.000
digitized some parts of the UI and other elements in the middleware that actually transformed

15:47.000 --> 15:51.520
data and make it presentable, were instantly available downstream for other applications

15:51.520 --> 15:52.520
as well.

15:52.520 --> 15:54.760
So there is immense reusability.

15:54.760 --> 16:00.520
We solved data integration problems effectively once and then reuse it for all cases.

16:00.520 --> 16:06.320
And by the way, having done that, you then point your data scientists at actually solving

16:06.320 --> 16:12.200
the complex problems, which is how do I now detect a compressor anomaly every 10 minutes

16:12.200 --> 16:14.440
with very high degree of certainty?

16:14.440 --> 16:15.440
That's the hard problem.

16:15.440 --> 16:22.200
Not the problem of stitching data together and building data transforms and doing the plumbing.

16:22.200 --> 16:26.760
And to that point, as it relates to scaling things up, that's where I believe C3 really

16:26.760 --> 16:29.160
makes a difference.

16:29.160 --> 16:37.720
The other piece that I'm talking to Darbert is to also use C3 as a means of experimentation

16:37.720 --> 16:38.720
as well, right?

16:38.720 --> 16:46.240
So the idea is if a model is a type with associated metadata, a Jupyter notebook is also a type

16:46.240 --> 16:48.200
with associated metadata.

16:48.200 --> 16:51.560
And so we are about to launch essentially what we call the notebook service.

16:51.560 --> 16:56.200
So you spin up a notebook on demand, whether it's an R notebook or a Python notebook.

16:56.200 --> 17:01.720
It turns out once you do that, the platform, which doesn't care what it's asked to store

17:01.720 --> 17:07.600
as long as it can represent it somehow, can store notebooks, can store versions of notebooks,

17:07.600 --> 17:15.880
can store variants of notebooks, including ownership data, can allow identity-based access

17:15.880 --> 17:20.880
control into those notebooks so that you're seamlessly connected and can get going.

17:20.880 --> 17:27.360
With no requirement whatsoever to store data in C3 or use the C3 platform for anything

17:27.360 --> 17:32.320
more than that, turns out that when you're actually solving a problem that has some gold

17:32.320 --> 17:38.040
as you mind, it is then a trivial exercise to push it into C3 and then scale with it,

17:38.040 --> 17:39.040
right?

17:39.040 --> 17:40.640
And that's where we're going on that side of it.

17:40.640 --> 17:44.960
So the idea is, start with scaling, we got that.

17:44.960 --> 17:49.440
But then let's also extend to experimentation, not just for core machine learning, but also

17:49.440 --> 17:54.400
then deep learning frameworks, NLP, etc.

17:54.400 --> 17:58.520
And just building on that, it's quite appealing because just to quantify some of the scale

17:58.520 --> 18:04.120
we're talking about, if you look at just one of our assets, it's spitting off around

18:04.120 --> 18:06.920
100,000 measurements per minute.

18:06.920 --> 18:11.480
So if you think of that over five years, you're talking between 70 and 80 billion rows

18:11.480 --> 18:13.800
of data.

18:13.800 --> 18:17.680
And if you're a data scientist in that sort of domain, actually just getting hold of that

18:17.680 --> 18:22.320
data is problematic and being able to put it in a form where you can process it.

18:22.320 --> 18:25.680
If C3's already done that data munging for us and put that in a form with some of these

18:25.680 --> 18:30.800
production apps that we're already developing, and they can manage versioning on the experimentation,

18:30.800 --> 18:32.400
it's quite an appealing prospect.

18:32.400 --> 18:34.120
So I think there's a lot of merit to the platform.

18:34.120 --> 18:37.720
I think the challenge is obviously getting it to the point where all of this works together,

18:37.720 --> 18:39.400
but we like the vision.

18:39.400 --> 18:44.480
So I think one of the things that you mentioned that I'm curious about in the process of

18:44.480 --> 18:53.560
this Valve project, replicating the data, I forget how many times, 440,000 or 2,000 times.

18:53.560 --> 18:58.040
In some of the previous conversations I've had today, one of the themes was not needing

18:58.040 --> 18:59.280
to replicate the data.

18:59.280 --> 19:02.840
Can you just reconcile those ideas for me?

19:02.840 --> 19:03.840
Yeah, it's absolutely.

19:03.840 --> 19:07.520
I think the idea of not replicating the data, which you might have heard from some of

19:07.520 --> 19:14.400
the other conversations you had with C3 folks, was really around this idea of referring

19:14.400 --> 19:18.720
to the capability of the C3 platform to virtualize any data source.

19:18.720 --> 19:21.960
And so the idea is, if you've already stored some data in the cloud, or you've already

19:21.960 --> 19:27.720
built something where it sits in a data warehouse that you can access, you do not need to then

19:27.720 --> 19:33.440
copy that over and make a copy of that entire data set inside the C3 cluster.

19:33.440 --> 19:39.480
Because the C3 object model allows external virtual references to any system that it can

19:39.480 --> 19:45.480
access, right? So in that sense, there is no need to copy data again if you've already

19:45.480 --> 19:47.040
made it available.

19:47.040 --> 19:53.520
What we're talking about when I said we replicated data 2,000 times was more to just purely demonstrate

19:53.520 --> 19:55.360
scalability, right?

19:55.360 --> 19:58.480
So it's, I have one unit of data.

19:58.480 --> 20:04.360
We have not yet had done the work to extract all the data from all parts of Shell, but

20:04.360 --> 20:08.640
I know it's there and I know I need to work on all of that data set at some point.

20:08.640 --> 20:13.880
But because Dan wanted to see it work at that scale, we said, well, you don't have it.

20:13.880 --> 20:15.920
What's the best way I can mill it?

20:15.920 --> 20:20.840
And so I literally ran a job on inside of C3 to replicate that 2,000 times and port it.

20:20.840 --> 20:21.840
Got it.

20:21.840 --> 20:23.480
So that's all we did.

20:23.480 --> 20:27.000
One of the things I find so interesting about this conversation that ties to some work

20:27.000 --> 20:32.120
I'm doing around this platform Zbook is this idea of scale.

20:32.120 --> 20:38.600
And like we throw this word around, and it means more than one thing at least.

20:38.600 --> 20:45.600
One of these things is we've got 100,000 measurements for this one system per year, and

20:45.600 --> 20:49.960
if we have 10 of these systems, we multiply, right?

20:49.960 --> 20:59.920
But there's also a sense in which it means the ability to scale our ability to get models

20:59.920 --> 21:01.280
in a production, right?

21:01.280 --> 21:03.760
The ability to operationalize more quickly.

21:03.760 --> 21:07.920
Dan, I'm wondering, when you think about, do you have different words for that, or how

21:07.920 --> 21:09.840
do you think about that whole space?

21:09.840 --> 21:14.480
Well, I think my problem is a number of, it's scale at every level, right?

21:14.480 --> 21:16.120
So let me just talk about that.

21:16.120 --> 21:22.880
So if you talk about my business, so we have 43,000 retail sites.

21:22.880 --> 21:25.240
That's just one part of Shell.

21:25.240 --> 21:28.720
We're bigger than Starbucks, bigger than McDonald's, and the retail business, in terms of our

21:28.720 --> 21:30.800
global network.

21:30.800 --> 21:34.960
So I have a problem that I have lots of things across my business, and I do all of those

21:34.960 --> 21:35.960
things create data.

21:35.960 --> 21:40.360
So I have a data problem, if you will, that I've got vast scale in terms of the data that

21:40.360 --> 21:41.760
I need to deal with.

21:41.760 --> 21:42.760
That's one.

21:42.760 --> 21:46.120
But then if I move that up a level, I've then got a problem which is I need to aggregate

21:46.120 --> 21:47.520
that data at scale.

21:47.520 --> 21:51.960
So I've got to be able to bring that into an aggregate environment in the cloud, because

21:51.960 --> 21:53.680
a lot of these things aren't cloud native.

21:53.680 --> 21:55.320
They have legacy systems.

21:55.320 --> 21:57.680
We need to be able to bring all that together.

21:57.680 --> 22:01.920
Then I have a problem at scale in terms of machine learning, because if you think about

22:01.920 --> 22:06.800
the vows problem, it's not good enough to do 16 vows and solve the problem for one business

22:06.800 --> 22:09.080
unit in a very small area of the business.

22:09.080 --> 22:13.040
I need to be able to do this for half a million vows worldwide, and so I've got a machine

22:13.040 --> 22:16.920
learning at scale problem, which means I've got a model management problem, which is something

22:16.920 --> 22:19.080
we talked about a little bit the last time.

22:19.080 --> 22:22.320
And then I've got a problem with the fact that I've then got users at scale.

22:22.320 --> 22:28.080
So the people that want to need to consume that, I've got potentially tens, hundreds, thousands

22:28.080 --> 22:30.360
of users across the whole of Shell.

22:30.360 --> 22:35.360
We're 80 plus thousand person organization.

22:35.360 --> 22:41.560
We've also got huge numbers of contractors, partners, suppliers, all of them potentially

22:41.560 --> 22:45.440
need to get insight from some of the applications that I'm developing.

22:45.440 --> 22:51.160
And so at the end of the day, it's that scale at all of those levels that's so problematic.

22:51.160 --> 22:56.040
And you actually have to have something that can solve all of those things if you want

22:56.040 --> 22:58.120
to get anything into production.

22:58.120 --> 23:02.560
And so we talk a lot about the focus on scaling and replicating, so getting it to that scale

23:02.560 --> 23:09.560
and then replicating it to allow it to generate benefits because the problem with digital technology

23:09.560 --> 23:14.680
is it's very easy to solve the problem once, but it also tends to be very expensive.

23:14.680 --> 23:18.720
And the benefits in any business model in the digital space comes from that replication

23:18.720 --> 23:19.720
thrust.

23:19.720 --> 23:23.800
And so the key thing is we've got to be able to create a platform strategy that allows

23:23.800 --> 23:27.680
us to do that because otherwise we become a very expensive cost center.

23:27.680 --> 23:35.360
Your last comment reminds me a bit of the Nvidia founder and CEO, whenever he gets up on

23:35.360 --> 23:39.400
the stage and holds up a new GPU, he's like, this GPU costs a billion dollars.

23:39.400 --> 23:40.400
Exactly.

23:40.400 --> 23:41.400
Right?

23:41.400 --> 23:45.800
Because that first up front effort is so significant, but then as you're able to replicate

23:45.800 --> 23:47.840
it, the incremental cost goes down.

23:47.840 --> 23:48.840
Exactly.

23:48.840 --> 23:49.840
Right.

23:49.840 --> 23:54.400
You mentioned in your talk, I think a new use case that we hadn't talked about previously

23:54.400 --> 23:55.400
the reward engine.

23:55.400 --> 23:56.400
Yeah.

23:56.400 --> 23:58.120
Is that one new?

23:58.120 --> 24:00.400
Yeah, it is new.

24:00.400 --> 24:03.760
It's one that we've just gone live with pretty recently.

24:03.760 --> 24:06.000
It's now live in the UK market.

24:06.000 --> 24:11.240
What it's all about is we talk a lot in our retail business about treating customers as

24:11.240 --> 24:12.400
a guest.

24:12.400 --> 24:16.000
So when you come to a shell station, we want you to feel like you're the most special

24:16.000 --> 24:19.680
person in the world to us and we want you to have a great experience.

24:19.680 --> 24:23.960
And that means treating you really well with our service champions as we call them.

24:23.960 --> 24:28.040
And you actually arrive on the forecourt or in the store, but it also means digitally you

24:28.040 --> 24:30.800
need to have a fantastic customer experience.

24:30.800 --> 24:32.320
And so we did a lot of thinking about that.

24:32.320 --> 24:36.320
And we basically took the same principles that Amazon and Facebook apply and we said,

24:36.320 --> 24:40.320
how can we make the experience that you get whenever you turn up at a shell station

24:40.320 --> 24:41.640
extremely personal?

24:41.640 --> 24:45.800
So we know what you want from us and we want you to have that experience.

24:45.800 --> 24:50.240
We want you to have the sensitivity around how much data you share with us.

24:50.240 --> 24:53.520
But once you do share your data, we want to give you a great experience with great

24:53.520 --> 24:56.280
offers and great benefits of being part of shell.

24:56.280 --> 24:58.720
And so that's what the reward engine is all about.

24:58.720 --> 25:03.560
It's a capability that sits under the hood of our loyalty system and it effectively dictates

25:03.560 --> 25:07.560
what you would experience you get at the site when you turn up to shell.

25:07.560 --> 25:09.280
And we're really proud of it.

25:09.280 --> 25:13.360
It's just gone live in the UK, like as I said, we're planning to roll it out globally.

25:13.360 --> 25:17.800
Can you talk a little bit about it technically in terms of data sources, types of models,

25:17.800 --> 25:19.280
what the models are doing?

25:19.280 --> 25:23.560
To a lot of the data comes from a combination of things, it comes from transaction history,

25:23.560 --> 25:26.880
it comes from information we have about the store, where it is, what the traffic around

25:26.880 --> 25:28.400
that is, et cetera, et cetera.

25:28.400 --> 25:33.760
We also have a whole bunch of information around, for example, the loyalty scheme you've

25:33.760 --> 25:36.920
done with us previously, how many visits you've had, what sort of things you've bought

25:36.920 --> 25:40.480
on the loyalty scheme, how many points you've got, how many visits you've got, I should

25:40.480 --> 25:41.480
say.

25:41.480 --> 25:46.240
And so we take all of that information and we effectively, we take a Bayesian inference

25:46.240 --> 25:49.040
approach effectively and I'm horribly simplifying it.

25:49.040 --> 25:54.920
But what we do is we say a shell customer typically buys fuel and then we overlay a clustering

25:54.920 --> 26:00.480
approach and a series of rules as well to create a combination of things we want you to

26:00.480 --> 26:01.480
have.

26:01.480 --> 26:03.120
So we want you to get rewarded for frequency.

26:03.120 --> 26:06.000
So we apply visit logic over the top.

26:06.000 --> 26:11.400
But we also infer from history of customers who behave in a similar way to the way that

26:11.400 --> 26:12.400
you behave.

26:12.400 --> 26:14.920
So it's kind of like a camins, if you think about it that way.

26:14.920 --> 26:20.000
And so we're trying to cluster the behavior, infer your behavior, but ultimately try to

26:20.000 --> 26:23.840
then create offers that are relevant, but then continually iterate on that.

26:23.840 --> 26:25.160
So that's the inference aspect.

26:25.160 --> 26:31.160
So we constantly overlay behavioral observations in real time, back onto that customer and then

26:31.160 --> 26:33.280
use that to make the offers.

26:33.280 --> 26:38.800
One of the things I've learned over the past, over the course of the day in some of the

26:38.800 --> 26:44.440
sessions here at the conferences, kind of the evolution of C3, right, started it with

26:44.440 --> 26:48.840
the focus of this data layer that we've talked a bunch about.

26:48.840 --> 26:50.440
And actually you correct me if I'm wrong.

26:50.440 --> 26:54.680
I mean, the impression I have is that the company's kind of been pulled into machine learning

26:54.680 --> 27:01.200
in AI and working with companies like Shell and NL and others and is kind of building on

27:01.200 --> 27:03.520
that underlying data platform.

27:03.520 --> 27:09.280
Yeah, I think the question that I'm getting is like, there's so much, you know, a data

27:09.280 --> 27:11.920
platform is a huge challenge.

27:11.920 --> 27:16.360
And then we're talking about Bayesian inference and TensorFlow and machine learning models

27:16.360 --> 27:21.400
like the, I think one of the speakers earlier talked about the number, it was maybe Tom talked

27:21.400 --> 27:24.840
about the number of connections between all these things.

27:24.840 --> 27:27.040
How do you manage that?

27:27.040 --> 27:32.280
And I'm presumably part of the goal is to keep that simple for the people that are using

27:32.280 --> 27:33.280
the platform.

27:33.280 --> 27:39.000
I imagine it then wants to keep things, you know, the platform view simple to his consumers.

27:39.000 --> 27:42.000
So let me answer that question two ways, Sam.

27:42.000 --> 27:46.520
The first is it would be helpful to your listeners to understand how C3 got to where we are,

27:46.520 --> 27:47.520
right?

27:47.520 --> 27:52.200
And a little bit of just our evolution and and why it is the way we do certain things

27:52.200 --> 27:53.520
in certain way.

27:53.520 --> 27:58.320
The second is to give you a sense of how we think about platform users and frankly the answer

27:58.320 --> 28:02.400
is there are many platform users, they're all very different and you've got to expose

28:02.400 --> 28:06.280
different parts of the platform to them, but not everything to everyone.

28:06.280 --> 28:10.680
And in doing so, the right way is how we manage down the complexity.

28:10.680 --> 28:12.200
So let me answer the first thing first, right?

28:12.200 --> 28:16.280
So so C3 got started as a carbon trading company, okay?

28:16.280 --> 28:22.400
And the original C was represented carbon, right?

28:22.400 --> 28:27.520
And it came out around, I mean, the conversation at the time, and this is going back to 2009,

28:27.520 --> 28:33.840
2010, was around this thesis that because of the carbon trading act, everyone will need

28:33.840 --> 28:36.600
to measure their carbon footprint.

28:36.600 --> 28:40.040
And then because some have better and some have worse carbon footprint relative to each

28:40.040 --> 28:46.000
other, that is the opportunity to both measure it, monitor it, and then mitigate it by trading.

28:46.000 --> 28:47.000
Right?

28:47.000 --> 28:54.040
So there's therefore a natural need for companies like Cisco and Dell and GE to actually

28:54.040 --> 28:57.200
have software that measured their carbon footprint.

28:57.200 --> 29:04.160
And you would need to therefore measure facility use, person use, travel use, all of these

29:04.160 --> 29:05.880
different things.

29:05.880 --> 29:11.840
And so the company that is now C3, was also called C3 at the time, but the C stood for

29:11.840 --> 29:13.120
carbon.

29:13.120 --> 29:19.480
And we got going in the business of really being able to ingest a huge amount of data

29:19.480 --> 29:23.080
all related to measuring a company's carbon footprint, where did this data come from?

29:23.080 --> 29:25.600
It came from facility plans.

29:25.600 --> 29:28.160
It came from energy utility bills.

29:28.160 --> 29:30.720
It came from energy utility meters.

29:30.720 --> 29:37.240
It came from self-reported lead certification of buildings, things like that.

29:37.240 --> 29:45.920
Turns out we built that company or that piece of software in a way that said, and this

29:45.920 --> 29:52.360
was I think ahead of its time, that recognized that if we went to 10 enterprises, each of

29:52.360 --> 29:57.120
those 10 enterprises would have a different data model, different sources of data, but

29:57.120 --> 29:59.400
they were all trying to solve the same problem.

29:59.400 --> 30:06.640
And so we built that first piece of software to essentially allow a modular separation between

30:06.640 --> 30:12.040
the data we were getting and the use of that data going up.

30:12.040 --> 30:17.080
And because a lot of our hunters came from civil systems, that was the core of how it

30:17.080 --> 30:18.080
was built.

30:18.080 --> 30:21.160
And so a lot of that DNA kind of got transferred over.

30:21.160 --> 30:25.240
Turns out that market went to bust, carbon trading sort of evaporated.

30:25.240 --> 30:28.760
And we said, okay, we built this technology, what do we do with it?

30:28.760 --> 30:35.880
What we did with it was point it at a problem where there was a huge amount of data available

30:35.880 --> 30:41.320
that was the same carbon trading problem time, 10 or times 100, where we believed we could

30:41.320 --> 30:44.600
engineer the scalability required to solve the problem.

30:44.600 --> 30:51.720
And that was really building software solutions for the electric grid or the utilities, right?

30:51.720 --> 31:00.680
Which was at the time, and this is 2011, 2012, the most censored industry of them all, right?

31:00.680 --> 31:02.760
Sensored in the sense of having a lot of censors.

31:02.760 --> 31:07.320
Having a lot of censors as opposed to anything else, right?

31:07.320 --> 31:13.520
Turns out that these guys have been actually pretty good at installing censors, deploying

31:13.520 --> 31:19.960
censors, not just for smart meters, but also transformers, vibration sensors on grid

31:19.960 --> 31:23.840
equipment, re-closers, the works.

31:23.840 --> 31:29.280
And now one of our largest customers whom you may have heard on stage today was our customer

31:29.280 --> 31:33.200
when we were that company called C3 Energy.

31:33.200 --> 31:37.760
And the first problem was, can you deal with all this data, turns out we could.

31:37.760 --> 31:41.280
Because again, because we had modularized and abstracted the way we built the original

31:41.280 --> 31:47.200
piece of software, adding cloud services, elastic scalability, et cetera, came naturally,

31:47.200 --> 31:49.800
while retaining that abstraction.

31:49.800 --> 31:53.640
This is kind of the genesis of what we call the type system.

31:53.640 --> 31:59.320
And so the first problem to us was, can you handle 800 terabytes of data that I sent

31:59.320 --> 32:03.520
to you and then process it at a million transactions per second?

32:03.520 --> 32:06.000
No AI, no machine learning, that was it, right?

32:06.000 --> 32:07.400
Turns out we could.

32:07.400 --> 32:12.360
And turns out we proved it to them in order of a few cycles, just like dance, one million

32:12.360 --> 32:14.680
model problem.

32:14.680 --> 32:19.480
Having sought that, they said, okay, what do we do with this technology?

32:19.480 --> 32:23.640
And it turns out the most valuable problems to them, again, going back to like, where's

32:23.640 --> 32:28.760
the money, were in the application of these very large data sets, but also machine learning

32:28.760 --> 32:30.480
algorithms on that.

32:30.480 --> 32:36.440
And so we built in the same modular approach, the ability to build both temporal and spatial

32:36.440 --> 32:41.480
analytics, and then machine learning models using those temporal and spatial analytics,

32:41.480 --> 32:43.960
again, in this abstracted way.

32:43.960 --> 32:47.760
So that everyone's writing as little code as possible, but you're expressing logic very

32:47.760 --> 32:50.280
quickly.

32:50.280 --> 32:56.560
That's been now hardened to the point where it works seamlessly on all flavors and

32:56.560 --> 33:04.360
all types of data over the next, over the subsequent five issues, right?

33:04.360 --> 33:10.920
And in doing so, we've gone from offering SaaS solutions to the utilities to a platform

33:10.920 --> 33:15.040
that has these capabilities to all industries.

33:15.040 --> 33:22.320
And in doing so, we've now focused on building out these capabilities that look like better

33:22.320 --> 33:29.120
cloud services, better performance, better ML and AI and application development tooling,

33:29.120 --> 33:32.120
and more applications, all using the same logic.

33:32.120 --> 33:38.720
So you could call it chance, you could call it a natural necessity of the types of problems

33:38.720 --> 33:41.760
we solved, and therefore we're exposed to next.

33:41.760 --> 33:49.040
So as I like to joke, the price for the pie eating contest is more pie, and we just kept

33:49.040 --> 33:52.520
winning them every few weeks.

33:52.520 --> 33:53.960
And that's been our evaluation, right?

33:53.960 --> 33:59.800
A lot of the initial impetus came from customer conversations, right?

33:59.800 --> 34:05.560
It's like, hey, what problems do you need that are real actually need solving?

34:05.560 --> 34:10.040
A lot of it came from our roadmap, and we see this constant conversation going on, and

34:10.040 --> 34:16.120
it happened today, and it will happen tomorrow in the rest of our conference and so on.

34:16.120 --> 34:17.520
And that's how we think about this, right?

34:17.520 --> 34:22.840
So there's, and if you listen to Tom talking about the types of people we hire, we hire

34:22.840 --> 34:27.320
people that are restless, that are not willing to say the jobs done were done, right?

34:27.320 --> 34:31.160
There's always something new to solve, and it's constantly evolving.

34:31.160 --> 34:35.520
I suspect, if you look at our road, if you fast forward a year and look back at what

34:35.520 --> 34:40.160
we built, about half of the things we built would have come from our own product managers

34:40.160 --> 34:42.280
thinking through like, what do we need?

34:42.280 --> 34:46.400
And the remaining half would come from customer demands, and it's like, hey, I need this to solve

34:46.400 --> 34:47.640
this problem.

34:47.640 --> 34:48.640
You haven't solved it yet.

34:48.640 --> 34:49.640
Well, fine.

34:49.640 --> 34:50.640
We'll go solve.

34:50.640 --> 34:51.640
Right?

34:51.640 --> 34:52.640
And that's really how this will evolve.

34:52.640 --> 34:53.640
As we go.

34:53.640 --> 34:56.760
It's managing the complexity of all that.

34:56.760 --> 34:57.760
So now let's go to that.

34:57.760 --> 34:58.760
Right?

34:58.760 --> 35:05.280
So as we grow our capabilities in the platform, I think about how we organize ourselves, right?

35:05.280 --> 35:08.120
Which gives you a window as to how we think about the world.

35:08.120 --> 35:12.600
We have a core platform engineering team and the core platform engineering team deals with

35:12.600 --> 35:19.720
everything related to infrastructure services, whether they are cloud or on premise.

35:19.720 --> 35:24.600
They do everything related to security and authentication.

35:24.600 --> 35:31.320
Is it there's a pod that deals with data management and data source externalization?

35:31.320 --> 35:37.320
So how do I write connectors to MongoDB or Snowflake or whatever else?

35:37.320 --> 35:42.000
And there is a processing analytics engine team within the platform team that deals with

35:42.000 --> 35:47.880
what's the most efficient way to store a time series across archival storage and SSD

35:47.880 --> 35:51.680
storage so that I can actually access data in the fastest way possible.

35:51.680 --> 35:55.800
And how do I parametrize that so that it's easy to solve?

35:55.800 --> 36:00.240
All of these pods in core platform engineering are building with the same mindset, which

36:00.240 --> 36:07.600
is how do I abstract the services I provide to the layer above me from the raw code I

36:07.600 --> 36:09.880
write to actually manage the layer below me?

36:09.880 --> 36:10.880
Right?

36:10.880 --> 36:11.880
Everyone's doing the same thing.

36:11.880 --> 36:15.320
So at that level, you've got those pods.

36:15.320 --> 36:21.480
If you solve those correctly, which is kind of what they do, we then offer up to the next

36:21.480 --> 36:28.320
team within C3, which looks like application development, tool development and machine

36:28.320 --> 36:33.360
learning engineering, a series of native services that they can then use to build their own

36:33.360 --> 36:34.360
services.

36:34.360 --> 36:35.360
What are they doing?

36:35.360 --> 36:39.680
Machine learning engineering is literally building what we call an ML pipeline, right?

36:39.680 --> 36:46.520
An ML pipeline is essentially a sequence of steps that's executable in a runtime that

36:46.520 --> 36:52.920
is declared on compile or previously that can use any framework that you want to pull

36:52.920 --> 36:59.360
in, whether it's TensorFlow or Keras or anything else, along with any data processing and outputs

36:59.360 --> 37:01.560
that you need to do with it, right?

37:01.560 --> 37:07.080
And what they're doing is really writing bindings that can use a series of these frameworks

37:07.080 --> 37:11.560
and a series of external tools that can work with the platform.

37:11.560 --> 37:16.680
Application engineering is building applications, inventory optimization, predictive maintenance,

37:16.680 --> 37:22.160
et cetera, with features, functionality directed by the product managers, invariably

37:22.160 --> 37:24.800
in conversation with our customers.

37:24.800 --> 37:32.000
And the tools team is building via metadata APIs that the platform team exposes, the tooling

37:32.000 --> 37:39.840
to then change platform settings through very minimal, either no code or no code tools

37:39.840 --> 37:42.880
that they make available through browsers, right?

37:42.880 --> 37:48.920
And then we have customer service and sales engineering teams that use all of these capabilities

37:48.920 --> 37:53.480
to then go solve customer problems with a rich toolkit that sort of hangs on their

37:53.480 --> 37:57.640
ways like Batman's utility belt and they're pulling out what they need, right?

37:57.640 --> 38:02.840
But very rarely are they solving problems that go down to say, I need a new database

38:02.840 --> 38:04.200
connector.

38:04.200 --> 38:07.840
If that's the case, the platform team goes solves and it makes available a service above

38:07.840 --> 38:10.800
that they use and so on.

38:10.800 --> 38:17.960
Everything in C3 is a type and therefore the database connector to Impala is a type,

38:17.960 --> 38:18.960
right?

38:18.960 --> 38:22.880
The machine learning pipeline that uses Tesseract is a type.

38:22.880 --> 38:29.380
The application widget on the inventory optimization screen that shows the latest deviation from

38:29.380 --> 38:32.240
safety stock is a type, right?

38:32.240 --> 38:40.480
And the data integration tool logic that finds the best match for any ingested data terms

38:40.480 --> 38:45.920
of its target source in terms of its target field is a type or a functionality, right?

38:45.920 --> 38:50.680
And so everything's abstracted always and you're really dealing with internal APIs that

38:50.680 --> 38:53.000
work seamlessly across the platform.

38:53.000 --> 38:54.680
That's how we manage down the complexity.

38:54.680 --> 39:00.640
Now if you're a customer, right, then we are, then the question to ask is, what are you

39:00.640 --> 39:02.400
in that customer?

39:02.400 --> 39:07.080
If you're an end user of a customer, you don't care about the C3 platform at all.

39:07.080 --> 39:11.080
All you care about is what's the URL you forget to and what's the big red shiny button

39:11.080 --> 39:12.080
I need to press, right?

39:12.080 --> 39:13.440
That's all you care about.

39:13.440 --> 39:18.040
If you're an application developer in a customer, at a customer site, like a lot of Dan's team

39:18.040 --> 39:20.440
that are building applications with us.

39:20.440 --> 39:26.440
You care about the data ingestion APIs, the machine learning engineering APIs, maybe

39:26.440 --> 39:28.640
the application logic APIs.

39:28.640 --> 39:32.320
You don't care about the platform fundamentals at all because that's given to you.

39:32.320 --> 39:36.560
If you're a data scientist on the safety platform, you care about what pipelines the machine

39:36.560 --> 39:42.480
learning engine team has made available and how I can call the time series engine or the

39:42.480 --> 39:47.040
data wrangling engine in Jupyter Notebook to do what I need to do.

39:47.040 --> 39:48.520
That's really all you care about.

39:48.520 --> 39:53.360
So depending on who you are, it actually changes what we expose and that then also manages

39:53.360 --> 39:55.280
down the complexity.

39:55.280 --> 40:01.520
Frankly, there are about three people in the company that know it all and let's just

40:01.520 --> 40:06.280
say that they are incredibly valuable.

40:06.280 --> 40:12.840
Before we started rolling, I was mentioning that one of the first articles I wrote seven

40:12.840 --> 40:18.160
years ago, seven plus years ago when I started my company was about machine learning platforms.

40:18.160 --> 40:24.200
And at the time, I heard a lot of feedback that says that we're never going to be able

40:24.200 --> 40:26.160
to generalize machine learning.

40:26.160 --> 40:29.760
Like we're going to have to, you know, it's always going to be a problem that's solved

40:29.760 --> 40:34.960
on a snowflake by snowflake basis on a problem by problem basis.

40:34.960 --> 40:38.880
Dan, you're building platforms, what's your take on that?

40:38.880 --> 40:43.160
What's your experience on that and how close are we to, you know, can we call it an

40:43.160 --> 40:48.360
Irvana of having a generalized platform that we can apply to a wide variety of business

40:48.360 --> 40:50.360
problems?

40:50.360 --> 40:53.400
I think we're getting closer and closer to that.

40:53.400 --> 40:55.800
I think I would, so I'd answer that in a couple of ways.

40:55.800 --> 41:01.560
I think the first is that we still have a view that you still solve machine learning problems

41:01.560 --> 41:02.560
case by case.

41:02.560 --> 41:04.440
So we start with the hypothesis.

41:04.440 --> 41:08.960
What is the core question you're trying to answer and how do you prove that with data?

41:08.960 --> 41:14.600
It's still fundamental to, you know, the scientific method, if you will, that we employ in the

41:14.600 --> 41:19.080
way we go about solving any problem we get from our business users.

41:19.080 --> 41:23.360
What I think is increasing the interest thing is that we are starting in the same way as

41:23.360 --> 41:27.080
the software industry figured out that even when you're solving a problem, you can create

41:27.080 --> 41:28.080
your reusability.

41:28.080 --> 41:31.840
We're seeing that same trend emerging in machine learning.

41:31.840 --> 41:38.520
So you can have generic pipelines which are mostly the same user-consistent set of frameworks

41:38.520 --> 41:44.920
and then ultimately can be deployed to solve different problems within a given space.

41:44.920 --> 41:47.760
So the example I always use is natural language processing.

41:47.760 --> 41:53.120
So I talked about Tesseract, right, Space C is becoming commonplace in that domain.

41:53.120 --> 41:57.080
At the end of the day, what you can do in most cases is stitch together a series of building

41:57.080 --> 42:02.000
blocks with Python, NLTK and those sorts of things and ultimately get to an end product

42:02.000 --> 42:05.760
where you can use that same pipeline to answer multiple questions.

42:05.760 --> 42:10.680
Now, you still need a data scientist that knows how to tweak the knobs if you want to

42:10.680 --> 42:11.680
put it that way.

42:11.680 --> 42:17.120
But at the same time, you can reduce the time to value so you're not rebuilding everything

42:17.120 --> 42:18.640
every time.

42:18.640 --> 42:23.160
And that's really what the theme across my team, we're really looking to say, when I go

42:23.160 --> 42:26.520
to solve that problem, can I take a reusable asset that someone else has built somewhere

42:26.520 --> 42:33.240
else and it reduced the time to value, whether that be at the data level or at the framework

42:33.240 --> 42:37.680
level to allow you to solve the business problem as fast as possible.

42:37.680 --> 42:42.680
Now I think the next generation of that is making that accessible so the end users can

42:42.680 --> 42:44.400
tweak those parameters.

42:44.400 --> 42:48.800
And that's the AutoML development, it's the work in the self-service and actually some

42:48.800 --> 42:52.960
of the things that C3 are trying to do to expose some of their core platform functionality.

42:52.960 --> 42:56.240
And you see that right across the industry right now.

42:56.240 --> 42:59.120
I think the other thing that's going to come and hit us quite hard, though, which is

42:59.120 --> 43:04.760
emerging is how do you infuse data security into all of that?

43:04.760 --> 43:09.680
Because I think there's been an explosion of ideas in this space and somehow we need to

43:09.680 --> 43:13.640
make sure that we do that responsibly and there's a lot of thinking going into that, particularly

43:13.640 --> 43:17.920
within Shell right now, to make sure that we've got the platform right underpin all of

43:17.920 --> 43:22.760
that and that those reusable assets have also security baked in.

43:22.760 --> 43:28.360
So I see the whole industry growing up, I see it becoming much more like the software

43:28.360 --> 43:29.360
industry.

43:29.360 --> 43:30.360
In general.

43:30.360 --> 43:32.160
It's a great analogy.

43:32.160 --> 43:37.440
We started this interview talking about the strategy you've made in four months.

43:37.440 --> 43:39.720
You want to predict us four months out?

43:39.720 --> 43:41.640
That's a great question.

43:41.640 --> 43:43.680
So there's a few things on our roadmap.

43:43.680 --> 43:45.360
I'll talk about some of those.

43:45.360 --> 43:50.280
I think for us, optimization is the next big horizon.

43:50.280 --> 43:55.480
So how do we start to use a combination of traditional optimization techniques, stuff

43:55.480 --> 44:00.880
like Cplex, for example, and traditional solvers, in conjunction with new optimization techniques

44:00.880 --> 44:05.280
like deep reinforcement learning, to start to solve some of the toughest optimization problems

44:05.280 --> 44:06.800
across the energy industry?

44:06.800 --> 44:09.000
That's a really exciting space for us.

44:09.000 --> 44:13.200
I think the other thing about that space is that's when you start ironically getting back

44:13.200 --> 44:18.520
to C3's original mission, which is trying to reduce carbon, because in that whole space,

44:18.520 --> 44:23.440
if you can really optimize, you can start to look at trade-offs between your greenhouse

44:23.440 --> 44:27.640
gas emissions and your production, as well as trying to look at places where you need

44:27.640 --> 44:29.320
to tighten up.

44:29.320 --> 44:32.680
So I think that's going to be a huge thing for us.

44:32.680 --> 44:39.400
I think the other thing that's super interesting is how do you start to look at an emerging

44:39.400 --> 44:47.960
class of energy customer who are typically very green in their outlook, tech savvy, and

44:47.960 --> 44:49.360
wants everything digitally?

44:49.360 --> 44:52.280
So how do we meet that need as an organization?

44:52.280 --> 44:56.120
And how do we make that, make it really easy to interact with Shell, and how do we give

44:56.120 --> 45:02.200
you a range of offerings that allow you to solve your energy needs, and do that taking full

45:02.200 --> 45:03.880
advantage of AI?

45:03.880 --> 45:06.000
And that's the vision that I'm super excited about.

45:06.000 --> 45:08.360
That's where we're really trying to push the envelope next.

45:08.360 --> 45:13.720
Well, Dan, Eddie, thanks so much for taking the time to chat with me, great conversation.

45:13.720 --> 45:14.720
Thank you.

45:14.720 --> 45:15.720
Thank you.

45:15.720 --> 45:21.920
All right, everyone, that's our show for today.

45:21.920 --> 45:26.160
For more information about today's show, visit twimmelai.com.

45:26.160 --> 45:32.480
Be sure to visit twimmelcon.com for information or to register for Twimmelcon AI platforms.

45:32.480 --> 45:36.280
Thanks again to C3 for their sponsorship of today's episode.

45:36.280 --> 45:39.960
To check out what they're up to, visit c3.ai.

45:39.960 --> 45:46.120
As always, thanks so much for listening and catch you next time.

