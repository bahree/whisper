WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:16.400
I'm your host, Sam Charrington.

00:16.400 --> 00:24.280
Hey, what's up everyone?

00:24.280 --> 00:27.680
Happy Memorial Day to those of you in the States.

00:27.680 --> 00:31.960
Although we might not be able to celebrate holidays like we once would, I encourage

00:31.960 --> 00:37.280
you to find a way to enjoy yourself this weekend, connect with family and friends and enjoy

00:37.280 --> 00:42.320
some good food and fun as best as you can.

00:42.320 --> 00:48.120
I am super excited to be hosting tomorrow's live panel discussion on advancing your data

00:48.120 --> 00:50.840
science career during the pandemic.

00:50.840 --> 00:55.080
This is going to be a great one featuring an amazing lineup and I encourage you to join

00:55.080 --> 00:56.080
us.

00:56.080 --> 01:02.480
You can check out twimalai.com slash DS careers for more information and to register.

01:02.480 --> 01:08.160
I also want to give you a heads up regarding my upcoming webinar with Algorithmia CTO

01:08.160 --> 01:09.640
Kenny Daniel.

01:09.640 --> 01:14.680
The hot topic at last year's TwimalCon on conference was a discussion on whether you

01:14.680 --> 01:19.240
should build or buy an ML or data science platform.

01:19.240 --> 01:23.600
Well, we'll be tackling this topic head on in our upcoming session.

01:23.600 --> 01:27.880
We'll discuss what goes into building a machine learning management platform, how to make

01:27.880 --> 01:33.000
the business case for ML ops at your company and how to evaluate off the shelf machine

01:33.000 --> 01:35.360
learning management solutions.

01:35.360 --> 01:41.440
Be sure to mark your calendar for 10 a.m. Pacific on June 9th and visit twimalai.com slash

01:41.440 --> 01:43.560
Algorithmia to register.

01:43.560 --> 01:50.320
That's twimalai.com slash ALGORITHMIA.

01:50.320 --> 01:53.920
All right, enjoy the show and enjoy the holiday.

01:53.920 --> 01:55.160
All right, everyone.

01:55.160 --> 01:57.400
I am on the line with Joey Gonzalez.

01:57.400 --> 02:02.400
Joey is an assistant professor at UC Berkeley in the EECS department.

02:02.400 --> 02:05.240
Joey, welcome to the Twimalai podcast.

02:05.240 --> 02:06.480
Thank you for having me.

02:06.480 --> 02:11.920
I'm really looking forward to diving into this conversation.

02:11.920 --> 02:18.440
And in particular, talking about ML systems and your recent paper on train large then

02:18.440 --> 02:24.280
compress, but before we do that, please share a little bit about your background and how

02:24.280 --> 02:27.360
you came to work in ML and AI.

02:27.360 --> 02:28.360
Yeah, excellent.

02:28.360 --> 02:30.240
So my story's a bit funny.

02:30.240 --> 02:35.680
I started my PhD at Carnegie Mellon with an interest in actually flipping helicopters

02:35.680 --> 02:39.440
because that was a trendy thing to do back in 2006, a while back.

02:39.440 --> 02:40.440
Flipping helicopters.

02:40.440 --> 02:41.440
Flipping helicopters.

02:41.440 --> 02:42.440
Flipping helicopters.

02:42.440 --> 02:43.440
Flipping helicopters.

02:43.440 --> 02:44.440
Flipping helicopters.

02:44.440 --> 02:45.440
Flipping helicopters.

02:45.440 --> 02:46.440
Flipping helicopters.

02:46.440 --> 02:47.440
Flipping helicopters.

02:47.440 --> 02:52.720
Colleague of mine, Peter Beale, now at Berkeley, when he was finishing up his thesis work,

02:52.720 --> 02:55.680
he was looking at how to do interesting control for helicopters.

02:55.680 --> 02:58.520
I thought that was really cool.

02:58.520 --> 03:03.160
And at CMU, I went to my thesis advice and you've worked on control as well.

03:03.160 --> 03:05.120
I'm kind of interested in flipping helicopters.

03:05.120 --> 03:10.560
I think that's really neat research and you know, I didn't know that was the thing.

03:10.560 --> 03:11.560
Well, it was.

03:11.560 --> 03:14.880
And it actually was some of the pioneering work to what we see today in reinforcement

03:14.880 --> 03:15.880
learning.

03:15.880 --> 03:20.360
But what's kind of cool about the story is my advisor at that time being a real machine

03:20.360 --> 03:21.360
learning researcher.

03:21.360 --> 03:26.520
I was like, you know what, flipping helicopters, that's exciting, but there's something

03:26.520 --> 03:29.640
more important, like we can actually help the world with sensors.

03:29.640 --> 03:34.320
We can build sensor networks to monitor fires and we can use kind of principled machine

03:34.320 --> 03:35.640
learning techniques.

03:35.640 --> 03:38.840
I should add that when I was looking at the flipping helicopters, like, you know what,

03:38.840 --> 03:41.120
we should flip them with neural networks.

03:41.120 --> 03:44.920
And the other thing my advisor said, which was good advice at the time, was neural networks

03:44.920 --> 03:47.280
aren't really serious research.

03:47.280 --> 03:51.800
We use more statistical methods, graphical models, things that have formal foundations

03:51.800 --> 03:57.440
that we can reason about and write kind of detailed analysis and understand what our models

03:57.440 --> 03:58.440
are doing.

03:58.440 --> 03:59.440
And that was good advice.

03:59.440 --> 04:04.760
And so I went down this path of how to build Gaussian processes, Bayesian on parametric

04:04.760 --> 04:08.920
methods to reason about link quality and sensor networks.

04:08.920 --> 04:12.480
And in that process of doing that, I kind of stumbled into a problem.

04:12.480 --> 04:17.560
I was writing a lot of MATLAB code to compute big matrix inverses and then approximations

04:17.560 --> 04:19.000
to that to make it run faster.

04:19.000 --> 04:22.280
And one of the things I enjoyed doing in the process of, you know, exploring these more

04:22.280 --> 04:26.720
efficient MATLAB programs was trying to make them more parallel.

04:26.720 --> 04:30.360
And I think my advisor included is a good advisor is like, you know what, maybe you enjoy

04:30.360 --> 04:31.360
that more.

04:31.360 --> 04:34.080
So maybe instead of focusing on the non-permetrics and the sensor networks, let's start

04:34.080 --> 04:37.160
to think about how to make machine learning more efficient.

04:37.160 --> 04:41.800
And in particular, at that point in time, Hadoop was taking off and said, you know what,

04:41.800 --> 04:44.760
MapReduce, that's going to change machine learning.

04:44.760 --> 04:48.320
And we were thinking, well, we're working on graphs and they just don't fit the MapReduce

04:48.320 --> 04:49.720
pattern.

04:49.720 --> 04:54.760
And the kinds of computation we were doing just, it wasn't, it didn't actually fit the

04:54.760 --> 04:57.120
technology that people were building.

04:57.120 --> 05:00.200
So we started to explore a different design of systems.

05:00.200 --> 05:05.360
So design of systems for computation on graphs, which took us down the design of graph processing

05:05.360 --> 05:06.360
systems.

05:06.360 --> 05:10.120
System that I ended up writing is kind of the end of my thesis was a graph lab for doing

05:10.120 --> 05:13.040
very large analysis of graphs.

05:13.040 --> 05:17.000
And so by the time I finished my PhD, I was actually writing systems papers, not machine

05:17.000 --> 05:18.480
learning papers.

05:18.480 --> 05:20.680
And the field was changing very, very rapidly too.

05:20.680 --> 05:25.960
This around 2012, if anyone's been following the history of machine learning around 2012,

05:25.960 --> 05:28.960
everyone started to realize maybe actually the neural nets were a good idea.

05:28.960 --> 05:33.680
The deep learning, these ideas actually really dated back the 1980s.

05:33.680 --> 05:35.880
They're actually really starting to work.

05:35.880 --> 05:38.760
And they were changing the field of machine learning.

05:38.760 --> 05:42.720
And graphs were also taking off, so we built actually a company around the systems that

05:42.720 --> 05:47.720
I was developing as a graduate student, it was graph lab, that evolved into a company

05:47.720 --> 05:52.800
for building tools for data scientists to do interesting machine learning at scale.

05:52.800 --> 05:56.120
That was ultimately acquired by Apple.

05:56.120 --> 05:59.840
And around that time, I also joined the UC Berkeley Amplab as a postdoc.

05:59.840 --> 06:03.240
And there was, you know, a chance to come out to California and it was a really exciting

06:03.240 --> 06:08.160
opportunity to do research in a different system, a system called Spark, which eventually

06:08.160 --> 06:09.680
became Apache Spark.

06:09.680 --> 06:13.480
And there we started to develop the graph processing foundation for the Apache Spark

06:13.480 --> 06:15.080
system.

06:15.080 --> 06:19.480
And again, as I started to explore more and more into the field, I learned more about

06:19.480 --> 06:25.120
research in data systems and transaction processing and how those connect back to machine learning.

06:25.120 --> 06:28.840
And so after finishing my postdoc, I came to Berkeley.

06:28.840 --> 06:33.080
In fact, I chose not to follow the much more lucrative path of the startup.

06:33.080 --> 06:34.920
It was going to ask about that.

06:34.920 --> 06:37.600
Yeah, I made a terrible financial decision.

06:37.600 --> 06:40.240
But I'm happy because I have a chance to work with students.

06:40.240 --> 06:44.320
I'm a little less happy because I'm not as wealthy as one could have been.

06:44.320 --> 06:49.040
But now I am teaching students that do research at the intersection of machine learning and

06:49.040 --> 06:50.360
systems.

06:50.360 --> 06:55.440
And so we have a pretty broad agenda around how to build better technologies for delivering

06:55.440 --> 07:00.040
models to manage machine learning life cycle, not just training, but prediction, how to

07:00.040 --> 07:04.640
prioritize training experiments on the cloud to use serverless computing to make machine

07:04.640 --> 07:07.720
learning more cost effective and easier to deploy.

07:07.720 --> 07:12.160
We have a big agenda around autonomous driving, building the actual platform that supports

07:12.160 --> 07:16.400
autonomous driving, not necessarily the models, but how they are connected together to make

07:16.400 --> 07:18.240
a reliable car.

07:18.240 --> 07:21.640
And we have work in natural language processing and computer vision.

07:21.640 --> 07:25.200
And one of those papers, one that I'm hoping to talk a bit about today, which is our work

07:25.200 --> 07:28.920
on making a vert models easier to train.

07:28.920 --> 07:33.600
And it, too, has a kind of funny story how we came to actually a realization that what

07:33.600 --> 07:35.840
we were thinking was entirely wrong.

07:35.840 --> 07:38.280
And that's what that paper talks a bit about.

07:38.280 --> 07:42.080
Well, let's get to that funny story in a second.

07:42.080 --> 07:45.400
There's so much interesting stuff that you just mentioned.

07:45.400 --> 07:48.840
It's there, there are at least three or four interesting podcasts in here.

07:48.840 --> 07:52.920
I'd love to dig into some of the stuff you're doing with serverless at some point and how

07:52.920 --> 07:59.320
that intersects with ML and AI, something I've looked at a little bit as well.

07:59.320 --> 08:05.800
But before we jump into even more of that, I'm curious.

08:05.800 --> 08:11.680
Your co-founder at GraphLab and Tury Carlos Gastron was one of my very first guests on

08:11.680 --> 08:12.680
this show.

08:12.680 --> 08:15.640
Tumel talk number seven, in fact.

08:15.640 --> 08:19.400
And I'm curious how you came to know and found the company with Carlos.

08:19.400 --> 08:20.720
Yeah, Carlos is awesome.

08:20.720 --> 08:22.640
So he was my thesis author.

08:22.640 --> 08:23.640
Oh, okay.

08:23.640 --> 08:28.440
When I came to CMU, Carlos was the guy who said, let's not flip helicopters, let's do

08:28.440 --> 08:31.520
something that could make an impact in the world.

08:31.520 --> 08:32.520
He was a great advisor.

08:32.520 --> 08:36.560
He pushed me down the right path in my PhD, the thing that reflected what I was interested

08:36.560 --> 08:37.560
in.

08:37.560 --> 08:41.680
And he was one of the pioneers in the modern field of machine learning and systems.

08:41.680 --> 08:42.680
Yeah.

08:42.680 --> 08:43.680
So that's how I came to know.

08:43.680 --> 08:44.680
He did go to Apple.

08:44.680 --> 08:45.680
He did.

08:45.680 --> 08:50.840
He saw him recently at Nurebs, most recently in Vancouver.

08:50.840 --> 08:52.920
It seems to be really having a good time there.

08:52.920 --> 08:57.040
Yeah, he's had a chance to have a lot of impact doing really cool stuff.

08:57.040 --> 09:02.280
You kind of laid out this broad space of research.

09:02.280 --> 09:05.240
It sounds very broad, actually, tied together by systems.

09:05.240 --> 09:09.800
I'm curious how you kind of, you know, is it rationalized by, hey, you've got a bunch

09:09.800 --> 09:14.040
of, you know, students and you're letting them flip helicopters in the way that they want

09:14.040 --> 09:20.120
to flip helicopters more so than you were, you know, and so it's you or it's challenging

09:20.120 --> 09:23.560
as faculty to decide what is your research agenda.

09:23.560 --> 09:26.880
One likes to imagine you sit there and go, here are the three two things.

09:26.880 --> 09:30.960
I want to study, usually not one, because you have to have enough for a couple of students

09:30.960 --> 09:33.400
to build, you know, their thesis around.

09:33.400 --> 09:37.840
The reality is that students pull you and I actually, I think sort of like artists, it's

09:37.840 --> 09:41.760
hard to compel people to follow the research agenda that you ultimately want.

09:41.760 --> 09:43.160
My advisor did a great job.

09:43.160 --> 09:44.760
It's not about telling you what to do.

09:44.760 --> 09:48.200
It's about showing you the exciting opportunities you can explore.

09:48.200 --> 09:53.800
And so with my students, I've pushed them in directions to think about how we make models

09:53.800 --> 10:01.200
more efficient to not just train, but to serve, how we support new emerging applications

10:01.200 --> 10:06.760
of machine learning that might require innovation both in the system and the modeling techniques.

10:06.760 --> 10:11.160
And actually what's kind of neat about the field of systems and machine learning is, again,

10:11.160 --> 10:12.960
when I started, it wasn't really a thing.

10:12.960 --> 10:17.080
In fact, some of my colleagues at CMU were like, you're just hacking, you're not actually

10:17.080 --> 10:20.800
doing research, you're not proving anything fundamental about machine learning, you're

10:20.800 --> 10:23.960
writing software, a little bit of that was true.

10:23.960 --> 10:25.600
We were definitely writing a lot of software.

10:25.600 --> 10:29.400
We were trying to prove some stuff too, but I think the impact might have actually been

10:29.400 --> 10:30.960
more on the software side.

10:30.960 --> 10:34.800
And one of the funny things about the broader field of systems and machine learning is

10:34.800 --> 10:40.520
that it actually has been kind of the undercurrent of a lot of the recent progress in AI.

10:40.520 --> 10:48.360
When we look at this revolution in deep learning, we can go back to the 2012, the Alex Net paper.

10:48.360 --> 10:51.720
That's actually not the beginning, it goes way back to the 1980s.

10:51.720 --> 10:56.320
In fact, the techniques are from the 1980s, the architectures, the models, even the algorithms

10:56.320 --> 10:58.080
that we're using are from the 1980s.

10:58.080 --> 11:02.400
If you actually read the Alex Net paper, more than half the papers devoted to how they got

11:02.400 --> 11:06.720
it to run on a GPU, how they got it to run on a very large image data set, and some of

11:06.720 --> 11:10.960
the optimizations they made to the training process to make it run at scale.

11:10.960 --> 11:15.200
So it is the movement to scale that really helped launch the revolution that we are on

11:15.200 --> 11:16.200
today.

11:16.200 --> 11:20.400
Now there's the other factor, which I think people overlook, and it's sort of when I was

11:20.400 --> 11:25.000
doing my PhD, we were writing the foretran of machine learning, we were writing MATLAB

11:25.000 --> 11:29.400
code to implement algorithms and debugging gradient procedures, and that's absurd.

11:29.400 --> 11:31.440
Today, it's just too easy.

11:31.440 --> 11:36.640
So a graduate student can pick up PyTorch, TensorFlow, MX Net, one of these packages,

11:36.640 --> 11:42.520
and very easily architect a new model, and train it on TPUs, GPUs, how do they barely

11:42.520 --> 11:47.840
understand, and get it to run at scale on data sets that they don't have to collect.

11:47.840 --> 11:50.480
So that is an enormous jump forward.

11:50.480 --> 11:54.600
And if you look really carefully and a little bit depressingly, the models didn't change

11:54.600 --> 11:55.600
that radically.

11:55.600 --> 11:57.320
The algorithms didn't change that radically.

11:57.320 --> 11:59.760
What changed was it became a lot easier.

11:59.760 --> 12:04.200
We developed the languages, the tools to make machine learning practical, and it really

12:04.200 --> 12:06.320
boiled down to getting the right abstractions.

12:06.320 --> 12:10.640
And maybe if you roll all the way back when Alex Net came out, they didn't quite have

12:10.640 --> 12:11.640
that.

12:11.640 --> 12:12.640
Alex Net came out.

12:12.640 --> 12:17.320
Theanos started to really take off, cafe at Berkeley started to take off, and it became

12:17.320 --> 12:20.760
so much easier to build that next model and the next model, and so on.

12:20.760 --> 12:24.680
And today we're stuck into a flood of archive papers because basically anyone can download

12:24.680 --> 12:28.880
one of these packages and start building state-of-the-art machine learning models.

12:28.880 --> 12:32.360
There's some learning that you go into the process, but the fundamental principles are

12:32.360 --> 12:37.720
to find your objective, to find your decision process, and then tune your decision process

12:37.720 --> 12:40.960
optimize it for that objective.

12:40.960 --> 12:44.440
That's it, and the undercurrent that drive all of this has been a lot of the innovation

12:44.440 --> 12:47.720
in the systems front, not necessarily the machine learning.

12:47.720 --> 12:52.520
And so my research is trying to find those right abstractions, and especially as we look

12:52.520 --> 12:56.000
at new frontiers, not just training models, but how we deliver them, how we support them

12:56.000 --> 13:00.000
and autonomous driving, and how we adjust the architectures of the models themselves

13:00.000 --> 13:02.960
to make them more efficient in these new kinds of applications.

13:02.960 --> 13:07.480
At when I first started doing these interviews, one of my favorite questions was looking

13:07.480 --> 13:16.400
to explore the way folks came up with new models and trying to find the science behind

13:16.400 --> 13:21.440
it, and I think that the takeaways where a lot of it was, the answer was like graduate

13:21.440 --> 13:22.440
student descent.

13:22.440 --> 13:28.240
We would just throw a graduate student at this, and they tweak something that pre-existed,

13:28.240 --> 13:36.040
but there wasn't necessarily a hard science behind how to come up with a new model architecture.

13:36.040 --> 13:40.360
But so we've seen a lot of innovation like and around, you know, Bert and the kinds

13:40.360 --> 13:47.280
of transformer models that we're seeing here, you know, has that, has, you know, would

13:47.280 --> 13:52.040
your answer to that be kind of similar, has it, you know, it changed a lot, or how do

13:52.040 --> 13:57.040
you think of, you know, beyond kind of that high level process, you just laid out?

13:57.040 --> 14:00.720
How do you think of the process for coming up with these new types of architectures?

14:00.720 --> 14:02.920
Yeah, so that's been a struggle for me.

14:02.920 --> 14:07.440
So remember, I start with this religion, this Bayesian philosophy of model development,

14:07.440 --> 14:12.200
they have these principles of priors and likelihoods that gave us at least the basic foundations

14:12.200 --> 14:14.840
of what to think about when building a model.

14:14.840 --> 14:18.960
That's all, that's not gone, but that's, you know, effectively gone for a lot of the

14:18.960 --> 14:19.960
machine learning world.

14:19.960 --> 14:22.240
And so we're left with a lot of the new, though, actually, right?

14:22.240 --> 14:25.880
Like the concept of modeling is on the rise, I think.

14:25.880 --> 14:29.480
So I should say it's not gone, and it's very important to note that a lot of the world

14:29.480 --> 14:34.120
actually runs on these more traditional techniques. It's the research community where we're writing

14:34.120 --> 14:38.680
these new papers for language modeling or speech or driving where there's very specific

14:38.680 --> 14:43.200
cases that have been kind of dominated by deep learning techniques.

14:43.200 --> 14:48.960
But the Bayesian methods are still, you know, fully alive and medicine and even traditional

14:48.960 --> 14:51.280
advertising algorithms.

14:51.280 --> 14:55.400
But with that in mind, so when I start to look at the deep learning work, how do I find

14:55.400 --> 14:56.800
those principles?

14:56.800 --> 15:00.440
And actually, they actually exist. They're a little smaller.

15:00.440 --> 15:04.560
And sadly, we start to embody the model with personality, like the model is trying to

15:04.560 --> 15:08.600
do X, which is sad because that's not how we like to, you know, formally think of things.

15:08.600 --> 15:15.160
But these building blocks, convolution, recurrence, attention, each of these becomes tools that

15:15.160 --> 15:18.720
we can use to architect our models.

15:18.720 --> 15:22.160
When students go, how deep should we make it while we try to go a little deeper?

15:22.160 --> 15:28.600
They start to look at how it affects convergence rates, variation in batch size and its relation

15:28.600 --> 15:29.600
to batch form.

15:29.600 --> 15:31.880
So we have little rules of thumb.

15:31.880 --> 15:36.680
And unfortunately, there's no great, like, my PhD students like two or three years to get

15:36.680 --> 15:39.840
up to speed with the rules of thumb in the area that they're working in.

15:39.840 --> 15:43.000
And once they have that, I hope they teach the next PhD students and so on because it's

15:43.000 --> 15:47.080
hard to really grasp what those are. It's more like, it comes from experience working with

15:47.080 --> 15:48.720
these models and going, ah, right?

15:48.720 --> 15:52.840
So like the transformer, in this particular piece of work that we've been exploring, like

15:52.840 --> 15:55.760
how to make it more efficient, we're like, should we make it deeper, should we make it

15:55.760 --> 15:56.760
wider?

15:56.760 --> 15:57.760
Who knows?

15:57.760 --> 15:58.760
So we start to measure each of these things.

15:58.760 --> 16:02.360
And that's one of the jokes that we make is that machine learnings become more like

16:02.360 --> 16:03.600
a biological science.

16:03.600 --> 16:08.240
It's driven by laboratory experiments, ah, by using compute to understand better the

16:08.240 --> 16:12.520
models that we're building, ah, as opposed to the more, ah, principled approach we might

16:12.520 --> 16:13.520
have had in the past.

16:13.520 --> 16:16.680
We tried to, to frame it in some probabilistic architecture.

16:16.680 --> 16:21.680
You mentioned that there was a story behind the, the work that led to train large than

16:21.680 --> 16:22.680
compress.

16:22.680 --> 16:23.680
Yeah.

16:23.680 --> 16:25.320
So I'm happy to go into that story.

16:25.320 --> 16:26.320
Sure.

16:26.320 --> 16:27.320
Sure.

16:27.320 --> 16:28.320
Yeah.

16:28.320 --> 16:32.320
So the story behind that the train large and compress work, ah, it starts in the following.

16:32.320 --> 16:36.640
So, ah, we've been doing a lot of work in how to make, ah, computer vision models more

16:36.640 --> 16:40.240
efficient, ah, in particular, not for training, but for inference.

16:40.240 --> 16:45.080
Ah, and so we have these skip net architectures, ways of making things more dynamic.

16:45.080 --> 16:47.800
And some of my colleagues go, you know what, maybe we should start thinking about language

16:47.800 --> 16:48.800
models.

16:48.800 --> 16:50.600
They seem to be eating up a lot of cycles.

16:50.600 --> 16:54.840
The, the transformer, the BERT model that's become a kind of a foundation for reason about

16:54.840 --> 16:55.840
text and context.

16:55.840 --> 16:59.040
Well, so that, that model is pretty expensive to run.

16:59.040 --> 17:03.280
Ah, and so we said, all right, maybe we'll explore what we can do in, in the context of

17:03.280 --> 17:05.440
making these BERT models more efficient.

17:05.440 --> 17:09.040
Now, I should say a lot of people are starting to think about that because BERT is, you know,

17:09.040 --> 17:13.840
incredibly expensive to run, ah, on, on text feeds and text is a pretty large, ah, you

17:13.840 --> 17:15.920
know, body of data that we might want to process.

17:15.920 --> 17:20.320
Yeah, I'll mention that, ah, for folks that want to dig into that particular point, I

17:20.320 --> 17:26.200
did an interview with Emma Strubel, ah, who has, um, you know, in, in fair amount of

17:26.200 --> 17:30.440
detail, kind of characterized the, both the cost and kind of environmental impact of

17:30.440 --> 17:33.360
training some of these large scale NLP models.

17:33.360 --> 17:34.360
And it is crazy.

17:34.360 --> 17:38.560
It's, it's crazy, actually, the, the CO2, ah, as narrative was one of the things that

17:38.560 --> 17:42.120
got me, especially, like, I was, ah, maybe we don't touch language, that's, that's

17:42.120 --> 17:45.960
there's plenty of people thinking about it, and then I, I saw Emma's papers, like, wow,

17:45.960 --> 17:49.400
ah, I'm here trying to make autonomous cars so that, you know, a little bit more environmentally

17:49.400 --> 17:52.840
friendly when it comes to driving, when I could go fix a, you know, fundamental prime

17:52.840 --> 17:57.320
right in my field, ah, and so, yeah, so we look at these language models and go, how

17:57.320 --> 18:01.240
can we make these better, ah, and the first step to doing that is we got to understand

18:01.240 --> 18:02.240
them.

18:02.240 --> 18:05.000
So we need to run some experiments, ah, and, and my students go, well, we're going

18:05.000 --> 18:07.480
to need a lot of machines, like, I can't afford a lot of machines.

18:07.480 --> 18:11.440
So if I look at Google at Facebook, they can throw a lot of compute and trying to understand

18:11.440 --> 18:15.240
something, and that's actually one of their, their tools that, that we don't really have

18:15.240 --> 18:19.120
access to, ah, we've actually, ah, started a collaboration with Google so we could get access

18:19.120 --> 18:20.200
to TPUs.

18:20.200 --> 18:23.160
We can't do it at the scale that they're going to run their experiments.

18:23.160 --> 18:24.640
So we had to get lean.

18:24.640 --> 18:29.120
So how can we rapidly iterate on variations in our architectures, ah, we want to look

18:29.120 --> 18:34.000
at different types of, of attention, different architecture sizes, understand the effects

18:34.000 --> 18:37.520
of these hyper parameters, and so my students go, ah, here's what we'll do, we'll make the

18:37.520 --> 18:41.640
models really small and we run a training run every day with different configurations

18:41.640 --> 18:45.160
and we've got a good picture of what's going on, ah, and they did that and so they made

18:45.160 --> 18:49.320
the model really small because that would, in theory, make it really fast to train, ah,

18:49.320 --> 18:53.480
but they also run really small in terms of, ah, good parameters or, yeah, so they made

18:53.480 --> 18:58.160
the model smaller in terms of both the height, ah, the number of layers and the width, the

18:58.160 --> 19:02.040
number of these hidden, ah, hidden layers, hidden parameters inside of each of the attention

19:02.040 --> 19:03.040
heads.

19:03.040 --> 19:06.840
So basically they tried to make the model so it would, ah, it would train faster because

19:06.840 --> 19:12.120
it had less to compute, ah, less to, to update, ah, this is more of a classic way of thinking,

19:12.120 --> 19:15.400
how I would approach a problem too, if it's too big, make the problem smaller, ah, it

19:15.400 --> 19:20.120
should go faster, right, it's less to do, ah, so they did that and it was working, but

19:20.120 --> 19:22.560
one of them was like, well, what if we make it a little bigger just, you know, to get

19:22.560 --> 19:26.760
a point of comparison and they applied the point of comparison on top of the, you know,

19:26.760 --> 19:30.360
the, the smaller models they were training and, and the point of comparison seemed to go

19:30.360 --> 19:33.800
down pretty quickly and I say, well, let's put it in time and you put it in time and

19:33.800 --> 19:39.120
actually the bigger model, the point of comparison was actually getting to a better accuracy quicker

19:39.120 --> 19:43.080
than the smaller models that we were supposed to be running because they're faster to train,

19:43.080 --> 19:46.520
ah, and then we started to wonder, hm, maybe it's the other way around, maybe we had

19:46.520 --> 19:50.080
this backwards all along and if we want to go faster we have to make the problem bigger,

19:50.080 --> 19:54.840
ah, which is really counterintuitive, but it actually turns out to be a really neat intersection

19:54.840 --> 20:00.200
between, ah, how these models converge and how they can take advantage of parallel resources

20:00.200 --> 20:05.120
in the GPUs and TPUs, ah, to get good scaling as you make them bigger, ah, and that sort

20:05.120 --> 20:08.600
of forms the foundation of, of this work that sort of went against what we thought would

20:08.600 --> 20:13.560
be the case, ah, and actually presents a neat way to approach training, ah, these expensive

20:13.560 --> 20:14.560
models.

20:14.560 --> 20:23.640
It is the idea related to the, kind of the rate of change of, ah, of kind of accuracy

20:23.640 --> 20:29.440
for these models and, you know, taking advantage of the idea that the larger models learn

20:29.440 --> 20:37.040
quicker, ah, but the, you know, I guess the, the area under that learning curve is proportional

20:37.040 --> 20:39.800
to your compute cost and you cannot kind of optimize that.

20:39.800 --> 20:41.000
Yeah, so there's a bunch of trade-offs.

20:41.000 --> 20:43.960
Let me, I try to walk through them because they were counterintuitive to me at first

20:43.960 --> 20:44.960
too.

20:44.960 --> 20:48.600
So the first trade-off to think about is, ah, actually let's talk about compute.

20:48.600 --> 20:52.320
So as I make my model bigger, I'm going to compute more, so it's more work and it should

20:52.320 --> 20:53.320
run slower.

20:53.320 --> 20:57.320
But the neat thing is that when I make these models wider at least, ah, I actually expose

20:57.320 --> 21:01.840
more parallelism, and if we look at the execution of these models, it's a little surprising.

21:01.840 --> 21:06.760
We've optimized those, those GPUs and TPUs to have a substantial amount of compute, often

21:06.760 --> 21:11.480
for computer vision models, ah, and so now we have an opportunity when we run a transformer.

21:11.480 --> 21:16.200
If we don't crank the batch, batch size up incredibly high, we actually have a fair amount

21:16.200 --> 21:17.960
of leftover compute that we can use.

21:17.960 --> 21:22.880
So making the model bigger doesn't necessarily translate to a linear increase in runtime.

21:22.880 --> 21:27.800
So we can afford to make the models bigger without, ah, linearly increasing execution.

21:27.800 --> 21:34.640
Parallelism and runtime don't correlate to cost because you're just running more compute

21:34.640 --> 21:35.640
at the same time.

21:35.640 --> 21:36.640
Yeah.

21:36.640 --> 21:38.200
So, ah, this is looking at a single CPU.

21:38.200 --> 21:39.480
Or carbon for that matter.

21:39.480 --> 21:40.480
Yeah.

21:40.480 --> 21:41.480
So, all right.

21:41.480 --> 21:42.480
This is, you're getting to the interesting stuff.

21:42.480 --> 21:45.480
So, so first, let's, ah, in the paper, we actually tried to control for this, because

21:45.480 --> 21:48.960
I was like, ah, on a second, you're, you're just going to increase the cost of compute.

21:48.960 --> 21:52.600
So we looked at one GPU, and what happens is you're not using all the cores on one

21:52.600 --> 21:54.640
GPU when we were looking at the smaller models.

21:54.640 --> 22:00.360
So as we make the models bigger, ah, for a fixed batch size, ah, we can get, ah, an increase

22:00.360 --> 22:02.200
in the utilization of the GPU.

22:02.200 --> 22:06.080
Um, and right now it's not easy to turn off those cores, and you're also paying a fixed

22:06.080 --> 22:10.480
overhead to power the, the actual box that the cores are living in, plus cooling.

22:10.480 --> 22:15.240
So trying to power throttle individual cores on GPUs, generally not a great idea, ah, especially

22:15.240 --> 22:19.120
if we can get better utilization of the cores that we have, ah, now you could say I should,

22:19.120 --> 22:20.120
I should have more GPUs.

22:20.120 --> 22:21.120
We did.

22:21.120 --> 22:26.160
We're going to burn more resources as we turn on more GPUs, ah, but the hope is that

22:26.160 --> 22:27.760
we can get to a solution quicker.

22:27.760 --> 22:32.520
And if, and if those GPUs are already attached to our box, which they often are, there's

22:32.520 --> 22:36.600
usually some incentive to go ahead and try to use those as efficiently as possible.

22:36.600 --> 22:41.400
Um, and so that brings us to the second question, which is if I make my model bigger, is it

22:41.400 --> 22:45.680
really improving any efficiency, which is what we'd like to think of as the improvement

22:45.680 --> 22:50.440
in our, our perplexity, our reduction in perplexity as we, ah, train.

22:50.440 --> 22:55.320
And so we'd like to, to reduce our errors quickly as possible, um, in, in walk clock time

22:55.320 --> 22:57.720
because we have to pay for the power of the building and so on.

22:57.720 --> 23:03.120
Um, so we want to, to train as fast as possible in time, the simpler way to look at that

23:03.120 --> 23:07.480
first is how is it, ah, how is he perplexing or error decreasing as a function of the amount

23:07.480 --> 23:09.120
of data that we touch?

23:09.120 --> 23:11.200
Um, and so there are two knobs there.

23:11.200 --> 23:14.840
So now we're getting to the, the weeds, but there's the batch size, which determines how

23:14.840 --> 23:19.880
much data we look at per step of our algorithm, um, the more data we look at, the better of

23:19.880 --> 23:25.280
an estimate of the direction that minimizes the loss, uh, which in principle should give

23:25.280 --> 23:29.960
us faster convergence, um, it also increases GPU utilization.

23:29.960 --> 23:33.840
So we can use that as another mechanism to get better, uh, utilization out of each of

23:33.840 --> 23:37.640
our piece of hardware, um, but it also has diminishing returns.

23:37.640 --> 23:42.240
So as we increase the batch size, our, our speed at which we're able to converge as a function

23:42.240 --> 23:47.360
of samples we look at, um, doesn't necessarily increase linearly, uh, and one of the other

23:47.360 --> 23:50.760
sort of side effects of this, which if you work in computer vision, you're like, oh,

23:50.760 --> 23:56.200
no, there's a problem, um, that as we increase the batch size, there's some risk of overfitting.

23:56.200 --> 24:00.600
Um, and this is a fact that shows up more in computer vision models where, where it's somewhat

24:00.600 --> 24:06.520
data poor, where in, uh, NLP, it seems at this point, at least that we have opportunities

24:06.520 --> 24:10.760
to, uh, overfit more before we actually are properly overfitting.

24:10.760 --> 24:15.000
So, uh, there's this question of the generalization gap, the gap between how well your models

24:15.000 --> 24:17.680
fitting the training data and the test data.

24:17.680 --> 24:23.280
And in NLP tasks, we're not at a point where we're, that generalization gap is disappearing,

24:23.280 --> 24:27.520
which means that we can increase the batch size quite a bit more, um, without overfitting,

24:27.520 --> 24:30.960
but it also means we can increase the model size quite a bit more.

24:30.960 --> 24:36.000
And so what this paper then does is tries to play off, uh, compare this trade off between

24:36.000 --> 24:39.480
model size and batch size to find the best combination.

24:39.480 --> 24:43.440
And one of the neat results we find is it actually cranking up the model size and a batch

24:43.440 --> 24:47.840
size, uh, to a certain extent as well, kind of gives us the best outcome, uh, it gets

24:47.840 --> 24:52.440
us to a, a model that's more sample efficient, the more samples it sees, the faster it reduces

24:52.440 --> 24:57.800
the, the test air, uh, and it also lets us better utilize our hardware, so we're actually

24:57.800 --> 25:00.240
getting, uh, a gain from parallelism.

25:00.240 --> 25:06.120
And those two forces interact to give us a faster in terms of walk lock time, um, reduction

25:06.120 --> 25:10.000
in the test perplexity or the, the, the air metric that we care about.

25:10.000 --> 25:15.320
In terms of this, uh, generalization gap and the, the differences between it, what you

25:15.320 --> 25:21.920
see in computer vision and, uh, what you see in NLP tasks is that related to the way

25:21.920 --> 25:29.200
the problems are formulated in terms of, uh, supervised versus self-supervised semi-supervised

25:29.200 --> 25:33.680
and the kind of availability of data and labels and that kind of thing.

25:33.680 --> 25:34.680
Absolutely.

25:34.680 --> 25:36.320
So, uh, you're hitting a key point.

25:36.320 --> 25:40.120
So in computer vision, we are largely still focused on supervised tasks.

25:40.120 --> 25:44.840
We need labeled data sets, which, uh, are big, but they're not, uh, as big as we want

25:44.840 --> 25:45.840
them to be.

25:45.840 --> 25:51.560
Uh, whereas in NLP, we can go to really large, uh, unlabeled data sets, uh, cause we're

25:51.560 --> 25:53.240
essentially predicting missing words.

25:53.240 --> 25:57.160
So we've created this self-supervised task, um, and that means that we have so much more

25:57.160 --> 26:02.240
data, we can support bigger models and bigger batch sizes without having this, this generalization

26:02.240 --> 26:03.240
gap disappear.

26:03.240 --> 26:07.560
Uh, we're able to, uh, sorry, without, uh, eliminating, uh, or causing our, our training

26:07.560 --> 26:13.080
error to, um, go to zero and our test error to, to, you know, dominate, uh, so, so there

26:13.080 --> 26:18.280
is, uh, this, this opening created by this, uh, self-supervised training that, that we're

26:18.280 --> 26:19.600
able to take advantage of.

26:19.600 --> 26:23.280
Now, in our experiments, we test both the self-supervised training task as well as the

26:23.280 --> 26:26.840
downstream, uh, translation or, or classification tasks.

26:26.840 --> 26:31.840
It would be applied to actual language modeling, you know, supervised training tasks, but that's

26:31.840 --> 26:36.600
typically done as a small fine tuning step on top of, uh, this extensive pre-training,

26:36.600 --> 26:40.160
which is where all the, the CO2 is going, uh, to pre-training these models.

26:40.160 --> 26:46.640
Uh, and then in, in your description of the, the train large, it, it sounded a little

26:46.640 --> 26:51.800
bit like you're ultimately saying, uh, you know, fully utilized whatever box you're

26:51.800 --> 26:54.920
training on, but there's a lot more nuance there.

26:54.920 --> 26:58.000
I am, yeah, elaborate on, on that.

26:58.000 --> 27:02.040
So this has been a big question in data center design, generally, as a systems person,

27:02.040 --> 27:06.440
like, should I turn off my GPUs, should I turn off my CPUs, uh, or should I try to utilize

27:06.440 --> 27:07.440
them better?

27:07.440 --> 27:12.280
Um, and the general consensus when we think about data centers, uh, is that we really

27:12.280 --> 27:14.920
do want to try to maximize our utilization.

27:14.920 --> 27:18.240
Part because we bought the data center, it's expensive, we should use it, uh, we have

27:18.240 --> 27:20.120
to keep it cool, we have to staff it.

27:20.120 --> 27:24.560
There's a lot of other factors that go into play, uh, and so we want to be able to use

27:24.560 --> 27:28.240
that hardware as much as possible and as efficiently as possible.

27:28.240 --> 27:31.520
Um, and part of the reason we might want to use it as efficiently as possible, you think

27:31.520 --> 27:35.200
of things like serverless, uh, if I'm not using the hardware, I can put something else

27:35.200 --> 27:39.600
there, uh, and we're creating markets for filling in the excess capacity.

27:39.600 --> 27:42.760
So the idea that I would turn off a GPU is sort of silly, I should always have something

27:42.760 --> 27:43.760
running.

27:43.760 --> 27:47.760
Um, now the question is, can I make that thing running on the GPU as efficient as possible?

27:47.760 --> 27:52.480
Uh, and so in our work, we're focusing on trying to maximize that efficiency, uh, in

27:52.480 --> 27:56.440
my lab, for example, students are competing for the GPUs, we, the one GPU experiment is

27:56.440 --> 27:59.960
definitely easier to run because they're not fighting for the entire box.

27:59.960 --> 28:04.240
Um, and so the other GPUs are being used for other experiments, uh, and then when we

28:04.240 --> 28:07.640
go to, to, you know, a GPUs, we're going to again use the whole box.

28:07.640 --> 28:11.840
So the general consensus or at least the thought process today, when we think about the data

28:11.840 --> 28:17.280
centers to really maximize our utilization and not try to, uh, power throttle, um, or

28:17.280 --> 28:19.920
limit the performance of each of the, the course.

28:19.920 --> 28:24.040
So it could be in the future and new kinds of hardware might change that trade off, um,

28:24.040 --> 28:27.360
but the underlying economics would sort of suggest that if you bought the device, you should

28:27.360 --> 28:29.920
really try to find ways to maximize its usage.

28:29.920 --> 28:34.120
And given machine learning has an infinite supply of things that we'd like to train, um,

28:34.120 --> 28:38.040
it's not hard to imagine that I can always fill my excess capacity with more training.

28:38.040 --> 28:44.080
So is the paper fundamentally an economics paper in the sense of, you know, you're trying

28:44.080 --> 28:49.640
to maximize utilization and those kind of things, or do you also get to results that talk

28:49.640 --> 28:54.840
about performance given a set of constraints, like your traditional computer science,

28:54.840 --> 28:56.000
he kinds of papers?

28:56.000 --> 28:57.000
Yeah.

28:57.000 --> 28:58.000
So it's, it's funny.

28:58.000 --> 28:59.000
We hadn't gone down the economics route.

28:59.000 --> 29:00.000
So it's, it's a funny.

29:00.000 --> 29:01.000
I mean, very loosely.

29:01.000 --> 29:02.000
Yeah.

29:02.000 --> 29:05.680
Well, so we, we are actually very much thinking about the economics of computing.

29:05.680 --> 29:09.440
When we look at serverless, that is going to fundamentally change our economics computing

29:09.440 --> 29:13.320
in a way that I think will make things more efficient, more cost effective, um, and

29:13.320 --> 29:14.320
actually easier.

29:14.320 --> 29:18.560
So it's, it's a win for everyone and we actually have a, uh, upcoming paper, um, on

29:18.560 --> 29:22.320
this at a hot cloud about, you know, the economics of serverless are going to generally be favorable

29:22.320 --> 29:26.320
for everyone, um, assuming we get some of the systems problems, uh, you know, ironed

29:26.320 --> 29:31.240
out, uh, this paper was really our students as in, you know, a first effort to really make

29:31.240 --> 29:36.080
progress in the BERT-P training space to find mechanisms that we in academia can use to

29:36.080 --> 29:37.080
go fast.

29:37.080 --> 29:40.840
Uh, and part of that is finding better ways to be more efficient about training.

29:40.840 --> 29:45.400
Um, it allows us to run experiments more quickly and so we can now innovate on BERT.

29:45.400 --> 29:48.680
And one of the things we're actually looking at is, uh, trying to make these models more

29:48.680 --> 29:52.360
non-parametric so they can leverage other data sources.

29:52.360 --> 29:56.240
One of the side consequences of this paper is sort of, uh, you know, if you're out there

29:56.240 --> 29:58.000
thinking about, oh, I should, that's really cool.

29:58.000 --> 30:00.400
I want to play, uh, to play that, but hey, wait a second.

30:00.400 --> 30:04.120
You made the model four or five X bigger, six, seven X bigger.

30:04.120 --> 30:05.120
That's six.

30:05.120 --> 30:06.120
Bensive for inference.

30:06.120 --> 30:07.600
Uh, what am I going to do about that?

30:07.600 --> 30:11.200
Um, and in fact, when we got this result, that was like my first conclusion, yay, we went

30:11.200 --> 30:14.600
on the training front, but we just made inference, which is actually the more expensive

30:14.600 --> 30:20.520
problem, uh, worse by seven, 10 X, uh, and if you think about it, training should only

30:20.520 --> 30:25.000
be a small part of the use of these models, uh, inference is where it really the cost

30:25.000 --> 30:26.000
should be.

30:26.000 --> 30:29.200
And it is when you look at a practical application, we might train it, but we're going to

30:29.200 --> 30:33.720
run that model 24 seven at every single tweet, every single web page that we encounter.

30:33.720 --> 30:35.440
That's a lot of inference.

30:35.440 --> 30:40.760
Um, and if you 100, so you're doing what a thousand, uh, when batch, the optimized, a thousand

30:40.760 --> 30:43.560
sentences per second, uh, which sounds good.

30:43.560 --> 30:47.880
But then you think of the amount of text in the web, that's a lot of expensive GPU hardware.

30:47.880 --> 30:53.080
Um, so making the models smaller after training was one of the questions that we had to solve.

30:53.080 --> 30:55.840
And so this is the second half of this paper comes back and goes, wait a second, so even

30:55.840 --> 30:59.600
the model is bigger to train faster, but now we need a way to squeeze them down.

30:59.600 --> 31:03.600
And maybe actually the bigger insight, which is also maybe a little less counterintuitive,

31:03.600 --> 31:08.560
is that the bigger models, we could actually make smaller more efficiently and actually,

31:08.560 --> 31:11.560
uh, with, with less of a degradation in accuracy.

31:11.560 --> 31:14.400
So we make a, we train a really large model and then we chop it up.

31:14.400 --> 31:18.520
So we, we both explored weight pruning, so eliminating weights, making the model more

31:18.520 --> 31:23.680
sparse, uh, and quantization, reducing the bit precision of each of the, uh, the weights.

31:23.680 --> 31:29.240
And so we are able to take our much larger models and then apply these, these, uh, compression

31:29.240 --> 31:31.040
techniques to make them smaller.

31:31.040 --> 31:35.400
And the, the effect of that is we can make the model actually smaller than the small models,

31:35.400 --> 31:38.040
uh, while retaining higher accuracy.

31:38.040 --> 31:39.040
And so that's something that we're still.

31:39.040 --> 31:43.120
So were you able to use the compression techniques off the shelf, or did you have to adapt

31:43.120 --> 31:48.280
them to this kind of model or, um, the specifics of the way you train them?

31:48.280 --> 31:49.280
Yeah.

31:49.280 --> 31:53.760
So, uh, getting close to the deadline, realizing our models are now 10x bigger, we're like,

31:53.760 --> 31:55.960
right, so how do we quickly figure out a compressed these?

31:55.960 --> 31:56.960
Good news.

31:56.960 --> 32:00.120
One of the students, Shen, uh, who's, uh, working on this project had just finished

32:00.120 --> 32:04.080
to work on quantization, uh, and so we're like, right, Shen, can we use your quantization

32:04.080 --> 32:05.080
technique?

32:05.080 --> 32:06.080
I don't know, maybe.

32:06.080 --> 32:10.160
We started playing math and it turns out that, it worked really well, uh, and so we looked

32:10.160 --> 32:12.560
at the standard quantization and standard pruning.

32:12.560 --> 32:16.280
So we tried not to innovate extensively in each of these pieces, more of an exploration

32:16.280 --> 32:20.720
how they interact with this kind of counter intuitive hypothesis that bigger models might

32:20.720 --> 32:24.760
actually be better, both for training and it turns out for inference as well, if we compress

32:24.760 --> 32:25.760
it.

32:25.760 --> 32:26.760
Got it, got it.

32:26.760 --> 32:29.760
So not that you have any of these numbers like right at your fingertips, but can you give

32:29.760 --> 32:34.440
us a sense for when you say train large, like what large means in this case and how that

32:34.440 --> 32:37.200
compares to what a Google might do typically?

32:37.200 --> 32:38.200
Yeah.

32:38.200 --> 32:41.880
So I think we were looking at like six X seven X bigger than was normally, was normally

32:41.880 --> 32:42.880
published.

32:42.880 --> 32:46.800
Uh, I'm guessing Google actually goes much larger still and they might already be benefiting

32:46.800 --> 32:48.200
from these ideas.

32:48.200 --> 32:53.480
And what, what order of magnitude is that in terms of number of, you know, servers or

32:53.480 --> 32:58.840
CPUs or GPUs or so we were at eight, uh, GPUs, we actually also ran experiments on a

32:58.840 --> 33:05.680
GPU, uh, V three TPU as well, uh, I'm trying to remember the exact sizes, I have a paper

33:05.680 --> 33:07.360
in front of me if I can find it.

33:07.360 --> 33:08.360
Many tabs.

33:08.360 --> 33:09.360
Uh, yeah.

33:09.360 --> 33:14.360
So I think we were at, we were up to like 20 hidden layers or so, uh, our 24 went up

33:14.360 --> 33:20.160
24 layers, um, and we tried, uh, hidden sizes of, you know, like the order of, uh, 15,

33:20.160 --> 33:25.320
36, so 1,536 hidden units, uh, for each of the layers.

33:25.320 --> 33:29.480
So we, we tried a pretty reasonable space, um, we, we built off off the Roberto work,

33:29.480 --> 33:32.960
which is actually if people haven't looked at, it's kind of neat, uh, sort of revisiting

33:32.960 --> 33:37.120
what Bert did, uh, and in some ways, Bert really had the right answer, just this broader

33:37.120 --> 33:40.080
experimentation of the, the trade off space makes a big difference.

33:40.080 --> 33:44.560
Um, so we built off of that and tried different variations on the sizes described in that paper.

33:44.560 --> 33:45.560
Yeah.

33:45.560 --> 33:49.480
The kind of rough magnitudes that I remember reading about and I don't remember if this

33:49.480 --> 33:55.480
was, you know, Bert or Almore, some of the different variations or, or, but there was,

33:55.480 --> 34:02.040
you know, on the order of like 64, you know, tens of GPUs for, you know, yeah, or more.

34:02.040 --> 34:03.040
Yeah.

34:03.040 --> 34:06.240
No, so, so we went, uh, we used a TPU cluster for our big experiments.

34:06.240 --> 34:10.480
So we actually tried to reproduce the Roberta setup, uh, so our, our comparisons are compared

34:10.480 --> 34:11.760
to these standard baselines.

34:11.760 --> 34:16.040
Uh, so we had to use a TPU cluster for several weeks, it's expensive to run the full experiment

34:16.040 --> 34:17.040
for the baselines.

34:17.040 --> 34:20.400
Um, but what was needed is by making the model bigger, we could get comparable results

34:20.400 --> 34:21.400
quicker.

34:21.400 --> 34:22.400
Yeah.

34:22.400 --> 34:23.400
Mm-hmm.

34:23.400 --> 34:27.920
And, um, so we've, we've kind of characterized bigger, now characterized quicker.

34:27.920 --> 34:30.080
What did, what did that mean in practice?

34:30.080 --> 34:36.160
So, uh, we used them, uh, I guess at about a hundred thousand, uh, seconds, are we able

34:36.160 --> 34:38.120
to get fairly competitive accuracy?

34:38.120 --> 34:40.800
Um, you should see the, the final number.

34:40.800 --> 34:45.600
So it sort of depends also on, on which tasks we, we also accounted for the, uh, downstream

34:45.600 --> 34:47.600
fine tuning that you'd need to do as well.

34:47.600 --> 34:51.840
I don't actually remember I'll top my head, uh, uh, uh, well, we'll leave it as an exercise

34:51.840 --> 34:54.480
to the listener to pull up the paper.

34:54.480 --> 35:03.440
So you've increased the size of the model and the number of resources, the CPU resource,

35:03.440 --> 35:10.560
or GPU rather resources that the model is running on and in turn decrease the, the training

35:10.560 --> 35:15.400
time of the model and the aggregate compute cost, right?

35:15.400 --> 35:20.960
Did you need to do anything special to accomplish that or was, is the paper primarily observing

35:20.960 --> 35:26.760
the fact that, you know, the, you know, the, in aggregate, you get the, the preferable

35:26.760 --> 35:30.480
approaches to increase the, uh, the model size.

35:30.480 --> 35:36.160
So, uh, we did small changes to where we placed the batch normalization, uh, we did pre-normalization,

35:36.160 --> 35:39.280
but it's like negligible changes in the underlying architecture.

35:39.280 --> 35:42.960
Most of it is really exploring this, this trade-off space, uh, these different parameters.

35:42.960 --> 35:46.360
So, uh, in some sense, it's a first step towards a bigger agenda.

35:46.360 --> 35:50.400
It wasn't intended to be like a ground, uh, changing work, but what's kind of neat is it

35:50.400 --> 35:54.040
does really make us at least rethink how we approach training of these models.

35:54.040 --> 36:00.320
Well, it's, you know, it's important stuff like, I think a lot of people, uh, will think,

36:00.320 --> 36:04.760
well, you know, if Berkeley's worrying about the cost of training these models, what,

36:04.760 --> 36:13.000
what hope is there for, you know, my lab in, you know, um, a non-Berkeley institution,

36:13.000 --> 36:20.160
you know, that has such close ties to Silicon Valley and, and relatively, awash and resources.

36:20.160 --> 36:26.240
Uh, and so if you can figure out how to make training these models more efficient, then,

36:26.240 --> 36:29.720
um, that's potentially a huge impact for a lot of people.

36:29.720 --> 36:35.560
There's a, an important, uh, sub narrative here, which is that, uh, training, the pre-training

36:35.560 --> 36:39.760
isn't something that everyone needs to do, um, and so, uh, Google has done a great job

36:39.760 --> 36:41.480
of offering these pre-trained models.

36:41.480 --> 36:45.520
So this really expensive part isn't something that every, you know, group in the world has

36:45.520 --> 36:48.160
to address, uh, and that's a good thing.

36:48.160 --> 36:52.720
Um, but if we want to innovate on that pre-training process, if you want to do research in it,

36:52.720 --> 36:57.320
or we want to, in fact, the data suggests that adding more data, uh, that's specialized

36:57.320 --> 36:59.800
to your domain can improve the quality of the model.

36:59.800 --> 37:03.840
So if we want to be able to push pre-training research forward, we do need to find ways

37:03.840 --> 37:09.000
to make it more efficient, uh, and I should say we started out with thinking, oh, we're

37:09.000 --> 37:13.160
going to invent a new bird, um, and discovered in the process and maybe, we don't necessarily

37:13.160 --> 37:16.760
need a new bird yet, but maybe approaches to how we do the training, how we choose our

37:16.760 --> 37:19.920
hyper parameters can make a really big difference.

37:19.920 --> 37:26.800
Your comment just prompted a thought to what degree has the, uh, kind of the, uh, I don't

37:26.800 --> 37:33.960
know, theoretical trade-off space around pre-training versus fine-tuning been explored.

37:33.960 --> 37:40.640
So that, you know, if I know that I have a unique domain, you know, and some, you know,

37:40.640 --> 37:48.360
corpus, uh, of documents or data available to me, you know, is there a, is there any kind

37:48.360 --> 37:55.200
of concrete research I can look to to help me understand, you know, if I should be pre-training

37:55.200 --> 38:00.680
from scratch versus fine-tuning and, or do I just need to try everything and see what

38:00.680 --> 38:01.680
works?

38:01.680 --> 38:07.040
Uh, the try everything is, is not terrible advice, but here's what I would tell my students,

38:07.040 --> 38:08.400
uh, so pre-training is expensive.

38:08.400 --> 38:12.600
So maybe start with fine-tuning, understand what, what is your prediction task, and this

38:12.600 --> 38:17.440
is what, you know, the, the practical world will do, um, so take your, your, your, uh, your

38:17.440 --> 38:21.440
prediction task, whether or not a translation or sentiment tagging, or maybe it's like, which

38:21.440 --> 38:25.800
call center, which call person should this, this, you know, message be forwarded to, um,

38:25.800 --> 38:30.240
focus on, on fine-tuning for that task first, um, there's a little bit of art in choosing

38:30.240 --> 38:34.240
learning rights and stuff to get your fine-tuning to work, so go through that process, uh, understand

38:34.240 --> 38:37.960
how well you can do, uh, by fine-tuning to your, your domain.

38:37.960 --> 38:42.680
And then if you have, and you might, you know, billions of call records from the past,

38:42.680 --> 38:47.120
you think you could really better improve the underlying representation, um, you could

38:47.120 --> 38:52.120
then try to go back to this mass-language model, uh, training, the, the pre-training process,

38:52.120 --> 38:56.280
uh, and then the work that we've done and, and, you know, other work that's, it's, it's

38:56.280 --> 39:01.200
going on around us, um, can help to make that process more efficient so that you can, in

39:01.200 --> 39:07.480
a matter of, of weeks or a week, in our case, um, take your V100 box and really make substantial

39:07.480 --> 39:12.080
progress, uh, towards, uh, pre-trained model that's now pre-trained on, on your domain

39:12.080 --> 39:13.080
as well.

39:13.080 --> 39:18.080
Um, and so where do you see this line of research leading?

39:18.080 --> 39:19.080
Yeah.

39:19.080 --> 39:23.080
So I asked my students this every year, so as we said in this group, like, what's next,

39:23.080 --> 39:24.080
guys?

39:24.080 --> 39:27.800
So we figured the original goal was to, like, be able to develop a new bird.

39:27.800 --> 39:30.240
So now we have the tools to start testing pre-training.

39:30.240 --> 39:31.240
What should we do next?

39:31.240 --> 39:35.960
Um, one of the things that I'm kind of excited about, uh, is, uh, well, the realization

39:35.960 --> 39:40.200
that we're trying to cram a lot of knowledge in the weights of our model, uh, and making

39:40.200 --> 39:43.960
the models bigger certainly helps with that, uh, another way to deal with knowledge is to

39:43.960 --> 39:45.760
not put it in the model at all.

39:45.760 --> 39:49.240
I, I actually have to look up stuff most of the time when I want to remember facts and

39:49.240 --> 39:50.320
terrible remembering facts.

39:50.320 --> 39:53.640
So I use the internet, um, I have a neural net in my head.

39:53.640 --> 39:56.400
It doesn't have to memorize the internet because I have the internet.

39:56.400 --> 39:59.920
So having access to a knowledge base can make a big difference, uh, and how much we need

39:59.920 --> 40:05.160
to encode in our model, make our model perhaps smaller, um, the ability to, uh, synthesize,

40:05.160 --> 40:08.640
uh, decisions or to apply logic on top of knowledge base.

40:08.640 --> 40:14.000
It seems like a really big opportunity for language modeling for, uh, for NLP broadly.

40:14.000 --> 40:17.360
And maybe even for these, these basic representations like Bert.

40:17.360 --> 40:20.640
And so we've been looking at, and starting to look at, and some of their groups actually

40:20.640 --> 40:25.200
have got some early published work on how to bring in a non-parametric or semi-parametric

40:25.200 --> 40:31.560
lens on, on these models so that, uh, we can reference into a large knowledge base, uh,

40:31.560 --> 40:33.880
in, in the construction of our embeddings themselves.

40:33.880 --> 40:37.400
Uh, and that has, you know, the advantage of maybe being more efficient, uh, allowing

40:37.400 --> 40:40.840
us to grow the knowledge base without having to retrain the model, uh, we could get more

40:40.840 --> 40:44.160
data, our model just gets better without having to adjust the model itself.

40:44.160 --> 40:47.360
Um, and maybe even giving us some explainability.

40:47.360 --> 40:51.960
So when we go to make a prediction about, like, how we embedded the sentence or how we represent

40:51.960 --> 40:55.920
this, you know, this decision for, you know, which called a route to, we can now actually

40:55.920 --> 41:00.280
point back at historical data and say, here's the data we use in our embedding to reason

41:00.280 --> 41:01.280
about that.

41:01.280 --> 41:02.440
And you know, that's terrible data.

41:02.440 --> 41:06.560
I don't want that in my data set or, you know, that actually, that makes sense.

41:06.560 --> 41:10.640
And so that, that connection to the data could actually also help with the explainability.

41:10.640 --> 41:14.400
So that's sort of the vision that I, that, that my students and, and I are pretty excited

41:14.400 --> 41:15.400
about right now.

41:15.400 --> 41:20.400
Does that pull from or kind of lead you to like memory based networks or like information

41:20.400 --> 41:22.400
retrieval types of problems or?

41:22.400 --> 41:23.400
Yeah.

41:23.400 --> 41:24.400
Yeah.

41:24.400 --> 41:28.040
So memory nets, I are all of these, these kind of, I say, I are more classic memory nets

41:28.040 --> 41:30.200
that also increase any more classic.

41:30.200 --> 41:32.120
Um, so, so those are our tools.

41:32.120 --> 41:35.040
Uh, one of the things we're looking at right now is something as simple as like, can

41:35.040 --> 41:39.640
we use embeddings from other piece of text and simple text similarity to, to recall those

41:39.640 --> 41:40.640
embeddings?

41:40.640 --> 41:42.880
Um, and, and there's some other work exploring this now.

41:42.880 --> 41:48.200
Uh, ultimately things like, uh, memory nets or pointer, mechanisms to point into a knowledge

41:48.200 --> 41:52.600
base and attend to an entire corpus would be really exciting and, and we're just starting

41:52.600 --> 41:53.600
to explore this.

41:53.600 --> 41:55.680
Uh, so there, there's a lot more to do.

41:55.680 --> 41:59.600
Uh, it does push us in the direction of IR, uh, imagine a neural net that can Google

41:59.600 --> 42:02.000
stuff for you, uh, to answer questions.

42:02.000 --> 42:08.320
So it's certainly, there's a, a well studied area that, that we'll have to build on.

42:08.320 --> 42:14.520
You touched on explainability in there as a possible, uh, benefit here.

42:14.520 --> 42:20.080
Uh, I'm, I'm curious about, uh, you know, maybe you're elaborating on that a little bit

42:20.080 --> 42:25.040
more and then also you've been doing some work on explainability as applied to reinforcement

42:25.040 --> 42:26.040
learning.

42:26.040 --> 42:29.760
Um, um, maybe a little, if you can share a little bit about that work as well.

42:29.760 --> 42:34.960
Yeah, so I'm the co-director of the UC Berkeley RISE lab, which stands for real time working

42:34.960 --> 42:36.640
on that intelligent working on that secure.

42:36.640 --> 42:41.240
So we have an interesting agenda on security, which is another time, um, and then the E.

42:41.240 --> 42:44.040
And then for a long time, I'm like, what does the E really, what, what, what should it

42:44.040 --> 42:45.040
mean?

42:45.040 --> 42:49.000
Uh, we were initially thinking execution, we're going to execute things securely, but

42:49.000 --> 42:51.080
that, that's actually some bad connotations.

42:51.080 --> 42:55.480
So, uh, maybe there's another agenda, which actually came out of our, our exploration

42:55.480 --> 43:00.520
lineage, uh, how we, we track relationship between data and the model development process,

43:00.520 --> 43:04.440
explainable, uh, decisions would be a good thing to be able to support.

43:04.440 --> 43:08.880
Um, and so we had an agenda around how to do X, or we have actually on going agenda,

43:08.880 --> 43:14.120
I had to, uh, explainable machine learning by connecting our training process to our data.

43:14.120 --> 43:18.200
But what's actually pretty exciting is, is pulling in some of the recent work and explainable

43:18.200 --> 43:19.200
AI.

43:19.200 --> 43:23.780
Actually, my, this advisor, Carlos has an exciting work, uh, line, which provides a black

43:23.780 --> 43:27.040
box mechanism for explaining, uh, model's decisions.

43:27.040 --> 43:31.760
Uh, so my students have been also exploring, uh, tremendous staying power.

43:31.760 --> 43:36.840
This is, you know, one of the things we talked about three plus years ago, and that, uh,

43:36.840 --> 43:42.240
went that early twinal episode and Lyme comes up all the time still.

43:42.240 --> 43:43.240
Yeah.

43:43.240 --> 43:47.040
So I, I run the risk of a, of a, another tangent, but the, the world of explainability

43:47.040 --> 43:51.720
is, it's, it's kind of rich and it is created by this need to, to make sense of models

43:51.720 --> 43:56.400
that no longer make sense, uh, and so this idea that I can inspect my model and go,

43:56.400 --> 44:01.360
hey, I like how that makes decisions, uh, that's gone, or at least, you know, to a first

44:01.360 --> 44:02.360
approximation.

44:02.360 --> 44:04.840
So we're left with justify the decision you made.

44:04.840 --> 44:08.240
Let's go back and at least connect it to the data, to even the input.

44:08.240 --> 44:12.040
Uh, so, so my group had started looking at that, and one of the things that we started

44:12.040 --> 44:14.680
to ask is, why can't we have some interpretability?

44:14.680 --> 44:19.480
Uh, and so one of the agendas that, uh, I'm exploring that's actually not in the language

44:19.480 --> 44:25.040
domain, but more in the, the vision domain, um, is how to apply decision trees to connect

44:25.040 --> 44:29.600
decision trees back with our, our, our deep learning models so that we can get the accuracy

44:29.600 --> 44:32.960
we want, but we can also go and interrogate our model and go, well, it's going to call

44:32.960 --> 44:36.240
this a cat, but in order to do that, it has to first identify that it's an animal and

44:36.240 --> 44:40.400
it has to cluster it in animals with, with legs, uh, and then, you know, with fur and

44:40.400 --> 44:41.960
then it gets us to cat.

44:41.960 --> 44:45.880
Um, so there is an opportunity to actually understand what the model is going to do at

44:45.880 --> 44:49.800
the high level, each of these decisions is governed by a neural net, so understanding

44:49.800 --> 44:54.320
that is sort of off the table, uh, but at least we now have an idea of the decision process

44:54.320 --> 44:56.760
the model will go through to make a decision.

44:56.760 --> 45:02.320
So this is our recent work on neural back decision trees, uh, when we look at language, it's

45:02.320 --> 45:05.800
been an interesting question of what, what would an explanation look like in the language

45:05.800 --> 45:06.800
domain?

45:06.800 --> 45:09.720
So there are techniques like grad cam that have been pretty popular in vision that would

45:09.720 --> 45:13.200
give us, you know, highlighting parts of an image that say, you know, this is the part

45:13.200 --> 45:14.520
of the image you're attending to.

45:14.520 --> 45:18.160
We could do that in their explorations of that in the language domain, but one of the

45:18.160 --> 45:22.520
neat things going back to our very beginning narrative is, can I connect my decisions back

45:22.520 --> 45:23.520
to data?

45:23.520 --> 45:26.080
Um, in many cases, that is sort of the ideal explanations.

45:26.080 --> 45:30.560
Like here's the data that informed my decision about what you've given me now.

45:30.560 --> 45:34.080
Um, and so, so that explanation is what we're exploring.

45:34.080 --> 45:37.480
One of the hopes in doing that is you can not only connect it, but you can even fix

45:37.480 --> 45:38.480
it.

45:38.480 --> 45:42.120
Uh, so one of the kind of ideal outcomes of an explainable model is when it gets it

45:42.120 --> 45:44.000
wrong, you go, that's wrong.

45:44.000 --> 45:45.640
Here's what is wrong about it.

45:45.640 --> 45:49.720
And that extra signal could be way more valuable than just some more labeled data.

45:49.720 --> 45:54.120
Uh, and so that's our hope and maybe being able to correct our knowledge base if we are

45:54.120 --> 45:57.360
referencing data so that we don't use that reference data in the future.

45:57.360 --> 46:00.920
That would be one mechanism in the case of decision tree changing the path.

46:00.920 --> 46:04.400
So, you know, cats can't be attached to the thing that are, you know, they're under

46:04.400 --> 46:05.400
water.

46:05.400 --> 46:06.400
That doesn't make sense.

46:06.400 --> 46:09.040
So I want to move my cat class somewhere else in my tree.

46:09.040 --> 46:13.080
So the opportunity to intervene in a model is something that I'm excited about when

46:13.080 --> 46:19.920
we look at explanations at do you draw a hard line between explanations and interpretability

46:19.920 --> 46:22.440
just when you're speaking about them casually?

46:22.440 --> 46:28.320
I do a little bit because at least classically to me, I was again, background being more

46:28.320 --> 46:32.280
in the traditional machine learning, uh, we really cared a lot about interpretal models

46:32.280 --> 46:36.040
and that meant that I could look at the individual pieces of the model and start to reason

46:36.040 --> 46:40.480
about, uh, conditional probabilities, what they would say about the, the priors that I'm

46:40.480 --> 46:42.240
trying to impose on my data.

46:42.240 --> 46:47.040
Um, so interpretability to me means something that is sort of independent of the kind of

46:47.040 --> 46:48.040
intrinsically.

46:48.040 --> 46:49.040
Yeah.

46:49.040 --> 46:50.040
It's intrinsic to the model.

46:50.040 --> 46:54.080
Whereas an explanation could also be called a justification, sort of looks retroactively.

46:54.080 --> 46:56.920
Here's a decision I made, provide an explanation.

46:56.920 --> 47:00.560
If we look at humans, humans are not interpretable, you can't look at their brain well.

47:00.560 --> 47:03.480
Most people can't look at the brain and go, okay, yeah, I know what you're going to do.

47:03.480 --> 47:07.120
But they provide meaningful explanations and that's maybe all we can hope for and we learn

47:07.120 --> 47:08.640
to work with that.

47:08.640 --> 47:12.880
And I hope with the work and explainability that we're explain is to sort of provide a

47:12.880 --> 47:17.520
little bit more of the interpretability, uh, and I think that's important one because

47:17.520 --> 47:21.280
it, you know, if I'm going to deploy a model, I'd like to be able to, in general, understand

47:21.280 --> 47:25.840
how it's going to behave, uh, not just on, on a, you know, candidate piece of data.

47:25.840 --> 47:28.560
The other lens that we're bringing is, be able to adjust the model when I get it, when

47:28.560 --> 47:31.280
it gets it wrong, I wouldn't be able to, to correct it.

47:31.280 --> 47:38.600
So this work with the decision trees is the set up such that the decision tree is, uh,

47:38.600 --> 47:46.200
is an intrinsic part of, like, when you refer to the model there is the model, a superset

47:46.200 --> 47:51.920
that includes the decision tree or is the decision tree a kind of a surrogate model that's

47:51.920 --> 47:56.560
used when you're asking, uh, explainability kinds of questions.

47:56.560 --> 48:00.160
No, so the challenge here was to, to make the decision tree the model.

48:00.160 --> 48:06.000
Uh, and so what we're doing, uh, we don't do really crazy complicated things.

48:06.000 --> 48:11.760
So in this set up, we're taking a standard ResNet 101 to find an embedding of our, our

48:11.760 --> 48:12.760
input.

48:12.760 --> 48:17.320
And then we're using the decision tree as a, a decision tree on top of that embedding.

48:17.320 --> 48:22.080
Um, and so that's allowing us to route our decision process based on that embedding from

48:22.080 --> 48:23.080
ResNet 101.

48:23.080 --> 48:27.720
So there's a part of the model that I can make no sense of that there's no deep, uh, interpretable

48:27.720 --> 48:30.440
or even explainable component of that little piece.

48:30.440 --> 48:33.480
Um, but there is now structure in how decisions are made.

48:33.480 --> 48:34.480
Right.

48:34.480 --> 48:41.320
So the ResNet is basically, um, kind of learning this space of relationships between

48:41.320 --> 48:44.520
the things that it's seeing, at least in a computer vision sense.

48:44.520 --> 48:50.720
And then the decision tree is on top, making decisions about what is what based on the

48:50.720 --> 48:52.840
space that the ResNet has locked.

48:52.840 --> 48:53.840
Yeah.

48:53.840 --> 48:55.320
So it is a funny recipe.

48:55.320 --> 48:59.560
And a lot of vision now, it's take a ResNet like architecture as a backbone and it's

48:59.560 --> 49:06.560
role is to embodying the things, you know, it wants to, but it's role is to extract, um,

49:06.560 --> 49:11.520
pixel information to extract texture, shape, things, image attributes that would be then

49:11.520 --> 49:13.080
used to make a decision.

49:13.080 --> 49:17.720
And it places them in a fairly high dimensional space and then the, the decision tree is, is

49:17.720 --> 49:22.040
constructed in a way that tries to, uh, use that, that space to make decisions.

49:22.040 --> 49:23.360
Now, that actually alone doesn't work.

49:23.360 --> 49:27.720
So you need to take that decision tree and fine tune the neural net, the ResNet backbone

49:27.720 --> 49:31.200
so that it's, uh, compatible with the decision tree we build.

49:31.200 --> 49:35.040
Um, so it's a, you know, a small twist that's needed, but that small twist now allows us

49:35.040 --> 49:39.880
to get competitive accuracy to the original model, we're still using the model, uh, but

49:39.880 --> 49:43.800
now have an interpretable path where like, uh, one of the fun examples is, uh, if we

49:43.800 --> 49:47.440
give it a picture of a zebra, it's a class we've never seen before, it'll route it down

49:47.440 --> 49:50.920
to near the horse, but then it doesn't know what, you know, it'll classify something

49:50.920 --> 49:52.760
in that one of the horse categories.

49:52.760 --> 49:58.200
Um, and so it, it does try to extract some structure to the classes, uh, that is

49:58.200 --> 50:03.280
semantically meaningful, but also a picture, uh, uh, in the, in the image domain meaningful.

50:03.280 --> 50:06.600
So, uh, things that look similar, yeah.

50:06.600 --> 50:16.320
Can you provide a, uh, kind of a quick intuition for why a fix is needed in the neural network

50:16.320 --> 50:22.400
domain, um, as opposed to just throwing the existing decision tree against the existing

50:22.400 --> 50:26.720
embeddings and what the kind of intuition of that fix is?

50:26.720 --> 50:27.720
Yeah.

50:27.720 --> 50:31.600
So, uh, the simple answer is we tried, uh, throwing the simple embedding into the decision

50:31.600 --> 50:36.640
tree and it didn't work, uh, the, the deeper answer is that, uh, it's, the decision tree

50:36.640 --> 50:40.160
wasn't up, sorry, the, the neural that wasn't optimized to give features that have some,

50:40.160 --> 50:44.400
uh, coherent structure that we can build this, this class hierarchy on top of.

50:44.400 --> 50:48.360
And so adding this extra decision tree loss, we can actually force a decision tree to cluster

50:48.360 --> 50:53.200
things using semantically similar structure, like we want horses to be nearby dogs and

50:53.200 --> 50:56.400
farther, like share less common ancestors to fish.

50:56.400 --> 51:01.040
Um, so, so we can impose some structure in our tree, uh, and then we can force the neural

51:01.040 --> 51:03.560
nets and embeddings to reflect that structure.

51:03.560 --> 51:06.880
So that, that is why we need to adjust the neural net to deal, to compensate or to be

51:06.880 --> 51:09.280
able to work in the context of the tree.

51:09.280 --> 51:15.320
So in the, and you're changing your loss function and the decision tree to accentuate kind

51:15.320 --> 51:22.080
of like maybe, you know, I'm envisioning kind of spreading out the embedding space or

51:22.080 --> 51:26.120
something like that so that the decision tree can, so we're more meaningful for the

51:26.120 --> 51:28.080
semantics of the decision tree.

51:28.080 --> 51:29.080
Cool.

51:29.080 --> 51:30.080
Awesome.

51:30.080 --> 51:34.600
Well, Joey, uh, this has been wonderful, uh, learning a little bit about what you're

51:34.600 --> 51:35.600
up to there.

51:35.600 --> 51:39.160
We still never got very deep into serverless, so we're going to have to put a pin in

51:39.160 --> 51:42.720
that one for, uh, next time, but very cool stuff.

51:42.720 --> 51:45.800
And thanks for taking the time to, uh, share it with us.

51:45.800 --> 51:46.800
Thank you.

51:46.800 --> 51:47.800
It's been fun.

51:47.800 --> 51:48.800
Awesome.

51:48.800 --> 51:49.800
Thanks.

51:49.800 --> 51:54.960
All right, everyone, that's our show for today.

51:54.960 --> 52:00.760
For more information on today's show, visit twomolai.com slash shows.

52:00.760 --> 52:16.680
As always, thanks so much for listening and catch you next time.

