1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:16,400
I'm your host Sam Charrington.

3
00:00:16,400 --> 00:00:24,480
Hey, what's up everyone?

4
00:00:24,480 --> 00:00:29,920
I recently attended the AWS re-invent conference in Las Vegas, and I'm excited to share

5
00:00:29,920 --> 00:00:38,960
a few of my many interesting conversations from that event here on the podcast this week.

6
00:00:38,960 --> 00:00:44,160
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our re-invent

7
00:00:44,160 --> 00:00:45,760
series.

8
00:00:45,760 --> 00:00:50,160
Capital One has been a huge friend and supporter of this podcast for some time now, and

9
00:00:50,160 --> 00:00:54,600
I'm looking forward to sharing my interview with Dave Castillo, Capital One's managing

10
00:00:54,600 --> 00:00:58,640
VP of Machine Learning with you on Thursday.

11
00:00:58,640 --> 00:01:02,400
Dave and I discussed the unique approach being taken at the company's Center for Machine

12
00:01:02,400 --> 00:01:07,480
Learning, as well as some of the interesting AI use cases being developed at the bank,

13
00:01:07,480 --> 00:01:11,880
and the platform they're building to support their ML and AI efforts.

14
00:01:11,880 --> 00:01:20,960
To learn more about Capital One's Machine Learning and AI efforts and research, visit capitalone.com-slash-tech-slash-explore.

15
00:01:20,960 --> 00:01:27,960
And now on to the show.

16
00:01:27,960 --> 00:01:28,960
All right, everyone.

17
00:01:28,960 --> 00:01:33,640
I am here at AWS re-invent in Las Vegas, and I am with Fung Yen.

18
00:01:33,640 --> 00:01:38,440
Fung is an assistant professor at the University of Nevada, Reno.

19
00:01:38,440 --> 00:01:41,280
Fung, welcome to the Tumel AI podcast.

20
00:01:41,280 --> 00:01:42,520
Thank you.

21
00:01:42,520 --> 00:01:49,320
So you just did a talk here on one of your research projects, which is focused on,

22
00:01:49,320 --> 00:01:56,280
or it's called alert wildfire, and it's focused on detecting wildfire smoke using machine

23
00:01:56,280 --> 00:01:58,280
learning.

24
00:01:58,280 --> 00:02:03,240
We're going to dive deep into that, but before we do, how did you get started working on

25
00:02:03,240 --> 00:02:04,240
this problem?

26
00:02:04,240 --> 00:02:11,760
What's your background, and how did your interest align with this particular problem?

27
00:02:11,760 --> 00:02:16,640
So my background actually, you know, I see machine learning and the computer system,

28
00:02:16,640 --> 00:02:20,560
so I thought it was going to cross this plane between these two areas.

29
00:02:20,560 --> 00:02:27,600
So actually, when I'm doing my PhD, I actually have like a more computer system background,

30
00:02:27,600 --> 00:02:33,600
so we kind of build a, you know, efficient systems for data centers for, you know, cloud.

31
00:02:33,600 --> 00:02:39,680
And then, you know, I had an internship in Microsoft research, and that's where I actually

32
00:02:39,680 --> 00:02:41,640
started working on machine learning.

33
00:02:41,640 --> 00:02:47,120
So I still remember like back then, you know, we had like machine learning system.

34
00:02:47,120 --> 00:02:54,920
At that time, we didn't have tens of loads, and we had a system called Adam.

35
00:02:54,920 --> 00:03:00,880
So that's actually, you know, the state of the ad system back then, and you know, I'm

36
00:03:00,880 --> 00:03:07,240
extremely lucky to have the opportunity to work with the group to work on this system.

37
00:03:07,240 --> 00:03:12,240
So that's where I started actually working on the machine learning, and it's actually

38
00:03:12,240 --> 00:03:15,680
back in 2014.

39
00:03:15,680 --> 00:03:21,240
So it's actually, you know, before, you know, even the AI machine learning, you know,

40
00:03:21,240 --> 00:03:22,920
becomes really hot and hot.

41
00:03:22,920 --> 00:03:23,920
It's current wave.

42
00:03:23,920 --> 00:03:24,920
Right.

43
00:03:24,920 --> 00:03:25,920
Before I treat the wave.

44
00:03:25,920 --> 00:03:26,920
Yeah.

45
00:03:26,920 --> 00:03:35,680
So, and then later I joined the University of Nevada Reno, and become faculty there.

46
00:03:35,680 --> 00:03:37,600
One like a special thing, right?

47
00:03:37,600 --> 00:03:43,280
Because I used to be in the East Coast after I moved to the West Coast, we do say, right?

48
00:03:43,280 --> 00:03:48,480
Like one specific, like, situation here is about the White Files.

49
00:03:48,480 --> 00:03:54,240
They actually, Nevada and California, they have a lot of White Files, which, you know,

50
00:03:54,240 --> 00:04:00,080
significantly impacts the air quality and the people's life, because, you know, the smoke

51
00:04:00,080 --> 00:04:04,400
contains a lot of particles that's bad for your hairs.

52
00:04:04,400 --> 00:04:10,280
And also, it will, you know, create these, like, low visibility situation that's, you

53
00:04:10,280 --> 00:04:12,680
know, dangerous for a lot of things.

54
00:04:12,680 --> 00:04:20,280
And it also creates this dry condition, and which will lead to even more White Files.

55
00:04:20,280 --> 00:04:21,280
Okay.

56
00:04:21,280 --> 00:04:26,440
So that's why, you know, I started to get kind of curious about, right?

57
00:04:26,440 --> 00:04:32,120
How, whether there's any efforts has been made into kind of monitoring, control, you

58
00:04:32,120 --> 00:04:36,160
know, the White Files and also the White Files smoke and air quality.

59
00:04:36,160 --> 00:04:47,160
So, and, like, very likely, like, Yonger has a team, a lead by Dr. Ken Graham Ken, and

60
00:04:47,160 --> 00:04:54,360
Ken Smith's daily seismology lab there, and they actually had an effort to build like

61
00:04:54,360 --> 00:04:58,960
a camera system to help, you know, to monitor the White Files.

62
00:04:58,960 --> 00:05:04,560
However, current stage, they are basically, you know, doing things manually.

63
00:05:04,560 --> 00:05:08,600
So from, like, computer science, you know, perspective, I'm thinking, right?

64
00:05:08,600 --> 00:05:13,960
So with my background in machine learning and the cloud computing, I'm wondering, with

65
00:05:13,960 --> 00:05:17,800
her, you know, I can help them to build, like, a more intelligent system.

66
00:05:17,800 --> 00:05:22,520
So that's, you know, where are we actually started to think about this project.

67
00:05:22,520 --> 00:05:30,160
And later, we worked together and actually got, like, an SF and AWS cost months or the

68
00:05:30,160 --> 00:05:34,080
big data, you know, grant to work on this project.

69
00:05:34,080 --> 00:05:38,840
So, you know, that's how I actually get into, you know, this topic.

70
00:05:38,840 --> 00:05:44,960
And so, yeah, and that's very fortunate that you walked into this environment where they've

71
00:05:44,960 --> 00:05:50,120
already got these cameras deployed to try to solve this problem, but no automation system

72
00:05:50,120 --> 00:05:51,120
in place.

73
00:05:51,120 --> 00:05:52,120
Right, right.

74
00:05:52,120 --> 00:05:57,360
How many cameras have been put in place to monitor the, the wildfire situation?

75
00:05:57,360 --> 00:06:02,240
Right now, we have, like, more than 150 cameras already deployed, and then there are hundreds

76
00:06:02,240 --> 00:06:05,520
more, you know, it's going to be deployed as soon.

77
00:06:05,520 --> 00:06:10,760
This is the camera system that's specifically built for monitoring wildfire.

78
00:06:10,760 --> 00:06:17,440
And the technology is way developed to actually, you know, use machine learning, age computing,

79
00:06:17,440 --> 00:06:23,280
cloud computing to help, you know, the monitoring accurate can be extended to, you know, the

80
00:06:23,280 --> 00:06:28,960
general camera system, we then have to be constrained by, like, a specific, you know,

81
00:06:28,960 --> 00:06:34,160
network, because right now, there's a lot of camera system, right, the use for traffic

82
00:06:34,160 --> 00:06:35,160
and that's the purpose.

83
00:06:35,160 --> 00:06:37,200
They're already being there, right.

84
00:06:37,200 --> 00:06:41,680
That's a technology can actually can be extended to other systems if needed.

85
00:06:41,680 --> 00:06:42,680
Okay.

86
00:06:42,680 --> 00:06:45,680
And how are the cameras deployed?

87
00:06:45,680 --> 00:06:53,080
Are they in a grid, like, in forested areas, or are they in more populated areas, but,

88
00:06:53,080 --> 00:06:55,760
you know, facing edges of forested areas?

89
00:06:55,760 --> 00:07:00,840
What's the rationale or methodology behind the camera system?

90
00:07:00,840 --> 00:07:06,800
The camera systems, they basically based on, you know, the areas that are more like prone

91
00:07:06,800 --> 00:07:07,800
to fires.

92
00:07:07,800 --> 00:07:08,800
Okay.

93
00:07:08,800 --> 00:07:09,800
Right.

94
00:07:09,800 --> 00:07:13,320
And of course, the specific actually technology about the camera itself is actually, you

95
00:07:13,320 --> 00:07:17,520
know, lead by, you know, it's a group of the system, only your lab.

96
00:07:17,520 --> 00:07:20,360
So we are more kind of on the software side.

97
00:07:20,360 --> 00:07:28,560
And so when I think about, you know, monitoring for wildfires and images, one of the immediate

98
00:07:28,560 --> 00:07:30,480
thoughts is, like, satellite imagery.

99
00:07:30,480 --> 00:07:31,480
Oh, yes.

100
00:07:31,480 --> 00:07:33,320
That's a very good question.

101
00:07:33,320 --> 00:07:38,040
Conventionally, you know, people are using satellite imaging, as well as mediaology to,

102
00:07:38,040 --> 00:07:45,400
you know, to do the wide-file smoke for casting and also the air quality prediction for

103
00:07:45,400 --> 00:07:46,400
casting.

104
00:07:46,400 --> 00:07:47,400
Okay.

105
00:07:47,400 --> 00:07:52,680
So the limitation of this conventional technique is the resolution.

106
00:07:52,680 --> 00:07:57,360
So actually, wide-file smoke, they can transfer accurate very fast.

107
00:07:57,360 --> 00:08:02,160
Like we had like a video, you know, this morning to show to the audience that, you know,

108
00:08:02,160 --> 00:08:08,000
for example, for like a major area of arena, like the whole major area has close to 1 million

109
00:08:08,000 --> 00:08:09,000
people.

110
00:08:09,000 --> 00:08:13,520
So it's not as big as Vegas, but still quite a big area.

111
00:08:13,520 --> 00:08:23,520
So actually, in a windy day, the smoke actually can spread over the entire city in just 22-30

112
00:08:23,520 --> 00:08:24,520
minutes.

113
00:08:24,520 --> 00:08:25,520
So, yeah.

114
00:08:25,520 --> 00:08:34,280
So for the conventional methodology, their data is very coarse-grand, for example, right?

115
00:08:34,280 --> 00:08:38,320
In terms of both time dimension and also the spatial dimension.

116
00:08:38,320 --> 00:08:44,400
For example, for the time dimension, actually, the data, right, no matter satellite image,

117
00:08:44,400 --> 00:08:50,600
data or media origin data, they refresh like every a few hours or even feel, you know,

118
00:08:50,600 --> 00:08:52,320
like at the day's level, right?

119
00:08:52,320 --> 00:08:53,320
Right.

120
00:08:53,320 --> 00:08:55,320
Because you've got to wait for a satellite to be overhead.

121
00:08:55,320 --> 00:08:56,320
Right.

122
00:08:56,320 --> 00:08:57,320
Right.

123
00:08:57,320 --> 00:09:04,640
So, and another thing about the spatial dimension, right, they basically have like, for example,

124
00:09:04,640 --> 00:09:08,880
the typical resolution is like 10 by 10 kilometers.

125
00:09:08,880 --> 00:09:14,640
So this is like pretty big, right, in terms of, you know, the forecasting.

126
00:09:14,640 --> 00:09:20,320
And think about, right, if we do want to like accurate forecasting, for example, for

127
00:09:20,320 --> 00:09:24,160
an area of a city or like a neighborhood, right?

128
00:09:24,160 --> 00:09:30,960
So we do need a much finer grand way to track the smoke and also to the air quality prediction.

129
00:09:30,960 --> 00:09:35,920
So that's why, right, we are thinking about, you know, using the camera data, right?

130
00:09:35,920 --> 00:09:38,640
Because camera data is very fragrant, right?

131
00:09:38,640 --> 00:09:44,160
You can, you know, capture the image like a dozen of them per second, right?

132
00:09:44,160 --> 00:09:50,440
And you can basically have infinite resolution as well as you have enough cameras, right?

133
00:09:50,440 --> 00:09:56,920
We thought to use like a hybrid approach to combine both the conventional data, like satellite

134
00:09:56,920 --> 00:10:03,600
imaging and as well as the media-oriented data together with this camera data.

135
00:10:03,600 --> 00:10:08,480
So that, you know, we can have like a much finer grand data.

136
00:10:08,480 --> 00:10:13,560
And another important thing about the traditional approach is, for example, for the satellite

137
00:10:13,560 --> 00:10:18,000
data, since it's shooting from the app, right?

138
00:10:18,000 --> 00:10:22,520
Actually like cloud and other situations can block the view, right?

139
00:10:22,520 --> 00:10:25,640
So that you will have lots of missing data.

140
00:10:25,640 --> 00:10:31,120
And while camera is actually on the ground, so it basically have like, you know, like a

141
00:10:31,120 --> 00:10:32,720
better view, right?

142
00:10:32,720 --> 00:10:36,520
But of course, problem with camera data is, right?

143
00:10:36,520 --> 00:10:41,160
They only have local, you know, views, they didn't have like a very good global, you

144
00:10:41,160 --> 00:10:42,480
know, view.

145
00:10:42,480 --> 00:10:45,960
So that's why it's kind of complementary to each other, right?

146
00:10:45,960 --> 00:10:47,920
That's why we need both of them.

147
00:10:47,920 --> 00:10:52,800
Your collaborators that deployed the camera system, did you say they're with the Department

148
00:10:52,800 --> 00:10:53,800
of SizeMology?

149
00:10:53,800 --> 00:10:57,720
Like, are they originally trying to detect earthquakes with the cameras or?

150
00:10:57,720 --> 00:11:04,160
Actually not, like, they, you know, I think they just had this idea, right?

151
00:11:04,160 --> 00:11:11,440
Because, you know, in Reno, we have a lot of, you know, wide file and smoke like situation.

152
00:11:11,440 --> 00:11:17,920
Like, by right now, they sort of just deliver the camera data to the, for example, the file

153
00:11:17,920 --> 00:11:18,920
fighters, right?

154
00:11:18,920 --> 00:11:25,280
They can manually check whether this area has a fire, and if so, they can use it to monitor

155
00:11:25,280 --> 00:11:26,280
it.

156
00:11:26,280 --> 00:11:32,240
But it doesn't have any kind of intelligent way to automatically alert when there's a

157
00:11:32,240 --> 00:11:37,360
fire or smoke, and it doesn't actually track, you know, it's a fire or smoke, right?

158
00:11:37,360 --> 00:11:38,760
Automatically.

159
00:11:38,760 --> 00:11:48,280
So you've got this data coming off of these cameras, 150 today, sounds like 10 frames

160
00:11:48,280 --> 00:11:49,280
a second.

161
00:11:49,280 --> 00:11:50,280
You were saying?

162
00:11:50,280 --> 00:11:51,280
10 frames a minute.

163
00:11:51,280 --> 00:11:52,280
I remember 10.

164
00:11:52,280 --> 00:11:53,280
Right.

165
00:11:53,280 --> 00:11:55,520
This is actually a parameter you can control, right?

166
00:11:55,520 --> 00:11:56,520
Sure.

167
00:11:56,520 --> 00:12:03,520
You can, you know, make it like one frame per second, or even one frame per minute, right?

168
00:12:03,520 --> 00:12:07,960
But you can also add to like maybe 30 frames per second, right?

169
00:12:07,960 --> 00:12:08,960
Yeah.

170
00:12:08,960 --> 00:12:11,280
So that's why, you know, you have a lot of data actually.

171
00:12:11,280 --> 00:12:12,760
It's significantly getting more.

172
00:12:12,760 --> 00:12:13,760
Exactly.

173
00:12:13,760 --> 00:12:14,760
Yeah.

174
00:12:14,760 --> 00:12:22,000
The scale is so much bigger than conventional satellite data or the media-oriented data.

175
00:12:22,000 --> 00:12:25,280
And so that's actually our focus, right?

176
00:12:25,280 --> 00:12:28,120
So think about in practice, right?

177
00:12:28,120 --> 00:12:30,480
Even though you have these cameras really, right?

178
00:12:30,480 --> 00:12:36,800
You can, you know, have a lot of data coming from the sensors, and then you also have

179
00:12:36,800 --> 00:12:42,600
the models, right, for conventional statistical models, as well as the new kind of machine

180
00:12:42,600 --> 00:12:44,640
or any different models, right?

181
00:12:44,640 --> 00:12:52,120
But between the data and the model, right, you actually need to collect and deliver the

182
00:12:52,120 --> 00:12:55,240
data to the model, right, in a timely way, right?

183
00:12:55,240 --> 00:12:56,240
Yeah.

184
00:12:56,240 --> 00:13:02,240
So that's why, you know, one of our efforts is how can we do some pre-processing at the

185
00:13:02,240 --> 00:13:10,160
age side, and then, you know, to make the communication or transferring the data more

186
00:13:10,160 --> 00:13:13,320
efficiently to the cloud.

187
00:13:13,320 --> 00:13:18,880
So that's why we have this age and the cloud working together to actually deliver the

188
00:13:18,880 --> 00:13:21,120
data in a timely way.

189
00:13:21,120 --> 00:13:27,120
And another important thing is when you have this massive amount of data, right?

190
00:13:27,120 --> 00:13:31,200
You do need to find a efficient way to process it, right?

191
00:13:31,200 --> 00:13:38,440
Because if your prediction detection is too slow, then basically if the file or the

192
00:13:38,440 --> 00:13:40,920
smoke is already passed by region, right?

193
00:13:40,920 --> 00:13:41,920
Right.

194
00:13:41,920 --> 00:13:42,920
Then it doesn't matter, right?

195
00:13:42,920 --> 00:13:48,320
So you have to do this, you know, fast enough to make it proactive, right?

196
00:13:48,320 --> 00:13:55,000
So that's why, you know, we, a lot of effort also being spent on how can we do like since

197
00:13:55,000 --> 00:14:02,400
in like low latency, give good scalability, and as well as more efficient, right?

198
00:14:02,400 --> 00:14:08,200
Because whenever you talk about this massive amount of data, and we're complicated

199
00:14:08,200 --> 00:14:10,320
machining the algorithms, right?

200
00:14:10,320 --> 00:14:13,880
So they basically consume a lot of computing resources, right?

201
00:14:13,880 --> 00:14:20,360
A lot of energy, a lot of computing resource, that's actually at this scale, that's matter,

202
00:14:20,360 --> 00:14:21,360
right?

203
00:14:21,360 --> 00:14:29,720
So that's why, you know, we consider both latency, scalability, and efficiency into consideration

204
00:14:29,720 --> 00:14:32,280
for designing our system.

205
00:14:32,280 --> 00:14:38,960
This may be dig into the model itself, and the process you took to develop the model.

206
00:14:38,960 --> 00:14:43,680
Sounds like the inputs are these images, or there are other features that you're feeding

207
00:14:43,680 --> 00:14:48,520
into the model, and what is the model ultimately trying to do?

208
00:14:48,520 --> 00:14:51,480
Yeah, so yeah, this is just between us.

209
00:14:51,480 --> 00:14:56,320
So actually, you know, since this project starts from the beginning of this year, so we

210
00:14:56,320 --> 00:14:58,720
are still at quite early stage.

211
00:14:58,720 --> 00:15:06,640
So some of our efforts in the, you know, models and the training pathway is still ongoing.

212
00:15:06,640 --> 00:15:12,760
So that way, it's, you know, not yet to show, because some of them are submitted for

213
00:15:12,760 --> 00:15:13,760
publication.

214
00:15:13,760 --> 00:15:16,560
Some of them are still, you know, you know, okay.

215
00:15:16,560 --> 00:15:24,160
So actually, to talk this morning, of course, we showed some preliminary results about how

216
00:15:24,160 --> 00:15:31,440
can we use machine learning models to actually detect the smoke, and I can share a little

217
00:15:31,440 --> 00:15:32,440
bit about that.

218
00:15:32,440 --> 00:15:33,440
Okay.

219
00:15:33,440 --> 00:15:42,400
And then actually another thing we have already accomplished at this stage is, after you

220
00:15:42,400 --> 00:15:47,520
have the model, right, you do need to do the inference or classification or detection,

221
00:15:47,520 --> 00:15:48,520
right?

222
00:15:48,520 --> 00:15:51,080
So, so this is also like a big thing, right?

223
00:15:51,080 --> 00:15:53,720
You need to do this in like a real time, right?

224
00:15:53,720 --> 00:15:54,720
Yeah.

225
00:15:54,720 --> 00:15:55,720
I can't talk a little bit more about this.

226
00:15:55,720 --> 00:15:56,720
Okay.

227
00:15:56,720 --> 00:15:57,720
Well, okay.

228
00:15:57,720 --> 00:16:02,960
So maybe let's start with the results that you presented this morning.

229
00:16:02,960 --> 00:16:03,960
Okay.

230
00:16:03,960 --> 00:16:09,960
Maybe a good place to start there is the, how are you formulating the problem that you're

231
00:16:09,960 --> 00:16:10,960
trying to solve?

232
00:16:10,960 --> 00:16:15,520
Is it a classification problem that you're trying to express it as smoke or not smoke?

233
00:16:15,520 --> 00:16:19,200
Or are you trying to determine how much smoke there is?

234
00:16:19,200 --> 00:16:23,520
Are you trying to determine the, you know, predict the future likelihood of smoke?

235
00:16:23,520 --> 00:16:25,520
How do you structure the problem?

236
00:16:25,520 --> 00:16:26,520
Okay.

237
00:16:26,520 --> 00:16:34,480
So, for the problem, first of all, right, for the processing the camera data is actually

238
00:16:34,480 --> 00:16:36,320
our focus of this project.

239
00:16:36,320 --> 00:16:37,320
Okay.

240
00:16:37,320 --> 00:16:42,080
So, of course, we do have a lot of fancy machine learning models right now, right?

241
00:16:42,080 --> 00:16:47,280
However, you know, these are more kind of towards like this benchmarking, right?

242
00:16:47,280 --> 00:16:50,280
So the data is more or less defined, right?

243
00:16:50,280 --> 00:16:55,200
So one big challenge of detecting smoke is think about it.

244
00:16:55,200 --> 00:16:59,520
The smoke is actually has like a various ship, right?

245
00:16:59,520 --> 00:17:04,240
I would say, right, in this world, maybe no two smoke looks the same.

246
00:17:04,240 --> 00:17:05,240
Right.

247
00:17:05,240 --> 00:17:06,240
Right.

248
00:17:06,240 --> 00:17:10,200
So, in other words, like you've got all these, you know, we've got, you know, these models

249
00:17:10,200 --> 00:17:14,360
like ResNet and other things that are, you know, trained on things like ImageNet where

250
00:17:14,360 --> 00:17:18,840
you have these very well-defined objects in the scene.

251
00:17:18,840 --> 00:17:24,320
And I guess they also have kind of technical parameters like, you know, they're 224, you

252
00:17:24,320 --> 00:17:25,320
know, dimension images.

253
00:17:25,320 --> 00:17:32,360
And you've got this stuff coming off of a camera, you know, so real images, you know, of

254
00:17:32,360 --> 00:17:36,280
things that for, you know, most of the time you see them in a picture aren't even really

255
00:17:36,280 --> 00:17:37,280
there.

256
00:17:37,280 --> 00:17:38,280
Right.

257
00:17:38,280 --> 00:17:39,280
Right.

258
00:17:39,280 --> 00:17:40,280
Exactly.

259
00:17:40,280 --> 00:17:43,080
So, actually, there are two challenges, right?

260
00:17:43,080 --> 00:17:45,320
One is about the environment, right?

261
00:17:45,320 --> 00:17:50,000
Thinking about in a real situation, you have different light conditions, right?

262
00:17:50,000 --> 00:17:54,040
And then you also have a various background, right?

263
00:17:54,040 --> 00:17:58,240
For example, in like a mountain area, right, when there's cloud, right?

264
00:17:58,240 --> 00:18:02,280
And basically, the cloud and the smoke can look very identical, right?

265
00:18:02,280 --> 00:18:03,280
Yeah.

266
00:18:03,280 --> 00:18:07,080
Even, you know, humans are difficult to tell the difference, right?

267
00:18:07,080 --> 00:18:12,280
And also, the smoke itself, right?

268
00:18:12,280 --> 00:18:17,280
First of all, it has various shifts, and the second is actually changing, right?

269
00:18:17,280 --> 00:18:19,640
The shift is changing over time, right?

270
00:18:19,640 --> 00:18:25,480
So that's, you know, the two challenges when you, you know, want to classify this, right?

271
00:18:25,480 --> 00:18:29,320
And so what has been your approach for overcoming those challenges?

272
00:18:29,320 --> 00:18:30,320
Yeah.

273
00:18:30,320 --> 00:18:34,440
So, yeah, actually, you know, we are at this stage, right?

274
00:18:34,440 --> 00:18:40,480
We tried some, you know, standard kind of models and we did transfer learning.

275
00:18:40,480 --> 00:18:46,560
And for some, like, either, like, things we actually can, you know, do already very well,

276
00:18:46,560 --> 00:18:52,280
you know, with existing techniques, better for some more kind of challenge, right?

277
00:18:52,280 --> 00:18:59,000
Background, or some, like, faster changing, you know, smoke, so we are still trying to

278
00:18:59,000 --> 00:19:01,200
find a way to solve it.

279
00:19:01,200 --> 00:19:03,560
It's still like ongoing project.

280
00:19:03,560 --> 00:19:04,560
Okay.

281
00:19:04,560 --> 00:19:05,560
Okay.

282
00:19:05,560 --> 00:19:10,040
It sounds like at this stage, the model formulation or the problem formulation would probably

283
00:19:10,040 --> 00:19:15,040
be something on the simpler side, like, just a classifier, smoke or no smoke.

284
00:19:15,040 --> 00:19:16,040
Yeah.

285
00:19:16,040 --> 00:19:19,600
So, for smoke and no smoke, right?

286
00:19:19,600 --> 00:19:25,680
We actually have already done some, you know, preliminary testing, and it's actually

287
00:19:25,680 --> 00:19:30,200
can work very well, except some, like, really challenging, you know, since, yeah.

288
00:19:30,200 --> 00:19:32,560
And our eventual goal is, right?

289
00:19:32,560 --> 00:19:39,880
So we can actually not only identify with smoke or no smoke, but also we can identify

290
00:19:39,880 --> 00:19:41,520
the density of the smoke, right?

291
00:19:41,520 --> 00:19:42,520
Okay.

292
00:19:42,520 --> 00:19:48,320
So, and then use that to map to, like, air quality, you know, measure.

293
00:19:48,320 --> 00:19:53,720
So that we can directly tell from the image about the air quality, basically.

294
00:19:53,720 --> 00:19:54,720
Okay.

295
00:19:54,720 --> 00:19:55,720
Yeah.

296
00:19:55,720 --> 00:20:03,920
And another big challenge when you want to do the machine learning in this classification

297
00:20:03,920 --> 00:20:07,840
task is you don't have the label data, right?

298
00:20:07,840 --> 00:20:17,040
So, so currently, we are, you know, still trying to use some supervised ways to train the

299
00:20:17,040 --> 00:20:22,880
model and to do the classification, but eventually, we do want to, you know, utilize, you

300
00:20:22,880 --> 00:20:30,000
know, semi-supervised learning and on-supervised learning, so that way, you know, use the camera

301
00:20:30,000 --> 00:20:31,480
data directly, right?

302
00:20:31,480 --> 00:20:36,960
So data to help building models and improve the models.

303
00:20:36,960 --> 00:20:43,120
So you've got some preliminary models that you've developed that can start to differentiate

304
00:20:43,120 --> 00:20:49,120
between smoke and non-smoke and you're continuing to work in this area so that you can do things

305
00:20:49,120 --> 00:20:53,800
like identify density patterns and things like that.

306
00:20:53,800 --> 00:21:00,040
But the main focus of the work that you've done thus far and if published on is, it sounds

307
00:21:00,040 --> 00:21:06,040
like on the inference side and maybe some of the characteristics related to the edge nature

308
00:21:06,040 --> 00:21:12,160
of the camera deployment, are you taking advantage of, actually, it is more of a question

309
00:21:12,160 --> 00:21:13,160
than the statement?

310
00:21:13,160 --> 00:21:16,440
Are you doing, like, inference at the edge?

311
00:21:16,440 --> 00:21:20,360
Is that part of what you're, what you're trying to build towards?

312
00:21:20,360 --> 00:21:21,360
Oh, yes.

313
00:21:21,360 --> 00:21:24,120
So, for the edge pattern, it's still ongoing.

314
00:21:24,120 --> 00:21:29,480
So what we have done so far is about how you can do efficient inference as a cloud

315
00:21:29,480 --> 00:21:30,480
side.

316
00:21:30,480 --> 00:21:31,480
On the cloud side.

317
00:21:31,480 --> 00:21:32,480
Got it.

318
00:21:32,480 --> 00:21:41,840
So on the cloud side, if you think about it, about a camera network system, when you

319
00:21:41,840 --> 00:21:48,440
do this tracking and prediction, basically, there will be cameras joining the network and

320
00:21:48,440 --> 00:21:55,360
leave the network because of smoke spreading to a new area as well as the network condition

321
00:21:55,360 --> 00:21:57,640
is very unstable.

322
00:21:57,640 --> 00:22:04,160
So basically, you have this kind of dynamic workload, because the number of cameras and

323
00:22:04,160 --> 00:22:11,640
number of requests send it to the cloud for classification and detection is actually

324
00:22:11,640 --> 00:22:14,280
changing over the time.

325
00:22:14,280 --> 00:22:22,880
And especially, for example, when the smoke moving to a metro area, we do need to pay more

326
00:22:22,880 --> 00:22:27,600
attention, which means we may need a final grand prediction.

327
00:22:27,600 --> 00:22:28,600
Right?

328
00:22:28,600 --> 00:22:35,880
Because the smoke can transport over to a neighborhood in a second level.

329
00:22:35,880 --> 00:22:42,440
So that's why, usually, when the smoke is approaching to a metro area, you will have

330
00:22:42,440 --> 00:22:49,560
suddenly a very high number of requests you need to process, because one thing is you

331
00:22:49,560 --> 00:22:53,560
have a lot of cameras in metro areas.

332
00:22:53,560 --> 00:23:03,600
The second is, it's very important to detect and to classify and detect things timely

333
00:23:03,600 --> 00:23:11,120
in a metro area, because there are denser populations of people.

334
00:23:11,120 --> 00:23:18,800
So in this case, when you have this kind of suddenly increased demand for inference,

335
00:23:18,800 --> 00:23:25,800
then you do need to have a good way to scale in the cloud.

336
00:23:25,800 --> 00:23:34,560
So we have a joint work with Hong Kong University of Science and Technology, where we did

337
00:23:34,560 --> 00:23:37,280
like auto scaling work.

338
00:23:37,280 --> 00:23:46,240
So right now, Amazon do have something called SageMaker that can automatically scale

339
00:23:46,240 --> 00:23:53,560
your number of instances to a high level when it observes increased load.

340
00:23:53,560 --> 00:23:58,320
However, this is a very coarse grand approach.

341
00:23:58,320 --> 00:24:06,000
First of all, it's a feedback approach, so it observes and then acts, which means it

342
00:24:06,000 --> 00:24:13,200
takes quite a while between it observes the high load and its actually scales.

343
00:24:13,200 --> 00:24:17,160
Usually, this is at a few minutes' level right now.

344
00:24:17,160 --> 00:24:25,120
However, if you want to do this real-time fine-grained monitoring of smoke, then basically you

345
00:24:25,120 --> 00:24:28,040
need to do things at a second level.

346
00:24:28,040 --> 00:24:37,080
So that's why we created a new system called Mark that can have much quicker scalability

347
00:24:37,080 --> 00:24:39,160
than SageMaker.

348
00:24:39,160 --> 00:24:41,520
So the key idea here is, right?

349
00:24:41,520 --> 00:24:46,880
So SageMaker is a sort of feedback way of doing things, right?

350
00:24:46,880 --> 00:24:49,880
It's observed when they interact, right?

351
00:24:49,880 --> 00:24:53,240
And there's another way you can do it over provisioning, right?

352
00:24:53,240 --> 00:24:58,400
So basically, I predict there might be like a high load, right?

353
00:24:58,400 --> 00:25:03,640
And then I increase the resources in advance, right?

354
00:25:03,640 --> 00:25:04,640
And of course, right?

355
00:25:04,640 --> 00:25:08,840
In this way, you will have extra cost.

356
00:25:08,840 --> 00:25:12,560
And another thing is you cannot always predict, right?

357
00:25:12,560 --> 00:25:13,560
Correctly.

358
00:25:13,560 --> 00:25:18,680
For example, if you look at the load curve, usually just like a stock market, right?

359
00:25:18,680 --> 00:25:25,400
It's randomly and can surge very high sometimes and suddenly, you know, jobs, right?

360
00:25:25,400 --> 00:25:32,560
So in this case, if you can not predict the load like accurately, right, then what can

361
00:25:32,560 --> 00:25:33,800
you do, right?

362
00:25:33,800 --> 00:25:40,800
So our solution is combining, you know, the infrastructure as a service together with

363
00:25:40,800 --> 00:25:46,880
function as a service or, you know, the popular service provided by a call the providers.

364
00:25:46,880 --> 00:25:48,880
And the idea here is, right?

365
00:25:48,880 --> 00:25:54,480
We still would like to use infrastructure as a service because it's much cheaper in terms

366
00:25:54,480 --> 00:25:55,880
of cost.

367
00:25:55,880 --> 00:26:01,360
However, for function service, it uses the container, right?

368
00:26:01,360 --> 00:26:05,000
And it can scale very fast at like a second level.

369
00:26:05,000 --> 00:26:06,000
Right.

370
00:26:06,000 --> 00:26:12,320
So for the normal load, right, we just use the infrastructure as a service, those virtual

371
00:26:12,320 --> 00:26:17,000
machines to provide the main computing power.

372
00:26:17,000 --> 00:26:20,280
And better when there is a surge of load, right?

373
00:26:20,280 --> 00:26:24,040
Of course, we will do some predictions, right?

374
00:26:24,040 --> 00:26:28,040
If the prediction is accurate, then we can do provisioning.

375
00:26:28,040 --> 00:26:38,760
But if it fails, we will use the serverless instance as a transition period so that it can

376
00:26:38,760 --> 00:26:45,200
immediately, you know, take the new load while you are starting new virtual machines.

377
00:26:45,200 --> 00:26:51,640
And once the new virtual machines has been studied, basically you can, the serverless instance

378
00:26:51,640 --> 00:26:56,520
can transfer the work back to the infrastructure service so that you will have like instant

379
00:26:56,520 --> 00:27:02,280
scalability while also, you know, control the cost.

380
00:27:02,280 --> 00:27:06,840
Thinking about this in the context of cloud, you know, this is maybe, I don't know if we

381
00:27:06,840 --> 00:27:11,120
talk about this a whole lot nowadays, but we used to talk about this concept of cloud

382
00:27:11,120 --> 00:27:17,320
bursting where you'd have like some normalized infrastructure capacity within your enterprise.

383
00:27:17,320 --> 00:27:22,400
And then you have some burst of activity and you process that off in the cloud.

384
00:27:22,400 --> 00:27:27,440
This is like function bursting or something like that where you're running everything

385
00:27:27,440 --> 00:27:35,640
on instances in the cloud and then bursting into Lambda for access capacity.

386
00:27:35,640 --> 00:27:36,640
Oh, yes.

387
00:27:36,640 --> 00:27:37,640
That's the general idea.

388
00:27:37,640 --> 00:27:38,640
Yeah, yeah, yeah.

389
00:27:38,640 --> 00:27:44,440
It's very similar to the concept, but instead of using purely, you know, Lambda, for

390
00:27:44,440 --> 00:27:51,920
example, right, we actually, you know, we actually do some experiment and find it, right?

391
00:27:51,920 --> 00:27:56,480
If you are, everything is on Lambda, since, you know, machine learning is very expensive,

392
00:27:56,480 --> 00:27:57,480
right?

393
00:27:57,480 --> 00:28:02,560
You actually need to like much higher, you know, cost eventually.

394
00:28:02,560 --> 00:28:04,320
So, so that's why, right?

395
00:28:04,320 --> 00:28:10,600
So we, you know, had this idea to combine the infrastructure as a service together with

396
00:28:10,600 --> 00:28:15,120
the function as a service to enjoy the benefit of both worlds, right?

397
00:28:15,120 --> 00:28:16,120
Right.

398
00:28:16,120 --> 00:28:17,120
Right.

399
00:28:17,120 --> 00:28:18,120
Right.

400
00:28:18,120 --> 00:28:22,440
So, the function is instant, you know, scalability.

401
00:28:22,440 --> 00:28:29,360
Yeah, another thing, you know, we didn't use the function as a service at this stage is

402
00:28:29,360 --> 00:28:35,200
they do have some limitations about, you know, the model size like the function size you

403
00:28:35,200 --> 00:28:36,200
can actually have.

404
00:28:36,200 --> 00:28:37,200
You only get certain amount of memory, right?

405
00:28:37,200 --> 00:28:38,200
Right.

406
00:28:38,200 --> 00:28:40,200
Certain amount of execution to that kind of thing.

407
00:28:40,200 --> 00:28:41,200
Yeah.

408
00:28:41,200 --> 00:28:46,320
And we actually have some effort is pushing on, you know, making, enabling the functions

409
00:28:46,320 --> 00:28:51,400
as a service to support these larger models, but it's still, you know, on their going.

410
00:28:51,400 --> 00:28:52,920
So I cannot show too much.

411
00:28:52,920 --> 00:28:53,920
Okay.

412
00:28:53,920 --> 00:28:55,160
That's the stage.

413
00:28:55,160 --> 00:29:01,320
So what's interesting about this is it, to me, is that, you know, independent of the

414
00:29:01,320 --> 00:29:07,640
domain that you're applying this to, yeah, as more folks are moving machine learning

415
00:29:07,640 --> 00:29:11,880
workflows to the cloud, I guess, more and more important to figure out creative ways

416
00:29:11,880 --> 00:29:18,680
to cost optimize, like, you know, building systems to move stuff over the spot instances

417
00:29:18,680 --> 00:29:22,040
versus, you know, regular instances.

418
00:29:22,040 --> 00:29:27,880
And this is another kind of way to arbitrage the cost difference between, you know, one

419
00:29:27,880 --> 00:29:32,080
service, the functions versus the infrastructure.

420
00:29:32,080 --> 00:29:33,080
Oh, yes.

421
00:29:33,080 --> 00:29:37,680
So actually, the cost of place are very important role here, right?

422
00:29:37,680 --> 00:29:42,440
Because think about this is that service, this is that continuous, you know, process, right?

423
00:29:42,440 --> 00:29:44,640
It's just not just one time effort, right?

424
00:29:44,640 --> 00:29:45,640
Right.

425
00:29:45,640 --> 00:29:46,640
So that's why, right?

426
00:29:46,640 --> 00:29:52,320
You know, if you think about the accumulated, right, the cost and the benefits is actually

427
00:29:52,320 --> 00:29:54,520
it's quite significant.

428
00:29:54,520 --> 00:30:00,000
Especially for these machine learning, right, models, they are quite kind of intensive,

429
00:30:00,000 --> 00:30:01,000
right?

430
00:30:01,000 --> 00:30:03,360
They consume a lot of resources.

431
00:30:03,360 --> 00:30:11,360
And so have you been successfully doing inference on Lambda, or is it, are you using some

432
00:30:11,360 --> 00:30:14,320
other functions capability?

433
00:30:14,320 --> 00:30:15,320
Oh, yes.

434
00:30:15,320 --> 00:30:22,600
We did actually being able to deprive some relatively smaller models on Lambda.

435
00:30:22,600 --> 00:30:28,440
However, for some later models, right now, we asked there are, you know, have some ongoing

436
00:30:28,440 --> 00:30:32,560
efforts to make it to be deprived on Lambda.

437
00:30:32,560 --> 00:30:35,320
There are two aspects of what you're doing.

438
00:30:35,320 --> 00:30:39,680
One is focused on how to achieve the scalability requirements.

439
00:30:39,680 --> 00:30:43,960
And this is this idea of bursting the Lambda or the functions.

440
00:30:43,960 --> 00:30:48,560
You also mentioned an element of this that's focused on latency.

441
00:30:48,560 --> 00:30:50,560
Now, what's the driver?

442
00:30:50,560 --> 00:30:55,640
There's just decreasing the amount of time it takes to get a prediction out to someone

443
00:30:55,640 --> 00:30:58,840
who can act on it, or is there another consideration?

444
00:30:58,840 --> 00:30:59,840
Oh, yes.

445
00:30:59,840 --> 00:31:01,480
This is a very good question.

446
00:31:01,480 --> 00:31:06,160
So about the latency part, actually, you know, it's very critical because all these, you

447
00:31:06,160 --> 00:31:10,920
know, machine learning models, they do take a lot of time to process a request, right?

448
00:31:10,920 --> 00:31:14,960
Think about, for example, an inception model, right?

449
00:31:14,960 --> 00:31:19,120
They have hundreds or even thousands of operations, right?

450
00:31:19,120 --> 00:31:25,600
So basically, each image you send, they need to go through all these operators, right?

451
00:31:25,600 --> 00:31:31,880
So if you're doing the sequential way, of course, it would be like a lot of time, right?

452
00:31:31,880 --> 00:31:34,640
It can be like seconds to even minutes, right?

453
00:31:34,640 --> 00:31:42,400
So one way to accelerate the processing speed is through parallelism and also batching.

454
00:31:42,400 --> 00:31:50,320
So for the parallelism part, so basically, almost all modern machine learning inference

455
00:31:50,320 --> 00:31:54,480
or serving frameworks, they do provide these control knobs.

456
00:31:54,480 --> 00:32:01,440
Basically you can have the request-level parallelism as well as operator-level parallelism.

457
00:32:01,440 --> 00:32:08,480
For example, as a request-level basically means you can process multiple requests in parallel,

458
00:32:08,480 --> 00:32:09,480
right?

459
00:32:09,480 --> 00:32:13,040
This is like, you know, just like traditional servings, it's very easy to understand.

460
00:32:13,040 --> 00:32:17,280
For the operation-level, basically, it means how many threads or how many cores will

461
00:32:17,280 --> 00:32:20,680
be assigned to execute each operator.

462
00:32:20,680 --> 00:32:25,240
So since operators, they do have dependency, right?

463
00:32:25,240 --> 00:32:30,280
So it's quite complicated, you know, how you configure, you know, this parallelism, right?

464
00:32:30,280 --> 00:32:33,880
Some people may say, I just said it to maximum, right?

465
00:32:33,880 --> 00:32:39,240
If my CPU have 20 cores, why I just said, you know, 20, right?

466
00:32:39,240 --> 00:32:47,520
However, this way, the experiment shows it actually doesn't, you know, work idea like

467
00:32:47,520 --> 00:32:53,600
this because there are more threads, you have, you will have more contention for the results

468
00:32:53,600 --> 00:32:59,800
because we know, you know, we have limited cache bandwidth also the cache capacity, right?

469
00:32:59,800 --> 00:33:04,600
So if you have too many threads working on just one operation, sometimes it will cause

470
00:33:04,600 --> 00:33:10,960
contention and actually this will create even more overhead and slow down your processing.

471
00:33:10,960 --> 00:33:18,520
So that's why, you know, it's not always like your since that maximum would give you the benefit.

472
00:33:18,520 --> 00:33:22,280
We actually, you know, find most of the time, actually, you know, it's just a random

473
00:33:22,280 --> 00:33:28,840
value in between depending on your model, depending on the specific system, yeah?

474
00:33:28,840 --> 00:33:32,160
And another important aspect is batching.

475
00:33:32,160 --> 00:33:34,760
So the batching basically-

476
00:33:34,760 --> 00:33:40,600
Before we move on to batching, when we're talking about operator parallelism, what's

477
00:33:40,600 --> 00:33:42,520
the operator in this context?

478
00:33:42,520 --> 00:33:44,960
Oh, yes, that's a good question.

479
00:33:44,960 --> 00:33:51,720
So for the operators, right, basically you can think about, right, all the, no matter,

480
00:33:51,720 --> 00:33:54,840
you know, what models you have, we're talking about things like low level things like

481
00:33:54,840 --> 00:33:59,920
multiplies and accumulates and that kind of thing or, yeah, so modern machine learning frameworks

482
00:33:59,920 --> 00:34:05,840
they actually take your model as input and then they will create a computational graph,

483
00:34:05,840 --> 00:34:12,120
right? For example, some operators, you know, doing like a matrix multiplication, right?

484
00:34:12,120 --> 00:34:16,920
Some just like a symbol, like element wise operation, right?

485
00:34:16,920 --> 00:34:23,080
So all these operations basically are executed in your system.

486
00:34:23,080 --> 00:34:26,960
So we're talking about kind of unrolling the computational graph and understanding how

487
00:34:26,960 --> 00:34:32,600
it can be parallelized across multiple cores, right, exactly.

488
00:34:32,600 --> 00:34:39,320
So however, since all these frameworks, they provide, you know, a control knob so that

489
00:34:39,320 --> 00:34:45,680
you can set, you know, different paradigms for operators as well as for request, for request

490
00:34:45,680 --> 00:34:48,240
basically, it's admission policy, right?

491
00:34:48,240 --> 00:34:54,440
However, the sense they don't tell you how to set it for a specific model and deploy

492
00:34:54,440 --> 00:34:56,640
the specific system.

493
00:34:56,640 --> 00:35:02,400
And we find if you set it in a different way, it actually can significantly impact your

494
00:35:02,400 --> 00:35:03,400
latency.

495
00:35:03,400 --> 00:35:04,400
All right.

496
00:35:04,400 --> 00:35:08,720
And so you're about to mention the second part of latency, which is the batch size.

497
00:35:08,720 --> 00:35:09,720
Yeah.

498
00:35:09,720 --> 00:35:10,720
Yeah.

499
00:35:10,720 --> 00:35:18,160
So another important thing people do for accelerating the processing speed is batch.

500
00:35:18,160 --> 00:35:26,160
So basically batch means you just, you don't execute a request like one by one, but rather

501
00:35:26,160 --> 00:35:31,840
you will form like you will put several images together into a batch.

502
00:35:31,840 --> 00:35:37,920
So the benefit of batch is, right, it creates more opportunities for optimization, right,

503
00:35:37,920 --> 00:35:39,960
parallelism optimization, right?

504
00:35:39,960 --> 00:35:47,400
For example, right, if you have like a small matrix, right, because it will do, for example,

505
00:35:47,400 --> 00:35:49,280
matrix multiplication, right?

506
00:35:49,280 --> 00:35:55,640
If you have two small matrix, of course, since can be done in power, but in like, you

507
00:35:55,640 --> 00:35:57,400
know, very limited degree.

508
00:35:57,400 --> 00:36:02,960
However, if you have two big matrix, right, then basically you can, you know, divide

509
00:36:02,960 --> 00:36:06,320
ism and, you know, beta-accelerate, right?

510
00:36:06,320 --> 00:36:12,160
So that's why, you know, the batching can actually help, you know, the efficiency of the

511
00:36:12,160 --> 00:36:13,640
computation, right?

512
00:36:13,640 --> 00:36:19,000
So all these, you know, low-level libraries, they actually, you know, develop to optimize,

513
00:36:19,000 --> 00:36:25,560
you know, the computation when you have like a larger degree of input dimension.

514
00:36:25,560 --> 00:36:26,560
Okay.

515
00:36:26,560 --> 00:36:30,760
So, however, right, batching has two sides, right?

516
00:36:30,760 --> 00:36:37,040
One is it can increase the computational efficiency and then accelerate, you know, to all

517
00:36:37,040 --> 00:36:38,040
computation.

518
00:36:38,040 --> 00:36:43,600
However, if you think about it, in order to form like a large batch, right, you're increasing

519
00:36:43,600 --> 00:36:44,600
latency.

520
00:36:44,600 --> 00:36:45,600
Right, right?

521
00:36:45,600 --> 00:36:46,600
It's not in a training, right?

522
00:36:46,600 --> 00:36:48,640
Because training all your data is there, right?

523
00:36:48,640 --> 00:36:51,600
You can create whatever batch size you want, right?

524
00:36:51,600 --> 00:36:58,240
But in the real-time inference, actually, request arrives in a random pattern, right?

525
00:36:58,240 --> 00:37:02,160
Sometimes, you know, there are more requests, sometimes less requests, right?

526
00:37:02,160 --> 00:37:07,160
And if you want to create like a larger batch, means the earlier arrived the request, they

527
00:37:07,160 --> 00:37:10,520
actually have to wait the later request, right?

528
00:37:10,520 --> 00:37:13,200
Then this penalized the earlier request.

529
00:37:13,200 --> 00:37:18,000
Another thing is, right, even though when you, for example, pretend requests together,

530
00:37:18,000 --> 00:37:23,200
right, it's faster than, you know, it's secure to the individual of them.

531
00:37:23,200 --> 00:37:25,600
However, it still takes longer, right?

532
00:37:25,600 --> 00:37:26,600
Right.

533
00:37:26,600 --> 00:37:32,720
Say like each request has takes 100 million seconds to process, right?

534
00:37:32,720 --> 00:37:37,240
Then if you do one by one, then you need to once again, right?

535
00:37:37,240 --> 00:37:43,840
However, if you do it in a batch, maybe it's just a 500 million seconds, right?

536
00:37:43,840 --> 00:37:49,360
Then still think about the first request, originally it only takes 100 million seconds, but not

537
00:37:49,360 --> 00:37:51,600
it takes 500 million seconds, right?

538
00:37:51,600 --> 00:37:56,720
Of course, the later request will benefit a lot from this, because think about the

539
00:37:56,720 --> 00:37:58,880
killing, waiting, perspective, right?

540
00:37:58,880 --> 00:38:02,080
They need to wait a little request finish, right?

541
00:38:02,080 --> 00:38:06,680
They can immediately, you know, go through the process and it's, you know, doing faster,

542
00:38:06,680 --> 00:38:07,680
right?

543
00:38:07,680 --> 00:38:14,240
So that's, you know, the reason you cannot do an arbitrary batch size, right?

544
00:38:14,240 --> 00:38:19,800
Actually, more than machine learning frameworks, they do provide two control knobs here.

545
00:38:19,800 --> 00:38:24,920
One is the batch size, maximum batch size, which means once you hit the threshold, it will

546
00:38:24,920 --> 00:38:27,960
be sent to the system, right?

547
00:38:27,960 --> 00:38:34,160
Even you have even more to, you know, still just send this size.

548
00:38:34,160 --> 00:38:41,160
The second is with something called the waiting timeout, which means you don't want to wait

549
00:38:41,160 --> 00:38:43,440
forever to create a batch, right?

550
00:38:43,440 --> 00:38:45,280
Since we are talking about latency, right?

551
00:38:45,280 --> 00:38:47,400
Every request matters, right?

552
00:38:47,400 --> 00:38:52,560
So I will say these two parameters in a real system is really complicated, right?

553
00:38:52,560 --> 00:38:54,800
It depends on a lot of factors.

554
00:38:54,800 --> 00:39:00,280
So if you think both parallelism and batch parameters, right?

555
00:39:00,280 --> 00:39:03,960
We have a lot of parameters here, you need to tune, right?

556
00:39:03,960 --> 00:39:08,640
I would say even system expert and machine learning expert, right?

557
00:39:08,640 --> 00:39:14,360
They work together, it's difficult, you know, to find a way to just manually tune these

558
00:39:14,360 --> 00:39:15,360
parameters.

559
00:39:15,360 --> 00:39:21,600
Plus, you know, based on different system situation and different workload, right?

560
00:39:21,600 --> 00:39:24,040
These parameters need to be changed, right?

561
00:39:24,040 --> 00:39:26,960
So that you can achieve the optimal, right?

562
00:39:26,960 --> 00:39:33,560
So that's why, you know, we created an automatic way, you know, to help people to configure

563
00:39:33,560 --> 00:39:35,040
these parameters.

564
00:39:35,040 --> 00:39:43,960
So I will actually first work published in supercomputing 2016, we actually built it like a coding

565
00:39:43,960 --> 00:39:50,080
model to actually, you know, to model the situation, you know, when you have, you know, these

566
00:39:50,080 --> 00:39:51,080
different parameters.

567
00:39:51,080 --> 00:39:52,080
Okay.

568
00:39:52,080 --> 00:39:59,040
And then we can mathematically, you know, compute our optimal solution for the scheduling.

569
00:39:59,040 --> 00:40:02,840
And this is all bringing me back to the stuff that I did in grad school.

570
00:40:02,840 --> 00:40:03,840
Oh, really?

571
00:40:03,840 --> 00:40:04,840
Wow.

572
00:40:04,840 --> 00:40:13,160
Doing theory and stochastic modeling and MMN cues and all that kind of stuff, right?

573
00:40:13,160 --> 00:40:17,320
And of course, the limitation of that work is, right?

574
00:40:17,320 --> 00:40:20,520
You do need to have some assumptions, right?

575
00:40:20,520 --> 00:40:23,040
For example, I have the arrival.

576
00:40:23,040 --> 00:40:24,040
Yeah, yeah, yeah.

577
00:40:24,040 --> 00:40:31,200
You do need to assume the random arrival, right, which follows the ID distribution, right?

578
00:40:31,200 --> 00:40:37,120
And also, you know, for that work, we also assume, you know, the request size deterministic,

579
00:40:37,120 --> 00:40:42,320
which means it's actually true for a lot of computer vision tasks, right?

580
00:40:42,320 --> 00:40:46,320
Think about all the pictures they have the same dimension, right?

581
00:40:46,320 --> 00:40:47,600
And the model is the same.

582
00:40:47,600 --> 00:40:50,920
Of course, they take roughly the same time to process.

583
00:40:50,920 --> 00:40:57,720
However, this is not true, actually, for some other applications like, you know, speech

584
00:40:57,720 --> 00:41:02,640
recognition, right, for natural language processing, right?

585
00:41:02,640 --> 00:41:05,880
So depends on your sentence is short or longer, right?

586
00:41:05,880 --> 00:41:08,720
The present time can be different.

587
00:41:08,720 --> 00:41:09,720
Think about it, right?

588
00:41:09,720 --> 00:41:14,920
So basically, the model based approach, they do need some assumptions, right?

589
00:41:14,920 --> 00:41:22,360
So, and make, if we want to make it more general in practice, we actually, you know, there's

590
00:41:22,360 --> 00:41:25,800
another way is doing model free approach, right?

591
00:41:25,800 --> 00:41:31,240
Of course, there are tons of different learning based approach for model free, right?

592
00:41:31,240 --> 00:41:36,040
So, of course, there are some simple things like a patient optimization, right?

593
00:41:36,040 --> 00:41:38,760
It's something is also very popular, right?

594
00:41:38,760 --> 00:41:45,640
So the reason we choose reinforcement learning is because, think about the dimension here,

595
00:41:45,640 --> 00:41:46,640
right?

596
00:41:46,640 --> 00:41:49,480
We actually have a lot of parameters you need to tune, right?

597
00:41:49,480 --> 00:41:55,200
So it's actually, you know, multi-dimensional problem, so quite complicated.

598
00:41:55,200 --> 00:42:00,160
So that's why we choose reinforcement learning as our approach, right?

599
00:42:00,160 --> 00:42:04,360
However, for reinforcement learning, there are some big limitations.

600
00:42:04,360 --> 00:42:08,600
It needs a lot of training samples, and it converges very slowly, right?

601
00:42:08,600 --> 00:42:14,240
So this is actually not ideal, right, in like a real time system, right?

602
00:42:14,240 --> 00:42:15,240
Right.

603
00:42:15,240 --> 00:42:18,000
So also, it quits a lot of, you know, overheads.

604
00:42:18,000 --> 00:42:23,440
Those problems are training time problems as opposed to inference time problems, yes or

605
00:42:23,440 --> 00:42:24,440
now?

606
00:42:24,440 --> 00:42:29,960
Well, if you want to use a reinforcement learning to configure your system, right, then this

607
00:42:29,960 --> 00:42:33,520
is like a training problem, but of course it's different then.

608
00:42:33,520 --> 00:42:35,240
So we have two machine learning parts, right?

609
00:42:35,240 --> 00:42:37,240
One is machine learning itself, right?

610
00:42:37,240 --> 00:42:41,840
And the other is to configure the system parameters, right?

611
00:42:41,840 --> 00:42:42,840
Right.

612
00:42:42,840 --> 00:42:49,680
So we actually use the machine learning model to find, you know, an optimal solution to

613
00:42:49,680 --> 00:42:52,240
do the machine learning inference, right?

614
00:42:52,240 --> 00:42:53,240
Right.

615
00:42:53,240 --> 00:42:54,240
Right.

616
00:42:54,240 --> 00:42:55,240
Yeah.

617
00:42:55,240 --> 00:43:01,840
So the conventional reinforcement learning, right, they do have like a relatively long learning

618
00:43:01,840 --> 00:43:03,640
cycle.

619
00:43:03,640 --> 00:43:08,160
So what we observe is for this specific program, right?

620
00:43:08,160 --> 00:43:10,120
So think about it, right?

621
00:43:10,120 --> 00:43:14,360
When you have, you know, this complex computational graph, right?

622
00:43:14,360 --> 00:43:19,880
You may have, you know, hundreds of different operators, right?

623
00:43:19,880 --> 00:43:26,440
So some operators may be more sensitive to this, you know, a parameter, right?

624
00:43:26,440 --> 00:43:29,920
Others may be more sensitive to other system parameters, right?

625
00:43:29,920 --> 00:43:32,760
So when you do like a very slight change, right?

626
00:43:32,760 --> 00:43:37,880
For example, just one of the parameters increased, maybe from two to three, right?

627
00:43:37,880 --> 00:43:45,040
So in this case, actually, a lot of maybe only some of the parameters they, you know, have

628
00:43:45,040 --> 00:43:50,000
like pronounced change, but most of others may be not so much, right?

629
00:43:50,000 --> 00:43:56,120
But all global, you know, situation is that it doesn't show much of the change, right?

630
00:43:56,120 --> 00:43:59,880
However, if you did a big change, right?

631
00:43:59,880 --> 00:44:08,240
Then almost all of them have different behaviors, then this will show like a more kind of pronounced

632
00:44:08,240 --> 00:44:10,080
changes globally, right?

633
00:44:10,080 --> 00:44:12,480
So if you draw like a hit map, right?

634
00:44:12,480 --> 00:44:13,480
You will see, right?

635
00:44:13,480 --> 00:44:19,080
When you have a big change, the latency will be totally different.

636
00:44:19,080 --> 00:44:21,760
However, if you do mean, right?

637
00:44:21,760 --> 00:44:24,200
See, I just do a small change.

638
00:44:24,200 --> 00:44:27,160
Actually, it's locally, it's very smooth.

639
00:44:27,160 --> 00:44:30,760
It's actually doesn't, you know, change latency much, right?

640
00:44:30,760 --> 00:44:32,680
So then we are thinking about, right?

641
00:44:32,680 --> 00:44:38,600
Why we just, you know, do less of the learning samples?

642
00:44:38,600 --> 00:44:44,280
And then we use, you know, the long-running sample to actually help estimate, right?

643
00:44:44,280 --> 00:44:46,120
Then you're by regions.

644
00:44:46,120 --> 00:44:51,080
So that's, you know, why we created an approach called region-based reinforcement

645
00:44:51,080 --> 00:44:54,760
running, or L in short.

646
00:44:54,760 --> 00:44:55,760
Okay.

647
00:44:55,760 --> 00:44:56,760
Yeah.

648
00:44:56,760 --> 00:45:02,360
So we find, you know, compared to state-of-the-art, a reinforcement running approach, you

649
00:45:02,360 --> 00:45:08,560
know, used for system configuration, we can significantly reduce, you know, the learning

650
00:45:08,560 --> 00:45:09,560
curve.

651
00:45:09,560 --> 00:45:15,520
So basically, the reinforcement running models can converge so much faster.

652
00:45:15,520 --> 00:45:18,360
And it also, think about, right?

653
00:45:18,360 --> 00:45:20,880
It's a continuous learning process, right?

654
00:45:20,880 --> 00:45:26,400
So whenever you have your models changed, or, you know, even, you know, the system can

655
00:45:26,400 --> 00:45:28,440
have, you know, changes, right?

656
00:45:28,440 --> 00:45:30,760
Especially if it's a cloud system, right?

657
00:45:30,760 --> 00:45:33,920
It's self-have-lot of randomness, right?

658
00:45:33,920 --> 00:45:36,120
So whenever there are such change, right?

659
00:45:36,120 --> 00:45:40,360
Basically, the models will learn to adapt to it, right?

660
00:45:40,360 --> 00:45:44,360
So that's why, you know, the faster convergence is critical here.

661
00:45:44,360 --> 00:45:51,640
I feel like I'm close to understanding what you're doing with the region-based RL.

662
00:45:51,640 --> 00:45:58,480
What I am envisioning is you're doing something where you're changing your optimization,

663
00:45:58,480 --> 00:46:07,640
your cost function, so that you're more focused on kind of this coarse-grained sensitivity

664
00:46:07,640 --> 00:46:10,040
analysis kind of thing.

665
00:46:10,040 --> 00:46:12,440
If you think about reinforcement running, right?

666
00:46:12,440 --> 00:46:15,760
It basically, you know, have two key things, right?

667
00:46:15,760 --> 00:46:17,920
One is action, one is state, right?

668
00:46:17,920 --> 00:46:18,920
Right.

669
00:46:18,920 --> 00:46:22,640
So it's just like, you know, we are running scenes or playing a game, right?

670
00:46:22,640 --> 00:46:23,640
Yeah.

671
00:46:23,640 --> 00:46:25,680
We would try different actions, right?

672
00:46:25,680 --> 00:46:28,120
And then we would get different rewards, right?

673
00:46:28,120 --> 00:46:33,000
Of course, in different states, you'll try different actions, you'll get different rewards,

674
00:46:33,000 --> 00:46:34,000
right?

675
00:46:34,000 --> 00:46:40,280
So that's basically the space of the whole learning process, right?

676
00:46:40,280 --> 00:46:46,400
So our approach basically says, right, if you try one action in a certain state, right?

677
00:46:46,400 --> 00:46:47,400
State.

678
00:46:47,400 --> 00:46:54,640
Basically, you probably didn't need to try another very close-by action because it gives

679
00:46:54,640 --> 00:46:59,760
you, well, be roughly, you know, similar amount of reward.

680
00:46:59,760 --> 00:47:05,320
So in other words, you're kind of quantizing the action space a little bit differently,

681
00:47:05,320 --> 00:47:06,840
more coarse-grained.

682
00:47:06,840 --> 00:47:10,440
Yeah, you can imagine kind of think about it, right?

683
00:47:10,440 --> 00:47:11,440
Sort of like that.

684
00:47:11,440 --> 00:47:12,600
Yes, yes, yes.

685
00:47:12,600 --> 00:47:18,680
So, and of course, you know, we do introduce like a hyper-primiter, right?

686
00:47:18,680 --> 00:47:21,440
To see how big is the region, right?

687
00:47:21,440 --> 00:47:29,320
So, if you have like, you know, too large region, then, you know, it will sort of, you know.

688
00:47:29,320 --> 00:47:32,720
So, so the region size basically means, right?

689
00:47:32,720 --> 00:47:38,760
If you have like larger region size, you are then faster, but if it's too big, then

690
00:47:38,760 --> 00:47:40,760
you will never converge, right?

691
00:47:40,760 --> 00:47:42,400
It's kind of like a learning rate.

692
00:47:42,400 --> 00:47:43,400
Right, right.

693
00:47:43,400 --> 00:47:48,920
If you have like two small ones, of course, you know, it will have less risk, but it's

694
00:47:48,920 --> 00:47:49,920
longer, right?

695
00:47:49,920 --> 00:47:56,640
If you have size one, basically, it's traditional conventional reinforcement, right?

696
00:47:56,640 --> 00:47:57,640
Right, right.

697
00:47:57,640 --> 00:48:04,720
I think the key message here, right, is even if you have good training model, right?

698
00:48:04,720 --> 00:48:12,080
So in order to do this in a real time detection, it's actually not that easy.

699
00:48:12,080 --> 00:48:16,840
So you do need to consider, you know, the machine learning actually are quite expensive

700
00:48:16,840 --> 00:48:20,360
compared to conventional statistical models.

701
00:48:20,360 --> 00:48:26,640
And as well as, right, you do have this changing demanding of the workload, right?

702
00:48:26,640 --> 00:48:27,800
So that's why, right?

703
00:48:27,800 --> 00:48:32,600
You need to consider, you know, the latency, as well as the scalability.

704
00:48:32,600 --> 00:48:35,200
And of course, right, if you want to go to cloud, right?

705
00:48:35,200 --> 00:48:37,560
The efficiency is always a problem, right?

706
00:48:37,560 --> 00:48:40,560
If you have infinite amount of budget, right?

707
00:48:40,560 --> 00:48:45,080
Of course, you just go for the most expensive equipment, right?

708
00:48:45,080 --> 00:48:46,080
Right.

709
00:48:46,080 --> 00:48:49,080
Actually, it's quite costly, right?

710
00:48:49,080 --> 00:48:53,480
So it's usually for machine learning inference, right?

711
00:48:53,480 --> 00:48:58,880
If you want to continuously rent, you know, a lot of machines, right?

712
00:48:58,880 --> 00:49:05,080
The actual computing resource and energy is tremendous, right?

713
00:49:05,080 --> 00:49:09,800
So any savings that on this line would be, you know, give you like a huge benefit.

714
00:49:09,800 --> 00:49:10,800
Awesome.

715
00:49:10,800 --> 00:49:11,800
Awesome.

716
00:49:11,800 --> 00:49:15,640
Well, thanks so much for taking the time to share with us what you're up to.

717
00:49:15,640 --> 00:49:16,640
Okay.

718
00:49:16,640 --> 00:49:17,640
Thank you.

719
00:49:17,640 --> 00:49:18,640
Thank you.

720
00:49:18,640 --> 00:49:24,360
All right, everyone, that's our show for today.

721
00:49:24,360 --> 00:49:32,360
To follow along with our reinvent series, visit twimmelai.com slash reinvent 2019.

722
00:49:32,360 --> 00:49:36,480
Thanks once again to Capital One for their sponsorship of this series.

723
00:49:36,480 --> 00:49:42,280
Be sure to check out capital one dot com slash tech slash explore to learn more about their

724
00:49:42,280 --> 00:49:45,040
ML and AI research.

725
00:49:45,040 --> 00:49:47,960
Thanks so much for listening and catch you next time.

