Hello and welcome to another episode of Twimmel Talk, the podcast where I interview
interesting people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week, I'm happy to bring you my interview with Calvin Seward, a research scientist with
Berlin-Germany-Based Zalando.
Now, our American listeners might not know the name Zalando, but they're one of the largest
e-commerce companies in Europe with a focus on fashion and shoes.
Calvin is a research scientist there, while also pursuing his doctorate studies at Johannes
Kepler University in Linz, Austria.
Our discussion continues the Industrial AI series here on the podcast, and focuses on
how Calvin's team tackled an interesting warehouse optimization problem using deep learning.
Before we dive into the show, take note.
This is the last podcast before we give away not one, but two tickets to the AI conference
in San Francisco, brought to you by O'Reilly and Intel Nirvana.
If you've listened to the podcast for a while, you already know that this is one of my favorite
events, and you've already heard some of the great speakers that it attracts.
If you'd like to attend, now is the time to enter our giveaway.
Folks really seem to like our new Streamline contest app that makes it super quick to make
up to 10 contest entries.
To get your entries in, just hop on over to twimmelai.com slash AISF, and stay tuned for
our announcement of the winners on next week's show.
And now, a quick shout out to our sponsors, bonsai, and wise.io at GE Digital.
If you've heard me mention bonsai before, bonsai offers an AI platform that lets enterprises
build and deploy intelligent systems for industrial applications and more.
Banzai's platform lets enterprises develop robust machine learning models that improve
system control and enhance real-time decision support.
Their platform also automates the management of deployed machine learning applications,
allowing businesses to use them to increase automation and improve operational efficiency
of industrial systems, including robotics, manufacturing, supply chain, logistics, energy
and utilities.
You can find more information about bonsai and their early access program at bonds.ai slash
twimmelai.
And undoubtedly you know GE, but did you know that GE was a software company too?
GE Digital is a leading software company focused on solutions for the industrial internet
of things, and is reimagining industry's infrastructure for connecting software, apps
and analytics to industrial businesses.
GE Digital creates software powered by their predix platform to design, build, operate
and manage the entire life cycle of physical assets, enabling industrial businesses to
operate faster, smarter, and more efficiently.
The wise.io team, now part of GE Digital, is building industrial machine learning applications
for GE and its customers.
For more information about GE Digital, visit GE.com slash digital.
And now on to the show.
All right, everyone, I am on the line with Calvin Seward.
Calvin is a Berlin Germany-based research scientist at Zalando, and he's also pursuing
his doctorate at Johannes Kepler University in Lens Austria.
Calvin, welcome to the show.
Welcome.
Folks, who are listening to the show know that I was recently in Berlin, and I am in fact
connected with you while I was in Berlin, but unfortunately we weren't able to connect
locally, but we're connected now.
So I'm really looking forward to diving into our topic today, which is some work that
you did not too long ago on warehouse optimization.
But before we dive into that, why don't we spend a little bit of time having you introduce
yourself to the audience?
Yeah, thanks.
See, as I said, I'm Calvin Seward.
Originally, I studied mathematics at the Humboldt University here in Berlin.
And since then, I've been working at Zalando, and so Zalando is the tech company that nobody's
heard of.
It's a very large online retailer based in Germany.
And so it's more centered on fashion retail in the European market.
So it's well known in Europe, but very not so well known in the rest of the world.
And in addition to working at Zalando, I also am pursuing my doctorate.
So the doctorate's program is cooperation between the Johannes Kepler University in Lens
in Austria and Zalando.
And so it's really nice, it's really fun, because I get to do research, but I also get to
use Zalando's resources and Zalando's data.
And so it's a really cool thing.
Oh, that sounds like a great opportunity.
It really is.
How did you initially get interested in artificial intelligence?
Somehow my career, it's always just been like, you know, whatever the next thing is, just
do it.
So I studied statistics, and so then when I started Zalando, I was
one of the first people doing data science.
And I don't even think my title was data scientist.
It was like data analyst or something like this.
And so at the beginning, we just did ad hoc reporting for for management, trying to answer
questions like, oh, should the article breadth be broader or so should we have more articles,
more different articles or should it be deeper?
So each article have more time so that so that it doesn't get sold out so quickly.
Different questions like that.
And then slowly but surely, the low-hanging fruit got dealt with, and it became apparent
that they didn't order to keep up with the competition.
You know, we had to step up our game.
And so that's how we got interested in artificial intelligence because, you know, it was definitely
the next big thing and is something that can can drive a lot of customer value.
Maybe you can tell us a little bit about the warehouse optimization problem that you've
worked on.
Yeah.
What we did was we first had one project called the O'Coffee Projects, where you can
imagine the inside of warehouse, it looks like an offline retailer actually.
It looks like targets or a safe way or Tesco or Kaufland or, you know, I don't know which
country my listeners are going to be sitting in, but just one of these large or Walmart,
one of these large stores with a bunch of aisles with items and cross aisles.
And people walk through these store with shopping carts and they put items in the shopping
cart and then they go to check out and leave.
And so it's the same way in the offline or in the online warehouse.
So what happens is our workers, they start off at this thing that we call the train station
and they pick up this cart that's empty.
And then they walk through the aisles and the cross aisles and they put items into their
cart.
And then when they've sort of completed their shopping list, they bring the cart back
to the train station and it's sent on to be processed and for the items to be sent
to the customers.
So then everybody knows when they're shopping that there's an efficient way and there's
an inefficient way to walk through the store.
And so you first, you first buy your butter and your milk because those are close to one
another and then you go off and buy the bread.
You don't do the butter than the bread and then the milk.
And so it's the same way in the online warehouse that we want to, we want to know how to order
the shopping list so that the worker can walk through the warehouse and not to move away.
So this is sort of a traveling salesman problem where you have a bunch of different locations
that the worker has to visit and then you want to be able to tell the worker how to
visit the locations in the most efficient way.
And so there was some research that was done about how to walk through the warehouse.
If the warehouse has a specific layout and it's this so-called rope ladder layout.
And so it's like, it's the same layout that you have in most large offline retail stores.
And the rope ladder layout is, well, tell us specifically what that means.
You have aisles and cross aisles and each cross aisle or each aisle is the same length
and they have and they're aligned with one another.
So it's not like you have this weird jog that you have to make.
Okay.
So it's kind of a simplification and I guess now that I'm visualizing it rope ladder is
just simply trying to create that visual of two long rows with a bunch of cross aisles,
that kind of thing.
Exactly, it looks like a rope ladder.
Got it.
Yeah.
And so there was some work that was already done on optimizing for this setup.
But what was missing was the people who work in the warehouse say they have these big
carts they push around and they're actually quite large because they can carry a lot of
items.
Okay.
But they're sort of difficult to handle and so it's the same way if you're shopping
offline, if you if your cart is fairly full, then you generally leave it in the cross
aisle and you walk into the aisle without the cart and collect your, collect your items
and then return to the cart.
And so we wanted to be able to tell the workers, okay, how best to, to handle the cart.
So we extended this algorithm and we come up with this, this algorithm we call the copy
algorithm.
And so this is called optimal cart pick algorithm that's able to tell the warehouse
workers how best to walk through the warehouse and where to leave the cart.
So that the warehouse worker knows he can always just leave his cart where the algorithm
suggests it and then I think walk to the different locations and get the items and he
knows that the, that the algorithm will always bring him back to the cart before he gets
too much stuff that he can carry, you know, too many things to carry.
And so this is, this has absolutely nothing to do with machine learning.
Let me just say that.
So if people think, oh, where's the machine learning?
It's still not here yet.
This is a fairly straightforward discrete optimization problem where you just have to, you
have this list of items, you have to decide how to order the items, you have these list
of cart locations that you can put the cart and you've got to insert the cart locations
in there somehow.
And then you've got to optimize the sky with a dynamic programming algorithm.
So it, the optimization works is linear in the number of, number of aisles.
So it's, it's got a reasonable enough complexity.
Okay, before we move on, you mentioned traveling salesman problem and, you know, folks that
have done any work in computer science would, you know, probably be familiar with that.
But in case there are folks that aren't familiar with the implications of, you know, something
being essentially a TSP, what does that mean in terms of, you know, how we know how to,
to solve that problem?
Oh, there's, there's lots of literature on, on solving traveling salesman problems.
But at the end of the day, it's an MP hard problem.
So MP hard means, means that it's something that's in the most general case is exponentially
complex.
And so there's lots of other risks about how to solve it.
But you can imagine that, that there's traveling salesman problems.
So, so the idea is that the way, the place that the name comes from is you have a bunch
of different locations, and you have some salesman, and he starts at his house, and he wants
to then go visit all the locations, and he wants to do it in a way to where, to where
the distance that he travels is minimal.
And so there's lots of other risks on it.
And obviously if the locations are organized in a nice way, then how best to do it is
fairly clear.
For example, if all the dimensions lie on all the locations lie on a one-dimensional line,
then it's fairly clear how best to travel.
But already in two dimensions, it becomes difficult.
So this rope ladder warehouse layout is one of these special cases.
You can think of it like the, the single dimensions case where you just have a line.
It's a special case to where you can efficiently solve the traveling salesman problem.
Okay, and then you mentioned your cart handling strategy component of this, where you're trying
to figure out where to, where the worker can park their cart.
Do you also need to consider anything along the lines of been packing or cart capacity
or anything like that?
Yeah, I mean, it's something you can't consider, but at the end of the day, it's not a huge
deal because basically just try and try and make the pick list as large as possible.
And then if the cart gets too full, then there's a button for the, for the warehouse worker
where he just says, my cart is full and then he goes back to the train station and gets
a new cart and the system deals with that.
So, okay, so that's not, that's not really a big problem.
Okay.
So you mentioned that the formulation of this problem, this okapi algorithm is, you know,
we're at discrete optimization.
It's not yet machine learning.
How is the discrete optimization problem solves in practice?
So for us, it's a dynamic programming algorithm.
That's what it, that's what it's called.
And so, so I mean, there's a couple of articles online.
You can, you can Google it where I explain this.
So let me, let me try and remember how best to explain it, but the idea of dynamic programming
is that you, you know that there's a lot of different combinations, but each combination
will, there's these transitions between combinations and they can only take on a certain finite
number of states.
And so you just look for the best combination for each transition.
And so the way this looks in, in practice is, is imagine we have a rope ladder way, lay
out with only two cross aisles, okay?
Okay.
And then, then imagine we, we sort of split it, split it in half, right?
So then there's these two cross aisles.
And so, and we're not, don't think about the cart.
We're not thinking about the cart.
We're just thinking about the optimal way to walk.
Okay.
Then there's a couple of different things that they can happen.
Either the worker, he, he doesn't walk.
So these, these two cross aisles.
So the, these two little cross aisle sections, they're in the middle, okay?
So either the worker doesn't walk on those.
So that means that his entire pick route was either the left or the right.
Or it can happen that he only walks.
So he's got a return to where he started, yeah.
So that means that he walks once on both of them.
So he walks on the, on the top cross aisle.
He walks once on the bottom cross aisle, he walks once, okay?
Or it's possible that he walks twice at the top.
Or it's possible that he walks twice at the bottom.
Or it's possible that he walks twice at both the top and bottom.
So these are all the different states that are possible.
And so what you can do then is no matter what your optimal solution is on the left and
no matter what your optimal solution is on the right.
They're going to have to communicate with each other via these, these seven different states.
So if you can sort of recursively find the optimal solution for the left.
What you can do is then you can recursively find seven different optimal solutions for
these seven different transitions.
I don't remember if it was exactly seven, but these seven different transitions, you
can know, okay, what's the optimal solution going to be.
And then you just calculate, okay, what's the, what's the optimal route for the next cross
aisle, or the next aisle section, and for these seven different transitions.
And then you do that recursively until you get to the end.
And so then instead of having to think about every single different combination, you're
always just trying to figure out, okay, what's the best one for these seven different states?
Okay, if I could try to paraphrase that you, again, we're looking at this warehouse with
this rope ladder configuration, and you can basically chunk it down into sections of the
rope ladder with two cross aisles and then solve the optimization problem locally.
And then that's all kind of strung together recursively to give you an overall optimal
strategy.
Exactly.
Exactly.
Okay.
It's hard to explain.
The first time I read the paper, I was also like, what's going on here?
I also saw you mentioned in some of your writing simulated annealing, is that this process
that we just described, or is that something different?
Okay, so that's definitely the next step.
So now what we got was we got this copy algorithm.
So a copy is short for optimal cart pick.
Okay.
And it works well, but since we have larger warehouses that are more than just two cross
aisles, and since we have this cart business going on, it takes like a second or so to calculate
calculate what the optimal route will end up being, right?
Okay.
So if you're just trying to figure out the route for the warehouse worker, you already
have your pick list together, then it's fine.
But the next step is, of course, to optimize the pick list, because every day the warehouse
worker is a pick, you know, hundreds of thousands of pick lists.
And if you can optimize these to where each pick list has items that are fairly close
to one another, and optimize it to where the pick lists are fairly large, and batches
don't have to be picked from very many zones of the warehouse, then this is a real good
thing.
But in order to do this, you have to sort of know how long a hypothetical pick list will
end up being.
And this one second thing is really a big constraint, because then you can't try out, you know, thousands
and thousands of different combinations without racking up a huge Amazon web services bill
that's prohibited.
So what we did is we just just to make sure I'm understanding there you.
With this Ocopy algorithm, that'll give you basically the routing for an individual,
you know, pick list and picker.
And then in order to figure out the best way to organize the pick list and like the orders,
you have to just brute force that.
So you go through and run this Ocopy algorithm a bunch of times with a bunch of pick lists.
Is that the idea?
Yeah, well, I mean, even if you're using simulated kneeling, you still have to, I mean, so
yeah, there's, there's methods better than brute force like simulated kneeling, but even
there you have to try out a bunch of different combinations.
Okay.
You're, you're never going to, you're never going to get around having to try out, try out
a bunch of different combinations of a pick list to see which ones end up being super
long and which ones end up being quite reasonable.
And so your problem comes in when you've got, when it takes one second to try out one
of these combinations to do that at scale, takes a really long time.
Exactly.
Exactly.
I think you had an example of, I don't know if this map to the kind of a real life Zolando
configuration, but like 2000 years to.
Yeah, exactly.
Exactly.
Jesus, Jesus would order his shoes and he still wouldn't have them now.
So you've got the two problems and your focus was on trying to reduce the time it takes
to calculate, is it to calculate an optimal route or to calculate the length of a route
given a pick list?
Yeah, to calculate the length of the optimal route given the pick list.
Got it.
Basically calculating for this stuff where we're trying to figure out which order goes
with which pick list or which order should be in which pick list.
We don't actually have to know what the route should be.
We just need to know how long the optimal route would end up being.
Okay.
And so we want to decrease that time because whether we're using brute force or simulated
annealing or some other kind of heuristic, it's still going to be dependent on the amount
of time it takes us to figure out the length of the optimal route for a given pick list.
Exactly.
Exactly.
So there's two different levers we can pull to speed things up.
One is better simulated annealing, heuristics, and the other one is just make this bottleneck
which is calculating the length of a pick list faster.
Mm-hmm.
All right.
And that's where machine learning comes in.
Exactly.
Now we're finally going to get to machine learning.
I hope the audience is still with us right now.
I'm sure they are because this is their time.
Okay.
Okay.
If you've been tuning out now is your time to tune back in.
So what we did then is we used this copy algorithm to generate data that we learn on.
So we just generate millions and millions of random pick lists.
And we put these through the copy algorithm.
So we had a bunch of different CPU cores just running at the same time, taking a random
pick list, calculating how long that would take, and then spitting out the answer.
And so these computers, they generated our training data.
And then with this training data, where we've got a pick list, so we know where the articles
are that need to be picked and how many there are.
And how long it would take for the optimal route to get picked.
So given these two guys, we can then feed this information into a neural network and train
that neural network and get some sort of a good result.
And so there's lots of stuff you can simulate, yeah.
And some of it will be good for neural networks and some of it probably won't.
For example, if you think of some sort of a fancy hashing algorithm that tries to avoid
hash collisions, you can obviously simulate this, but you wouldn't be able to get a neural
network to to give you the same output because the whole idea of hashing algorithms is that
it's, you know, as non-continuous as possible.
But for this copy thing, it works very nicely because you can imagine if you just have one
pick and you move it around in the warehouse.
If you just move it a little bit, it won't change, change the pick routes very much.
So if you move it a little bit in the cross aisle or in the aisle, it won't change the
pick route very much because the worst thing that can happen is if you move it, you know,
a foot, the worst thing that can happen is the warehouse worker has to walk an extra foot
to get there.
It's not so linear in the cross aisle.
So if you have a pick that's in the middle of the aisle and you move it the next aisle
over, it could be that it creates a big jump.
Because then the warehouse worker, he may have been going to that aisle already.
And then this new aisle, he wasn't planning on going there.
And so then the distance is quite a lot larger.
But still it's, you know, it's not these huge jumps.
So that's one reason why it's a nice thing to model with neural networks.
And then the dependencies between the articles are also nice because obviously it's not
just some linear sum, you know, if this article is here and this article is here, the distance
for the guy has to walk is the sum of the two distances.
But it's this complex dependency between all the different articles.
But this is this sort of more locally dependent.
So it's less of one of these situations where the butterflies, wings in Japan cause an
earthquake and an LA situations.
But it's more that as you shift articles around, it really only has, generally, I mean,
sometimes there's exceptions, but generally it really just has an effect on how the warehouse
worker walks in that area and it doesn't have too much of an effect on, on far flowing
corners of the warehouse.
And so because of this, because we don't have too many jumps and the function is somewhat
continuous, makes it through it's good for neural networks.
Because, because local structures are more important than global structures, we can use
convolutional neural networks because the convolutional filters, they only focus on local
features.
And then they combine these local features together as they move up through the hidden layers.
Hmm.
So then let me take a second to kind of recap basically, we want to get the time to figure
out the optimal path length for this, you know, for picking an order down.
Neural networks is probably a great way to do that.
And essentially what we're trying to do is we're trying to train a neural network to approximate
this Ocopi algorithm.
And we do that by generating a bunch of random pick lists and throwing them through the
Ocopi algorithm to generate the path length.
So basically generating our training data through by throwing random data through this Ocopi
algorithm.
And then using that to train our neural network, exactly, exactly.
And so I guess one question that I've got is it, you know, you found that it, you know,
at one second per route to run things through this, or it takes a second to run a route
through this Ocopi algorithm. And in order to, you know, fully explore the state space
for like a real warehouse, it would take 2,000 something years if you did all of those,
you know, what, how much coverage do you need in order to, you know, accurately train
a neural network to do this?
What, you know, what percent of that state space or how many training samples do you need?
Well, we didn't really say, oh, we have to have exactly this accuracy.
But we just said, oh, you know, we've got this machine here.
Let's create enough pick lists where the machine runs for a week or so.
And then, and then because we've got other things to do this week, and then next week we'll
come back and look at the results.
So that's the way it works a lot of times, you know, and then we look to the results.
And we're like, it's good enough.
And then, and then that was, and that's how it happened.
So it wasn't like we explored, oh, how many, how many pick lists do we have to create
to get this accuracy?
But it was just sort of these practical considerations that were more at the, at the forefront.
And then, you know, I typically associate convolutional neural nets with image processing
types of tasks, but yet it worked in this case based on the, you know, this locality aspect
of the problem.
Because it was the, did you have to jump through any intermediate steps to kind of format the
process so that it, or format the input data so that it looked like an image, or you're
just kind of feeding it, you know, data that, you know, was natural to the problem.
And, you know, without any kind of intermediate steps.
I think that it didn't a day, we used cafe for this.
So this was a, this was back in the dark days before TensorFlow.
And so there you had different, different input layers.
And I think we figured out that the, the TTIF, it's an image format.
I think that we figured out that that was the most convenient, convenient layer to use.
So yeah, the pick list, we, we transformed them into a TTIF image and we had these images
in there.
Okay.
Interesting.
But, I mean, this is really just a stupid technical detail, it was just, it was just because
of the, the different input layers and cafe.
And so if we had done it with TensorFlow now, it would have been a, a whole different
story.
Okay.
And so the, you used a convolutional network to model this.
How did you arrive at the ultimate architecture of that network and the, you know, the number
of layers and the configuration of those layers and all that stuff?
Oh, we tried out a couple of different things and then one of them worked and we were happy.
But, okay.
I mean, there's a little bit of dark, dark arts, this sort of thing.
And we, we knew, we knew sort of like what the interactions are between different articles.
And so we said, okay, you know, because of these interactions, maybe the filter size
should be about this, this large and we knew how complex interactions could be.
So the depth of the network should be around here and then tried out a couple of different
combinations.
And then today, I mean, these, these TTIF images were, we're not, not particularly large
because each, each aisle is then one, one pixel in each, each cross aisle or, and then,
and then the depth, the depth of the cross aisle, that's, that's the, or the depth of an
aisle is sort of a prox, and prox mission because obviously these articles are, are located
some discrete spots or some continuous spot, though you have to make it into a discrete
pixel.
So it's a little bit of an approximation, but who cares?
And so, and so the images were not particularly large, we were feeding through and so you
could train it very quickly and within, within the course of, of a day and just try out a
bunch of different things and just see what works best be done with it.
Okay.
All right.
Awesome.
And what were you able to accomplish in terms of getting your, your time down that you
were trying to accomplish?
Oh, yeah.
No, that was, that was really good.
So I don't remember these act numbers, but it's definitely under, under a millisecond.
So so I did, I did a benchmark and that's, that's in one of the blog articles where, where
we just then, they ran it and we, we had different configurations because obviously with a,
with this sort of thing, it's, it's a whole lot more efficient if you don't just calculate
one pick list, but calculate a batch of pick lists together.
Mm-hmm.
And so you're able to take your pick lists and put them in a big batch and calculate that
whole batch.
And if you had a large enough batch, maybe 30 or 40 pick lists, then you were, you were
well under a millisecond.
And so this was a huge improvement.
Okay.
Awesome.
Awesome.
Yeah.
And also, also just, I mean, you didn't even have to use fancy hardware, a millisecond
on the CPU, yeah, so if we had used the GPU, sure, it would have been even faster, but.
And the millisecond corresponds to inference against the trained network as opposed to,
and then that's why it's so fast.
Yeah.
Yeah.
Learning is obviously a little bit slower because there you have the forward pass, the
backwards pass, the weight updating and all this other business going on.
Mm-hmm.
And so that, that you really want to do on the GPU.
But when you're just inferring afterwards, it's a small network.
So you can, you can just infer a small picture of a small network.
So you can just infer on the CPU.
And you're already, you're already very fast.
And obviously for, for when you're deploying it to a live system, people always like it
when you can deploy it to a large number of different hardware setups and you don't
have to have one specific GPU.
So that's always a big advantage.
Awesome.
So you solved this problem.
Where did you go next?
That was one of the last warehouse problems that we actually dealt with because at the end
of the day, you can think, think of it this way, you know.
If you solve these warehouse problems, it's really great because it makes it to where fulfillment
costs are lower, the item gets there faster.
So everybody wins.
The customers, they're happier because they don't have to pay as much for fulfillment costs.
We win because we don't have to have such large warehouses because everything works more
efficiently.
But at the end of the day, the maximum, you know, win you can make is whatever your fulfillment
costs are.
If you can manage to be so efficient, the fulfillment is for free, then that's the maximum
you can get.
Whereas if you develop new products that really excites customers and make it to where you
can engage with customers who weren't engaging with you beforehand, the potential winnings
are, you know, just through the roof.
There's no limit to that.
And so we said, okay, we want to try and get away from just incrementally eking out
a little bit of efficiency here and there and we want to get into things that really create
new ways of interacting with, with fashion and interacting with e-commerce.
And so, and so I think you see this a lot, I go to a lot of, you know, industry conferences
and I really see this a lot that there's this progression in companies.
And the first thing that they do is they, they have all this data sitting around and the
first thing they do is they realize, oh, we can use this data to drive efficiency.
And that was one of the first projects that I helped contribute to was this forecast
where we, you know, tried to predict which articles would be returned so that we could
have enough workers at the warehouse on the day that these articles would be coming back
because we didn't, you know, the customers, they just put them in the mail when they
want to return them.
So we don't really know how many articles are going to come to us on a given day.
Yeah, but we were able to use data and fairly accurately predict which articles were going
to be returned when and make it to where the right number of workers were there.
So this was, it was a driver of efficiency is very nice.
So a lot of companies, that's the first thing that they do is they take some sort of
existing process and they make it a little bit more efficient with, with data science
and with machine learning.
And then you can go to the next thing, which is this project that I've talked about now,
which is you create new processes that drive efficiency through, through data science.
So any sort of, any sort of consideration of how to split the pick list or split the
orders between pick list to optimize, to optimize the actual walking distance.
This was completely impossible without a copy.
So no one even thought about trying to do it, but by having a copy, you were able to
come up with new processes, and by having data science, you were able to come up with
new processes that drive efficiency.
But for the customer, nothing has changed.
There's absolutely no new products for him.
It could have been, you know, just a bunch of business people sitting around, you know,
coming up with business rules.
He doesn't notice the difference, but what's really great is when you can then come up
with new products like you're really great, recommend their really great search.
All these self-driving cars is obviously also an example, we don't have that, we're not
working with that.
But these are products that are genuinely new and wouldn't have been possible and are
new for the customer too.
And this is where the big future is at.
It's interesting that your examples are recommenders and searches.
But think of those as kind of these existing things that we do, much like the warehouse,
we do them, you know, using either brute force or, you know, brute force things that
feel like brute force in their sophistication.
And not necessarily like, you know, wholly new kind of user experiences.
Do you have some examples of the way that you're thinking about recommendations and search
that illustrate, you know, where the new opportunities come in for you?
Yeah.
So it's a great question because, yeah, obviously, for example, for documents search,
you can always have some strange business rules that come up with documents.
But I guess you're old enough to remember, you know, what searching the internet was like
before Google came around.
It was completely different animal.
Yes, there was search, but it was so frustrating.
And it's the same way with like old speech recognition systems.
Yes, it was so there were systems, but they were so frustrating that it's as if, you
know, if something's unusable, then it's pointless, you know?
So I'm saying, sure, you can always come up with something that does something.
But if it's so frustrating, then it's pointless and it's the same with self-driving cars.
I mean, even back in the day, they had these cars that somehow were able to tell where
the line was and stay between it, but they were in no way shape or form safe.
And so, you know, what's the point if it is this Russian roulette?
But by using data science, you can cross this threshold between something that's unusable
and something that's usable, and that's what I mean by enabling a new product, enabling
a product that's usable, that's what I guess I mean.
Okay, and so to extend this, there are different ways of searching, you know, the classic search
is just you have, it's this is a document-based search.
You have your text field, you type something in, blue dress or whatever, and then you get
a list of blue dresses.
And I think that there's definitely ways that we can improve this.
So we came up, we came up, for example, with a little tinder, a tinder for fashion
articles, ones where you swipe left, swipe right, and you get shown different stuff.
And so it wasn't like, it wasn't all that usable, but it was a fun thing and it was like
just a way of showing that search can be different than box at the top and then display
results at the bottom.
Right.
I've also seen companies in this space experimenting with image-based search, you know,
similarity, things like that, is that the general direction that you guys are headed
with this kind of stuff?
Oh yes, definitely, definitely, we, I mean, fashion, it's so, so I mean, if you're like
searching, you know, Amazon, they start off with books, and then when you search for
a book, it's fairly simple, because you don't, you don't judge a book by its cover, you
search for the author and the title.
But in fashion, you do judge a dress by, by its picture.
And it's very difficult to, even for experts, to describe, address, use words to describe
exactly what it's going to look like.
And so, and then if you have a layperson and they're, they're trying to wait for our
huge assortment and trying to find something fairly specific and, and this assortment
with over 100,000 articles, then it's going to be very difficult for them to find the
right article.
And thus, we have more clever search, search things than just using metadata, just using
some, some description that some, that some people have hand annotated, which obviously
it works for books, it works for computers, you just sort of say, what sort of, what's
a RAM you want, what sort of processor you want, and bam, you've got your results, but
you can't do that with dresses and fashion articles.
I guess when I think of how that process is done today, not necessarily for fashion,
but, you know, with other e-commerce search experiences, it seems like the, you know,
that search and research experience is largely driven by, as you said, metadata in the
fashion space to the extent that it's currently, you know, still driven by metadata, where
does that come from?
Is that, I guess you already said, it's basically hand, it's all handcrafted, right?
Yeah, yeah, it's handcrafted and it's just been, it's been a very open question as to how
to extract data from unstructured, or how to extract insights, useful insights from unstructured
data, and pictures are very much unstructured data, but like one thing that you can do that's
fairly easy, even with today's understanding is it's fairly easy to come up with, with
neural networks, they're able to say if fashion articles are similar or dissimilar based
on the picture, and then you can, you can use this similarity measure to help improve
your recommendation system, so then at the bottom it's always like, oh, customers who
like this also clicked on this, you know, and then, then you can improve the recommendation
experience that way, that's definitely an easy, low-hanging fruit, even with today's
image recognition stuff.
Okay.
Well, let me ask you this, when you're working on a problem like the warehouse optimization
problem, where there are existing processes, or if not existing processes, existing costs
associated with that, and you're comparing that to something that is more or less wholly
new like a new user experience for discovery or for searching for items, it strikes me
that you have to take a very different approach and building out the business case for these
two types of things, and that depending on the culture of an organization, the management
team, et cetera, it may be more difficult to do one than the other.
Do you have any insights on navigating that process based on your experience working
across these different types of problems, and as you described it, kind of the maturity
of starting from one type of problem and moving to the next and then moving to the next?
Yeah, that's a really great question, and I would say that the most important thing
is that you have management that's for thinking enough to know that these sort of things
is not an easy win, and it's not a win that happens immediately, but it's something
where you have to invest for a while before you get something good, and you really don't
know exactly how long that's going to take.
So management is really important, and then these managers, they then have to say, okay,
we're going to not try and devote all of our resources just to the next release cycle,
but we're also going to devote some resources to things that are not going to pay off immediately.
And so I work in Salando Research, and that's what we do.
That's what we try and focus on, is things that are more of long-term benefit and less
short-term.
And so it's really great that the Salando management team has decided that this is a really
important thing, and this is really something that I find hard to find in a lot of companies,
and so I'm really happy about this.
And then from our perspective, it's helpful to not put all your eggs in one basket.
So don't just focus on one deliverable, but focus on portfolio of deliverables.
So we try and focus on four main things.
We focus on prestige as one, so things like what I'm doing right now, with talks and podcasts
and generating excitement for Salando and the great research we're doing.
We focus on products, new products, we focus on papers, so academic publications to really
sell the great research we're doing within the academic community, and we focus on patents,
patenting, whenever we come up with new ideas, we patent them, because the patent portfolio
is also very valuable.
Interesting.
So you mentioned prestige, patents, publications, and some other things.
It sounds like from that that you guys are very active out in the machine learning community,
as well as e-commerce communities, how can folks in the listening audience who want
to learn more about the kinds of things you're doing, how can folks find you and find all
that good stuff?
That's a great question.
We have a tech blog, so the Salando tech blog, and it's not just us that they're right
to this blog, but there's lots of different techies at Salando who write to this.
And so, and there's also Salando research page where we're different projects are detailed
that we're working on.
There's two great ways to get started.
If you like what you see, you can always send us a job application.
We're always looking for new, excited data scientists and researchers.
So those are two great ways to get into what we're doing.
Awesome.
And I'll include links to those in the show notes.
Yes.
Calvin, I really enjoyed this conversation.
Thank you so much.
Yes.
Yes.
Thanks for taking the time.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For the notes for this episode, to ask any questions or to let us know how you like the
show, please, please, please leave a comment on the show notes page at twimolei.com slash
talk slash 38.
Thanks again to our sponsors, bonsai and wise.io at GE Digital.
For more information about bonsai, visit bons.ai slash twimolei.
And for more on GE Digital, visit GE.com slash digital.
Once you're done with this show, take 30 seconds to head over to twimolei.com slash AISF to enter
our giveaway for a free ticket to the AI conference in San Francisco.
You could be one of two lucky winners.
For more information on industrial AI, my report on the topic or the industrial AI podcast
series, visit twimolei.com slash industrial AI.
Thanks again for listening and catch you next time.
