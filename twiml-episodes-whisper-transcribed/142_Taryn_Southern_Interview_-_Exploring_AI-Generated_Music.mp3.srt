1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:33,520
I'm your host Sam Charrington. A big thanks to everyone who made it out to our meet-up

4
00:00:33,520 --> 00:00:38,680
this week. It was our biggest event yet with nearly 50 participants online to discuss

5
00:00:38,680 --> 00:00:47,320
the YOLO Object Detection System. If you missed it, you can check it out at twimlai.com-meetup.

6
00:00:47,320 --> 00:00:51,880
While you're there, you can also sign up to learn deep learning with us this summer.

7
00:00:51,880 --> 00:00:56,400
I'll be working through the fast AI practical deep learning for coders course starting in

8
00:00:56,400 --> 00:01:00,880
June and I'm organizing a study and support group via the meet-up.

9
00:01:00,880 --> 00:01:05,960
I've been blown away by the interest in this. This is a great course and fast AI co-founder

10
00:01:05,960 --> 00:01:11,520
Jeremy Howard encouraged our group on Twitter noting that groups that take the course together

11
00:01:11,520 --> 00:01:15,000
have a higher success rate. So let's do this.

12
00:01:15,000 --> 00:01:20,480
Three simple steps to join. One, sign up for the meet-up at twimlai.com-meetup, noting

13
00:01:20,480 --> 00:01:27,400
fast AI and know what you hope to learn box. Two, using the email invitation you'll receive,

14
00:01:27,400 --> 00:01:33,480
join our Slack group. And three, once you're there, join the fast AI channel. This is going

15
00:01:33,480 --> 00:01:34,480
to be great.

16
00:01:34,480 --> 00:01:39,960
Alright, in this episode, I'm joined by Taran Southerne, a singer, digital storyteller

17
00:01:39,960 --> 00:01:47,800
and YouTuber whose upcoming album, IMAI, will be produced completely with AI-based tools.

18
00:01:47,800 --> 00:01:52,480
Taran and I explore all aspects of what it means to create music with modern AI tools

19
00:01:52,480 --> 00:01:57,600
and the different processes she's used to create her singles, break free, voices in my

20
00:01:57,600 --> 00:02:02,440
head, and more. She also provides a rundown of the many tools that she's used in this

21
00:02:02,440 --> 00:02:08,320
space, including Google Magenta, Watson Beat, Amper, Lander, and more. This was a super

22
00:02:08,320 --> 00:02:11,320
fun interview that I think you'll get a kick out of.

23
00:02:11,320 --> 00:02:15,320
And now on to the show.

24
00:02:15,320 --> 00:02:22,480
Alright, everyone. I am on the line with Taran Southerne. Taran is a digital storyteller,

25
00:02:22,480 --> 00:02:28,600
a YouTuber, and a singer whose new album, IMAI, is composed and produced entirely with

26
00:02:28,600 --> 00:02:33,320
artificial intelligence. Taran, welcome to this week in Machine Learning and AI.

27
00:02:33,320 --> 00:02:35,360
Thank you so much for having me.

28
00:02:35,360 --> 00:02:39,480
How do we get started by having you tell us a little bit about your background and what

29
00:02:39,480 --> 00:02:42,360
sparked your interest in AI-generated music?

30
00:02:42,360 --> 00:02:49,040
Sure. I've done a lot of things in my background as an artist. I started out in Los Angeles

31
00:02:49,040 --> 00:02:53,480
as an actress and a writer for television. And then a few years into that, I started

32
00:02:53,480 --> 00:02:59,200
making YouTube videos, mainly just as a way to appease my boredom. And it became very

33
00:02:59,200 --> 00:03:04,280
clear after a few years of doing that that there was actually a way to make a living.

34
00:03:04,280 --> 00:03:08,440
I had a number of friends who were also in the YouTube world who had been early pioneers

35
00:03:08,440 --> 00:03:13,600
on the platform and had built up very large audiences and were living the dream, making

36
00:03:13,600 --> 00:03:18,240
whatever content they wanted to make. And I thought, I want to do that. So I started

37
00:03:18,240 --> 00:03:26,000
making weekly videos in 2012 and did that for about four years. I made a weekly video,

38
00:03:26,000 --> 00:03:31,920
comedy, sketches, music videos, you name it pretty much anything and everything. And it

39
00:03:31,920 --> 00:03:36,040
was a really great experience. I also started producing videos for other companies as well

40
00:03:36,040 --> 00:03:41,040
and built up a digital production company. So over the course of ten years, I made approximately

41
00:03:41,040 --> 00:03:47,080
1500 videos, which is a lot of content. And I think, yeah, it's a tremendous amount.

42
00:03:47,080 --> 00:03:52,400
And I think why this is relevant to what we're talking about now is as a YouTuber or content

43
00:03:52,400 --> 00:03:59,680
creator in the digital world, for better or for worse, your success is ultimately defined

44
00:03:59,680 --> 00:04:06,760
more by the quantity of creation than the quality. And so as a result, you start finding

45
00:04:06,760 --> 00:04:15,040
a lot of tools to help increase your production and optimize your content to look as good as

46
00:04:15,040 --> 00:04:20,720
possible and be as good as possible in the shortest amount of time possible. And so you

47
00:04:20,720 --> 00:04:25,760
almost become like a hacker of sorts in terms of how you edit, how you shoot. You're always

48
00:04:25,760 --> 00:04:31,520
trying to find like the quickest but best path forward to make the best possible content

49
00:04:31,520 --> 00:04:38,240
in a very, very quick period. So as a YouTuber, I learned everything. I learned how to edit,

50
00:04:38,240 --> 00:04:46,400
how to light, how to do makeup, how to write, how to market, how to Photoshop photos, and

51
00:04:46,400 --> 00:04:51,880
how to do all of those things in an extremely short period of time. And with usually with

52
00:04:51,880 --> 00:04:58,200
simpler tools than perhaps what professionals used in each of those categories. And so

53
00:04:58,200 --> 00:05:05,160
ultimately around 2015, 2016, I grew very tired of the hamster wheel of content. And so I

54
00:05:05,160 --> 00:05:12,360
decided to throw in the towel and jump ship into a new area, something that would just really

55
00:05:12,360 --> 00:05:17,040
excite me and interest me. So I found myself drawn to VR and wanting to learn how to make

56
00:05:17,040 --> 00:05:23,120
VR content and do something new and different that didn't require me to make content super

57
00:05:23,120 --> 00:05:29,360
fast. And through that, I began working on a project utilizing artificial intelligence.

58
00:05:29,360 --> 00:05:35,680
I really wanted to explore how humans were using technology. And so due to a grant from Google,

59
00:05:35,680 --> 00:05:41,760
I was working on this experiential VR piece. And I really wanted to incorporate AI as

60
00:05:41,760 --> 00:05:47,120
much as I could. So I was just researching every tool that there was out there. And in my

61
00:05:47,120 --> 00:05:52,800
similar to my YouTube roots, I wanted to find tools that are user friendly, easy to use,

62
00:05:53,760 --> 00:05:58,960
don't require me to learn a bunch of code. And that's how I ended up stumbling upon some of

63
00:05:58,960 --> 00:06:04,080
the current tools that I'm using on my album. And it really was just one of those happy accidents

64
00:06:04,080 --> 00:06:09,520
where I wanted to create something new and different. I liked the idea of not having a roadmap.

65
00:06:09,520 --> 00:06:15,280
And within a few weeks of working with some of the AI tools that I'd stumbled upon, I realized,

66
00:06:15,280 --> 00:06:20,000
wow, I can actually make some cool music with this. Maybe I should turn this into a separate

67
00:06:20,000 --> 00:06:23,120
project all on its own. And that's exactly what I ended up doing with the album.

68
00:06:23,120 --> 00:06:27,600
Oh, wow. On your website, you describe yourself as the first artist to work with artificial

69
00:06:27,600 --> 00:06:34,240
intelligence as the sole composition and instrumentation tool on a music album. What exactly does that mean?

70
00:06:34,240 --> 00:06:40,480
Good question. And I think that the definitions get a little bit hazy when you're looking at how AI

71
00:06:40,480 --> 00:06:45,920
is currently used for music today. And quite frankly, I'm not the first album to incorporate AI.

72
00:06:45,920 --> 00:06:51,040
In fact, Flow Machines just released an album two months ago. I haven't even released my album yet.

73
00:06:51,040 --> 00:06:56,480
I've released two singles publicly. I've got a third one coming out in two weeks and then

74
00:06:56,480 --> 00:07:00,560
a fourth one coming out in June. And then I release the album in September.

75
00:07:00,560 --> 00:07:07,360
What was, I suppose, unique about the first song I released and the second song is September

76
00:07:07,360 --> 00:07:13,360
of last year in January of this year, respectively, was that all of the instrumentation that you hear

77
00:07:13,360 --> 00:07:20,720
in that music was spit out from the AI tool that I used to compose with it. Whereas a lot of the

78
00:07:20,720 --> 00:07:27,280
other AI tools or AI songs that you'll hear currently on the market today, those were software,

79
00:07:27,280 --> 00:07:34,720
those were built from software that essentially created a MIDI track, which then a human used

80
00:07:34,720 --> 00:07:41,280
to transpose into instruments of their choosing. So they could essentially extract the notes

81
00:07:41,280 --> 00:07:48,000
from the MIDI track, separate them according to how they want those notes to be heard and

82
00:07:48,000 --> 00:07:53,360
rearrange those notes, reconstruct those notes. I mean, there was a lot of, there's a lot of human

83
00:07:53,360 --> 00:07:58,160
interaction with those, with those musical pieces and then transpose it into the instrumentation,

84
00:07:58,160 --> 00:08:02,720
which is not a bad thing. It's a great, I mean, it's just, it's another tool that humans can use,

85
00:08:02,720 --> 00:08:08,320
but it requires a certain level, I think, of musical acumen on the part of the user to be able to

86
00:08:08,320 --> 00:08:12,800
make those kinds of choices. Whereas the first two songs I released, you could, you could say that

87
00:08:12,800 --> 00:08:19,200
the tool I used was like the AI for beginners tool, or you don't need to know anything about music.

88
00:08:19,200 --> 00:08:25,600
All you need to do is make choices like a, like a film editor would make choices with, with film

89
00:08:25,600 --> 00:08:30,480
footage. So I think, I think that's actually the best possible example of what I'm talking about

90
00:08:30,480 --> 00:08:37,360
is the tool I use for those two songs ampere. Very easy user interface, front facing, you make choices

91
00:08:37,360 --> 00:08:43,760
like what BPM you want the music to be, what genre, what types of instruments you want to be included,

92
00:08:43,760 --> 00:08:49,680
you can specify them, and then it spits out a finish track. Now from that finish track, you can

93
00:08:49,680 --> 00:08:55,520
download the stems and you can remix them, you can make them, you can take some instruments and cut

94
00:08:55,520 --> 00:09:04,480
them out, et cetera, but essentially those instruments were inputted into the AI system with the

95
00:09:04,480 --> 00:09:08,640
thousands of different notes of variation, different ways of playing the instruments and then the

96
00:09:08,640 --> 00:09:13,920
AI is actually making those choices for you. So, so I was not the one choosing those instruments

97
00:09:13,920 --> 00:09:18,080
or making any of those sounds unlike a lot of the AI other, other AI projects that you might see

98
00:09:18,080 --> 00:09:24,800
out there. Hmm, you mentioned stems. What are those? Stems are like the individual pieces of,

99
00:09:24,800 --> 00:09:28,480
it's like the individual file for each instrument. So when you download a finished song, you've got

100
00:09:28,480 --> 00:09:34,160
an MP3, but a stem would be separating out the trombones, the drums, the guitar. So that way

101
00:09:34,160 --> 00:09:39,840
in a mix, you can actually have them at different volumes come in and out, have more variation of

102
00:09:39,840 --> 00:09:44,080
sound. That's what gives you that full sound when you're listening to the radio. So you mentioned

103
00:09:44,720 --> 00:09:50,240
some of the parameters that you kind of tune with this, the beats per minute and some others.

104
00:09:50,240 --> 00:09:56,320
What's the, how much control do you feel like you have as an artist given this approach to creating

105
00:09:56,320 --> 00:10:05,440
music? The question is how much time you have. Yeah, I mean, you really are, when people ask

106
00:10:05,440 --> 00:10:10,800
whether or not working with AI reduces the creativity of the artist, I don't think so at all,

107
00:10:10,800 --> 00:10:16,000
I think it just changes the role of the artist. So, for instance, in my case, I don't come from

108
00:10:16,000 --> 00:10:23,520
a musical background, I can sing, I write melodies, vocal melodies, I can hear harmonies in my head,

109
00:10:23,520 --> 00:10:27,520
but if I had to actually put my hands on a piano or a guitar, I wouldn't know what in the world

110
00:10:27,520 --> 00:10:31,840
to do with it because I just don't have that tactile memory and I haven't taken the time to learn

111
00:10:31,840 --> 00:10:36,320
and it's very hard to learn those things and this is an adult. When I'm working with these tools,

112
00:10:36,320 --> 00:10:42,000
I'm basically getting a bunch of raw data. I can then pick and choose from that raw data

113
00:10:42,000 --> 00:10:47,040
for as much time as I have in a day. I can listen to hundreds, if not thousands, of pieces of music,

114
00:10:47,840 --> 00:10:53,120
discern which piece I find interesting and then from there, I can iterate on that one piece of

115
00:10:53,120 --> 00:10:56,880
music as many times as I'd like. I can change the instruments, I can change the BPM, I can change

116
00:10:56,880 --> 00:11:03,840
the key, I can change how, you know, whether that's a mixolodian key or a major or I can change

117
00:11:03,840 --> 00:11:08,080
the genre so that it's got more of an empowering and themic feel rather than like a sad

118
00:11:09,920 --> 00:11:15,200
ballad type of feel. And so I can make all of these choices and those choices end up changing

119
00:11:15,200 --> 00:11:19,840
the piece dramatically. It's almost as if you handed three editors, a bunch of raw film footage

120
00:11:19,840 --> 00:11:24,160
and you told them all to make a movie and they're all going to make a very, very different kind of

121
00:11:24,160 --> 00:11:29,520
movie and it ultimately just depends on how much time you have. But I just think the work becomes

122
00:11:29,520 --> 00:11:38,880
a lot more editorial based. But it is, I think these kinds of tools will see a ton of creative

123
00:11:38,880 --> 00:11:44,240
input and it's just, it's a matter of time to see how people will utilize them and what other

124
00:11:44,240 --> 00:11:52,400
things will come out of their use. Where did the music creation part of the process fit in

125
00:11:52,400 --> 00:12:00,080
relative to creating the song and other aspects of the songwriting process? Did you already have

126
00:12:00,080 --> 00:12:05,280
the song and you created the music for it? Did you create the song for the music? Did somehow the

127
00:12:05,280 --> 00:12:11,520
AI tool, did you sing the song and the AI tool map to the song as song or did you change the way

128
00:12:11,520 --> 00:12:16,320
you sung the song to the tool? How does that whole workflow come together for you?

129
00:12:16,320 --> 00:12:21,040
So it's different for every song and I'll give you two different examples. The two songs that you

130
00:12:21,040 --> 00:12:27,200
heard that have already been released break free in life support. I wrote those songs to the music

131
00:12:27,200 --> 00:12:33,040
once the music was almost complete. Basically, I got to a point where I was very happy with

132
00:12:33,760 --> 00:12:40,480
one of the exported songs probably after dozens of iterations within Amper. And from there,

133
00:12:40,480 --> 00:12:46,400
the song had a certain feeling to it and I was really writing to that feeling and imagining

134
00:12:47,120 --> 00:12:53,280
imagining the world that I wanted to create based on what the music was sort of doing inside of me.

135
00:12:53,280 --> 00:12:59,520
And so the first song I wrote, I wrote a song about what it might be like to be an AI in the future

136
00:12:59,520 --> 00:13:04,400
who's so intelligent that she doesn't know whether she's AI or human. And the song had a lot of

137
00:13:04,400 --> 00:13:11,200
synth elements and kind of quirky machine-like sounds. So it felt like it fit. And then once I was

138
00:13:11,200 --> 00:13:15,440
done with the lyrics, then I went back to the song and I said, is there any part of this that

139
00:13:15,440 --> 00:13:19,280
doesn't really work or that needs to be changed? And so I went back in and iterated. So it was a

140
00:13:19,280 --> 00:13:25,520
little bit of a back and forth process, but definitely the lyrics and even the vocal melodies,

141
00:13:25,520 --> 00:13:33,760
those were all born out of the music that had essentially been finished. At least the melodies were

142
00:13:33,760 --> 00:13:39,120
in a place that I was really happy with. And then when I'm working with the tool like IBM Watson,

143
00:13:39,120 --> 00:13:45,360
it's quite a bit different. I wrote a song recently with a couple hundred people, which sounds absurd,

144
00:13:45,360 --> 00:13:52,960
but and it kind of is, but it was really fun, utilizing some blockchain technology. And we basically,

145
00:13:52,960 --> 00:13:58,640
we had people submitting song lyrics in a certain category. We decided we wanted to write a song that

146
00:13:58,640 --> 00:14:06,960
was essentially the blockchain anthem for this new generation of people who kind of want to do

147
00:14:06,960 --> 00:14:11,920
things differently, want to break the rules, want to build something from scratch again,

148
00:14:13,360 --> 00:14:18,800
and just write something that kind of had that impact for them. And so we did that. And that was

149
00:14:18,800 --> 00:14:24,800
just based on submissions of lyrics from everyone. And then I decided to start ingesting everyone's

150
00:14:24,800 --> 00:14:30,000
favorite anthem into IBM Watson. So I actually asked the group, what are your favorite anthemic

151
00:14:30,000 --> 00:14:35,360
songs from the 1700s, 1800s, 1900s? I don't care, just give me an anthem. And we did once I

152
00:14:36,080 --> 00:14:40,080
transpose those songs into middies, I was able to feed it into Watson and then Watson was able to

153
00:14:40,080 --> 00:14:45,680
learn from those anthems and create something new out of it. And I found that to be really fun,

154
00:14:45,680 --> 00:14:53,280
like giving the software inspiration collectively from the group that represented something to them.

155
00:14:53,280 --> 00:15:02,720
And so the result is a mixture of sort of an anthemic synth pop song. And I still wrote the lyrics

156
00:15:02,720 --> 00:15:09,680
had been done, but I really made sure that whatever IBM, whatever Watson was spitting out,

157
00:15:11,040 --> 00:15:15,200
kind of fit the lyrics. So there were a few songs that had to throw out even though I liked them,

158
00:15:15,200 --> 00:15:20,640
just because they just weren't good foot fits. And so I think the process can go both ways.

159
00:15:20,640 --> 00:15:29,520
What does it mean to feed those MIDI files into Watson? For example, which of the Watson products did

160
00:15:29,520 --> 00:15:35,680
you use? I used Watson Beat, which you can currently download on GitHub. It runs entirely

161
00:15:35,680 --> 00:15:42,000
in terminals. So it's not the most user friendly. It's not like Amper where anyone can just go in

162
00:15:42,000 --> 00:15:46,800
and use it. You have to have some some basic knowledge of code. And there's a you know, there's a

163
00:15:46,800 --> 00:15:53,920
user guide in the GitHub station that you can use to also find it. But you can feed a number of

164
00:15:53,920 --> 00:15:58,640
different things into the system. You can feed it data, but you can also utilize some of the existing

165
00:15:59,920 --> 00:16:03,920
MIDI tracks that they have available to you. Like you can make a song based on the

166
00:16:03,920 --> 00:16:08,640
the learnings of Mary had a little lamb or other royalty free songs. And then you can set

167
00:16:08,640 --> 00:16:15,120
parameters similar to how you can do with Amper around BPM, key, the instrumentation at what point

168
00:16:15,120 --> 00:16:20,080
section two or section three or section four of the song begins and ends. There's actually a lot

169
00:16:20,080 --> 00:16:27,600
of differentiation that you can utilize with Watson. And what's the what's the relationship

170
00:16:27,600 --> 00:16:33,840
fundamentally between the the MIDI files, the data that you're feeding into this tool and what

171
00:16:33,840 --> 00:16:39,920
it's producing at the on the output? And where does really artificial intelligence come in as

172
00:16:39,920 --> 00:16:45,600
opposed to just some kind of computer generated aggregation of stuff that you've provided?

173
00:16:46,240 --> 00:16:52,160
Right. Well, I'm sadly not the programmer using it. So I can only tell you what I have been told.

174
00:16:52,160 --> 00:16:57,680
And I would love for other AI programmers and researchers to look further into some of these

175
00:16:57,680 --> 00:17:04,800
tools and and actually analyze, you know, their integrity in terms of of what type of network they

176
00:17:04,800 --> 00:17:10,160
are and and how they work. But from what I understand with Google magenta with both Google magenta

177
00:17:10,160 --> 00:17:16,400
and IBM Watson, they study the patterns in the music that it's being fed. And so from those

178
00:17:16,400 --> 00:17:22,640
patterns, they can discern what types of music should be coming next. It sounds like a lot of

179
00:17:22,640 --> 00:17:29,520
its statistical analysis running, you know, if you have a G chord, then 70% of the time you'll move

180
00:17:29,520 --> 00:17:36,560
to a D. If this is a anthemic pop song versus if this is a sad ballad, you know, 30% of the time

181
00:17:36,560 --> 00:17:42,560
you'll move to a D. And then it will sort of transpose and learn from there. That's the general

182
00:17:42,560 --> 00:17:50,720
model. But with magenta, you can feed it a lot more music in a grouping. And same with Ava,

183
00:17:50,720 --> 00:17:56,960
which is another, it's an orchestral AI where they still feed it up to 10,000 songs at one time.

184
00:17:56,960 --> 00:18:01,760
So there's a lot more generative analysis. Okay. So the idea is that you're feeding

185
00:18:02,480 --> 00:18:08,400
these tools, some input data and it's going to produce output that's similar to

186
00:18:09,440 --> 00:18:15,200
whatever the input tunes that you're giving it. But within, you know, some sort of parameters or

187
00:18:15,200 --> 00:18:19,040
constraints that you've. That's right. Because you're applying new constraints or parameters,

188
00:18:19,040 --> 00:18:28,080
like new styles, new genres on top of an existing set of music that gives it some sense of statistical

189
00:18:28,080 --> 00:18:36,560
patterning. And so your inspiration for this was coming out of the YouTube world and kind of going

190
00:18:36,560 --> 00:18:45,200
for volume or quantity over quality. Is that do you see that as being really the only role for

191
00:18:45,200 --> 00:18:55,040
this type of creativity or is there? Definitely not. Yeah. So I mean, like using AI is not fast or easy

192
00:18:56,080 --> 00:19:01,280
to make something good takes a significant amount of time. I think what I intended with my

193
00:19:01,280 --> 00:19:07,040
statement was to demonstrate that there's a whole new group of people out there who I think

194
00:19:07,040 --> 00:19:11,440
as a result of the internet era, really, we've just seen a whole new group of people who

195
00:19:11,440 --> 00:19:19,040
do in order to, to wear a hundred hats, which they typically do now in blogging and in video

196
00:19:19,040 --> 00:19:25,520
creation, they have to find really great tools to do so. And I'm not a fan of like the fast, quick

197
00:19:25,520 --> 00:19:30,160
and dirty way of doing things because to me, I really want to make great art. And I really want

198
00:19:30,160 --> 00:19:35,920
to tell great stories and doing things super quickly doesn't necessarily work. But being able to

199
00:19:35,920 --> 00:19:43,600
have complete control over your over the entire process can be really important for artistry.

200
00:19:43,600 --> 00:19:49,840
And so I know in my case, like if I want to make a song on my own, I've got to call up a music producer.

201
00:19:49,840 --> 00:19:55,760
I probably have to pay them a couple thousand dollars if they're good. And I've got to drive halfway

202
00:19:55,760 --> 00:20:00,640
across the 405 in Los Angeles to get there and record with them. And there's nothing wrong with

203
00:20:00,640 --> 00:20:05,680
that model, but that model is inherently prohibitive. It keeps a lot of people from being

204
00:20:05,680 --> 00:20:10,000
able to create art. It certainly, there's a, you know, high barrier to entry in terms of financial

205
00:20:10,000 --> 00:20:15,920
resources and or skill sets that are required to do that. And so now I can use these tools to actually

206
00:20:15,920 --> 00:20:21,520
make music on my own. That's really exciting. And I think that that same kind of hacker DIY

207
00:20:21,520 --> 00:20:28,240
mentality has, has proliferated across, you know, all types of content creation because of,

208
00:20:29,600 --> 00:20:33,280
because of the internet and because of the tools that we have at our disposal. And everyone just

209
00:20:33,280 --> 00:20:38,960
wants to, everyone wants to create. I mean, if we look at like photography alone, right? I mean,

210
00:20:38,960 --> 00:20:43,360
thanks to filters and Instagram. It's like everyone's a photographer. And that might be a bad thing

211
00:20:43,360 --> 00:20:49,120
for photographers. So, but it's also a really good thing for people that want to express themselves

212
00:20:49,120 --> 00:20:54,800
and have a hand in the creative process. So who are the photographers?

213
00:20:56,160 --> 00:21:01,440
Thousands of people who post on Instagram and think they're photographers. You know, or who

214
00:21:01,440 --> 00:21:07,600
who are able to make decent photographs, who may not have been able to do, I mean, look at all,

215
00:21:07,600 --> 00:21:13,840
even like it's crazy to think about the, the, the huge boom in, in makeup over the last five,

216
00:21:13,840 --> 00:21:18,720
six years because of YouTube makeup tutorials. It's one of the largest categories of videos

217
00:21:19,360 --> 00:21:26,800
online. And so all these girls, they, they have learned new ways of creating makeup looks. And so

218
00:21:26,800 --> 00:21:31,040
they're like makeup artists in their own right. And I'm sure that it's been pretty hard on the makeup

219
00:21:31,040 --> 00:21:36,080
artist community. But so all of these things are, are both suffering and benefiting from these

220
00:21:36,080 --> 00:21:42,400
new tools that allow for us to create quickly, easily. But I do think that everywhere you find

221
00:21:42,400 --> 00:21:47,040
these new tools, you also find artists who are using them and spending hours and hours and hours

222
00:21:47,040 --> 00:21:53,040
hours to make something awesome with them. So like, even for me, I would not say that my time

223
00:21:53,040 --> 00:21:58,960
has been reduced and being able to make this album. You mentioned so far,

224
00:21:58,960 --> 00:22:05,280
Amber Watson Magenta and Eva Eva, are there other tools that you've come across or used in the

225
00:22:05,280 --> 00:22:12,000
process of creating this album and AI tools in particular? There are, there's a, there's a tool

226
00:22:12,000 --> 00:22:17,040
called Lander, which allows, it's an AI tool that masters the music, which is essentially a

227
00:22:17,040 --> 00:22:24,080
process of taking a finished mix and then pulling out certain high and low decibels within the range

228
00:22:24,080 --> 00:22:32,800
for like a radio or a surround stereo mix. That's typically a process that's expensive and

229
00:22:33,360 --> 00:22:39,360
and you need like a very specialized audio engineer to do. There's also a company called AI Music

230
00:22:39,360 --> 00:22:43,680
out of the UK, which is doing some very interesting things. I'm not sure if they've launched yet,

231
00:22:43,680 --> 00:22:49,760
but it will, I believe allow people to, I saw an early demo of it, which will allow people to

232
00:22:49,760 --> 00:22:58,320
sing into a microphone and the AI can analyze the melody and basically play music in tandem

233
00:22:58,320 --> 00:23:02,880
while you are singing. So you kind of have a live duet partner. So it's so to speak. And then

234
00:23:02,880 --> 00:23:09,760
there are also tools through the, through both Google and, and IBM that do other things in the

235
00:23:09,760 --> 00:23:16,240
music sphere like AI duet or incense, which allows you to create new sounds by combining sounds

236
00:23:16,240 --> 00:23:22,720
like a cat and a harp together. Now you can have a new instrument called a carp. I mean, there's

237
00:23:22,720 --> 00:23:30,160
all kinds of wacky things that have a variation of of uses, but I'm trying to utilize as much as I

238
00:23:30,160 --> 00:23:35,920
can on the album in fun ways, but I'm sure that we'll just see more and more time goes on.

239
00:23:36,800 --> 00:23:43,840
So also juke deck, juke deck is another UK based company that's great for jingles and very

240
00:23:43,840 --> 00:23:47,680
quick kind of little diddies that you might use for a corporate video or something like that.

241
00:23:48,320 --> 00:23:53,520
What are the things that you've learned, maybe the top two or three things that you've learned

242
00:23:53,520 --> 00:23:58,640
that, you know, might be useful for someone who wants to experiment with AI-generated music?

243
00:24:00,800 --> 00:24:09,040
Top two things I've learned. Well, I would say you have to, oh my goodness, I've learned a lot.

244
00:24:09,040 --> 00:24:13,840
I think it just depends on the software that you're using. There aren't a lot of people making

245
00:24:14,560 --> 00:24:19,680
music right now with AI software, so it takes a lot of trial and error. There aren't any great

246
00:24:19,680 --> 00:24:26,240
user guides. I think understanding the limitations of each program is important because you can really

247
00:24:27,680 --> 00:24:34,240
devise a strategy based on those limitations and make what you feel is the best possible thing

248
00:24:34,240 --> 00:24:42,000
that you can make with that software. I would also say that it's good to go in with a general

249
00:24:42,000 --> 00:24:47,360
idea of what kind of style and tastes you might have in the music sphere. So, for instance,

250
00:24:47,360 --> 00:24:53,840
when I use Amper, I really focus on cinematic and symphonic electronic sounds.

251
00:24:54,800 --> 00:25:00,160
I really like kind of soundtrack, movie soundtrack sounding music, and so that was my

252
00:25:00,160 --> 00:25:04,960
my north star in creating. Otherwise, you just end up with way too many options,

253
00:25:04,960 --> 00:25:12,080
and it's hard to actually boil something down that you like. Was that related to that particular

254
00:25:12,080 --> 00:25:19,200
project or a particular strength of Amper? I think it was both strength of Amper for sure.

255
00:25:19,200 --> 00:25:24,160
I went through a bunch of their styles and I found that the cinematic style was one of my favorites,

256
00:25:24,160 --> 00:25:31,680
whereas other styles might have been weaker in terms of my musical preferences. Then I also just

257
00:25:31,680 --> 00:25:39,200
felt like I wanted this project to have an epic feel to it. I wanted there to be that movie

258
00:25:39,200 --> 00:25:44,800
magic feeling to it, but I also love electronic music, so I had to combine those two things.

259
00:25:45,440 --> 00:25:51,200
There are a bunch of interesting parallels here with Data Science, which is the typical

260
00:25:51,200 --> 00:25:55,680
conversation topic on this podcast. One of the things that you mentioned was

261
00:25:56,720 --> 00:26:02,800
just the iterative nature of creation with these tools, and that's certainly the case for folks

262
00:26:02,800 --> 00:26:12,320
that are trying to solve business or engineering types of problems with these AI tools,

263
00:26:12,320 --> 00:26:17,840
and you also mentioned just understanding the limitations of the tool, and I guess that's

264
00:26:17,840 --> 00:26:25,520
important with any use of any tool, particularly technology tools. Are there any other observations

265
00:26:25,520 --> 00:26:32,320
like that that you've come across? I think that's primarily it. What I found is that the AI

266
00:26:32,320 --> 00:26:38,560
is not just giving me data. It's giving me new sources of inspiration. What I try to do

267
00:26:39,280 --> 00:26:44,880
is stretch outside my comfort zone, because if I were to collaborate with someone here in LA,

268
00:26:44,880 --> 00:26:50,400
for instance, let's say I was able to find a couple thousand dollars to collaborate with a record

269
00:26:50,400 --> 00:26:57,200
producer here. The likelihood of that person being a pop producer who's trained in radio,

270
00:26:57,200 --> 00:27:01,760
pop hits that understands that formula is very high, because that's what you're going to find.

271
00:27:02,320 --> 00:27:08,240
Whereas if I'm working with an AI, depending on what software or program I'm working with,

272
00:27:08,240 --> 00:27:12,800
and the parameters that the engineers have set, and depending on what type of data I feed it,

273
00:27:12,800 --> 00:27:18,560
I could get something totally random. That's exactly what I found with all of these programs,

274
00:27:18,560 --> 00:27:23,760
to varying degrees, is that there's a lot of randomness that comes out of collaborating with

275
00:27:23,760 --> 00:27:29,040
this software. I love that because I think what it does is it forces me to think outside of my own

276
00:27:29,040 --> 00:27:36,240
box, and it gives me a new collaborative partner that's not in obvious, that's not going to go in

277
00:27:36,240 --> 00:27:44,320
an obvious direction. I guess I would just encourage people to think about these new tools as really

278
00:27:44,320 --> 00:27:50,480
new and unique sources of inspiration, and it might be the thing that's very strange or offbeat

279
00:27:50,480 --> 00:27:56,960
that is actually the most brilliant seed of an idea. We're clearly just at the beginning of

280
00:27:56,960 --> 00:28:02,880
all of this. Where do you see it going based on your experience? Creating this album,

281
00:28:02,880 --> 00:28:09,440
working with these tools, do you have a sense of the direction this will all take?

282
00:28:10,320 --> 00:28:16,080
Oh my goodness. Yeah, a little bit. In the same way, when I started making YouTube videos,

283
00:28:16,080 --> 00:28:24,000
I thought to myself in a few years, everyone is going to be a YouTuber. Not everyone, but I

284
00:28:24,000 --> 00:28:28,640
thought a lot of people are going to become content creators, because making content is fun,

285
00:28:28,640 --> 00:28:34,320
and be because people love to express themselves, and they want to be seen. Before YouTube was around,

286
00:28:34,320 --> 00:28:39,760
you couldn't just be seen. There were huge gatekeepers. You had to live in LA, have an agent,

287
00:28:39,760 --> 00:28:44,560
be going out on additions. That was the only way that you could actually have your material seen,

288
00:28:44,560 --> 00:28:49,040
and all of a sudden you had this platform where millions of people could listen to you, whether or not

289
00:28:49,040 --> 00:28:54,960
you deserve to be watched. It was another question, but I think with these new AI tools,

290
00:28:54,960 --> 00:29:02,400
you'll see the same thing. Specifically with music, I think we'll see a lot of new artists,

291
00:29:02,400 --> 00:29:09,040
a huge democratization of the music creation process. It's just becoming so much easier

292
00:29:09,040 --> 00:29:14,400
to make something that sounds great without all of the super expensive tools that you needed 10

293
00:29:14,400 --> 00:29:19,520
years ago. They had to go into a studio to use. Some people will say that's not a good thing,

294
00:29:19,520 --> 00:29:24,320
and maybe that's the case. I think there's an argument on the other side as well,

295
00:29:24,320 --> 00:29:30,880
that everyone deserves their shot, and we are all artists that deep down want to create,

296
00:29:30,880 --> 00:29:39,120
and make stuff. That's part of the human experience. I think we'll see a democratization

297
00:29:39,120 --> 00:29:46,160
in that regard. Honestly, what I'm most excited about is the fact that out of this democratization

298
00:29:46,160 --> 00:29:53,680
will inevitably come new forms of art that we can't even conceive, whether that's figuring out

299
00:29:53,680 --> 00:30:02,800
how to create music that shifts someone's mood through biotracking devices, or is immersive

300
00:30:02,800 --> 00:30:10,000
musical art in the form of not just sounds, but three-dimensional visuals, augmented reality,

301
00:30:10,000 --> 00:30:16,880
virtual reality. I think we'll see a whole new crop of artists come up that are not just

302
00:30:16,880 --> 00:30:24,400
music artists or visual artists or video artists, but create an entire experience for someone.

303
00:30:26,000 --> 00:30:32,560
I think that that's really what we have to be excited about is the fact that every time we get

304
00:30:32,560 --> 00:30:40,080
really good at one thing, e.s. humans usually figure out something new as a result of that,

305
00:30:40,080 --> 00:30:50,560
and that becomes part of our culture. It just occurred to me that this would be a good time

306
00:30:50,560 --> 00:30:57,520
to throw this out there for our audience, really. Maybe you can give them some tips if anyone

307
00:30:57,520 --> 00:31:05,760
wants to take on this challenge. My producer and editor are not huge fans of the royalty-free

308
00:31:05,760 --> 00:31:13,040
intro music that I use here at the podcast and have used for the past couple of years. We really

309
00:31:13,040 --> 00:31:20,160
should have an AI-generated track, AI-generated intro theme song. If anyone in the audience wants to

310
00:31:20,160 --> 00:31:26,480
take that on, you are certainly welcome to do that. Taren, do you have any tips as to where they

311
00:31:26,480 --> 00:31:31,680
should start? Where would you say your audience sits on the spectrum of AI capabilities?

312
00:31:31,680 --> 00:31:41,040
I'd say they're pretty capable. They tend to be fairly technical and fairly excited about AI.

313
00:31:41,680 --> 00:31:47,760
Awesome. Well, if they have any musical experience whatsoever with the DAW workstation, like logic

314
00:31:47,760 --> 00:31:53,280
or protocols or garage band even is sufficient, I would recommend they use either Google Magenta

315
00:31:53,280 --> 00:31:57,840
or IBM Watson's tool, just because they'll have more control over the inputs and parameters,

316
00:31:57,840 --> 00:32:05,120
and they can actually go in and change the code if they so choose. If they are very AI proficient,

317
00:32:05,120 --> 00:32:10,720
but musically not proficient, then maybe Ampers a good place for them to go, just to play around,

318
00:32:10,720 --> 00:32:16,240
and they could certainly make you something very quickly. So I would recommend any of those

319
00:32:16,240 --> 00:32:21,680
tools to start. And I might add that if anyone else stumbles upon anything new, just because I'm

320
00:32:21,680 --> 00:32:26,000
head down with the album right now, please send it my way. They can just tweet it to me. I'm always

321
00:32:26,000 --> 00:32:33,360
excited to learn about new interfaces. And I'm also simultaneously running a contest for the last

322
00:32:33,360 --> 00:32:39,520
song on my album. I'm putting it out there so we can also extend this to your audience as well

323
00:32:39,520 --> 00:32:47,600
that anyone who writes a composes a track that I like using one of the tools that are out there

324
00:32:47,600 --> 00:32:53,440
with AI, I will co-write the song with them and they will be a co-writer on that track on my album.

325
00:32:53,440 --> 00:33:02,800
So taking submissions up until June 30th. Awesome. So two AI music contests announced right here

326
00:33:02,800 --> 00:33:11,440
the Twimmel intro contest and Taren Sutheran's last track on her album, AI Music Contest. And

327
00:33:12,240 --> 00:33:18,080
if we have any overachievers in the audience, what you should be doing is writing a composition

328
00:33:18,080 --> 00:33:26,000
that can serve as both the Twimmel intro track and Taren's last track on her album and maybe

329
00:33:26,000 --> 00:33:34,240
she'll write the track about the podcast or something. Awesome. Awesome. Well Taren, it's been really,

330
00:33:34,240 --> 00:33:40,320
really great to get to chat with you about this. Thanks so much for taking the time. Thanks Sam,

331
00:33:40,320 --> 00:33:49,440
you too. Take care. All right everyone, that's our show for today. For more information on Taren

332
00:33:49,440 --> 00:33:56,240
or any of the topics covered in this episode, head on over to twimmelai.com slash talk slash 139.

333
00:33:56,240 --> 00:34:09,760
Thanks so much for listening and catch you next time.

