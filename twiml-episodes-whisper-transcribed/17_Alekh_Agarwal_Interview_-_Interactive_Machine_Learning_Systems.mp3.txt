Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Once again, thanks so much to everyone who sent in their favorite quote from last week's
podcast.
Your stickers are on the way.
We've had a blast receiving and reviewing all of your quotes.
Don't forget to send us your favorite quote from today's show as well because this contest
will continue while our sticker supply lasts.
You can do that via a comment on the show notes page or a comment or post on our Facebook,
Twitter, YouTube or SoundCloud pages.
If you've been listening to the podcast for a while, you know that I really enjoy hearing
from listeners and I appreciate all of the comments and feedback I get from all of you.
Now I haven't mentioned this in a while, but one of the most important ways you can provide
feedback is through an iTunes review.
According to our stats, the vast majority of our listeners come from iOS and iTunes
and many of them, one can only suppose, find the podcast via the iTunes directory.
That's why your reviews there are so important.
The more and better they are, the more people will want to check out the podcast.
So please take a moment to review the show on iTunes.
We'll have our link to the iTunes page in the show notes so you can click right through
from there and you don't need to be a regular iTunes user to leave a review.
Of course, if iTunes just isn't your thing and you've got other ways you prefer to spread
the word about the podcast, those are appreciated as well.
That's so much for spreading the word.
The interview you'll hear on today's show was recorded last fall at the O'Reilly AI
conference in New York City.
On the subject of that conference, it's returning to New York in June and this time around
we'll be giving away passes to two lucky Twomo listeners.
Stay tuned because that giveaway is coming soon.
If you happen to be in New York City now, I want to call your attention to another event
that'll be taking place, this one next week.
That event is called the Future Labs AI Summit and it'll be held at NYU on Wednesday afternoon.
Speakers at the event will include Facebook and NYU's Jan LaCoon and NYU's Gary Marcus.
I'll drop a link to this one in the show notes.
Check it out if you're in the city.
It sounds like a great event.
Finally, I'd like to take a minute to remind you to check out my upcoming event, the
Future of Data Summit, which will be held May 15th and 16th in Las Vegas, Nevada.
If you haven't already checked out the event, I really encourage you to take a look.
The person I had in mind when I created this event is someone who's in a role where they're
responsible for helping to chart an organization's data strategy, or someone who wants to be
in that kind of role, or someone who needs to understand how all of this will come together.
This isn't the place where we'll go deep on neural nets, pun intended, or the latest research
paper.
Rather, this is an interdisciplinary summit where you'll get to hear from and engage with
experts presenting on various aspects of our data-centric future.
You'll hear from a soft Iraqi, an expert in big data infrastructure at Intel, on the coming
advances in hardware and what they'll allow us to do in machine learning AI analytics
and more.
You'll hear about how cloud, IoT, and big data shift the cybersecurity threat landscape,
and how we can secure these systems from IBM's global executive security advisor, Diana
Kelly.
I'll be leading a discussion on data privacy and algorithmic ethics, with Accenture AI's
Roman Shoudry and Ericsson's Jonathan King.
Erics Samar, founder and CTO of Reconna, will talk about the role of AI in optimizing the
enterprise data center, so-called AI ops.
And Endeavor VR founder Amy Peck will talk about the emerging role of virtual and augmented
reality in the enterprise.
These are just a few of the speakers I've lined up for you and I'll be announcing more
shortly.
To learn more about the summit, visit twimmolai.com slash future of data.
Now, about today's show.
This week, my guest is Alec Agarwal.
Alec is a researcher with Microsoft Research in New York City, where his work is focused
on interactive machine learning.
As I mentioned before, Alec and I recorded this show at the ORILE AI conference where
he delivered a talk called interactive learning systems, why now and how.
Interactive learning systems are different from traditional supervised machine learning
systems and that they need to explore and learn from their environments.
This is an exciting area of research and one that really interests me personally.
In part because it incorporates concepts like active learning, reinforcement learning,
contextual bandits and much more.
If you're interested in this topic, when you're done with this show, you should listen
to the show I did with Georgia Tech's Charles Isbell, if you haven't already.
That was show number four.
The notes for this show can be found at twimmolai.com slash talk slash 17.
Now on to the show.
Hey, everyone.
I'm here with Alec Agarwal with Microsoft Research and we're here at day two of the ORILE AI
conference.
Alec just did a great presentation on interactive learning systems and he was kind
enough to join us to talk a little bit about that presentation.
Alec wanted to start off by introducing yourself.
Yeah, sure.
So I'm Alec Agarwal.
I'm a researcher at Microsoft Research actually here in New York City.
I've been here for four years prior to that.
I was doing my PhD at UC Berkeley and my work really touches upon a lot of teams in AI but
one that is particularly in machine learning but one that has particularly been off
interest lately is what I call interactive machine learning.
So think that thing about problems where the machine learning algorithm is not just learning
from a static pool of data that was hand annotated and collected by somebody else but think about
really how the algorithm has to interact within a larger system, within a larger environment
to collect that data to gather learning cues and then incorporate learning cues into
the model in order to improve over time and just leads to several kind of paradigms in machine
learning, things like active learning, reinforcement learning or you know, subsets of reinforcement
learning like contextual bandits and so on, all of which I worked on and were touched
upon in the talk today.
Great, great.
So you mentioned the machine learning, learning from the environment and one of the ways you
illustrated that in your talk was, you showed a demonstration of a Super Mario Brothers
game.
Can you talk about what you were intending to show with that demo and?
Yeah.
So in some sense, whenever we are thinking about interactive learning systems, right?
So one of the question is, what are environments in which we can safely run these experiments
in which we can have this are basically algorithmic agents interact with their environment or
manipulate the environment in a safe manner.
Of course, you know, the natural or maybe canonical embodiment even in everybody is thinking
of such agents are robots, right?
But robots are hard to program.
It's hard to in fact, like control all of their sensors and actuators and it takes time,
it takes resources even to get one.
And so a lot of people have found designing agents for various sort of games, either computer
games or even traditional board games, right?
Like baggammon, chess, gold, now, and other such games as kind of more controlled environments
in which we can still have this interaction either with another player or with the environment
of the game.
And we can run these experiments kind of over and over again with actually remarkably
high throughput.
And overclock these games, so it allows for very fast experimentation.
And so that's kind of the reason why people have really gravitated towards using games
in particular now.
There is, you know, this Atari learning environment, which is basically using Atari games again
as a platform to test interactive learning situations.
And what I was trying to show in the particular Super Mario demo actually, so I should give
a credit to Stefan Ross, whose work that video comes out of.
So, you know, what they were trying to demonstrate was supervised learning is not adequate or a
static pool of data is not adequate to do well in interactive learning problems because
when you manipulate your environment, then if you act a certain way, you see situations
that may or may not be present in your training data.
So you know, if I know how to drive well, then I might not get into a car accident or at
least not an obvious one, or I might not get stuck behind a slow driving car very often.
And now, of course, you don't, if you don't see any data for how to recover from those
errors, then, you know, your driving agent is in trouble.
So that's the part I was trying to emphasize that when we try to do supervised learning
in these interactive scenarios, then often our algorithms tend to make mistakes that we
don't have in the training data and then they don't know how to recover from those.
And so they get, you know, stuck in corners and they just fall maybe in pits and do all
sorts of silly things that we just don't do.
Right.
So, walk us through how the approach is to interactive learning that you talked about
addressed that problem.
Yeah, so in some sense, here's an alternative we could think about, right?
So let's say I was thinking about a conversational agent like a chatbot.
So I have two options.
I could look through many, many transcripts of, you know, how people have talked to
each other, maybe in a controlled domain, even like a call center or something.
And I can try to train an agent from this data.
Again, this might have issues that maybe I said something that doesn't make sense to
the user, maybe the respond in a way that a call center human agent would never encounter
and I don't know how to respond now.
Now imagine in this situation, instead of just being stuck and floundering, I could
actually sort of fall back to a human agent and say, hey, I'm kind of uncertain about
what to do right now.
Can you bell me out, please?
Right now I have learned how to recover from this mistake.
So in future, even if I'm about to make this mistake, I'll be able to recover from it,
right?
And so there is this concept of learning in situations that I encounter that interactive
learning algorithms usually embody, right?
And another kind of thing that often comes up is just even, you know, these are like
very ambitious tasks, of course, right?
So let's think about something even which seems much more mundane to us, like, you know,
ranking search results or recommending news articles, personalizing news articles.
So maybe I have data of what users have been clicking on and I learned from this data
and I think I have found something better to choose, right?
How do I evaluate it?
Well, if this other, these other sort of choices are never displayed to the users by my previous
system, I have no data that can actually back that these choices are good, right?
And so we only know the performance of the thing that the user actually clicked on.
We don't have any information about how the other things would have performed.
Exactly.
And so we run into this problem that we don't even know how to evaluate a system that does
something different from what we have in the data and basically interactive learning
exactly tries to break this gap or tries to come up with techniques for essentially how
do you have to kind of learn on the job?
You have to learn on the fly in order to address these situations.
No, it seems like you've talked about two different things.
One is, you know, you've got your machine prediction.
How do we kind of pop out of that loop and get input from the human and the other makes
me think more of like multivariate testing, AB testing, that kind of explore exploit kind
of scenarios.
How are those related?
Yeah.
So in some sense, explore exploit is kind of one step further from this, you know, like
evaluation problem that I'm mentioning.
If you think about exploration, why do you explore, right, you explore because sometimes
you'll, you'll try one thing, sometimes you'll try another thing.
So let's kind of in a statistical sense, if we zoom out and think about what's happening
in expectation, roughly, you're trying everything.
And in expectation, at least sort of as a population level, you're getting feedback
about everything, right, and what, as a result, that enables you to do is it enables you
to evaluate every choice that you could have made, right?
So implicit in explore exploit is this ability to evaluate in interactive settings.
And so what I try to do is I, I mean, explore exploit kind of in my mind consists of two parts.
There is this ability to evaluate, which I think is very crucial in its own because people
currently do often evaluation by doing, like you're saying, A, B testing or multivariate
testing, which is a horribly inefficient way of going about the task.
And explore exploit basically gives you a much more data efficient way of doing the same
thing.
And then it lets you, in fact, do something much more, it lets you actually refine your
models in real time and update them and, you know, do this sort of really online learning,
which is, which is great.
But even if you don't want to do that, just this evaluation is something you can already
get a lot of value out of.
So that's why I kind of try to present both the pieces in their own right.
But in some sense, yes, explore exploit is the is the general sort of overarching solution
that addresses basically both of the issues.
And why is it, why is it in that being so much more data efficient than the traditional
purchase?
Right.
So there are two two important hallmarks.
So so let's, let's first start from a non contextual setting, right?
So let's just say you're trying to basically find the most popular new story that works
for everyone.
Then the a B testing way, so to say of going about it would be, let's say you have 10,
you give one 10th of traffic to each one of them, week later, you see which one did the
best.
Okay.
The explore exploit we are doing is you say, okay, well, initially I'll start giving 10%
traffic to each one.
The moment something starts to look better than the other second, I dynamically adjust
my traffic.
Right.
So everything is getting only enough traffic that I need to rule it out as being inferior,
right?
And so that's already data efficient.
All right.
Now let's think about personalization, about contextualization, right?
So so you said you're from St. Louis, you're hopefully not the only user from St. Louis
who visits MSN.
So so you visit, I, I maybe give you a randomized choice of new story.
And maybe another user who has more or less the same features as you visits, right?
I maybe try a different new story for them.
Now what I've done is, she let's back up.
So so so usually for personalization, let's again contrast with it with an a B test.
So maybe test would say, okay, so, you know, I'll, I'll run this experiment to on, on some
percent of data to evaluate, be on some percent of data to evaluate a, right?
So each data point is either going to a or to be, right?
There is no sort of data sharing happening.
How, when you have a large class of available options, available models that you might want
to pick out of.
Right.
If you've got tons of news, one set of parameters for neural network defining one model.
So you really have an infinite number of them.
But let's just pretend for now, they're a billion, okay?
So of course, for a given user, many of these models would make the same choice, right?
So if you randomize the new story that you present to the user, then and you look the,
look at the outcome, then suddenly you have information about all the different models
that made the story you displayed that made the same recommendation to this user, right?
And you can use, share this user's information across all of them.
So you're effectively training a billion models in parallel?
You're effectively evaluating a billion models you parallel, right?
And then because you're evaluating them in parallel, you can just choose the best one
at the end.
Right.
Right.
Well, you have optimization, right?
So it's this data sharing that's crucial and there is a certain sense, there's a precise
mathematical sense in which you can prove that this is, as a result, if you think about
doing the same evaluation of a billion things through A, B testing versus what we call
multi-world testing, then you require exponentially fewer samples in our approach.
And there is kind of a precise theory in papers backing up that claim.
So that's kind of the crux.
Okay.
Okay.
So we've touched on this throughout the discussion, but one of the big areas that you apply
this on is impersonalization.
Can you talk a little bit about the use case and some of the background there?
Yeah.
So, you know, I think really, I mean, you know, it's 2016, I think it's great that some
of the things we use actually do adapt to our tastes over time, but it's a pity that more
of them don't, right?
And I feel like everything I do should learn about me, especially because they are collecting
a ton of data about me.
So, it might as well put it to some good.
And basically, I think this is, personalization is the one that's most interesting to me also
is because like I was describing, once you start thinking, if you're just trying to
evaluate 10 things, fine, there is some difference between A, B testing and multiple testing, but
it's really when you start thinking about creating personalized, and if you're only doing
10 things, then in some sense, a very smart person might just look at them and by their
gut feeling, pick out the best one, right?
But once you start thinking about personalized models, then first of all, it doesn't scale
for humans to do it.
It doesn't scale for A, B testing to do it.
You really need a different technology to do it, and that's why I think it's a very
well-suited scenario to something like multiple testing and contextual bandits.
And so, that's where we've found the most, also, excitement from, you know, we've
talked to a broad range of product managers, and that's the aspect that usually receives
the most appeal to them.
And what kind of results are you seeing with that?
So with, I mean, I have most substantive results with MSN.
We have a lot of other kind of experiments going on, but I think MSN ones are the one
I can speak of most authoritatively, and basically, they've reliably, so, you know, they
have like this web page with many different A, which they kind of logically partition
into many different areas, so they've kind of applied our system to many different
areas now, and reliably in all of those, they found kind of with minimal to no tuning,
they found always that they were getting actually improvement in most kind of user engagement
metrics that they track.
What was even more interesting, and this kind of really reflects the personalization aspect,
right?
So they had been running these experiments in the US market, and then the Olympics came
around, and somebody had an idea, hey, let's try this in Brazil, right?
So this Portuguese, we've not changed anything in the system, except they, you know, like
the user browsing history, they had it for those users on the Portuguese articles, of course,
and they did like a Portuguese topic model for, so just the feature extraction part was
a little language specific, nothing about the machine learning changed, and we got, you
know, double to triple digit improvement over the existing system, just deploying this
out of the box, and they started running it on 100% traffic, so this, it really does work.
Is that level of improvement on par with what you saw in English, or greater, or slightly
less?
You know, that's kind of difficult to judge very well, because also they have different
amounts of data and different level of performance of the baseline system in different markets,
so it's a bit hard to compare that.
But I think the important thing for them was that it was really with very little customization
things were robustly able to transfer from one market to another, right?
And so this approach, you've talked a little bit about it being more data efficient,
but how does that translate to the actual implementation and computational efficiency?
Yeah, so, I mean, is that an issue for these kinds of problems at that scale, or?
I mean, there, yes, you have to be mindful about computation, we are mindful about computation,
we've tried to make these things as efficient as possible, so with MSN, for instance, we
were running in, you know, the front end of their servers, and we had like five percent
of show performance overhead on their system, which was deemed fine for the value, so there
is obviously some performance that is lost, that is incurred, but I mean, we've implemented
these algorithms very carefully, and a lot of the costs can be amortized essentially.
So now the 5% running on the front end, that's for model evaluation, is there a training
step?
Yeah, so the nice thing is that training step is running asynchronously in the cloud,
and that, I mean, that's entirely, again, I mean, currently, even at MSN, first of all,
one thing that's nice about doing things online is, right, you're not really thinking about
having to deal with a billion or a trillion records all at once, you're just streaming
over them.
So scales become nicer, but, but, so with MSN, we were still able to do all the training
on one machine, in fact, in the background, but even if that's not the case, it's pretty
easy to parallelize the training algorithms, and we support that.
So, yeah, I mean, you can, and we've, the only thing you have to be careful about is, so
we've made sure that, we try to keep the system very reproducible, because one of the frustrating
things we've encountered over and over again is when something goes wrong in these complex
systems, it's really hard to trace downward, with parallelization, sometimes it can be
a little bit more tricky, because all of the order of events and so on, so we recommend
as far as possible to avoid it, but it's definitely possible to do it.
All right, if you can do it for MSN on one machine, then a lot of people will be able to get
pretty far on a single machine.
And you've published papers about this, like, what have someone wanted to, to try out this
approach?
Like, what's the best way to, for them to learn about it?
Yeah, so there is a short URL, aka.ms-mwt, that website has a ton of resources on it, and
it has both, you know, more, like, do it yourself, type, guide, type things, if you directly
want to get your hands dirty, if you want to learn more about the science, it provides
extensive, there's a very extensive white paper that provides links to you in more extensive
research papers, and so on, so really, depending on how much detail you want, all of the resources
are available on that website.
Awesome, awesome.
And the project itself is on GitHub, so, you know, all of the code is open source.
You can play with the machine learning algorithms, the machine learning algorithms actually
have been open source for several years now, so they've been, you know, tried out not
just by us, but by others across the research community as well.
And what are the algorithms based on?
Like, what general classes of algorithms do these look like?
So, I mean, so the way we do things is we take these interactive or, you know, contextual
bandit learning problems, and we basically sort of what the learning algorithms look like,
their massage and massage and massage the data, till you can essentially think of it as
some sort of a multi-class classification problem.
Okay.
And so, now, go to party with your favorite multi-class classification algorithm.
I mean, we implement our own for the complete pipeline, but, and we support a lot of different
types of models, like, you know, linear models, shallow, feed-forward nets.
We have, you know, matrix factorization type models.
We have a whole variety of sort of very, very quick feature manipulations that are ingrained
into the models that you can just do, so it's, so all of this is happening in a software
called Wobbit, the all of the machine learning part, which has been around for several years
and is one of the more performant software tools for machine learning out there.
So, it's, it, it provides a lot of functionality, and if you want something that's not in it,
then, you know, there are ways to plug into other machine learning libraries as well.
And the, your use case there was on personalization, are there other use cases that you've seen
to supply to?
So, I mean, it depends on how far you want to extend personalization, the definition, right?
So, one of the things we are currently trying to work on is, so, so we have users of Microsoft
Bank and, you know, a lot of people in this country suffer from sleep disorders or just, you
know, is the health band?
Yes.
Because of like stress or other reasons, they're just not sleeping well.
And so, the band was for a while trying to give sleep insights, basically some, some
recommendation to change your lifestyle in some small way that might help you sleep better.
And so, for instance, we are trying to now do an experiment where we would choose which
recommendations to show based on how the users sleep then respond to the recommendations
or do this.
So, I mean, I think of this as within the realm of personalization, but, you know, and,
I mean, again, we haven't had conversations or more medical domain so far, but we are
really hoping that we can get there in future.
So, that's definitely one realm, other definitely, definitely a good chunk of the conversations
we've had other than that are, I would say, around personalization of various sorts.
But even like one of the interesting use cases, some of our actually, so some of the people
in our research team are, so because of the system, we also have some systems researchers
on the team, and, you know, system itself does a number of resource allocation choices
and, you know, server is kind of distributing, doing load balancing and a lot of other resource
allocation problems, right?
So, what they've been curious is if they can apply even some of these techniques to core
systems problems.
We have some preliminary experiments with that, nothing I would call convincing yet, but
it's actually pretty broadly applicable.
And by core systems problems, are you thinking things like, you know, allocation of resources
within a data center?
Yeah, you can apply it at several different scales, so you can think about applying it
at the level of a router, at the level of a NIC, at the level of an OS, level of data center
or scale, do you learn in a data center, there are many different places you can think
of it.
And in some sense, I mean, a lot of these are basically currently working on top of hand-designed
rules that some very smart people thought about the problem very carefully and designed
it, right?
But there's no reason why we can't make them more adaptive and more intelligent.
So yeah, I think there is definitely a lot of potential there in like machine learning
for systems type of area for these interactive learning situations.
And so how would you kind of taking a step back to summarize?
How would you characterize at the highest level, you know, if you've got a problem that
looks like X, you know, this is, you know, a solution.
Wonderful question.
I actually do, when I start to talk to people who are interested, I usually give them a template
and ask them if their problem fits into that template, right?
So at the high level, there is this loop of, you observe the world, you take an action
and you observe a reward, right?
So it's important that you face this loop over and over again.
One of the things you have to be careful about, for instance, is often when we start this
conversation, people don't necessarily have a well-defined notion of a reward they can
point to.
And that's very important.
If we don't define it well, the system will just learn some garbage, right?
The other thing that's kind of important to think about is, like I said, contextual bandage
problem, I was saying in the top, makes this assumption that when I take an action, it
does not have influence on the next context I see, right?
So something like a conversation just does not fit this, you know, what you say is not
independent of what I said before.
But something like recommendation systems is largely true.
So that's something you have to keep in mind when you're thinking about how good a fit
this might be to your problem.
Now, that said, we have a lot of research expertise and research advances in also working
out the situations where your actions modify the state is just that they're the software
not quite there yet.
I mean, we have some software, but it's not really a full-fledged system yet.
Okay.
Great.
Great.
Any other considerations or things that folks should know when they're thinking about
this space?
No, I would say, again, think, if you're thinking about these problems, think really hard
about the reward.
That's the one that usually goes wrong.
And the other thing is, I think we've tried very carefully in the various materials we've
prepared to outline all the usual things that go wrong because one of the things we find
is even after we talk to people, they often fall back into those traps, so it's very important
to think through those carefully and make sure you don't fall into them.
And what are some of those traps?
So a lot of those traps are, well, essentially, even, I mean, it's really tempting to say that
I have some observational data collected from my system.
Let's just do some machine learning with it.
And this almost never works in a reliable manner.
And there are various levels at which this manifests.
So one thing you might want to do is, oh, I ran this experiment and actually things are
working quite well.
Now, why don't I turn off the experimentation?
I turn off the randomization.
No.
I mean, you know, preferences change or just various, various subtle bugs arise just due
to the way people are recording things.
So of course, if you do everything with our system, then, you know, we've designed things
in a way that they shouldn't arise, but often people want to use their own custom
components for parts of things.
So if you're thinking about doing that, it's really important that you look into the
failure modes that we emphasize and make sure you don't fall into those.
And the other thing is, yeah, even on top of our system, when you're building something,
it's important to think about the reproducibility of everything, because that's the one thing
we found really was key when even with MSN, right?
It wasn't all sort of a bed of roses initially, there were quite a few hiccups and because
we kept everything reproducible, we could quickly figure out where the problem was.
Right.
Right.
Awesome.
Awesome.
But why don't you repeat that URL once more time?
Yeah.
It is aka.ms slash MWT.
Okay.
And can folks, if they've got questions, can they contact you through that URL?
Yes.
Absolutely.
Awesome.
Awesome.
Well, thanks so much, Alex.
Great to meet you and appreciate the talk.
Yeah.
I'm talking to you.
Thanks.
All right, everyone.
That's our show for today.
Once again, thanks so much for listening and for your continued support.
Don't forget to share your favorite quotes for a twimmel sticker.
These stickers are great.
You're going to love them.
You can share your favorite quote via the show notes page, via Twitter, via our Facebook
page, or via a comment on YouTube or SoundCloud.
And don't forget to hit that iTunes link and leave us a review.
The notes for this show will be up on twimmelai.com slash talk slash 17, where you'll find links
to Alex and the various resources mentioned in the show.
Catch you next time.
